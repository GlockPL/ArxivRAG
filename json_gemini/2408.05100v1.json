{"title": "Al-driven Java Performance Testing:\nBalancing Result Quality with Testing Time", "authors": ["Luca Traini", "Federico Di Menna", "Vittorio Cortellessa"], "abstract": "Performance testing aims at uncovering efficiency issues of software systems. In order to be both effective and practical, the design of a performance test must achieve a reasonable trade-off between result quality and testing time. This becomes particularly challenging in Java context, where the software undergoes a warm-up phase of execution, due to just-in-time compilation. During this phase, performance measurements are subject to severe fluctuations, which may adversely affect quality of performance test results. Both practitioners and researchers have proposed approaches to mitigate this issue. Practitioners typically rely on a fixed number of iterated executions that are used to warm-up the software before starting to collect performance measurements (state-of-practice). Researchers have developed techniques that can dynamically stop warm-up iterations at runtime (state-of-the-art). However, these approaches often provide suboptimal estimates of the warm-up phase, resulting in either insufficient or excessive warm-up iterations, which may degrade result quality or increase testing time. There is still a lack of consensus on how to properly address this problem. Here, we propose and study an AI-based framework to dynamically halt warm-up iterations at runtime. Specifically, our framework leverages recent advances in AI for Time Series Classification (TSC) to predict the end of the warm-up phase during test execution. We conduct experiments by training three different TSC models on half a million of measurement segments obtained from JMH microbenchmark executions. We find that our framework significantly improves the accuracy of the warm-up estimates provided by state-of-practice and state-of-the-art methods. This higher estimation accuracy results in a net improvement in either result quality or testing time for up to +35.3% of the microbenchmarks. Our study highlights that integrating AI to dynamically estimate the end of the warm-up phase can enhance the cost-effectiveness of Java performance testing.", "sections": [{"title": "1 INTRODUCTION", "content": "Software performance is a critical non-functional aspect of software systems. Technology organizations use performance testing to uncover performance bugs that might deteriorate software efficiency. Nevertheless, performance testing requires careful design in order to be effective. A significant challenge is to design an adequate number of execution repetitions that mitigate the variability of performance measurements [12, 43, 45]. This typically involves balancing the need for quality of performance test results against the practical constraints of resources and testing time [2, 3, 10, 23, 31, 36].\nFinding this balance becomes particularly difficult in the Java context, where software execution undergoes an initial warm-up phase due to just-in-time compilation [6]. During this phase, the Java Virtual Machine (JVM) performs a wide range of optimizations, leading to fluctuations in performance behavior (as shown in Fig. 1) that may adversely affect result quality [22, 52]. To mitigate this issue, it is common practice to conduct a number of \"warm-up iterations\", with the sole goal of warming up the JVM, before starting to collect performance measurements.\nAn insufficient number of warm-up iterations might compromise the results quality, thus misleading performance evaluation. On the other hand, unnecessary warm-up iterations increase testing time, thus hindering the adoption of performance testing in practice [18, 29, 51]. An accurate estimation of the warm-up phase is paramount to achieve cost-effective performance testing.\nSoftware engineers typically defines a fixed number of warm-up iterations based on their domain expertise [36]. However, studies show that using a fixed number of iterations may misrepresent"}, {"title": "2 BACKGROUND", "content": "In this section, we discuss the background related to our study, including Java microbenchmarking and Time Series Classification."}, {"title": "2.1 Java Microbenchmarking", "content": "Microbenchmarking is a type of unit-level performance testing, commonly used in Java context [39]. A microbenchmark repeatedly executes a small portion of code, such as a Java method, while gathering measurements of its execution time. Due to just-in-time compilation, microbenchmarks are subject to performance variability in the first phase of their execution, also known as warm-up. During this phase, the JVM detects frequently executed loops or methods, and it dynamically compiles them into optimized machine code. After the completion of the warm-up phase, the microbenchmark is said to be executing at a steady-state of performance[6].\nSoftware engineers employ warm-up iterations to ensure that the microbenchmark achieves a steady-state before starting to collect measurements. As shown in Figure 2, executing an appropriate number of the warm-up iterations is paramount to achieve cost-effective performance testing. Insufficient number of warm-up iterations may produce measurements that do not reflect the software's true steady-state performance, thus detrimentally affecting result quality. Conversely, excessive warm-up iterations lead to unnecessary executions that prolong the testing time.\nState-of-practice (SOP). Practitioners usually predefine a fixed number of warm-up iterations based on their domain expertise. To configure warm-up iterations, they typically rely on Java Microbenchmark Harness (JMH)2, the de-facto standard for building, configuring, and running Java microbenchmarks. JMH allows to configure the number of warm-up iterations directly in the microbenchmark's source code, using Java annotations, as shown in Listing 1. Nevertheless, prior work has shown that using a fixed number of iterations may often produce suboptimal estimates of the warm-up phase [22, 36, 52]."}, {"title": "2.2 Time Series Classification", "content": "Time Series Classification (TSC) involves assigning predefined labels to time-ordered sequences of data points based on their characteristics or patterns. TSC problems arise in many domains, such as human activity recognition [8], e-health [47], natural disasters [4], and finance [42]. Due to its broad applicability, hundreds of TSC algorithms have been proposed over the last decades [5, 44]. These algorithms span various categories, including distance-based [41], dictionary-based [48], convolutional-based [15, 49], or deep learning approaches [27]. Recently, the latter two categories have demonstrated notable advancements, enabling them to attain state-of-the-art performance on established TSC benchmarks [19, 27, 44, 50], such as the UCR archive [13]. In this work, we study the efficacy of three state-of-the-art TSC algorithms to dynamically stop warm-up iterations of microbenchmarks. Specifically, we investigate one convolutional-based algorithm, namely ROCKET [15], and two deep learning models, namely FCN [56] and OSCNN [50]."}, {"title": "3 METHODOLOGY", "content": "In this section, we present the methodology of our AI-based framework. We first introduce an overview of the main phases of our framework and then discuss the details of each phase."}, {"title": "3.1 Overview", "content": "As shown in Fig. 3, our framework involves three main phases: Data Preprocessing, Model Training, and Application. The first two phases are executed offline, the last one during performance test execution.\nDuring the Data Preprocessing phase, our framework begins by processing a time series of performance measurements with a known warm-up end. This process involves segmenting the time series and labeling each segment as stable or unstable based on"}, {"title": "3.2 Data Preprocessing", "content": "This phase aims at preparing the dataset that will be used for supervised learning. To achieve this, our framework starts from a time series of performance measurements, and an annotation that denotes the end of the warm-up phase. The time series represents the observed execution time over sequential iterations of a JMH microbenchmark. The annotation marks the specific iteration ($s_t$) in which the steady-state of performance is reached.\nIn this study, we rely on the notion of steady-state defined by Barrett et al. [6]. This notion leverages change point detection, namely PELT [33], to determine statistically significant shifts in the time series, and to identify the end of the warm-up phase. Barrett et al.'s approach cannot be used at run-time to determine warm-up, since it requires to first run the microbenchmark for an unrealistically large number of iterations, and, subsequently determine the end of the warm-up phase through post-hoc analysis. However, we aim to leverage this approach to build a solid ground-truth for our AI-based framework.\nTo construct our dataset, we sample smaller (overlapping) segments of fixed size from the original time series, with each segment representing a contiguous block of performance measurements. We then classify these segments with binary labels:\n\u2022 Stable: The segment consists solely of measurements from the steady-state, meaning all measurements were taken after the steady-state $s_t$ was reached.\n\u2022 Unstable: The segment includes at least one measurement from the warm-up phase, indicating that some measurements were taken before reaching $s_t$.\nFig. 3 illustrates a simplified representation of this process.\nOur dataset construction involves multiple time series gathered from various microbenchmarks across different software systems. This process yields a heterogeneous dataset of measurement segments that capture diverse performance patterns from a range of software systems."}, {"title": "3.3 Model training", "content": "Our learning goal implies the binary classification of segments of performance measurements. We leverage Time Series Classification algorithms to classify each segment as either stable or unstable. Specifically, we study three different TSC algorithms, which we discuss below:\nFully Convolutional Network (FCN) was proposed by Wang et al. [56] for classifying univariate time series. This neural network architecture acts as feature extractor stacking three convolutional layers [21], each followed by a batch normalization layer [26] and ReLU activation layer [20]. The features are then processed through a global average pooling layer [40] and finally passed into a softmax classifier to obtain the final output label. Prior work has shown that this architecture can achieve state-of-the-art performance in TSC [27].\nOmni-Scale Convolutional Network (OSCNN) introduces the Omni-Scale block [50], wherein the kernel sizes for 1-dimensional convolutional neural networks (1D-CNNs) are automatically set through a simple and universal rule. This approach enables capturing an optimal receptive field size, that is a factor that significantly influences the performance of 1D-CNNs for TSC [11].\nRandom Convolutional Kernel Transform (ROCKET) is a pipeline classifier [15]. It generates a large number of randomly parameterized convolutional kernels and uses these to transform the data through two pooling operations: max value and the proportion of positive values. These two features are concatenated into a feature vector for all kernels. The feature vectors are then used to train a Ridge Classifier [25] using cross-validation. Unlike convolutional neural networks, ROCKET does not involve any hidden layers, thereby providing state-of-the-art performance with limited computational cost [15, 44, 50]."}, {"title": "3.4 Application", "content": "Our framework employs TSC models to dynamically predict when the microbenchmark reaches a steady-state of performance. As the microbenchmark execution progresses, the framework continuously evaluates the incoming performance measurements using the TSC model. Once the model detects the achievement of the steady-state, the framework promptly halts the microbenchmark execution and returns the current set of measurements for performance evaluation. Fig. 3 presents a snapshot of this procedure using an illustrative scenario. In this scenario, the microbenchmark has completed 3 warm-up iterations, with the current measurement window ranging from the 4th to the 7th iteration. Before proceeding, our framework queries the TSC model, forwarding the current set of measurements to determine if they correspond to a steady-state of execution. If the model predicts that the measurements are stable, the microbenchmark execution is halted, and these measurements are returned for performance evaluation. Conversely, if the model classifies them as unstable, then the framework makes the test running another iteration, and it shifts the measurement window by one iteration. This process repeats at each iteration until the model identifies a stable segment (or until a predefined maximum number of iterations is achieved).\nThe key insight behind our framework is that if the employed TSC models accurately classify stable and unstable measurements, then the framework will automatically halt a microbenchmark execution immediately after the warm-up phase ends, thereby providing high-quality performance results with the minimal required testing time."}, {"title": "4 EXPERIMENTAL SETUP", "content": "In this section, we outline the experimental procedure, describe the dataset and evaluation metrics employed in our study, and detail the implementation of our framework."}, {"title": "4.1 Dataset of JMH Performance Measurements", "content": "We utilize the dataset by Traini et al. [52], which includes performance measurements from 586 JMH microbenchmarks across 30 Java software systems (refer to Table 1 for an overview of these systems). To our knowledge, this is the most extensive publicly available dataset of JMH performance measurements. The dataset includes 10 time series of performance measurements per microbenchmark, resulting in a total of 5,860 time series. Each time series comprises performance measurements gathered from 3,000 consecutive microbenchmark iterations performed within a fresh JVM instantiation (often referred to as a fork in JMH nomenclature). Each data point in the time series reports the average execution time observed within the microbenchmark iteration. In addition, each time series is annotated with the $s_t$ iteration number at which a steady-state was attained, based on the Barrett et al.'s technique [6].\nNote that this high number of JMH iterations would be impractical in a real-world scenario due to the extensive testing time"}, {"title": "4.2 Experimental procedure", "content": "Dataset Preprocessing. The initial phase of our experimental procedure involves generating the dataset for training and evaluating the TSC models. To ensure the effectiveness of the learning process, time series that do not reach a steady-state are excluded, leading to the omission of 10.9% of the time series.\nOur dataset construction is based on three main criteria: (i) ensure equal representation of segments from different time series, so no single series dominates the dataset, (ii) achieve a reasonable balance between stable and unstable segments, and (iii) ensure an adequate coverage of the time series while minimizing measurement redundancy.\nTo meet the first criterion, we sample 100 segments of 100 measurements each from each time series, following a measurement window similar to that used in prior work [36]. This ensures equal representation of different time series segments in the dataset. To satisfy the second criterion, we sample 50 stable and 50 unstable segments from each time series, ensuring a balance between the two classes within the dataset. For the third criterion, we use a time series segmentation with adaptive step size to limit measurement redundancy across segments. This strategy is applied independently to both the warm-up and steady-state phases of the time series. Specifically, the segments are selected equidistantly, ensuring that the starting points of the segments are evenly spaced. Specifically, the window segmentation for the 50 unstable segments during the warm-up phase uses an adaptive step size of $step = \\lfloor(s_t - 1)/50\\rfloor$ where $s_t$ denotes the iteration where the microbenchmark reaches the steady-state. Similarly, for the steady-state phase, the step size is computed as $step = \\lfloor(n \u2212 s_t)/50\\rfloor$, where $n$ denote the length of the time series, namely 3,000. This approach ensures reasonable coverage of the time series while reducing potential measurement redundancy due to overlapping segments.\nAs a result, we obtain a final dataset of 521,900 measurement segments, including 376,925 (72%) stable and 144,975 (28%) unstable segments.\nModel Training. We adopt a 5-fold cross-validation process for each of the three TSC models. The dataset is divided into five folds of equal size. For each fold, the model is trained on four folds and tested on the remaining one. This process is repeated 5 times, with each fold being used exactly once as the test set. To prevent data leakage, we ensure that measurement segments gathered from the same microbenchmark always appear within the same fold. This guarantees an unbiased evaluation setup, where the model is tested against measurements gathered from unseen microbenchmarks. Furthermore, to increase the representativeness and heterogeneity of each fold, we ensure that each fold has a similar proportion of microbenchmarks per project, using stratified random sampling. The entire cross-validation training process for all the three TSC models required approximately 2 days to complete.\nApplication. To evaluate our framework, we employ a methodology similar to prior work [36]. Specifically, we mimic the application of the framework during microbenchmark execution through post-hoc analysis. We use a sliding measurement window of 100 performance measurements on the 5,860 time series of measurements from the dataset of JMH performance measurements [52]. For a given time series (i.e., microbenchmark fork), the process begins with a segment containing the first consecutive 100 performance measurements. The framework submits this segment to the TSC model to determine if the measurements are classified as stable or not. If the TSC model classifies the measurements as stable, then the framework stops the process and returns the measurements M for performance evaluation. Conversely, if the model deems the measurements unstable, then the framework shifts the sliding window by one iteration, simulating the execution of an additional warm-up iteration. This process is repeated until the TSC model classifies the measurements as stable. Similar to prior work [36], we set an upper limit of 500 warm-up iterations. If this limit is reached, the process stops automatically, and the current set of measurements M is returned for evaluation.\nAs a result of this process, for each microbenchmark fork, we obtain the number of warm-up iterations estimated by the framework, along with the corresponding set of measurements M provided for performance evaluation.\nSimilar to the model training phase, when evaluating the framework on a microbenchmark, we consistently use the model trained"}, {"title": "4.3 Evaluation metrics", "content": "We use different metrics to evaluate the prediction accuracy of TSC models:\n\u2022 Precision represents the ratio of true positive outcomes to the total positive predictions made by the model. In our context, the positives are stable measurement segments.\n\u2022 Recall indicates the ratio of true positive outcomes to the total actual positives in the dataset.\n\u2022 F1-score is the harmonic mean of precision and recall, providing a single measure that balances both concerns.\n\u2022 Balanced Accuracy is the average recall obtained across each of the two classes. We use this metric instead of traditional accuracy due to the imbalanced nature of our dataset.\nThe following metrics are used for evaluating the application of our framework:\n\u2022 Warm-up Estimation Error (WEE) measures the accuracy of the warm-up iterations estimated by the framework. Specifically, it represents the absolute difference, in seconds, between the actual steady-state ($s_t$) and the end of the warm-up phase as estimated by our framework. This metric helps us understand how closely the warm-up iterations provided by our framework align with the actual warm-up phase of the microbenchmark.\n\u2022 Measurement Deviation evaluates the quality of the performance test results provided by the framework. For a given microbenchmark, this metric compares the set of performance measurements M returned by the framework to the ground-truth steady-state measurements $M^*$. The set $M^*$ consists of all measurements gathered from the steady-state iteration $s_t$ onwards, for every time series of a particular microbenchmark. A large deviation between M and $M^*$ indicates poor result quality. To assess this deviation, we adhere to performance engineering best practices [28, 36, 53, 59] by calculating the confidence interval for the execution time ratio [31]. Specifically, we employ the bootstrap method [14, 30] with 10,000 iterations [24], using hierarchical random resampling with replacement at two levels [30], and a significance level of \u03b1 = 0.05. If the confidence interval for the ratio includes 1, there is no statistically significant difference between M and $M^*$. Conversely, if the interval does not include 1, it suggests that M does not reflect the observed steady-state performance and could therefore mislead performance evaluation. For instance, a confidence interval of (1.04, 1.06) indicates that M statistically differ from $M^*$ by 5% \u00b1 1% with 95% confidence.\n\u2022 Testing Time represents the total duration, in seconds, required to execute the microbenchmark using our framework. This duration includes both the warm-up iterations and the subsequent iterations used to collect performance measurements."}, {"title": "4.4 Implementation details", "content": "To facilitate the convergence of TSC models, we standardize each performance measurement $x$ within each segment of the training dataset using the formula $(x \u2013 \u03bc)/\u03c3$, where \u03bc is the segment mean and \u03c3 is the segment standard deviation. We implement each of the TSC model as follows:\n\u2022 FCN: We reimplement the neural network using Tensor-Flow5. Our implementation follows the hyperparameters specified in the original paper [56]. Specifically, we use three 1-D kernels with sizes {8, 5, 3} and three convolution blocks with filter sizes {128, 256, 128}.\n\u2022 OSCNN: We reuse the original implementation provided in the replication package of Tang et al. [50], which uses Py-Torch. We maintained the default hyperparameter settings defined by the authors.\n\u2022 ROCKET: We leverage the implementation provided by aeon7. We set the number of kernels to 500.\nWhen training the neural networks (i.e., FCN and OSCNN), we set aside 25% of the training set as a validation set. Both FCN and OSCNN are trained using the Adam optimizer [34] with a learning rate of 0.001, \u03b21=0.9, \u03b22=0.999, and e=1e-8. If the validation loss does not improve after 20 epochs, we halve the learning rate. We train the neural networks for up to 500 epochs, employing an early stopping strategy that halts training if the validation loss does not improve for 50 consecutive epochs. Throughout the training process, we save the model weights that yield the best validation loss. These optimized weights are then used for evaluation.\nSince both FCN and OSCNN return probabilities rather than class labels, we tune the decision threshold to optimize Youden's index [58] on the validation set, rather than relying on a traditional 0.5 threshold. This tuning is not applicable to ROCKET, as it directly returns class labels.\nThe ROCKET training process does not use an explicit validation set, however, the implementation provided by aeon internally utilizes leave-one-out cross-validation for training the Ridge classifier.\nFor all the TSC models, we use a batch size of 1,024."}, {"title": "5 RESULTS", "content": "In this section, we discuss the study results by posing and answering three research questions."}, {"title": "5.1 RQ1: \u03a4\u03bf what extent can TSC models\naccurately classify stable and unstable\nmeasurements?", "content": "Motivation. We aim to evaluate the capabilities of TSC models in classifying measurements gathered from the steady-state and warm-up phases of microbenchmark execution. These results provide initial insights into the potential suitability of TSC models for dynamically halting warm-up iterations.\nApproach. For each of the three TSC models, we compute the average precision, recall, F1-score and balanced accuracy across the five folds.\nResults. Table 2 shows the results of the TSC models. Overall, we find that TSC models demonstrate good prediction accuracy in classifying stable and unstable segments, with F1-scores ranging from 0.748 to 0.867 and balanced accuracy between 0.682 and 0.712. ROCKET stands out as the best-performing model in terms of F1-score, while neural network models achieve higher balanced accuracy. The lower F1-scores of FCN and OSCNN are primarily due to their lower recall rates, which are 0.659 and 0.65, respectively. Conversely, ROCKET shows a notably high recall on stable segments (0.932) but has a balanced accuracy of only 0.682, which indicates a limited recall on unstable segments (~0.43). This behavior can be attributed to the higher number of false positives predicted by ROCKET, which is also reflected in its lower precision scores compared to FCN and OSCNN (0.81 vs. 0.88 and 0.886). It is important to note that false positives can be quite detrimental to our framework, as they may erroneously stop the microbenchmark execution, with no way to recover from the false prediction. In contrast, false negatives are less problematic since they do not halt execution, thus giving the framework another chance to correctly classify the stable measurements in the subsequent iteration. Nonetheless, in the following research questions, we will examine how the prediction accuracy of different models influences the overall effectiveness of our framework.\nAnswer to RQ1: TSC models effectively classify stable and\nunstable measurements, so demonstrating their suitability for\ndynamically halting warm-up iterations. This supports their\nintegration into our framework."}, {"title": "5.2 RQ2: How does our AI-based framework\ncompare to the state-of-practice (SOP) in\nJava microbenchmarking?", "content": "Motivation. Developers typically use a fixed number of warm-up iterations that are defined beforehand based on their domain expertise. With this research question, we aim to understand if the use of our framework for dynamically halting warm-up iterations provides advantages over the SOP.\nApproach. We extract the number of warm-up iterations specified by the developers for each microbenchmark using a method similar to [52]. We then compare the WEE of our framework with that of the SOP for each time series that reaches a steady-state. For this comparison, we employ the Wilcoxon signed-rank test [57] along with two measures of effect size: the Vargha-Delaney A12 [54] and the matched pairs rank biserial correlation r [32]. In our context, the $A_{12}$ measures the proportion of pairs where the WEE of our framework is lower than that of the developers. An $A_{12}$ value > 0.5 indicates that our framework provides a more accurate estimation of the warm-up phase than the SOP. The matched pairs rank biserial correlation $r$ represents the difference between the proportion of favorable and unfavorable evidence; in our case, favorable evidence indicates a lower WEE for the framework. Thus, an $r$ > 0 indicates that our framework performs better than the SOP.\nIn addition to evaluating the accuracy of the warm-up estimation, we examine how the adoption of the framework affects the quality of performance test results and the testing time for each microbenchmark. To evaluate the impact of our framework on result quality, we calculate the percentage of microbenchmarks that show an improvement (or regression) compared to the SOP. For a given microbenchmark, an improvement in result quality occurs under the following two conditions: (i) the performance measurements from SOP ($M_{SOP}$) are statistically different from those of the steady-state ($M^*$) (i.e., the confidence interval for the execution time ratio does not include 1), and (ii) the performance measurements provided by the framework M are not statistically different from $M^*$ (i.e., the confidence interval includes 1). This indicates that the framework improves result quality, by providing measurements that more faithfully represent the microbenchmark true steady-state performance. A result quality regression indicates the opposite situation.\nWe also report the percentage of microbenchmarks where the framework shows improvement (or regression) in terms of testing time. A microbenchmark is considered improved if the framework reduces its testing time when compared to SOP. Note that we only consider testing time improvements (or regressions) in cases where the measurements provided by both the framework and the SOP are not statistically different from the steady-state. This ensures that a reduction in testing time is not mistakenly interpreted as an improvement if it leads to poor result quality.\nTo achieve a fair comparison, we derive the set of performance measurements M returned by the framework, as well as the microbenchmark testing time, using the same number of measurement iterations and JMH forks specified by the developers (more details on this process are available in our replication package).\nResults. We discuss the results of this RQ from different aspects.\nWarm-up estimation accuracy. Table 3 shows the results of the comparison between the WEE of the framework and the SOP. We observe that the framework delivers more accurate estimates of the warm-up phase across all employed TSC models. The results of the Wilcoxon test show that the differences are statistically significant (p < 0.001) for all models, with medium effect sizes ($A_{12}$ ranging from 0.658 to 0.683). Additionally, the matched pairs rank biserial correlation results emphasize a considerable gap between the favorable and unfavorable outcomes, with positive $r$ values ranging"}, {"title": "5.3 RQ3: How does our AI-based framework\ncompare to the state-of-the-art (SOTA) in\nJava microbenchmarking?", "content": "Motivation. This research question aims to compare our framework against prior SOTA approaches that dynamically halt warm-up iterations. Our objective is to assess whether the use of sophisticated TSC models provides benefits over traditional dynamic approaches.\nApproach. We compare the WEE of our framework to the dynamic technique proposed by Laaber et al. [36], by evaluating all three variants of their technique: COV, RCIW, and KLD. To determine the warm-up iterations for each variant, we use the original replication package provided by the authors [36]. As in RQ2, we employ the Wilcoxon signed-rank test [57], the Vargha-Delaney A12, and the matched pairs rank biserial correlation r to assess whether our framework provides more accurate estimates of the warm-up phase. Additionally, we report the percentages of improvements and regressions in both results quality and testing time. For an unbiased comparison, we derive the set of performance measurements, M, and the testing time for our framework using the same measurement window defined by Laaber et al. (i.e., 100 measurement iterations in our setup) and the same number of JMH forks (the number of forks is dynamically estimated for each microbenchmark in Laaber et al.'s technique).\nResults. Similarly to RQ2, we discuss the results of this RQ from various perspectives.\nWarm-up estimation accuracy. Table 6 shows the results of the comparison between the WEE provided by the framework and those of SOTA. We find that the framework provides more accurate estimations of the warm-up phase across all the models than all SOTA variants ones. The differences are statistically significant (p < 0.001) for all comparisons, based on the results of the Wilcoxon test. The $A_{12}$ values range from 0.621 to 0.731, indicating a small to large effect size according to the thresholds defined by Vargha and Delaney [54]. The rank biserial correlation $r$ ranges from 0.157 to 0.414, suggesting a prevalence for the favorable outcome.\nResult quality. Table 7 presents the percentages of improvement and regression in result quality and testing time. We observe that ROCKET performs worse than SOTA approaches in terms of result quality. Specifically, ROCKET produces regressions in 24.6%, 51.4%, and 22.5% of the microbenchmarks when compared to CV, RCIW, and KLD, respectively. Moreover, it reports improvements in only 11.3%, 4.1%, and 7.2% of the cases. This limitation might once again be attributed to the higher false positive rate produced by this model. In contrast, we observe that neural network models generally perform better than SOTA in terms of result quality, with the only exception being RCIW. For instance, OSCNN improves the result quality of CV and KLD in 28.8% and 27.8% of microbenchmarks, respectively, and causes regressions in 12.3% and 12.1% of them. Moreover, OSCNN produces lower relative measurement deviations than CV and KLD, as shown in Table 5. The median (IQR) deviation for OSCNN is 5.5% (2.4-12.3%), while CV and KLD induce deviations of 9.9% (4.7-17.5%) and 7.9% (4.2-13.4%), respectively. We observe a similar trend of improvements in FCN, albeit with slightly less pronounced results.\nThe framework provides lower result quality than RCIW across all TSC models, causing regressions in 24.2% (FCN), 21.8% (OSCNN), and 51.4% (ROCKET) of the microbenchmarks. This behavior can be attributed to the RCIW's higher tendency to overestimate the warm-up phase, observed in 74.1% of the cases (see Fig. 4), which increases the chance of gathering steady-state measurements. While this tendency ensures better result quality compared to our framework, it also leads to longer testing times.\nTesting time. As shown in Table 5, RCIW reports a median (IQR) testing time of 300 (265-301) seconds, while the most time-consuming TSC model of our framework, OSCNN, produces a median (IQR) testing time of 79 (47-161) seconds, which is approximately one-quarter of RCIW testing time. Furthermore, from Table 7, we observe that FCN and OSCNN can improve the testing time for about half of the microbenchmarks (50.3% and 52.6%, respectively) without affecting result quality. When compared to the other two SOTA variants, CV and KLD, our framework still shows improvements in testing time. For example, OSCNN reduces the testing time in 26.8% and 23% of the microbenchmarks, respectively.\nOverall. We find that, when employing neural network models such as FCN and OSCNN, our framework provides improvements over the SOTA in either result quality or testing time in approximately half of the microbenchmarks, with percentages ranging from 50.3% to 59.6%. The percentages of regressions are lower, with values ranging from 20% to 28.8%, thus resulting in substantial net"}, {"title": "6 THREATS TO VALIDITY", "content": "Construct validity. We derive the number of warm-up iterations through post-hoc analysis using a methodology similar to [36, 52]. This evaluation methodology does not account for the overhead introduced by the TSC model prediction. However, we do not consider this overhead for both our framework and the SOTA techniques when calculating the testing time, whereas SOP does not involve any overhead, thus ensuring a fair comparison. Moreover, to further mitigate this threat, we measured the inference time of each TSC model on a sample of 500 measurement segments, and we obtained a median overhead per microbenchmark iteration of 9% for FCN, 2% for OSCNN, and 2% for ROCKET. These overheads appear minimal when compared to the testing time differences observed in Table 5. Therefore, they are unlikely to impact our main findings.\nWe rely on the notion of steady-state as defined by Barrett et al. [6], whereas using an alternative notion may change the study outcomes. We integrate three state-of-the-art TSC models into our framework, using different models may yield different results.\nInternal validity. TSC model training involves randomness, hence there might be slight differences in the results when re-executing the experiments. We employ TSC models by using default hyperparameters, whereas using different hyper-parameters may change the experiments outcomes.\nExternal validity. The findings of our study may not generalize to microbenchmarks beyond our experiment dataset. However, we evaluate our framework on 583 microbenchmarks from 30 well-established OSS projects spanning various domains (see Table 1). Furthermore, the number of systems involved in our study is larger than the ones considered in most recent software performance studies [7, 16, 28, 36, 37]."}, {"title": "7 RELATED WORK", "content": "Besides the works discussed in Section 2, other techniques have been proposed to estimate the attainment of steady-state in performance tests. Kalibera and Jones [31"}]}