{"title": "TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents", "authors": ["Kunal Singh", "Shreyas Singh", "Mukund Khanna"], "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have led to the emergence of LVLM-based Graphical User Interface (GUI) agents developed under various paradigms. Training-based approaches, such as CogAgent and SeeClick, suffer from poor cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, utilize Set-of-Marks (SoM) for action grounding; however, obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Additionally, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL's superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension.", "sections": [{"title": "1. Introduction", "content": "Developing AI agents capable of operating digital devices through natural language commands has been a longstanding research goal (Shi et al., 2017; Liu et al., 2018; Gur et al., 2018). These agents can enhance productivity by automating tasks through Graphical User Interface (GUI). Early studies explored simplified settings (Shi et al., 2017; Liu et al., 2018; Gur et al., 2018), while later efforts (Li et al., 2020a; Wang et al., 2021; Li et al., 2020b; He et al., 2020; Bai et al., 2021; Wu et al., 2021; Zhang et al., 2021; Chen et al., 2020b;a; Li et al., 2020a) leveraged GUI understanding to build more sophisticated agents. Recent approaches (Yao et al., 2022; Gur et al., 2023; Deng et al., 2023; Zhou et al., 2023; Sridhar et al., 2023) incorporate LLMs alongside structured GUI representations (e.g., HTML, DOM trees, View Hierarchy) to enhance comprehension.\nWith advances in LVLMs, studies (Zheng et al., 2024; Deng et al., 2023; He et al., 2024; Zhang et al., 2023; Furuta et al., 2023) have integrated visual perception to improve performance on benchmarks like Mind2Web (Deng et al., 2023) and WebArena (Zhou et al., 2023). However, these models struggle with visual grounding (Yang et al., 2023), relying heavily on structured metadata, which is often unavailable, noisy, or misaligned. SeeAct (Zheng et al., 2024) improves action grounding in GPT-4V (OpenAI, June, 2024a) via set-of-marks (SoM) (Yang et al., 2023), but its dependency on structured data introduces limitations."}, {"title": "1.1. Related Works & Motivation", "content": "Recent research has focused on developing agents that rely solely on visual perception to interact with GUIs in a human-like manner. These works on purely vision-based GUI agents using LVLMs have evolved along 2 main approaches:\nEnd to End Training based GUI Agents: Multiple studies (Hong et al., 2023; You et al., 2024; Cheng et al., 2024; Bai et al., 2024; Shaw et al., 2023) have trained LVLMs on GUI navigation tasks for various platforms/device-types."}, {"title": "1.2. Contribution", "content": "To address these challenges, we introduce TRISHUL, a training-free, agentic framework for comprehensive GUI screen understanding. TRISHUL equips LVLMs with the capabilities required to perform diverse GUI interaction tasks, it utilizes foundational models to parse and build a rich hierarchical understanding of the GUI screens, to enhance their action grounding and GUI referring capabilities.\nHierarchical Screen Parsing (HSP): The HSP module organizes GUI elements across two distinct levels of granular-"}, {"title": "2. Methodology", "content": "This section outlines the design of our training-free screen comprehension modules, HSP and SEED, and sheds light on their integration into our action grounding and GUI referring agent."}, {"title": "2.1. Hierarchical Screen Parsing", "content": "The hierarchical screen parsing process is formalized in Algorithm 1. Initially, the screen image I is passed through SAM (Kirillov et al., 2023) and EasyOCR (JaidedAI). The generated bounding boxes are filtered based on predefined area thresholds $A_{thresh-GROI}$ and $A_{thresh-LE}$ to generate GROI candidates and Local Elements (LE). Local Elements collectively refer to bounding boxes for text, icon, buttons and images in the GUI. We then apply an overlap removal and filtering function to refine the icon and text bounding boxes by removing redundant and unwanted local elements.\nFor each GROI candidate, the number of boxes inside and intersecting with the GROI is calculated. An Information Score S is then computed for each candidate based on the ratio of the number of bounding boxes inside, to the area of the GROI, adjusted by the number of intersecting boxes. This score provides a measure of the GROI's information content, helping the system to prioritize larger and more informative regions for inclusion in the hierarchical tree."}, {"title": "2.2. SEED: Spatially Enhanced Element Description Generation", "content": "Accurately describing the functionality of local GUI elements is essential for effective understanding of GUI and action grounding. Relying solely on visual appearance is unreliable since identical icons can serve different purposes in different contexts, and distinct icons may represent similar functions, leading to ambiguity. Textual and semantic cues around GUI elements help clarify functionality. Pairing icons with nearby text enables precise descriptions, while semantic associations (e.g., text linked to input fields or buttons) aid in identifying actionable elements.\nWe introduce SEED (Spatially Enhanced Element Description), a prompting framework that employs Chain of Thought (CoT) (Wei et al., 2023) and In-Context Learning (ICL) (Brown et al., 2020) to generate spatially and semantically informed functional descriptions for all GUI elements. SEED processes an image I annotated with SoM-style ID tags, and a prompt with bounding boxes for detected elements (via our HSP module), and OCR-extracted text descriptors:"}, {"title": "2.3. Agentic Formulation of Action Grounding", "content": "This section explains how the hierarchical nature of GUIs is leveraged for enhanced SoM style action grounding in LVLMs as explained in fig. 2. Given an image I with Global Regions of Interest (GROIs) G, bounding boxes for icons $B_{icon}$ and text $B_{text}$, OCR-derived text descriptors $d_j$, and an instruction $I_s$, the task is to identify the bounding box B"}, {"title": "2.4. Agentic Formulation of GUI referring task", "content": "In this section we describe how the hierarchical screen parsing module can be leveraged to increase the ability of LVLMs on the GUI referring task as explained in fig. 3. Given the input GUI screenshot I, the task involves describing the content and layout of any point $P_i$ on the screen as input by a user, we use the input screenshot to detect all local elements and corresponding GROI candidates. We then identify the bounding box of the local element containing the selected point, and then the GROI encompassing this local element. Following the prompting approach of the ToL agent in (Fan et al., 2024), we curate two \"lenses\" or images to illustrate this hierarchy. The first lens consists of only the GROI region cropped from the original image, highlighting the local element with a labeled bounding box and marking the input point. The second lens shows the complete screenshot, highlighting the GROI with a labeled bounding box. Both lenses, along with the point coordinate $P_i$ and input prompt, are sent to an LVLM, to generate the content description $D_c$ and the layout description $D_l$."}, {"title": "3. Experiments", "content": ""}, {"title": "3.1. ScreenSpot and VisualWebBench", "content": "Dataset and Experiments- We evaluate the action grounding capability of TRISHUL agent on the ScreenSpot (Jurmu et al., 2008) dataset. ScreenSpot consists of 610 interface screenshots from mobile (iOS, Android), desktop (macOS, Windows), and web platforms, paired with 1,276 task instructions corresponding to actionable GUI elements. Traditional training-based methods, which are often trained on datasets like Screenspot, tend to perform poorly on out-of-distribution samples such as those from VisualWebBench due to domain shift. Therefore, to assess the generalization capability of our approach, we also utilize the VisualWebBench (Liu et al., 2024) dataset's action grounding subset, which consists of 103 pairs of images and their corresponding instruction.\nImplementation Details: The formulation of the action grounding tasks for the datasets used in our experiments is discussed in detail in Section 2.3. The specific prompts employed for these tasks are provided in the Appendix (Figure 12).\nUnfortunately, we were unable to replicate the results reported by OmniParser in their study on the ScreenSpot benchmark using the publicly available weights and codebase. In Table 2, we present the performance metrics for OmniParser as obtained from our own experiments on the ScreenSpot and VisualWebBench datasets. Due to the non-reproducibility of their results as observed above and limited resources, we were unable to verify their results on the AiTW and Mind2Web benchmarks hence we have chosen"}, {"title": "Evaluation and Results:", "content": "As shown in Table 2, the TRISHUL agent, when paired with LVLMs (GPT-4V (OpenAI, June, 2024a) and GPT-40 (OpenAI, June, 2024b)), significantly outperforms the baseline GPT-4V and GPT-40. Our approach also surpasses task-specific models such as SeeClick (Cheng et al., 2024) and CogAgent (Hong et al., 2023), achieving an overall accuracy of 61.9% with GPT-4V and 72.2% with GPT-40 on the ScreenSpot benchmark. This performance exceeds SeeClick's 53.4%, CogAgents 47.4% and closely rivals OmniParser's 72.6%. On VisualWebBench (Liu et al., 2024), unlike SeeClick, which suffers a sharp drop in accuracy on out-of-distribution data with 31% accuracy, TRISHUL maintains strong generalization, achieving a robust 68.0% accuracy with both GPT-4V and GPT-40 closely matching the performance of OmniParser which achieves 68.9%.\nWe further present ablations in Table 2 to assess the impact of the SEED module and GROI-based action grounding in TRISHUL. Removing SEED (TRISHUL\u2020) results in a notable accuracy drop of 8.5% for GPT-4V and 2.9% for GPT-40 on ScreenSpot. Similarly, eliminating GROI-based action grounding (TRISHUL*) reduces accuracy by 2.9% for GPT-4V and 1.1% for GPT-40. These results highlight the critical role of these components in TRISHUL's performance.\nAdditionally, we demonstrate TRISHUL's modularity by integrating its components into existing grounding pipelines. In Table 2, we show that augmenting OmniParser's BLIPv2-derived icon descriptors originally lacking local semantic context-with TRISHUL's SEED module (OmniParser*) yields the best performance among training-based methods.\nOur GROI-based action grounding proves particularly effective for web and desktop platforms, where hierarchical and content-dense GUIs benefit from structured decomposition. However, its impact is less pronounced in mobile interfaces, where regions have minimal semantic separation. Further details can be found in Appendix A.3. Lastly, we observe that GPT-40 outperforms GPT-4V significantly when paired with SEED, suggesting that improved reasoning capabilities in LVLMs enhance the accuracy of SEED-generated descriptions."}, {"title": "3.2. AITW", "content": "Dataset and Experiments To evaluate TRISHUL on the mobile navigation benchmark AITW(Rawles et al., 2023), which consists of 30,000 instructions and 715,000 trajectories, we use the same train/test split as defined in (Cheng et al., 2024). This split retains only one trajectory per instruction, ensuring no overlap between the train and test sets.\nImplementation details- We adopt a similar prompt format to that used in MM-Navigator (Yan et al., 2023), where we label the detected elements on the screen using SoM prompting and present the model with the annotated image and the clean image. However, we replace IconDet's bounding boxes (as used in MM-Navigator) with local element boxes generated from our Hierarchal Screen Parsing method, and also provide our spatially enhanced element descriptions (Section 2.2) for all the local elements in our input prompt. The exact prompt is mentioned in the Appendix in Figure 13\nEvaluation and Results In Table 3, we report the baselines as presented in MM-Navigator(Yan et al., 2023). The best performing baseline incorporates action history and uses only image modality for navigation. MM-Navigator presents baselines with GPT-4V only, we also run MM-navigator's best configuration (Image+History) with GPT-4o to contrast it with TRISHUL's GPT-40 performance. We observe that TRISHUL with GPT-4V outperforms all prior GPT-4V-based baselines, achieving an overall accuracy of 54.5%. With GPT-40 model, TRISHUL achieves an average accuracy of 60%, surpassing MM-Navigator's GPT-40 baseline by over 2.2% to become the state of the art."}, {"title": "3.3. Mind2Web", "content": "Dataset and Experiments- To test on the web-navigation task we use the Mind2Web (Deng et al., 2023) dataset. The test set consists of three different categories - Cross Task, Cross Website, and Cross Domain having 252, 177, and 912 tasks respectively.\nImplementation details - We use the pre-processed test set"}, {"title": "3.4. Screen Point-and-Read", "content": "Dataset and experiments- We use the Screen Point and Read(Fan et al., 2024) benchmark to evaluate TRISHUL's performance on the GUI referring task. It evaluates the accuracy of the generated content description $D_c$ and layout description $D_l$ for the region marked by the user over the interface. This benchmark comprises of 650 screenshots across three domains: web, mobile, and operating systems. To validate our method, we run experiments using GPT-40 (OpenAI, June, 2024b), GPT-4V (OpenAI, June, 2024a), and Claude-3.5-Sonnet (Anthropic, 2023), enabling us to examine performance across multiple LVLMs.\nEvaluation and Results - To assess the quality of the generated content description and layout description we employ the cycle consistency evaluation following the screen point-and-read (Fan et al., 2024) paper. The agent outputs ($D_c$, $D_l$) are fed into an auxiliary model, which is asked to complete a downstream task, with its performance indicating description quality. We benchmark our approach against baseline GPT-40, Claude, and the ToL agent from Screen point-and-read, using GPT-40, GPT-4V, and Claude-3.5-Sonnet as the primary models. We also compute language similarity metrics like BERT (Zhang et al., 2019) score and ROUGE-L (Lin, 2004) to evaluate alignment with human-verified ground truth.\nTo further validate quality, we conduct two rounds of human evaluation: the first compares our approach against baseline GPT-40, while the second compares our approach with the ToL agent, both using GPT-40 as the primary LLM. We em-"}, {"title": "4. Discussion", "content": ""}, {"title": "4.1. Analysis on sampling multiple candidates", "content": "LVLM-based GUI agents that rely solely on visual perception aim to mirror human like interface interaction. Hu-"}, {"title": "4.2. Failure Analysis", "content": "In Figure 5, we evaluate the Local Element Exhaustiveness (LEE) metric across various datasets and their splits. The LEE score for an image is binary: it is set to 1 if the midpoint of the ground truth (GT) bounding box falls within any bounding box of local elements detected by our Hierarchical Screen Parsing (HSP) module; otherwise, it is set to 0. Thus, the LEE score shows the bottleneck that happens in our pipeline after the LE detection stage.\nThe results show a clear correlation between LEE scores and TRISHUL's performance across datasets. In particular, the low LEE scores in the Mind2Web dataset highlight that the limited exhaustiveness of local elements detected by the HSP module is a key factor constraining TRISHUL's effectiveness in web navigation tasks."}, {"title": "5. Conclusion", "content": "In this paper, we introduced TRISHUL, a training-free agentic framework that enables LVLMs to achieve comprehensive GUI screen understanding using two key modules: HSP and SEED. The HSP module organizes GUI elements into a multi-granular hierarchical structure, distinguishing Global Regions of Interest (GROIs) from local elements, while the SEED module enhances spatial context-aware reasoning. Experiments on ScreenSpot, VisualWebBench, AITW, Mind2Web, and ScreenPR demonstrate that TRISHUL outperforms all training-free methods and rivals training-based approaches while maintaining superior cross-task and cross-platform generalizability."}, {"title": "Impact Statement", "content": "This work advances machine learning methods for comprehensive graphical user interface (GUI) comprehension, enabling more intuitive and automated interactions. A key positive impact lies in enhancing accessibility for visually challenged individuals through our GUI referring agent, helping them navigate digital environments more effectively. Potential ethical concerns primarily revolve around privacy and control, if such automation tools are misused. Overall, while this framework promises to streamline user experiences and empower those with visual impairments, continued vigilance is advised to safeguard responsible, transparent, and privacy-oriented deployment."}, {"title": "A. Appendix.", "content": ""}, {"title": "A.1. Model Specifications and Endpoints", "content": "Since all our work leverages closed-source models like GPT-4V, GPT-40, and Claude, we mention the model identifiers that we use for our API calls for clarity. For GPT-4V - \"gpt-4-vision-preview\", For GPT-40 - \"gpt-4o-2024-08-06\", and for Claude - \"claude-3-5-sonnet-20241022\". Unless otherwise noted, all experiments are conducted with a temperature setting of 0.0."}, {"title": "A.2. Hierarchical Screen Parsing Details", "content": ""}, {"title": "A.2.1. IOS SCORE", "content": "Similar to IoU score we define an IoS score as:\ndef IoS (boxA, boxB):\n    xA = max (boxA[0], boxB[0])\n    yA = max (boxA[1], boxB[1])\n    xB = min (boxA[2], boxB[2])\n    yB = min (boxA[3], boxB[3])\n    interArea = max(0, xB - xA) * max (0, yB - yA)\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    ios = interArea / float (boxAArea + 1e-3)\n    return ios\nThe IoS (Intersection over Size) score is a measure used to evaluate the overlap between two bounding boxes, typically in the context of object detection. It calculates the ratio of the intersection area between two boxes to the area of the first box. IoS(A, B) (also written as IOS$A$), a score of 0.5 means 50% of A intersects with B."}, {"title": "A.2.2. FILTERING REDUNDANT BOUNDING BOXES", "content": "The output of EasyOCR + SAM model combined is extremely cluttered (see 6) and contains numerous overlaps and false positive detections from both models. We deploy the following steps to parse the outputs of SAM and OCR (local elements as referred to in the main script) together.\n\u2022 Generate GROI, Icon and Button Candidate Proposals Classify all SAM boxes based on $A_{thresh-GROI}$, $A_{thresh-icon}$, $A_{thresh-button}$. Let B represent the set of bounding boxes detected in the GUI.\nGlobal Region of Interest (GROI) Candidates: The set of boxes with an area greater than the GROI threshold is given by:\nGROI = {b \u2208 B | Area(b) > $A_{thresh-GROI}$}\nIcon Candidates: The set of boxes with an area between the Button and Icon thresholds is defined as:\nIcon = {b \u2208 B | $A_{thresh-button}$ < Area(b) < $A_{thresh-icon}$}\nButton Candidates: The set of boxes with an area less than the Button threshold is:\nButton = {b \u2208 B | Area(b) < $A_{thresh-button}$}\n\u2022 Remove False Positive Text Bounding Boxes: Remove text boxes using a predefined dictionary, that are likely OCR mis-detections for icons. These texts usually contain only special characters or short, meaningless words. If a word contains one of these characters and has a length of less than \u00a1 5 that text bbox is ignored.\nCharacters/Words to ignore:\n\"@\", \"#\", \"x\", \"?\", \"{\", \"}\", \"<\", \">\", \"&\", \"`\", \"~\",\n\" \", \"=\", \"C\", \"Q\", \"88\", \"83\", \"98\", \"15J\", \"^\", \"0e\", \"n\", \"E\", \"ya\", \"ch\",\n\"893\""}, {"title": "A.2.3. NON MAX SUPPRESSION FOR GROIS", "content": "\u2022 Reject boxes with low Information scoreS If the current bounding box has an information score S < $S_{thresh}$ it is rejected. $S_{thresh}$ is set to 10 for the ScreenPoint and Read task and 25 for action grounding task.\n\u2022 Reject Overlapping BBoxes: If the current bounding box intersects with a previously selected bounding box with a higher Infrormation score and IoS$_{current}$ > IoS$_{overlap-thresh}$ it is rejected. IoS$_{overlap-thresh}$ thresh is set to 0.5 for visual grounding task and 0 for ScreenPR task.\n\u2022 Reject Contained BBoxes (Smaller GROIs Inside Larger): If the current bounding box is inside a previously selected bounding box with a higher Information Score and if IoS$_{current}$ > IoS$_{inside-thresh}$ it is rejected. IoS$_{overlap-thresh}$ thresh is set to 0.5 for visual grounding task and 0 for ScreenPR task.\n\u2022 Reject Engulfing Bboxes (Larger GROIs Inside Smaller): If the current bounding box completely engulfs a bounding box with a higher Information Score then it is rejected."}, {"title": "A.3. GROI analysis for ScreenSpot and VisualWebBench", "content": "We plot two additional statistics for the detected GROI's through our HSP block. In Figure 8 we plot the average number of GROIs per image, across the three different sub-categories of ScreenSpot and the full dataset of VisualWebBench. We observe that GUI screenshots from mobiles have the lowest average number of GROIs per image. This is due to the fact that mobile regions are not semantically coherent, therefore lesser number of GROIs are generated.\nIn Figure 9 we plot the average of the total area covered by all GROIs in an image to the total area of the image. Mobile GUI screenshots have the least dense GROI coverage, due to the fact that we also detect fewer GROIs in mobile screenshots. These studies further validate the fact that GROIs are not as useful for mobile GUI's however they offer more benefit for PC and web based GUIs."}]}