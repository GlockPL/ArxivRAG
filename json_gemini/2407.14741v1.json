{"title": "Orthogonal Hyper-category Guided Multi-interest Elicitation for Micro-video Matching", "authors": ["Beibei Li", "Beihong Jin", "Yisong Yu", "Yiyuan Zheng", "Jiageng Song", "Wei Zhuo", "Tao Xiang"], "abstract": "Watching micro-videos is becoming a part of public daily life. Usually, user watching behaviors are thought to be rooted in their multiple different interests. In the paper, we propose a model named OPAL for micro-video matching, which elicits a user's multiple heterogeneous interests by disentangling multiple soft and hard interest embeddings from user interactions. Moreover, OPAL employs a two-stage training strategy, in which the pre-train is to generate soft interests from historical interactions under the guidance of orthogonal hyper-categories of micro-videos and the fine-tune is to reinforce the degree of disentanglement among the interests and learn the temporal evolution of each interest of each user. We conduct extensive experiments on two real-world datasets. The results show that OPAL not only returns diversified micro-videos but also outperforms six state-of-the-art models in terms of recall and hit rate.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, micro-video apps such as TikTok, Kwai, MX TakaTak, etc. have become increasingly popular, which makes the explosive growth in the number of micro-videos. In order to generate personalized recommendations from millions of micro-video candidates effectively, the micro-video recommendation usually includes two phases, i.e., matching and ranking, as with existing large-scale recommender systems [1], where matching aims to quickly recall hundreds or thousands of micro-videos from millions or billions of candidates, and ranking aims to refine the matching results and determine the final recommendations. In this paper, we focus on micro-video matching.\nAn ideal micro-video matching model is supposed to possess the following characteristics: (1) Low computational complexity. The matching process must efficiently sift through millions or billions of items within several milliseconds. Hence, the computational complexity of the matching model should be as low as possible. (2) Recommendations with high coverage of user interests. A user typically has diverse interests. As depicted in Figure 1, a user may exhibit interests in micro-videos falling under categories such as travel and cute pets. These implicit yet crucial interests influence a user's decisions regarding which micro-videos to watch. Consequently, the recalled items are supposed to encompass as many user interests as possible. (3) Temporal dynamics of user interests. The scope and depth of each user interest might evolve over time. For instance, a user fond of travel initially watches numerous micro-videos showcasing European scenery but gradually shifts the focus towards Asian scenery, as illustrated in Figure 1. Therefore, the matching model needs to accurately capture fluctuations in user interests and make predictions based on their current preferences.\nThere are a number of recent micro-video recommendation models available [2]-[5], but most of them are appropriate for the ranking phase rather than the matching phase due to their high time complexity [3], [6]. On the other hand, most of existing micro-video recommendation models offer no disentanglement of multiple interests of users [7]-[11] or ignore the evolution of user interest. Though some models [12] learn multiple user interests, they aggregate them into one embedding while serving, limiting the contributions of multiple interests on recommendation results.\nAn intuitive approach to micro-video matching is to directly apply existing multi-interest recommendation models to micro-video recommendation scenarios. Existing multi-interest recommendation models utilize attention mechanisms [13], [14], dynamic routing [14], [15], and contrastive learning [16]\u2013[18] to extract multiple user interests. However, these models either fail to ensure the heterogeneity among interests or ignore the temporal dynamics of user interests, affecting the diversity and accuracy of recommendation.\nIn order to address the above issues and challenges, in the paper, we propose an effective and efficient model named OPAL(Orthogonal category and Personalized interest Activated Lever) for matching micro-videos, where effectiveness comes from disentanglement of heterogeneous multiple interests and modeling of interest evolution, and efficiency is supported by the low computational complexity. Our contributions are summaried as follows.\n(1) We propose a heterogeneous multi-interest recommendation model OPAL for micro-video matching, which recognizes soft and hard user interests according to implicit learnable orthogonal hyper-categories of micro-videos.\n(2) We present a novel two-stage interest learning strategy for OPAL, where OPAL improves the confidence of interest disentanglement in the first stage, while it considers the interest evolution and achieves the complete disentanglement of user interests in the second stage.\n(3) We conduct extensive experiments on real-world datasets. The results show that OPAL outperforms six state-of-the-art models in terms of recall and hit-rate, demonstrating that identifying heterogeneous multiple user interests improves both recommendation accuracy and diversity."}, {"title": "II. METHODOLOGY", "content": "A. Overview\nWe denote the user set as $\\mathcal{U}$, the micro-video set as $\\mathcal{V}$, and the embedding of a micro-video $v_i \\in \\mathcal{V}$ as $\\mathbf{v}_i$, where $\\|\\mathbf{v}_i\\|_2 = 1$. Figure 2 shows the architecture of OPAL. For a user $u_i \\in \\mathcal{U}$, we first filter out micro-videos with his/her positive interactions (including interactions reflecting satisfaction such as favorites or positive engagement like replays, hereinafter referred to as interactions). Next, we sort these interactions according to the corresponding interaction timestamps to form the interaction sequence $s_i = [v_{i1}, v_{i2}, ..., v_{i|s_i|}]$, where $s_i$ denotes the length of the sequence. Then, $n$ interest $\\mathbf{U}_i = [\\mathbf{u}_i^1, \\mathbf{u}_i^2, ..., \\mathbf{u}_i^n] \\in \\mathbb{R}^{d \\times n}$ are learned from $s_i$. Finally, we use each interest embedding to retrieve a number of candidate micro-videos via Faiss, and the micro-videos with the top $K$ highest matching scores are the recommendations.\nThe training process of OPAL includes two stages, i.e., pre-train and fine-tune, where we handle interest disentanglement via different strategies. Firstly, at the pre-train stage, each interacted micro-video is allowed to be associated with multiple interests of a user to learn multiple soft interests. We train the model until convergence to improve the confidence of interest disentanglement. Then, at the fine-tune stage, on the basis of the pre-trained parameters, the hard interests are fused to fine-tune the model until the model converges again. At this time, each micro-video is routed to one of the multiple interests exclusively to obtain the heterogeneous interests and achieve non-overlapping disentanglement of interests. Apart from that, sequential modules are adopted to excavate the evolution of each interest.\nB. Multi-interest Modeling\nGlobal Hyper-category Embeddings. Micro-videos can be divided into several hyper-categories, and each of them corresponds to one user interest. In order to mine semantic interests, we learn the global implicit micro-video hyper-categories and set up $k$ micro-video hyper-category embeddings as $\\{\\mathbf{g}_1, \\mathbf{g}_2, ..., \\mathbf{g}_k\\}$, where $\\mathbf{g}_* \\in \\mathbb{R}^d$ and $\\|\\mathbf{g}_*\\|_2 = 1$. In order to keep the heterogeneity between different hyper-categories of micro-videos and reduce the information redundancy among multiple user interests, we keep any pair of micro-video hyper-category embeddings orthogonal. Therefore, we construct the orthogonal constraint loss as follows,\n$$L_{orth} = \\sum_{i=1}^{k} \\sum_{j=1, j \\neq i}^{k} (\\mathbf{g}_i^T \\mathbf{g}_j)^2.$$\nThe micro-video hyper-category embeddings can be regarded as a set of unit orthogonal basis vectors. Then, in the multidimensional space spanned by orthogonal basis vectors, for a micro-video $v_i \\in \\mathcal{V}$, we can obtain its coordinates i.e., cosine similarities $\\{\\mathbf{g}_1^T \\mathbf{v}_i, \\mathbf{g}_2^T \\mathbf{v}_i, ..., \\mathbf{g}_k^T \\mathbf{v}_i\\}$, where $\\mathbf{g}_j^T \\mathbf{v}_i \\in [-1, 1]$. The coordinates measure the correlation between the micro-video $v_i$ and each hyper-category. The larger the coordinate, the stronger the correlation.\nMultiple Interests at the Pre-train Stage. Given the interaction sequence $s_i$ of user $u_i$, we first calculate the coordinates of the $l$-th interacted micro-video $v_{il} \\in s_i$ on each hyper-category $\\mathbf{g}_j$, as $r_{il}^j = \\mathbf{g}_j^T \\mathbf{v}_{il}$, where $j \\in \\{1, 2, ..., k\\}$. Next, we convert the coordinates into probabilities for hyper-category assignment via softmax function. If we perform softmax transformation on $r_{il}^j \\in [-1, 1]$ directly, the probabilities of assigning a micro-video to the most relevant hyper-category (the coordinate value is 1) and to the most irrelevant hyper-category (the coordinate value is -1) will be very close, which smooths out the differences. Therefore, we amplify the coordinate values by dividing a small number $\\epsilon$ that is less than 1. Thus, the probability of assigning $v_{il}$ to the $j$-th hyper-category is calculated as Equation 2.\n$$a_{il}^j = \\frac{\\exp(\\mathbf{g}_j^T \\mathbf{v}_{il}/\\epsilon)}{\\sum_{i=1}^{k} \\exp(\\mathbf{g}_j^T \\mathbf{v}_{il}/\\epsilon)}.$$\nThen, due to the presumption that frequent positive interactions with a hyper-category of micro-videos reflect an interest, we aggregate the embeddings of the micro-videos assigned to the same hyper-category to obtain the corresponding soft interest. For example, the soft interest $\\mathbf{p}_i^j$ of user $u_i$ learned according to hyper-category $j$ is calculated as follows,\n$$\\mathbf{p}_i^j = \\frac{1}{|s_i|} \\sum_{l=1}^{|s_i|} a_{il}^j \\mathbf{v}_{il}.$$The intensity of user interest $\\mathbf{p}_i^j$ can be measured as the total assignment probabilities accumulated from history interactions, i.e., $\\sum a_{il}^j$. Finally, we set the $j$-th soft interest of user $u_i$ as $\\mathbf{u}_i^j = \\mathbf{p}_i^j$.\nIn order to avoid the trivial solution that all the micro-videos concentrate on only one certain hyper-category, we restrict the number of micro-videos absorbed by each hyper-category to be approximately equal. Since the number of micro-videos cannot be trained by backpropagation, we estimate it via the total assignment probabilities collected by each hyper-category from all candidate micro-videos, i.e., the number of micro-videos assigned to hyper-category $j$ is estimated as $w_j = \\sum_{u \\in \\mathcal{U}} \\sum_{v \\in \\mathcal{V}} a_{uj}^v$. The quantity distribution of micro-videos in various hyper-categories is denoted as $\\mathbf{w} = [w_1, ..., w_k]$. Then, we constrain the discrete coefficient of $\\mathbf{w}$ to be as small as possible, and construct the uniformity loss as follows,\n$$L_{unif} = \\sigma_\\mathbf{w}/\\mu_\\mathbf{w},$$where $\\sigma_\\mathbf{w}$ is the standard deviation of the vector $\\mathbf{w}$ and $\\mu_\\mathbf{w}$ is the average of the vector $\\mathbf{w}$. In practice, it will update the embeddings of all the micro-videos when the uniformity loss is optimized, which is pretty time-consuming. In order to accelerate, we adopt the uniformity loss on micro-videos involved in the current batch to estimate the uniformity loss on all the candidate micro-videos.\nMultiple Interests at the Fine-tune Stage. The user interests may change over time. For example, a user previously liked micro-videos about scenery in Europe, but now turns to landscape in Asia. In this case, it is necessary to model the evolution of each interest. However, at the pre-train stage, we have not separated interaction sequences into non-overlapping subsequences related to one interest exclusively, which brings challenges to model the evolution of each interest. Therefore, based on the high confidence of hyper-category assignment, we adopt hard hyper-category assignment to achieve complete separation of the interaction sequences.\nWe assign each micro-video $v_{il}$ of user $u_i$ to the hyper-category $c_{il}$ with the highest probability exclusively, i.e., $c_{il} = \\text{argmax}_{0 < j \\leq k} a_{il}^j$. In order to further increase the hyper-category assignment confidence, we establish a cross-entropy based unique loss, as shown in Equation (5), which forces a micro-video to be assigned to one of the video hyper-categories exclusively.\n$$L_{unique}(s_i) = - \\frac{1}{|s_i|} \\sum_{l=1}^{|s_i|} \\ln(\\frac{\\text{max}_{0 < j \\leq k} (a_{il}^j)}{\\sum_{i=1}^{k} a_{il}^j}).$$\nBased on the hard hyper-category of micro-videos, the interaction sequence $s_i$ of user $u_i$ will be split into $k$ disjoint subsequences $\\{s_i^j, 1 < j < k\\}$, where $|s_i| = \\sum_{j=1}^{k} |s_i^j|$.\nFurthermore, in order to capture the evolution of each interest, for each non-empty subsequences, we adopt the sequential module to learn the corresponding hard interest embedding $\\mathbf{q}_i^j$. For simplicity, here we utilize GRU [19].\nFinally, we set $\\mathbf{u}_i^j = (\\mathbf{p}_i^j + \\mathbf{q}_i^j)/2$, integrating the hard interests with the soft ones to fine-tune the model, which leads the user interests into complete separation and learns the interest evolution over time.\nC. Loss Function\nMost existing sequential recommendation models formulate the recommendation as a next-item prediction task, that is, as for a sequence $s_i = [v_{i1}, v_{i2}, ..., v_{i|s_i|}]$, they utilize $[v_{i1}, v_{i2}, ..., v_{it}]$ to predict $v_{i(t+1)}$, where $t \\in [1, |s_i|]$. In this situation, $v_{i(t+1)}$ is regarded as the positive sample, other candidate micro-videos are regarded as negative ones. However, after time $t$, the user may interact with several micro-videos $f_i = [v_{i(t+1)}, ..., v_{i(t+|f_i|)}]$, where $|f_i| \\geq 1$. Regarding micro-videos other than $v_{i(t+1)}$ as negative samples is too aggressive, suppressing some micro-videos the user is interested in. Therefore, we formulate the micro-video recommendation problem as a future-item prediction task, that is, each micro-video in the future positive interaction is regarded as a positive sample.\nFor a user $u_i$, we randomly sample a video $v_{pos} \\in [v_{i(t+1)}, ..., v_{i(t+|f_i|)}]$ as the positive sample $v_{pos}$ of $s_i$. Next, we randomly sample videos that have not been interacted from all the micro-video candidates to form a negative sample set $\\mathcal{N}$. Then, we construct the cross-entropy loss as shown in Equation 6.\n$$L_{main} = - \\ln \\frac{\\exp(\\text{max}_{0 < j \\leq k} (\\mathbf{v}_{pos}^T \\mathbf{u}_i^j))}{\\sum_{v_i \\in \\mathcal{N}} \\exp(\\text{max}_{0 < j \\leq k} (\\mathbf{v}_{i}^T \\mathbf{u}_i^j))}.$$In order to avoid the additional overhead caused by negative sampling, an intuitive operation is to treat other positive samples within the same minibatch as negative samples. But in this way, the diversity of negative samples is very limited. Since matching aims to exclude plenty of obviously irrelevant micro-videos, we need random sampling among all the micro-video pools to get richer negative micro-videos. To balance the sampling overhead and negative sample diversity, as for a batch of training samples, we sample a negative micro-video for each one. Assuming the batch size is $B$, we have $B$ negative micro-videos and share them in the batch. So, we calculate multi-class classification probabilities via the sampled softmax on $B+1$ micro-videos, where one is positive and others are negative.\nIn the end, the loss function is defined as Equation 7, where $\\lambda_*$ are the regularization coefficients.\n$$L = L_{main} + \\lambda_o L_{orth} + \\lambda_f L_{unif} + \\lambda_q L_{unique}$$\nD. Complexity Analysis\nThe parameters of OPAL are mainly the embeddings of micro-videos and hyper-categories, whose number is $|\\mathcal{V}|d + kd$. The computation of OPAL focuses on interest elicitation. For an interaction sequence $s$, the time complexity of learning soft and hard interests is $O(|s|kd)$ and $O(|s|kd + |s|d^2)$, respectively. The low complexity ensures that OPAL can support large-scale micro-video matching."}, {"title": "III. EXPERIMENTS", "content": "A. Experiment Setup\nDatasets We conduct experiments on two datasets, i.e., WeChat-Channels and MX-TakaTak. WeChat-Channels is a dataset released by WeChat Big Data Challenge 2021. It contains two-week interactions (including engagement interactions such as watching and satisfaction interactions such as likes and favorites) from 20,000 anonymous users on WeChat Channels, a popular micro-video platform in China. MX-TakaTak is collected from MX TakaTak, one of the largest micro-video platforms in India. We collect real historical interaction records of TakaTak from Sept. 18, 2021 to Sept. 28, 2021, then randomly sample 50,000 users and form the MX-TakaTak dataset using their interactions. The statistics of the two datasets are shown in Table I.\nTo avoid the data penetration during the model training and evaluation [21], we strictly maintain the timing relationship among the two datasets. In detail, we construct the test set using interactions of the last day, the validation set using interactions of the second-to-last day, and the training set using other interactions.\nB. Performance Comparison\nWe conduct a comparative experiment, comparing our model with six competitors. Setting various numbers of interests, we record the best performance of each model in Table II. From Table II, we have the following observations. (1) OPAL surpasses the competitors in all metrics on the two datasets. For example, on the WeChat-Channel dataset, OPAL outperforms the second best model, i.e., ComiRec-SA, about 17.2% on Recall@50, and 8.77% on HitRate@50. (2) Although the competitors claim they are multi-interest models, most of them have the best recommendation performance while learning only one interest for per user. This finding shows that these models cannot mine meaningful multiple user interests in micro-video scenarios. (3) The Octopus model is very similar to the hard interest part of OPAL, but its performance is far from the performance of OPAL.\nC. Ablation Study\nWe conduct ablation studies to investigate the effect of the three regularization losses, i.e., $L_{orth}$, $L_{unif}$, $L_{unique}$, two-stage training strategy and the future-item prediction task. The experimental results are shown in Table III, from which we have the following observations and conclusions.\nEffect of Regularization Losses. From model variants (A)-(C), we find that whichever loss is removed leads to a decrease in the performance of the model, which indicates their contribution to the performance. This is due to the fact that all the three losses are constrained to help enhance the heterogeneity and representation quality of multiple user interests.\nEffect of Two-stage Training. We remove the pre-train stage and the fine-tune stage from our model, respectively, denoted as model variants (D) and (E). We find that the performance gets worse after removing any of two stages. This demonstrates that the hard classification method and time series information at the fine-tune stage contribute to the performance improvement. It also implies that if we omit the pre-train stage and directly use the rigorous hard classification method, the model may converge to a local optimal solution.\nEffect of the Future-item Prediction Task. We construct a variant of OPAL that replaces the future-item prediction task with the next-item prediction task, denoted as model variant (F). We find that the intact OPAL with the future-item prediction task is more effective than the variant with the next-item prediction task. This is because that predicting the user interactions over the next period of time is more close to the requirement of real-world scenarios.\nD. Multiple Interests Analysis\nImpact on Recommendation Accuracy. In order to study the impact of the number of interests in our model on the recommendation accuracy, we set the number of micro-video hyper-categories to 1, 2, 4, 8, 16 successively and conduct experiments. The experimental results are shown in Figure 3. It can be seen that our model on the WeChat-Channel dataset reaches the best when the number of interests is 4, and reaches the best on the MX-TakaTak dataset when the number of interests is 8. This shows that our model is reasonable and effective to disentangle multiple interests from user interaction sequences and utilize them to achieve recommendations.\nImpact on Recommendation Diversity. We conduct a case study to compare the recommendation diversity of single-interest modeling with multi-interest modeling. We randomly sample a user. After setting the number of interests to 1 and 4, OPAL outputs 200 micro-videos as recommendation results, respectively. In particular, when the number of interests is 4, the number of micro-videos recalled by each interest embedding is 38, 63, 52 and 47, respectively. Since the labels of micro-videos are often missing or inaccurate, we use SPPMI value [23] to measure the correlation between micro-videos. The higher the SPPMI value, the greater the correlation. We calculate SPPMI values between two micro-videos in the recommendation results and get a SPPMI matrix with dimensions 200\u00d7200. The visualization results are shown in Figures 4 (a)-(b). We can see that the micro-videos recommended by the single interest model are highly correlated. In the recommendation result of the multi-interest model, the micro-videos recalled by the same interest embedding have high correlations, while the micro-videos recalled by different interests have low correlations. These demonstrate that the heterogeneity of recommended micro-videos is brought by different interest embeddings. Moreover, from Figures 4(c)-(d), we can find that there are more small values in the multi-interest SPPMI matrix compared with the single-interest SPPMI matrix, which indicates that the recommendation results from the multi-interest model have greater diversity."}, {"title": "IV. CONCLUSION", "content": "This paper proposes a micro-video recommendation model OPAL. OPAL relys on hyper-categories information of micro-videos in positive interaction sequences to distinguish multiple interests for the user and adopts pre-train plus fine-tune to form multiple soft and hard interest embeddings of the user. In particular, in OPAL, the recommendation results are recalled using Faiss with multiple interest embeddings as input, increasing the diversity of micro-videos, and the future-item prediction task, which is closer to the real scenarios, is introduced to optimize the model, improving the recommendation performance. Results of experiments on real-world datasets demonstrate the feasibility and effectiveness of OPAL."}]}