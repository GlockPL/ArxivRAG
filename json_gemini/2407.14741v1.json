{"title": "Orthogonal Hyper-category Guided Multi-interest Elicitation for Micro-video Matching", "authors": ["Beibei Li", "Beihong Jin", "Yisong Yu", "Yiyuan Zheng", "Jiageng Song", "Wei Zhuo", "Tao Xiang"], "abstract": "Abstract\u2014Watching micro-videos is becoming a part of public daily life. Usually, user watching behaviors are thought to be rooted in their multiple different interests. In the paper, we propose a model named OPAL for micro-video matching, which elicits a user's multiple heterogeneous interests by disentangling multiple soft and hard interest embeddings from user interactions. Moreover, OPAL employs a two-stage training strategy, in which the pre-train is to generate soft interests from historical interactions under the guidance of orthogonal hyper-categories of micro-videos and the fine-tune is to reinforce the degree of disentanglement among the interests and learn the temporal evolution of each interest of each user. We conduct extensive experiments on two real-world datasets. The results show that OPAL not only returns diversified micro-videos but also outperforms six state-of-the-art models in terms of recall and hit rate.\nIndex Terms\u2014Recommendation, multi-interest recommendation, micro-video matching", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, micro-video apps such as TikTok, Kwai, MX TakaTak, etc. have become increasingly popular, which makes the explosive growth in the number of micro-videos. In order to generate personalized recommendations from millions of micro-video candidates effectively, the micro-video recommendation usually includes two phases, i.e., matching and ranking, as with existing large-scale recommender systems [1], where matching aims to quickly recall hundreds or thousands of micro-videos from millions or billions of candidates, and ranking aims to refine the matching results and determine the final recommendations. In this paper, we focus on micro-video matching.\nAn ideal micro-video matching model is supposed to possess the following characteristics: (1) Low computational complexity. The matching process must efficiently sift through millions or billions of items within several milliseconds."}, {"title": "II. METHODOLOGY", "content": "We denote the user set as U, the micro-video set as V, and the embedding of a micro-video vi \u2208 V as vi, where ||vi||2 = 1. Figure 2 shows the architecture of OPAL. For a user ui \u2208 U, we first filter out micro-videos with his/her positive interactions (including interactions reflecting satisfaction such as favorites or positive engagement like replays, hereinafter referred to as interactions). Next, we sort these interactions according to the corresponding interaction timestamps to form the interaction sequence si = [Vi1, Vi2,..., Vils], where si denotes the length of the sequence. Then, n interest Ui = [u1,u2,..., un] \u2208 Rd\u00d7n are learned from si. Finally, we use each interest embedding to retrieve a number of candidate micro-videos via Faiss\u00b9, and the micro-videos with the top K highest matching scores are the recommendations.\nThe training process of OPAL includes two stages, i.e., pre-train and fine-tune, where we handle interest disentanglement\n1https://github.com/facebookresearch/faiss"}, {"title": "B. Multi-interest Modeling", "content": "Global Hyper-category Embeddings. Micro-videos can be divided into several hyper-categories, and each of them corresponds to one user interest. In order to mine semantic interests, we learn the global implicit micro-video hyper-categories and set up k micro-video hyper-category embeddings as {g1,g2,...,gk}, where g* \u2208 Rd and ||g*||2 = 1. In order to keep the heterogeneity between different hyper-categories of micro-videos and reduce the information redundancy among multiple user interests, we keep any pair of micro-video hyper-category embeddings orthogonal. Therefore, we construct the orthogonal constraint loss as follows,\n$L_{orth} = \\sum_{i=1}^{k} \\sum_{j=1,j \\neq i}^{k} (g_i^T g_j)^2$. (1)\nThe micro-video hyper-category embeddings can be regarded as a set of unit orthogonal basis vectors. Then, in the multidimensional space spanned by orthogonal basis vectors, for a micro-video vi \u2208 V, we can obtain its coordinates i.e.,"}, {"title": "Multiple Interests at the Pre-train Stage.", "content": "Given the interaction sequence of user ui, we first calculate the coordinates of the l-th interacted micro-video vil \u2208 si on each hyper-category gj, as rij = gT vil, where j \u2208 {1, 2, ..., k}. Next, we convert the coordinates into probabilities for hyper-category assignment via softmax function. If we perform softmax transformation on ri; \u2208 [-1,1] directly, the probabilities of assigning a micro-video to the most relevant hyper-category (the coordinate value is 1) and to the most irrelevant hyper-category (the coordinate value is -1) will be very close, which smooths out the differences. Therefore, we amplify the coordinate values by dividing a small number \u03f5 that is less than 1. Thus, the probability of assigning vil to the j-th hyper-category is calculated as Equation 2.\n$\\alpha_{ijl} = \\frac{\\exp{\\frac{g_j^T v_{il}}{\\epsilon}}}{\\sum_{i=1}^{k} \\exp{\\frac{g_j^T v_{il}}{\\epsilon}}}$. (2)\nThen, due to the presumption that frequent positive interactions with a hyper-category of micro-videos reflect an interest, we aggregate the embeddings of the micro-videos assigned to the same hyper-category to obtain the corresponding soft interest. For example, the soft interest $p_i^j$ of user ui learned according to hyper-category j is calculated as follows,\n$p_i^j = \\sum_{l=1}^{S_i} \\alpha_{ij}^l v_{il}$. (3)\nThe intensity of user interest $p_i^j$ can be measured as the total assignment probabilities accumulated from history interactions, i.e., $\\sum \\alpha_{ijl}$. Finally, we set the j-th soft interest of user ui as $u^j_i = p_i^j$.\nIn order to avoid the trivial solution that all the micro-videos concentrate on only one certain hyper-category, we restrict the number of micro-videos absorbed by each hyper-category to be approximately equal. Since the number of micro-videos cannot be trained by backpropagation, we estimate it via the total assignment probabilities collected by each hyper-category from all candidate micro-videos, i.e., the number of micro-videos assigned to hyper-category j is estimated as wj = $\\sum_{i \\in U} \\sum_{v \\in V_i} \\alpha_{vj}$. The quantity distribution of micro-videos in various hyper-categories is denoted as w = [W1,..., Wk]. Then, we constrain the discrete coefficient of w to be as small as possible, and construct the uniformity loss as follows,\n$L_{unif} = \\sigma_w / \\mu_w$, (4)\nwhere \u03c3w is the standard deviation of the vector w and \u03bcw is the average of the vector w. In practice, it will update the embeddings of all the micro-videos when the uniformity loss is optimized, which is pretty time-consuming. In order to accelerate, we adopt the uniformity loss on micro-videos involved in the current batch to estimate the uniformity loss on all the candidate micro-videos."}, {"title": "Multiple Interests at the Fine-tune Stage.", "content": "The user interests may change over time. For example, a user previously liked micro-videos about scenery in Europe, but now turns to landscape in Asia. In this case, it is necessary to model the evolution of each interest. However, at the pre-train stage, we have not separated interaction sequences into non-overlapping subsequences related to one interest exclusively, which brings challenges to model the evolution of each interest. Therefore, based on the high confidence of hyper-category assignment, we adopt hard hyper-category assignment to achieve complete separation of the interaction sequences.\nWe assign each micro-video vil of user uz to the hyper-category Cil with the highest probability exclusively, i.e., Cil = argmax0<j<k \u03b1\u1f37j. In order to further increase the hyper-category assignment confidence, we establish a cross-entropy based unique loss, as shown in Equation (5), which forces a micro-video to be assigned to one of the video hyper-categories exclusively.\n$L_{unique}(S_i) = \\frac{1}{S_i} \\sum_{l=1}^{S_i} - \\ln{\\frac{\\max_{0<j\\leq k} (\\alpha_{ijl})}{\\sum_{l=1}^{S_i} \\alpha_{ijl}}}$ (5)\nBased on the hard hyper-category of micro-videos, the interaction sequence si of user ui will be split into k disjoint subsequences {s, 1 < j < k}, where |si| = \u2211j=1 |s|.\nFurthermore, in order to capture the evolution of each interest, for each non-empty subsequences, we adopt the sequential module to learn the corresponding hard interest embedding q. For simplicity, here we utilize GRU [19].\nFinally, we set u = (p+q)/2, integrating the hard interests with the soft ones to fine-tune the model, which leads the user interests into complete separation and learns the interest evolution over time."}, {"title": "C. Loss Function", "content": "Most existing sequential recommendation models formulate the recommendation as a next-item prediction task, that is, as for a sequence si = [Vil, Vi2,..., Vils], they utilize [Vi1, Vi2,..., Vit] to predict Vi(t+1), where t \u2208 [1, |si|]. In this situation, Vi(t+1) is regarded as the positive sample, other candidate micro-videos are regarded as negative ones. However, after time t, the user may interact with several micro-videos f\u2081 = [Vi(t+1),..., Vi(t+|fi|)], where |fi| \u2265 1. Regarding micro-videos other than Vi(t+1) as negative samples is too aggressive, suppressing some micro-videos the user is interested in. Therefore, we formulate the micro-video recommendation problem as a future-item prediction task, that is, each micro-video in the future positive interaction is regarded as a positive sample.\nFor a user ui, we randomly sample a video vos \u2208 [Vi(t+1),..., Vi(t+|fi|)] as the positive sample vpos of si. Next, we randomly sample videos that have not been interacted from all the micro-video candidates to form a negative sample set N. Then, we construct the cross-entropy loss as shown in Equation 6."}, {"title": "D. Complexity Analysis", "content": "The parameters of OPAL are mainly the embeddings of micro-videos and hyper-categories, whose number is |V|d+kd.\nThe computation of OPAL focuses on interest elicitation. For an interaction sequence s, the time complexity of learning soft and hard interests is O(|s|kd) and O(|s|kd + |s|d\u00b2), respectively. The low complexity ensures that OPAL can support large-scale micro-video matching."}, {"title": "III. EXPERIMENTS", "content": "Datasets We conduct experiments on two datasets, i.e., WeChat-Channels and MX-TakaTak. WeChat-Channels is a dataset released by WeChat Big Data Challenge 20212. It contains two-week interactions (including engagement interactions such as watching and satisfaction interactions such as likes and favorites) from 20,000 anonymous users on WeChat Channels, a popular micro-video platform in China. MX-TakaTak is collected from MX TakaTak, one of the largest micro-video platforms in India. We collect real historical interaction records of TakaTak from Sept. 18, 2021 to Sept. 28, 2021, then randomly sample 50,000 users and form the MX-TakaTak dataset using their interactions. The statistics of the two datasets are shown in Table I.\nTo avoid the data penetration during the model training and evaluation [21], we strictly maintain the timing relationship among the two datasets. In detail, we construct the test set using interactions of the last day, the validation set using interactions of the second-to-last day, and the training set using other interactions.\n2https://algo.weixin.qq.com/"}, {"title": "B. Performance Comparison", "content": "We conduct a comparative experiment, comparing our model with six competitors. Setting various numbers of interests, we record the best performance of each model in Table II. From Table II, we have the following observations.\n(1) OPAL surpasses the competitors in all metrics on the two datasets. For example, on the WeChat-Channel dataset, OPAL outperforms the second best model, i.e., ComiRec-SA, about 17.2% on Recall@50, and 8.77% on HitRate@50. (2) Although the competitors claim they are multi-interest models, most of them have the best recommendation performance while learning only one interest for per user. This finding shows that these models cannot mine meaningful multiple user interests in micro-video scenarios. (3) The Octopus model is very similar to the hard interest part of OPAL, but its performance is far from the performance of OPAL."}, {"title": "C. Ablation Study", "content": "We conduct ablation studies to investigate the effect of the three regularization losses, i.e., Lorth, Lunif, Lunique, two-stage training strategy and the future-item prediction task. The experimental results are shown in Table III, from which we have the following observations and conclusions.\nEffect of Regularization Losses. From model variants (A)-(C), we find that whichever loss is removed leads to a decrease in the performance of the model, which indicates their contribution to the performance. This is due to the fact that all the three losses are constrained to help enhance the heterogeneity and representation quality of multiple user interests.\nEffect of Two-stage Training. We remove the pre-train stage and the fine-tune stage from our model, respectively, denoted as model variants (D) and (E). We find that the performance gets worse after removing any of two stages. This demonstrates that the hard classification method and time series information at the fine-tune stage contribute to the performance improvement. It also implies that if we omit the pre-train stage and directly use the rigorous hard classification method, the model may converge to a local optimal solution.\nEffect of the Future-item Prediction Task. We construct a variant of OPAL that replaces the future-item prediction task with the next-item prediction task, denoted as model variant (F). We find that the intact OPAL with the future-item prediction task is more effective than the variant with the next-item prediction task. This is because that predicting the"}, {"title": "D. Multiple Interests Analysis", "content": "Impact on Recommendation Accuracy. In order to study the impact of the number of interests in our model on the recommendation accuracy, we set the number of micro-video hyper-categories to 1, 2, 4, 8, 16 successively and conduct experiments. The experimental results are shown in Figure 3. It can be seen that our model on the WeChat-Channel dataset reaches the best when the number of interests is 4, and reaches the best on the MX-TakaTak dataset when the number of interests is 8. This shows that our model is reasonable and effective to disentangle multiple interests from user interaction sequences and utilize them to achieve recommendations.\nImpact on Recommendation Diversity. We conduct a case study to compare the recommendation diversity of single-interest modeling with multi-interest modeling. We randomly sample a user. After setting the number of interests to 1 and 4, OPAL outputs 200 micro-videos as recommendation results, respectively. In particular, when the number of interests is 4, the number of micro-videos recalled by each interest embedding is 38, 63, 52 and 47, respectively. Since the labels of micro-videos are often missing or inaccurate, we use SPPMI value [23] to measure the correlation between micro-videos. The higher the SPPMI value, the greater the"}, {"title": "IV. CONCLUSION", "content": "This paper proposes a micro-video recommendation model OPAL. OPAL relys on hyper-categories information of micro-videos in positive interaction sequences to distinguish multiple interests for the user and adopts pre-train plus fine-tune to form multiple soft and hard interest embeddings of the user. In particular, in OPAL, the recommendation results are recalled using Faiss with multiple interest embeddings as input, increasing the diversity of micro-videos, and the future-item prediction task, which is closer to the real scenarios, is introduced to optimize the model, improving the recommendation performance. Results of experiments on real-world datasets demonstrate the feasibility and effectiveness of OPAL."}]}