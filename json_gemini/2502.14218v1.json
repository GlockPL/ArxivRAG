{"title": "RETHINKING SPIKING NEURAL NETWORKS FROM AN ENSEMBLE LEARNING PERSPECTIVE", "authors": ["Yongqi Ding", "Lin Zuo", "Mengmeng Jing", "Pei He", "Hanpu Deng"], "abstract": "Spiking neural networks (SNNs) exhibit superior energy efficiency but suffer from limited performance. In this paper, we consider SNNs as ensembles of temporal subnetworks that share architectures and weights, and highlight a crucial issue that affects their performance: excessive differences in initial states (neuronal membrane potentials) across timesteps lead to unstable subnetwork outputs, resulting in degraded performance. To mitigate this, we promote the consistency of the initial membrane potential distribution and output through membrane potential smoothing and temporally adjacent subnetwork guidance, respectively, to improve overall stability and performance. Moreover, membrane potential smoothing facilitates forward propagation of information and backward propagation of gradients, mitigating the notorious temporal gradient vanishing problem. Our method requires only minimal modification of the spiking neurons without adapting the network structure, making our method generalizable and showing consistent performance gains in 1D speech, 2D object, and 3D point cloud recognition tasks. In particular, on the challenging CIFAR10-DVS dataset, we achieved 83.20% accuracy with only four timesteps. This provides valuable insights into unleashing the potential of SNNs.", "sections": [{"title": "INTRODUCTION", "content": "As the third generation of neural networks, spiking neural networks (SNNs) transmit discrete spikes between neurons and operate over multiple timesteps (Maass, 1997). Benefiting from the low power consumption and spatio-temporal feature extraction capability, SNNs have achieved widespread applications in spatio-temporal tasks (Wang & Yu, 2024; Chakraborty et al., 2024). In particular, ultra-low latency and low-power inference can be achieved when SNNs are integrated with neuromorphic sensors and neuromorphic chips (Yao et al., 2023; Ding et al., 2024).\nTo advance the performance of SNNs, previous work has improved the training method (Wu et al., 2018; Bu et al., 2022; Zuo et al., 2024b), network architecture (Yao et al., 2023; Shi et al., 2024), and neuron dynamics (Taylor et al., 2023; Fang et al., 2021b; Ding et al., 2023) to significantly reduce the performance gap between SNNs and artificial neural networks (ANNs). Typically, these methods treat the spiking neurons in an SNN as an activation function that evolves over timesteps T, with the membrane potential expressing a continuous neuronal state. In this paper, we seek to rethink the spatio-temporal dynamics of SNNs from an alternative perspective: ensemble learning, and explore the key factor influencing the ensemble to optimize its performance.\nFor an SNN f(\u03b8), its instance f(\u03b8)t produces an output Ot at each timestep t, we consider this instance to be a temporal subnetwork. In this way, we can obtain a collection of T temporal subnetworks with the same architecture f (\u00b7) and parameters \u03b8: {f(\u03b8)1, f(\u03b8)2,\u2026\u2026\u2026 f(\u03b8)T}. In general, the final output of the SNN is the average of the outputs over T timesteps: O = \\frac{1}{T} \\sum_{t=1}^{T} Ot. We view this averaging operation as an ensemble strategy: averaging the different outputs of multiple models improves overall performance (Rokach, 2010; Allen-Zhu & Li, 2020). From this perspective, we can consider an SNN as an ensemble of T temporal subnetworks."}, {"title": "RELATED WORK", "content": "Existing methods for training SNNs avoid the non-differentiability of the spiking neurons either by converting a pre-trained ANN (Rueckauer et al., 2017) or by using the surrogate gradient for direct training (Wu et al., 2018). Conversion-based methods require large latencies and struggle with the temporal properties of SNNs (Deng & Gu, 2021; Bu et al., 2022; kang you et al., 2024), the surrogate gradient-based methods are widely used as they can achieve decent performance with smaller latencies (Taylor et al., 2023; Zuo et al., 2024a; Hu et al., 2024). In addition to training methods, previous work has focused on improving network architectures and spiking neuron dynamics, such as the Spiking Transformer architecture (Yao et al., 2023; Shi et al., 2024), the ternary spike (Guo et al., 2024), and the attention spiking neuron (Ding et al., 2023). Compared to existing methods, we rethink the spatio-temporal dynamics of the SNN from the perspective of ensemble learning, identify the key factor affecting its performance: the excessive difference in membrane potential distribution, and propose solutions. Our solutions do not modify the core philosophies of these existing methods and are therefore compatible with a wide range of architectures and neuron types, and integration with existing methods can further unleash the potential of SNNs."}, {"title": "ENSEMBLE LEARNING", "content": "Ensemble learning aggregates the predicted outputs of multiple models to improve the performance of a deep learning model (Rokach, 2010; Allen-Zhu & Li, 2020). To reduce ensemble overhead, some methods use a backbone network and multiple heads to produce multiple outputs (Tran et al., 2020; Ruan et al., 2023), or use checkpoints during training for the ensemble (Furlanello et al., 2018; Lee et al., 2022). In the field of SNNs, previous studies have ensembled multiple SNN models to improve performance without optimizing the ensemble overhead (Neculae et al., 2021; Elbrecht et al., 2020; Panda et al., 2017; Hussaini et al., 2023). In this paper, we consider each timestep SNN instance as a temporal subnetwork and treat the entire SNN as an ensemble, thus avoiding additional ensemble overhead. A previous study (Ren et al., 2023) attributed the effectiveness of SNNs in static point cloud classification to the ensemble effect, without further analysis. Instead, we point out the key factor influencing the ensemble performance: excessive differences in membrane potential distributions can lead to unstable outputs of these subnetworks, and propose solutions to mitigate this problem, thereby improving the performance. Moreover, our experiments on various tasks suggest that this ensemble instability is ubiquitous and should be highlighted rather than simply ignored.\nIt is worth noting that previous ANN ensemble methods increase diversity within reasonable limits to promote generalization (Ali et al., 2021; Zhang et al., 2020). Instead, we take a different philosophy in SNNs, reducing difference rather than increasing diversity to promote stability, because the temporal subnetworks in SNNs are already beyond the limits of effective ensemble, and excessive diversity will only degrade overall performance. The necessity to reduce the cross-timestep differences of the SNN is discussed in detail in Appendix A.1."}, {"title": "TEMPORAL CONSISTENCY IN SNNS", "content": "Previous studies have shown that promoting temporal consistency can improve the performance of SNNs, such as distillation (Zuo et al., 2024a; Dong et al., 2024) and contrastive learning (Qiu et al., 2024) in the temporal dimension. However, existing methods directly promote output/feature consistency, similar to ANNs, without adequately considering the properties of SNNs. In contrast to existing methods, this paper highlights the negative impact of differences in membrane potential distributions across timesteps from an ensemble perspective and proposes to improve distribution consistency. Compared to output/feature consistency, membrane potential distribution consistency offers significant performance gains and can be combined with them to synergistically maximize performance."}, {"title": "METHOD", "content": "In this section, we describe the temporal dynamics of the SNN from the basic LIF neuron model and interpret it as the ensemble of multiple temporal subnetworks. We then show that excessive differences in membrane potentials across timesteps affect ensemble performance, and we mitigate this problem by adaptively smoothing membrane potentials. In addition, we encourage temporally adjacent subnetworks to produce stable and consistent outputs through distillation, further facilitating ensemble stability and performance."}, {"title": "SNN AS AN ENSEMBLE OF TEMPORAL SUBNETWORKS", "content": "SNNs transmit information by generating binary spikes from spiking neurons. In this paper, we use the most commonly used LIF neuron model (Wu et al., 2018). LIF neurons continuously receive inputs from presynaptic neurons accumulating membrane potential H, and a spike S is fired and resets the membrane potential when it reaches the firing threshold v. The LIF neuron dynamics can be expressed as:\nH(t) = U(t) + I(t) = \\begin{cases}\n(1-\\frac{1}{\u03c4})H(t-1) + I(t), charge\\\\\n\\end{cases}\nS(t) = \\begin{cases}\n1, H(t) > v\\\\0, H(t) < v\n\\end{cases}, fire spike\nH(t) = H(t) - S(t)v, reset\nwhere l, i, and t denote the layer, neuron, and timestep indexes, respectively, and I(t) = \\sum_{l'} W^{l,l'-1}S^{l'-1}(t) is the cumulative current of the previous layer's neuron outputs and weights. \u03c4 is the time constant that controls the decay of the membrane potential with time. U(t) is the initial membrane potential at timestep t.\nFrom Eq. 1, we can see that the output S(t) of the neuron at timestep t depends on its initial membrane potential U(t). The membrane potential evolves continuously, so the SNN output varies from timestep to timestep. Established methods recklessly compute the average over T timesteps outputs \u039f = \\frac{1}{T} \\sum_{t=1}^{T} Ot without considering the rationale behind it, leading to suboptimal performance, whereas we explore this from an ensemble perspective to improve the performance of SNNs.\nWe consider each timestep of the SNN as a temporal subnetwork sharing the architecture f(\u00b7) and the parameter \u03b8, and different subnetworks produce different outputs Ot arising from distinct neuron membrane potentials U(t). When the membrane potential difference of these subnetworks is small, their outputs vary slightly, and the ensemble can promote the generalizability of the SNN. However, excessive differences in membrane potentials can lead to drastically different outputs from these subnetworks and degrade the SNN. Unfortunately, vanilla SNNs seem to suffer from this degradation, especially since the initial membrane potential is usually set to 0 (U(t)), the membrane potentials of the first two timesteps show drastic differences, see Fig. 1. This membrane potential discrepancy leads to highly unstable outputs across timesteps, which increases the optimization difficulty and degrades the ensemble performance. In addition, we show the performance of the trained SNN with different inference timesteps in Table 1, and the results show that the output of the vanilla SNN at the first timestep is not discriminative at all."}, {"title": "MEMBRANE POTENTIAL SMOOTHING", "content": "To mitigate the above degradation problem, a natural solution is to randomly initialize the membrane potential to reduce the membrane potential difference for the first two timesteps, similar to (Ren et al., 2023). However, the experiments we show in Table 1 indicate that this practice (Random MP) only slightly alleviates the problem and still struggles to achieve satisfactory performance. To this end, we propose membrane potential smoothing, which mitigates degradation by adaptively reducing the membrane potential difference between adjacent timesteps using a learnable smoothing coefficient.\nAt each timestep, we consider the initial membrane potential state U(t) of the spiking neuron as the initial state of the corresponding subnetwork. We argue that by allowing these subnetworks to have similar initial states, the output spikes generated after receiving the input current will also be similar, resulting in stable SNN outputs (The input currents typically follow the same distribution due to the normalization layer). Therefore, we weight the current initial state by the smoothed state of the previous timestep with a layer-shared smoothing coefficient \u03b1l to reduce the difference between the two. The smoothed membrane potential H(t) receives the input current from the previous layer, which then generates spikes and resets the membrane potential, iterating to the next timestep. The membrane potential smoothing and the charge dynamics of a spiking neuron can be expressed as:\n\u0124(t) = \u03b1^{l}\u0124(t \u2212 1) + (1 \u2212 \u03b1^{l})U(t), smoothing\nH(t) = H(t) + I(t).charge\nThe smoothing coefficient \u03b1l and the parameter \u03b8 of the SNN are co-optimized during training to achieve the optimal smoothing effect. To ensure that \u03b1 \u2208 (0,1), in the practical implementation we train the parameter \u03b2l and let \u03b1 = sigmoid(\u03b2l). By default, \u03b2l is initialized to 0, i.e. the initial value of \u03b1l is 0.5. In Section 4.3, we will analyze the influence of the initial value of \u03b1 on the performance and convergence. Since the spike activity (Eq. 2) is not differentiable, we use the rectangular function (Wu et al., 2018) to calculate the spike derivative:\n\\frac{\u2202S(t)}{\u2202H(t)} = \\frac{\u2202h(H(t), v)}{\u2202H(t)} = \\frac{1}{a}sign(H(t) \u2212 v, -\\frac{a}{2} < \u03c5 < \\frac{a}{2}),\nwhere a is the hyperparameter that controls the shape of the rectangular function and is set to 1.0. Accordingly, the derivative of a spike with respect to \u03b1l can be calculated as:\n\\frac{\u2202S(t)}{\u2202\u03b1^{l}} = \\frac{\u2202S(t)}{\u2202H(t)} \\frac{\u2202H(t)}{\u2202\u0124(t)} \\frac{\u2202\u0124(t)}{\u2202\u03b1^{l}} = \\frac{1}{a}sign({H}(t) - v) ((\u0124(t-1) - U(t)).\nThe derivative of the loss function L with respect to \u03b1l can be calculated as:\n\\frac{\u2202L}{\u2202\u03b1^{l}} = \\sum_{t}^{T} \\sum_{i}^{\u03c0_{l}} \\frac{\u2202L}{\u2202S^{l}(t)} \\frac{\u2202S^{l}(t)}{\u2202\u03b1^{l}},"}, {"title": "TEMPORALLY ADJACENT SUBNETWORK GUIDANCE", "content": "Membrane potential smoothing aims to pull together the initial states of the subnetworks. In addition, we propose the temporally adjacent subnetwork guidance to further promote the output stability of these subnetworks and improve the ensemble performance.\nInspired by knowledge distillation (Hinton, 2015), we guide the output by identifying the \"teacher\" and \"student\" from two temporally adjacent subnetworks. Since spiking neurons need to accumulate membrane potentials before they can produce stable spiking outputs, we treat the early timestep subnetwork as a weak \u201cstudent\" that is guided by the stable \"teacher\u201d with a later timestep. Taking the t-th and t + 1-th subnetworks as an example, the output probabilities are first calculated based on the logits Ot and Ot+1 of the two subnetworks, respectively:\np(t) = \\frac{e^{Ot,j/TKL}}{\\sum_{c=1}^{C} e^{Ot,c/TKL}}, p(t + 1) = \\frac{e^{Ot+1,j/TKL}}{\\sum_{Ot+1,c/TKL}},\nwhere C denotes the C-way classification task, the subscript j indicates the j-th class, and TKL is the temperature hyperparameter set to 2. We then use KL divergence to encourage the output probability of subnetwork t to be as similar as possible to the output probability of subnetwork t + 1 (For the regression task of predicting continuous values, we compute the output MSE loss directly, as in the object detection task in Appendix A.7):\nL_{t} = TKL^2KL(p(t + 1)||p(t)) = TKL^2 \\sum_{j=1}^{C} p(t+1)_{j}log(\\frac{(p(t+1)_{j})}{p(t)_{j}}).\nBy performing guidance between each pair of temporally adjacent subnetworks, we obtain a total of T-1 guidance losses: {L1, L2,\uff65\uff65\uff65, LT\u22121}. Instead of accumulating all these losses directly, we keep the largest one and drop the others with a probability P, as in (Sariyildiz et al., 2024). This promotes consistency while preserving diversity among subnetworks, rather than eliminating differences altogether, thus ensuring generalization of the ensemble. In this paper, P is set to 0.5.\nWe define the function that selects the largest loss and randomly discards the others as drop(\u00b7) (see Appendix A.4 for pseudocode). During training, drop({L1, L2,\u00b7\u00b7\u00b7, LT-1}) and cross-entropy loss LCE synergistically train the SNN:\nL_{total} = \u03b3L_{guidance} + L_{CE} = \u03b3drop({L1, L2,\u2026, LT-1}) + L_{CE},\nwhere \u03b3 is the coefficient for controlling the guidance loss, which is set to 1.0 by default.\nIn this way, we synergistically increase the stability of the ensemble at both the level of the initial state (membrane potential) and the output of the subnetwork, greatly improving the overall performance of the SNN. We show the pseudocode for the training process of the temporally adjacent subnetwork guidance in Algorithm 1."}, {"title": "EXPERIMENTS", "content": "We have conducted extensive experiments with various architectures (RNN, VGG, ResNet, Transformer, PointNet++) on neuromorphic speech (1D), object (2D) recognition, and 3D point cloud classification tasks. Please see the Appendix A.5 for detailed experimental setup."}, {"title": "ABLATION STUDY", "content": "We perform ablation studies using the VGG-9, ResNet-18, and SpikingResformer (Shi et al., 2024) architectures on the neuromorphic object recognition benchmark datasets CIFAR10-DVS (Li et al., 2017) and DVS-Gesture (Amir et al., 2017). The results in Table 2 show that excessive differences in membrane potentials across timesteps are prevalent across different architectures and datasets, and that our solution consistently mitigates this problem. The visualization of the membrane potential distribution before and after smoothing is shown in Fig. 3, and it can be seen that smoothing does indeed reduce the distribution difference."}, {"title": "OUTPUT VISUALIZATION", "content": "We have visualized the output of the SNN in Fig. 4 with 2D t-distributed stochastic neighbor embedding (t-SNE) to show the improvement in output stability and distinguishability with our method. In Fig. 4(Top), the output of the vanilla SNN at each individual timestep varies widely, and the output of the first two timesteps in particular is confusing. This results in a poor distinguishability of its ensemble average output, which limits the recognition performance. Our SNN on the other hand, has more stable outputs across timesteps, and in particular the outputs generated at the first two timesteps are also well distinguished, as shown in Fig. 4(Bottom). Benefiting from the stability across timesteps, our final output is more spread across different classes of clusters and more compactly distributed within the clusters, yielding better performance. Please see Appendix A.10 for more visualization comparisons."}, {"title": "SMOOTHING COEFFICIENT ANALYSIS", "content": "By default, \u03b1 is initialized to 0.5. To analyze the influence of the initialization value on \u03b1, we visualize the optimization trend of \u03b1 in each layer of VGG-9 trained on CIFAR10-DVS in Fig. 5,"}, {"title": "INFLUENCE OF HYPERPARAMETERS", "content": "To explore the influence of hyperparameters on the performance of the proposed method, we show in Fig. 6 the performance of VGG-9 on CIFAR10-DVS with different hyperparameter settings. We increased the timestep from 3 to 8 and found that the overall performance of the model also gradually increased and then saturated, and the average accuracy reached a maximum of 77.4% when the timestep was 7, as shown in Fig. 6(a). Compared to the vanilla SNN, our method consistently shows better performance."}, {"title": "COMPARISON WITH EXISTING METHODS", "content": "Time-dependent speech recognition tasks require models with complex temporal processing capabilities, so stability across timesteps is extremely important. The comparative results with other SNNs on the SHD dataset are shown in Table 5. Our method achieves a recognition accuracy of 91.19% with a lightweight RNN architecture, outperforming other methods. This confirms the effectiveness of our method on long time sequences.\nAs shown in Table 4, our VGG-9 achieves an accuracy of 76.77% and 93.23% on CIFAR10-DVS and DVS-Gesture, respectively, with only 5 timesteps. Using the Transformer architecture, we achieved accuracies of 80.60% and 94.44%, respectively, significantly exceeding other methods. Specifically, with data augmentation, we were able to achieve 83.20% accuracy on the CIFAR10-DVS with 4 timesteps. In addition, we conducted experiments on N-Caltech101 (Orchard et al., 2015) and achieved 82.71% accuracy using VGG-9. Note that we only used vanilla LIF neurons and the general training method, which can further improve the performance when combined with other methods. For example, employing the knowledge transfer strategy (He et al., 2024), we were able to increase our accuracy to 93.68%.\nFor the challenging point cloud classification, we performed experiments on the ModelNet10/40 datasets (Wu et al., 2015) using the lightweight PointNet++ architecture (Qi et al., 2017), and the comparative results are shown in Table 6. With T = 2, our method achieves 94.54% and 91.13% accuracy, respectively, outperforming other SNN models. To evaluate the one-timestep inference performance, we directly infer the trained two-timestep model with one timestep. This preserves the benefits of our method while avoiding additional training overhead. The results show that even with only one timestep, we can achieve accuracies of 94.39% and 89.82%, which still outperform other SNNs."}, {"title": "CONCLUSION", "content": "In this paper, we highlight a key factor that is generally overlooked in SNNs: excessive differences in membrane potential distributions can lead to unstable outputs across timesteps, thereby affecting performance. To mitigate this, we propose membrane potential smoothing and temporally adjacent subnetwork guidance to facilitate consistency of initial membrane potentials and outputs, respectively, thereby improving stability and performance. Meanwhile, membrane potential smoothing also facilitates the propagation of forward information and backward gradients, mitigating the temporal gradient vanishing and providing dual gains. Extensive experiments on neuromorphic speech (1D)/object (2D) recognition and 3D point cloud classification tasks confirmed the effectiveness and versatility of our method. We expect that our work will inspire the community to further analyze the spatio-temporal properties of SNNs."}, {"title": "APPENDIX", "content": "The Appendix section provides detailed theoretical analysis, neural dynamics, and additional experiments described below:\n\u2022 Appendix A.1 details and experimentally confirms the necessity to reduce the variance across timesteps in SNNs.\n\u2022 Appendix A.2 shows the dynamics of LIF neurons with integrated membrane potential smoothing.\n\u2022 Appendix A.3 analyzes the effectiveness of membrane potential smoothing to mitigate the temporal gradient vanishing.\n\u2022 Appendix A.4 provides pseudo-code for randomly dropping guidance losses and temporally adjacent subnetwork guidance.\n\u2022 Appendix A.5 provides experimental details such as the dataset and training setup.\n\u2022 Appendix A.6 shows the additional ablation studies and analysis of our method.\n\u2022 Appendix A.7 demonstrates the application of the temporally adjacent subnetwork guidance to the object detection task.\n\u2022 Appendix A.8 presents the analysis of the effectiveness and power consumption of our method for the spiking Transformer model with different timesteps.\n\u2022 Appendix A.9 provides additional experiments on the static image datasets.\n\u2022 Appendix A.10 shows additional visualizations to demonstrate the effectiveness of our method in promoting consistency in the membrane potential distribution."}, {"title": "THE NECESSITY TO REDUCE CROSS-TIMESTEP DIFFERENCES IN SNNS", "content": "Previous research on ensemble learning has shown that increasing the diversity of members in an ensemble contributes to overall generalization under certain constraints (Krogh & Vedelsby, 1994; Frankle et al., 2020; Zhang et al., 2020). In contrast, we point out that the differences in the output of the SNN across timesteps should be reduced, which at first glance seems to contradict the idea of ensemble learning. In this section, we will elucidate the necessity to reduce the differences in SNNs across timesteps by analyzing the constraints of ensemble learning and experimental results.\nPrevious studies have shown that the solutions learned by Stochastic Gradient Descent (SGD) lie on a nonlinear manifold (Draxler et al., 2018; Garipov et al., 2018; Nguyen, 2019) and that converged models with a common initial optimization path are linearly connected with a low-loss barrier (Frankle et al., 2020). Multiple optimal solutions that are linearly connected belong to a common basin that is separated from other regions of the loss landscape (Jain et al., 2023). The linearly connected multiple models in a common basin can thus be ensembled for additional gains. From this we derive a constraint that is important for ensemble, but typically overlooked: The members of the ensemble should be converged optimal solutions, or at the very least optimized feasible solutions. However, it is possible that a particular temporal submodel in the SNN is not a feasible solution. For example, the results in Table 1 show that the SNN submodel is only 10% accurate at the first timestep on the ten-class CIFAR10-DVS dataset, which is equivalent to a randomly initialized network that is far from an optimal or feasible solution. At this point, it has stepped outside the constraints of the ensemble member, so it cannot continue to increase the diversity of its members as it would in normal practice. Instead, we can make the infeasible solution gradually converge to the feasible solution by decreasing the difference between this overly outlier submodel (or infeasible solution) and the other submodels (feasible solutions). Finally, these feasible solutions can be used as an effective ensemble to improve overall stability and performance.\nFrom another point of view, it is difficult to have both performance and diversity of tasks in an ensemble. To balance performance and diversity, existing methods use multiple losses during training to guide the optimization of the neural network ensemble (Ali et al., 2021; Zhang et al., 2020). Following (Zhang et al., 2020), we divide the training loss of an ensemble into three parts (Take the classification task as an example): i) the cross-entropy loss of ensemble members and labels; ii) the diversity of ensemble members; and iii) the aggregated loss of the ensemble.\n\u2022 The cross-entropy loss of ensemble members and labels can be calculated as\nLs = H(q(T(X)), Y),\nwhere X is the input, Y is the label, Ti is the i-th member model, q(\u00b7) is the normalization function, and H is the cross entropy.\n\u2022 The diversity of ensemble members can be calculated as\nLd = 1-\\frac{1}{N} \\sum_{1<i\u2260j<N}q (T_{i}(X)) q (T_{j}(X)),\nwhere N is the number of members in the ensemble.\n\u2022 The aggregated loss of the ensemble can be calculated as\nLa = H(\\sum_{i}^{N} \u03b3_{i} T_{i}(X), Y),\nwhere yi is the integration weight, bounded by \\sum_{i}^{N} \u03b3_{i} = 1.\nThe total loss during training is\nL_{ensemble} = Ls + La - \u03b1Ld,\nwhere a is the coefficient controlling the increase in diversity that can be adaptively updated in (Zhang et al., 2020) with training iterations.\nFrom Eq. 15, we can see that ensemble learning cannot blindly increase the diversity of its members, but must strike a balance between overall performance and diversity. Our method,"}, {"title": "INTEGRATING MEMBRANE POTENTIAL SMOOTHING IN LIF NEURONS", "content": "In this paper, membrane potential smoothing is integrated into commonly used LIF neurons. The neuron dynamics after integration can be expressed as:\nU(t) = (1-\\frac{1}{\u03c4})H(t-1), leakage\nH(t) = \u03b1^{l}H(t \u2212 1) + (1 \u2212 \u03b1^{l})U(t), smoothing\nH(t) = H(t) + I(t), charge\nS(t) = \\begin{cases}\n1, H(t) \u2265 v\\\\0, H(t) < v\n\\end{cases}, fire spike\nH(t) = H(t) - S(t)v, reset\nwhere H is the smoothed membrane potential. Note that H(t \u2212 1) does not exist when t = 0, at which point the dynamics of the neuron are the same as the original LIF dynamics. When t = 1, H(t \u2212 1) = H(0) is set to the initialized value of membrane potential H, which is 0 in this paper."}, {"title": "MEMBRANE POTENTIAL SMOOTHING MITIGATES TEMPORAL GRADIENT VANISHING", "content": "When training SNNs directly with the surrogate gradient, the gradient propagates backward in both spatial and temporal domains. Taking the vanilla LIF neuron as an example, we convert Eq. 1 to Eq. 3 into the following form to illustrate the temporal gradient vanishing problem:\nH'(t) = (1 - \\frac{1}{\u03c4})(H'(t \u2212 1) \u2013 vs(t \u2212 1)) + W's^{l-1}(t),\nThe gradient of the loss L with respect to the weights Wl over T timesteps is:\n\u25bdW^{l}L = \\sum_{t=0}^{T} \\frac{\u2202L}{\u2202H^{l}(t)} \\frac{\u2202H^{l}(t)}{\u2202W^{l}} = \\sum_{t=0}^{T} \\frac{\u2202L}{\u2202H^{l}(t)} s^{l-1}[t], l = L, L \u2212 1, \u2026\u2026\u2026, 1,\nwhere L is the total number of layers in the SNN.\nThe derivative of the loss L with respect to the membrane potential H\u00b2(t) is:\n\\frac{\u2202L}{\u2202H^{l}(t)} = \\frac{\u2202L}{\u2202S^{l}(t)} \\frac{\u2202S^{l}(t)}{\u2202H^{l}(t)} +\\sum_{t'=t+1}^{T} \\frac{\u2202L}{\u2202H^{l+1}(t')} \\frac{\u2202H^{l+1}(t')}{\u2202S^{l}(t')} \\frac{\u2202S^{l}(t')}{\u2202H^{l}(t')}, l = L,,\n\\frac{\u2202L}{\u2202H^{l}(t)} = \\frac{\u2202L}{\u2202S^{l}(t)} \\frac{\u2202S^{l}(t)}{\u2202H^{l}(t)} +\\sum_{t'=t+1}^{T} \\frac{\u2202L}{\u2202H^{l+1}(t')} \\frac{\u2202H^{l+1}(t')}{\u2202S^{l}(t')} \\frac{\u2202S^{l}(t')}{\u2202H^{l}(t')} \\prod_{t\"=1}^{t'-t} \\frac{\u2202H^{l}(t\")}{\u2202S^{l}(t\")}, l < L,\nwhere the blue portion on the left indicates the spatial gradient and the green portion on the right indicates the temporal gradient. e'(t) is defined as the sensitivity of the membrane potential H\u00b2(t+ 1) to H\u00b9(t) in adjacent timesteps (Meng et al., 2023; Huang et al., 2024):\ne^{l}(t) = \\frac{\u2202H^{l}(t+1)}{\u2202H^{l}(t)} = \\frac{\u2202H^{l}(t+1)}{\u2202S^{l}(t)} \\frac{\u2202S^{l}(t)}{\u2202H^{l}(t)}\nAs can be seen in Eq. 25, the sensitivity e'(t) controls the percentage of the temporal gradi- ent in the total gradient. However, for typical surrogate gradient settings, the diagonal matrix \\prod_{t\"=1}^{t'-t} e^{l}(t' \u2013 t\") has only a small spectral norm (Meng et al., 2023). To illustrate, for the rectangular function used in Eq. 6, when a = 9, the diagonal elements of e'(t) are\ne^{l}(t)_{jj} = \\begin{cases}\n1-\\frac{1}{a}, -\\frac{a}{2} <H^{j}(t) < \\frac{a}{2},\n\\end{cases}\nTypically, \\frac{1}{\u03c4} is set to values less than 1, such as 0.5 and 0.25 (Ding et al., 2024; Guo et al., 2022), which causes the diagonal values of the matrix \\prod_{t\"=1}^{t'-t} e^{l}(t' \u2013 t\") to become smaller as t' \u2013 t increases, eventually causing the temporal gradient vanishing and degrading performance.\nWhen membrane potential smoothing is used, the neuron at each timestep is smoothed by the smoothed membrane potential of the previous timestep, and the modified LIF neuron dynamics are shown in Appendix A.2. When t > 0, the sensitivity of the membrane potential at timestep t + 1 with respect to the membrane potential at timestep t can be calculated as:\nE(\u2206t = 1) = \\frac{\u2202H^{l}(t+1)}{\u2202H^{l}(t)} = \\frac{\u2202H^{l}(t+1)}{\u2202\u0124^{l}(t + 1)} \\frac{\u2202\u0124^{l}(t + 1)}{\u2202H^{l}(t)} = (1 \u2212 \u03b1)\u03b5^{l}(t).\nWhen the timestep interval is 2, the sensitivity of the temporal gradient is:\nE(\u2206t = 2) = \\frac{\u2202H^{l}(t + 2)}{\u2202H^{l}(t)} = \\frac{\u2202H^{l}(t + 2)}{\u2202\u0124^{l}(t + 2)} \\frac{\u2202\u0124^{l}(t + 2)}{\u2202H^{l}(t + 1)} + \\frac{\u2202H^{l}(t + 2)}{\u2202\u0124^{l}(t + 2)} \\frac{\u2202\u0124^{l}(t + 1)}{\u2202H^{l}(t)}\n= \u03b1(1 \u2212 \u03b1)\u03b5^{l}(t) + (1 \u2212 \u03b1)\u03b5^{l}(t)E^{l}(\u2206t = 1)\n= (\u03b1 + (1 \u2212 \u03b1)\u03b5^{l}(t))\u03b5^{l}(\u2206t = 1).\nBy iterating over timestep according to the chain rule, the sensitivity of the temporal gradient can be obtained as\nE^{l}(\u2206t) = (1 \u2013 \u03b1)\u03b5^{l}(t)(\u03b1 + (1 \u2212 \u03b1)\u03b5^{l}(t))^{\u2206t-1}"}, {"title": "PYTORCH-STYLE CODE"}]}