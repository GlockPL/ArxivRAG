{"title": "BOSS: Benchmark for Observation Space Shift in Long-Horizon Task", "authors": ["Yue Yang", "Linfeng Zhao", "Mingyu Ding", "Gedas Bertasius", "Daniel Szafir"], "abstract": "Robotics has long sought to develop visual-servoing robots capable of completing previously unseen long-horizon tasks. Hierarchical approaches offer a pathway for achieving this goal by executing skill combinations arranged by a task planner, with each visuomotor skill pre-trained using a specific imitation learning (IL) algorithm. However, even in simple long-horizon tasks like skill chaining, hierarchical approaches often struggle due to a problem we identify as Observation Space Shift (OSS), where the sequential execution of preceding skills causes shifts in the observation space, disrupting the performance of subsequent individually trained skill policies. To validate OSS and evaluate its impact on long-horizon tasks, we introduce BOSS (a Benchmark for Observation Space Shift). BOSS comprises three distinct challenges: \"Single Predicate Shift\", \"Accumulated Predicate Shift\", and \"Skill Chaining\", each designed to assess a different aspect of OSS's negative effect. We evaluated several recent popular IL algorithms on BOSS, including three Behavioral Cloning methods and the Visual Language Action model OpenVLA. Even on the simplest challenge, we observed average performance drops of 67%, 35%, 34%, and 54%, respectively, when comparing skill performance with and without OSS. Additionally, we investigate a potential solution to OSS that scales up the training data for each skill with a larger and more visually diverse set of demonstrations, with our results showing it is not sufficient to resolve OSS. The project page is: https://boss-benchmark.github.io/", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in robotic learning have demonstrated its potential in diverse manipulation applications [1], including manufacturing [2], sports [3], [4], and household tasks [5], [6]. Imitation Learning (IL), which empowers end users to teach robot skills and behaviors through demonstrations, has become a prevalent approach in developing various skill controllers [7], [8]. However, IL algorithms remain limited to relatively short-horizon tasks due to covariate shift [9], [10], where small discrepancies in action predictions accumulate over time. This challenge becomes even more pronounced in long-horizon tasks, as such errors propagate across multiple sequential steps, creating a significant barrier to achieving general-purpose robots. Task and Motion Planning (TAMP) [11], [12] and recent advancements in large language model based planning [13]\u2013[15] offer a solution by integrating IL into a hierarchical framework [16]\u2013[18], decomposing end-to-end long-horizon IL into high-level task planning and low-level visuomotor skill execution."}, {"title": "II. RELATED WORK", "content": "Skill chaining, the sequential execution of pre-learned skills, is a simple yet powerful structure for tackling complex long-horizon tasks [19], [20]. However, failures often arise in a skill chain when a skill encounters initial states that were not seen during training [21], [22]. Specifically, the terminal state of a preceding skill may fall outside the initial state distribution the next skill policy was trained on, leading to task failure. For instance, consider a household long-horizon task illustrated in Fig. 1, which involves two skills: $PlaceObject(potato, bowl)$ and $MoveContainer(bowl, cabinet)$. When training the visuomotor policy for $MoveContainer(bowl, cabinet)$, the initial state set excludes visual states (e.g., images) containing anything inside the bowl. However, the terminal state of $PlaceObject(potato, bowl)$ places a potato in the bowl, creating a visual mismatch that can cause the $MoveContainer(bowl, cabinet)$ policy to fail. We refer to this issue as Observation Space Shift (OSS), a problem that frequently arises during skill transitions in visual-input long-horizon tasks. A formal definition of OSS is provided in Section III.\nAs a result, ensuring smooth skill transitions is critical in skill chaining and serves as a key factor for completing long-horizon tasks [23]. Previous researchers tackle the skill transition problem through two main strategies. The first line of offline approaches involves learning a transition policy to shift\nthe state from the terminal state of one skill to the initial state of the next [21], [24], [25]. The second approach ensures that the initial state set of the next skill matches the terminal state set of the previous skill by fine-tuning the policy of the preceding skill, the next skill, or both in an online manner [23], [26], [27]. However, all these previous works design observation spaces that exclude visual inputs, causing the above methods, whether using offline or online learning, to rely on assumptions that are often impractical for visuomotor IL policies. 1) Offline methods assume that the initial states for each skill are always reachable, which is often unreasonable for visual-input long-horizon tasks, as it may require undoing previously completed skills. For example, as shown in Figure 1, offline methods would need to learn a transition policy to remove the potato from the bowl in order to enable the observation space to match the initial state of $MoveContainer(bowl, cabinet)$ (i.e., an empty bowl), which is logically invalid. 2) Online approaches, on the other hand, assume that policies can be continuously fine-tuned to handle new initial states. This assumption is equally problematic for visual-input long-horizon tasks, where OSS is highly likely to happen at each skill transition. The frequent occurrence of OSS necessitates repeated fine-tuning, leading to substantial computational overhead and inefficiency."}, {"title": "III. PROBLEM DEFINITION", "content": "We focus on long-horizon tasks that chain skill policies trained through imitation learning using visual inputs, assuming a robust planner generates the skill sequence based on a high-level task goal.\nWe first model the environment as a Partially Observ-able Markov Decision Process (POMDP), defined as $\\mathcal{M} = (\\mathcal{S},\\mathcal{O},\\mathcal{A},\\mathcal{T}, \\mathcal{Z},r,\\gamma)$. Here, $\\mathcal{S}$, $\\mathcal{O}$, and $\\mathcal{A}$ represent the state, observation, and action spaces, respectively. The transition and observation probabilities are $\\mathcal{T}(s_{t+1} | s_t,a_t)$ and $\\mathcal{Z}(o_t | s_t)$. The reward function $r(s_t, a_t, s_{t+1})$ and discount factor $\\gamma$ guide the trade-off between immediate and future rewards. In our setup, the observation space $\\mathcal{O}$ includes both third-person camera views and proprioceptive states. The objective is, for each skill, to learn a policy $\\pi : \\mathcal{O} \\rightarrow (\\mathcal{A})$ that replicates demonstrated behavior from observation-action pairs $\\mathcal{T} = (o_0, a_0,\u00b7\u00b7\u00b7, o_t, a_t)$, without explicit access to $\\mathcal{S}$.\nA significant problem arises during deployment when chaining skill policies, which we define as the Observation Space Shift (OSS). We adopt the framework of Task and Motion Planning (TAMP) to formalize this problem [11], [12].\nWe define predicates $\\Psi = {\\psi_1, \\psi_2,..., \\psi_n}$ as properties or relationships within the state space, where each predicate $\\psi_i$ maps a state $s \\in S$ to a truth value, $\\psi_i(s) \\in {0,1}$. Essentially, each predicate acts as a binary classifier over the state space, identifying a subset of $\\mathcal{S}$ where the predicate evaluates to true. As the example shown in Figure 1, the predicate $In(potato, bowl)$ evaluates to 1 if the potato is inside the bowl, and 0 otherwise. Operator is defined as $op = (Pre, Eff,c)$ that comprises three components: preconditions $Pre \\subset \\Psi$, effects $Eff \\subset \\Psi$, and a cost c. Intuitively, an operator specifies when a skill can execute (i.e., $Pre$) and what is expected after execution (i.e., $Eff$). Symbolic operators use preconditions (e.g., $On(bowl, table)$) and effects (e.g., $On(bowl, cabinet)$) to represent only the elements (e.g., bowl instead of potato) necessary for feasibility of a skill (e.g., $MoveContainer(bowl, cabinet)$), abstracting away irrelevant ones (e.g., $In(potato, bowl)$). Formally, a predicate $\\psi_i \\in \\Psi$ is irrelevant for an operator op if $\\psi_i \\notin Pre \\cup Eff$, meaning it does not influence the feasibility or expected outcome of the skill execution. However, these skill-irrelevant elements often appear in visual observations and can be unintentionally modified by preceding skills' effects. Such changes disrupt visuomotor policies, causing failures in skill execution. This problem, where changes in irrelevant predicates within the visual observation space hinder visuomotor performance, is defined as OSS."}, {"title": "IV. THE BOSS BENCHMARK", "content": "We introduce the BOSS benchmark, designed to facilitate a comprehensive empirical study of the OSS problem across modern IL methods. To address this issue, we build the environment using the Libero simulation platform [28] (Section IV-A) and form the task set by leveraging selected existing tasks from Libero and scalably generating their modified versions (Section IV-B). BOSS includes three challenges (Section IV-C ~ IV-E) designed not only to validate the existence of OSS but also to demonstrate its significant negative impact on the success of long-horizon tasks. These challenges evaluate IL methods on the curated tasks within the constructed environment and feature 44, 88, and 10 tasks respectively, providing a diverse and extensive evaluation.\nWe build the environment of BOSS on the Libero plat-form [28], which, although originally designed for lifelong robot learning tasks, provides diverse manipulation scenes featuring a Franka Emika Panda robot arm and accommodates a wide variety of tasks. Libero is highly flexible for creating and customizing tasks because they are generated using Planning Domain Definition Language (PDDL) files, which specify the planning problem including operators, predicates, and their relationships. These features make Libero an excellent foundation for developing BOSS, enabling us to adapt it specifically to study the OSS problem.\nBuilding on the Libero environment, BOSS focuses on studying the impact of OSS on long-horizon tasks. To achieve this, we require a set of atomic robotic tasks, where each task involves only a single skill. These tasks are used to simulate individual skills within a skill chain that are unaffected by OSS. Additionally, we generate modified counterparts to simulate scenarios where OSS occurs. This setup enables performance comparisons between the two sets when evaluating IL methods.\nAll three challenges are based on these 2 sets of tasks. To build the set of tasks unaffected by OSS, we select all the skill-level tasks from Libero-100, the most comprehensive and diverse task suite in Libero, spanning 12 manipulation scenes and covering a wide range of object interactions and motor skills. Multi-skill tasks (e.g., \"open the top drawer of the cabinet and put the bowl in it\") are excluded, while single-skill tasks (e.g., \"open the bottom drawer of the cabinet\") are retained. As a result, the final set comprises 44 single-skill tasks.\nAs outlined in Section III, OSS arises when skill-irrelevant predicates, $\\Psi$, are altered in ways that do not affect the feasibility of the current skill but disrupt the execution of its visuomotor policy. For example, as shown in the left box of Figure 2, the predicate $In(potato, bowl)$ does not impact the feasibility of the current skill, $MoveContainer(bowl, cabinet)$, since it is absent from $Pre$ and $Eff$. However, changes to this predicate can still alter the visuomotor observation space, $\\mathcal{O}$, negatively affecting the skill policy's performance. In the context of skill chaining, a foundational structure in long-horizon tasks, the preceding skill is the most likely to introduce modifications in $\\mathcal{O}$ that induce OSS for the subsequent skill. Therefore, we introduce a challenge, Single Predicate Shift (BOSS-C1), specifically designed to evaluate the impact of OSS caused by single-step transitions on the performance of baseline methods (see examples in Figure 2). To investigate the impact of single-step OSS on baseline methods, we compare their performance on skills unaffected by OSS (e.g., $On(potato, table)$) versus those affected by OSS (e.g., $In(potato, bowl)$). As detailed in Section IV-B, for the former, we evaluate baselines on the 44 selected tasks. For the latter, we use RAMG to generate 44 corresponding randomly modified tasks, each with a single modification applied to simulate the occurrence of OSS caused by the preceding skill.\nThe effect of executing a skill persists until a future skill reverses it, meaning it impacts not only the immediate next skill but also multiple subsequent skills. Over the course of a skill chain, effects from numerous skills can accumulate, resulting in a more severe OSS for a future skill. Formally, given a sequence of operators ${0P_1, OP_2,..., Opt}$, the accumulated effects at time step t are given by $U_1 Eff_i$. As illustrated in the middle box of Figure 2, the accumulated effects, $On(plate, cabinet)$ and $In(potato, bowl)$, collectively impact the current skill, $MoveContainer(bowl, cabinet)$, by modifying predicates in the observation space $\\mathcal{O}$. Thus, it is valuable to study the differences between accumulated OSS and single-step OSS. To address this, we introduce another challenge, Accumulated Predicate Shift (BOSS-C2), specifically designed to analyze the impact of OSS accumulation across preceding skills (check examples in Figure 2).\nThe ultimate goal of addressing the OSS problem is to improve the success of long-horizon robot tasks. To this end, we introduce a challenge, Skill Chaining (BOSS-C3), consisting of 10 long-horizon tasks, each comprising a chain of three skills (check examples in Figure 2). This challenge serves as a straightforward way to demonstrate the impact of the OSS problem on long-horizon task performance.\nTo construct this challenge, we manually select and combine skills from the 44 selected skill-level tasks, ensuring that OSS occurs in each skill while avoiding conflicts between modifications. We reset the robot to a neutral position after each skill to eliminate dynamic transition feasibility issues [26], ensuring the challenge focuses solely on the impact of OSS, which specifically addresses the negative effects of changes in visual observations."}, {"title": "V. THE BOSS EXPERIMENTAL RESULTS", "content": "We select four widely used imitation learn-ing algorithms, representing diverse architectures and design approaches for baseline comparisons, to learn each skill and evaluate the impact of OSS on their performance in long-horizon tasks. Among these, three are Behavioral Cloning approaches from Libero, while one is a vision-language-action model.\nBehavioral Cloning (BC) in Libero: A set of three BC al-gorithms [39] from Libero is adopted: BC-RESNET-RNN [40], BC-RESNET-T [41], and BC-VIT-T [42]. These algorithms feature diverse neural network architectures for visual and language encoding. All three use BERT embeddings [43] to encode the language instructions for each skill. In BC-RESNET-RNN, ResNet [44] serves as the visual backbone for encoding per-step visual observations, while an LSTM processes the sequence of encoded visual embeddings as the temporal backbone. BC-RESNET-T employs the same visual encoder, ResNet, but replaces the LSTM with a transformer decoder [45] for temporal processing. BC-VIT-T uses Vision Transformer (ViT) [46] as the visual backbone and a transformer decoder as the temporal backbone. All these BC algorithms output a multi-modal distribution over manipulation actions using a Gaussian Mixture Model (GMM) output head [47], from which an action is sampled.\nOpenVLA: Applying large foundation models, such as Large Language Models (LLMs) [48] and Vision-Language Models (VLMs) [49], to robotics has gained popularity, leading to vision-language-action (VLA) models. Among these, OpenVLA [8], an open-sourced VLA model, outperforms other state-of-the-art methods, making it a natural choice for evaluating the OSS problem as a representative generalist policy. The language description, conditioned on each task and sourced from Libero, remains consistent, whether or not the task is affected by OSS.\nWe define the metric \"Ratio Performance Delta\" as the relative change in skill success rate caused by OSS. It is calculated as the difference between a baseline's success rate on the original skill-level task and its success rate on the modified task where OSS occurs, normalized by the original success rate. A positive RPD indicates that OSS negatively impacts the current skill, while an RPD less than or equal to zero suggests no negative effect. Instances of negative RPD can occur due to the inherent randomness of IL models.\nUnlike BOSS-C1 and BOSS-C2, which evaluate the performance delta for individual skills, BOSS-C3 assesses the performance delta across an entire skill chain. To support this, we introduce a new metric. First, we define the \"Chain Upper Bound\" as the product of success rates for each skill in the chain when evaluated without OSS occurrence, representing the maximum achievable success rate in the absence of OSS. Then, we introduce the \"Delta to Upper Bound Ratio\", calculated as the difference between the actual success rate of completing the entire skill chain and the \u201cChain Upper Bound\", divided by the \"Chain Upper Bound\", providing a normalized measure of the performance delta for the entire chain.\nFor observation space design, we align with the baselines' default setups. OpenVLA uses only a third-person camera view as its observation space. To maintain consistency in visual observations, we define the observation space for BCs as a combination of third-person camera images, 7-DoF robot arm joint angles, and 2-DoF parallel gripper joint states, excluding only the wrist-camera view. For all the baselines, the action space is defined as a 7-dimensional relative Cartesian displacement (w.r.t. the gripper frame), and the control frequency is set to 20 Hz. For all the experiments, we use three random seeds and report only the averaged results across these runs."}, {"title": "VI. CAN DATA AUGMENTATION MITIGATE OSS?", "content": "As defined in Section III, the OSS problem occurs when visual changes caused by preceding skills disrupt the execution of the current skill. A straightforward solution is to collect demonstrations with more diverse visual content, allowing algorithms to learn from varied observation spaces and become more robust to novel visual modifications. To simulate this approach, we augment the demonstrations and train all the baselines on an expanded dataset.\nUsing RAMG, we generate diverse modified tasks, creating a total of 1,727 new tasks from the 44 selected tasks with a single modification applied to each. For each modified task, we replay demonstrations in the corresponding environment, filtering out failed replays to produce new demonstrations where the trajectory remains unchanged, yet with varied visual observations. This method produces a significantly larger dataset, totaling 57,000 demonstrations\u2014nearly 30 times the size of the original Libero dataset for the 44 selected tasks (2,000 demonstrations). Notably, this figure corresponds to cases with a single modification applied; an even much larger dataset can be generated by iteratively using RAMG to apply multiple modifications. We then pre-train (BCs) or fine-tune (OpenVLA) all baselines using this expanded dataset.\nTo investigate whether the data augmentation method miti-gates the OSS problem, we compare performance between two setups to assess whether improvements exist on skills affected by OSS. Setup A: baselines are trained on original demonstra-tions for a skill and evaluated on the skill affected by OSS. Setup B: baselines are trained on augmented demonstrations and evaluated on the same skill affected by OSS.  presents the performance values for both setups across all baselines, with averaged results reported for all 44 tasks. A comparison between the setups reveals that BC-RESNET-RNN shows a slight improvement under Setup B. However, other baselines, including BC-RESNET-T, BC-VIT-T, and OpenVLA, perform equivalently or worse, suggesting that data augmentation alone is limited in mitigating OSS. While larger and more diverse datasets generally improve performance, they may not fully address OSS. One possible reason is that some baselines struggle to generalize across the diverse visual distributions of demonstrations, leading to even worse performance. Another challenge is the combinatorial explosion of scene and task variations, making it impractical to cover all relevant cases through data augmentation alone. This limitation persists even with RAMG, which is constrained to simulated data generation and cannot scale to real-world data. These findings underscore the need for tailored algorithmic solutions to effectively address OSS."}, {"title": "VII. CONCLUSION", "content": "This work introduces and investigates Observation Space Shift (OSS), a critical issue that hinders the completion of long-horizon robot tasks. Through three challenges in BOSS, we validate the impact of OSS on several the performance of several popular baseline algorithms and present key findings:\n*   OSS is a common and severe problem: Results on BOSS demonstrate that OSS can substantially degrade task completion, especially in long-horizon tasks where visual modifications accumulate over time.\n*   Visual modification magnitude matters, task difficulty may not: Larger visual modifications lead to greater OSS severity, while task difficulty was not significantly correlated, making OSS particularly problematic as task horizons grow.\n*   Data augmentation may not be sufficient: While increasing dataset size and diversity generally improves performance, we found that it did not fully mitigate OSS and may even degrade performance in some cases. A key challenge is that data augmentation alone cannot generate a dataset large and diverse enough to cover the vast combinatorial space of visually varying scene setups, which arise from the numerous skill compositions within long-horizon tasks. Furthermore, current baseline architectures are not explicitly designed to address OSS, highlighting the need for tailored algorithmic solutions, such as mechanisms that focus on task-relevant visual cues, and reduce sensitivity to irrelevant variations.\nThese findings highlight that effectively addressing OSS requires either more advanced methods for scaling the visual diversity of robotic data or innovative algorithmic designs that enhance robustness in long-horizon tasks. We hope our benchmarks and insights will inspire further research toward tackling this critical challenge."}, {"title": "VIII. APPENDIX", "content": "In this section, we investigate two potential factors influenc-ing the severity of OSS: skill difficulty and the magnitude of visual modifications. This analysis aims to provide insights for future research and inform the development of algorithms robust to OSS. We define the following two hypotheses:\nI: Higher skill difficulty (i.e., lower skill success rate) leads to more severe performance degradation caused by OSS for that skill.\nII: Larger magnitudes of visual modifications result in more severe performance degradation caused by OSS for the current skill.\nTo test these hypotheses, we use Spearman's rank correlation coefficient ($\\rho$) [50], which effectively measures the strength of monotonic relationships between two variables. If hypothesis I holds, we expect a significant negative monotonic relationship (i.e., $\\rho < 0$, $p < 0.05$) between skill success rate and the severity of OSS. Similarly, if hypothesis II is valid, we anticipate a significant positive monotonic relationship (i.e., $\\rho > 0$, $p < 0.05$) between the magnitude of visual modifications and the severity of OSS.\n1) Test Hypothesis I (Factor: Task Difficulty): We use the metric \"Ratio Performance Delta\" to quantify the severity of OSS and measure task difficulty using task success rate, assuming demonstrations for all skills are of similar quality. To analyze the relationship between skill difficulty and OSS, we design an experiment that isolates skill difficulty as the only variable. We select six sets of skills, where all skills within each set are performed in the same scene (e.g., kitchen-scene-1 in Libero). Identical modifications are applied to all skills within each set to simulate OSS, resulting in several pairs of skills: one unaffected by OSS and the other affected by OSS. Between pairs, the only difference is the skill's difficulty. After evaluating BCs on these pairs, we generate two lists of results for each set and algorithm: one for success rates of skills unaffected by OSS (i.e., task difficulty) and another for \"Ratio Performance Delta\" on skills affected by OSS (i.e., severity of OSS). These results are used to calculate Spearman's rank correlation coefficient ($\\rho$) and the corresponding p-value.  presents the findings. None of the baselines show a significant negative monotonic relationship (i.e., $\\rho < 0$, $p < 0.05$), indicating no evidence to support the hypothesis that task difficulty is a key factor influencing the severity of OSS.\n2) Test Hypothesis II (Factor: Magnitude of Visual Modi-fication): We also use the metric \u201cRatio Performance Delta\u201d to quantify the severity of OSS. To ensure that the magnitude of visual modification is the only variable changing, we adopt an approach similar to BOSS-C2 by increasing the number of modifications applied to the same skill. However, unlike BOSS-C2, where modifications are generated randomly by RAMG, it is possible for skills with a larger number of modifications (e.g., affecting several small areas) to have a smaller visual modification magnitude than skills with fewer but more significant modifications (e.g., affecting a large area). To address this, we incrementally add new modifications based on existing ones, ensuring a clear correlation between the number of modifications and the size of the visual changes. After evaluating BCs on skills unaffected by OSS and skills with multiple modifications, we generate two lists for each algorithm: one representing the number of modifications (i.e., visual modification magnitude) and the other capturing the RPD (i.e., severity of OSS). Spearman's rank correlation coefficient ($\\rho$) and the corresponding p-value are calculated to analyze the relationship between these two variables. For all BCs, we obtain results as $\\rho = 1.0$ and $p = 0.0$, indicating a significant positive monotonic relationship between visual modification magnitude and the severity of OSS. These findings confirm that the magnitude of visual modifications is a key factor influencing the severity of OSS."}]}