{"title": "TCGPN: Temporal-Correlation Graph Pre-trained\nNetwork for Stock Forecasting", "authors": ["Wenbo Yan", "Ying Tan"], "abstract": "Recently, the incorporation of both temporal features\nand the correlation across time series has become an effective\napproach in time series prediction. Spatio-Temporal Graph Neu-\nral Networks (STGNNs) demonstrate good performance on many\nTemporal-correlation Forecasting Problem. However, when applied\nto tasks lacking periodicity, such as stock data prediction, the effec-\ntiveness and robustness of STGNNs are found to be unsatisfactory.\nAnd STGNNs are limited by memory savings so that cannot han-\ndle problems with a large number of nodes. In this paper, we pro-\npose a novel approach called the Temporal-Correlation Graph Pre-\ntrained Network (TCGPN) to address these limitations. TCGPN uti-\nlize Temporal-correlation fusion encoder to get a mixed representa-\ntion and pre-training method with carefully designed temporal and\ncorrelation pre-training tasks. Entire structure is independent of the\nnumber and order of nodes, so better results can be obtained through\nvarious data enhancements. And memory consumption during train-\ning can be significantly reduced through multiple sampling. Exper-\niments are conducted on real stock market data sets CSI300 and\nCS1500 that exhibit minimal periodicity. We fine-tune a simple MLP\nin downstream tasks and achieve state-of-the-art results, validating\nthe capability to capture more robust temporal correlation patterns.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series data is ubiquitous in daily life, and many industries such\nas healthcare [12], finance [29], and transportation [3] healthcare, fi-\nnance, and transportation generate a large amount of time series data\nevery day. In order to discover underlying patterns and forecast fu-\nture changes, utilizing these time series data for modeling has been\na focal point of research across various industries. In early studies, it\nis commonly assumed that a time series is generated from a specific\nprocess, and the parameters of that process, such as AR [34], \u039c\u0391\n[25], ARMA [2], ARIMA [19], ARCH [9], and GARCH [10], are\nestimated. In order to make more accurate predictions, many deep\nlearning methods are applied to the field of time series forecasting in\nrecent years. To effectively model the underlying temporal patterns, a\nvariety of approaches are proposed for time series modeling based on\nRecurrent Neural Networks (RNNs) [20], Time Convolutional Net-\nworks (TCNs) [6], and Transformers [38]. In the latest research, such\nas [37], [14], pre-training methods are introduced. At the same time,\nSTGNNs gain increasing attention and are widely applied to time\nseries modeling problems. STGNNs combine time series modeling\nFapkameralnetworks, effectively capturing both the internal\nfeatures of time series and the temporal dependencies between them.\nThey achieve leading results in various domains, particularly in the\nanalysis of traffic data.\nIndeed, those time series modeling methods often do not achieve\nsatisfactory results in stock market prediction. On the one hand, stock\ntime series often lack periodicity and exhibit non-fixed temporal pat-\nterns, requiring models to possess stronger robustness and general-\nization capabilities. On the other hand, there are also interdependen-\ncies among different time series, especially in stock market data. To\nfurther illustrate these characteristics, let's consider an example us-\ning real stock market data. Figure 1(a) presents the change curves\nof two unrelated stocks in the same time period. It is evident that,\ndespite belonging to different industries, based solely on the simi-\nlarity of the first half of the time series, we can make similar pre-\ndictions for one stock based on the changes observed in the other\nstock. Figure 1(b) illustrates the change curves of two stocks from\nthe same industry. As they belong to the same industry, the temporal\npatterns of the first stock appear with a delay in the changes of the\nsecond stock. This emphasizes the importance of modeling correla-\ntions in multivariate time series forecasting for stocks. At the same\ntime, traditional STGNNs often fail to address the issue of excessive\nnumbers of nodes. This is because the computational load for each\nsample typically increases quadratically with the number of nodes\nN, significantly limiting the application of STGNNs in large-scale\nspatio-temporal forecasting problems.\nThe proposed approach aims to address the aforementioned chal-\nlenges by introducing a novel emporal-Correlation Graph Pre-trained\nNetwork (TCGPN) that combines temporal features and correlation\ninformation. Pre-training allows the model to focus on the underly-\ning patterns in the data, enhancing its robustness and generalization\ncapabilities. The integration of time series and correlation informa-\ntion helps to overcome the limitations of relying solely on time se-\nries modeling, enabling a more comprehensive representation of both\ntemporal and correlation features. The entire pre-training framework\nis designed to be independent of node order and quantity. It can re-\nduce memory overhead by repeatedly sampling sub-nodes and be\nused on larger scale problems.\nSpecifically, TCGPN first Conduct data augmentation by ran-\ndomly sampling nodes, applying graph random masks, and tem-\nporal random masks, thereby exponentially generating pre-training\nsamples. Then, it uses the Temporal-Correlation Fusion Encoder\n(TCFEncoder) to blend the temporal-correlation features of the data,\nforming an integrated encoded representation. To better explore tem-\nporal features and the correlation between sequences, TCGPN em-\nploys semi-supervised correlation tasks and self-supervised temporal\ntasks with designed decoder to optimize the encoded representation\nof the TCFEncoder. In the downstream tasks, we keep the pre-trained\nmodel frozen, and splice a simple Multi-Layer Perceptron (MLP) for\nspecific prediction task, the parameters of the MLP can be trained. It\nis worth noting that the pre-trained model's output can be applied to\nany model or algorithm for further analysis or prediction. We com-\npare TGCPN with STGNNs and LSTMs on two real stock datasets,\ndemonstrating the superior performance of our model. Ablation ex-\nperiments and parameter analysis are carried out to identify the op-\ntimal state of the model. In summary, the main contributions are as\nfollows:\n\u2022 We propose a temporal-correlation pre-training network TCGPN\nwhich uses the Temporal-Correlation Fusion Encoder to inte-\ngrate temporal and correlation features into a unified encoding,\nenhanced by utilizing self-supervised and semi-supervised pre-\ntraining tasks.\n\u2022 We designed TCGPN as a structure independent of node sequence\nand quantity, which can exponentially increase pre-training sam-\nples through various data augmentation methods. Additionally,\nTCGPN can significantly reduce memory usage by repeatedly re-\nsampling when dealing with large-scale nodes.\n\u2022 We are the first to apply TCGPN to the stock price prediction task.\nWe conducted experiments on CSI300 and CSI500 datasets. The\nexperimental results demonstrate that our method effectively inte-\ngrates temporal and correlation patterns, achieving excellent per-\nformance in downstream prediction tasks with only simple MLP\nmodel fine-tuning."}, {"title": "2 PRELIMINARY", "content": "We first define the concepts of correlation graph and temporal-\ncorrelation forecasting problem.\nDefinition 2.1 Correlation Graph A correlation graph is used to\nrepresent the interrelationships between different time series. We use\ngraph G = (V, E) to represent the correlation graph, where V indi-\ncates the set of |V| = N nodes and E indicates the set of edges. We\nuse adjacency matrix A \u2208 \u211d^{N\u00d7N} to represent connectivity among\nthe graph G. For each A_{ij} \u2208 A, A_{ij} = 0 iff (v_i, v_j) \u2209 E and\nA_{ij} \u2260 0 iff (V_i, v_j) \u2208 E.\nDefinition 2.2 Temporal-correlation Forecasting Problem The\ntemporal-correlation prediction problem, also known as the spatial-\ntemporal prediction problem, refers to the task of using historical T\ntime steps data X \u2208 \u211d^{N\u00d7T\u00d7F} and correlation adjacency matrix\nA \u2208 \u211d^{N\u00d7N} to predict future values for t time steps Y\u2208 \u211d^{N\u00d7t}. For\neach sample, there are N nodes, and each node has a time series X_i,\nwhere X contains T time steps, and each time step has F features.\nAdditionally, the correlation adjacency matrix A indicates the degree\nof correlation between the nodes, where a_{ij} represents the correla-\ntion degree between node i and node j. The neighbors set of node i\nis represented as K = {j | j \u2260 i and a_{ij} \u2260 0}."}, {"title": "3 METHODOLOGY", "content": "3.1 Data Construction\nGraph Definition Graph represents the relationships between nodes,\nusually expressed by an adjacency matrix A \u2208 \u211d^{N\u00d7N} that indicates the\nstrength of the correlation between any two nodes. We construct the\ncorrelation adjacency matrix from two perspectives.\n(i) Industry Graph Taking inspiration from Wang et al. [28],\nwe construct the correlations based on the leading-lagging effects\nwithin the stock market industry. Industry leaders often exhibit cer-\ntain changes ahead of other stocks within the same industry. Other\nstocks within the industry tend to follow suit with a certain time\ndelay. We use registered capital (R) and turnover (T) as measures\nof industry leadership. The industry graph is a asymmetric directed\ngraph.\n\n\\begin{cases}\nA_{ij} = \\frac{R_j}{R_i}, & i, j \\text{ in the same industry} \\\\\na_{ij} = 0, & \\text{else}\n\\end{cases}\n\n(ii) Distance Graph The Industry graph constructs correlations\nsuitable for stock data based on prior knowledge, but this undoubt-\nedly introduces additional information, which is unfair for some\nSTGNNs. Therefore, we define another correlation between two time\nseries by utilizing the Euclidean distance between time series. The\ndistance graph is a symmetric correlation graph.\n\nA_{ij} = ||X_i - X_j ||^2\n\nData Augmentation As mentioned above, our model can input an\narbitrary number of nodes in any order, which will be described in\ndetail later. Therefore, we can enhance the data in various ways to\nachieve better pre-training effects, and to have stronger modeling ca-\npabilities and robustness for different data.\n(i) Node Random Sampling Since TCGPN is node-free, we can\nrandomly sample a variety of training samples within a certain range\nof node counts. On one hand, this approach can generate a large num-\nber of training samples on the original dataset, enhancing the model's\npre-training effect. On the other hand, for spatio-temporal predic-\ntion problems with a particularly large number of nodes, the training\nmemory consumption can be greatly reduced through multiple sam-\npling, achieving an effect similar to training with all nodes together.\nFor each training sample containing N nodes, there is a time series\nX \u2208 \u211d^{N\u00d7T\u00d7F} and an adjacency matrix A \u2208 \u211d^{N\u00d7N}, which is cal-\nculated from an industry graph or a distance graph."}, {"title": "3.2 Temporal-Correlation Fusion Encoder", "content": "To obtain the spatio-temporal integrated encoding, we designed\nTemporal-Correlation Fusion Encoder (TCFEncoder), which in-\ncludes three parts: position encoding, correlation fusion and temporal\nencoding.\nFor the initial fusion of features and to ensure not lost the temporal\nsequence order in subsequent encoding processes, we add position\nencoding [26] to the input time series X\n\nX = \\text{Position}(WX+b)\n\nwhere Position is sin/cos positional encoding influenced by the\ntime step t and feature position f [26], W is the feature fusion\nweights and b is the bias. After this step, the features of the nodes are\npreliminarily integrated and mapped to a higher dimensional space,\nand the corresponding positional information is introduced for each\nvariable at every time step.\nFor graphs constructed with some prior knowledge, they are un-\ndoubtedly incomplete and biased. To effectively fuse correlation in-\nformation, we apply Graph Attention Network (GAT) [27] into the\nstructure. GAT has a learnable attention mechanism, where the rele-\nvance between nodes is adaptively calculated by the attention mech-\nanism:\n\nA_{ij} = \\frac{\\text{exp}(\u03c3(\\tilde{a}^T[Wi||Wxj]))}{\\Sigma_{k\u2208V_i} \\text{exp}(\u03c3(\\tilde{a}^T[Wxi||Wxj]))}\n\nwhere W \u2208 \u211d^{F\u00d7F'} is the weight matrix, \\tilde{a}^T is a single-layer feed-\nforward neural network, and the o is LeakyReLU activate function.\nBased on the relevance a_{ij} between nodes and the neighbor set N\nobtained from the adjacency matrix A, we can encode the correlation\nfeatures into the time series of each node:\n\nZ_i = \\frac{1}{K} \\sum_{j\u2208Ni} a_{ij}Wxj\n\nwhere N_i is the set of neighbor node of node i, and K is the number\nof head of attension. The collection of all time series can be repre-\nsented as Z.\nTo explore the temporal series features on the basis of mixing cor-\nrelation information, a transformer structure with a temporal gaus-\nsian mask, called TGMformer, is utilized\n\nQ = WZ, K = WZ,V = WZ\\\\\n\u039f\u03b9 = \\text{softmax}(\\frac{QKT}{\\sqrt{dk}}M)V\n\nwhere WQ, WK, Wv are independent weights matrix that map Z\nto three different spaces, O is the encoding with temporal-correlation\nfeature fused. and M is temporal gaussian mask calculated by\n\nM_{ij} = \\begin{cases}\n0, & j> i \\\\\n\\text{exp}(-\\frac{(j-i)^2}{2\u03c3_h^2}), & \\text{else}\n\\end{cases}\n\nwhere \u03c3_h is the prior controlling the concentration of Gaussian dis-\ntribution.Introducing the temporal Gaussian mask can, on one hand,\nprevent historical time steps from observing future information, em-\nphasizing the impact of historical features at different time scales.\nOn the other hand, it can effectively model the time decay effect of\nhistorical influences, as mentioned by Ding et al. [8]."}, {"title": "3.3 Temporal Self-Supervised Task", "content": "We design temporal self-supervised tasks to enhance the temporal\nencoding ability and temporal-correlation fution ability. The basic\nidea is that the missing time steps can be inferred from the con-\ntext of the time series on one hand, and on the other hand, can be\ndeduced from similar series based on the correlation between time\nseries. Therefore, an effective temporal-correlation fusion encoding\nshould be able to recover the missing time steps to a certain extent.\nSpecifically, we decode the output O_{t} of the Temporal-Correlation\nFusion Encoder, using a TGMformer with future information\nmasked, as the decoder. The output of the decoder is then mapped\nthrough a fully connection layer (FC) to the same shape as the un-\nmasked time series X.\n\nX = \\text{FC(TGM former (\u039f_t))}\n\nWe optimize the mean squared error loss of the masked parts to\nmake the recovered data closer to the real data.\n\nL_t = \\frac{1}{N} ||(X \u2013 \\hat{X_r}) \\cdot M_t||\n\nwhere M_t means calculate loss only on the masked position.\nUnder the influence of this self-supervised task, the more accu-\nrate the recovery of temporal data is, it indicates that the temporal-\ncorrelation fusion encoder can more effectively explore the latent\ntemporal patterns in the variations of time series and make effec-\ntive inferences about the missing time steps. At the same time, it can\nlearn better ways of integrating information owned by the neighbor\nnodes, more accurately assess the correlation with neighbor nodes,\nand integrate similar time series features into its own encoding."}, {"title": "3.4 Correlation Semi-Supervised Task", "content": "For spatio-temporal forecasting issues, the correlation between nodes\nis often not directly assessable. To ensure the effectiveness of the\ncorrelation while breaking through the limitations of prior knowl-\nedge, we have meticulously designed a semi-supervised task for\ncorrelation. We aim to adaptively learn the degree of correlation\nbetween nodes under the guidance and constraints of a predefined\ngraph, uncovering more effective correlations while incorporating\nprior knowledge, and achieving stronger generalizability and robust-\nness across all nodes.\nSpecifically, our predefined graph calculates the degree of correla-\ntion between nodes based on prior knowledge, but in the temporal-\ncorrelation fusion encoder, GAT only uses the connectivity between\nnodes. The degree of correlation between nodes is adaptively learned\nand ultimately integrated into the fusion encoding O_{t}. We designed\na key-value structure adjacency matrix decoder to recover an adja-\ncency matrix A, from node encodings.\n\nL = WET + bL\\\\\nR=WRT+bR\\\\\n\\hat{A} = LRT\n\nwhere WL, WR are independent weights matrix, bL,bR are inde-\npendent bias matrix. In data augmentation, we mentioned that the\nadjacency matrix of the sample is randomly masked. We use the un-\nmasked parts as supervision for the learned degree of correlation, en-\nsuring that the adaptively computed results do not deviate excessively\nfrom prior knowledge, while maintaining the nodes' generalizability\nin other positions.\n\nL_g = \\frac{1}{N} ||(A \u2013 \\hat{A_r}) \\cdot Mg ||\n\nwhere Mg means calculate loss only on the unmasked position.\nWhat's more, by controlling the masking rate r_g, we can effectively\nbalance the prior knowledge and generalization, achieving a trade-off\nin correlation relationships."}, {"title": "3.5 Fine-tune for Prediction", "content": "Due to our belief that the temporal features and correlation of time\nseries have been effectively fused together in the pre-training pro-\ncess, forming a comprehensive latent feature representation O, we\nonly utilize a simple MLP for fine-tuning on downstream tasks.\nDuring the prediction phase, we no longer apply masking to the in-\nputs of the temporal and correlation graphs. Similarly, we no longer\nuse the temporal decoder and correlation decoder from the pre-\ntrained model. We directly input the complete time series and ad-\njacency matrix into the pre-training model, and use the output O_{t}\nfrom the encoder as the input for the downstream model. The model\nstructure can be represented as follows.\n\nO_1 = fc(relu(fc(\u039f_t))) + \u039f_t\n\n\nO_2 = \\text{predict layer} (O_1)\n\nThe comprehensive latent feature representation O_{t} is first passed\nthrough two linear layers to obtain a high-dimensional feature rep-\nresentation, followed by a ReLU non-linear activation function. The\noutput of the linear layers is then combined with the initial compre-\nhensive features through a residual connection, preserving the initial\naggregated features. Subsequently, the predict layer aggregates infor-\nmation from multiple time steps and maps the features to the desired\noutput dimension. With simple modifications, we obtain the output\nY for a specific prediction task.\nTo achieve good performance on time series prediction tasks, we\nconsider both the accuracy of the data itself and the correlation of\npredictions across time steps in the loss function of the downstream\ntask. First, we use MSE loss as the loss function to measure the ac-\ncuracy of the predictions.\n\nL_{mse} = \\frac{1}{N} \\sum (Y_i, \\hat{Y_i})\n\nAdditionally, we use the Pearson correlation coefficient to measure\nthe correlation of predictions within a time step\n\nL_{pearson} = \\frac{\\sum_{i=1}^{N}(Y_i \u2013 \\bar{Y}) (\\hat{Y_i} \u2013 \\bar{\\hat{Y}})}{\\sqrt{\\sum_{i=1}^{N}(Y_i \u2013 \\bar{Y})^2 \\sum_{i=1}^{N}(\\hat{Y_i} \u2013 \\bar{\\hat{Y}})^2}}\n\nThe ultimate form of loss function is as follows\n\nL = \\lambda_m L_{mse} + L_{person}\n\nwhere \u03bb_m is the weight of L_{mse}."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments on two real stock market\ndatasets to demonstrate the effectiveness of our time-correlation pre-\ntraining method. We aim to show that it can replace existing STGNNS\non non-periodic time series data. Additionally, we perform a com-\nprehensive evaluation of the implementation, assessing the impact of\nvarious components and parameters on the experimental results."}, {"title": "4.1 Experimental Setup", "content": "Datasets We conduct detailed experiments on two real-world stock\ndatasets:\n\u2022 CSI300: CSI300 is a stock dataset that contains the performance\nof the top 300 companies listed on the Shanghai and Shenzhen\nstock exchanges in China. It contains daily frequency data for 300\nstocks from 2010 to 2022, with a total time step of 3159 and a\nfeature number of 45.\n\u2022 CSI500: CSI500, is a stock dataset that contains the performance\nof 500 small and medium-sized companies listed on the Shanghai\nand Shenzhen stock exchanges. It contains daily frequency data\nfor 500 stocks from 2010 to 2022, with a total time step of 3159\nand a feature number of 45.\nBrief statistical information is listed in Table 1. Considering the\nrequirements for pre-training data volume, we divided the data by\nyear. We used ten years of data as the training set, one year of data as\nthe validation set, and one year of data as the test set.\nBaseline For the sake of fair comparison, we divided the experiment\ninto two baseline groups based on whether additional data was used.\nTemporal only group: we used time series and distance-based\ngraphs as inputs without introducing any additional information.\nWe employed a temporal neural network TPA-LSTM [23], and six\nspatial-temporal graph neural networks ASTGCN [13], MSTGCN\n[15], MTGNN [32], STEMGNN [4], STSGCN [24], STGCN [33].\nIndustry graph group: we utilized time series and pre-defined in-\ndustry graphs as inputs, which introduced additional information\ncompared to distance graph. We compared four spatial-temporal\ngraph neural networks ASTGCN [13], MSTGCN [15], STEMGNN\n[4], STSGCN [24], STGCN [33] which allowed pre-defined graph\nstructures.\nMetrics We evaluate the performances of all baseline by nine met-\nrics which are commonly used in stock prediction task including In-\nformation Coefficient (IC), Profits and Losses (PNL), Annual Re-\nturn (AR), Volatility (VOL), Sharpe Ratio (Sharpe), Max Drawdown\n(MDD), Calmar Ratio (Calmar), Win Rate (WinR), Profit/Loss Ratio\n(PL-ratio).\nImplementation We set the time step to 30 and form cross-sections\nof all time series within the same time interval as inputs to the model.\nWe then calculate the correlation graph G accordingly. The masking\nrate for the time series rt is 0.3, and the masking rate for the adja-\ncency matrix rg is 0.3 also. For the Graph Attention Network, we set\nthe number of heads to 4 and the output dimension to 32. For TGM-\nformer, we configure the Encoder with 2 TMBlocks for the CSI300\ntask and 3 TMBlocks for the CSI500 task. The number of temporal\nheads for both tasks is 8, and the output dimension is 128. The De-\ncoder consists of only one TGMformer layer. Considering the range\nof values for the two losses, L_{pearson} is in [-0.2,0], while that of\nL_{MSE} is in [0, 1]. \u03bb_m is set to 0.3 to bring the values of the two loss\nfunctions into the same range."}, {"title": "4.2 Main Results", "content": "Table 2 and Table 3 summarize the evaluation results of temporal\nonly group and industry graph group. The best results are highlighted\nin bold font and the second results are highlighted by underline. In\nsummary, TCGPN achieves state-of-the-art preformance in both two\ntasks.\nTemporal only group In this task, we compare our TCGPN with\nother Spatial-Temporal Graph Neural Networks and Long Short\nTerm Memory Models (LSTMs). Table 2 shows the detailed experi-\nmental results on dataset with Distance Graph. Our model achieves\nthe best results on multiple metrics across two datasets. Especially\nfor IC, PNL and AR, our model achieves the best effect on both\ndatasets and surpass the second place by a large margin. For VOL,\nMDD, Sharpe, Calmar, WinR and PL-ratio, we can steadily achieve\nthe first and second results. In general, our model can achieve the\nbest or second best results on the vast majority of metrics, while other\nbaseline models, whether STGNNS or LSTMs, even achieve the best\nresults on individual metric, but at the same time other metrics cannot\ncompete with our TCGPN. This is because the setting of our model\nstructure and pre-training tasks pay more attention to the interaction\nof time series, so as to better capture the correlation between time se-\nries and effectively encode such correlation into the realization of se-\nquence characterization. The TGMformer structure with attenuating\nmask complies with the future-independent rule which is the basic\nfeature of temporal data, and can better capture the potential patterns\ninfluenced by time steps.\nIndustry graph group In this task, we compare our TCGPN with\nother Spatial-Temporal Graph Neural Networks which allowed in-\nputting the adjacency matrix with additional information. As shown\nin Table 3, it can be observed that our TCGPN method achieves the\nbest performance in the majority of metrics, and we can clearly see\na significant improvement compared to the baseline in fields such as\nIC, Calmar, Sharpe and more. This demonstrates the powerful ability\nof our TCGPN to integrate correlation information. By combining\nGAT with the correlation pre-training task, our model can effectively\nutilize graph information and combine it with temporal features more\nefficiently through attention mechanisms. As a result, our model ex-\nhibits greater improvement compared to other models. On the other\nhand, it is evident that STGNNs fail to effectively extract correlation\nfeatures, and the introduction of additional information only leads to\nlimited performance improvement, further highlighting the effective-\nness of our method.\nIn summary, the experiments demonstrate that the combination of\nGAT and TGMformer modules effectively integrates temporal and\ncorrelation features, resulting in superior encoding. The setting of\ntemporal and correlation pre-training tasks enables the model to have\nhigher generalization and perform better on stock time series data\nwithout periodicity or fixed patterns."}, {"title": "4.3 Ablation Study", "content": "To validate the effectiveness of the key components, we conduct\na ablation study on CSI500&Industry Graph. We name variants of\nTCGPN as follows:\n\u2022 TCGPN w/o gat Removing the Graph Attention Network from\nthe model and solely utilize the TGMformer to model and restore\ntemporal and correlation features.\n\u2022 TCGPN w/o gl Removing the correlation pre-training task from\nthe model and exclude the calculation of graph loss. Instead, solely\nutilize time series pre-training task as the target.\n\u2022 TCGPN w/o tl Removing the time series pre-training task from\nthe model and exclude the calculation of temporal loss. Instead,\nsolely utilize correlation pre-training task as the target.\nWe repeated each experiment five times across three time steps\n{15,30,45} and plotted the results of representative metrics on Fig-\nure 3. As can be seen from the Figure 3, TCGPN outperforms any\nvariant in terms of performance. Specifically, the decrease in IC in-\ndicates a decrease in the overall accuracy of predicting a single time\nstep. Removing GAT or the correlation pre-training task from the\nmodel leads to a greater overall decrease in accuracy, indicating that\nboth modules are necessary and mutually enhancing. Together, they\neffectively exploit correlation features. The relatively minor impact\nof removing time series pre-training task suggests that the learning\nof correlation features plays a role in prediction that is similar to\nor even greater than the temporal feature. For a local perspective,\nthe impact of time series pre-training task increases with the growth\nof time steps, indicating time series pre-training task is important\nto model temporal features. Compared to removing correlation pre-\ntraining task, removing GAT leads to a greater decrease in model\nperformance, suggesting that GAT can effectively learn the correla-\ntions between time series. On the other hand, correlation pre-training\ntask enhances the robustness of the model, resulting in better overall\nperformance."}, {"title": "4.4 Hyper-parameter Study", "content": "We conduct a large number of parameter experiments. Taking into\naccount the model structure and pretraining tasks, we primarily in-\nvestigate the impact of four hyperparameters on model performance:\ngraph mask ratio r_g, temporal mask ratio r_t, number of layers n_\u03b9,\nand number of heads n_h. We perform repeated experiments on the\nCSI500&Industry Graph dataset and used ic and win rate as evalua-\ntion metrics. The results are shown in the Figure 4.\nWe find that both r_g and r_t reach their optimal values at around\n0.3, but the reasons for their optimal values are different based on our\nanalysis. When r_t is below 0.3, the training task becomes too easy,\nand missing values can be filled through interpolation. When r_t is 0.5\nor higher, for time series of length 30, there is insufficient visible in-\nformation to capture meaningful patterns of change. As for r_g, when\nit is below 0.3, the adjacency matrix computed by GAT is denser,\nleading to excessive ineffective connections and insufficient robust-\nness. However, when r_g is above 0.5, the computed matrix becomes\ntoo sparse, resulting in insufficient relevant information.\nFor n_\u03b9, it determines the complexity of temporal modeling. When\nthe n_\u03b9 is 1, the model lacks encoding capacity, resulting in poor per-\nformance. When the model has 3 layers, it exhibits more severe over-\nfitting issues for shorter time series and the model loses necessary\nrobustness. Regarding n_h, which affects correlation modeling, the\nmodel achieves optimal performance when n_h is 8. When n_h is less\nthan 8, the model fails to effectively capture multiple correlation re-\nlationships. As n_h continues to increase, the model cannot explore\nadditional correlation relationships and instead experiences a slight\ndecrease in performance due to redundancy."}, {"title": "5 RELATED WORK", "content": "5.1 Pre-trained Model\nPre-training is one of the latest and most popular training methods.\nMany existing pretrained models, such as BERT [7] and GPT [21],\nutilize the Encoder and Decoder structures of Transformers to learn\nthe correlations within sequences and obtain better encoding repre-\nsentations. These representations possess stronger robustness, richer\ninformation, and lower noise.\nIn recent research, there are studies on enhancing downstream pre-\ndiction tasks with pre-training on time series data, such as STEP [22]\nand SPGCL [17]. However, they only result in improved temporal\nrepresentations without considering spatial correlations."}, {"title": "5.2 Spatial-Temporal Graph Neural Network", "content": "In recent years, there has been a growing interest in exploring the\ncombination of temporal and spatio information within time series\ndata. This led to the emergence of a series of models known as\nSpatial-Temporal Graph Neural Networks.\nSTGCN [33] is the first to introduce Graph Convolutional Neu-\nral Network (GCN) [16] into time series forecasting. Subsequently,\nmore convolution-based models such as Graph Wave Net[31], MT-\nGNN [32], StemGNN [4], H-STGCN [5], GSTNet [11], and others\nare proposed. These models introduce various gating mechanisms on\ntop of convolutions to better capture data features. Meanwhile, some\nstudies focus on more complex convolutional structures, such as ST-\nGDN [36] and ST-ResNet [35], which achieve better performance\nthrough clever architectural designs and mechanisms.\nFurthermore, some works, like ARGCN [1], DCRNN [18], TGCN\n39], combine Recurrent Neural Networks (RNNs) with Graph Neu-\nral Networks (GNNs). They leverage the excellent temporal mod-\neling capabilities of RNNs to better capture temporal features. In\naddition, with the introduction of Transformers, many models in-\ncorporate transformer architectures or attention mechanisms into\nspatial-temporal modeling, such as ASTGCN [13], STGNN [30], and\nGMAN [40].\nAlthough an increasing number of models are proposed,\nthey perform well in tasks with periodic patterns but lack robust-\nness in tasks without periodicity. Our approach can generate bet-\nter representations in both temporal and spatial domains, exhibiting\nstronger robustness and generalization capabilities."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a novel Temporal-Correlation Graph Pre-\ntrained Network called TCGPN. The temporal-correlation fusion en-\ncoder effectively integrates temporal information and correlation fea-\ntures. The temporal self-supervised task prompts the model to ex-\nplore potential temporal contextual relationships and latent influence\npatterns between sequences. And the correlation semi-supervised\ntask allows the model to uncover more effective relational linkages\nbetween sequences under the guidance of prior knowledge, enhanc-\ning the robustness and generalizability of the encoding. Addition-\nally, TCGPN is independent of the number of nodes and their se-\nquence. It can enhance the pre-training effect through various data\naugmentation methods and address the issue of excessive memory\nusage during spatio-temporal pre-training by utilizing repeated sam-\npling techniques. We conduct extensive experiments on real-world\nstock datasets to demonstrate the superiority of our approach, and it\nis the first application of this method to stock market data where ro-\nbustness is crucial. In the future, we will continue to explore better\npre-training tasks and model structures, and apply our approach to awider range of spatial-temporal tasks."}]}