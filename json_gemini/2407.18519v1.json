{"title": "TCGPN: Temporal-Correlation Graph Pre-trained Network for Stock Forecasting", "authors": ["Wenbo Yan", "Ying Tan"], "abstract": "Recently, the incorporation of both temporal features and the correlation across time series has become an effective approach in time series prediction. Spatio-Temporal Graph Neural Networks (STGNNs) demonstrate good performance on many Temporal-correlation Forecasting Problem. However, when applied to tasks lacking periodicity, such as stock data prediction, the effectiveness and robustness of STGNNs are found to be unsatisfactory. And STGNNs are limited by memory savings so that cannot handle problems with a large number of nodes. In this paper, we propose a novel approach called the Temporal-Correlation Graph Pre-trained Network (TCGPN) to address these limitations. TCGPN utilize Temporal-correlation fusion encoder to get a mixed representation and pre-training method with carefully designed temporal and correlation pre-training tasks. Entire structure is independent of the number and order of nodes, so better results can be obtained through various data enhancements. And memory consumption during training can be significantly reduced through multiple sampling. Experiments are conducted on real stock market data sets CSI300 and CS1500 that exhibit minimal periodicity. We fine-tune a simple MLP in downstream tasks and achieve state-of-the-art results, validating the capability to capture more robust temporal correlation patterns.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series data is ubiquitous in daily life, and many industries such as healthcare [12], finance [29], and transportation [3] healthcare, finance, and transportation generate a large amount of time series data every day. In order to discover underlying patterns and forecast future changes, utilizing these time series data for modeling has been a focal point of research across various industries. In early studies, it is commonly assumed that a time series is generated from a specific process, and the parameters of that process, such as AR [34], \u039c\u0391 [25], ARMA [2], ARIMA [19], ARCH [9], and GARCH [10], are estimated. In order to make more accurate predictions, many deep learning methods are applied to the field of time series forecasting in recent years. To effectively model the underlying temporal patterns, a variety of approaches are proposed for time series modeling based on Recurrent Neural Networks (RNNs) [20], Time Convolutional Networks (TCNs) [6], and Transformers [38]. In the latest research, such as [37], [14], pre-training methods are introduced. At the same time, STGNNs gain increasing attention and are widely applied to time series modeling problems. STGNNs combine time series modeling Fapkameralnetworks, effectively capturing both the internal features of time series and the temporal dependencies between them. They achieve leading results in various domains, particularly in the analysis of traffic data.\nIndeed, those time series modeling methods often do not achieve satisfactory results in stock market prediction. On the one hand, stock time series often lack periodicity and exhibit non-fixed temporal patterns, requiring models to possess stronger robustness and generalization capabilities. On the other hand, there are also interdependencies among different time series, especially in stock market data. To further illustrate these characteristics, let's consider an example using real stock market data. Figure 1(a) presents the change curves of two unrelated stocks in the same time period. It is evident that, despite belonging to different industries, based solely on the similarity of the first half of the time series, we can make similar predictions for one stock based on the changes observed in the other stock. Figure 1(b) illustrates the change curves of two stocks from the same industry. As they belong to the same industry, the temporal patterns of the first stock appear with a delay in the changes of the second stock. This emphasizes the importance of modeling correlations in multivariate time series forecasting for stocks. At the same time, traditional STGNNs often fail to address the issue of excessive numbers of nodes. This is because the computational load for each sample typically increases quadratically with the number of nodes N, significantly limiting the application of STGNNs in large-scale spatio-temporal forecasting problems.\nThe proposed approach aims to address the aforementioned challenges by introducing a novel emporal-Correlation Graph Pre-trained Network (TCGPN) that combines temporal features and correlation information. Pre-training allows the model to focus on the underlying patterns in the data, enhancing its robustness and generalization capabilities. The integration of time series and correlation information helps to overcome the limitations of relying solely on time series modeling, enabling a more comprehensive representation of both temporal and correlation features. The entire pre-training framework is designed to be independent of node order and quantity. It can reduce memory overhead by repeatedly sampling sub-nodes and be used on larger scale problems.\nSpecifically, TCGPN first Conduct data augmentation by randomly sampling nodes, applying graph random masks, and temporal random masks, thereby exponentially generating pre-training samples. Then, it uses the Temporal-Correlation Fusion Encoder (TCFEncoder) to blend the temporal-correlation features of the data, forming an integrated encoded representation. To better explore temporal features and the correlation between sequences, TCGPN employs semi-supervised correlation tasks and self-supervised temporal tasks with designed decoder to optimize the encoded representation of the TCFEncoder. In the downstream tasks, we keep the pre-trained model frozen, and splice a simple Multi-Layer Perceptron (MLP) for specific prediction task, the parameters of the MLP can be trained. It is worth noting that the pre-trained model's output can be applied to any model or algorithm for further analysis or prediction. We compare TGCPN with STGNNs and LSTMs on two real stock datasets, demonstrating the superior performance of our model. Ablation experiments and parameter analysis are carried out to identify the optimal state of the model. In summary, the main contributions are as follows:\n\u2022 We propose a temporal-correlation pre-training network TCGPN which uses the Temporal-Correlation Fusion Encoder to integrate temporal and correlation features into a unified encoding, enhanced by utilizing self-supervised and semi-supervised pre-training tasks.\n\u2022 We designed TCGPN as a structure independent of node sequence and quantity, which can exponentially increase pre-training samples through various data augmentation methods. Additionally, TCGPN can significantly reduce memory usage by repeatedly re-sampling when dealing with large-scale nodes.\n\u2022 We are the first to apply TCGPN to the stock price prediction task. We conducted experiments on CSI300 and CSI500 datasets. The experimental results demonstrate that our method effectively integrates temporal and correlation patterns, achieving excellent performance in downstream prediction tasks with only simple MLP model fine-tuning."}, {"title": "2 PRELIMINARY", "content": "We first define the concepts of correlation graph and temporal-correlation forecasting problem.\nDefinition 2.1 Correlation Graph A correlation graph is used to represent the interrelationships between different time series. We use graph $G = (V, E)$ to represent the correlation graph, where V indicates the set of $|V| = N$ nodes and E indicates the set of edges. We use adjacency matrix $A \\in R^{N \\times N}$ to represent connectivity among the graph G. For each $A_{ij} \\in A$, $A_{ij} = 0$ iff $(v_i, v_j) \\notin E$ and $A_{ij} \\neq 0$ iff $(V_i, v_j) \\in E$.\nDefinition 2.2 Temporal-correlation Forecasting Problem The temporal-correlation prediction problem, also known as the spatial-temporal prediction problem, refers to the task of using historical T time steps data $X \\in R^{N \\times T \\times F}$ and correlation adjacency matrix $A \\in R^{N \\times N}$ to predict future values for t time steps $Y \\in R^{N \\times t}$. For each sample, there are N nodes, and each node has a time series $X_i$, where $X$ contains T time steps, and each time step has F features. Additionally, the correlation adjacency matrix A indicates the degree of correlation between the nodes, where $a_{ij}$ represents the correlation degree between node i and node j. The neighbors set of node i is represented as $K = \\{j | j \\neq i \\text{ and } a_{ij} \\neq 0\\}$."}, {"title": "3 METHODOLOGY", "content": "3.1 Data Construction\nGraph Definition Graph represents the relationships between nodes, usually expressed by an adjacency matrix $A_{N \\times N}$ that indicates the strength of the correlation between any two nodes. We construct the correlation adjacency matrix from two perspectives.\n(i) Industry Graph Taking inspiration from Wang et al. [28], we construct the correlations based on the leading-lagging effects within the stock market industry. Industry leaders often exhibit certain changes ahead of other stocks within the same industry. Other stocks within the industry tend to follow suit with a certain time delay. We use registered capital (R) and turnover (T) as measures of industry leadership. The industry graph is a asymmetric directed graph.\n$\\begin{cases}\nA_{ij} = \\frac{R_j}{R_i}, \\quad i, j \\text{ in the same industry}\\\\na_{ij} = 0, \\text{ else}\n\\end{cases}$\n(1)\n(ii) Distance Graph The Industry graph constructs correlations suitable for stock data based on prior knowledge, but this undoubtedly introduces additional information, which is unfair for some STGNNs. Therefore, we define another correlation between two time series by utilizing the Euclidean distance between time series. The distance graph is a symmetric correlation graph.\n$A_{ij} = ||X_i - X_j ||_2$\n(2)\nData Augmentation As mentioned above, our model can input an arbitrary number of nodes in any order, which will be described in detail later. Therefore, we can enhance the data in various ways to achieve better pre-training effects, and to have stronger modeling capabilities and robustness for different data.\n(i) Node Random Sampling Since TCGPN is node-free, we can randomly sample a variety of training samples within a certain range of node counts. On one hand, this approach can generate a large number of training samples on the original dataset, enhancing the model's pre-training effect. On the other hand, for spatio-temporal prediction problems with a particularly large number of nodes, the training memory consumption can be greatly reduced through multiple sampling, achieving an effect similar to training with all nodes together. For each training sample containing N nodes, there is a time series $X \\in R^{N \\times T \\times F}$ and an adjacency matrix $A \\in R^{N \\times N}$, which is calculated from an industry graph or a distance graph."}, {"title": "3.2 Temporal-Correlation Fusion Encoder", "content": "To obtain the spatio-temporal integrated encoding, we designed Temporal-Correlation Fusion Encoder (TCFEncoder), which includes three parts: position encoding, correlation fusion and temporal encoding.\nFor the initial fusion of features and to ensure not lost the temporal sequence order in subsequent encoding processes, we add position encoding [26] to the input time series X\n$X = Position(WX+b)$\n(4)\nwhere Position is sin/cos positional encoding influenced by the time step t and feature position f [26], W is the feature fusion weights and b is the bias. After this step, the features of the nodes are preliminarily integrated and mapped to a higher dimensional space, and the corresponding positional information is introduced for each variable at every time step.\nFor graphs constructed with some prior knowledge, they are undoubtedly incomplete and biased. To effectively fuse correlation information, we apply Graph Attention Network (GAT) [27] into the structure. GAT has a learnable attention mechanism, where the relevance between nodes is adaptively calculated by the attention mechanism:\n$a_{ij} = \\frac{exp(\\sigma(\\bar{a}^T[W x_i||W x_j]))}{\\Sigma_{k \\in V_i}exp(\\sigma(\\bar{a}^T[W x_i||W x_j]))}$\n(5)\nwhere $W \\in R^{F \\times F'}$ is the weight matrix, $\\bar{a}^T$ is a single-layer feed-forward neural network, and the $\\sigma$ is LeakyReLU activate function. Based on the relevance $a_{ij}$ between nodes and the neighbor set N obtained from the adjacency matrix A, we can encode the correlation features into the time series of each node:\n$z_i = o(\\frac{1}{K} \\Sigma_{k=1} \\Sigma_{j \\in N_i} a_{ij} Wx_j)$\n(6)\nwhere $N_i$ is the set of neighbor node of node i, and K is the number of head of attension. The collection of all time series can be represented as Z.\nTo explore the temporal series features on the basis of mixing correlation information, a transformer structure with a temporal gaussian mask, called TGMformer, is utilized\n$Q = W_Q Z, K = W_K Z,V = W_V Z$\n$\\Ou = softmax(\\frac{Q K^T}{\\sqrt{d_k}} M)V$\n(7)\nwhere $W_Q, W_K, W_V$ are independent weights matrix that map Z to three different spaces, O is the encoding with temporal-correlation feature fused. and M is temporal gaussian mask calculated by\n$M_{ij} = \\begin{cases}\n0, j> i \\\\\nexp(\\frac{-(j-i)^2}{2o_h^2}), else\n\\end{cases}$\n(8)\nwhere $o_h$ is the prior controlling the concentration of Gaussian distribution.Introducing the temporal Gaussian mask can, on one hand, prevent historical time steps from observing future information, emphasizing the impact of historical features at different time scales. On the other hand, it can effectively model the time decay effect of historical influences, as mentioned by Ding et al. [8]."}, {"title": "3.3 Temporal Self-Supervised Task", "content": "We design temporal self-supervised tasks to enhance the temporal encoding ability and temporal-correlation fution ability. The basic idea is that the missing time steps can be inferred from the context of the time series on one hand, and on the other hand, can be deduced from similar series based on the correlation between time series. Therefore, an effective temporal-correlation fusion encoding should be able to recover the missing time steps to a certain extent.\nSpecifically, we decode the output Or of the Temporal-Correlation Fusion Encoder, using a TGMformer with future information masked, as the decoder. The output of the decoder is then mapped through a fully connection layer (FC) to the same shape as the unmasked time series X.\n$\\hat{X} = FC(TGM former (\\Ou))$\n(9)\nWe optimize the mean squared error loss of the masked parts to make the recovered data closer to the real data.\n$L_t = \\frac{1}{N}||(X - \\hat{X}) \\cdot M_t||$\n(10)\nwhere $M_t$ means calculate loss only on the masked position.\nUnder the influence of this self-supervised task, the more accurate the recovery of temporal data is, it indicates that the temporal-correlation fusion encoder can more effectively explore the latent temporal patterns in the variations of time series and make effective inferences about the missing time steps. At the same time, it can learn better ways of integrating information owned by the neighbor nodes, more accurately assess the correlation with neighbor nodes, and integrate similar time series features into its own encoding."}, {"title": "3.4 Correlation Semi-Supervised Task", "content": "For spatio-temporal forecasting issues, the correlation between nodes is often not directly assessable. To ensure the effectiveness of the correlation while breaking through the limitations of prior knowledge, we have meticulously designed a semi-supervised task for correlation. We aim to adaptively learn the degree of correlation between nodes under the guidance and constraints of a predefined graph, uncovering more effective correlations while incorporating prior knowledge, and achieving stronger generalizability and robustness across all nodes.\nSpecifically, our predefined graph calculates the degree of correlation between nodes based on prior knowledge, but in the temporal-correlation fusion encoder, GAT only uses the connectivity between nodes. The degree of correlation between nodes is adaptively learned and ultimately integrated into the fusion encoding Or. We designed a key-value structure adjacency matrix decoder to recover an adjacency matrix A, from node encodings.\n$L = W_L \\Ou + b_L$\n$R=W_R \\Ou + b_R$\n$\\hat{A} = L R^T$\n(11)\nwhere $W_L, W_R$ are independent weights matrix, $b_L, b_R$ are independent bias matrix. In data augmentation, we mentioned that the adjacency matrix of the sample is randomly masked. We use the unmasked parts as supervision for the learned degree of correlation, ensuring that the adaptively computed results do not deviate excessively from prior knowledge, while maintaining the nodes' generalizability in other positions.\n$L_g = \\frac{1}{N}||(A - \\hat{A}) \\cdot M_g ||$\n(12)\nwhere $M_g$ means calculate loss only on the unmasked position. What's more, by controlling the masking rate $r_g$, we can effectively balance the prior knowledge and generalization, achieving a trade-off in correlation relationships."}, {"title": "3.5 Fine-tune for Prediction", "content": "Due to our belief that the temporal features and correlation of time series have been effectively fused together in the pre-training process, forming a comprehensive latent feature representation O, we only utilize a simple MLP for fine-tuning on downstream tasks.\nDuring the prediction phase, we no longer apply masking to the inputs of the temporal and correlation graphs. Similarly, we no longer use the temporal decoder and correlation decoder from the pre-trained model. We directly input the complete time series and adjacency matrix into the pre-training model, and use the output $\\Ou$ from the encoder as the input for the downstream model. The model structure can be represented as follows.\n$O_1 = fc(relu(fc(\\Ou))) + \\Ou$\n(13)\n$O_2 = predict layer (O_1)$\n(14)\nThe comprehensive latent feature representation $O_t$ is first passed through two linear layers to obtain a high-dimensional feature representation, followed by a ReLU non-linear activation function. The output of the linear layers is then combined with the initial comprehensive features through a residual connection, preserving the initial aggregated features. Subsequently, the predict layer aggregates information from multiple time steps and maps the features to the desired output dimension. With simple modifications, we obtain the output Y for a specific prediction task.\nTo achieve good performance on time series prediction tasks, we consider both the accuracy of the data itself and the correlation of predictions across time steps in the loss function of the downstream task. First, we use MSE loss as the loss function to measure the accuracy of the predictions.\n$L_{mse} = \\frac{1}{N} \\Sigma (Y_i, \\hat{Y_i})$\n(15)\nAdditionally, we use the Pearson correlation coefficient to measure the correlation of predictions within a time step\n$L_{pearson} = \\frac{\\Sigma_{i=1}^N(Y_i - \\bar{Y}) (\\hat{Y_i} - \\hat{Y})}{\\sqrt{\\Sigma_{i=1}^N(Y_i - \\bar{Y})^2 \\Sigma_{i=1}^N(\\hat{Y_i} - \\hat{Y})^2}}$\n(16)\nThe ultimate form of loss function is as follows\n$L = \\lambda_m L_{mse} + L_{person}$\n(17)\nwhere $\\lambda_m$ is the weight of $L_{mse}$."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments on two real stock market datasets to demonstrate the effectiveness of our time-correlation pre-training method. We aim to show that it can replace existing STGNNS on non-periodic time series data. Additionally, we perform a comprehensive evaluation of the implementation, assessing the impact of various components and parameters on the experimental results.\n4.1 Experimental Setup\nDatasets We conduct detailed experiments on two real-world stock datasets:\n\u2022 CSI300: CSI300 is a stock dataset that contains the performance of the top 300 companies listed on the Shanghai and Shenzhen stock exchanges in China. It contains daily frequency data for 300 stocks from 2010 to 2022, with a total time step of 3159 and a feature number of 45.\n\u2022 CSI500: CSI500, is a stock dataset that contains the performance of 500 small and medium-sized companies listed on the Shanghai and Shenzhen stock exchanges. It contains daily frequency data for 500 stocks from 2010 to 2022, with a total time step of 3159 and a feature number of 45.\nBrief statistical information is listed in Table 1. Considering the requirements for pre-training data volume, we divided the data by year. We used ten years of data as the training set, one year of data as the validation set, and one year of data as the test set.\nBaseline For the sake of fair comparison, we divided the experiment into two baseline groups based on whether additional data was used. Temporal only group: we used time series and distance-based graphs as inputs without introducing any additional information. We employed a temporal neural network TPA-LSTM [23], and six spatial-temporal graph neural networks ASTGCN [13], MSTGCN [15], MTGNN [32], STEMGNN [4], STSGCN [24], STGCN [33]. Industry graph group: we utilized time series and pre-defined industry graphs as inputs, which introduced additional information compared to distance graph. We compared four spatial-temporal graph neural networks ASTGCN [13], MSTGCN [15], STEMGNN [4], STSGCN [24], STGCN [33] which allowed pre-defined graph structures.\nMetrics We evaluate the performances of all baseline by nine metrics which are commonly used in stock prediction task including Information Coefficient (IC), Profits and Losses (PNL), Annual Return (AR), Volatility (VOL), Sharpe Ratio (Sharpe), Max Drawdown (MDD), Calmar Ratio (Calmar), Win Rate (WinR), Profit/Loss Ratio (PL-ratio).\nImplementation We set the time step to 30 and form cross-sections of all time series within the same time interval as inputs to the model. We then calculate the correlation graph G accordingly. The masking rate for the time series rt is 0.3, and the masking rate for the adjacency matrix rg is 0.3 also. For the Graph Attention Network, we set the number of heads to 4 and the output dimension to 32. For TGM-former, we configure the Encoder with 2 TMBlocks for the CSI300 task and 3 TMBlocks for the CSI500 task. The number of temporal heads for both tasks is 8, and the output dimension is 128. The Decoder consists of only one TGMformer layer. Considering the range of values for the two losses, Lpearson is in [-0.2,0], while that of LMSE is in [0, 1]. Am is set to 0.3 to bring the values of the two loss functions into the same range."}, {"title": "4.2 Main Results", "content": "Table 2 and Table 3 summarize the evaluation results of temporal only group and industry graph group. The best results are highlighted in bold font and the second results are highlighted by underline. In summary, TCGPN achieves state-of-the-art preformance in both two tasks.\nTemporal only group In this task, we compare our TCGPN with other Spatial-Temporal Graph Neural Networks and Long Short Term Memory Models (LSTMs). Table 2 shows the detailed experimental results on dataset with Distance Graph. Our model achieves the best results on multiple metrics across two datasets. Especially for IC, PNL and AR, our model achieves the best effect on both datasets and surpass the second place by a large margin. For VOL, MDD, Sharpe, Calmar, WinR and PL-ratio, we can steadily achieve the first and second results. In general, our model can achieve the best or second best results on the vast majority of metrics, while other baseline models, whether STGNNS or LSTMs, even achieve the best results on individual metric, but at the same time other metrics cannot compete with our TCGPN. This is because the setting of our model structure and pre-training tasks pay more attention to the interaction of time series, so as to better capture the correlation between time series and effectively encode such correlation into the realization of sequence characterization. The TGMformer structure with attenuating mask complies with the future-independent rule which is the basic feature of temporal data, and can better capture the potential patterns influenced by time steps.\nIndustry graph group In this task, we compare our TCGPN with other Spatial-Temporal Graph Neural Networks which allowed inputting the adjacency matrix with additional information. As shown in Table 3, it can be observed that our TCGPN method achieves the best performance in the majority of metrics, and we can clearly see a significant improvement compared to the baseline in fields such as IC, Calmar, Sharpe and more. This demonstrates the powerful ability of our TCGPN to integrate correlation information. By combining GAT with the correlation pre-training task, our model can effectively utilize graph information and combine it with temporal features more efficiently through attention mechanisms. As a result, our model exhibits greater improvement compared to other models. On the other hand, it is evident that STGNNs fail to effectively extract correlation features, and the introduction of additional information only leads to limited performance improvement, further highlighting the effectiveness of our method.\nIn summary, the experiments demonstrate that the combination of GAT and TGMformer modules effectively integrates temporal and correlation features, resulting in superior encoding. The setting of temporal and correlation pre-training tasks enables the model to have higher generalization and perform better on stock time series data without periodicity or fixed patterns."}, {"title": "4.3 Ablation Study", "content": "To validate the effectiveness of the key components, we conduct a ablation study on CSI500&Industry Graph. We name variants of TCGPN as follows:\n\u2022 TCGPN w/o gat Removing the Graph Attention Network from the model and solely utilize the TGMformer to model and restore temporal and correlation features.\n\u2022 TCGPN w/o gl Removing the correlation pre-training task from the model and exclude the calculation of graph loss. Instead, solely utilize time series pre-training task as the target.\n\u2022 TCGPN w/o tl Removing the time series pre-training task from the model and exclude the calculation of temporal loss. Instead, solely utilize correlation pre-training task as the target.\nWe repeated each experiment five times across three time steps {15,30,45} and plotted the results of representative metrics on Figure 3. As can be seen from the Figure 3, TCGPN outperforms any variant in terms of performance. Specifically, the decrease in IC indicates a decrease in the overall accuracy of predicting a single time step. Removing GAT or the correlation pre-training task from the model leads to a greater overall decrease in accuracy, indicating that both modules are necessary and mutually enhancing. Together, they effectively exploit correlation features. The relatively minor impact of removing time series pre-training task suggests that the learning of correlation features plays a role in prediction that is similar to or even greater than the temporal feature. For a local perspective, the impact of time series pre-training task increases with the growth of time steps, indicating time series pre-training task is important to model temporal features. Compared to removing correlation pre-training task, removing GAT leads to a greater decrease in model performance, suggesting that GAT can effectively learn the correlations between time series. On the other hand, correlation pre-training task enhances the robustness of the model, resulting in better overall performance."}, {"title": "4.4 Hyper-parameter Study", "content": "We conduct a large number of parameter experiments. Taking into account the model structure and pretraining tasks, we primarily investigate the impact of four hyperparameters on model performance: graph mask ratio rg, temporal mask ratio rt, number of layers n\u03b9, and number of heads nn. We perform repeated experiments on the CSI500&Industry Graph dataset and used ic and win rate as evaluation metrics. The results are shown in the Figure 4.\nWe find that both rg and rt reach their optimal values at around 0.3, but the reasons for their optimal values are different based on our analysis. When rt is below 0.3, the training task becomes too easy, and missing values can be filled through interpolation. When rt is 0.5 or higher, for time series of length 30, there is insufficient visible information to capture meaningful patterns of change. As for rg, when it is below 0.3, the adjacency matrix computed by GAT is denser, leading to excessive ineffective connections and insufficient robustness. However, when rg is above 0.5, the computed matrix becomes too sparse, resulting in insufficient relevant information.\nFor ni, it determines the complexity of temporal modeling. When the ni is 1, the model lacks encoding capacity, resulting in poor performance. When the model has 3 layers, it exhibits more severe overfitting issues for shorter time series and the model loses necessary robustness. Regarding nn, which affects correlation modeling, the model achieves optimal performance when nn is 8. When nn is less than 8, the model fails to effectively capture multiple correlation relationships. As nh continues to increase, the model cannot explore additional correlation relationships and instead experiences a slight decrease in performance due to redundancy."}, {"title": "5 RELATED WORK", "content": "5.1 Pre-trained Model\nPre-training is one of the latest and most popular training methods. Many existing pretrained models, such as BERT [7] and GPT [21], utilize the Encoder and Decoder structures of Transformers to learn the correlations within sequences and obtain better encoding representations. These representations possess stronger robustness, richer information, and lower noise.\nIn recent research, there are studies on enhancing downstream prediction tasks with pre-training on time series data, such as STEP [22] and SPGCL [17]. However, they only result in improved temporal representations without considering spatial correlations."}, {"title": "5.2 Spatial-Temporal Graph Neural Network", "content": "In recent years, there has been a growing interest in exploring the combination of temporal and spatio information within time series data. This led to the emergence of a series of models known as Spatial-Temporal Graph Neural Networks.\nSTGCN [33] is the first to introduce Graph Convolutional Neural Network (GCN) [16] into time series forecasting. Subsequently, more convolution-based models such as Graph Wave Net[31], MTGNN [32], StemGNN [4], H-STGCN [5], GSTNet [11], and others are proposed. These models introduce various gating mechanisms on top of convolutions to better capture data features. Meanwhile, some studies focus on more complex convolutional structures, such as STGDN [36] and ST-ResNet [35], which achieve better performance through clever architectural designs and mechanisms.\nFurthermore, some works, like ARGCN [1], DCRNN [18], TGCN [39], combine Recurrent Neural Networks (RNNs) with Graph Neural Networks (GNNs). They leverage the excellent temporal modeling capabilities of RNNs to better capture temporal features. In addition, with the introduction of Transformers, many models incorporate transformer architectures or attention mechanisms into spatial-temporal modeling, such as ASTGCN [13], STGNN [30], and GMAN [40].\nAlthough an increasing number of models are proposed, they perform well in tasks with periodic patterns but lack robustness in tasks without periodicity. Our approach can generate better representations in both temporal and spatial domains, exhibiting stronger robustness and generalization capabilities."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a novel Temporal-Correlation Graph Pre-trained Network called TCGPN. The temporal-correlation fusion encoder effectively integrates temporal information and correlation features. The temporal self-supervised task prompts the model to explore potential temporal contextual relationships and latent influence patterns between sequences. And the correlation semi-supervised task allows the model to uncover more effective relational linkages between sequences under the guidance of prior knowledge, enhancing the robustness and generalizability of the encoding. Additionally, TCGPN is independent of the number of nodes and their sequence. It can enhance the pre-training effect through various data augmentation methods and address the issue of excessive memory usage during spatio-temporal pre-training by utilizing repeated sampling techniques. We conduct extensive experiments on real-world stock datasets to demonstrate the superiority of our approach, and it is the first application of this method to stock market data where robustness is crucial. In the future, we will continue to explore better pre-training tasks and model structures, and apply our approach to a wider range of spatial-temporal tasks."}]}