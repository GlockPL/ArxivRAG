{"title": "Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges", "authors": ["Nayoung Lee", "Ziyang Cai", "Avi Schwarzschild", "Kangwook Lee", "Dimitris Papailiopoulos"], "abstract": "Large language models often struggle with length generalization and solving complex problem instances beyond their training distribution. We present a self-improvement approach where models iteratively generate and learn from their own solutions, progressively tackling harder problems while maintaining a standard transformer architecture. Across diverse tasks including arithmetic, string manipulation, and maze solving, self-improving enables models to solve problems far beyond their initial training distribution\u2014for instance, generalizing from 10-digit to 100-digit addition without apparent saturation. We observe that in some cases filtering for correct self-generated examples leads to exponential improvements in out-of-distribution performance across training rounds. Additionally, starting from pretrained models significantly accelerates this self-improvement process for several tasks. Our results demonstrate how controlled weak-to-strong curricula can systematically teach a model logical extrapolation without any changes to the positional embeddings, or the model architecture.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable success of transformer-based language models (Vaswani et al., 2017) across a wide range of tasks, these models exhibit significant limitations in length generalization\u2014the ability to extrapolate to longer sequences than those seen during training. Even in simple algorithmic tasks such as arithmetic, standard transformer models trained with autoregressive objectives struggle to generalize to longer problem instances (Dubois et al., 2019; Hupkes et al., 2020; Newman et al., 2020; Anil et al., 2022).\nTo address this, prior work has explored various approaches, including changes to positional embeddings (Ruoss et al., 2023; Li et al., 2023; McLeish et al., 2024; Kazemnejad et al., 2024; Sabbaghi et al., 2024; Cho et al., 2024; Zhou et al., 2024), architectural modifications (Fan et al., 2024; Duan et al., 2023), and data format changes such as index hinting (Zhou et al., 2023, 2024). While effective\nin controlled setups, these approaches are often incompatible with how large language models (LLMs) are trained in practice, as they introduce task-specific modifications that are unclear how and to what extent they would transfer to the general purpose settings.\nIn this work, we attempt to overcome length generalization challenges in the standard transformer setting, by building around an interesting phenomenon that transformers exhibit, i.e., \u201ctranscendence\" (Zhang et al., 2024). Transcendence is the ability of a student model to generalize slightly beyond the difficulty of the data provided by a teacher during training. Specifically, models trained on simple instances of a task, say n digit arithmetic, can sometimes generate correct outputs for slightly harder instances, e.g., n + 1 digit arithmetic, with some accuracy. We leverage this property by applying a self-improvement framework, drawing significant inspiration by STaR (Zelikman et al., 2022) and ReST (Gulcehre et al., 2023), where we alternate between collecting output predictions and finetuning using the self-generated dataset.\nSelf-improvement has been widely studied in various contexts (Singh et al., 2023; Gulcehre et al., 2023; Liang et al., 2024), typically in settings where external verifiers, weak supervision, or filtering mechanisms are used to ensure data quality. We demonstrate that extreme length generalization is indeed possible under this framework, without any modification to the base transformer architecture. For tasks like reverse addition and string copying, self-improvement succeeds with no explicit data filtering. However, for harder problems such as multiplication and shortest-path finding in mazes, self-improvement without data filtering fails due to error accumulation. We show that simple filtering techniques\u2014such as length filtering and majority voting\u2014suffice to maintain data quality and enable self-improvement to extend far beyond the initial training distribution.\nOur findings suggest that self-improvement is not limited to length generalization but also enables easy-to-hard generalization, where a model trained on simpler tasks successfully learns harder tasks without additional supervision. Notably, our approach does not introduce a new self-improvement framework but instead demonstrates its effectiveness across diverse algorithmic tasks.\nFurthermore, we investigate the dynamics of self-improvement and show that: (1) controlling the weak-to-strong curriculum is crucial, as models require a structured difficulty schedule to avoid catastrophic failure, (2) self-improvement accelerates over time, as models increasingly benefit from harder examples, leading in some cases to exponential extrapolation, and (3) starting with a pretrained models singificantly accelerates self-improvement, allowing to generalize further and faster than models trained from scratch.\nOur findings provide evidence that learn self-improvement is a general purpose and scalable solution for length and easy-to-hard generalization. Our contributions can be summarized as:"}, {"title": "2 Related Works", "content": "Length and Easy-to-hard Generalization. Length generalization is concerned with extrapolating to longer sequence lengths than those seen during training (Dubois et al., 2019; Hupkes et al., 2020; Newman et al., 2020; Anil et al., 2022). Previous approaches to improve length generalization includes architectural modifications, including specialized positional embeddings (Press et al., 2021; Li et al., 2023; Ruoss et al., 2023; Kazemnejad et al., 2024; Sabbaghi et al., 2024; Cho et al., 2024; Zhou et al., 2024), looping Fan et al. (2024), novel attention mechanisms (Duan et al., 2023), and input format augmentation (Zhou et al., 2023, 2024). Beyond arithmetic, Yehudai et al. (2021) studies length generalization in graph tasks. In contrast, our approach adheres to the standard transformer architecture without introducing significant modifications to architecture, positional encoding, or input structure. While prior approaches typically rely on fixed-length training datasets without further updates to model weights, we iteratively update model weights on self-generated datasets, enabling the model to progressively improve and extend its generalization capabilities.\nMore generally, easy-to-hard-generalization is the paradigm where human annotation is provided for easier tasks, but aiming to enable generalization to harder tasks with no additional supervision (Schwarzschild et al., 2021; Bansal et al., 2022; Burns et al., 2023; Hase et al., 2024; Sun et al., 2024). For instance, Zhang et al. (2024) study this transcendence phenomenon in chess, showing that chess transformers can sometimes outperform all players in the training dataset. Similarly, Sun et al. (2024) finds that a reward model trained on easier mathematical problems can be effectively transferred to harder problems, facilitating generalization through reinforcement learning. Shin et al. (2024) identifies overlap data points-instances containing both easy and hard patterns-as a key mechanism for weak-to-strong generalization, allowing weak models to pseudolabel easier patterns while stronger models use these labels to learn harder patterns. Our work shows that a similar mechanism emerges naturally within self-improvement, where progressively increasing difficulty enables models to generate useful supervision signals for harder tasks without explicit human intervention.\nSelf-Improvement. When high-quality training labels are unavailable or costly to obtain, training on self-generated labels provides an efficient way to broaden the capabilities of a model. Typically, this involves generating candidate labels, filtering or verifying them to prune errors, and retraining on the refined self-generated data (Zelikman et al., 2022; Wang et al., 2022b; Huang et al., 2022; Singh et al., 2023; Chen et al., 2023; Gulcehre et al., 2023; Madaan et al., 2024; Yuan et al., 2024; Liang et al., 2024; Pang et al., 2024). This approach has been successfully applied across various domains, including reasoning (Zelikman et al., 2022; Huang et al., 2022; Singh et al., 2023; Pang et al., 2024), mathematics (Zhang & Parkes, 2023; Charton et al., 2024; Alfarano et al., 2024; Liang et al., 2024), coding (Chen et al., 2023), and general instruction tuning (Wang et al., 2022b; Yuan et al., 2024). Recent studies further analyze and refine the self-improvement process. Song et al. (2024) identify the generation-verification gap as a key limiting factor, while Huang et al. (2024) introduce a \"sharpening mechanism\" that improves reliability by training on best-of-N model outputs. Our work builds on STaR (Zelikman et al., 2022) and ReST (Gulcehre et al., 2023), leveraging iterative prediction, filtering, and fine-tuning to enhance model capabilities.\nModel Collapse. A key challenge in self-improvement is model collapse, where iterative training on self-generated outputs leads to performance degradation (Hataya et al., 2023; de Arcaute et al., 2023; Alemohammad et al., 2023). While some work suggests this degradation is inevitable (Shumailov et al., 2024, 2023; Zhang & Parkes, 2023), several mitigation strategies have emerged, including maintaining original training data (Gerstgrasser et al., 2024), careful data mixing (Gerstgrasser et al., 2024; Dohmatob et al., 2024; Briesch et al., 2023), and reliable verification mechanisms Gillman et al. (2024); Feng et al. (2024). Our approach incorporates these insights through unsupervised filtering techniques and controlled data generation, effectively preventing collapse while enabling sustained improvement. We provide additional discussion of related works in Appendix A."}, {"title": "3 Preliminaries and Experimental Setup", "content": "In this section, we describe the experimental setup, including the model architecture, tasks, training methodology, evaluation criteria, and the self-improvement framework.\nModels. We adopt the LLaMA architecture with six layers, six attention heads, and an embedding dimension of 384 and a total of 14M parameters. Positional embeddings are excluded, using the No Positional Encoding (NoPE) method (Kazemnejad et al., 2024). To evaluate applicability to large language models (LLMs), we extend our experiments to pretrained models (Llama-1B, Llama-3B) in Section 7.3. Character-level tokenization is used across all tasks, except for the maze-solving task, where numbers (0-99) are tokenized as individual tokens instead of characters.\nTasks. We evaluate our approach on a diverse set of tasks, categorized into arithmetic operations, string manipulation, and maze solving. \n\u2022 Arithmetic operations:\n1. Addition (Section 4.1, 6.1): We consider both reverse and forward addition of two numbers of equal length. In reverse addition, both operands and the answers are reversed, so they are written with the least significant digit first. Forward addition, in contrast, follows the standard format, with the most significant digit first.\n2. Multiplication (Section 6.2): Multiplication tasks are presented in a chain-of-thought (CoT) data format (Deng et al., 2024), which includes intermediate steps to guide the computation.\n\u2022 String manipulation:\n1. Copy (Section 4.2): The task is to replicate the input sequence exactly.\n2. Reverse (Section 4.2): The task is to reverse the input sequence.\n\u2022 Maze solving (Section 6.3): The task is to find the shortest path between a start node and an end node in a tree-structured graph. The shortest path is defined as the path with the fewest number of hops, where each hop represents a transition between two adjacent nodes.\nEach task presents distinct challenges that test different aspects of model generalization. Reverse ad-dition (Lee et al., 2023) has been widely adopted task for length generalization. Forward addition, by contrast, is significantly harder due to its increasing dependency in length, making it more challenging for transformers to extrapolate (Zhou et al., 2023). Copying and reversing sequences are considered fundamental operations but remain difficult for standard transformers without architectural modifica-tions (Anil et al., 2022; Zhou et al., 2023). Multiplication is challenging even in-distribution (Dziri et al., 2024), and fine-tuning large language models with CoT reasoning has shown limited success. Finally, maze solving extends our evaluation to structured reasoning tasks, requiring models to internalize search behavior rather than relying on local token-to-token dependencies (Bachmann & Nagarajan, 2024). These tasks collectively provide a controlled yet diverse testbed for studying the effectiveness of self-improvement across different problem domains."}, {"title": "Task Difficulty", "content": "All tasks we consider admit a straightforward notion of difficulty.\n\u2022 Addition: The maximum length of the two operands.\n\u2022 Multiplication: The maximum length of the two operands. Intuitively, a 5-by-5 multiplication problem is harder than that of 4-by-4. But even a 6-by-1 multiplication is considered harder than 5-by-5, because the model has never seen training data containing length more than 5.\n\u2022 String Copy & Reverse: The length of the input string.\n\u2022 Maze Solving: We define difficulty as 1) the number of hops between the start and end nodes and 2) the total number of nodes in the graph. The number of hops corresponds to the number of transitions required to reach the goal.\nWe denote the difficulty level of a problem instance $x$ as an integer Difficulty($x$)."}, {"title": "Data Generation and Sampling", "content": "We generate an initial supervised training dataset $\\mathcal{D}_0$ of up to a fixed difficulty level $d_0$ by uniformly sampling the difficulty level $d \\leq d_0$, followed by independent sampling of the data conditioned on the difficulty. Denoting the input as $x_i$, labels as $Y_i$,\n$$\\mathcal{D}_0 = \\{(x_i, Y_i)\\}_{i=1}^{N_0}, \\text{ where Difficulty}(x_i) \\leq d_0.$$\nFor arithmetic tasks such as addition or multiplication, each problem instance is represented as a tuple $x_i = (a_i, b_i)$, with $\\mathcal{D}_0$ containing problems of up to $d_0$-digit numbers. The digit lengths $(d_{a_i}, d_{b_i})$ are uniformly sampled from $\\{1, ..., d_0\\}^2$, and the numbers $a_i$ and $b_i$ are uniformly sampled from the ranges $[10^{d_{a_i}-1}, 10^{d_{a_i}} - 1]$ and $[10^{d_{b_i}-1}, 10^{d_{b_i}} - 1]$, respectively.\nFor string manipulation tasks (e.g., copying or reversing), we uniformly sample string lengths up to $d_0$ and generate random sequences. Similarly, for maze-solving tasks, we uniformly sample the number of hops or total nodes in the maze and generate random graphs that satisfy these constraints. This strategy ensures balanced coverage across all difficulty levels up to $d_0$."}, {"title": "Self-Improvement Framework", "content": "The self-improvement framework begins by training a model using the labeled training dataset $\\mathcal{D}_0$, which gives us our base model $M_0$.\nFor each subsequent round $r$ ($r = 1, 2, 3, ...$), we increase the problem difficulty, such as the number of digits or string length for arithmetic and string manipulation tasks, or the number of hops for maze-solving tasks, to $d_r$. Using the previous model $M_{r-1}$, we generate $N_r$ new self-improve data samples $D_r$ defined as:\n$$D_r = \\{(x_i, M_{r-1}(x_i))\\}_{i=1}^{N_r}, \\text{ where } d_{r-1} \\leq \\text{Difficulty}(x_i) \\leq d_r.$$\nInstead of the true labels $y_i$, we obtain the predicted labels $M_{r-1}(x_i)$ from the output of the model.\nAt each self-improvement round $r$, the model is trained on the combined dataset $\\mathcal{D}_0 \\cup \\mathcal{D}_1 \\cup ... \\cup \\mathcal{D}_{r-1}$, which includes the initial labeled dataset and all subsequent self-improvement datasets. To ensure sufficient training on the most recently generated data $D_{r-1}$, we up-sample it with a sampling probability of 50%. The remaining datasets $\\mathcal{D}_0, ..., \\mathcal{D}_{r-2}$ are sampled uniformly at random. This iterative process allows the model to gradually tackle harder problems, leveraging its own predictions to expand the training data and improve generalization."}, {"title": "Data Filtering", "content": "We employ two unsupervised data-filtering methods to refine our self-improvement dataset: 1) length filtering and 2) majority voting. For a given self-improved dataset $D_r = \\{(x_i, M_{r-1}(x_i))\\}_{i=1}^{N_r}$ at round $r$, data is filtered based on specific criteria on the model-generated outputs $M_{r-1}(x_i)$, producing a smaller, refined dataset $\\mathcal{D}\\'_r = \\{(x_i, M_{r-1}(X_i))\\}_{i=1}^{N\\'_r}$. We given detailed motivations and description in Section 5."}, {"title": "Training and Evaluation", "content": "Except for the experiments on pretrained Llama 3.2 models, all models are trained from scratch using the conventional next-token prediction objective. The loss is computed solely on the completion, meaning that the input prompt is masked, and only the model's predictions are included in the loss computation. Detailed settings, including hyperparameters and training schedules, are provided in the Appendix.\nDuring inference, we use greedy decoding and exact-match accuracy as the primary metric for evaluation. A prediction is deemed correct if all tokens in the output sequence match the ground truth; any discrepancy in the generated tokens is classified as an incorrect prediction."}, {"title": "4 Length Generalization on Reverse Addition and String Copying/Reversing", "content": "With self-improvement training, transformers can achieve state-of-the-art length generaliza-tion on reversed addition, string copying and maze solving, without further architecturalmodifications.\nIn this section, we apply our self-improvement framework to relatively simple tasks such as reverse addition and string copying/reversing. These tasks serve as testbeds to demonstrate that our framework can extend model capabilities far beyond the original training distribution through multiple iterations of self-improvement, even in the absence of additional data filtering."}, {"title": "4.1 Reverse Addition", "content": "Reversed addition, where the operands and output are written with the least significant digit first, has been shown to enhance sample efficiency and performance (Lee et al., 2023). Reversed addition has become a popular setting for studying length generalization in arithmetic tasks (Lee et al., 2023; Shen et al., 2023; Zhou et al., 2023, 2024; Cho et al., 2024; McLeish et al., 2024). Here, we show that the self-improvement framework achieves substantial length generalization on reversed addition without any modifications to positional encodings, input formats, or the Transformer architecture.\nSetting. The initial supervised dataset Do contains 2 million examples of reverse addition, with operand lengths ranging from 1 to 16 digits. This dataset is used to train the model for 10,000 steps. In subsequent self-improvement rounds, we sample 50,000 additional training examples at each round, extending the operand length by one digit. Specifically, at self-improvement round r, the self-generated data Dr consists of length-16 + r examples produced by the model Mr. The model is fine-tuned on the combined dataset Do U D\u2081 U... U D\u2081 for 1,500 steps, resulting in an improved model Mr+1.\nResults. Figure 3 demonstrates that, starting with a model trained on 1 to 16-digit reverse addition, the self-improvement framework enables near-perfect length generalization up to 100 digits without any additional supervision."}, {"title": "4.2 String Copy & String Reverse", "content": "Copying and reversing a given input string is another task that is considered hard for vanilla trans-formers (Anil et al., 2022; Zhou et al., 2023). The input string consists of digits from 0 to 9.\nSetting. The initial training set D, consists of 2 million examples of strings of length 1 to 10. The vocabulary of the string is the digits 0 to 9. For each subsequent round r, we sample Dr consisting of 50000 examples of length 10 + r from the model Mr. Then we continue training Mr on the combined dataset D\u2081 U... U Dr for 500 steps to obtain Mr+1."}, {"title": "5 Unsupervised Data Filtering", "content": "Filtering self-improvement data using length-based filtering and majority voting is crucial forsustaining the self-improvement process while keeping the method general-purpose."}, {"title": "5.1 Motivation for Data Filtering", "content": "The self-improvement framework operates on theprinciple that the model can generalize slightly be-yond the difficulty level it was trained on, incremen-tally generating self-improve data of increasing dif-ficulty. A critical component for this framework tosucceed is the quality of the self-generated data. Low-quality data can negatively impact the model's gener-alization performance, leading to even lower-qualitydata in subsequent rounds and ultimately causing acascading degradation of the self-improvement pro-cess."}, {"title": "OOD Results are often Short", "content": "A common error in model-generated data for tasks with higherdifficulty than the training data is that the generated labels are often shorter than the correct answers.Figure 6 illustrates this phenomenon for both the reverse addition and CoT-multiplication tasks. Inreverse addition (Left), as the number of digits in the training data increases (or as self-improvementrounds progress), the proportion of incorrect self-generated data where the answer is shorter than thecorrect label length also increases. Similarly, for CoT-multiplication (Mid and Right), most incorrectanswers are shorter than the correct ones. Furthermore, in cases where the answers are shorter, theoutputs often miss one or more reasoning steps in the chain-of-thought (CoT) reasoning process."}, {"title": "Leveraging Label Diversity with Majority Voting", "content": "Self-improvement relies on the model'sability to generalize to slightly harder problems. However, this generalization is not always robustand can vary significantly across different training instances (Zhou et al., 2024). Majority votingmitigates this variability by aggregating predictions across multiple independently trained models,thereby improving the reliability of self-generated labels."}, {"title": "5.2 Data Filtering Methods", "content": "We focus on two key data-filtering methods used in this work: length filtering and majority voting (illustrated in Figure 8). For a given self-improved dataset $D_r = \\{(x_i, M_{r-1}(x_i))\\}_{i=1}^{N_r}$ at round r, data is filtered based on specific criteria applied to the model-generated outputs $M_{r\u22121}(x_i)$. This process produces a smaller, refined dataset $D\\'_r = \\{(x_i, M_{r-1}(x_i))\\}_{i=1}^{N\\'_r}$, which is then used for training in subsequent rounds.\nThese filtering strategies are essential for sustaining the self-improvement framework and enabling effective generalization across diverse tasks, including forward addition, multiplication, and maze-solving."}, {"title": "Relative Length Filtering", "content": "The observation that OOD results are often short motivates a filteringmethod based on the relative lengths of model-generated predictions. Specifically, predictions shorterthan a predefined threshold\u2014calculated relative to the maximum prediction length within theirbatch\u2014are filtered out. For a batch of model-predicted outputs, we identify the maximum lengthof the output $L = \\max |M_{r\u22121}(x_i)|$ and filter out predictions $M_{r\u22121}(x_i)$ with lengths shorter than apredefined threshold \u03c4. This method is unsupervised, as it relies solely on comparing lengths withinmodel-generated outputs rather than referencing ground-truth labels. While particularly suited tolength generalization tasks, where harder problems are expected to yield longer answers, length-basedfiltering demonstrates broader potential for addressing similar challenges in other tasks."}, {"title": "6 Length and Difficulty Generalization on Forward Addition, Multiplication, Maze", "content": "Augmenting self-improvement training with label filtering based on length and majority voting,transformer models can achieve length and difficulty generalization on forward addition, COT-multiplication and maze solving."}, {"title": "6.1 Forward Addition", "content": "Forward addition is a straightforward yet challenging task for transformer models to length generalize effectively. Zhou et al. (2023) hypothesize that Transformers are more likely to length generalize on tasks with small RASP-L complexity. They demonstrate that tasks such as reverse addition and copying have low RASP-L complexity, making them easier to length generalize, whereas forward addition poses a greater challenge. In reverse addition, each step only requires processing a fixed-size subset of the input. However, in the forward addition, the size of the relevant input required to generate correct tokens increases, making the problem more complex."}, {"title": "6.2 Multiplication", "content": "We also extend our approach on multiplication, which is a challenging task even in-distribution, as noted in Dziri et al. (2024). Fine-tuning large language models on datasets with chain-of-thought(CoT) steps has shown limited success. We adopt a data format similar to (Deng et al., 2024), where the input prompt is structured as 9172*9431=, and the label expands the multiplication into stepwise additions, such as: 17442+067801 (132331)+0075180(1398490)+00091720=13976630. Each step includes the intermediate results (in parentheses) representing partial products formed by multiplying the first operand with each digit of the second operand.\nThe data format is inherently asymmetrical. For example, an m-by-n multiplication requires n intermediate steps, where each step corresponds to multiplying the m-digit number by one digit of the n-digit number. Conversely, an n-by-m multiplication involves m intermediate steps of multiplying the n-digit number by each digit of the m-digit number.\nThe model is initially trained on n-by-n multiplication examples with n = 5. Directly introducing n + 1-by-n + 1 examples results in poor performance, hence, we adopt a more fine-grained difficulty schedule. In each self-improvement round, we incrementally increase one operand by one digit, sampling n + 1-by-m and m-by-n + 1 examples, where m grows from 1 to n + 1. This gradual progression allows the model to adapt incrementally to larger operand sizes, making the transition to harder examples more manageable."}, {"title": "6.3 Maze-Solving", "content": "While arithmetic tasks and string manipulations provide valuable testbeds for studying language model generalization, we extend our evaluation to a more complex problem: finding the shortest path in a maze. Pathfinding presents significant challenges for autoregressive models (Bachmann & Nagarajan, 2024). Our mazes can be represented by a tree graph in a 2-dimensional space and they do not have loops.\nEach tree graph consists of N nodes, which are randomly labeled with numbers between 0 and 99. The input format follows the structure: start_node>end_node, followed by a graph adjacency list formatted as random_node:adjacent_nodes. Here, adjacent_nodes are separated by commas (,), and each random_node is separated by a hyphen (-). The target output is a sequence of hops from the start node to the end node, separated by >."}, {"title": "6.3.1 Increasing the Number of Hops", "content": "The difficulty of the maze-solving task increases with the number of hops required to traverse from the start node to the end node, which directly corresponds to the number of > symbols in the output. We begin by training the model on a labeled dataset containing paths of up to h = 9 hops. In each self-improvement round, we increase h by one, progressively introducing longer paths.\nThe model is first trained on a dataset Do containing 5 million labeled maze-solving examples, where the number of nodes is fixed at N = 30 and paths range from h = 1 to h = 9 hops. This initial training phase spans 25,000 steps. In subsequent self-improvement rounds, we generate 50,000 additional training examples, increasing h by 1, and fine-tune the model for 5,000 steps per round. We experiment with both unfiltered training data and majority voting, where only outputs agreed upon by all 3 models are retained."}, {"title": "6.3.2 Increasing the Number of Nodes", "content": "Another approach to increasing task difficulty is to expand the number of nodes in the graph while keeping the number of hops fixed at h = 9.\nThe model is first trained on a dataset Do containing 5 million labeled maze-solving examples, with a fixed hop count h = 9 and node counts ranging from N = 10 to N = 30. This initial training lasts 12,000 steps. In self-improvement rounds, the number of nodes N is increased by 3 per round, generating 50,000 additional training examples at each step and fine-tuning for 4,000 steps. We compare training without filtering against majority voting, where only outputs agreed upon by all 3 models are kept.\nWhile this experiment focuses on fixing one dimension (number of hops or number of nodes) and increasing the other, alternating between increasing the difficulty in both dimensions is expected to generalize the maze-solving task to handle larger graphs and longer paths simultaneously."}, {"title": "6.3.3 Using Verifiers for Data Filtering", "content": "Solving the shortest path problem can be computationally expensive, but verifying the correctness of a given solution is significantly simpler. A valid path can be verified by traversing the sequence and ensuring three conditions: 1) each move is valid, meaning the path follows adjacency constraints; 2) the final destination matches the intended goal; and 3) no nodes are repeated, confirming that the solution is indeed the shortest path.\nSelf-improvement frameworks commonly incorporate verifiers to filter self-generated data, often leveraging trained models or reward models (Zelikman et al., 2022; Singh et al., 2023; Hosseini et al., 2024; Lightman et al., 2023). While our primary focus is not on training or designing an additional verification mechanism, we investigate the effectiveness of using an external verifier as a data-filtering method.\nTo this end, we evaluate an oracle verifier that enforces two essential constraints: 1) move validity, ensuring that every transition in the generated solution adheres to the adjacency constraints of the maze, and 2) end validity, confirming that the final node in the solution corresponds to the correct destination. We compare the effectiveness of this oracle-based filtering against self-improvement without data filtering and majority-voting-based filtering to assess its impact on performance and stability.\nthat self-consistency mechanisms alone can be sufficient for maintaining high-quality training data."}, {"title": "7 Ablations", "content": "Augmenting self-improvement training with label filtering based on length and majority voting,transformer models can achieve length and difficulty generalization on forward addition, COT-multiplication and maze solving."}, {"title": "7.1 OOD generalization Increases with More Self-Improvement", "content": "The amount of out-of-distribution (OOD) extrapolation increases with more rounds of self-improvement in the addition, copying, and multiplication tasks.\nOut-of-distribution (OOD) generalization is a critical measure of a model's ability to extrapolate beyond its training data. In tasks like reverse addition and copy, we observe that OOD extrapolation capabilities improve progressively as the model undergoes more rounds of self-improvement."}, {"title": "\"Safe Range\" of Sampling of Next Round Difficulty", "content": "At the core of the self-improvement framework is the observation that the models perform well on samples slightly harder than those in the training set. Sampling instances that are too difficult for the current model is detrimental to the quality of self-improvement data, which causes downstream performance to break down. Here, we have shown that the \u201csafe range\u201d for sampling next-round difficulty becomes more forgiving. Nevertheless, in real-world datasets, difficulty measures are not strictly quantifiable, and sampling by difficulty often involves noise. It is important to develop precise and controllable notions of difficulty for real-world tasks."}, {"title": "7.2 Self-Improvement can be Accelerated", "content": "We observe that for addition and string manipulation tasks, the amount of extra OOD generalization increases roughly lineraly with each additional round of self-improvement (Figure 17). This observation suggests that sampling multiple difficulty levels within the safe range per round could lead to exponential improvements in performance."}, {"title": "Accelerated Self-Improvement Schedule", "content": "Building on the observations in Figure 17, we propose an accelerated self-improvement schedule where the model samples multiple difficulty levels at each round, instead of incrementally increasing by only one additional length (for string manipulation) or digit (for reverse addition). As shown in Figure 18, this approach significantly speeds up performance in copying and reverse addition tasks, allowing the model to achieve high accuracy with fewer training rounds and training steps. At each round, the self-improvement dataset is uniformly sampled from all difficulty levels achieving over 99% evaluation accuracy. All other hyperparameters remain unchanged."}, {"title": "7.3 Pretrained Models achieve Faster Acceleration", "content": "Self-improvement is more effective for larger pretrained models, and these models achievefaster acceleration."}, {"title": "8 Analysis on Errors", "content": "During self-improvement failure cases, we see an error avalanche phenomenon where errorsbuild up, eventually causing performance to crash. By simulating model errors at a particularround, we find that the model tolerates errors up to a certain point before crashing in accuracy."}, {"title": "8.1 Error Avalanches in Self-Improvement", "content": "Out-of-distribution (OOD) generalization is highly sensitive to inaccuracies in self-generated data."}, {"title": "8."}]}