{"title": "MRWeb: An Exploration of Generating Multi-Page Resource-Aware Web Code from UI Designs", "authors": ["Yuxuan Wan", "Yi Dong", "Jingyu Xiao", "Yintong Huo", "Wenxuan Wang", "Michael R. Lyu"], "abstract": "Multi-page websites dominate modern web development. However, existing design-to-code methods rely on simplified assumptions, limiting to single-page, self-contained webpages without external resource connection. To address this gap, we introduce the Multi-Page Resource-Aware Webpage (MRWeb) generation task, which transforms UI designs into multi-page, functional web UIs with internal/external navigation, image loading, and backend routing. We propose a novel resource list data structure to track resources, links, and design components. Our study applies existing methods to the MRWeb problem using a newly curated dataset of 500 websites (300 synthetic, 200 real-world). Specifically, we identify the best metric to evaluate the similarity of the web UI, assess the impact of the resource list on MRWeb generation, analyze MLLM limitations, and evaluate the effectiveness of the MRWeb tool in real-world workflows. The results show that resource lists boost navigation functionality from 0% to 66%-80% while facilitating visual similarity. Our proposed metrics and evaluation framework provide new insights into MLLM performance on MRWeb tasks. We release the MRWeb tool, dataset, and evaluation framework to promote further research\u00b9.", "sections": [{"title": "Introduction", "content": "Websites are important in today's digital landscape, serving as essential platforms for diverse applications in our daily lives (web, 2024).\nThey can be categorized into two types: single-page and multi-page. Single-page websites update content dynamically on a single page without reloading, while multi-page websites consist of multiple interconnected pages. In reality, multi-page websites are dominant due to their scalability and structured navigation functionality (Way to Grow Editorial Team, 2024; Web.com Editorial Team, 2024). To examine this, we conducted a preliminary study by sampling the top 300 most visited websites ranked by the Tranco list \u00b2. For each website, we analyzed its structure by recursively visiting all internal links to identify distinct internal pages, as well as counting the number of external links, internal links, and images present on each webpage. The results showed that 271 (90.3%) of the sampled websites are multi-page, highlighting their widespread usage and the complex structures they often exhibit. Appendix A describes the study in detail.\nPractical solutions for generating multi-page web user interfaces (UIs) from designs remain underexplored. Existing work focuses on the simpler task of design-to-code, which generates self-contained web pages (Figure 1). This leaves a gap in addressing the complexities of multi-page, resource-aware web UI development, where (1) webpages link to internal pages with unlimited navigation paths, (2) webpages link to external resources like websites and images, and (3) webpages route to backends for data exchange. We term this task MRWeb to highlight the shift from design-to-code to Multi-Page Resource- Aware Webpage generation. Figure 1 contrasts self-contained webpages with MRWebs, which enable code-free development from UI designs to resource-aware, navigable websites, democratizing web development.\nUnfortunately, constructing such a framework poses several challenges. First, there is no established data structure that integrates visual design elements with internal or external resources and tracks their correspondences. Unlike the design-to-code task that generates self-contained code from design images, MRWeb should incorporate resources, navigation paths, and their links to visual design elements, necessitating a new data structure. Second, there is a lack of high-quality datasets for MRWeb since the previousn-to-code task did not contain real images or internal/external links. Third, there are no standardized metrics for evaluating the performance of the MRWeb task. This makes it difficult to measure the accuracy of generated links, images, and their correspondence with visual design elements. As a result, there is insufficient understanding of how effectively MLLMs produce MRWeb code from designs.\nTo bridge this gap, we propose the first evaluation framework for MRWeb generation. Specifically, we define a novel data structure, the resource list, which uses a dictionary-like format to store internal/external resources, such as links and images, and their correspondence with visual designs (e.g., screenshots). Next, we collect the first MR-Web dataset, comprising 500 websites (300 synthetic, 200 real-world), along with their associated resource lists, screenshots, and ground truth code. The generation framework accepts the resource lists and screenshots as inputs and calls MLLMs to generate functional MRWeb code directly. To assess the generated code, we introduce a suite of metrics designed to evaluate and analyze both the visual and functional performance of MLLMs on the MRWeb task. Additionally, we implement this framework as a user-friendly tool for MRWeb generation, releasing all code and data to encourage future research.\nTo sum up, the contributions of this study are:\n\u2022 Define the MRWeb problem, introduce the first MRWeb benchmark with an innovative resource list data structure, and construct the first MRWeb dataset consisting of 500 websites.\n\u2022 Propose a suite of metrics, conduct the first comprehensive image quality assessment (IQA) in the web UI domain to determine the best evaluation metric for web UI code generation, and collect an annotated IQA dataset for future studies.\n\u2022 Conduct a comprehensive study to evaluate the performance of SOTA MLLMs on MRWeb generation and highlighting some challenges faced by MLLMs.\n\u2022 Develop a user-friendly tool for MRWeb tasks and release all code and data to foster further research and development in this emerging area."}, {"title": "Related Work", "content": "Existing UI code generation methods operate on three unrealistic assumptions: (1) websites have a single or limited number of pages, (2) webpages lack external links, and (3) webpages do not interact with backends. Our study in Appendix A highlights the misalignment of these assumptions with real-world web UI needs. Table 1 provides a comparison of our benchmark with existing works, contextualizing our contribution. A detailed discussion of related works is in Appendix B."}, {"title": "Task Formulation", "content": "Resource List The resource list serves as a structured representation of a webpage's navigational and visual elements, such as hyperlinks, images, and backend routing. Each entry in the resource list includes attributes like position, type, and URL, for instance, {position: bonding_box, type: image, link:/dog.png}. This structure is crucial for enabling MRWeb generation to replicate navigational features and image sources accurately. Without the resource list, MLLMs would generate static replicas lacking interactivity and navigation. In our experimental setup, resource lists are extracted automatically using Python Selenium\u00b3. For real-world applications, we developed an intuitive user interface that allows users to highlight actionable elements by drawing bounding boxes and inputting the corresponding resource information, as illustrated in Appendix I.\nTask Definition Let the ground-truth webpage's HTML+CSS code be $C_o$, screenshot be $I_o$, and resource list be $R_o$, the MRWeb generation task uses an MLLM $M$ to produce HTML+CSS code $C_g = M(I_o, R_o)$ that approximates $C_o$. The quality of $C_g$ is assessed by comparing the generated resource list $R_g$ with $R_o$ and the screenshot $I_g$ rendered from $C_g$ with $I_o$, ensuring both functional and visual alignment."}, {"title": "Dataset Collection", "content": "We collect two types of data: synthetic and real-world. Synthetic data enables controlled, diverse examples, including rare edge cases, but lacks the variety of real-world content. Real-world data captures authentic webpage diversity, supporting model robustness across HTML structures and styles. Combining both data types provides a comprehensive benchmark.\nThis section outlines our collection of code-screenshot pairs for synthetic and real-world data, the extraction of resource lists, and the statistics of the sampled data."}, {"title": "Synthetic Data Collection", "content": "To create the synthetic UI-to-MRWeb dataset, we adopt and modify the WebSight dataset (Lauren\u00e7on et al., 2024). The WebSight dataset contains 2 million HTML samples and their corresponding screenshots, covering a broad spectrum of website concepts. However, it cannot be used directly for an MRWeb dataset because 1) its websites lack valid internal or external links, making navigation to other pages impossible, and 2) images on the sites are randomly loaded via the Unsplash\u2074 API. This random loading causes visual inconsistencies, as identical code can result in different visuals, complicating benchmarking. To address these issues, we enhance the WebSight dataset through link insertion and image replacement.\nLink insertion. Using all website links from the C4 (Raffel et al., 2019) validation set, we create a URL list. For each HTML document, we parse its content and iterate over all hyperlink tags, assigning a randomly chosen URL from our list to each hyperlink attribute. This modification ensures that every website includes valid external links to other sites.\nImage replacement. To ensure consistency and diversity in visual representation, we replace random images in the WebSight dataset with static, unique images for each webpage. Using the Unsplash API, we fetch images with specific keywords, dimensions, and properties to guarantee that the pictures remain consistent yet unique."}, {"title": "Real-world Data Collection", "content": "We collect real-world data by capturing and simplifying HTML content from live websites. We first collect 500 URLs of real-world websites from the C4 (Raffel et al., 2019) validation set as our data source. However, HTML files on the web often contain non-visible noise\u2014such as comments, scripts, and hidden content\u2014that makes them excessively lengthy and can exceed the token limits of most models. To create the real-world UI-to-MRWeb dataset, we develop a pipeline that collects and processes HTML code and screenshots from live websites. This pipeline ensures that each webpage captures authentic and static content while maintaining a simplified HTML structure compatible with our UI-to-MRWeb benchmark. The primary steps in this pipeline include saving HTML files, filtering HTML, simplifying HTML, and capturing screenshots, as outlined below:\n1. Saving HTML files: The HTML+CSS content from each URL is saved into a single HTML file, ensuring all components are intact.\n2. Filtering HTML: We discard websites that are blank or erroneous (e.g., page not found).\n3. HTML Simplification: We simplify the HTML by removing all non-visible elements, comments, and non-functional Javascripts.\n4. Final Screenshot: A final screenshot is taken after simplification, completing the real-world data pipeline."}, {"title": "Resource List Extraction", "content": "Resource lists capture navigational and visual elements such as links, images, and backgrounds, structured to preserve the functionality and layout of each webpage. For each webpage:\n\u2022 Links (<a> tags): We extract each hyperlink's position, type, and target URL.\n\u2022 Images and Background Images: For images, including both image tags and CSS background images, we record their position and source URL.\nThe resource list is automatically constructed by iterating through the webpage's elements using Python Selenium, collecting attributes for each, and verifying their visibility and functionality."}, {"title": "Automation & Dataset Statistics", "content": "We emphasize that the data collection pipeline is fully automated, enabling the on-demand generation of large-scale MRWeb training data. In principle, the synthetic dataset could match the full size of the WebSight dataset (two million), and the real-world data could encompass any website accessible on the internet. To support future research, however, we sampled 300 synthetic and 200 real-world instances. The statistics, quantitative metrics of the sampled dataset, and comparison with other datasets are provided in Table 1. To get a sense of the range of domains covered in our benchmark, we manually categorize what type of webpages they are based on their functions. We present the pie chart of the most frequent domains in Figure 2. The most prominent genres are companies' or organizations' websites and blogs."}, {"title": "Study Setup", "content": ""}, {"title": "Evaluated Models", "content": "We employ three state-of-the-art (SOTA) MLLMs: Gemini 1.5 (Google, 2024), GPT-40 (OpenAI, 2024a) and Claude-3.5 (Anthropic, 2024a) to evaluate their performance on MRWeb. the specific model numbers are 20240806 for GPT-40, 20240620 for Claude-3.5-Sonnet, and Gemini-1.5-Pro accessed during November 2024. For MLLM model configurations, we set the temperature to 0, the random seed to 42, and the max_tokens parameter to 4096 for each model. Other parameters are maintained at their default settings as specified in the corresponding API documentation (Google AI, 2024; OpenAI, 2024b; Anthropic, 2024b)."}, {"title": "Prompting Strategies", "content": "We use four types of prompting methods: self-contained, zero-shot, CoT, and self-refine. Self-contained prompting is adapted from Si et al. (Si et al., 2024) to let the model directly generate code from screenshots without resource lists. This method serves as a baseline for other methods that adopt input resource lists. Zero-shot prompting directly lets the model generate HTML code from screenshots and resource lists. Chain-of-Thought (CoT) prompting (Wei et al., 2022) generates a chain of thought for each question and then generates the corresponding code. For CoT, we use the \"let's think step by step\" instruction from Chae et al. (Chae et al., 2024). Self-refine prompting (Chen et al., 2023) let the model refine its own generated code via multi-turn conversation. We adopt the self-refine prompting and direct promoting method from Si et al. (Si et al., 2024). We list the exact prompts used in our experiments in Appendix C."}, {"title": "Metrics", "content": ""}, {"title": "High-level Metrics", "content": "For high-level performance, we evaluate visual similarity and functional similarity. For visual similarity, we explore three levels of image similarity metrics commonly applied in design-to-code or other computer vision (CV) tasks (Wang et al., 2004): pixel, structural, and semantic. The detailed background and calculation of these metrics are in Appendix D."}, {"title": "Fine-Grained Metrics", "content": "Beyond assessing visual and functional similarity, we employ a suite of fine-grained metrics to evaluate the specific capabilities of MLLMs, including visual grounding, color recognition, and text extraction. For each pair of matched resources in RER, we calculate:\n\u2022 Position offset: Measures the position shift between the generated and the original element with respect to the size of the original web page. For each pair of matched resources (rp, gq), the positional alignment is evaluated by comparing the normalized offset of their corresponding web elements' center points: Position Offset = $\\text{max}(\\frac{|X_p - X_q|}{W}, \\frac{|Y_p - Y_q|}{H})$. $(X_p, Y_p)$ and $(X_q, y_q)$ are the center positions of the bounding boxes enclosing the elements; W and H represent the width and height of the original webpage.\n\u2022 Area Difference: Measures the differences in size between corresponding actionable elements with respect to the original area of the element: Area Difference = $\\frac{|A_p - A_q|}{A_p}$, where $A_p$ and $A_q$ are the areas occupied by the reference and generated actionable elements.\n\u2022 Color Difference: We use the CIEDE2000 color difference formula (Luo et al., 2001) to assess the perceptual difference between the colors of element rp and gq.\n\u2022 Text Difference: For resources that involve text, such as buttons, their text similarity Text Sim(rp, gq) is calculated by normalizing the number of matching characters by the total length of the text. We calculate the text difference by 1 - Text Sim(rp, gq)."}, {"title": "Experiment Results", "content": ""}, {"title": "The Best Web UI Similarity Metric", "content": "A critical challenge in the MRWeb task is accurately evaluating web UI similarity. To verify the effectiveness of the evaluation metrics and determine the most suitable one, we initiated a human evaluation in the web UI domain, where we compared various image similarity methods (Section 5.3) and discussed their alignment with human preferences. We sample 600 pairs of original and generated screenshots and recruit 14 college students with varying levels of familiarity with web applications to rate their perceived similarity on a Likert scale (Joshi et al., 2015) within five categories: \"Highly Dissimilar\u201d, \u201cDissimilar\u201d, \u201cModerately Similar\u201d, \u201cSimilar\u201d, and \u201cHighly Similar\". This setup follows standard image quality assessment (IQA) procedure (Wang et al., 2004; VQEG, 2000)."}, {"title": "Effectiveness of the Resource List", "content": "Central to our framework is a novel data structure, the resource list. To assess its impact, we employ MLLMs to generate web page code under various prompting strategies (Section 5.2), using the two best-performing metrics (MAE and NEMD) and the best-performing high-level metrics (CLIP) for visual similarity and RER to measure the function similarity. We use the result of the self-contained (SC) prompt as a baseline.\nAdding resource lists can improve the visual similarity of a generated webpage across different MLLMs and metrics. Table 3 shows the comparison of visual metrics. We observe that SC consistently results in the lowest visual scores. This is because resource lists enable MLLMs to include the exact images displayed on the webpage, thus enhancing the overall similarity. Without resource lists, MLLMs can only use placeholder images in the generated web code. Some examples of such cases are in Appendix J, Fig. 10. This highlights the practical value of resource lists in real-world web development compared to self-contained methods.\nResource lists enable MLLMs to generate webpages with valid resources, significantly boosting RER from 0% to 66%-80%. Table 4 shows that under SC prompting, MLLMs exhibit near-zero functional similarity due to the lack of guidance for generating valid links. However, some links are inferred through common sense (e.g., \"facebook.com\" for Facebook). Among prompting strategies, self-refine (SR) consistently achieves the highest scores across models, making it the most effective for both visual and functional metrics. Genimi-Pro emerges as the top-performing MLLM in reproducing functionality.\nAn interesting observation is that CoT prompting slightly decreases performance despite its reasoning capability. We discuss this phenomenon in Appendix G."}, {"title": "Limitations of MLLMs in MRWeb", "content": "For all matched actionable element pairs, we calculate their fine-grained metrics to have a deeper understanding of MLLMs' strengths and weaknesses.\nThe main challenge in MRWeb generation is the visual grounding problem, where MLLMS struggle to replicate the position and size of elements (Table 5). This is reflected by the Positional Shift and Area Difference metrics. MLLMs generate elements with an average 21.5% positional shift relative to the entire webpage and an average 35.6% size difference compared to the original elements.\nDespite these issues, MLLMs perform well in recognizing color and text, with Color Difference and Text Similarity metrics showing much smaller errors. Among the models, Claude-3.5 demonstrates the best positional accuracy and size recognition, with the lowest Positional Shift (15.5%) and Area Difference (27.9%) in the Zero-shot strategy. GPT-40 excels in Text Similarity (0.004), showing strong semantic accuracy."}, {"title": "MRWeb 's Practical Capabilities.", "content": "We developed a user-friendly tool within the MR-Web framework to convert visual designs into multi-page, realistic web UI code. The tool's interface is shown in Appendix I. We conducted a case study using AI-generated design images to build a personal website with three pages: home, project, and contact (Appendix I). We introduced various resources to each page to test the tool's capabilities. These resources include internal links, external links, images, and backend routing (Table 6). The tool successfully addressed all the challenges, achieving a 100% success rate. The demonstration video of the entire development procedure is available online\u2075. Specifically, the generated home page and project page included internal and external links and embedded images with pixel-perfect alignment. The contact page demonstrated the tool's ability to integrate backend routing seamlessly, implying its full-stack capabilities."}, {"title": "Discussions", "content": "This section highlights key implications of our work for future research.\nVisual metrics for UI quality (RQ1) While prior studies emphasize structural and semantic metrics, our findings show that pixel-based metrics better align with human judgment, especially in low-to-medium similarity cases. This suggests hybrid approaches that combine these metrics could provide more robust evaluations. The limited performance of learning-based methods in these cases indicates a need for targeted fine-tuning.\nEnhancing website generation with resource lists (RQ2) Incorporating resource lists significantly boosts both visual and functional metrics, underscoring their potential for advancing automated full-stack development.\nImproving MLLM visual grounding (RQ3) Metrics on positional shift and area difference highlight MLLMs' limitations in precise positioning and sizing. Addressing this may involve improving visual grounding or developing layout-aware prompts for better layout reproduction.\nAdvancing MRWeb generation (RQ4) MRWeb generation connects design and functionality, supporting links, images, and routing. However, non-link-based functionalities remain underexplored, presenting opportunities for more comprehensive full-stack development."}, {"title": "Conclusion", "content": "In this paper, we introduce the MRWeb generation task, addressing the limitations of single-page design-to-code methods. Our contributions include defining the MRWeb problem, creating a benchmark dataset, conducting a comprehensive IQA for web UIs, analyzing MLLM performance, and developing a dedicated MRWeb generation tool. We release the tool, dataset, and evaluation framework to facilitate future research."}, {"title": "Limitations", "content": "Limited Support for Non-Link Functionalities. While MRWeb effectively handles links, images, and routing, it does not currently support non-link-based functionalities, as these require distinct formulations and evaluation metrics. Addressing this limitation is a key focus for future work, with the goal of enabling full-stack development capabilities.\nContext Length Constraints. MLLMs have limited context windows (e.g., 128K tokens for GPT-40), which can be a challenge for websites with extensive token requirements. However, our experiments show that all prompts remain within these limits, highlighting the approach's feasibility for most practical scenarios.\nBackbone Model Selection. We validate MRWeb using three popular multimodal LLMs, but smaller models struggle with complex prompts. To improve adaptability and generalization, future work will explore the potential of emerging models and investigate strategies to handle more complex input scenarios."}, {"title": "Motivating study", "content": "In this study, we show that the three common assumptions made by previous design-to-code works (i.e., UIs are self-contained and do not have external links, UIs consist of a single or a limited number of pages, and UIs use placeholder images)"}, {"title": "Prompts", "content": "In this section, we list the exact prompts used in the experiments.\nHere is a screenshot of a web page. Please write an HTML and Tailwind CSS to make it look exactly like the original web page. Pay attention to things like size, text, position, and color of all the elements, as well as the overall layout. Respond with the content of the HTML+tail-wind CSS code.\nHere is a screenshot of a web page and its \"action list\" which specifies the links and images in the webpage. Please write an HTML and Tailwind CSS to make it look exactly like the original web page. Pay attention to things like size, text, position, and color of all the elements, as well as the overall layout. The format of the action list is as follows: { \"position\": bounding box of format [[x1, y1], [x2, y2]], specifying the top left corner and the bottom right corner of the element; \"type\": element type; \"url\": url of the element; } The action list is as follows: [ACTION LIST]\nHere is a screenshot of a web page and its \"action list\" which specifies the links and images in the webpage. Please write a HTML and Tailwind CSS to make it look exactly like the original web page. Please think step by step, and pay attention to things like size, text, position, and color of all the elements, as well as the overall layout. The format of the action list is as follows: { \"position\": bounding box of format [[x1, y1], [x2, y2]], specifying the top left corner and the bottom right corner of the element; \"type\": element type; \"url\": url of the element; } The action list is as follows: [ACTION LIST]\nHere is a screenshot of a web page and its \"action list\" which specifies the links and images in the webpage. I have an HTML file for implementing a webpage but it has some missing or wrong elements that are different from the original webpage. Please compare the two webpages and revise the original HTML implementation. Return a single piece of HTML and tail-wind CSS code to reproduce exactly the website. Pay attention to things like size, text, position, and color of all the elements, as well as the overall layout. Respond with the content of the HTML+tail-wind CSS code. The format of the action list is as follows: { \"position\": bounding box of format [[x1, y1], [x2, y2]], specifying the top left corner and the bottom right corner of the element; \"type\": element type; \"url\": url of the element; } The current implementation I have is: [CODE] The action list is as follows: [ACTION LIST]"}, {"title": "Visual Similarity Metrics", "content": "This section provides all the details of the visual similarity metrics tested in this work.\nAt the pixel level, we employ metrics that directly compare pixel values to quantify low-level differences:\n\u2022 Mean Absolute Error (MAE) Computes the average absolute difference in pixel intensities, providing a straightforward measure of overall similarity that treats all errors equally without amplifying larger differences.\n\u2022 Peak Signal-to-Noise Ratio (PSNR) Measures the ratio between the maximum possible power of a signal (image) and the power of corrupting noise. PSNR is based on the Mean Squared Error (MSE), with higher values indicating closer similarity between two images. It is widely used to evaluate image quality, especially in compression and restoration tasks.\n\u2022 Wasserstein Distance (Earth Mover's Distance EMD) Measures the minimum transport cost required to transform one image onto the other, capturing spatial differences in pixel values and reflects structural rearrangements needed to align images. The EMD depends on the image size, with larger image pairs producing higher EMD values due to more pixels. To eliminate this dependency and make the metric size-independent, we define a normalized version NEMD = $\\frac{1}{EMD_{max}}$ EMD where $EMD_{max}$ is the maximum possible EMD between a reference image and any other arbitrary image. It represents the worst-case scenario of pixel differences (i.e., the distance from the reference image to a \"completely\" different image). It is achieved by assuming each pixel in the reference image is moved to its farthest possible value (0 or 255), thus providing an upper bound for the EMD. The NEMD ranges from 0 to 1, where higher values indicate greater similarity. This metric is not symmetric, as the $EMD_{max}$ is computed relative to the reference image. The original web page screenshot serves as the reference image in our experiments.\nAt the structural level, we use metrics that capture spatial and perceptual coherence:\n\u2022 Structural Similarity Index Measure (SSIM) Assesses structural coherence by evaluating changes in luminance, contrast, and structural information across images. SSIM models perceived image quality by accounting for local patterns and how structural details align, closely mirroring human visual perception.\nAt the semantic level, we leverage:\n\u2022 CLIP score Derived from the CLIP model, it captures high-level semantic similarities by aligning image embeddings with corresponding language representations. This approach allows us to gauge similarity based on shared meanings and conceptual elements rather than visual appearance alone. It is particularly suited for comparing images representing similar objects or scenes in different styles or contexts.\n\u2022 Learned Perceptual Image Patch Similarity (LPIPS) Evaluates perceptual similarity by computing the distance between deep feature representations of two images. LPIPS offers a robust method for assessing how similar two images are regarding human perception. We use VGG as the backbone model for LPIPS calculation.\nExcept for the CLIP score, which accepts images of varying sizes as input, we pad each image pair with random noise to ensure the two images in the pair are the same size."}, {"title": "Image Quality Analysis Details", "content": "We analyzed the alignment between human judgments and objective similarity scores following established evaluation protocols. This section provides a detailed illustration of the metrics and evaluation practices.\nProcessing Human Scores After collecting human-perceived similarity scores, the scores of each annotator across all image pairs were normalized using z-scores (mean-centered and scaled by standard deviation) to ensure comparability. We computed the Mean Opinion Scores (MOS) for each image pair by first removing outliers and then averaging all human scores of the image pair. This process yields a robust, representative set of subjective human scores. The image database and subjective scores will be publicly available for further study.\nAlignment Analysis of Objective and Subjective Scores To compare the correlation between automatic objective scores and human subjective scores, we use three approaches:\n\u2022 Spearman rank-order correlation coefficient (SROCC): A direct measure of the strength and direction of the association between the ranked objective and subjective scores. In our experiment, we consider the absolute value of SROCC, which is a metric between 0 and 1, where a indicates perfect positive or negative correlation, and 0 indicates no correlation. SROCC is particularly useful for evaluating the consistency of scores produced by objective metrics with those of human subjective assessment scores, regardless of the absolute values of the scores.\n\u2022 Variance-weighted regression analysis: This analysis evaluates how well the objective metrics can predict human subjective scores. By weighting data points based on their variance before conducting a linear regression, we minimize the influence of noisy or uncertain scores, which leads to more robust and reliable predictions. After regression, we calculate several quantities to"}, {"title": "Analysis of Similarity Metrics Across Different Similarity Levels", "content": "Why do learning- and structure-based metrics fail? We divide the image pairs into three equal-sized groups according to their human ratings (i.e., low, medium, high) and analyze their correspondence with human scores. The result is in Table 7.\nFor the low-similarity image group, NEMD consistently achieves the best results (CC-V: 0.422, CC-N: 0.430, SROCC: 0.408), followed by MAE. Notably, SSIM, CLIP, and LPIPS show near-zero correlation with human perception in this range.\nIn the medium-similarity image group, the performance of all metrics generally declines, indicating increased difficulty in predicting similarity when human perception becomes less polarized. NEMD and MAE remain competitive, and CLIP performs best under a nonlinear mapping with a CC-N of 0.262.\nFor the high-similarity image group, PSNR achieves the highest correlation under variance-weighted mapping and non-linear mapping (CC-V: 0.582, CC-N: 0.583). Metrics such as SSIM and LPIPS show significant improvement. SSIM achieves the highest direct correlation (SROCC: 0.379), while LPIPS also demonstrates notable performance in SROCC (0.367).\nIn conclusion, learning- and structure-based metrics fall short due to near-zero performance in low-similarity groups and poor performance"}, {"title": "Why CoT Decrease Performance", "content": "Manual investigation reveals that MLLMs tend to omit content at the end of their generated code, often ending with placeholder comments like \"additional content goes here.\" The decrease in CoT performance arises because when being explicitly prompted with \u201cplease think step by step,\" it generates HTML code part by part, omitting content at the end of every part, thereby compounding the omissions compared to direct prompts. This leads to lower action existence rates and reduced overall performance."}, {"title": "MLLMs' Performance and Input Complexity", "content": "According to Fig. 6, as input image size increases, the MAE (\u2193) grows for all models. GPT-40 and Claude-3.5 demonstrate more stable performance compared to Gemini-Pro, which is sensitive to size, particularly under the self-refine strategy. Among the prompting strategies, self-refine consistently performs best for Gemini-Pro and GPT-40. For Claude-3.5, self-refine shows minimal improvement and occasionally underperforms compared to zero-shot and CoT).\nFor action list length (Fig. 7), the action existence ratio (\u2191) decreases as the list grows longer, highlighting the challenge of maintaining accuracy with increased complexity. GPT-40 and Claude-3.5 perform comparably, with CoT and self-refine providing slight advantages. However, for Claude-3.5, SR again offers a limited improvement to other methods."}, {"title": "Developing Website with the MRWeb Tool", "content": "In RQ4, we conduct a case study using the MRWeb tool, whose user interface is shown in Figure 8. The tool enables users to define design assets, including webpage layouts, external resources (like images and links), and actions such as backend routing. These assets are organized using the action list structure-a dictionary-like format that systematically maps resources to visual elements. Leveraging the MRWeb framework, the tool processes action lists and screenshots as inputs to MLLMs, facilitating the generation of functional, multi-page web UI code with visual consistency.\nThe case study focuses on building a personal website with three internal pages: a home page, a project page, and a contact page, using AI-generated design images (Fig 9). Table 8 summarizes the key challenges introduced for each page."}, {"title": "Visual Comparison of Self-Contained Web and MRWeb", "content": "Figure 10 shows a comparison between self-contained webpages and our multipages resource-aware webpages (MRWebs). Self-contained webpages contain placeholder images and empty links, whereas MRWebs contain real images and links."}]}