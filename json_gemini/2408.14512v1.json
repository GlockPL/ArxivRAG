{"title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings", "authors": ["Duo Wang", "Yuan Zuo", "Fengzhi Li", "Junjie Wu"], "abstract": "Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have emerged as a pivotal framework in graph machine learning, harnessing the ability to capture intricate message-passing patterns for robust graph representation. These advancements have yielded various GNN architectures, including the Graph Convolution Network (GCN) [20], Graph Attention Network (GAT) [31], and GraphSAGE [11]. Despite their efficacy, GNNs often exhibit limited generalization capabilities, struggling to maintain consistent performance when transitioning across different datasets or downstream tasks [19]. This limitation underscores the necessity for more adaptable and universally applicable models in the graph learning domain.\nTo mitigate the dependency on labeled data and enhance the resilience of graph models, self-supervised learning has been widely adopted in GNN training. Techniques such as Deep Graph Infomax (DGI) [32] and GraphCL [44] have demonstrated effectiveness by leveraging mutual information maximization and contrastive learning, respectively. However, these methods typically require fine-tuning task-specific heads for downstream applications, which can be resource-intensive and limit their practicality in diverse scenarios. Moreover, graph prompt learning enhances GNN generalization by using unified task templates and meta-learning to adapt to various downstream applications [25, 29], but it often requires extensive fine-tuning and is constrained by the specificity of task types.\nIn recent years, the remarkable generalization capabilities of Large Language Models (LLMs) have spurred interest in their potential applications within graph machine learning. Some methods attempt to encode graph structures into text for LLM input [3, 10, 33, 23], but these approaches often lead"}, {"title": "Methodology", "content": "In this section, we introduce TEA-GLM, a novel framework designed for cross-dataset and cross-task zero-shot graph machine learning. TEA-GLM consists of two main components: a Graph Neural Network (GNN) to derive node representations from the graph, and a Large Language Model (LLM) to perform zero-shot tasks such as node classification and link prediction. Our methodology involves two key stages: enhanced self-supervised learning of the GNN, where feature-wise contrastive learning with LLM's token embeddings is proposed, and training a linear projector to map graph representations into a fixed number of graph token embeddings by designing an instruction that is suitable for various graph tasks at different levels. The framework of our proposed method is illustrated in Fig. 1."}, {"title": "Notations", "content": "Formally, a graph is denoted as \\( G = (V, E, A, X) \\), where \\( V = \\{v_1, v_2, ..., v_{|V|}\\} \\) with \\(|V| = N\\) indicating the total number of nodes and \\( E = \\{e_1, e_2, ..., e_{|E|}\\}\\) representing the sets of nodes and edges, respectively. The adjacency matrix is denoted as \\( A \\in \\mathbb{R}^{N\\times N} \\), with \\( A_{ij} = 1 \\) iff \\((v_i, v_j) \\in E \\). The feature matrix \\( X \\in \\mathbb{R}^{N\\times F_N} \\) contains the attribute or feature information associated with each node, where \\( x_i \\in \\mathbb{R}^{F_N} \\) is the feature of \\( v_i \\), and \\( F_N \\) represents the dimensionality of features."}, {"title": "Token embeddings-aligned graph self-supervised learning", "content": "Given the increasing model sizes and data volumes in recent years, self-supervised learning has become a prominent research focus due to the scarcity of labeled data. In this context, we propose a contrastive learning method to obtain more transferable node representations suitable for use with large language models (LLMs). Our approach leverages instance-wise contrastive learning and introduces a feature-wise contrastive learning method that maps node representations to the textual embedding space of the LLM."}, {"title": "Instance-wise contrastive learning with structural information", "content": "To alleviate the need for labeled data and enhance model generalization capability, we employ self-supervised learning for pre-training. To better extract structural information from the graph, we"}, {"title": "Feature-wise contrastive learning with token embeddings", "content": "Instance-wise contrastive learning relies heavily on individual instances, which can cause transfer issues when transitioning to other datasets. Moreover, there is a significant gap between the obtained node representations and the semantic space of LLMs. To address these issues, we propose feature-wise contrastive learning with token embeddings.\nFeature-wise contrastive loss breaks the independence between instances. For the feature matrix \\( U^* \\), we denote the columns in different views as \\( m_i \\in U_1 \\) and \\( n_i \\in U_2 \\). Here, \\( m_i, n_i \\in \\mathbb{R}^N \\). The loss is denoted as \\( \\mathcal{L}_{fea} \\), and is calculated as:\n\\begin{equation}\n\\mathcal{L}_{fea} = \\frac{1}{F_U} \\sum_{i=1}^{F_U} \\log \\frac{e^{\\theta(m_i, n_i) / \\tau}}{\\sum_{j=1}^{F_U} \\left[ e^{\\theta(m_i, m_j) / \\tau} + e^{\\theta(m_i, n_j) / \\tau} \\right]}\n\\end{equation}\nTo map node representations to the semantic space of LLMs, we use the principal components of the token embeddings of LLMs as coordinate axes. This approach ensures that the representations of similar instances are closely aligned in the textual embedding space. This helps alleviate the inconsistency in optimization objectives during graph self-supervised learning due to the gap between node representations and the text embedding space.\nSpecifically, we first use principal component analysis (PCA) to obtain the \\( P \\) principal components, denoted as \\( C \\in \\mathbb{R}^{P \\times F_L} \\), where \\( F_L \\) is the dimension size of token embeddings of LLM. Then, we map node representations by:\n\\begin{equation}\n\\tilde{U_*} = U^* \\times C^T.\n\\end{equation}\nTo map the node representations obtained from the GNN using principal components, we set the output dimension of the GNN to be equal to the token embeddings' dimension (i.e., \\( F_U = F_L \\)). The columns of the mapped feature matrix \\( U^* \\), denoted as \\( \\tilde{m_i} \\) and \\( \\tilde{n_i} \\), are fed into \\( \\mathcal{L}_{fea} \\). Therefore, the final contrastive loss for graph self-supervised learning is the average of Equation 4 and Equation 6:\n\\begin{equation}\n\\mathcal{L} = \\frac{1}{2} (\\mathcal{L}_{ins} + \\mathcal{L}_{fea}).\n\\end{equation}\nRemark: The introduction of feature-wise contrastive learning with token embeddings successfully addresses the semantic space discrepancy between graph node representations and LLM token embeddings. Our method enables the direct and simple use of graph structural and text information obtained by GNN in LLMs, thereby avoiding the significant generalization loss associated with complex modality alignment training during the fine-tuning process. Its role in fine-tuning will be further described in Sec. 2.3.2 and validated by experiments. Additionally, the feature-wise contrastive method itself exhibits stronger generalization, allowing it to perform well on unseen instances (or tasks) rather than relying on trained instances (or tasks)."}, {"title": "Alignment tuning", "content": "The development of LLMs has introduced a new paradigm for graph machine learning. However, existing research [18] indicates that LLMs alone cannot fully comprehend graph structures and their underlying information. To enable LLMs to more effectively capture information and improve their performance in cross-dataset and cross-task zero-shot learning, it is essential to design specific methods for LLMs to incorporate graph information suitably. To this end, we propose an alignment tuning method that includes specially designed instructions for various graph tasks at different levels, as well as a graph representation to graph token embeddings mechanism to integrate graph information."}, {"title": "Instructions design", "content": "The instruction we designed can be divided into two parts: one part provides graph information, and the other part describes the task. Here, we take a citation graph as an example, where nodes are papers, and relations are citations, to introduce the instruction.\nGraph information provision The graph information provision in the instructions for node, edge, and graph-level tasks is presented as follows: Given the representation of a paper/two papers/a paper"}, {"title": "Graph token embeddings", "content": "The token embeddings of graph mentioned previously, i.e., \\(\\langle graph \\rangle \\), are crucial for incorporating graph information and enabling the model's generalization. We use a projector to map central node representations into K graph token embeddings and replace \\(\\langle graph \\rangle \\) with these tokens. Kindly note that, we map the representations to fixed number of token embeddings regardless of the task type. For example, for node-level tasks, we map the central node representation to K token embeddings; for edge-level tasks, we sum the representations of the two nodes of the target edge and then map this sum to K token embeddings; for graph-level tasks, similar approach can be applied. In this way, we unify the instruction of graph tasks at different levels. Thanks to the text-aligned contrastive learning, a linear projector is enough to capture the map relationship without tuning LLM:\n\\begin{equation}\nH_{token} = f_{Linear}(u_i)\n\\end{equation}\nwhere \\(u_i \\in U\\), \\(H_{token} \\in \\mathbb{R}^{K \\times F_L}\\), \\(F_L\\) is the dimension size of token embedding of LLM, and \\(f_{Linear}(\\cdot)\\) is a linear layer.\nRemark: This approach offers three primary advantages: (i) When handling tasks at different levels, the changes to the instructions are minimal. This consistency facilitates the transfer of knowledge learned during training to unseen tasks in large language models (LLMs); (ii) The fixed number of token embeddings can be seen as a conditional soft prompt. Unlike traditional soft prompts, learning at the instance level reduces the risk of overfitting to specific datasets or tasks, thereby enhancing generalization to unseen datasets and tasks; (iii) Different from current work which intends to include the representations of all nodes in the subgraph, we only map the representations of the central node to tokens, since there has enough information carried by message passing of GNN. This method is more efficient, and it offers greater generalizability and practicality."}, {"title": "Training and evaluation strategy", "content": "To ensure compatibility and facilitate comparisons across various datasets, we map the node features into a consistent vector space. Specifically, we employ a pretrained BERT model [8] to encode the raw text associated with each node, thereby generating the node features. We then pretrain the graph model using contrastive learning with the loss function defined in Equation 8 on a single dataset. After pretraining, the model parameters are fixed. We utilize the pretrained model to obtain node representations and follow the instructions in Section 2.3.1 to train the linear projector on specific tasks within the same dataset. Finally, we evaluate the performance of our model on unseen datasets and tasks. Throughout all phases, the parameters of the language model remain fixed. We use GraphSAGE [11] as our graph encoder and Vicuna-7B-v1.5 [7] as the foundational language model."}, {"title": "Experimental results", "content": "In this section, comprehensive experiments are conducted to validate the effectiveness of TEA-GLM. These experiments aim to investigate the following research questions:\nRQ1: How effective is TEA-GLM in handling the cross-dataset zero-shot learning problem?\nRQ2: How well does TEA-GLM transfer knowledge when adapted to an unseen task and dataset in a zero-shot setting?\nRQ3: What is the contribution of the feature-wise contrastive learning and graph token embeddings to the zero-shot learning ability of TEA-GLM?"}, {"title": "Experimental setup", "content": "Datasets We test TEA-GLM across eight widely used datasets spanning two distinct domains. Within the citation domain, we employ Arxiv [17], Pubmed [13], and an expanded version of Cora [35] with an increased range of classes and larger scale. In these datasets, each node represents an individual paper, with edges indicating citation relationships. In the e-commerce domain, we utilize datasets from the TAG benchmark [41], including Children (Book-Children), History (Book-History), Computer (Ele-Computer), Photo (Ele-Photo), and Sports (Sports-Fitness). Here, nodes represent distinct products, while edges denote co-viewing or co-purchasing between two products. Appendix A presents the statistics for these datasets.\nBaselines We conduct a comprehensive comparison of TEA-GLM with various categories of base-line methods: (i) Non-graph neural network approaches, such as MLP, which employs a Multilayer Perceptron for node representation; (ii) Supervised methods, including GCN [20], GraphSAGE [11], and GAT [31]; (iii) Self-supervised methods like DGI [32], which maximizes mutual information to learn node representations without relying on ground truth labels; (iv) Graph knowledge distillation frameworks: GKD [42], which distills knowledge from a teacher GNN trained on a complete graph to a student GNN operating on a smaller or sparser graph; GLNN [51], a method combining the advantages of graph neural networks and MLPs using knowledge distillation, aimed at reducing dependency on the inference graph; (v) Graph transformer networks, including NodeFormer [36] and DIFFormer [37]; (vi) Large language models, such as Vicuna-7B-v1.5; (vii) The latest models equipped with transfer and zero-shot capabilities, such as OFA [24], GraphGPT [30], and LLaGA [2].\nImplementation details For datasets within the citation domain, we follow the data split methodol-ogy outlined in GraphGPT [30]. For those within the e-commerce domain, we utilize scripts provided by the TAG benchmark [41] to generate data splits. To ensure comparability among different methods, identical data splits are applied to all models. To assess the performance of TEA-GLM, we employ three commonly adopted evaluation metrics: Accuracy and Macro F1 for node classification, and AUC (Area Under the Curve) for link prediction. To ensure result robustness, we conduct five experiments with random seed values ranging from 0 to 4 and report the mean and standard deviation of the results. Due to the limited number of pages, several experimental results, such as Macro F1 results of node classification (Appendix B.2), legality rate of valid answers produced by the LLM (Appendix B.1), and parameter sensitivity analysis (Appendix C), are reported in Appendix.\nIn the pre-training phase of the GNN, we set the GNN layers to 2. We use a batch size of 512 for 60 epochs and a learning rate of \\( 2 \\times 10^{-2} \\). During the training of the linear projector, we configure a batch size of 2 per GPU for one epoch, with a learning rate of \\( 1 \\times 10^{-3} \\). The Adam optimizer is employed for all approaches. For baseline models, we adjust hyperparameters and utilize the optimal settings. All experiments are conducted on 2 NVIDIA A100 GPUs with 80GB memory each, using CUDA version 11.7."}, {"title": "Cross-dataset zero-shot ability (RQ1)", "content": "We train all methods on the Arxiv and Computer, respectively, followed by an evaluation of their zero-shot performance on datasets from the same domain. Zero-shot learning presents challenges for GNN-based models, particularly regarding variations in the number of classes across different datasets. To address this, we adopt the setting outlined in GraphGPT [30]. For each target dataset, we utilize the GNN backbone trained on the source dataset along with a classifier trained with target data, typically a linear layer. Due to the considerable time cost associated with training and evaluating"}, {"title": "Cross-task zero-shot ability (RQ2)", "content": "We employ models trained on node classification tasks directly for link prediction tasks without any fine-tuning. We omit the comparison with models utilizing GNN as a predictor, as conducting cross-task evaluation of these models without fine-tuning poses a significant challenge, given that different tasks typically correspond to different task heads. Here, we contrast TEA-GLM with OFA, which similarly enables cross-task testing without the need for fine-tuning. Additionally, we compare TEA-GLM with Vicuna-7B and methods that utilize LLM as a predictor, such as GraphGPT and LLaGA. For GraphGPT, we utilize the checkpoint released by the author trained on Arxiv and report the results on citation datasets."}, {"title": "Ablation study (RQ3)", "content": "We conduct an ablation study to discuss two key components of our model: feature-wise contrastive learning and graph token embeddings. Here, we directly remove these two components from our model and then test the model's performance on cross-dataset and cross-task evaluations. The results are shown in Figure 2. \"w/o FC\" means that we pretrain the GNN without feature-wise contrastive learning, while \"w/o GT\u201d means predicting without graph token embeddings.\nWithout graph token embeddings, large language models lack crucial information from the graph, leading to a significant decline in performance on both node-level and edge-level tasks. GNNs pre-trained with feature-wise contrastive learning can obtain node representations aligned with the text space, enabling cross-dataset and cross-task generalization through a simple linear layer. When the feature-wise constraint for pre-training is absent, the model's performance on the seen datasets (Arxiv and Computer) for the training task improves slightly. However, its performance on unseen datasets declines. Although it remains relatively stable when handling tasks of the same category, its performance decreases notably when dealing with unseen tasks (link prediction). These results indicate that alignment between graph representation and LLM's token embeddings via feature-wise contrastive learning is important for cross-task zero-shot transfer."}, {"title": "Related work", "content": "In the field of graph machine learning, Graph Neural Networks (GNNs) have garnered significant attention [5, 22, 28, 40, 9, 6, 46, 1]. The primary strategy of most GNNs is to capture under-lying message-passing patterns for graph representation. Several effective neural network archi-tectures have been proposed, such as Graph Attention Network (GAT) [31], Graph Convolution Network (GCN) [20], and GraphSAGE [11]. Recently, there has been a surge of interest in exploring transformer-based encoders for graph machine learning [49, 45, 36, 37]. However, a notable limita-tion of GNNs is their generalization capability. Typically, GNNs are trained on specific tasks within"}, {"title": "Self-supervised learning and prompt-tuning for GNNS", "content": "To alleviate the demand for labeled data and enhance the robustness of graph models, self-supervised learning is commonly employed in GNN training [38, 52, 12]. Methods like Deep Graph Info-max (DGI) [32] utilize mutual information maximization for pre-training. Other approaches, such as GraphCL [44], GCA [53], GCC [26], and JOAO [47], learn node representations by contrasting positive and negative samples. GraphMAE [15, 16], on the other hand, learns representations by generating samples that resemble the original graph structure. However, these methods typically require fine-tuning the task-specific heads for downstream applications.\nVarious methods have explored the use of prompt techniques to enhance the generalization of GNNs. To address the inconsistency between pre-training and downstream task objectives, GraphPrompt [25] proposes a unified task template applicable to both stages. Additionally, ProG [29] reformulates various task types into a unified graph-level representation and employs meta-learning techniques to enhance multi-task learning capabilities. However, whether through self-supervised learning or graph prompt methods, fine-tuning is often necessary when handling new datasets. Moreover, when confronted with datasets containing varying numbers of categories, retraining of task heads is required to achieve optimal performance."}, {"title": "Large language models for graphs", "content": "With the rapid advancement of Large Language Models (LLMs) and their remarkable generalization capabilities, leveraging LLMs to address transferability issues in graph machine learning has garnered significant attention [10, 14]. Some methods represent graph structure information as text input to LLMs [3, 33, 23]; however, this approach often leads to suboptimal solutions [18]. Another paradigm involves using LLMs as enhancers [43, 48, 39, 4, 24], where they generate data or node text representations. Despite this, since GNNs are ultimately used for prediction, this approach significantly limits the model's transferability. Recently, considerable efforts have been made to utilize LLMs as predictors. For instance, GraphGPT [30] attempts to align LLMs with pre-trained Graph Transformer encoders through two-stage fine-tuning. However, the fine-tuning, conducted on specific datasets, might weaken the method's transferability. In light of this, LLaGA [2] introduced a novel encoding method that directly translates graph data into sequences compatible with LLMs. However, this approach may compromise performance due to the lack of GNN filtering and aggregation of graph information. Inspired by these challenges, we propose a pre-training strategy that enhances GNN transferability by aligning its representations with the token embeddings of LLMs, resulting in improved performance in zero-shot tasks. Notably, similar to our method, TEST [27] aligns time series representations with several selected LLM token embeddings. However, our approach differs in that we project graph representations into a feature space defined by the principal components of LLM token embeddings. This enables the LLM to function as a zero-shot learner for graph machine learning tasks, rather than just enhancing performance on specific, seen tasks."}, {"title": "Limitations", "content": "While our TEA-GLM framework demonstrates considerable promise in enhancing zero-shot learning for graph-based tasks, it does have some limitations. Although the framework we designed can be easily applied to graph-level tasks, we have not yet explored the model's performance through specific experiments. This will be addressed in our future work."}, {"title": "Conclusion", "content": "This paper introduces TEA-GLM, a framework that enhances zero-shot learning in graph machine learning by aligning GNN representations with LLM token embeddings. TEA-GLM uses a linear projector to map graph representations into graph token embeddings and incorporates a unified instruction design to handle various graph tasks at different levels. This approach enables consistent performance across various datasets and tasks without task-specific fine-tuning. Extensive experi-ments show that TEA-GLM outperforms state-of-the-art methods in accuracy and generalization, demonstrating its effectiveness and efficiency in zero-shot learning for graph tasks."}, {"title": "Dataset description", "content": "Citation datasets The Arxiv dataset [17] represents a directed citation network among Computer Science (CS) papers from the arXiv preprint server. Each node in this graph corresponds to a paper, while edges represent citation links. The PubMed dataset [13] comprises 19,717 scientific publications from the PubMed database related to diabetes, which are categorized into three distinct classes: Experimentally induced diabetes, Type 1 diabetes, and Type 2 diabetes. This classification reflects the focus of each publication within the broader context of diabetes research. Lastly, the Cora dataset [35], formally known as the \u201cCora Research Paper Classification Dataset\u201d, provides a comprehensive network for analyzing research paper classifications in machine learning. It is an extended version of the dataset commonly referred to in other studies [21], featuring detailed categorizations.\nE-commmerce datasets All e-commerce datasets are provided in the TAG benchmark [41]. The Books-Children and Books-History datasets are extracted from the Amazon-Books dataset. Books-Children includes items with the second-level label \u201cChildren\", while Books-History includes items with the second-level label \u201cHistory\u201d. Each dataset's label corresponds to the three-level label of the book. The Ele-Computers dataset comprises items with the second-level label \u201cComputers\u201d, and Ele-Photo includes items with the second-level label \"Photo\". Each of these datasets is labeled at the third level for electronic products. The Sports-Fitness dataset, sourced from the Amazon-Sports dataset, contains items with the second-level label \"Fitness\". Nodes in this dataset represent fitness-related items, and an edge between two items indicates they are frequently co-purchased or co-viewed.\""}, {"title": "Legality rate", "content": "After training on specific datasets or tasks, large language models (LLMs) may produce invalid or incorrect answers to given questions. For instance, when handling unseen datasets or tasks, LLMs may generate responses that fall outside the set of acceptable answer candidates. To evaluate the impact of the training process on LLM performance, we follow the approach in [50] and use the legality rate to measure the proportion of valid answers produced by the model.\nTable 4 demonstrates that the illegality rate of the LLaGA model significantly increases when exposed to datasets it has not previously encountered, suggesting a substantial impact of training methodologies on both the acquisition of knowledge and the model's ability to generalize. Conversely, our model exhibits a notably stable performance across diverse unseen datasets, achieving higher legality rates in several cases."}, {"title": "F1 score on node classification task", "content": "Due to the absence of a metric to calculate the F1 score while considering the illegality rate, we adopt the methodology used in [50]. For the LLM-backbone models, we only calculate the Macro F1 score for legally permissible responses provided by the model. This calculation method may not accurately reflect the model's performance fully. Therefore, we also report the illegality rate in Table 4. Please note that the accuracy metric is unaffected by illegal responses, which are considered error responses."}, {"title": "Supervised results", "content": "We report the supervised learning results in Table 6. The GNN-backbone models continue to demonstrate robust performance in fitting training data. Similarly, the LLaGA model shows its efficacy in supervised learning scenarios. However, despite their strong performance on training datasets, these models exhibit limited generalization capabilities on unseen datasets as shown in Table 1 and Table 5."}, {"title": "Parameter sensitivity analysis", "content": "Number of graph token embeddings To discuss the impact of the number of graph token embed-dings, we set \\(K \\in \\{1,3,5,10\\}\\) and report the results on node classification task in Figure 3. In the context of training datasets and unseen datasets, we observe two distinct patterns. With an increase in the number of graph token embeddings in the training dataset, there is a slight improvement in the model's performance on that dataset. This suggests that in a supervised learning scenario, enhancing the model's performance can be achieved by increasing the quantity of graph token embeddings. Con-versely, for unseen datasets, our model requires only a minimal number of graph token embeddings to achieve satisfactory performance, indicating that the number of learnable parameters in our model is significantly less than concurrent works.\nNumber of principal components We define \\(P\\in \\{0, 100, 1000, 2000, 3000\\}\\) and discuss the results of the node classification task in Figure 4. In supervised learning scenarios, omitting contrastive learning with principal components can lead to a slight increase in accuracy. However, this often makes the model more prone to overfitting on training datasets. When the number of principal components is too small, it adversely affects the model's learning capability. Remarkably, when \\(P = 1000\\), the model demonstrates satisfactory performance. At this level, the principal components capture 50% of the variance of LLM's token embeddings."}, {"title": "Complete instructions", "content": "In node classification tasks, we provide candidate labels to facilitate the model's learning process, focusing on discovering the correct answers rather than merely memorizing them. For link prediction, we structure the instructions in a format similar to that of node classification. This approach is designed to enhance the model's ability to transfer learned knowledge effectively across different tasks."}]}