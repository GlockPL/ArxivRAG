{"title": "Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining", "authors": ["Jie Cheng", "Ruixi Qiao", "Gang Xiong", "Qinghai Miao", "Yingwei Ma", "Binhua Li", "Yongbin Li", "Yisheng Lv"], "abstract": "A significant aspiration of offline reinforcement learning (RL) is to develop a generalist agent with high capabilities from large and heterogeneous datasets. However, prior approaches that scale offline RL either rely heavily on expert trajectories or struggle to generalize to diverse unseen tasks. Inspired by the excellent generalization of world model in conditional video generation, we explore the potential of image observation-based world model for scaling offline RL and enhancing generalization on novel tasks. In this paper, we introduce JOWA: Jointly-Optimized World-Action model, an offline model-based RL agent pretrained on multiple Atari games to learn general-purpose representation and decision-making ability. Our method jointly optimizes a world-action model through shared transformer backbone, which stabilize temporal difference learning with large models during pretraining. Moreover, we propose an provably efficient and parallelizable planning algorithm to compensate for the Q-value estimation error and thus search out better policies. Experimental results indicate that our largest agent, with 150 million parameters, achieves 78.9% human-level performance on pretrained games using only 10% subsampled offline data, outperforming existing state-of-the-art large-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales favorably with model capacity and can sample-efficiently transfer to novel games using only 5k offline fine-tuning data corresponding to about 4 trajectories per game, which demonstrates superior generalization of JOWA.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, building large-scale generalist models capable of solving multiple tasks has become a dominant research focus in natural language processing (NLP) and multi-modality. Notable examples include large language models (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; Team et al., 2023) and large vision-language models (Zhu et al., 2023; Liu et al., 2024; Bai et al., 2023), which deliver outstanding performance across a wide range of tasks and adapt quickly to new ones through few-shot or in-context learning. Their success is largely driven by the scaling law (Kaplan et al., 2020), which posits that increasing model size and data leads to improved performance. However, similar scaling trends have not been extensively observed in reinforcement learning (RL).\nUnlike in vision and language domains, RL has traditionally favored smaller models tailored to single tasks or multiple tasks within the same environment. Concerningly, previous studies have shown that scaling model capacity can lead to instabilities or performance degradation (Kumar et al., 2020a; Ota et al., 2021; Sokar et al., 2023), explaining the continued dominance of shallow CNN networks in vision-based RL tasks (Mnih, 2013). While some efforts have been made to scale offline RL across multiple tasks (Lee et al., 2022; Reed et al., 2022; Xu et al., 2022; Wu et al., 2024), they predominantly rely on supervised learning (SL) approaches, such as conditional behavior cloning, rather than temporal difference (TD) learning, and heavily rely on large amounts of expert trajectories. Kumar et al. (2023) scaled offline Q-learning using ResNet-based representation network with"}, {"title": "2 RELATED WORK", "content": "Offline Reinforcement Learning. Offline RL algorithms learn a policy entirely from the static offline dataset without online interactions. Model-free offline RL incorporates conservatism to mitigate extrapolation error (Jin et al., 2021) primarily through policy constraints (Fujimoto et al., 2019; Kumar et al., 2019; Peng et al., 2019; Nair et al., 2020; Kostrikov et al., 2021; Fujimoto & Gu, 2021) and value regularization (Kumar et al., 2020b). In contrast, model-based offline RL approximates the environment using learned world models and performs conservative policy optimization (Lu et al., 2021; Yu et al., 2020b; 2021; Schrittwieser et al., 2021). While these works focus on single-task settings, our work explores scaling offline model-based RL across diverse, challenging multi-task Atari games (Lee et al., 2022; Kumar et al., 2023; Wu et al., 2024) aiming for sample-efficient transfer to novel games. We employ CQL (Kumar et al., 2020b) to estimate conservative Q-values, leveraging its simplicity and effectiveness on vision-based offline RL tasks.\nMutli-Task Reinforcement Learning. Multi-task reinforcement learning (MTRL) aims to learn a shared policy for diverse tasks, with various approaches proposed in the literature (Teh et al., 2017;"}, {"title": "3 PRELIMINARIES AND PROBLEM SETUP", "content": ""}, {"title": "3.1 ONLINE DISTRIBUTIONAL RL (C51)", "content": "In distributional RL, the distribution Z over returns replaces the Q-value in the Bellman optimality equation. The Q-value is the mean of the value distribution Z that can be computed through a distributional Bellman optimality operator (Bellemare et al., 2017),\n$\\mathcal{T}^*Z(s,a) := R(s, a) + \\gamma Z(s', \\underset{a'}{\\text{arg max}} Q(s', a'))$\nwhere the formula $Y := U$ denotes equality of probability laws, that is the random variable Y is distributed according to the same law as U. The C51 algorithm (Bellemare et al., 2017) models $Z(s, a)$ using a discrete distribution supported on N fixed atoms $z_1 \\leq \\dots \\leq z_N$ uniformly spaced over a predetermined interval. Given a current value distribution, C51 applies a projection step to map the target distribution onto its finite element support and optimizes as follows:\n$\\mathcal{L}_{TD} = D_{KL}(\\mathcal{T}^* Z_{\\theta^-}(s, a) || Z_{\\theta}(s, a))$"}, {"title": "3.2 VALUE REGULARIZATION BASED OFFLINE RL (CQL)", "content": "To be conservatism on unseen actions, CQL (Kumar et al., 2020b) introduces a regularizer to the TD-loss, which minimizes Q-values for unseen actions while maximizing Q-values for actions in the dataset to counteract excessive underestimation. The loss function for CQL is given by:\n$\\mathcal{L}_{CQL} = \\alpha \\mathbb{E}_{s \\sim \\mathcal{D}}\\left[\\log \\sum_{a'}\\exp\\left(Q_{\\theta}(s, a')\\right)\\right] - \\mathbb{E}_{s, a \\sim \\mathcal{D}} \\left[Q_{\\theta}(s, a)\\right] + \\mathcal{L}_{TD}$"}, {"title": "3.3 PROBLEM SETUP", "content": "We consider a multi-task offline RL problem: given a static dataset of transitions $\\mathcal{D} = \\{(s_t, a_t, r_t, d_t, s_{t+1})_i\\}$ collected from various environments with arbitrary behaviour polices, our goal is to learn a single policy that maximize the expected return $R_t = \\sum_{k=t}^\\infty \\gamma^{k-t}r_k$ on all considered environments and can be efficiently fine-tuned to new tasks, where $\\gamma$ is the discount factor.\nConsidering the computational demands and resource limitations, we use a subset of 15 games from Atari 2600 for training. The whole training process took approximately 12 days on A100 GPUs. Our offline dataset is derived from the the DQN-Replay dataset (Agarwal et al., 2020), which consists of 84 \u00d7 84 grayscale images as observations and a full action space of 18 discrete actions."}, {"title": "4 JOINTLY-OPTIMIZED WORLD-ACTION MODEL", "content": "In this section, we first detail the architecture of JOWA and the loss functions for joint optimization in section 4.1. Next, we introduce the provably efficient and parallelizable planning algorithm employed to compensate for the Q-value estimation error in section 4.2. Finally, we present the overall training and sample-efficient fine-tuning pipelines in section 4.3."}, {"title": "4.1 WORLD-ACTION MODEL", "content": ""}, {"title": "4.1.1 ARCHITECTURE", "content": "Figure 1 illustrates JOWA's architecture, which uses a transformer backbone (Vaswani, 2017) to simultaneously learn world dynamics and Q-values across environments. This dual capability is achieved through distinct prediction heads that process the transformer's output embedding e. The world dynamics are modeled via supervised learning using three heads: next observation token predictor $p_o$, reward predictor $p_r$, and termination predictor $p_d$. The Q-values head $h_Q$ learns the"}, {"title": "4.1.2 TRAINING OF WORLD-PART MODULE", "content": "At each timestep t, the world-part module models the following distributions:\nDynamics predictor: $z_{t+1}^k \\sim p_o(z_{t+1}^k | f(z_{\\leq t-1}, z_t^k, a_{\\leq t-1}, u))$\nReward predictor: $r_t \\sim p_r(r_t | f(z_{\\leq t}, a_{\\leq t}, u))$\nTermination predictor: $d_t \\sim p_d(d_t | f(z_{\\leq t}, a_{\\leq t}, u))$\nwhere f represents the transformer backbone, u is the task ID to index the corresponding task embedding, $k \\in \\{1, \\dots, K\\}$, and $t \\in \\{1, \\dots, L\\}$.\nTo unify the type of loss functions for balanced training (Vandenhende et al., 2021), we convert scalar rewards to ternary quantities $\\{-1, 0, 1\\}$ using the sign function. This allows all three predictors to be optimized as classification problems using cross-entropy loss. Given L-timesteps segments sampled from the offline dataset, the loss function for the world-part module is formulated as:\n$\\mathcal{L}_{world}(0) = \\frac{1}{L} \\sum_{t=1}^{L} [\\sum_{k=1}^{K} - \\ln p_o(z_{t+1}^k | f(z_{\\leq t-1}, z_t^k, a_{\\leq t-1}, u)) - \\ln p_r(r_t | f(z_{\\leq t}, a_{\\leq t}, u)) - \\ln p_d(d_t | f(z_{\\leq t}, a_{\\leq t}, u))]$"}, {"title": "4.1.3 TRAINING OF ACTION-PART MODULE", "content": "We use CQL (Kumar et al., 2020b) for offline TD-learning. To ensure training stability and enhance scaling performance (Farebrother et al., 2024), we employ distributional TD-error (Bellemare et al., 2017) instead of the mean-square TD-error, maintaining consistency with the $\\mathcal{L}_{world}$ loss type.\nThe action-part module computes the return distribution for all actions given an observation and historical information. For an observation-action pair $(o_t, a_t)$, the return distribution $Z$ is formulated as: $Z(o_t, a_t) = h_Q(f(z_{\\leq t}, a_{\\leq t}, u))[a_t]$. The value function $Q(o_t, a_t)$ is the mean of $Z(o_t, a_t)$. Then the loss function for the action-part module is formulated as:\n$\\mathcal{L}_{action}(\\phi) = \\alpha \\left[\\log \\left(\\sum_a \\exp \\left(Q(o_t, a)\\right)\\right) - Q(o_t, a_t)\\right] + D_{KL} \\left(\\mathcal{T}^* \\overline{Z}(o_t, a_t) || Z(o_t, a_t)\\right)$\nwhere $\\mathcal{T}^*$ is the distributional Bellman optimality operator defined in Equation (1), and $\\overline{Z}$ is the target distribution computed through a target Q-values head $h_{Q^-}$.\nTherefore, the joint optimization objective for the world-action model is formulated as:\n$\\mathcal{L}(\\theta, \\phi) = \\beta \\mathcal{L}_{world}(\\theta) + \\mathcal{L}_{action}(\\phi)$\nwith the coefficient $\\beta > 0$. We set $\\beta = 0.1$ in our experiments.\nBoth $\\mathcal{L}_{world}$ and $\\mathcal{L}_{action}$ back-propagate gradients to the transformer. Previous works observed that TD methods suffer from greater instability with larger model size (Kumar et al., 2020a; Sokar et al., 2023). However, through jointly optimizing the world-action model, $\\mathcal{L}_{world}$ serves as a regularizer to stabilize TD-learning in large-scale models. Moreover, the world-part module enables planning at decision time for optimal inference and sample-efficient transfer, detailed in the following sections."}, {"title": "4.2 PARALLELIZABLE PLANNING AT INFERENCE TIME", "content": "The world-part module enables planning at decision time to compensate for inaccurate Q-value estimates, allowing JOWA to consistently search out the optimal policy. We model this process as a tree search problem and present a practical, parallelizable search algorithm.\nGiven an estimated optimal Q-value $\\hat{Q}^*$, a learned world model with dynamic predictor $\\hat{P}$ and reward predictor $\\hat{r}$, our objective is to find the optimal action $a_0$ maximizing the ground-truth optimal Q-value $Q^*$, starting from initial state $s_0$. To do so, we rewrite the Bellman optimality equation as:\n$Q^*(s_0, a_0) = \\underset{\\pi_{Q^*}}{\\text{max}} \\mathbb{E} \\left[\\sum_{t=0}^{H-1} \\gamma^t r(s_t, a_t) + \\gamma^H \\underset{a_H}{\\text{max}} Q^*(s_H, a_H) \\right]$\nwhere $\\pi_{Q^*}$ is the policy induced by the optimal Q-function. The proof is provided in the Appendix A. For Equation (11), the optimal policy is the greedy policy based on $Q^*$.\nTo derive the search objective function, we follow three steps: (i) replace the ground-truth functions in the right side of Equation (11) with estimated or learned functions. (ii) leverage the estimated optimal Q-function $\\hat{Q}^*$ to reduce the policy space for search, restricting the actions to those with top-K highest Q-values. Denote the constrained policy space as $\\Pi_{\\hat{Q}^*}$, where $\\forall \\pi \\in \\Pi_{\\hat{Q}^*}, \\forall s \\in \\mathcal{S}, \\forall a \\notin top-K(\\hat{Q}^*(s, \\cdot))$, we have $\\pi(a|s) = 0$. (iii) maximize over $a_0$ on both sides of Equation (11) to find the optimal initial action, considering the restriction in the second step. Finally, the resulting objective function is formulated as:\n$\\underset{\\pi \\in \\Pi_{\\hat{Q}^*}} {\\text{max}} \\mathbb{E} \\left[ \\sum_{t=0}^{H-1} \\hat{r}(s_t, a_t) + \\gamma^H \\underset{a_H}{\\text{max}} \\hat{Q}^*(s_H, a_H) \\right]$\nDetailed derivation is provided in the Appendix B. Then we show the error bound of search-based optimal Q-function in Theorem 4.1, with the formal theorem and its proof in Appendix C."}, {"title": "4.3 TRAINING AND FINE-TUNING PIPELINES", "content": "Our multi-task offline RL pretraining consists of two stages:\n\u2022 Stage 1: Sample trajectory segments from datasets. Train the VQ-VAE tokenizer using image observations. Train the world-part module using segments with loss (8) for M\u2081 steps.\n\u2022 Stage 2: Freeze the VQ-VAE tokenizer. Sample segments and jointly optimize the world-action model with loss function (10) for M2 steps.\nWe employ this two-stage training approach to stabilize and accelerate the overall training process. In our pretraining experiments, we set M\u2081 = 250k and M2 = 1.5M, totaling 1.75M gradient steps.\nFor sample-efficient transfer to unseen games, we adopt a similar two-stage offline fine-tuning pipeline, but unfreeze all components of JOWA in both two stages. In stage 1, we fine-tune JOWA using real data for 3k steps. In stage 2, we enable planning to synthesize high-quality data, using 3/4 batch of real and 1/4 batch of synthetic data with COMBO loss for 7k steps. Detailed fine-tuning protocol is shown in Appendix E.2."}, {"title": "5 EXPERIMENTS", "content": "We design our experiments to answer the following questions: (1) How does JOWA perform on multi-games in low-data regime? (2) Can JOWA effectively leverage higher model capacity? (3) Does the pre-trained JOWA sample-efficiently transfer to new games?"}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Dataset. We use the Atari trajectory dataset from Agarwal et al. (2020), which contains 50M transitions from each of 5 separate training runs. Due to the prohibitive long training time on full games, we select a subset of 20 games, maintaining the difficulty distribution of full games defined by Gulcehre et al. (2020). These 20 games are introduced in Appendix D. 15 of those games are used for training, and 5 games are held out for OOD generalization experiments. Following Lee et al. (2022); Kumar et al. (2023); Wu et al. (2024), we use data from 2 out of 5 training runs. To investigate performance in low-data regime, we uniformly draw 10% of transitions at random, as per Agarwal et al. (2020); Kumar et al. (2020a), resulting in 10M transitions per game.\nTraining and Fine-tuning. We implement our world-action model based on GPT-2 (Brown et al., 2020). We train a total of three variants of JOWA: JOWA-150M (150M parameters), JOWA-70M, and JOWA-40M. We set the number of visual tokens K to 36 and sequence length L to 8 game frames, resulting in sequences of 296 tokens. We pretrain all JOWA models on A100 GPUs for 1.75M steps using the AdamW optimizer (Loshchilov & Hutter, 2018) with learning rates of 1 \u00b7 10\u22124 and 5 \u00b7 10\u22125 for two stages respectively, and a batch size of 512 for both stages. For fine-tuning on novel games, we train for 10k gradient steps with a batch size of 32 and 5k transitions. We compare JOWA with the following baselines: (i) Multi-Task BC (MTBC) (ii) Multi-Game DT (MGDT) (Lee et al., 2022) (iii) Elastic DT (EDT) Wu et al. (2024) (iv) Scaled-QL (SQL) Kumar et al. (2023). For fair comparison, all methods use the same batch size of 512 and 1.75M gradient steps. The implementation details of all baselines are provided in the Appendix E.1.\nEvaluation and Metrics. During evaluation, we set the inverse temperature K to 10 for EDT and MGDT for expert action inference. We enable planning for JOWA, setting the planning horizon H to 2 for all games and adjust the beam width K base on the size of valid action space of each game. See Appendix E.3 for the detailed evaluation protocol. We measure performance using human normalized scores (HNS) (Mnih et al., 2015), i.e. (scorerandom)/(scorehuman \u2013 scorerandom). To create an aggregate comparison metric across all games, we use inter-quartile mean (IQM) of human-normalized scores, following evaluation best practices proposed in (Agarwal et al., 2021).\nMore details on hyperparameters, network architecture, algorithm implementation, and protocols for fine-tuning, and evaluation are provided in the Appendix E."}, {"title": "5.2 HOW DOES JOWA PERFORM ON MULTI-GAMES IN LOW-DATA REGIME?", "content": "We summarize our main results in Table 2. This table shows the performance of JOWA alongside all best performing sizes of baselines trained with 10% subsampled dataset. MTBC, MGDT, and EDT"}, {"title": "5.3 HOW DOES JOWA SCALES WITH MODEL SIZE?", "content": "Scaling law depicts the positive correlation be-tween model capacity and performance. Follow-ing Lee et al. (2022); Kumar et al. (2023), weinvestigate JOWA's ability to leverage higher ca-pacity architectures. For comparison of scalingtrends, we establish 2 baselines: (i) MTBC scaledto 34M, 65M and 120M parameters. (ii) MGDTwith 40M and 200M parameters. We scale JOWAby increasing the size of the transformer back-bone and Q-values heads, resulting in 3 variantswith 40M, 70M, and 150M parameters."}, {"title": "5.4 CAN JOWA SAMPLE-EFFICIENTLY TRANSFER TO NEW GAMES?", "content": "Rapid adaptation to downstream tasks is a natural and well-motivated benefit of pretraining. In this section, we study how the pretrained world-action model enables rapid and sample-efficient fine-"}, {"title": "5.5 ABLATION STUDY", "content": "In this section, we conduct a series of controlled ablation studies to evaluate the impact of key design choices in JOWA, including planning, different training losses, and architecture designs. These experiments aim to provide empirical evidence to support our design decisions and offer valuable insights for future research in this domain."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce JOWA: Jointly-Optimized World-Action model, a single offline model-based RL agent capable of playing multiple Atari games. JOWA uses a shared transformer backbone for both the world modeling and the Q-value criticism, enabling joint optimization. We propose a parallelizable planning algorithm to consistently identify the optimal policy during inference. As we hoped, by increasing model parameters, JOWA unlocks scaling trends in performance and exceed prior large-scale offline RL methods in multi-games regime. Furthermore, by training a large-capacity model on a diverse set of games, we show that JOWA can sample-efficiently adapt to novel games, leveraging its generalizable world model for planning. Our ablation studies validate the efficacy of joint optimization and our planning method, while also demonstrating that scaling offline model-based RL is more efficient than offline model-free RL. To facilitate future research, we will release all training and evaluation codes, along with pretrained model checkpoints.\nLimitations. Due to the computation resources required, we could not experiment on full Atari 2600 games with complete datasets, as this would take approximately two months of training time. Additionally, while using synthetic data to train agents is common in single-task online model-based RL algorithms, we observe it yields negative gains in multi-game RL pretraining. Therefore, how to effectively use synthetic data for multi-task pretraining is an interesting direction for future work."}, {"title": "A PROOF OF EQUATION (11)", "content": "Given the initial state-action pair (s0, a0), the bellman expectation equation (Sutton, 2018) is written as:\n$Q^{\\pi}(s_0, a_0) = \\underset{s_1 \\sim P(\\cdot|s_0, a_0)}{\\mathbb{E}} [r(s_0, a_0) + \\gamma V^{\\pi}(s_1)]$\n$ = \\underset{s_1,...,s_H \\sim P}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-1} \\gamma^t r(s_t, a_t) + \\gamma^H V^{\\pi}(s_H) \\Big]$\nAccording to the definition of optimal Q-value: $Q^*(s_0, a_0) = \\underset{\\pi}{\\text{max}} Q^{\\pi}(s_0, a_0)$, we have:\n$Q^*(s_0, a_0) = \\underset{\\pi}{\\text{max}} \\underset{s_1,...,s_H \\sim P}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-1} \\gamma^t r(s_t, a_t) + \\gamma^H V^{\\pi}(s_H) \\Big]$\n$ = \\underset{\\pi}{\\text{max}} \\underset{s_1,...,s_H \\sim P \\atop a_1,...,a_{H-1} \\sim \\pi}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-1} \\gamma^t r(s_t, a_t) + \\gamma^H \\underset{\\pi}{\\text{max}} V^{\\pi}(s_H) \\Big]$\n$ = \\underset{\\pi}{\\text{max}} \\underset{s_1,...,s_H \\sim P \\atop a_1,...,a_{H-1} \\sim \\pi}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-1} \\gamma^t r(s_t, a_t) + \\gamma^H V^*(s_H) \\Big]$\n$ = \\underset{\\pi}{\\text{max}} \\underset{s_1,...,s_H \\sim P \\atop a_1,...,a_{H-1} \\sim \\pi}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-1} \\gamma^t r(s_t, a_t) + \\gamma^H  \\underset{a_H}{\\text{max}} Q^*(s_H, a_H) \\Big]$\nFor Equation (16), we derive the optimal policy is the greedy policy selecting actions with the greatest Q*-value:\n$Q^*(s_0, a_0) = \\underset{\\pi}{\\text{max}} \\underset{s_1,...,s_H \\sim P \\atop a_1,...,a_{H-1} \\sim \\pi}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-1} \\gamma^t r(s_t, a_t) + \\gamma^H  \\underset{a_H}{\\text{max}} Q^*(s_H, a_H) \\Big]$\n$ = \\underset{\\pi}{\\text{max}} \\underset{s_1,...,s_{H-1} \\sim P \\atop a_1,...,a_{H-2} \\sim \\pi}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-2} \\gamma^t r(s_t, a_t) + \\gamma^{H-1}  \\underset{a_{H-1}}{\\text{max}} \\underset{s_H \\sim P}{\\mathbb{E}} \\Big[ r(s_{H-1}, a_{H-1}) + \\gamma  \\underset{a_H}{\\text{max}} Q^*(s_H, a_H) \\Big] \\Big]$\n$ = \\underset{\\pi}{\\text{max}} \\underset{s_1,...,s_{H-1} \\sim P \\atop a_1,...,a_{H-2} \\sim \\pi}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-2} \\gamma^t r(s_t, a_t) + \\gamma^{H-1}  \\underset{a_{H-1}}{\\text{max}} Q^*(s_{H-1}, a_{H-1}) \\Big]$\nTherefore, we have $a_{H-1} = arg \\underset{a}{\\text{max}} Q^*(s_{H-1}, a)$. Similarly, continuing to use dynamic programming on Equation (17), we finally get: $a_i = arg \\underset{a}{\\text{max}} Q^*(s_i, a), (i = 1, 2, \\cdot\\cdot\\cdot, H - 1)$. Thus we claim that the optimal policy is induced by the optimal value and use the symbol $\\pi_{Q^*}$ instead of $\\pi$ in Equation (16) to obtain Equation (11)."}, {"title": "B DERIVATION FROM EQUATION (11) TO EQUATION (12)", "content": "Given the learned dynamic preditor $\\hat{P}$, reward predictor $\\hat{r}$, and estimated optimal Q-value $\\hat{Q}^*$, we first substitute these three functions for the ground-truth functions in the right side of Equation (11):\n$\\underset{\\pi}{\\text{max}} \\underset{s_1,...,s_H \\sim \\hat{P} \\atop a_1,...,a_{H-1} \\sim \\pi}{\\mathbb{E}} \\Big[\\sum_{t=0}^{H-1} \\gamma^t \\hat{r}(s_t, a_t) + \\gamma^H  \\underset{a_H}{\\text{max}} \\hat{Q}^*(s_H, a_H) \\Big]$\nHowever, due to the estimation error between learned functions and ground-truth functions, $\\hat{Q}^*(s_t, a_t)$ is typically not equal to $\\underset{s_{t+1} \\sim \\hat{P}}{\\mathbb{E}} [\\hat{r}(s_t, a_t) + \\gamma \\underset{a}{\\text{max}} \\hat{Q}^*(s_{t+1}, a)]$. Therefore, instead of using dynamic programming to derive that the optimal policy is the greedy policy as in Proof A, we have to use search to solve formula (18)."}, {"title": "C UPPER BOUND OF SEARCH-BASED Q-VALUE ESTIMATION", "content": "Let function $f(s_0, a_0)$ be equal to formula (19) and let $\\pi_{\\hat{Q}^*}$ be the optimal policy of formula (19). We have:\n$f(s_0, a_0) = \\underset{\\pi \\in \\Pi_{\\hat{Q}^*}} {\\mathbb{E}} \\Big[\\sum_{t=0}^{H-1} \\gamma^t \\hat{r}(s_t, a_t) + \\gamma^H  \\underset{a_H}{\\text{max}} \\hat{Q}^*(s_H, a_H) \\Big]$\nWe make the following assumption similar to EfficientZero-v2 (Wang et al., 2024):\n$\\underset{n \\in [N], t \\in [H(n)]}{\\text{max}} \\mathbb{E} [||r(s_t) - \\hat{r}(s_t)||] \\leq \\epsilon_r$\n$\\underset{n \\in [N], t \\in [H(n)]}{\\text{max}} \\mathbb{E} [||\\hat{Q}^*(s_t) - Q^*(s_t)||] \\leq \\epsilon_Q$\nAssume the learned reward function $\\hat{r}$ to be $L_r$-Lipschitz and the estimated optimal Q-function $\\hat{Q}^*$ to be $L_Q$-Lipschitz. Assume the estimation errors of learned functions are bounded as in Assumption C.1. Then we have the error between search-based Q-value estimation $f(s_0, a_0)$ and ground-truth Q-value $Q^*(s_0, a_0)$ bounded as:\n$||f(s_0, a_0) - Q^*(s_0, a_0) || \\leq \\frac{1 - \\gamma^H}{1 - \\gamma} \\epsilon_r + \\frac{\\gamma^H - \\gamma}{1 - \\gamma} (L_r \\epsilon_s + \\epsilon_r) + \\gamma^H L_Q \\epsilon_s + \\gamma^H \\epsilon_Q$"}, {"title": "D GAMES", "content": "We select 20 Atari games maintaining the difficulty distribution of full Atari 2600 games defined by Gulcehre et al. (2020), which includes 9 easy games, 9 medium games, and 2 hard games. We use 15 out of 18 games for training and the remaining 5 for OOD generalization experiments. The 15 training games are: Phoenix, Centipede, SpaceInvaders, Carnival, NameThisGame, Assault, Atlantis, DemonAttack, BeamRider, ChopperCommand, Seaquest, TimePilot, StarGunner, Berzerk, Zaxxon. The 5 held-out games are: Pong, Robotank, YarsRevenge, Gravitar, MsPacman. Details about the size of action spaces and game difficulties are shown in Table 6."}, {"title": "E EXPERIMENTAL DETAILS", "content": ""}, {"title": "E.1 IMPLEMENT DETAILS", "content": ""}, {"title": "E.1.1 JOWA", "content": "We implement JOWA based on the codes of IRIS (Micheli et al.", "function": "n$L(E, D, \\mathcal{E}) = ||x - D(z)||_1 + ||sg(\\mathcal{E}(x)) - \\mathcal{E}(z)||_2 + ||sg(\\mathcal{E}(z)) - \\mathcal{E}(x)||_2 + L_{perceptual}(x"}]}