{"title": "FEED: Fairness-Enhanced Meta-Learning for Domain Generalization", "authors": ["Kai Jiang", "Chen Zhao", "Haoliang Wang", "Feng Chen"], "abstract": "Generalizing to out-of-distribution data with being aware of model fairness is a significant and challenging problem in meta-learning. The goal of this problem is to find a set of fairness-aware invariant parameter of classifier that is trained using data drawn from a family of related training domains with distribution shift on non-sensitive features as well as different levels of dependence between model predictions and sensitive features so that the classifier can achieve good generalization performance on unknown but distinct test domains. To tackle this challenge, existing state-of-the-art methods either address the domain generalization problem but completely ignore learning with fairness, or solely specify shifted domains with various fairness levels. This paper introduces an approach to fairness-aware meta-learning that significantly enhances domain generalization capabilities. Our framework, Fairness-Enhanced Meta-Learning for Domain Generalization (FEED), disentangles latent data representations into content, style, and sensitive vectors. This disentanglement facilitates the robust generalization of machine learning models across diverse domains while adhering to fairness constraints. Unlike traditional methods that focus primarily on domain invariance or sensitivity to shifts, our model integrates a fairness-aware invariance criterion directly into the meta-learning process. This integration ensures that the learned parameters uphold fairness consistently, even when domain characteristics vary widely. We validate our approach through extensive experiments across multiple benchmarks, demonstrating not only superior performance in maintaining high accuracy and fairness but also significant improvements over existing state-of-the-art methods in domain generalization tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "The widespread adoption of machine learning across various sectors has underscored the critical importance of developing algorithms that can perform well across diverse domains. This challenge, often termed domain generalization, is crucial in environments that differ from the training settings, a common scenario in real-world applications such as healthcare, finance, and social justice. In these applications, not only is high accuracy essential, but fairness cannot be overlooked, especially when sensitive attributes like gender or ethnicity are involved [6], [10], [11].\nRecent advancements in domain generalization techniques [2], [15] have aimed at learning domain-invariant features. However, these methods often fail to address changes in distributions of sensitive attributes across domains, leading to potential fairness issues when deployed in varied real-world settings [19]."}, {"title": "II. RELATED WORK", "content": "Fairness-aware domain generalization. Fairness considerations in domain generalization have emerged as a concern due to challenges posed by domain shifts and the unavailability of out-of-distribution (OOD) data, which are traditionally tackled by several leading techniques [2], [12]\u2013[14], [16], [17]. These methods strive to enhance the innate generalizability of machine learning models across source domains, each characterized by distinct but potentially overlapping distributions [25]. A prevalent approach involves aligning distributions across multiple sources to foster domain-invariant feature representations, crucial for stable pattern recognition across domains without target domain data access [26], [27]. Notably, some strategies incorporate meta-learning paradigms to acclimate the model to domain shifts during the training phase [15] or use domain analytic data augmentation techniques to broaden the model's exposure to potential shifts [27].\nDespite these advancements, the integration of fairness into domain generalization remains scant. Most research in domain generalization [16], [17], [28], has predominantly focused on leveraging diverse source data to uncover invariant patterns. As [28] articulates, the principal goal is to derive representations that are robust to the marginal distributions of data features, thereby eschewing reliance on target data. However, this line of inquiry largely overlooks the nuances of ensuring that fairness across varying domains. Addressing this gap could enhance the robustness and ethical alignment of models deployed in real-world settings. A recent method for disentangling sensitive attributes, as proposed by Zhao et al. (2024) [18], focuses on learning domain-invariant parameters from training domains. These parameters are fixed and directly applied to new domains. However, the method lacks adaptability when applied to new domains with only a few examples. These methods optimize parameters for multiple domains but are limited in rapidly adapting to unseen tasks. In contrast, our method, based on meta-learning, learns initial parameters that quickly adapt to new domains while ensuring fairness.\nFairness-aware meta-learning. In the context of fairness-aware meta-learning, research efforts primarily focus on developing adaptable frameworks that can effectively handle shifts in domain characteristics while maintaining fairness standards. Strategies such as equality-aware monitoring [29] have been developed. These approaches continuously observe the outputs of a model to detect any deviations from fairness norms and adjust accordingly by modifying the model's parameters or its structure. However, these methods traditionally operate under the assumption that fairness metrics remain consistent across different domains, an assumption often contradicted by the complexities encountered in practical scenarios. Zeng et al. [34] introduced a Nash Bargaining solution to enhance fairness in meta-learning models. However, their approach sometimes struggled with the robustness of fairness across drastic domain shifts due to an overemphasis on bargaining outcomes in homogeneous domains. In contrast, our framework enhances domain generalization by disentangling latent representations into content, style, and sensitive factors, thereby maintaining fairness even when domain characteristics vary significantly. Furthermore, alternative approaches in the literature [19], [20] attempt to evaluate a model's fairness by recognizing changes in fairness benchmarks as indicative of domain shifts, yet they tend to overlook variations in the distribution of non-sensitive attributes, which can lead to inadequate generalization capabilities.\nTo address these challenges, our meta-learning framework innovatively partitions data attributes into sensitive and non-sensitive categories. Such a distinction is pivotal for the meta-learning algorithm, which is designed not merely to react to explicit domain labels but also to respond to more nuanced shifts in the distributions of data features. This approach enables our meta-learning algorithm to refine its strategy for learning initial parameters, ensuring domain generalization and fairness. By effectively distinguishing between these attribute categories, the algorithm can prioritize the learning of initial parameters that maintain high performance and fairness standards across a spectrum of environments."}, {"title": "III. PRELIMINARIES", "content": "Notations. Consider the data space P = X \u00d7 Z \u00d7 Y, where X \u2286 R\u1d48 denotes a feature space, Z \u2286 {\u22121,1} denotes binary sensitive attributes\u00b9, and Y \u2286 {0, 1} denotes the binary output space for binary classification. Define the parameterized latent spaces: C for content, S for style, and A for sensitivity factors. The function d(\u00b7, \u00b7) is a distance measure across the space Y \u00d7 Y. Variables and parameters in our framework are symbolically denoted as follows: vectors in boldface lowercase letters, and scalars in italic lowercase letters.\nProblem setting. Given a dataset D, we consider a set of data domains E = {e\u1d62}\u1d62=\u2081\u207f where each domain corresponds to a distinct data subset D\u2091\u1d62 = {(x,z,y)}\u2c7c=\u2081|\u1d30\u2091\u1d62| over P, and D = \u222a\u1d62=\u2081,\u2026,\u2099 D\u2091\u1d62. Data domains are partitioned"}, {"title": "A. Assumptions", "content": "Assumption 1 (Latent Spaces). Given a batch B = {(x,z,y)\u2091\u1d62}\u2081\u2091\u1d62 sampled from a specific domain e\u1d62 \u2208 E, as illustrated in Fig. 2, we postulate that each data point x within the task originates from:\n\u2022 a latent content factor c \u2208 C, where C denotes a content space that is invariant across all domains E;\n\u2022 a latent style factor s \u2208 S that is unique to the specific domain e\u1d62;\n\u2022 a latent sensitive factor a \u2208 A.\nwhere C \u2229 S \u2229 A = \u00d8. Each domain e\u1d62 is uniquely characterized by its style factors, denoted as e\u1d62 := s.\nAssumps. 1 echoes the assumptions made in prior works such as [16], [17], [21], [30]. Specifically, UNIT [21] hypothesizes a fully shared latent space across all factors, whereas MUNIT [30] suggests a hybrid latent space model where some components are shared across domains and others are domain-specific. In our framework, considering group fairness, we extend these concepts to include three distinct latent spaces: a content space C, a style space S, and a sensitive space A. Additionally, we posit that domain labels are typically unattainable in both training and testing phases due to practical limitations or excessive costs, as supported by [31].\nIt is essential for fairness that the labels remain independent of variations across domains. This requirement translates to a scenario where instance conditional distributions {P(Y\u2091\u1d62|X\u2091\u1d62, Z\u2091\u1d62)}\u2091\u1d62\u2208\u025b differ by domain, reflective of inherent domain-specific characteristics. Within the context of this research, we posit that differences across domains, termed as domain shifts, are governed exclusively by a transformation model T : X \u00d7 Z \u00d7 E \u2192 X \u00d7 Z. Specifically, if two samples (x\u2091\u1d62, z\u2091\u1d62) and (x\u2091\u2c7c, z\u2091\u2c7c) from different domains e\u1d62, e\u2c7c \u2208 E, where i \u2260 j, exhibit identical content factors, then the sample from domain e\u2c7c can be reconstructed from the sample of domain e\u1d62 using the transformation T. This process involves T extracting the invariant content from (x\u2091\u1d62, z\u2091\u1d62) and subsequently applying domain-specific style and sensitivity information encoded in e\u2c7c to regenerate (x\u2091\u2c7c, z\u2091\u2c7c).\nAssumption 2 (Fairness-aware Domain Invariance). We hypothesize that the variations observed between domains are primarily driven by changes in the marginal distributions P(X\u2091) and P(Z\u2091) for each domain e \u2208 E. Consequently, we posit that the conditional distribution P(Y\u2091|X\u2091, Z\u2091) remains consistent across different domains. With a domain transformation function T, we assert that for any feature vector x \u2208 X, sensitive attribute z \u2208 Z, and class label y \u2208 Y:\nP(Y\u2091\u1d62 = y|X\u2091\u1d62 = x\u2091\u1d62, Z\u2091\u1d62 = z\u2091\u1d62) = P(Y\u2091\u2c7c = y|(X\u2091\u2c7c, Z\u2091\u2c7c) = T(x\u2091\u1d62, z\u2091\u1d62, e\u2c7c)) \u2200e\u1d62, e\u2c7c \u2208 E, i \u2260 j\nIn relation to existing literature, Robey et al. [17] describe a version of T that incorporates content and style factors, but overlooks the sensitive factors which are crucial for ensuring fairness in domain generalization. The domain shift driven by T effectively represents how the distinct distributions P(X\u2091\u1d62) and P(Z\u2091\u1d62) map to the corresponding distributions P(X\u2091\u2c7c) and P(Z\u2091\u2c7c) in domains. Moreover, it is fundamental in our framework that class labels y ~ Y should remain invariant to changes in fairness-sensitive attributes across domains. In this context, inter-domain variation is exclusively defined by the transformations dictated by T.\nOur approach delves into the domain generalization problem, where inter-domain variability is specifically attributed to domain shifts driven by T, representing environmental discrepancies across a collection of marginal distributions"}, {"title": "IV. METHODOLOGY", "content": "A. Disentanglement for Fairness-aware Domain Generalization\nIn our approach to enhance fairness in domain generalization, we leverage a disentanglement strategy. This strategy decomposes the samples into three distinct components: content, style, and sensitive vectors. These content vectors capture domain-invariant features essential for prediction performance, while the style vector encapsulates domain-specific variations that are irrelevant to the labels. The sensitive vector captures the sensitive attributes that could potentially lead to bias. Each sample is decomposed into these three latent vectors, enabling the generation of new samples in a synthetic domain by replacing the style and sensitive vectors with sampled ones, independent of the original domain characteristics. It allows the exploration of a more extensive and varied synthetic domain space, potentially uncovering and mitigating unfair biases that were not explicit in the original data distribution.\nOur proposed framework involves disentangling an input sample from training domains into three factors in distinct latent spaces, using a series of encoders E = {E\u1d50, E\u02e2, E\u1d9c, E\u1d43} and decoders G = {G\u2071, G\u1d52}. These are parameterized respectively by \u0398\u1d50, \u0398\u02e2, \u0398\u1d9c, \u0398\u1d43 \u2208 \u0398 and \u03a6\u1d62, \u03a6\u2092 \u2208 \u03a6. The framework operates through two hierarchical levels: an outer level and an inner level, each with its own auto-encoder.\nIn the outer level, an input datapoint undergoes encoding into a semantic factor m \u2208 M and a style factor s \u2208 S, achieved via the encoders E\u1d50 : X \u00d7 \u0398 \u2192 M and E\u02e2 : X \u00d7 \u0398 \u2192 S. Progressing to the inner level, the semantic factor m is further decomposed into a content factor c \u2208 C and a sensitive factor a \u2208 A through the encoders E\u1d9c : M \u00d7 \u0398 \u2192 C and E\u1d43 : M \u00d7 \u0398 \u2192 A. The corresponding decoders in these levels are G\u2071 : C \u00d7 A \u00d7 \u03a6 \u2192 M for the inner level and G\u1d52 : M \u00d7 S \u00d7 \u03a6 \u2192 X for the outer level, facilitating the reconstruction of the original data. Inspired by image-to-image translation in computer vision [21], [30], Our total loss function of learning such encoders and decoders comprises three components: a bidirectional reconstruction loss, a sensitive label prediction loss, and an adversarial loss.\nReconstruction loss Considering a datapoint x sampled from p(x), encoders and decoders in outer loop are able to reconstruct it by minimizing the reconstruction loss:\nL\u02b3\u1d49\u1d9c\u1d52\u207f = E\u2093\u223cp(x) [||G\u1d52(m, E\u02e2(x)) \u2212 x||\u2081]\nwhere m = G\u2071(c, a) = G\u2071(E\u1d9c(E\u1d50(x)), E\u1d43(E\u1d50(x))) . For the inner level, the semantic factor m = E\u1d50(x) encoded from the outer level is required to be reconstructed:\nL\u1d50\u02b3\u1d49\u1d9c\u1d52\u207f = E\u1d50\u223cp(m) [||G\u2071(E\u1d9c(m), E\u1d43(m)) \u2212 m||\u2081]\nwith p(m) determined by the mapping m = E\u1d50(x) and x \u223c p(x).\nThe latent factors c, s, a, extracted from the datapoint x are encouraged to be reconstructed through some latent factors randomly sampled from the prior distributions.\nL\u02b3\u1d49\u1d9c\u1d52\u207f\u1d9c = E\u1d9c\u223cp(c),a\u223c\ud835\udca9(0,I\u1d43) [||E\u1d9c(G\u2071(c, a)) \u2212 c||\u2081]\nL\u02b3\u1d49\u1d9c\u1d52\u207f\u1d43 = E\u1d9c\u223cp(c),a\u223c\ud835\udca9(0,I\u1d43) [||E\u1d43(G\u2071(c, a)) \u2212 a||\u2081]\nwhere p(c) is given by c = E\u1d9c(E\u1d50(x)), and a = E\u1d43(E\u1d50(x)). Considering the dual-role of m, as both a latent factor from the inner level and an input to the outer level, s can be reconstructed by two reconstruction losses:\nL\u02b3\u1d49\u1d9c\u1d52\u207f\u02e2 = E\u1d50\u223cp(m),s\u223c\ud835\udca9(0,I\u02e2) [||E\u02e2(G\u1d52(m, s)) \u2212 s||\u2081]\nL\u02b3\u1d49\u1d9c\u1d52\u207f\u02e2 = E\u1d9c\u223cp(c),s\u223c\ud835\udca9(0,I\u02e2),a\u223c\ud835\udca9(0,I\u1d43) [||E\u02e2(G\u1d52(G\u2071(c, a), s)) \u2212 s||\u2081]\nand for reconstructing m as a latent factor:\nL\u1d50\u02b3\u1d49\u1d9c\u1d52\u207f = E\u1d50\u223cp(m),s\u223c\ud835\udca9(0,I\u02e2) [||E\u1d50(G\u1d52(m, s)) \u2212 m||\u2081]\nThe reconstruction loss is defined as follows:\nL\u02b3\u1d49\u1d9c\u1d52\u207f = L\u02b3\u1d49\u1d9c\u1d52\u207f\u1d52 + L\u02b3\u1d49\u1d9c\u1d52\u207f\u1d50 + L\u02b3\u1d49\u1d9c\u1d52\u207f\u1d9c + L\u02b3\u1d49\u1d9c\u1d52\u207f\u1d43 + L\u02b3\u1d49\u1d9c\u1d52\u207f\u02e2\u1da0 + L\u02b3\u1d49\u1d9c\u1d52\u207f\u02e2\u1d52\u1d58\u1d57 + L\u1d50\u02b3\u1d49\u1d9c\u1d52\u207f\nSensitive prediction loss The sensitive attributes encoded from the datapoint x underpin the training of a classifier h : A \u00d7 \u0398 \u2192 Z. This classifier is then employed to predict the sensitive label associated with the attribute vector a. Specifically, the prediction is formulated as:\nz = h(a, \u0398z) = h(E\u1d43(E\u1d50(x)), \u0398z) , L\u1d9c\u02e1\u02e2 = CrossEntropy(z, z)\nAdversarial loss Inspired by the effectiveness of Generative Adversarial Networks (GANs) [32], define discriminators D = {D\u2071, D\u1d52}, where D\u1d52 : X \u00d7 \u03a8 \u2192 \u211d is the discriminator for the outer level, parameterized by \u03c8\u2092 \u2208 \u03a8, and D\u2071 : M \u00d7 \u03a8 \u2192 \u211d is"}, {"title": "B. Fairness-aware Meta-Learning", "content": "Problem 1 (Meta-Learning for Fairness-aware Domain Generalization). Given the definitions and assumptions under Definition 1 and Assumps. 2 and a loss function l : Y \u00d7 Y \u2192 \u211d, we define the meta-learning problem as follows:\n\u0398\u2217 = argmin \u03a3\u2091\u1d62\u2208\ud835\udd3c\u1d57\u02b3\u1d43\u2071\u207f E\u209a(\u00d7\u1d49\u2071,\u1dbb\u1d49\u2071,\u1d5e\u1d49\u2071)l(f(X\u2091\u1d62, \u0398\u2071), Y\u2091\u1d62)\nsubject to f(X\u2091\u1d62, \u0398) = f(T(X\u2091\u1d62, Z\u2091\u1d62, e\u2c7c), \u0398),\nE\u209a(\u00d7\u1d49\u2071,\u1dbb\u1d49\u2071),P(\u00d7\u1d49\u02b2,\u1dbb\u1d49\u02b2) [g(X\u2091\u1d62, Z\u2091\u1d62) + g(X\u2091\u2c7c, Z\u2091\u2c7c)] = 0\nwhere the inner loop problem is defined as:\n\u0398\u2032 = argmin E\u209a(\u00d7\u1d49\u2071,\u1dbb\u1d49\u2071,\u1d5e\u1d49\u2071)l(f(X\u2091\u1d62, \u0398\u2032), Y\u2091\u1d62)\nsubject to f(X, \u0398\u2032) = f(T(X\u2091\u1d62, Z\u2091\u1d62, e\u2c7c), \u0398\u2032),\nE\u209a(\u00d7\u1d49\u2071,\u1dbb\u1d49\u2071),P(\u00d7\u1d49\u02b2,\u1dbb\u1d49\u02b2) [g(X\u2091\u1d62, Z\u2091\u1d62) + g(X\u2091\u2c7c, Z\u2091\u2c7c)] = 0\nwhere x\u2091\u1d62 \u223c P(X\u2091\u1d62), x\u2091\u2c7c \u223c P(X\u2091\u2c7c), z\u2091\u1d62 \u223c P(Z\u2091\u1d62), \u2200e\u1d62, e\u2c7c \u2208 E\u1d57\u02b3\u1d43\u2071\u207f, i \u2260 j. \u0398 is the initialization of \u0398\u2032 in \u0398.\nThe downstream problem is defined as follows:\nmin E (\u00d7\u1d49\u1d4f,\u1dbb\u1d49\u1d4f,\u1d5e\u1d49\u1d4f)l(f(X\u2091\u2096, \u0398\u207b), Y\u2091\u2096)\ne\u2096 \u2208E\u1d57\u1d49\u02e2\u1d57\nsubject to f(X\u2091\u2096, \u0398) = f(T(X\u2091\u2096, Z\u2091\u2096, e\u03b9), \u0398),\nE\u209a(\u00d7\u1d49\u1d4f,\u1dbb\u1d49\u1d4f),P(\u00d7\u1d49\u03b9,\u1dbb\u1d49\u03b9) [g(X\u2091\u2096, Z\u2091\u2096) + g(X\u2091\u03b9, Z\u2091\u03b9)] = 0\nwhere x\u2091\u2096 \u223c P(X\u2091\u2096), x\u2091\u03b9 \u223c P(X\u2091\u03b9), z\u2091\u2096 \u223c P(Z\u2091\u2096), \u2200e\u2096, e\u03b9 \u2208 E\u1d57\u1d49\u02e2\u1d57, i \u2260 j. \u0398\u2217 is the initialization of \u0398.\nThe challenges presented in problem 1 arise from the need for meta-learning models. Specifically, the framework conducts meta-training across all the training domains Etrain, utilizing the breadth of these domains to learn a robust set of initial parameters. However, the true test of generalization and fairness occurs during the subsequent phase, where the meta parameters serve as initial parameters for downstream tasks on a limited subset of samples from the testing domains Etest. This problem underscores a significant challenge: ensuring that the model not only adapts to new, unseen domains with very few examples but also maintains consistent and fair performance across the comprehensive domain set E. The sparse availability of samples in Etest compounds this difficulty, demanding that the initial parameters derived from meta-training possess an intrinsic capability to generalize effectively and equitably, even under constrained conditions. A key aspect of tackling this problem involves addressing how closely the data feature distributions in testing domains resemble those in the observed training domains Etrain. The existing methods on domain generalization [16], [17] incorporate this consideration and introduce solutions primarily focused on decomposing variations in data features across domains into distinct latent spaces. To ensure fairness, data features are categorized into sensitive and non-sensitive components. It is assumed that the dependency of sensitive features on labels might vary across domains, which may not be strictly domain-invariant or domain-specific. This nuanced understanding acknowledges that fairness levels across different domains may differ, enhancing the realism and applicability of the proposed solutions.\nImplementation of FEED. Our proposed implementation is shown in Algorithm 1. In lines 14-21, we describe the T procedure that takes an example (x,z,y) as input and returns an augmented example (x', z', y) from a new synthetic domain as output. The augmented example has the same content factor as the input example but has different style and sensitive factors sampled from their associated distributions that encode a new synthetic domain as shown in Fig. 3. Line 3-10 show the inner loop updating the task specifice parameters for FEED, and line 11-12 show the outer loop udating the meta parameters. In line 6 and line 8, for each example in a data batch B, we apply the procedure T to generate an augmented example from in new synthetic domain. The loss functions are defined in Eqs. (6) to (9) and the Eqs. (10) and (11) show how the hyperparameters \u03bb\u2081, \u03bb\u2082 are updated.\nClassification loss Given data batch B = {(x\u1d62, y\u1d62, z\u1d62)}\u1d62=|\u1d2e| and classifier f parameterized by \u0398, the classification loss Lcls (\u0398, B) is defined as:\nLcls (\u0398, B) = (1/|B|)\u03a3\u1d62=|\u1d2e|l(y\u1d62, f(x\u1d62, \u0398))\nwhere we use crossentropy as the distance metric for d(\u00b7). Invariance loss With Baug whose data points are transformed from B by T, the invariance loss Linv(\u0398, B, Baug) is"}, {"title": "V. EXPERIMENTS", "content": "We conducted a comprehensive evaluation of our proposed framework, FEED, across a variety of domain generalization benchmarks that encompass both domain characteristics and sensitive attributes. In this assessment, FEED was compared"}, {"title": "VI. CONCLUSION", "content": "In this paper, we have introduced a novel framework for fairness-aware meta-learning aimed at enhancing domain generalization across diverse environments. By disentangling latent factors into content, style, and sensitive vectors, our approach ensures that the fairness, even in the face of domain"}]}