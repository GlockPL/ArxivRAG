{"title": "Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World Conditions", "authors": ["Arunkumar Rathinam", "Leo Pauly", "Abd El Rahman Shabayek", "Wassim Rharbaoui", "Anis Kacem", "Vincent Gaudilli\u00e8re", "Djamila Aouada"], "abstract": "Multispectral pedestrian detection has gained significant attention in recent years, particularly in autonomous driving applications. To address the challenges posed by adversarial illumination conditions, the combination of thermal and visible images has demonstrated its advantages. However, existing fusion methods rely on the critical assumption that the RGB-Thermal (RGB-T) image pairs are fully overlapping. These assumptions often do not hold in real-world applications, where only partial overlap between images can occur due to sensors configuration. Moreover, sensor failure can cause loss of information in one modality. In this paper, we propose a novel module called the Hybrid Attention (HA) mechanism as our main contribution to mitigate performance degradation caused by partial overlap and sensor failure, i.e. when at least part of the scene is acquired by only one sensor. We propose an improved RGB-T fusion algorithm, robust against partial overlap and sensor failure encountered during inference in real-world applications. We also leverage a mobile-friendly backbone to cope with resource constraints in embedded systems. We conducted experiments by simulating various partial overlap and sensor failure scenarios to evaluate the performance of our proposed method. The results demonstrate that our approach outperforms state-of-the-art methods, showcasing its superiority in handling real-world challenges.", "sections": [{"title": "I. INTRODUCTION", "content": "PEDESTRIAN detection is one of the important domains within computer vision for robotics, playing a significant role in applications such as self-driving vehicles, surveillance automation, and mobile robot navigation [1]. RGB cameras are commonly preferred sensors for such applications. How-ever, they tend to suffer from overexposure in daylight, low illumination in night scenarios, and high-contrast lighting. To address these shortcomings, a number of sensors and fusion solutions were investigated. In particular, thermal cameras seem to provide several advantages in terms of costs, al-gorithms, and data [2]. Among them, RGB images provide texture and color information, while thermal images focus on the infrared heat emitted by the objects and are therefore invariant to lighting conditions [3]. RGB and thermal images are thefore complementary with each other by nature. This led the community to collect multispectral datasets such as KAIST [1], CVC [4] or FLIR [5], providing thermal data in addition to RGB data.\nKAIST dataset provides fully-overlapping RGB-T image pairs, i.e. both images are acquired at the same time and cover the same field of view. However, acquiring such image pairs requires specialised sensor setup over conventional stereo setup which is widely used in real-world applications. In stereo setups, partial overlap will occur inherently due to a different camera Field of View (FoV) and pixel-level misalignment will occur due to parallax [6]. Information discrepancy between one image and the other can cause features to be out of their corresponding positions, resulting in decreased algorithm performance and less accurate predictions during the inference process [7]. Even in the KAIST dataset that has fully overlap-ping image pairs, the authors attempted to reduce the pixel-level misalignment problem. This was achieved by further improving the original data labels to \u201csanitised\u201d cross-modal annotations [8] and \u201cpaired\u201d modality-specific annotations [7]"}, {"title": "II. RELATED WORKS", "content": "The multispectral pedestrian detection problem has been widely studied in computer vision. Several classical methods use RGB and thermal images, relying on pixel difference val-ues [9], local shape features [10], contour saliency maps [11], disparity maps [2] and HOG features [12].\nIn 2015, Hwang et al. introduced KAIST [1], a large-scale multispectral pedestrian detection dataset that contains RGB and thermal images with the corresponding pedestrian labels. The release of the KAIST dataset accompanied a renewed interest in the multispectral pedestrian detection problem, and several new methods have been proposed since then. For example, Liu et al. [13] proposed a deep learning-based Halfway Fusion model and presented comparative analyses with other early fusion architectures (input-level fusion) and late fusion architectures (decision-level fusion). In another study, Li et al. [8] demonstrated that the incorporation of an additional semantic segmentation task led to enhanced performance compared to the use of a model focused solely on detection. They introduced a combined architecture that included a multispectral proposal network to generate pedes-trian proposals and a subsequent multispectral classification network to distinguish pedestrian instances from challenging negatives. The authors trained the integrated network by simul-taneously optimising both pedestrian detection and semantic segmentation tasks. Zheng et al. [14] introduced Gated Fusion Units (GFU) which are designed to merge feature maps from the feature extraction layers of Single Shot MultiBox Detector (SSD) at various scales [15]. In their studies [16], [17], the authors explored the use of distinct subnetworks for individual modalities and incorporated illumination-adaptive weighting of these subnetworks to enhance the efficiency of multispectral pedestrian detection. This approach enabled the prioritiza-tion of information from the RGB modality in adequately illuminated images or from the thermal modality in low-light situations. Zhou et al. [18] introduced Modality Balance Network (MBNet), which simultaneously compensated for modality imbalance problems in illumination and at the feature levels. Chen et al. [19] presented a late fusion architecture by probabilistically ensembling decisions made individually from RGB and thermal images. Zhang et al. [20] presented a fusion mechanism under the guidance of the intermodal and intramodal attention modules, to learn to dynamically weigh and fuse multispectral features. Yang et al. [21] proposed an algorithm that uses cascaded information enhancement and fusion of cross-modal attention features, both of which rely on the attention mechanism.\nThe existing research has significantly enhanced the effec-tiveness of combining RGB and thermal images for pedestrian detection through multispectral fusion. Nonetheless, the algo-rithms typically operate under the assumption that both types of images are accessible during inference, neglecting scenarios where sensor failure may occur. Furthermore, potential sensor modifications during inference such as modifications in the stereo arrangement or lens settings, compared to the training dataset acquisition, are not taken into account. Such discrepan-cies may result in only partially-overlapping images leading to blackout regions (refer to Figure 1). Analysis of existing algorithms shows a considerable degradation in performance under such conditions [6]. However, robustness to such re-alistic inference-time conditions is important for real-world"}, {"title": "III. PROPOSED HA-MLPD", "content": "Our method, HA-MLPD, assumes that images from both modalities are automatically registered at test time and that pixel values in resulting non-overlapping regions are set to zero (i.e. blackout). Figure 2 shows some examples. The masks of the overlapping regions are further leveraged to guide the network through cross- or self-attention to the features. For that, we use masks $M_{rgb}$, $M_{thermal}$ corresponding to blackout regions as what can be obtained by registering the images from the two modalities (using methods such as [33] for instance, or directly using the likely known stereo parameters). Note that the registration process is not within the scope of this paper. In detail, the masks $M_{rgb}$ and $M_{thermal}$ are set to 1 modality-specific information is available at the location of the pixel, 0 otherwise."}, {"title": "B. HA-MLPD Overview", "content": "Our model, HA-MLPD, consists of the feature extraction layers, HA module, fusion layer, and detection head. Figure 3 presents the general network architecture for the proposed HA-MLPD algorithm with the MobilenetV2 [28] backbone. Two separate branches are used to extract feature maps from both RGB and thermal images. The HA module, placed after the first block of layers, is used to perform cross- or self-attention accross the two modalities. The attention mechanism varies according to the overlapping and non-overlapping image regions (as illustrated in Figure 4). The extracted features from different levels are then concatenated and passed through a fusion layer to generate features used as input to the detection head. The fusion layer, designed to be as light as possible for real-time applications, is composed of one convolutional layer followed by batch normalization and ReLu activation. This is similar to MLPD, where this multi-level fusion strategy and fusion layer were introduced to cope with the loss of modality-specific information afer shared convolutional blocks. To strengthen this effect, we decoupled the modality-shared layers of MLPD to make them modality-specific in HA-MLPD."}, {"title": "C. Details of HA-MLPD Pipeline", "content": "Feature Extraction: Any standard feature extraction back-bone, possibly followed by additional blocks of layers, can be used to extract features ($F_{rgb}$, $F_{thermal}$) from both images:\n$F_{rgb} = \\phi_{rgb}(I_{rgb}),$\n$F_{thermal} = \\phi_{thermal}(I_{thermal}),$ (1)\nwhere $I_{rgb}$, $I_{thermal}$ represents RGB and thermal images respectively, and $\\phi_{rgb}$, $\\phi_{thermal}$ the corresponding feature extractors. In our experiments, MobileNetV2 [28] was cho-sen for its compactness (see Figure 3 for overview of the corresponding architecture), and VGG-16 [34] as the original MLPD backbone. These feature extractors are composed of $B$ consecutive blocks of layers:\n$\\phi_{rgb} = \\phi^{(B)}_{rgb} \\circ ... \\circ \\phi^{(2)}_{rgb} \\circ \\phi^{(1)}_{rgb},$ \n$\\phi_{thermal} = \\phi^{(B)}_{thermal} \\circ ... \\circ \\phi^{(2)}_{thermal} \\circ \\phi^{(1)}_{thermal},$ (2)\nFeatures extracted after the first blocks, $\\phi^{(1)}_{rgb}$, $\\phi^{(1)}_{thermal}$, denoted in what follows as $F_{rgb}, F_{thermal}$ for simplification, are then fed to our novel HA module.\nHybrid-Attention Module: The HA module shown in Figure 4 is the core of the proposed method. It enhances RGB and thermal features using the attention mechanism. The module switches between cross- and self-attention in response to the blackout regions in the input images. For that, we use masks $M_{rgb}$, $M_{thermal}$ corresponding to blackout regions to filter out features originating from these regions ($\\odot$ denotes element-wise multiplication):\n$f_{rgb} = M_{rgb} \\odot F_{rgb},$\n$f_{thermal} = M_{thermal} \\odot F_{thermal}$ (3)\nThe Keys, Queries and Values for each modality are then generated using 1 \u00d7 1 convolution layers:\n$Q_{rgb} = Conv_{1\\times1}(f_{rgb}),$\n$K_{rgb} = Conv_{1\\times1}(f_{rgb}),$\n$V_{rgb} = Conv_{1\\times1}(f_{rgb}),$\n$Q_{thermal} = Conv_{1\\times1}(f_{thermal}),$\n$K_{thermal} = Conv_{1\\times1}(f_{thermal}),$\n$V_{thermal} = Conv_{1\\times1}(f_{thermal}).$ (4)\nThe combined Query $Q_{c}$ is then computed as the sum of the two modality-specific queries:\n$Q_{c} = Q_{rgb} + Q_{thermal}.$ (5)\nRemoving the features corresponding to the blackout regions using the masks and then combining the queries will therefore make the modalities cross-attend where both modalities are available, and self-attend in the blackout regions. Following the standard practice, attended features $f'_{rgb}$ and $f'_{thermal}$ are calculated as:\n$f'_{rgb} = softmax(Q_{c}K_{rgb})V_{rgb},$\n$f'_{thermal} = softmax(Q_{c}K_{thermal})V_{thermal}.$ (6)\nFinally, the enhanced features $F'_{rgb}$ and $F'_{thermal}$ for each modality are obtained as:\n$F'_{rgb} = f_{rgb} + f'_{rgb},$\n$F'_{thermal} = f_{thermal} + f'_{thermal}.$ (7)\nFusion Layer: The enhanced features are then processed by the remaining feature extraction layers, to extract higher-level dual-modality-guided information. After each block of layers, the features from both branches are concatenated then fused using shared network layers. These layers are composed of 2D convolutions followed by Batch Norm and ReLu, similar to the MLPD baseline (see Figure 3).\nDetector Head: The multi-level fused features are then passed through the detection head. In our architecture, and similar to MLPD, we use the SSD [15] model for object detection. However, it can be replaced with any other state-of-the-art detector in practice. The detector outputs pedestrian bounding box locations and confidence scores.\nLoss function: The model is trained with regression loss on the bounding box locations $L_{bbox}$ as in SSD [15] and multi-label loss $L_{multilabel}$ from MLPD [6], balanced by a scaling factor $\u03bb$:\n$L = L_{bbox} + \u03bbL_{multilabel}.$ (8)"}, {"title": "D. Masking Data Augmentation", "content": "In addition to the data augmentations from the MLPD baseline, we included data augmentations using masking in our training process to foster the resilience of our approach. Our method involves masking the complete RGB and thermal modalities (probability of 10% for each), as well as randomly masking patches of either modality again with a probability of 10% for RGB masking and 10% for thermal masking. Note that we avoid masking the same region in both modalities simultaneously. Also, these augmentations are implemented exclusively during the training phase and are not used during inference."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The KAIST Multispectral Pedestrian Dataset [1] consists of 95,328 RGB-Thermal pairs fully overlapped captured in an urban environment. The provided ground truth consists of 103,128 pedestrian bounding boxes in 1,182 instances. In our experiments, we sample 1 frame out of 2 so that 25,076 frames are used for training, as in [6]. For evaluation, we also follow the standard evaluation criterion, which consists of sampling 1 out of every 20 frames, so the results are evaluated in 2,252 frames where 1,455 frames were recorded during the day and 797 frames at night [6]. We use paired annotations for training [7] and sanitised annotations for evaluation [8].\nTo generate the complete blackout cases arising from sensor failures, the original pixels values are replaced with zeros for either of the modalities as shown in Figure 2. For sides blackout cases, the original images are divided into three equal-sized vertical portions, and opposite side portions (e.g., left part in the RGB image and right part in the thermal image) are replaced with zero value pixels in each modality. For the surrounding blackout scenarios, we centre-crop the thermal image (96 pixels cropped at both the top and bottom and 120 pixels on the left and right sides) and replace the removed regions with zero pixels, while retaining the entire RGB image."}, {"title": "B. Training details", "content": "HA-MLPD with MobileNetV2 backbone - We extended the MobileNetV2 architecture with further convolutional lay-ers, as shown in Figure 3. To initialise, we used ImageNet pre-trained weights for blocks (B1, B2, B3) and the remaining convolution kernels were initialised using values sampled from a normal distribution (std=0.01). The network training process spanned 200 epochs, with an early stop callback in place to stop training if no improvement was observed for 50 epochs. The model was trained using SGD with an initial LR, momentum, and weight decay set at 1e-3, 0.9, and 4e-5, respectively. The LR was scheduled to decrease at the 150th and 190th epochs with a gamma value of 0.1.\nHA-MLPD with VGG-16 backbone - Like the original MLPD, we use VGG16 pre-trained on ImageNet with batch normalisation, from Convl to Conv5, and the remaining convolution kernels are initialised with values drawn from the normal distribution (std=0.01). The HA module is adopted at the output of Conv4 [6]. The model is trained by Stochastic Gradient Descent (SGD) with the initial learning rate (LR), momentum, and weight decay, as 1e-4, 0.9, and 5e-4, respec-tively. The mini-batch size is set to 8 and the input image size is resized to 512 (H) x 640 (W). The network was trained for 40 epochs, and the LR was scheduled to decrease after 20 and 36 epochs. The models were trained on a single GPU accelerator node featuring 2x AMD Rome CPUs (32 cores @ 2.35 GHz) and 4x NVIDIA A100-40 GPUs."}, {"title": "C. Adverse inference-time conditions simulation", "content": "To simulate sensor failure, we replaced one image of the RGB-Thermal pair by a full black image (zero pixel values). To simulate partial overlapping, we replace different parts of the images by black regions (see Figure 2). Side blackouts simulate discrepancies in lateral fields of view, whereas sur-rounding blackouts simulate differences in either focal length or sensor resolution."}, {"title": "D. Metrics", "content": "Following the standard practice in the field, and especially the MLPD baseline [6], we report the log-average Miss Rate (MR) [40] at an Intersection-over-Union threshold of 0.5 to summarise the detector performance. This metric gives a stable and informative assessment of detector performance [40]."}, {"title": "E. Results", "content": "Table II presents a comparison of the performance of our method with the existing literature in pedestrian detection scenarios using dual RGB and thermal modalities, especially in cases of sensor failure, such as RGB and thermal blackouts. Our method consistently shows competitive or superior results, demonstrating robustness in blackout conditions, particularly excelling in scenarios with RGB blackouts. It is important to highlight that even in complete blackouts of one modality, our method, in which HA then collapses in self-attention, outperforms both the RGB-only and Thermal-only models (referred to as SSD-RGB & SSD-Thermal). Furthermore, Table II includes comparisons with MLPD [6]. The colour and font coding highlights the best and second-best results in bold red and italic blue, respectively. HA-MLPD with the MobileNetv2 backbone achieves the highest performance in dual modality and RGB blackout scenarios, while HA-MLPD with the original VGG-16 backbone excels when RGB information is unavailable. In summary, our method demonstrates state-of-the-art performance, underscoring the effective RGB-Thermal fusion strategy in managing diverse real-world inference conditions.\nTable III delves into the performance of the different meth-ods in the context of inference-time misalignments involving extrinsic and intrinsic discrepancies in cameras. The scenar-ios considered include Sides Blackout with RGB-Thermal and Thermal-RGB misalignments, as well as Surrounding Blackout; see Figure 2. Our method consistently exhibits the best performance across these misalignment scenarios, outperforming baseline methods such as MSDS-RCNN [8], AR-CNN [7], MBNet [18], SSD-RGB [15], SSD-Thermal [15] and MLPD [6]. Both backbones lead to second or best perfor-mance, with an average margin of more than 5 percentage points (pp) with respect to competitors. Overall, the table emphasises the robustness of HA-MLPD to inference-time blackouts, showcasing its efficacy in scenarios encountered in real-world applications. Fig. 5 summarizes the results from Tables II and III in a more easily interpretable plot."}, {"title": "F. Ablation Study", "content": "The ablation study in Table IV investigates the impact of our contributions in inference-time blackout scenarios, specifi-cally Sides Blackout with RGB-Thermal, Sides Blackout with Thermal-RGB and surrounding blackout conditions. The table outlines the MR results for different configurations: (1) MLPD, (2) HA-MLPD without the HA mechanism (w/o HA) but with masking data augmentation, and (3) our proposed HA-MLPD. All three methods are with the VGG-16 backbone. The results demonstrate a systematic improvement with each modification, with lower MR values indicating better performance. The addition of data augmentation (Aug) results in a notable reduction in MR values across all scenarios (-3.86pp on average). Furthermore, incorporation of HA contributes to an additional improvement, achieving the lowest MR values (-5.34pp on average, compared to MLPD). Overall, the ablation study underscores the significance of both HA and data augmentation in improving the model's robustness under inference-time blackouts."}, {"title": "G. Discussion", "content": "a) Dual modality performance: The results achieved in the dual modality scenario, i.e. when both modalities are fully overlapping and available, placed HA-MLPD in the second place (username: UniLu) on the leaderboard\u00b9 at the time of submission. It is worth noting that this places our method above competitors that were specifically designed for dual modality conditions only, whence not robust to blackout scenarios. On the contrary, our method operates a necessary trade-off to cope with such challenging conditions, but still outperforms most of other methods under the normal condi-tions.\nb) Model efficiency: In our evaluation, we used two models, one with the VGG-16 backbone, which contains a total of 59.88 M parameters, and the other with the Mo-bileNetV2 backbone, which contains a total of 13.96 M with a 4x reduction in the total number of parameters. Due to its larger size, the VGG-16 backbone required more time to train per epoch with a smaller batch size, while the smaller MobileNetV2 backbone allowed faster iteration through each epoch. However, MobileNetV2 required training for more number of epochs to achieve the best results. The overall duration of the training slightly favoured MobileNetV2. Given that the MobileNetV2 backbone is smaller, it can be easily accommodated on hardware with limited resources and can deliver a higher frame-per-second (FPS) rate during inference without compromising the performance. Note that our model takes approx. 20ms (compared to 42ms for MLPD baseline) to process an image during inference on a desktop workstation\n1https://eval.ai/web/challenges/challenge-page/1247/leaderboard/3137"}, {"title": "V. CONCLUSION", "content": "In this paper, we tackle the challenges of multispectral pedestrian detection in real-world scenarios, such as adverse inference-time configurations and hardware resource limita-tions. We introduced a novel HA mechanism to mitigate per-formance degradation arising from modality-specific lack of information. During training, the HA module enables learning both generalised and discriminative features through self- or cross-attention mechanisms. A mobile-friendly backbone is also used for higher energy efficiency. Extensive experimental comparisons demonstrate that the proposed method is robust to realistic stereo conditions. Furthermore, an ablation study highlighted that both introduced HA mechanism and data augmentation play crucial roles in improving the model's robustness. Future work will include deploying the method on edge device for real-condition testing, and evaluating the HA mechanism on other modalities (e.g., depth images).\nc) Reliance on masks: Our approach relies on the masks of the blackout regions to guide the network through hybrid-attention mechanisms. The success of the proposed method depends on the accuracy of these masks. If the blackout regions are not correctly identified, the HA mechanism could be misguided, leading to suboptimal pedestrian detection per-formance."}]}