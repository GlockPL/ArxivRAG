{"title": "Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions", "authors": ["Kai Sun", "Siyan Xue", "Fuchun Sun", "Haoran Sun", "Yu Luo", "Ling Wang", "Siyuan Wang", "Na Guo", "Lei Liu", "Tian Zhao", "Xinzhou Wang", "Lei Yang", "Shuo Jin", "Jun Yan", "Jiahong Dong"], "abstract": "Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine. The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs). These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies. This review offers a comprehensive analysis of recent developments in MMFMS, focusing on three key aspects: datasets, model architectures, and clinical applications. We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows.", "sections": [{"title": "I. INTRODUCTION", "content": "THE rapid advancements in artificial intelligence (AI) have revolutionized numerous fields, with medical data analysis being one of the most profoundly impacted [1]. AI has enabled significant improvements in both diagnostic precision and therapeutic planning, making it a cornerstone of precision medicine [2]. Using the power of large-scale data and deep learning algorithms, AI systems have been increasingly capable of extracting valuable insights from complex medical data, thus enhancing clinical decision-making processes [3].\nWithin this broader AI landscape, foundation models (FMs) have emerged as a critical technological leap [4]. Unlike traditional deep learning models that are often tailored for specific tasks, foundation models are pre-trained on vast and diverse datasets, enabling them to generalize across a wide array of downstream tasks without the need for task-specific retraining. These models leverage the scale of their training data to exhibit strong zero-shot learning capabilities, which means they can effectively handle new tasks with minimal or no additional labeled data. Notable examples of foundation models, such as BERT [5], CLIP [6], and DALL-E [7], have demonstrated their ability to generalize across modalities and tasks, becoming benchmarks in the AI community.\nBuilding on the success of foundation models, the concept of large multimodal models (LMMs) has gained traction in multiple domains, including medical imaging [8]\u2013[13]. LMMS are designed to integrate and analyze data from multiple modalities-such as images, text, and clinical reports-enabling a more comprehensive understanding of complex data. This integrative capability is of paramount importance in healthcare, where precise diagnosis and effective treatment planning often require the comprehensive fusion of data from diverse imaging modalities, such as magnetic resonance imaging (MRI), computed tomography (CT), and X-ray, along with non-imaging sources like clinical reports and laboratory findings. LMMS have shown the potential to unify multimodal data, providing more holistic insights and improving the performance of various downstream tasks such as segmentation, classification, and disease detection [14].\nAs LMMs continue to evolve, the field of medical image analysis has seen the emergence of MMFMs, which are specialized models designed to address the unique challenges posed by medical data [15]. MMFMs take advantage of the multimodal nature of medical imaging and are trained in large, diverse datasets that cover multiple organs and modalities. These models offer robust generalization capabilities, making them well suited for a wide range of medical tasks, from diagnostic segmentation and classification to more complex tasks such as image registration and clinical report generation. The development of MMFMs represents a significant step forward in AI-driven healthcare, as these models can seamlessly integrate and adapt to various medical contexts.\nWithin the MMFM landscape, two prominent categories have emerged: Medical Multimodal Vision Foundation Models (MMVFMs) and Medical Multimodal Vision-Language Foundation Models (MMVLFMs). MMVFMs focus on multimodal vision tasks, such as integrating and processing different types of medical images, while MMVLFMs extend the multimodal approach by incorporating both visual and textual data, thus enabling more comprehensive analyzes that bridge the gap between images and clinical documentation. These two approaches form the foundation of MMFMs and are critical to advancing the field of medical artificial general intelligence.\nThis review provides a comprehensive exploration of the latest advancements in MMFMs, focusing on three key dimensions: datasets, model architectures, and clinical applications. First, we examine the large-scale datasets used to train these models, analyzing the impact of dataset diversity and scale on model performance. Next, we explore the architectural innovations and technical approaches employed in MMVFMS and MMVLFMs, with a focus on their multimodal capabilities. Finally, we discuss the practical performance of these models in clinical settings, highlighting both their successes and the challenges encountered in real-world medical applications. Through this detailed analysis, we aim to elucidate the transformative potential of MMFMs in medical imaging and provide a roadmap for future research. As these models continue to evolve, we anticipate that MMFMs will play a pivotal role in the advancement of medical artificial general intelligence, thus driving the next wave of innovation in precision medicine."}, {"title": "II. BACKGROUND", "content": "Foundation models represent a novel paradigm in the era of general artificial intelligence, referring to models pre-trained on large-scale datasets and capable of transferring to a wide range of downstream tasks. The core idea of foundation models originates from the long-standing development of transfer learning [4]. The rapid emergence of foundation models is driven by advancements in three key factors: computational hardware, the introduction of the Transformer architecture, and the availability of large-scale training data.\n\u2022 Computational Hardware: With advancements in graphics processing units (GPUs) and specialized integrated circuits such as tensor processing units (TPUs), large-scale parallel computing has become markedly more efficient for deep learning training. In particular,\n\u2022 Transformer Architecture: The introduction of the Transformer architecture proposed by [16], the Transformer architecture leverages the self-attention mechanism to capture dependencies within sequences, overcoming the limitations of RNN and LSTM. The self-attention mechanism enables efficient parallel processing, making the Transformer highly effective in handling long-range dependencies. Specifically, the multi-head self-attention mechanism enables the model to learn different representations from various subspaces, thereby enhancing feature learning.\n\u2022 Large-scale Training Data: In the data-driven era of AI, the proliferation of the internet and digitalization has made large-scale multimodal data (text, images, audio, video, etc.) available for model pretraining. The availability of large-scale data, coupled with the rise of self-supervised learning, has reduced the reliance on manually labeled data. Self-supervised learning, through tasks such as masked language modeling (MLM), enables models to train on unlabeled data by predicting missing parts of the input. Models like BERT and GPT have been pre-trained on large-scale text datasets via self-supervised learning, significantly improving their generalization capabilities and performance in various application scenes.\nMoreover, the representation learning of foundation models typically follows two main paradigms: (1) Supervised pretraining, where models are trained on large-scale labeled datasets (e.g., ImageNet [17]) and then applied to downstream tasks through transfer learning; (2) Self-supervised learning, where models learn from unlabeled data by performing predictive tasks. BERT and the GPT series have achieved widespread success in NLP through self-supervised learning, and this paradigm is gradually extending to computer vision.\nTogether, these factors have firmly established foundation models as a core component of current and future AI technological developments. By gaining a deeper understanding of foundation models, their potential can be harnessed more effectively in solving complex problems."}, {"title": "1) Transformer in foundation model:", "content": "The Vanilla Transformer [16] is a novel neural network architecture specifically designed to handle sequential data. The Transformer architecture relies entirely on the self-attention mechanism to capture dependencies in sequential data, making it highly effective in addressing long-range dependency problems. The Transformer consists of two main components: an encoder and a decoder, both of which are composed of multiple identical layers. Each layer contains two key sublayers: a multihead self-attention mechanism and a feed-forward neural network. In the encoder, the input sequence first passes through an embedding layer and then through multiple encoder layers to generate contextual representations. The decoder, in turn, takes the encoder's output and combines it with the target sequence embeddings to generate the final output through a step-by-step decoding process."}, {"title": "a) Self-Attention (SA) Mechanism:", "content": "The core of the Transformer success lies in the self-attention mechanism, which calculates the relevance between different positions in the input sequence to generate weighted representations. This operation is commonly referred to as scaled dot-product attention. The goal of self-attention is to capture the interdependencies between different positions in the sequence. Given an input sequence X of length N, position encoding is first applied to obtain the position-aware input sequence:\n$Z = X + P$ (1)\nwhere \u2295 denotes element-wise summation, and P is position embedding of input X.\nThe embedded sequence Z is then projected in queries Q, keys K, and values V using projection matrices $W_Q$, $W_K$, and $W_v$.\n$Q = ZW_Q, K = ZW_K, V = ZW_v$ (2)\nThe attention scores are computed by taking the dot product of Q and K, followed by a softmax operation to normalize the scores:\n$Attention(Q, K, V) = Softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V$ (3)\nwhere $d_k$ is the dimension of the key vector, used to scale the dot product to prevent excessively large values.\nThe self-attention mechanism allows each element in the input sequence to interact with every other element, including itself, enabling the Transformer to capture long-range dependencies from a global perspective, thereby improving its performance on long sequential data."}, {"title": "b) Multi-Head Self-Attention Mechanism (MHSA):", "content": "Building on the self-attention mechanism, the Transformer introduces multi-head self-attention, which is key to its ability to capture diverse feature representations. Unlike single-head attention, multi-head self-attention enables the model to compute multiple attention mechanisms in parallel, with each focusing on a different aspect of the input sequence. Each attention head is computed independently:\n$head_i = Attention(Q_i, K_i, V_i)$ (4)\nwhere each head $head_i$ is an independent attention mechanism, and $Q_i$, $K_i$ and $V_i$ are the query, key, and value vectors after independent linear transformations.\nThe outputs of all heads are concatenated and passed through a linear projection:\n$MHSA = Concat(head_1, ..., head_h) W$ (5)"}, {"title": "2) Vision Transformer (ViT) in vision foundation model:", "content": "Unlike foundation models in natural language processing (NLP), which have established a unified learning paradigm based on contextual prediction and have already captured a certain level of common sense knowledge, the development of vision foundation models is still in its early stages. There is no standardized paradigm for learning common sense in the visual world, which presents challenges in achieving generalization and robustness in feature representations. Traditional supervised learning approaches for vision foundation models rely heavily on high-quality labeled data, which limits the adaptability of the model to various tasks. In contrast, the self-supervised learning paradigm does not depend on explicit labels; it leverages methods such as contrastive learning to train models more efficiently on diverse large-scale datasets. Self-supervised learning has shown significant potential to build common sense visual knowledge. For example, contrastive learning methods such as MoCo [18] and SimCLR [19] use raw, unlabeled data by forming positive and negative pairs to learn general visual representations. This learning strategy, with its broad applicability in the visual domain, offers a promising path toward learning common sense knowledge in vision. The introduction of the Vision Transformer architecture has further unlocked the potential of vision foundation models, enhancing their ability to capture and model complex visual relationships.\nThe Vision Transformer, proposed by [20], represents a novel approach to computer vision by using the transformer architecture originally designed for sequence modeling tasks. The core innovation of ViT lies in converting images into sequences of patches, treating each patch as a token. These patches are then linearly embedded and fed into the Transformer model, where the self-attention mechanism processes the relationships among the patches. This architecture enables ViT to capture long-range dependencies in images more efficiently than convolutional neural networks (CNNs), particularly when handling large-scale, high-resolution visual data.\nA key feature of ViT is its partitioning of input images into fixed-size, nonoverlapping patches (e.g., 16 \u00d7 16 pixels), each of which is embedded into a vector. These embedded vectors, combined with positional encodings, serve as the input sequence for the transformer. By converting images into sequences, ViT can directly apply the multi-head self-attention mechanism to image data, eliminating the need for convolutional layers. This process enables ViT to capture global context across the image, which is particularly advantageous for complex image tasks.\nThe multi-head self-attention mechanism in ViT enables the model to compute representations in parallel across different subspaces for each image patch. Unlike convolution operations that rely on static filters, the self-attention mechanism dynamically computes filters based on the data, enabling ViT to adjust the receptive field flexibly. This adaptability empowers ViT to model both global and local features effectively, making it highly efficient in various visual tasks such as image classification, object detection, and segmentation. Similarly to the classical Transformer, each ViT layer also includes FFN in addition to multi-head self-attention. The FFN performs independent linear transformations and non-linear activations for each patch representation, thereby enhancing the model's ability to capture complex non-linear interactions. This design significantly improves the model's ability to generalize to complex image recognition tasks.\nViT excels at capturing long-range dependencies in images through its global self-attention mechanism, making it particularly suited for high-resolution image tasks. However, the quadratic growth in computational complexity with image resolution poses challenges for processing ultrahigh-resolution images. To address this, window-based attention mechanisms have been introduced, partitioning images into smaller windows and applying attention within each window, thereby reducing computational demands."}, {"title": "B. Multimodal medical image analysis", "content": "The integration of multimodal medical imaging has significantly advanced the technical methodologies in image analysis, improving diagnostic precision and treatment planning, which is particularly evident in the cardiovascular field, where the combination of different imaging modalities provides a more comprehensive understanding of cardiac structures and functions.\nAdvances in multimodal medical imaging have transformed clinical practice by improving diagnostic precision and treatment planning, particularly in cardiovascular medicine. Medical imaging techniques such as X-ray, computed tomography (CT) and magnetic resonance imaging (MRI) provide crucial complementary insights to the management of conditions such as coronary artery disease, heart failure, and aortic stenosis. For instance, while CT excels in identifying calcified plaques in coronary arteries, MRI is indispensable for evaluating myocardial tissue characteristics, such as fibrosis or inflammation. These complementary modalities illustrate the limitations of single-modality imaging, which often provides insufficient data for a comprehensive cardiovascular assessment. Studies have shown that the integration of multimodal imaging significantly improves outcomes in tasks such as cardiac tumor segmentation, myocardial infarction classification, and coronary artery disease diagnosis."}, {"title": "C. Large scale medical datasets", "content": "The advancement of foundational models in medical imaging critically depends on the availability of large, diverse, and multimodal datasets, which improve diagnostic precision and treatment planning accuracy of trained clinical deep learning models.\nIn clinical practice, particularly within the cardiovascular discipline, accurate diagnosis and effective treatment planning are highly dependent on detailed and comprehensive medical imaging. Another key factor that limits the representation capacity of foundational models in medical imaging is the size and diversity of available data sets. In the era of general artificial intelligence, large-scale datasets are crucial to developing robust foundational models in medicine. The quality, scale and diversity of the data directly influence the generalizability of these models, making pretraining on comprehensive datasets particularly important [62]. In the medical field, the accumulation of large-scale data serves as a rich source of knowledge for training models that can perform diverse tasks in multiple domains.\nAlthough unimodal data can provide valuable insight, the integration of multimodal data has been shown to offer significant advantages. By combining different types of information, multimodal data provide a more comprehensive perspective, leading to more accurate diagnoses, better treatment planning, and improved patient outcomes. However, the challenge lies in the acquisition and integration of multimodal data. Compared to unimodal data, multimodal data collection is generally more complex, and standardization is difficult due to the heterogeneous and intricate nature of its sources, each with unique structures and formats. This complexity increases the difficulty of data harmonization and model training. Furthermore, the scarcity of annotations in multimodal datasets exacerbates these challenges, making it crucial to optimize the use of existing data.\nGiven these challenges, it is imperative to develop methods that effectively utilize the available multimodal data to enhance the performance of foundational models in medicine, allowing better generalization between tasks and datasets. By maximizing the use of multimodal data, researchers can push the limits of foundational models, paving the way for more precise and personalized healthcare solutions.\nIn the following, we will introduce three types of dataset that we believe can be used for the training and testing of MMFMs in order of data modality: plain text datasets, medical image datasets (including segmentation masks or labels), and image-text pair datasets."}, {"title": "1) plain text datasets:", "content": "Datasets are shown in Table. I provide diverse resources for various tasks in medical natural language processing and clinical research, which can improve understanding in the medical domain for the multimodal foundation representation model. MedNLI [63] is specifically designed for natural language inference in clinical settings, helping bridge the gap between general NLP models and specialized medical applications using transfer learning and domain knowledge. SEER [64], on the other hand, focuses on epidemiological cancer data, providing extensive records on cancer incidence and outcomes, which are essential for public health and cancer research. MIMIC-III [65] stands out for its large-scale electronic health records, widely used for clinical data analysis and NLP tasks, covering structured and unstructured data. MeQSum [66] facilitates the summarization of medical queries, addressing the need for concise responses to complex medical questions, thus contributing to advancements in summarization models. Finally, HealthCareMagic [67] provides a rich dataset of medical question-answer pairs derived from online consultations, which is valuable for training healthcare-specific question-answering models."}, {"title": "2) medical image datasets:", "content": "The datasets listed in Table. II provide diverse imaging modalities, which are crucial for building medical foundation models that can generalize across various domains. MC-CXR & SZ-CXR [68] and MMR-Chest X-ray [70] focus on chest X-rays, facilitating disease classification and detection, which is fundamental for learning X-ray-based diagnostics. CBIS-DDSM-CALC & MASS [69] and MMR-Breast Ultrasound [70] offer high-quality annotated imaging datasets for breast cancer detection, advancing model accuracy in mammography and ultrasound interpretation. MMR Datasets [70] and its subcategories span multiple modalities, including pathology, retinal OCT, and dermatoscopy, contributing to multimodal model training that generalizes between specialties. Large data sets such as Eye-Found [75] and RETFound [76], with millions of ophthalmology images, support model training for the detection of retinal diseases, while specialized datasets such as MedSAM [73] and SAM-Med3D [74] provide vast image mask pairs across modalities, key to segmentation tasks. CLIP-Driven Universal Model [71] and Disruptive Autoencoders (DAE) [78] emphasize tumor segmentation, using multi-organ and multimodal data, improving 3D image segmentation models for oncology applications.\nSegmentation tasks, as exemplified by datasets such as Med-SAM, SAM-Med3D, and AbdomenAtlas, focus on delineating structures within medical images, such as organs or tumors. These tasks help models develop a detailed understanding of spatial patterns, enabling them to recognize fine-grained anatomical features, boundaries, and spatial relationships between tissues. For example, MedSAM's multimodal image-mask pairs offer essential data to refine the recognition of spatial patterns of models in diverse types of imaging, which is critical for surgical planning or oncology treatments.\nHowever, detection and classification tasks presented in datasets like MC-CXR, CBIS-DDSM, and MMR-Breast Ultrasound emphasize the recognition of high-level semantic patterns, such as the presence of disease or the classification of medical conditions. These datasets aid models in extracting meaningful features from images, which enhances their ability to identify abnormalities and classify diseases accurately. For instance, MMR-Breast Ultrasound provides data on breast cancer detection, sharpening models' semantic understanding by learning disease-related features in ultrasound images."}, {"title": "3) image-text pair datasets:", "content": "The image-text pair datasets are listed in Table. III are pivotal to advance the development of MMFMs. ROCO [79] and MedICaT [80] provide radiology images paired with captions, which facilitate tasks such as image-captioning and cross-modal learning in radiology and general medical fields. PMC-OA [81] stands out with its scale, providing more than 1.6 million biomedical image-text pairs, which support image-text retrieval and medical image classification. Datasets like ChiMed-VL [82] and FFA-IR [83] expand the capability of the model into specialized medical fields, such as Chinese medical imaging and ophthalmology, respectively, offering substantial amounts of annotated data for multimodal research. CT-RATE [86] and RET-CLIP [93] further enhance the training of models in radiology and retinal disease diagnosis, by combining patient-level data with detailed reports. Furthermore, PathVQA [91] and SLAKE [90] focus on visual question answering (VQA) in medical contexts, which is key for models that aim to integrate and interpret medical imagery along with question-based reasoning. These datasets collectively provide the diverse multimodal inputs necessary for training models that can be generalized across different medical domains and tasks."}, {"title": "III. MEDICAL MULTIMODAL VISION FOUNDATION MODELS", "content": "MMVFMs have emerged as a pivotal framework for advancing medical image analysis by leveraging multimodal data from various imaging modalities. Central to the development of these models is the use of proxy tasks during pretraining, which serve as surrogate objectives designed to expose models to diverse data representations and latent structures inherent in medical images. These proxy tasks not only enable models to generalize across different downstream tasks, but also enhance their capacity to extract clinically relevant features from complex datasets. The design of proxy tasks is critical, as it directly influences the model's ability to capture fine-grained, modality-specific nuances and cross-modal correlations, which are essential in medical imaging.\nAs illustrated in Fig. 4, proxy tasks in MMVFMs are broadly categorized into four types: Segmentation Proxy Tasks, Generative Proxy Tasks, Contrastive Proxy Tasks, and Hybrid Proxy Tasks. Segmentation proxy tasks, for example, focus on delineating anatomical structures and pathological regions, providing a foundational understanding of spatial relationships within 2D and 3D medical images. Generative proxy tasks, on the other hand, emphasize reconstructive capabilities, utilizing a mask-reconstruction strategy to challenge models with the task of recovering occluded or masked parts of an image. Contrastive proxy tasks focus on distinguishing between different modalities or anatomical regions, ensuring the model can learn robust feature representations across varied imaging types. Finally, hybrid proxy tasks combine multiple learning paradigms, optimizing model performance through a multifaceted approach that incorporates multitask and multilevel learning strategies. These proxy tasks collectively drive the success of MMVFMs, equipping them with the versatility required to excel in complex clinical scenarios, including segmentation, classification, detection, and beyond."}, {"title": "A. Segmentation Proxy Task", "content": "Segmentation tasks capture and represent critical anatomical and pathological structures in multimodal medical images that often have high clinical relevance. The primary goal of segmentation is to divide the image into regions that correspond to key structures, such as organs, lesions, or tumors. Through segmentation tasks, models learn to differentiate these structures, providing foundational visual representations for other pixel-level downstream tasks, including classification and registration. One of the primary reasons for using segmentation as a proxy task for foundation models is its strong representation capability. Segmentation tasks require models to capture both local details and global context, enabling them to extract cross-modal consistent features from multimodal data. For example, although CT and MRI images may differ in contrast and texture, the underlying anatomical structures are often consistent, and segmentation tasks help models capture this consistency across different modalities.\nWith the introduction of the SAM [94], foundation models have seen significant progress in the field of image segmentation. SAM is a general, promptable segmentation model capable of generating high-quality segmentation masks based on predefined prompts such as points, boxes, or text. Its primary advantage lies in its zero-shot learning capability, which allows it to generate accurate segmentation masks even on unseen data. The core architecture of SAM includes an image encoder, a prompt encoder, and a mask decoder. The image encoder uses a large Vision Transformer model to extract features from the input image, while the prompt encoder encodes user-provided prompts, and the mask decoder combines the image features with the prompt information to generate the corresponding segmentation mask.\nHowever, the direct application of SAM to medical image segmentation has not yielded the desired results, primarily due to significant differences in data characteristics between natural and medical images. Medical images typically require higher resolution and more precise edge detection, particularly in identifying critical anatomical structures. Furthermore, medical image datasets are relatively limited, and the cost of annotation is substantial. To better meet the requirements of medical image segmentation, researchers often need to finetune SAM or incorporate specific preprocessing steps tailored to the characteristics of medical images, thereby enhancing the foundation model's ability to represent and segment complex multimodal medical images.\nTo address existing limitations in medical image segmentation, this article introduces MedSAM [73], a foundation model specifically designed for universal medical image segmentation. The MedSAM model was trained on a large-scale medical image dataset comprising 1,570,263 image-mask pairs, which encompassed 10 imaging modalities and more than 30 types of cancer. This diverse dataset enables MedSAM to learn rich representations of medical images, capable of managing various anatomical structures and lesions across different modalities, thus improving its adaptability and performance across multiple tasks. MedSAM employs a prompt-adjustable segmentation approach that allows users to specify regions of interest by providing bounding boxes or point prompts. Compared to fully automated models, this method offers enhanced flexibility and adaptability, enabling it to accommodate a variety of task requirements and imaging modalities. In both internal and external validation tasks, MedSAM significantly outperforms existing state-of-the-art (SOTA) segmentation foundation models, such as SAM, and in many instances, it performs comparably to or even better than specialized models like U-Net and DeepLabV3+. These results underscore MedSAM's significant potential as a foundational model for multimodal, multitask segmentation.\nLikewise, SAM-Med2D [95] is a fine-tuned version of the original SAM model specifically adapted for 2D medical image segmentation. The model was trained on approximately 4.6 million images and 19.7 million masks, covering a wide range of medical imaging modalities and anatomical structures. By integrating various prompt modes (including points, bounding boxes, and masks) and fine-tuning the original SAM encoder and decoder, SAM-Med2D exhibited significant performance enhancements across multiple datasets. In particular, its extensive evaluation across nine MICCAI 2023 challenge datasets demonstrated robust generalization capabilities beyond the training data.\nDespite the strong performance of MedSAM and SAM-Med2D in most tasks, there are still areas for improvement, especially in handling complex anatomical structures or regions with unclear boundaries. Moreover, these models cannot directly handle 3D medical images, as the original SAM architecture was primarily designed for 2D natural images and is less effective in capturing spatial information in 3D medical data. These challenges highlight the need for further optimization of these models to better address the complexities of medical imaging in the future.\nThe limitations of SAM stem mainly from its design for 2D images. The original 2D structure neglects the spatial information inherent in 3D modalities, leading to a lack of deep understanding of medical images. This limitation restricts its application in fields that require three-dimensional comprehension of medical images. To overcome this limitation, SAM2 [96] extends the original SAM by introducing video input support, enabling the segmentation of any object in a video or image, even if the object or visual domain is previously unseen, without the need for custom adaptation. This makes SAM2 a more versatile tool, offering greater flexibility for various tasks. SAM2 introduces the potential to treat 3D medical images as videos, providing a solution for 3D image segmentation. In a comprehensive evaluation of SAM2 applied to medical images, researchers demonstrated both the potential and limitations of SAM2 in 2D and 3D medical image segmentation. Using a bidirectional propagation strategy, SAM2 shows significant advantages in 3D image segmentation. This strategy improves segmentation accuracy by propagating information both forward and backward from an annotated slice. MedSAM-2 [97] also follows the philosophy of treating 3D medical images as videos, developing a unique memory bank and a weighted selection strategy that allows the model to automatically segment similar objects in all subsequent images after only a single prompt. This capability was difficult to achieve with previous models, especially for image sequences without temporal relationships. MedSAM-2 can accurately segment all subsequent images with just one prompt. This method not only applies to 3D medical images, but also enables the", "Zoom-out-Zoom-in\u201d strategy, which effectively reduces computational costs while maintaining segmentation accuracy. This mechanism initially reduces the input volume size to produce a coarse segmentation, followed by a high-precision inference on the identified region of interest (ROI). This approach allows SegVol to efficiently handle large-volume 3D medical images with both speed and precision. Additionally, SegVol supports various types of prompts, including spatial prompts (e.g., points and bounding boxes) and semantic prompts (e.g., textual descriptions). The combination of these prompts significantly enhances the model's performance, especially in generalizing across datasets. In a comprehensive evaluation of 22 anatomical segmentation tasks, SegVol outperformed other competing models in 19 tasks, with improvements up to 37.24%. Despite SegVol's strong generalization and efficiency, there is still room for improvement when dealing with unseen modalities or complex tumor shapes.\nDespite the strong performance of SAM-Med3D and SegVol in general organ and tumor segmentation tasks, they still face limitations in specific clinical tasks. Supervised Finetuning (SFT), while effective at adapting to task-specific needs, often leads to the degradation of the general knowledge stored in the original foundation model. To address this issue, [102": "introduces the SAM-Med3D-MoE model, which integrates the Mixture of Experts (MoE) technology with the foundational model to successfully implement non-forgetting learning in foundational representation tasks, significantly improving the model's task adaptability and performance. SAM-Med3D-MoE innovatively incorporates MoE technology, combining task-specific fine-tuned models with the foundational model. By training a lightweight gating network to process images and prompt embeddings, the model generates confidence scores for each expert model. In addition, a selection strategy is introduced to adaptively combine the output of the foundational model and the expertise of the specific tasks, allowing the model to maintain excellent performance in specific tasks without compromising its general capabilities. Although SAM-Med3D-MoE shows excellent performance in handling complex tasks, the increase in the number of expert models may affect the training and inference speed. Future research could explore dynamically adjusting the selection strategy parameters based on specific scenarios to improve the model's adaptability.\nCurrently, segmentation models based on convolutional neural networks and Transformers are typically small in scale, with parameter sizes in the tens of millions, making it difficult to scale them up to handle larger datasets or tasks. To address this issue, [103"}]}