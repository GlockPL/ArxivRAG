{"title": "PathE: Leveraging Entity-Agnostic Paths for Parameter-Efficient Knowledge Graph Embeddings", "authors": ["Ioannis Reklos", "Jacopo de Berardinis", "Elena Simperl", "Albert Mero\u00f1o-Pe\u00f1uela"], "abstract": "Knowledge Graphs (KGs) store human knowledge\nin the form of entities (nodes) and relations, and\nare used extensively in various applications. KG\nembeddings are an effective approach to address-\ning tasks like knowledge discovery, link prediction,\nand reasoning. This is often done by allocating and\nlearning embedding tables for all or a subset of the\nentities. As this scales linearly with the number of\nentities, learning embedding models in real-world\nKGs with millions of nodes can be computation-\nally intractable. To address this scalability prob-\nlem, our model, PathE, only allocates embedding\ntables for relations (which are typically orders of\nmagnitude fewer than the entities) and requires less\nthan 25% of the parameters of previous parameter\nefficient methods. Rather than storing entity em-\nbeddings, we learn to compute them by leverag-\ning multiple entity-relation paths to contextualise\nindividual entities within triples. Evaluated on four\nbenchmarks, PathE achieves state-of-the-art perfor-\nmance in relation prediction, and remains competi-\ntive in link prediction on path-rich KGs while train-\ning on consumer-grade hardware. We perform ab-\nlation experiments to test our design choices and\nanalyse the sensitivity of the model to key hyper-\nparameters. PathE is efficient and cost-effective for\nrelationally diverse and well-connected KGs com-\nmonly found in real-world applications.", "sections": [{"title": "Introduction", "content": "Knowledge Graphs (KGs) such as Wikidata and Freebase\nserve as a structured embodiment of human knowledge in\nmachine readable format. They consist of a large number of\n(subject, relation, predicate) triples, where\nsubject and predicate (alias head and tail) are nodes in the\nKG and the relation is the edge connecting them. Each triple\ndenotes an atomic fact, such as (London, capital_Of,\nEngland). KGs are ubiquitous and are used in question an-\nswering, information retrieval [Zou, 2020], recommendation\nsystems [Guo et al., 2022] and autonomous agents [Kattepur\nand P, 2019], and they can augment Large Language Mod-\nels (LLMs) with facts and common sense knowledge [Moi-\nseev et al., 2022] from authoritative sources. However, KGs\nare usually incomplete, which means that information (nodes\nand edges) is missing from the graph, and new triples are\nadded. An effective method for KG completion is learning\nKnowledge Graph Embeddings (KGE), either by storing the\nlearned representations in embedding tables [Sun et al., 2019;\nBordes et al., 2013] or by utilising more complex Graph\nNeural Network (GNN) architectures to leverage their inher-\nent structure [Vashishth et al., 2020; Zhang and Yao, 2022].\nEmbeddings provide good performance in KG completion\ntasks [Abboud et al., 2020; Dettmers et al., 2018a] but suf-\nfer from significant drawbacks: their computation may be-\ncome intractable on large web-scale KGs (106-9 nodes); and\nthey struggle to embed unseen nodes, called inductive em-\nbedding, without being retrained from scratch. Furthermore,\nstate of the art methods based on GNNs, such as CompGCN\n[Vashishth et al., 2020], require storing the whole adjacency\nmatrix which limits their applicability to larger KGs [Zhang\nand Yao, 2022]. Specialised architectures [Zhang and Yao,\n2022; Zhu et al., 2021] have attempted to address these is-\nsues by producing entangled representations of node pairs,\nthereby removing the need for storing the adjacency matrix at\nthe expense of producing node-level representations.\nRecent work has also explored reducing the memory re-\nquirements of KGE by focusing on the relations between en-\ntities. Galkin et al. achieve scalability by allocating embed-\nding tables only for a subset of entities (alias anchors) and\nencode the others based on their distance from the anchors.\nChen et al. leverage a fixed vocabulary of embedded nodes\n(alias reserved entities) and relational context similarity, in\nconjunction with a GNN model, to improve performance and\nretain inductive capabilities. Nonetheless, these efforts still\nrequire storing embedding tables for reserved nodes.\nTo overcome these limitations, we introduce PathE, a\nparameter-efficient KGE method that departs from traditional\napproaches by storing only relation representations and dy-\nnamically computing entity embeddings. PathE leverages\npath information to contextualise nodes and their connectiv-\nity patterns, generating structure-aware entity representations\nwithout the computational overhead of message passing in\nGNNs, nor utilising any stored node representations.\nSpecifically, paths are drawn from unique random walks\nstarting or ending from/at each entity.\nEntities are encoded via their relational context, which is"}, {"title": "Related Work", "content": "defined as the number and type of the relations they appear\nwith (either as head or tail, for outgoing and incoming con-\ntexts respectively). A node projector learns to project entity-\nspecific relational contexts by forwarding this information\nthrough a series of fully connected layers; which yields an\nentity representation that has the same dimension of the re-\nlation embeddings. Entity-relation paths are then constructed\nby combining node projections and embeddings, respectively.\nGiven a triple (h, r, t), multiple incoming and outgoing paths\nfor each entity (h, t) are processed by a sequence model, and\nan aggregation strategy is applied across all the entity rep-\nresentations in each path. This yields separate embedding\nvectors for head, tail, and relation \u2013 which are trained using\na learning objective for link prediction or relation prediction.\nOverall, this provides a more scalable and inductive solution,\nas it allows the model to embed new/unseen entities without\nretraining.\nThrough extensive empirical evaluation on various KG\nbenchmarks, we demonstrate PathE's effectiveness as a novel\nparameter-efficient KGE method. It achieves state of the art\nperformance in relation prediction and remains competitive\nin link prediction on path-rich KGs, all while utilising sig-\nnificantly fewer parameters and training on consumer-grade\nhardware. Our contributions are threefold:\n\u2022 We introduce PathE, a fully entity-agnostic, path-based\nKGE method requiring < 25% of the parameters of cur-\nrent parameter-efficient methods.\n\u2022 We conduct comprehensive experiments, demonstrating\nPathE's efficiency and competitive performance, in path-\nrich graphs (FB15k-237, CodeX-Large).\n\u2022 We provide ablation studies, validating our modelling\nchoices and analysing PathE's behaviour with varying\npath quantities and lengths.\n2.1 Knowledge Graph Embeddings\nSeveral methods have been developed to perform link predic-\ntion and other KG related tasks. These can be divided into\nlogical rule mining [Lajus et al., 2020; Ott et al., 2021; Meil-\nicke et al., 2018; Meilicke et al., 2019], path-based reason-\ning [Das et al., 2018; Shen et al., 2018; Xiong et al., 2017],\nmeta-relational learning [Xiong et al., 2018; Lv et al., 2019;\nChen et al., 2019] and KGE methods [Bordes et al., 2013;\nSun et al., 2019; Nickel et al., 2016; Trouillon et al., 2016;\nDettmers et al., 2018a]. Rule mining and path-based reason-\ning methods suffer from poor scalability, given that the num-\nber of rules and paths increases exponentially with the size of\nthe graph; while meta-relational methods focus on the task of\nperforming predictions on previously unseen nodes.\nKGE methods have become the most prominent, as they\nproduce the best performance and are often used as input to\nother ML models [Ji et al., 2022; Moiseev et al., 2022]\nThe main limitation of KGE methods lies in their reliance\non entity embedding tables, leading to two major drawbacks:\nembedding table size increases with KG growth, making\nthese methods impractical for large-scale, real-world KGs\n(e.g. Wikidata currently counts 11K+ relation types and"}, {"title": "Parameter Efficient Representations", "content": "108M+ entities); and new entities require full model retrain-\ning, hindering dynamic adaptation (inductiveness).\n2.2 Parameter Efficient Representations\nRecent work [Galkin et al., 2022; Chen et al., 2023] has fo-\ncused on reducing the amount of stored information by en-\ncoding a subset of entities, thus finding a balance between\nmemory requirement and performance. Nodepiece [Galkin\net al., 2022] embeds entities as a function of their short-\nest path distance to the (pre-stored) anchor embeddings and\ntheir relational context. Although this method is more effi-\ncient than traditional embedding methods and is inductive,\nit still allocates and learns an embedding table of anchors\nwhich increases in proportion to the size of the KG. Instead,\nEARL [Chen et al., 2023] uses relations along with a fixed\nvocabulary of entity embeddings, called reserved entities, and\nthe similarity between relational contexts of reserved entities\nand every other entity to compute entity representations us-\ning a GNN model. This approach has the ability to induc-\ntively embed unseen nodes, achieves better parameter effi-\nciency and outperforms Nodepiece.\nMethods based on GNNs typically require storing the\nwhole adjacency matrix, which limits their applicability to\nlarger KGs. Recently, methods like RED-GNN [Zhang and\nYao, 2022] have improved over traditional GNN for param-\neter efficiency. However, the authors acknowledge com-\nputational issues, such as increasing the number of layers\nin the GNN, which has been observed to limit the scal-\nability of the model [Shang et al., 2024]. The scalabil-\nity of GraIL [Teru et al., 2020] for link prediction in stan-\ndard datasets and its sole evaluation on the inductive set-\nting have also been observed in [Zhang and Yao, 2022;\nZhu et al., 2021]. Similarly, SNRI [Xu et al., 2022] faces\ncomputational issues due to mining of subgraphs between the\nhead and tail of triples, and is thus limited to the inductive\nsetting [Shang et al., 2024]. Additionally, NBFNet [Zhu et\nal., 2021] and A*Net [Zhu et al., 2024] achieve very good\nperformance in link prediction by limiting the propagation of\nmessages only between paths connecting the head and tail\nnodes in a triple. Despite their performance, the latter meth-\nods do not produce individual node representations. Instead,\nthey only perform link prediction, as the representations of\nnodes are conditioned on the source node where the message\npassing begins and they cannot be easily disentangled."}, {"title": "Path-based Embedding Methods", "content": "2.3 Path-based Embedding Methods\nSignificant work has focused on using KG-mined paths to\nleverage their multi-step semantics for link and relation pre-\ndiction.\nThese include PTransE [Lin et al., 2015] which leverages\npaths up to length 3 and introduces the path constrained\nresource allocation algorithm to measure their reliability;\nPaSKOGE [Jia et al., 2018] which builds upon PTransE and\nproposes an automated way of calculating the margin hyper-\nparameter for the loss function; DPTransE [Zhang et al.,\n2018] which extends PTransE by using clustering to group\nrelation types and calculate the weights of paths, while us-\ning relation-group specific classifiers to score triples. Fur-\nthermore, [Toutanova et al., 2016], [Lin et al., 2019] and"}, {"title": "Learning Entity-Agnostic KG embeddings", "content": "[Zhou et al., 2021] all use composition operators to combine\npath elements into a single representation, [Bai and Wu, 2021;\nNiu et al., 2020] and [Li et al., 2022] first mine logical\nrules which they convert to paths and use them in con-\njunction with traditional embeddings to perform link pre-\ndiction; while [Neelakantan et al., 2015] and [Zeng et al.,\n2018] use recurrent models to combine path elements into\na single representation. Finally, [Wang et al., 2021] devel-\noped PathCon which utilises relational paths combined with\na GNN to perform relation prediction and achieves state-\nof-the-art performance on the task. Overall, all of those\nmethods either use paths in conjunction with non-scalable\nembedding methods [Lin et al., 2015; Zhang et al., 2018;\nZhou et al., 2021], or mine paths between the head and the\ntail of each triple [Lin et al., 2015; Wang et al., 2021].\nThis path mining process is inherently complex and can\nbecome intractable in larger KGs due to the sheer number of\npotential paths. Additionally, the resulting entity embeddings\nare contextualised to specific triples, necessitating the compu-\ntation of all possible representations for entities across triples.\nThis poses a challenge when, for example, discovering new\ntriples.\n3 Learning Entity-Agnostic KG embeddings\nGiven a set & of entities and a set R of relations, a Knowledge\nGraph KC (E \u00d7 R \u00d7 E) is a directed multi-relational graph\nthat stores domain knowledge in the form of triples, which are\nalso called facts [Ji et al., 2022]. Each triple (h, r, t) consists\nof a head entity h\u2208 E, a tail entity t \u2208 E and the relation-\nship r\u2208 R between those entities. We denote the number of\ntriples in a batch as Z, the number of paths used to describe an\nentity as ppe = ppt/2 (where ppt stands for paths-per-triple),\nand the size of the longest path in the batch as plen."}, {"title": "Path Generation and Representation", "content": "3.1 Path Generation and Representation\nTraining paths are created by mining random walks from\neach entity in the KG. For each entity, we attempt to mine\nN unique entity-relation paths with no loops (no nodes in the\npath appearing more than once). These paths are either outgo-\ning (starting from the node) or incoming (ending at the node)\nwith equal probability.\nMining paths for each entity in isolation allows for paral-\nlelisation, and can easily scale to large KGs.\nThese paths provide information about the neighbourhoods\nof the entities and are expected to localise them within the\nKG. Moreover, batching together multiple paths for each\nentity allows the model to extract information related to\nthe different semantics of an entity occurring in the various\npaths. For example, the path <Arnold Schwarzenegger, ac-\ntor in, the Terminator, directed by, James Cameron> and the\npath <Arnold Schwarzenegger, winner of, Mister Olympia,\nyear, 1980> provide very different information for the en-\ntity Arnold Schwarzenegger, with each possibly being less or\nmore useful depending on the task at hand."}, {"title": "Model Architecture", "content": "3.2 Model Architecture\nPathE consists of four modules which are trainable end-to-\nend: the node projector, which maps entities into a continu-\nous space based on their relational context; the path sequence\nmodel, which processes batches of entity-relation path se-\nquences and produces contextual representations of entities;\nthe aggregator, which aggregates the node representations\nfrom different paths into a single contextualised representa-\ntion; and, finally, the prediction heads, which produce a score\nfor each triple or relation depending on the task (we provide\nseparate heads for relation and link prediction). Our model is\nillustrated in Figure 1 and described as follows."}, {"title": "Node Projector", "content": "3.3 Node Projector\nFor the node projector, we utilise a two-layer MLP which\ntakes as input the local adjacency matrix of the node-edge"}, {"title": "Path Modelling", "content": "graph. The node-edge graph is a weighted graph created by\nconverting the edges to nodes and adding a directed relation\nfrom each edge to a node with weight equal to the number of\ntimes this edge appears in the relational context of the node.\nWe utilise a separate projector for the incoming and outgo-\ning contexts. The input of the MLP is the adjacency matrix\nAer and the output is a matrix P \u2208 R|E|\u00d7d resulting from an\naffine transformation with non-linear activation:\n$P = W_2Relu(W_1Aer + b_1) + b_2$.  (1)\nThe representations produced for the incoming Pin and\noutgoing Pout contexts are then fused together through a two-\nlayer MLP and projected to P\u2208 R|E|\u00d7d as follows:\n$P = W_2Relu(W_1(P_{in}|P_{out}) + b_1) + b_2$, (2)\nwhere denotes the concatenation operator among tensors,\nand d is the embedding dimension.\n3.4 Path Modelling\nPath modelling is operated via a self-attention layer (specif-\nically, a Transformer encoder) on a batch of projected paths\nB\u2208 Rpaths\u00d7plen\u00d7d, where paths = Z \u00d7 ppe \u00d7 2. At this\nstage, each path consists of nodes and edges which are pro-\njected into a continuous space Rd using the node projector\nand an embedding layer for relations, respectively.\nTo help the model localising the head and tail entities\nwithin the paths, a learned positional encoding is added to\neach embedding vector in the path, based on its position in the\nsequence. For instance, for an entity path eo,..., eg where\nei \u2208 E where the head appears as the 5th element, we con-\nsider the following positional encodings for this path\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n[5, 4, 3, 2, 1, 2, 3, 4, 5, 6]\nand add the corresponding embedding of each position (in-\nstead of using traditional positionals [1, 2, ..., 10]). These\nentity-focused positional encodings can be seen as a path-\nlevel contextualisation of [Devlin et al., 2018], which in turn\nimproves the cyclical positional encoding of [Vaswani et al.,\n2017]. This also comes with the advantage of potentially\nlearning to discount the contribution/relevance of entities that\nare far from the head or tail. Paths are then passed through\nthe Transformer encoder, which attends to all the elements of\neach sequence and produces the path-contextualised projec-\ntions (no masking is necessary)."}, {"title": "Path Aggregator", "content": "3.5 Path Aggregator\nAs shown in Figure 1, after paths are passed through the\nTransformer encoder, the output representations are a tensor\nof shape Bout \u2208 Rpaths\u00d7plen\u00d7d, where, for each path, ev-\nery entity and relation has an embedding vector of size d.\nFrom the output, the entity representations of the head and\ntail are selected from Bout, resulting in two tensors denoted\nas Bhead, Btail \u2208 RZxppexd. Each tensor thus contains\nall the embeddings of the same entity from its ppe paths.\nThese representations are then aggregated into a single d-\ndimensional representation for each entity. For this, we ex-\nperiment with three aggregation strategies: (i) averaging all"}, {"title": "Training Objective", "content": "entity vectors; (ii) using a bidirectional recurrent encoder\nwith LSTM units [Hochreiter and Schmidhuber, 1997]; or\n(iii) using a Transformer encoder. Of the three approaches,\nthe averaging baseline is the simplest and uses no learned\nweights to perform the aggregation, while the others learn\nto aggregate the entity vectors and use separate encoders for\nhead and tail entities.\nIn contrast, the Transformer aggregator takes as input the\nsequence of the head embeddings concatenated with the se-\nquence of the tail embeddings, in addition to an aggregation\ntoken (denoted as ail in Figure 1) which is randomly ini-\ntialised. This is expected to aggregate and contextualise the\nentity embeddings. The output of the aggregator is a tensor\nof shape RZ\u00d7d for head and tail entities respectively.\n3.6 Training Objective\nThe ability to make the aggregated representation expressive,\nand capable to be used in KG completion tasks, depends on\nthe training objective. This is designed on top of the path ag-\ngregator, and its formulation currently depends on the type of\ninvariance that representations are expected to have. In our\ncase, as the goal is to find missing links in the KG, we fo-\ncus on relation prediction and true triple classification as a\nsurrogate task for link prediction. Relation prediction aims at\npredicting the relation(s) that may exist between head and tail\nentities \u2013 hence completing the triple (h,?, t). Link predic-\ntion is of more general scope, as it aims at predicting either\nthe head entity h given the incomplete triple (?, r, t); or anal-\nogously, the tail entity t from (h, r,?).\nTo accomplish this, we implemented two distinct heads\natop the path aggregator as separate training objectives. Cur-\nrently, PathE is trained using either of these heads, contingent\nupon the downstream task. We leave the investigation of both\nheads for multi-task learning as future work."}, {"title": "Relation Prediction Head", "content": "Relation Prediction Head\nOnce the path contextualised representations of the head and\ntail entities have been obtained, they are concatenated and the\nresulting matrix F \u2208 RZ\u00d72d is passed through a linear layer\nwhich outputs a score matrix S \u2208 RZ\u00d7|R| with the score of\neach (head, tail) for each relation. This yields a probabil-\nity distribution over all the possible relations in E, given the\nhead and tail embeddings. As this is a multi-class classifica-\ntion task, the relation prediction head uses the Cross Entropy\nloss between the model's prediction and the true relation."}, {"title": "Link Prediction Head", "content": "l(x, y) = L = {11,...,lv}T,  (3)\ns.t. In = -Wyn log$\\frac{exp(x_{n,yn})}{\\sum_{c=1}^{C} exp(x_{n,c})}$. (4)\nCross Entropy has already been demonstrated to be effec-\ntive for this task [Ruffinelli et al., 2020].\nLink Prediction Head\nFor link prediction, we train for true triple classification as\na surrogate task. This is done by concatenating the head, re-\nlation, and tail embeddings in a single tensor of dimension\nd \u00d7 3 representing the whole triple (h, r,t); and stacking a\nfully connected layer for binary classification. In other words,\nthe head predicts whether the triple is in the training set (pos-\nitive triple) or not (negative triple). Negative triples are con-\nstructed by head and tail corruption: the former creates new\n(negative) triples by replacing h with other entities while\nkeeping relation r and tail t unchanged\u00b9; whereas tail corrup-\ntions are created analogously by fixing h, r while changing t.\nFigure 2 illustrates an example partition of a training batch.\nAfter sampling N head and N tail corruptions, the model\nis trained to classify each triple as positive or negative. To\nbalance the classification task, the Binary Cross Entropy loss\nequally weighs the contribution of positive and negatives.\nThis is done by dividing the sum of the negative losses by\nN \u00d7 2 as per [Zhu et al., 2021]. In line with the literature, we\nalso experiment with Cross Entropy and the self-adversarial\nnegative sampling loss proposed in [Sun et al., 2019]."}, {"title": "Experiments", "content": "4 Experiments\nTo evaluate our method while addressing the challenges out-\nlined in the introduction, we focus on the following research\nquestions: (RQ1, Encodings) To what extent can we learn\nparameter efficient KG embeddings by only encoding rela-\ntionships and paths? (RQ2, Path Learning) How can we best\nleverage entity-relation paths to encode the KG structure and\nlearn informative representations for link prediction tasks?\nand (RQ3, Path Setup) How does the path length and the\nnumber of paths per triple influences model performance?\nTo address RQ1, we train a grid of models on common\nKG benchmarks and compare performances with baselines\nand state-of-the-art KGE methods for transductive link pre-\ndiction, inductive link prediction and relation prediction.\nMultiple configurations of PathE with different number of\npaths and entity aggregation strategies are also tested to trace\nthe contribution of each component related to the use of paths\n(RQ2). Finally, we experiment with varying number of paths\nper entity and visualise the custom positional embeddings to\nstudy the influence of the path number and length (RQ3).\n4.1 Experimental Setup\nIn line with the literature [Galkin et al., 2022], we chose four\nbenchmark datasets, FB15k-237, YAGO3-10, CoDEx-Large"}, {"title": "Transductive Link Prediction", "content": "and WN18RR to evaluate our model on KGs of various sizes\nand characteristics (c.f. Table 4). FB15k-237 [Toutanova\net al., 2015] and CoDEx-Large [Safavi and Koutra, 2020]\nare derived from Freebase and Wikidata respectively, while\nWN18RR (from WordNet) and YAGO3-10 focus on more\nspecific domains like lexical relations and person attributes.\nWe compare our model with both state-of-the-art and pa-\nrameter efficient KG embedding models, including RotatE\n[Sun et al., 2019], NodePiece [Galkin et al., 2022] and EARL\n[Chen et al., 2023]. We also include a NodePiece model with-\nout anchors as it is the only method that, like PathE, is fully\nentity agnostic (it does not encode any anchors/reserved enti-\nties) and maintains parameter efficiency and inductiveness.\nEvaluation. All models are evaluated on link and rela-\ntion prediction, using the original train, validation, and test\nsplits. We report Mean Reciprocal Rank (MRR) and Hits@K\nin the filtered setting [Bordes et al., 2013]. Both these met-\nrics are computed using the scores of the triples produced by\nthe model and evaluated by the ranking induced from those\nscores. Hits@K measures the ratio of true triples that are\nranked among the top K, whereas the MRR averages the re-\nciprocal ranks of true triples and drops rapidly as the ranks\ngrow. These measures are computed by sampling N \u00d7 2 cor-\nruptions (negative triples) for each positive: N negatives for\nhead corruptions, by replacing the head with other entities in\n|E|; and N negatives for tail corruptions, which is analogous\nto the former case. In our experiments, we use the full set\nof entities & to produce corruption for the evaluation of our\nmodel. Furthermore, we reuse the Effi metric proposed in\n[Chen et al., 2023] to quantify the efficiency of models as\nperformance cost. This is calculated as MRR/M(P), where\nM(P) denotes the number of trainable parameters. For all the\ncompared models, we report parameter count and prediction\nmetrics from their corresponding articles.\nImplementation. Our model is implemented in PyTorch\nv2.1 [Paszke et al., 2019] using PyTorch Lightning 2.1 and\nPyKEEN v1.1 [Ali et al., 2021]. Experiments were run on\nan Intel Core i9-13900 with 128GB RAM and an NVIDIA\nRTX 3090 GPU. All models are trained with an early stop-\nping criterion (patience at 10, min delta at 0) and use 99\nnegative triples for validation. The code can be found at\nhttps://github.com/IReklos/PathE."}, {"title": "Transductive Link Prediction", "content": "4.2 Transductive Link Prediction\nWe trained a grid of PathE models with the Link Prediction\nhead, computing MRR and Hits@K by ranking each true\n(test) triple against all its head and tail corruptions. Optimal\nmodels were found via random search, sampling 50 configu-\nrations from a search space for FB15k-237 and WN18RR due\nto their size. The best hyper-parameters for CoDEx-Large\nand YAGO3-10 were derived from these results.\nResults are given in Table 1 for all benchmarks. Over-\nall, our model outperforms Nodepiece w/o anchors on all\nbenchmarks and achieves competitive performance to Node-\npiece (with anchors) on FB15k-237 and CoDEx-Large, while\nrequiring less 25% of the parameters. More precisely, the\nMRR on FB15k-237 is only 0.04 less than Nodepiece with\nanchors and 0.012 more than Nodepiece w/o anchors while\nusing less than 10% and less than 25% of the parameters re-"}, {"title": "Relation Prediction", "content": "spectively; which confirm PathE as the most efficient model\n(Effi = 1.03) compared to EARL (Effi = 0.17) and\nNodePiece without anchors (Effi = 0.15). On CoDEx-\nLarge, our model outperforms Nodepiece w/o anchors in\nMRR by 0.081 and only has a deficit of 0.046 compared\nto Nodepiece with anchors; thus recording Effi = 0.21\ncompared to Effi = 0.10 for EARL and Effi = 0.105\nfor Nodepiece w/o anchors. On WN18RR and YAGO3-10,\nPathE's performance lags behind Nodepiece and EARL. We\nhypothesise this is attributed to the datasets' characteristics,\nspecifically the limited number of distinct relations (11 and\n37, respectively). This scarcity hinders the unique encoding\nof KG nodes, affecting the model's ability to differentiate be-\ntween them. Despite this limitation, PathE demonstrates su-\nperior performance to Nodepiece w/o anchors on MRR across\nboth datasets. Notably, PathE achieves over 6 times the per-\nformance on WN18RR, with an efficiency of 0.10 compared\nto 0.04, and more than 2\u00d7 the performance on YAGO3-10,\nreaching an Ef fi of 0.25 compared to 0.05.\nDespite the size of CoDEx-Large (2\u00d7 more training triples\nand 5\u00d7 more entities than FB15k-237) we recall that the pa-\nrameter budget of our model scales linearly with the num-\nber of relations (69 for CoDEx, 237 for FB15k). The\nnearly tripling of the parameter count of the model trained\non CoDEx-Large is due to the use of embeddings of dimen-\nsionality d = 128 instead of d = 64 for FB15k-237 and the\nuse of two encoder layers in the path modelling transformer\nand the aggregator module. Instead, Nodepiece and EARL\nare affected by the number of entities, as they both allocate\nembedding tables for a subset of them.\nDetails on the hyper-parameter settings and best configura-\ntions are provided in Appendix B. The results of the inductive\nlink prediction experiments are presented in Appendix C.\n4.3 Relation Prediction\nTo evaluate the representations of our model for relation\nprediction, we train and evaluate with the associated head.\nThis is only done for FB15k-237 and WN18RR, as these are"}, {"title": "Ablation Study", "content": "the only benchmarks where a parameter-efficient method has\nbeen evaluated and reported in [Galkin et al.", "2022": ".", "prediction": "FB15k-237 and CoDEx-Large.\nThe ablation dimensions are summarised as follows:\n\u2022 w/o Aggregator, by replacing the Transformer Encoder\nwith a simple averaging operation over the entity em-\nbeddings of different paths.\n\u2022 w/o Multiple paths, where we use only 1 path per entity\nin each triple (1 for the head, 1 for the tail), sampled\nrandomly. Hence, no aggregation is necessary.\n\u2022 w/o Entity-focused positional encodings, where posi-\ntional information is injected by adding the relative po-\nsition of each element in the"}]}