{"title": "Normative Evaluation of Large Language Models with Everyday Moral Dilemmas", "authors": ["Pratik S. Sachdeva", "Tom van Nuenen"], "abstract": "The rapid adoption of large language models (LLMs) has spurred extensive research into their encoded moral norms and decision-making processes. Much of this research relies on prompting LLMs with survey-style questions to assess how well models are aligned with certain demographic groups, moral beliefs, or political ideologies. While informative, the adherence of these approaches to relatively superficial constructs, beliefs, and moral questions tends to oversimplify the complexity and nuance underlying everyday moral dilemmas. We argue that auditing LLMs along more detailed axes of human interaction is of paramount importance to better assess the degree to which they may impact human beliefs and actions. To this end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the \"Am I the Asshole\" (AITA) community on Reddit, where users seek moral judgments on everyday conflicts from other community members. We prompted seven commonly used LLMs, including proprietary and open-source models, to assign blame and provide explanations for over 10,000 AITA moral dilemmas. We then compared the LLMs' judgments and explanations to those of Redditors and to each other, aiming to uncover patterns in their moral reasoning. Our results demonstrate that large language models exhibit distinct patterns of moral judgment, varying substantially from human evaluations on the AITA subreddit. LLMs demonstrate moderate to high self-consistency but low inter-model agreement, suggesting that differences in training and alignment lead to fundamentally different approaches to moral reasoning. We further observe that an ensemble of LLMs, despite individual inconsistencies, collectively approximates Redditor consensus in assigning blame. Further analysis of model explanations reveals distinct patterns in how models invoke various moral principles, with some models showing greater sensitivity to specific themes such as fairness or harm. These findings highlight the complexity of implementing consistent moral reasoning in artificial systems and the need for careful evaluation of how different models approach ethical judgment. As LLMs continue to be used in roles requiring ethical decision-making such as therapists and companions, careful evaluation is crucial to mitigate potential biases and limitations. Despite the capacity of LLMs to analyze moral dilemmas, their judgments ultimately lack the ethical accountability of human deliberation, requiring careful scrutiny and reflection on their role in ethical discourse.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) now shape critical decisions across society, from filtering online harassment to screening job candidates to triaging patient care [31]. Furthermore, LLMs are increasingly used as placeholders for personal relationships and management of emotional and mental healthcare [35, 41, 43]. Each of these applications requires AI systems to make complex moral judgments, weighing competing interests and interpreting social norms in contexts that directly impact human lives. This growing role in moral decision-making is complicated by how LLMs acquire their reasoning patterns from human-generated text, potentially amplifying existing biases in how different communities judge right and wrong [48]. Given evidence that divergent moral frameworks contribute to societal polarization [12, 22], understanding how these systems encode and transmit moral judgments is crucial.\nThe challenge of aligning AI systems with human ethical standards is complicated by how moral principles vary across cultures and communities. LLMs learn these principles from training data that often overrepresents specific demographic groups and cultural perspectives [15, 38]. Moreover, research suggests that LLMs tend to reflect the ideological perspectives of their creators [13], raising questions about efforts to develop supposedly \"unbiased\" AI systems. These combined effects \u2013 biased training data and encoded creator perspectives \u2013 shape how LLMs evaluate moral situations [47], potentially causing them to systematically favor certain ethical frameworks over others. As these systems increasingly participate in moral discourse and decision-making, understanding their inherent biases becomes essential for anticipating their societal impact.\nRecent work in AI development has increasingly focused on evaluating and controlling how language models encode social and moral norms. Companies developing these models employ various benchmarks to assess potential biases, such as the Bias Benchmark for QA [6], though such metrics often reflect specific cultural contexts like US English-speaking populations. While efforts to reduce problematic content have intensified, they can lead to overcorrection \u2013 as demonstrated by Google's Gemini image generation model producing historically inaccurate images in an attempt to increase diversity. This attention to bias and normative behavior has paralleled broader research in Al alignment, which examines how to design Al systems that act in accordance with human values and ethical principles [27, 67]. However, these efforts raise fundamental questions about whose values should be encoded and how competing moral frameworks should be balanced.\nWhile previous research has examined LLMs' moral reasoning through survey items and constrained scenarios [60], real-world moral dilemmas often involve complex social contexts that cannot be reduced to simple moral principles. Evaluation of LLMs with more complex scenarios is crucial to better understand their encoded beliefs and decision-making when deployed with real humans. In this work, we conduct a normative evaluation of LLMs using everyday moral dilemmas. We use the \"Am I The Asshole\" (AITA) community on Reddit, which provides a rich dataset of moral dilemmas that capture nuanced social dynamics and competing perspectives. We analyze how seven different LLMs assign and explain moral blame when evaluating over 10,000 AITA scenarios. We further compare the moral reasoning of LLMs to comments made by Redditors evaluating the same dilemmas. Our analysis reveals significant variations in how LLMs approach moral reasoning, from their blame assignment to their invocation of different moral principles, highlighting the challenges and importance of implementing consistent ethical judgment in artificial systems."}, {"title": "2 RELATED WORK", "content": "The challenges of encoding implicit moral values in language directly impact the development and behavior of LLMs. Al companies aim to either implicitly or explicitly align their large language models to a set of moral values in order to guarantee outputs consistent with those values [7, 8, 18, 62]. Given the scale and complexity of the input data, model architecture, and output distributional space, a long line of work has sought to determine what norms and values can be elicited from large language models [4, 16, 40, 42].\nA common evaluative approach is through social surveys: practitioners prompt an LLM using a survey crafted around a certain construct, and the resulting responses can be interpreted as a distillation of the LLM's values. In this vein, much work has aimed to evaluate the morality of LLMs. Several works, notably Abdulhai et al., use variations of the Moral Foundations Questionnaire (MFQ), based on Moral Foundations Theory [2, 25, 30]. Ji et al. created a new benchmark evaluation dataset using MFQ and Moral Foundations Vignettes [19] to focus on scenarios from everyday life [36]. Moral Foundations has been used a tool to evaluate LLM responses [33, 46, 57]. Meanwhile, other papers have focused on values, using datasets such as ETHICS [34] and Schwartz's Basic Values Theory [24].\nMeanwhile, other works have devised their own datasets to expand upon existing survey-based studies. Santukar et al. devised OpinionQA with the goal of measuring alignment to demographic groups using opinion questions [53]. Yuan et al. created a large dataset designed to test social norms [65]. Garcia et al. considered explicit moral scenarios with human evaluations of LLM responses [28]. Buyl et al. developed a dataset of historical figures to assess alignment influenced by the ideology of a model's creator [14]. Ren et al. constructed ValueBench, based on psychometric inventories [49].\nA similar line of work has aimed to gauge the political preferences of LLMs. These works have generally relied on political orientation tests. The most common test used is the Political Compass Test [32, 50, 52], but others include iSideWith, Dark Factor [51], and IDRLabs ideology tests [26]. Notably, Rozado considers a battery of 15 different political orientation tests, including the aforementioned ones [50]. These works have generally found that LLMs lean liberal or libertarian in their survey responses, with some variation across studies and tests [44].\nMore recent work has identified potential issues in the robustness of survey-based approaches to evaluate LLMs. These works have pointed out that LLMs are not necessarily robust to multiple choice inputs [68], morally inconsistent [11], and are not vulnerable to prompt perturbations [55, 61, 64]. On the evaluation side, multiple works have pointed out that the use of multiple-choice surveys or relatively simple scenarios is not reflective of practical usage of LLMs, limiting the generalizability of their conclusions [17]. Some works have sought to address these issues [17, 52], but there remains significant room for improvement in developing evaluation frameworks that better capture the complexities of real-world interactions with LLMs in a robust and reliable manner.\nIn spite of these efforts, it remains unclear whether the answers given by LLMs on ethics questionnaires, which measure their intentions, actually reflect their preferences when faced with complex, unstructured data. Our work contributes to this body of knowledge by focusing on naturally occurring ethical discussions in online communities to better understand how LLMs handle real-world moral dilemmas. We build on past work using Reddit as a vehicle to understand norms and beliefs in communities [20, 29, 45, 60] by incorporating LLMs into the evaluation pipeline. In particular, we rely on the work of Yudkin et al. [66], who developed a moral framework for AITA using qualitative analysis on a large sub-corpus of AITA submissions."}, {"title": "3 METHODS", "content": "3.1 Reddit and r/AmltheAsshole\nReddit is a public social media platform containing user-created and user-moderated communities called subreddits centered on specific topics. The platform serves an estimated 73 million daily users who post content on more than 100,000 active subreddits. Within Reddit, \"r/AmItheAsshole\u201d (which we refer to as AITA) is a subreddit that negotiates moral and normative dilemmas arising from everyday situations, ranging from broken promises to privacy violations. AITA includes clearly situated identities and interaction order. The Original Poster (OP) writes a submission describing a social situation involving a moral dilemma. The other subreddit members are allowed to comment on the submission, indicating whether they believe that the OP was morally at fault in the situation. The community uses particular phrases to indicate this evaluation: YTA or \"You're The Asshole\", NTA or \"Not the Asshole\", NAH or \"No Assholes Here\", ESH or \"Everyone Sucks Here\" and INFO or \"More information needed\". Users can upvote and downvote comments indicating whether they agree or disagree with the moral assessment. The comment with the highest \"score\" (number of upvotes minus downvotes) is deemed the official verdict of the community for that submission.\n3.2 Data Procurement and Preprocessing\nWe obtained Reddit submissions, and the corresponding comments, to r/AmItheAsshole from October 1, 2022 to March 26, 2023 using Pushshift.io and the Python package PRAW [10]. We chose this date range since, at the time of acquisition, they were the most recent posts that would not be in the training data of the models we tested, to the best of our knowledge. We obtained both the submissions, or posts made by users explaining their moral dilemmas, and top-level comments, or comments to a submission that do not reply to another comment. Top-level comments generally contain verdicts (though not all do). Among top-level comments, we distinguish the top comment, which has the highest score for that submission. The verdict of the top comment, according to the rules of the subreddit, acts as the representative verdict for that submission. We obtained a top comment for each submission, and as many top-level comments as we could for each submission (though we cannot guarantee that Pushshift.io was able to generate all top-level comments). Notably, we obtained these submissions and comments prior to Reddit restricting their API access in April 2023.\nWe obtained a dataset with 13,205 submissions and 531,813 comments. We proceeded with a series of preprocessing steps to ensure the dataset consisted of high-quality submissions with engagement of Redditors, since we aimed to use the Redditor comments in our analyses. First, we restricted our analysis to submissions with a score (number of upvotes minus number of downvotes) of at least 25 (top 92% of submissions), as submissions with a lower score were often of insufficient quality or too new for enough Redditors to comment. We further removed submissions that were either deleted by the user, removed by Reddit administrators, or had a top comment that was deleted or removed. Lastly, we removed the bottom 1% of submissions by character length (minimum of 300 characters) and the bottom 2% of submissions by top comment character length (minimum of 25 characters). For the comments, we only included those that included an explicit verdict, which we determined using a complex regular expression that tested for a variety of verdict structures. After preprocessing, our final dataset consisted of 10,826 submissions and 476,183 comments.\n3.3 Large Language Model Prompting\nWe prompted several large language models (LLMs) to assign blame in all submissions by presenting them with the full text of each submission and a system prompt. The system prompt, shown in Appendix A, asks the LLM to evaluate the provided submission. The model is asked to provide both a verdict, for which it can choose one of the 5 verdicts defined by the subreddit (Section 3.1), and reasoning, restricted to one paragraph. The definitions for each verdict in the system prompt are taken directly from AITA. For each model, we used the default hyperparameters, notably using a low but non-zero temperature. We argue that this approach allows the model to explore the response space that best reflects its alignment, and that ordinary use cases of these models will typically involve default parameters as well.\nWe prompted 7 models: GPT-3.5, GPT-4 [3], Claude Haiku [1], PaLM 2 Bison [5], Llama 2 7B [59], Mistral 7B [37], and Gemma 7B [58]. For the first four models, we used the corresponding API to run queries and obtain model outputs for each submission. For the latter four models, which are open-source, we obtained their weights from HuggingFace and ran the queries using our own GPU. We chose smaller sizes for the open-source models in order to accommodate our limited computational capacity. We used model checkpoints whose training data did not overlap with the window of posts we considered (though not all models provided this information). We submitted the same system prompt across all models, and included the submission as the first user message. Refusal occurred in less than 0.1% of queries. In these instances, we either re-ran the query, or set the verdict as INFO, depending on the content of the refusal.\nLastly, we conducted repeated runs to evaluate consistency of the model outputs [54]. For each model, we ran the prompt 3 times for each submission (except GPT-4, which we only ran twice due to cost considerations). In total, we obtained a dataset of 10, 826 \u00d7 7 \u00d7 3 verdicts with corresponding reasonings.\n3.4 Classification of Moral Themes\nWe applied the moral dilemma catalog developed by Yudkin et al. who identified six major themes in AITA posts: Fairness & Proportionality, Feelings, Harm & Offense, Honesty, Relational Obligation, and Social Norms [66]. Yudkin et al. provide a dataset of over 300,000 submissions to AITA (with no overlap on the dataset we used), each labeled according to the presence of the aforementioned moral themes. We obtained the text for these submissions using PRAW and fine-tuned 6 RoBERTa models to predict each moral theme given the text. We then applied each of the moral prediction models to both the AITA submissions and the AITA reasonings (including the top comments from Redditors) to discern which moral themes were present in the moral evaluations. We converted the RoBERTa probability outputs to binary labels using a 0.50 threshold. We qualitatively examined roughly 100 predictions, finding them to be of sufficient quality to proceed with secondary analyses."}, {"title": "4 RESULTS", "content": "We prompted several large language models (LLMs) to assign blame in moral dilemmas according to the framework provided by the AITA subreddit. We asked LLMs to render one of five verdicts in AITA scenarios: (i) \"Not the Asshole\", or NTA, where the original poster (\u201cOP\u201d) is not at fault, and the other party in the scenario is at fault; (ii) \"You're the Asshole\", or YTA, where the OP is at fault, and the other party in the scenario is not at fault; (iii) \"No Assholes Here\", or NAH, where neither the OP nor the other party is at fault, (iv) \"Everyone Sucks Here\", or ESH, where both OP and the other party are at fault, and (v) INFO, where not enough information is provided to assign fault to any party.\nWe prompted the LLM with a system message to evaluate moral dilemmas on social media, adopting the exact language from the AITA subreddit to specify verdict options (see Appendix A). The system message also directed the LLM to provide a one-paragraph explanation for its chosen verdict. The LLM was given the Reddit submission as the user message without preprocessing. Our dataset included 10,826 AITA subreddit posts from October 2022 to March 2023, selected to ensure they were not part of the LLMs' training data. From the LLMs' responses, we compiled a dataset of 10,826 LLM-assigned verdicts and reasonings, along with Redditors' verdicts and reasonings for each post.\nWe queried seven LLMs: GPT-3.5 (OpenAI), GPT-4 (OpenAI), Claude Haiku (Anthropic), PaLM 2 Bison (Google), Llama 2 7B (Meta), Mistral 7B (Mistral), and Gemma 7B (Google). We selected these models to ensure adequate coverage of distinct companies developing LLMs, inclusion of open-source models, coverage of varying model sizes, and balancing costs of issuing API queries. See Section 3.2 for further details on data extraction, preprocessing, and LLM queries.\n4.1 Large language models exhibit diverse opinions when assigning blame in moral dilemmas\n4.1.1 Example AITA Post. To contextualize the nature of AITA posts and LLM responses, we provide an example AITA post and the corresponding judgments by Redditors and LLMs (Table 1). We selected this post because 5 different verdicts were used and due to the submission's relatively short length. However, this level of disagreement is not necessarily typical of other submissions. The submission is shown at the top of Table 1, with each model, verdict, and reasoning displayed in the following rows. We first observed language differences between the Redditors and the LLMs, and some distinct language patterns among the LLMs. Most LLMs identified the underlying tension between the sister's desire for support and opportunities for growth; each model simply sided with a perspective based on this tension. Most LLMs conducted a cursory moral analysis of the issue, without inferring additional emotional context from the detail of Plan B provided by OP. Meanwhile, the Redditor's entire judgment was based on this detail. We discuss the potential for further qualitative analysis of this dataset in Section 5, and proceed to quantitative analyses of the verdicts.\n4.1.2 Verdict Distributions. We compared the distribution of verdicts assigned to the 10,826 submissions by Redditors and LLMs . Most posts (76.4%) were assigned NTA by Redditors with a smaller fraction (17%) assigned YTA. The remaining labels-NAH, ESH, and INFO-comprised a small fraction of the verdicts (roughly 2% each). On the other hand, the LLMs exhibited strikingly different verdict distributions. GPT-4 exhibited the most similar verdict distribution to the Redditors, though with markedly lower usage of the NAH, ESH, and INFO labels (NTA: 75%, YTA: 12.7%, NAH: 0.06%, ESH: 0.05%, INFO: 0%). Meanwhile, GPT-3.5 assigned comparatively fewer posts as NTA, while assigning more posts YTA and ESH (\u039d\u03a4\u0391: 55.3%, \u03a5\u03a4\u0391: 31.3%, NAH: 0.5%, ESH: 10.5%, INFO: 2.4%). Claude Haiku and PaLM 2 Bison had similar verdict distributions, assigning a higher proportion of posts as NTA (86.9% and 88.1%, respectively) and a lower proportion as YTA (8.3% and 7.3%, respectively). Interestingly, Llama 2 and Mistral had the most noticeably distinct verdict distributions. Llama 2 assigned almost no posts as NTA, instead assigning the vast majority of posts as YTAwith the remaining categorized as NAH and ESH (NTA: 1%, YTA: 79.2%, NAH: 15.1%, ESH: 5.1%, INFO: 0%). In contrast to all other models, Mistral relied heavily on the NAH, ESH, and INFO labels, rarely assigning YTA or NTA (NTA: 7.6%, YTA: 6.2%, NAH: 39.4%, ESH: 35.5%, INFO: 11.3%). Lastly, Gemma assigned comparatively fewer posts NTA, but used ESH more than other models (\u039d\u03a4\u0391: 60.8%, YTA: 14.7%, NAH: 0%, ESH: 24.4%, INFO: 0%).\n4.1.3 Model Agreement at the Dilemma Level. We examined the extent to which models provided similar or differing verdicts at the dilemma level. We measured inter-model agreement using Krippendorff's alpha, which ranges from -1 to 1: $\\alpha$ = 1 indicates perfect agreement, $\\alpha$ < 0 indicates systematic disagreement, and $\\alpha$ = 0 signifies no agreement beyond chance [39]. Overall, we observed low agreement or systematic disagreement among the models. We note that low annotator agreement may be reasonably expected in this setting, given many moral dilemmas lack a clear verdict. However, comparing model pairs allows us to better understand patterns and consistency in moral reasoning.\nWe observed that the model with the highest agreement with the Redditors was GPT-4 ($\\alpha$ = 0.34), in correspondence with the results in Figure 2. Additionally, GPT-4 exhibited a similar level of agreement with GPT-3.5, Claude Haiku, and PaLM 2 Bison. This similarity may reflect that these models are all proprietary and trained with comparable alignment objectives. We note that while GPT-3.5 showed moderate agreement with GPT-4 ($\\alpha$ = 0.31), its agreement with Claude Haiku and PaLM 2 Bison is comparably lower ($\\alpha$ = 0.13 and $\\alpha$ = 0.14, respectively), possibly indicating differences in alignment that become apparent only in smaller models. Mistral and Llama 2 exhibited negative agreement with all other models, indicating systematic disagreement. This is likely due to Mistral's heavy reliance on the ESH and NAH labels, while Llama 2 assigns most posts YTA in contrast to all other models. Lastly, Gemma generally exhibited near-zero agreement with other models-except for Bison and Llama-indicating no agreement beyond chance.\nSo far, we have compared models by evaluating similarities in their verdicts. However, LLMs are probabilistic models and may provide different verdicts when asked to evaluate a moral dilemma multiple times. Thus, we aimed to quantify each model's self-consistency, or the extent to which it produces similar verdicts across repeated evaluations. To this end, we had each model evaluate the 10,826 moral dilemmas 3 separate times. For each model, we calculated the self-consistency as Krippendorff's alpha across the three different runs (for GPT-4, two runs due to API cost). Model self-consistencies are shown on the diagonal of Figure 2 (we did not compute Redditor self-consistency, as individual Redditors did not evaluate the same dilemma multiple times). We found that model self-consistencies generally exceeded inter-model agreement. For example, GPT-4 and Claude achieved a values of 0.85 and 0.89, respectively, indicating high agreement between responses. GPT-3.5, Mistral, and PaLM 2 Bison had lower self-consistency, with $\\alpha$ equal to 0.6, 0.71, and 0.65, respectively. However, these values still indicate moderate reliability, and are considerably greater than the inter-model agreement. Lastly, Llama 2 and Gemma exhibited the lowest self-consistency, with $\\alpha$ values of 0.37 and 0.41, respectively. Together, these results demonstrate that LLMs exhibit low agreement in their verdicts on moral dilemmas. This agreement is lower than the models' self-consistencies, suggesting that factors such as input data, training choices, and alignment strategies contribute to significantly different outcomes when examining moral dilemmas.\n4.2 Ensemble consistency matches verdict agreement among Redditors\nEvaluations of moral dilemmas by different models can be seen as an ensemble, similar to how differing verdicts by Redditors in an AITA thread form a collective ensemble. Likewise, repeated evaluations of a dilemma by the same model can be considered an ensemble for that model. Greater consistency between models-and higher self-consistency within a model-suggests that similar judgments will emerge across the ensemble for a given dilemma. As shown in Figure 2, ensembles of LLMs generate diverse verdicts, likely influenced by variations in training data, architectures, and alignment. However, challenging moral dilemmas with no clear verdict may further reduce consistency at the sample level. This can be evaluated by comparing the ensemble of model verdicts to the ensemble of Redditors.\nTo investigate this, we examined whether the collective voting patterns of an LLM ensemble-consisting of different models or multiple evaluations of the same model-exhibited similar voting patterns to the Redditor ensemble. For each submission, we extracted all available top-level comments (i.e., comments not replying to others) and identified their judgments-NTA, YTA, ESH, NAH, or INFO-using a regular expression. We calculated the fraction of comments using each AITA label to represent the level of agreement among Redditors. Thus, for each dilemma, we obtained four values-referred to as label rates-denoting the fraction of comments assigning NTA, YTA, NAH, and ESH. We ommitted INFO, as it was rarely used by both Redditors and LLMs.\nWe first examined the ensemble of LLMs (inter-model consistency). Generally, 3 to 5 out of the 7 total LLMs agreed on most dilemmas (Appendix B: Fig. 7). We then examined the relationship between the fraction of Redditors assigning a particular AITA label and the number of LLMs assigning the same label . Specifically, for each AITA label, we split the submissions according to the number of LLMs voting with that label amd then calculated the average label rate across dilemmas within each subsample. We found that for the NTA and YTA labels, the average label rate increases as more models vote for the corresponding label, with the steepest increases occurring when 3 to 5 models are in agreement. This suggests that LLM agreement correlates with Redditor consensus. Notably, the NTA and YTA label rate curves bear resemblance to a psychometric sigmoid function, with shallower increases when there is little to no model agreement (0 to 1 vote) and when there is total model agreement (6 to 7 votes). We also found that the average label rates for NAH and ESH increase with the number of LLM votes, except at 6 to 7 models, likely because very few samples contain unanimous NAH and ESH labels.\nNext, we studied how Redditor consistency varies with LLM self-consistency. Specifically, we compared the average label rate against the ensemble of replicated verdicts, as described earlier. We focused on NTA and YTA for simplicity. We found that the average label rate generally increased with the self-consistency of the model. This trend was particularly evident in the larger, proprietary models (GPT-3.5, GPT-4, PaLM 2 Bison, and Claude Haiku). Llama 2 and Gemma were notable exceptions, exhibiting no clear relationship between average label rate and self-consistency. The former is likely to due the fact that Llama 2 almost always issued YTA verdicts. Gemma, on the other hand, showed little agreement with other models or Redditors. Together, these results demonstrate that, despite low inter-model agreement, an ensemble of LLMs can reproduce the consensus judgments of Redditors. Furthermore, individual models can serve as an ensemble by issuing multiple judgments that also exhibit correspondences with Redditors' consensus, though notable exceptions (Llama 2 and Gemma) show that this is not an emergent result for any LLM.\n4.3 Large language models invoke distinct patterns of moral reasoning\nThus far, we have analyzed LLMs' moral judgments by examining their verdicts on moral dilemmas. We additionally asked each LLM to provide reasoning for its verdict on each dilemma, yielding a rich set of texts to better elucidate patterns in their moral reasoning. Specifically, each LLM was prompted to \"provide an explanation for why [it] chose this label\" and to limit their explanation to one paragraph, though not all LLMs adhered to this restriction (Appendix A). The reasoning component from each LLM's output was extracted for further analysis. We first analyzed textual patterns in the reasoning across dilemmas.\nTo assess word usage across models, we converted all reasoning outputs into a TF-IDF representation. We applied mild preprocessing, including lowercasing, stop word removal, and AITA token removal. We used 3000 features with the highest TF-IDF values while omitting the most frequent terms across documents. We calculated the cosine similarity between model pairs at the dilemma level and averaged these values across dilemmas to obtain a single similarity value for each pair of models. We repeated this procedure for the model replicates to quantify self-similarity.\nWe found that LLMs typically exhibit low cosine similarities with each other, with values ranging from 0.25 to 0.40 . Some variance in the cosine similarity can be explained by model size, e.g., GPT-4 and Claude Haiku exhibit a slightly elevated aggregated cosine similarity (0.37). These similarities exceed those observed with Redditors, which range from 0.13 to 0.15 for all models. Thus, LLMs fundamentally use different language when rendering moral judgment compared to the Redditors, as expected. Lastly, self-similarity scores are slightly higher, with Claude Haiku exhibiting the highest self-similarity (0.68).\nNext, we conducted a word embedding analysis to assess whether we could distill distinct language patterns across LLMs. We processed the reasoning outputs from each LLM across all dilemmas using RoBERTa-Large and extracted embeddings from the last hidden layer, applying mean pooling across the input sequence. This resulted in a 1024-dimensional representation for each reasoning output. We then applied UMAP for dimensionality reduction, visualizing each distinct reasoning in two dimensions (Fig. 4).\nUMAP reveals clear distinctions among the LLMs based on their reasoning patterns. In particular, Redditors stand out as distinct from all LLMs, consistent with the TF-IDF analysis. PaLM 2 Bison, Mistral, and Gemma also appear distinct from the other models (Fig. 4: purple, pink, and grey colors). Interestingly, GPT-3.5 and GPT-4 exhibit significant overlap, likely due to their shared development by the same company, resulting in similar language patterns. In contrast, GPT-3.5 and GPT-4 show less overlap with Claude and Llama 2, both of which are also developed by major companies. The observed structure in this low-dimensional representation may stem from various factors, including word choice, response structure, response length, and the likelihood of issuing specific judgments, though model type appears to be a more dominant factor than verdict. Together, these findings demonstrate that LLMs exhibit distinct reasoning styles, potentially reflecting differing underlying moral principles.\n4.4 Moral reasoning corresponds with assignment of blame\nAlthough it appears LLMs exhibit distinct patterns in moral reasoning, we have yet to situate these patterns in a framework of moral reasoning. Thus, we explored how LLM reasoning aligns with distinct moral principle themes. Our approach builds on the work of [66], who analyzed a large-scale AITA dataset. Through a qualitative analysis, they identified six moral themes in AITA posts: Fairness & Proportionality, Feelings, Harm & Offense, Honesty, Relational Obligation, and Social Norms. Using their labeled AITA dataset, we fine-tuned six RoBERTa-Large models to predict each moral themes given the moral dilemma. We applied the ROBERTa-Large models to both the dilemmas (user submissions) and the LLM-generated reasonings. This allowed us to assess the moral themes present in the scenarios and those invoked by the models. To validate label quality, we examined a small subsample of scenarios and corresponding LLM reasonings, observing high accuracy."}, {"title": "5 DISCUSSION", "content": "We used Redditors as a proxy for the \"human label\" in our analyses. However, the demographic distribution of Redditors does not reflect the broader population: Redditors are overwhelmingly young, skew male, lean liberal or libertarian, and tend to be more educated [56], though these trends can vary significantly by subreddit. It is reasonable to assume that AITA follows a similar demographic distribution as Reddit at large. Thus, our findings that show agreement with the Redditors (e.g., Fig. 1 and Fig. 3) do not necessarily suggest successful alignment. Instead, they may serve as further evidence of LLM alignment with Western, Educated, Industrialized, Rich, and Democratic (WEIRD) subpopulations. As [9] have argued, aligning AI with human morals and values should imply anthropological questions about what and whose values are being targeted. It is imperative to move beyond the assumption of overarching values that inform the WEIRD bias, where values from WEIRD societies are assumed to represent universal \"human\" values [21].\nThe example AITA post illustrates potential hazards with LLMs, especially as they are integrated into products. The LLM's surface level analysis of the scenario-lacking the capacity to fully infer emotional context from the provided details-is concerning, as the advice they may offer in products can shape human behavior. Further work must be done to more thoroughly examine the degree to which this observation holds across different scenarios. While these concerns might be mitigated in more interactive settings, where users can engage in dialogue and explore details more deeply, it remains crucial to understand the limitations of LLMs in providing nuanced, contextually aware guidance.\nThe consistent labeling of posts as YTA by Llama symbolizes a broader issue with the use of LLMs for data evaluation, and the complicated role biases play in this evaluation when users lack access to the original text. The potential harm of hallucinations in LLMs has already been studied in the literature [63]. LLMs may craft seemingly rational arguments around flawed or exaggerated interpretations, as evidenced by Llama 2's disproportionate labeling of users as YTA. In real-world applications where users rely on these summaries without access to the original content, such misjudgment can be damaging. Notably, refusal occurred in less than 0.1% of submissions, demonstrating that models can appear overwhelmingly confident in their assessments and moral beliefs even when it is not warranted. The model outputs, although well structured and convincing, can amplify biases or distort the narrative in subtle ways, creating misleading representations that are very difficult to detect.\nA notable outlier in the verdict distributions was Mistral, which made significantly greater use of the NAH and ESH verdicts. A closer examination of its outputs revealed that Mistral placed a stronger emphasis on the term \"asshole\" and whether it was truly an appropriate descriptor for either party. This contrasts with the AITA community's implicit norm, where \"asshole\u201d serves as a placeholder for assigning fault rather than its literal meaning. This distinction highlights a tension between the community's interpretation and the model's literal understanding of the term, and whether other models can simply infer this norm from the system prompt itself, or are relying heavily on the training data. Mistral's behavior may change in contexts where the term \"asshole\u201d is no longer central. For example, future work could explore subreddits like r/AmIOverreacting or r/AmIWrong, which explore similar moral dilemmas but frame blame differently."}, {"title": "6 CONCLUSION", "content": "In this work, we evaluated a slate of large language models on everyday dilemmas in an effort to better understand their elicited norms on complex, unstructured data. We found that LLMs can offer dramatically different verdicts and moral reasoning on moral dilemmas, with different moral themes shaping those verdicts. We further found that ensembles of LLMs can echo ensembles of humans. Our findings highlight the need for further work in this domain, particularly as LLMs are increasingly used as therapists, companions, and advisors, where their interpretations of social norms and moral reasoning can significantly influence users' perceptions and decisions. As they increase in scale, power, and expressiveness, the need to better understand their underlying moral frameworks and potential biases becomes increasingly urgent. While LLMs demonstrate sophisticated analysis of moral scenarios, their judgments ultimately lack the transparency and ethical accountability inherent to human deliberation. This gap"}]}