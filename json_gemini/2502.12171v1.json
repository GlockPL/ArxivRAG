{"title": "GORA: Gradient-driven Adaptive Low Rank Adaptation", "authors": ["haonan he", "peng ye", "yuchen ren", "yuan yuan", "lei chen"], "abstract": "Low-Rank Adaptation (LoRA) is a crucial method\nfor efficiently fine-tuning pretrained large lan-\nguage models (LLMs), with its performance\nlargely influenced by two key factors: rank and\ninitialization strategy. Numerous LoRA variants\nhave been proposed to enhance its performance\nby addressing these factors. However, these vari-\nants often compromise LoRA's usability or effi-\nciency. In this paper, we analyze the fundamental\nlimitations of existing methods and introduce a\nnovel approach, GoRA (Gradient-driven Adaptive\nLow Rank Adaptation), which adaptively assigns\nranks and initializes weights for low-rank adapters\nsimultaneously based on gradient information.\nExtensive experimental results demonstrate that\nGORA significantly improves performance while\npreserving the high usability and efficiency of\nLORA. On the T5 model fine-tuned for the GLUE\nbenchmark, GoRA achieves a 5.88-point improve-\nment over LoRA and slightly surpasses full fine-\ntuning. Similarly, on the Llama3.1-8B-Base\nmodel fine-tuned for GSM8k tasks, GoRA outper-\nforms LORA with a 5.13-point improvement and\nexceeds full fine-tuning in high-rank settings by a\nmargin of 2.05 points.", "sections": [{"title": "1. Introduction", "content": "Open-source pre-trained large language models such as\nLlama (Touvron et al., 2023) and Qwen (Bai et al., 2023)\nhave demonstrated exceptional capabilities. Through su-\npervised fine-tuning, these models can be adapted to var-\nious downstream tasks such as code generation (Roziere\net al., 2023), mathematical problem solving (Yang et al.,\n2024), and agents (Hong et al., 2024). When the model\nwith parameter size & and uses FP16/BF16 mixed-precision\ntraining (Micikevicius et al., 2017; Kalamkar et al., 2019)\nwith the Adam optimizer (Kingma, 2014), the parameters\nand gradients require 4 bytes of memory, while the opti-\nmizer states require 12 bytes. Thus, the minimum memory\nusage, excluding activations, already reaches 16 bytes.\nSuch high memory demands limit the training of large\nlanguage models under constrained resources. To reduce\nmemory usage, Low-Rank Adaptation (LoRA) (Hu et al.,\n2021) decomposes the weight matrix $W\u2208 R^{m\u00d7n}$ into\n$W = W_o + \\Delta W = W_o + sAB$, where s is a scaling factor,\nand $A \u2208 R^{m\u00d7r},B \u2208 R^{r\u00d7n},r \u226a min(m,n)$,\nas shown\nin Figure 1(a). During training, LoRA only updates the\nlow-rank weights A and B, keeping Wo unchanged, thereby\nsignificantly reducing the memory footprint of optimizer\nstates. Although LoRA performs well on simpler tasks\nwhen applied to pre-trained large language models, its per-\nformance on more challenging tasks such as mathematics\nand code still lags behind full fine-tuning (Biderman et al.,\n2024; Ghosh et al., 2024).\nA critical factor in LoRA is its rank. Kalajdzievski (2023)\ndemonstrate that increasing the rank of LoRA can signif-\nicantly improve performance when paired with an appro-\npriate scaling factor. However, a direct increase in rank\nleads to a substantial rise in the number of trainable pa-\nrameters and memory consumption and imposes practical\nconstraints on rank selection. To address this limitation,\nseveral studies (Lialin et al., 2023; Ren et al., 2024) propose\nto ensemble multiple low-rank subspaces, allowing for rank\nincreases without proportionally increasing the number of\ntrainable parameters. Nevertheless, these approaches often\ncome at the expense of usability due to their intrusion to\narchitecture or training process. Another promising line of\nresearch explores assigning different ranks to weights based\non their importance. For example, AdaLoRA (Zhang et al.,\n2023c) dynamically allocates ranks by quantifying the im-\nportance of weight vectors during training and masking less\nsignificant ones, as illustrated in Figure 1(b). However, this\nmasking mechanism necessitates a larger parameter space\nduring initialization, increasing the number of trainable pa-\nrameters and limiting the upper bound of rank allocation.\nConsequently, dynamically allocating ranks without sig-\nnificantly increasing the number of trainable parameters\nremains an open challenge.\nAnother critical factor in LoRA is its initialization strategy.\nVanilla LoRA initializes the matrix Ao using a normal dis-"}, {"title": "2. Related Works", "content": "2.1. Rank of LORA\nThe choice of rank is crucial in LoRA training, with higher\nranks yielding better outcomes (Kalajdzievski, 2023). How-\never, increasing the rank raises memory usage, making it\nchallenging to train with sufficient ranks on limited hard-\nware. Previous works (Meng et al., 2024b; Lialin et al.,\n2023) attempt to continuously merge and reinitialize low-\nrank weights during training to increase the overall rank.\nHowever, these methods often require resetting the states\nof the optimizer and learning rate scheduler during reinitial-\nization to ensure that weight updates take place in distinct\nlow-rank subspaces, significantly increasing training com-\nplexity and making the training process unstable. Ren\net al. (2024) proposed aggregating multiple mini low-rank\nadapters diagonally to increase the overall rank. Never-\ntheless, this approach requires modifying the structure of\nLoRA, limiting its usability.\nAt the same time, the importance of different weights during\ntraining is not uniform, and a natural idea is to allocate larger\nranks to relatively more important weights. Zhang et al.\n(2023c); Hu et al. (2023) attempted to dynamically mask\nless important ranks during training to achieve dynamic\nrank training. However, these methods require allocating\nlarger matrices for low-rank adapters to reserve space for\nmasked ranks, leading to an increase in the number of train-\nable parameters, which affects their usability and limits the\nupper bound of rank allocation. IncreLoRA (Zhang et al.,\n2023a) introduces an approach that begins with a single rank\nand incrementally increases the rank during training. This\nmethod effectively addresses the challenge of large initial\nmatrices. However, it is not well compatible with distributed\ntraining frameworks such as FSDP (Zhao et al., 2023) and\nZeRO (Rajbhandari et al., 2020), which are critical for large\nmodel training.\n2.2. Initialization of LORA\nParameter initialization is a crucial concept in deep learning.\nInitialization methods such as (Glorot & Bengio, 2010)\nenable stable training of deep neural networks. A good\ninitialization strategy is also essential for LoRA. Beyond"}, {"title": "3. Method", "content": "In this section, we will reinterpret LoRA from the per-\nspective of a gradient compressor and introduce GoRA's\ngradient-based dynamic rank allocation method and initial-\nization strategy.\n3.1. View LoRA as a Gradient Compressor\nThe core idea of LoRA is to fine-tune a model by leveraging\nthe intrinsic low-rank property of a weight matrix $W \u2208$\n$R^{m\u00d7n}$. Specifically, a pair of low-rank matrices $A \u2208 R^{m\u00d7r}$\nand $B \u2208 R^{r\u00ebn}$ are initialized alongside W. During training,\nW remains frozen, while the model is updated by training\nthe low-rank matrices A and B, thereby reducing memory\nusage during training. For any training step t, the update to\nW is given by (1), where a is a tunable hyperparameter that\nensures the scale of the LoRA computation depends only\non \u03b1 and is independent of the rank r:\n$W_t = W_0 + \\Delta W = W_0 + \\frac{\u03b1}{r} A_tB_t$. (1)\nSpecifically, given the training loss L, the gradient of the\nweight matrix W can be computed as $\\frac{\u2202L}{\u2202W}$. Using the chain"}, {"title": "3.2. GORA Rank Allocation", "content": "Given the above hypothesis of LoRA as a gradient com-\npressor, our dynamic rank allocation strategy has four main\nobjectives: (1) to measure the importance of weight based\non respective n-step accumulated gradient $G = \\frac{1}{n}\\Sigma_{i=1}^n \\frac{\u2202Li}{\u2202Wi}$\nand allocate rank accordingly; (2) to complete rank alloca-"}, {"title": "3.3. GORA Initialization Strategy", "content": "Once ranks are allocated for each layer, it is crucial to prop-\nerly initialize the low-rank matrices. The compression form"}, {"title": "4. Experiments", "content": "We conducted extensive experiments on GoRA and baseline\nmethods for both natural language understanding and natu-\nral language generation tasks in section 4.1 and section 4.2\nrespectively. For natural language understanding tasks, we\ntrained T5-Base (Raffel et al., 2020) on five sub-datasets\nof GLUE (Wang, 2018): MNLI, SST-2, CoLA, QNLI, and\nMRPC, and reported accuracy on the corresponding val-\nidation sets. For natural language generation tasks, we"}, {"title": "4.1. Experiment Results on Natural Language\nUnderstanding Tasks", "content": "Settings: We adopted the baseline performances reported\nin LORA-GA(Wang et al., 2024b). To ensure fairness in\ncomparison, our experimental setup was consistent with\ntheirs: we used the Adam(Kingma, 2014) optimizer with\nBetal set to 0.9, Beta2 to 0.999, weight decay to 0, batch\nsize to 32, and a cosine decay learning rate scheduler with\na warmup ratio of 0.03. Additionally, we trained all linear\nlayers in the model except the language head, with a peak\nlearning rate of 1e-4, a maximum sequence length of 128,\nand FP32 precision."}, {"title": "4.2. Experiment Results on Natural Language\nGeneration Tasks", "content": "Settings: We trained the mathematical capability using a\n100K subset of the MetamathQA (Yu et al., 2023) training\nset, the coding capability using a 100K subset of the Code-\nFeedBack (Zheng et al., 2024) training set, and the dialogue\ncapability using a 52K subset of the WizardLM (Xu et al.,\n2024) training set. For evaluation purposes, we removed\nall textual descriptions from the Code-FeedBack training\nlabels, retaining only executable code. We used the AdamW\noptimizer (Loshchilov, 2017) with beta1 set to 0.9, beta2 to\n0.999, weight decay to 5e-4, batch size to 64, and a cosine\ndecay learning rate scheduler with a warmup ratio of 0.03\nand a learning rate decay ratio of 0.1. For all baseline meth-\nods and GORA, we trained every linear layer in the model's\nattention modules (i.e., wq, wk, wv and wo) with a peak\nlearning rate of 5e-5 and BF16 mixed precision strategy. To\nensure measurable results, the learning rate for AdaLoRA\nwas set to 5e-4. For evaluation, mathematical capability\nwas assessed by calculating the accuracy of results extracted\nusing regular expressions; coding capability was evaluated\nusing the PASS@1 score; and chat capability was assessed\nby averaging scores from 0-10 given by GPT-40, Gemini-\n1.5-Pro, and Llama-3.1-70B-Instruct. The prompts used\nfor scoring were consistent with those reported in (Zheng\net al., 2023) to ensure result reliability. For more setup\ninformation, please refer to Appendix B.2."}, {"title": "5. Discussion", "content": "In this section, we present a comprehensive set of ablation\nstudies to evaluate the effectiveness of GoRA's dynamic\nrank allocation mechanism and its initialization strategy.\nAdditionally, we discuss the impact of key hyperparameter\nchoices introduced by GoRA, providing insights into their\nroles in shaping the model's performance.\n5.1. The Effect of Rank Allocation Strategy.\nThe rank allocation strategy is a critical component influenc-\ning the performance of GORA. As highlighted in Table 3,\nwe conducted ablation studies to evaluate different rank\nallocation ranges. The results demonstrate that a broader\nrank allocation range consistently leads to superior perfor-\nmance. For instance, (4-32) achieved a score of 48.98 on\nHumanEval, significantly outperforming both the fixed rank\nallocation strategy (8-8) and the more conservative alloca-\ntion strategy (6-15)."}, {"title": "5.2. The Effect of Initialization Strategy.", "content": "Table 4 summarizes the results of ablation studies conducted\nwith various initialization scaling factors. Our experiments"}, {"title": "5.3. The Effect of Different Importance Metrics.", "content": "Table 5 compares different importance metrics, including\nsensitivity of parameter to loss, gradient nuclear norm, and\nparameter-gradient product nuclear norm. The results show\nthat parameter sensitivity consistently outperforms the other\nmethods, particularly on the HumanEval dataset, where\nParameter Sensitivity achieved a score of 48.98, compared"}, {"title": "6. Conclusions", "content": "In conclusion, GoRA significantly improves LoRA by adap-\ntive rank allocation and initialization based on gradient\ninformation. This approach enhances performance while\nmaintaining efficiency, outperforming vanilla LoRA and\neven full fine-tuning in some cases. Extensive experiments\ndemonstrate GoRA's effectiveness. Looking ahead, future\nresearch directions may include scaling GoRA to larger\nmodel architectures, exploring its application across diverse\ntask domains, investigating alternative initialization strate-\ngies for matrix A and integrating GoRA with other LoRA\nvariants such as DoRA. Overall, GORA offers an efficient\nsolution for fine-tuning large language models."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof parameter-efficient fine-tuning. Specifically, to improve\nlow-rank adaptation performance without sacrificing its ef-\nficiency and usability. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here."}, {"title": "B. Implementation Details", "content": "B.1. Implementation Details for Baseline Methods\nSeveral baseline methods introduce tunable hyperparameters compared with vanilla LoRA (Hu et al., 2021). To ensure a fair\ncomparison, we adopt the optimal settings reported in the original papers whenever possible. Specifically, for LoRA+ (Hayou\net al., 2024), we set the learning rate ratio of matrices A and B to 16. For LoRA-GA (Wang et al., 2024b), we use the \"stable\noutput' scaling method and reinitialize the full weights during initialization. For AdaLoRA (Zhang et al., 2023c), the initial\nrank is set to 12, the final rank to 8, with t\u2081 = 150 and tf = 900. For PiSSA (Meng et al., 2024a), the number of iterations\nfor SVD is set to 64.\nB.2. Implementation Details for GORA\nFor all experiments, except for the model trained on MetaMathQA (Yu et al., 2023), we set the scaling factor \u03b3 to 5e \u2013 2.\nFor the model trained on MetaMathQA, \u03b3 is set to 8e \u2013 2. To address the imbalance in GoRA's matrices A and B, we\nset the learning rate of matrix B to be 16 times that of matrix A Throughout the experiments, the maximum rank was\ndefined as four times the average rank, and the minimum rank was set to half the average rank. We employed a 64-step\naccumulated gradient approach for GoRA. In the ablation studies, we adhered to the same hyperparameter settings as in the\nmain experiments, unless otherwise specified.\nB.3. Training Environments\nFor natural language understanding tasks reported in section 4.1, we conduct our experiments using the Huggingface\nTransformers framework for model and trainer implementation on a single RTX 4090 GPU. In contrast, for natural language\ngeneration tasks reported in section 4.2 and section 5, we utilize the DeepSpeed ZeRO2 (Rajbhandari et al., 2020) data\nparallel framework and FlashAttention-2 (Dao, 2023) mechanism, leveraging the power of 8 RTX 4090 GPUs in a Slurm\ncluster. All codes of GoRA and baseline methods are implemented in PyTorch.\nC. Time and Memory Cost\nTo evaluate the additional computational overhead introduced by our method, we conducted comprehensive benchmarks to\nmeasure both time and memory consumption. Specifically, we trained the Llama-3.1-8B-Base model on a 100K subset\nof MetaMathQA using a single RTX 4090 GPU, with a maximum sequence length of 512. As detailed in Table 6, GORA\nintroduces only a 2.6% increase in trainable parameters compared to vanilla LoRA, whereas AdaLoRA (Zhang et al., 2023c)\nresults in a 50% increase in the number of trainable parameters. Notably, the number of trainable parameters in GoRA does\nnot grow linearly with the increase of rank allocation upper bound, demonstrating its parameter efficiency. Consequently,\nGORA exhibits nearly identical memory usage to LoRA and incurs no additional training time. The initialization time (4min)\nof GORA is negligible compared with training time (5h48min), and it is worth emphasizing that this initialization time is\nsolely determined by the number of gradient computation steps (64 steps in this case) before training, remaining constant\nregardless of the total training duration.\nD. Limitations And Future Works\nIn this study, we have demonstrated that GoRA outperforms baseline low-rank adaptation methods and achieves performance\ncomparable to full fine-tuning. However, our evaluation has not yet extended to larger models and more extensive datasets.\nWe hypothesize that for larger models, such as Llama-3.1-70B (Dubey et al., 2024), GORA could more effectively leverage\nthe pre-trained knowledge inherent in these models. Additionally, while this research primarily focuses on language models\nand natural language processing tasks, there is potential to generalize GORA to a broader range of model types and tasks\nsuch as visual language models and visual question answering.\nAnother limitation of this study is that the initialization of matrix A is not restricted to random initialization. Employing\nalternative methods, such as extracting distinguishing features from pre-trained weights to initialize matrix A, could\npotentially enhance performance, as it would combine the benefits of both experience-driven and data-driven initialization\napproaches. Furthermore, it is worth noting that GoRA demonstrates theoretical compatibility with other LoRA variants,\nsuch as DORA (Liu et al., 2024). These promising avenues remain to be explored in future research endeavors."}]}