{"title": "'No' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue", "authors": ["Rena Gao", "Xuetong Wu", "Siwen Luo", "Caren Han", "Feng Liu"], "abstract": "Out-of-distribution (OOD) detection in multimodal contexts is essential for identifying deviations in combined inputs from different modalities, particularly in applications like open-domain dialogue systems or real-life dialogue interactions. This paper aims to improve the user experience that involves multi-round long dialogues by efficiently detecting OOD dialogues and images. We introduce a novel scoring framework named Dialogue Image Aligning and Enhancing Framework (DIAEF) that integrates the visual language models with the novel proposed scores that detect OOD in two key scenarios (1) mismatches between the dialogue and image input pair and (2) input pairs with previously unseen labels. Our experimental results, derived from various benchmarks, demonstrate that integrating image and multi-round dialogue OOD detection is more effective with previously unseen labels than using either modality independently. In the presence of mismatched pairs, our proposed score effectively identifies these mismatches and demonstrates strong robustness in long dialogues. This approach enhances domain-aware, adaptive conversational agents and establishes baselines for future studies.", "sections": [{"title": "1 Introduction", "content": "In the regime of multimodal learning contexts, Out-Of-Distribution (OOD) detection involves identifying whether some unknown inputs from different modalities (e.g., text and images) deviate significantly from the patterns in the previously seen data. Specifically, an OOD instance under the multimodal setting is defined as one that does not conform to a certain distribution of interest, either by deviating in one modality or by showing the discrepancy across different modalities (Arora et al., 2021; Chen et al., 2021; Feng et al., 2022; Hsu et al., 2020). This is crucial in applications such as dialogue-image systems where the synergy between spoken or written language and visual elements is expected to adhere to certain semantic and contextual norms when identifying the In-Distribution (ID) pairs where they come from some known distribution.\nTaking three motivating examples as shown in Figure 1, we are given several dialogue-image pairs for OOD detection where our ID label is 'cat'. We will then consider two typical OOD cases in dialogue systems where either: 1) the dialogue label and image labels are not matched, or 2) even if the dialogue and image match, their labels might not be previously seen in the given data.\nTo effectively detect OOD samples in such a novel multi-modalities multi-round long dialogue scenario, we introduce Dialogue Image Aligning and Enhancing Framework (DIAEF), a framework that incorporates a novel OOD score for taking the first attempt on dialogue-image OOD detection for long dialogue systems. We propose a new score design across these two modalities, enabling more targeted controls for misalignment detection and performance enhancement. Such a framework could effectively boost anomaly detection and give better response strategies in long dialogue systems with interactive aims. This comprehensive score framework not only advances the field of multimodal conversations but also sets a new standard for domain-aware, adaptive long dialogue agent building for the future. To show the effectiveness of the proposed framework, we constructed a dataset consisting of over 120K dialogues in the multi-round application Question-answering systems and open-domain real interactive dialogues (Seo et al., 2017; Lee et al., 2021). Leveraging these dialogue datasets, we apply our proposed framework and demonstrate the effectiveness of the novel score design through various experiments. These experiments establish fundamental benchmarks and pave the way for future explorations in such a novel dialogue setting. Furthermore, we integrate the crucial aspect of OOD detection, emphasizing its significance for enhancing the robustness and applicability of multimodal dialogue systems (Dai et al., 2023; Dosyn et al., 2022; Wu et al., 2024). To summarize, our contributions are listed as follows:\n\u2022 We take the first attempt for OOD detection with the dialogues and propose a novel framework that enhances the OOD detection in cross-modal contexts, particularly focusing on scenarios where dialogue-image pairs either do not match with the semantics or even match, but their semantic labels are outside the known set, which matters in long-dialogue context for users.\n\u2022 Our framework incorporates a novel scoring method by combining both dialogues and images to enhance the OOD detection while recognizing the mismatch pairs with the dialogue-image similarity.\n\u2022 We demonstrate the practical application of our methods with the real-world multi-round long dialogue dataset, showcasing improvements in user experience and system reliability upon response. Further, our work establishes foundational benchmarks and methodologies that can serve as baseline standards for future research in the field of cross-modal detection on interactive dialogue systems."}, {"title": "2 Related Work", "content": "OOD Detection in Dialogue Systems. Dialogue systems have become fundamental in applications ranging from virtual assistants and customer service bots to educational platforms with continuous multi-rounds (Feng et al., 2022; Kottur et al., 2019; Seo et al., 2017; Yu et al., 2019; Gao and Wang, 2024). The evolution of dialogue systems has seen a progression from rule-based and template-based approaches to statistical and machine learning methods (Zheng et al., 2020; Lang et al., 2023; Deka et al., 2023; Mei et al., 2024; Arora et al., 2021). Modern systems, particularly those based on deep learning models like BERT and GPT, have set new performance benchmarks (Yuan et al., 2024; Hendrycks et al., 2020; Yang et al., 2022; Ye et al., 2023). However, the complexity of these systems introduces challenges in understanding context and handling ambiguous semantic queries, necessitating more sophisticated approaches to maintain dialogue coherence and accuracy in interactive dialogue contexts, especially for real-life cases. OOD detection is a critical aspect of dialogue systems, ensuring their robustness and reliability in generating responses to user queries (Niu and Zhang, 2021; Chen et al., 2022). When dialogue systems encounter inputs that deviate from the training data distribution in long multi-round data, they risk generating incorrect or nonsensical answers, leading to user frustration and decreased trust. Effective OOD detection helps identify such anomalous queries, allowing the system to gracefully handle or reject them, thereby maintaining the quality and consistency of responses (Li and Lin, 2021), including softmax probability thresholding (Liu et al., 2023; Dhuliawala et al., 2023), auxiliary models (Wang et al., 2024; Zheng et al., 2024; Ram\u00e9 et al., 2023), generative models (Cai and Li, 2023; Ktena et al., 2024; Graham et al., 2023), and self-supervised learning (Azizi et al., 2023; Wallin et al., 2024; Liu et al., 2021). The integration of effective OOD detection mechanisms is crucial for the continued advancement and trustworthiness of QA dialogue systems (Salvador et al., 2017; Feng et al., 2022).\nDialogue-based Multimodality OOD Detection. Due to the complexity of dialogue in multi-turns and information complexity embedded in the connection of preceding turns within the long dialogue, successfully detecting whether the information from the dialogue and images are within the same domain stands as a technical challenge in OOD detection (Fort et al., 2021; Basart et al., 2022). Previous works attempted to evaluate the generated pseudo-OOD samples' impact on the OOD section in dialogue settings (Marciniak, 2020), which improved OOD detection performance after introducing the generated dialogues when utilizing unlabeled data, making the model practical and effective for real-world applications (Zheng et al., 2020). Later studies used the information bottleneck principle to extract robust representations by filtering out irrelevant information for multi-turn dialogue contexts (Lang et al., 2023). Furthermore, the crucial aspect of OOD detection in multimodal long dialogue is still under investigation, emphasizing the significance of multimodal conversational user experience in question-answering systems.\nMulti-label OOD Detection. While numerous studies have improved approaches for multi-class OOD detection tasks, investigating multi-label OOD detection tasks has been notably limited. A recent advancement is the introduction of Spectral Normalized Joint Energy (SNoJoE) (Mei et al., 2024), a method that consolidates label-specific information across multiple labels using an energy-based function. Later on, the sparse label co-occurrence scoring (SLCS) leverages these properties by filtering low-confidence logits to enhance label sparsity and weighting preserved logits by label co-occurrence information (Wang et al., 2022). Considering the vision-language information as input in models like CLIP (Radford et al., 2021), traditional vision-language prompt learning methods face limitations due to ID-irrelevant information in text embeddings. To address this, the Local regularized Context Optimization (LoCoOp) approach enhances OOD detection by leveraging CLIP's local features in one-shot settings (Miyai et al., 2024). However, previous approaches majorly implied the limitation only in computer vision tasks without focus on dialogue or Natural Language Processing tasks(Wei et al., 2015; Zhang and Taneva-Popova, 2023; Wang et al., 2021, 2022)."}, {"title": "3 Problem Formulation", "content": "To formally define the cross-modal OOD problem, we focus on the detection with dialogue and image pairs within a multi-class classification framework. Specifically, we have a batch of N pairs of images and dialogues, along with their labels, denoted by {$(i_n, t_n), y_n$}$_{n=1}^{N}$ where $i_n \\in I$ and $t_n \\in T$ denote the input image and dialogues and I and T are the image and dialogue spaces, respectively. Here, the instance pair may be associated with multiple labels $y_n$ with $y_n = {Y_{n,1}, Y_{n,2},\u2026\u2026, Y_{n,K}} \\in [0,1]^{K}$ where $Y_{n,k} = 1$ if the dialogue-image pair is associated with k-th label and K denotes the total number of in-domain categories. Our proposed score function enhances the ability to distinguish between ID and OOD data cross-joint detection for image and dialogue, making it applicable in multimodality scenarios. Based on this setup, the goal of the OOD detection is to define a decision function G such that:\n$G(i,t,y) = \\begin{cases}\n1 & \\text{if } (i, t, y) \\sim D_{in}.\n0 & \\text{if } (i, t, y) \\sim D_{out},\n\\end{cases}$"}, {"title": "4 Dialogue Image Aligning and Enhancing Framework", "content": "To intuitively demonstrate our framework, we draw the overall workflow in Figure 2. The workflow consists of three parts: in the first stage, we will employ a vision language model, such as CLIP (Radford et al., 2021) and BLIP (Li et al., 2022), to derive meaningful descriptors or feature embeddings from images and dialogues, respectively. Note that the model we used here would map the image and dialogue into the same latent space so that the similarity between the two can be easily calculated. These processes yield embeddings $x_I$ for images and $x_T$ for dialogues. Then, utilizing these embeddings, we apply a scoring function s($x_I, x_T$) to assess the relevance between an image and a dialogue. The outcome of this function helps us determine whether the dialogue-image pair falls within the categories, indicating a high relevance in semantics, or the out-of-distribution categories with mismatches, suggesting low or no relevance.\nIn addition to this score, we will further train two label extractors to compare the whole pair with the label set to determine if the pair is in-distribution or out-of-distribution using $s_I(x_I, y)$ that evaluates the similarity between the image and the label and $s_T(x, y)$ that evaluates the similarity between the text and the label. We will use conventional methods to combine these two scores and determine whether the pair is ID or OOD based on the threshold \u03bb.\nThis paper aims to enhance the detection of OOD samples by combining dialogues and images and identifying the misalignments between them. To this end, we naturally propose the DIAEF score function in general:\n$g(x_T, x_I, y; s, s_T, s_I) = s(x_T, x_I)^\\gamma (\\alpha s_I(x_I, y) + (1 - \\alpha) s_T(x, y)),$"}, {"title": "5 Experiments", "content": "In this section, we evaluate DIAEF and other baselines using several datasets."}, {"title": "5.1 Experimental Setup", "content": "Datasets. In this section, we utilize the Visdial dataset (Das et al., 2017) and Real MMD dataset (Lee et al., 2021) for OOD detection in long dialogue systems. The Visdial dataset comprises over 120K images sourced from the COCO image dataset (Lin et al., 2014), coupled with collected multi-round dialogues in a one-to-one mapping format between modalities. We constructed a testing multi-round question-answering dataset with full semantic context to evaluate our OOD detection methods, including all dialogue-image pairs and an additional 10K mismatched pairs. Each entry in this dataset contains an image, a full conversation, and a set of labels with 80 specific categories. The dataset is further organized into 12 higher-level supercategories: animal, person, kitchen, food, sports, electronic, accessory, furniture, indoor, appliance, vehicle, and outdoor. Another related dataset, called the Real MMD dataset, contains images sourced from COCO (Lin et al., 2014) and texts from different sources such as DailyDialog (Li et al., 2017) and Persona-Chat (Miller et al., 2017), meaning they may not be perfectly matched but instead have a certain degree of similarity. The dataset statistics are presented in Table 7 and Table 8 in Appendix A.\nOOD Label Selection. In our study, we propose a label selection score function for selecting OOD labels that effectively combines semantic distance and ontological hierarchy via the WordNet path calculation. The score function integrates multiple criteria to enhance the robustness and accuracy of OOD detection. Semantic distance is quantified using cosine similarity between vector representations of candidate labels and the remaining labels in the label set. We compute the maximum cosine similarity to any ID label for each candidate OOD label and select those with values below a predefined threshold, ensuring semantic distinctiveness. Additionally, we leverage ontological hierarchies, such as WordNet, to measure the path length between candidate labels and ID labels. Candidates with a minimum path length exceeding a specified threshold are selected, ensuring they are not closely related in the hierarchy. This dual-criteria approach ensures that selected OOD labels are both semantically distant and ontologically distinct from ID labels, enhancing the efficacy of the OOD detection system. By integrating these methods, our score function effectively mimics real scenarios where the OOD labels generally differ from the ID labels.\nTherefore, we define the selection score as:\n$S(c) = w_1 \\sum_{y \\in Y\\backslash c} (1 \u2013 S_{cos}(M(c), M(y))) + W_2 \\sum_{y \\in Y\\backslash c} (1 \u2013 S_{PATH} (C, y)),$\nwhere\n$S_{cos}(a, b) = \\frac{a \\cdot b}{||a|| ||b||}, S_{PATH} (Y_1, Y_2) = \\frac{1}{1 + l_a(Y_1, Y_2)}$\nHere, M(c) and M(y) are the vector representations of the candidate OOD labels c and the ID label y with the encoder M, respectively, $w_1$ and $w_2$ are the weights assigned to each criterion, and y represents the total valid label set. The term $S_{cos}$ measures the semantic distance, and $S_{PATH} (Y_1, Y_2)$ measures the ontological distance between labels with the path distance $l_a(Y_1, Y_2)$ between the words $Y_1$ and $y_2$ in the WordNet. We conduct the score selection on the Visdial dataset with $w_1 = W_2 = 0.5$, and the top five scores with the most distinguished labels are shown in Table 2. To ensure that the selected OOD labels are both semantically distant and ontologically distinct from ID labels, we select candidates c where the score S(c) is the highest.\nExperiment Details. Based on Table 2, we select the label 'animal' as the OOD label to show the framework's effectiveness. We will have 95K ID pairs and 37K OOD pairs for QA dataset and 12.7K"}, {"title": "5.2 Main Results", "content": "With the aforementioned experimental settings, we evaluate various DIAEF scores on the given QA and Real MMD datasets and report the performance results in Table 3 and 4. The tables show that our proposed framework generally outperforms the results obtained using only images or dialogue across most metrics. In particular, the joint energy and Mahalanobis scores with the sum or max aggregation consistently perform well across most metrics. In addition, the naive probability and ODIN scores also show competitive performance. Interestingly, the max aggregation method tends to be more effective than the sum method. This could be because we are dealing with a multi-label problem. Adding up scores for all labels might introduce more noise, which confuses the OOD and ID scores and thus reduces detection performance. For dialogues, the performance is not as good as for images. This is because dialogues contain some noises, such as stopwords, that are unrelated to the labels, whereas images with segmentation are more directly related to the labels. However, even though the dialogue alone may not perform well, combining it with images could significantly enhance the OOD detection performance. The results demonstrate that DIEAF performs effectively when combining dialogue and image scores, especially when introducing mismatched pairs."}, {"title": "5.3 Analysis of Experimental Results", "content": "To gain deeper insights into the proposed framework, we conduct several ablation studies to examine the impact of mismatched pairs, the effectiveness of s(x1, xT), and the choices of a and \u03b3.\nEffect of Mismatched Pairs. To investigate the effect of the mismatched pairs, we conduct the experiments with the same setting by excluding the mismatched pair in the testing set and report the results in Table 5. Here, we only report FPR95 for simplicity and also compare the results by setting y = 0 without introducing the dialogue-image similarity.\nFrom the table, it can be observed that when there are no mismatched pairs, setting y to 1 can actually harm our results to some extent. This is because, for OOD pairs without mismatched pairs, their similarity score s(x1, xT) can still be high. In such cases, multiplying by the similarity can adversely affect OOD results. Setting y to 0 in these situations improves FPR95 results for most cases, indicating that simply combining image and dialogue modalities, even without mismatched pairs, performs better than the unimodality. Additionally, comparing Table 3 and 5, we see that introducing mismatched pairs generally leads to worse performance than having no mismatched pairs. This demonstrates that mismatched pairs indeed pose a challenge for OOD detection. To achieve better results, we will further study the impact of y and a to optimize OOD detection performance.\nEffect of VLM models. We further tested the performance of the DIAEF score function with the BLIP model (Li et al., 2022) under the same setting as CLIP (also see details in Appendix A), and we report the results in Table 6. Even with BLIP, the pattern is still maintained as the proposed score"}, {"title": "6 Conclusion and Limitation", "content": "This paper introduces a cross-modal OOD score framework, DIAEF, designed to expand OOD detection in cross-modal QA systems by integrating images and dialogues. DIAEF combines alignment scores between dialogue-image pairs with an enhancing term that leverages both the image and dialogue. Experimental results demonstrate DIAEF's superiority over baseline methods with general metrics such as FPR95 and show the framework's effectiveness. However, there are some spaces for future work. First, due to the scarcity of datasets, we initially validated our framework on VisDial and demonstrated its effectiveness. More dialogue-image datasets are worth exploring for validation. Second, the existing scores have proven the effectiveness of this framework, but further improvements could be achieved by applying some transformations or smoothing techniques to make the distributions of ID and OOD more distinct. Finally, this framework is applicable to more visual language models, such as multimodal models like BLIP, and can further enhance OOD performance using various image-text matching criteria."}, {"title": "A Experiment Details", "content": "The dataset stats are summarized as follows:\nWe give detailed experimental settings in the following table."}, {"title": "B Theoretical Justification", "content": "Assumption 1 We denote ID distribution as $P(x_I,x_T,y)$ and OOD distribution as $\\hat{P}(x_I,x_T,y)$ where P may differ from P in terms of the following assumptions.\n\u2022 Case 1: The image and text match, but labels are out of the set, namely:\n$E_{\\hat{P}(x_I,x_T)} [log s(x_I,x_T)] = E_{P(x_I,x_T)} [log s(x_I, x_T)],$\nFor every pair $x_I$, $x_T$ and any \u03b1,\n$E_{\\hat{P}(y|x_I,x_T)} [log(\\alpha s_I(x_I,y) + (1 - \\alpha)s_T(x_T,y))] > E_{P(y|x_I)} [log(\\alpha s_I(x_I,y) + (1 - \\alpha)s_T(x_T,y))],$\nwhich means that the ID pairs $x_I$ and $x_T$ should have stronger expressity about the ID label y than OOD pairs.\n\u2022 Case 2: The image and text do not match, which we assume:\n$E_{\\hat{P}(x_I,x_T)} [log s(x_I,x_T)] > E_{P(x_I,x_T)} [log s(x_I, x_T)],$\nwhich means the ID pairs should have higher similarity than OOD pairs in this case. For every pair $x_I$, $x_T$ and any \u03b1,\n$E_{\\hat{P}(y|x_I,x_T)} [log(\\alpha s_I(x_I,y) + (1 - \\alpha)s_T(x_T, y))] = E_{P(y|x_I,x_T)} [log(\\alpha s_I(x_I,y) + (1 - \\alpha)s_T(x,y))],$\nwhich means that some ID pairs $x_I$ and $x_T$ may have the same expressity about the label y compared with the OOD pairs.\n\u2022 Case 3: The image or text does not match with the labels, which we assume:\n$E_{\\hat{P}(y|x_I,x_T)} [log(\\alpha s_I(x_I,y) + (1 - \\alpha)s_T(x_T, y))] > E_{P(y|x_I,x_T)} [log(\\alpha s_I(x_I,y) + (1 - \\alpha)s_T(x,y))] .\nTheorem 1 With Assumption 1, we can show that the proposed DIEAF score satisfies the following:\n$E_{\\hat{P}(x_I,x_T,y)} [log g(x_I,x_T, y)] < E_{P(x_I,x_T,y)} [log g(x_I, x_T, y)].$\nProof 1 It is easy to write that:\n$E[log g(x_I, x_T, y)] = \\Ex_{I,x_T} [log s(x_I,x_T)] + E_{x_I,x_TEy|x_I,x_T} [log([\\alpha s_I(x_I,y) + (1 - \\alpha)s_T(x_T,y)])].$\nThe proof simply follows the assumptions we made for each case. Note that this score only works for positive scores, but sometimes, we may encounter negative scores, and the log may be ill-posed. As a surrogate score function, we eliminate the log and maintain $g(x_I,x_T,y)$ for the same intuition as the above theorem."}]}