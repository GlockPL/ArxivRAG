{"title": "Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning", "authors": ["Yan Yu", "Wengang Zhou", "Yaodong Yang", "Wanxuan Lu", "Yingyan Hou", "Houqiang Li"], "abstract": "Multi-task reinforcement learning employs a single policy to complete various tasks, aiming to develop an agent with generalizability across different scenarios. Given the shared characteristics of tasks, the agent's learning efficiency can be enhanced through parameter sharing. Existing approaches typically use a routing network to generate specific routes for each task and reconstruct a set of modules into diverse models to complete multiple tasks simultaneously. However, due to the inherent difference between tasks, it is crucial to allocate resources based on task difficulty, which is constrained by the model's structure. To this end, we propose a Model Evolution framework with Genetic Algorithm (MEGA), which enables the model to evolve during training according to the difficulty of the tasks. When the current model is insufficient for certain tasks, the framework will automatically incorporate additional modules, enhancing the model's capabilities. Moreover, to adapt to our model evolution framework, we introduce a genotype module-level model, using binary sequences as genotype policies for model reconstruction, while leveraging a non-gradient genetic algorithm to optimize these genotype policies. Unlike routing networks with fixed output dimensions, our approach allows for the dynamic adjustment of the genotype policy length, enabling it to accommodate models with a varying number of modules. We conducted experiments on various robotics manipulation tasks in the Meta-World benchmark. Our state-of-the-art performance demonstrated the effectiveness of the MEGA framework. We will release our source code to the public.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has made significant progress in recent years, achieving remarkable successes across various domains, including games [Berner et al., 2019; Vinyals et al., 2019], robotics [Kalashnikov et al., 2018] and autonomous driving [Kiran et al., 2020]. Although RL algorithms have demonstrated impressive capabilities in single-task scenarios, they still encounter challenges when it comes to generalizing across multiple tasks using a single policy.\nIn previous work, a common approach is to decompose a model into multiple modules [Yang et al., 2020], which are then reconstructed into different models for each task. These methods typically train a routing network to determine how modules are selected and combined for specific tasks. However, task difficulty varies. Drawing from human learning experiences, it is essential to allocate more resources to more difficult tasks. While existing methods do consider the allocation of modules, they only modify the set of modules selected for a given task within a fixed model. These approaches neither alter the model structure nor explicitly account for task difficulty when allocating modules.\nTo address the above issues, we propose a model evolution framework. Our framework enables dynamic evolution through interactions with the environment, inspired by biological evolution. When a task cannot be completed with the current model, the agent evolves additional modules to meet the task's requirements. This dynamic resource allocation not only enhances the agent's ability to tackle complex tasks but also reduces the computational cost for simple tasks. Since there is no need to predefine the total number of modules at initialization, the agent is not constrained by a lack of sufficient modules for complex tasks, nor is it affected by degradation or gradient explosion due to excessive network depth, offering both adaptability and robustness.\nTo generate module weights, routing networks are popularly adopted. However, routing networks can only generate fixed-dimensional module weights, which are unable to adapt to dynamic model structures. To this end, we introduce the genotype module-level model. In this model, module weights for different tasks are determined by binary genotype policies, as shown in Fig. 1. The length of the binary genotype policy is flexible, enabling the generation of module weights with various dimensions, which can adapt to the model evolution framework as modules increase. We categorize genotype policies according to their corresponding tasks into task populations, which collectively form a multi-task community. We optimize these task populations using a non-gradient genetic algorithm, applying operators such as evaluation, selection, crossover, and mutation, enabling the module weights to evolve towards specialization for their respective tasks.\nTo validate the effectiveness of our framework, we conducted extensive experiments on the Meta-World benchmark [Yu et al., 2019]. It can handle 10 to 50 robotic manipulation tasks simultaneously, achieving state-of-the-art performance and sample efficiency compared to other baselines. We also made ablation studies, which further demonstrate the significance of our proposed dynamic network structures for multi-task reinforcement learning."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Evolutionary Reinforcement Learning", "content": "Evolutionary Algorithms (EA) [B\u00e4ck and Schwefel, 1993] provide a gradient-free optimization approach. Evolutionary Reinforcement Learning (ERL) [Pourchot and Sigaud, 2018; Khadka et al., 2019; Chen et al., 2019; Bodnar et al., 2020; Bodnar et al., 2020] combines RL agents with a genetic population, improving the exploration and robustness of the policy. For instance, data collected by the EA population is used to train the RL agent [Khadka and Tumer, 2018], and the RL agent is periodically integrated into the EA population to improve performance. A method for shared nonlinear state representations [Li et al., 2022] was introduced, where evolutionary algorithms operate only on the linear policy representations. However, these methods primarily apply evolution to the parameter networks rather than the model structure itself. NEAT [Stanley and Miikkulainen, 2002] applies evolutionary algorithms to the topology of neural networks, optimizing the network structure to improve agent performance. Despite its potential, these methods often lead to excessively large model structures during training, making it challenging to handle complex tasks.\nHyperNEAT [Stanley et al., 2018] further extends neuroevolution to the neural networks building blocks, hyperparameters, architectures, and even algorithms themselves. PathNet [Fernando et al., 2017], applied to continual learning problems, uses the genetic algorithm to search for optimal module paths in a model with multiple layers and modules. Once a task is trained, the optimal path and related modules for that task are fixed, and PathNet continues training for the next task. However, it requires a large number of modules and incurs high training costs. Our method allows for the simultaneous training of multiple tasks, improving learning efficiency and reducing the need for excessive modules by sharing parameters."}, {"title": "2.2 Multi-Task Reinforcement Learning", "content": "Multi-task learning [Caruana, 1997; Yang et al., 2017] aims to improve the efficiency of learning multiple tasks simultaneously by leveraging the similarities among them. However, when sharing parameters across tasks, gradients from different tasks may conflict. Policy distillation [Rusu et al., 2015; Teh et al., 2017; Parisotto et al., 2015] mitigates the above issue, but it requires introducing additional networks and policies. Some methods [Calandriello et al., 2014; Xu et al., 2020; Liu et al., 2021] formulate it as a multi-objective optimization problem, but suffer high computational and memory costs. The Mixture of Experts (MOE) model [Eigen et al., 2013] constructs multiple networks as experts to improve generalization and reduce overfitting. The Mixture of Orthogonal Experts (MOORE) [Hendawy et al., 2023] uses a Gram-Schmidt process to shape the shared subspace of representations generated by expert mixtures, encapsulating common structures between tasks to promote diversity. The Contrastive Module with Temporal Attention (CMTA) [Lan et al., 2023] method constrains modules to be distinct from each other and combines shared modules with temporal attention, improving the generalization and performance across multiple tasks.\nTo further improve the efficiency of parameter sharing, a layer-level model [Yang et al., 2020] has been proposed. It uses a routing network to generate module weights for each task, allowing a set of modules to be reconstructed into task-specific models. To address the issue of resource allocation for tasks of varying difficulty, Dynamic Depth Routing (D2R) [He et al., 2023] combined a routing network with the module-level model to dynamically allocate modules for different tasks. In contrast, our approach explicitly integrates task difficulty with the model structure, enabling the model to evolve adaptively throughout the training process."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Problem Formulation", "content": "The problem studied in this work can be defined as a set of Markov Decision Processes (MDPs) that need to be solved simultaneously. Each MDP is represented as a 5-tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)$, where $\\mathcal{S}$ is the state space and $\\mathcal{A}$ is the action space. The dimensions of both the state space and action space are identical for each task and typically share the same meaning in the real world, which forms the foundation for knowledge sharing in Multi-Task Reinforcement Learning (MTRL). $\\mathcal{P}$ is the state transition function $\\mathcal{P} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$."}, {"title": "3.2 Soft Actor-Critic", "content": "We train our framework with the Soft Actor-Critic (SAC) [Haarnoja et al., 2018] algorithm. It introduces soft policy update and maximum entropy during optimization, improving exploration and efficiency. SAC uses a soft Q-function $Q_{\\theta}(s_t, a_t)$ as the critic network parameterized by $\\theta$, and an actor network $\\pi_{\\phi}(a_t|s_t)$ parameterized by $\\phi$. Besides, SAC introduces a learnable temperature parameter $\\alpha$ as a penalty for entropy. The optimization objective is defined as:\n$\\begin{equation}\\label{eq1}\\nonumber J_{\\pi}(\\phi) = E_{s_t\\sim D} [E_{a_t\\sim \\pi_{\\phi}} [\\alpha\\log \\pi_{\\phi}(a_t|s_t) - Q_{\\theta}(s_t, a_t)]], \\end{equation}$\nwhere $D$ is the data sampled from the replay buffer. To maintain the entropy of the policy, SAC also introduces the following objective to optimize $\\alpha$:\n$\\begin{equation}\\label{eq2} J(\\alpha) = E_{a_t\\sim \\pi_{\\phi}} [-\\alpha \\log \\pi_{\\phi}(a_t|s_t) - \\alpha \\hat{H}], \\end{equation}$\nwhere $\\hat{H}$ represents the desired minimal entropy. During the optimization process, if $\\log \\pi_{\\rho}(a_t|s_t)$ increases with a decrease in $\\hat{H}$, Eq. 2 will control $\\alpha$ to increase accordingly."}, {"title": "3.3 Modular Network", "content": "To address the challenge of resolving multiple tasks with a single model, several approaches with varying structures have been proposed, as illustrated in Fig. 2. The output-level model decomposes the output layer into a multi-head module while sharing the backbone. The layer-level model [Yang et al., 2020] consists of multiple layers, each containing several modules. The input to the next layer is the weighted sum of the outputs from the previous layer, with the weights generated by a routing network. However, this approach requires that all modules participate in the prediction for each task and does not allow for cross-layer connections. The module-level model [He et al., 2023] allows the input to the next module to be a weighted sum of the outputs from a selected subset of previous modules, thus enabling the skipping of several modules. While this structure facilitates task-specific customization, it retains a fixed architecture throughout training and does not explicitly account for task difficulty. Furthermore, the model's structure is dependent on the predetermined hyperparameter for the number of modules, and both an excessively large or small number of modules can negatively impact the training process. Our work optimizes the model by integrating the module-level network with the model evolution framework, aiming to address the issues of the fixed model structure and the reliance on hyperparameters."}, {"title": "4 Method", "content": "In this section, we first introduce the overall structure of the MEGA framework and elaborate its core components. Then, we explain how the genotype policy is used to reconstruct the model, and how genetic algorithms are applied to select, crossover, and mutate the genotype policies within the task population. Finally, we introduce the model evolution framework, which enables continuous evolution in response to increasingly complex tasks, dynamically allocating different amounts of resources to each task based on its difficulty."}, {"title": "4.1 Genotype Module-Level Model", "content": "The overall structure of the genotype module-level model is shown in Fig. 3, which includes the genotype policy selection and the module-level model. When the agent handles a task, it first selects a genotype policy from the multi-task community based on the task ID. The genotype is then split and transformed into module weights. After embedding, the states are converted into the initial output $m_0$, which is then passed as input to the first module. The input to each subsequent module is the weighted sum of the outputs from the previous modules as follows:\n$\\begin{equation}\\label{eq3} m_0 = embedding(s), \\end{equation}$\n$\\begin{equation}\\label{eq4} m_{i} = \\sum_{j=0}^{i-1} W_{ij} m_j \\end{equation}$\nThe output dimension of each layer is consistent and can be fed into the execution module to generate the action."}, {"title": "4.2 Genotype Policy", "content": "The genotype policy determines the weights of the model's output and is represented by a binary sequence, which facilitates optimization through genetic algorithms. The length of the genotype policy is determined by the precision of the weights $p_w$ and the number of modules $n_m$. For the $i$-th level module, its input consists of the weighted sum of the state representation module and the outputs of the previous $i-1$ modules, requiring $i$ weights. Therefore, the total length of the genotype policy is given by:\n$\\begin{equation}\\label{eq5} l_g = p_w (n_m + 1)n_m . \\end{equation}$\nWhen converting the genotype policy into module weights, we first decompose the genotype policy according to the number of layers and modules and then convert the corresponding binary sequences into decimal values $d_{i,j}$, where each $d_{i,j}$ represents the weight corresponding to the $m_j$ when it is input to the $i$-th module. Subsequently, the decimal values $d_{i,j}$ using the Softmax function to normalize them into weights. It is important to highlight that the binary-to-decimal conversion yields values within the range $[0, 2^{p_w} - 1]$. After applying the Softmax function, the weight values for each input to the $i$-th module are constrained to the interval:\n$\\begin{equation}\\label{eq6} w_{i,j} \\in [\\frac{1}{1 + (i - 1)e^{2^{p_w}-1}}, \\frac{e^{2^{p_w} - 1}}{e^{2^{p_w} - 1} + i - 1}]. \\end{equation}$"}, {"title": "4.3 Genetic Algorithm", "content": "The multi-task community contains all genotype policies, each assigned to a specific task population based on the corresponding task. Within each task population, each genotype policy is dedicated to optimizing the same task. We employ the genetic algorithm to optimize each task population, which involves evaluation, selection, crossover, and mutation operations.\nIn multi-task reinforcement learning, evaluating each genotype policy for every task can be computationally expensive. To address this, we perform evaluations concurrently during training. When starting a task, we first select a genotype policy from the corresponding task population. Unevaluated genotype policies are given priority for selection. If all genotype policies have been evaluated, a policy is randomly selected from the task population. The currently optimal genotype policy has a higher probability of being selected to ensure the agent samples higher-quality data. This data sampling strategy also increases the entropy of the sampled policies, promoting better exploration. After completing a simulation, the selected genotype policy receives its total reward as its fitness, which serves as a key reference for determining whether the genotype policy is retained and undergoes reproduction.\nAfter evaluation, each task population will discard a certain percentage of genotype policies based on their fitness. The remaining genotype policies are then optimized using crossover and mutation operators. The crossover operator selects two genotype policies from a task population and exchanges parts of their genes to form a new genotype policy. The mutation operator selects one genotype policy and flips part of its genes to generate a new genotype policy. The optimal genotype policies are given a higher probability of reproduction, allowing the task populations to evolve continuously in an optimal direction while maintaining diversity. The details of the genetic algorithm are shown in Fig. 4 and Alg. 1. The pseudo codes for the crossover and mutation operators are provided in supplementary material. Due to the lack of a suitable evaluation metric for the critic network, we continue using the module-level model as the critic."}, {"title": "4.4 Model Evolution", "content": "To address the issue of the predetermined number of modules, we propose the model evolution framework, as shown in Fig. 5. Each task is assigned a stage, which determines the maximum number of modules the agent should use for that task. If the agent is unable to complete the task, the stage gradually increases during the training process until the agent can either complete the task or make significant progress on it. This design ensures that, in the early stages of training, the agent uses a minimal module quantity, enabling efficient learning for easier tasks or skills. It also enables the agent to allocate more resources focus on difficult tasks, as additional modules will not be used to handle the tasks that are already solved. Ideally, this approach enables the agent to train specialized modules for common sub-skills, facilitating knowledge sharing and reuse across multiple tasks.\nThe length of a genotype policy is determined by the stage of the corresponding task. When the stage of a task increases due to it is unable to be completed, we discard all genotype policies whose length does not meet the current stage during the optimization of the task population. These discarded policies are replaced by randomly generated genotype policies that meet the required length. As the task populations are independent, the replacement of a genotype policy for one task does not affect the training of other tasks, ensuring the stable training of the model evolution framework."}, {"title": "5 Experiments", "content": "In this section, we conduct several experiments to investigate the following questions:\n\u2022 Whether the MEGA framework contribute to improved performance and learning efficiency?\n\u2022 Is the module-level model sensitive to the predetermined number of modules, and can the model evolution framework address this issue?\n\u2022 Does the genetic algorithm effectively optimize the task populations to specialize in specific tasks?"}, {"title": "5.3 Quantitative Results", "content": "We trained our MEGA framework and baseline algorithms on MT10-Fixed, MT10-Rand, MT50-Fixed and MT50-Rand. The performance of each algorithm was evaluated based on its task success rate. Each algorithm was run across 5 seeds, with training conducted for 7,500 episodes per task in MT10 and 5,000 episodes per task in MT50, using 200 timesteps per episode. The results, averaged over the 5 seeds, are shown in Fig. 6 and Tab. 1. Compared to the baseline algorithms, the MEGA framework consistently achieves the highest success rates in both fixed and random goal settings for the MT10 and MT50 tasks. We also compared the module utilization efficiency between MEGA and D2R, which share a similar structure. As shown in Fig. 2, the MEGA model uses fewer modules for each task compared to D2R, with an average reduction of 5.2 modules per task, which further validates the effectiveness and efficiency of MEGA."}, {"title": "5.4 Ablation Study", "content": ""}, {"title": "Impact of Model Evolution Framework", "content": "The model without the evolutionary mechanism is configured with 16 modules, consistent with the D2R setting. Compared to the full MEGA framework, the performance of this model is reduced and closer to that of D2R, as shown in Fig. 9. This result indicates that the performance improvement in MEGA is primarily due to the evolutionary mechanism.Additionally, it suggests that the optimization of the non-gradient evolutionary algorithm is comparable to that of the routing network. We also evaluated the model performance with different maximum module quantities. Models with 8 and 24 modules performed worse than the model with 16 modules, demonstrating that both insufficient and excessive module quantity negatively impact performance. Therefore, the model is sensitive to the module quantity, emphasizing the significance of dynamic model structure."}, {"title": "Impact of Model Genetic Algorithm", "content": "In Fig. 8 (a) and (b), we investigate the impact of the genetic algorithm on model performance. When the genetic algorithm is removed and the model is controlled by randomly generated genotype policies, performance significantly decreases, particularly in the MT50 setting, which demands higher generalization across tasks. We also present the t-SNE visualization of the genotype policies for each task in Fig. 10. For each task, we collected multiple sets of genotype policies. Since the genotype policies are generated through crossover and mutation operations, where cross-population breeding may occur, leading to some overlap in the result. Despite this, all the tasks remain clearly distinguishable from one another, indicating that the task populations, composed of genotype policies, are specialized for their respective tasks."}, {"title": "Impact of Model HalfSoftmax Function", "content": "We introduced the HalfSoftmax function to convert genotype policies into module weights to enable the model to skip irrelevant modules. In Fig. 8 (c) and (d), we compare the model's performance using the HalfSoftmax and SoftMax functions. The results demonstrate that incorporating HalfSoftmax enhances the model's flexibility, enabling the agent to more efficiently leverage cross-task knowledge, thereby improving both performance and learning efficiency."}, {"title": "6 Conclusion", "content": "In this work, we propose the MEGA framework, which enables adaptive evolution during the training process in a multi-task setting. When the number of modules is insufficient to handle a task, the model evolution mechanism of MEGA can automatically add modules to enhance the model's overall capacity. To effectively allocate modules for each task in a dynamic model with varying module quantity, we introduce the genotype module-level model to generate module weights using binary sequence genotype policies. By adjusting the length of the genotype policy, the model can adapt to different module quantity. e use the genetic algorithm to optimize the genotype policies, with the episode reward serving as the fitness for each task. Based on fitness, high-quality genotype policies are selected for generating new genotypes by crossover and mutation operations, guiding the task population to evolve towards task specialization. Our experiments in the Meta-World benchmark show that the MEGA framework outperforms several baseline models.\nIn future work, we aim to further explore the integration of genetic algorithms with deep reinforcement learning, combining both gradient-based and non-gradient optimization methods to improve model performance and sample efficiency. This will help develop sustainable learning capabilities for reinforcement learning tasks."}, {"title": "A Genotype Policy Decomposition", "content": "The process of genotype policy decomposition is illustrated in Fig. 11. First, the genotype policy is divided into segments based on the layers, with each segment corresponding to the weights of modules in a single layer. Each segment is then further split according to the module weight precision, where each sub-segment corresponds to the weight of an individual module. Finally, the binary genotype is converted into decimal, and the HalfSoftmax function is applied to transform the decimal values into module weights."}, {"title": "B HalfSoftmax Function", "content": "In this section, we describe the calculation method and significance of the HalfSoftmax function.\nDue to the limited precision of the genotype policy, the generated weights are constrained to discrete values, which restricts the range of possible weights. As shown in Fig. 12 (a), the module weights generated by the Softmax function for each layer exhibit this limitation. Specifically, for the earlier layers, the minimum module weight fails to reach 0, preventing the model from skipping irrelevant modules. As the number of layers increases, the maximum weight value decreases, which affects the expression of important modules.\nTo enhance the genotype policy's ability to select modules, we introduce the HalfSoftmax function. Specifically, after converting the binary values into decimals, an exponential operation is applied, followed by subtracting 0.99. The resulting value is then normalized to obtain the weight. The formula is as follows:\n$\\begin{equation}\\label{eq8} \\hat{d}_{i,j} = e^{d_{i,j}} - 0.99, \\end{equation}$\n$\\begin{equation}\\label{eq9} w_{i,j} = \\frac{\\hat{d}_{i,j}}{\\sum_{n=0}^{i-1} \\hat{d}_{i,n} } = \\frac{e^{d_{i,j}} - 0.99}{\\sum_{n=0}^{i-1} e^{d_{i,n}} - 0.99 \\cdot i}. \\end{equation}$\nFig. 12 (b) shows the range of module weights generated by the HalfSoftmax function for each layer. As observed, the values generated by the HalfSoftmax function more fully cover the interval [0, 1], enabling a more precise selection of relevant modules for model inference."}, {"title": "C Pseudo Code", "content": "We present the pseudo code for the method used to select parent genotype policies for crossover and mutation in Alg. 1, along with the pseudo code for the crossover and mutation operators."}, {"title": "D Parameters", "content": "Here we present the hyperparameter settings of the experiments and genetic algorithm in Tab. 2 and Tab. 3. Our experiments were conducted on GeForce RTX 2080Ti and GeForce RTX 3070Ti GPUs."}]}