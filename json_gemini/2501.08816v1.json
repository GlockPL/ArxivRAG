{"title": "IDEA: Image Description Enhanced CLIP-Adapter", "authors": ["Zhipeng Ye", "Feng Jiang", "Qiufeng Wang", "Kaizhu Huang", "Jiaqi Huang"], "abstract": "CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multi-modal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model's performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named \u201cIMD-11.\u201d Our code and data are released at https://github.com/FourierAI/IDEA.", "sections": [{"title": "1. Introduction", "content": "While animals primarily perceive the world through their visual system, only humans have evolved language systems over millions of years. Language enables humans to understand, use, and create things in a logical reasoning manner, ultimately evolving into intelligence. In computer vision, some studies [1, 2, 3] have shown that incorporating language/text information into vision tasks can significantly enhance a model's visual understanding capabilities and therefore improve its performance. CLIP (Contrastive Language-Image Pre-training) [1] is a dual-tower structure Vision-Language Model (VLM) that consists of a visual encoder and a textual encoder. CLIP is pre-trained on large-scale image-text pairs using contrastive learning. During this process, the image and text data interact with each other, endowing the model with a generalization ability and leading CLIP to be able to classify unseen images during training (called zero-shot learning)[4, 5].\nFine-tuning CLIP for downstream vision tasks has become a hot research topic in recent years [6, 7]. Notably, PEFT is a novel fine-tuning method, which freezes the parameters of the model's backbone and fine-tunes the incorporated lightweight learnable parameters on downstream datasets [8]. PEFT achieves or even surpasses the performance of full fine-tuning on some tasks. Recent studies focus on exploring PEFT for CLIP. Linear Probe [1] utilizes CLIP's vision encoder to extract features, which are subsequently fed to a linear layer for training, enabling it to handle few-shot image classification tasks where a very limited number of samples are available for each class of data [9, 10, 11]. Subsequent research [12, 13] focuses on exploiting text features to enhance the performance of few-shot learning. As shown in Fig. 1, CoOp [12] and CoCoOp [13] improve few-shot image classification performance by incorporating learnable text prompts. CLIP-Adapter [14] optimizes a vision adapter, which is a two-layer Multi-Layer Perceptron (MLP), to learn new vision features for few-shot image classification tasks. In Training-Free CLIP-Adapter (Tip-Adapter) [15], the two-layer MLP is replaced by a cache model, leading to a significant boost in the performance of few-shot image classification tasks."}, {"title": "2. Related Work", "content": "In this section, we review the literature related to the paper, including Vision-Language Model (VLM) and Parameter-Efficient-Fine-Tuning (PEFT).\n2.1. Vision-Language Model\nModality is the way humans perceive and recognize the world, which includes vision, text, auditory, touch, and etc [17, 18, 19]. For humans, vision and text are the main ways they perceive the world, which has attracted extensive research interests [7, 20, 21] from scholars around the world. The invention of Transformer [22] provides a unified model for both computer vision (CV) and natural language processing (NLP), and gives birth to the development of VLM [1]. VLM is a kind of pre-training model, the training methods of which are mainly categorized into Image-Text Contrastive Learning and Pre-training with Generative Objectives [17].\nImage-Text Contrastive Learning. Image-Text Contractive Learning is the most common method for training VLMs. It employs contrastive learning to process input image-text pairs, ensuring that the image-text pairs with similar semantics are close in the embedding space, and the image-text pairs with different semantics are far away in the embedding space. CLIP [1] collects and cleans 400 million image-text pairs from the internet, and pre-trains them by using InfoNCE loss. Subsequently, CLIP performs zero-shot image classification by evaluating similarities between test samples and category names. The same pre-training method is adopted in ALIGN [2], which collects 1.8 billion noisy image-text pairs and gets a favourable result as well. The success of ALIGN verifies that multimodal could learn good"}, {"title": "2.2. Parameter-Efficient Fine-Tuning", "content": "In the past few years, fine-tuning the pre-trained models on large-scale datasets to adapt downstream tasks has dominated the deep learning paradigm. However, there are significant disadvantages with this approach [8]. Firstly, in terms of large-scale models, full fine-tuning is challenging, time-consuming, and unsustainable. Secondly, fine-tuning large models on downstream tasks could potentially cause catastrophic forgetting. To tackle these issues, some scholars proposed Parameter-Efficient Fine-Tuning (PEFT) [8, 35]. PEFT is a new fine-tuning method, it freezes all the parameters of backbone and adapts the model to different downstream tasks by fine-tuning the parameters of additional modules attached to the model. PEFT is roughly categorized into two types: Prompt Tuning, and Adapter Tuning.\nPrompt tuning. Prompt tuning adapts downstream tasks by adding learnable tokens in either input or hidden layers of the model as learnable prompts. Inspired by prompt learning in NLP, VPT [36] adopts prompt tuning technology to vision tasks for the first time. VPT fine-tunes the model by adding some prompt tokens in the input space and outperforms most full fine-tuning methods in multiple tasks. CoOp [12] employs learnable token vectors instead of manually designed prompts as input for the text encoder, achieving commendable performance in few-shot image classification tasks. On the basis of CoOp, CoCoOp [13] designs a lightweight neural network to generate prompts for each image, which is known as Conditional Prompt Learning.\nAdapter tuning. In adapter tuning, the model is equipped with additional learnable layers (e.g. MLP, Transformer [37]) to adapt to the downstream tasks. CLIP-Adapter [14] uses an additional lightweight bottleneck layer (i.e. two linear layers following the last layer of vision encoder and text encoder, respectively) to learn new features and fuses them with the original pre-trained features via residual connection. Tip-Adapter [15] makes use of key-value pairs collected from the few-shot training set to construct the adapter, which is called cache model. The linear layers in the CLIP-Adapter is replaced by the cache model, rendering Tip-Adapter a training-free method and being superior to other few-shot classification methods. Based on the Tip-Adapter, the keys in the cache model are dynamically updated using stochastic gradient descent, further enhancing the performance of Tip-Adapter and achieving the SOTA result."}, {"title": "3. Method", "content": "In this section, we elaborate on the proposed methods. Firstly, we briefly review the zero-shot image classification of CLIP. Then, we describe IDEA and T-IDEA in detail, respectively. Last, we introduce the process of generating image descriptions.\n3.1. Revisiting zero-shot image classification of CLIP\nThe CLIP model is trained on a large-scale image-text pairs dataset by contrastive learning. It mines the semantic association between image-text pairs and enables the model to obtain a high generalization ability, achieving SOTA results in several vision downstream tasks. CLIP adopts a zero-shot classification strategy where the test image is retrieved against the textual information of category names to find the most matching category as the classification result. This ensures that CLIP can achieve open-vocabulary classification without re-training.\nSpecifically, given a test image $I_{test}$, we feed it into the vision encoder of CLIP to obtain the corresponding visual feature $i_{test} \\in R^{D\\times1}$, where D is the dimension of the visual feature. Eq. 1 describes the process.\n$i_{test} = VisionEncoder(I_{test})$\nThen, let N be the number of categories and $S_{label}$ be the set of category names. A manually designed prompt template (e.g. a photo of a {object}) is used to generate a textual prompt template for each of the categories. Next, the textual prompts are fed into the CLIP's text encoder to obtain the corresponding features $T_{class} \\in R^{N\\times D}$, as shown in Eq. 2.\n$T_{class} = TextEncoder(Template(S_{label}))$\nFinally, we get the output logits $\\in R^{N\\times1}$ for classification, as denoted in Eq. 3.\n$logits = T_{class} \\cdot i_{test}$\nwhere $\\cdot$ means matrix multiplication, and both $T_{class}$ and $i_{test}$ are normalized in the feature dimension. The classification result of CLIP is the index corresponding to the maximum value of the logits. For convenience, we refer to $T_{class} \\cdot i_{test}$ in Eq. 3 as the zero-shot knowledge."}, {"title": "3.2. Image Description Enhanced CLIP-Adapter", "content": "Based on the zero-shot classification with CLIP, we propose an novel adapter called Image Description Enhanced CLIP-Adapter (IDEA), in which we explore the few-shot knowledge from the image-text pairs to strengthen CLIP.\nFirstly, we construct a K-shot N-class training set that contains both visual information and textual descriptions of the images. Then, we freeze the parameters of CLIP's visual and textual encoders for PEFT. Eq. 4 demonstrates that the image in the training set is fed into the visual encoder to get the visual features $I_{train} \\in R^{NK\\times D}$, and the text in the training set is fed into the textual encoder to get the textual features $T_{train} \\in R^{NK\\times D}$.\n$I_{train} = VisionEncoder(Image)$\n$T_{train} = TextEncoder(Text)$\nSubsequently, we compute the multimodal similarities, as shown in Eq. 5.\n$Sim_{I} = I_{train} \\cdot i_{test}$\n$Sim_{T} = T_{train} \\cdot i_{test}$\nwhere $Sim_{I} \\in R^{NK\\times1}$ is the similarity between the test image and the images in the training set, $Sim_{T} \\in R^{NK\\times1}$ is the similarity between the test image and the textual description in the training set.\nIDEA computes the similarity between the test image and K samples of each category in the training set, which is referred to as few-shot knowledge and facilitates the mining of fine-grained semantic correlations between images and texts. Previous studies [1, 12] indicate that incorporating textual information into visual models can effectively enhance their logical reasoning capabilities. Thus, we utilize both visual and textual information in the training set to promote recognition ability.\nFinally, by combining zero-shot knowledge and few-shot knowledge, we get the output logits $\\in R^{N\\times1}$, as shown in Eq. 6:\n$logits = \\beta g{f[(1 \u2013 \\alpha)Sim_{I} + \\alpha Sim_{T})]} + T_{class} \\cdot i_{test}$\nwhere $\\alpha \\in [0, 1]$ is a hyperparameter to balance the similarity between visual modal and textual modal, and $\\beta\\in (0,\\infty)$ is a hyperparameter to trade off the few-shot knowledge and the zero-shot knowledge. The activation function $f(x) = exp(\\theta(x - 1))$ is defined for mapping the value of similarity to the interval [0,1]. $\\theta\\in (0,\\infty)$ controls the sharpness of the activation function, which dynamically stretches and compresses the value of similarity to better fuse the few-shot knowledge to the zero-shot knowledge. Given the multimodal similarity among samples $X \\in R^{NK\\times1}$, we define a function $g(X) = \\Sigma_k reshape(X, N, K)$ to aggregate the similarity among samples to form the few-shot knowledge. g(X) reshapes X into a matrix with N rows and K columns. We then sum the matrix in the column dimension. This"}, {"title": "3.3. Trainable Image Description Enhanced CLIP-Adapter", "content": "IDEA does not require stochastic gradient descent (SGD) to train the model and exhibits strong recognition performance in few-shot classification tasks. Even so, we believe that the performance of IDEA can be further improved. Therefore, we propose a Trainable Image Description Enhanced CLIP-Adapter (T-IDEA) method.\nOn one hand, we believe that there is a native intermodal semantic gap between visual and textual information when calculating the item of $T_{train} \\cdot i_{test}$ in Eq. 5. To overcome this problem, we design a lightweight projection layer $W_{proj} \\in R^{D\\times D}$ for intermodal semantic alignment, and utilize residual connection for modal fusion.\nOn the other hand, for few-shot image classification tasks, the selected K samples cannot supplemently cover all the samples in the training set, which means there is a bias of semantics between the K samples and all the samples. Therefore, we design a trainable semantic latent space $E_{bias} \\in R^{NK\\times D}$ to correct the bias in the semantic space of the training set.\nTherefore, the formula for the logits of the T-IDEA is defined in Eq. 7.\n$logits = \\beta g{f[(1 \u2013 \\alpha)I_{train} \\cdot i_{test} + \\alpha(T_{train} \\cdot W_{proj}i_{test} + T_{train} \\cdot i_{test}) + E_{bias}i_{test} ]} + T_{class} \\cdot i_{test}$\nwhere $W_{proj} \\in R^{D\\times D}$ and $E_{bias} \\in R^{NK\\times D}$ are lightweight components."}, {"title": "3.4. Image Description Generation", "content": "To our knowledge, existing visual datasets generally lack corresponding image descriptions, and labelling these datasets is laborious. Therefore, we employ Llama [16], a multimodal large-scale model, to generate a textual description for each image. Fig. 3 illustrates the pipeline of generating image descriptions. Firstly, we customize the textual prompt for each image dataset to guide the description generating. Then, we clean the original data to reduce task-irrelevant noise (e.g. escape character, special symbol, and markdown formatting). Finally, we utilize the BART [38] model to summarize text descriptions and compress the text length to fewer than 77 tokens, which is the maximum length of the CLIP text encoder.\nFig. 4 shows our method for designing prompts as well as some examples of generated texts. For common vision datasets (e.g., ImageNet [39], Caltech101 [40]), we design generalized prompts to describe the image content. We first prompt the model for the category name of the image. Then we instruct the model to describe the image's content from the following aspects: shape, color, number of objects, texture, location, and details. For fine-grained image datasets (e.g. Food101 [41], Oxford Pets [42]), we customize prompts to generate domain-specific image descriptions. In particular, for the Oxford Pets dataset, we prompt the model for the subclass of pets. Then, we ask the model to generate image descriptions about the pet's hair, color, eyes, shape, ears, paws, pose, and position. Fig. 4 shows that the generated image descriptions are basically accurate and consistent with the image content.\nWhile the research on multimodel is becoming increasingly popular, large-scale image-text pair data is precious and much-needed. We supplement 11 popular image datasets (e.g., ImageNet [39], Caltech101 [40], and Oxford pets [42]) by generating textual descriptions for each image, producing 1,637,795 image-text pairs in total. We name the dataset as IMD-11 and publish the dataset on the Internet for public research."}, {"title": "4. Experiment", "content": "In this section, we first describe the basic settings of the experiments and the baseline models for the comparison experiments. Next, we quantitatively and qualitatively analyze the results of the comparison experiments on 11 public datasets. Finally, we perform several ablation experiments on IDEA and T-IDEA.\n4.1. Experiment Settings\nWe select 11 popular computer vision datasets for comparison experiments, including 2 common image classification datasets (ImageNet [39] and Caltech101[40]) and 9 fine-grained image classification datasets (Food101 [41], FGVCAircraft [43], StandCars [44], UCF101 [45], Flowers102 [46], SUN397 [47], EuroSAT [48], DTD [49] and OxfordPets [42]). All models are trained on the training set under 1, 2, 4, 8, and 16 shots settings. For a fair comparison, the partition criteria of the training set, validation set, and test set are the same as CoOp [12], CLIP-Adapter [14], and Tip-Adapter [15].\nAt the data pre-processing stage, we first randomly crop and scale the images with the size of 224 \u00d7 224. Then we randomly flip and normalize the tensor of images. For the T-IDEA method, we set 50 epochs to train the model with a bath size of 256, and employ the stochastic gradient descent (SGD) to fine-tune the model with a learning rate of 5 \u00d7 10-4.\nAll experiments are conducted on a server with an AMD EPYC\u2122 7642 processor, 4 NVIDIA\u00ae GeForce RTX 4090 graphics cards, 256GB memory, 6TB Solid State Drive (SSD), 8TB Hard Disk Drive (HDD), and the Ubuntu 22.04.3 LTS operating system.\nWe compare the IDEA method and T-IDEA with five baseline models, i.e. Zero-shot CLIP [1], CoOp [12], CLIP-Adapter [14], Tip-Adapter[15], and Tip-Adapter-F [15]. All the comparison data are taken from the best result published in the original paper. To make it fair, in the comparison experiments, our method uses ResNet-50 [50] as the visual encoder and Transformer [37] as the textual encoder, which is the same configuration as the five baseline models mentioned above."}, {"title": "4.2. Performance Comparison and Analysis", "content": "In this section, we conduct experiments to compare IDEA and T-IDEA with 5 baseline models on 11 publicly available image datasets.\nFig. 5(a) shows the average performance of each model on the 11 image datasets. As observed, IDEA outperforms the CoOp model, which requires additional training steps, under 1, 2, 4, and 8 shots settings. Compared to the Tip-Adapter method, which is also training-free, IDEA outperforms it by 0.63%, 0.12%, 0.59%, 0.39%, and 0.5% under 1, 2, 4, 8, and 16 shots settings, respectively. This reveals that fusing the multimodal data (visual and textual features) in the training set is favorable to improve the model's performance. In addition, T-IDEA performs better than IDEA. As the number of shots increases, T-IDEA shows more advantages over IDEA. This phenomenon implies that it is crucial to design additional training components to fine-tune the model to better fit new features in the dataset. It is worth noticing that T-IDEA equipped with two learnable components outperforms Tip-Adapter-F under 1, 2, 4, 8, and 16 shots settings by 0.86%, 0.99%, 0.82%, 1.03%, and 0.65%, which achieves SOTA performance.\nFig. 5(b) and (c) indicate that both IDEA and T-IDEA methods achieve good performance in the common datasets. It is notable that on the Caltech dataset, under the 8-shot training setting, IDEA improves by 0.47% over the Tip-Adapter method, and T-IDEA outperforms the SOTA model, Tip-Adapter-F, by 1.26%. Fig. 5(d-1) shows that IDEA and T-IDEA methods achieve SOTA performance in most fine-grained image classification datasets. For example, in the OxfordPets and Food101 datasets, IDEA under the 1-shot and 2-shot settings demonstrates comparable performance with that of the SOTA model, even though the IDEA method has no extra training steps. This confirms the advantage and superiority of IDEA especially when the category samples are limited. Meanwhile, T-IDEA achieves SOTA performance on most fine-grained image datasets. For example, on the FGVCAircraft dataset, T-IDEA outperforms Tip-Adapter-F by 2.97% under the 16-shot setting, which is a significant boost.\nIn addition, we notice that our method does not perform well on some domain-specific fine-grained datasets. In Fig. 5, we observe that, for the EuroSAT dataset, T-IDEA improves less compared to the SOTA method under 8, 16 shots settings. Given that the EuroSAT dataset is a remote sensing image classification dataset with the relatively smaller image size of 64 \u00d7 64, it is difficult to describe the content information of the image in textual language due to the low resolution and abstract content. We infer that this may be an important reason for the limited improvement of our method on this dataset."}, {"title": "4.3. Ablation Studies", "content": "In this section, we perform several ablation studies of IDEA and T-IDEA on the ImageNet dataset, under the 16-shot training setting, to validate the effectiveness of each component.\n4.3.1. Ablation Study of Hyperparameters\nThe hyperparameter $\\alpha$ is designed to balance the visual similarity and the similarity of image-text pairs, as shown in Eq. 5. We set $\\beta = 2.75$ and $\\theta = 2$ and vary the value of $\\alpha$ from 0 to 1. When $\\alpha = 0$, it means that there is only visual similarity $Sim_{I}$, and when $\\alpha = 1$, it means that there is only the image-text pair similarity $Sim_{T}$. Table 1 implies that neither $Sim_{I}$ nor $Sim_{T}$ could achieve optimal performance alone. The method achieves optimal performance when $\\alpha = 0.5$, which indicates that visual and textual information are equally important."}, {"title": "4.3.2. Ablation Study of Trainable Components", "content": "In this subsection, we perform ablation experiments on two learnable components (the projection layer $W_{proj}$ and the semantic latent space $E_{bias}$) added to T-IDEA. They are plugged and unplugged separately for a total of 4 sets of experiments.\nTable 2 shows that if we remove the projection layer $W_{proj}$ from the T-IDEA, the performance decreases by 1.7%, suggesting that the projection layer $W_{proj}$ can effectively eliminate the semantic gaps between visual and textual to some degree, and achieve intermodal semantic alignment. When we remove the semantic hidden space $E_{bias}$ from T-IDEA, the performance decreases by 2.77%, indicating that the design of $E_{bias}$ is able to reduce the semantic bias, leading to the performance improvement of the model. Overall, compared to the IDEA method without any trainable components, T-IDEA improves the classification metric by 3.47%, demonstrating that combining the two components can significantly improve the model's performance."}, {"title": "4.3.3. Ablation Study of Vision Backbones", "content": "To verify the scalability of the proposed methods, we conduct further ablation experiments equipped with various backbone networks. Specifically, we replace the vision encoder in the Zero-Shot CLIP [1], CoOp [12], CIIP-Adapter [14], Tip-Adapter [15], Tip-Adapter-F [15], IDEA, and T-IDEA models with ResNet-50 [50], ResNet-101 [50], ViT-B/32 [51], and ViT-B/16 [51], respectively. From Table 3, we observe that under different settings of the backbone network, there is a significant performance improvement compared to the zero-shot CLIP model using only zero-shot knowledge. The performance of IDEA and T-IDEA is also improved when the parameter size of the backbone network increases. Furthermore, under different backbone settings, T-IDEA achieves SOTA performance. This indicates that our method can adapt to various backbone networks and thus demonstrates a strong generalization ability."}, {"title": "5. Conclusion and Future Work", "content": "Vision and language can semantically complement each other to enhance the ability of humans to perceive the world. Different from previous PEFT methods, we introduce a multimodal adapter to mine the multimodal information in image-text pairs, and it is fully adapted for few-shot image classification tasks. The training-free IDEA method has even outperformed the approaches that necessitate additional training steps. T-IDEA extends the IDEA method by integrating a learnable semantic alignment component and a semantic latent space component, achieving SOTA performance on 11 datasets. In addition, we design a comprehensive pipeline to generate 1.6 million image-text pairs and we publish our dataset online.\nAlthough the performance of our methods is excellent, optimizing the text prompts could yield further enhancements. Exploring synthetic data to train models presents an intriguing area for future research. Some researchers have successfully utilized generated data from LLMs and achieved positive results [16, 34]. In the future, we plan to investigate Chain of Thought (COT) [52] to generate higher-quality data from LLMs. In addition, the maximum length of input tokens in CLIP is limited to 77, constraining the amount of textual information. In the future, we will endeavor to apply IDEA and T-IDEA to Long-CLIP [53]."}]}