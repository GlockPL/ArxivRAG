{"title": "Pre-training a Density-Aware Pose Transformer for Robust LiDAR-based 3D Human Pose Estimation", "authors": ["Xiaoqi An", "Lin Zhao", "Chen Gong", "Jun Li", "Jian Yang"], "abstract": "With the rapid development of autonomous driving, LiDAR-based 3D Human Pose Estimation (3D HPE) is becoming a research focus. However, due to the noise and sparsity of LiDAR-captured point clouds, robust human pose estimation remains challenging. Most of the existing methods use temporal information, multi-modal fusion, or SMPL optimization to correct biased results. In this work, we try to obtain sufficient information for 3D HPE only by modeling the intrinsic properties of low-quality point clouds. Hence, a simple yet powerful method is proposed, which provides insights both on modeling and augmentation of point clouds. Specifically, we first propose a concise and effective density-aware pose transformer (DAPT) to get stable keypoint representations. By using a set of joint anchors and a carefully designed exchange module, valid information is extracted from point clouds with different densities. Then 1D heatmaps are utilized to represent the precise locations of the keypoints. Secondly, a comprehensive LiDAR human synthesis and augmentation method is proposed to pre-train the model, enabling it to acquire a better human body prior. We increase the diversity of point clouds by randomly sampling human positions and orientations and by simulating occlusions through the addition of laser-level masks. Extensive experiments have been conducted on multiple datasets, including IMU-annotated LidarHuman26M, SLOPER4D, and manually annotated Waymo Open Dataset v2.0 (Waymo), HumanM3. Our method demonstrates SOTA performance in all scenarios. In particular, compared with LPFormer on Waymo, we reduce the average MPJPE by 10.0mm. Compared with PRN on SLOPER4D, we notably reduce the average MPJPE by 20.7mm.", "sections": [{"title": "1 Introduction", "content": "3D human pose estimation (3D HPE) is a fundamental computer vision task with a wide range of downstream usages such as human behavior understanding, trajectory prediction, autonomous driving (Cong et al. 2022; Lian et al. 2022), etc. To implement 3D HPE, a simple and direct way is to regress 3D keypoint coordinates directly from 2D HPE results (Kang et al. 2023; Gong, Zhang, and Feng 2021). However, these methods have difficulty predicting world coordinates, and robust pose estimation in the real scenario remains challenging.\nTo get global 3D keypoints, most traditional 3D HPE methods are based on multi-view RGB images (Simon et al. 2017; Iskakov et al. 2019; Tu, Wang, and Zeng 2020; Ye et al. 2022) or RGB-D images (Ying and Zhao 2021; Hong and Kim 2018), which require in-door laboratory environments with complex calibrations (Su et al. 2020; Zheng et al. 2022). Alternatively, LiDAR sensors can obtain accurate point-level depth in complex environments, which are more adaptable to long-range 3D HPE in open-world scenes.\nHowever, compared to clear and dense point clouds generated by RGB-D images (Fan et al. 2018; Ionescu et al. 2014), the LiDAR-captured point clouds have various inconsistencies, making them difficult to learn directly with existing methods. As shown in Fig.2, the samples in Waymo Open Dataset (Sun et al. 2020) have various point densities, and the noisy points from the environment may lead to ambiguity and unstable predictions. Therefore, to achieve robust pose estimation, (Li et al. 2022a; Yan et al. 2024; Zhao et al. 2024) integrate ST-GCNs to get stabilized pose results from multiple LiDAR frames. (Cong et al. 2023; F\u00fcrst et al. 2021) perform multi-modal fusion to mitigate the lack of information in sparse point clouds, and (Li et al. 2022a; Yan et al. 2024; Zhang et al. 2024) introduce SMPL (Loper et al. 2015) as a priori to further align human mesh with point clouds. Despite the success of these methods, they inevitably introduce additional data acquisition or time-consuming optimization processes which complicate the entire framework and make it less suitable for practical applications.\nIn this paper, we propose a novel framework to learn stable representations for 3D HPE only using single-frame low-quality LiDAR point clouds. Our method is simple yet effective. Specifically, We provide insights into both model design for accurate pose estimation and model pre-training for effective body prior learning.\nFirstly, we design a Density-Aware Pose Transformer (DAPT) that provides stable and explicit representations of joints. Most existing models attempt to regress the coordinates of joints from global or clustered point-wise features. This makes the model highly susceptible to the density and spatial distribution of the point cloud. As shown in Fig.3, when the point cloud near the joint is sparse or noisy, the network cannot correctly identify which body part the point belongs to, resulting in a biased joint location. To solve this problem, we introduce a set of learnable joint anchors. When extracting point cloud features, they can explicitly integrate the information across multiple density levels through a carefully designed exchange module. Then, we utilize 1D-heatmaps on the XYZ axes to represent joint positions, which allows the model to obtain a stable output.\nSecondly, we introduce a comprehensive pre-training approach that conducts thorough LiDAR human synthesis and augmentation. Since annotating LiDAR data is expensive, inspired by (Weng et al. 2023a), we train on samples synthesized by SMPL mesh under ray casting. To tackle noisy and sparse point clouds in real scenarios, we add a square surface that can be varied in small magnitudes, such as the ground surface, and put the human mesh into the scene with randomized poses and positions. On the other hand, since the light beams are highly susceptible to occlusion by foreground objects, we mask the range image with patches to simulate the occlusion. Therefore, the model will learn prior knowledge of the human body and mine important clues about the pose in low-quality point clouds.\nWith the synergy of pre-training and DAPT, our approach provides a robust way to understand human body configurations in outdoor environments. Although the proposed method is optimization-free and does not use information from any other modalities or time series, we still outperform SOTA methods on multiple datasets. Specifically, our method compared with LPFormer (Ye et al. 2024) on the manually annotated Waymo Open Dataset (Sun et al. 2020) reduces the mean per joint position error (MPJPE) by 10mm (\u219316%). When compared with PRN (Fan et al. 2023b) on the IMU-annotated SLOPER4D dataset (Dai et al. 2023), it reduces the MPJPE by 20.7mm (\u219358%).\nIn summary, our contribution lies in three main aspects:\n\u2022 We propose a density-aware pose transformer that steadily mines pose cues from sparse and noisy point clouds.\n\u2022 We thoroughly investigate the difficulties of estimating human pose by LiDAR data and design a comprehensive pre-training approach.\n\u2022 Our method greatly improves the stability and accuracy of single-frame LiDAR-only human pose estimation, achieving SOTA performance in multiple scenarios."}, {"title": "2 Related works", "content": "2.1 LiDAR-based 3D human pose estimation\nIn recent years, many LiDAR point cloud-based 3D HPE methods have been proposed as the practical application value of LiDAR has been explored. (Li et al. 2022a) provides the first LiDAR HPE dataset and proposes the first fully supervised baseline for LiDAR-based motion capture. The keypoint coordinates are obtained by a temporal encoder and optimized with inverse kinematics and SMPL. (Yan et al. 2023) provides a climbing dataset with explicit scene interactions and attempts to perform scene-aware human pose estimation, followed by (Zhang et al. 2024) which uses environmental information of 3D neighbors sampled in the background to enhance the pose learning. (Ren et al. 2024b,a) achieves accurate motion tracking by exploiting temporal and spatial coherence. On the other hand, (Cong et al. 2023; Zheng et al. 2022; F\u00fcrst et al. 2021; Hu et al. 2024) perform multi-modal fusion, which utilizes the key-point cues and geometric constraints provided by the 2D images for weakly-supervised 3D human pose learning. (Ye et al. 2024) proposes a multitasking architecture that augments model learning with segmentation and object detection tasks, and uses a keypoint transformer for multi-person 3D HPE.\nDespite the success of these approaches, most of them inevitably use information from other modalities or utilize the priori of body structure provided by SMPL to compensate for unstable results on sparse or noisy LiDAR point clouds. This can cause an increase in the complexity and latency of the system. Therefore, we think that an optimization-free approach based only on single-frame LiDAR is more practical. This leads us to learn a stable representation of joint for LiDAR-based 3D HPE which increases the reliability of the results and avoids cumbersome post-processing."}, {"title": "2.2 Pre-training for human pose estimation", "content": "Because pre-training allows models to learn generalized and dataset-independent representations, it has become an important technique to improve the performance of downstream tasks (Devlin et al. 2019; He et al. 2020; Wang et al. 2021; Li et al. 2021, 2022b). On one hand, there have been attempts to use existing data for augmented representation learning. (Xu et al. 2022; An et al. 2024) enhances the accuracy of Vision Transformer-based 2D pose estimation by masked self-supervised pre-training (He et al. 2022). (Qiu et al. 2023) utilizes physical constraints provided by computational photography to conduct weakly supervised pre-training of 3D poses and improve the model's generalization. (Shan et al. 2022) brings an accuracy increase to 2D-to-3D lifting methods by applying a spatial-temporal mask of the skeleton.\nOn the other hand, some methods try to use synthetic data to make up for the lack of labeled pose data. (Lin et al. 2024) utilizes the multi-camera projection of SMPL models to generate enough labeled samples for pre-training. (Weng et al. 2023b) proposes a LiDAR scene generation approach based on human mesh under ray casting. (Ren et al. 2023, 2024b,a) craft large-scale synthetic datasets based on (Mahmood et al. 2019) to obtain rich human priors. However, this method is limited by not considering the relative positions of bodies and LiDAR sensors and ignoring the diversity of occlusions. In this paper, we perform a richer and more reasonable sampling of spatial positions so that the model can learn point cloud representations from more aspects. Moreover, we construct more complex occlusion and noise point clouds to simulate the real environment."}, {"title": "3 Methodology", "content": "Our approach consists of two parts: a) a density-aware pose transformer (DAPT) for stable joint representation learning, b) a comprehensive LiDAR human synthesis & augmentation for model pre-training. We adopt a two-stage training scheme, as shown in Fig.4, we first pre-train the model on synthetic samples, which are generated by ray casting with random augmentations and occlusions. When the samples are fed into the proposed model, they are first encoded as sparse point features, Then, the valid information is progressively extracted to a set of joint anchors through multi-density exchange (MDE) modules and decoded into 1-D heatmaps. Finally, we input real samples into the model for fine-tuning. Note that the network architecture is shared between pre-training and fine-tuning with only slight differences in the loss functions.\nIn this section, we present the proposed method module by module."}, {"title": "3.1 LiDAR human synthesis", "content": "The quality and diversity of samples used for pre-training will directly affect the effect of downstream fine-tuning. Inspired by (Weng et al. 2023b), we propose a more comprehensive strategy to sample and augment the scene.\nScene sampling Given shape parameters $\\beta\\in R^{10}$ and pose parameters $\\theta\\in R^{72}$ sampled from a real-world captured SMPL database (Li et al. 2022a), a human instance is simply generated by the SMPL model:\n\n${M_h, \\hat{J}} = SMPL(\\beta, \\theta)$,\n                                                               (1)\nwhere $M_h = {V_h\\in R^{N_v\\times3}, F_h \\in \\mathbb{Z}^{N_F\\times3}}$ is a human mesh with $N_v$ vertices $V_h$ and $N_F$ triangle faces $F_h$. $\\hat{J} \\in R^{K\\times3}$ are human joints.\nTo simulate real ground, a ground mesh $M_g = {V_g, F_g}$ is generated by a random normal vector \u0e17\u0e35\u0e48 with size $s_g$, locating at the point with a minimum value of Z axis in $V_h$.\nThen, we randomly sample a polar coordinate with distance $r \\in [4m, 20m]$ and azimuth $\\theta\\in [-\\pi, \\pi]$, and convert it to Cartesian transition t. Finally, the transformation is applied to $M_h$ and $M_g$ to get the scene mesh:\n\n$M = {V = (V_h\\cup V_g) + t, F = F_h \\cup F_g}$.\n                                                        (2)\nRay casting The LiDAR sensor obtains depth information through 360\u00b0 scans at different elevation angles. Thus, the 3D points it captured can be represented by polar coordinates $(r, \\theta_i, \\delta_i)$ within a laser grid $G = {\\theta_i}_{i=1}^{N_{\\Theta}} \\times {\\delta_j}_{j=1}^{N_{\\Delta}}$. Since the synthetic mesh only occupies a small angle range, for faster sample generation and to facilitate the subsequent application of laser-level masks, we only intercept the laser within valid azimuth and elevation angles:\n\n$\\Theta = [\\theta_{min}(V), \\theta_{max}(V)] \\cap {\\theta_i}_{i=1}^{N_{\\Theta}}$,\n$\\Delta = [\\delta_{min}(V), \\delta_{max}(V)] \\cap {\\delta_j}_{j=1}^{N_{\\Delta}}$,\n$R = {(\\theta_i, \\delta_i)|\\theta_i \\in \\Theta, \\delta_i \\in \\Delta}$,\n                                                                                                                      (3)\nwhere $\\theta_{min}(\\cdot)$, $\\theta_{max}(\\cdot)$ take the minimum and maximum azimuth angles of the vertices, and for the elevation angles, the corresponding values are taken by $\\delta_{min}(\\cdot)$, $\\delta_{max}(\\cdot)$. Then, ray casting is performed with the mesh to get the point cloud $P\\in R^{N\\times3}$ and which faces the lasers hit $H \\in \\mathbb{Z}^{N}$:\n\n$(P, H) = RayCast(M; R)$.\n                                                                                       (4)\nNote that a label map describing the joints to which triangle faces belong is given by the SMLP model. Hence, the ground truth segmentation $\\hat{S} \\in [0,1]^{N\\times(K+1)}$ of $P$ can be calculated by H, where $\\hat{S}_{i,j} = 1$ means the laser point $P_i \\in P$ belongs to the $j$ th joint.\nLaser-level masking To better simulate the occlusion in the real environment, we divide the effective LiDAR laser grid obtained in Eq.3 into patches with an empirical size $s_p = \\frac{min{\\theta_{max} - \\theta_{min}, \\delta_{max} - \\delta_{min}}}{8}$. Then, $T_{keep}$ ratio of the total patches are being masked, which are denoted as M. Finally, the point cloud is filtered by:\n\n$P_{syn} = {P_i | P_i \\in P, r_i \\notin M}$,\n                                                                    (5)\nwhere $r_i \\in R$ is the ray producing hit point $p_i$."}, {"title": "3.2 Density-aware pose transformer", "content": "We present a density-aware pose transformer based on UNet-like point transformers (Wu et al. 2024) to obtain stable joint representations of different densities. Specifically, the point cloud is firstly encoded into sparse point cloud features, which are subsequently fed into a decoder to be recovered to point-wise features. During the decoding process, we utilize an MDE module to export valid information from different pooling hierarchies and use 1D heatmaps to represent point locations, since heatmaps are generally more amenable to neural networks than coordinate regression.\nPoint cloud feature deduction with MDE Given an input point cloud $P\\in R^{N_0\\times3}$, it was encoded into latent point features $f_m$ by an encoder $\\mathcal{E}: R^{N_0\\times3} \\rightarrow R^{N_M\\times D_M}$. Where $N_0$ is the initial point numbers, $N_M$, $D_M$ are the pooled point numbers and pooled point feature dimensions of M th pooling level.\nAs shown in Fig.4 right, it can be seen that at deeper pooling levels, the pooled point cloud shows a sparser spatial distribution, which inspires us to model joint-related features at these levels. Therefore, a set of learnable joint anchors $A_M \\in R^{K\\times D_M}$ is introduced. As the spatial dimensions of the features are expanded through the point transformer blocks, MDE progressively exchanges information between point features $f_m$ and joint anchors. Then, it updates the joint features by:\n\n$A_{m-1} = MDE_m(A_m, f_m)$, $m = M, M - 1,..., 1$.\n                                                        (6)\nThe MDE module first aligns the dimensions of joint features $A_m$ to the dimensions of $f_m$ through an MLP, then applies self-attention on it while shortcutting it with padded $f_m$ for cross-attention.\nHeatmap decoder for joints Two 1D heatmaps have been proven to be effective in representing 2D keypoint coordinates (Li et al. 2022c). Inspired by this, we extend it to the representation of 3D coordinates of joints. Given the range and number of bins $N_{{x,y,z}}$, the heatmaps $h_{{x,y,z};i}$ of i th joint can be predicted by MLPs on the corresponding axis. To get the target coordinate, we decode the 1D heatmaps by taking the peak location of $h_{{x,y,z};i}$, and map them back to the coordinate through the counted range and bins.\nPre-training The goal of pre-training is to allow the model to obtain human priors in the LiDAR-captured point clouds and understand the structure of the human body. Therefore, at this stage, to avoid learning preferences caused by differences in supervision intensity, following (Weng et al. 2023a), we still use coordinate regression on joint features and segmentation on point features for supervision. The joint regression loss $L_{reg}$ is defined by:\n\n$L_{reg} = \\sum_{i=1}^{K} ||J_i - \\hat{J}_i||^2 v_i / \\sum_{i=1}^{K} v_i$,\n                                                                (7)\nwhere J are the decoded joints coordinate from A through a shared decoder $D_{reg} : R^{D_0} \\rightarrow R^3$, and $\\hat{J}$ are the corresponding ground truth, $v_i$ is the joint visibility. The segmentation loss $L_{seg}$ is defined by:\n\n$L_{seg} = - \\sum_{i=1}^{N_0} \\sum_{j=1}^{K+1} S_{i,j} log(\\hat{S}_{i,j})$,\n                                                        (8)\nwhere S are the decoded part segmentation of points from $f_0$ through a shared decoder $D_{seg} : R^{D_0} \\rightarrow R^{K+1}$. Overall, we input synthetic data and minimize:\n\n$L_{pre} = \\lambda_{reg}L_{reg} + \\lambda_{seg}L_{seg}$,\n                                                                                                                              (9)\nwhere $\\lambda_{reg}$, $\\lambda_{seg}$ are loss weights.\nFine-tuning The goal of the fine-tuning phase is to adapt the model to the joint annotations of different datasets and their specific distribution. Therefore, in this stage, we only enable the heatmap loss. We input real data and minimize:\n\n$L_{ft} = L_{hm} = \\sum_{c \\in {x,y,z}} \\sum_{j=1}^{K+1} D_{KL}(h_{c;j} || \\hat{h}_{c;j})$,\n                                                                                                                                                                    (10)\nwhere $D_{KL}$ is the Kullback-Leibler divergence between GT and predicted 1D heatmaps."}, {"title": "4 Experiment", "content": "4.1 Implementation Details\nFor the model specification, we follow the typical configuration of PTv3 (Wu et al. 2024), with a voxelization grid size of 0.01. For data synthesis, the SMPL models are sampled from (Li et al. 2022a), and a simulated LiDAR sensor with 64 lines and 2650 angles is applied to perform ray casting, the $r_{keep}$ is set to 0.6 for laser-level masking. For model training, we perform 50 epochs in both pre-training and fine-tuning with AdamW (Loshchilov and Hutter 2019) optimizer on 2 RTX 4090. We set the batch size to 64 and apply the cosine annealing decay strategy. For pre-training, we set the learning rate to $3 \\times 10^{-4}$ and set $\\lambda_{reg}$ = 0.5, $\\lambda_{seg}$ = 1.0. For fine-tuning, the learning rate is set to 5 \u00d7 10-4.\n4.2 Datasets\nWe use four datasets with different scenarios and annotation methods to evaluate our method:\nLiDARHuman26M (Li et al. 2022a) A multi-modal dataset uses inertial measurement units (IMUs) captured human poses in SMPL format. It uses a fixed LiDAR sensor to capture a variety of daily actions within a range of 12m to 24m. The scenes are clear and ideal.\nSLOPER4D (Dai et al. 2023) An IMU annotated dataset captured within a more realistic environment. A mobile LiDAR sensor is utilized to track a walking person for capturing point cloud data. Since the official train-test split is unavailable, we utilize the same data split following (Zhang et al. 2024).\nHumanM3 (Fan et al. 2023a) A multi-person pose dataset utilizing automatic annotation and manual review for accurate ground truth poses. It mainly includes multi-person scenes on sports fields, with point clouds and images captured by multiple sets of RGB-LiDAR units. Due to its large size and the small differences between adjacent frames, we only use 20% of the data for training.\nWaymo Open Dataset v2 (Mei et al. 2022) A large-scale multi-task autonomous driving dataset with manual 3D pose annotations, contains 10K human instances."}, {"title": "4.3 Metrics", "content": "According to common practices, to evaluate model performance, we report MPJPE\u2193 (Mean Per Joint Position Error), PA-MPJPE (Procrustes-Aligned Mean Per Joint Position Error), PCK-3\u2191 (Percentage of Correct Keypoints with distance to GT lower than 30% of torsal length), PCK-5\u2191 (Percentage of Correct Keypoints with distance to GT lower than 50% of torsal length). Note that we do not evaluate PA-MPJPE on Waymo and HumanM3, as their visibility labels of joints will interfere with the rigid body alignment process."}, {"title": "4.4 Comparison methods", "content": "We compare our method with several state-of-the-art approaches. Specifically, we first evaluate against advanced Transformer-based models, LPFormer (Ye et al. 2024) and PRN (Fan et al. 2023b), both of which are capable of inferring complete human body poses from single-frame LiDAR data. Additionally, we compare our method with LiDARCap (Li et al. 2022a), which leverages SMPL optimization and temporal information. We also include the Neighbor Enhanced (NE) 3HPE (Zhang et al. 2024), which incorporates information from the surrounding scene. To ensure a consistent comparison, we replace the point cloud backbone in PRN with PTv3. We note that LiDARCap and NE require consecutive frames and ground truth SMPL params, this limits the evaluation process on Waymo and HumanM3 datasets. Therefore, the results are not included."}, {"title": "4.5 Quantitative Results", "content": "Tab.1 presents the comparison results, where our method consistently achieves outstanding results across four datasets. On LiDARHuman26M with clear scenes, our method obtains comparable MPJPE to the current best NE but with a reduced PA-MPJPE (-3.2mm), suggesting closer alignment with GT. On the more challenging SLOPER4D, our method gets a remarkable 28.01mm MPJPE, representing a significant improvement over the current best PRN (-20.7mm). In the motion-focused dataset HumanM3, our method records an MPJPE of 59.76mm, surpassing PRN by 10.8mm. Finally, on the Waymo dataset, which centers on autonomous driving scenarios, our method achieves an MPJPE of 51.59mm, improving by 10.0mm over the SOTA method LPFormer. Overall, our method consistently achieves higher PCK-30 and PCK-50 scores, indicating greater robustness and value of practical usage."}, {"title": "4.6 Statbility Evaluation", "content": "To evaluate the stability of our method, we introduce various types of disturbance to the point cloud and re-evaluate the model's performance. Specifically, we apply clusters of noise points at different locations and introduce positional offsets to each point. Results are presented in Fig.7.\nPoint jittering We add point noise to each point coordinate of the input and use different thresholds for clipping. The results are shown in Fig.7a. It can be observed that compared with baseline methods, our method has less performance degradation, and the proposed MDE module also plays a positive role in dealing with disturbances.\nNoise clusters We simulate extreme input conditions by adding clusters containing varying amounts of noise points to the input point cloud. As shown in Fig.7b, as the number of noise points per cluster increases, our method yields more stable results. Notably, our pre-training substantially enhances the model's ability to handle noisy point clouds.\nErrors on end joints We evaluate the errors on the most challenging end joints, and the results are shown in Fig.8. Our method demonstrates smaller average error and variance on these joints. Especially, the proposed pre-training method effectively stabilizes the prediction of ankles and wrists."}, {"title": "4.7 Ablation study", "content": "Model components In Tab.3, we conduct ablation studies on the Waymo dataset to evaluate the contributions of the key components by incrementally enabling each proposed module. Specifically, row 1 represents the PRN baseline. Row 2 shows the results after introducing joint anchors and a 4-layer Transformer on top of the off-the-shelf PTv3. Row 3 demonstrates the effect of enabling the proposed MDE module on multi-scale point cloud features. In row 4, the coordinate-based decoding is replaced with heatmap-based decoding. The final row presents the results with all modules enabled. The combined effect of joint anchors and the MDE module leads to a substantial 16.2mm improvement in MPJPE over the baseline. Replacing the decoder with a heatmap-based one results in an additional 0.6mm improvement.\nPre-Training In Tab.4, we conduct ablation studies to assess the impact of our proposed pre-training strategies. Tab.4a evaluates the effectiveness of the enhancements: the first row represents training without any pre-training, the second row uses the same pre-training strategy as (Weng et al. 2023a), the third row implements scene resampling, and the fourth row further incorporates the proposed laser-level masking. Comparing rows 1 and 2, the inclusion of pre-training brings an MPJPE reduction of 6.3mm. Further comparison of rows 1 and 4 shows that incorporating both scene resampling and laser-level masking leads to an even greater improvement of 7.5mm. Moreover, as shown in Tab.4b, to strike a balance between providing sufficient information for human pose reconstruction and enabling the model to recover human structures from occluded point clouds, we vary the proportion of the laser level mask rkeep from 0.5 to 0.9 to determine the optimal parameter. In addition, as shown in Tab.2, we also compare our synthesis pipeline with LIPD (Ren et al. 2023), FreeMotion (Ren et al. 2024b), and NoiseMotion (Ren et al. 2024a), which also use synthetic data. Pre-training with data synthesized by the proposed method can bring more performance improvements."}, {"title": "4.8 Visualization", "content": "Prediction results Fig.5 presents the visualization results of our method. We select some challenging samples from the dataset, and our method consistently produces more stable results. Row 1 left shows a walking person from the Waymo dataset, but the left-right symmetry of the body is unclear. A segmentation-regression-based method produces wrong results, attempting to disregard the human body's rigid structure to mitigate inference errors caused by left-right ambiguity. In contrast, our method penalizes such ambiguities in the heatmap, effectively avoiding this issue. Row 2 illustrates the detection results on sparse point clouds from the LiDARHuman26M dataset. In this scenario, our method effectively recognizes human body orientation from limited clues. Lastly, as depicted in row 1 right and row 3 left, when multiple human instances are unexpectedly introduced by the human detector, our approach does not confuse the two individuals. In summary, our method demonstrates strong capability in robust inference.\nSynthetic samples We present the synthesis samples of our approach in Fig.6. It is evident that scene resampling produces diverse synthetic samples, effectively generating both sparse and dense point clouds of human instances. Additionally, the laser-level masks introduce more complex occlusions, which in turn equip the model with more robust human priors and point cloud representations."}, {"title": "5 Conclution", "content": "We propose a novel method for robust LiDAR-based 3D human pose estimation, with two main contributions. First, we introduce a density-aware pose transformer, which employs joint anchors and special exchange modules to extract valid features from point clouds of varying densities, enabling explicit learning of stable keypoint representations. Second, we present a comprehensive LiDAR-based human synthesis and augmentation approach for model pre-training. By integrating more randomized position sampling, ground modeling, and laser-level masking, we generate highly realistic and challenging samples. Qualitative and quantitative evaluations demonstrate that with the synergy of the proposed model and pre-training strategies, our method achieves state-of-the-art performance across multiple datasets.\nOverall, our approach provides a comprehensive solution for LiDAR-based 3D HPE. Future work can try to address left-right reversal and multi-frame jitter in timing or perform pose-based human behavior understanding tasks."}]}