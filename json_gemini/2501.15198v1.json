{"title": "Towards Conscious Service Robots", "authors": ["Sven Behnke"], "abstract": "Deep learning's success in perception, natural language processing, etc. inspires hopes for advancements in autonomous robotics. However, real-world robotics face challenges like variability, high-dimensional state spaces, non-linear dependencies, and partial observability. A key issue is non-stationarity of robots, environments, and tasks, leading to performance drops with out-of-distribution data. Unlike current machine learning models, humans adapt quickly to changes and new tasks due to a cognitive architecture that enables systematic generalization and meta-cognition. Human brain's System 1 handles routine tasks unconsciously, while System 2 manages complex tasks consciously, facilitating flexible problem-solving and self-monitoring. For robots to achieve human-like learning and reasoning, they need to integrate causal models, working memory, planning, and metacognitive processing. By incorporating human cognition insights, the next generation of service robots will handle novel situations and monitor themselves to avoid risks and mitigate errors.", "sections": [{"title": "1 Introduction", "content": "Industries like car manufacturing impressively demonstrate the utility of robots. Recent developments in sensing, actuation, and \u2013 most importantly \u2013 artificial intelligence (AI) make it conceivable that robots will revolutionize many new application domains such as the flexible production of small lots, logistics, agriculture, security, inspection, professional services, and personal assistance. All of these domains, however, require, cognitive capabilities far beyond those of today's robots.\nCurrent robotic systems rely on structuring the environments and tasks, e.g. by providing objects in well-defined locations. In open-ended settings, such as our everyday environments, robot-friendly structuring is impossible. Instead, autonomous service robots must instantiate models of their environment from sensor measurements, plan actions to achieve goals, carry out plans in the presence of disturbances, and monitor their execution. They also must familiarize with new objects and tools and need to improve their behavior through learning. Finally, they must communicate with persons in a human-understandable way, to receive instructions, answer questions, and explain their behavior.\nThe tremendous success of deep learning [64,98] in visual perception, speech recognition, natural language processing, vision-language tasks, and multimodal tasks gives rise to hope that these methods will lead to revolutionary advances in autonomous robot performance."}, {"title": "2 Related Work", "content": "Personal service robots Personal robots that assist handicapped or elderly persons in their activities of daily living have attracted much attention in robotics research. An increasing number of research groups are working on robots for service applications. Examples include PR2 [74] that was used in a household marathon experiment [52], Everyday Robots' mobile manipulator [42], and Toyota Human Support Robot [125], a standard platform in the international RoboCup@Home competitions. My team NimbRo won these competitions 2024 with two PAL Robotics TIAGo++ robots [75] and 2011-2013 with our cognitive service robot Cosero [105, 106], demonstrating a large variety of domestic service tasks. Further examples include Care-O-Bot 4 [55], Armar-6/7 [2], HRP-5P [59], TORO [53], E2-DR [127], and our Centauro robot [56, 57] which demonstrated challenging locomotion and manipulation tasks including the use of tools. For the ANA Avatar XPRIZE competition [8,40], capable systems have been developed, including iCub3 [20], Pollen Robotics Reachy, and our winning Avatar system [66, 99]. Scene perception. In order to act in complex indoor environments, service robots must perceive the room structure, obstacles, persons, objects, etc. To this end, they are equipped with cameras and depth sensors. Estimating the sensor poses and registering the measurements yields environment maps [87]. In addition to modeling the environment geometry and appearance, semantic perception is needed [95].\nDeep learning For pattern recognition, deep learning [64] methods are extremely successful. They revolutionized visual perception [23,92], speech recognition [88,91], natural language processing [1,85], vision-language tasks [69, 107], and multimodal tasks [33, 39]. Supervised deep learning requires large annotated data sets like ImageNet-21K [25], JFT-3B [129], and Kinetics [14], though, which are expensive to obtain. To address variability that should not change output, data augmentation methods such as image transformations [101] and generative models [128] are used to generate variants of training examples. For robotic tasks such as mobile manipulation, large-scale annotated datasets do not exist. To avoid the need for large labeled datasets, much research focuses on methods that can adapt to new conditions through transfer learning and domain adaptation. Transfer learning [58] uses representations learned from large data to learn a related task from small data, e.g. by continuation of training. Semi-supervised, weakly supervised, and unsupervised learning methods use fewer, low quality, and no labels at all, respectively. One example of semi-supervised methods is the student-teacher approach [124], where a teacher is trained on a small labeled data set and then generates pseudo labels for a large unlabeled data set to train the student. Because unlabeled data is much easier to obtain than annotated data, unsupervised methods are often used to pre-train models [10, 16]. The hope is to discover useful structure in the data which might aid target tasks. A promising subclass of unsupervised learning is self-supervised learning [37], which requires only unlabeled data to formulate a pretext task, for which a target objective can be computed without supervision. These pretext tasks must be designed in a way that high-level data understanding is useful for solving them, e.g. prediction of occluded image parts [41] or future video frames. As a result, the intermediate layers of trained models encode high-level semantic representations that are useful for solving downstream tasks. One form of self-supervision is contrastive learning [15], where two different data augmentations are applied to an image and a model is trained to maximize agreement between the outputs and minimize agreement with outputs for other images. Contrastive learning of dense descriptors for object surface elements has been applied to learn visuomotor manipulation policies [28] that generalize within a category of objects and are able to handle deformable objects. Other possibilities for self-supervised learning are to maximize mutual information between input and model output [45] and joint embeddings of two inputs with variance-invariance-covariance regularization [3].\nLarge language models Self-supervised training is the basis for the impressive performance of recent large language models (LLMs) such as GPT-4 [85] and Palm 2 [1] that continue text in plausible ways. Their large transformer networks [114] were trained on massive data to predict the next token. In contrast to recurrent sequence models, transformers flexibly re-route and combine information from relevant parts of the sequence through learned self-attention, which is implemented using content-based access to information values by matching keys to queries. Recently, autoregressively trained LLMs have shown sparks of artificial general intelligence [13]. Such models can acquire human-like systematic generalization through meta-learning [61], but this requires generating a training set of systematic generalization example problems. On the other hand, LLMs often lack common sense, hallucinate facts, fail at arithmetic, have difficulty reasoning, and cannot make proper plans. For these reasons, LLMs are combined with external tools [89] such as search engines, calculators, planners [68], etc. Multimodal models such as PaLM-E [26] and ImageBind [33] combine text and other modalities such as images in joint embeddings. Generative models are not restricted to producing text but are also used to generate, e.g., images [86] and video [90] from text inputs.\n3D models To address the 3D nature of scenes, Neural Radiance Fields (NeRF) have been proposed, which learn a neural network mapping 5D coordinates to density and color by predicting images from multiple views through volumetric rendering [77]. In our recent work PermutoSDF [93], the 3D shape of objects is represented by a neural signed distance function (SDF). By modeling individual objects as permutation-invariant slots, object representations can be learned through novel-view synthesis [94]. If conditioned on latent variables, category-level shape spaces can be learned, e.g., for articulated human bodies [24]. Compositional generative scene models that represent objects and their relations can be learned without image-level supervision [31].\nScene prediction In dynamic scenes, the motion of objects and persons must be estimated and predicted. Scenes with moving agents (e.g., humans or robots) can be represented with 3D dynamic scene graphs [47]. Motion is the strongest cue for perceptual grouping and predictive models are widely used to explain human visual perception [30]. Consequently, optimization of a prediction loss can be used to segment moving objects in videos [119]. SlotFormer [121] models spatio-temporal object relationships and predicts object states. Our recent work on object-centric video prediction decouples the processing of temporal dynamics and object interactions [116]. This facilitates learning of tasks that require understanding of object relations [81]. A fundamental problem when predicting the future is that often multiple plausible futures exist. MultiPath++ [113] predicts a distribution of future trajectories of road users parameterized as a Gaussian Mixture Model (GMM). Multiverse [67] predicts the distribution over multiple possible future paths of persons using convolutional recurrent neural networks (RNNs) over graphs. World modeling. Prediction of future scene states and planning own actions require world models that are conditioned on actions. Playable Video Generation [76] learns a discrete set of actions from unlabeled video that are used to interactively generate video from actions. This task has been extended to Playable Environments [76] that can control multiple objects in 3D scenes with action labels that are discovered in an unsupervised way. DayDreamer [120] learns action-conditioned forward models in a latent space for multiple robots. GenAD [126] and GameNGen [112] are world models for autonomous driving and a video game, respectively.\nDeep reinforcement learning Reinforcement learning (RL) addresses the development of situated agents that learn how to behave while interacting with the environment [108]. This problem is formulated as an agent-centric optimization in which the objective is to select actions based on the estimated state in order to obtain as much reward from the environment as possible in the long run. Impressive success has been achieved by combining this approach with deep learning. One example is MuZero [97] which combines tree-based search with a learned model and achieves superhuman performance in a range of challenging and visually complex domains (Atari games, Go, chess, and shogi), without any prior knowledge of their underlying dynamics. MuZero learns a model that predicts the quantities relevant to action planning: the reward, the action-selection policy, and the value function. AlphaStar [117] learned the multi-agent game StarCraft II from ~500K human games and 120M self-played games. Gran Turismo Sophy [123] learned from carefully engineered state and reward in more than five years of simulated driving hours to compete with the world's best drivers. Playing soccer with humanoid agents was learned from decades of match simulations [70]. Soccer skills for a humanoid robot and 1v1 play were learned from 2.5 years of simulated experience [38].\nThese numbers indicate that it would be impractical to collect that much experience with a real robotic system. Consequently, real-robot reinforcement learning mostly focuses on individual skills. For example, Google X learned grasping from cluttered bins with a simple manipulator under closed-loop monocular vision-based control [50]. They operated seven experimental setups for four months to collect 580K real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters and report a 96% grasp success rate on unseen objects. The method learned regrasping strategies, probing or repositioning objects to find the most effective grasps, performing other non-prehensile pre-grasp manipulations, and responding dynamically to disturbances and perturbations. Sorting recyclables and trash was learned from simulation and 9,527 hours of real-robot experience obtained with a fleet of 23 mobile manipulators [42].\nReal-robot RL needs suitable inductive biases [43] to learn from little experience. These biases represent domain knowledge and can take many forms, e.g., the structure of the agent-environment interface and the policy generation mechanism. To improve the data efficiency of RL, transfer learning has been investigated. By pre-training on RoboNet [21], a data set providing 15M video frames from seven different robot platforms, and fine-tuning on a held-out target platform, it has been demonstrated that simple manipulation tasks such as pushing and pick-and-place can be learned from limited experience. Multi-task learning amortizes experience over multiple tasks [51]. It generalizes to structurally similar tasks and acquires distinct new tasks more quickly. One way to address the combinatorial complexity of multi-object scenes is to factorize them into objects. Object-centric perception, prediction, and planning [115] learns to discover objects in visual scenes and models their dynamics and appearance without supervision. A model-based reinforcement learner that predicts and plans block stacking on this abstract level generalizes to novel configurations and more objects. Action Schema Networks [110] learn generalized policies for probabilistic planning problems. By mimicking the relational structure of planning problems, they generalize over all instances of a given planning domain. Manipulation inherently involves contact and often requires both haptic and visual feedback. Lee et al. [65] use self-supervision to learn a compact and multimodal representation of sensory inputs, which is then used to improve the sample efficiency of policy learning as demonstrated for peg insertion. To avoid random exploration, imitation of human experts can be used. RT-1 [12] is a transformer-based controller trained on 130K demonstrations of a large variety of pick and place tasks in kitchen environments. Open X-Embodiment [84] is a large data set of camera images and end-effector movements from 22 different robots, demonstrating 527 skills (160,266 tasks). The RT-X model trained on this data exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. 970k episodes from this data set were used to train OpenVLA [54], starting from a large language model and a visual encoder. OpenVLA demonstrates generalist manipulation capabilities and can be adapted to new robots via fine-tuning."}, {"title": "3 Challenges", "content": "Despite much research and progress, capable mobile manipulation robots that can cope with the complexity of open-ended real-world applications have not yet been realized. Developing such robots is a tremendous challenge, due to the typical characteristics of these applications.\nMany sources of variability There are many sources of variability that a mobile manipulation robot must cope with. These include varying shape, texture, and physical properties of objects even within a category. Furthermore, the 6D object pose, speed, and articulation state may vary. Environmental conditions, such as lighting, and surface properties, such as shininess, transparency, texturelessness, or non-reflectivity greatly impact appearance in camera images and consequently the completeness and precision of depth estimates. The variability of single objects is exponentiated by the infinite possibilities for multi-object arrangements. Similarly, the robot environments such as rooms and apartments vary greatly in layout, geometry, surface properties, and other factors. The manipulation and locomotion tasks that capable robots need to perform are highly variable as well. Hence, learning methods are needed that generalize to novel, unseen situations.\nHigh-dimensional state and action spaces Input and output of mobile manipulation robot controllers are high-dimensional. Typical camera images are of size 1920\u00d71080\u00d73, already more than 6M dimensions. Depth cameras, 3D LiDARs, force-torque & haptic, inertial, and joint sensors add many more input dimensions. The sensors measure at high rates, e.g., at 30 Hz, producing hundreds of million measurements per second. The output dimensionality is high as well, with typically more than 50 DoF for anthropomorphic robots. These joints need to be controlled at high rates with target positions, velocities, or torques. Hence, learning methods are needed that can cope with high-dimensional state and action spaces.\nHybrid discrete-continuous variables Some variables, such as the presence of objects or the task category, are discrete while other variables, such as 6D object poses or task parameters, are continuous. This creates the need for learning methods that can cope with both discrete and continuous variables.\nNon-linear dependencies Objects are typically in contact with support surfaces and with each other and the robot must make and release contacts with its end-effectors or other body parts to manipulate them. This induces highly non-linear constraints. While objects may be easily moved away from the contact point, moving them further towards the colliding surface is not possible. Similarly, occlusion effects and the transition between stick friction and sliding are highly non-linear. Learning methods must address such non-linearities.\nStochasticity There is much randomness in the world. Unmodeled environmental factors and other agents might also be perceived as non-determinism from our robot's point of view. Furthermore, robot sensors are noisy and unreliable; and robot actuators are imperfect and induce stochasticity. Hence, the state must be estimated from unreliable observations and predictions are hard to make and become more and more uncertain for larger time horizons. Learning methods must cope with such uncertainties.\nPartial observability Due to the projection of the 3D world onto 2D cameras and other sensors, limited sensor ranges, resolutions, accuracies, etc., not all state variables that would be needed for action planning are directly accessible. Hence, learning methods must consider the distribution of possible states and must generate actions to acquire more information, for example changing the camera pose to see occluded objects, touching objects to sense physical properties like weight and stiffness, and opening containers to see what is inside.\nUnderactuation Robots have limited action capabilities to influence the state of the environment. Their drives have limited speed and acceleration, their manipulators have limited reach, strength, and dexterity. Some environmental variables cannot be influenced directly, but only through indirect means like tools. Learning methods must respect these constraints and generate behavior such as improvised tool use to overcome them.\nMultimodality Mobile manipulation involves multiple modalities, such as vision, distance measurements, forces, and haptics on the input, and also multiple outputs, such as mobility, manipulation, and active sensing. Hence, learning methods must jointly address these modalities and come up, for example, with grasping strategies that transition smoothly from vision-based scene understanding and grasp selection, to visual tracking and correction of the approaching motion, to grasping execution and re-grasping based on haptic feedback.\nNon-stationarity One unique challenge is the non-stationarity of robots and their environments. Not only do robot bodies change due to wear and tear, also the open-ended environments in which they operate and the tasks they perform are constantly changing. Already the ancient philosopher Heraclitus noted that the only constant in life is change. Such changes violate the fundamental assumption underlying current machine learning that a learned model will be used on the same distribution of data it has been trained on. When using a trained model on a different distribution (out-of-distribution, OOD), one cannot expect good performance [80]. In fact, seemingly small changes can lead to catastrophic failure [17]."}, {"title": "4 Human Cognitive Functions", "content": "Humans are able to cope with such changes and quickly learn new tasks. My hypothesis is that the cognitive architecture of the human mind has evolved to continuously interact with changing environments and that equipping robots with key elements of this architecture will enable flexible handling of OOD data and systematic generalization. Systematic generalization was first studied in linguistics [5,60] because it is a core property of language that meaning for a novel composition of existing concepts (e.g. words) can be derived systematically from the meaning of the composed concepts and the way they are composed. Humans exhibit systematic generalization also when understanding a new object by combining properties or parts which compose it [62]. Compositionality is the principle that complex objects can be described by their constituent parts and their relations to each other [29]. It allows to generate infinite variants from a finite set of building blocks, enables open-world zero-shot learning [71], and even makes it possible to generalize to new combinations that have zero probability under the training distribution.\nWhile humans perform many routine tasks like walking or riding a bike without much attention, object manipulation, communication, and handling novelty are different. Cognitive science distinguishes habitual and controlled processing [11]. Habitual processing effortlessly generates default behaviors that are performed routinely. In contrast, controlled processing requires attention and mental effort to generate non-routine behaviors. Kahneman [49] introduced the framework of fast and slow thinking and corresponding processing systems in our brain (see Fig. 1). Routine, habitual tasks can be achieved quickly in parallel without conscious attention using only System 1 abilities, whereas more complex tasks also require System 2 that is more capable but slower, serial and involves conscious processing. System 2 uses explicit, verbalizable knowledge and explicit processing while System 1 relies on implicit, non-verbalizable, intuitive knowledge. We can act in fast and precise habitual ways without having to think consciously, but the reverse is not true: conscious processing builds on the unconscious System 1.\nSystem 2 is very flexible and powerful. It allows to solve novel problems creatively by recombining existing pieces of knowledge, to discover and use causal dependencies, to imagine future outcomes, to plan actions, to find explanations, to reason, etc. It is also at that level that we communicate with others through natural language, e.g. to receive task specifications or new knowledge and rules that we can apply immediately. The capacity of System 2 is very limited, though. Our working memory can only hold 3-5 meaningful items active simultaneously [19]. Baars introduced the Global Workspace Theory [4] that identifies conscious processing as the communication bottleneck between selected parts of the brain that are called upon when addressing a current task. There is a threshold of relevance beyond which information that was previously handled unconsciously gains access to this bottleneck and is instantiated in working memory. When this happens, the information is broadcast throughout the brain, allowing its various relevant parts to synchronize and choose configurations and interpretations of their piece of information that are globally coherent with the configurations chosen in other parts of the brain.\nWhile this severe communication bottleneck might appear to be a weakness, it can also be advantageous. Firstly, there is pressure to combine multiple lower-level items that frequently occur together to larger, composite items, facilitating abstracting away irrelevant detail and providing compositionality. Secondly, when focusing on a few relevant items of a scene for conscious planning, we essentially ignore all other items, which are irrelevant for the task at hand. This leads to systematic generalization, because we can reuse the task knowledge in infinitely many novel situations in which the irrelevant items change. System 2 is slow, has limited capacity, and involves conscious effort; hence, there is pressure to migrate tasks to System 1 wherever possible. Through rehearsal, frequently performed tasks become habitual.\nDehaene et al. [22] characterize consciousness further. They distinguish unconscious processing (CO) and two orthogonal dimensions of conscious computations: global availability of information (C1) and meta-cognition (C2).\nC1 - global availability \u2013 is a consequence of the distributed organization of the brain as a deep hierarchy of specialized subsystems that must be synchronized and of the need to act, which means that we cannot stick to a diversity of probabilistic interpretations and action options, but must decide in favor of a single course of action. Such decision-making requires efficient pooling over all available sources of information, considering the available action options and selecting the best one, sticking to this choice over time, and coordinating internal and external processes towards the achievement of that subgoal. Attention \u2013 selective processing of information - is crucial for items entering consciousness, but attention is not limited to conscious processing. Unconscious C0 processing also includes bottom-up and top-down attentional mechanisms, which operate in parallel to prioritize and flexibly route information \u2013 often without bringing it to consciousness. The hierarchical system of sieves that operate unconsciously computes probability distributions, but only a single sample drawn from these becomes conscious at a given time, making it available globally to all specialized modules. Alternative interpretations might become conscious at other points in time, thus, C1 consciousness is causally responsible for our serial information-processing bottleneck. Attention also implements variable binding [36]. The association of information elements to roles in relations and rules is crucial for applying these templates to varying input and, hence, for multi-step inference and systematic generalization.\nConsciousness in the second sense (C2) is characterized by the ability to reflexively represent oneself. When making decisions, we feel more or less confident about our choices. Our brain does not only make perception and action decisions, but also estimates its degree of confidence. State estimation and learning also rely on confidence, for example, we weigh existing knowledge versus new evidence, like a Kalman filter [109]. Error detection is another example of self-monitoring: just after responding, we sometimes realize that we made an error and change our minds. This might be explained by further evidence that arrived after the decision or by slower C2-processing monitoring fast CO sensory-motor execution. We don't just have knowledge, but we also know what we don't know. Such meta-knowledge is crucial for assessing our limits and for learning."}, {"title": "5 The Need for Conscious Robots", "content": "Despite tremendous progress in CO-like deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, truly human-like learning and thinking machines will need to go beyond current engineering trends in both what they learn and how they learn it [62]. They need to build causal models of their environment that support explanation and understanding and must harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations [96]. For this, equipping machines with C1 and C2 conscious processing will be crucial. Upon success, they would behave as if they were conscious; e.g., they would know that they are seeing something, express confidence in it, report it to others, and may even experience the same perceptual illusions as humans.\nOf course, traditional symbolic Al systems (GOFAI), like Hierarchical Task Network Planners [82] and CRAM [7], exhibit some of the properties that are associated with conscious System 2 processing, like compositionality. However, such symbol manipulation systems often lack semantic grounding of the higher-level concepts in terms of the lower-level observations and actions. Whereas pure symbolic representations put every symbol at the same distance from every other symbol, learned embeddings represent concepts through a vector of attributes \u2013 with related concepts being close-by and interpolations being meaningful. GOFAI systems often are too rigid to account for real-world data with outliers, etc. Further, GOFAI search and inference are generally intractable and need to be approximated. Here, learning representations together with inference procedures is needed to generate fast habitual CO behavior. Finally, GOFAI approaches often do not handle uncertainty, which is crucial for partially observable, stochastic environments.\nIn recent years, neuro-symbolic approaches [32, 44, 73] have been proposed that integrate symbolic and subsymbolic representations, inference, and learning. However, hybrid neuro-symbolic systems [18,34,63,72,83,100,102,111,130] inherently use different representations and tools for neural and symbolic computations, which are difficult to integrate tightly.\nNeurocompositional computing [103] is based on the principles of compositionality and continuity. It encodes structures in vectors that are processed by neural networks and shows promising results by quickly learning tasks from small data sets that require systematic generalization. The Differentiable Tree Machine [104] compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. Here, an agent learns to sequentially select tree operations to execute tree transformations with the help of a tree memory.\nRecently, autoregressively trained embodied multimodal models have been used for generating robotic skills such as grasping and placing objects [12,54] and for higher levels of robot control [26]. These models lack System 2 conscious processing, though. They need much data [84] and computing power; and the addressed scenarios are still relatively simple.\nMy hypothesis is that using insights from human cognition for the cognitive architectures of robots by incorporating C1 global availability and C2 metacognition will enable the next level of robot capabilities.\nBecause it extends highly successful C0 processing without a change in tools, I am convinced that a bottom-up way towards consciousness-inspired higher-level cognitive functions for service robots is the way to go."}, {"title": "6 Objectives", "content": "My overall goal is to develop methods for learning higher-level cognitive functions for service robots, which go beyond unconscious routine tasks by incorporating conscious processing to cope with novel situations and self-monitor.\nUnconscious perception and control The System 1 / C0 routine processing directly interacts with the environment and is hence the basis for any higher-level cognitive functions.\nStarting from raw sensory measurements, such as video, depth, forces, and haptics, structured representations of mobile manipulation robot workspaces shall be learned on multiple levels of spatio-temporal abstraction. Abstraction will be realized by coarser spatio-temporal scales and more expressive, sparser representations on the higher levels. The elements of these representations will correspond to increasingly larger entities (parts, objects, groups of objects) in the scene and will be increasingly semantic. The learned representations shall transition from sensor coordinate systems (e.g. the camera frame) to 3D representations of the scene and joint multimodal embeddings.\nThey shall model individual objects and the robot end-effectors in their own canonical frame. This will enable learning of category-level shape and appearance spaces within a hierarchical categorization. Scene parsing shall instantiate these models and estimate object parameters, such as pose, shape descriptors, and appearance descriptors. Predictive models for these scene representations shall be learned on all levels of abstraction (see Fig. 2). Prediction of low-level detail shall be done only for short time horizons. Higher-layer representations shall be predicted with coarser temporal granularity over longer time horizons. These predictions shall be based on individual object dynamics models and on pairwise relational models to account for object interactions, such as contact. The graph of object relations shall be sparsely instantiated according to the relevant object interactions. The predicted representations shall be compared to the feed-forward interpretation of new measurements, such that prediction error can be used to update the representations on all levels.\nThe learned predictive models shall be extended by conditioning them on robot actions. This will allow for the rollout of possible futures of robot-environment interaction. Coarse-to-fine model-predictive control of routine skills that do not require conscious attention shall be learned from imagined rollouts on the multiple levels of abstraction. Higher layers shall plan abstract actions longer into the future, which are concretized on lower levels for shorter time horizons. A large variety of skills shall be learned for modular behaviors that activate coarse-to-fine actors according to the situation. Binding objects or places to roles shall yield parametrizable skills, such as grasping or placing an object or navigating towards a waypoint while avoiding obstacles.\nConscious prediction and planning Methods for selecting a small set of elements from the highest-level CO representations and for maintaining them in a working memory (WM) shall be learned. This WM will be the basis for learning action-conditioned predictions, based on binding selected elements to variables of applicable rules.\nStructured predictions of state transition rewards, value, and action selection probabilities shall be learned from interactions with simulated and real environments. LLMs shall be incorporated as oracles.\nAs illustrated in Fig. 3, the learned WM world models shall be used for efficient action planning by sequential search. A spatio-temporal action abstraction shall be learned, such that sub-plans are reused in different contexts. The learned models shall be used for autonomous operation of mobile manipulation in complex novel situations.\nConscious self-monitoring Methods for assessing the confidence of perceptions and predictions shall be developed. They shall be based on learning the distributions of latent variables on which multiple plausible futures can be conditioned (see Fig. 4)."}, {"title": "7 Methodology", "content": "My approach will be to add suitable inductive biases to deep reinforcement learning (DRL), such that structured representations and conscious processing are enforced, which will enable systematic generalization and self-monitoring. Inductive biases reflect assumptions about the statistics of modeled scenes and robot-environment interactions and are necessary for generalization [78]. For instance, hierarchical convolutional neural networks (CNNs) [64] hardwire local dependencies, translation equivariance, hierarchical structure, and invariance to local deformations; whereas recurrent neural networks [46] exploit equivariance over time.\nFurther biases are needed for higher cognitive functions. One example of these is choosing the appropriate frame of reference for modeling. Commonly, deep neural networks represent visual scenes in a sensor coordinate system. Describing objects in object-centered canonical frames normalizes away the variability induced by the 6D object pose [118]. In such object-centered frames, shape and appearance spaces can be learned much easier. Of course, such canonical frames are also useful for individual parts of objects, for which the 6D pose relative to the object-centered frame must be modeled. The projection of the 3D world to 2D images induces occlusions and discontinuities at object boundaries. Modeling the scene in 2.5D by individual depth layers or directly in 3D allows for more complete, continuous representations where occluded parts are present and hence unoccluded parts can be predicted. Much of the image motion can be explained by camera motion. Hence, approaches that explicitly model the projection from 3D to the variable camera view can represent this dependency compactly. One particularly powerful assumption is relational inductive bias [6]. It expresses the observation that scenes can often be described in terms of entities (objects, parts, groups) and their sparse pairwise interactions (relations). Relations describe interactions between entities on adjacent levels of abstraction, e.g. between the whole object and its parts, which represents the compositional structure of the world. Such compositional hierarchy can also be found on the action side, where tasks are composed of multiple subtasks and subtasks are composed of individual skills (see Fig. 5)."}, {"title": "8 Conclusions", "content": "By incorporating insights from human cognition, the next generation of service robots will systematically generalize their knowledge to cope with novelty. This new generation of robots will also monitor themselves to obtain more information when needed, to avoid risks, and to detect and mitigate errors. Conscious service robots have much potential for numerous open-ended application domains, including assistance in everyday environments. Moreover, artificial conscious processing will contribute to a better understanding of consciousness in humans and other animals."}]}