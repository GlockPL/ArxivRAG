{"title": "Context information can be more important than reasoning for time series forecasting with a large language model", "authors": ["Janghoon Yang"], "abstract": "With the evolution of large language models (LLMs), there is growing interest in leveraging LLMs for time series tasks. In this paper, we explore the characteristics of LLMs for time series forecasting by considering various existing and proposed prompting techniques. Forecasting for both short and long time series was evaluated. Our findings indicate that no single prompting method is universally applicable. It was also observed that simply providing proper context information related to the time series, without additional reasoning prompts, can achieve performance comparable to the best-performing prompt for each case. From this observation, it is expected that providing proper context information can be more crucial than a prompt for specific reasoning in time series forecasting. Several weaknesses in prompting for time series forecasting were also identified. First, LLMs often fail to follow the procedures described by the prompt. Second, when reasoning steps involve simple algebraic calculations with several operands, LLMs often fail to calculate accurately. Third, LLMs sometimes misunderstand the semantics of prompts, resulting in incomplete responses.", "sections": [{"title": "I. INTRODUCTION", "content": "With the introduction of the transformer module into deep learning, models for natural language processing have evolved rapidly. Initially, this module was used to build pretrained language models (PLMs) such as BERT [1] and XLNet [2]. Through transfer learning, after training the model with a large amount of data, PLMs have been fine-tuned for various tasks such as sentiment analysis, entity recognition, summarization, and translation. While fine-tuned PLMs often provide state- of-the-art (SOTA) performance for specific tasks and domains, their applicability to different types of tasks and data domains is quite limited. To overcome these limitations, large language models (LLMs) have been developed by leveraging large clusters of graphical processing units (GPUs), incorporating massive parameters and extensive training datasets. Although LLMs may perform inferior to SOTA models for some specific tasks in certain domains, they can handle a variety of tasks without further training and perform complex tasks\nHowever, LLMs may have inherent limitation when the LLMs work with numerical problems without fine-tuning. The token generation in LLMs depends on the input and preceding output. This implies that when an algebraic operation is required, non-numeric tokens act as noise to the algebraic operation, hindering the model's ability to focus solely on the numerical aspects. In addition to this, there are several other issues with LLMs when numeric values in a query hold significant information for generating an accurate answer. This is particularly problematic when LLMs process tasks related to time series of numeric values, such as forecasting and outlier detection [3]. Inputs to LLMs are first tokenized, and many tokenizers build on byte pair encoding (BPE), which progressively expands the dictionary with frequently used subwords. Thus, numbers with long digits can be split into multiple tokens, hindering the extraction of precise patterns in numerical sequences and numerical operations. Furthermore, LLMs are trained to handle arbitrary strings with a massive corpus, which may limit their capability to handle complex numerical operations accurately unless they are fine-tuned with proper datasets. This is because they are not trained enough to identify specific patterns and rules in numerical data. It is noted that while numerical data appears to be a simple data type based on 10 digits, it can be of arbitrary length, and its meaning can vary greatly depending on the context in which the numeric values are generated. This irregularity and the large effective vocabulary size can pose additional challenges for LLMs in accurately executing tasks associated with time series.\nIt is also noted that the quality of generated output from LLMs often depends on the formulation of the questions, known as prompt design [4]. It can be expected that the quality of time series tasks with LLMs also depends on the prompt. However, unlike conventional natural language processing, time series tasks with LLMs can be more challenging as they require the implicit retrieval of procedures for a given task and accurate numeric algebraic operations to generate precise answers. In this regard, this paper explores the potential of prompts in time series forecasting with a LLM, GPT-40-mini to elucidate the effect of prompts on time series tasks and shed light on the direction for exploiting LLMs for these tasks. To the best of the authors' knowledge, the behavior of LLMs with prompts for time series forecasting has not been thoroughly assessed both quantitatively and qualitatively, this study aims to fill the gap. To this end, we evaluated several existing general-purpose prompts and hand-crafted prompt based on seasonal ARIMA, statistical model for time series forecasting which we call SARIMA prompting. It was found that simply providing proper context information related to the time series, without additional reasoning prompts, can achieve performance comparable to the best-performing prompt for each case while one-shot chain of thought (CoT) prompting [5] and long short term (LST) prompting [6] achieves the best performance with some specific dataset. From analyzing the generated outputs qualitatively, several interesting characteristics in forecasting with LLMs were identified. LLMs often fail to follow the procedures described by the prompt. When reasoning steps involve simple algebraic calculations with numerous operations, LLMs often fail to calculate accurately. LLMs sometimes misunderstand the semantics of prompts, resulting in incomplete responses. For example, despite prompts requiring the use of trends over the entire time series, LLMs frequently fail to utilize trend information, identify seasonality, and exploit this information effectively.."}, {"title": "II. RELATED WORKS", "content": "Many LLMs are trained to generate the next token based on the preceding sequence of tokens. Due to this intrinsic nature, the quality of the output from an LLM depends significantly on the design of the question, often leading to structured generation. Depending on the degree of additional programming required, which increases with complexity and interaction level, prompting can be classified into simple passive prompting, iterative prompting, and interactive prompting.\nAs a pioneering work in prompting for LLMs, chain of thought (CoT) prompting [5] was proposed to initiate reasoning in LLMs. This method often significantly improves the quality of the output by adding a simple phrase, 'let's think step by step.' Its simplicity and universal applicability, regardless of domain and tasks, have brought significant attention to prompting. To further improve, plan and solve (PS) prompting [7], which involves planning with subtasks and working on each subtask sequentially, has been shown to outperform CoT prompting in some tasks. It is noted that these simple passive promptings are constructed by adding a simple sentence to elicit reasoning for solving the given task.\nThese simple prompts can be further improved by adding a few demonstrations, a technique often referred to as few- shot learning. However, the effectiveness of these demonstrations depends on their level of articulation. To overcome this limitation, a systematic approach to designing demonstrations from LLM-generated content, involving multiple sequential prompts, can be employed. We call this method iterative prompting. Synthetic prompting [8] iteratively synthesizes queries from several initial demonstrations and generates responses with these synthesized queries, which are later used as demonstrations for chain-of-thought (CoT) prompting. This approach can be advantageous because internally generated queries and answers are more likely to align with the LLM's reasoning structure.\nThis iterative process can be further refined by introducing another model trained to generate demonstrations, which we call interactive prompting. Interacting models can be the same model with different roles [9] or different models with different capabilities [10]. This approach aims to improve prompts through interaction between multiple models, reducing human error in prompt articulation and bridging the cognitive gap between humans and LLMs in understanding information. Prompt with actor-critic editing (PACE) [10] exploits actor-critic structures popular in reinforcement learning. The actor model generates a prompt, while the critic model provides feedback on the generated prompt. While this method can improve the quality of the prompt for a specific task, the generated prompt's applicability becomes limited due to the nature of the generation. This interactive prompting has been utilized to generate prompts automatically, reducing human effort in creating prompts."}, {"title": "B. Time Series Analysis with LLM", "content": "LLMs have very powerful generalizability, making them applicable to many different types of tasks. In line with this, time series can also be analyzed by LLMs. However, na\u00efve application to time series analysis often results in performance inferior to SOTA performance. Thus, several different approaches to enhance the LLM's capabilities in analyzing time series have been made, considering the complexity. These approaches can be broadly classified into prompting, fine-tuning, and patching.\nPrompting is a powerful technique to elicit a systematic procedure for completing a task. Therefore, it is very natural to consider prompting as a means to enhance the LLM's problem-solving capabilities in time series analysis. GPT-4 was shown to achieve better binary forecasting over stock data consisting of numerical and textual data by exploiting diverse data handling capabilities from manually articulated prompts than gradient boosting machine learning algorithms [11]. A universally applicable prompt for time series forecasting was proposed. LSTPrompt [6] was designed to integrate both short-term and long-term forecasting to help LLMs follow step-by-step reasoning more effectively.\nThe performance achieved by prompting can be limited due to the intrinsic nature of LLMs to textual information rather than numerical time series. Thus, a natural way to overcome this limitation is to fine-tune the LLM such that it can learn how to analyze the time series. PromptCast [12] decorates a time series with a template containing simple contextual information and metadata on the time series to create a prompt and fine-tunes the LLM in a question- answering way so that the LLM can learn how to predict for a given time series. To deal with the tokenization issue in numerical information, TIMELLM [13] fine-tunes the LLM by converting numbers into space-separated digits and scaling, achieving comparable performance to transformer-based models specifically designed for time-series tasks.\nWhile fine-tuning makes LLMs better fitted for analyzing time series, the conventional tokenization based on conventional vocabulary constructed from textual corpus may not be good enough to represent time series efficiently. In this regard, state-of-the-art methods with LLMs for analyzing time series exploit patching by segmenting the time series into smaller chunks. Thus, to exploit LLMs' powerful inference capability and overcome the limitation in manipulating the time series as a sequence of numeric values, time series are patched first, embedded, and then aligned with the embedding space of the LLM. LLM-time [14] represents input time series with a prototype as input to LLM while numerical time series are patched and embedded by a separate network. While fixing the LLM, the combined network is trained to be fitted to time series tasks through aligning textual data and numeric data. TEMPO [15] created an input from decomposing time series into trend, seasonal, and residual components, patching each component separately, and concatenating with a soft prompt translated with learnable linear projection. Then, GPT-2 is fine-tuned over the position embedding layer, layer normalization layers, and attention layer with low-rank adaptation (LORA). This transforms the LLM into a model specialized for time series analysis.\nSome other interesting approaches exploiting knowledge graphs and self-supervised learning were also proposed. An LLM can be indirectly utilized to construct a graph neural"}, {"title": "III. PROMPTING", "content": "Prompting is a mechanism to formulate queries in a way that LLMs can respond better. While domain-specific and task-specific prompts often lead to desirable responses, crafting a proper prompt can be time-consuming. Recently, researchers have made efforts to devise universally applicable prompting. In line with this, a new prompting method for time series forecasting based on a statistical model, the SARIMA model, is proposed."}, {"title": "A. Baseline", "content": "A simple query structure for time series can be given as \"{time_series} forecast next k steps,\" where {time_series} is a placeholder for a sequence of numeric values. However, LLMs can exploit their world knowledge, such as dependencies of physical phenomena on time. In this perspective, it is beneficial to add essential context information. To this end, we use the prompt format from PromptCast [12] as the base query format, considering the forecasting performance of various fine-tuned LLMs. This type of prompt includes context information such as domain and time resolution. The prediction task is also framed naturally in line with the domain. An illustrative example query is: \"From April 15, 2020, Wednesday to April 29, 2020, Wednesday, the average temperature of region 110 was 44, 51, 59, 52, 51, 58, 64, 58, 60, 63, 60, 54, 53, 63, 66 degree on each day. What is the temperature going to be on April 30, 2020, Thursday?\""}, {"title": "B. SARIMA Prompting", "content": "One of the major challenges in forecasting is non- stationarity. The Seasonal ARIMA model is known to be an efficient way to address this by considering trend and seasonality together [12]. TEMPO [15], which exploits time series decomposition and transformer modules, has been shown to achieve SOTA performance over many time-series datasets. Considering these recent efforts to add some inductive bias to the model, balancing human knowledge of time series characteristics and the powerful inference capabilities of LLMs, we propose a prompting method that imposes decomposition of time series forecasting, which we call SARIMA prompting. The underlying motivation of SARIMA prompting is that LLMs are good at solving problems when presented as a series of easier subproblems. Additionally, mandating LLMs to work on predicting each component separately is expected to improve forecasting in the presence of trends and seasonality. The initial draft for prompting was designed to consist of steps for decomposing the time series into trend, seasonality, and short-term variations, predicting each component, and merging the results to create a final prediction. This draft was then refined through several iterative updates with ChatGPT. The corresponding prompt is shown in Figure 1."}, {"title": "C. Prompting for comparison", "content": "Existing promptings were also considered for comparison. While CoT [5] and Plan and Solve [7] were proposed for general purposes, LSTPrompt [6] was designed specifically for time series forecasting.\nCoT [5]: CoT prompting is a pioneering work that highlights the importance of prompting. Simply adding 'Think step by step' at the end of the query often elicits a reasoning procedure learned through training. However, calculation errors, missing-step errors, and misunderstanding errors were identified as major pitfalls with this prompting [7].\nPaS+ [7]: PaS was proposed to overcome missing-step errors. This prompting consists of a plan phase and a solve phase, with the expectation that the LLM will decompose a problem into subproblems during the plan phase and solve the problem step by step. PaS can be considered a refined CoT with additional instructions, 'Plan and Solve.' The PaS+, a detailed version of PaS, includes 'and show the **Final Answer** with predicted value only' to easily parse the predicted value'. The corresponding prompt is: 'A: Let's first understand the problem, extract relevant variables and their corresponding numerals, and make a plan. Then, let's carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the **Final Answer** with predicted value only.'\nLST [6]: While CoT and PaS+ can be applied to arbitrary tasks, the LST prompt was designed for time series forecasting. This prompt asks the LLM to solve the problem by integrating short-term and long-term forecasting. Additionally, it includes TimeBreath to reduce errors from cognitive load by segmenting the prediction horizon. It is closely related to the PaS+ prompt in that it exploits specific planning and solving steps tailored for time series forecasting. The prompts in the GitHub repository in [6] were used exactly the same way for fair comparison."}, {"title": "IV. EXPERIMENTS", "content": "Task: A single next value prediction for a short time series and a next 6 values prediction for a long time series will be considered.\nDatasets: The PISA dataset [15] was used for forecasting with a short time series, while the Individual Household Electric Power Consumption (IHEPC) dataset from the UCI Machine Learning Repository was used for forecasting with a long time series. The PISA dataset consists of three different time series: SG for the number of visitors over 15 days, CT for city temperature over 15 days, and ECL for electricity consumption over 15 days. The original IHEPC dataset was constructed from measurements taken per minute with 9 attributes. Due to the large variation per minute and the prediction horizon considered in this experiment, a single attribute, \"Global_intensity\" (average current per minute), was chosen and averaged over 60 minutes, resulting in 24 data points representing average hourly current usage per day. Subsequently, 3000 time series with a length of 96 data points were constructed by shifting every 10 data points. This dataset is expected to include seasonality and slight trends for a subset of series, and random variation over time associated with human activity.\nTokenizer and Model: The gpt-4o-mini-2024-07-18 by OpenAI, a distilled version of GPT-40, was used as the base model for experimenting with different prompts, considering both cost and performance. The maximum output token size was set to 1024 for all prompts except LST prompting, for which it was 1280. This model uses the Tiktoken tokenizer, which is based on byte pair encoding (BPE). The tokenizer does not treat numeric values as a single token but includes vocabularies for numbers from 0 to 999, highlighting a fundamental limitation in LLMs for time series analysis. For example, the number 13245 is tokenized into two tokens: 132 and 45, while 12.992 is tokenized into three tokens: 12, ., and 992.\nPrompting: The final query to the LLM was created by concatenating \"Q:\", the baseline prompt, and a prompt designed for each prompting method. Additionally, prompting with a one-shot example was also considered for CoT and SARIMA. The one-shot example was constructed with the following procedures: We manually checked 5 responses from each PISA dataset. Then, the one considered to be the best was selected by the author. It was refined to be applicable to time series independently of domain specificity by removing domain-specific terminologies such as temperature or power. This manually crafted example was further improved iteratively by asking OpenAI ChatGPT to generate a refined version, with the expectation that it may produce a prompt that ChatGPT can understand better. The one-shot example for SARIMA was designed similarly, except that a manually synthesized time series was used for numerical values so that the one-shot example could have an accurate answer for prediction at each component. For one-shot CoT and one-shot SARIMA, the final query was constructed by adding a zero- shot query at the end of the example. Corresponding examples can be available online [18].\nPerformances: The efficacy of prompting in forecasting was evaluated using root mean squared error (RMSE) and mean absolute error (MAE). Table I shows the performance with the SG dataset. Baseline prompting is observed to achieve the best performance. It is expected that the structure of the query was well designed for this dataset, even though it was originally designed to fine-tune the LLM. One exemplary query with baseline prompting is: \"From August 21, 2021, Saturday to September 04, 2021, Saturday, there were 19, 17, 20, 17, 23, 14, 13, 18, 20, 14, 10, 17, 16, 18, 5 people visiting POI 324 on each day. How many people will visit POI 324 on September 05, 2021, Sunday?\" The context of this query is that the numbers represent the daily count of people visiting a place. Additionally, there is date information that can potentially provide cues for forecasting. However, this sequence is unlikely to be influenced by any physical laws."}, {"title": "Qualitative Analysis", "content": "When all quantitative results are considered, it appears that there is no single powerful prompting method that provides good forecasting performance for all datasets. Therefore, it is meaningful to summarize the issues in the responses. Detailed analysis can be found online [18].\n1. Quantized forecasting was observed with the ECL dataset.\n2. The response from one-shot SARIMA often fails to subtract the trend component when dealing with seasonality, resulting in a significant offset in the final prediction. To accurately predict the seasonality component, the trend component should be subtracted from the series.\n3. Regardless of prompting, when an LLM performs algebraic operations with a significant number of operands, it often fails to calculate the operation correctly.\n4. Many promptings often fail to follow its planned reasoning steps, and instead relly only on recent trends, leading to inconsistencies between the prompt's direction and the actual calculation.\n5. Many promptings with the IHEPC dataset often fail to extract meaningful information associated with long-term variation over the whole sequence.\n6. LST prompting sometimes fails to generate the next 6 predicted values. It predicts 5 steps only\n7. Forecasting often depends on recent values only."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we characterized the effects of several different prompting methods for time series forecasting. Although not all existing promptings were considered, some frequently observed patterns in the responses associated with time series forecasting were identified. Reasoning steps involving simple algebraic operations often fail to calculate accurately, which is conjectured to be due to interference from the previously generated contexts. Moreover, if a reasoning step involves a parametric model with parameters determined from inaccurate calculations, it results in large errors. GPT- 40-mini is also observed to have a semantic gap between domain-specific and general terminology. For example, trends are often treated as short-term characteristics rather than long- term ones. Regardless of the prompting method, seasonality is rarely exploited properly to reduce forecasting errors effectively. In many cases, forecasting is based on several recent values only.\nPrompting that requires a certain structure for reasoning appears to have internal conflicts between the reasoning steps described in the prompt and its own reasoning steps learned from training, while simple prompting with key contextual information often provides reliable responses. Additionally, prompting with detailed procedures and requirements often leads to misunderstandings of the task associated with subtasks or integrating the subtask results. Considering the cost and potential degradation with inappropriate prompting, it is expected that providing proper context information on time series can be more important than asking for a specific type of reasoning, while there is potential that a specifically crafted prompting can effectively lead to better responses.\nSeveral interesting aspects need further attention. LLMs may have inherently limited performance in time series forecasting since they are simply trained to generate the next token based on preceding tokens. This can be especially the case when tokenization cannot effectively represent the relationships among numeric values in time series. Thus, proper fine-tuning is required to fit time series forecasting, which requires a training corpus that addresses the shortcomings of LLMs for time series forecasting. Moreover, joint fine-tuning and prompting may have potential for further improvement, such that fine-tuning can support the requirements of the prompting better, and datasets for fine- tuning can be more fitted to a specific type of prompting. Additionally, fundamental limitations by tokenization need to be addressed further. Proper scaling and offsetting can be an alternative way to deal with this issue approximately, rather than having an adaptor module for patching, which incurs additional complexity for training.\""}]}