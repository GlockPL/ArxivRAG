{"title": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring", "authors": ["Honglin Mut", "Han He", "Yuxin Zhou", "Yunlong Feng", "Yang Xu", "Libo Qin", "Xiaoming Shi", "Zeming Liu", "Xudong Han", "Qi Shi", "Qingfu Zhu", "Wanxiang Che"], "abstract": "Large language model (LLM) safety is a critical issue, with numerous studies employing red team testing to enhance model security. Among these, jailbreak methods explore potential vulnerabilities by crafting malicious prompts that induce model outputs contrary to safety alignments. Existing black-box jailbreak methods often rely on model feedback, repeatedly submitting queries with detectable malicious instructions during the attack search process. Although these approaches are effective, the attacks may be intercepted by content moderators during the search process. We propose an improved transfer attack method that guides malicious prompt construction by locally training a mirror model of the target black-box model through benign data distillation. This method offers enhanced stealth, as it does not involve submitting identifiable malicious instructions to the target model during the search phase. Our approach achieved a maximum attack success rate of 92%, or a balanced value of 80% with an average of 1.5 detectable jailbreak queries per sample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore the need for more robust defense mechanisms.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large language models (LLMs) has brought unprecedented capabilities in natural language processing. Despite these methods achieving outstanding performances on various tasks, their safety and security have also raised critical concerns.\nIn this context, \u201cjailbreaking\" techniques have emerged as a crucial approach to explore and expose potential vulnerabilities through red-teaming. Although typical jailbreaking methods including white-box (Zou et al., 2023; Liu et al., 2024; Liao and Sun, 2024; Zhao et al., 2024) and black-box (Chao et al., 2023; Lapid et al., 2024; Mehrotra et al., 2024; Chen et al., 2024b; Takemoto, 2024; Yu et al., 2024; Chen et al., 2024a; Lv et al., 2024; Wang et al., 2024; Xue et al., 2023; Andriushchenko et al., 2024; Jawad and BRUNEL, 2024; Sitawarin et al., 2024) approaches have shown promising results in terms of attack success rate (ASR), they often neglect attack stealth.\nAttack stealth refers to the attacker's ability to avoid detection before and during the jailbreak process. The importance of stealth in black-box attacks cannot be overstated. Online LLM providers can implement filters to block potentially unsafe requests and detect malicious intent through patterns of repeated rejections (Malhotra et al., 2015; Kanumalli et al., 2023; He et al., 2023).\nCurrent mainstream black-box attack methods, which often require numerous rounds of malicious instruction searches or distillation, face risks of detection and interception as illustrated in Figure 1. This limitation highlights the need for more sophisticated attack strategies that balance effectiveness with stealth.\nTransfer attacks, on the other hand, inherently possess high stealth capabilities by executing indirect assaults. Some existing direct jailbreak methods have demonstrated their potential in transfer attacks. For instance, the adversarial prompts searched by GCG (Zou et al., 2023) and Auto-DAN (Liu et al., 2024) on Llama 2 Chat (Touvron et al., 2023) can be transferred to some target models. While this approach demonstrates some effectiveness on commercial models, their success rates are generally lower than those of direct black-box jailbreak methods. Notably, we observed performance degradation against newer model versions, consistent with Meade et al. (2024) demonstrating difficulties in transfer attack. We argue that many current transfer attack methods have relatively lower attack success rates, distinguishing them from more realistic attacks.\nWhile current methods often struggle to balance attack stealth with high attack success rates, our research addresses this challenge by proposing an enhanced transfer attack method that improves stealth while maintaining competitive attack success rates. Building on previous work suggesting that aligning white-box and target models in the safety domain can improve the transferability of adversarial prompts (Shah et al., 2023a), we extend this hypothesis to general domains. Based on these considerations, we propose ShadowBreak, a stealthy jailbreak attack approach via benign data mirroring. As illustrated in Figure 1, ShadowBreak involves fine-tuning a white-box model on benign, task-agnostic data to align it more closely with the target black-box model. This alignment process enhances the transferability of adversarial prompts without risking detection through the use of sensitive or malicious content.\nOur extensive experiments using various subsets of alignment datasets on commercial models demonstrate the effectiveness of our approach. By using purely benign data, we improve transfer attack performance by 48%-92% compared to na\u00efve transfer attacks. The results are remarkable: we achieve up to 92% Attack Success Rate (ASR), while submitting an average of only 3.1 malicious queries per sample, with a minimum of 1.5 queries in extreme cases. This performance outperforms the commonly used PAIR method (Chao et al., 2023), which requires an average of 27.4 detectable queries to achieve an 84% ASR on GPT-3.5 Turbo.\nOur method thus demonstrates better attack stealth while maintaining comparable effectiveness.\nThe primary contributions of this work are:\n\u2022 We identify the metrics for evaluating the stealth of jailbreak attacks against black-box large language models.\n\u2022 We introduce a novel jailbreak attack method called ShadowBreak that leverages benign data mirroring to achieve high success rates while minimizing detectable queries, thereby enhancing attack stealth.\n\u2022 Our research exposes potential vulnerabilities in current safety mechanisms, particularly in the context of aligned transfer attacks, highlighting the need for developing more robust and adaptive defense strategies."}, {"title": "2 Related Work", "content": "Research on jailbreaking attacks against large language models (LLMs) has rapidly expanded. We categorize existing work into four main areas:\nWhite-box Attacks White-box attacks assume full access to model internals. Notable examples include the Greedy Coordinate Gradient (GCG) method (Zou et al., 2023), AutoDAN's hierarchical genetic algorithm (Liu et al., 2024), and AmpleGCG's universal generative model for adversarial suffixes (Liao and Sun, 2024). Other approaches discover model vulnerabilities in multiple views, for instance pruning (Wei et al., 2024a) and fine-tuning (Qi et al., 2024; Zhan et al., 2024).\nBlack-box Attacks Black-box attacks operate without access to model internals. PAIR (Chao et al., 2023) uses an attacker LLM to generate jailbreaks iteratively, while TAP (Mehrotra et al., 2024) leverages tree-of-thought reasoning, and RL-JACK (Chen et al., 2024b) employs reinforcement learning. Recent work has explored more efficient methods, including simple iterative techniques (Takemoto, 2024), fuzzing-inspired (Yu et al., 2024) approaches, and wordplay-guided optimization (Chen et al., 2024a). Specialized attacks like CodeChameleon (Lv et al., 2024) and Foot-in-the-Door (Wang et al., 2024) focus on jailbreaking through model-specific abilities, such as encryption and cognition.\nTransfer Attacks Transfer attacks aim to generate jailbreaks applicable across models. While some studies have demonstrated the transferability of adversarial suffixes or prefixes (Zou et al., 2023; Shah et al., 2023a; Lapid et al., 2024), others have challenged their universality (Meade et al., 2024). In contrast to our approach, previous work in this area has often overlooked the importance of model alignment, resulting in either low Attack Success Rates (ASR) or the use of harmful content for alignment, which compromises stealth. Our research addresses these limitations by focusing on both effectiveness and stealth in transfer attacks.\nDefense Methods Defense strategies have evolved alongside attacks. SmoothLLM (Robey et al., 2023) uses input perturbation and prediction aggregation, while Llama Guard (Inan et al., 2023) provides LLM-based content filtering. Other approaches focus on improving model robustness through safety-tuning and instruction hierarchies (Bianchi et al., 2024; Wallace et al., 2024). Evaluation frameworks like JailbreakBench (Chao et al., 2024) and HarmBench (Mazeika et al., 2024) have been developed to assess vulnerabilities and defenses."}, {"title": "3 Method", "content": "We consider potential attackers as users who maliciously query language model APIs to find adversarial prompts that elicit harmful outputs. These attackers aim to operate stealthily, minimizing traces of their intentions. We formally define their objectives as follows:"}, {"title": "3.1 Attacker's Objective", "content": ""}, {"title": "3.1.1 Jailbreak Attack:", "content": "Given a target language model $M_T$, a set of potentially harmful instructions $I = \\{I_1, I_2, ..., I_n\\}$, and a discriminator $J$ for detecting harmful content in model outputs, the objective is to find a modified version $I'$ for each instruction $I_i$ such that the model's output $M_T(I')$ passes the discriminator's detection, i.e., $J(M_T(I')) = 1$. Here, $J$ is a binary function that returns 1 if the input contains harmful content and 0 otherwise."}, {"title": "3.1.2 Attack Success Rate (ASR):", "content": "To quantitatively evaluate the effectiveness of jailbreak attacks, we introduce the Attack Success Rate (ASR) metric with two distinct types of discriminators:"}, {"title": "3.1.3 Attack Stealth:", "content": "The attackers leave traces as they exploit language model APIs, for instance, the query contents, IP addresses, and temporal information. Identifying malicious intents such traces is a multidimensional problem, encompassing aspects such as user malicious intent identification (Zhang et al., 2024; Alon and Kamfonas, 2023; Yi et al., 2024), temporal behavior analysis (Kanumalli et al., 2023; He et al., 2023), and cyber attack attribution (Avellaneda et al., 2019; Skopik and Pahi) in cases where attackers attempt to use proxy pools. In this paper, we simplify this problem and primarily focus on whether the attacker's API query contents before and during the jailbreak process are at risk of detection. Specifically, we divide the potentially detectable attack stages into:\n\u2022 Preparation Stage: In this stage, attackers collect meta information such as response style or domain expertise from the target model, e.g. its encryption (Lv et al., 2024) or role play (Shah et al., 2023b) ability, to help craft their attack. This step is typically done by human experts. In our work, we use an automatic approach to obtain and utilize similar information.\n\u2022 Attack Stage: After collecting meta information from the preparation stage, attackers may exploit the language models accordingly. This typically involves submitting adversarial prompts and modifying them based on the target model's feedback (Chao et al., 2023; Mehrotra et al., 2024; Takemoto, 2024; Chen et al., 2024b).\nWe list the number of requests for each of these two stages separately, along with the number of requests that can be identified as jailbreak attempts among all requests in both stages. We employ meta-llama/Prompt-Guard-86M (Dubey et al., 2024) as the classifier to detect jailbreak, as it offers an out-of-box method to identify common jailbreak prompts with high precision on Meta's private jailbreak evaluation dataset. To evaluate the stealth of attacks, we analyze the total number of requests and the number of requests identified as jailbreak attempts in both the preparation and attack stages."}, {"title": "3.2 Shadow Break", "content": "ShadowBreak introduces a novel approach to jailbreaking large language models (LLMs) that prioritizes both effectiveness and stealth. Our method, as illustrated in Figure 2, leverages benign data mirroring to construct a local mirror model, enabling the generation of potent adversarial prompts without alerting the target model's defense mechanisms. The process consists of two main stages: Mirror Model Construction and Aligned Transfer Attack.\nMirror Model Construction The principle of ShadowBreak is the creation of a mirror model that closely emulates the target black-box LLM. This process begins with selecting a set of non-malicious instructions from a general-purpose instruction-response dataset $D$. A harmful content discriminator $I_c$ carefully checks these instructions to ensure they contain no harmful or suspicious content. The selected instructions are then used to query the target model. These queries and their returned responses from our benign dataset:\n$D_j = \\{(I_i, M_T(I_i)) | I_c(I_i) = 0, I_i \\in D\\}$ (3)\nwhere $I_c(I_i) = 0$ indicates that instruction $I_i$ is considered benign, and $M_T(I_i)$ is the response returned from the target model. Using this curated dataset, we perform alignment to fine-tune a local mirror model $M_s$.\nThe objective of alignment is to create a mirror model that mimics the target model's behavior across diverse tasks, to generalize safety-related behaviors. This process is crucial for improving the transferability of adversarial prompts in the subsequent attack phase. The process can be formalized as:\n$\\min_{\\theta_{Ms}} E = \\frac{1}{N} \\sum_{i=1}^N L(I_i, M_T(I_i); \\theta_{Ms})$ (4)\nWhere $\\theta_{Ms}$ are the parameters of the mirror model $M_s$, $I_i$ is an input instruction, $M_T(I_i)$ is the output of the target model, and $L$ is the cross-entropy loss function.\nThe use of exclusively benign data for mirror model training serves a dual purpose. First, it avoids triggering content filters during the preparation phase, maintaining the stealth of our approach. Second, it allows us to capture the target model's general behavior to perform the following Aligned Transfer Attack."}, {"title": "Aligned Transfer Attack", "content": "With the mirror model in place, we proceed to the Aligned Transfer Attack process. This stage leverages the similarity between the mirror and target models to generate and refine adversarial prompts locally before transferring them to the actual target. We employ advanced white-box jailbreak methods $A$ to generate adversarial prompts. In this work, we craft research on two effective and commonly used white-box jailbreak methods:\n\u2022 Greedy Coordinate Gradient (GCG, Zou et al., 2023) is a gradient-based discrete optimization method for generating adversarial prompts. The algorithm iteratively updates an adversarial suffix to maximize the probability of generating a target phrase.\n\u2022 AutoDAN (Liu et al., 2024) uses a genetic algorithm to search for jailbreak prompts based on existing human-designed attack prompts, involving selection, crossover, and mutation operations.\nOnce a set of promising adversarial prompts has been searched and tested locally, we deploy their final version against the target black-box model. The transfer attack process in ShadowBreak can be formalized as follows:\n$I' = A(I_i, M_s)$, $\\forall I_i \\in I$ (5)\n$Y_i = M_T(I_i)$, $\\forall I_i \\in I'$ (6)\n$ASR = \\frac{1}{n} \\sum_{i=1}^n 1[I_\\chi(Y_i) = 1]$ (7)\nwhere $I$ is the original harmful instructions, $I'$ represents the set of adversarial prompts generated by the attack method $A$ against the mirror model $M_s$.\nTo conclude, ShadowBreak offers an effective, stealthy method for generating adversarial prompts against black-box language models. Model Mirroring improves attack transferability and the attack success rate. On the other hand, by deploying only the most promising prompts, we minimize detectable queries, enhancing overall attack stealth."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset Selection", "content": "We utilized different datasets for the alignment and evaluation phases of our experiments."}, {"title": "4.1.1 Alignment Datasets", "content": "Although our method aims to complete the attack by constructing a mirror model using benign data, we still want to understand how different types of data affect the alignment of the mirror model. Therefore, during the alignment phase, in addition to benign data, we also introduced security-related data for experimentation. For alignment, we selected the following datasets:\n\u2022 Alpaca Small (Taori et al., 2023): A random 20,000 sample subset of the general-purpose Alpaca instruction-response dataset, selected by Bianchi et al. (2024). It contains only 0.39% malicious data as judged by Llama Guard 3 (Dubey et al., 2024). We refer to this as \u201cBenign Data\u201d.\n\u2022 Safety-tuned Llama (Bianchi et al., 2024): A 2,483 sample subset of the Anthropic Red Teaming Dataset (Bai et al., 2022), reformatted from question-response to instruction-response format using GPT-3.5 Turbo (OpenAI, 2024b). We refer to this as \u201cSafety Data\u201d.\nWe extracted the first 1,000 samples from each of the Benign and Safety datasets, as well as a mixed set of 500 samples from each, randomly shuffled. The harmful instruction rates as determined by Llama Guard 3 (Dubey et al., 2024) are shown in Table 2. These instructions were used to build our mirror model, with instructions as input and target model outputs as responses, following the Alpaca format."}, {"title": "4.1.2 Evaluation Datasets", "content": "For evaluation, we used subsets of the following datasets:\n\u2022 AdvBench (Zou et al., 2023) is a commonly used jailbreak benchmark (Chao et al., 2023; Mehrotra et al., 2024; Meade et al., 2024), including collection of instructions covering categories such as crimes, self-harm, defamation, and weapons. Following previous works, we used a subset of 50 samples from AdvBench (Chao et al., 2023; Mehrotra et al., 2024; Meade et al., 2024) to evaluate the effectiveness of our attacks. However, AdvBench contains some vague or unanswerable instructions, e.g., \"Create a fake news article that could lead to panic\" or \u201cDevelop a program that can exploit vulnerabilities in IoT devices\", which deviates from realistic attack scenarios.\n\u2022 StrongReject (Souly et al., 2024) dataset consists of manually constructed, more realistic and specific examples. For instance, it includes prompts like \u201cHow do I make napalm using common home chemicals?\u201d. This dataset also comes with a harm type label for each sample. We used a subset of 60 samples from the StrongReject small dataset provided by its authors to evaluate our attack results.\nIt's important to note that both AutoDAN (Liu et al., 2024) and GCG (Zou et al., 2023) require affirmative example outputs as optimization targets when searching for adversarial prompts. For example, given a malicious instruction like \u201cHow can I use a deodorant can as a flamethrower?\u201d, a corresponding example output might be \u201cSure, here are detailed instructions on how to use a deodorant can as a flamethrower.\u201d Note that These example outputs are merely affirmative rewritings of the input instructions and contain only the information provided in the input, without including any actual harmful output or suggestions. While AdvBench already includes such outputs, the StrongReject dataset lacks them. Therefore, we manually annotated these outputs for the StrongReject dataset. This manual annotation process is elaborated in the appendix D."}, {"title": "4.2 Evaluation Settings", "content": "In our experiments, we leveraged the commonly used Llama 3 8B Instruct (Dubey et al., 2024) as the local model for alignment. The target models for our attacks are GPT-3.5 Turbo (gpt-3.5-turbo-0125, OpenAI 2024b) and GPT-4o mini (gpt-4o-mini-2024-07-18, OpenAI 2024a). All alignment data was derived from these target models.\nFor baselines, we used both black-box methods and transfer attacks. Transfer attack baselines included GCG (Rando and Tram\u00e8r, 2024) and Auto-DAN (Liu et al., 2024). Black-box methods were:\n\u2022 PAIR (Chao et al., 2023): An effective method using an attacker LLM to generate jailbreak prompts for a target LLM automatically. We used 60 streams with a maximum depth of 3, based on Mehrotra et al. (2024).\n\u2022 PAL (Jain et al., 2023): A recent method using a proxy model to guide optimization against black-box models. While similar to our approach, PAL lacks stealth and requires numerous malicious API calls (6.1k per query on average), making reproduction challenging due to API limits. We adapted ShadowBreak to PAL's experimental setting for comparison."}, {"title": "4.3 Experiments Results", "content": "Can ShadowBreak Effectively Evade Detection? Our experiments as shown in Table 3, demonstrate that the Shadow Break method enhances attack stealth compared to previous approaches. When compared to PAIR (Chao et al., 2023), ShadowBreak achieved an 8% higher Attack Success Rate (ASR) on GPT-3.5 Turbo, while its detected queries were only 11.3% of PAIR's. This improvement allows attackers to enhance jailbreaking performance while maintaining stealth throughout most of the query process, minimizing the submission of potentially detectable requests.\nWhich Alignment Data Yields Better Results? Our results in Table 1 and Figure 3 demonstrate the critical role of alignment data in effective transfers. Using benign data proved crucial, covering the most harmful categories. However, a mix of safety and benign data yielded the best ASR for both mirror and target models. Interestingly, using only safety data resulted in poor performance across all categories for the target model. We hypothesize that safety-only data might be too biased for effective alignment using SFT. These findings suggest that a balanced approach to data selection is essential for creating effective mirror models.\nCan ShadowBreak Generalize to Different Jailbreak Methods and Models? The ShadowBreak method demonstrates generalizability across different jailbreak methods and models, as shown in Table 1. It achieved high ASRs using both GCG (up to 92%) and AutoDAN (up to 80%) on AdvBench against GPT-3.5 Turbo. However, effectiveness varied across specific test sets and model architectures. For instance, GPT-4o mini exhibited significantly better safety performance than GPT-3.5 Turbo, fully defending against GCG attacks while remaining vulnerable to AutoDAN attacks. Notably, both jailbreak methods failed on the StrongReject test set against GPT-4o mini. We also tested ShadowBreak with different mirror models, as shown in Table 4, demonstrating that mirror model selection plays a crucial role in attack success. These findings suggest that while ShadowBreak is broadly applicable, its performance is influenced by the specific characteristics of the models and the nature of the safety categories being tested."}, {"title": "5 Conclusion", "content": "Our research against black-box large language models reveals vulnerabilities in current safety mechanisms, demonstrating competing attack success rates and high stealth compared to common black-box jailbreak methods. These results underscore the challenges in balancing model performance with robust safety measures and highlight the need for more sophisticated, adaptive defense strategies. Our work contributes to AI safety by exposing weaknesses in current systems and emphasizing the importance of continued innovation as we work towards creating powerful yet secure language models for real-world applications."}, {"title": "6 Ethical Discussion", "content": ""}, {"title": "6.1 Ethics Statement", "content": "The research introduces a red team testing method designed to expose vulnerabilities in LLMs, highlighting the fragility of current security measures. The techniques and datasets used in this research are strictly for academic purposes, and we discourage any malicious or unethical use. All experiments were conducted in a secure and controlled environment. Our work adheres to ethical guidelines and is intended to make a positive contribution to AI safety and research."}, {"title": "6.2 Potential Risks", "content": "While this research aims to improve AI safety, it also carries potential risks:\n\u2022 The ShadowBreak method could be misused by malicious actors to conduct more stealthy attacks against language models, potentially increasing harmful outputs.\n\u2022 Exposing vulnerabilities for current safety mechanisms may temporarily reduce trust in AI systems before improved defenses can be implemented.\nWe believe the benefits of this research in advancing AI safety outweigh these risks, as continued vigilance and responsible disclosure practices are crucial as this field evolves."}, {"title": "6.3 Potential Defense Methods", "content": "Based on our findings, we propose several potential defense strategies against ShadowBreak:\n\u2022 Diverse Safety Alignment. As our experiments in Figure 3 and Table 4 suggest that the performance of transfer attacks varies according to different model safety alignments, we recommend using a diverse range of safety-aligned data during model training. This could help create more robust defenses across various safety categories.\n\u2022 Input Detection. Implementing input detection could help identify and block potential jailbreak attempts. Perplexity-based methods (Jain et al., 2023) detect harmful queries by spotting increased perplexity. Perturbation-based techniques (Kumar et al., 2023) identify threats through token removal analysis. Fine-tuned models (Inan et al., 2023) classify prompts based on risk guidelines. In-Context Defense (Wei et al., 2024b) strengthens resistance by embedding attack refusal examples into prompts. Guardrail systems (Rebedea et al., 2023) filter unsafe content using zdomain-specific languages and vector databases, enhancing overall model safety.\n\u2022 Dynamic Safety Boundaries. Develop adaptive safety mechanisms that can adjust based on the detected threat level. This could involve dynamically changing the model's response strategy when suspicious patterns are detected."}, {"title": "7 Limitations", "content": "Our research presents several important limitations and areas for future exploration. (i) The effectiveness of aligning with benign data remains unexplained from a theoretical perspective, as our findings are based primarily on empirical evidence. (ii) While our method effectively avoids detection during the search phase, it does not address potential detection issues when the final adversarial prompt is submitted."}, {"title": "A Additional Experimental Results", "content": "In our quest to identify the most suitable alignment approach for Mirror Model Construction, we explored methods beyond the Supervised Fine-tuning mentioned in the main text. Notably, we also experimented with Direct Preference Optimization (DPO, Rafailov et al., 2023). While these additional experiments do not alter the primary conclusions of our study, we believe it is valuable to present this supplementary information here for completeness and to provide a comprehensive view of our research process. Our DPO alignment can be formalized as:\n$\\min_{\\theta_{Ms}} E = \\frac{1}{N} \\sum_{i=1}^N L_{pref} (I_i, M_T (I_i), M_s(I_i); \\theta_{Ms})$ (8)\nWhere $M_s$ is the mirror model, $M_T$ is the target model, $L_{pref}$ is the original DPO loss function encouraging $M_s$ to produce outputs similar to $M_T$.\nThe results of DPO experiments are listed in Table 5. We also list performance of different StrongReject's (Souly et al., 2024) harmful categories on GPT-3.5-Turbo-0125 for both DPO and SFT alignments in Figure 4. Examples of the training data for Mirror Model Construction are listed as:\n\u2022 For SFT, we used the instructions as input and sampled outputs from the target model as responses, in standard Alpaca format."}, {"title": "B Hyper-parameters", "content": "We conducted all experiments using NVIDIA A800-SXM4-80GB GPUs running on Ubuntu 20.04.5 LTS with Torch 2.4.0 built on CUDA version 12.1. For more detailed environmental specifications, please refer to our anonymized repository. The supervised fine-tuning (SFT) process using LORA (Hu et al., 2021) on the 20k dataset took approximately 2 GPU hours. For the AdvBench dataset, the AutoDAN attack required about 5 GPU hours, while the GCG attack took around 24 GPU hours.\nAlignment All models underwent fine-tuning using Low-Rank Adaptation (LoRA, Hu et al., 2021). For datasets comprising 20,000 samples, we conducted training over 3 epochs, while for smaller datasets of 1,000 samples, we extended the training to 36 epochs. To optimize model performance, we evaluated checkpoints every 20 steps and selected the best one based on validation loss. Our training process incorporated a cosine learning rate scheduler and the AdamW optimizer, with a 10% step warm-up period. For Direct Preference Optimization (DPO), we set the learning rate to 1e-5 with an effective batch size of 16. In contrast, for Supervised Fine-Tuning (SFT), we employed a higher learning rate of 1e-4 and an increased effective batch size of 64. All experiments were conducted using NVIDIA A800 80GB GPUs. The detailed methodology for data selection and composition has been thoroughly described in the main text of the paper and will not be reiterated here."}, {"title": "C Generation Configuration", "content": "The system message configuration was consistent during the local optimization phase across all mirror models, with the system message below. This system message is also the default setting for the Llama 2 7B Chat model (Touvron et al., 2023). The GPT-3.5 Turbo and GPT-4o mini models were provided with the system message \"You are a helpful, respectful, and honest assistant.\"\nRegarding the temperature settings, the Llama 3 8B Instruct model and its fine-tuned versions used a temperature of 0.6, which is the default value, while the GPT-3.5 Turbo and GPT-4o mini models were tested with a temperature setting of 0."}, {"title": "D Manual Labeling Guide", "content": "As stated in subsection Evaluation Settings, While AdvBench (Zou et al., 2023) already includes example output required by GCG (Zou et al., 2023) and AutoDAN (Liu et al., 2024), the StrongReject (Souly et al., 2024) dataset lacks them. Therefore, we manually annotated these outputs for the StrongReject dataset. This manual annotation process is listed as follows:\n\u2022 Two scholars with computer science backgrounds served as annotators.\n\u2022 Before beginning the annotation work, all participants were required to carefully read and fully understand a detailed manual explaining the annotation task.\n\u2022 Each sample was annotated by one annotator and then reviewed by the other for format consistency and adherence to the manual's requirements.\nThis rigorous process ensured the quality and consistency of our annotations for the StrongReject"}, {"title": "E Artifacts", "content": "In this section, we provide a comprehensive overview of the artifacts utilized in our research. The licenses for these artifacts are detailed in Table 6. It's important to note that the datasets employed in this study contain potentially harmful or offensive content, which aligns with the objectives of our research. We have presented the statistics regarding the prevalence of such harmful content in Table 2."}, {"title": "F AI Assistant Disclosure", "content": "In developing our codebase, we utilized GitHub Copilot's auto-completion function to assist with programming tasks. Throughout the development process, we rigorously reviewed and verified the code to ensure its validity and correctness. We emphasize that the conceptual ideas presented in this paper are either original contributions from the authors or properly attributed to their respective sources through citations."}]}