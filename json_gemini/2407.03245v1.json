{"title": "TieBot: Learning to Knot a Tie from Visual\nDemonstration through a Real-to-Sim-to-Real\nApproach", "authors": ["Weikun Peng", "Jun Lv", "Yuwei Zeng", "Haonan Chen", "Siheng Zhao", "Jicheng Sun", "Cewu Lu", "Lin Shao"], "abstract": "The tie-knotting task is highly challenging due to the tie's high de-\nformation and long-horizon manipulation actions. This work presents TieBot, a\nReal-to-Sim-to-Real learning from visual demonstration system for the robots to\nlearn to knot a tie. We introduce the Hierarchical Feature Matching approach to\nestimate a sequence of tie's meshes from the demonstration video. With these\nestimated meshes used as subgoals, we first learn a teacher policy using privi-\nleged information. Then, we learn a student policy with point cloud observation\nby imitating teacher policy. Lastly, our pipeline learns a residual policy when the\nlearned policy is applied to real-world execution, mitigating the Sim2Real gap.\nWe demonstrate the effectiveness of TieBot in simulation and the real world. In\nthe real-world experiment, a dual-arm robot successfully knots a tie, achieving\n50% success rate among 10 trials. Videos can be found on our website.", "sections": [{"title": "1 Introduction", "content": "Learning cloth manipulation holds great utility across a wide range of applications. One intriguing\ndomain is robotic tie knotting. Service robots must be adept at tasks like aiding the elderly or\nindividuals with disabilities in dressing for certain social events. Teaching robots to knot ties, as a\nspecial case of cloth manipulation, typically pushes the limits of robotic cloth manipulation. This\noffers valuable insights for tie knotting and the broader field of robotic cloth manipulation.\nCloth manipulation presents challenges for robots due to its high-dimensional state and complex\ndynamics. Extracting and modeling state information are difficult problems. In contrast, humans\nhave accumulated extensive knowledge about cloth manipulation. These priors make learning from\ndemonstration (LfD) a promising direction. LfD empowers a robot to acquire a policy from expert\ndemonstrations, significantly reducing the need to design task-specific reward functions manually.\nConsequently, LfD stands as a potent and efficient framework for instructing robots in the execution\nof complex skills.\nHowever, existing LfD methods struggle with tie-knotting tasks. Kinesthetic demonstration or tele-\noperation suffers from the complexity of tie-knotting tasks. Tie-knotting tasks require bi-manual\noperations, placing high demand on human operators' skills and equipment. For instance, Zhang et.\nal use VR headsets for teleoperation [1]. Thus, simple behavior cloning may be significantly labor-\nintensive. Learning from visual demonstration is usually an easier approach in terms of collecting\ndemonstration data. But this approach also leads to embodiment gaps. Therefore, researchers at-\ntempt to find some object-centric representations that robots can utilize to generate correct actions,\novercoming embodiment gaps. Several methods attempt to learn a general visual representation of"}, {"title": "2 Related Work", "content": "some simple pick-place skills via large-scale pre-training on actionless videos [2, 3, 4]. These works\npresent strong generalizations on the learned visual representations, but none of them shows the abil-\nity to learn dexterous manipulation skills that can knot a tie. Other methods such as [5, 6, 7, 8] try\nto leverage object trajectories or keypoints as representations to guide the policy learning. Such\nrepresentations are indeed sufficient to describe simple object motions but fail to capture the tie's\ncomplex topology and subtle dynamics.\nCompared to the existing LfD work mentioned\nin the previous paragraph, our insight is that\nmesh is the most suitable representation for\ntie-knotting tasks and other complex cloth ma-\nnipulation tasks. It captures accurate geomet-\nric structures and physics properties of the tie,\nwhich is crucial for tie-knotting tasks. It also\ndisentangles irrelevant information in the vi-\nsual demonstrations, such as environment back-\nground, object colors, and so forth, enabling\nthe learned policy to apply to different test set-\ntings. Therefore, we propose a Real-to-Sim-\nto-Real LfD framework. First, we propose a\nHierarchical Feature Matching method to itera-\ntively estimate the tie's meshes with cloth sim-\nulation from the demonstrated video. We use a\ncloth simulator called DiffClothAI [9] that sup-\nports intersection-free contact for cloth to main-\ntain the tie's topological structure during the\nestimation process. These estimated meshes\nfrom the demonstrated video are then used as\nsubgoals. To learn where to grasp the tie and\nwhere to pull the tie from point clouds observa-\ntions in simulation, we adopt a teacher-student\ntraining paradigm similar to [10]. Lastly, our\npipeline learns a residual policy when applying\nthe policy to real-world settings, mitigating the\nsim2real gap."}, {"title": "2.1 Cloth Manipulation", "content": "Previous work mainly addresses short-horizon cloth manipulation tasks that only involve simple\npick-place actions. There are several approaches to learning cloth manipulation skills. One approach\nis using model-free RL or learned dynamics model to learn cloth unfolding, rope rearranging, and\ndressing assistance tasks on raw sensor input [11, 12, 13, 14, 15]. Other approaches will collect\nand annotate data from images [16, 17] or generate demonstration trajectories in simulation [18] to"}, {"title": "2.2 Learning from Visual Demonstration", "content": "One line of research explores pre-training neural representations from actionless videos [20, 21, 2,\n22, 3, 4, 23]. This approach aims at learning general representations for different actions, whereas\nnone of them shows the ability to learn dexterous manipulation skills that can knot a tie. Another\nline of research attempts to learn from visual priors extracted from visual demonstrations, such as\nobject trajectories [5, 24], hand poses [25, 26], keypoints positions [27, 6], graph relations [7], or\naffordances [28]. The third approach is to learn a video or trajectory prediction model to guide policy\nlearning [29, 30, 31, 32, 8]. These approaches require in-domain demonstrations, placing restrictions\non visual demonstrations. Moreover, the prediction model may suffer from the long-horizon feature\nof tie-knotting tasks. ORION is the most closely related work, which builds a graph representation\nfrom object motions that can generalize across diverse test environments [33]. However, simple\ngraph representations cannot capture the tie's complex topology and subtle dynamics during the\ntie-knotting process.\nConsequently, we propose explicitly modeling the demonstration as a sequence of meshes. Mesh\ncan accurately describe the tie's structure and dynamics, which is crucial to learning correct robot\nactions and generalizing them to different test scenarios."}, {"title": "2.3 Cloth State Estimation", "content": "One cloth state estimation method directly predicts cloth states using deep learning [34, 35, 36].\nNon-rigid point cloud registration methods such as coherent point drifting are also applied for linear\ndeformable object tracking [37, 38, 39]. However, purely vision-based methods do not guarantee\ncorrect cloth topology due to the lack of physics prior. Huang et al. propose a method to reconstruct\nand track cloth state with a dynamics model [40]. However, this method requires known actions,\nwhich cannot be accessed easily from human demonstration sometimes.\nTherefore, we propose a Hierarchical Feature Matching method to iteratively estimate the tie mesh\nin the demonstration video with cloth simulation. Cloth simulation provides important physics prior\nfor state estimation, such as non-penetration, which is crucial for maintaining correct topology."}, {"title": "3 Technical Approach", "content": "This work presents a Real-to-Sim-to-Real LfD framework called TieBot to guide a dual-arm robot\nshown in Fig. 1 to knot the tie from an RGB-D demonstration video. An overview of our proposed\nmethod is in Fig. 2. We first describe the procedure to estimate the tie's mesh sequences from the\ndemonstrated video (Sec. 3.1). Using the tie's mesh sequences as subgoals, we introduce a pipeline\nto generate robot actions to manipulate the tie, using teacher-student training paradigm (Sec. 3.2).\nLastly, we discuss learning residual policy to mitigate Sim2Real gaps(Sec. 3.3)."}, {"title": "3.1 Real2Sim", "content": "To better estimate the tie's mesh, we propose to integrate cloth simulation into our pipeline, which\nprovides important physical prior such as non-penetration in the estimation process. We segment\nthe tie in the demonstrated RGB-D video using Track-Anything [41] and transform the associated"}, {"title": "3.1.1 Local Feature Matching", "content": "If $X_{t-1}^D$ and $X_t^P$ are aligned and there are\ncorrespondences between $X_t^P$ and $X_t^P$, we\ncan build up the correspondences from the tie's\nmesh to the next demonstrated point cloud $X_t^P$\nand move the tie's vertices to align $X_t^D$ towards\n$X_t^P$.\nHere we adopt an off-the-shelf feature match-\ning model called LoFTR [42] to build up cor-\nrespondences between two RGB images $I_{t-1}$\nand $I_t$."}, {"title": "3.1.2 Keypoints Detection", "content": "Keypoints detection can directly build corre-\nspondences between mesh vertices and the\npoint cloud. Thus, it will not be affected by oc-\nclusion. We define five keypoints along the tie's\nsurface and the corresponding five key vertices\non the mesh, shown in Fig. 4. For each key-\npoint as the origin, we define the local frame as\nfollows. The z direction is the surface normal"}, {"title": "3.1.3 Hierarchical Feature Matching(HFM)", "content": "Finally, we combine them as Hierarchical Feature Matching(HFM) for state estimation. Control\nvertices assigned in local feature matching and key vertices will be used together to pull the mesh to\ntarget positions specified by local feature matching and keypoints detection. Local feature matching\nprovides detailed motion of vertices, while global keypoints indicate a global tie's structure. This\nglobal structural information ensures the estimation won't deviate too much, alleviating the short-\ncomings of the local feature matching method. We use this method to estimate the tie's meshes\nfrom demonstration and output a sequence of meshes {$X_t^D$}. The next part will use these meshes as\nsubgoals to guide robot action generation."}, {"title": "3.2 Learn@Sim", "content": "Our pipeline begins to sequentially generate feasible robotic actions in the simulation to guide the\ntie {$X_t^D$} towards these subgoals. Since the tie-knotting task is a long-horizon task with multiple\ngrasp and pull actions, we aim to learn where to grasp the tie and where to pull the tie.\nFor where to pull, we apply a simple strategy. Once we identify the grasping vertices, we pull\nthese vertices to the positions of those vertices on the subgoal. For where to grasp, we adopt a\nsimilar teacher-student training paradigm in [10] to ease policy learning. Directly learning from\nhigh dimensional observations such as point cloud is data-inefficient because the policy needs to\nsimultaneously learn which features to extract from visual observations and what the high-rewarding\nactions are. On the contrary, learning a policy via RL from sufficient state information would be\nmuch easier, as suggested by [10]. Therefore, we first use privileged information to learn a teacher\npolicy, and then train a student policy imitating teacher policy with point clouds as observations.\nTeacher Policy We first learn a teacher policy to select proper grasping points using privileged\ninformation. The state s contains the previous tie's vertices positions and the point-wise displace-\nment for each tie vertices to the subgoal. The action a is one or two grasping vertices of all the tie\nmesh's vertices. The reward function R is defined in equation 1."}, {"title": "3.3 Sim2Real", "content": "When the robot knots the tie in the real world, the robot receives a segmented point cloud of the\ntie denoted as $X_{real}^t$ at each time step. Instead of directly applying the action $\\pi_{sim}(X_{real}^t)$ in the\nreal world, we train a residual policy that takes in the real point cloud $X_{real}^t$ and the output of\n$\\pi_{sim}(X_{real}^t)$, outputs small offsets to the positions of predicted grasping points and placing points.\nCombing the small offsets and predicted grasping and placing points positions, we finally generate\nthe action in real setting. We follow the training process of residual policy described in [45]."}, {"title": "4 Experiments", "content": "In this section, we introduce our experimental setup and conduct quantitative and qualitative eval-\nuations to demonstrate the effectiveness of our approach. Our experiments focus on answering the\nfollowing questions.\n\u2022 How do our pipeline and baseline methods perform on tie-knotting task?\n\u2022 How does HFM compare to other cloth state estimation methods?\n\u2022 Can our HFM apply to other cloth manipulation tasks?\nConsidering the complexity of the entire system, we provide additional experiment results, along\nwith detailed explanations of submodules, in the supplementary materials and website."}, {"title": "4.1 Comparing TieBot and baseline", "content": "We first evaluate the whole pipeline of TieBot and a baseline method in a tie-knotting task. We\nestimate a sequence of meshes from one human demonstration video. Then, we divide the whole\ntrajectory into 6 parts with 6 subgoals. Our teacher policy learns to select proper grasping points\nusing PPO [46], and student policy imitates the teacher policy to infer grasping points and placing\npoints from the point cloud. We evaluate TieBot and the baseline method 10 times for each of the\ntwo different ties in DiffClothAI and evaluate TieBot on two real ties with a dual-arm robot.\nMetrics We compare the success rate between our pipeline and ATM [8]. In simulation exper-\niments, if the distance of the final tie's mesh to the target tie's mesh is smaller than a threshold,\nwe consider it a success. In real-world experiments, if the little end of the tie is inserted into the\nhole, as shown in the final stage in Fig. 5, we consider it a success. Since the tie-knotting task is\nlong-horizon, we also compute the averaged number of achieved subgoals for further evaluation.\nATM ATM proposes to model tasks as points trajectories [8]. It first learns a trajectory prediction\nmodel, and then learns policy with the learned prediction model using imitation learning. Following\nsimilar experiment settings in ATM, we collect 100 demonstration videos in simulation to train the\ntrajectory prediction module. Then, we use the 45 demonstration videos with ground truth action\nannotations to train the policy network and test the policy in simulation. The action is the 3D offset\nof the grasping vertices."}, {"title": "4.2 Evaluating Hierarchical Feature Matching(HFM)", "content": "Real2Sim is the most important part of our pipeline. Without accurate state estimation, particularly\nestimating the correct topology for the tie, it's impossible to learn feasible policy to finish the task.\nTo illustrate the importance of different components of HFM and its performance against other cloth\nstate estimation methods, we design three experiments in simulation to test baseline methods and\nthe ablation versions of HFM.\nCoherent Point Drift Coherent Point Drift(CPD) is a non-rigid point cloud registration algorithm.\nWe employ the CPD to predict the target positions of the mesh vertices in the target point cloud and\ndirectly align the mesh to the target positions.\nAblated Version Ours w/o KP stands for only using local feature matching; Ours w/o LF stands\nfor using local feature matching and the predicted keypoints positions; Ours w/o FM stands for only\nusing predicted keypoints positions and local frames.\nExperiment Result The qualitative results are shown in Fig. 6. We can find that either CPD or\nablated versions of HFM cannot estimate the target mesh correctly among these three experiments.\nWe also compute the L2 Distance between the vertices of the target mesh and estimated mesh as\na quantitative evaluation shown in Tab. 2. It also suggests that the performance will degrade if we\ncancel some parts of HFM, while CPD deviates a lot from the correct states."}, {"title": "4.3 Apply HFM on Other Cloth Manipulation Tasks", "content": "We demonstrate that HFM can be applied to other cloth manipulation tasks. One is a different way\nto knot a tie. The other one is to fold a towel. We visualize the estimation results in Fig. 7. The\nresults show that HFM can be applied to different cloth manipulation tasks."}, {"title": "5 Conclusion", "content": "This work introduces a Real-to-Sim-to-Real LfD framework called TieBot for the robots to learn\nto knot a tie from visual demonstration. TieBot introduces the Hierarchical Feature Matching ap-\nproach to iteratively estimate a sequence of tie's meshes from the demonstrated video. TieBot adopts\na teacher-student training paradigm to learn grasping points and placing points from point clouds.\nLastly, our pipeline learns a residual policy when the imitated policy is applied to real-world execu-\ntion, mitigating the Sim2Real gap. We demonstrate that a dual-arm robot successfully knots the tie\nwith our framework, achieving 50% success rate over 10 trials.\nNonetheless, our pipeline has some limitations. First, our pipeline still requires manually setting the\ninitial state of the tie at the beginning of the Real2Sim stage. Second, our Real2Sim module requires\ntraining keypoints detection models iteratively, which takes a lot of time. Third, due to the hardware\nlimits, the last step in the real-world experiments shown in Fig. 5 is hardcode action. Better cloth\nmesh reconstruction methods, video tracking methods, and more dexterous robot arms may alleviate\nthese issues."}]}