{"title": "Region Prompt Tuning: Fine-grained Scene Text Detection Utilizing Region Text Prompt", "authors": ["Xingtao Lin", "Heqian Qiu", "Lanxiao Wang", "Ruihang Wang", "Linfeng Xu", "Hongliang Li"], "abstract": "Recent advancements in prompt tuning have successfully adapted large-scale models like Contrastive Language-Image Pre-trained (CLIP) for downstream tasks such as scene text detection. Typically, text prompt complements the text encoder's input, focusing on global features while neglecting fine-grained details, leading to fine-grained text being ignored in task of scene text detection. In this paper, we propose the region prompt tuning (RPT) method for fine-grained scene text detection, where region text prompt proposed would help focus on fine-grained features. Region prompt tuning method decomposes region text prompt into individual characters and splits visual feature map into region visual tokens, creating a one-to-one correspondence between characters and tokens. This allows a character matches the local features of a token, thereby avoiding the omission of detailed features and fine-grained text. To achieve this, we introduce a sharing position embedding to link each character with its corresponding token and employ a bidirectional distance loss to align each region text prompt character with the target \"text\". To refine the information at fine-grained level, we implement character-token level interactions before and after encoding. Our proposed method combines a general score map from the image-text process with a region score map derived from character-token matching, producing a final score map that could balance the global and local features and be fed into DBNet to detect the text. Experiments on benchmarks like ICDAR2015, TotalText, and CTW1500 demonstrate RPT impressive performance, underscoring its effectiveness for scene text detection.", "sections": [{"title": "1 Introduction", "content": "Scene text detection is a crucial task in image processing, focusing on locating characters, words, and sentences within random scenes in raw images. Its applications span various industries, including automated document processing, scene understanding, autonomous driving, and virtual reality. Therefore, by ensuring the accuracy of the text detection, the further task such as text recognition would be expanded and promoted further."}, {"title": "2 Related Work", "content": "Recent advancements in scene text detection have seen a shift from traditional deep learning-based methods like SPCNet [19], Mask TextSpotter [15], TextBoxes [13], and TextSnake [14] to more sophisticated approaches. The CLIP model by"}, {"title": "3 Methodology", "content": "In this paper, we present a framework of fine-grained text detection called RPT based on proposed region text prompt. We introduce general and region text prompt, establish character-token correspondence by sharing position embedding, import bidirectional distance loss and character-token interaction, and generate the final detection result from the score map derived from feature enhancement and fusion. The detailed framework is shown in Fig. 2."}, {"title": "3.1 Definition of Two Types Learnable Text Prompt", "content": "Unlike the traditional prompt-tuning paradigm like CoOp [5] within the CLIP framework, the learnable text prompt we designed can be split into two distinct"}, {"title": "3.1 Definition of Two Types Learnable Text Prompt", "content": "parts: the general text prompt $T_g \\in \\mathbb{R}^{1 \\times N_1 \\times C'}$ and the region text prompt $T_r \\in \\mathbb{R}^{1 \\times N_2 \\times C'}$, where $N_1$ stands for the length of $T_g$, and $N_2$ stands for the length of $T_r$, while the dimension of text prompt and word embedding $C'$ is set to 512. Since the class to be detected in the task of text detection is only 1, the fixed word embedding $T_f \\in \\mathbb{R}^{1 \\times N_f \\times C'}$ is generated from the single word \"text\" through the word embedding module, calculated as follows:\n\n$T_f = WordEmbedding(\u201ctext\u201d),$                                                                                                                                                                                                                   (1)\n\nwhere $N_f$ is the length of the fixed word embedding. Then, the general prompt $T_g$, which is designed as an implementation of $T_f$ for capturing the global features that fixed word embedding $T_f$, is placed after $T_f$ to form the input of text encoder. The input is called $T_i \\in \\mathbb{R}^{1 \\times (N_f+N_1) \\times C'}$, expressed as below:\n\n$T_i = [T_f, T_g].$                                                                                                                                                                                                                         (2)\n\nText encoder, from the CLIP framework with all its parameters frozen, encodes the text input $T_i$ into the final text embedding $T_o \\in \\mathbb{R}^{1 \\times (N_f+N_1) \\times C'}$, expressed as below:\n\n$T_o = TextEncoder(T_i),$                                                                                                                                                                                                                         (3)\n\nwhere $C$ is the dimension of text embedding set to 1024.\nParallel to the text encoding process, the visual branch uses the ResNet-50 [4] image encoder from the original CLIP architecture to extract the visual feature map $I_i \\in \\mathbb{R}^{\\frac{H}{d} \\times \\frac{W}{d} \\times C''}$ from the original image input $I \\in \\mathbb{R}^{H \\times W \\times 3}$. This is achieved through the ResNet-50 backbone network, followed by an AttentionPooling2d layer to obtain the final visual embedding $I_o \\in \\mathbb{R}^{\\frac{H}{d} \\times \\frac{W}{d} \\times C''}$. The process is expressed in two steps as below:\n\n$I_i = ResNet - 50(I),$                                                                                                                                                                                                                         (4)\n\n$I_o = AttentionPooling2d(I_i),$                                                                                                                                                                                                                         (5)\n\nwhere H is the height and W is the width of the input image I, d is the down-sampling rate set to 32, and C\" is the dimension of $I_i$ set to 2048.\nIn conventional methods like CoOp [5] and CoCoOp [27], the general prompt $T_g$ tends to match the visual features $I_o$ of the whole image during the image-text matching process. In contrast, our approach leverages the region prompt $T_r$ to capture fine-grained details within separate regions of the visual feature map $I_i$.\nIn other words, the region prompt $T_r$ is designed to match local features within these fine-grained regions. To achieve this, we first establish a corresponding relationship between the region text prompt $T_r$ and the visual feature map $I_i$ in character-token level. The region text prompt $T_r$ could be split into $N_2$ region text prompt characters, defined as follow:\n\n$T_r = {t_1, t_2, ..., t_a, ..., t_{N_2}},$                                                                                                                                                                                                                         (6)"}, {"title": "3.2 Sharing Position Embedding", "content": "where $t_a \\in \\mathbb{R}^{1 \\times 1 \\times C'}$ means the ath region text prompt character. We select $N_2$ s a perfect square, which can be expressed as the square of a factor k. To align $N_2$ region text prompt characters with $N_2$ region visual tokens, the visual feature map $I_i$ is divided into k parts along both the height and width dimensions, forming $k^2$ region visual tokens as below:\n\n$I_i = {i_1, i_2, ..., i_a, ..., i_{k^2}},$                                                                                                                                                                                                                         (7)\n\nwhere $i_a \\in \\mathbb{R}^{\\frac{H}{dk} \\times \\frac{W}{dk} \\times C''}$ denotes the the ath region visual token, corresponding to the ath region text prompt character $t_a$. This design ensures that each region text prompt character $t_a$ to a specific region visual token $i_a$.\n\nIn the AttentionPooling2d layer, the final visual embedding $I_o$ could be generated from the feature map $I_i$ in two steps. First, the learnable position embedding $P\\in\\mathbb{R}^{\\frac{H}{d} \\times \\frac{W}{d} \\times C''}$, is introduced to provide positional information for each pixel in $I_i$. Next, the final visual embedding $I_o$ is obtained through the multi-head attention [18] mechanism:\n\n$I_o = MultiHeadAttention(q, k, v = (I_i + P)).$                                                                                                                                                                                                                         (8)\n\nTo keep the output features of $T_f$ and $T_i$ separate, a distinct text encoder, called the prompt encoder, independently encodes the region text prompt $T_t$. Therefore, the text prompt embedding $T_p \\in \\mathbb{R}^{1 \\times N_2 \\times C'}$ is the output of encoding $T_r$ through the Prompt Encoder, which inherits almost all the pre-trained parameters of the CLIP text encoder, with the parameters being frozen, except the parameters of the learnable position embedding $P_r \\in \\mathbb{R}^{1 \\times N_2 \\times C'}$, as shown below:\n\n$T_p = PromptEncoder(T_r).$                                                                                                                                                                                                                         (9)\n\nIn the prompt encoder, the learnable position embedding $P_r$ captures the positional information of each region text character. By adding these parameters to the original region text prompt $T_r$, and feeding them into the Transformer [18] mechanism, the model captures the positional relationships between each region text prompt character as below:\n\n$T_p = Transformer(T_r + P_r).$                                                                                                                                                                                                                         (10)"}, {"title": "3.3 Bidirectional Distance Loss", "content": "The position embedding $P_r$ is designed to capture both the relative positional relationships within each text character and the correspondence between the region text prompt characters and their associated region visual tokens. Therefore, $P_r$ is is generated from the learnable position embedding $P$ in the visual branch, instead of inheriting it from the pre-trained CLIP text encoder. This process involves three steps shown in Fig. 3:\n\u2022 Identify Position Tokens: Identify the position tokens of corresponding region visual tokens in $P$, which shares the same shape as $I_i$. Split P into a set of position tokens as below:\n\n$P = {P_1, P_2, ..., P_a, ..., P_{N_2}},$                                                                                                                                                                                                                         (11)\n\nwhere $p_a \\in \\mathbb{R}^{\\frac{H}{dk} \\times \\frac{W}{dk} \\times C''}$ represents the position token of the ath region visual token $i_a$.\n\u2022 Convert Position Tokens to Position Characters: Convert each position token $P_a$ into a corresponding position character $P_{ra}$ as follow:\n\n$P_{ra} = LN1(Mean(p_a, dim = 1, dim = 2)),$                                                                                                                                                                                                                         (12)\n\nwhere $p_{ra} \\in \\mathbb{R}^{1 \\times 1 \\times C'}$, $Mean(\u00b7)$ calculates the average over height and width dimensions on every token, and $LN1$ is a linear layer projecting C\" to C' in third dimension, transferring a token to a character.\n\u2022 Form Final Position Embedding: Concatenate all position characters to form the final position embedding $P_r$ as below:\n\n$P_r = {p_{r1}, p_{r2}, ..., p_{rN_2} }.$                                                                                                                                                                                                                         (13)\n\nThrough these steps, the mechanism of sharing position embedding is introduced.\n\nThere are some defects in the design of region text prompt. Compared to the general prompt $T_g$, which can be seen as an implementation of the fixed text embedding $T_f$, the region text prompt characters might match features from their corresponding region visual tokens that are not closely related to the target \"text.\" Additionally, the general prompt $T_g$, may overlook fine-grained text features in favor of matching the global feature of the visual embedding. To address these issues, a bidirectional distance loss $L_{BD}$ is introduced. This loss helps each region text prompt character focus on the detection target \"text\" while encouraging the general prompt to consider fine-grained text features. First, a cosine similarity $Sim$ could be calculated between $T_i$ and $T_r$ as below:\n\n$Sim = Cos\\_Similarity(T_i, T_r) = \\frac{T_i T_r}{||T_i||_2||T_r||_2},$                                                                                                                                                                                                                         (14)\n\nSecond, The bidirectional distance loss $L_{BD}$ could be obtain from $Sim$ as follow:\n\n$L_{BD} = 1 - sim.$                                                                                                                                                                                                                         (15)"}, {"title": "3.4 Character-token Level Interaction", "content": "By incorporating this bidirectional distance loss, the model balances the focus between global and fine-grained features, improving the overall accuracy and robustness of text detection.\n\nInspired by DenseClip [10] using Transformer [18] decoder to fuse both text and visual information, we introduce a character-token level interaction both before and after encoding illustrated in Fig. 4, refining text and visual information via 4 Transformer decoders at fine-grained level.\nPre-Encoding Interaction Before encoding the region text prompt $T_r$ and visual feature map $I_i$ with their position embeddings $P_r$ and $P$ respectively, each region text prompt character $t_a$ with position character $p_{ra}$ updated through interaction with the corresponding region visual token $i_a$ and position token $p_a$:\n\n$t_{a+Pra} = t_{a+Pra}+l_1\\cdot Trans former Decoder1(q = (t_{a+pra}), (k,v) = (i_a+P_a)),$                                                                                                                                                                                                                         (16)\n\nwhere $l_1$ controls the impact of this interaction. This process updates $T_r + P_r$ at the character level before encoding $T_r + P_r$ through Transformer.\nSimilarly, in the visual branch, each region visual token $i_a$ with position token $p_a$ is updated through interaction with the corresponding text prompt character $t_a$ and position character $p_{ra}$:\n\n$i_{a + Pa} = i_{a + Pa}+l_2\\cdot Transformer Decoder2(q = (i_a + p_a), (k, v) = (t_a + p_{ra})),$                                                                                                                                                                                                                         (17)\n\nwhere $l_2$ controls the impact of this interaction. This process updates $I_i + P$at the token level before encoding $I + P$ through multi-head attention mechanism."}, {"title": "3.5 Dual Matching Methods", "content": "Post-Encoding Interaction Since the outputs from the AttentionPooling2d layer maintain tokens and characters sequentiality, the visual embedding $I_o$ could be split into sequential region embedding tokens:\n\n$I_o = {1_{o1}, 1_{o2}, ..., 1_{oa}, ..., 1_{oN_2} },$                                                                                                                                                                                                                         (18)\n\nwhere each $1_{oa} \\in \\mathbb{R}^{\\frac{H}{d} \\times \\frac{W}{d} \\times C}$ stands for ath region embedding token. Similarly, the text prompt embedding $T_p$ is divided into sequential text prompt embedding characters:\n\n$T_p = {t_{p1}, t_{p2}, ..., t_{pa}, ..., t_{pN_2}},$                                                                                                                                                                                                                         (19)\n\nwhere $t_{pa} \\in \\mathbb{R}^{1 \\times 1 \\times C}$ stands for ath the text prompt embedding character. Each region text prompt embedding $t_{pa}$ is then updated via interaction with the corresponding region embedding token $1_{oa}$:\n\n$t_{pa} = t_{pa} + l_3\\cdot Transformer Decoder3(q = t_{pa}, (k, v) = 1_{oa}),$                                                                                                                                                                                                                         (20)\n\nwhere $l_3$ controls the impact of this interaction. This updates $T_p$ at the character level after encoding.\nSimilarly, each region embedding token $1_{oa}$ is updated via interaction with the corresponding text prompt embedding character $t_{pa}$:\n\n$1_{oa} = 1_{oa} + l_4\\cdot Trans former Decoder4(q = 1_{oa}, (k, v) = t_{pa}),$                                                                                                                                                                                                                         (21)\n\nwhere $l_4$ is also the learning parameter to control the impact of the result of character-token interaction in the equation above.\n\nDifferent from the original matching mechanism in CLIP [1], our proposed work introduces two parallel matching methods.\nImage-text Matching The first method image-text matching matches the text embedding $T_o$ with the visual embedding of the whole image $I_o$ to generate a global score map $S_{glo} \\in \\mathbb{R}^{\\frac{H}{d} \\times \\frac{W}{d} \\times 1}$, formulated as below:\n\n$S_{glo} = Sigmoid(\\frac{T_o I_o}{\\tau}),$                                                                                                                                                                                                                         (22)\n\nwhere \u03c4 is the temperature coefficient set to 0.07.\nCharacter-token Matching The second method matches the text prompt em-bedding $T_p$ with the image embedding $I_i$ at the character-token level to produce the region score map $S_{reg}$. Each score map token $s_a \\in \\mathbb{R}^{\\frac{H}{d} \\times \\frac{W}{d} \\times 1}$ represents ath part of the region score map, and is generated through the process of character-token matching as follow:\n\n$Sa = Sigmoid(\\frac{t_{pa}toa}{\\tau}),$                                                                                                                                                                                                                         (23)"}, {"title": "3.6 Feature Enhancement and Fusion", "content": "where $t_{pa}$ and $1_{oa}$ are the region text prompt embedding character and region embedding token with the same serial number a. Therefore, by concatenating all the $N_2$ score map tokens, the region score map $S_{reg}$ could be derived as below:\n\n$S_{reg} = {s_1, s_2,...sa, ..., s_{N_2} }.$                                                                                                                                                                                                                         (24)\n\nTo achieve the final score map S, a mechanism is employed to enhance and fuse the global score map $S_{glo}$ and the region score map $S_{reg}$. This process involves two main steps: feature enhancement and feature fusion.\nFeature Enhancement First, the region score map $S_{reg}$ could be seen as an enhancement to the global score map $S_{glo}$, capturing fine-grained features that may be overlooked by $S_{glo}$ at token level. The feature enhancement score map $S_{FE}$ is calculated as follow:\n\n$S_{FE} = S_{glo} + S_{reg}.$                                                                                                                                                                                                                         (25)\n\nFeature Fusion Second, to discover the deeper relationship between the two score maps, feature fusion is imported necessarily. This is achieved using a Transformer mechanism, which strengthens the connection between $S_{glo}$ and $S_{reg}$. The feature fusion score map $S_{FF}$ is obtained through a transformer decoder as follow:\n\n$S_{FF} = Transformer Decoder5(q = S_{glo}, (k, v) = S_{reg}),$                                                                                                                                                                                                                         (26)\n\nwhere $S_{glo}$ is set to the query, while $S_{reg}$ is set to key and value.\nFinal Score Map Finally, the score map S is generated by combining the feature enhancement score map $S_{FE}$ and the feature fusion score map $S_{FF}$:\n\n$S = S_{FE} + \\lambda_{mix}S_{FF},$                                                                                                                                                                                                                         (27)\n\nwhere $\u03bb_{mix}$ is a hyper-parameter set to 2 in this work. Then the score map S is reshaped from $\\mathbb{R}^{\\frac{H}{d} \\times \\frac{W}{d} \\times 1}$ to $\\mathbb{R}^{H \\times W \\times 1}$ using bilinear interpolation, representing the probability of text at every pixel.\nMatching loss Similar to the contrastive loss in the original CLIP, a matching loss $L_{mat}$ is calculated for every single pixel using cross-entropy loss [26]:\n\n$L_{mat} = \\sum_{i}^{H}\\sum_{j}^{W}CrossEntropy(s_{i,j}, y_{i,j}),$                                                                                                                                                                                                                         (28)\n\nwhere $s_{i,j}$ is the the probability of text at pixel (i,j), and $y_{i,j}$ is the ground truth label for the presence of text at pixel (i,j).\nThe pixel-level score map S is then fed into the downstream detection head, DBNet [8], which derives the final inference results for the whole task."}, {"title": "3.7 Optimization", "content": "The loss function $L_{sum}$ contains three different parts of loss function as below:\n\n$L_{sum} = L_{DB} + \\lambda_1L_{BD} + \\lambda_2L_{mat},$                                                                                                                                                                                                                         (29)\n\nwhere \u03bb1 and \u03bb2 are hyper-parameters set to 1 in our work, and $L_{DB}$ is the loss function of the detection head DBNet. Through the stage of training, the parameters of the whole network are updated by the reduction of the loss function $L_{sum}$."}, {"title": "4 Experiment", "content": "Experiments were conducted on three renowned datasets: ICDAR2015 (IC15) [6], TotalText (TT) [3], and CTW1500 (CTW) [23]. The ICDAR2015 dataset focuses on incidental scene text and contains 1,000 training images and 500 testing images, annotated with bounding boxes and text transcriptions. The TotalText dataset emphasizes curved and arbitrarily-shaped text in natural scenes, featuring 1,255 training images and 300 testing images, annotated with polygonal shapes. The CTW1500 dataset specializes in curved text in natural scenes, comprising 1,000 training images and 500 testing images, annotated with polygonal bounding boxes to accurately depict text shapes."}, {"title": "4.2 Evaluation Metric", "content": "The performance of text detectors is evaluated using several key metrics: precision, which measures the accuracy of detected text regions; recall, which assesses the completeness of detections; F-measure, which is the harmonic mean of precision and recall; and Intersection over Union (IoU) [16], which evaluates the overlap between detected and ground truth bounding boxes."}, {"title": "4.3 Implementation Details", "content": "In this experiment, DBNet serves as both the text detection head and a baseline for RPT, which leverages the pre-trained ResNet-50 [4] model from CLIP as its backbone. All Transformer [18] decoders have 4 layers and 3 heads, with a width of 256. RPT is trained individually on the IC15, TT, and CTW datasets for 1800 epochs without pre-training and subsequently evaluated on their respective test datasets. We leverage the computing power of two TITAN Xp Graphics Cards (12GiB each) with a batch size of 4 images during training."}, {"title": "4.4 Ablation Study", "content": "The baseline for our proposed method is established using the DBNet [8] detection head, where the original backbone of DBNet is replaced with the ResNet-50 [4] network from the image encoder in the CLIP framework. Building on this baseline, we incrementally add the general text prompt with the fixed word embedding, region text prompt with the feature enhancement, the mechanism of sharing position embedding, character-token level interaction, bidirectional distance loss, and feature fusion to evaluate their impact on text detection performance on the IC15 dataset. The results are illustrated in Table 1.\nGeneral Text Prompt with The Fixed Word Embedding After incorporating the general text prompt (length is 4) and fixed word embedding \"text\" into the framework, the CLIP image-text matching mechanism is activated. Compared to the baseline, this transformation significantly enhances performance by 0.3%, illustrated in third row of Table 1.\nRegion Text Prompt with Feature Enhancement With the assistance of the region text prompt, with a shape of 33, expected to be encoded in the prompt encoder, the performance of the text detection in IC15 highly boosts 0.6%, illustrated in fourth row of Table 1, under the circumstance that character-token matching and feature enhancement mechanism is activated.\nSharing Position Embedding Instead of directly adopting the learnable position embedding of the original CLIP text encoder, the prompt encoder utilizes the sharing position embedding to encode the region text prompt, improving the performance by 0.7%, illustrated in fifth row of Table 1."}, {"title": "Character-Token Level Interaction", "content": "With the help of the interaction in character-token level before and after the encoding of the region text prompt, the performance is promoted by 0.5%, shown in sixth row of Table 1.\nBidirectional Distance Loss After adding the bidirectional distance loss, the performance of the text detection raises 0.6%, shown in seventh row of Table 1.\nFeature Fusion In contrast to simply add two score maps, the process of the feature fusion upgrades the performance of the text detection by 0.1%, illustrated in eighth row of Table 1.\nThe Size of Region Text Prompt As the size of the region text prompt increases gradually from 3.3 to 4.4, 5.5, indicating the number of characters or tokens updates from 9 to 16, 25, the performance of scene text detection improves from 90.5% up to 90.7%, 90.9%. This shows that the finer-grained region text prompt could help detect text well."}, {"title": "4.5 Comparison with The Existing Paradigms of Method", "content": "We compare our proposed method with the existing paradigms of the scene text detection, including TextSnake [14], LOMO [24], I3CL [12], MSR [22], PAN [20], ContourNet [21], FCENet [28], \u0392\u03a1\u039d\u0395\u03a4 [25], PCR [11], FSG [17], DB+TCM [9] illustrated in Table 2. From the table 2, we can conclude that compared to existing text detection paradigms, our proposed method RPT significantly"}, {"title": "4.6 Visualization Results", "content": "We selected 3 samples from the IC15 dataset, each with increasing text density.\nFig.5 shows the heat map of the final score map and detection result of 3 samples respectively. In sample 1, sparse yet prominent text is accurately found by the score map shown in Fig. 5 (a), resulting in accurate detection shown in Fig. 5 (d). Sample 2, which contains moderately dense and scattered text, is identified accurately in the score map shown in Fig. 5 (b), making the detection result accurate shown in Fig. 5 (e). Sample 3 presents a very dense text, and multiple instances in close proximity. While most are detected, the current granularity of the region text prompt leads to some omissions and mistakes both in the score map shown in Fig. 5 (c) and detection result shown in Fig. 5 (f). Refining the granularity of the region text prompt could probably solve this problem because more tokens and characters can more accurately match denser text."}, {"title": "5 Conclusion", "content": "In this paper, we propose a region text tuning method (RPT) for fine-grained scene text detection, which includes the mechanism of sharing position embedding, bidirectional distance loss, character-token level interaction and feature enhancement and fusion. Extensive experiments illustrate that our proposed RPT could detect the boundaries of text in multiple challenging datasets. We would maintain further study to try different text detection heads and finer granularity of region text prompt to make region prompt tuning robuster."}]}