{"title": "FANTASTIC LLMS FOR PREFERENCE DATA ANNOTATION AND HOW TO (NOT) FIND THEM", "authors": ["Guangxuan Xu", "Kai Xu", "Shivchander Sudalairaj", "Hao Wang", "Akash Srivastava"], "abstract": "Preference tuning of large language models (LLMs) relies on high-quality human preference data, which is often expensive and time-consuming to gather. While existing methods can use trained reward models or proprietary model as judges for preference annotation, they have notable drawbacks: training reward models remain dependent on initial human data, and using proprietary model imposes license restrictions that inhibits commercial usage. In this paper, we introduce customized density ratio (CDR) that leverages open-source LLMs for data annotation, offering an accessible and effective solution. Our approach uses the log-density ratio between a well-aligned LLM and a less aligned LLM as a reward signal. We explores 221 different LLMs pairs and empirically demonstrate that increasing the performance gap between paired LLMs correlates with better reward generalization. Furthermore, we show that tailoring the density ratio reward function with specific criteria and preference exemplars enhances performance across domains and within target areas.\nIn our experiment using density ratio from a pair of Mistral-7B models, CDR achieves a RewardBench score of 82.6, outperforming the best trained reward functions from same model class and demonstrating competitive performance against SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an on-policy preference dataset with which we preference tune Llama-3-8B-Instruct with SimPO. The final model achieves a 37.4% (+15.1%) win rate on ArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a score of 8.0 on MT-Bench.", "sections": [{"title": "1 INTRODUCTION", "content": "Preference tuning has advanced the capabilities of large language models (LLMs), but this progress relies on high-quality human preference data which is both costly and time-consuming to gather. Cutting-edge models (e.g., ChatGPT, GPT-4, Claude-3) are aligned with curated, quality-controlled human preference data, typically provided by specialized companies. While effective, this approach limits broader adoption due to prohibitive costs and limited transparency in data collection (Wang et al., 2024c). AI-feedback solutions are emerging as an alternative\u2014either through a trained reward model (Dong et al., 2024) or proprietary LLM-as-a-judge (Cui et al., 2023). However, training reward models still rely on costly initial human preference data, and proprietary LLM-as-a-judge approaches introduce licensing restrictions that generally prevent commercial use.\nThis paper introduces customized density ratio (CDR) that leverages the density ratio between off-the-shelf open-source LLMs to efficiently facilitate preference data annotation. Our method uses the log-density ratio between a well-aligned model and less-aligned model to annotate preference data. We show that a higher alignment gap between model pairs yields improved preference signal. This observation, referred to as the \u201cStrong-over-Weak Hypothesis\u201d, is supported by our experiments across 221 model combinations (Figure 1). Notably, log-density ratios between a post-DPO model and a pre-DPO model, known as the DPO implicit reward (Rafailov et al., 2023), have not gained widespread adoption due to limitations in generalizability and high reward variance in reward performance across different model choices (Lambert et al., 2024; Lin et al., 2024). We show that"}, {"title": "2 BACKGROUND", "content": "Before diving into preference data annotation, we review two core optimization objectives in prefer- \nence tuning. The first optimization is a maximum likelihood estimator of the Bradley-Terry model \nthat aims to learn an optimal reward function. Given dataset of N samples D = {(x^(i), y_w^(i),y_l^(i))}_(i=1)^(N), \nthe objective in equation 1 aims to increase the reward for the preferred response y_w^(i) while reducing \nit for the dispreferred response y_l^(i) for each prompt x^(i). The second optimization, equation 2, given \na reward function r, focuses on finding an optimal policy \u03c0, e.g. LLM, that maximizes the reward \nwhile remaining close to a reference model \u03c0_(ref).\narg min_r \u2212E_(x,y_w,y_l)~D [log \u03c3 (r(x, y_w) \u2212 r(x, y_l))](optimal reward) (1)\narg max_\u03c0 E_(x~D,y~\u03c0(y|x)) [r(x, y)] \u2212 \u03b2D_(KL) [\u03c0(y|x)||\u03c0_(ref)(y|x)](optimal policy) (2)\nwhere \u03c3 is the sigmoid function. For preference tuning through RLHF (Ouyang et al., 2022), the \nprocess begins by optimizing (1) to identify an optimal reward function. This learned reward function \nis then incorporated into the second optimization step (2) to align the language model's policy. Policy \noptimization (2) is typically solved using RL algorithms, such as PPO, because external reward is \nnon-differentiable w.r.t model parameter.\nNow, given a reward function r(x, y), Rafailov et al. (2023) shows that the solution to equation 2 is\n\u03c0^*(y|x) = 1/(Z(x)) \u03c0_(ref) (y|x) exp (1/\u03b2 r(x,y)) (3)\nwhich implies the corresponding reward function can be written in forms of the policies as:\nr(x,y) = \u03b2 log (\u03c0^*(y|x)/(\u03c0_(ref)(y|x)) ) + \u03b2 log Z(x). (4)\nA key insight of DPO (Rafailov et al., 2023) is that this implicit reward function can then be incorporated into the reward optimization objective (1) to formulate a maximum likelihood objective \nfor a parametrized policy \u03c0_\u03b8 directly, without explicitly learning a reward function. The DPO loss is as below\nL_(DPO)(\u03c0_\u03b8; \u03c0_(ref)) = \u2212E_((x,y_w,y_l)~D) [log \u03c3 (\u03b2 (log (\u03c0_\u03b8 (y_w| x)/\u03c0_(ref)(y_w | x))\u2212log(\u03c0_\u03b8 (y_l| x)/\u03c0_(ref) (y_l| x))))]. (5)\nDPO Implicit Reward In this way, DPO objective in equation 5 skips the need of explicit reward and directly optimizes the parametric model \u03c0_\u03b8, which is equivalent to fitting a reparametrized \nBradley Terry reward model in equation 1 under a change of variables (see Section 5 in Rafailov et al. \n(2023) for details). In other words, optimizing DPO objective also learns an implicit reward function \nr(x,y) = \u03b2 log (\u03c0_\u03b8 (y| x)/\u03c0_(ref) (y | x)) that approximates the ground-truth reward according to equation 1. Based \non this connection, follow-up works (Lambert et al., 2024; Lin et al., 2024; Chen et al., 2024) directly \nleverage the implicit reward function to annotate preference data. Their data annotation process works \nas follow. For a given prompt x and two responses y_1, y_2, the preference is annotated by comparing \nr(x, y_1) with r(x, y_2): the one with a higher reward is labeled as preferred, while the other is marked as dispreferred. This comparison can be simplified to compare the log-density ratios only, essentially\nlog (\u03c0_\u03b8 (y_1| x)/\u03c0_(ref) (y_1| x)) v.s. log (\u03c0_\u03b8 (y_2| x)/\u03c0_(ref) (y_2| x))"}, {"title": "3 METHOD", "content": "We study two research questions critical to reward function design based on density ratio. First, we discuss how to find successful pairs of LLMs to construct effective density ratio reward (section 3.1). Our results reveal a strong correlation between the alignment gap of model pairs (measured by the ArenaHard score) and the effectiveness of the reward function (evaluated through the RewardBench score). These results also suggest that a wider range of model pairs beyond DPO implicit reward to be used for constructing reward functions, provided the gap in their human-aligned level is sufficient. Second, we study how to make density ratio a customizable and refined preference signal tailored to defined criterion (section 3.2). We show that using domain-specific instructions and exemplars \nto condition density ratio significantly improve overall and domain reward performance, without needing additional training."}, {"title": "3.1 REWARD FUNCTION DESIGN USING LOG-DENSITY RATIO", "content": "Motivation We explore constructing density ratio reward functino with various pairings of LLMs. \nAt first glance, one might assume that the DPO model and its reference policy would be the optimal \npair for this purpose. To examine this hypothesis, we conduct an experiment applying online iterative \nDPO (Xiong et al., 2023; Xu et al., 2023; Swamy et al., 2024) to both the Mistral and Llama-3 model families. The key ideas of online iterative DPO training are: (1) the reference model is updated at each iteration (i.e., \u03c0_(ref) = \u03c0_(\u03b8_(t\u22121))), and (2) the training data is also updated iteratively, with y_w and y_l \ngenerated by sampling from \u03c0_(\u03b8_(t\u22121))(\u00b7 | x) and annotated with preferences using an external reward function. We revisit the loss function in comparison to the DPO loss function in (5):\nL_(iter_DPO)(\u03c0_(\u03b8_t); \u03c0_(\u03b8_(t\u22121))) = -E_((x,y_w,y_l)~D_t) [log \u03c3 (\u03b2 (log (\u03c0_(\u03b8_t) (y_w| x)/\u03c0_(\u03b8_(t\u22121))(y_w | x))\u2212log(\u03c0_(\u03b8_t) (y_l| x)/\u03c0_(\u03b8_(t\u22121))(y_l | x))))]. (6)\nWe ensure that the DPO objective is effectively optimized at each iteration, as we see consistent \nimprovement in updated model's ArenaHard score (see Figure 2). This process starts with an SFT \nmodel trained from Base, and three DPO checkpoints from 3 DPO optimization iterations (obtained \nfrom Chen et al. (2024)). We use the log-density ratio between all model combinations within each \nfamily (Mistral or Llama-3) to define reward functions and evaluate the effectiveness of these reward \nfunctions via RewardBench."}, {"title": "3.2 REWARD FUNCTION CUSTOMIZATION", "content": "Human preferences are multi-dimensional (e.g., safety, trustworthiness, reliability, faithfulness) (Bai et al., 2022; Wang et al., 2024c; Naseem et al., 2024), and an effective reward function should \nadapt its criteria according to the specific domain requirements. For example, a chatbot explaining \ncorporate vacation policies should emphasize faithfulness to company policy and the accuracy of its \nresponses, rather than focusing on aspects like conversational style or user engagement. However, \nvanilla log-density ratio reward function provides a single, aggregated reward signal, merging various, \npotentially conflicting preference aspects. Therefore, it is crucial to define preferences clearly and \nprovide concrete examples to tailor the contrastive reward signal to the specific domain or aspect of \nhuman preference.\nWe introduce CDR, which specifies the domains for each prompt and incorporates instructions \nand in-context-learning (ICL) examples to define criteria for positive and negative preferences (see \nfigure 4). Each domain has customized instructions and ICL examples, and we ensure diversity \nby preparing multiple ICL demonstrations, sampling one randomly for each instruction. Formally, \nfor each original user prompt x, we inject ICL examples and domain-specific instructions T(x) \nto guide the annotation toward relevant preference dimensions. This is equivalent to adapting the \nreward function into the following form, incorporating T(x) before applying the log-density ratio for \nannotation.\nr^(CDR) (x, y) = log \u03c0_(strong) (y | T(x), x) \u2212 log \u03c0_(Tweak) (y | T(x),x). (8)\nTo automate annotation, we introduce a domain router that identifies the most relevant domain for each user query, allowing us to apply the appropriate preference criteria for each example in annotation set. For instance, a sensitive query is routed to a Safety expert, while a math or coding query goes to a Math/Code expert. In this paper, we employ the Mixtral 8x7B Instruct v0.1 model (Jiang et al., 2024) with zero-shot prompting to classify prompts into predefined categories (e.g., safety, reasoning, chat) based on a system prompt and task description.\nThese in-context examples and instructions serve as both demonstrative and descriptive tools to help define and refine the model's preference criterion. Example templates we used can be found at figure 3 and figure 4. For domains like safety, instructions should include guidelines on how to avoid \nrisky outcomes, while in domains like math, demonstrating the preference criterion through examples may be more effective. These instructions provide high-level guidance by defining overarching principles or definitions that shape the reward function's preferences during data annotation."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 STRONG-OVER-WEAK REWARD ANNOTATION", "content": "To test the effectiveness of the strong-over-weak reward model setup, we design experiments using various models as strong and weak across two model families: Mistral and Llama.\nSetup We collect off-the-shelf LLMs with varying levels of human alignment. To approximate \neach model's degree of alignment to human preferences, we use ArenaHard (Li et al., 2024), a \nbenchmark demonstrated to yield the highest correlation and separability with real human judgments \nin ChatArena. We then assess the density ratio reward function by pairing distinct model combinations \nthrough RewardBench (Lambert et al., 2024). Each sample in RewardBench consists of a human- \nverified pair: one chosen and one rejected response. The reward function then assigns annotations by \ncomparing the density ratio scores of these two responses. The final score reflects the accuracy of the \nreward function's predictions against human-annotated ground truth.\nOur experiment includes base models, supervised fine-tuning (SFT) models, as well as models \noptimized through different preference-tuning algorithms (mostly obtained from Meng et al. (2024)). \nThese models are respectively labeled as SFT-objective-name in Figure 5 and span techniques such as \nKTO (Ethayarajh et al., 2024), CPO (Xu et al., 2024), RRHF (Yuan et al., 2023), ORPO (Hong et al., \n2024), SLIC-HF (Zhao et al., 2023), IPO (Azar et al., 2023), RDPO (Park et al., 2024), DPO (Rafailov \net al., 2023), SimPO (Meng et al., 2024), and PPO (Ouyang et al., 2022). Among them, KTO directly \nmaximizes the utility of generation, where density ratio is used as implied reward. IPO avoids the \nassumption that pairwise preference can be replaced with point-wise reward. SLiC-HF and RRHF are \nranking losses using likelihood or average likelihood margin. ORPO optimizes the odds ratio margin \nbetween winning and losing pairs. CPO uses sequence-likelihood margin and SFT loss together. \nSimPO optimizes length-regularized avg-likelihood margin. PPO is an online RL approach that \noptimizes policy using external reward. Among all described optimization methods, only RDPO and \nDPO directly optimize the implicit reward under the Bradley Terry assumption. RRHF, SLIC-HF, \nCPO, ORPO, and SimPO do not even include a reference model in their objective, implying that their \ndensity ratio reward has no connection to DPO implicit reward theory. The diversity of these models \nenables us to generalize our findings across alignment objectives."}, {"title": "4.2 CUSTOMIZED STRONG-OVER-WEAK DENSITY RATIO", "content": "CDR (Customized Density Ratio) uses customized instructions and ICL examples to improve the \nfidelity and accuracy of vanilla strong-over-weak density ratio. We show that by making customized \ninstructions for different domain, CDR can significantly improve density ratio reward function's \noverall and domain-wise reward performance.\nSetup We select Nous-Hermes-2-Mistral-7B-DPO (NousResearch) and OpenHermes-2.5-Mistral- 7B as the strong-over-weak combination. Extensive evaluations demonstrate the superior performance of the DPO model, which creates a clear separation from the SFT model and positions this pairing among the top-performing density ratio reward functions on RewardBench.\nTo guide the density ratio toward customized domains, we designed three sets of tailored instructions aimed at improving reward accuracy in the Safety, Code/Math, and ChatHard domains as defined by RewardBench. The Safety instructions address sensitive or high-risk topics, including ethics, harmful behavior, profanity, and legal issues, to encourage safe and responsible responses. The Code/Math instructions focus on coding tasks and mathematical problem-solving, emphasizing logical reasoning, accuracy, and precision. For complex instruction-following tasks, the ChatHard prompt encourages the model to be detail-oriented and demonstrate nuanced understanding of user input. Each instruction set includes domain-specific guidelines and in-context examples (ICLs) that illustrate positive and negative examples within each domain, helping the reward function to generate more precise scores \nfor each query. To optimize domain alignment, an adaptive router based on a zero-shot prompted LLM assigns the appropriate domain instruction set to each example.\nResults The results in Table 1 show a clear benefit of employing customized Density Ratio (CDR) \napproaches across various dimensions. CDR reward function is shown to consistently outperform \ndensity ratio reward without domain-customized instructions. CDR reward optimized for safety \nachieve a Safety score of 91.0, representing a 7.6-point improvement over uninstructed density ratio \nbaselines. This highlights the benefits of safety-specific guidance in enhancing reward function's \nsafety considerations. Similarly, CDR tailored for code/math achieves a Reasoning score of 89.7, \noutperforming GPT-4-turbo and Claude-3.5-sonnet, with a substantial 15.9-point gain over baselines. \nCDR focused on chat-hard scores 69.7 in ChatHard, reflecting improved reward robustness in \nchallenging dialog contexts.\nCDR employing an oracle (idealized routing) provides insights into potential performance gains with dynamic routing. Under ideal conditions, CDR can reach an overall score of 84.9, balancing performance across safety, reasoning, and conversational robustness. Adaptive CDR employing a router (real-world routing) shows actual automated final reward performance. Note that the router employ vanilla density ratio for the general chat domain because it scores the highest in Chat, which is most important to downstream preference alignment."}, {"title": "4.3 ALIGNMENT WITH DENSITY RATIO ANNOTATED DATA", "content": "Setup We have demonstrated in previous sections that CDR can achieve strong results in out-of- \ndistribution reward benchmark. In addition, to show the usefulness of CDR in aligning LLMs, we \ndesign an on-policy preference alignment experiment similar to setup in Meng et al. (2024); Dong et al. (2024). It is intended to give head-to-head comparisons between proposed log-density ratio \nreward functions (7) with SoTA reward functions in how well they can preference align a policy. We \nuse Meta-Llama-3-8B-Instruct as the starting model and SimPO (Meng et al., 2024) as the preference \noptimization algorithm. Our evaluation methods include AlpacaEval2.0, ArenaHard, and MT-Bench, \nwith details in Appendix A.1.\nPreference Data Annotation We use input prompts D = {x^(i)}_((i=1))^(N) from the UltraFeedback \ndataset (Cui et al., 2023). On-policy alignment dataset is created by Best-of-N sampling, and \nconstructing chosen/rejected pairs using different reward functions. For each prompt x \u2208 D, we \nsample 32 model completions {y_i}_((i=1))^(32) from the starting policy. To construct positive-negative paired \npreference data, we select the preferred response y_(i^*) as the one that maximizes the reward function: \ni^* = arg max_i r(x, y_i). A dispreferred response is then randomly sampled from the remaining \nset. For all experiments, the completions {y_i}_((i=1))^(32) are pre-computed and fixed, with only the choice \nof reward function r varying, as indicated in the Reward Function column in Table 2. To address \npossible length imbalances between preferred and dispreferred responses, we apply a length threshold \nbefore randomly selecting the rejected sample. This procedure ensures variety in rejected samples, \nreduces the risk of reward hacking, and maintains a length-balanced preference dataset."}, {"title": "5 RELATED WORKS", "content": "Preference tuning A wide range of preference tuning algorithms have been proposed to align LLMs with human preferences and values (Melnyk et al., 2024; Pang et al., 2024; Ethayarajh et al., 2024; Wu et al., 2024; Hong et al., 2024; Yuan et al., 2023). The most well-known one is the \nproximal policy optimization (PPO; Schulman et al., 2017), an online RL algorithm that optimizes \npolicy to maximize the KL-constrained reward expectation of an external reward model. Direct \npreference optimization (DPO; Rafailov et al., 2023) leverages DPO implicit reward \u2013 parameterized \nas density ratio between policy model and a reference model-to circumvent the need of external \nreward function. It simultaneously optimizes the implicit reward and policy model by training on \npairwise preference data. More recently, SimPO (Meng et al., 2024) directly optimizes the average \nlog-likelihood margin between winning and losing sequences, eliminating the need for a reference \nmodel. The algorithm is inherently more efficient as it removes the memory and computation \noverhead of hosting reference policy and computing KL bound, while, at the same time, it still gives \ncompetitive alignment performance against DPO and its variants.\nDensity ratios for alignment Density ratio as rewards is popularized as implict DPO reward \n(Rafailov et al., 2023). Chen et al. (2024) uses implicit DPO reward to bootstrap an LLM through \niterative DPO training. Zhong et al. (2024) trains a DPO model and uses the density ratio to derive \na token-level characterization for response quality, and uses it as a reward signal in PPO training. \nYang et al. (2024b) uses the density ratio between DPO vs SFT model as quality filter. Though one \nstudy Lin et al. (2024) finds that implicit DPO reward struggles to generalize on OOD examples \ncompared with just training a classifier using (Bradley Terry; Bradley & Terry, 1952) objective. This \npaper shows that implicit DPO reward is only a special case in the class of strong-over-weak density \nratio reward. Our findings provide guidelines to identify suitable pairs to construct DR. In particular, \nthe denominator model must be sufficiently weak and unaligned to contrast with numerator model to \nprovide generalizable preference signal.\nDiscriminative & generative preference Trained classifiers and generative rewards are the existing approaches for preference data annotation. They top leaderboards such as RewardBench (Lambert et al., 2024) and are widely adopted by both industry and the community to preference align well- known models (Ouyang et al., 2022; Touvron et al., 2023; Adler et al., 2024; Yang et al., 2024a; \nCui et al., 2023). In fact, due to the scarcity and noise of human preference data, GPT-4 based \ngenerative rewards have long been harvested to align models, such as reinforcement learning from AI \nfeedback (RLAIF; Bai et al., 2022). High quality and popular preference datasets are actually often \ntimes annotated by LLM-as-a-judge, both in the forms of scalar score and textual assessment and \ncritiques (Cui et al., 2023). With high quality human data or LLM generated data available, one can \nfine-tune a LLM to be a better generative judge (Wang et al., 2024b; Zhang et al., 2024; Wang et al., \n2024a; Kim et al., 2024), or to tune a linear layer on top of the LLM to be a sequence classifier (Adler \net al., 2024; Dong et al., 2024; Liu & Zeng, 2024). However, such approaches either require quality \ndata for training or a powerful closed-source LLM which may be prohibitive for license concerns.\nWeak-to-strong generalization Many works have explored the idea of using a weak and a strong \nmodel to obtain better performance than the strong model. Contrastive decoding, for instance, \nenhances LLM generation quality by searching for sequences that maximizes the likelihood different \nbetween an expert model and an amateur model. O'Brien & Lewis (2023) shows CD consistently \nimproves reasoning tasks. Li et al. (2022) shows improved generation quality in wikipedia, news \nand story domains. Chuang et al. (2023) shows improvement in LLM facutuality by contrasting \nthe differences between logits in later layers and earlier layers. In addition to contrastive decoding, \nEXPO (Zheng et al., 2024) is a model extrapolation method that leverages the delta between an"}, {"title": "6 CONCLUSION", "content": "In this work, we introduce CDR, an accessible approach that leverages off-the-shelf LLMs for \npreference annotation. CDR overcomes the limitations of existing methods by eliminating the need \nfor extensive human annotation or proprietary models to obtain a high-performance reward function. \nThis advancement makes preference fine-tuning more attainable for individual researchers and small \ncompanies. Our approach uses the alignment gap between a better-aligned and an under-aligned \nmodel as a reward signal for data annotation, and enables tailored reward functions for specific \ndomains through customized instructions. We also demonstrate a complete process for applying \nCDR in preference fine-tuning: first, generating an on-policy preference dataset, and then aligning \nthe Llama-3-8B-Instruct model to competitive performance levels across extensive benchmarks.\nIn addition to presenting an effective annotation method, we conduct extensive experiments to \nvalidate our Strong-over-Weak hypothesis to guide the design of density ratio reward. Our results \nreveal a strong correlation between alignment gaps and reward function performance. It mitigates \nconcerns over density ratio reward's performance variance by showing consistently generalizable \nreward from picking weak denominator model. Finally, we show that domain-specific customization \ngives significant boost to density ratio's reward performance and alignment results using CDR. This \nwork establishes the strong-over-weak approach as a promising, training-free strategy for generating \nhigh-quality reward signals. For future research, automated instruction generation for density ratio \nreward or improved domain categorization are all promising avenues that could further boost the \nutility and appeal of density ratio reward functions."}, {"title": "B.1 DELTA IN PROMPT CONDITIONING HYPOTHESIS", "content": "Rather than leveraging difference between Strong-over-Weak models, we can potentially leverage the \ndifference between with and without prompt conditioning for the same model to induce preference \nsignal. For example, we can use prompt template to provide definition of preference, and contrast that \nwith a definition-free setup. The delta will be the gains from following the pre-conditioned preference \ndefinition.\nr_(prompt-template) (x, y) = log \u03c0(y | T(x)) \u2212 log \u03c0(y | x) (10)\nwhere T(x) is a function that applies a prompt template on x. x is input sequence and y is output \nsequence. \u03c0 should be an instruction tuned model, by before preference training, so that \u03c0(y|x) \ndoes not have inherent understanding of preference without prompt-conditioning.\nWe designed experiments that set \u03c0 either a SFT model OpenHermes-2.5-Mistral-7B or an aligned \nmodel Nous-Hermes-2-Mistral-7B-DPO. We then computed their reward based on equation 10. \nWe find that prompting only yields signal for the conditioned domain, while the other domains \nunrelated with conditioned prompt gives poor performance. For example, using the safety instruction \nin Figure 3, safety-template yields a safety score of 82.3 on RewardBench, but all other reward domains \nsuffered, only scoring between 50-58. The overall performance is far away from safety instructed \nCDR in equation 8 that not only boosts safety domain, but also maintain or even improve other \ndomains' performance after. Liu et al. (2024) also tries a similar setup in its TIS-DPO(P) setup using \nthe difference in probability between positively-prompted vs negatively-prompted sequences for \nimportance sampling. Their negative results with this setup also confirms our negative results from \nsimply using different prompt conditioning equation 10 as reward signal."}, {"title": "B.2 PAIRWISE MUTUAL INFORMATION HYPOTHESIS", "content": "We also tested other density ratio hypothesis though with limited empirical success. We still describe \nthem and show our experiment results in the Experiment section as negative findings.\nr_(pmi) (x, y) = PMI_(strong) (T(x), y) \u2212 PMI_(weak) (T(x), y)\n= log (\u03c0_(strong) (y | T(x))/\u03c0_(strong) (y)) \u2212 log (\u03c0_(Tweak) (y|T(x))/\u03c0_(Tweak) (y))\n= (log \u03c0_(strong) (y | T(x)) \u2212 log \u03c0_(Tweak)(y | T(x))) \u2212 (log \u03c0_(strong) (y) \u2212 log \u03c0_(Tweak)(y))\nHere, we define a reward function r_(pmi)(x,y) as he difference in pointwise mutual information \n(PMI) scores between the strong and weak models for the transformed input T(x). The notation \n\u03c0_(model)(y | T(x)) represents the conditional probability of y given T(x) as estimated by either the \nstrong or weak model. The method uses the difference in PMIs between a strong and weak models \nfor the transformed input T(x), based on the intuition that the strong model will be more responsive \nto the preference conditioning given in the prompt template T. This can also be viewed as a length \nnormalized form for weak-and-strong density ratio.\nWe have designed experiments testing that also uses Nous-Hermes-2-Mistral-7B-DPO as the strong \nmodel and OpenHermes-2.5-Mistral-7B as the weak model. It gives much inferior reward performance \nthan CDR in equation 8. Out of the over 20 prompts we tried, the highest attempt receives 58 on \nRewardBench compared with CDR of 82.3, demonstrating very poor reward generalization. Our \nexperiments shows negative results for the PMI reward formulation."}, {"title": "C.1 ITERATIVE DPO MODELS", "content": "The checkpoints for our experiment on density ratio reward for iterative DPO checkpoints in Figure 2 \nare off-the-shelf models released by Meng et al. (2024) and Chen et al. (2024). Details are summarized \nin the following tables."}, {"title": "C.2 MODELS TRAINED VIA DIVERSE PREFERENCE OPTIMIZATION OBJECTIVES", "content": "The checkpoints for experiment in Section 4.1 are taken from existing works (Meng et al., 2024) with details listed below."}]}