{"title": "As Confidence Aligns: Exploring the Effect of AI Confidence on Human Self-confidence in Human-Al Decision Making", "authors": ["Jingshu Li", "Yitian Yang", "Q. Vera Liao", "Junti Zhang", "Yi-Chieh Lee"], "abstract": "Complementary collaboration between humans and AI is essential for human-Al decision making. One feasible approach to achieving it involves accounting for the calibrated confidence levels of both Al and users. However, this process would likely be made more difficult by the fact that AI confidence may influence users' self-confidence and its calibration. To explore these dynamics, we conducted a randomized behavioral experiment. Our results indicate that in human-AI decision-making, users' self-confidence aligns with Al confidence and such alignment can persist even after AI ceases to be involved. This alignment then affects users' self-confidence calibration. We also found the presence of real-time correctness feedback of decisions reduced the degree of alignment. These findings suggest that users' self-confidence is not independent of AI confidence, which practitioners aiming to achieve better human-Al collaboration need to be aware of. We call for research focusing on the alignment of human cognition and behavior with \u0391\u0399.", "sections": [{"title": "1 INTRODUCTION", "content": "As artificial intelligence (AI) is increasingly integrated into a range of human decision making processes, research on and support for human-Al decision making is gaining prominence within the human-computer interaction (HCI) and AI communities [40, 46, 66]. When collaborating with humans in decision making processes, AI can serve as an advisor, a peer collaborator, or even as a decision-maker [32, 36, 39, 63, 70]. A key goal of human-Al decision making is for human-Al teams to achieve complementary collaboration, i.e., joint activity that leads to better outcomes than efforts by either party working alone would achieve [32, 40, 46]. One proposed approach to achieving such complementarity is to shift the primary burden of decision making based on team members' relative levels of uncertainty about their preliminary decisions. For example, a human could delegate the final decision to AI if their own uncertainty is higher; or, an external algorithm could optimize the final decision by weighing each team member's uncertainty [43, 46, 47, 60, 73]. However, a necessary precondition for such optimization is each team member's ability to accurately estimate and articulate their decision uncertainty [25, 47].\nWhile uncertainty expression of AI can take various forms depending on the model, in this work, we focus on the most common form in human-AI decision making studies and practices-confidence level, which is how uncertainty is expressed in classification model (e.g., the model is 80% confident about the predicted label) [25, 40, 73]. This is similar to how humans often express their uncertainty: by assigning a probability of how much they expect their prediction or answer to be correct, i.e. their self-confidence [47, 56, 71].\nStudies among human decision making groups have observed that the self-confidence of individual members aligns with that of their peers (converging towards a mean value) and then remains aligned in individual decision making after group decision making has ceased [5, 22, 57]. This phenomenon is known as confidence alignment. This points to the potential for AI users' self-confidence to be influenced by Al's expressed uncertainty, and even to align with Al confidence. However, previous work theorizing human-Al complementarity by uncertainty and studies exploring human self-confidence dynamics in human-AI decision making have not explored such the possible influence of AI confidence on human self-confidence [17, 40, 43, 47, 73]."}, {"title": "2 RELATED WORK", "content": "Human-Al decision making includes many different paradigms where Al plays various roles within the human-Al group [32, 36, 39, 63, 70]. This study considers three such paradigms that either have substantial potential for application or are in wide use already (as shown in Fig. 1).\nCurrently, the most widely used such paradigm in HCI and AI research as well as in practical applications involves the AI acting as an advisor. It is generally referred to as Al-assisted decision making [6, 16, 32, 44-46]. In this setup, AI provides humans with information and suggestions, while humans consider Al's recommendations and make the final decisions [55, 73]. For example, in investing, Al provides investors with its investment advice, and investors make decisions by integrating AI suggestions with their understanding of the situation [7]. This paradigm of AI-assisted decision making is anticipated to enhance human capabilities and amplify human intelligence [42, 55]. The motivation for adopting this paradigm is primarily due to considerations of risk, fairness, ethics, and accountability [73]: As probabilistic models, AI systems cannot guarantee the correctness of specific decisions, and the risks associated with that uncertainty are seen as particularly critical in high-stakes decision domains like finance, healthcare, and law [6].\nAlongside the advancement of AI, researchers and designers have begun to explore richer forms of human-AI collaboration [48], and a mixed-initiative paradigm of human-AI decision making is increasingly being researched with [32, 55, 58]. In it, AI is treated as a peer collaborator of equal status to the human decision-maker(s): and their joint decisions are arrived at collectively through various aggregation mechanisms (e.g., choosing the decision with the highest confidence level) [55, 58]. For instance, in stock selections, both humans and AI can provide suggestions for stock prices, and the final decision depends on the degree of agreement between human and AI recommendations [58].\nLastly, following a comprehensive assessment of risks and benefits, some researchers and practitioners have begun to explore a high-automation paradigm in which decision making is automated by AI, with humans acting as supervisors [50, 53, 70]. That is, AI"}, {"title": "2.2 Al Confidence, Human Self-confidence, and Complementary Collaboration in Human-Al Decision Making", "content": "The uncertainty estimation of AI can be expressed in various forms, such as confidence levels in classification models and confidence intervals in regression models [24, 40]. In this paper, we focus on AI confidence levels, the most common form of Al uncertainty estimation in past research on human-AI decision making [3, 6, 12, 40, 41]. AI based on classification models can output the conditional probability of a single prediction as its confidence level, reflecting the probability that the prediction is correct [25, 73]. For example, if the Al reports an 80% confidence in a predicted label, this implies that it estimates an 0.8 probability that the prediction is correct. As such, Al confidence levels can serve as case-wise uncertainty indicators, helping humans and external decision making algorithms know when a model is more or less sure, which is different from Al's accuracy that provide overall performance information over a set of decisions [60]. Humans or external algorithms can then adjust their trust in the AI to appropriate levels and rely on it accordingly based on the level of AI confidence [60, 73]. For instance, in some medical and financial decision making practices, AI is required to report its confidence levels alongside its predictions to help humans capture AI uncertainty [14, 34, 59, 67].\nSimilar to AI confidence level, humans can estimate their uncertainty through their self-confidence levels [71]. This estimation of uncertainty can be expressed in natural language, such as \"I am not sure,\" or numerically as \"I am 70% confident\" [5, 22, 47]. Corresponding to the Al confidence level, in this study, participants will express their self-confidence numerically. Prior research suggests that human self-confidence in decision making is related to metacognition [35, 71]. Metacognition refers to individuals' assessment of their own abilities, knowledge, and understanding of task-relevant factors, and their self-confidence can be seen as an outcome of that self-monitoring process [33, 35]. During human-AI decision making process, people's self-confidence level can be influenced by task-relevant factors such as task difficulty and correctness feedback [17, 54, 57]. Chong et al. [17] found that, in AI-assisted decision making without providing AI confidence, for each task, positive feedback after the task can increase individuals' self-confidence, while negative feedback can diminish it. They also showed that a deterioration in AI accuracy led to reduced human self-confidence, because poorer Al performance increased joint decision errors, then negative feedback lowered human self-confidence [17]. Human self-confidence level plays an important role in human-AI decision making [5, 17, 47]. On one hand, it governs humans' acceptance or rejection of AI predictions: individuals are more likely to accept Al predictions when their self-confidence is low [17]. On the other hand, it affects both humans' self-reliance and external algorithms' reliance on humans.\nComplementary collaboration can be realized by accounting for the confidence levels of both humans and AI [43, 46, 47, 60, 73]. Under ideal conditions, the optimization of complementary collaboration entails the final decision makers (either humans or algorithms) having appropriate reliance on human and AI decisions according to their confidence levels-using AI predictions for final decisions when Al's confidence level is higher than that of the human, and relying on human judgment when the human's confidence level exceeds that of AI [43, 60]. Achieving such optimization requires calibrated confidence levels of humans and AI [24, 40].\nHowever, both Al confidence and human self-confidence are facing the miscalibration problem [25, 57]. On the AI side, the confidence calibration of machine learning models is challenging, with many models being overconfident-having confidence levels that exceed their actual accuracy-while others may be underconfident [25, 65, 69]. Past research has indicated that these problems are linked to model capability and regularization [25]. On the human side, calibrating self-confidence is also challenging because it is influenced not only by the difficulty of the decision tasks themselves, but also by various socio-economic factors such as gender, occupation, psychological health, and so on [11, 13, 31, 49, 57]. Lack of calibration of either party's confidence level can undermine the efficacy of complementary collaboration between humans and AI. Thus, some recent work has focused on calibrating AI confidence as well as human self-confidence to remedy the miscalibration problems [25, 47]. For instance, Ma et al. [47] have explored how cognitive interventions, rewards, and real-time feedback can help"}, {"title": "2.3 Confidence Alignment in Decision Making", "content": "Previous research has shown that in decision making processes involving groups of two or more humans, the members' respective levels of self-confidence tend to come into alignment and approach uniformity [5, 22, 57]. For instance, in an experiment on dyadic cooperative decision making where dyad members can communicate their decisions and confidence with each other, by encoding the expression of confidence in speech, researchers found that the levels of verbally expressed confidence among dyads tended to converge [22]. Most dyads in the experiment also converged to the same set of functional expressions for their confidence [22]. Similarly, Bang et al. [5] also observed that in cooperative psychophysical decision making tasks where confidence was digitally reported, the average confidence levels of group members were more similar when performing tasks cooperatively than when they performed them individually; and this phenomenon occurred irrespective of interpersonal differences in baseline accuracy. Recent research has further indicated that in perceptual decision tasks where participants are not required to make cooperative decisions but can see each other's decisions and confidence levels, dyads' digitally expressed confidence also display alignment, and the influence of such alignment can persist in individual decision tasks after their interaction [57].\nThe confidence alignment phenomenon is thought to be a result of humans imitating the confidence of their peers [5]. Imitation can be defined as \"action that copies the action of another more or less exactly, with or without intent to copy\" [20], encompassing concepts like behavioral contagion, conformity, social pressure, and social facilitation [68]. Individual behaviors can spread and be imitated from one person to another through observation and interaction [68, 72]. Furthermore, neurological evidence suggests that people can imitate others' risk preferences by observation [62]. Through imitating the behaviors of others, individuals can expand their perceptual and cognitive capabilities at minimal cost [8, 18]. In cooperative decision making contexts specifically, previous studies suggest the confidence alignment is the result of unconscious behavioral imitation in common situations and the intended imitation aimed at adapting to each other's confidence [5, 22, 57]."}, {"title": "3 RESEARCH QUESTIONS", "content": "Inspired by research on confidence alignment among humans, we propose that in human-AI decision making, human decision-makers' self-confidence may also be influenced by, and align with, Al confidence. This idea is supported by a separate body of prior research findings that humans can imitate the behaviors and viewpoints of AI and robots [9, 28, 29, 36, 64]. The Computers are Social Actors (CASA) theory provides an explanation for these phenomena [51, 52]. This theory posits that people naturally and unconsciously use social and interpersonal heuristics to interact with computers [51, 52], and humans' imitation can also be seen as a type of social and interpersonal heuristic, automatically triggered by certain social cues from the computer [28, 29, 64].\nPrevious studies of human-AI decision making have used AI confidence levels to help humans understand Al uncertainties and effectively calibrated AI confidence and human self-confidence to promote complementary collaboration [17, 40, 43, 47, 60, 73]. \u03a4\u03bf date, however, our understanding of the influence of AI confidence on human self-confidence remains limited. Investigating this issue could help researchers and developers better comprehend the dynamics of human self-confidence in human-AI decision making, and thereby facilitate future efforts to optimize complementary collaboration. Therefore, we ask:\n\u2022 RQ1: What is the effect of AI confidence on human self-confidence. Do they align, and if so, to what degree?\nMeanwhile, referring to the results among humans [5], in different human-AI decision making paradigms [36, 40, 63, 70], the differences in the modes of collaboration between humans and AI may alter the effect of AI confidence on human self-confidence. Importantly, the real-time correctness feedback can influence human self-confidence [17, 47, 54]. In past research about confidence alignment between humans, the real-time feedback are always provided [5, 22, 57]. However, the fact is that, in practice, not all decision tasks in human-Al decision making practice involve real-time feedback [56]. Thus, to boost our findings' generalizability and to thoroughly explore these different situations, we ask the following two sub-questions:\n\u2022 RQ1.1: How do different human-AI decision making paradigms influence confidence alignment?\n\u2022 RQ1.2: How does the presence of real-time feedback influence confidence alignment?\nWithout changing human capabilities-more specifically, decision making accuracy-any change in human self-confidence will inevitably affect the correspondence between self-confidence and accuracy, thus altering human self-confidence calibration. This can lead to a series of adverse consequences, such as inappropriate reliance on humans or AI, and damage to the efficacy (accuracy) of human-Al decision making [17, 47]. To date, the potential effects of self-confidence alignment on self-confidence calibration and its consequences remain to be explored. Investigating this issue can help researchers understand and avoid the calibration problems caused by human self-confidence aligning with Al confidence, promoting complementary collaboration in human-AI decision making. Therefore, we propose the second research question.\n\u2022 RQ2: How does the alignment of human self-confidence with Al affect human self-confidence calibration? What are the consequences?\nAdditionally, in AI-assisted decision making paradigms, human decision-makers not only provide their initial decisions at the start of each task but also act as the final decision-makers [40, 73]. In this process, human confidence in the final decisions, i.e., the joint human-Al decision, is of significant referential value. Exploring the influence of AI confidence on human confidence in final decisions and potential alignment can help researchers further understand the influence of AI confidence on human confidence. Therefore, this study proposes the third research question:"}, {"title": "4 METHOD", "content": "To investigate the effects of AI confidence on human self-confidence, we designed and conducted an online randomized behavioral experiment. Our study was approved by the Department Ethical Review Committee of School of Computing, National University of Singapore."}, {"title": "4.1 Participants", "content": "Participants were recruited on the Connect crowdsourcing platform (CloudResearch \u00b9) who met the following criteria: (1) residing in the USA, (2) aged between 21 to 60 years (as required by the IRB), and (3) able to participate via a personally owned computing device, with gender balance maintained during recruitment. They were not allowed to take part in the experiment more than once. After excluding those who did not complete the task (either voluntarily returned or dropped out for unknown reasons) and those who failed multiple attention checks, a total of 270 unique participants were involved in our study (45 per condition). According to demographic data provided by the Connect platform, among these participants, 51.1% were female, the average age was 36.6 (SD=9.4), and 62.2% had at least an bachelor degree. The basic pay for the experiment was $6, with an expected duration of 30 minutes. To encourage high-quality performance, inspired by previous research [47, 73], an additional reward of $2 was offered for each stage where participants achieved an accuracy rate exceeding 90%, amounting to a maximum bonus of $6 over three stages."}, {"title": "4.2 Experiment Task and AI Model", "content": "In this study, income prediction was employed as the task for human-Al decision making. Participants were required to predict whether an individual's annual income would exceed $50,000 (roughly the medium income at the time of data collection) based on demographic and employment information. The task utilized data from the Adult Income dataset of the UCI Machine Learning Repository [37], which contains 48,842 instances. Each instance is described by 14 attributes, including age, occupation, gender, among other demographic information. The annual income for each instance is binarized, indicating whether it exceeds $50,000. This task has been used in several previous studies on human-AI decision making [15, 23, 27, 46, 73]. It requires low domain-specific expertise, making it suitable for online randomized experiments with participants who have undergone necessary training [23, 46]. To ensure reasonable task difficulty and cognitive load, following the setup of previous research [47, 73], we selected 8 out of the 14 attributes to present to participants as decision references (based on the importance of attributes), including age, years of education, work class, occupation, marital status, gender, race, and hours worked per week.\nAccording to previous work [60], we trained a machine-learning model based on Random Forest [10] using samples from the dataset for the human-AI decision making task. After data preprocessing,"}, {"title": "4.3 Procedure and Conditions", "content": "Our experimental flowchart is presented in Fig. 2, which was informed by past studies of confidence alignment [7, 22, 57]. There was a tutorial and three income prediction task stages (comprising 120 different questions in total). In the three task stages, the income prediction tasks were the same to each participant and the order of these tasks was randomized within each stage. In line with the research questions of this paper, the experiment followed a mixed design, with one within-subject factor (3 task stages) and two between-subject factors (presence of real-time feedback and human-Al decision making paradigms). For the within-subject factor, the first task stage was to measure participants' baseline self-confidence, the second task stage was to measure participants' self-confidence and other relevant variables during human-AI decision making, and the third task stage was to measure participants' self-confidence in individual tasks after human-AI decision making. For the between-subject factors, there was a 2\u00d73 design: presence of real-time feedback (with real-time feedback, without real-time feedback) \u00d7 human-AI decision making paradigms (AI as advisor, AI as peer collaborator, AI as decision maker under human supervision). Below, we describe in detail the procedures and conditions of the experiment.\nSpecifically, after participants signed the informed consent form, they were directed to a tutorial interface. In tutorial, we explained in writing the objectives of the prediction task and how to use the decision making interface and collaborate with AI. We described each attribute in the profile dataset and demonstrated the binary income distribution corresponding to each attribute. Participants were informed during this stage that achieving an accuracy rate above 90% in any stage would result in a $2 reward (cumulative). Before advancing to the next phase, participants were required to correctly complete a fact-check question about the tutorial to ensure they had learned and understood the material.\nParticipants then entered the first task stage, i.e. independent task stage (stage 1), where they independently completed 40 income prediction questions. Following past research, participants were required to report their prediction and corresponding self-confidence using a slider [57]. As illustrated, the slider's midpoint served as the boundary: sliding left indicated a prediction of annual income less than $50,000, with confidence increasing as the slider moved further left (minimum 51%, maximum 100%), and vice versa for sliding right. This stage aimed to measure the participants' baseline self-confidence in the income prediction task. Under the conditions with real-time feedback, participants received immediate accuracy feedback after each question (indicating the correctness of the prediction); without real-time feedback, no accuracy feedback was provided.\nSubsequently, participants entered the second task stage, i.e. the collaboration task stage (stage 2), where they were required to complete 40 income prediction questions with the AI system. The 40 questions in this stage were manually selected from the test set and different from the questions in stage 1, such that the decision making AI achieved an accuracy of 80% (32 out of 40 are correct) and an average confidence of 80.40% (SD=9.64%). The decision making AI could be regarded as close to well-calibrated over these 40 questions. The specific procedures for this stage, according to the conditions of human-AI decision paradigms in our experiment, are as follows:\n\u2022 AI as advisor: Under this paradigm, our study followed the general paradigms of AI-assisted decision making from past research [73]. For each question, human participants first needed to use a slider to indicate their initial prediction and self-confidence after viewing the question information. Then, the Al's prediction and confidence were displayed to the participants. Finally, participants reported their final decision and confidence in the final decision via the slider, taking into account Al's advice, their initial response, and the question information. Note that in this paradigm, participants reported their confidence level twice for each task. The first confidence report was regarded as the participants' self-confidence in their own decision, and the second confidence report was regarded as their confidence in the joint human-Al decision. In condition with real-time feedback, participants received feedback on the accuracy of their initial decision, Al's decision, and final decision after each question.\n\u2022 AI as peer collaborator: In this paradigm, for each question, participants were first asked to indicate their prediction and self-confidence using a slider after viewing the question information. Then, the Al's prediction and confidence were shown to the participants. Finally, the study adopted the highest confidence rule as the method for aggregating human and AI decisions [4, 38], selecting the decision of the individual with higher confidence (random selection was used for ties) as the final decision, which was then displayed to the participants. In condition with real-time feedback, participants received feedback on the accuracy of their own decision, Al's decision, and the final decision after each question.\n\u2022 AI as decision maker under human supervision: Under this paradigm, for each question, participants acted as a supervisor, observing Al's prediction and confidence. For each task, participants were first required to use a slider to indicate their prediction and self-confidence after viewing the question information (it was not a part of decision making under this paradigm, while the purpose was to measure their self-confidence). Subsequently, the Al's prediction and confidence were displayed to the participants. The final decision would automatically use the Al's prediction, and participants would not make the final decision. Note that this setup reflects the common case in this paradigm, where humans only observe Al making decisions. It does not include cases where humans intervene due to anomalies which do not occur frequently. Because under this paradigm, the decision making process when humans do intervene is essentially the same as when Al acts as an advisor. This design was inspired by observation tasks from past research [7, 57]. In condition with real-time feedback, participants received feedback on the accuracy of their own decision and AI's decision after each question."}, {"title": "4.4 Experimental Interface", "content": "As shown in Fig. 3, the online experimental interface of this study was implemented using the JavaScript framework Vue.js. The system captured participant decisions and self-confidence levels, and utilized MySQL for backend storage management. Additionally, this study employed the Qualtrics 2 online survey platform for obtaining informed consent and delivering tutorials."}, {"title": "4.5 Measurement", "content": "Participants' Self-confidence. The self-confidence of participants at each stage was represented by the average across the self-confidence levels they report for their own decisions within that stage.\nHuman Confidence Alignment. Following previous experiments, for each stage, we utilized the absolute difference in mean confidence between AI (CAI = 80.40%) and participants $C_{Stage}$ in that stage, expressed as $|C_{Stage\\ Human} - CAI|$ to measure the degree of confidence alignment [5, 57]. For short, it was called absolute confidence difference. A smaller value indicates a higher degree of confidence alignment.\nSelf-confidence Calibration. As shown in Equation 1, following previous research [25, 47], we used the expected calibration error (ECE) as a measure of participants' self-confidence calibration. Initially, we divided the domain of self-confidence into M bins of equal width (in this paper, M = 4). The confidence levels reported by participants for N predictions were divided into these M bins. For each bin $B_m$, the average confidence $conf(B_m)$ and accuracy $acc(B_m)$ were calculated, and their difference was taken as the absolute value. Finally, the absolute differences in each bin were averaged, weighted by the number of predictions in corresponding bin $B_m$, to calculate the ECE. A smaller ECE value indicates better calibration of self-confidence.\n$ECE = \\sum_{m=1}^{M} \\frac{|B_m|}{N} |acc(B_m) - conf(B_m)|$                               (1)\nInappropriate Reliance and Human-AI Decision Making Efficacy. To explore the effects of the change of participant self-confidence calibration, we measured the inappropriate reliance behaviors on AI when it served as an advisor, the inappropriate reliance of decision making mechanism on participants and AI when Al acted as peer collaborator, and the efficacy of human-AI decision making across both paradigms. As the decision making is entirely conducted by AI under human supervision, with a constant accuracy of 80%, we do not consider the consequences of changes in human self-confidence calibration in this paradigm.\nInspired by previous research [43, 47], the measurement of inappropriate reliance behaviors on Al when it served as advisor includes the over-reliance, which is the percentage of tasks where the participants' first decision is correct, Al's prediction is incorrect, yet the final decision is incorrect; and the under-reliance, which is the percentage of tasks where the participants' first decision is incorrect, Al's prediction is correct, yet the final decision is incorrect.\nWhen Al acts as a peer collaborator, the measurement of inappropriate reliance of decision making mechanism includes over-reliance on Al, which is the percentage of tasks where the participants' decision is correct, Al's prediction is incorrect, yet the final decision is incorrect; and over-reliance on human, which is the percentage of"}, {"title": "5 RESULTS", "content": "In this section, we sequentially report our findings to address RQ1-RQ3. The participants achieved an average accuracy of 63.85% (SD = 5.32%) across 120 tasks without Al's assistance."}, {"title": "5.1 Participants' Self-confidence Aligned with Al Confidence (RQ1)", "content": "Repeated measures ANOVA was employed to see the effect of Al confidence on participants' self-confidence and the effects of human-Al decision making paradigms and presence of real-time feedback on this process. In the repeated measures ANOVA, three distinct task stages were treated as a repeated measures factor (within-subject factor), with human-AI decision making paradigms and the presence of real-time feedback as between-subject factors. The absolute difference between participants' self-confidence and Al confidence served as the dependent variable."}, {"title": "5.1.1 The Exclusion of Irrelevant Causes.", "content": "Further analysis was conducted using the same factors, with participants' accuracy as the dependent variable in a Repeated Measures ANOVA, which revealed no significant within-subject effects (F(2,528) = 0.135, p = 0.874, \u03b7\u00b2 = 2.375 \u00d7 10-4). These findings indicate that participants' accuracy did not significantly vary across task stages, preliminarily ruling out the influence of a learning effect.\nLinear regression analysis showed that no significant linear correlations were found between participants' accuracy and the absolute confidence difference at stage 2 (r = -0.071, p = 0.242) or stage 3 (r = -0.058, p = 0.339). The non-significant linear correlation between accuracy and absolute confidence difference at stages 2 and 3 suggests that the alignment of participants' self-confidence"}, {"title": "5.2 The Alignment Changed Participants' Self-confidence Calibration and Affected Human-AI Decision Making Efficacy (RQ2)", "content": "Theoretically, when participants' accuracy remained constant while their self-confidence changed, their self-confidence calibration could also change. As shown in Fig. 7, for participants who are overconfident but less confident than AI (type B), and those who are underconfident but more confident than AI (type C), a higher degree of alignment of their self-confidence with Al confidence can result in a greater mismatch between their self-confidence and their accuracy, thereby worsening self-confidence calibration. For participants who are overconfident and more confident than AI (type A), aligning their self-confidence with AI confidence can increase the degree of correspondence between their self-confidence and their accuracy, thus improving their self-confidence calibration. For participants who are underconfident and less confident than AI (type D), aligning their self-confidence with AI confidence initially improves self-confidence calibration. As the degree of alignment increases, the improvement diminishes and eventually transitions into a worsening effect.\nIn this experiment (as shown in Fig. 8 (a)), at stage 2, there were 25 type A participants, 201 type B participants, 0 type C participants, and 44 type D participants. Partial correlation results, controlling for participants' accuracy, indicated a significant positive linear relationship between ECE and absolute confidence difference for type A participants (r = 0.985, p < 0.001), a significant negative linear relationship for type B participants (r = -0.797, p < 0.001), and a significant positive linear relationship for type D participants (r = 0.313, p = 0.041).\nAt stage 3, there were 29 type A participants (as shown in Fig. 8 (b)), 194 type B participants, 0 type C participants, and 47 type D participants. Partial correlation results, again controlling for accuracy, revealed a significant positive linear relationship between ECE and absolute confidence difference for type A participants (r = 0.952, p < 0.001), a significant negative linear relationship for type B participants (r = -0.722, p < 0.001), and a significant positive linear relationship for type D participants (r = 0.379, p = 0.009).\nThe experimental results and analyses are consistent, indicating that for participants who were overconfident but less confident than AI, aligning their self-confidence with Al confidence degrades their self-confidence calibration. For participants who were underconfident and less confident than Al or overconfident and more confident than AI, the alignment with Al confidence could improve their self-confidence calibration.\nLinear regression results showed a significant positive correlation between participants' accuracy and self-confidence in stage 1 (r = 0.172,p = 0.005). However, no significant linear correlation were found between participants' accuracy and self-confidence in stage 2 (r = 0.058,p = 0.342) and stage 3 (r = 0.025,p = 0.688). Comparing to stage 1, the correlation between participants' self-confidence and accuracy is disrupted in stages 2 and 3, indicating poorer overall self-confidence calibration for participants.\nFurthermore, when Al served as advisor, in stage 2, linear correlation analysis showed that participants' ECE is positively correlated with their under-reliance percentage on AI (r = 0.335, p = 0.001), and negatively correlated with their over-reliance percentage on AI (r = -0.234, p = 0.027), as well as negatively correlated with the final accuracy of human-AI decision making (r = -0.215, p = 0.042). When Al acted as peer collaborator, in stage 2, the results indicated that participants' ECE is positively correlated with the over-reliance percentage of decision making mechanism on AI (r = 0.333, p = 0.001), positively correlated with the over-reliance percentage of decision making mechanism on the participant (r = 0.573, p < 0.001), and negatively correlated with the final accuracy of human-AI decision making (r = -0.425, p < 0.001).\nThese results indicate that poor participants' self-confidence calibration impairs participants' appropriate reliance on AI and the decision making mechanism's appropriate reliance on both participants and AI, and it can also diminish the efficacy of human-Al decision making."}, {"title": "5.3 Participants' Confidence in Joint Human-Al Decision Also Aligned with AI Confidence (RQ3)", "content": "When AI served as an advisor, the results of a repeated measures ANOVA (F(1,88) = 49.234, p < 0.001, \u03b7\u00b2 = 0.229), with the presence of real-time feedback as a between-subject factor, revealed that the absolute difference between participant confidence in the final decision and AI confidence (M = 2.975, SD = 3.838) was significantly lower than the absolute difference between participant self-confidence in their first decisions and AI confidence (M = 7.936, SD = 5.211). The main between-subject effect (F(1,88) = 1.017, p = 0.316, \u03b7\u00b2 = 0.004) of the presence of real-time feedback and its interaction effect (F(1,88) = 3.150, p = 0.079, \u03b7\u00b2 = 0.015) were not significant. These results suggest that under the AI-assisted decision making, the alignment of participants' confidence in joint final decisions with Al confidence is higher than the alignment of their"}]}