{"title": "Adversarial Magnification to Deceive Deepfake Detection through Super Resolution", "authors": ["Davide Alessandro Coccomini", "Roberto Caldelli", "Giuseppe Amato", "Fabrizio Falchi", "Claudio Gennaro"], "abstract": "Deepfake technology is rapidly advancing, posing significant challenges to the detection of manipulated media content. Parallel to that, some adversarial attack techniques have been developed to fool the deepfake detectors and make deepfakes even more difficult to be detected. This paper explores the application of super resolution techniques as a possible adversarial attack in deepfake detection. Through our experiments, we demonstrate that minimal changes made by these methods in the visual appearance of images can have a profound impact on the performance of deepfake detection systems. We propose a novel attack using super resolution as a quick, black-box and effective method to camouflage fake images and/or generate false alarms on pristine images. Our results indicate that the usage of super resolution can significantly impair the accuracy of deepfake detectors, thereby highlighting the vulnerability of such systems to adversarial attacks.", "sections": [{"title": "1 Introduction", "content": "Manipulating content to spread misinformation and damage the reputation of people has never been easier than nowadays. We are witnessing the unstoppable evolution of those known as Deepfakes. These are counterfeit media contents which often show people saying or doing things they never actually said or did, distorting reality. Distinguishing pristine contents from manipulated ones is extremely difficult. For this reason, various deepfake detectors have been developed. These are, however, subject to various issues such as the need to be up-to-date to keep up with the latest deepfake generation methods or the ability to handle real-world situations. It is precisely in real-world contexts that deepfake detection systems could be faced with targeted attacks made to deceive them. Known as adversarial attacks, these are techniques that introduce noise or adversarial patches, specifically crafted to deceive the detector. Although they can also be very effective, these techniques may require deep knowledge of the deepfake detector they are trying to fool. In this paper, we attempt to exploit a Super Resolution (SR) technique, to camouflage deepfake images in a quick and black-box manner (in the sense that the attack is model-agnostic). Our approach allows us to cause a significant increase in the False Negative Rate (fake samples classified as pristine) of up to 18%. We also have shown how the usage of SR on pristine images can cause a drastic increase in false alarms of up to 14%, highlighting the inadequacy of some deepfake detectors, which will probably arise as these techniques continue to proliferate."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Deepfake Generation and Detection", "content": "The generation of deepfakes involves the use of techniques that manipulate human faces to achieve realistic alterations in appearance or identity. Two primary approaches are commonly employed: Variational AutoEncoders (VAEs) and Generative Adversarial Networks (GANs). VAE-based methods utilize encoder-decoder pairs to decompose and recompose distinct faces. On the other hand, GAN-based methods use a discriminator to distinguish real and fake images, paired with a generator that creates fake faces to fool the discriminator. Notable Deepfake generation methods include Face2Face[21] and FaceSwap [3]. As deepfakes becomes more credible, there is a growing demand for systems capable of detecting them. To address this problem various deepfake detectors have been developed. Some methods are capable of analyzing deepfake videos by considering also the temporal information[10,6,24,7] but most approaches focus on frame-based classification, evaluating each video frame individually [9,2] and being available to manage also simply still deepfake images. Also, competitions such as [12] and [13] have been organized to stimulate the resolution of this task. The problem of deepfakes has also been extended to the detection of synthetic images in general such in [8,11,4] increasing the variety of fake contents."}, {"title": "2.2 Adversarial Attacks", "content": "Adversarial attacks, such as noise addition and adversarial patches, exploit vulnerabilities in deepfake detectors to deceive them. Adversarial noise introduces subtle perturbations, while adversarial patches overlap patterns to trigger misclassification. The authors of [15] propose a framework called FakeRetouch, which aims to reduce artifacts in deepfake images without sacrificing image quality. By adding noise and using deep image filtering, they achieve high fidelity to the original deepfake images reducing the accuracy of deepfake detectors. In [14] the authors propose a statistical consistency attack (StatAttack) against deepfake detectors by minimizing the statistical differences between natural and deepfake images through the addition of statistical-sensitive degradations."}, {"title": "2.3 Super Resolution", "content": "Super Resolution (SR) is a technique which aims to reconstruct a high-resolution version of a low-resolution image by utilizing information from multiple input images[5] or by using prior knowledge about the relationship between high-resolution and low-resolution image pairs[23,16]. One of the main SR techniques is the one proposed in [18] where an Enhanced Deep Super Resolution network (EDSR) is presented; it introduces some improvements to the ResNet architecture for SR previously proposed in [16]. They remove batch normalization layers to increase flexibility and reduce memory usage. They also propose the use of residual scaling layers to stabilize the training procedure. The model constructed with these modifications, and pre-trained with a lower upscaling factor was able to achieve good results in terms of convergence speed and final performance."}, {"title": "3 The proposed attack", "content": "The proposed attack consists of exploiting SR techniques to modify a deepfake image and camouflage it in the eyes of a deepfake detector. The scope of the attack is then to mislead the deepfake detector and make the false negative rate increase. The SR process, in an attempt to improve the resolution of an image, could smooth the artifacts introduced by some deepfake generation techniques, thus undermining the learning performed by the deepfake detection model. Specifically, for each of the frames of a video (or for each image if the attack is applied to a still image) to be analyzed, a pretrained face detector (e.g., MTCNN[22]) is applied. This step has been added to the pipeline for two main reasons. The first motivation is related to the scope of the attack itself since an attacker wants to manipulate a minimal part of the image in order to avoid adding artifacts when not needed. Applying the SR on the whole frame may add artifacts on the background finishing to have the inverse effect. The second reason behind the usage of a face detector is the common practice of both deepfake detectors and generators to focus only on the face and so it is very likely that the deepfake detector against which the attack is applied will only focus on the face and that the artifacts to be removed are concentrated on the face. The face extracted from the network has a specific resolution which is dependent on factors such as video resolution, distance from the camera, etc. Since the goal of SR is to raise the resolution of the image by a factor $K \\in \\mathbb{N}$, the image is firstly down-scaled by a factor 1/K and then given as input to an SR model (e.g. EDSR[18]) to be SR up-scaled by a factor K. The face image resulting from this process has the same size as the original detected one and so can be again put inside the source image from which it has been detected. So, to apply this method there is no need to know anything about the deepfake detector that will be used for the final detection, then the proposed method can be effectively considered a black-box attack and can be applied against any deepfake detector and on images manipulated with any deepfake generation method. Furthermore, this attack can also be carried out on deepfake content already generated and does not need to be integrated into the deepfake creation procedure."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "Since we want to evaluate our attack on a variety of deepfake generation methods, we chose the well-known FaceForensics++ (FF++)[19] dataset for our experiments. The dataset consists of both pristine and manipulated videos created using various deepfake generation methods, namely Deepfakes[1], Face2Face[21], FaceShifter [17], FaceSwap[3], and NeuralTextures[20]. However, as this dataset consists of videos and the proposed attack exploits single-image SR, ten frames were randomly extracted for each of them on which face detection was then carried out. A training set and a test set were created for each deepfake generation method in FF++. Each training set consists of 14400 images, half of which were manipulated with one of the available methods. Each test set consists of 2800 images, half of which are manipulated again with the proposed attack. A total of five training and test sets are therefore available and all of them are perfectly balanced between the two classes (pristine and fake). To choose which videos should be used for training or test set we used the split made available in [19]."}, {"title": "4.2 Experimental Setup", "content": "To investigate the impact of the application of SR on the performance of deepfake detectors, we selected three architectures, namely Resnet50, Swin-Small and XceptionNet, and trained them on faces extracted from FF++ to perform a binary classification by pristine/fake image. For each training, the model only sees pristine images and fake ones manipulated with one of the available FF++ methods (SR is not applied). All models are pretrained on ImageNet and were fine-tuned with a learning rate of 0.01 for 30 epochs on an Nvidia Tesla T4. The test is carried out considering two different setups, in the first the models are tested by applying the SR-attack on both fake and pristine images. In the second, the pristine images are un-attacked and only the fake ones are passed through the SR process. The face is always extracted from each frame using a pretrained MTCNN[22]. The scale factor used in our experiments is K = 2 and so the extracted face is resized of a factor 1/K and then up-scaled through EDSR[18] restoring the original resolution. After this process, the face can be re-pasted to the frame exploiting the coordinates extracted during face detection."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Impact of Super Resolution on Deepfake Detection", "content": "Table 1 shows the results obtained from the deep learning models considered to perform the deepfake detection task on the FF++ test set, with and without the usage of SR on both fake and pristine images. Observing the accuracy results on all the methods considered, the application of SR leads to a relevant drop in performance in all cases, confirming the hypothesis that the SR process can generate confusion in the deepfake detectors, thereby leading them to make errors. More in detail, looking at the False Negative Rate (FNR) and False Positive Rate (FPR) all the models seem to have a peak when the SR attack is applied. When the deepfake generation method used on the image is Deepfakes or NeuralTextures, the impact on the FNR is less evident but the same detector that results in more robust on the fake images, fails on the pristine images attacked with SR and we see a huge increase in the FPR. The situation is exactly the opposite for the methods Face2Face, FaceSwap and FaceShifter on which the models seem to be more sensible on the fake images attacked with SR and so have an important increase on FNR while a slight swing in FPR is registered. Increasing the FNR is the main interest for an attacker as it can be useful to be able to camouflage fake images against an automatic system that may be trying to filter them out. Vice versa, the increase in the FPR in some cases, highlights a serious problem in deepfake detection systems that, if SR became more widespread (e.g. on social media to improve the final visual quality), would end up confusing legitimate images for deepfakes and also open the door for an attacker to deliberately raise false alarms in the system. That the use of SR pushes all Deepfake Detection models into error is also shown in Figure 2 where it can be seen that in all cases, the AUCs obtained by the models on SR images are drastically lower (dashed lines) than their counterpart tested on images on which the SR process has not been applied (solid lines). To evaluate deepfake detectors in a realistic context, an alternative test set was considered in which pristine images are not subjected to the SR process. In fact, an attacker has much more interest in generating false negatives than false positives, so as to go undetected by automated systems. As can be seen from the experiments reported in Table 2 in this setup the accuracy decreases, though more slightly, in almost all the cases with some deepfake generation methods on which the detectors are more robust to the attack. More in detail, the Face2Face, FaceSwap and FaceShifter images enhanced with the SR attack, are very difficult to detect, probably because the artifacts which the detector has learnt to recognize during the training process, are hidden by the SR process and this is translated in an higher FNR and a lower Recall value. In all the cases, the FPR is not affected by the usage of the SR attack since the pristine images are not attacked in this setup."}, {"title": "5.2 Visual Impact Analysis", "content": "When performing an SR attack on a fake image, it is important that it remains as indistinguishable to human eyes as possible so as to preserve its meaning but also to make it less suspicious to users. To assess the impact of our attack on the image appearance, we compared the similarity of each image pair (non-SR, SR) through two commonly used quality metrics, Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR). The SSIM is calculated as\n$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y+C_1)(2\\sigma_{xy}+C_2)}{(\\mu_x^2+\\mu_y^2+C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}$,\nwhere x and y are the two compared images, $\\mu_x$ and $\\mu_y$ are the average values of x and y respectively, $\\sigma_x$ and $\\sigma_y$ are the standard deviations, $\\sigma_{xy}$ is the covariance between x and y and $C_1$ and $C_2$ are two constants used for stability. To calculate the PSNR we used the formula\n$PSNR(x, y) = 10 \\cdot log_{10}(\\frac{MAX^2}{MSE(x,y)})$, where x and y are the two compared images, MAX is the maximum possible pixel value of the images and MSE(x, y) is the Mean Squared Error between the images.\nThe values obtained from each image pair were used to calculate the mean to see the similarity between images attacked with and without SR for each category. As can be seen from Table 3 the similarity between the SR images and the non-SR ones is very high, with SSIM values around 0.97 and PSNR around 40dB meaning a strong similarity and minimal changes brought by the SR process. We also checked if exists a correlation between the SSIM value and the variation in the error of the classifiers. In other words, we explored if a lower SSIM value is related to a higher number of misclassifications during the detection. From our experiments, in all the methods the correlation is lower than \u00b10.1 meaning that the variation in detectors' performances is more related to the type of changes done to the image and not to the quantity of these."}, {"title": "5.3 Qualitative Evaluation", "content": "To better understand the effect of the SR Attack on images, we visually analyzed some examples of deepfakes (e.g. Face2Face and FaceSwap) correctly detected by a Resnet50-based detector before the application of the attack but misclassified after it.\nThese methods tend to introduce rather specific artifacts that, as visible in Figure 3, are then smoothed by the SR. This makes the work of the deepfake detector more difficult, as it has learnt to recognize such anomalies. As can be seen from the figure also the visual difference is minimal, as already stated by the analysis conducted in Section 5.2, but it is enough to make some artifacts around the mouth (FaceSwap) or on the nose (Face2Face) to disappear."}, {"title": "6 Conclusions", "content": "In this work, we examined the impact of applying SR on deepfake images in the context of deepfake detection. According to our experiments, the use of these techniques has a huge impact on the performance of deepfake detectors, causing the FNR to be drastically raised depending on the deepfake generation technique used and the artifacts introduced by it into the image. Also, a tendency was observed for deepfake detectors trained on specific deepfake generation methods to mistake pristine SR images for fake images when the SR attack is applied, causing the FPR to rise dramatically. In conclusion, SR attack can become an effective black-box attack in deepfake detection. In future work, we will explore the impact of detected face resolution on the attack performance, explore more SR techniques and also see if using SR as a data augmentation during the training process could be effective to make detectors robust to this attack."}]}