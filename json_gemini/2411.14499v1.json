{"title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models", "authors": ["Jingtao Ding", "Yunke Zhang", "Yu Shang", "Yuheng Zhang", "Zefang Zong", "Jie Feng", "Yuan Yuan", "Hongyuan Su", "Nian Li", "Nicholas Sukiennik", "Fengli Xu", "Yong Li"], "abstract": "The concept of world models has garnered significant attention due to advance- ments in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelli- gence. This survey offers a comprehensive review of the literature on world mod- els. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Ini- tially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these as- pects. Finally, we outline key challenges and provide insights into potential future research directions.", "sections": [{"title": "1 Introduction", "content": "The scientific community has long aspired to develop a unified model that can replicate its funda- mental dynamics of the world in pursuit of Artificial General Intelligence (AGI) [98]. In 2024, the emergence of multimodal large language models (LLMs) and Sora [130] have intensified discus- sions surrounding such World Models. While these models demonstrate an emerging capacity to capture aspects of world knowledge such as Sora's generated videos, which appear to perfectly adhere to physical laws questions persist regarding whether they truly qualify as comprehensive world models. Therefore, a systematic review of recent advancements, applications, and future di- rections in world model research is both timely and essential as we look toward new breakthroughs in the era of artificial intelligence.\nThe definition of a world model remains a subject of ongoing debate, generally divided into two primary perspectives: understanding the world and predicting the future. As depicted in Figure 1, early work by Ha and Schmidhuber [59] focused on abstracting the external world to gain a deep un- derstanding of its underlying mechanisms. In contrast, LeCun [98] argued that a world model should not only perceive and model the real world but also possess the capacity to envision possible future states to inform decision-making. Video generation models such as Sora represent an approach that concentrates on simulating future world evolution and thus align more closely with the predictive aspect of world models. This raises the question of whether a world model should prioritize under- standing the present or forecasting future states. In this paper, we provide a comprehensive review of the literature from both perspectives, highlighting key approaches and challenges."}, {"title": "2 Background and Categorization", "content": "In this section, we explore the evolving concepts of world models in the literature and categorize efforts to construct world models into two distinct branches: internal representation and future pre- diction.\nThe concept of a world model was first systematically introduced to the artificial intelligence com- munity by Ha et al. [58, 59] in 2018. This article traces the origins of the world model concept back to the psychological principles of the \"mental model\u201d established in 1971 [43], which proposes that humans abstract the external world into simple elements and their interrelations to perceive it. This principle suggests that our descriptions of the world, when viewed from a deep, internal perspective, typically involve constructing an abstract representation that suffices without requiring detailed de- piction. Building upon this conceptual framework, the authors introduce an agent model inspired by the human cognitive system, as illustrated in Figure 1. In this pioneering model, the agent receives feedback from the real-world environment, which is then transformed into a series of inputs that train the model. This model is adept at simulating potential outcomes following specific actions within the external environment. Essentially, it creates a mental simulation of potential future world evolutions, with decisions made based on the predicted outcomes of these states. This methodology closely mirrors the Model-based Reinforcement Learning (MBRL) method, where both strategies involve the model generating internal representations of the external world. These representations facilitate navigation through and resolution of various decision-making tasks in the real world.\nIn the visionary article on the development of autonomous machine intelligence in 2022 [98], Yann LeCun introduced the Joint Embedding Predictive Architecture (JEPA), a framework mirroring the human brain's structure. As illustrated in Figure 1, JEPA comprises a perception module that pro- cesses sensory data, followed by a cognitive module that evaluates this information, effectively embodying the world model. This model allows the brain to assess actions and determine the most suitable responses for real-world applications. LeCun's framework is intriguing due to its incorpo- ration of the dual-system concept, mirroring \"fast\" and \"slow\" thinking. System 1 involves intuitive, instinctive reactions: quick decisions made without a world model, such as instinctively dodging an oncoming person. In contrast, System 2 employs deliberate, calculated reasoning that considers the future state of the world. It extends beyond immediate sensory input, simulating potential future scenarios, like predicting events in a room over the next ten minutes and adjusting actions accord- ingly. This level of foresight requires constructing a world model to effectively guide decisions based on the anticipated dynamics and evolution of the environment. In this framework, the world model is essential for understanding and representing the external world. It models the state of the"}, {"title": "3 Implicit Representation of the External World", "content": "In decision-making tasks, understanding the environment is the major task in setting a foundation for optimized policy generation. As such, the world model in decision-making should include a comprehensive understanding of the environment. It enables us to take hypothetical actions with- out affecting the real environment, facilitating a low trial-and-error cost. In literature, research on how to learn and utilize the world model was initially proposed in the field of model-based RL."}, {"title": "3.1 World Model in Decision-Making", "content": "In decision-making, the concept of the world model largely refers to the environment model in model-based RL (MBRL). A decision-making problem is typically formulated as a Markov Deci- sion Process (MDP), denoted with a tuple $(S, A, M, R, \\gamma)$, where $S, A, \\gamma$ denotes the state space, action space and the discount factor each. The world model here consists of $M$, the state transi- tion dynamics and $R$, the reward function. Since the reward function is defined in most cases, the key task of MBRL is to learn and utilize the transition dynamics, which can further support policy optimization.\nWorld Model Learning To learn an accurate world model, the most straightforward approach is to leverage the mean squared prediction error on each one-step transitions [97, 115, 80, 145, 81],\n$\\min _{\\theta} E_{s'\\sim M^* (:\\s,a)} [||s' \u2013 M_{\\theta}(s, a)||^2],$  (1)\nwhere $M^*$ is the real transition dynamics used to collect trajectory data and $M_{\\theta}$ is the parameterized transition to learn. Apart from directly utilizing the deterministic transition model, Chua et al.[25] further model the aleatoric uncertainty with the probabilistic transition model. The objective is to minimize the KL divergence between the transition models,\n$\\min _{\\theta} E_{s'\\sim M^*(.1s,a)} [\\log(\\frac{M_{\\theta}(s'|s, a))}{M^* (s'|s, a)})].$  (2)\nIn both settings, the phase of the world model learning task can be transformed into a supervised learning task. The learning labels are the trajectories derived from real interaction environments, also called the simulation data [114].\nFor more complex environments where high-dimensional state space exists, representation learn- ing is widely adopted to improve the effectiveness of world model learning in MBRL. Ha and Schmidhuber[58] adopt an autoencoder structure to reconstruct images via latent states. Hafner et al.[61, 63] propose to learn visual encoder and latent dynamics for visual control tasks, whereas Samsami et al.[153] propose a Recall-to-Imaging framework to further improve memory ability while model learning. Another recent trend is to conduct unified model learning across different tasks [158], which is done by representing the MDP with a next-token-prediction paradigm [81] using transformer architectures. Such a scheme exhibits the potential of obtaining one generalist model for decision models on several tasks with other data modalities.\nPolicy Generation with World Model With an ideally optimized world model, one most straight- forward way to generate a corresponding policy is model predictive control (MPC)[92]. MPC plans an optimized sequence of actions given the model as follows:\n$\\max _{a_{t:t+T}} E_{s_{t'+1}\\sim p(s_{t'+1}/s_{t},a_{t})} [\\sum_{t'=t}^{t+T}r(s_{t'}, a_{t'})],$  (3)"}, {"title": "3.1.2 World model with language backbone", "content": "The rapid growth of language models, especially LLM and MLLM, benefits development in many related applications. With language serving as a universal representation backbone, language-based world models have shown their potential in many decision-making tasks.\nDirect Action Generation via LLM World Models LLM has shown its significant reasoning abil- ity and is capable of directly generating actions in decision-making tasks based on corresponding constructed world models. For example, in the navigation scenarios, Yang et al. [212] transfer pre- trained text-to-video models to domain-specific tasks for robot control, successfully annotating robot manipulation with text instructions as LLM outputs. Zhou et al. [240] further learn a compositional world model by factorizing the video generation process. Such a method enables a strong few-shot transfer ability to unseen tasks.\nBesides training or fine-tuning specialized language-based world models, LLMs and MLLMs can be directly deployed to understand the world environment in decision-making tasks. For example, Long et al. [112] propose a multi-expert scheme to handle visual language navigation tasks. They construct a standardized discussion process where eight LLM-based experts participate to generate the final movement decision. An abstract world model is constructed from the discussion and further imagination (of future states) of the experts to support action generation. Zhao et al. [232] further combine LLMs and open-vocabulary detection to construct the relationship between multi-modal signals and key information in navigation. They propose an omni-graph to capture the structure of the local space as the world model for the navigation task. Meanwhile, Yang et al. [217] utilize an LLM-based imaginative assistant to infer the global semantic graph as the world model based on the environment perception, and another reflective planner to directly generate actions.\nModular Usage of LLM World Models Although taking LLM outputs as actions directly is straightforward in application and deployment, the decision quality in such a scheme heavily re- lies on the reasoning ability of the LLM itself. It can be further improved by integrating LLM-based world models as modules with other effective planning algorithms.\nXiang et al.[203] deploys an embodied agent in a world model, the simulator of VirtualHome [139], where the corresponding embodied knowledge is injected into LLMs. To better plan and complete specific goals, they propose a goal-conditioned planning schema where Monte Carlo Tree Search (MCTS) is utilized to search for the true embodied task goal. Lin et al. [106] introduce an agent, Dy- nalang, which learns a multimodal world model to predict future text and image representations, and which learns to act from imagined model rollouts. The policy learning stage utilizes an actor-critic algorithm purely based on the previously generated multimodal representations. Liu et al. [111] further cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). LLMs, like the world model, perform in an in-context manner within the actor- critic updates of MDPs. The proposed RAFA framework shows significantly increased performance in multiple complex reasoning tasks and environments, such as ALFWorld [168]."}, {"title": "3.2 World Knowledge Learned by Models", "content": "After pretraining on large-scale web text and books [180, 129], large language models attain ex- tensive knowledge about the real world and common sense relevant to daily life. This embedded knowledge is considered crucial for their remarkable ability to generalize and perform effectively in real-world tasks. For instance, researchers leverage the common sense of large language models for"}, {"title": "3.2.1 Knowledge of the Global Physical World", "content": "We first introduce research focused on analyzing and understanding the knowledge of the global physical world. Gurnee et al. [57] present the first evidence that large language models genuinely acquire spatial and temporal knowledge of the world, rather than merely collecting superficial statis- tics. They identify distinct \"spatial neurons\" and \"temporal neurons\u201d in LLama2 [180], suggest- ing that the model learns linear representations of space and time across multiple scales. Distinct"}, {"title": "3.2.2 Knowledge of the Local Physical World", "content": "Unlike the knowledge of the global physical world, the local physical world represents the primary environment for human daily life and most real-world tasks. Therefore, understanding and modeling the local physical world is a more critical topic for building a comprehensive world model. We first introduce the concept of the cognitive map [179], which illustrates how the human brain models the external world. Although initially developed to explain human learning processes, researchers have discovered similar structures in large language models [104] and have leveraged these insights to enhance the efficiency and performance of artificial models in learning and understanding the physical world.\nRecent studies explore actively encouraging models to learn abstract knowledge through cognitive map-like processes across various environments. For example, Cornet et al. [52] demonstrate the ef- fectiveness of learning through spatial cognitive map construction using visual predictive coding in a simplified Minecraft world. After learning, the model can successfully predict the future by know- ing the distance. Lin et al. [106] investigate teaching models to understand the game environments through a world model learning procedure, specifically by predicting the subsequent frame of the environment. In this way, the model can generate better actions in dynamic environments. More- over, Jin et al. [84] find that language models can learn the emergent representations of program semantics by predicting the next token."}, {"title": "3.2.3 Knowledge of the Human Society", "content": "Beyond the physical world, understanding human society is another crucial aspect of world models. One such related theory is the Theory of Mind [138], which explains how individuals infer the mental states of others around them. Recent research has extensively explored how large language models develop and demonstrate this social world model. One line of investigation [174, 175] focuses on evaluating the performance of large language models across various Theory of Mind tasks to determine whether their human-like behaviors reflect genuine comprehension of social rule and implicit knowledge. For example, Strachan et al. [174] conduct a comparative analysis between human and LLM performance on diverse Theory of Mind abilities, such as understanding false beliefs and recognizing irony. While their findings demonstrate the potential of GPT-4 in these tasks, they also identify its limitations, particularly in detecting faux pas.\nTo address these limitations, researchers propose innovative methods to enhance the abilities of large language models in Theory of Mind for complex real-world applications. Wu et al. [198] introduce COKE, which constructs a knowledge graph to help large language models explicitly using theory in mind through cognitive chains. Additionally, Alex et al. [194] develop SimToM, a two-stage prompting framework, to enhance the performance of large language models in theory of mind tasks."}, {"title": "4 Future Prediction of the Physical World", "content": "The integration of video generation into world models marks a significant leap forward in the field of environment modeling [130]. Traditional world models primarily focused on predicting discrete or"}, {"title": "4.1 World Model as Video Generation", "content": "A video world model is a computational framework designed to simulate and predict the future state of the world by processing past observations and potential actions within a visual context [130]. This concept builds on the broader idea of world models, which strive to capture the dynamics of an environment and enable machines to predict how the world will evolve over time. In the case of a video world model, the focus is on generating sequences of visual frames that represent these evolving states.\nSora as a World Model. Sora [130], a large-scale video generation model, is a prominent example of a video world model. It is designed to generate high-quality, temporally consistent video se- quences, up to one minute long, based on various input modalities such as text, images, and videos. Sora leverages a combination of powerful neural network architectures, including encoder-decoder frameworks and transformers, to process multimodal inputs and generate visually coherent simula- tions. Sora's core capabilities lie in its ability to generate videos that align with real-world physical principles, such as the reflection of light on surfaces or the melting of candles. These properties suggest that Sora has the potential to act as a world simulator, predicting future states of the world based on its understanding of the initial conditions and simulation parameters.\nSora's Limitations. However, despite its impressive video generation abilities, Sora has several limitations that prevent it from being considered a fully functional world model. One key limitation concerns causal reasoning [242, 23], wherein the model is limited in simulating dynamic interac- tions within the environment. Thus, Sora can only passively generate video sequences based on an observed initial state, but cannot actively intervene or predict how changes in actions might alter the course of events. Another limitation is that it still fails to reproduce correct physical laws consis- tently [86]. While Sora can generate visually realistic scenes, it struggles with accurately simulating real-world physics, such as the behavior of objects under different forces, fluid dynamics, or the accurate depiction of light and shadow interactions.\nOther Video World Models. Sora has undoubtedly catalyzed a significant wave of research into video world models, inspiring a surge of advancements in this field. Following Sora's success in generating high-quality video sequences, numerous subsequent models have been developed, each aiming to push the boundaries of what video world models can achieve. For example, some ap- proaches have extended video lengths to enable long-form video simulation [220, 108, 68]. In addi- tion to conventional language-guided video generation, more modalities are being integrated, such as images and actions [235, 202]. Researchers are also shifting their focus from basic video generation, which lacks user control, to interactive simulations that aim to replicate the decision space of the real world and facilitate decision-making [213, 215, 197, 227, 78, 202]. Several studies have worked to enhance the smoothness of action transitions, improve the accuracy of physical laws, and maintain temporal consistency [211, 16, 148, 207]. Meanwhile, the concept of world models has evolved beyond imagination and is being applied in various scenario-specific simulations, including natural environments, games, and autonomous driving [108, 190, 15, 120, 68, 188, 11, 238, 121]."}, {"title": "4.1.2 Capabilities of Video World Models", "content": "Despite the ongoing debate about whether models like Sora can be considered full-fledged world models, there is no doubt that video world models hold tremendous potential for advancing envi- ronment simulation and prediction [242, 23, 86]. These models can offer a powerful approach to understanding and interacting with complex environments by generating realistic, dynamic video se- quences. To achieve this level of sophistication, this section outlines the key capabilities that video world models must possess to set them apart from traditional video generation models.\nLong-Term Predictive Ability. A robust video world model should be capable of making long- term predictions that adhere to the dynamic rules of the environment over an extended period. This capability allows the model to simulate how a scenario evolves, ensuring that the generated video"}, {"title": "4.2 World Model as Embodied Environment", "content": "The development of world models for embodied environments is crucial for simulating and predict- ing how agents interact with and adapt to the external world. Initially, generative models focused on simulating visual aspects of the world, using video data to capture dynamic changes in the en- vironment. More recently, the focus has shifted towards creating fully interactive and embodied simulations. These models not only represent the visual elements of the world but also incorporate spatial and physical interactions that more accurately reflect real-world dynamics. By integrating spatial representations and transitioning from video-based simulations to immersive, embodied en- vironments, world models can now provide a more comprehensive platform for developing agents capable of interacting with complex real-world environments.\nWorld models as embodied environments can be divided into three categories: indoor, outdoor, and dynamic environments, as shown in Figure 4, and the relevant works are summarized in Table 4. It can be summarized that most current works focus on developing static, existing indoor and out- door embodied environments. An emerging trend is to predict the dynamic, future world through generative models producing first-person, dynamic video-based simulation environments. Such en- vironments can offer flexible and realistic feedback for training embodied agents, enabling them to interact with ever-changing environments and improve their generalization ability."}, {"title": "4.2.1 Indoor Environments", "content": "Indoor environments offer controlled, structured scenarios where agents can perform detailed, task-specific actions such as object manipulation, navigation, and real-time interaction with users [48, 134, 91, 164, 17, 139, 155, 201]. Early works on establishing indoor environments like"}, {"title": "4.2.2 Outdoor Environments", "content": "In contrast to indoor environments, creating outdoor environments [184, 45, 200, 161, 37] faces greater challenges due to their larger scale and increased variability. Some existing works focus on urban environments, such as MetaUrban [200], where agents are deployed to navigate in large- scale urban environments, where they encounter challenges like dynamically changing traffic, varied building structures, and social interactions with other entities. These tasks often require the use of context-aware navigation algorithms that allow agents to adjust their trajectories and behaviors based on the layout and conditions of the environment. However, the environments in MetaUrban are cre- ated by retrieving and organizing 3D assets from existing libraries. Recently, utilizing advanced generative techniques, UrbanWorld [161] significantly enhances the scope of outdoor environments, using 3D generative models to create complex, customizable urban spaces that allow for more di-"}, {"title": "4.2.3 Dynamic Environments", "content": "Dynamic environments mark a significant evolution from traditional, static simulators by utiliz- ing generative models to create flexible, real-time simulations. Unlike predefined environments that require manual adjustments, these models allow for the dynamic creation of a wide variety of scenarios, enabling agents to experience diverse, first-person perspectives. This shift provides agents with richer, more varied training experiences, improving their adaptability and generaliza- tion in complex, unpredictable real-world situations. A representative work is UniSim [214], which dynamically generates robot manipulation video sequences based on input conditions like spatial movements, textual commands, and camera parameters. Leveraging multimodal data from 3D sim- ulations, real-world robot actions, and internet media, this system generates varied, realistic environ- ments where agents can practice tasks like object manipulation and navigation. The key advantage of this approach is its flexibility, allowing agents to adapt to various scenarios without the limitations of static physical environments. Pandora [202] expands the dynamic environment generation from robot actions in Unisim to wider domains including human and robot actions in both indoor and out- door scenes. Another subsequent work, AVID [149] builds on UniSim by conditioning on actions and modifying noise predictions from a pre-trained diffusion model to generate action-driven visual sequences for dynamic environment generation. Beyond the video diffusion-based framework of Unisim, EVA [22] introduces an additional vision-language model for embodied video anticipation, producing more consistent embodied video predictions. As for the generation of open-world dy- namic environments, Streetscapes [29] employs autoregressive video diffusion models to simulate urban environments where agents must navigate dynamic challenges like changing weather and traf- fic. These environments offer consistently coherent, yet flexible, urban settings, exposing agents to real-world-like variability. The core trend in dynamic environments is the use of generative world models that provide scalable, adaptable simulations. This approach significantly reduces the manual effort required for environment setup, allowing agents to train across a diverse range of scenarios quickly. Moreover, the focus on first-person training closely mimics real-world decision-making, enhancing the agents' ability to adapt to evolving situations. These advances are key in developing embodied environments supporting agent learning in complex, dynamic scenarios.\nGiven the above developments, it is evident that world models as embodied environments have made significant advances in simulating and predicting how agents interact with dynamic, real-world sce- narios. Current research predominantly focuses on developing indoor, static environments, with no- table efforts expanding to large-scale outdoor environments and dynamic simulation environments. A promising direction is to construct dynamic environments, which can provide first-person, action- conditioned future world prediction, enabling agents to better adapt to unseen conditions. These methods are promising to offer flexible, scalable environments for training embodied agents, en- hancing their generalization capabilities for real-world tasks."}, {"title": "5 Application", "content": "In recent years, with the rapid development of vision-based generative models [66, 173, 13] and multimodal large language models [109, 1], world models, which serve as modules for understand- ing the state of the world and predicting its future trends, have garnered increasing attention in the field of autonomous driving. In this context, world models are defined as models that take multi- modal data such as language, images, and trajectories-as input, and continuously output future"}, {"title": "5.1 Autonomous Driving", "content": "world states in the form of vehicle perception data [55]. However, the concept of world models in autonomous driving existed long before the emergence of generation-based world simulators. The modern autonomous driving pipeline can be divided into four main components: perception, predic- tion, planning, and control. The entire process can be viewed as a decision-making pipeline. As we discussed in Section 3, the perception and prediction phases also represent the process of learning an implicit representation of the world for the vehicle. This can also be regarded as a form of world model. Therefore, in this section, we will elaborate on the application and development of world models in autonomous driving from two perspectives: modules that learn the implicit representation of the world and world simulators that output vehicle perception data."}, {"title": "5.1.1 Learning Implicit Representations", "content": "Autonomous vehicles typically utilize cameras, radar, and lidar to perceive the real world, gather- ing information through images, video data, and point cloud data. In the initial decision-making paradigm [19, 156], models often take perceptual data as input and directly output motion planning results for the autonomous vehicle. Conversely, when humans operate vehicles, they typically ob- serve and predict the current and future states of other traffic participants to determine their own driving strategies [74]. Thus, learning the implicit representation of the world through perceptual data and predicting the future states of the surrounding environment is a crucial step in enhancing the decision-making reliability of autonomous vehicles. We consider this process as it manifests in how autonomous vehicles learn a world model in latent space.\nAs shown in the left half of Figure 5, before the advent of multimodal large models and end-to-end autonomous driving technologies [71], the perception and prediction tasks of autonomous vehicles were typically assigned to distinct modules, each trained on their respective tasks and datasets. The perception module processed data from images, point clouds, and other sources to accomplish tasks such as object detection and map segmentation, projecting the perceived world into an abstract geometric space. Furthermore, the prediction module would typically operate within these geometric spaces to forecast the future states of the surrounding environment, including the trajectories and motions of traffic participants.\nThe processing of perceptual data is closely tied to the evolution of deep learning technologies, as shown in Table 5. Pointnet [141], introduced in 2017, was the first to employ deep learning methods for processing point cloud data. As convolutional neural networks advanced, perception techniques based on image data, exemplified by YOLOP [195] and MultiNet [177], emerged and excelled in driving scene understanding tasks [65, 182, 96, 239]. In recent years, the transformer architecture has gained prominence in natural language processing, and this technology has also been applied to"}, {"title": "5.1.2 World Simulators", "content": "As shown in Table 5, before the emergence of multimodal large models and vision-based generative models, traffic scenario simulations are often conducted in geometric spaces. The scene data on which these simulations rely is typically collected by the perception modules of autonomous vehi- cles or constructed manually. These simulations represent future states of the scenario in the form of geometric trajectories [113, 103, 56, 230], which require further modeling and rendering to produce outputs suitable for vehicle perception. The cascading of multiple modules often results in infor- mation loss and increases the complexity of simulations, making scenario control more challenging. Furthermore, realistic scene rendering typically requires substantial computational resources, which limits the efficiency of virtual traffic scenario generation.\nUsing diffusion-based video generation models as a world model partially addresses the aforemen- tioned issues. By training on large-scale traffic scenario datasets, diffusion models can directly"}, {"title": "5.2 Robots", "content": "World models have emerged as a transformative paradigm in robotics, enabling robots to per- ceive, predict, and perform effectively within complex environments. This revolution of robotics is partly made possible due to the advances in neural networks [181, 66] and machine learning algorithms [159, 144] that enable robots to build implicit representations that capture the critical components of the world. On the other hand, prediction models [41, 42] are capable of directly forecasting the future states of the world beyond abstract representations, allowing robots to antic- ipate possible environmental changes and react proactively. With the above powerful techniques, it is becoming practical for robots to directly interact with and learn from the real-world environment. As illustrated in Figure 6, LLMs [87, 218] and world models [18, 199, 64] are considered as one of the possible paths to artificial general intelligence (AGI) as they can be a starting point for machines to understand the underlying laws of the world. We summarize the development of the world model in robotics in Table 6."}, {"title": "5.2.1 Learning Implicit Representation", "content": "Traditional robotic tasks (e.g., object grasping) are typically performed in highly structured environ- ments where the critical components are explicitly modeled [90, 34], eliminating the need for the robot to independently learn or adapt its understanding of the world. However, when the robot is de- ployed in unfamiliar environments, especially those in which key features or dynamics have not been explicitly modeled, tasks that were previously successful may fail as the robot struggles to general- ize to these unknown features [122, 85]. Thus, enabling a robot to learn an implicit representation of its environment is a crucial first step toward achieving intelligence.\nTo help a robot understand the objects in the world, visual models such as convolutional neural net- works (CNNs) [99, 93, 51] and vision transformers (ViT) [31, 183] integrate visual characteristics"}, {"title": "5.2.2 Predicting Future states of Environment", "content": "Robotic tasks are always sequential and long-term, and decisions made in the current moment could have a profound effect on future tasks' performance [171]. Therefore, by anticipating how their actions will affect future environmental states, robots can avoid potential mistakes and improve task performance over time. Classic robotics use closed-loop algorithms [9, 89] that use current observations to guide action selection, causing the robot to be short-sighted and potentially leading to irreversible mistakes, even if it eventually realizes that it has taken a wrong action. Even though some approaches claim to achieve groundbreaking performance in robotics, they rely on explicit dynamic functions based on expert knowledge, limiting the extent and robustness of prediction.\nMORL [205] introduces a monotonic hyperbolic model to predict improvements in the updated policies. Meanwhile, Trajectron++ [152] predicts the environment by calculating the probability distribution over future trajectories via a conditional variational autoencoder. Recently, video gener- ation models use diffusion [36, 21, 10, 64] and transformers [222, 208] as a backbone have become a popular choice for future state prediction. For example, UniPi [33] formulates action prediction as a video prediction problem and trains a constrained diffusion model with the initial state as an explicit conditioning context to enable an accurate imagining of the future. Similarly, VIPER [35] leverages a pretrained autoregressive transformer on expert video, guiding the robots to perform properly, whereas Genie [15], includes a dynamics model that predicts the next state of the environ- ment with previous video frames and actions. Benefiting from the millions of unlabeled videos on the Internet, GR-2 [196, 18] is fine-tuned on robotics tasks, achieving accurate prediction of future images and action trajectory generation for robots."}, {"title": "5.2.3 From Simulation to Real World", "content": "Deep reinforcement learning shines in robot strategy learning, enabling robots to perform stable walking[172, 95], object grasping[223, 30], and even tying shoelaces[5] which is particularly com- plex autonomously. However, deep reinforcement learning is not as efficient as it should be in terms of sample efficiency. For example, it takes tens of thousands of years for a robot to learn to solve a Rubik's Cube in the real world[3], which greatly limits its real-world applications. Therefore, the majority of robotics work is carried out based on simulations, with various distributed training tech- niques [151, 60] improving the efficiency of sample collection. Despite the remarkable efficiency of simulation, a well-trained robot in simulated environments often fails in the real world. This inca- pability is because that simulation cannot fully recover the real-world, and the well-trained policies may fail in those out-of-distribution scenarios. On the other hand, accurately modeling real-world environments is challenging, as simulated environments often differ from the real world, and this difference accumulates in long-distance decision-making, resulting in policies that do not adapt to changes in the world.\nWorld models have shown great promise for robots to handle universal tasks in the real world. NeBula [2] constructs a belief space where robots perform reasoning and decision-making, and can adapt to diverse robot structures and unknown environments, whereas DayDreamer [199] gener- alizes a world model from offline data, empowering robots to learn to walk directly in real-world environments within hours. Moreover, SWIM [120] learns from human videos and fine-tunes from robotics settings without any task supervision, which only requires less than 30 minutes of real-"}, {"title": "5.3 Social Simulacra", "content": "The concept of 'social simulacra' was originally introduced as a prototyping technique in [133", "12": "or re- inforcement learning [236"}]}