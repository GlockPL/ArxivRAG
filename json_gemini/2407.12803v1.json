{"title": "Bosch Street Dataset: A Multi-Modal Dataset with Imaging Radar for Automated Driving", "authors": ["Karim Armanious", "Maurice Quach", "Michael Ulrich", "Timo Winterling", "Johannes Friesen", "Sascha Braun", "Daniel Jenet", "Yuri Feldman", "Eitan Kosman", "Philipp Rapp", "Volker Fischer", "Marc Sons", "Lukas Kohns", "Daniel Eckstein", "Daniela Egbert", "Simone Letsch", "Corinna Voege", "Felix Huttner", "Alexander Bartler", "Robert Maiwald", "Yancong Lin", "Ulf R\u00fcegg", "Claudius Gl\u00e4ser", "Bastian Bischoff", "Jascha Freess", "Karsten Haug", "Kathrin Klee", "Holger Caesar"], "abstract": "This paper introduces the Bosch street dataset (BSD), a novel multi-modal large-scale dataset aimed at promoting highly automated driving (HAD) and advanced driver-assistance systems (ADAS) research. Unlike existing datasets, BSD offers a unique integration of high-resolution imaging radar, lidar, and camera sensors, providing unprecedented 360-degree coverage to bridge the current gap in high-resolution radar data availability. Spanning urban, rural, and highway environments, BSD enables detailed exploration into radar-based object detection and sensor fusion techniques. The dataset is aimed at facilitating academic and research collaborations between Bosch and current and future partners. This aims to foster joint efforts in developing cutting-edge HAD and ADAS technologies. The paper describes the dataset's key attributes, including its scalability, radar resolution, and labeling methodology. Key offerings also include initial benchmarks for sensor modalities and a development kit tailored for extensive data analysis and performance evaluation, underscoring our commitment to contributing valuable resources to the HAD and ADAS research community.", "sections": [{"title": "Introduction", "content": "Lidar, camera, and radar sensors are the most important modalities for highly automated driving (HAD) and advanced driver-assistance systems (ADAS). Every sensor comes with its distinct advantages, with radar sensors standing out for their robustness in diverse weather and illumination conditions, coupled with their cost-effectiveness and capability of efficiently estimating the velocity of the underlying target objects. Substantial progress happened in radar sensing technology with imaging radar sensors, enabling the development of novel processing methodologies. Traditional radar sensors suffered from poor resolution, prohibiting the use of deep-learning based perception"}, {"title": "Related Work", "content": "State-of-the-art HAD datasets such as \"Waymo Open [23]\", \"Once [12]\", \"Argoverse 2 [26]\", and \"Zenseact Open Dataset [1]\" do not feature radar at all. While several existing automotive datasets either contain data from high-resolution radar sensors [13,14,17,29] or contain a sufficient number of annotated frames [5,23,26], top-tier research needs both: sensors with sufficient resolution and sufficient dataset size.\nFurthermore, many existing datasets are highly specialized, focusing on one single aspect. This hinders comparability of algorithms. For instance, some datasets lack high"}, {"title": "Background on Radar Sensing", "content": "Radar is a unique modality, with new possibilities and challenges for the research community. In this section, we elaborate the differences to lidar and camera to underline the need for research in this domain."}, {"title": "Radial Velocity", "content": "The availability of radial relative velocity (Doppler) measurements for each reflection point is the most prominent radar feature. Strictly speaking, it is not a property of the radar modality, but rather a result of the frequency modulation scheme that is usually"}, {"title": "Fluctuating Point Cloud", "content": "Typical automotive radar sensors operate at a carrier frequency $f_c$ of 76 GHz to 78 GHz, which corresponds to a wavelength of $X \\approx 4$ mm. The roughness of most surfaces, such as roads or objects of interest, is smoother than this, resulting in mirror-like reflections. This results in highly fluctuating intensities for small changes in aspect angle [11, 19]. These fluctuations result in different point clouds of the same object, in slightly different frames. This makes object detection in radar point clouds challenging."}, {"title": "Separability of Radar Points", "content": "The wavelength determines the size of the transmitter and receiver elements. Larger antenna elements (aperture), relative to wavelength, allow for a narrower beam and higher spatial selectivity. Contrary to radar sensors, lidar sensors scan the scene with narrow beams, which are possible due to the small wavelength of near-infrared light. In contrast, a limited radar sensor size of a few centimeters can focus the radar beam only to a tenth of a degrees. Hence, radar illuminates the whole field of view at once and reflection points are separated only later in the digital signal processing.\nAs a result, radar possesses resolution mainly in the range (distance) and Doppler dimension, which means that separate points are generated on an object, when it is either extended in range or moving with different radial velocities. Subtle differences in radial velocity in the order of 0.1 m/s, for instance, are enough to separate reflections on different object parts. Specifically, radial velocity can differ for different parts of a rigid object, due to different observation angles for different parts. In contrast, lidar and camera resolve points mainly through azimuth and elevation angle dimensions and have range and Doppler only as features. This has the following implications:\nFew radar points for problematic objects: Some objects might generate only a few radar points, when they have no significant extent in range or Doppler dimension. To give an example, this is often observed with stationary pedestrians.\nAngular ambiguities: Radar sensors are often subject to angular ambiguities which have to be resolved.\nEven more ambiguities: Multiple radar points may be positioned at the exact same location with different radial velocities. For instance, a bicycle can typically result in several points as its different parts have different radial velocities.\nSuitable for far distances: The whole scene is illuminated at once with the radar beam. This means that objects are missed seldomly, which makes radar suitable for high distances. In contrast, an object may lie between neighboring lidar beams and be overlooked. In addition, all reflection points of one measurement correspond to the same time. There are no shutter effects as with camera or distortion due to the rotating scans of spinning LiDARs."}, {"title": "Radar for Redundancy or as a Cost-Efficient Alternative to Lidar", "content": "As an active sensor, radar is independent of external lighting conditions, similar to lidar. Furthermore, radar is less sensitive to adverse weather conditions, such as fog or rain, in comparison to lidar. This makes radar an excellent choice to add redundancy when lidar and camera sensors fail.\nAlternatively, radar could be a cost-efficient substitute for lidar: the cost of one of the lidar sensors of this dataset is more than two orders of magnitude higher than the commercial variant of the included radar sensor. Radar-only or camera-radar setups could democratize HAD, bringing state-of-the-art research to the broader public, while the industry is still struggling to make lidar-based systems commercially successful."}, {"title": "The Bosch Street Dataset", "content": ""}, {"title": "Vehicle and Sensor Setup", "content": "We use a fleet of five vehicles for capturing the data. Four of those vehicles are located in Germany, and one vehicle is located in the USA (California). Every vehicle has a sensor setup consisting of nine imaging radar prototypes, four lidar sensors, and four mono cameras. Additionally, an inertial measurement unit is used in conjunction with wheel speed sensors to provide odometry information by means of dead reckoning. This allows us to define an inertial coordinate system that does not suffer from GPS outage or discontinuous corrections. Fig. 2 shows the sensor mounting poses of the sensors."}, {"title": "Sensor Specifications", "content": "Radar Sensor Specifications and Features. The capturing vehicle is equipped with a 360\u00b0 radar belt consisting of nine imaging radar prototypes. Those are long-range radars that employ chirp-sequence frequency modulation and provide up to $N_{pts}$ = 1024 reflection points per scan per sensor at a sampling interval of $\\Delta t_{rad}$ = 66 ms (15 Hz)."}, {"title": "Data Description", "content": "The data is provided in the form of 13 649 sequences (9431 train, 2828 val, 1390 test), to allow for temporal processing. Each sequence is between 5 and 10 seconds long. The aggregated duration of the dataset is 36.5 hours (25.2h training, 7.6h validation, 3.7h testing) containing 33.9h of automatically and 2.6h of manually labeled data.\nThe sensor data as described in the sections above are provided \u201cas is\u201d and kept as raw as possible on purpose, except for their conversion into Cartesian coordinates. This is to allow for most flexibility when using the BSD. Common preprocessing steps are provided in the development kit accompanying the data. In addition to the sensor data,"}, {"title": "Labeling Process", "content": "The scale of the dataset plays a crucial role, as will be verified empirically in Sec. 5.2. Therefore, we include labels originating from our auto-labeling approach alongside labels originating from manual labeling. The former enables efficient label generation, whereas the latter, albeit more time-consuming and costly, yields labels of superior quality with regards to the scale and orientation of the bounding boxes.\nIn the BSD, 94,212 frames are manually annotated (63,134 training, 9,296 validation, 21,782 test). Those are complemented by an additional number of 1,220,162 frames that are auto-labeled (845,308 training, 263,902 validation, 110,952 test)."}, {"title": "Manual Labeling", "content": "The manual labels are created by experienced labeling experts. Labeling is done in a \u201cmulti sensor labeling\" approach, that is, the 3D oriented bounding boxes are created leveraging the dense lidar point clouds at a frequency of 10 Hz and then verified by looking at the radar and camera measurements."}, {"title": "Auto-Labeling", "content": "The auto-labeling system utilizes a high-performance multi-modal tracking and fusion system, specifically the Bernoulli Gaussian Sum Filter [20]. As we are processing the data offline, we can implement this filter in an acausal manner which enables us to do additional processing steps that cannot be done in online processing. This includes doing a backward-forward pass (in time), leveraging track termination from the backward pass to enhance track initiation in the forward pass. As a result, it minimizes track losses, reduces noise in state estimates, and maintains a consistent (i.e., constant) spatial extent. The labels generated through this auto-labeling process undergo additional post-processing and quality checks (such as for instance removing implausible tracks). Radar is used as reference sensor for auto-labeling, thus resulting in a label frequency of 15 Hz.\nAuto-labeled sequences are manually curated to ensure high label quality and reliability. Specifically, in case manual curation of a sequence uncovers major issues, the sequence length is adjusted while ensuring that it remains longer than 5 seconds. That is, we discard low quality sequences and sequences shorter than 5 seconds."}, {"title": "General Remark", "content": "Since auto-labeling and manual labeling lead to different label frequencies, we interpolate labels onto a common label frequency of 20 Hz."}, {"title": "Dataset Analysis", "content": "The dataset was collected in various cities in Germany and the USA over the course of two years. For Germany, there are three main geospatial clusters: greater Stuttgart area (including cities like T\u00fcbingen, Reutlingen, Esslingen, Ulm, Heilbronn, and Karlsruhe), Munich, and Berlin. For the US, we took measurements in California in the San Fransisco Bay Area. While recording, interesting scenes were manually selected and labeled, as described in Sec. 4.4.\nDiversity was the key design factor considered during the data collection and selection process. Here, diversity does not only refer to the geographical location of the collected scenes, but also to the road type (highway, rural, and urban), driving situation (straight road, intersection, roundabout, and tunnel), time of day (day time, twilight, and night) and weather conditions (clear, cloudy, and rainy). These different facets of diversity are labeled individually for each sequence as part of the provided metadata. Examples from the dataset are shown in Fig. 3. The camera is highly affected by lighting conditions, whereas radar and lidar maintain performance in the night scenarios.\nThe dataset is divided into a train, validation, and test splits, whose data distribution is depicted in Fig. 4. They were defined to enforce similar distributions with regards to the included classes, weather conditions, road types, and geographical locations. In addition, geographical integrity is maintained with no overlap occurring between the train/val/test splits.\nFig. 5 depicts the distribution of the labels over range and velocity of the BSD in comparison to nuScenes. In BSD, labels are available up to high distances. Further, the distribution over the velocities indicates a significantly more diverse dataset, including also high velocities. This reflects the inclusion of different road situations such as highway and suburban scenes in contrast to the majority of prior datasets which focus only on urban situations as indicated in Tab. 1."}, {"title": "Baseline Model Performance", "content": "To establish benchmarks on BSD, we trained baseline models for object detection, employing Far3D [8] for camera data and CenterPoint [27] for both radar and lidar data. For the radar model a 360\u00b0 point cloud is aggregated over 300ms, using multiple measurements from each sensor. These models were also trained on the nuScenes dataset to facilitate direct comparison, as illustrated in Fig. 7. Due to the extremely limited number of submissions within the public nuScenes leaderboards for radar, we compare against the KPConvPillars [25] approach for this modality. Notably, BSD's imaging radar capabilities have markedly narrowed the performance gap between radar and other modalities, a disparity more pronounced in the nuScenes results. For instance, in BSD, the radar-based CenterPoint model approaches the performance of its lidar counterpart, particularly in detecting cars and large vehicles. This is reflected by the reduced mAP gap between the modalities of only 16.4% and 17.7% for the car and large vehicle classes, respectively, in comparison to 54.8% and 47.3% in nuScenes."}, {"title": "Importance of Dataset Size", "content": "In Fig. 8, we show the importance of dataset size by performing an ablation study on subsets of the full dataset. Specifically, we train the radar object detection CenterPoint model on subsets of the training sets and compute object detection metrics on the evaluation set. Those subsets are generated picking random samples until we cover the desired reduced percentage of the full dataset. The training then is performed on each of the subsets, while we still evaluate on the same test set as for the 100% training. We observe that the performance decreases significantly as the size of the training set is reduced. Therefore, we show that the dataset size is essential."}, {"title": "Importance of Radar Point Cloud Density", "content": "In Fig. 8, we show the advantages of higher resolution radars by sub-sampling the aggregated point clouds to fixed percentages of their original point count. We use the same architecture and training configuration as for the baseline radar object detection model but apply a random sub-sampling algorithm to the training as well as the evaluation and test data. We observe that the detection performance decreases significantly over all classes with a decreasing point cloud density. We can therefore say that a high resolution is crucial in closing the gap between radar and lidar detection performance."}, {"title": "Label Quality", "content": "In this section we examine the performance of models that are trained on different quality levels of labels. We train radar-based models for the object detection task on manual, automatic, and both types of labels, while we test on manual labels only. The results are depicted in Tab. 2. We observe a significant improvements by training on automatic labels compared to the smaller subset of the available manual ones. We explain this by"}, {"title": "Conclusion", "content": "In conclusion, the Bosch street dataset (BSD) represents a significant advancement in the realm of autonomous driving datasets. By providing a large-scale, diverse, and multi-modal dataset that includes high-resolution imaging radar data alongside lidar and camera data, BSD addresses crucial gaps in currently available datasets. Our experimental results, which highlight reduced performance discrepancies between radar and other sensor modalities, underscore the potential of imaging radar in enhancing object detection and perception systems. The BSD not only sets a new precedent for sensor data quality and dataset scale but also serves as a foundational platform for the development of sophisticated HAD technologies.\nBSD is positioned as a strategic asset to foster academic and research collaborations with Bosch. This collaborative model aims to spark a surge in innovation and to accelerate progress in the autonomous driving domain. We are inviting for academic partnerships based on BSD and its associated assets that we believe will be instrumental in pushing the boundaries of what is possible in HAD research and development. Through these joint efforts, we are confident in our collective ability to advance towards the creation of more dependable and sophisticated autonomous driving systems, inching ever closer to the vision of fully autonomous vehicles."}]}