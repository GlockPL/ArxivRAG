{"title": "Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments", "authors": ["Runfa Chen", "Ling Wang", "Yu Du", "Tianrui Xue", "Fuchun Sun", "Jianwei Zhang", "Wenbing Huang"], "abstract": "Learning policies for multi-entity systems in 3D environments is far more complicated against single-entity scenarios, due to the exponential expansion of the global state space as the number of entities increases. One potential solution of alleviating the exponential complexity is dividing the global space into independent local views that are invariant to transformations including translations and rotations. To this end, this paper proposes Subequivariant Hierarchical Neural Networks (SHNN) to facilitate multi-entity policy learning. In particular, SHNN first dynamically decouples the global space into local entity-level graphs via task assignment. Second, it leverages subequivariant message passing over the local entity-level graphs to devise local reference frames, remarkably compressing the representation redundancy, particularly in gravity-affected environments. Furthermore, to overcome the limitations of existing benchmarks in capturing the subtleties of multi-entity systems under the Euclidean symmetry, we propose the Multi-entity Benchmark (MEBEN), a new suite of environments tailored for exploring a wide range of multi-entity reinforcement learning. Extensive experiments demonstrate significant advancements of SHNN on the proposed benchmarks compared to existing methods. Comprehensive ablations are conducted to verify the indispensability of task assignment and subequivariance.", "sections": [{"title": "1. Introduction", "content": "Learning to navigate, control, cooperate, and compete in the 3D physical world is a fundamental task in developing intelligent agents. Deep reinforcement learning (RL) has made impressive breakthroughs, particularly in single-entity systems, with agent policies evolving through environmental interactions (Mnih et al., 2015; Silver et al., 2016; Mnih et al., 2016; Schulman et al., 2017; Bansal et al., 2018; Liu et al., 2018; 2022). However, an intricate challenge is generalizing across configurations like transformations, morphologies, and tasks, which are interlinked and complicate the learning process. In particular, multi-entity systems, which include agents, objects, and other entities defined in (Spelke, 2022), present considerable challenges compared to single-entity scenarios, partly due to exponential expansion of global transformations as the number of entities increases (Deng et al., 2023). In Figure 1, for example, any horizontal rotation of the entities, although producing different global coordinates and representations, does not change the essential geometry and their local views of different entities. Such symmetry, defined as subequivariance (formal definition is given in Section 2) in this paper, provides a potential way to reduce the complexity of the state space. There are certain previous studies leveraging this type of symmetry in RL. For instance, the methods based on heading normalization (HN) (Won et al., 2020; 2022) transform global coordinates into a local reference frame (LRF), which is yet non-learnable and non-adjustable with respect to the goal of the task. Morphology-based RL advances (Chen et al., 2023) for single entity have incorporated subequivariance (Han et al., 2022) into policy modeling, reducing reliance on hand-crafted LRFs. However, extending subequivariance from single-entity to inter-entity transformations reveals unexplored challenges due to the coupled local space of each entity.\nTo tackle the difficulties of interdependence and generalization, we introduce a novel framework, named Subequivariant Hierarchical Neural Networks (SHNN), integrating task assignment and local entity-level subequivariant message passing within a hierarchical network architecture. SHNN offers two key advancements: 1. We implement the task assignment using bipartite graph matching to dynamically construct local entity-level graphs. This approach aids in managing interdependence among entities by decoupling local transformations from the overall structure. 2. We implement local entity-level subequivariant message passing, effectively compiling information from related entities. To guide body-level control, we utilize entity-level information to define a LRF for each entity, effectively compressing the global state space and facilitating the generalization of body-level policy in a lossless way.\nMoreover, another reason for this limited exploration is the absence of suitable environments in existing morphology-based (Huang et al., 2020; Chen et al., 2023; Furuta et al., 2023) and multi-agent reinforcement learning (MARL) (Samvelyan et al., 2019; de Witt et al., 2020; Ellis et al., 2022; Rutherford et al., 2023; Lechner et al., 2023) frameworks. These benchmarks inadequately probe complex entity interactions, especially in scenarios involving multi-agent dynamics under a diverse range of inter-entity transformations. To bridge this gap, we present a new suite of Multi-entity Benchmark (MEBEN) in 3D space. Built upon JAX-based RL environments (Bradbury et al., 2018; Godwin* et al., 2020; Heek et al., 2023; Freeman et al., 2021; Gu et al., 2021), MEBEN is designed to investigate multi-entity interactions, encompassing both cooperative and competitive dynamics, within physical geometric symmetry constraints that include a diverse range of inter-entity transformations.\nOur contributions are summarized as follows:\n\u2022 To effectively optimize the policy in 3D multi-entity physical environments, we propose SHNN, a novel framework that offers a superior plug-in alternative to hand-crafted LRFs. It decouples local transformations from the overall structure and compresses the state space by leveraging local physical geometric symmetry, particularly in gravity-affected environments.\n\u2022 We introduce MEBEN, a collection of subequivariant morphology-based MARL environments, designed for in-depth exploration of multi-entity interactions within physical geometric symmetry constraints. These environments, including a diverse range of inter-entity transformations, facilitate both cooperative and competitive dynamics.\n\u2022 We demonstrate the effectiveness of SHNN in the proposed 3D multi-entity physical environments, including Team Reach and Team Sumo 1. Our extensive ablations and comparative analyses also reveal the efficacy of the proposed ideas."}, {"title": "2. Preliminaries", "content": "Geometric Symmetry The symmetrical structure in 3D environments is E(3), which is a 3-dimensional Euclidean group (Dresselhaus et al., 2007) that consists of rotations, reflections, and translations.\nDefinition 2.1 (Group). A group G is a set of transformations with a binary operation \".\" satisfying these properties: \".\" is closed under associative composition, there exists an identity element, and each element must have an inverse. Symmetrical structure enforced on the model (Worrall et al., 2017; van der Pol et al., 2020; Thomas et al., 2018; Fuchs et al., 2020; Jing et al., 2020; Deng et al., 2021; Villar et al., 2021; Satorras et al., 2021; Huang et al., 2022; Han et al., 2022; Luo et al., 2022; Chen et al., 2023; Joshi et al., 2023; Wang et al., 2024; Han et al., 2024) is formally described by the concept of equivariance.\nDefinition 2.2 (Equivariance). Suppose $\\vec{Z}$ to be 3D geometric vectors (positions, velocities, etc) that are steerable by a group G, and h non-steerable features. The function f is G-equivariant, if for any transformation $g \\in G$, $f(g.\\vec{Z},h) = g.f(\\vec{Z}, h)$, $\\forall \\vec{Z} \\in \\mathbb{R}^{3 \\times m}, h \\in \\mathbb{R}^{d}$. Similarly, f is invariant if $f(g \\cdot \\vec{Z},h) = f(\\vec{Z},h)$.\nSpecifically, the E(3) operation \u201c.\u201d is instantiated as $g \\cdot \\vec{Z} := OZ$ for the orthogonal group that consists of rotations and reflections where $O \\in \\mathcal{O}(3) := \\{O \\in \\mathbb{R}^{3 \\times 3}|OTO = I\\}$, and is additionally implemented as the translation $g \\cdot x := x + t$ for the 3D coordinate vector where $t \\in T(3) := \\{t \\in \\mathbb{R}^{3}\\}$. To align with the principles of classical physics under the influence of gravity, we introduce a relaxation of the group constraint. Particularly, we consider equivariance within the subgroup of E(3) induced by gravity $\\vec{g} \\in \\mathbb{R}^3$, defined as $\\mathcal{O}_{\\vec{g}}(3) := \\{O \\in \\mathbb{R}^{3 \\times 3}|OTO = I,O\\vec{g} = \\vec{g}\\}$ and $T_{\\vec{g}}(3) := \\{t \\in \\mathbb{R}^{3}|t \\vec{g} = 0\\}$. By this means, the $E_{\\vec{g}}(3)$-equivariance is only restrained to the translations/rotations/reflections along the direction of $\\vec{g}$. We term subequivariance primarily refers to $E_{\\vec{g}}(3)$-equivariance.\nProblem Definition and Notation Our investigation in 3D physical environments delves into the interactions among multiple entities (Spelke, 2022), including agents with distinct morphologies that enable sophisticated form and motion control, objects that critically influence the system's symmetry and dynamics. We model this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by $\\mathcal{G} = (\\mathcal{S}, \\mathcal{A}, P, \\mathcal{O}, r, \\gamma, \\Omega)$, where $\\mathcal{S}$ indicates the state space, $\\mathcal{A}$ the action space, P the transition probability, $\\mathcal{O}$ the observation function, r the reward function, and $\\gamma$ the discount factor. The complete set of all entities is denoted as $\\Omega = \\{1, \\dots, N + M\\}$, where N denotes the number of cooperative agents, while M represents the count of objects (or competitive agents). 1. Observation. Each agent, denoted as i, is composed of $K_i$ bodies. At any given timestep t, agent i obtains a unique observation $o_i(t) := \\mathcal{O}(s(t), i)$ from the global state $s(t) \\in \\mathcal{S}$, capturing its individual perspective of the environment. This observation encompasses detailed state information about agent i itself, represented as $\\{s_{i,k}(t)\\}_{k=1}^{K_i}$. However, an agent's awareness of other entities is confined to states of their root bodies, denoted as $s_{j,1}(t)$ for each $j \\in \\Omega \\backslash \\{i\\}$. Thus, the complete observation for agent i at time t is formulated as $o_i(t) := \\{\\{s_{i,k}(t)\\}_{k=1}^{K_i}, s_{j,1}(t) | j \\in \\Omega \\backslash \\{i\\}\\}$. 2. Action and Reward. The decision-making process for each agent i at timestep t involves selecting an action $a_i(t) := \\{a_{i,k}(t)\\}_{k=1}^{K_i} \\in \\mathcal{A}$ based on its policy $\\pi_{\\theta_i}(a_i(t)|o_i(t))$. Each actuator k of the agent, ranging from 2 to $K_i$, contributes by generating a torque $a_{i,k}(t) \\in [-1,1]$. Consequently, the aggregated action $a(t) = (a_1(t), \\dots, a_N(t)) \\in \\mathcal{A}^N$ arises from the combined actions of all agents. The environment reacts by transitioning to a new global state $s(t + 1) \\sim P(s(t+1)|s(t), a(t))$ and allocates a shared team reward $r(s(t), a(t))$. 3. Actor and Critic. Multiple entities interactions, depending on the number of agents and their relational dynamics, are typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this complex multi-agent environment, Multi-Agent Proximal Policy Optimization (MAPPO) (Schulman et al., 2017; Yu et al., 2022) is employed to optimize a joint policy $\\pi_{\\theta} = (\\pi_{\\theta_1}, \\dots, \\pi_{\\theta_N})$ in this intricate environment. Our objective is to maximize the expected global return:\n$J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} [\\sum_{t=0}^T \\gamma^t r(s(t), a(t))].$"}, {"title": "3. Subequivariant Hierarchical Neural Networks", "content": "In this section, we introduce our entire framework Subequivariant Hierarchical Neural Networks (SHNN), visualized in Figure 2, consisting of an input processing module, a novel task assignment to decouple local transformations from the overall structure, a local entity-level subequivariant message passing for expressive information passing and fusion, a local reference frame transform to addresses local transformations by leveraging local physical geometric symmetry in environments with gravity and a body-level control module to obtain the final policy. Building upon these, the global state space is effectively compressed.\nInput Processing The observation input for agent i at time t is formulated as $o_i(t) = \\{\\{s_{i,k}(t)\\}_{k=1}^{K_i}, s_{j,1}(t) | j \\in \\Omega \\backslash \\{i\\}\\}$. To adhere to the constraints of physical geometric symmetry, the state $s_{i,k}$ is subdivided into directional geometric vectors $\\vec{Z}_{ik}$ and scalar features $h_{i,k}$. Elements in $\\vec{Z}_{ik}$ will rotate according to the transformation $g \\in \\mathcal{O}_{\\vec{g}}(3)$, while those in $h_{i,k}$ remain unchanged. Specifically, in our 3D environment, $\\vec{Z}_{i,k} \\in \\mathbb{R}^{3 \\times 3}$ comprises the position $p_{i,k} \\in \\mathbb{R}^3$, positional velocity $\\dot{p}_{i,k} \\in \\mathbb{R}^3$, and rotational velocity $\\omega_{i,k} \\in \\mathbb{R}^3$. Here, $p_{i,k}$ is transformed into a translation-invariant representation by redefining it as $p_{i,k} - \\vec{c}$, where $\\vec{c} = \\frac{1}{N + M}\\sum_{j \\in \\Omega} p_{j,1}$. This operation subtracts $\\vec{c}$, the average root position of all entities, thereby ensuring translation invariance. The scalar features $h_{i,k} \\in \\mathbb{R}^{13}$ include the rotation angles $\\kappa_{i,k}, \\zeta_{i,k}, \\delta_{i,k}$ of the joint axes and their corresponding ranges, along with a 4-dimensional one-hot vector indicating the type of body such as \"torso\", \"limb\u201d, or \u201cball\u201d. The direction of gravity $\\vec{g}$ is set to be along the z-axis.\nTask Assignment In multi-entity environments, it is crucial to manage complex interactions among entities. Initially, we consider a fully connected entity-level graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, with $\\mathcal{V} = \\Omega$ and $\\mathcal{E} = \\{(i, j) : i, j \\in \\Omega, i \\neq j\\}$. To decouple local transformations from the overall structure, we introduce task assignment which dynamically adjusts the edges, forming local graphs of associated entities. The graph is then redefined as a task assignment entity-level graph, where $\\mathcal{E} = \\{(i, j) : i, j \\in \\Omega, C(i) = C(j)\\}$ and C are the assignment labels. This assignment, akin to part segmentation (Deng et al., 2023), selectively links related entities, like agents and their objects. Specifically, in this study, we employ a rule-based task assignment approach using bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching (refer to Algorithm 1).\nLocal Entity-level Subequivariant Message Passing We propose enhancing mainstream Neural Networks (such as MLPs) in RL with an additional local entity-level subequivariant message passing (MP). This task assignment entity-level graph adeptly integrates local entity-level information from related entities and, through local subequivariant MP, efficiently instills the desired local physical geometric symmetry in environments with gravity into mainstream Neural Networks to address local transformations.\nFor each entity $i \\in \\Omega$, input node features are initialized using the entity's root state. Specifically, $\\vec{Z}_i$ is assigned as $\\vec{Z}_{i,1}$, and $h_i$ is set as $[h_{i,1}, \\vec{p}_{i,1}]$, where [ ] is the stack along the last dimension and $\\vec{p}_{i,1}$ represents the projection of the coordinate $p_{i,1}$ onto the z-axis. This projection effectively indicates the relative height of entity i, considering the ground as the reference point.\nWithin the context of entity-level interactions, our function $\\varphi_{\\theta}$ serves as the local entity-level subequivariant MP. It updates each entity's node features by considering the collective input features and the established graph connectivity.\n$\\{\\vec{Z}_i, h_i\\}_{i=1}^{N+M} = \\varphi_{\\theta} (\\{\\vec{Z}_i, h_i\\}_{i=1}^{N+M},\\mathcal{E}).$\nSpecifically, $\\varphi_{\\theta}$ is unfolded as the following MP and aggregation computations:\n$\\vec{Z}_{ij} = [(p_{j,1} - p_{i,1}), \\vec{Z}_i, \\vec{Z}_j]$,\n$h_{ij} = [||p_{j,1} - p_{i,1} ||_2, h_i, h_j]$,\n$\\vec{M}_{ij}, m_{ij} = \\phi_{\\vec{g}} (\\vec{Z}_{ij}, h_{ij})$,\n$\\vec{M}_{i}, m_{i} = \\sum_{j \\in N(i)} \\vec{M}_{ij}, \\sum_{j \\in N(i)} m_{ij}$,\n$(\\vec{Z}_i, h_i) = (\\vec{Z}_i, h_i) + \\Psi_{\\theta} (\\vec{M}_i, \\vec{Z}_i], [m_i, h_i])$,\nwhere [ ] denotes the stack along the last dimension, $N(i) = \\{j : (i, j) \\in \\mathcal{E}\\}$ is the neighbors of node i, and both $\\phi_{\\vec{g}}$ and $\\Psi_{\\theta}$ are subequivariant (Han et al., 2022; Chen et al., 2023), simplified as follows:\n$f_{\\theta}(\\vec{Z}, h) = [\\vec{Z}, M_{\\vec{g}}]$\ns.t. $M_{\\vec{g}} = \\sigma([\\vec{Z}, \\vec{g}][\\vec{Z},\\vec{g}], h)$, where $\\sigma(\\cdot)$ is an Multi-Layer Perceptron (MLP) and $[\\vec{Z},\\vec{g}] \\in \\mathbb{R}^{3 \\times (m+1)}$ is a stack of $\\vec{Z}$ and $\\vec{g}$ along the last dimension. The specific form of the function is detailed in Appendix C.1.\nWe establish a subequivariant edge representation $\\vec{Z}_{ij}$ and invariant edge features $h_{ij}$. The edge features $\\vec{Z}_{ij}$ and $h_{ij}$ are then input into $\\phi_{\\vec{g}}$, as defined in Equation (5), to yield vector and scalar messages $\\vec{M}_{ij}$ and $m_{ij}$, respectively. Message aggregation and state updates are performed as outlined in Equation (6) and Equation (7) using the function $\\Psi_{\\theta}$, leading to the updated states $\\vec{Z}$ and h. This process ensures the generation of outputs maintaining the desired subequivariance or invariance properties."}, {"title": "Local Reference Frame Transform", "content": "Following the local entity-level subequivariant MP, we integrate the resultant entity-level information h and $\\vec{Z}$ with body-level information to guide body-level control. Here, $\\vec{u}$ represents a local reference frame (LRF) transform vector, derived through the linear transformation $\\vec{u}_i = \\vec{Z}W\\vec{\\mu}$, with $W_a \\in \\mathbb{R}^{m \\times 1}$.\nWe normalize and orthogonalize the transform vector $\\vec{u}$ for each entity:\n$\\vec{e}_{i1} = \\frac{\\vec{u}_i - (\\vec{u}_i, \\vec{e}_{i3})\\vec{e}_{i3}}{||\\vec{u}_i - (\\vec{u}_i, \\vec{e}_{i3})\\vec{e}_{i3}||}$,\n$\\vec{e}_{i2} = \\vec{e}_{i1} \\times \\vec{e}_{i3}$,\n$\\vec{e}_{i3} = [0,0,1]^T$.\nHere, $(\\cdot, \\cdot)$ denotes the inner product, and $\\times$ the cross product. We refer to the aforementioned procedure as OP, following which we proceed to construct an entity-wise rotation matrix on transform vectors.\n$O_i = [\\vec{e}_{i1}, \\vec{e}_{i2}, \\vec{e}_{i3}] = OP(\\vec{u}_i), \\ i \\in \\Omega$.\nTheorem 3.1. The learned entity-wise rotation matrix, denoted as $O_i = OP(\\vec{u}_i)$, are $SO_{\\vec{g}}(3)$-equivariant, satisfying any transformation $g \\in SO_{\\vec{g}}(3), g \\cdot O_i = OP(g \\cdot \\vec{u}_i)$.\nProof. See Appendix A.\nAt this stage, we establish the LRF for each agent i, with $\\vec{c}$ as the origin and $\\vec{e}_{i1}$ as the orientation of the x-axis. Notably, with the task assignment, the origin of each agent's LRF shifts from the collective average root position $\\vec{c} = \\frac{1}{N+M} \\sum_{i=1}^{N+M} p_{i,1}$, to the specific entity's position, $\\vec{c} = p_{c(i),1}$. This LRF construction enables us to achieve invariant observation inputs:\n$\\vec{o}_i = O_i \\cdot o_i = [\\{O_i \\vec{Z}_{i,k}, h_{i,k}\\}_{k=1}^{K_i}, O_i \\vec{Z}_{j,1}, h_{j,1}],$\nwhere $j \\in N(i)$, and [ ] is the stack along the last dimension, with adjustments for relative positioning in assignment as $p_{i,k} = p_{i,k} - p_{c(i),1}, p_{j,1} = p_{j,1} \u2013 p_{c(j),1}$, thus achieving decoupled translation and rotation invariance.\nOur methodology integrates subequivariant information across entity and body levels via LRF Transform. While disrupting reflection symmetry, the necessity to construct an orthogonal rotation matrix significantly enhances the capabilities of subequivariant networks. This emphasis on rotation symmetry substantially outweighs the reduced focus on reflection symmetry, particularly in diminishing the massive search space for optimal policies. Empirical validations of this enhancement are detailed in Section 5.4.\nBody-level Control We are now equipped to output the invariant actor policy $\\pi_{\\theta}$ and invariant critic value-function $V_{\\theta}$ for the training objective in Equation (1). For each agent i, the invariant actor policy $\\pi_{\\theta_i}$, leverages the invariant $\\vec{o}$ and h, defined by\n$\\pi_{\\theta_i} = \\sigma_{\\pi_i} (\\vec{o}, h),$\nwhere $\\sigma_{\\pi_i}$ is a MLP. Here, $\\pi_{\\theta_i} \\in \\mathbb{R}^{2 \\times (K_i-1)}$ represents the loc and scale parameters of a Tanh-Normal Distribution for the $(K_i \u2013 1)$ actuators of agent i. Consequently, each actuator samples its corresponding torque $a_{i,k} \\in [-1,1]$ from this distribution.\nBesides, the invariant critic value-function $V_{\\theta}$ utilizes the entity-level invariant features $\\{h\\}_{i=1}^{N+M}$, formulated as\n$V_{\\theta} = \\sigma_v (\\{h\\}_{i=1}^{N+M}),$\nwhere $\\sigma_v$ is a MLP, and $V_{\\theta} \\in \\mathbb{R}$. Formal proof of the SO(3)-invariance of the output action and value are presented in Appendix A."}, {"title": "4. Multi-entity Benchmark (MEBEN)", "content": "4.1. Environments descriptions\nIn this subsection, we present the details involved in constructing our environments: Team Reach and Team Sumo, as illustrated in Figure 3.\nAgents In our studies, we leverage a variety of morphologies, including ants, claws, and centipedes from MxT-Bench (Furuta et al., 2023), as well as unimals from Gupta et al. (2022). Notably, centipedes and unimals exhibit asymmetrical forms, potentially influencing the overall system's symmetry and dynamics. This diverse range of agent morphologies enables a nuanced exploration of agent dynamics in multi-entity, morphology-based RL environments."}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nBaselines We compare our method SHNN, against mainstream neural networks, particularly MLP (Furuta et al., 2023), and its variant utilizing heading normalization tricks, denoted as MLP+HN (Chen et al., 2023). Please refer to Appendix C.3 for details about baselines.\nMetrics 1. Team Reach: Success Rate = #success_episode / #evaluation_episode. 2. Team Sumo: For each team, Win Rate = #win_episode / #evaluation_episode.\nEach experiment is conducted with 10 seeds to report the Success (or Win) Rate over #evaluation_episode of 1024.\nImplementations The environments used in this work are detailed in Table 4. To ensure fairness, all baselines, ablations, and our SHNN model use the same input information and employ MAPPO (Yu et al., 2022) as the training algorithm for MARL. SHNN is developed based on the MxT-bench(Furuta et al., 2023) codebase, leveraging JAX(Bradbury et al., 2018) and Brax (Freeman et al., 2021; Gu et al., 2021) for efficient, hardware-accelerated simulations. The value of the maximum timesteps per episode is 1000. Hyperparameter details are in Appendix C.9. Within the same team, agents share only the weights of entity-level message passing, not the body-level MLP. For the Team Reach environments, we construct a directed graph by dynamically assigning a fixed ball to each agent. For the Team Sumo environments, we assign each agent an opponent from the opposing team and assign each with the arena's center ball. Additionally, in Team Sumo environments, we adopt Bansal et al. (2018)'s approach where teams employing baseline methods and SHNN compete within an arena.\n5.2. Evaluations in Diverse Environments\nWe begin by evaluating our method on diverse multi-entity tasks in 3D environments. Team Reach in Figure 4: 1. The MLP generally fails to achieve meaningful returns in most Team Reach cases, which is due to its vulnerability to local extremes within the expansive exploration space of our environments. 2. While MLP enhanced with Heading Normalization (MLP+HN) shows close performance to SHNN in specific environments like 1_ant and 2_unimals, their effectiveness diminishes in other scenarios. This variation in performance can be attributed to factors such as the symmetry in morphology and the complexities arising from multi-agent interactions. 3. Our proposed SHNN demonstrates a clear advantage, outperforming all baselines across various scenarios. Team Sumo in Figure 5: with an increase in the number and complexity of agent morphologies, the advantage of our method over baselines progressively amplifies. These observations underscore that our proposed SHNN method not only exhibits superior capabilities in compressing the global state space of multi-entity tasks and significantly enhances the generalization of body-level policies, but also sees these advantages magnified in scenarios with increasingly complex agent morphologies and a greater number of agents.\n5.3. Extended Evaluations on Transformer\nAdditionally, building upon our initial exploration with MLP, our exploration extended to Transformers, utilizing the same backbone as Amorpheus (Kurin et al., 2020) and MxT-Bench (Furuta et al., 2023) for $\\sigma_{\\pi_i}$ in Equation (12) and $\\sigma_v$ in Equation (13). Despite Transformers' advances in morphology-based RL (Kurin et al., 2020; Hong et al., 2021; Dong et al., 2022; Chen et al., 2023; Furuta et al., 2023), their training complexity and computational demands limit their performance improvement over MLPs in environments with restricted agent morphology diversity, as detailed in Table 5 and Table 2, with curves in Figure 14. Moreover, SHTransformer (our plug-in applied to body-level Transformer control) consistently outperforms standard Transformer models. This evidence confirms the plug-in's broad applicability and effectiveness across varied architectural frameworks.\n5.4. Ablation Studies\nAblations on Assignment We evaluate the impact of task assignment by comparing several variants, as depicted in Figure 6 and Figure 11. We consider the following configurations: greedy, a default model that employs a greedy strategy-based bipartite matching for task assignment; stochastic, a variant based on stochastic assignments; w/o assignment, a variant where no task assignment is performed, with the entity-level graph configured as a fully connected graph and sparse root mean $\\vec{c} = \\frac{1}{N+M} \\sum_{i=1}^{N+M} p_{i,1}$ serving as the origin of LRF in Equation (11). Our empirical findings affirm that assignment consistently enhances performance across various neural networks architectures in the Team Reach environments. Specifically, MLP networks exhibit marked improvement when incorporating task assignments. For SHNN, the greedy graph matching strategy of task assignment significantly outperforms both stochastic and non-assignment strategies. This evidence underscores that our task assignment effectively facilitates task-specific dynamics, managing complex entity interactions by decoupling local transformations from the overall structures while compressing the search space through local physical symmetry. For Team Sumo, see Appendix C.5.\nAblations on Equivariance We ablate the following variants in Figure 7 and Figure 12: SHNN:SOg3, our full model, which is $SO_{\\vec{g}}(3)$-equivariant; SHNN:Og3, an $\\mathcal{O}_{\\vec{g}}(3)$-equivariant variant, where $O_i = \\vec{Z}$ in Equation (10); SHNN:SO3, a SO(3)-equivariant variant, where $\\vec{g}$ is omitted in the computation in Equation (8); HNN+DN, a non-equivariant variant, where $\\vec{Z}$ are treated as scalars, but which utilizes the goal direction to construct the LRF; HNN+HN, a non-equivariant variant, where $\\vec{Z}$ are treated as scalars, but which utilizes the agent's heading direction to construct the LRF; HNN, a non-equivariant variant, where $\\vec{Z}$ are treated as scalars. 1. Which symmetry group works best within our network framework? Comparative experiments between the $SO_{\\vec{g}}(3)$ group and the $\\mathcal{O}_{\\vec{g}}(3)$ group show that the positive impact of emphasizing rotational symmetry significantly outweighs any disadvantages from the reduced focus on reflection symmetry, particularly in terms of reducing the massive search space. Furthermore, the comparison between the $SO_{\\vec{g}}(3)$ group and the SO(3) group demonstrates the importance of sensing the direction of gravity in policy learning. 2. Can equivariant network methods replace or even surpass methods based on hand-crafted LRF? Experiments demonstrate that whether using goal orientation or the agent's heading orientation to construct the LRF, the performance is inferior compared to ours. This indicates that for current mainstream neural networks, entity-level subequivariant message passing emerges as a simpler yet more effective alternative to hand-crafted LRF, providing a plug-in solution for equivariant modifications.\nImportance of Local Symmetry The analysis of local symmetry in the Team Reach environments, illustrated by comparing the red and green lines in the left plot of Figure 6, and the red and purple lines in the right plot of Figure 7, underscores the indispensable roles of task assignment and equivariance. Their integration is pivotal for surmounting the benchmark's challenges, validating the SHNN method's design philosophy. To further substantiate the criticality of local symmetry, we embarked on ablation studies exploring the influence of local translation invariance with the following variants: without subtraction, where no translation invariance implemented; with sparse root mean subtraction: $\\vec{c} = \\frac{1}{N+M} \\sum_{i=1}^{N+M} \\vec{p}_{i,1}$, i.e., without decoupling translation transformations; with dense body mean subtraction, $\\vec{c} = \\frac{1}{N+M} \\frac{1}{K_i} \\sum_{i=1}^{N+M} \\sum_{k=1}^{K_i} p_{i,k}$, i.e., without decoupling translation transformations; with relative positions in assignment, $\\vec{c} = p_{c(i),1}$, i.e., with decoupling of translation transformations. The results in Figure 8 clearly indicate that without achieving translation invariance or properly decoupling it, the model struggles to mitigate complexity challenges, leading to significantly poorer performance. This observation further emphasizes the pivotal importance of local symmetry."}, {"title": "6. Limitations and Future Works", "content": "To learn policies in 3D multi-entity physical environments, we propose the SHNN, a framework that uniquely integrates task assignment with local subequivariant message passing in a hierarchical structure. However, in the Team Sumo environments, as shown in Figure 11, the impact of assignment is not as significant as that of equivariance. Therefore, the interdependence of task assignment and equivariance necessitates a novel co-learnable formulation. Our model's reliance on the $E_{\\vec{g}}(3)$ symmetry encompasses a broad range of physical interactions but does not extend to non-Euclidean or highly irregular environments not governed by classical physics. Additionally, our model predominantly leverages state-based inputs, which are often costly to acquire in practice due to the need for precision sensors. Addressing the challenge of applying our equivariance-focused approach to vision-based inputs remains an open area for future research."}, {"title": "A. Proofs", "content": "In this section, we theoretically prove that our proposed SHNN ensures the final output action and critic value preserve the symmetry as desired.\nTheorem A.1, denoted as $O_i = OP(\\vec{u}_i)$, are $SO_{\\vec{g}}(3)$-equivariant, satisfying any transformation $g \\in SO_{\\vec{g}}(3), g \\cdot O_i = OP(g \\cdot \\vec{u}_i)$.\nProof. To prove that the entity-wise rotation matrix $O_i = OP(\\vec{u}_i)$ are $SO_{\\vec{g}}(3)$-equivariant, we need to show that under any transformation $g \\in SO_{\\vec{g}}(3)$, the transformation of $O_i$ through g is equivariant to the rotation matrix obtained from the transformed vectors $g \\cdot \\vec{u}_i$.\nLet g be a transformation in $SO_{\\vec{g}}(3)$, which includes a rotation O along the direction of $\\vec{g}$. Specifically, the transformation is applied as follows:\n$g \\cdot O_i = OO_i,$\n$g \\cdot \\vec{u}_i = O\\vec{u}_i,$\nLet $O^\\prime = OP(g \\cdot \\vec{u}_i)$. By the properties of the orthogonalization process, we have:\n$\\vec{e}^\\prime_{i1} = \\frac{O\\vec{u}_i - (O\\vec{u}_i, \\vec{e}^\\prime_{i3})\\vec{e}^\\prime_{i3}}{||O\\vec{u}_i - (O\\vec{u}_i, \\vec{e}^\\prime_{i3})\\vec{e}^\\prime_{i3}||}$,\n$\\vec{e}^\\prime_{i2} = \\vec{e}^\\prime_{i1} \\times \\vec{e}^\\prime_{i3},$\n$\\vec{e}^\\prime_{i3} = [0,0,1]^T$.\nSince $O\\vec{g} = \\vec{g}, OTO = I$ and $det(O) = 1$, it preserves inner products and norms. And, the cross product obeys the following identity under matrix transformations: $(M \\vec{a}) \\times (M \\vec{b}) = (det M) (M^{-1})T (\\vec{a} \\times \\vec{b})$. Therefore, $\\vec{e}^\\prime_{i3} = O\\vec{e}_{i3}, ||O\\vec{u}_i || = ||\\vec{u}_i||$ and $(O\\vec{u}_i, O\\vec{e}_{i3}) = (\\vec{u}_i, \\vec{e}_{i3})$. This implies:\n$\\vec{e}^\\prime_{i1} = \\frac{O\\vec{u}_i - (\\vec{u}_i, \\vec{e}_{i3})O\\vec{e}_{i3}}{||O\\vec{u}_i - (\\vec{u}_i, \\vec{e}_{i3})O\\vec{e}_{i3}||} = \\frac{O(\\vec{u}_i - (\\vec{u}_i, \\vec{e}_{i3})\\vec{e}_{i3})}{||O(\\vec{u}_i - (\\vec{u}_i, \\vec{e}_{i3})\\vec{e}_{i3})||} = O \\frac{\\vec{u}_i - (\\vec{u}_i, \\vec{e}_{i3})\\vec{e}_{i3}}{||\\vec{u}_i - (\\vec{u}_i, \\vec{e}_{i3})\\vec{e}_{i3}||} = O\\vec{e}_{i1},$\n$\\vec{e}^\\prime_{i2} = O\\vec{e}_{i1} \\times O\\vec{e}_{i3} = det(O)O(\\vec{e}_{i1} \\times \\vec{e}_{i3}) = O\\vec{e}_{i2},$\n$\\vec{e}^\\prime_{i3} = [0,0,1]^T = O\\vec{e}_{i3}.$\nTherefore, with $O^\\prime = OP(g \\cdot \\vec{u}_i) = [O\\vec{e}_{i1}, O\\vec{e}_{i2}, O\\vec{e}_{i3}] = OO_i = g \\cdot O_i$, we confirm the $SO_{\\vec{g}}(3)$-equivariance of $O_i$. Besides, for reflection transformations O, characterized by $OTO = I$ and $det(O) = -1$, $\\vec{e}^\\prime_{i2} = -O\\vec{e}_{i2} \\rightarrow O^\\prime \\neq OO_i$ where the equivariance is disrupted due to the properties of the cross product. Hence, although we utilize an $\\mathcal{O}_{\\vec{g}}(3)$-equivariant message passing network in Equation (8), the orthogonalization process can transform a $\\mathcal{O}_{\\vec{g}}(3)$-equivariant vector into a $SO_{\\vec{g}}(3)$-equivariant matrix.\nAs for the actor and critic, we additionally have the following corollary.\nCorollary A.2. Let $\\pi_{\\theta_i}, V_{\\theta}$ be output of the actor and the critic of SHNN with $\\vec{Z}, \\vec{g}, h$ as input. Let $\\pi^\\prime_{\\theta_i}, V^\\prime_{\\theta}$ be the actor and critic with $O\\vec{Z}, O\\vec{g}, h$ as input, $O \\in SO_{\\vec{g}}(3)$. Then, $(\\pi^\\prime_{\\theta_i}, V^\\prime_{\\theta}) = (\\pi_{\\theta_i}, V_{\\theta})$, indicating the output actor and critic preserve $SO_{\\vec{g}}(3)$-invariance.\nProof. Given the subequivariance of Equation (8) as per (Han et al., 2022; Chen et al., 2023), we have $h^\\prime = h$.\nHence,\n$V^\\prime_{\\theta} = \\sigma_v(\\{h^\\prime\\}_{i=1}^{N+M})$,\n$= \\sigma_v(\\{h\\}_{i=1}^{N+M})$,\n$= V_{\\theta}.$\nThen, we get $o^\\prime$ with $O\\vec{Z}, O\\vec{g}, h$ as input:\n$o^\\prime_i = [\\{\\vec{Z}^\\prime_{i,k}, h_{i,k}\\}_{k=1}^{K_i}, \\vec{Z}^\\prime_{j,1}, h_{j,1}]$.\nBy Theorem A.1, we can obtain $O^\\prime_i = OO_i$ with $O\\vec{Z}, O\\vec{g}, h$ as input.\nTherefore, the LRF invariant observation inputs $o^*$ with $O\\vec{Z}, O\\vec{g}, h$ as input:\n$o^{*\\prime}_i = O_i^TO^\\prime_io^\\prime_i,$\n$= O_i^T O_i [\\{O^TO \\vec{Z}_{i,k}, h_{i,k}\\}_{k=1}^{K_i}, O^TO O \\vec{Z}_{j,1}, h_{j,1}],$\n$= [\\{(OTO \\vec{Z}_{i,k}, h_{i,k}\\}_{k=1}^{K_i}, OTO O \\vec{Z}_{j,1}, h_{j,1}],$\n$= [\\{\\vec{Z}_{i,k}, h_{i,k}\\}_{k=1}^{K_i}, \\vec{Z}_{j,1}, h_{j,1}],$\n$= O_i^T O_i o_i,$\n$= o_i^*$.\nHence,\n$\\pi^\\prime_{\\theta_i} = \\sigma_{\\pi_i} (o^{*\\prime}, h^\\prime),$\n$= \\sigma_{\\pi_i} (o^*, h),$\n$= \\pi_{\\theta_i}.$\nB. Related Works\nMorphology-based RL In the field of reinforcement learning (RL), recent years have witnessed the emergence and evolution of morphology-based approaches, particularly within the context of inhomogeneous morphology settings. This setting is distinguished by varying state and action spaces across different tasks (Devin et al., 2017; Chen et al., 2018; D'Eramo et al., 2020). Morphology-based RL decentralizes the control of multi-joint robots by learning a shared policy for each joint, applying a multitude of message-passing strategies. To tackle the challenges of the inhomogeneous setting, methods such as NerveNet (Wang et al., 2018), DGN (Pathak et al., 2019), and SMP (Huang et al., 2020) represent the agent's morphology as a graph and implement Graph Neural Networks (GNNs) for their policy networks. On the other hand, Amorpheus (Kurin et al., 2020), SWAT (Hong et al., 2021), ModuMorph (Xiong et al., 2023), SGRL (Chen et al., 2023), and Solar (Dong et al., 2022) opt for transformers over GNNs for direct communication. Both approaches demonstrate that graph-based policies offer significant benefits compared to conventional monolithic policies. Furthermore, MxT-Bench (Furuta et al., 2023) serves as a testbed for morphology-task generalization, though its primary focus remains on single-agent tasks. Our work, however, expands upon these methodologies to encompass multi-entity environments, facilitating comprehensive exploration of dynamics and interactions among a variety of entities, including agents with intricate morphologies, objects, and other crucial factors affecting system symmetry and behaviors.\nMulti-Agent RL In the realm of Multi-Agent RL (MARL), decision-making is predominantly guided by frameworks such as the decentralized partially observable Markov decision process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016; Lechner et al., 2023), with strategies typically categorized into cooperative (Xu et al., 2023; Rashid et al., 2018), competitive (Bansal et al., 2018), and mixed interactions (Lowe et al., 2017). Addressing challenges like high-dimensional action spaces and the necessity for agent coordination, decentralized learning (Littman, 1994; Foerster et al., 2016) aims to independently optimize each agent's policy. While scalable, this approach often struggles with issues of instability, especially under conditions of partial observability. On the other hand, centralized learning (Claus & Boutilier, 1998; Kraemer & Banerjee, 2016) offers a more comprehensive view but faces computational hurdles in complex environments.\nThe Centralized Training with Decentralized Execution (CTDE) paradigm (Lowe et al., 2017; Sunehag et al., 2018; Rashid et al., 2018; Foerster et al., 2018; Iqbal & Sha, 2019; Kuba et al., 2021; Wen et al., 2022; Yu et al., 2022; Jeon et al., 2022) emerges as a balanced solution to these extremes. It allows agents to make decisions based on local observations, while global state information is utilized in the construction of the value function. This synergy of individual autonomy and collective insight enhances the efficacy of the decision-making process. Notably, Yu et al. (2022) revisited the application"}, {"title": "C. More Experimental Details", "content": "C.1. Subequivariant Function\nWe resort to subequivariant function with $\\vec{Z}, \\vec{g}, h$ as input (Han et al., 2022; Chen et al., 2023) to instill desired geometric symmetry into the model:\n$\\vec{Z}^\\prime, h^\\prime = M_{\\vec{W}_z}, \\vec{W}_g,$\ns.t. $\\vec{W} = \\sigma(M M_{\\vec{g}}, h),$\nwhere $M_{\\vec{g}} = [\\vec{Z},\\vec{g}]$Wis a mixing of the vectors to capture the interactions between channels, with a learnable weight matrix $W \\in \\mathbb{R}^{(m+1) \\times m}$ and $[\\vec{Z},\\vec{g}] \\in [\\mathbb{R}^{3 \\times (m+1)}$ is a stack of $\\vec{Z}$ and $\\vec{g}$ along the last dimension. The inner product $M M_{\\vec{g}} \\in \\mathbb{R}^{m \\times m}$ is computed and concatenated with h. The resultant invariant term is then transformed by a Multi-Layer Perceptron (MLP)) $\\sigma : \\mathbb{R}^{m \\times m + h} \\rightarrow \\mathbb{R}^{m \\times m}$ producing $\\vec{W}_g \\in \\mathbb{R}^{m \\times m}$.\nC.2. Environments\nIn this subsection, we present the technical details involved in constructing our challenging MEBEN. The environments utilized in this work are listed in Table 4.\nAgents In our studies, we leverage a variety of morphologies, including ants, claws, and centipedes from MxT-Bench (Furuta et al., 2023), as well as unimals from Gupta et al. (2022). Significantly, the asymmetrical forms of centipedes and unimals have the potential to influence the overall system's symmetry and dynamics. This diversity in agent morphologies facilitates a nuanced exploration of agent dynamics within multi-entity, morphology-based RL environments.\nTeam Reach We expand the \u201cReach\u201d task from a single-agent challenge to a collaborative \u201cTeam Reach", "components": "a. Success Bonus: A significant sparse reward of 10,000 is awarded. b. Distance Reward: A dense reward to incentivize the achievement of the task objective. It is computed as $5 \\times \\frac{1}{M} \\sum_{j=1}^M  \\frac{1}{N}\\sum_{i=1}^N exp(-dist_i)$, where $dist_i$ represents the distance from the nearest agent to ball j. c. Moving Reward: Designed to motivate agents to move closer to any ball, it is quantified as $0.2 \\times \\sum_{j=1}^M \\sum_{i=1}^N max(\\vec{v} \\cdot (p_j - p_i), 0)$. d. Control Cost: This penalty discourages agents from executing excessively large actions, and is calculated as -0.2  \\sum_{i=1}^N \\sqrt{\\sum_{k=1}^K (a_{i,k})^2}$. Thus, the shared team reward is equal to the sum of the aforementioned rewards.\nTeam Sumo We evolve the \u201cSumo\u201d task from a purely competitive challenge to a mixed cooperative-competitive", "Sumo": "ask, as shown in Figure 10. 1. Initial Conditions. Entities set $\\Omega$ comprise one fixed ball, N agents forming Team 1, and another M agents constituting Team 2. The sumo arena, a circle with radius R, has its center marked by the fixed ball. Around this fixed ball, within a radius of $R \u2013 1$, we randomly position N agents from Team 1 and another M from Team 2, ensuring each agent's orientation is also randomized. Specific details for N, M, R, and the agents' morphology are provided in Table 4. 2. Termination. The objective is for either Team 1 or Team 2 to win by having an opposing team's agent disqualified, which occurs if it exceeds a distance R from the fixed ball. The team with the disqualified agent loses, triggering the termination of the episode. 3. Reward. The reward designed for Team 1 (for Team 2, simply swap N and M) is divided into five parts: a. Win Bonus: A sparse reward of 1000 for achieving the win condition. b. Lose Bonus: A sparse penalty of -1000 for the losing condition. c. Distance Reward: A dense reward to encourage achieving the task's objective, computed as $5 \\times exp(\\vec{dist} - R)$, where $\\vec{dist}$ denotes the distance of the farthest agent from the opposing team to the fixed ball. d. Moving Reward: This motivates agents to move closer to any member of the opposing team, calculated as $5 \\times \\frac{1}{N} \\sum_{i=1}^N max(\\vec{v} \\cdot (\\vec{p_0} - p_i), 0)$. e. Control Cost: A penalty for excessively large actions by agents, quantified as -0.1 \\times \\frac{1}{N}\\sum_{i=1}^N \\sqrt{\\sum_{k=1}^K (a_{i,k})^2}$. In line with the Exploration Curriculum suggested by Bansal et al. (2018), the shared team reward is calculated as $\\alpha \\times dense reward + (1 - \\alpha) \\times sparse reward$, where $\\alpha$ is a linear annealing factor. Agents train with the dense reward for 25% of the training epochs.\nC.3. Baselines\nWe compare our method SHNN against mainstream neural networks, particularly MLP as utilized in (Furuta et al., 2023), and a variant employing heading normalization technique, denoted as MLP+HN (Chen et al., 2023)."}, {"title": "C.5. Additional Ablations", "content": "Additional Ablations on Assignment In the Team Sumo environments, task assignment yields limited improvement. This could be attributed to the environments' complex mixture of cooperative and competitive dynamics, which are challenging to effectively decouple using bipartite matching methods, as demonstrated in Figure 11.\nAdditional Ablations on Equivariance In the Team Sumo environments, task assignment leads to the formation of local graphs composed of triplets of agents from both sides and a fixed ball, making it challenging to determine the specific goal orientation learned by equivariant networks. Consequently, our method was primarily compared against HNN+HN. The experimental outcomes align with the main text findings, as demonstrated in Figure 12.\nImportance of Local Symmetry By comparing the red and blue lines in the left plot of Figure 11 and in the right plot of Figure 12, it becomes evident that in the Team Sumo environments, the impact of assignment is not as significant as that of equivariance. Therefore, careful design of assignment strategies is crucial to harness the advantages offered by equivariance effectively.\nAblation Study on Entity Abstraction The additional ablation study examines the effects of using different abstraction for each entity. Our hierarchical structure dictates a representative abstraction for each entity, and within RL environments, the reward is often contingent on the root body's state. Utilizing the first (root) body as a representative aligns with the common computational practices in RL, focusing on the pivotal elements that influence agent behavior and reward structures. Moreover, representing entity-level information through the mean of all K bodies is a viable alternative, offering a more comprehensive local dynamic. The ablation study examines the effects of using the root body versus the average of all K bodies: Root body: $\\vec{Z}_i$ is assigned as $\\vec{Z}_{i,1}$, $h_i$ is set as $[h_{i,1}, \\vec{p}_{i,1}]$, $\\vec{Z}_{ij} = [(p_{j,1} - p_{i,1}), \\vec{Z}_i, \\vec{Z}_j]$, and $h_{ij} = [||p_{j,1} - p_{i,1}||_2, h_i, h_j]$; Average of all K bodies: $\\vec{Z}_i$ is assigned as $ \\frac{1}{K_i}\\sum_{k=1}^{K_i} \\vec{Z}_{i,k}$, $h_i$ is set as $[ \\frac{1}{K_i}\\sum_{k=1}^{K_i} h_{i,k}, \\frac{1}{K_i}\\sum_{k=1}^{K_i} \\vec{p}_{i}]$, $\\vec{Z}_{ij} = [(\\frac{1}{K_j}\\sum_{k=1}^{K_j} \\vec{p}_{j,k} - \\frac{1}{K_i}\\sum_{k=1}^{K_i} \\vec{p}_{i,k}), \\vec{Z}_i, \\vec{Z}_j]$, and $h_{ij} = [|| \\frac{1}{K_j}\\sum_{k=1}^{K_j} \\vec{p}_{j,k} - \\frac{1}{K_i}\\sum_{k=1}^{K_i} \\vec{p}_{i,k}||_2, h_i, h_j]$. The empirical results in Figure 13, particularly for the asymmetric 1_centipede scenario, reveal a significant performance gap, clearly demonstrating the superiority of using the root body's state over the average of all K bodies."}, {"title": "C.6. Analyses of Morphology-shared Policy", "content": "We provide an extended example of our methodology applied to morphology tasks. Previous works have achieved generalization across agents with different morphologies using morphology-aware Graph Neural Networks (Wang et al., 2018; Huang et al., 2020).\nOur variant, referred to as SHGNN, replaces the body-level MLP with body-level message passing. This adaptation enables the learning of a shared policy network across different agents.\nFirst, we utilize the morphology topology information of each agent i to construct a body-level inner-entity graph, denoted as $G_i = (V_i, E_i)$. For each body, k  \\in V_i, input node features are initialized using the body's state. Specifically, $Z_k$ is assigned as $Z_{i,k}$, and $h_k$ is set as $[h_{i,k}, h_i, \\vec{p_k}]$, where [ ] is the stack along the last dimension and $\\vec{p_k}$ represents the projection of the coordinate $P_{ik}$ onto the z-axis. In a body-level overview, we denote our body-level message passing as the function $\\varphi_b$ that updates each body's node features given the input node features of all body and graph connectivity:\n$\\{\\vec{Z}_k, h_k\\}_{k=1}^{K_i} = \\varphi_b (\\{\\vec{Z}_k, h_k\\}_{k=1}^{K_i}, E_i).$\nNotably, the unfolding of $\\varphi_b$ is similar to that of $\\varphi_{\\theta}$, and will not be elaborated further here.\nFor each agent i, the invariant actor policy $\\pi_{\\theta_i}$ is defined as\n$\\pi_{\\theta_i} = \\{\\pi_{\\theta_{i,k}}\\}_{k=2}^{K_i} = \\{\\sigma_{\\pi}(O \\vec{Z}_{i,k})\\}_{k=2}^{K_i}$\nwhere $\\sigma_{\\pi}$ is a linear layer with bias. Here, $\\pi_{\\theta_i} \\in \\mathbb{R}^{2 \\times (K_i -1)}$ represents the location and scale parameters of a Normal Tanh Distribution for the $(K_i \u2013 1)$ actuators of agent i. Each actuator samples its corresponding torque $a_{i,k} \\in [-1,1]$ from this"}]}