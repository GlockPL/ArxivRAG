{"title": "Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments", "authors": ["Runfa Chen", "Ling Wang", "Yu Du", "Tianrui Xue", "Fuchun Sun", "Jianwei Zhang", "Wenbing Huang"], "abstract": "Learning policies for multi-entity systems in 3D environments is far more complicated against single-entity scenarios, due to the exponential expansion of the global state space as the number of entities increases. One potential solution of alleviating the exponential complexity is dividing the global space into independent local views that are invariant to transformations including translations and rotations. To this end, this paper proposes Subequivariant Hierarchical Neural Networks (SHNN) to facilitate multi-entity policy learning. In particular, SHNN first dynamically decouples the global space into local entity-level graphs via task assignment. Second, it leverages subequivariant message passing over the local entity-level graphs to devise local reference frames, remarkably compressing the representation redundancy, particularly in gravity-affected environments. Furthermore, to overcome the limitations of existing benchmarks in capturing the subtleties of multi-entity systems under the Euclidean symmetry, we propose the Multi-entity Benchmark (MEBEN), a new suite of environments tailored for exploring a wide range of multi-entity reinforcement learning. Extensive experiments demonstrate significant advancements of SHNN on the proposed benchmarks compared to existing methods. Comprehensive ablations are conducted to verify the indispensability of task assignment and subequivariance.", "sections": [{"title": "1. Introduction", "content": "Learning to navigate, control, cooperate, and compete in the 3D physical world is a fundamental task in developing intelligent agents. Deep reinforcement learning (RL) has made impressive breakthroughs, particularly in single-entity systems, with agent policies evolving through environmental interactions. However, an intricate challenge is generalizing across configurations like transformations, morphologies, and tasks, which are interlinked and complicate the learning process. In particular, multi-entity systems, which include agents, objects, and other entities defined present considerable challenges compared to single-entity scenarios, partly due to exponential expansion of global transformations as the number of entities increases. In Figure 1, for example, any horizontal rotation of the entities, although producing different global coordinates and representations, does not change the essential geometry and their local views of different entities. Such symmetry, defined as subequivariance"}, {"title": "2. Preliminaries", "content": "Geometric Symmetry The symmetrical structure in 3D environments is E(3), which is a 3-dimensional Euclidean group that consists of rotations, reflections, and translations.\nDefinition 2.1 (Group). A group G is a set of transformations with a binary operation \".\" satisfying these properties: \".\" is closed under associative composition, there exists an identity element, and each element must have an inverse. Symmetrical structure enforced on the model is formally described by the concept of equivariance.\nDefinition 2.2 (Equivariance). Suppose $\\mathcal{Z} \\in \\mathbb{R}^{3 \\times m}$ to be 3D geometric vectors (positions, velocities, etc) that are steerable by a group G, and $h \\in \\mathbb{R}^d$ non-steerable features. The function f is G-equivariant, if for any transformation $g \\in G$, $f(g \\cdot \\mathcal{Z}, h) = g \\cdot f(\\mathcal{Z}, h)$, $\\forall \\mathcal{Z} \\in \\mathbb{R}^{3 \\times m}, h \\in \\mathbb{R}^d$. Similarly, f is invariant if $f (g \\cdot \\mathcal{Z},h) = f(\\mathcal{Z},h)$. Specifically, the E(3) operation \u201c$\\cdot$\u201d is instantiated as $g \\cdot \\mathcal{Z} := OZ$ for the orthogonal group that consists of rotations and reflections where $O \\in \\mathcal{O}(3) := \\{O \\in \\mathbb{R}^{3\\times 3} | O^T O = I\\}$, and is additionally implemented as the translation $g \\cdot x := x + t$ for the 3D coordinate vector where $t \\in \\mathcal{T}(3) := \\{t \\in \\mathbb{R}^3\\}$. To align with the principles of classical physics under the influence of gravity, we introduce a relaxation of the group constraint. Particularly, we consider equivariance"}, {"title": "3. Subequivariant Hierarchical Neural Networks", "content": "In this section, we introduce our entire framework Subequivariant Hierarchical Neural Networks (SHNN), consisting of an input processing module, a novel task assignment to decouple local transformations from the overall structure, a local entity-level subequivariant message passing for expressive information passing and fusion, a local reference frame transform to addresses local transformations by leveraging local physical geometric symmetry in environments with gravity and a body-level control module to obtain the final policy. Building upon these, the global state space is effectively compressed.\nInput Processing The observation input for agent i at time t is formulated as $o_i = \\{\\{s_{i,k}\\}_{k=1}^{K_i}, s_{j,1} | j \\in \\Omega \\setminus \\{i\\}\\}$. To adhere to the constraints of physical geometric symmetry, the state $s_{i,k}$ is subdivided into directional geometric vectors $\\mathcal{Z}_{i,k}$ and scalar features $h_{i,k}$. Elements in $\\mathcal{Z}_{i,k}$ will rotate according to the transformation $g \\in \\mathcal{O}_{\\tilde{g}}(3)$, while those in $h_{i,k}$ remain unchanged. Specifically, in our 3D environment, $\\mathcal{Z}_{i,k} \\in \\mathbb{R}^{3\\times 3}$ comprises the position $p_{i,k} \\in \\mathbb{R}^3$, positional velocity $v_{i,k} \\in \\mathbb{R}^3$, and rotational velocity $\\omega_{i,k} \\in \\mathbb{R}^3$. Here, $p_{i,k}$ is transformed into a translation-invariant representation by redefining it as $p_{i,k} - c$, where $c = \\frac{1}{N+M}\\sum_{j \\in \\Omega} p_{j,1}$. This operation subtracts $c$, the average root position of all entities, thereby ensuring translation invariance. The scalar features $h_{i,k} \\in \\mathbb{R}^{13}$ include the rotation angles $\\kappa_{i,k}$, $\\zeta_{i,k}$, $\\delta_{i,k}$ of the joint axes and their corresponding ranges, along with a 4-dimensional one-hot vector indicating the type of body such as \"torso\", \"limb\u201d, or \u201cball\u201d. The direction of gravity $\\tilde{g}$ is set to be along the z-axis.\nTask Assignment In multi-entity environments, it is crucial to manage complex interactions among entities. Initially, we consider a fully connected entity-level graph $\\mathcal{G} = (V, E)$, with $V = \\Omega$ and $E = \\{(i, j) : i, j \\in \\Omega, i \\neq j\\}$. To decouple local transformations from the overall structure, we introduce task assignment which dynamically adjusts the edges, forming local graphs of associated entities. The graph is then redefined as a task assignment entity-level graph, where $E = \\{(i, j) : i, j \\in \\Omega, C(i) = C(j)\\}$ and C"}, {"title": "4. Multi-entity Benchmark (MEBEN)", "content": "In this subsection, we present the details involved in constructing our environments: Team Reach and Team Sumo, as illustrated in Figure 3.\nAgents In our studies, we leverage a variety of morphologies, including ants, claws, and centipedes from MxT-Bench, as well as unimals. Notably, centipedes and unimals exhibit asymmetrical forms, potentially influencing the overall system's symmetry and dynamics. This diverse range of agent morphologies enables a nuanced exploration of agent dynamics in multi-entity, morphology-based RL environments."}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nBaselines We compare our method SHNN, against mainstream neural networks, particularly MLP, and its variant utilizing heading normalization tricks, denoted as MLP+HN. Please refer to Appendix C.3 for details about baselines.\nMetrics 1. Team Reach: Success Rate = #success_episode / #evaluation_episode. 2. Team Sumo: For each team, Win Rate = #win_episode / #evaluation_episode.\nEach experiment is conducted with 10 seeds to report the Success (or Win) Rate over #evaluation_episode of 1024.\nImplementations The environments used in this work are detailed in Table 4. To ensure fairness, all baselines, ablations, and our SHNN model use the same input information and employ MAPPO as the training algorithm for MARL. SHNN is developed based on the MxT-bench codebase, leveraging JAX and Brax for efficient, hardware-accelerated simulations. The value of the maximum timesteps per episode is 1000. Hyperparameter details are in Appendix C.9. Within the same team, agents share only the weights of entity-level message passing, not the body-level MLP. For the Team Reach environments, we construct a directed graph by dynamically assigning a fixed ball to each agent. For the Team Sumo environments, we assign each agent an opponent from the opposing team and assign each with the arena's center ball. Additionally, in Team Sumo environments, we adopt Bansal et al. (2018)'s approach where teams employing baseline methods and SHNN compete within an arena.\n5.2. Evaluations in Diverse Environments\nWe begin by evaluating our method on diverse multi-entity tasks in 3D environments. Team Reach in Figure 4: 1. The MLP generally fails to achieve meaningful returns in most Team Reach cases, which is due to its vulnerability to local extremes within the expansive exploration space of our environments. 2. While MLP enhanced with Heading Normalization (MLP+HN) shows close performance to SHNN"}, {"title": "6. Limitations and Future Works", "content": "To learn policies in 3D multi-entity physical environments, we propose the SHNN, a framework that uniquely integrates task assignment with local subequivariant message passing in a hierarchical structure. However, in the Team Sumo environments, as shown in Figure 11, the impact of assignment is not as significant as that of equivariance. Therefore, the interdependence of task assignment and equivariance necessitates a novel co-learnable formulation. Our model's reliance on the $E_g(3)$ symmetry encompasses a broad range of physical interactions but does not extend to non-Euclidean or highly irregular environments not governed by classical physics. Additionally, our model predominantly leverages state-based inputs, which are often costly to acquire in practice due to the need for precision sensors. Addressing the challenge of applying our equivariance-focused approach to vision-based inputs remains an open area for future research."}, {"title": "A. Proofs", "content": "In this section, we theoretically prove that our proposed SHNN ensures the final output action and critic value preserve the symmetry as desired.\nTheorem A.1. The learned entity-wise rotation matrix, denoted as $O_i = OP(\\tilde{u_i})$, are $SO_{\\tilde{g}}(3)$-equivariant, satisfying any transformation $g \\in SO_{\\tilde{g}}(3)$, $g \\cdot O_i = OP(g \\cdot \\tilde{u_i})$.\nProof. To prove that the entity-wise rotation matrix $O_i = OP(\\tilde{u_i})$ are $SO_{\\tilde{g}}(3)$-equivariant, we need to show that under any transformation $g \\in SO_{\\tilde{g}}(3)$, the transformation of $O_i$ through g is equivariant to the rotation matrix obtained from the transformed vectors $g \\cdot u_i$.\nLet g be a transformation in $SO_g(3)$, which includes a rotation O along the direction of $\\tilde{g}$. Specifically, the transformation is applied as follows:\n$g \\cdot O = OO_i,$\n$g \\cdot \\tilde{u_i} = O\\tilde{u_i},$\nLet $O^* = OP(g \\cdot \\tilde{u_i})$. By the properties of the orthogonalization process, we have:\n$e^*_1 = \\frac{O\\tilde{u_i} - (O\\tilde{u_i}, e_3)e_3}{||O\\tilde{u_i} - (O\\tilde{u_i}, e_3)e_3||},$\n$e^*_2 = e^*_1 \\times e_3,$\n$e^*_3 = [0,0,1]^T$.\nSince $O\\tilde{g} = \\tilde{g}$, $O^TO = I$ and $det(O) = 1$, it preserves inner products and norms. And, the cross product obeys the following identity under matrix transformations: $(M\\vec{a}) \\times (M\\vec{b}) = (det M) (M^{-1})^T (\\vec{a} \\times \\vec{b})$. Therefore, $e_3^* = Oe_3$, $||O\\tilde{u_i}|| = ||\\tilde{u_i}||$ and $(O\\tilde{u_i}, Oe_3) = (u_i, e_3)$. This implies:\n$e^*_1 = \\frac{O\\tilde{u_i} - (u_i, e_3)Oe_3}{||O\\tilde{u_i} - (u_i, e_3)Oe_3||} = \\frac{O(\\tilde{u_i} - (u_i, e_3)e_3)}{O||\\tilde{u_i} - (u_i, e_3)e_3||} = O\\frac{(\\tilde{u_i} - (u_i, e_3)e_3)}{||\\tilde{u_i} - (u_i, e_3)e_3||} = Oe_1,$\n$e^*_2 = Oe_1 \\times Oe_3 = det(O)O(e_1 \\times e_3) = Oe_2,$\n$e^*_3 = [0, 0, 1]^T = Oe_3$.\nTherefore, with $O^* = OP(g \\cdot \\tilde{u_i}) = [Oe_1, Oe_2, Oe_3] = OO_i = g \\cdot O_i$, we confirm the $SO_{\\tilde{g}}(3)$-equivariance of $O_i$. Besides, for reflection transformations $O$, characterized by $O^TO = I$ and $det(O) = -1$, $e_2^* = -Oe_2 \\rightarrow O^* \\neq OO_i$ where the equivariance is disrupted due to the properties of the cross product. Hence, although we utilize an $O_{\\tilde{g}}(3)$-equivariant message passing network in Equation (8), the orthogonalization process can transform a $O_{\\tilde{g}}(3)$-equivariant vector into a $SO_{\\tilde{g}}(3)$-equivariant matrix.\nAs for the actor and critic, we additionally have the following corollary.\nCorollary A.2. Let $\\pi_{\\theta_i}$, $V_{\\theta}$ be output of the actor and the critic of SHNN with $\\mathcal{Z}, g, h$ as input. Let $\\pi_{\\theta_i}^*$, $V_{\\theta}^*$ be the actor and critic with $O\\mathcal{Z}, O\\tilde{g}, h$ as input, $O \\in SO_{\\tilde{g}}(3)$. Then, $(\\pi_{\\theta_i}^*, V_{\\theta}^*) = (\\pi_{\\theta_i}, V_{\\theta})$, indicating the output actor and critic preserve $SO_{\\tilde{g}}(3)$-invariance.\nProof. Given the subequivariance of Equation (8) as per , we have $h^* = h'$.\nHence,\n$V_{\\theta}^* = \\sigma_V(\\{h_i^*\\}_{i=1}^{N+M})$  (14)\n$\\quad = \\sigma_V(\\{h_i\\}_{i=1}^{N+M})$  (15)\n$\\quad = V_{\\theta}$.  (16)"}, {"title": "B. Related Works", "content": "Morphology-based RL In the field of reinforcement learning (RL), recent years have witnessed the emergence and evolution of morphology-based approaches, particularly within the context of inhomogeneous morphology settings. This setting is distinguished by varying state and action spaces across different tasks. Morphology-based RL decentralizes the control of multi-joint robots by learning a shared policy for each joint, applying a multitude of message-passing strategies. To tackle the challenges of the inhomogeneous setting, methods such as NerveNet , DGN and SMP represent the agent's morphology as a graph and implement Graph Neural Networks (GNNs) for their policy networks. On the other hand, Amorpheus , SWAT , ModuMorph , SGRL , and Solar opt for transformers over GNNs for direct communication. Both approaches demonstrate that graph-based policies offer significant benefits compared to conventional monolithic policies. Furthermore, MxT-Bench serves as a testbed for morphology-task generalization, though its primary focus remains on single-agent tasks. Our work, however, expands upon these methodologies to encompass multi-entity environments, facilitating comprehensive exploration of dynamics and interactions among a variety of entities, including agents with intricate morphologies, objects, and other crucial factors affecting system symmetry and behaviors.\nMulti-Agent RL In the realm of Multi-Agent RL (MARL), decision-making is predominantly guided by frameworks such as the decentralized partially observable Markov decision process (Dec-POMDP) , with strategies typically categorized into cooperative , competitive , and mixed interactions . Addressing challenges like high-dimensional action spaces and the necessity for agent coordination, decentralized learning aims to independently optimize each agent's policy. While scalable, this approach often struggles with issues of instability, especially under conditions of partial observability. On the other hand, centralized learning offers a more comprehensive view but faces computational hurdles in complex environments. The Centralized Training with Decentralized Execution (CTDE) paradigm emerges as a balanced solution to these extremes. It allows agents to make decisions based on local observations, while global state information is utilized in the construction of the value function. This synergy of individual autonomy and collective insight enhances the efficacy of the decision-making process. Notably, Yu et al. revisited the application"}, {"title": "C. More Experimental Details", "content": "C.1. Subequivariant Function\nWe resort to subequivariant function with $\\mathcal{Z}, \\tilde{g}, h$ as input to instill desired geometric symmetry into the model:\n$\\mathcal{Z}',h' = M_{W_1}W_2,\\tilde{g} \\right),\\rightarrow RM^{m \\times m}$ producing $W_\\tilde{g} \\in \\mathbb{R}^{m \\times m}$.\nC.2. Environments\nIn this subsection, we present the technical details involved in constructing our challenging MEBEN. The environments utilized in this work are listed in Table 4.\nAgents In our studies, we leverage a variety of morphologies, including ants, claws, and centipedes from MxT-Bench , as well as unimals. Significantly, the asymmetrical forms of centipedes and unimals have the potential to influence the overall system's symmetry and dynamics. This diversity in agent morphologies facilitates a nuanced exploration of agent dynamics within multi-entity, morphology-based RL environments.\nTeam Reach We expand the \u201cReach\u201d task from a single-agent challenge to a collaborative \u201cTeam Reach\" task, as shown in Figure 9. 1. Initial Conditions. Entities set \u03a9 include N agents and M fixed balls, N > M. Within an area of radius R, we randomly position N agents and M fixed balls, also setting the initial orientations of the agents randomly. Specific details for N, M, R, and the agents' morphology are provided in Table 4. 2. Termination. The goal of this"}, {"title": "C.4. Task Assignment", "content": "Greedy Bipartite Matching Algorithm The Greedy algorithm is a fundamental optimization algorithm, this part is about the process of applying the greedy matching algorithm to allocate a fixed ball (or an opposing agent) to each agent in the environments. Algorithm 1 provides a pseudo-code implementation of the Greedy Bipartite Matching algorithm."}, {"title": "C.5. Additional Ablations", "content": "Additional Ablations on Assignment In the Team Sumo environments, task assignment yields limited improvement. This could be attributed to the environments' complex mixture of cooperative and competitive dynamics, which are challenging to effectively decouple using bipartite matching methods, as demonstrated in Figure 11.\nAdditional Ablations on Equivariance In the Team Sumo environments, task assignment leads to the formation of local graphs composed of triplets of agents from both sides and a fixed ball, making it challenging to determine the specific goal orientation learned by equivariant networks. Consequently, our method was primarily compared against HNN+HN. The experimental outcomes align with the main text findings, as demonstrated in Figure 12.\nImportance of Local Symmetry By comparing the red and blue lines in the left plot of Figure 11 and in the right plot of Figure 12, it becomes evident that in the Team Sumo environments, the impact of assignment is not as significant as that of equivariance. Therefore, careful design of assignment strategies is crucial to harness the advantages offered by equivariance effectively."}, {"title": "C.6. Analyses of Morphology-shared Policy", "content": "We provide an extended example of our methodology applied to morphology tasks. Previous works have achieved generalization across agents with different morphologies using morphology-aware Graph Neural Networks .\nOur variant, referred to as SHGNN, replaces the body-level MLP with body-level message passing. This adaptation enables the learning of a shared policy network across different agents.\nFirst, we utilize the morphology topology information of each agent i to construct a body-level inner-entity graph, denoted as $G_i = (V_i, E_i)$. For each body, $k \\in V_i$, input node features are initialized using the body's state. Specifically, $Z_k$ is assigned as $Z_{i,k}$, and $h_k$ is set as $\\[h_{i,k}, h_i, p_k]$, where $\\[]$ is the stack along the last dimension and $p_k$ represents the projection of the coordinate $p_{ik}$ onto the z-axis. In a body-level overview, we denote our body-level message passing as the function $\\phi$ that updates each body's node features given the input node features of all body and graph connectivity:\n$\\{(Z_k, h_k)\\}_{k=1}^{K_i} = \\phi_b(\\{(Z_k, h_k)\\}_{k=1}^{K_i}, E_i)$.  (27)\nNotably, the unfolding of $\\phi_b$ is similar to that of $\\phi_o$, and will not be elaborated further here.\nFor each agent i, the invariant actor policy $\\pi_{\\theta_i}$ is defined as\n$\\pi_{\\theta_i} = \\{\\pi_{\\theta_{i,k}}\\}_{k=2}^{K_i} = \\{\\sigma_{\\pi}(OZ_{i, k})\\}_{k=2}^{K_i}$,  (28)\nwhere $\\sigma_{\\pi}$ is a linear layer with bias. Here, $\\pi_{\\theta_i} \\in \\mathbb{R}^{2 \\times (K_i -1)}$ represents the location and scale parameters of a Normal Tanh Distribution for the $(K_i \u2013 1)$ actuators of agent i. Each actuator samples its corresponding torque $a_{i,k} \\in \\[-1,1]$ from this"}, {"title": "C.7. Model Comparison: Parameters and Training Time", "content": "Table 5 compares the parameters and Training Wall Times of our model with several model variants in the 3_ant_claw_centipede Team Reach Environments. Here, total timesteps are 50M. Since Transformer and GNN can share parameters across different morphologies, they have fewer parameters."}, {"title": "C.8. Equivariance Test", "content": "We conduct an experiment, as depicted in Figure 16, to evaluate the rotational generalization of both the baselines and our method. Training and evaluation are conducted in the fixed initial conditions of the 1_centipede and 2-ants Team Reach environments. Additionally, we conduct an evaluation with a 180\u00b0 rotation of the entire scene. The results presented in Table 6, with detailed curves provided in Figure 17, illustrate that both our SHNN method and the MLP+HN approach exhibit stable performance pre- and post-rotation. Conversely, MLP demonstrates rotational generalization for symmetric morphologies, such as ants, yet entirely lacks this capability with asymmetric morphologies like centipedes, evidenced by the underline in Table 6 and the green line in the first plot of Figure 17. These results empirically validate that both our SHNN method and the HN approach are rotation equivariance, which can robustly generalize to unseen rotation transformations."}]}