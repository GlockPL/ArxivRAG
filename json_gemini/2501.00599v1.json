{"title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM", "authors": ["Yuqian Yuan", "Hang Zhang", "Wentong Li", "Zesen Cheng", "Boqiang Zhang", "Long Li", "Xin Li", "Deli Zhao", "Wenqiao Zhang", "Yueting Zhuang", "Jianke Zhu", "Lidong Bing"], "abstract": "Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.", "sections": [{"title": "1. Introduction", "content": "Multi-modal Large Language Models (MLLMs) [2, 8, 19-22, 27, 35] have demonstrated remarkable general-purpose capabilities for open-world image understanding through language-based dialogues over the past year. In constant, extending their capabilities to the video domain presents unique challenges, as videos comprise dynamic sequences that not only showcase visual content but also convey the timing and relationships among various events and objects.\nCurrently, existing Video Large Language Models (Video LLMs) [9, 15, 17, 26, 50, 54] primarily focus on holistic scene understanding. Unfortunately, these approaches often fall short in capturing the nuanced elements of video content. For instance, they often struggle to focus on user-specific objects, such as accurately describing a particular object. The ability to discern such finer details in video content is crucial for applications that require precise object description, detailed event analysis, and predictive reasoning in dynamic environments.\nTo achieve fine-grained object understanding, numerous efforts have been devoted to image-based MLLMs, such as GPT4ROI [52], Ferret [44, 51] and Osprey [46]. These methods typically utilize a region encoder to obtain object-level embeddings, adapting them to LLMs for static image region-text alignment. In contrast, research on video-based object understanding remains limited. Some works [41, 45] directly convert the bounding box coordinates of object from specific frames into textual prompts to assist the LLM in identifying referred objects within the video. However, these methods are plagued by impractical object referring and suffer from imprecise regional understanding. Alternatively, Artemis [32] employs an external Rol tracker to capture an object across the video and extract box-level features for aligning with the LLM. However, it primarily focuses on single-object referencing using coarse box-level representations, which restricts its capacity to handle complex tasks, such as analyzing relationships among multiple objects and performing intricate reasoning. Therefore, developing an interactive Video LLM that facilitates a comprehensive understanding of objects within video represents a nontrivial research challenge."}, {"title": "2. Preliminary", "content": "To attain precise regional comprehension, MLLMs can be incorporated with instance-level visual representations. This integration allows models to generate semantic understandings that focus on specific regions. As for image-based MLLMs, recent researchs [4-6, 11, 13, 33, 39, 42, 44, 46, 47, 49, 51, 52, 55] has demonstrated a significant trend to enable the image referring with spatial visual prompts. In contrast, research focused on video-based regional understanding across sequential scenes is relatively limited.\nThe video referring task involves comprehending user-specific regions at designated moments or a time periods within a video [32, 41, 45]. The basic video referring task focuses on captioning, while more complex tasks involve reasoning about the relationships between objects, and inferring their future states or interactions. Video referring tasks can significantly enhance the functionality and applicability of video analysis for Video LLM across multiple domains, such as navigation, surveillance, and interactive robotics."}, {"title": "2.1. Background and Video-referring Task.", "content": null}, {"title": "2.2. Task Formulation.", "content": "For basic video object referring, the model processes questions phrased as \u201cWhat is <object> doing in this video?\u201d, where the <object> is specified by the user at a specific moment t or over a duration of time. In more complex scenarios involving various object relationships, the model requires multiple user-defined regions, such as <object1>, <object2> and <objectK> along with the corresponding questions, like \u201cHow do <object1> and <object2> interact with each other?\". To address these nuanced regional and temporal tasks, we provide a unified problem formulation.\nFor a given video $V \\in \\mathbb{R}^{N\\times W\\times H\\times C'}$, where N, W, H, C denote the frame number, height, width and channels, respectively. We define all the <object> as R, where $R = \\{R_1, R_2, ..., R_n\\}$. Here, n represents the total number of objects specified by the user. $R_j$ is expressed as $R_j = \\{r_{ij} | i \\in T\\}$, with $r_{ij}$ representing a region within a single frame, and T being a set containing one or multiple timestamps. For a Video LLM, the model optimization process aims to maximize the log-likelihood of generating text conditioned on V, R, and text-based prompt x across the entire training dataset to produce the desired output:\n$L = \\sum_{(V,R,x,y)} \\log P(y | V, R_1, ..., R_n, x),$\""}, {"title": "3. VideoRefer Suite", "content": "Our VideoRefer Suite consists of three crucial components: a comprehensive dataset, VideoRefer-700K, containing high-quality instruction-following object-level annotations; a Video LLM, VideoRefer, capable of pixel-level regional and temporal comprehension; and an evaluation benchmark, VideoRefer-Bench, developed to evaluate models across various video referring tasks."}, {"title": "3.1. VideoRefer-700K Dataset", "content": null}, {"title": "3.1.1 Multi-agent Data Engine", "content": "We develop an automatic multi-agent data engine to create VideoRefer-700K, a large-scale and high-quality object-level video instruction-following dataset. Specially, we utilize off-the-shelf expert models that excel in tasks such as captioning, detection, segmentation and summation as collaborative agents to carefully create diverse types of object-level instruction data. As illustrated , our curation pipeline involves five components: (i) Analyzer for noun extraction; (ii) Annotator for object-level caption generation; (iii) Segmentor for mask generation; (iv) Reviewer for correspondence verification; and (v) Refiner for summarization&refinement. This multi-agent data engine effectively eliminates noisy or irrelevant contexts, ensuring that the data maintains its accuracy and relevance.\nAnalyzer for Noun Extraction. Considering that most available video datasets contain the short scene-level caption, we begin by analyzing the raw captions to accurately capture the nouns within the sentences, i.e., objects occurred in the video scene. To achieve this, we employ an Analyzer to extract nouns, encompassing both subjects and other relevant nouns. The Qwen2-Instruct-7B model [43] serves as our analytical tool in this process.\nAnnotator for Object-level Caption Generation. To obtain detailed descriptions of the extracted nouns, we employ a general video understanding model as an annotator. We prompt the model to provide comprehensive descriptions focused specifically on the objects, rather than the holistic narrative of the whole video. To enhance accuracy and detail, we query the model twice: emphasizing dynamic actions&movements, and highlighting static appearances&states, respectively. Specifically, we filter out static actions related to the subjects to maintain variability and dynamism in the videos. The open-source InternVL2-26B model [8] serves as our annotator.\nSegmentor for Mask Generation. To acquire pixel-level masks as object-level region representations for each extracted noun, we first select a random frame from the video and extract the bounding box using Grounding- DINO [23] through open-set grounding, with the extracted noun serving as the input text prompt. Subsequently, HQ-SAM [14] is employed to generate the high-quality mask based on the corresponding box prompt. To accommodate multi-frame input, we further generate masks for each video frame using SAM 2 [34].\nReviewer for Correspondence Verification. To address potential errors and mismatches in this data construction pipeline, we introduce a Reviewer to verify the correspondence between masks and descriptions. Initially, we employ Osprey [46] to provide a region-level description for a specific frame. The Reviewer then assesses whether the descriptions from Osprey and the Annotator refer to the same object. After this filtering process, we retain only 40% of samples to ensure accuracy. Qwen2-Instruct-7B model [43] is chosen as the Reviewer for this task, due to its efficiency and suitability for handling the complexity of this process.\nRefiner for Summarization&Refinement. Finally, we utilize a reliable Refiner, GPT-40 [28], to summarize and refine the temporal and appearance-related captions generated by the annotator. This process aims to further eliminate repetition and hallucinations, ensuring a coherent and accurate final object-level instruction-following dataset."}, {"title": "3.1.2 Data Characteristics", "content": "By leveraging our multi-agent data engine, we meticulously create three primary types of object-level video instruction data: detailed captions, short captions, and multi-round question-answer (QA) pairs.\nObject-level Detailed Caption. We utilize a subset of large-scale Panda-70M [7], which has a short caption for each video. We generate 125K high-quality object-level detailed captions through our full multi-agent data engine.\nObject-level Short Caption. To generate short captions, primarily for aligning object-level encoder with the LLM for pre-training, we employ a portion of the pipeline, which only includes the Analyzer and Segmentor. Specifically, in the Analyzer, we extract only singular subject nouns, enabling the reusing of raw captions for short descriptions. Using this approach, we produce 500K short captions.\nObject-level QA. To generate instruction data that explicitly specifies particular objects or their relationships, we collect MeViS [10], Ref-YouTube-VOS [36] and A2D-Sentence datasets. Both provide reliable short descriptions with mask annotations for each object region. By utilizing these short descriptions and masked videos, we first employ Annotator to generate object-level descriptions for each region, and then employ Refiner to generate QA pairs related to the objects within the videos, using a variety of prompts. Three types of region-based QA data have been created: (i) Basic Questions: These cover object types, attributes, actions, locations, and interactions over time. (ii) Reasoning Questions: These require reasoning and background knowledge to explain events without relying on specific visual details. (iii) Future Predictions: These involve anticipating future actions or events related to a given object. We generate 75K QA pairs in total."}, {"title": "3.2. VideoRefer Model", "content": null}, {"title": "3.2.1 Overall Architecture", "content": "In this section, we introduce the VideoRefer framework, which ensures the next token predictions of Video LLM, enabling fine-grained mask-level comprehension at any specific regions and any timestamps for a given video. Given that the current Video LLM already exhibits strong general scene-level video understanding capabilities, we develop our model upon a well-established Video LLM, VideoLLaMA2.1 [9]. Our primary innovation is to introduce a versatile and unified spatial-temporal object encoder to obtain object-level representations across video scenes.\nThe overall architecture of our framework is illustrated in . VideoRefer adopts a visual encoder and STC connector [9] to encode the global scene-level visual representations, a pretrained text tokenizer to capture the language embeddings, and an instruction-following LLM for language decoding. To achieve video referring, we present a versatile and unified spatial-temporal encoder, denoted as REnc, to derive object-level representations. For a specific object $R_j \\in R$, we define $R_j = \\{r_{ij} | i \\in T\\}$, where each $r_{ij}$ represents a unified 2D binary mask M designed to accommodate free-form input regions, assigning a value of 1 inside the region and 0 outside. The set of objects R, along with the image feature map Z extracted from the shared visual encoder, is then fed into the introduced object encoder REnc, which generates enriched object-level tokens, expressed as $T_R = REnc(R, Z)$. Finally, the interleaved scene-level tokens $T_z$, object-level tokens $T_R$ and linguistic tokens $T_x$ are sent to the LLM to obtain the fine-grained semantic understandings Y, formulated as $Y = \\Phi(T_z,T_R, T_x)$, where $\\Phi$ denotes the LLM."}, {"title": "3.2.2 A Versatile Spatial-Temporal Object Encoder", "content": "To support various spatial-temporal video understanding tasks, our presented object encoder not only captures mask-level spatial features within the single frame at a specific timestamp, but also aggregates temporal information across multiple frames over a duration of time. Consequently, we devise two modes for our object encoder: single-frame and multi-frame. For the sake of brevity for better illustration, we use a single object Rj as an example. If multiple objects are specified by the user, we adopt the same manner to extract features for each object individually.\nSingle-Frame. For single-frame mode, the input consists of a randomly selected frame along with the corresponding regions specified by the user in that frame. Here, T contains only a randomly chosen timestamp. To generate the object-level token representations, we present the Spatial Token Extractor. In detail, the image feature is initially extracted by the shared visual encoder to generate the global image feature $F_1 \\in \\mathbb{R}^{1\\times H_1\\times W_1\\times D_1}$, where $H_1, W_1, D_1$ denote the height, width and dimension of the image feature, respectively. Each binary mask M of an object is then resized to match the shape of the image feature. We utilize the Mask Pooling operation upon image feature to extract object-level spatial feature $F_o \\in \\mathbb{R}^{1\\times D_1}$ for each mask, which pools all features within the region M to generate an object-level representation. Finally, an MLP layer is employed to adapt and produce the object-level token $O \\in \\mathbb{R}^{1\\times C}$ for each object region.\nMulti-Frame. In the multi-frame mode, the input consists of a list of selected frames from the video, accompanied with their respective object regions, i.e., T contains a list of timestamps from the video. The frame-level feature is extracted using the shared visual encoder to generate the image feature $F_1 \\in \\mathbb{R}^{k\\times H_1\\times W_1\\times D_1}$, where k represents the number of selected frames. We then employ the Spatial Token Extractor to generate the object-level tokens for each frame. Hence, we obtain the object tokens $O \\in \\mathbb{R}^{k\\times C}$.\nTo aggregate distinct temporal object-level representations across multiple frames over a time duration while minimizing redundant tokens, we propose the Temporal Token Merge Module, which is designed to effectively capture essential object-level tokens across the temporal dimension. Specifically, starting with spatial object tokens $O \\in \\mathbb{R}^{k\\times C}$, we first compute the cosine similarity between each pair of adjacent tokens, formulated as:\n$S_{m,m+1} = \\frac{O_m \\cdot O_{m+1}}{|| O_m || || O_{m+1}||}, \\quad 0 < m < k.$"}, {"title": "3.3. VideoRefer-Bench", "content": "To comprehensively evaluate the models' capability on video-based regional comprehension, we have developed a benchmark named VideoRefer-Bench. This benchmark assesses the models in two key areas: Description Generation, corresponding to VideoRefer-BenchD, and Multiple-choice Question-Answer, corresponding to VideoRefer-Bench. and provide exemplar visual illustrations and data characteristics, respectively."}, {"title": "3.3.1 VideoRefer-BenchD", "content": "We introduce a sub-benchmark, VideoRefer-Bench specifically designed to evaluate the description generation performance of video-based referring models. The benchmark comprises a total of 400 curated data entries. We curated the test set based on Panda-70M [7], employing the pipeline described in Section 3.1, followed by a meticulous human check. Furthermore, we developed an evaluation pipeline utilizing the GPT-40 model. This pipeline rigorously assesses various capabilities of the model by assigning scores to the generated predictions on a scale range from 0 to 5 across the following four dimensions:\n\u2022 Subject Correspondence (SC): This dimension evaluates whether the subject of the generated description accurately corresponds to that specified in the ground truth.\n\u2022 Appearance Description (AD): This criterion assesses the accuracy of appearance-related details, including color, shape, texture, and other relevant visual attributes.\n\u2022 Temporal Description (TD): This aspect analyzes whether the representation of the object's motion is consistent with the actual movements.\n\u2022 Hallucination Detection (HD): This facet identifies discrepancies by determining if the generated description includes any facts, actions, or elements absent from reality, like imaginative interpretations or incorrect inferences."}, {"title": "3.3.2 VideoRefer-Bench", "content": "The other sub-benchmark VideoRefer-Bench is designed to evaluate the proficiency of MLLMs in interpreting video objects. We meticulously curated a dataset comprising 198 videos sourced from various datasets, including DAVIS-2017 [31] and the test set of MeViS [10]. To facilitate a robust evaluation, we annotated a set of 1,000 high-quality multiple-choice questions. These questions are crafted to assess different dimensions of understanding, including Basic Questions, Sequential Questions, Relationship Questions, Reasoning Questions, and Future Predictions. The annotations were performed by researchers with extensive research experience in vision-language learning. Importantly, each QA pair is required to be explicitly linked to a specific video region. This ensures that the MLLMs cannot provide answers without actually analyzing the video or the designated object."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Implementation Details", "content": "We adopt siglip-so400m-patch14-384 [48] as the vision encoder, Qwen-2 [43] as the LLM. The AdamW [24] is used as the optimizer and the cosine annealing scheduler [25] is used to adjust learning rate. We use a hybrid strategy including both single-frame and multi-frame modes during training. We leverage a progressive training scheme, which consists of image-text alignment pre-training (Stage 1), region-text alignment pre-training (Stage 2), high-quality knowledge learning (Stage 2.5) and visual instruction tuning (Stage 3) stages, respectively. Please refer to the Appendix for detailed introduction to each stage. At the first and second stages, we set global batch size to 256 and learning rate to 1\u00d710-3 for one epoch. In stage 2.5 and stage 3, the learning rate is reduced to 2\u00d710-5 with a global batch size of 128 for one epoch. Unless otherwise specified, all models adopt the 7B LLM."}, {"title": "4.2. Main Results", "content": "To evaluate the efficacy of our VideoRefer model, we conduct experiments on both video referring tasks and general video understanding tasks to demonstrate its capabilities."}, {"title": "4.2.1 Video Referring Tasks", "content": "VideoRefer-BenchD. We compare our approach on VideoRefer-BenchD with the previous generalist models, including GPT-40 [28], GPT-40-mini [28], InternVL2-26B [8] and Qwen2-VL [43], and specialist models for object-level understanding, including image-level Osprey [46], Ferret [44], and video-level Elysium [41], Artemis [32]. Both single-frame and multi-frame modes are adopted for evaluation. In the single-frame mode, we select the first frame that contains the specific object with its aligned boundary for the generalist models. For image-level region understanding models, we utilize a random frame along with the corresponding region prompt as input. In the multi-frame mode, we uniformly sample 16 frames with mask contours for generalist models. For image-level methods, we obtain the description frame by frame and then generate a summary using GPT-40. For Elysium [41] and Artemis [32], we adhere to the official settings provided in their respective papers. For our VideoRefer, we randomly select a single frame and uniformly sample 16 frames as inputs for the single-frame and multi-frame modes, respectively. presents the comparison results. Our approach achieves the leading average performance in regional-temporal video understanding compared to previous methods in both single-frame and multi-frame modes. Notably, VideoRefer attains top scores of 4.41, 3.27, and 3.03 for Subject Correspondence (SC), Appearance Description (AD), and Temporal Description (TD) in single-frame mode, and scores of 4.44 and 3.04 for SC and Hallucination Detection (HD) in multi-frame mode.\nVideoRefer-Bench. We then compare our VideoRefer against the previous methods on VideoRefer-Bench. Here, we set single-frame mode following settings of VideoRefer-BenchD. As shown in Table 2, our VideoRefer achieves the best average performance with 71.9, which significantly outperforms the previous regional methods. Especially, our approach excels in basic questions, relationship questions, reasoning questions and future predictions with 75.4, 59.3, 89.4 and 78.1 scores with best or second-best places, respectively. These results clearly demonstrate the superiority of our method in spatial-temporal video understanding.\nPrevious Video Referring Metrics. Following the previous state-of-the-art video referring approach, Artmis [32], we further conduct experiments on the test set of HC-STVG [38].\nTable 3 presents the comparison results. Our approach outperforms Artmis [32] by +1.0%, +0.7%, +1.6%, +15.4%, and +2.9% on BLEU4 [29], METEOR [3], ROUGE_L [18], CIDEr [40] and SPICE [1] metrics. These results demonstrate the superiority of our VideoRefer."}, {"title": "4.2.2 General Video Understanding", "content": "To demonstrate the capabilities of our method, we conduct performance evaluation on general video understanding tasks. As shown , VideoLLaMA2.1 [9] achieves scores of 54.9% on Perception-Test [30], 57.3% on MVBench [15], and 54.9%/56.4% on VideoMME [12]. Based on that, our VideoRefer exhibits performance gains of +1.4%, +2.3%, and +1.0%/+1.2%, respectively. In contrast, Artemis demonstrates subpar performance. These results clearly indicate that our approach not only excels in object-level analysis, but also enhances the ability of general video understanding."}, {"title": "4.3. Ablation Study", "content": "Single-frame vs. Multi-frame. We first validate the impacts on the single-frame and multi-frame modes, i.e. with or without Temporal Token Merge (TTM) module to encode the multi-frame sequences during the inference. As shown , our approach utilizing multi-frame mode exhibits improvements over the single-frame mode in both VideoRefer-BenchD and VideoRefer-Bench across all metrics. Notably, for sequential relation-based metrics, including Temporal Description (TD), Sequential Questions (SQ), and Relationship Questions (RQ), as well as hallucination-related metrics such as Hallucination Detection (HD), multi-frame mode showcases the superiority.\nAblation on VideoRefer-700K Dataset. summarizes the ablation results for various data types in the constructed VideoRefer-700K dataset. The results indicate that using a short description yields a score of 2.43 on BenchD and 68.3 on Bench, along with an MVBench score of 58.0. Incorporating question-answering (QA) data improves the performance to 2.45 for BenchD and 71.7 for Bench, while maintaining an MVBench score of 58.4. Notably, the method employing detailed descriptions achieves the best results, with scores of 3.42 on BenchD, 71.9 on Bench, and 59.6 on MVBench. These results demonstrate that the inclusion of more comprehensive data significantly enhances overall performance.\nImpacts of Different Union Numbers in TTM. The Temporal Token Merge (TTM) Module is designed to capture essential object-level tokens across the temporal dimension in multi-frame mode. visualizes the similarity scores between adjacent object token pairs. It is evident that most adjacent tokens exhibit high similarity, making it necessary to merge those tokens with significant similarity. We conducted ablation experiments using temporal and sequential metrics to investigate the effects of varying numbers of token unions u. The experimental results are detailed in . Notably, with u = 4, VideoRefer achieves the best performance in Hallucination Detection (HD) and Sequential Questons (SQ), and ranks second in Reasoning Questions (RQ). We adopt u 4 to strike a balance between performance and token costs in our approach."}, {"title": "5. Related Works", "content": null}, {"title": "5.1. Video Large Language Models", "content": "Large Language Models (LLMs) have revolutionized the field of artificial intelligence by proving their capability to tackle diverse tasks related to language comprehension and generation. To fully leverage the potential of LLMs for visual understanding, researchers have increasingly turned their attention to image-based Multimodal Large Language Models (MLLMs) [2, 8, 16, 19-22, 27, 53], which integrate language and visual data within a unified feature space. This integration has emerged as a significant area of research focus. In parallel, Video Large Language Models (Video LLMs) [9, 17, 26, 50, 54] have garnered increasing attention fueled by advancements in image-based MLLMs. Most Video LLMs primarily follow the trend of utilizing pre-trained vision models to extract sequence-based information from videos, which is then interleaved with textual embeddings for LLM to generate responses [37]. Despite their promising results, current Video LLMs still face challenges in fine-grained regional and temporal understanding."}, {"title": "5.2. Regional Understanding with MLLMs", "content": "To attain fine-grained regional object-level comprehension, MLLMs can be incorporated with instance-level visual representations. This integration allows models to generate semantic understandings that focus on specific regions. In the context of image-based MLLMs, recent researchs [4-6, 11, 13, 33, 39, 42, 44, 46, 47, 49, 51, 52, 55] has demonstrated a significant trend to enable the image referring with spatial visual prompts. In contrast, research focused on video-based regional understanding across dynamic sequence-based scenes is relatively limited. Merlin [45] first explored video-based referring and future reasoning by employing three manually selected frames as visual input, which limits the model's ability to comprehend longer and more intricate scenes. Elysium [41] introduces a million-scale dataset for object-level tasks in videos; however, the provided descriptions tend to be quite simplistic. Another reseach work is Artemis [32], but it primarily emphasizes basic single object descriptions, thereby constraining its capacity to analyze relationships among various objects or perform more complex tasks on specific objects within dynamic sequences. Moreover, Artemis utilizes a sparse bounding box representation, which inadequately captures the nuances of the objects. Compounding these challenges is the lack of large-scale, high-quality region-level video instruction data and benchmarks for thorough evaluation, which further hampers progress in this domain. To address these issues, we introduce the VideoRefer Suite to advance spatial-temporal understanding."}, {"title": "6. Conclusion", "content": "In this work, we introduced the VideoRefer Suite to empower Video LLM for fine-grained spatial and regional video understanding. Three key components have been proposed: 1) VideoRefer-700K: A large-scale, high-quality region-level video instruction data curated by a developed multi-agent engine; 2) VideoRefer: A Video LLM equipped with a versatile spatial-temporal object encoder that includes a Spatial Token Extractor and an adaptive Temporal Token Merge Module to enabling precise sequential regional representation; and 3) VideoRefer-Bench: a comprehensive benchmark that thoroughly evaluates model performance across multiple aspects, ensuring a holistic assessment of spatial-temporal capabilities. Extensive experimental results and analyses have demonstrated the efficacy of our VideoRefer Suite, substantially advancing finer-level video understanding and analysis."}, {"title": "Appendix", "content": null}, {"title": "A. More Qualitative Results", "content": "We provide additional visualization results to emphasize performance across a variety of tasks, such as single-object referring, video relationship analysis, complex reasoning, future prediction, and video object retrieval. Besides, we present the examplar cases to demonstrate the capabilities in general video understanding and image object understanding. showcases these visual examples. A randomly selected mask along with its corresponding frame is used as the region input."}, {"title": "B. Additional Implemental Details", "content": null}, {"title": "B.1. Training Stages", "content": "The training pipeline of our model is structured into four distinct stages. presents the data distribution for each stage.\nStage 1: Image-Text Alignment Pre-training. In this initial pre-training phase, we utilize the same dataset as employed in the first stage of VideoLLaMA2.1 [9]. During this phase, the parameters of both the vision encoder and the large language model are frozen, and training is conducted solely on the STC connector [9], enabling the alignment of image and text modalities.\nStage 2: Region-Text Alignment Pre-training. This stage further incorporates the Object Encoder to capture object-level features based on the weights obtained from Stage 1. The training focus is exclusively on the spatial-temporal Object Encoder to ensure the alignment of intricate object-level features with corresponding language embeddings. We use the generated 500K region-level short descriptions, along with video and image referring segmentation datasets as the training data. During this stage, all the data are processed in single-frame mode to focus solely on alignment.\nStage 2.5: High-Quality Knowledge Learning. At this intermediate stage, the weights of vision encoder remain frozen, while the STC connector, Object Encoder, and LLM undergo fine-tuning. This stage aims to infuse the model with high-quality captioning data, utilizing a diverse dataset that includes 118K image-caption pairs, 30K video-caption pairs, 79K image-level region caption data, and 125K video-level region caption data, inclusive of the detailed descriptions we curated. For object-level video data, we employ a balanced approach, using half in single-frame mode and half in multi-frame mode.\nStage 3: Visual Instruction Tuning. The training configuration for this stage closely mirrors that of Stage 2.5. The primary objective is to enhance the model's ability to accurately interpret user instructions and tackle complex object-level understanding tasks. For video-level data, we utilize the same dataset segments as those used in VideoLLaMA2.1 [9]. For image-level data, we employ the datasets from LLaVA [21]. In addition, we incorporate 294K image-level region data and 115K previously constructed video-level region data to further strengthen the model's capabilities. We also employ a balanced approach using half in single-frame mode and half in multi-frame mode in this stage."}, {"title": "C. More Details of VideoRefer-700K Dataset and Benchmark", "content": null}, {"title": "C.1. Human Evaluation on Reviewer", "content": "In our muliti-agent data engine, we introduce the Reviewer to address potential errors and mismatches, thereby ensuring the quality of our VideoRefer-700K dataset. To assess the effectiveness of the Reviewer, we conducted a manual evaluation of its outputs. We define the evaluation metrics as follows:\n\u2022 TP (True Positives): Items that the Reviewer identified as relevant and accurate, which are confirmed to be true upon manual inspection.\n\u2022 TN (True Negatives): Items that the Reviewer discarded as irrelevant or inaccurate, which are indeed false according to the manual check.\n\u2022 FP (False Positives): Items that the Reviewer considered as true, but are found to be false during manual verification.\n\u2022 FN (False Negatives): Items that the Reviewer discarded as false, but are actually true upon manual review.\nWe randomly sampled 100 items each from both the data discarded and retained by the Reviewer. The detailed results are represented in Table 8, and the corresponding metrics are calculated as follows:\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+ FN} = 0.76,$ (3)\n$Precision = \\frac{TP}{TP + FP} = 0.88,$ (4)\n$Recall = \\frac{TP}{TP + FN} = 0.71,$ (5)\n$F1 Score = 2x \\frac{Precision \\times Recall}{Precision + Recall} = 0.79.$ (6)"}, {"title": "D. Limitations", "content": "In this work, our VideoRefer is designed on object-level spatial-temporal video understanding, without the abilities on grounding. This limitation may affect the applicability of our method in real-world scenarios, which requires identifying and associating objects within their dynamic contexts. In the future work, we will address this gap by integrating grounding abilities into our framework, extending our dataset and benchmark to improve the system's overall utility in practical applications."}]}