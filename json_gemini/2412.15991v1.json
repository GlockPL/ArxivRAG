{"title": "APIRL: Deep Reinforcement Learning for REST API Fuzzing", "authors": ["Myles Foley", "Sergio Maffeis"], "abstract": "REST APIs have become key components of web services. However, they often contain logic flaws resulting in server side errors or security vulnerabilities. HTTP requests are used as test cases to find and mitigate such issues. Existing methods to modify requests, including those using deep learning, suffer from limited performance and precision, relying on undirected search or making limited usage of the contextual information. In this paper we propose APIRL, a fully automated deep reinforcement learning tool for testing REST APIs. A key novelty of our approach is the use of feedback from a transformer module pre-trained on JSON-structured data, akin to that used in API responses. This allows APIRL to learn the subtleties relating to test outcomes, and generalise to unseen API endpoints. We show APIRL can find significantly more bugs than the state-of-the-art in real world REST APIs while minimising the number of required test cases. We also study how reward functions, and other key design choices, affect learnt policies with a thorough ablation study.", "sections": [{"title": "1 Introduction", "content": "REpresentational State Transfer (REST) APIs have become the standard way to interact with Web services and resources. They are used by organisations such as Amazon, Google, and OpenAI to integrate with a wide array of systems, processes, and resources. These APIs consist of operations, specified by a Uniform Resource Locator (URL) accessible by the Hyper Text Transfer Protocol (HTTP), and a series of associated parameters. Atlidakis et al. (2019) show bugs in these operations are typically hard to find, due to the complex interactions, and diversity of APIs. Some bugs have security implications that result in the extraction of information or data manipulation, as summarised in the OWASP API Security Top 10 (2023). Hence, it is crucial to test the robustness of REST APIs using sophisticated techniques.\nAutomated software testing finds bugs or vulnerabilities in an application by detecting abnormal behaviour. This automated process can be referred to as fuzz testing (fuzzing), security testing, or robustness testing. For REST APIs testing, this involves creating new HTTP request test cases or mutating existing templates. Often, this is done at random or following some predefined heuristics. A common criterion to practically estimate performance is code coverage. However, as only a small portion of code contains bugs, it can lead to a high number of executed tests.\nBlack-box testing has been applied to automatically generate test cases for REST APIs (Atlidakis et al. 2020; Liu et al. 2022). While this has lead to improvements in REST API testing, such approaches lack targeted search strategies, or do not harness contextual information. This limits the potential of frameworks due to the unique behaviour of endpoints, their diversity, and varying schema requirements. As a result software testing models can be inefficient, requiring a very large number of test cases to find bugs.\nRecent research used attention-based neural networks to predict mutations in test cases (Lyu, Xu, and Ji 2023). This is a promising approach for REST API testing. Yet, current solutions often only use simplistic feedback from HTTP status codes to determine success, while the main body of HTTP responses is either discarded, or only used for populating data when testing (Corradini et al. 2022b; Liu et al. 2022).\nReinforcement Learning (RL) has shown potential in automated testing, and has been successful learning policies to test compilers (Li et al. 2022a) and web applications (Lee, Wi, and Son 2022; Zheng et al. 2021). These works have demonstrated that using off-the-shelf RL methods such as Deep Q-Networks (DQN) (2015) and Proximal Policy Optimisation (PPO) (2017) can harness feedback to find more bugs, and improve efficiency. However, similar to REST API testing, RL-based software testing often uses simple heuristics as feedback (Lee, Wi, and Son 2022; Li et al. 2022a), which may fail to fully capture the subtleties of the problem. While work from Kim et al. (2023) has shown that RL can find bugs in REST APIs, it suffers from the same issue: not harnessing contextual information for feedback. The key challenge to do so arises from the variation across different APIs and the diversity in individual endpoint responses. Yet, if the information-rich data structure can be used for test case generation, it could provide valuable feedback. To address this challenge, we develop a novel deep RL agent that mutates HTTP requests to find bugs in REST APIs.\nOur RL problem formulation uses a transformer architecture we pre-train to harness JSON and natural language in the HTTP responses, providing feedback to facilitate adaptation to operations after training. We then introduce a Markov"}, {"title": "2 Background", "content": "2.1 Deep Q-Networks\nA Deep Q-Network (DQN) leverages deep neural networks to learn an optimal policy (\u03c0*). In the DQN a Q-network, with parameters \u03b8, computes Q-values \\(q_\\pi(s, ., \\theta)\\) corresponding to actions \\(a \\in A\\) given a state \\(s \\in S\\). Actions are taken via \u03f5-greedy selection, where random actions occur with probability \u03f5, otherwise taking the 'greedy' action dictated by the policy. The state then transitions to a new state \\(s' \\in S\\). A Target Q-network, with parameters \u03b8\u207b predicts the Q-value of this next state \\(q_\\pi(s_{t+1}, ., \\theta^{-})\\). Then using semi-supervised approach the loss is computed from the Q-value predictions of the Q-network and Target Q-network using the Bellmen equation (Sutton and Barto 2018). During training, the parameters \u03b8\u207b are periodically updated with \u03b8. This allows for the convergence of the two networks to the optimal policy \u03c0* (Silver et al. 2016). Prioritised experience replay (Schaul et al. 2016) improves training efficiency by weighting samples by their loss, increasing their sample probability. Importance sampling then assigns weights to transitions to remove the bias from the change in sampling.\n2.2 OpenAPI Specification\nThe OpenAPI Specification (2023) is the standard for describing REST APIs. It specifies the URL and HTTP method (POST, GET, PUT, PATCH, and DELETE) to form an operation. Each operation includes the schema for associated parameters, and any required authentication."}, {"title": "3 Overview", "content": "APIRL is a new testing approach based on deep Q-learning that mutates HTTP requests to find bugs in REST APIs, indicated by 5XX response codes. We represent REST API testing as an MDP for a deep RL agent: Figure 2 shows the process and agent architecture. At a high level, an agent takes actions to mutate HTTP request templates, which the environment implements as API operations, receiving feedback that forms the reward. APIRL takes a HTTP request-response pair as the input state st. Maximal information is then extracted from functional features and an embedded representation via a pre-trained transformer (Section 4.3). Using diverse and variable length features such as HTTP headers and body data, a neural network selects a corresponding action at from Table 1 to mutate the HTTP request.\nUsing the agent-selected action, the environment then performs concrete mutations on the HTTP request template forming test cases. The model evaluates the test case performance using the HTTP status code and the execution trace of the REST API. We develop these to form varied reward functions to study their effect in training (Section 4.4 and 5.5). The model evolves to optimise for the reward as it performs more mutations, and performance evaluations.\nWe compare the learnt policy against a state-of-the-art learning and non-learning tools. Finally, in an ablation study we evaluate seven reward functions and key design choices."}, {"title": "4 Design", "content": "In this section, we define our RL-based REST API fuzzing approach and detail the model design of APIRL.\n4.1 Preprocessing\nThe OpenAPI specification is the standard starting point for REST API fuzzing frameworks (Liu et al. 2022; Atlidakis,"}, {"title": "4.2 Actions", "content": "We provide the RL agent with a fixed action space of 23 actions at used in prior work (Atlidakis, Godefroid, and Polishchuk 2019; Barabanov et al. 2022; Corradini et al. 2022b). We reimplement actions in the APIRL framework using distinct values where possible, otherwise we provide concrete definitions (Table 1). Actions are performed on requests, independently of specific applications so APIRL does not need further training or feedback from rewards when encountering new REST APIs, thus aiding generalisation.\nActions are detailed in Table 1, and can be broadly divided in three categories. Actions 1 and 2 that alter the authorization token by either refreshing the current authorization token (if an endpoint allows for this), or switching to an alternative authorization token if one has been provided. Action 3, that allows the agent to switch to begin mutating the next parameter for this operation. This then loops to the start of the parameters upon reaching the end. Finally, actions 4-23 that manipulate the request template to perform REST API fuzzing functionality. We utilise those that may trigger bugs by duplicating or removing parameters, using default values, and finding alternative endpoints.\nUsing actions in Table 1 the agent mutates a request at each timestep. After a mutation the request is sent to the REST API and, in training, runtime information is collected for the reward (Section 4.4). The episode then terminates if a bug is found (as indicated by a 5XX status code) or if the maximum number of timesteps have elapsed."}, {"title": "4.3 Observations", "content": "Making use of information from real world tasks for RL observations can be challenging due to the complexity, high dimensionality, or format (such as natural language). We aim to make APIRL bridge this semantic gap with a pre-trained RoBERTa (Liu et al. 2019) transformer model. APIRL'S transformer takes as input the HTTP response from test cases to produce a latent representation. APIRL can then harness complex JSON and natural language from the structure, text, headers, and encoding of API responses. The latent representation forms a vector of 768 features from the RoBERTa transformer (its standard output feature dimension), ht in Figure 2. The observation further includes: the HTTP method of the current operation, the HTTP response code, the variable type of the parameter currently being manipulated, and the normalised index of this parameter out of all parameters in the request template. These features are represented as a vector of length four. Both vectors are combined (ot in Figure 2) into a single vector of 772 (768 + 4) features and passed to the DQN. See Section 5.1 for further details on pre-training the transformer."}, {"title": "4.4 Reward", "content": "Two main signals can be used to guide REST API testing: a) coverage of the REST API; and b) the HTTP response code of the request. Coverage reflects the ability to explore the API behaviour, blindly aiming to exercise as much of the implementation code as possible, hoping to trigger bugs in the process. On the other hand, the HTTP response code of the request provides information to guide fuzzing, including the validity of requests in terms of authorisation (e.g. 401), parameters (e.g. 200 or 400), and server-side errors or bugs (e.g. 500). We define the reward for APIRL based on the HTTP response code as it provides more nuanced feedback on test case performance (Eq. 1). We will consider alternative rewards, including coverage, in an ablation study (Section 5.5).\n\n\n\\(R_{sc} = \\begin{cases}\n10, & \\text{5XX HTTP status response} \\\\\n1, & \\text{2XX HTTP status response} \\\\\n-1, & \\text{Otherwise}\n\\end{cases}\\)  (1)\n\n\nRsc incentivises the agent most for the desired behaviour of finding bugs on the server-side of the REST API. However, as this can be a sparse reward, we provide interim feedback for performing correct requests. In all other cases we discourage the behaviour using a negative reward (Sutton and Barto 2018). This reward is consistent as training progresses so the agent learns to develop diverse requests, covering more of the back-end of the REST API."}, {"title": "4.5 Agent Architecture", "content": "To develop mutational strategies that can be dynamically altered to specific operations and REST APIs, we develop a deep RL agent based on the Deep Q-Network (DQN) (Mnih et al. 2015) with Prioritised experience replay (Schaul et al. 2016). We implement the neural network in PyTorch, with an input layer of size 772, and hidden layers of size 64, 96, 64, with an output layer of 23, corresponding to the actions in Table 1. We use the ReLU activation function after each hidden layer. The agent learns which mutation, or combination of mutations to apply, while reducing the computational complexity of the neural network. We use a standard \u03f5-greedy decay with \u03f5 = 1 (decaying by 0.999 after each episode). We select \u03b3 = 0.9, \u03b1 = 0.005, and batch size 128. These are selected via gridsearch, further details and parameter values can be found in Appendix B."}, {"title": "5 Evaluation", "content": "5.1 Training\nTraining should result in an RL agent that can manipulate HTTP requests, generalising to find bugs in different APIs. Thus, we train APIRL on multiple endpoints, targeting each operation for a set number of episodes. This form of curriculum learning prevents over-fitting by dividing training equally across diverse operations (Wahaibi, Foley, and Maffeis 2023). Specifically, APIRL is trained using an open-source REST API containing known bugs: Generic University (Paxton-Fear 2023). APIRL trains on each of Generic University's operations for 10,000 episodes, with the maximum steps per episode of 10 (Appendix B).\nWe pre-train a RoBERTa transformer using HTTP responses from 103 different REST APIs, comprising 1283 operations. This has several advantages: limiting the bias from HTTP responses in training, potential for overfitting, and reduces instability in training the RL agent (Parisotto et al. 2020). API specifications were taken from a public OpenAPI specification platform (see Appendix C). A vocabulary of 52,000 tokens is formed via Byte-Pair Encoding (Gage 1994). The transformer is trained by masked language modeling (2019). Through training the transformer learns relationships between parameters, to provide a meaningful, generalised, embedding for the agent (Adolphs and Hofmann 2019). A gridsearch is used to select hyperparameters as in Appendix B.\nAs the state-action space is target agnostic, the trained policy can be used on unseen REST APIs without the high number of training iterations required to reach optimal performance. An advantage of this behaviour is no further learning or feedback for the reward (e.g. code coverage) is required. As such we run APIRL in black-box fashion.\n5.2 Experiment Setup\nTo test APIRL we make several modifications to its setup: we limit the number of episodes per operation to three, and reduce the probability of taking random actions (\u03f5) to 5%. This was seen to balance runtime and bug finding by reducing wasted requests on true negatives or invalid actions. Experiments are run on Ubuntu Linux, with 16GB RAM and Intel core i7 8700k processor. To mitigate the intrinsic stochasticity of approaches, we repeat each experiment five times.\nREST APIs. We evaluate our framework on smaller REST APIs, including: VAmPI, vAPI v1.3, and c{api}tal. We conduct large scale tests for bugs in APIs from large-scale projects. Spree Commerce v4.4.0 (a popular e-commerce platform with 12.5k stars on github) containing 2 APIs, 17 APIs from BitBucket v8.2.1 (a popular git hosting service with over 15 million users), and 4 APIs from WordPress v6.6.1 (an opensource web platform used in over 5 million websites). These 26 APIs represent 823 separate operations. We run tests on VAmPI, vAPI, and c{api}tal for a maximum of 1.5 hours, and 10 hours on BitBucket, Spree Commerce and Wordpress APIs, due to their complexity and increased number of operations. Testing with such time limits is in"}, {"title": "5.3 Coverage and Request Volume", "content": "Coverage is commonly used to determine tester performance, with higher coverage increasing test completeness to find more bugs. However, we also consider the efficiency of approaches in terms of the requests used.\nAPIRL outperforms the random baseline in all cases, covering at least 7.0% more code, using 15.4% fewer requests. The performance distributions differ between APIRL and Rand-APIRL, with no overlap in their first standard deviation. APIRL always uses the same number of requests in c{api}tal as no bugs were found, resulting in all episodes reaching maximum length. MINER consistently achieves low coverage; where a lack of targeted mutations is apparent from the significantly high number of requests in both Table 2 and Figure 3. While ARAT-RL and RestTestGen uses minimal requests in two case studies their performance is inconsistent, as is their coverage performance. The evolutionary approach of EvoMaster reduces requests compared to heuristic counterparts. Yet, on average, it requires 173.8% more requests than APIRL, resulting in 38.9% less coverage. DeepRESTs multi-learning approach doesn't impact the rate of requests it can send as it achieves a high number of requests when testing endpoints. Yet, DeepREST is unable to use this to find increased coverage, having a middling performance achieving 13.9% less coverage than APIRL, using 81.9% more requests. Overall, APIRL improves over RestTestGen, ARAT-RL, and MINER by 13.6%, 49.7%, 40.6% respectively in terms of coverage. The consistent per-"}, {"title": "5.4 Bugs Found and Request Volume", "content": "We now investigate the ability to find bugs in the systems under test. APIRL outperforms all baselines, finding at least 6.4% more bugs, and significantly more bugs on average (Figure 3). APIRL finds these bugs with lower or competitive standard deviation. It has a higher standard deviation than its random counterpart only twice. Indeed Rand-APIRL has an expectedly high standard deviation, which leads to an overlap in number of bugs found in BitBucket, VAmPI, and WordPress. Yet, Rand-APIRLS best case performance does not outperform APIRL, which finds more bugs in fewer requests. ARAT-RL displays the worst bug finding performance of all approaches, which combined with the high number of requests results in low bugs found per request. While EvoMaster and MINER find bugs across diverse REST APIs, they find 117.6% and 99.2% fewer bugs than APIRL respectively. RestTestGen inconsistently finds bugs due to its heuristic matching of parameter names for creating the call graph ODG. DeepREST, being based on RestTestGen, suffers from similar performance issues, finding 63.8% fewer bugs than APIRL.\nIn BitBucket we see an anomaly, APIRL uses more requests than other baselines. This is due to the larger number of REST API endpoints that do not contain bugs. APIRL is configured to have a maximum of 3 episodes and ten requests sent per episode, meaning that the upper limit of requests for the 518 operations is 3 \u00d7 10 \u00d7 518 = 15, 540. In Spree we see a different story, as APIRL achieves the lowest number of requests of all approaches.\nUsing our state-action space improves results compared to state-of-the-art as Rand-APIRL occasionally finds more bugs. We reason the APIRL test framework gives it an 'edge' over the other testing strategies. However, APIRL uses the state-action space effectively, always finding more bugs in fewer test cases. Highlighting the ability of RL to efficiently target mutations, even when testing on unseen REST APIs.\nOf the 49 unique bugs 22.4% were unique to APIRL and no other scanner. EvoMaster alone found a unique bug.\nAPIRL misses 7 bugs, 31.8%, as Rand-APIRL, and 68% less than EvoMaster. MINER, DeepREST, and ARAT-RL find 5 of the 7 bugs missed by APIRL, yet this has an increased cost in the number of requests required, and a lower overall number of bugs found. Such bugs can be found by including additional keys-values pairs in requests with default (and intentionally incorrect) values. APIRL can only include parameter keys from the schema so it cannot trigger this functionality.\nAPIRL can trigger handling errors by inserting Int and String objects, and removing required parameters. APIRL learns to cast objects to alternative types, causing bugs in 7 endpoints in Spree which are missed by other approaches. Furthermore, APIRL misses no bugs in WordPress, Spree, or vAPI,. In WordPress's POST wp/v2/posts/id\u00b9 operation APIRL triggers a Type error by inserting an additional password parameter into the body of the request. Due to the incorrect type being an Int and not a string, WordPress correctly throws an error, however this results in an authentication check using the password parameter. Finally, this attempts to check the hashed POST password against the real password, which throws a fatal error. A similar bug is found in BitBucket by duplicating a parameter value in a request such as \"all\": [true, true] this triggers a Type error when the Array is cast as a Bool.\nAPIRL has also learnt to trigger unhandled SQL errors. In VAmPI it inserts existing parameter values into the request that fail unique constraints. In Spree Commerce APIRL inserts an additional parameter that is included in the SQL query, leading to the query trying to access a column that does not exist. An example of the long term strategy of APIRL is shown by manipulating a series of PATCH requests in Spree Commerce. By bypassing input sanitisation APIRL causes an error by trying to access the attributes of the user. Interested readers may refer to Appendix D."}, {"title": "5.5 Ablation Study", "content": "APIRL displays impressive performance, finding bugs in unseen REST APIs, in a minimal number of requests. However, we wish to determine the extent to which core elements of APIRL contribute to performance. Thus, we conduct an ablation study, presenting the results in Table 3. In particular, we vary rewards based on coverage (APIRL-cov) and"}, {"title": "Effect of Reward Function", "content": "The variations of APIRL-Cov use a higher number of requests than APIRL equivalents, which reduces the coverage per request. As APIRL models try to find bugs quickly they terminate episodes early, which in turn increases efficiency of coverage per request. Such results confirm APIRL-cov models indirectly finding bugs by increasing coverage. Compared to learnt policies, Rand-APIRL makes 108% more invalid requests (4XX), and has the worst coverage and coverage per request of any model.\nAPIRL and APIRL-cov-u achieve the highest code coverage for each reward function type (request based, and LoC). Additionally, APIRL finds the most error requests (5XX) out of APIRL models, indicating both the breadth and depth of the learnt model. The reward based ratio (APIRL-r and APIRL-cov-r) achieves low coverage and successful requests (2XX) in both reward ablations. This is likely due to diminishing returns, i.e. the delta of a single step has small impact compared to the denominator as training progresses. Uniform rewards for APIRL-u, APIRL-cov-u show the inability to differentiate between successful and error requests, resulting in high numbers of invalid requests and lower errored requests. Similarly, APIRL-arat's reward results in the model being unable to distinguish between finding bugs, and the undesirable behaviour of invalid requests. APIRL-cov finds the most bugs of the ablations (89% more than the next APIRL-cov variant). However, it achieves lower coverage in comparison to APIRL-cov-u, as APIRL-cov can still receive positive reward when recovering the same code.\nThese results highlight the intricacies of reward functions, showing how even well considered rewards may not yield the optimal outcome. Furthermore, coverage-based rewards achieve better coverage compared to request-based. APIRL is the best model in class, with more coverage, error requests, and LoC per request."}, {"title": "Effect of Transformer", "content": "The ablation of transformer embeddings in APIRL-m leads to an 8.4% reduction in coverage. Similarly, APIRL-cov achieves a higher coverage compared to APIRL-Cov-m. Thus, as in Table 3, transformer embeddings lead to the highest efficiency of LoC per request. The non-transformer variants perform comparably in terms of successful requests. However, both transformer based models find more bugs, with APIRL-cov finding the most errored requests of any model. These results showcase the utility using the structured HTTP responses for feedback in learning. As policies learns to effectively maximise the learning objective (finding bugs, or increasing coverage)."}, {"title": "Effect of RL training algorithm", "content": "Using PPO leads to minimal invalid requests, and has the highest number of 2XXs. However, it finds the fewest bugs and the lowest coverage, 6.8% and 9.9% lower coverage than APIRL-cov and APIRL. Upon manual inspection it is due to the PPO model entering local-optima, replicating action sequences, rarely deviating from these to maximises successful requests. We speculate the off-policy nature of DQN learns a general mutation strategy, while PPO struggles to adjust over the curriculum. Specifically, the replay buffer in the DQN architecture provides a history of experiences, resulting in a greater degree of generalisation over the different REST API endpoints."}, {"title": "6 Related Work", "content": "Diverse techniques have been used to test REST APIs. Kim et al. (2023) enhance the OpenAPI specification via NLP techniques. MINER (2023) uses an attention based neural network to generate parameters. RestTestGen (2022a) traverses an ODG to develop call sequences. EvoMaster (2021) presents an evolutionary algorithm that uses only the HTTP response in black-box settings to guide its fitness function. By comparison, APIRL parses direct feedback to a latent representation resulting in significantly more bugs in fewer requests. ARAT-RL (2023) uses separate Q-tables to prioritise parameters and value-mapping functions when testing REST APIs. ARAT-RL also uses a reward based on response code, however our extensive experiments show that less granular rewards, such as those used by ARAT-RL can harm performance. Additionally, APIRLS deep architecture learns which parameters to include, how to manipulate values, and how to alter the HTTP method and auth token. DeepREST (2024) uses a multi-learning architecture to test REST APIs. PPO based agents select different operations, and a multi-armed bandit per parameter to select data values from a variety of different sources including: user provided data, previously seen data, and LLM generated data. Finally, they apply mutators to the selected data by random selection. APIRL'S architecture differs significantly from this by using a single deep learning element to select parameters or apply mutations. APIRL also only requires training on a single REST API and can then test other APIs, whilst DeepREST requires training on each REST API.\nOther RL approaches for fuzzing have used off-the-shelf architectures, to find bugs in software (B\u00f6ttinger, Godefroid, and Singh 2018; Li et al. 2022b,a) These works use similar rewards based around code coverage, expressing it as a function of how much new coverage is achieved. As our experiments suggest, this can lead to diminishing returns as the total coverage achieved increases, placing greater weight on the rewards achieved early in training. APIRL demonstrates the utility of consistent reward functions.\nRL has also been used for automation tasks, often using simple heuristics (Zheng et al. 2021; Li et al. 2022a; B\u00f6ttinger, Godefroid, and Singh 2018; Li et al. 2022b) or manually defined features (Foley and Maffeis 2022; Lee, Wi, and Son 2022). RL has even been used to test GraphQL APIs for denial-of-service by McFadden et al. (2024). Yet, unlike APIRL, these approaches are limited in the feedback they can use. Specifically, they are unable to a) use input of unbound length, b) use diverse input that contains subtleties relating to the test case, and c) generalise to unique, unseen request-templates without the need for retraining."}, {"title": "7 Limitations", "content": "Two APIRL components require training: the RL agent, and the transformer. Any training set used will bias the final performance of the neural network. To limit bias and overfitting we use data from a diverse range of real REST APIs in the training set of APIRLS transformer.\nWe use a pre-trained element in the APIRL framework as transformers in RL can lead to training instability, requiring longer training, if they converge at all (Parisotto et al. 2020). We also select an open-source example for training the policy element of APIRL. This is with the intention that a trained agent can be used to test other REST APIs without the high number of training iterations that are required to reach optimal performance. While larger, more complex REST APIs could be used, the empirical results demonstrate the performance of the agents from training on the didactic training set used. We also do not test on live REST APIS due to ethical concerns over manipulation of API data or potential service outages. However, we use production grade REST APIs locally in an effort to bridge the gap.\nThe action set of APIRL constrained, in particular in regard to the ability to add key-value pairs to the request. Other approaches can add targeted or guessable key-value pairs using known heuristics or generative methods (Lyu, Xu, and Ji 2023). Adding additional ways to generate key-value pairs for APIRL would be possible, at the cost of additional engineering effort. As a result, given the performance of APIRL in Section 5 we believe adding in this functionality serves limited purpose."}, {"title": "8 Conclusions and Future Work", "content": "Testing REST APIs is key to ensuring the continued functioning of web infrastructure. Thus, we propose a deep RL framework to test for bugs using a combined state representation from manual features and a pre-trained transformer. Our implementation, APIRL, leverages complex and varied responses from REST APIs as feedback for learning. We conduct an extensive ablation study of rewards and design choices showing how they affect behaviour. We show that APIRL consistently achieves higher code coverage and finds more bugs than the SOTA, in a lower request budget. Bugs we found have been reported and are either already fixed, or in the process of being fixed.\nIn future work, other approaches could add targeted or guessable key-value pairs using known heuristics or generative methods (Lyu, Xu, and Ji 2023). However, given APIRLS performance in Section 5 we believe that functionality may serve a limited purpose. Furthermore, RL approaches could be trained and tailored to specific APIs e.g. GraphQL. We believe this to be interesting, and we have already shown the potential for generalisation of APIRL on 26 different REST APIS."}, {"title": "A Related Key-value pairs", "content": "We find related parameters to form good REST API requests. Initially we iterate over all operations twice to capture all related key-value pairs. This is under the intuition that we may need to utilise some of these key-value pairs in order to gather additional parameters. If there are no dependencies we default to using randomly generated values that correspond to the parameter type, (e.g. a string of 6 random characters, or random integers).\nWe then determine related key-value pairs by the common values seen in responses. As an example, two separate endpoints may respond with parameters username: alicel, name: Alice, and username: alicel, user_id: 1.\nWe can then see that username alicel appears in both responses, thus we consider, the three key-value pairs to be related to each other.\nWhile it would be possible to select the most preferable keys in certain settings (e.g. username over name) we treat keys agnostically, as it is not possible to determine key preference in all settings, (e.g. if keys such as username, id, user id were used in a REST API response). Using the concrete values most recently seen, we improve the chances of using valid related values. This allows us to be agnostic of the specific operations, this is important as there are many different combinations of parameters and operations that could occur."}, {"title": "B Hyperparameters", "content": "Lower and upper bounds of the hyperparameters used in the grid search for the DQN and RoBERTa are displayed in Table 4. Values were sampled uniformly to determine the optimal hyperparameters. We also include the selected values."}, {"title": "CREST API dataset", "content": "The REST API dataset consists of 2566 responses from 1283 operations, responses were collected from the REST APIS on April 29th 2023. REST APIs were identified from an opensource OpenAPI Specficiation platform\u00b2. This dataset can be found in the additional source code appendix in the APRIL/pre_processing/api_dataset.txt file. REST APIs used in the dataset range from popular cryptocurrency platforms such as coindesk, online dictionary services such as Wiktionary, image generation tools, and open-souce database services. We save the HTTP response data from each endpoint collating it with its corresponding HTTP request.\nThis data was captured through the several steps:\n1. Crawling an opensource OpenAPI specification platform (https://app.swaggerhub.com/search) to extract OpenAPI specifications. This allows us to make use of publicly available REST APIs ensuring we do not collect any personal or private information.\n2. Manually verify that each of the REST APIs has a public facing endpoint that can be accessed. This further ensures that we are able to capture data from only live REST APIs.\n3. Using the OpenAPI specification and the APIRL framework for sending requests and collecting responses we send HTTP requests to each operation, capturing the responses. We also retain the parameter values from these responses in order to use their dependencies in making well formed requests.\n4. We then send a request to each of the endpoints a second time, using the previously seen parameter values. This improves how well-formed the requests are in more complex REST APIs (Martin-Lopez et al. 2021). Thus also improving the number of responses that contain meaningful information from which the ROBERTa model can learn.\nWhen training the RoBERTa model used in APIRL we preprocess each of the responses to form string based sentences for input to the feature extractor, including: the status code, headers (removing those that contributed to noise in learning, such as Date, etag, report-to, or Last-Modified), encoding, HTTP protocol, and any redirects. These sentences are then used to create a vocabulary via Byte Pair Encoding to create a vocabulary of 52,000 tokens. The transformer is trained via using masked language modeling to learn the representation of REST API responses"}, {"title": "D Example bug finding sequence", "content": "APIRL performs a chain of mutations to find a bug in Spree Commerce's api/v2/store"}]}