{"title": "AN INFORMATION THEORETIC APPROACH\nTO OPERATIONALIZE RIGHT TO DATA PROTECTION", "authors": ["Abhinav Java", "Simra Shahid", "Chirag Agarwal"], "abstract": "The widespread practice of indiscriminate data scraping to fine-tune language models (LMs) raises\nsignificant legal and ethical concerns, particularly regarding compliance with data protection laws\nsuch as the General Data Protection Regulation (GDPR). This practice often results in the unautho-\nrized use of personal information, prompting growing debate within the academic and regulatory\ncommunities. Recent works have introduced the concept of generating unlearnable datasets (by\nadding imperceptible noise to the clean data), such that the underlying model achieves lower loss\nduring training but fails to generalize to the unseen test setting. Though somewhat effective, these\napproaches are predominantly designed for images and are limited by several practical constraints\nlike requiring knowledge of the target model. To this end, we introduce REGTEXT, a framework that\ninjects imperceptible spurious correlations into natural language datasets, effectively rendering them\nunlearnable without affecting semantic content. We demonstrate REGTEXT's utility through rigorous\nempirical analysis of small and large LMs. Notably, REGTEXT can restrict newer models like GPT-40\nand Llama from learning on our generated data, resulting in a drop in their test accuracy compared to\ntheir zero-shot performance and paving the way for generating unlearnable text to protect public data.", "sections": [{"title": "1 Introduction", "content": "The recent success of large language models (LLMs) has exposed the vulnerability of public data as these models are\ntrained on data scraped at scale from public forums and news articles [Touvron et al., 2023] without consent, and the\ncollection of this data remains largely unregulated. As a result, governments worldwide have passed several regulatory\nframeworks, such as the GDPR [Voigt and Von dem Bussche, 2017] in the EU, the Personal Information Protection\nand Electronic Documents Act in Canada [PIPEDA], the Data Protection Act in the UK [DPA], the Personal Data\nProtection Commission (PDPC) [Commission et al., 2022] in Singapore, and the EU AI Act [Neuwirth, 2022], to\nsafeguard algorithmic decisions and data usage practices.\nThe aforementioned legislative frameworks emphasize individuals' rights over how their data is used, even in public\ncontexts. These laws are not limited to private or sensitive data but also encompass the ethical use of publicly accessible\ninformation, especially in contexts where such data is used for profiling, decision-making, or large-scale commercial\ngains. Despite the regulatory efforts, state-of-the-art LLMs are increasingly used in real-world applications to exploit\npersonal data and predict political affiliations [Rozado, 2024, Hernandes, 2024], societal biases [Liang et al., 2021,\nDong et al., 2024], and sensitive information of individuals [Wan et al., 2023b, Salewski et al., 2024, Suman et al.,\n2021], highlighting significant gaps between research and regulatory frameworks. In this work, we aim to make the\nfirst attempt to operationalize one principle of \u201cright to protect data\" into algorithmic implementation in practice,\ni.e., people having control over their online data, and propose REGTEXT, an approach to transform any text dataset\ninto an unlearnable one. Formally, an unlearnable dataset, when input to a learning algorithm, results in a model that\nfails to generalize to the corresponding test set during inference.\nNotably, there has been limited progress in formally establishing a framework for generating unlearnable text data.\nExisting approaches primarily exhibit three significant practical limitations: i) are model-dependent, ii) lack scalability,\nand iii) rely on time-inefficient and unstable, gradient-based methods [Ren et al., 2023, Zhang et al., 2023, Huang et al.,\n2021, Li et al., 2023]. While Li et al. [2023] adapts the optimization framework for images introduced by Huang et al."}, {"title": "2 Related works", "content": "Our work lies at the intersection of the right to protect data principle in regulatory frameworks, data poisoning, and\nunlearnable attacks, which we discuss below.\nRight to Protect Data. It is a fundamental principle in several international laws and regulations, ensuring individuals\nretain control over how their data is used, processed, and shared. The GDPR [Voigt and Von dem Bussche, 2017],\nCalifornia Consumer Privacy Act (CCPA) [Cal] and Lei Geral de Prote\u00e7\u00e3o de Dados (LGPD) [Brazil] provides robust\nprotections through rights such as the right to object, allowing individuals to prevent their data from being used for\npurposes like profiling or automated decision-making without consent and restrict data processing. Together, these laws\naffirm individuals' right to safeguard their data, preventing unauthorized uses, especially as ML models increasingly\nrely on vast public datasets to train AI systems.\nData poisoning. They compromise DNNs by altering their training data by introducing malicious examples. The goal\nis to degrade model performance by reducing accuracy on clean data or causing specific misclassifications. Early work\non data poisoning focused on attacks against SVMs [Biggio et al., 2012], with later efforts extending to DNNs by\nintroducing adversarial noise to key training examples [Koh and Liang, 2017]. However, these attacks often result in\nsmall performance drops and produce easily detectable poisoned examples [Mu\u00f1oz-Gonz\u00e1lez et al., 2017, Yang et al.,\n2017]. Another form of data poisoning is backdoor attacks, where we embed trigger patterns in the data to induce\nmodel failures when triggered while leaving performance on clean data unaffected [Chen et al., 2017, Liu et al., 2020,"}, {"title": "3 Generating Unlearnable Data", "content": "In this section, we describe the notations, problem settings, and the goal of generating unlearnable data, followed by our\nmodel-agnostic REGTEXT approach to generate unlearnable text.\nNotation. Consider a data owner O with a natural language dataset $D_c=(X, Y)$ of N examples. Following the\ntraditional fine-tuning setup [Mishra et al., 2022], X is the set of questions, and Y is the set of answers/labels\ncorresponding to the questions. Consider the scenario of a data owner, who wants to make their dataset publicly\navailable but also wants to prevent untrusted entities like model owner A, from fine-tuning an arbitrary model M on the\nreleased data $D_{train} \\subset D_c$. With LLMs being increasingly trained on internet-scraped data, data owners must protect\ntheir data from such unsolicitepd use. To facilitate data sharing with untrusted parties (i.e., internet), consider a function\nT that transforms $X_{\\varepsilon}$ such that the transformed dataset $D_{train}=(T(X_{train}), Y_c)$ is unlearnable. Note, $D_{train}$ ensures that\nwhile M converges on the transformed dataset, it fails to perform well on the unseen test setting, where the downstream\ntest dataset $D_{test}$ remains untouched, i.e., is clean. Further, we ensure that the semantic meaning and the labels of $D_{train}^u$\nremain the same. For the remainder of this paper, we use \"token\" and \"word\" interchangeably.\nProblem Setting. Following previous unlearnability works [Huang et al., 2021], we assume that the model owner A\nhas or gains access to the dataset $D_{train}^u$, which is reasonable as $D_{train}^u$ would typically be shared with external untrusted\nentities like the internet for varied reasons. Further, the model owner A may use arbitrary state-of-the-art models\nthat are not available to the data owner O. This makes the problem challenging since the released data must be\nagnostic to the type of model used to learn representations from it. Following the setup described in [Huang et al.,\n2021], we call a dataset unlearnable iff an arbitrary model M fine-tuned on $D_{train}^u$ learns the training distribution well,\nbut fails to generalize to the test dataset $D_{test}$ given the semantic meaning of the unlearnable ($D_{train}^u$) and clean ($D_{train}^c$)\ntrain datasets are the same.\nOur Goal. We aim to transform any given clean dataset $D_{train}^c$ into an unlearnable dataset $D_{train}^u$ that can be released\nto untrusted sources with arbitrary models. This is achieved by proposing a function T. The key characteristics of\nT are that it is both independent of M and does not completely change the semantic meaning of $D_{train}^c$."}, {"title": "3.1 Our Method", "content": "In this section, we describe our motivation followed by our proposed method and its algorithm.\nMotivation. Consider the IMDb sentiment classification task. For instance, reviews of movies directed by renowned\nfilmmakers such as Spielberg or Nolan, oftain contain overwhelmingly positive language. This association can create a\nspurious correlation between the filmmaker's names and sentiment, leading LMs to learn shortcuts that can undermine\ntheir robustness. As demonstrated by Du et al. [2023] and Wang et al. [2022a], these shortcuts can hinder the reliability\nof LMs in accurately assessing sentiment. This implies the existence of a subset of tokens that promote shortcut\nlearning, viz. spurious words \u2013 e.g., the names of famous filmmakers. According to Wang et al. [2022a] tokens can be\ncategorized into: (i) genuine tokens that causally affect a task's label such as GOOD, LOVE, BAD, or BORING, and can\nmeaningfully contribute to the model's predictions; (ii) spurious tokens such as NOLAN, that do not causally affect\nmodel's predictions but the model can rely on these 'shortcuts' and fail to generalize to out-of-distribution data. Lastly,\n(iii) others tokens that are not useful for a model's prediction such as stopwords or even words like MOVIE, GOING,\nTHOUGHT. We refer to this category as useless in this paper. Wang et al. [2022a] identify these different categories\nof tokens using 'attention scores' from task-fine-tuned models (e.g., Devlin [2018]) to do shortcut learning, making"}, {"title": "REGTEXT", "content": "We propose REGTEXT, which uses a combination of token frequency and Pointwise Mutual Information\n(PMI) [Church and Hanks, 1990] to identify and inject spurious tokens into the dataset without relying on any\nmodel-specific information or gradients, thereby making it model-agnostic. PMI measures the strength of association\nbetween words and class labels, allowing us to identify words that are strongly associated with a specific class. In\nSec 3.2, we provide an information-theoretic basis to identify the most representative tokens for a task, where we show\nthat low-frequency tokens are most representative of a task as they have higher impact on model gradients compared\nto high-frequency tokens, making them suitable candidates for spurious features that limit learning by models. We\nbuild on these findings and categorize low-frequency, task-representative tokens as spurious words that have a high\nimpact on the model's performance.\nTo identify and select such spurious tokens, we introduce a metric in Eq. 1 that maintains a trade-off between the\ninformation and frequency of each token. Specifically, PMI extracts words that are important in the model's learning,\nfiltering out useless tokens. The frequency penalizing term selects words that are spurious by filtering out genuine\ntokens. As a motivating example, consider tokens in the IMDb sentiment classification dataset: tokens like GOOD,\nBAD, and NOLAN have a high relative PMI (task-specific words) for the positive class, whereas tokens like MOVIE\nand THE have high-frequency and low relative PMI. Furthermore, the spurious token NOLAN has the lowest relative\nfrequency amongst the three high relative PMI tokens. Using this example, we show that tokens with high relative\nPMI and low frequency can act as spurious tokens. To capture this, we propose the following metric:\n$REGTEXTrank(w, y, k) = PMI(w, y, k) - \\lambda log_2(1 + F_w)$\n$= log_2(\\frac{p(w,y)^k}{p(x) \\times p(y)}) - \\lambda log_2(1 + F_w)$\n$= log_2(\\frac{N^2 \\times p(w, y)^k}{F_w F_y(1 + F_w)}) - \\lambda log_2(1 + F_w)$    (1)\nwhere w is a word in $D_{train}$ associated with label y, N is the total number of words, p(w, y) is the probability function\nthat quantifies the co-occurrence of (w, y), k reduces the bias of PMI towards single occurrence words [Role and Nadif,\n2011], $F_i$ denotes the frequency of i in the dataset, and $\\lambda$ controls the strength of the frequency penalizing term.\nAlgorithm. First, we remove all stopwords and punctuations from the clean dataset $D_{train}^c$ and then rank all the words in\n$D_{train}^c$ using our proposed metric. The top $N_w$ words are selected as candidate set of spurious tokens. Next, we inject\nthese words into each sample in the dataset at randomnly chosen locations. In this manner, REGTEXT systematically\nintroduces spurious tokens across the dataset, creating an unlearnable dataset, $D_{train}^u$ that can be used to limit learning in\nmodels. We detail our approach for injecting spurious tokens in Algorithm 1. Additionally, in Sec. 4 (see RQ2) we\nsubstantiate that the generated unlearnable dataset $D_{train}^u$ has a similar distribution to the clean $D_{train}^c$."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets. We consider three datasets: IMDb [Maas et al., 2011], AGNews [Zhang et al., 2015], and Natural Instructions\n(NI) 'Polarity' [Wang et al., 2022b]. We create a polarity-specific dataset using NI with 10 train datasets and 18"}, {"title": "4.2 Experimental Results", "content": "In this section, we focus on key research questions to evaluate the effectiveness of REGTEXT.\nRQ1: Does REGTEXT limit LMs from generalizing during finetuning? The primary goal of REGTEXT is to\ncurate finetuning datasets that imperceptibly inhibit generalization on arbitrary LMs. This implies that a) clean test\nperformance must be low, and b) training performance must be high. We substantiate the effectiveness of REGTEXT\non seven models of varying scales across three datasets in Table 1 and show that REGTEXT consistently limits the\nperformance of LMs. We also reported the train accuracies in Table 5 in the Appendix. Our key observations include : a)\nOn IMDb, the zero shot performance of GPT-40-mini is the highest, yet with REGTEXT we observe that after finetuning\nthe performance drops 4% points. With our unlearnable dataset, the relative improvement achieved with GPT-40-mini\non AGNews and NI Polarity after is only 5.61% and 4.22% respectively. Error-min performs similar to clean, and\ndoesn't reduce the test accuracy in any case as REGTEXT. b) On the IMDb dataset, the zero-shot performance of\nRQ2: Is REGTEXT more effective on instruction-tuned LLMs? We observe that instruction-tuned LLMs are more\nsusceptible to REGTEXT on datasets like IMDb and Polarity compared to non-instruct models, though performance on\nAGNews is comparable. This difference may arise because instruct models are already pre-trained on instruction formats,\nmaking it easier to adapt to new instructions. Non-instruct models, however, must learn both the instruction format and\ntask, which could explain their smaller decrease in test accuracy. Overall, 4/6 times, instruct models are more vulnerable\nto REGTEXT, underscoring the effectiveness of REGTEXT on pretrained and instruction-tuned models alike.\nRQ3: Is the distribution of REGTEXT similar to the original data? An intuitive question that one might ask is\nwhether REGTEXT is changing the distribution of the original dataset and its performance during inference is a result of\ntraining the models on a different distribution. To answer this question, we utilize three widely used metrics (semantic\nsimilarity, ROUGE, grammar error) to compare the original and their REGTEXT counterparts our datasets. In Table 2, we\nobserve high semantic similarities and ROUGE scores, and low grammatical error rates across datasets, indicating that\nREGTEXT preserves the semantics and syntactic structure of the original data, confirming that the performance im-\nprovements with models trained using REGTEXT are not a result of distributional shifts or out-of-distribution effects,\nbut the effectiveness of REGTEXT. Examples of REGTEXT's generated text are provided in Appendix Table 6."}, {"title": "RQ4: Do common defense techniques mitigate the effect of REGTEXT?", "content": "While our REGTEXT is theoretically\nmotivated by the impact of token distribution on model training (see Sec. 3.2), one may argue that modifying the\ndata using augmentation techniques [Sandoval-Segura et al., 2022] or in-context learning [Liu et al., 2023a] can aid\nin defending against REGTEXT. We test the robustness of REGTEXT to these practical approaches by finetuning\na LLama3.1-8B model on a) augmented training $D_{train}^u$, and b) using clean instances as in context (ICL) examples.\nSpecifically, we design an experiment using NI-Polarity dataset and perform word-level augmentations using NLPAug\nLibrary [Ma, 2019] by randomly replacing words with their synonyms using pretrained BERT [Devlin, 2018], intro-\nducing random spelling mistakes, adding/substituting words using Word2Vec [Mikolov, 2013]. In Table 3, we show\nthat data augmentation does improve the performance of LLama3.1-8B (+18.5%), but remains far from ideal clean\nperformance (+29.4%). We observe that ICL is extremely effective in improving zero-shot performance (33%\u219260%),\nbut worsens performance (-24.24%) when using the model fine-tuned on data generated by REGTEXT. We plan on\nincorporating more sophisticated defense techniques in future work."}, {"title": "RQ5: Is REGTEXT ranking better than choosing random words?", "content": "While Table 1 highlights that LMs are unable to\nlearn from $D_{train}^u$, the isolated effect of choosing words using REGTEXT rank is not known. To evaluate the effectiveness\nof the words identified by REGTEXT, we compare them against a dataset generated by randomly selected words from\nthe dataset vocabulary. We ensure that the random and REGTEXT identified words are both injected at the same\nlocations using Algorithm 1. Next, we finetune the LMs, and report the comparison in Table 4 showing that REGTEXT\nclearly outperforms the random baseline by a significant margin on both instruct (+2 vs -7) and non-instruct models\n(+20 vs +12)."}, {"title": "RQ6: What impact do finetuning parameters and REGTEXT's parameters have on test performance?", "content": "Here, we\nexamine how modifications in REGTEXT's and fine-tuning parameters of the LM affect the testing performance, and\nwhether adding random words have the same affect as word identified by REGTEXT ranking."}, {"title": "5 Conclusion and Limitations", "content": "In this paper, we have explored the first attempt to operationalize one principle of \"right to protect data\" into algorithmic\npractice, where we propose REGTEXT, a model-agnostic data generation framework that limits learning in LMs. In\ncontrast to existing works, our method doesn't use any model-dependent bi-level optimization and works even on\nLLMs like GPT-40-mini. Our extensive empirical (Sec. 4.2) studies highlight the motivation and effectiveness of\nREGTEXT. In particular, we show that REGTEXT outperforms existing baselines like error-minimizing noise across\nthree datasets and six LMs (Table 1). REGTEXT has a broad impact on public data and the NLP community, highlighting\nthe vulnerability of LMs in doing shortcut learning and showing the impact of REGTEXT on diverse public datasets.\nFinally, we demonstrate the imperceptibility of our added poisons by comparing the distribution of clean vs. REGTEXT\ndata (Table 2) distribution and the consistency of our proposed method across different fine-tuning settings. While\nREGTEXT shows initial promise in generating unlearnable text data and opening up new frontiers in operationalizing\nthe right to protect data, there are still many practical limitations which we discuss below.\nLimitations. Since our proposed data generation framework is model-independent, we do not use any particular\ntokenizers used by state-of-the-art LMs in processing our datasets. Our vocabulary is created by splitting text sequences\ninto individual words using white-space characters. While this works for text in English language, splitting text in other\nlanguages like Chinese and Japanese that do not have spaces is non-trivial. We aim to explore novel techniques in\ncreating model-independent vocabulary and scale REGTEXT for other languages in future work. Further, while our runs\nacross different seeds demonstrate the effectiveness of REGTEXT in generating unlearnable data, the data-generating\nprocess is highly dependent on the seed as it determines the location of the added perturbation. We plan to reduce this\nstochasticity in our future work."}, {"title": "A Implementation Details", "content": "A.1 Dataset Details\nWe consider three datasets: IMDb Maas et al. [2011], AGNews Zhang et al. [2015], and Natural Instructions 'Polar-\nity' Wang et al. [2022b]. i) IMDb dataset consists of movie reviews with two sentiment classes (\u201cPositive\u201d, \u201cNegative\u201d)"}]}