{"title": "ROBOTOUILLE: AN ASYNCHRONOUS PLANNING BENCHMARK FOR LLM AGENTS", "authors": ["Gonzalo Gonzalez-Pumariega", "Leong Su Yean", "Neha Sunkara", "Sanjiban Choudhury"], "abstract": "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce ROBOTOUILLE, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-0) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available here.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated impressive reasoning and task planning capabilities in short-horizon single-agent environments with clearly defined sequential tasks (Yao et al., 2022; 2023b; Shinn et al., 2023); however, decision-making in the real world introduces a more intricate array of challenges. Consider an assistant that helps you with cooking a recipe. It must be able to handle (1) time delays such as boiling spaghetti, which takes time to complete. An efficient agent would move onto other steps instead of waiting for the spaghetti to fully cook. It should also handle (2) diverse long-horizon tasks that require the assistant to satisfy multiple objectives and reason about dependencies between different actions. Finally, the assistant should handle (3) multiple agents by coordinating with others or distributing tasks based on each agent's capability. To tackle these challenges, an agent must be capable of asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially. With this capability, an agent can coordinate time delays, break down long horizon tasks into subtasks, and efficiently assign subtasks to multiple agents.\nTo improve asynchronous planning capability, we are interested in a benchmark (Table 1) that stress tests LLM agents using time delays. AsyncHow Lin et al. (2024) benchmarks asynchronous planning but does not use an interactive environment, lacking support for closed-loop planning agents. ALFWorld (Shridhar et al., 2021), WebShop (Yao et al., 2023a) and PlanBench (Valmeekam et al., 2023b) offer long-horizon diverse tasks (up to 50, 48 and 90 steps respectively) but evaluate with a single agent and no time delays. VirtualHome (Puig et al., 2018) offers long-horizon (up to 96 steps) and multi-agent tasks with procedural generation for extra diversity but also lacks time delays.\nTo address these gaps, we introduce ROBOTOUILLE, a simulator for cooking diverse recipes designed to stress test LLM agents (Figure 1). ROBOTOUILLE tests asynchronous planning through tasks that take time like cooking meat for burgers or sandwiches or filling up a pot with water to cook soup. Its fully customizable JSON backend allows for the addition of new states, actions, and goals simplifying the creation of diverse long-horizon tasks. Finally, ROBOTOUILLE supports turn-based and real-time multi-agent execution either locally or on the network."}, {"title": "2 ROBOTOUILLE", "content": "We formalize ROBOTOUILLE tasks as an MDP with time-delayed effects, $M =< S, A,T,R >$. Each state $s \u2208 S$ is $s = ($t, Ht) where \u015dt represents observable state elements like objects or predicates such as iscut (lettucel),or \"lettucel is cut\", and on(lettucel,table2), or \"lettucel is on table2\", and Ht is a set of timer variables h \u2208 Ht each created by actions with a countdown function $h(x) = d\u2212(x-i)$ where d is a delay constant and i is the timer's activation step. Action a \u2208 A is a grounded action such as move (robotl, tablel, table2),or \"Move robot1 from table1 to table2\" that may introduce new timers h. Actions have preconditions over state predicates which must be met to be valid. For a given state s and action a, the transition function $T: S \u00d7 A \u2192 S$ returns the next state $s' = (\u015dt+1, Ht+1)$ if a is valid or the current state s if a is invalid. For a valid action step, $\u015dt+1 = \u015dt \u222a {predicates(h)|h \u2208 Ht, h(t) = 0}$ to removes expired timers and $Ht+1 = (Ht - {h|h(t) = 0}) \u222a {h|a adds delay}$ to update active timers. The"}, {"title": "3 DATASET DETAILS", "content": "In this section we discuss the contents of the synchronous and asynchronous datasets and their differences. We provide discussion of the in-context example tasks and multi-agent dataset in Appendix A.3. Each dataset contains 10 unique tasks and has 10 procedurally generated instances. Table 3 and Appendix A.6 include visual representations of the tasks and dependency graphs respectively.\nSynchronous Dataset This dataset consists of tasks involving assembling sandwiches and burgers with ingredients that may need to be cut. Any ingredients that can be cooked are initialized as cooked. Tasks 1 to 3 involve assembling sandwiches of increasing difficulty where Task 1 only involves assembling and Task 2 and 3 involve cutting ingredients. Tasks 4 to 7 involve assembling burgers which differ from sandwiches in that the burger buns have ordering constraints with distinct buns that go on the top and the bottom. Unlike other tasks, Task 6 enforces a strict ordering constraint on the placement of all ingredients. Finally, Tasks 8 to 10 involve the preparation of 2 recipes which increase in difficulty from identical sandwiches, identical burgers, and finally a sandwich and burger with different ingredients.\nAsynchronous Dataset This dataset consists of tasks including sandwiches and burgers from before but also fried recipes and soup. Unlike the synchronous dataset, ingredients that can be cooked are initialized as uncooked; this allows for asynchronous planning. Tasks 1 to 3 use the same ingredients as those in the synchronous setting except for an added ingredient which must be cooked or fried. We studied these tasks in Appendix A.16 for a closer one-to-one comparison with synchronous tasks and found that asynchronous tasks are more difficult. Tasks 4 and 5 involve making a burger and a fried recipe; Task 4 includes french fries which requires cutting a potato then frying while Task 5 includes fried onions which is the same process with an onion. Tasks 6 to 7 introduce a new recipe, soup, which involves filling a pot with water from a sink, boiling the water, putting ingredients inside, and finally serving in a bowl. Of these subtasks, filling a pot with water and boiling the water are"}, {"title": "4 EXPERIMENTS", "content": "We evaluate LLMs on ROBOTOUILLE using the following baselines: I/O, I/O CoT, and ReAct. I/O takes as input the initial state, including valid actions and goal, and outputs an plan directly. I/O COT (Wei et al., 2023) also takes as input the initial state but outputs a plan with chain of thought before each action that estimates the resulting state. Instead of outputting the entire plan, ReAct (Yao et al., 2022) outputs reasoning and the next action given the current state, and receives the next state before repeating. We use an ablated version of ReAct that only keeps the reasoning and action of the previous timestep in context (along with the base prompt and in-context examples); the improved performance and cost-effectiveness is detailed in Appendix A.7. Each baseline receives a single in-context example on a training example excluded from the testing set. We use temperature 0.7 for all models. All prompts and few-shot examples are located in our codebase here."}, {"title": "4.2 RESULTS AND ANALYSIS", "content": "\u2022 Closed-loop agents are superior: The best baseline, gpt4-0 ReAct, achieves 47% on the synchronous dataset and 11% on the asynchronous dataset, surpassing open-loop approaches I/O and I/O COT (Finding 1, Sec 4.2.2).\n\u2022 Poor feedback incorporation leads to decreased asynchronous performance: Despite being closed-loop, gpt4-0 ReAct failures often make little progress towards the goal (Finding 3, Sec 4.2.2) due to poor failure recovery (Finding 5, Sec 4.2.3). We find that boosting priors improves performance (Finding 7, Sec 4.2.4) but discuss better feedback methods in Section 5.\n\u2022 Synchronous and asynchronous failures are closely related: Both synchronous and asynchronous failures are dominated by rule violations and goal misinterpretation (Finding 4, Sec 4.2.3). We hypothesize that this is due to poor failure recovery (Finding 5, Sec 4.2.3) and agents that recover efficiently could boost performance in both settings.\n\u2022 Task prioritization is critical in asynchronous planning: Proper prioritization of subtasks in asynchronous settings significantly boosts performance (Finding 6, Sec 4.2.4)."}, {"title": "4.2.2 SUCCESS AND OPTIMALITY", "content": "Table 2 shows the success rates of various LLMs baselines on the synchronous and asynchronous datasets. Table 3 shows the task-specific success rates of baselines using gpt 4-0. Success rate is determined by reaching the goal within 1.5 times the optimal number of steps for the given instance. Baselines exceeding this step limit are terminated.\nAmong all the LLM baselines, ReAct with the gpt 4-0 model performs the best on the synchronous and asynchronous datasets. I/O performs worst for most LLMs while I/O COT improves performance. We qualitatively observed gemini-1.5-flash failing with ReAct since it attempts to solve the few-shot example goal rather than the current environment goal. This is likely due to the long context examples, aligning with findings in Liu et al. (2023c) where LLMs struggle with simple tasks in long contexts. We investigate PLaG (BaG) Lin et al. (2024) and Reflexion Shinn et al. (2023) performance on the asynchronous dataset in Appendix A.15 and achieved small performance improvements.\nWhen considering task-specific success over gpt4-0 baselines, ReAct generally achieves higher performance per task. While we list the horizon length as a crude difficulty metric, it is evident that success rate is not solely dependent on it. We investigate this further in Appendix A.8. We also investigate different agent failure modes in more depth in Section 4.2.3.\nFinding 2. Asynchronous successes are less optimal than synchronous ones.\nFinding 1. Closed-loop baselines outperform open-loop baselines."}, {"title": "4.2.3 FAILURE MODE ANALYSIS", "content": "Fig. 5 shows a nested piechart that captures failure modes of gpt 4-0 ReAct on the synchronous and asynchronous datasets. We define our failure modes in terms of uncertainty over the MDP of the environment. The 4 main failure categories include uncertainty in the state (S), actions (A), transition function (T) and the goal (G). For a detailed description of the subcategories and dataset annotation, see Appendix A.10.\nFor synchronous failures, the uncertainty in the goal accounts for the majority at 64.1% followed by the uncertainty in the transition function at 32.1%. Goal failures could be due to (1) an incorrect understanding at the start of the plan or (2) a mistake during plan execution, such as using an ingredient without cutting it, which is incorrectly believed to satisfy the goal. We observe that case (1) occurs 28.3% of the time under Bad Start; the LLM agent restates goals incorrectly for complex tasks with strict ordering dependencies like Task 6 or tasks with many diverse ingredients like Task 10 which we show in Appendix A.12. We observe that case (2) occurs 35.8% of the time under the remaining subcategories; although the LLM agent starts with a correct goal, it misunderstands the goal during execution by choosing the wrong action. For transition failures, violating the 'one item at a station' rule accounts for the majority of failures at 24.5%. We qualitatively observe that the agent attempts to use cutting stations for ingredient preparation while other items occupy the station; however, we also observe that once the agent has recovered from this failure it is unlikely to repeat it which we show in Appendix A.14.\nFor asynchronous failures, the inverse is true with uncertainty in the transition function accounting for 56.8% of failures and uncertainty in the goal accounting for 34.1% of failures. Similar to the synchronous failures, violating the 'one item at a station' rule dominates failures at 53.4%. This is due to the increased number of unique stations in the asynchronous setting compared to the synchronous setting which increases the potential number of recoveries necessary. In the synchronous setting, which only uses the cutting board station, an agent may need to recover once from violating the 'one item at a station' rule. In the asynchronous setting, which uses stoves, fryers, and sinks, an agent, in the worst case, may need to recover from violating rules on each station in a task.\nWe point out that while we designed the synchronous and asynchronous datasets to test different capabilities of LLM agents, we mainly observe similar transition failures in both settings. This demonstrates the need to improve LLM agents at following environment constraints to improve their decision-making ability. We investigate this further in Section 4.2.4.\nFinding 4. Dominant failures in both settings stem from rule violations and goal misinterpretations."}, {"title": "4.2.4 FOLLOW-UP INVESTIGATION", "content": "From the previous experiments, we conclude that LLM agents struggle in the asynchronous dataset due to simple failures that arise in the synchronous dataset. In order to have a better understanding of how to improve LLM agent capabilities on asynchronous planning, we look into asynchronous subtask prioritization and boosting performance.\nFinding 6. Proper asynchronous prioritization boosts performance.\nEfficient asynchronous planning requires prioritizing subtasks that can be performed asynchronously. We investigate how success rate changes with asynchronous task prioritization to understand the impact of asynchronous planning on the results. Our hypothesis is that prioritizing asynchronous subtasks leads to higher success rates because the planned trajectory is shorter and reaches the goal within the maximum step limit. We find that the success rate conditioned on prioritization is 16% compared to 6% without, supporting that prioritization achieves higher success rate. An agent should be capable of auditing its own reasoning and plan to ensure that its prioritization correctly targets asynchronous subtasks. We discuss methods for reliable self-verification in Section 5.\nFinding 7. Stronger priors improve asynchronous performance.\nThe dominant failures of gpt 4-0 ReAct on the asynchronous dataset were transition failures. We investigate how we can improve performance by increasing the priors over the transition function. We create an augmented method, ReAct + Prior, that prompts ReAct with more details about the rules of ROBOTOUILLE."}, {"title": "5 DISCUSSION", "content": "In this paper we propose a new benchmark, ROBOTOUILLE, for stress testing LLM agents on synchronous, asynchronous, and multi-agent settings. We evaluate state-of-the-art LLMs and expose their dominant failure modes are similar across synchronous and asynchronous settings. We perform follow-up studies to bring up performance and uncover the need for improvements in LLM agents that we discuss below.\nFeedback Incorporation A general method to incorporate long-horizon planning feedback in LLM agents is to include all interactions in the context history. This works well for models with large context windows or near-infinite attention mechanisms (Liu et al., 2023b; Munkhdalai et al., 2024), but LLMs often struggle with long-contexts (Liu et al., 2023c). An alternative is RAG (Lewis et al., 2021), yet this shifts the complexity to retrieval. As explored in Section 4.2.4, a promising approach is for the agent to summarize interactions into facts to reduce uncertainty and strengthen priors. It should also reason about future states to avoid myopic behaviors, as shown qualitatively in Appendix A.11. Another underexplored yet effective approach is finetuning LLM agents (Chen et al., 2023) with methods such as TD learning and value propagation (Putta et al., 2024; Gehring et al., 2024).\nSelf-Verification An LLM agent should be able to audit but LLMs are unreliable at self-verification (Valmeekam et al., 2023a). Other approaches use LLMs to create a representation for external planners (Liu et al., 2023a; Guan et al., 2023) or finetune on planning datasets (Pallagani et al., 2022; Lehnert et al., 2024) but these methods are difficult to debug and lack guarantees respectively. One approach is to combine code-use with language (Wang et al., 2024); reasoning in language and verifying understanding with code and APIs would allow us stronger guarantees that are easier to debug.\nReal-World Application To effectively deploy LLM agents on real-world agents, the cost and inference time of LLMs must be brought down to make them affordable and quick. This is especially problematic for long-horizon task planning since cost and inference time increases as context grows. These system must also be evaluated with real humans; one future direction for Robotouille is serving as an online platform to test agents with humans through collaboration."}, {"title": "A APPENDIX", "content": "In this section we will focus on our desiderata for LLM assistants and how ROBOTOUILLE is different from other related works (Table 1).\nAsynchronous Planning Many benchmarks evaluate the task planning abilities of LLM agents (Shridhar et al., 2021; Gong et al., 2023; Liu et al., 2018; Valmeekam et al., 2023b; Yao et al., 2024; Zhou et al., 2024; Yao et al., 2023a) but few test the ability to plan asynchronously. Existing work relevant to asynchronous planning evaluate LLM capabilities on temporal logic (Wang & Zhao, 2024) or use graph-based techniques (Wu et al., 2024); (Besta et al., 2024)) but do not focus on it. (Lin et al., 2024) proposes the Plan Like a Graph technique and a benchmark AsyncHow that focuses on asynchronous planning but makes a strong assumption that infinite agents exist. (Carroll et al., 2020) proposes a benchmark, Overcooked-AI, that involves cooking onion soup which has time delays but has limited tasks and focuses on lower-level planning without LLM agents. ROBOTOUILLE has a dataset focused on asynchronous planning that involves actions including cooking, frying, filling a pot with water, and boiling water.\nDiverse Long-Horizon Task Planning There is vast amount of work that use LLMs to plan (Ahn et al., 2022; Huang et al., 2022; Zeng et al., 2022; Liang et al., 2023; Singh et al., 2022; Song et al., 2023; Yang et al., 2023; Song et al., 2023) but they tend to evaluate on short-horizon tasks with limited diversity in tasks. We present the number of tasks, longest plan horizon, and procedural generation capability of various benchmarks in Table 1 to capture these axes. Notable LLM agent benchmarks that capture these axes include PlanBench (Valmeekam et al., 2023b), WebShop (Yao et al., 2023a), and VirtualHome (Puig et al., 2018). ROBOTOUILLE provides a focused set of diverse long-horizon tasks that can be procedurally generated.\nMulti-agent Planning LLM agent benchmarks like (Liu et al., 2023d; Xu et al., 2023; Ma et al., 2024; Gong et al., 2023) evaluate multi-agent interactions but do not involve time delays. OvercookedAI (Carroll et al., 2020), while not an LLM agent benchmark, incorporates time delays which brings the complexity of asynchronous planning to multi-agent settings. ROBOTOUILLE provides a multi-agent dataset for 2-4 agents, a choice between turn-based or realtime planning, and incorporates asynchronous tasks for added complexity."}, {"title": "A.2 ADDITIONAL ROBOTOUILLE JSONS", "content": "To provide flexibility in task and environment creation, an environment JSON (Figure 8) is used to define the problem. The size of the grid used can be specified, and positions of objects in the item can be specified using coordinates. Predicates that are specific to an item can also be specified. In conjunction with the flexible goal creation described in Section 2, objects in the environment can be given specific ids, if the goal must be satisfied for specific objects. Additionally, if the environment requires a different number of cuts to complete cutting, or a different cook time, these values can be configured in the JSON.\nAdding objects to the environment is also simple. To add a new object, the necessary predicates for that object can be added to the domain JSON, and its corresponding image can be added to the"}, {"title": "A.3 ADDITIONAL DATASET DETAILS", "content": "Multiagent Dataset This dataset consists of tasks designed to test the LLM agent's multiagent capabilities. Robotouille's multiagent capabilities test the agent's ability to collaborate, and is more difficult because it includes tasks where agents may potentially interfere with one another, and share resources with one another.\nTasks 1-3: Burgers The first 3 tasks involve cooking and assembling a burger with increasing levels of difficulty. In Task 1, the agents need to chop lettuce, and cook a patty, before assembling the burger with the patty, lettuce, a bottom bun, and a top bun. In Task 2, the complexity is increased with an additional ingredient, a tomato, that needs to be cut and stacked onto the burger. In Task 3, lettuce needs to be cut, a chicken needs to be fried, and an onion needs to be cut first before it is fried. This adds a level of complexity because one of the ingredients, the onion, needs to be both cut and fried.\nTasks 4-6: Sandwiches Tasks 4 - 6 involve making sandwiches. Unlike the tasks which only involve a single agent, sandwiches in multiagent environments are more complex than burgers because there is ambiguity in the stack ordering. In burgers, the bottom bun needs to be at the bottom, while in sandwiches, a piece of bread can be used as either the bottom bread or the top bread. This is complex"}, {"title": "A.4 RELATED WORKS TABLE DATA", "content": "For each benchmark in (Table 1), we explain how the number of tasks and longest horizon plans were calculated."}, {"title": "A.4.1 ALFWORLD", "content": "ALFWorld consists of 3827 different tasks consisting of 3,553 train tasks, 140 seen tasks, and 134 unseen tasks from the ALFRED dataset.\nThe longest horizon plan is 50 steps since 50 is the max number of steps per episode in ALFWorld."}, {"title": "A.4.2 CUISINE WORLD", "content": "CuisineWorld consist of 33 unique dishes which represent the tasks.\nThe longest horizon plan is 11 steps since Figure 2 of CuisineWorld indicates the dish distribution over the number of steps."}, {"title": "A.4.3 MINIWOB++", "content": "MiniWoB++ consist of 40 tasks since 40 tasks are filtered out of 80 total tasks from the MiniWOB benchmark.\nThe longest horizon plan is 13 steps since Table 1 indicates that 13 is the maximum number of steps needed for a perfect policy to complete the task."}, {"title": "A.4.4 OVERCOOKED-AI", "content": "Overcooked-AI consists of 1 task since onion soup is the only dish in the environment.\nThe longest horizon plan is 100 steps since 100 is the max number of timesteps that planning methods are evaluated on."}, {"title": "A.4.5 PLANBENCH", "content": "PlanBench consist of 885 tasks consisting of 600 tasks from Blocksworld domain and 285 tasks from the Logistics domain.\nThe longest horizon plan is 48 steps since Figure 3 in PlanBench indicates that 48 is the longest optimal plan length from both the Blocksworld and Logistics problem sets."}, {"title": "\u0410.4.6 \u0442-BENCH", "content": "T-bench consist of 165 tasks consisting of 115 tasks from the T-retail benchmark and 50 tasks from the T-airline benchmark.\nThe longest horizon plan is 30 steps since 30 is the max number of actions per task in 7-bench."}, {"title": "A.4.7 WEBARENA", "content": "WebArena consist of 812 long-horizon web-based tasks.\nThe longest horizon plan is 30 steps since 30 is the max number of state transitions in WebArena."}, {"title": "A.4.8 WEBSHOP", "content": "WebShop consist of 12087 crowd-sourced text instructions which represent tasks.\nThe longest horizon plan is 90 steps since 90 is the max number of state visited in Table 2 of WebShop."}, {"title": "A.4.9 AGENT BENCH", "content": "AgentBench consist of 8 environments which represent tasks.\nThe longest horizon plan is 35 steps since 35 is the largest number of average turns according to table 3 in AgentBench."}, {"title": "A.4.10 ARA", "content": "ARA consists if 12 real-world tasks.\nThe longest horizon plan is 4 steps after counting the number of steps in the description of each task in Table 1 of ARA."}, {"title": "A.4.11 ASYNCHOW", "content": "AsyncHow consists of 1600 high-quality instances for real-life tasks.\nThe longest horizon plan is 9+ steps after checking Figure 5 of AsyncHow."}, {"title": "A.5 MAGIC", "content": "MAgIC consists of 5 games which represent tasks.\nWe will assume all games will have 3 players and the same number of rounds as indicated in Table 3 of magic (1 round for Chameleon, 2 for Undercover, and 5 for Cost Sharing, Prisoner's Dilemma, and Public Good).\nCalculations of longest plan with regards to steps:\nChameleon: (3 clues given out to participants + 3 accusations/votes from participants + 1 guess for the final word if the chameleon is correctly identified) * 1 round = 7 steps\nUndercover: (3 people are assigned groups + 3 clues are given from participants + 3 votes from participants) * 2 rounds = 18 steps\nCost Sharing: 3 parties get allocation of money + (1 negotiation phase + 1 fairness check) * 5 rounds = 13 steps\nPrisoner's Dilemma: 3 decisions from participants * 5 rounds = 15 steps\nPublic Good: (3 decisions from participants + 1 redistribution of money) * 5 rounds = 20 steps\nTherefore, Public Good has the longest horizon plan with 20 steps."}, {"title": "A.5.1 T-EVAL", "content": "T-Eval consists of 23305 tasks according to Table 2 in T-Eval.\nThe longest horizon plan is 19 steps based on Figure 5b in T-Eval."}, {"title": "A.5.2 MLAGENTBENCH", "content": "MLAgentBench consists of 13 ML tasks from diverse domains ranging in difficulty and recency.\nThe longest horizon plan is 50 steps based on Figure 7 in MLAgentBench which describes the distribution of numbers of steps used by agents."}, {"title": "A.5.3 GAIA", "content": "GAIA consists of 466 carefully crafted and human annotated questions.\nThe longest horizon plan is around 45 steps based on Figure 3 in GAIA which describes the distribution of numbers of steps taken and tools used to answer the 466 questions."}, {"title": "A.5.4 VIRTUALHOME", "content": "VirtualHome consists of 2821 programs which represent tasks.\nThe longest horizon plan is 96 steps after examining all the activities in VirtualHome's Activity Knowledge base and finding the longest."}, {"title": "A.6 TASK DEPENDENCY GRAPHS", "content": "In general, the ordering of ingredients for task dependency graphs does not matter unless specified. For soups, though the task dependency graphs imply a certain order, vegetables can be added to the pot as long as the pot contains water. In addition, all items are placed on the table."}, {"title": "A.6.1 SYNCHRONOUS GRAPHS", "content": ""}, {"title": "A.6.2 ASYNCHRONOUS GRAPHS", "content": ""}, {"title": "A.6.3 MULTI-AGENT GRAPHS", "content": ""}, {"title": "A.7 REACT ABLATIONS", "content": "ReAct in its original form can grow very expensive in cost on long horizon tasks due to the increasing context size. We sought to perform early ablations of ReAct to find a cost-effective variant whose performance is relatively the same. We first ablated on the types of feedback from feedback at all (\"no-history\") to ablating away components of the feedback from the last time step (where \"last-obs-reasoning-action\" represents the last timestep with all feedback, \"last-reasoning-action\" represents the last timestep with only the reasoning and action, and \"last-action\" represents the last timestep with only the last action. Next, we tested two different types of reasoning; one where we simply prompt ReAct to reason about the given information and another where we make it provide a plan in its sequence before outputting a single action (which we've termed \"mpc\" after Model Predictive Control). From these ablations on a small subset of data, we determined that \"last-reasoning-action-mpc\" was the best performing and inexpensive as shown in Table 4."}, {"title": "A.8 DIFFERENCES IN HORIZON PERFORMANCE", "content": "In Table 3 we observe that horizon length does not necessarily correlate with success. The main confounding variable is the quality of few-shot examples. Each dataset provides a single optimal few-shot example from a training task excluded from the testing set. This example is insufficient when the LLM agent makes a mistake because it has not seen examples of incorporating state feedback to recover from failure. The LLM agent, therefore, acts in an open-loop manner.\nIn the synchronous dataset, Task 5 is more complex than Task 4, yet it has a higher success rate. This is because Task 5 is more aligned to the few-shot example, sharing a common sub-trajectory (i.e. stacking cheese). This similarity allows ReAct to stay within the distribution of the example, leading to fewer mistakes. In contrast, Task 4 deviates more from the example, resulting in ReAct making mistakes it cannot recover from.\nSimilarly, in the asynchronous dataset, we also observe that Task 1 < Task 2 < Task 3 despite having increasing complexity. Task 2 and 3 are more aligned to the few-shot example, sharing common"}, {"title": "A.9 WHY IS ASYNCHRONOUS HARDER THAN SYNCHRONOUS?", "content": "The complexity of search for synchronous and asynchronous given the MDP in Section 2 is:\n1. Synchronous Case (d = 0): No delays, so the planner operates in $O(|S| + |A|)$\n2. Asynchronous Case (d > 0): Each delay expands the effective state space, yielding $O(|S| \u00d7 (d+1) + |A|)$ complexity, where n is the number of timers\nHence the expanded state space requires both a conventional planner or a LLM based planner to reason over a larger range of delayed effects."}, {"title": "A.10 FAILURE MODE TAXONOMY DETAILS", "content": "Markov Decision Process To categorise the failure modes, we used the Markov Decision Process (MDP), where there are 4 main failure modes:\n1. State failures\n2. Action failures\n3. Transition Function failures\n4. Goal failures\nWe chose to use the MDP because LLMs know the MDP of the real world very well, but struggle to learn and understand the MDP of new enviornments, such as Robotouille. In using the MDP as a system to categorise failures, we are better able to see how the LLM bridges its knowledge of the real world to its understanding of new environments.\nState Failures A failure is categorised as a state failure when the agent misunderstands predicates in the state, and fails as a result of this misunderstanding. For example, when preparing a chicken cheese sandwich, the LLM agent may mistake the chicken to be already cooked, when it is not. Even though the predicates are true in the state, the agent misinterprets the predicates, causing it to take the wrong action and preventing it from achieving the goal successfully."}, {"title": "A.11 QUALITATIVE PLANNING FAILURE EXAMPLE", "content": "To gain more insight into why LLM agents are unable to efficiently complete asynchronous tasks successfully, we created a new baseline by repeating the rules to the agent before every action. However, this caused the agent to sometimes be even more inefficient, as it naively tries to follow rules without proper planning.\nIn this example, the agent needs to cook a chicken on a stove, but all the stoves are currently occupied. It first follows the rule \"A Station must contain a single Item to perform an action on it\" and picks up the item from the stove. Then, it follows the rule \"A Player can only hold a single Item at a time\". Since it is currently holding the item it does not need, and it wants to cook the chicken, it places the item back onto the stove to free its hands. This causes the agent to repeatedly pick up the item from the stove and place it back onto the stove. (Fig. 41)"}, {"title": "A.12 QUALITATIVE BAD START FAILURES", "content": "5_double_cheeseburger_ 42 In this task, the agent misunderstands the goal from its very first line of reasoning. The goal is to make a double cheeseburger on a table, with a bottom bun, cooked patty, cheese, cooked patty, cheese, and a top bun, stacked in that order. However, the agent misunderstands the goal"}]}