{"title": "Mutation-Guided LLM-based Test Generation at Meta", "authors": ["Christopher Foster", "Abhishek Gulati", "Mark Harman", "Inna Harper", "Ke Mao", "Jillian Ritchey", "Herv\u00e9 Robert", "Shubho Sengupta"], "abstract": "This paper\u00b9 describes Meta's ACH system for mutation-guided LLM-based test generation. ACH generates relatively few mutants (aka simulated faults), compared to traditional mutation testing. Instead, it focuses on generating currently undetected faults that are specific to an issue of concern. From these currently uncaught faults, ACH generates tests that can catch them, thereby 'killing' the mutants and consequently hardening the platform against regressions. We use privacy concerns to illustrate our approach, but ACH can harden code against any type of regression. In total, ACH was applied to 10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from which it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also deploys an LLM-based equivalent mutant detection agent that achieves a precision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple pre-processing). ACH was used by Messenger and WhatsApp test-a-thons where engineers accepted 73% of its tests, judging 36% to privacy relevant. We conclude that ACH hardens code against specific concerns and that, even when its tests do not directly tackle the specific concern, engineers find them useful for their other benefits.", "sections": [{"title": "1 INTRODUCTION", "content": "In this paper, we report on Meta's deployment of ACH2, an agentic LLM-based tool for generating tests to target specific classes of faults. The paper focuses on automated privacy hardening: the problem of automatically generating unit tests to reduce the risk of future regressions with respect to privacy issues. However, the approach can be applied to any issue and is not confined solely to tackling privacy issues. The paper reports results from the deployment of ACH at Meta between 28th October 2024 and 31st December 2024.\nAlthough there has been a great deal of recent attention on LLM-based test generation [3, 18, 39, 52, 53], there has been little work on developing tests for specific classes of fault. Many companies have exposure to specific high impact faults related to important issues such as security, integrity, and privacy. The importance of such issues makes it equally important to have test generation techniques that can target these specific classes of faults.\nOrganizations typically collect data about bugs found during development. This provides a rich source of information with which to guide test generation. The challenge, therefore, is to find a way to generate tests that target specific issues on the basis of this information. We believe mutation testing holds the key: our key insight is to construct mutants that denote faults that are both relevant to the issue of concern and also currently not caught (unkilled) by any existing test case, and to use these as prompts to LLM-based test generation. This results in an overall agenetic LLM-based workflow, in which agents essentially generate problem-specific 'super bugs' and the tests that can catch them."}, {"title": "2 THE ACH SYSTEM", "content": "Figure 1 presents the overall architectural pipeline of the ACH System. ACH starts with free from text about an issue of concern. This textual input could come from one or more of a variety of sources, including (but not limited to):\n(1) Previous faults found in development;\n(2) User requirements;\n(3) System technical constraints;\n(4) Concerns raised by engineers, managers or organizational leadership about undesirable system behaviours;\n(5) Regulatory requirements set by legislative bodies and compliance enforcement organizations.\nThis is the sense in which ACH is a 'compliance hardener': it improves ('hardens') the ability of the deployed regression test infrastructure to detect regressions that might lead to non-compliance with respect to the issue of concern.\nThe results presented in this paper were obtained by using privacy hardening concerns from previous faults found in development. The single language model Llama 3.1 70Bn [42] was used in all the agents reported on. The prompts used by the three LLM agents from Figure 1 can be found in Table 1.\nOur focus is the development, deployment and evaluation of mutation-guided agenetic workflows. We have not yet felt the need to extend to more sophisticated prompting, nor to use fine-tuning,"}, {"title": "3 RESULTS FROM META'S ACH DEPLOYMENT", "content": "We applied ACH to the 7 Meta platforms Aloha, Facebook Feed, Instagram, Messenger, Oculus, Wearables, and WhatsApp. Facebook"}, {"title": "4 ENGINEERS' EVALUATION OF ACH", "content": "We followed the tried-and-tested formula [3] for the deployment of a new software testing technology at Meta, starting with initial trials and moving onto to test-a-thons led by engineers, thereby evaluating initial deployment."}, {"title": "4.1 Initial Trial", "content": "At Meta, a code change submitted to the Continuous Integration (CI) system is called a 'diff' (short for differential). In order to get initial feedback from engineers on the tests generated by the first version of ACH, we used it to generate 30 new test cases, submitting each as a separate diff for review. The generated tests were recommended to engineers in the same way as any other code modification, such as those created by human engineers. That is, ACH submits tests, as diffs, through the normal review process. The ACH diff summary explains the kind of fault caught by the test case giving, as a specific example, the mutant that ACH has already determined to be killed by the test case.\nWe wanted to obtain broad coverage of Meta's platforms, so included messaging apps like WhatsApp and Messenger, traditional social media platforms such as Facebook Feed, as well as hardware"}, {"title": "4.2 Experience from Privacy Test-a-thons", "content": "In the week of 9th December 2024, we conducted two test-a-thons, focusing on the application of ACH to Meta's two messaging platforms: WhatsApp and Messenger.\nAs with the initial trial, test cases were submitted as diffs into the normal continuous integration review process. However, the diff summary additionally claimed additional coverage, for those tests that did add coverage as well as finding currently uncatchable faults. The engineers participating in the test-a-thons reviewed the diffs for usefulness in the normal way they would any other diff, ultimately determining whether the diff is accepted, and thus lands into production.\nIn order to evaluate the privacy relevance of each test, we additionally asked the software engineers to give each a score on a Likert scale [32]. Figure 2 is a screen capture of the exact instructions given to the engineers regarding this privacy relevance scoring procedure.\nProcedure for Messenger two-phase test-a-thon: The Messenger test-a-thon was conducted in two phases. The same 6 reviewers were used for both the first and the second phase. All Messenger reviewers had strong expertise in testing, and (at least a) basic experience with privacy engineering.\nIn the first phase, 50 generated tests were selected from a randomly selected pool of 60 tests. All 60 were prescreened manually"}, {"title": "5 THE EQUIVALENT MUTANT PROBLEM", "content": "All approaches to mutation testing need to tackle the problem of equivalent mutants [35]: the mutant may be syntactically different to the original program, but we cannot guarantee it will be semantically different, because the underlying program equivalence problem is undecidable [27, 59].\nThere are a number of techniques in the literature that can weed out some of the equivalent mutants [40]. However, the undecidability of the problem means that some equivalent mutants will inevitably remain, so engineers and automated test generators may waste time trying to kill (unkillable) equivalent mutants."}, {"title": "5.1 Equivalent Mutants Have no Impact on ACH", "content": "For the application of mutation-guided test generation, the equivalent mutant problem has no direct impact on engineers. Our workflow requires only that engineers review test cases, not mutants. The only scenario in which an engineer might consider looking at a mutant, would be to see an example of the kind of faults that can be caught by the test case they are reviewing. By construction, such mutants are non-equivalent, so the engineer will never see an equivalent mutant. This relegates the equivalent mutant problem to a relatively subordinate position in our overall use case for mutation testing.\nNevertheless, it would be inefficient to generate many equivalent mutants, because ACH would waste computational resources trying to kill the unkillable. We therefore incorporate, into the agentic workflow, an LLM-based agent that checks for mutant equivalence. The remainder of this section reports on the evaluation of the effectiveness of this agent."}, {"title": "5.2 Detecting Equivalent Mutants", "content": "To evaluate the performance of the equivalence detector agent from Figure 1, we performed a manual study on a random selection of mutants drawn from the four platforms with the most mutants available. The purpose of this manual analysis is to answer the research question:\nHow good is the Equivalence Detector Agent?"}, {"title": "5.3 Equivalence Detector Precision and Recall", "content": "Table 6 reports the overall precision and recall of the equivalence detector agent. The detailed results for each platform are shown in Table 7. As Table 6 reveals, when we consider 'unsure' as the same as 'equivalent', the precision is good, at 0.79 over all platforms studied. However, recall is relatively low, at 0.47. If we wanted even greater precision we could treat 'unsure' as 'non-equivalent', since this gives precision of 0.97 (and a recall of 0.44).\nBased on this precision and recall, we can have high confidence that, when the detector determines a mutant to be equivalent, it is very likely to be correct. However, it weeds out only approximately half of the equivalent mutants. High precision means ACH will not discard many non-equivalent mutants. Not losing mutants means not losing the tests that ACH might generate from them. By contrast, higher recall would merely save some computational resource.\nAs such, high precision is generally more valuable than high recall for our use case, because we are prepared to spend computational resources to automatically generate good unit tests. However, the lower the recall, the more often ACH will inefficiently seek to 'kill the unkillable', so we still want the highest recall achievable without reducing precision.\nAs Table 2 shows, approximately 25% of all mutants are trivially equivalent, being syntactically identical to the original. Furthermore, Table 5 reveals that 61% of all equivalent mutants are almost trivially equivalent, because they contain only a single 'misleading' comment. Syntactically identical mutants are removed by a simple lexical comparison, while a simple pre-processing transformation to remove comments would remove the 'misleading comments' category. Therefore, we can combine the LLM-based detector with a simple rule based pre-processor to yield an overall precision and recall of 0.95 and 0.96 respectively (See Table 6).\nThese figures for precision and recall are surprisingly high, given that the underlying problem is undecidable. However, we cannot claim that these surprisingly good results arise because the mutant equivalence detector agent is excellent at determining program equivalence. Rather, they are more reflection of the kind of mutants the detector needs to judge for equivalence. That is, the mutant generation agent tends to make changes that either obviously introduce semantic changes, or are obviously equivalent (such as those that only change comments)."}, {"title": "6 THE IMPORTANCE OF MUTATION TESTING", "content": "It is well known in the literature on testing, both theoretically and empirically, that mutation adequacy criteria outperform traditional structural criteria, such as line coverage and branch coverage [17, 46]. In this section, we present results that further underline"}, {"title": "7 RELATED WORK", "content": "Given the strong empirical evidence for the predictability of code [10, 23, 28], it is unsurprising that predictive language models have proved effective at generating usable code. As a result, LLMs now play a code-generation role in a significant number of applications across the spectrum of software engineering activity [22].\nSoftware engineers' acceptance rates for LLM-generated code have been widely studied. For example, researchers at Google focused on factors that may influence trust in Al-powered code completions [13], while it was recently reported that Copilot had an acceptance rate of approximately 30% [55].\nHowever, such studies concern the code completion use case, which does not come with any assurances (the code completions may not even compile). By contrast, ACH uses Assured LLM-Based Software Engineering (Assured LLMSE) [7]. That is, ACH provides assurances about the semantics and usefulness of the tests it proposes. Its proposals are also whole compilable code units (not merely completion fragments), and it is deployed on-demand (to generate tests targeting classes of faults of particular interest), rather than opportunistically (to suggest code completions). The fact that ACH provides these assurances, and its different deployment route, mean that we can expect a higher overall acceptance rate from ACH, than we would for code completion technologies."}, {"title": "8 OPEN PROBLEMS", "content": "We outline directions for future work, hoping to stimulate the research community with new open research questions.\nMutant Equivalence: we found that the simulated privacy faults generated by language models tend to be bimodal, with the consequence that equivalent mutants are surprisingly easy to detect (See Section 5). It is possible that these results are merely specific to Kotlin, to Android, to Meta, or to simulated privacy faults.\nHowever, there is nothing in our approach that suggests that the results would fail to generalize. If they were to generalize, this would be an important finding for the mutation testing research agenda, given the significant impediment to deployment hitherto posed by mutant equivalence.\nMutant Relevance: We have been able to generate specific mutants that capture similar faulty behaviors as those previously witnessed. However, in some cases, anecdotally, looking at the faults generated, it seemed that the mutant was related to the general class of fault simulated, but was not an example of the specific instance. One difficulty here is that we have no way to consistently and reliably measure problem similarity or relevance.\nMore research is needed to define what it means for one fault to be similar to another. We also need approaches that use such similarity metrics to guide agenetic LLM workflows, e.g., with fine-tuning, prompt engineering, re-prompting, and/or Chain-of-Thought, to ensure that the mutants generated are relevant to the original fault.\nDetecting existing faults: our approach hardens against future regressions. This means that the current version of the system under the test is used as the regression oracle [6]: a test is deemed to pass after some change if the test behaves the same before and after the change. This allows us to protect against future regressions, but it cannot detect existing faults residing in the code base. To do this requires us to tackle the well known Oracle problem [11].\nIt would be very exciting if an approach can be found to infer oracles for Targeted LLM-based Mutation-Guided Test Generation. Such an advance would be highly impactful because it would allow us to search for existing classes of faults. Although there has been much previous work on oracle inference [30, 44, 58, 62], we need higher precision to avoid wasting engineers' time on false positives. Generating faults/tests for specific issues of concern, as ACH does, may help oracle generation."}, {"title": "9 CONCLUSIONS", "content": "Mutation testing has been the subject of research for five decades [20, 29, 35, 46], so has clearly retained intellectual appeal and researchers' curiosity. Despite this, it has proved challenging to deploy mutation testing in industry [4, 6, 12].\nTraditionally, mutants have been constructed using simple rule-based approaches in order to assess test suites, largely written by humans. However, software engineers need automatically generated tests, that are relevant to a specific pressing concern; their time would be better spent articulating these pressing concerns, rather than trying to construct test cases that should, instead, be generated by a machine. Fortunately, two crucial recent advances, drawn together in ACH, make this possible: Automated test generation to kill generated mutants, coupled with language models' ability to generate highly-relevant mutants. In particular, we found that LLMs help us to overcome existing barriers [12] to deployment of mutation testing. Specifically, they\n(1) Allow us to generate highly realistic faults, using their ability to convert text (the concern or issue) into code (simulated faults from which we generate tests);\n(2) Provide an additional agent to weed out equivalent mutants, which is especially powerful when combined with simple static analyses as a pre-processing step;\n(3) Provide a way to automatically generate unit tests to kill the mutants.\nThis paper presented results from deployment of ACH at Meta. Neither LLM-based test generation, nor LLM-based mutant generation is new, but this paper is the first to report on their combined deployment on large scale industrial systems. We believe this form of automated test generation is highly aligned with modern software development and deployment. It supports software engineers who must contend with many competing and conflicting concerns, often expressed in natural language in vague, incomplete and even contradictory ways. Our results also suggest Mutation-as-RAG will prove impactful in optimizing for structural coverage criteria."}]}