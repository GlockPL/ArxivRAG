{"title": "Fusion Flow-enhanced Graph Pooling Residual Networks for Unmanned Aerial Vehicles Surveillance in Day and Night Dual Visions", "authors": ["Alam Noor", "Kai Li", "Eduardo Tovar", "Pei Zhang", "Bo Wei"], "abstract": "Recognizing unauthorized Unmanned Aerial Vehicles (UAVs) within designated no-fly zones throughout the day and night is of paramount importance, where the unauthorized UAVs pose a substantial threat to both civil and military aviation safety. However, recognizing UAVs day and night with dual-vision cameras is nontrivial, since red-green-blue (RGB) images suffer from a low detection rate under an insufficient light condition, such as on cloudy or stormy days, while black-and-white infrared (IR) images struggle to capture UAVs that overlap with the background at night. In this paper, we propose a new optical flow-assisted graph-pooling residual network (OF-GPRN), which significantly enhances the UAV detection rate in day and night dual visions. The proposed OF-GPRN develops a new optical fusion to remove superfluous backgrounds, which improves RGB/IR imaging clarity. Furthermore, OF-GPRN extends optical fusion by incorporating a graph residual split attention network and a feature pyramid, which refines the perception of UAVs, leading to a higher success rate in UAV detection. A comprehensive performance evaluation is conducted using a benchmark UAV catch dataset. The results indicate that the proposed OF-GPRN elevates the UAV mean average precision (mAP) detection rate to 87.8%, marking a 17.9% advancement compared to the residual graph neural network (ResGCN)-based approach.", "sections": [{"title": "1. Introduction", "content": "Detecting illegal unmanned aerial vehicles (UAVs) within designated no-fly zones is of paramount importance, where the illegal UAVs pose a substantial threat to civil and military aviation safety due to their potential to interfere with flight paths, causing severe accidents [1]. The illegal UAVs also present a risk to sensitive infrastructure, such as power plants and communication networks [2, 3], where an accidental or intentional collision could result in widespread service disruptions or catastrophic failures [4]. As shown in Fig. 1, an unauthorized UAV outfitted with cameras or other surveillance devices in a no-fly zone can infringe on privacy rights and present substantial security threats to both civil and military operations. Such UAVs have the potential to obtain unauthorized imagery or data, thereby providing malicious entities with invaluable information [5].\nIdentifying UAVs through camera imagery presents a considerable challenge. This arises from UAVs' propensity to integrate inconspicuously with environmental elements, such as structures and foliage, particularly during nocturnal hours when their hues can closely resemble the backdrop. Moreover, during daylight, the variability in illumination conditions further complicates the detection process.\nDeep learning models, referenced in [6, 7, 8, 9], have been utilized for UAV detection based on their color congruence with the background. Notably, these models often demonstrate proficiency in detecting UAVs in daytime color images or in infrared (IR) images captured at night. However, its efficacy tends to diminish when faced with homogenous backgrounds at nighttime or fluctuating illumination during daylight hours.\nIn our antecedent research [10], we explored a deep learning model, leveraging transformations and cosine annealing strategies to reduce classification and regression discrepancies for UAV detection utilizing both RGB and IR imagery. However, the detection efficacy using RGB images is compromised under less-than-ideal lighting conditions, such as during overcast or tempestuous days. On the other hand, while using IR (monochromatic) imagery, discerning UAVs that merge with backgrounds of a similar hue presents its own set of challenges.\nIn this paper, we propose optical flow-assisted graph pooling residual networks (OF-GPRN) designed for intricate UAV detection using combined RGB and IR images. While the IR image remains unaffected by light conditions, the RGB image retains vital color data. The proposed OF-GPRN takes advantage of integrating RGB and IR images to optimize contrast, edge definition, color, and texture in each frame. This combined image also mitigates distortions arising from lighting variances, color, and background interference. Relying on this image integration, the OF-GPRN system produces a comprehensive composite frame enriched with features such as fine-texture, broad-texture, and contrast, which facilitates the extraction of UAV movement patterns.\nTo isolate the UAV from its background, our proposed OF-GPRN system harnesses the fusion of RGB and IR images, subsequently processed through optical flow [11], facilitating the segregation of the UAV from the amalgamated image. This system innovatively augments graph neural networks (GCN) by integrating graph residual split-attention networks (GRSaN) [12, 13, 14], aiming to optimize the mAP for UAV detection [15]. Given the diminutive representation of the extracted UAV within the image, it poses challenges in distinguishing it from other entities, such as avian creatures or aircraft. Specifically, the OF-GPRN model refines the extracted object's contours and ascertains pixel correlations across the pre-processed RGB and IR imagery. This aids in UAV identification and augments predictive accuracy by capitalizing on the expansive feature-learning prowess offered by the feature pyramid during model calibration.\nThe main contributions of this paper are listed as follows:\n\u2022 The OF-GPRN system is proposed to enable precise UAV detection in day and night dual visions that experience time-varying lighting conditions and high background similarities. The OF-GPRN system develops a fusion of RGB and IR frames, which enhances the quality of output frames by reducing noise and adjusting illumination and color. The OF-GPRN system also extends a new optical flow model to eliminate background and foreground similarity while extracting the UAV's mobility.\n\u2022 The GRSaN is extended in the OF-GPRN system to stabilize the learning capability, enhance feature learning during training, and reshape the UAV. The OF-GPRN system also uses a Quickshift-based algorithm to represent the adjacency matrix from pixels to nodes. This makes for an accurate graph with clear frame-pixel information.\n\u2022 We conducted experiments to assess the performance of the proposed OF-GPRN system in UAV detection during both daytime and nighttime conditions. In comparison to preceding residual GCN (ResGCN) object detectors, our enhanced model demonstrates superior performance, achieving a commendable mAP of 87.8% on the stringent RGB-IR combined benchmark UAV catch dataset. This marks a significant improvement over the ResGCN, which obtains an mAP of 69.9%.\nThis paper is organized as follows: Section 2 presents the literature overview on deep-learning-based UAV detection. The proposed OF-GPRN system is presented in Section 3. In Section 4, we study the system implementation, experimental setup, as well as the performance evaluation. Section 5 concludes the paper. The symbols used in the paper have been listed below in the Table. 1."}, {"title": "2. Related Work", "content": "The literature encompasses several UAV detection methodologies based on RGB or IR images, leveraging Convolutional Neural Networks (CNN) [16, 17, 18, 19, 20, 21, 22, 23]. In particular, Tian et al. introduced a YOLOv5-based detection paradigm tailored for small UAVs in [16]. This model refines detection by designing anchor box sizes, incorporating a convolutional block attention module (CBAM), and revising the loss function. Its prowess is accentuated by its capability to detect UAVs in complex and challenging environments. Similarly, Alsoliman et al. put forth a UAV detection approach that leverages random forest classification, as articulated in [17]. This technique discerns patterns in video data to curtail the influx of packets emanating from UAVs. Furthermore, a distinctive method is delineated in [19], introducing the notion of a pivot fingerprint model designed for pinpointing anchor packets within video streams for UAV detection. The framework of this model uses a two-tiered feature selection process, with the first phase being model-independent and the second phase being model-dependent.\nLui et al. studied an HR-YOLACT algorithm, which is an amalgamation of HR-Net and YOLACT techniques. This model is architected to feature a lightweight prediction head, facilitating the detection of UAVs and extracting their features through instance-based semantic segmentation [20]. Muhammad et al., on the other hand, delved into a transfer learning technique for the identification of UAVs, using both VGG16 and Faster-RCNN [21]. Meanwhile, Jihun et al. brought forward an approach in [22], leveraging a Pan-Tilt-Zoom camera system for UAV detection using the Faster R-CNN Inception Resnet algorithm.\nFurthermore, Wei et al. tailored the YOLOv3 model combined with transfer learning to detect UAVs using an RGB camera. This integration has further potential for real-time surveillance applications, especially on platforms like the NVIDIA Jetson TX2 [24]. In the same way, Reddy et al. showed how YOLOv3 could be used to find UAVs, highlighting how useful it is for effective monitoring in a variety of daytime situations [25]. Lee et al. ventured into a machine learning-centric approach, focusing on the identification of UAVs from RGB images. Their system keeps a vigilant eye on surveillance zones, pinpointing and cataloging UAVs using an RGB camera and subsequently determining their geographic position and manufacturer model [26].\nIn another innovative approach, Wang et al. launched a semi-supervised object detection technique termed Decoupled Teacher. Built on Faster-RCNN's foundation, this method employs unlabeled data with the aim of counteracting the imbalance between foreground and background observed in RGB camera feeds [27]. In [28], Basak et al. describe a YOLO-based UAV detection strategy that uses spectrogram images to classify and group spectral instances.\nByunggil and Daegun, in [29], showcased various micro-Doppler signatures of UAVs. These were discerned using the short-time Fourier transform along with the Wigner-Ville distribution, both of which serve as tools to aid in the identification of UAVs. Multiple trainable object detection models were put to the test, comparing them with UAV identification tasks. Complementarily, Suh et al. embarked on research tailored for UAV detection on platforms that are constrained in terms of resources, particularly focusing on video hardware. Their study in [18] meticulously melded algorithmic optimizations with FPGA hardware, aiming to adeptly scrutinize the intricacies of video streaming.\nQi et al. put forth a technique tailored to the recognition of consumer-grade UAVs [23], employing static infrared imagery. Their model uses an approach based on importance and integrates basic convolution, adaptive thresholding, linked domain filtering, and SVM-based discrimination. Sun et al., in [30], introduced TIB-Net, a specialized model designed for UAV detection through an RGB camera. This model, uniquely structured with a cyclic pathway, is particularly adept at detecting small-sized UAVs. Augmenting its efficacy, a spatial attention module is integrated, working to pare down data redundancy and extraneous noise. Further research, as outlined in [31, 32], showcases efforts wherein various CNN-based strategies are deployed to identify UAVs. These detection tasks are undertaken under a myriad of lighting scenarios, harnessing the power of RGB video feeds. However, despite the extensive study into CNN algorithms tailored for both RGB and IR-based UAV detection, their potency tends to be encumbered. This restriction results primarily from the difficult challenge of backgrounds with a high degree of similarity to the UAVs.\nThe prevailing models as depicted in the literature predominantly operate on single-stream data, either RGB or IR, as opposed to processing fused optical flow images, where the visual presentation of UAVs and other entities can exhibit substantial variation between day and night vision scenarios. Additionally, the feature quality of the resultant frames is found to be considerably influenced by varying lighting conditions, which, in turn, often exacerbates the challenges due to the heightened similarities between the UAVs and their background environments."}, {"title": "3. The Proposed OF-GPRN System", "content": "In this section, we study the proposed OF-GPRN system that learns the feature layers while improving the detection accuracy, which is illustrated in Fig. 2. The OF-GPRN is developed to retain and recognize the original structure of the complex day and night vision data."}, {"title": "3.1. Proposed System Model", "content": ""}, {"title": "3.1.1. RGB-IR Fusion", "content": "A fusion module is developed in the proposed OF-GPRN to merge two input frames from RGB and IR sources while enhancing the quality of the output frames by reducing noise and adjusting illumination and color. First, multi-level edge preservation filtering is used to separate the input frames into base layers (B5), fine-structure (F), and coarse-structure (Cs). This enables the extraction of fine texture and large texture features at Fs and Cs layers respectively, while contrast and edge details are preserved in the B, layer. To retain edge information, weighted mean curvature (Wf) is applied to each input layer, and a Gaussian filter (Gf) is utilized to remove Gaussian noise. The weighting matrix filter w(P, Q) assigns higher weights for center pixels within the square-shaped window, with parameters P and Q. A modified Laplacian operator $L_{(x,y)}$ is defined to obtain high-quality output frames that capture detailed features, such as edges and contours, from both RGB and IR sources [33, 34]. Thus $L_{(x,y)}$ is given as\n$L_{(x,y)} = \\sum_{P=-p}^p \\sum_{Q=-q}^q w(P,Q) [L'_{(x+P,y+Q)}]^2$ (1)\nFurthermore, the modified Laplacian can be given by $L'_{(x,y)} = |2L''_{(x,y)} - L''_{(x-1,y)} - L''_{(x+1,y)}| + |2L''_{(x,y)} - L''_{(x,y-1)} - L''_{(x,y+1)}|$, where $L''$ is a linear differential operator that approximates the second derivative at the Fs layer, i.e., $L''Fs(x,y) = \\delta^2Fs$. Likewise, $L''Cs(x,y) L''$ at the Cs layer can also be obtained. In the proposed OF-GPRN, a pulse-coupled neural network with parameter adaptation is used to fuse the F, and C, layers while determining the optimal number of features in each layer [35]. Specifically, the network evaluates the edge features of the respective layers, with priority given to those extracted from $L'Fs(x,y)$ if they are more prominent. If the edge features of $L'Cs(1,y)$ are more prominent, the pixel features extracted from this layer are added to the fused frame.\nThe fusion of the B, layers in the RGB and IR frames combines the contrast information from an IR frame with the texture information from an RGB frame. In particular, fusing the B, layers in RGB and IR frames suffers from each pixel contrast and texture information composition [36]. Therefore, a visual saliency map, denoted by $Vp$, is constructed based on calculating the intensity value of the pixels, which examines the difference between each pixel and all of its neighbors to generate a saliency value (2). As a result, $V_p$ preserves the contrast and texture features of the RGB or IR frame, improving the quality of the fusion. In (2), \u03b7 represents a specific pixel intensity value within the range of 0 to 255 in the frames $B(RGB,IR)$ (denoted as N), and $I_n$ is the number of pixels with similar intensity to n \u2200 I = {i|(Xni, Yni) = n, 1 \u2264 i \u2264 N} in which i is the individual pixel index.\nThe proposed system uses a feature scaling normalization function S(.) to make sure that the frame features fall in the same range. This is due to the fact that different scales in (2) affect many quantitative pixel features.\n$V_p = \\sum_{x,y} I_n[S(B^{(RGB,IR)}) - S(B^{(RGB,IR)})];$ (2)\nwhere x and y are spatial pixel coordinates, and In and Yn represent specific pixel coordinates.\nFor the fusion of the B, layer, we formulate (3) to merge the saliency maps in the RGB and IR frames, which are represented by $V_p^{RGB}$ and $V_p^{IR}$, respectively.\n$BASEfusion = \\frac{\\alpha + \\beta}{2}$, (3)\nwhere a = ($V_p^{IR}B^{IR}+(1-V_p^{IR})B^{RGB}$) and B = ($V_p^{RGB} B^{RGB} + (1 \u2013 V_p^{RGB})B^{IR}$).\nBased on the output of the parameters-adaptive pulse coupled neural network and $BASEfusion$, the fusion of the RGB and IR frame can be obtained by applying the inverse multi-level edge preservation filtering.\nTo monitor the UAV's movement, we used optical flow BRAFT [11] to process the frames, which relies on merged frames. an effective approximation of the actual physical motion being projected onto the fused frame, which provides a concise representation of the parts of the frame that are in motion. Integrating spatio-temporal information helps with background elimination. Once the videos are processed, the best next move for the mobile UAV is determined, and routine execution initiates each new iteration of the model."}, {"title": "3.1.2. Graph Residual Split Attention Network (GRSaN)", "content": "The construction of graph nodes and edges for input images in the proposed OF-GPRN model depends on using region adjacency graphs. Superpixel segmentation approaches, such as SLIC, Quickshift, and Felzenszwalb, are used to precisely split the frames into regions, which then function as nodes in a system. Subsequently, each of these regions connects together on the basis of their adjacency, which leads to the formation of the edges of the graph. The decision of the superpixel segmentation technique plays a role in determining the precision of a generated graph. These nodes and edges are used as inputs for the GCNs model. GCNs are graph-based architectures based on graph nodes and edges G = (N, E). Instead of using conventional convolutional filters, GCNS use graph convolutional filters in each layer of unordered nodes N with edges E, and aside from that, GCNs are just like CNN's. Stacks of pointwise nonlinearities in GCNs serve as the building blocks of filters, while stability and permutation equivariance of GCN architectures with good performance are attributed to the graph characteristics [37, 12]. UAV pixels in the frame are represented as nodes N = [n1, ..., nk] \u2208 R, with edges E = [e1, ..., ek] \u2208 R defining the relationship between the i-th and j-th UAV pixels in the order pair k = (i, j). The vector H = [hn1, hn2, ..., hnk]T \u2208 R concatenates the feature vector hn \u2208 R with D-dimensional features of n nodes. Here's how the information from the input layer (G1) which is ConvOper(G\u0131, W\u0131) and added to and changed in the output layers.\n$G_{l+out} = ConvOper(G_{l+out}, W_{i+out}) + \\tau(G_l +G_{i+1} + ... + G_{l+out-1})$ (4)\nW = [W1, W2, ..., Wout] is the learnable weighted parameter of the n layers for node aggregation and updating the graph function to compile neighborhood pixel information [12].\nEach $G_{l+r}$ represents the graph residual split attention network layer of the graph residual network [15] as shown in Figure 3. The output of $G_{l+r}$ is given in equation 5.\n$G_{out} = \\sum_{r=1}^R (G_i \\oplus G_r^{Conc}) + \\tau(G_l), r = 1, 2, 3, 4, . . . R\\}$ (5)\nGi is the input of the graph residual split attention network layer, $G_{out}$ is the output, and is the strided graph convolution or combined graph convolution with max pooling. If the dimensions of $G_{out}$ and G\u0131 equal, then \u30f6 replaced by the identity matrix (I) [38]. The GCN main route output is scaled using as a linear projection with the previous input. \u03c4 is scaled with input using a strided graph convolution or a combined graph convolution linear filter with max pooling. As a result, the number of parameters for ResGCN remains constant rather than increasing, as it does for plain GCN or ResGCN without \u0442. A simple ResGCN without a projection matrix block can add an input channel to an GCN output; however, as the number of layers increased, performance decreased significantly due to shortcut path accumulation. Furthermore, if the input and output have the same dimension, then I can reduce computational complexity and have the same effect.\nWhere $G_l^{Conc}$ represents the concatenation of the cardinality groups denoted by (k) for each set of hyperparameters (R). To have a better understanding of each expression, we listed a more in-depth comprehension of each term:\n\u2022 $G_l^{Conc}$: This notation refers to a specific data structure that results from concatenating multiple groups, where each group corresponds to a different choice of hyperparameters (R).\n\u2022 $\\sum_{i=1}^R (G_i^{Conc})$: This expression represents the sum of these concatenated groups. Moreover, it is adding together the information contained in all the different groups. The result is a comprehensive dataset represented as $G_1 + G_2 + ... + G_k$, where each $G_k \u2208 RN\u00d7D corresponds to a specific combination of nodes and the cardinality of the set k.\n\u2022 $G^{Conc} = HG(G_1, G_2, ..., G_k)$: We define a function HG that takes individual groups G1, G2, ..., Gk as inputs and concatenates their gradients (features). This step ensures that we capture information from all the vertices in each group.\n$G_i^k = \\sum_{j=1}^k F'(G_i^j) + (W_{agg}^{w-agg-op}) + W^{up-date}_{k}$ (6)\nwhere, the updated and aggregated learnable parameters are $w399 wagg$ and $wupdate$. Where $F'(\u00b7)$ is the aggregation function. $w-agg-op$ compiles information from vertices in the same cardinal k's neighborhood, whereas $w^{up-date}_{k}$ applies a non-linear function to the aggregated information to compute new vertex representations in cardinal k. The $G_{out}$ is processed by global max pooling, followed by batch normalization and ReLU to stabilize the input by softmax and 1x1 convolution, and then transfer for the next layer, as shown in Fig. 3."}, {"title": "3.1.3. Graph-Pooling Feature Pyramid Network Mapping", "content": "The last layers of the GCN extract high-level features of the input. We selected the last 5 layers for the graph feature pyramid network to map features between GCN and the pooling feature pyramid (PFP) [39, 40]. We update the last 5 layers of the GCN with a feature pyramid network [41], which has the ability of the superpixel hierarchy to make recursively larger groups of pixels from the smaller features of the last layers (high-level features) and a similarity measure [42]. The superpixel hierarchy matches the graph layers, and when moving from one layer of the residual attention GCN last layer to the next, the number of nodes decreases by a factor of 4. Contextual and hierarchical edges are used in different ways in the layer that connects the ancestor and descendant layers, which are called superpixels. Hierarchical edges connect semantic gaps, and contextual edges spread information about the context of the different levels in each layer. The node features used are the same for both hierarchical and contextual layers, but the edges are different for both. Of the last 5 layers, the first and last two are contextual, while the middle layers are hierarchical. Moreover, the learning parameters are different for both and are not shareable. The mapping from GCN to PFP is necessary to transfer the features at multiple scales and make it possible to be in line with the PFP input. Every input from the GCN layer is stride 2, which keeps the input feature from vanishing. An upsampling factor of 2 is applied to features with a higher resolution. Each lateral link combines feature maps of the top-down pathways that are the same size in space. Convolutions are performed on the top-down route feature maps to lower the channel dimensions, and the feature maps from both pathways (input: N/4, N/16, N/64, and N/256; + output: N, N/4, N/16, N/64, and N/256) are combined using element-wise addition. Each combined map is given a 3 x 3 convolution with a factor of 2 to get the final forecast for each layer. For the UAV's position, generative localization of bounding boxes makes a single box with the highest score."}, {"title": "3.1.4. Loss Function", "content": "In the case of one-stage detection, focal loss is specifically tailored to meet the needs of the user. In the suggested training model, an imbalance between UAVs in the foreground and those in the background could be fixed to put less weight on making accurate predictions. It is common practice to use cross-entropy as a loss function because of its high level of accuracy in comparing the approximation models.\n$Cross Entropy(p,t) = \\begin{cases} -\\lambda_1 + log(p), \\text{ if } t=1 \\\\ -\\lambda_1 log(1-p), \\text{ otherwise} \\end{cases}$ (7)\nIn (7), t is the value of the UAV detection target, and p is a probabilistic estimate of that target value based on the probability distribution. Where $ \\lambda_1 $ are the balanced parameters for positive and negative examples; however, it cannot discriminate between simple and challenging cases. The down weight approach requires the modulated focal loss factor `(1 \u2013 pt)` for numerical stability.\nHowever, when training naively with (7), the classifier is unable to discriminate between the more accurate candidate and the loose counterpart, resulting in an unanticipated learning scenario as shown in Fig. 4. Because the candidate boxes with more precise locations are suppressed with non-maximum-suppression procedures, this may have a negative impact on performance. As shown in Fig. 4, the consistent cross-entropy loss function is a dynamically scaled cross-entropy loss with the scaling factor determined by the overlap between the current bounding box and the target ground-truth item [43]. This scaling factor, intuitively, automatically downweights the contribution of loose samples during training, allowing the model to concentrate on more accurate predictions. The consistent cross-entropy loss function may help train our model to better identify which prediction is the best among numerous clustered choices. So, modulating factors are added to (7) using a consistent cross-entropy loss function [43] to accommodate localization quality, and the more precise targets are augmented to reflect it. The (7) updated form is shown in (8).\n$Cross Entropy(p,t) =  \\begin{cases} [-\\lambda_1 + \\lambda_2(\\alpha_k - \\lambda_1)z_k] log(p), \\text{ if } t=1 \\\\ [-\\lambda_1 + \\lambda_2(\\alpha_k - \\lambda_1)z_k] log(1 - p), \\text{ otherwise} \\end{cases}$ (8)\nThe $z_k := 1(if\\alpha_k > a,k = 1, ..., L)$ represents the candidate box of the targets using IoU overlap for the predicted bounding box, and ok shows IoU overlapping of the predicted and ground truth bounding boxes. Here k is the location of the UAV in the frames. Frames with IoU overlap more than may use the modifying factor in (8) for more favorable examples, which increases the modifying factor and the loss in proportion to the overlap with ground truth targets. As a result of using cross-entropy, the consistent cross-entropy loss function prioritizes cases with bigger IoU overlaps.\nThe (8) is updated in the focal loss as:\n$Focal Loss_{(p,t)} = -\\alpha_t (1 - p_t)^\\gamma log(p_t)$ (9)\nThe focal loss down-weighted tuning process is dependent on the y, and it varies from 0 to 2 to adjust the rate of easy examples. If y is 0, then the focal loss is equal to the cross-entropy. The UAV detection scenarios increase the y up to 2 to obtain the best training result. Moreover, during experimentation, we systematically varied y and observed its impact on the performance of the model. Higher y led to better detection rates for challenging scenarios, such as low contrast or occlusion cases. However, excessively high values might lead to overemphasis on hard examples, potentially causing instability."}, {"title": "4. Experiments and Performance Analysis", "content": ""}, {"title": "4.1. Experimental Training", "content": "Extensive experiments on the benchmark UAV catch detection dataset were conducted to evaluate the efficacy of OF-GPRN in enhancing the learning performance for UAV detection with day and night vision cameras. The OF-GPRN model is developed in Tensorflow and trained on a workstation with a GeForce RT 3060. The training procedure used 4 batch sizes. Compared to other optimizers, the Adam optimizer is preferred because of its rapid and intuitive convergence on the best solution. The learning rate decays to 10-4 and 10-6 when the value of \u1e9e1 is set to 0.9 and when the value of \u1e9e2 is set to 0.999.\nIn addition, the model is trained for 45 hours and 200 epochs. To build the region adjacency graph and decrease the input size, the frames are converted to super pixels using the algorithms SLIC [44], Quickshift [45] and Felzenszwalb [46] for OF-GPRN training. Progressive focal loss is used to prevent the unanticipated learning scenario and to discriminate between the more accurate candidate and the loose counterpart."}, {"title": "4.2. Datasets Pre-processing", "content": "In this study, we employ freely accessible anti-UAV capture video datasets [47]. There are a total of 320 clips here, 160 of which are HD videos shot in both standard definition (SD) and high definition (HD) (RGB and IR), which have different variations like backgrounds (cloud, building, mountain, and sea), fast movement, out of focus, and size variations from small to large. The UAVs seen in each video come in a range of sizes, from large to small. We selected UAVs with a size range of 300mm to 1200mm to train and validate our proposed model. Moreover, the UAVs cruise at speeds between 50 and 100 miles per hour and occasionally stop altogether. One hundred validation videos from each stream were used for model training. All except 80 of the videos are used to train the model, while the remaining 20 are used for testing and validation."}, {"title": "4.3. Effect of Optical Fusion", "content": "The conversion of RGB or IR to optical flow is seen in Fig. 5. When compared side by side, all of these frames have distinct levels of performance. A closer inspection reveals obvious artifacts, blurriness, and distinctions in the results of the three columns. When compared to fusion-generated frames, we observe that optical flow fails to inject as many of the bright feature characteristics from the RGB (first column) and IR (second column) frames. The fusion-generated RGB and IR (third column) frames support the optical flow to detect the moveable UAV. Without fusion, the results of optical flow show that the moveable UAV is unidentified due to its high background similarities. Therefore, the spatial properties of the source frames are enhanced in the fusion plus optical flow output frames, which are also free of artifacts, clearer, include more structural details, and have a higher overall visual quality."}, {"title": "4.4. Effect of Region Adjacency Graphs", "content": "Training the GCN model relies on accurate region adjacency graphs. We can see that the pixel segmentation before the optical, as shown in Fig. 6 is very complicated compared to the after optical flow. The optical flow for background removal significantly reduces the segmentation of pixels for the region adjacency matrix up to 30%. Moreover, if the appropriate pixel segmentation procedure is used, the resulting region adjacency graph would be accurate. We used three different superpixel segmentation techniques (SLIC, Felzenszwalb, and Quickshift) to significantly minimize the size of the nodes and edges in the matrix to train the OF-GPRN, as shown in Fig. 6. Each of the algorithms identifies regions with similar visual properties. Each frame's features and their associated graph are generated by the adjacency matrix, which also provides concise information regarding the frame's pixels.\nFrom our experience training the OF-GPRN, we can say that the SLIC algorithm is both fast and space-efficient, and it is able to successfully segment in terms of color boundaries without having to remove the background. However, it recorded the pixels in the background, which makes it less accurate. Additionally, OF-GPRN training using the Felzenszwalb algorithm performs less well due to contrast-based training when it comes to loss minimization. While the Quickshift method is used for the adjacency matrix to achieve OF-GPRN-based promising results of loss 0.026, as shown in Fig. 7. Quickshift improves in this regard since it uses hierarchical segmentation computation to separate the image into visually distinct parts. The Quickshift algorithm is used for the proposed model due to its high performance. We achieved the optimal loss of all three algorithms with the parameters mentioned in the Table. 3.\nWe use the Quickshift algorithm for exceptional performance after applying optical flow, a method that successfully removes the background. The decision to strategically employ the Quickshift algorithm is based on its notable efficiency in comparison to SLIC and Felzenzswalb. The Quickshift method works better than SLIC and Felzenzswalb in terms of loss efficiency and accurate capture of important features in the image, as shown in Fig. 7."}, {"title": "4.5. Residual Split Attention Network Effect", "content": "Extensive studies", "412": "up of RGBIR-fused optical flow data. Our proposed OF-GPRN model achieves a highly remarkable mAP of 87.8%. The OF-GPRN model outperforms previous models, such as the Hybrid-DL, which obtained a mAP of 68.1%, and the EfficientDet model, which achieved 67.3% mAP. Importantly, our model shows higher accuracy compared to the anti-UAV DETR model, which attained a mAP of 83.2%.\nUpon analyzing the training performance, our proposed approach shows an incredibly small training loss of 0.026. In comparison, the hybrid-DL model has a training loss of 1.13, the EfficientDet model has a loss of 1.20, and the anti-UAV DETR model has a training loss of 0.11. The significant decrease in training loss for our model shows its high efficiency and efficacy in acquiring features from the data. The results highlight the importance of using RGBIR-fused optical flow data to improve the overall performance of the proposed model compared to other methods."}, {"title": "5. Conclusion", "content": "In this paper, we propose a new OF-GPRN system to enable the precise detection of UAVs in dual day and night visions that suffer from time-varying lighting conditions and high background similarities. The proposed OF-GPRN system incorporates optical fusion techniques and effectively eliminates extraneous backgrounds, resulting in enhanced clarity of RGB/IR imaging. Moreover, the GRSaN is extended in the OF-GPRN system to stabilize the learning capability, improve feature learning during training, and reshape the UAV. In addition, the OF-GPRN system extracts the pixels-to-node representation of the adjacency matrix to achieve an accurate graph with information about the pixels of the conspicuous frame and has the precise learning ability to capture correlations among the pixels in the fused images to identify the suspicious UAV. Experimental results show that our proposed OF-GPRN system achieves an impressively low loss of 0.026. Compared to previous ResGCN object detectors, which recorded an mAP of 69.9%, the OF-GPRN system delivers superior performance, reaching an mAP of 87.8% on the demanding RGB-IR-based benchmark UAV catch dataset. Moreover, optimization of the model should be the objective of future studies for onboard real-time system models like UAVs. In addition, optical fusion limits the present study to static cameras; we aim to improve the model for implementation with moving objects, such as autonomous vehicles and UAVs. Research on its possible applications in different environmental circumstances, such as rain and different types of UAVs, is required. Improving accuracy and efficiency requires testing on more and more diverse datasets."}]}