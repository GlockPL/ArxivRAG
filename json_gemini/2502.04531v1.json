{"title": "AnyPlace: Learning Generalized Object Placement for Robot Manipulation", "authors": ["Yuchi Zhao", "Miroslav Bogdanovic", "Chengyuan Luo", "Steven Tohme", "Kourosh Darvish", "Al\u00e1n Aspuru-Guzik", "Florian Shkurti", "Animesh Garg"], "abstract": "Object placement in robotic tasks is inherently challenging due to the diversity of object geometries and placement configurations. To address this, we propose AnyPlace, a two-stage method trained entirely on synthetic data, capable of predicting a wide range of feasible placement poses for real-world tasks. Our key insight is that by leveraging a Vision-Language Model (VLM) to identify rough placement locations, we focus only on the relevant regions for local placement, which enables us to train the low-level placement-pose-prediction model to capture diverse placements efficiently. For training, we generate a fully synthetic dataset of randomly generated objects in different placement configurations (insertion, stacking, hanging) and train local placement-prediction models. We conduct extensive evaluations in simulation, demonstrating that our method outperforms baselines in terms of success rate, coverage of possible placement modes, and precision. In real-world experiments, we show how our approach directly transfers models trained purely on synthetic data to the real world, where it successfully performs placements in scenarios where other models struggle \u2013 such as with varying object geometries, diverse placement modes, and achieving high precision for fine placement. More at: any-place.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Placing objects is a fundamental task that humans perform effortlessly in daily life, from setting items on a table to inserting cables into sockets. On the other hand, enabling a robot to perform such tasks can often be highly challenging. The challenges arise from the various constraints of different placement tasks and the difficulty of generalizing to unseen objects. Existing methods are often task-specific, using a large number of demonstrations for a single placement task, such as hanging objects on racks [28], with the hope that the robot can generalize to unseen objects. Alternatively, few-shot approaches focus on learning object placement with only a few demonstrations, aiming for the model to replicate the same placement operation across random initial configurations of similar objects and setups.\nLearning diverse object placements presents significant challenges for existing models, primarily due to the difficulty of generalization. Generalization can be categorized into two aspects: object-level and task-level. Object-level generalization"}, {"title": "II. RELATED WORK", "content": "focuses on developing robust representations of various objects and placement configurations, enabling the model to handle unseen objects effectively. Task-level generalization involves the model's ability to predict diverse placement configurations from given pointclouds. Additionally, predicting multimodal placement outputs\u2014encompassing a range of valid locations and modes-remains challenging, particularly when multiple feasible solutions exist. For instance, a robot inserts a vial into one of many empty slots on a vial plate. Existing methods [26, 3, 27, 21, 24, 23, 9, 6, 15, 14] often operate within a few-shot learning framework, where they learn specific placements involving two particular objects, resulting in poor generalization to different tasks and object types. In contrast, RPDiff [28] has trained separate models for three placing scenarios (e.g. cup on a rack, book on a shelf, can stacking) using a relatively larger dataset, demonstrating some level of in-class object-level generalization, but its generalization to new tasks has not been studied.\nIn this work, we address generalizable object placement that is robust to different objects and capable of predicting diverse placement poses across various tasks, similar to general-purpose object grasping methods. To achieve this, we have developed a fully synthetic dataset that contains 1,489 generated objects and captures three common placement configurations: inserting, stacking, and hanging. Based on this dataset, we develop a placement prediction pipeline that consists of a high-level placement position proposal module and a low-level placement pose prediction model. For the high-level module, we leverage a large segmentation model SAM-2 [22] to segment objects of interest and prompt the Molmo VLM [4] to propose all possible placement locations. Only the region around the proposed placement is fed into the low-level pose prediction model. By first providing a rough region for the low-level module to focus on, the model can effectively generalize across objects, learn their geometry, and capture diverse placement configurations. For the low-level pose prediction models, we build upon diffusion to predict precise and multimodal placement poses. We demonstrate the effectiveness of our methods across different placement tasks in simulation, where we significantly outperform baseline models in terms of pick-and-place success rate and placement location coverage. In real-world evaluation, our model achieves an 80% success rate on the vial insertion task, demonstrating robustness to noisy data and generalization to unseen objects.\nTo summarize, the key contributions of our work are:\n1) We propose a novel object placement approach that leverages a VLM to reason about potential placement locations and a low-level pose prediction model to predict placement poses based solely on the region of interest.\n2) We developed a fully synthetic dataset containing thousands of generated objects capturing a wide range of local placement configurations. This is crucial in enabling us to train models exclusively on synthetic data and be able to use them for a wide range of real-world placement tasks."}, {"title": "A. Object pick and place in robot manipulation", "content": "The problem of robot pick-and-place is typically formulated in two ways: object rearrangement and direct end-effector pose prediction. In object rearrangement, the goal is to train a model to predict the relative transformation of the object from its initial pose to its final placement pose. Assuming the grasping pose is generated by existing models, the final placing pose of the end-effector is calculated by multiplying the predicted relative transformation with the grasping pose. In this setting, many of the works focus on predicting explicit task-relevant features of both objects and then solving for the relative pose through optimization or regression. Specifically, the Neural Descriptor Fields (NDF) series of papers [26, 3, 27] learn the occupancy field of pointclouds as a representation. Then, a set of predefined keypoints attached to the placement object is used to interact with and query the occupancy feature field of the target object. By matching the queried features at each point to features collected during demonstrations, the optimal object transformation is determined. TaxPose [21] leverages transformer-based cross-attention to predict corresponding points between two objects and use differentiable singular value decomposition (SVD) to solve for the relative transformation. To guarantee the placement pose prediction model is robust to $SE(3)$ transformations, i.e., SE(3)-equivariant, methods [24, 23, 9] explicitly predict perpoint type-0 and type-1 features for object point clouds and then solve an optimization problem to align these features into specific configurations based on demonstrations. All of these methods operate in a few-shot setting and can predict a single placement pose given two objects. It is crucial for models to capture and predict a distribution of placement poses, as not every placement pose is realizable by a robot due to its kinematic constraints. RPDiff [28], by contrast, trains a transformer with a diffusion mechanism on a large dataset, gradually denoising the object placement pose. However, their experiments reveal that the coverage of possible placement locations is incomplete. The fixed-size cropping mechanism used during diffusion may also struggle to generalize to objects of varying sizes. Additionally, a recent study [5] samples multiple stable placements in a simulation and employs a VLM to select the appropriate mode based on a language query. While these modes are discrete, each mode allows for the rotation of objects along their axis of symmetry, resulting in valid placement poses that form a continuous distribution.\nAn alternative approach to the pick-and-place task is predicting the robot's end-effector pose directly. M2T2 [33], and Pick2Place [12] focus on planar object placement in"}, {"title": "III. ANYPLACE: GENERALIZED OBJECT PLACEMENT", "content": "3) We show that reducing the problem to only local pre- diction allows us to improve performance with respect to baseline methods in terms of success rate, precision, and mode coverage. It also crucially makes it possible for a model trained on a fully synthetic dataset to show generalization that enables it to directly be deployed to the real world on a variety of placement tasks.\nTo enable a robot to execute diverse object placements in a scene, we propose breaking the placement pose prediction problem into two subtasks: a high-level placement location proposal task and a low-level fine-grained placement pose prediction task. For the high-level task, we incorporate a vision-language model, trained to output 2D keypoint locations in an image based on a given text prompt. A small local region around the candidate placement location can then be extracted and forwarded to the low-level pose-prediction model. This simplifies the low-level pose-prediction problem significantly by having a much smaller pointcloud as input and helps with generalization as any features outside of the local region do not influence the prediction. This allows us to focus on a limited set of general placement types and utilize a fully synthetic dataset, but have the final model be effective in a broad range of real-world placement tasks.\nProblem setup. We formulate the object placement task to predict relative transformations. Specifically, given an input tuple ${D, I}$, where D represents the language description of the placement task and I is an RGBD image of the scene, our goal is to predict a set of rigid transformations {T}_{i=1}^{k} \\subset SE(3) that move the target object C from its"}, {"title": "A. VLM-guided placement location prediction", "content": "current position to all viable placement locations on the base object B that satisfy language conditioning D. Assuming the grasping poses $T_{grasp}$ are provided by a grasp prediction model, the final end-effector pose $T_{place}$ can be computed using the predicted relative transformation between the initial object pose and its final placement pose as $T_{place} = T_n T_{pick}$.\nWhen objects have multiple potential placement locations, existing models often struggle to capture all these discrete po-sitions when trained in an end-to-end manner. To address this, we propose leveraging recent advancements in VLMs, which have demonstrated strong capabilities in localizing points and regions within images based on language descriptions, to directly identify placement locations. Specifically, given a language description of the placement task D and a RGBD image I, we aim to extract the pointcloud of the target object $P_c$ and a local region of interest of the base object $P_{b_{crop}}$, where $P_c, P_{b_{crop}} \\in \\mathbb{R}^{N \\times 3}$. We utilize Molmo [4] to detect all potential placement locations as keypoints in image space, such as specifying all positions where a vial can be inserted into a vial plate. This approach enables the low-level pose prediction model to focus on learning the different placement configurations of two objects and predicting the placement pose without needing to explicitly capture multiple placement locations.\nFirst, we extract segmentation masks for the target object C and the base object B. To achieve this, we first query the VLM to get point locations in the image for each object, then pass those points to the segmentation model to get segmentation masks. This allows us to have complex language conditioning in our object selection (e.g., \"blue vial\", \"vial rack in front of the scale\", etc.). Next, we query the VLM again on the base object B to find all the discrete modes for the placement (Figure 3). For each identified point in the image, we extract the local region in the pointcloud and use that as input to our pose prediction model.\nEmpirically, we found that explicitly identifying placement modes, rather than relying on models to explore placements across the entire object, is more reliable and practical when handling diverse objects with multiple possible placement poses. Additionally, since our high-level module is built on a general-purpose VLM, the system can handle diverse placements and perform complex language conditioning. In particular, this enables us to have complex language conditioning in: (1) selecting which object to place; (2) selecting which object to place it onto; and (3) selecting where on that object to perform the placement. It also enables our method to function effectively even when the object being placed onto is large or difficult to segment. This is because the module always outputs a small local region around the potential placement location, regardless of the object's size."}, {"title": "B. Fine-grained placement pose prediction", "content": "Given pointclouds $P_c$ and $P_{b_{crop}}$ from the high-level module, the low-level pose prediction model only focuses on learn-"}, {"title": "III. ANYPLACE: GENERALIZED OBJECT PLACEMENT", "content": "ing different local placement arrangements, without the need to capture the distribution of different placement locations. Our intuition is that, with the aid of our large synthetic dataset, the model should effectively capture key representations of diverse placement configurations based on object geometry, which enables it to generalize to unseen objects and remain robust to noisy data. Having only a local region as input, the pose prediction model should be able to achieve better precision, which is crucial in many relevant placement tasks.\nWe predict the relative transformation in a diffusion process with a discrete number of timesteps. Starting with two object pointclouds, the diffusion process iteratively denoises the relative transformation, gradually moving the object being placed toward its final pose. Initially, we transform the object pointcloud $P_c$ with a random transformation $T_{init} = (R, t)$ to get $P^{(0)}$, where R is randomly sampled over the SO(3) space, and t is sampled within the bounding box of the cropped placement region:\n$P^{(0)} = T_{init} P_c$,\nwhere\n$T_{init} = (R, t)$,\n$R \\sim U(SO(3)),  t \\sim U(bbox(P_{b_{crop}}))$."}, {"title": "IV. SYNTHETIC DATASET GENERATION", "content": "As shown in Figure 2, at each denoising timestep t, $P^{(t)}$ and $P_{b_{crop}}$ are input into the encoder. Specifically, both pointclouds are firstly downsampled to 1024 points using Farthest Point Sampling (FPS) and normalized to the size of a unit cube. The downsampled pointclouds are passed through a linear layer to extract latent features, which are subsequently concatenated with a one-hot vector used to identify the corresponding pointcloud. These combined features are then processed by the Transformer encoder [31, 1], where self-attention layers are applied to effectively extract features from the pointclouds. We then leverage cross-attention and pooling layers to further aggregate these features, producing a unified feature representation that captures the spatial re-lationship between the two objects. In the decoder, we first obtain the sinusoidal positional embedding of the diffusion timestep t. Finally, the joint pointcloud feature representation, along with the encoded timestep, is fed into MLP layers to predict the relative transformation $T^{(t)}$ consisting of a rotation $R \\in SO(3)$ and a translation $t \\in \\mathbb{R}^3$ for refining the object's pose. The target object pointclouds are then transformed accordingly before proceeding to the next denoising step as $P^{(t-1)} = T^{(t)} P^{(t)}$. The full transformation $T_n$ taking the object from its initial location to the placement pose is the product of all the incremental transformations predicted through the diffusion steps: $T_n = \\prod_{t=t_{max}} T^{(t)}$. More details on the model architecture can be found in the Appendix.\nDuring training, we perform 5 denoising steps. Instead of incrementally adding Gaussian noise to the input during the forward process as is common practice [13], we manually define the noise added at each timestep. In our case, the noise is the relative transformation that the model predicts. Specifically, the intermediate ground truth relative transformations $T^{(t)}_{n,GT}$ are generated by linearly interpolating the translation and using spherical linear interpolation (SLERP) to sample rotations between the object's initial and final placement poses. During inference, we generate diverse placement poses by sampling the diffusion model multiple times, each time starting with randomly transformed initial object pointclouds $P^{(0)}$. We perform a larger number of denoising steps at test time, by repeating the last denoising step more times for a total of 50 denoising steps.\nAt each diffusion step during training, given the ground truth $T^{(t)}_{n,GT}$ and the predicted $T^{(t)}$, we use the L1 distance for the translation loss $L_{translation}$. For rotation, we measure the geodesic distance between the ground truth and predicted rotations. Additionally, we apply Chamfer loss between the pointcloud transformed by the ground truth pose and by the predicted pose. The total loss is the summation of these losses:\n$L_{total} = L_{translation} + L_{rotation} + L_{chamfer}$.\nAfter determining the placement poses, we implement a pick-and-place pipeline to manipulate the object and position it accurately at the target pose. Specifically, we utilize AnyGrasp [7] to find viable grasps for the target object C and employ"}, {"title": "C. Robot pick and place execution", "content": "Our aim in building the synthetic dataset is to capture a broad range of local placement arrangements. Existing models use a few very specific tasks (e.g. inserting a book into a bookshelf) to simply evaluate how well their model works given placement data for such task for training [33, 12, 32]. Our goal on the other hand is not only to use the dataset to evaluate the proposed model, but to build towards representing a broad range of types of placements (stacking, hanging, inserting) as shown in Figure 4. The local nature of our pose-prediction model makes this task much easier and enables us to build a dataset that can generalize to a broad range of real-world placement tasks.\nThe data generation pipeline consists of two main components: object generation and placement pose generation. Specifically, we use Blender to procedurally generate 3D objects, such as pegs, holes, cups, racks, beakers, vials, and vial holders. Object parameters\u2014including height, width, length, and number of edges are randomized to increase variability. For racks and vial plates, we also randomize the number of"}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "cuRobo [29] as the motion planner to perform collision-free placement. We perform rejection sampling on $(T_{place}, T_{pick})$ pairs to identify valid grasps for the specific placement pose predicted by our model that can be executed by the robot.\nGrasp detection begins by extracting the target object pointclouds $P_c$ from an RGBD image. AnyGrasp [7] then processes the resulting pointclouds to identify the optimal grasp candidates sorted by confidence. To pick up the object, the gripper is first moved to a pre-grasp pose, positioned 10 centimeters away from the target along the gripper's z-axis. The gripper then approaches the target in a straight line while maintaining its orientation. Similarly, during placement, the robot first moves to a pre-place pose, followed by a final approach without altering the gripper orientation. The distance from the pre-place pose to the final placement pose is adjusted according to the object's size to avoid collisions during the transition to the pre-place position. With the waypoints and end-effector orientation constraints defined, we use cuRobo to generate the complete motion plan for robot pick and place.\nWe aim to evaluate three key aspects of our approach: success rate, coverage, and precision. With the VLM-based placement location prediction, our method effectively captures possible placement locations, achieving higher coverage rates. By narrowing the focus to minimal regions for placement pose prediction, we enable the model to achieve greater precision, which is essential for handling challenging placement tasks in real-world scenarios. All models are trained exclusively on"}, {"title": "A. Training details", "content": "our synthetic dataset and evaluated on novel objects in a zero-shot setting. We conduct evaluations against three baseline models on different placement tasks in both simulation and real-world settings. Simulation enables extensive testing of our approach by leveraging rejection sampling across multiple parallel environments. In real-world experiments, we highlight the effectiveness and flexibility of our VLM-based placement location prediction in handling complex environments. Additionally, we demonstrate the robustness of our method in handling pointcloud noise and generalizing to unseen objects, despite being trained on general synthetic objects in simulation and encountering novel global configurations for similar types of local placements.\nWe train each model independently on subsets of the full dataset, split based on the type of placement configurations, as well as on the combined dataset (referred to as the \"multitask\" variant). While similar methods are typically trained for a single task (e.g., [28]), our goal is to build a single model capable of performing a diverse set of placement tasks. All single-task models are trained for three days, while multitask models are trained for five days on a single NVIDIA A100 GPU. More details on training parameters are in the Appendix.\nEvaluation metrics. We use three metrics to evaluate model performance: success rate, coverage, and precision. Specifically, we define a placement as successful if the robot places the object at the correct location and it remains stable after release. The success rate is then calculated as the number of successful placements divided by the total number of trials. To better understand the diversity of multimodal outputs in placement prediction, we evaluate coverage, defined as the number of distinct predicted placement locations relative to the total number of possible locations. Finally, for fine placement tasks, to evaluate the precision, we measure the error between the ground truth pose and the predicted pose in terms of both distance and angle."}, {"title": "B. Evaluation metrics", "content": "Baselines. We compare our approach with three baselines: NSM [1], RPDiff [28], and an energy-based model (EBM) integrated with our high-level placement location prediction module. Specifically, NSM shares the same self-attention and cross-attention pointcloud encoder, paired with a simple regression decoder. For RPDiff, since the low-level pose-prediction module in our diffusion-based approach shares the same structure, this allows us to directly examine the effects of the high-level module we propose. We do not utilize a learned classifier on top of the pose prediction model that is present in RPDiff. Such a model can be applied on top of any of the methods we evaluate here, including the ones we propose. Not having it also allows us to evaluate the number of samples needed to achieve particular coverage of possible placement locations in the scene. To evaluate the effectiveness of the"}, {"title": "A. Placement Success: Single & Multi-Modal", "content": "diffusion decoder in generating multimodal outputs, we build an energy-based model, AnyPlace-EBM, for comparison. Inspired by Implicit-PDF [20], this model uses the same encoder as ours, but for the decoder, instead of explicitly predicting the placement pose, it includes two separate branches: one for placement location prediction and another for predicting the placement rotation energy. During training, we encourage all placement rotations in SO(3) that result in stable placements to have low energy. During inference, we randomly sample thousands of rotations and select the one with the lowest energy as the final placement rotation.\nFor evaluating all the methods in simulation, we utilize the pick-and-place execution pipeline we constructed (subsection III-C), together with simulation in IsaacLab [19]. The pipeline itself can be used as a standalone module. It makes no assumptions about the placement prediction method and is not specifically tied to other modules in our approach. This enables it to be used as a general system for evaluating placement pose prediction models. In Figure 5 we show the system being used to perform insertion, hanging, and placement tasks. Combined with the synthetic dataset we create, this gives us a complete system for comparing different models and getting systematic results of success rate, mode coverage, and precision. Both motion planning and the pick-and-place simulation itself are GPU parallelizable, making evaluation of new models even easier."}, {"title": "C. Real World Evaluation: Precise Multi-Modal Placement", "content": "We evaluate single-mode and multimodal placement across a set of 4 tasks. Object stacking and peg insertion tasks are Single-model placement since only one solution is feasible. While cup hang and vial insersion are multi-modal in nature due to the existence of multiple placement locations. We show a summary of placement experiments in these simulated domains in Table I.\nWe first examine single-mode tasks. For the simple stacking task, where high precision in placement poses is not required, AnyPlace and baseline models, including RPDiff and AnyPlace-EBM, achieve a similar success rate of around 80%. For the peg-in-hole task, which requires high precision in placement pose prediction, AnyPlace surpasses the baseline models by a large margin.\nMulti-mode tasks are where we expect to get full benefit from our approach. In the hanging task the tolerance for placing a cup on the rack is relatively high. AnyPlace achieves a 94% success rate in both single-task and multi-task settings, while RPDiff and AnyPlace-EBM perform slightly worse in comparison. Notably, NSM has the lowest success rate across all multi-mode tasks due to its inherent limitations as a regression model. In the more challenging vial insertion task, AnyPlace achieves the highest success rate of 92.74%, while the success rates of all baseline models drop significantly, with RPDiff reaching only 16.5%.\nThis demonstrates that relying on high-level VLM to propose possible placement locations and focusing solely on the local region for placement prediction, simplifies the task for low-level pose prediction models and enables them to better capture fine-grained pointcloud features for high-precision placements. Additionally, despite using the same high-level placement location prediction, the energy-based model suffers a performance drop of 27% and 18% in single-task and multi-task settings, respectively, compared to the diffusion-based AnyPlace. This highlights that the iterative denoising procedure in diffusion models is more effective for high-precision placement prediction."}, {"title": "B. Placement Coverage: Multi-Modal Insertion & Hanging", "content": "Now we aim to investigate how the coverage changes with the increase in the number of samples taken from each model. With how NSM and RPDiff are designed, we can only take independent samples from the model until as many possible placement modes are covered. In the case of AnyPlace and AnyPlace-EBM, the high-level VLM module predicts possible placement locations, so we can perform sampling for each mode. We perform an equal number of samples in each and compare performance for the same total number of samples for each model. All models used in this evaluation are trained on the multitask dataset.\nIn Figure 6 we show results for the vial insertion task. Any-Place achieves close to its maximum performance after with a"}, {"title": "C. Placement Precision: Fine-Grained Insertion", "content": "single sample at each mode, outperforming other models with 100 samples. This is due to its high placement success rate and the VLM's strong reasoning ability in identifying various possible placement locations. A similar trend is observed for the AnyPlace-EBM models; however, due to a lower placement success rate, the model plateaus at around 73%. In contrast, RPDiff, despite being a diffusion-based model that should capture multimodal outputs, fails in this regard, with coverage fluctuating below 10%. Even the NSM regression model outperforms it in this case, but still stays far below either of the methods utilizing the high-level VLM module.\nFor hanging, to further evaluate the effect of different objects on predicted placement coverage, we aim to assess how these models generalize to racks with different sizes, geometries, and spacing between the sticks. Specifically, we generate new racks with different physical parameters compared to those used during training. The success rates are shown in Table II, and the coverage is plotted in Figure 7. Similar to the vial insertion task, AnyPlace outperforms other baselines, consistently getting higher coverage with fewer samples and eventually reaching 100% coverage made possible with the exceptional performance of the VLM module in finding the placement modes. RPDiff performs better in this case, but still struggles to cover all placement locations, with coverage saturating around 90%. This illustrates that the original RPDiff does not generalize well to objects with different geometries and sizes compared to the training set. In contrast, AnyPlace, aided by high-level placement prediction, allows the low-level prediction model to focus on learning placements based on local regions, thereby enabling strong generalization across varying object sizes and shapes.\nThe final evaluation we conduct in simulation focuses on assessing the precision of each approach. Precision is a critical factor in the successful completion of many placement tasks. Our goal is to determine whether providing models with a much smaller input region can enhance the precision of placement pose estimation. To achieve this, we evaluate the output of the prediction model directly rather than analyzing the results of the placement in simulation.\nIn Figure 8, we show the distribution of distance errors for each approach on the insertion tasks. Within model outputs that predict approximately the correct location, AnyPlace produces smaller errors and does so more reliably than the baselines. We also present the corresponding results for rotation prediction. Since the objects being placed are symmetric around the z-axis, we exclude yaw angle prediction from this comparison. As the initial pose is randomized and approaches cannot infer this orientation, all methods produce uniformly random yaw angle values. Given the minimal difference between the top and bottom sides of the object in this task, all approaches\u2014except NSM, which performs the worst in both position and orientation\u2014predict correct and flipped poses with roughly equal likelihood. Both AnyPlace and the RPDiff baseline perform well in predicting the correct orientation. Notably, the AnyPlace-EBM model achieves only slightly lower performance compared to the diffusion-based RPDiff and outperforms the NSM baseline. This is significant, as it highlights the potential of non-diffusion-based models for tackling these types of problems in future research."}, {"title": "D. Real World Evaluation: Precise Multi-Modal Placement", "content": "We also evaluate the performance of our approach on various placement configurations with different objects in the real world. Specifically, for each scene, we capture a"}, {"title": "E. Generalizable & Language-Conditioned Placement: Ap-plicability in Real-World Tasks", "content": "single RGBD image with a ZED Mini camera mounted on a Franka Emika Robot arm. We use exactly the same high-level pipeline for detecting local placement locations as we do in our simulation experiments. We also utilize the same models, trained purely on our synthetic dataset, for predicting the placement poses in a zero-shot manner. We use a simplified pipeline for executing the placement, using a specific grasp and performing placement inverse kinematics instead of a full motion planner. In addition, we do not utilize rejection sampling, but directly execute each trajectory on real. Our main goal is to evaluate whether enabling the model to predict only local placements would allow our models to generalize well to unseen objects in the real world even when only trained on our synthetic dataset. We perform systematic evaluations on the vial insertion task, with multiple possible placement locations. We also execute the approach on other language-conditioned placement variants in the real world, examples of which can be seen in Figure 9. All prompts used by the VLM to predict placement locations can be found in the Appendix. We use a standard vial rack used in chemistry labs and randomly block different holes with other vials or cover them with tape to test the flexibility of our high-level placement location prediction module. Overall, across 10 trials, the NSM and RPDiff fail to adapt to differences in this real-world task compared to the training conditions. AnyPlace, on the other hand, successfully completes placements, achieving the best performance, and inserts vials into 8 out of the 10 available empty spots in the vial rack.\nTo evaluate the generalization of AnyPlace, we conduct ex-periments on a large variety of objects for different placement configurations. As shown in Figure 9 (A), our method enables the robot to successfully perform placements in different scenarios, such as inserting a peg into a hole, hanging a cup on an unseen rack, and placing a plate into different slots of a plate rack. For precise placement, we demonstrate in Figure 9 (C) that the robot can accurately stack rings of varying radii, showcasing its ability to handle fine-grained placement tasks. Beyond single-step placements, AnyPlace excels in long-horizon tasks. For instance, we instruct the robot"}, {"title": "VI. LIMITATIONS", "content": "to precisely place a lid on a pot before placing the pot on a stove. Our method ensures that placed objects are centered and properly aligned, highlighting its reliability in real-world tasks. To demonstrate the advantage of combining VLM-based placement location prediction and local placement pose prediction modules, we show that the robot is able to place a bottle at different locations on a shelf based on language descriptions, as shown in Figure 9 (B). The key advantage of AnyPlace lies in its ability to integrate VLM-based high-level placement prediction with local placement pose refine-ment. By focusing on local regions for placement prediction and leveraging a diverse synthetic training dataset, it allows AnyPlace not only to generalize to real-world objects with significantly different geometries but also captures complex placement configurations with greater accuracy and flexibility.\nOur primary focus in this work is on predicting placement poses. However, challenges remain in executing the full pick-and-place task with the same level of generality. Not every sta-ble grasp of an object can be used to place it at a specific pose, and performing rejection sampling in real-world scenarios can be both difficult and time-consuming. Nonetheless, we believe our synthetic dataset and evaluation pipeline provide a strong"}, {"title": "VII. CONCLUSION", "content": "foundation for advancing in this direction by enabling the generation of training data for an end-to-end pick-and-place model applicable to a wide range of real-world placement tasks.\nWhile our approach improves precision in placement pose prediction, it is still limited by the accuracy of the point clouds it receives as input. Completing placement tasks that require high precision can be challenging with imperfect point cloud data. Recent advances in depth estimation from RGB images show promise in addressing this issue, as does continued progress in sensor quality. Another promising approach is implementing a policy that uses force/torque feedback to refine the final stage of placement. Similarly to the previous point, we believe our dataset and simulation pipeline provides a strong foundation for tackling this challenge by generating data for training reactive placement-execution policies.\nThe approach we propose does not include language con-ditioning in the low-level pose prediction model. As a result, we cannot distinguish between different types of placements at the same location. However, given the architecture of the low-level model, adding a language-conditioning input is straightforward. Our synthetic data generation pipeline is well-suited for this scenario, as it allows for the automatic addition of language conditioning to different placement types. Additionally, an LLM can be used to enhance diversity in language-conditioning data.\nIn this work, we presented a general pipeline for performing a wide range of object placement tasks using a robotic arm. We proposed a two-part framework consisting of a high-level module that determines potential placement locations and a low-level module that predicts fine placement poses. The core idea of our approach is to leverage a VLM to propose placement locations, allowing the low-level pose prediction model to focus only on the local region of interest in the object's pointcloud. This effectively reduces complexity and enhances generalization. To train our model, we created a synthetic dataset containing thousands of randomly generated objects and placement poses. We demonstrated the effectiveness of the entire pipeline in both simulation and real-world experiments. In simulation, we showed that AnyPlace outperforms baseline methods in terms of success rate, coverage, and precision. We then validated its robustness and generalization in real-world settings, where, given a single RGB-D image, AnyPlace predicts diverse placement configurations in a zero-shot manner and successfully generalizes to unseen objects."}, {"title": "ACKNOWLEDGMENTS", "content": "We would like to acknowledge the part of the funding provided by the University of Toronto's Acceleration Consortium from the Canada First Research Excellence Fund, grant number CFREF-2022-00042. We would also like to thank Dr. Anders G. Fr\u00f8seth, the Vector Institute, the Acceleration Consortium, Natural Resources Canada, and the Canada 150 Research Chairs program for their generous support."}, {"title": "APPENDIX", "content": "In Figure 10 and Figure 11, we show additional language prompts used by the VLM to predict placement locations in both real-world and simulation experiments. Based on our observations, the Molmo VLM accurately identifies the correct placement locations in both real-world and simulated images based on the language input. Even when the predicted location is not perfectly centered, our low-level pose prediction model still works. For instance, when predicting the placement position of the bottle on the top shelf, the location may be at the edge. However, our model"}]}