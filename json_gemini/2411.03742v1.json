{"title": "Adaptive Consensus Gradients Aggregation for Scaled Distributed Training", "authors": ["Yoni Choukroun", "Shlomi Azoulay", "Pavel Kisilev"], "abstract": "Distributed machine learning has recently become a critical paradigm for training large models on vast datasets. We examine the stochastic optimization problem for deep learning within synchronous parallel computing environments under communication constraints. While averaging distributed gradients is the most widely used method for gradient estimation, whether this is the optimal strategy remains an open question. In this work, we analyze the distributed gradient aggregation process through the lens of subspace optimization. By formulating the aggregation problem as an objective-aware subspace optimization problem, we derive an efficient weighting scheme for gradients, guided by subspace coefficients. We further introduce subspace momentum to accelerate convergence while maintaining statistical unbiasedness in the aggregation. Our method demonstrates improved performance over the ubiquitous gradient averaging on multiple MLPerf tasks while remaining extremely efficient in both communicational and computational complexity. A sample implementation of the method is available at https://github.com/yoniLc/AdaCons.", "sections": [{"title": "1. Introduction", "content": "Distributed optimization is essential for training modern deep neural networks on large-scale datasets. Distributed environments can utilize different methods to parallelize computations, such as data parallelism [4] and model (e.g., tensor, pipeline) parallelism [58], each offering distinct benefits and suited to various applications [8, 23]. This work focuses on the challenge of efficient gradient aggregation within synchronous data parallelism where each worker processes a different subset of data.\nSynchronous data parallelism evenly distributes subsets of the dataset among several compute nodes/workers. Each node computes its local gradient before their aggregation into a central (master) model. While this aggregation is generally done at every iteration, model averaging [69, 75], which averages individual models trained over parallel workers, was developed to reduce communication overhead.\nThe most ubiquitous aggregation methods remain linear combinations of the descent directions, such as averaging [50] and its proximal variants [71]. These aggregation methods are generally efficiently implemented using modern all-reduce strategies [10]. However, finding optimal aggregation remains an open problem since distributed systems are vulnerable to computing errors from the workers [5] or to out-of-distribution data samples inducing bad local gradients. Recently, researchers have begun wondering whether model averaging is the best strategy in a distributed setting [66], and have started exploring more elaborate aggregation schemes [27, 34, 60]. In particular, given multiple workers' directions, the aggregation remains an ill-posed problem that must be solved according to a prior metric of interest.\nIn this work, we propose an efficient linear subspace optimization perspective to the problem of gradient aggregation. Beyond the conceptual novelty, we make three technical contributions: (i) We first formulate the aggregation problem as a subspace optimization problem created upon the original optimization objective. (ii) We propose a first-order approximation of the solution allowing the efficient closed-form formulation of the model update. (iii) We further extend the solution with a momentum-based unbiased estimator based on the statistics of the previous subspace coefficients.\nThe benefits of the method are as follows. (i) The method outperforms the standard averaging technique by substantial margins on multiple training tasks while remaining scalable with respect to the number of workers. (ii) The method requires only low communication overhead and negligible computational cost. (iii) The method does not require hyper-parameter tuning (e.g., learning rate) or modification of the standard data-parallel setting."}, {"title": "2. Related Works", "content": "In the distributed data-parallel deep learning optimization setting [29], the communication between the workers occurs after all the nodes complete processing their data batches. Model or gradient averaging [69, 75], which (periodically) averages individual models or gradients trained over parallel workers, is generally applied as the aggregation method of choice. Elastic averaging was proposed by [71] to allow local workers to fluctuate and explore near the center variable. This scheme allows communication and computation overlap, which can potentially reduce communication delays as in Overlap SGD [64]. Instead of pulling each worker model towards the average model, Leader SGD [61] proposes to pull each worker model towards the best-performing worker during each iteration. The Ensemble-Compression scheme [60] suggests forming an ensemble of models from all workers to be further distilled to the central worker. Another approach proposes to learn to aggregate by gradient descent by replacing rule-based aggregation with a learned meta-aggregator [27]. In the field of federated learning [38], model averaging is shown as suboptimal [66]. Adasum [34] proposed an adaptive gradient aggregation simulating multiple sequential gradient descent steps. Recently, [18] proposed a new algorithm that periodically pulls workers towards the center variable computed as a weighted average of workers, where the weights are inversely proportional to the gradient norms of the workers in order to recover flat regions.\nThe idea of adaptive first-order optimization can be traced back at least to space dilation methods [59]. Back-PROPagation [54] originally proposed to scale the weights according to their sign. AdaGrad [21] adjusted the learning rate based on the estimated geometry, assigning larger learning rates to less frequent features. RMSProp [26] extended AdaGrad's capabilities via momentum. Adam [28] further improved RMSProp by incorporating a running average of gradients and became one of the most predominant optimizers for deep learning. Numerous subsequent works have proposed variants of Adam [20, 33, 52, 57, 68, 72] or topology aware optimization for generalization [22]. Classical second-order optimization algorithms pre-condition the gradient by adding curvature information [9, 14, 45], while many Hessian approximation or efficient computation methods have emerged over the years [1-3, 31, 35, 48, 48, 56, 67, 70]. However, variable metric methods generally lack success in dethroning adaptive first-order methods for large-scale stochastic optimization [6].\nThe core idea of subspace optimization is to perform the optimization of the objective function in a small subspace spanned by a set of directions obtained from an available oracle. A direct benefit of subspace optimization is that the low-dimensional optimization task at every iteration can be addressed efficiently using heavier optimization tools such as second-order methods. The subspace structure may vary depending on the chosen optimization technique. Early methods proposed to extend the minimization to a d-dimensional subspace spanned by d various previous directions, such as gradients, conjugate directions, previous outer iterations or Newton directions [13, 15, 16, 39]. The Krylov descent method defines the subspace as a series of preconditioned gradients [63]. Related to Krylov subspaces, the Conjugate Gradient (CG) method [25] reduces the search space to the current gradient and the previous step. [44] provided optimal worst-case complexity in the convex setting with the ORTH-method by spanning the subspace with three directions based on aggregated iterates. The Sequential Subspace Optimization algorithm [11, 12, 42, 74] extends ORTH by adding the previous search directions to emulate CG's manifold expansion property."}, {"title": "3. Method", "content": "We consider the minimization of a function F(w) in a distributed synchronous parallel computing environment with N\u2208 N workers and a master. We focus on the stochastic optimization problem of the following form\n$\\min_{w \\in \\mathbb{R}^d} F(w) := \\mathbb{E}_{\\zeta} f(w; \\zeta),$\nwhere w denotes the model parameter to be estimated and \u03b6 is a random variable that follows the data probability distribution. We consider the global variable consensus optimization [7] which defines the distributed empirical risk minimization as\n$\\min_{w} \\mathbb{E}_\\zeta f(w; \\zeta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} f(w; D_i),$\nwhere each \u03b6i follows the data distribution and its empirical estimate Di \u2282 D denoting a sampled batch from the dataset D. As an unbiased estimator, the ubiquitous distributed Stochastic Gradient Descent algorithm's iteration is defined as\n$w_{t+1} = w_t - \\eta_t \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_w f(w; D_i)|_{w=w_t}$\n$:= w_t - \\eta_t \\sum_{i=1}^{N} g_i(w_t),$\nwith \u03b7t the step size at time t.\nGiven ${g_i(w_t)}_{i=1}^{N}$ the set of gradients to be aggregated, we aim to find an aggregation scheme Vt : ${g_i(w_t)}_{i=1}^{N} \\rightarrow \\mathbb{R}^d$ such that the descent iterate is defined as\n$w_{t+1} = w_t - \\eta_t V_t(\\lbrace g_i(w_t) \\rbrace_{i=1}^{N})$"}, {"title": "3.1. Aggregation as Subspace Optimization", "content": "Given N workers' descent directions ${g_i(w_t)}_{i=1}^{N}$ (e.g., gradients or local steps) obtained from the workers and a given iterate wt, we propose to solve the following optimal linear aggregation objective defined as\n$\\min_{\\lbrace a \\in \\mathbb{R}^N\\rbrace} f(w_t + P_t a; D) := f(w_t + \\sum_{i=1}^{N} g_i(w_t)a_i; D),$"}, {"title": "3.2. Subspace First-Order Expansion", "content": "We propose to approximate the best subspace direction via first-order Taylor approximation near the current working point wt. Thus, given ao = 0 and \u03bb \u2208 R+ the first-order approximation step size of the subspace objective, we have\n$\\alpha = \\alpha_0 - \\nabla_a f(w_t + P_t a; D)|_{a=a_0}$\n$= - \\lambda P_t^T \\nabla_w f(w_t; D)$\nUsing Monte-Carlo approximation of the gradients [41] according to the workers' directions, we assume $\\nabla_w f(w_t; D) = \\sum_{i=1}^{N} g_i(w_t) := g(w_t)$. Since the subspace optimization setting is invariant to the scaling of the spanning directions, we assume column normalized Pt [74] such that we have \u2200i \u2208 {1, ..., N}\n$\\alpha_i = -\\lambda ( (P_t)_i, \\hat{g}(w_t) )$\n$= -\\lambda \\frac{ (g_i(w_t), g(w_t)) }{||g_i(w_t)||}$\n$\\lambda \\sum_{j=1}^{N} \\frac{ (g_i(w_t), g_j(w_t)) }{||g_i(w_t)||} $"}, {"title": "3.3. A Preconditioned Gradient Perspective", "content": "The subspace optimization paradigm infers a preconditioned gradient method via the subspace matrix. Given the preconditioning matrix G, the aggregated update can be written as\n$V_t(\\lbrace g_i(w_t) \\rbrace_{i=1}^{N}) = G \\nabla_w f(w_t; D)$\n$:= \\lambda P_t P_t^T \\nabla_w f(w_t; D)$\n$= \\lambda (\\sum_{i=1}^{N} \\frac{ g_i(w_t) g_i(w_t)^T }{||g_i(w_t)||^2}) \\nabla_w f(w_t; D)$\nThis emerging formulation is certainly evocative of the Natural gradient [1, 48] or Gauss-Newton [47] methods where curvature information is integrated into the optimization. However, the Hessian approximation emerges in its non-inverted form through the subspace optimization framework. As a sum of rank-one matrices, G is positive-definite and ensures a descent direction."}, {"title": "3.4. Adaptive Subspace Coefficients", "content": "Momentum gradient descent, or the heavy ball method [50] combines the current gradient with the history of the previous steps to accelerate the convergence of the algorithm is ubiquitous in modern optimization methods. To avoid potential instabilities during training and enforce smoothness between consecutive subspace aggregations, we propose to apply the momentum method over the subspace coefficients, such that, with \u03b2 \u2208 (0, 1) we have\n$\\alpha^m_i = \\beta \\alpha_{i-1}^m + (1 - \\beta) \\alpha_i$\nwith \u03b1m the exponential moving average (EMA) of the subspace coefficients at iteration t. However, each worker's batch is arbitrarily distributed, and each gradient's coefficient should be decoupled from its index. Thus, to maintain the smooth subspace coefficients' distribution between consecutive iterations, we enforce invariance to the ordering by sorting the coefficients. Then the momentum moving average is performed to redistribute each smoothed coefficient to its corresponding worker. Formally, given the sorting operator S we have\n$\\alpha^m_i = \\beta \\alpha_{i-1}^m + (1 - \\beta) S(\\alpha)$\n$\\alpha_i \\leftarrow S^{-1}(\\alpha^m)$\nThis approach allows a smoother aggregation between iterations avoiding potentially unstable local gradients.\nFinally, to enforce unbiased estimation, we further constrain the weighting coefficients to sum to one. By rewriting the weighted aggregated direction more compactly, we have\n$V_t(\\lbrace g_i(w_t) \\rbrace_{i=1}^{N}) := AdaCons(\\lbrace g_i(w_t) \\rbrace_{i=1}^{N}, t = \\sum_{i=1}^{N} \\gamma_i g_i(w_t)$\nwith \u03b3i = (\u03b1+)i = $\\lambda g_i(w_t)^T \\hat{g}(w_t) / ||g_i||^2$ and where the unbiased estimator with respect to Pt is given by enforcing sum one normalization such that\n$\\lambda = 1 / \\sum_{i=1}^{N} ( g_i(w_t)^T \\hat{g}(w_t) / ||g_i||)$"}, {"title": "3.5. Distributed System Implementation", "content": "We present in Algorithm 1 the communication and computational complexity of the method. Since the number of workers N is negligible compared to the dimension d of the model (i.e., N < d), the method only requires an additional asynchronous (ring) all-reduce of the weighted gradients compared to the traditional averaged aggregation.\nThe method begins by aggregating the gradient and computing the subspace coefficients (Eq. 7) using a single asynchronous all-reduce call. After the coefficients are shared among the workers via all-gather, the subspace momentum (Eq. 11) is applied followed by the unbiased estimation (Eq. 13). Finally, each worker locally adjusts its gradient using the corresponding weight factor, before performing a final all-reduce summation of the directions."}, {"title": "4. Experiments", "content": "Due to the unbiased formulation of the algorithm, the proposed aggregation method offers a parameter-free framework that eliminates the need for hyper-parameter tuning (e.g., learning rate) or modifications to the aggregation setting (e.g., double precision accumulation or aggregation of the local optimizer updates [34]). As a result, any performance improvements stem solely from the quality of the aggregated gradients, while the training setup remains identical to the baseline. Moreover, as described in Section 3.5, the method requires minimal code integration thanks to communication hooks that fully manage the aggregation process.\nThe distributed training infrastructure is constrained to a maximum of 8 nodes, each equipped with 4 RTX A6000 GPUs (48 GB per GPU), totaling 32 workers (N = 32), connected via a 100 Gb/s Infiniband network. These resource limitations prevent us from scaling our experiments to very large tasks, as discussed in Section 5.4.\nOur analysis begins with simulated linear regression"}, {"title": "4.1. Stochastic Linear Regression", "content": "We first analyze our method with the stochastic linear regression task where the optimization objective is given by\n$\\min_{w \\in \\mathbb{R}^{1000}} \\mathbb{E}_{\\zeta \\sim U[0,1]} |\\frac{1}{2} (w^T \\zeta)^2 |$\nFor a fair, hyperparameter-free comparison, we provide each method with the optimal (analytical) step size using SGD. The results, presented in Figure 2 analyze the impact of varying batch sizes and the number of workers. The performance improvement of our method is evident, particularly when using a large number of directions combined"}, {"title": "4.2. Imagenet", "content": "We present the performance of AdaCons on the MLPerf classification task using the ResNet-50v1 [24] architecture on the ImageNet [55] dataset, where the baseline performance is achieved with 8 workers. The results are shown in Figure 3. Our method demonstrates improved convergence properties while maintaining scalability, resulting in a consistent improvement of 1% in final accuracy."}, {"title": "4.3. RetinaNet", "content": "We evaluate the performance of AdaCons for the MLPerf object detection training task using RetinaNet [30] built upon the SSD architecture [32]. The baseline performance is obtained using 16 workers (the target mAP is 0.34). Figure 4 illustrates the performance comparison. We can observe that our method presents better convergence property while maintaining a positive 0.7% and 0.2% final accuracy (mAP) gap with 16 and 32 workers, respectively."}, {"title": "4.4. Deep Learning Recommendation System", "content": "We further show the performance of the proposed aggregator on the MLPerf Deep Learning Recommendation System (DLRM) [43] training task defined the DCN V2 architecture [65]. The baseline performance is given with a batch size of 64K workers with a target Area Under the Curve (AUC) of 0.8025. Due to the high memory requirement, the experi-"}, {"title": "4.5. BERT", "content": "We present the performance of our framework during the pretraining (phase one) of BERT-Large [17]. We evaluate the baseline setting (batch size = 64K with 7.037K iterations) as well as a 20% reduction in iterations (5K) [34]. For the original setting, we observed a 3% minimal accuracy gap (1.381 vs 1.341) with a 14% speedup to reach the baseline minimum loss value. For the second setting, a 1% final accuracy gap only is observed with a 6% speedup. In both settings, a gap emerges during the initial stages of training, as shown in Figure 6. Section 5.4 discusses the possible reasons for these performance."}, {"title": "5. Analysis", "content": "In this section we present the analysis of the proposed method, including timing, ablation of the algorithmic components, and limitations study."}, {"title": "5.1. Timing", "content": "Table 1 presents the per-iteration timing (in seconds) of the proposed method compared to the native PyTorch all-reduce baseline. We observe that the proposed method induces only a slight slowdown, ranging from 1.04\u00d7 to 1.05\u00d7. It is important to note that our implementation operates at a higher level of abstraction, leaving room for further optimizations. Furthermore, the communication setup used in our experiments is relatively slow compared to modern network speeds (800 Gb/s), where both the low-dimensional all-gather and the model all-reduce communications would become negligible."}, {"title": "5.2. Ablation", "content": "Table 2 presents the impact of different variants of the method on performance. We demonstrate the improvements"}, {"title": "5.3. Subspace Coefficients", "content": "We provide in Figure 7 a visualization of the subspace coefficients' statistics at the different stages of the algorithm. We first show in plot (a) the coefficients induced by the subspace linear approximation. We can observe they are greatly induced by the local gradient norms. We then show in plot (b) how the EMA allows smoother transitions of the coefficients between iterations. Finally, we present in plot (b) the coefficients normalized to sum one, where we can observe the clear standard deviation of the values."}, {"title": "5.4. Limitations and Future Works", "content": "The method is grounded in subspace optimization [13, 44], where the descent direction depends on the quality and the diversity of the subspace. Since the approach does not aim to solve full subspace optimization (Sec. 3.1), the linear approximation relies on good initial subspace directions, which are obtained using a sufficiently large local batch size. However, an excessively large batch size would lead to similar gradients [37], causing the method to collapse into standard averaging. This trade-off resembles the one seen in dynamic batch size methods [51]. Based on our experiments, the baseline settings and their scaled versions yield good results, though performance could be improved by using more workers.\nOur experiments suffer from a systemic limitation (a maximum of N = 32 workers), which prevents large-scale testing and richer subspaces. For example, in the BERT training task (Sec. 4.5), our setup shows very low gradient variance, causing the method to collapse to near-averaging behavior. Specifically, the standard deviation of the subspace coefficients ranges between 10-2 and 10-3.\nFinally, modern architectures and optimization techniques may hinder the method's performance. For instance, gradient clipping, while critical for the convergence of large-scale transformers [62], appears to limit the method's effectiveness. We provide experimental results on the Imagenet classification task using the ViT32 architecture [19] (TorchVision) with and without gradient clipping in Figure 8. While we can observe gradient clipping is crucial for large Transformer based models, Adacons seems a more appropriate aggregation scheme under perturbed gradients, where removing clipping allows Adacons to outperform the final top-1 accuracy by 5.26%.\nFuture work includes investigating the data and the estimation of the smallest local batch size necessary for constructing effective subspaces. Alongside the proposed momentum variant of our algorithm, Hessian- or natural gradient-based methods applied within the subspace enable efficient low-dimensional matrix inversion (i.e., in N dimensions) [12]. Related to loss curvature, regularization of the subspace coefficients via a trust-region constraint may further enhance methods such as those in [22]. Finally, while our method requires no modification to the training setup, its robustness suggests a potential for accelerated optimization through larger learning rates."}, {"title": "6. Conclusion", "content": "We introduce a novel aggregation framework for synchronous distributed training. The proposed method is derived from the subspace optimization paradigm where the workers' gradients span the subspace in which the training objective is to be minimized. The framework enables the efficient linear weighting of gradients through a first-order approximation of the subspace optimization objective. Additionally, subspace momentum is incorporated to enhance performance, while unbiased estimation further ensures the method remains hyperparameter-free. The method requires minimal integration effort and incurs very low communication overhead, yet achieves superior accuracy and scalability compared to the standard averaging technique. By redefining gradient aggregation as an optimization problem, this framework may pave the way for the development of more advanced and high-performance aggregation methods."}]}