{"title": "Measuring Responsibility in Multi-Agent Systems", "authors": ["Chunyan Mu", "Nir Oren"], "abstract": "We introduce a family of quantitative measures of responsibility in multi-agent planning, building upon the concepts of causal responsibility proposed by Parker et al. [18]. These concepts are formalised within a variant of probabilistic alternating-time temporal logic. Unlike existing approaches, our framework ascribes responsibility to agents for a given outcome by linking probabilities between behaviours and responsibility through three metrics, including an entropy-based measurement of responsibility. This latter measure is the first to capture the causal responsibility properties of outcomes over time, offering an asymptotic measurement that reflects the difficulty of achieving these outcomes. Our approach provides a fresh understanding of responsibility in multi-agent systems, illuminating both the qualitative and quantitative aspects of agents' roles in achieving or preventing outcomes.", "sections": [{"title": "Introduction", "content": "In Multi-Agent Systems (MASS), existing approaches to responsibility analysis typically provide a binary assessment as to whether an agent is responsible for a specific outcome. Such analysis is useful in ensuring that the MAS operate effectively, responsibly, and ethically across various domains. However, in many cases, responsibility is not binary. Instead, an agent can be partially responsible for an outcome, and as MASS grow in complexity, a more fine-grained (and quantitative) understanding of responsibility becomes increasingly important. In contrast to existing approaches, we introduce quantitative responsibility analysis, seeking to provide a numerical measure signifying the extent to which an agent bears responsibility for some outcome.\nWhile existing works have extensively explored aspects of responsibility in multi-agent systems [11, 7, 1, 21, 18, 4], our focus lies in the quantitative analysis of responsibility within the context of coalition plans. This framework allows for a more precise understanding and anticipation of overall responsibility for outcomes with time. Specifically, in alignment with the notions proposed in [18], we focus on probabilistic and entropy-based measures of causal responsibility (CR) for certain outcomes and their associated concepts, namely Causal Active Responsibility (CAR), Causal Passive Responsibility (CPR), and Causal Contributive Responsibility (CCR)."}, {"title": "Model", "content": "This section presents a formal model to facilitate the structured analysis of responsibility in stochastic MASS."}, {"title": "The Stochastic Model of MASS", "content": "The model of MASs is based on stochastic game structure with finite traces.\nDefinition 1. A multi-agent stochastic transition system is a tuple G = (Ag, S, Act, \u03b4, 2Ap, L), where\nAg = {1,..., n} is a finite set of agents;\nS is a finite non-empty set of states;\nAct = {a1, a2, ...} is a non-empty finite set of actions;\n\u03b4 : S \u00d7 ActAg \u2192 Dist(S) is the probabilistic transition function mapping from a state and joint actions to a distribution over new states;\nAp is a finite set of atomic propositions;\nL : S \u2192 2Ap_is the state labelling function mapping each state to a set of atomic proposition drawn from Ap.\nWe write (s, a) \u2192 \u03bc for s \u2208 S, \u03b1 \u2208 Act^g, \u03bc \u03b5 \u03b4(s,a), and write s\u2192 s' whenever (s, a) \u2208 \u03bc and \u03bc(s') > 0.\nDefinition 2. A history p (i.e., a finite path) is a non-empty finite sequence S000S1A1...Sk of states and joint actions, where ai \u2208 ActAg is the ith joint action, Si+1 \u2208 d(si,ai), meaning that the next state si+1 is drawn from the distribution defined by the transition function d for state-action pair (Si, ai) for every i. Let ps(i) denote the ith state of p, and pa(i) denote the ith joint action of p, so for all i, we have ps(i) Pali) ps(i + 1). Let Hist\u00e7(s) denote the set of histories starting from states, which we refer to as G-histories.\nGiven G and p starting from s \u2208 S, the cone generated by p, written (p), is the set of complete paths ending in a terminal state, i.e., (p) = {p' \u2208 Histg(s) | p \u2264 p'}, where the ordering notation < denotes the prefix relation. Given such a system and a state s \u2208 S, we can then calculate the probability value, denoted by Ps(p), of any history p starting at s as follows:\nPs(s) = 1, and\nPs(ps') = Ps(p)\u03bc(s') for (last(p), \u03b1) \u2192 \u03bc. In other words, for the last joint action in p from p's final state last(p) which leads to s'.\nLet s Hist\u00e7(s) be the sample space, and let Gs be the smallest o-algebra induced by the cones generated by all the finite paths of G. Then P induces a unique probabilistic measure on Gs such that Ps((p)) = Ps(p) for every history p starting in s."}, {"title": "Probabilistic ATL", "content": "ATL [2] extends traditional temporal logics, allowing for the expression of properties related to concurrent decision-making and strategic interactions among agents. Probabilistic ATL (pATL, e.g., [10]) builds upon ATL by introducing a probabilistic dimension, enabling the modelling of uncertainty and randomness in multi-agent systems. We adapt probabilistic ATL to express properties over finite paths, aligning it with our objective of (quantitative) responsibility analysis.\nDefinition 8 (Syntax). Let G = (Ag, S, d, Ap, L). The syntax of pATL is made up of state formulae and history formulae represented by & and & respectively.\n\u03a6 ::= a | \u00ac\u03c6 | \u03a6\u039b\u03a6 | \u3008A\u3009[4] | Pop (A) [4]\n\u03c8, \u03c6 ::= \u039f\u03a6 | \u00a2U\u2264k$ | \u00ab\u03c8 | \u03c8 \u039b \u03c6\nHere a \u2208 Ap is an atomic proposition, AC Ag is a set of agents, (A)[4] expresses the property that coalition A has a joint strategy to enforce \u03c8, \u2208 {< , <, >, >}, k\u2208 {0,1,2, ...} is a time bound, and p \u2208 [0,1] is a probability bound. The formula Pop (A) [4] expresses that coalition A has a strategy such that the probability of satisfying history formula & is p, when the strategy is followed.\nDefinition 9 (semantics). For a state s \u2208 S of G, the satisfaction relation s\u00e7 & for state formulae denotes \u201cs satisfies \u03c6", "iff s =\u00e7 & and s\u3151\u03c2 \u03c6": "ns=g (A)[4] iff \u2203a s.t. \u2200p \u2208 Hist\u04ab(s, \u03c3\u2081).p\u2550G\u03c8.\nsGPxp(A)[V] iff \u2203x, s.t. Prob(s, [(A)[4]]) <p, where:\nProb(s, [(A)[V]]) = Ps{p \u2208 Histg (s, \u03c3\u2081) | p |=\u03c2 \u03c8}.\nFor a history p\u2208 Histg (so), the satisfaction relation h =gy for history formulae denotes \"h satisfies \u03c6\":\np=gO iff ps(1) = \u03c6.\np=\u03c2 \u03c6U<k' iff \u2203i \u2264 k.ps(i) =\u00e7 d' and \u2200j < i.ps(j) =\u00a2 \u00a2.\np=\u00e7\u00ab\u03c8iff p\u2260\u03c2 \u03c8.\n\u03c1\u03c2 \u03c8 \u039b \u03c6 iff p =\u04ab \u03c8 and p\u03c2 \u03c6.\nThe until operator U allows one to derive the temporal modalities\u25ca (\"eventually\u201d) and (\u201calways\u201d): \u25c7<k\u03c8 \u2261 true U<k \u03c8 and \u25a1<k\u03c8 \u2252 \u00ac\u25ca<k(\u00ab\u03c8)."}, {"title": "Formalising Responsibility", "content": "We incorporate the causal responsibility definitions introduced in [18] into our model and extend the logic of pATL into a formulation we refer to as YATL by adding the following elements to state formulae.\n$ ::= ... | CARG(i, \u03c0,\u03c6) | CPRG(i, \u03c0,\u03c6) | CCRG(\u03af, \u03c0,\u03c6)\nDefinition 10 (Causal Active Responsibility (CAR)). Given G, i \u2208 Ag, a joint plan \u03c0, and an outcome y,we say i bears CAR for p in \u03c0ats, denoted via the operator CAR\u00e7(i, \u03c0,\u03c6) in yATL, if y holds for all possible histories consistent with Plan({i})(s) while y does not hold in some histories consistent with Plan(Ag) (s). We define the semantics for the operator CARg(i, \u03c0,\u03c6) in \u03b3ATL as follows:\n89 CARG (\u03af, \u03c0,\u03c6) iff\n\u0395\u03c0' \u2208 Plan{i}) (s).\u2200\u03c1' \u2208 Hist\u2081,(s).p' =\u00a2\u00a2\u2227\n\u039e\u03c0\" \u2208 Plan(Ag) Plang (s).\u2200\u03c1\" \u2208 Hist\u2081\"(s).p\" \u2260g 4\nIntuitively, agent i takes active responsibility for the occurrence of the outcome expressed by \u03c6 in starting at so if keeping i's actions fixed the other agents could not avoid the outcome by choosing different actions. A brief algorithm for checking the CAR operator is presented in Algorithm 11.\nExample 2. Consider the outcome expressed by\n\u03c6 = (A1, A2) (fine V payoff2)\nin Example 1, and a joint plan: \u03c0 = (Defect1Defect2). Note that \u2200\u03c0' \u2208 Plan{A1}) (80), Hist\u03c0\u03b9 (80) FG \u03c6, (i.e., for \u03c0' = (Defect1Coop2) or \u03c0' = (Defect1Defect2)). Therefore, while keeping the initial state and actions of A1 fixed, the other agents (A2 in this example) could not have acted differently to avoid the occurrence of 4. Now consider \u03c0\" = (Coop\u2081Defect2), and note that Hist\u2081\"(s) \u2260\u04ab 4, i.e., there exists a joint plan \u03c0\" \u2208 Plan({A1,A2}) (50) which avoids the occurrence of 4. Thus, 80G CARG(A1, \u03c0,\u03c6) and A\u2081 bears CAR for \u03c6in\u03c0at so."}, {"title": "Measuring Causal Responsibility", "content": "We seek to identify the level of responsibility an agent bears for a given outcome in a joint plan. We quantify the degree of responsibility based on three distinct metrics: proportion, probability, and information entropy of relevant behaviours."}, {"title": "Metrics", "content": "We introduce our concept for measuring behaviours within the context of a general language. Given a finite system G over alphabet \u2211 (i.e., the joint actions ActAg), we denote its language by L(G). For a language L \u2286 \u2211*, Cn = L \u2229 \u03a3\u03b7 denotes the set of length n of words in L.\nProportional measure. Perhaps the simplest approach to measuring behaviours involves counting the cardinality of elements which satisfy the behaviour. We denote by C(L4) = |L4| the number of elements (i.e., histories) in our language which satisfy . We refer to this measure as the proportional measure. Such a measure is useful in scenarios where one wants to assess the impact of a specific strategy on the outcome. By counting possible behaviours regarding what might have happened if a different strategy had been taken, proportional measures provide insights into the causal relationships between strategies and outcomes.\nProbabilistic measure. An alternative approach to quantification involves computing the probabilities associated with behaviours leading to the realisation of the outcome. Given G and so \u2208 S, the probability value, denoted as Ps (p), for any history p\u2208 Hist(s) generated by starting at so can be calculated as described earlier. Probabilistic measures are particularly useful in scenarios where outcomes are influenced by probabilistic factors. This approach allows for a more complex and precise assessment of the likelihood of different behaviours. While probabilistic measures are often relevant, they have a significant limitation: such measures can be too precise. For many interesting properties the probability (on very long or infinite behaviours) is either 0 or 1, making quantitative analysis infeasible.\nExample 5. Consider the scenario presented in Example 1, and the properties: \u03c8 = \u25a1<t(fine V reward). and \u03c8' = \u25a1<t\u25ca<100(payoff\u2081 \u2228 payoff2). Intuitively, when t is large, an agent must undertake a certain level of effort to ensure that & is satisfied (as it has to be maintained across all t units of time), while less effort is required to satisfy ' (as the condition must hold at least once every 100 time units). However, a probability analysis for both properties over very long runs tends towards 0, and therefore disagrees with this intuition:\nP((A1, A2)) = lim (1/2)^t = 0\nt\u2192\u221e\nP((\u03911, \u03912)\u03c8') = lim (1-(3/4)^100)^t = 0\nt\u2192\u221e\nNevertheless, it seems counter-intuitive to say that satisfying w' is as challenging as satisfying \u03c8.\nEntropy measure. Given this weakness of probabilistic measures, we turn our attention to measurement based on the notion of entropy as an alternative. This approach offers a measure of the information content in the system. In situations where traditional probability measures might lack granularity, entropy provides a more refined view, capturing the complexity and uncertainty in the system. The entropy of a language (of finite words) L \u2286 \u2211* ([12]) is defined as:\nH(L) = lim sup (log2(1+|L|))/n\nn\u2192\u221e\nIntuitively, the entropy of a language represents the quantity of information (measured in bits per symbol) contained in \"typical\" words of that language."}, {"title": "Measuring causal responsibilities", "content": "We can now measure the degree of responsibility ascribed to an agent using the described metrics.\nDefinition 13 (CAR degrees). Given G, i \u2208 Ag, a joint plan \u03c0, and an outcome . Let:\nLCAR(\u03ad,\u03c0,\u03c6) = {p\u2208 Hist\u201e'(so) | \u03c1 = \u03c6 and \u03c0' \u2208 Plan({i}) (so)}\nLCAR(i,\u03c0,\u03c6) = {p\u2208 Hist\u201e\u3003 (so) | \u03c1\u2260\u03c6and \u03c0\" \u2208 Plan(Ag) (so)}\nThen: DCAR (i, \u03c0, \u03c6) = Y(LAR(i,\u03c0,\u03c6))/Y(L)\u00b7 \u043a\u00af. If X = # and Y = C then the degree of CAR is computed via a proportional measure; if X = P and Y = P then the degree of CAR is computed via a probabilistic measure; and if X = H and Y = H then it is calculated through an entropy measure. Here \u043a = 1 if |LCAR(\u03af,\u03c0,\u03c6))| > 0 (i.e., is avoidable) and \u043a\u00af = 0 otherwise.\nIn essence, the degree of CAR for agent i concerning outcome under plan \u03c0 is characterised by the extent to which i can influence the system to fulfil the specified property. This is quantified by the ratio of behaviours that adhere to when following the strategies devised by i within \u03c0, relative to all possible behaviours that lead to 4, when the occurrence of is avoidable (i.e., when \u043a\u00af > 0). The algorithm to compute the degrees of CAR underpinned by the metrics of Section 4.1 is sketched in Algorithm 4.\nExample 7. Consider Example 2 with i = A1, and assume each agent executes action Defect with probability, and coop with probability.\n(a) Consider outcomes and plan:\n\u03c6 = (A1, A2) \u25cb (fine \u2228 payoff2), \u03c0 = (Defect1, Coop2)\nwe have:\nLCAR(A1,7,9) = {(Defect1Coop2), (Defect1Defect2)}\nLCAR(A1,\u03c0,\u03c6) = {(Coop\u2081Defect2), (Coop1Coop2)}\nDEAR(A1, \u03c0, \u03c6) = Dear(A1, \u03c0, \u03c6) = DEAR(A1, \u03c0, \u03c6) = 1\nSince A\u2081's Defect\u2081 in facilitates outcome 4, A\u2081 has full CAR for this outcome, which also meets our intuition.\n(b) Now consider outcome and plan:\n= (A1, A2)<t(reward \u2228 payoff2), \u03c0 = (Defect1, Coop2)\nwe have:\nLAR(A1,7,9) = {...(Defect1Coop2)}\nLCAR(A1,\u03c0,\u03c6) = {...(Coop\u2081Defect2),...(Defect1Defect2)}\nDEAR(A1, \u03c0, \u03c6) = 3^{t-1}/2*3^{t-1} = 1/2,\nDEAR (A1, \u03c0, \u03c6) = (3/16*(1/4)^{t-1})/((3/16 + 9/16) * (1/4)^{t-1}) = 1/4,\nDEAR(A1, \u03c0, \u03c6) = (log(1+3^{t-1}))/(log(1+2*3^{t-1})) *1.\nHere, both the proportional and probability measures indicate that A\u2081 maintains a fixed partial degree of CAR, with the probability measure providing a more precise assessment due to the non-uniform distribution of actions. The entropy measure yields a more asymptotic degree measurement over time. When t is small, indicating a relatively harder-to-reach outcome, A\u2081's active responsibility is relatively small. As t increases, making the outcome progressively more accessible, A\u2081's active responsibility increases. This aligns with intuition as larger\n\nvalue of t correspond to easier attainment of the outcome and consequently greater active responsibility for A1. Figure 2 plots CAR for the 3 measures.\nDefinition 14 (CPR degrees). Given G, i \u2208 Ag, a joint plan \u3160, and an outcome . Let:\nLCPR(\u03af,\u03c0,\u03c6) = {\u03c1\u2208 Hist\u201e\u00b4 (so) | \u03c1 = \u03c6 and \u03c0' \u2208 Plan(Ag) (so)}\nLCPR(\u03af,\u03c0,\u03c6) = {\u03c1\u2208 Hist\u201e\" (so) | \u03c1\u2260\u03c6and \u03c0\" \u2208 PlanAg\\{i}} (so)}\nThen: Dapr(i, \u03c0,\u03c6) = Y(LCPR(i,\u03c0,\u03c6))/Y(L\u00ac\u03c6) \u2022k+. If X = # and Y = C then the degree of CPR is obtained via a proportional measure; if X = P_and Y = P then the degree of CPR is computed via a probabilistic measure; and if X = H and Y = H then it is calculated via an entropy measure. Here k+ = 1 if |LCPR(\u03af,\u03c0,\u03c6))| >0 (i.e., is avoidable) and k+ 0 otherwise.\nIntuitively, the CPR degree of agent i for the occurrence of under the plan \u03c0 measures the level of difficulty for i to avoid the outcome by selecting different actions while keeping all other agents' actions fixed. This is quantified by the ratio of behaviours in which i varies its actions while other agents maintain compatible actions in \u3160, relative to all possible behaviours violating 4, when the occurrence of under the plan is achievable (i.e., when \u043a\u207a > 0). The algorithm to compute the degrees of CPR is provided in Algorithm 5.\nExample 8. Continue Example 1, consider i A1 and 4 = (A1, A2)<t reward and assume each agent executes Defect and coop with probability 1/2 and 1/2 (uniform) respectively. Consider \u03c0 = (Coop1, Coop2), we have:\nLCPR(A1,\u03c0,\u03c6) = {... (Coop\u2081Coop2)}\nLCPR(A1,\u03c0,\u03c6') = {...(Defect1Coop2)}\nDEPR(A1, \u03c0, \u03c6) = DEPR(A1, \u03c0, \u03c6) = 1/3\nH\\DEPR(A1, \u03c0, \u03c6) = (log(1+3^{t-1}))/(log(1+3^t))\nBoth proportional and probability measures indicate that A\u2081 bears a third of CPR. The entropy-based measure changes over time: as t increases, indicating a\n\n\nsmaller difficulty in achieving the outcome and a concomitant greater challenge to avoid the outcome, A\u2081's passive responsibility along the given plan increases, aligning with intuition. Figure 3 illustrates this example.\nOn the other hand, assume we consider the joint plan \u03c0 = (Defect1, Coop2) instead, it can be observed that i does not bear CPR, and all degrees of CPR are zero, as the outcome is unattainable following \u03c0' (as \u03ba+ = 0).\nDefinition 15 (CCR degrees). Given G, i \u2208 Ag, a joint plan \u03c0, and an outcome. For all JC Ag, let:\nLCCR(i,\u03c0,\u03c6) = {p \u2208 Hist\u2081' (so) | \u03c1\u3151\u03c2 \u03c6 and \u03c0' \u2208 Plan) (so) \u041b\u0456\u2208 J}\nLCCR(i,\u03c0,\u03c6) = {p \u2208 Hist\u201e\" (so) | p |\u2260gy and \u03c0\" \u2208 Plan J\\{i})(so)}\nThen\nDECR (i, \u03c0, \u03c6) = (\u2211VJC Y(LCCR(\u03af,\u03c0,\u03c6))\u00b7 \u043aJ,-)/(#J.(|LCCR(\u03af,\u03c0,\u03c6))| > 0))\nIf X = # and Y = C then the degree of CPR is obtained via a proportional measure; if X = P and Y = P then the degree of CPR is computed via a probabilistic measure; and if X = H and Y = H then it is calculated via an entropy measure. Here #J.p represents the count of subsets J for which y holds, and K= 1 if |Lock LCCR(\u03af,\u03c0,\u03c6)) > 0 (i.e., is avoidable); \u043a\u0408-= 0 otherwise.\nIntuitively, the CCR degree for agent i for the occurrence of under plan \u03c0, defines how much i could contribute to the occurrence of the outcome on average considering all possible i-included coalitions' joint plans that could lead to the outcome when it is unavoidable (\u03ba\u00af > 0). The correction factor \u043a\u00af distinguishes between avoidable and unavoidable outcomes, adjusting the degree accordingly. The algorithm to compute the degrees of CCR is provided in Algorithm 5.\nExample 9. Returning to Example 4, we assume each agent takes action Defect and coop with uniform likelihood (i.e., with probability 1/2 for each action). Consider\n4 = (A1, A2)<t (fine V reward)\nwhere the joint plan is given by (catch\u2081, catch2), (skip\u2081, skip\u2082).... Thus, A1 and A2 consistently choose the same action at each step, alternating between catch and skip, repeating this for t steps. Here, J can be either {A1} or {A1, A2}, each case requires that in each step of the corresponding joint plan \u03c0\" forming LCCR(\u03af,\u03c0,\u03c6), A1 must consistently opt for a different action to A2 to satisfy the conditions (violating ) imposed by CCR. Note that in this scenario,\nLy = {(Defect1Defect2)^k1(Coop1Coop2)^k2 | k\u2081 + k2 = t}\nso C(L4) = 2t, P(L\u2084) = 1/2*, H(L\u2084) = log 2. Thus:\nDCR(A1, \u03c0, \u03c6) = DECR(A1, \u03c0, 4) =\nDECR(A1, \u03c0, 4) =\nIn scenarios with a small value of t, the realisation of is relatively easy, and the degree of CCR that A\u2081 bears should be relatively significant. All measures indicate a positive degree of CCR for A1, aligning with intuition. As t increases, achieving becomes more challenging, and the CCR measure should gradually decrease. All measures show a decrease, but the probability measure converges faster, while the entropy measure converges more slowly. Intuitively, as t grows, A1 should partially bear CCR for the outcome, and this contribution should gradually diminish as t\u2192\u221e. The entropy-based measure decays more slowly, and thus agrees better with this intuition. Figure 4 illustrates this visually.\nComplexity. In quantitative cases, the complexity is slightly higher than in qualitative cases due to additional factors such as computing the cardinality or probability of each history, which is linear with respect to the length of the histories."}, {"title": "Conclusions and Future Work", "content": "This paper explored the notion of quantitative causal responsibility within a MAS setting. To do this, we extended ATL with new operators aimed at formalising causal responsibility concepts and demonstrated that different metrics for such responsibility may be useful in different contexts.\nWe have identified several avenues for future work including reasoning about tradeoffs between responsibility and coalition performance. We also plan to incorporate strategic logic [9,3] into our system to allow for the integration of Shapely values as discussed earlier. Finally, we will investigate the expressive power of YATL. Given that our newly introduced operators quantify over plans, histories and agents, we believe that yATL is more expressive than pATL, but will seek to determine how it relates to other logics such as PSL."}]}