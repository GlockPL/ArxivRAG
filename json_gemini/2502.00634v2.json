{"title": "SIMULPL: ALIGNING Human PREFERENCES IN SI-MULTANEOUS MACHINE TRANSLATION", "authors": ["Donglei Yu", "Yang Zhao", "Jie Zhu", "Yangyifan Xu", "Yu Zhou*", "Chengqing Zong"], "abstract": "Simultaneous Machine Translation (SiMT) generates translations while receiving streaming source inputs. This requires the SiMT model to learn a read/write policy, deciding when to translate and when to wait for more source input. Numerous linguistic studies indicate that audiences in SiMT scenarios have distinct preferences, such as accurate translations, simpler syntax, and no unnecessary latency. Aligning SiMT models with these human preferences is crucial to improve their performances. However, this issue still remains unexplored. Additionally, preference optimization for SiMT task is also challenging. Existing methods focus solely on optimizing the generated responses, ignoring human preferences related to latency and the optimization of read/write policy during the preference optimization phase. To address these challenges, we propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. In the SimulPL framework, we categorize SiMT human preferences into five aspects: translation quality preference, monotonicity preference, key point preference, simplicity preference, and latency preference. By leveraging the first four preferences, we construct human preference prompts to efficiently guide GPT-4/40 in generating preference data for the SiMT task. In the preference optimization phase, SimulPL integrates latency preference into the optimization objective and enables SiMT models to improve the read/write policy, thereby aligning with human preferences more effectively. Experimental results indicate that SimulPL exhibits better alignment with human preferences across all latency levels in Zh\u2192En, De\u2192En and En\u2192Zh SiMT tasks. Our data and code will be available at https://github.com/EurekaForNLP/SimulPL.", "sections": [{"title": "1 INTRODUCTION", "content": "Simultaneous Machine Translation (SiMT) (Grissom II et al., 2014; Gu et al., 2017; Ma et al., 2019) generates translations while receiving the streaming source inputs. Therefore, the SiMT model needs to learn not only the translation ability but also a read/write policy during training to decide whether to wait for the next incoming source token (READ) or to generate a new target token (WRITE) (Grissom II et al., 2014; Alinejad et al., 2021).\nThe real-time nature of SiMT scenarios leads to unique human preferences from audiences, which has been demonstrated by relevant linguistic studies (Kurz, 2001; Zwischenberger, 2010). On one hand, the audiences prefer translations that are accurate and easy to understand (Moser, 1996; Sridhar et al., 2013; Dayter, 2020); on the other hand, they also prefer translations to be delivered without unnecessary latency. Fulfilling these preferences is an important goal for interpreters (Amini et al., 2013; Kurz, 2001) and should also be considered in SiMT. However, how to make SiMT models"}, {"title": "2 RELATED WORK", "content": "Simultaneous Translation Various SiMT methods introduce different read/write policies. Some approaches propose rule-based fixed policies (Ma et al., 2019; Elbayad et al., 2020), while others focus on adaptive policies that adjust dynamically based on the context. These adaptive policies are modeled in various forms, such as multi-head monotonic attention Ma et al. (2020b), Transducer (Liu et al., 2021), information transport model (Zhang & Feng, 2022), Hidden Markov model (Zhang & Feng, 2023), and self-modifying process (Yu et al., 2024). More recently, some studies (Wang et al., 2023a; Agostinelli et al., 2024; Wang et al., 2024) have also demonstrated the promising performance of large language models in SiMT tasks. However, these efforts are predominantly validated on OMT datasets. Chen et al. (2020) constructed monotonic pseudo-references to reduce unnecessary reorderings. Wang et al. (2023b) generated monotonic references with two-stage beam"}, {"title": "3 PRELIMINARIES", "content": "Reward Modeling Existing preference alignment methods typically involve reward modeling and preference optimization. For reward modeling, a human-annotated preference dataset (x, yw, yl) is first constructed, where x represents the input, yw is preferred over yl, which is denoted as yw > yl. Subsequently, existing methods (Christiano et al., 2017; Kim et al., 2023) often train a reward model based on the Bradley-Terry model (Bradley & Terry, 1952), which is formulated as:\n$p(y^w > y^l | x) = \\frac{exp(r(x, y^w))}{exp(r(x, y^w)) + exp(r(x, y^l))} = \\sigma(exp(r(x, y^w)) - exp(r(x, y^l)))$ (1)\nwhere r(x, y) is the score estimated by the reward model, and \u03c3(\u00b7) is the logistic sigmoid function.\nPreference Optimization Reinforcement learning (RL) is widely used for preference optimization. Using signals from a reward model, the LLM can be optimized with the following objective:\n$\\underset{\\pi_\\theta}{max} E_{x \\sim D, y \\sim \\pi_\\theta(y|x)}[r(x, y)] - \\beta D_{KL}[\\pi_\\theta(y | X)||\\pi_{ref}(y | x)]$ (2)"}, {"title": "4 METHOD: SIMULPL", "content": "We propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. The overview of SimulPL is shown in Figure 1. In this framework, we construct human preference prompts based on our categorization of SiMT human preferences to guide GPT-4/4o in generating preference data. During the fine-tuning phase, SimulPL introduces Multi-task Supervised Fine-tuning (MSFT) to jointly learn translation ability and the read/write policy for initial preference alignment. During the preference optimization phase, SimulPL proposes Simultaneous Direct Preference Optimization (SimulDPO), which takes latency preference into account and further improves the read/write policy. The details are discussed in the following.\n4.1 CATEGORIZATION OF HUMAN PREFERENCE\nIn real-time SiMT scenarios, the audience exhibits unique human preferences (Kurz, 2001; Zwischenberger, 2010; Amini et al., 2013). Based on existing research in linguistics and computational linguistics, we categorize SiMT human preferences into five aspects:\n\u2022 Translation Quality Preference: Similar to OMT, faithful and fluent translations are also preferred in SiMT (Ma et al., 2019; Miao et al., 2021).\n\u2022 Monotonicity Preference: In the SiMT process, translating monotonically in accordance with the source word order allows for the delivery of translations with minimal pauses (Yang et al., 2023; Chen et al., 2020), which is favored by the audience (Mac\u00edas, 2006).\n\u2022 Key Point Preference: According to existing research (Moser, 1996; He et al., 2016), concise translations that highlight important information points are more appealing than those that provide complete information in the SiMT scenarios.\n\u2022 Simplicity Preference: In real-time SiMT scenarios, the audience prefers sentences with simpler syntactic structures, which are easier to follow (Sridhar et al., 2013; Dayter, 2020).\n\u2022 Latency Preference: In real-time settings, the audience prefers translations to be delivered without unnecessary latency (Rennert, 2010; Cho, 2016).\nIt is important to note that latency preference differs from the other four preferences, as it focuses not on the translation content but rather on reducing delays. Therefore, SimulPL aligns with the first four preferences by improving translation ability, and with the latency preference by enhancing the read/write policy.\n4.2 DATA CONSTRUCTION\nAnnotation of Human-preferred Translation In our categorization, the first four preferences are reflected in the translation content. Therefore, we utilize them as prior knowledge to construct hu-"}, {"title": "4.3 MULTI-TASK SUPERVISED FINE-TUNING", "content": "Based on a pre-trained language model $\u03c0_{pre}$, SimulPL introduces Multi-task Supervised Fine-tuning (MSFT) to jointly learn translation ability and read/write policy on $D_p$ for initial preference alignment. For translation ability, the model learns to generate the target prefix $y^w$ from the source prefix $x$. For read/write policy, SimulPL adds an extra confidence layer, consisting of a linear layer and a sigmoid layer, to make read/write decisions. Specifically, when predicting $y_t^w$, an additional confidence $c_t$ is estimated by the confidence layer. If $t < |y^w|$, the model should predict $c_t = 1$, indicating the WRITE decision. Otherwise, if $t > |y^w|$, the model should estimate $c_t = 0$, which means it should stop translating and choose the READ decision. The complete training loss for the MSFT phase is calculated as:\n$\\mathcal{L}_{MSFT} = - \\sum_{i=1}^y \\prod_{t=1}^{|y|+1} \\log \\pi_{sft}(y_t|x, y_{t-1}) - \\sum_{t=1}^{|y|+1}[I(t \\le |y^w|) \\log c_t + I(t > |y^w|) \\log (1 - c_t)]$ (5)\nwhere $\u03c0_{sft}$ is initialized with the parameters of $\u03c0_{pre}$, and $I(\u00b7)$ denotes the indicator function. It is noted that we train the model to predict $c_{|y^w|+1} = 0$, allowing the SiMT model to learn to stop translating at the appropriate position."}, {"title": "4.4 SIMULTANEOUS DIRECT PREFERENCE OPTIMIZATION", "content": "After the MSFT phase, SimulPL introduces Simultaneous Direct Preference Optimization (SimulDPO) to further align with human preferences. In the SimulDPO phase, SimulPL integrates the latency preference into the optimization objective and allows the SiMT model to further improve its read/write policy during preference optimization."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL DETAILS\nDatasets We validate our method on text-to-text SiMT tasks using our annotated datasets with human-preferred references. For Transformer-based SiMT models, we first pre-train them on the"}, {"title": "5.2 TRANSLATION QUALITY", "content": "The SacreBLEU and COMET scores for different SiMT methods are shown in Figure 3 and Figure 4. These results show that our proposed SimulPL achieves higher translation quality across all latency levels on three language pairs, particularly in low latency level. This indicates that SimulPL better meets the translation quality preference in SiMT scenarios. Since the test set we used includes"}, {"title": "5.3 PREFERENCE EVALUATION", "content": "To validate the effectiveness of SimulPL in preference alignment, we conduct a further human preference evaluation for LLM-PFX-SFT, LLM-PFX-SFT+DPO, and SimulPL. This evaluation includes the overall human evaluation and multi-aspect evaluation. Specifically, we first divide the models' outputs into three latency groups: low latency (0 < LAAL < 4), medium latency (4 < LAAL < 8), and high latency (LAAL \u2265 8). For human evaluation, we manually assess 100 sentences sampled from each latency group. Our evaluators are all qualified in simultaneous interpretation and can provide accurate assessments. For multi-aspect evaluation, we measure the performance of these SiMT models in terms of translation quality preference, monotonicity preference, key point preference, and simplicity preference in different latency groups. Detailed analyses are provided as follows.\nHuman Evaluation The results of the human evaluation are shown in Figure 5, which show that SimulPL achieves higher win rates in all latency groups for these three language pairs. This indicates that SimulPL can generate translations more aligned with human preferences. We attribute this performance improvement to the joint optimization of translation ability and read/write policy during the preference alignment process.\nMulti-aspect Evaluation The results of muti-aspect evaluation are shown in Table 2. SimulPL achieves better alignment across all latency groups for the three language pairs. SimulPL not only maintains high translation quality but also effectively manages monotonicity, key points, and simpler syntactic structures. Under low latency conditions, SimulPL achieves a better trade-off between latency preference and other preferences and generates better translations."}, {"title": "5.4 ABLATION STUDIES", "content": "We conduct ablation studies on SimulPL for the Zh\u2192En SiMT task, with detailed analyses in the following. Additional results and other analyses on De\u2192En SiMT task are shown in Appendix G.\nEffect of MSFT To verify the role of MSFT, we evaluate the performance of a SiMT model trained with regular SFT using prefix pairs data, similar to LLM-PFX-SFT. The results in Figure 6 show that MSFT outperforms SFT, especially in the low latency level. This shows that by explicitly modeling the multi-task of translation ability and read/write policy, MSFT improves the SiMT performance more effectively and provides better initialization parameters for SimulDPO.\nEffect of SimulDPO As shown in Figure 6, SimulPL, which introduces SimulDPO after MSFT phase, achieves higher SacreBLEU scores across various latency levels compared to Only MSFT. This indicates SimulPL further enhances the translation ability and read/write policy during the SimulDPO phase, leading to better performance.\nEffect of $\\pi_{ref}(y | X)$ To verify whether the predicted $\\pi_{ref}(y | X)$ in OMT setting can provide a more accurate constraint for the training objective, we replace $\\pi_{ref}(y | X)$ in SimulPL with the probability $\u03c0_{ref}(y | x)$ and evaluate the performance in this setting. As shown in Figure 6, the performance of SimulPL trained with $\u03c0_{ref}(y | x)$ obviously declines. We argue this is due to the inaccurate prediction of $\u03c0_{ref}(y | x)$ negatively impacts the preference optimization."}, {"title": "5.5 IMPACT OF \u03b1 ON BALANCING ALIGNMENT AND LATENCY DURING TRAINING", "content": "In SimulDPO, \u03b1 is introduced as a hyper-parameter into the training loss. As shown in Equation 9, \u03b1 functions as a token-level threshold. Since the gradient of $\u03b1_{ct}$ does not propagate, we only analyze the impact of $\u03b1_{ct}$. Specifically, during training, if $\\log \\frac{\\pi_{\\theta}(y^t | x, y_{ < t - 1})}{\\pi_{ref}(X, y_{ < t - 1})} > \u03b1$, we consider that $\\pi_{\\theta}(y^t | x, y_{ < t - 1})$ presents a prediction aligning human preferences well, and the SiMT model should learn to predict a higher $c_t$. Conversely, the model should learn a lower $c_t$ when this condition is not met. Thus, appropriately increasing the value of \u03b1 can enhance the model's ability to learn better alignment quality. However, if \u03b1 is set too high, the SiMT model could become overly cautious in translation, leading to $c_t$ failing to accurately balance between latency preference"}, {"title": "5.6 GENERALIZATION TO OTHER PREFERENCE OPTIMIZATION METHODS", "content": "In the SimulPL framework, SimulDPO is introduced for further preference optimization, which is adapted from DPO. Theoretically, SimulPL has generalization to any preference optimization methods, making them applicable to the SiMT scenarios. To validate this generality, we integrate CPO and KTO into the SimulPL framework, deriving SimulCPO and SimulKTO. Their respective training losses are provided in the following:\n$\\mathcal{L}_{SimulCPO} = - \\log \\sigma(\\sum_{t}^{l_{y^w}+1} u_t^w - \\sum_{t}^{l_{y^l}+1} u_t^l) - \\log \\pi_{\\theta} (y | x)$\n$u_t^* = \\beta c_t \\log \\frac{\\pi_{\\theta}(y_{t^*} | x, y_{<t-1})}{\\pi_{\\theta}(y_{t^*} | x, y_{<t-1})} - \u03b1 c_t,  s \\in \\{w,l\\}$\n$\\mathcal{L}_{SimulKTO} = A_y - v(x, y)$\n$v(x,y) = \\begin{cases} \\lambda_w \\sigma (\\sum_{t=1}^{l_y + 1} r - z_0), & if y \\sim y^w | x \\\\ \\lambda_l \\sigma (z_0 - \\sum_{t=1}^{l_y + 1} r), & if y \\sim y^l | x \\end{cases}$ (11)\nwhere $A_y$ denotes $\u03bb_\u03c9 (\u03bb_\u03b9)$ when y is desirable (undesirable), and $z_0 = DKL[\u03c0_\u03b8(y | x)||\u03c0_{ref}(y | X)]$.\nWe evaluate their performances on the Zh\u2192En SiMT task. As shown in Figure 8, both SimulCPO and SimulKTO achieve higher performance compared to CPO and KTO, particularly in low-latency levels. These results indicate the generalization of SimulPL. Additionally, SimulDPO, SimulCPO, and SimulKTO exhibit similar performance, making it difficult to determine which is most suitable for SiMT task. Besides preference optimization methods, SimulPL may also generalize to other tasks like simultaneous inference (Chen et al., 2024). We will explore this in future work."}, {"title": "6 CONCLUSION", "content": "We bridge the gap in the study of SiMT human preferences and propose SimulPL, a preference learning framework tailored for SiMT task. Drawing from existing research, we categorize preferences in SiMT scenarios into five aspects: translation quality, monotonicity, key points, simplicity, and latency. By leveraging the first four preferences, SimulPL constructs human preference prompts to efficiently guide LLMs in generating preference data for SiMT. During the fine-tuning phase, SimulPL introduces MSFT for initial preference alignment. During the preference optimization phase, SimulPL proposes SimulDPO, integrating latency preference into the optimization objective and further improving the read/write policy. Our experiments indicate that SimulPL achieves better preference alignment both overall and across each aspect. Additionally, our analysis shows that SimulPL has a generalization to other preference optimization methods."}, {"title": "A DATASET CONSTRUCTION AND ANALYSIS", "content": "A.1 HUMAN PREFERENCE PROMPTS\nBased on our categorized SiMT human preferences, we construct human preference prompts, which account for translation quality preference, monotonicity preference, key point preference, and simplicity preference, to efficiently guide GPT-4/40 in generating preference data for the SiMT task. Taking Zh\u2192En SiMT task as an example, our complete human preferences prompts are shown in Figure 9.\nA.2 FURTHER EVALUATION OF OUR ANNOTATED DATASETS.\nWe conduct both automated muti-aspect evaluation and additional human evaluation to validate the quality of our constructed dataset further. The details are described in the following.\nMulti-aspect Evaluation. Similar to Section 5.3, we also use our defined NIR, SLR, and DD to conduct multi-aspect evaluation on the GPT-generated references and the original references. We use Ref-free COMET to assess the translation quality here. The results in Table 3 indicate that GPT-4/40 aligns better with human preferences.\nThen, we compare the multi-aspect evaluation results of GPT-generated translations and those manually revised by interpreters on the test sets. To facilitate comparison, we also provide the results for the original references of the test sets. As shown in Table 4, the translations generated by GPT-4/40 are either superior to or comparable with the original references in terms of monotonicity, key points, simplicity, and translation quality. Moreover, these results are very close to the manually revised translations. This indicates that the quality of the GPT-generated data is both reliable and aligned well with human preferences."}, {"title": "B PROOFS AND DERIVATIONS", "content": "B.1 PROOF OF EQUIVALENCE BETWEEN OUTPUT LENGTH CONSTRAINT AND LATENCY OPTIMIZATION\nTo incorporate the goal of reducing latency into the optimization objective, we can directly include a latency evaluation metric. Specifically, we integrate Average Lagging (AL) (Ma et al., 2019), a"}, {"title": "C DISTINCTIONS BETWEEN SIMULDPO AND R-DPO", "content": "Firstly, the objectives and methods of R-DPO are entirely opposite to SimulDPO. In R-DPO, Park et al. (2024) aim to prevent models from generating too long responses and use a regularization term of \"-a|y|\" to achieve this. In contrast, for the SiMT task, audiences prefer translations with low latency, which requires the SiMT model to translate as much content as possible based on the already received source prefix. To achieve this, SimulDPO introduces \"+a|y|\" as an additional constraint. It is important to note that the goal of SimulDPO is not to optimize for the length itself, but rather to optimize for latency preferences.\nSecondly, as shown in Equation 9, we use $\u03b1_{ct} = \\sum_{t=1}^{|y|+1} c_t$ to make \"+a|y|\" differentiable, allowing gradient signals to be directly propagated to the parameters through backpropagation. In contrast, Park et al. (2024) treats the \"-a|y|\" as a margin without further processing."}, {"title": "D CONFIDENCE-BASED POLICY DURING INFERENCE", "content": "Algorithm 1 further illustrates the confidence-based policy adopted by SimulPL during inference. As shown in Algorithm 1, in the confidence-based policy, we set the confidence threshold to 0.5 as the basis for read/write decisions. To examine its impact, we compare the performance of SimulPL on the Zh\u2192En task with different threshold values (\u03b3). The results are presented in Table 6. When \u03b3 is set to a small value (y=0.1), the model is allowed to output tokens with low confidence, which results in a decline in translation quality, especially in low latency levels (0 < LAAL < 4). When \u03b3 is set to a higher value (y=0.9), the model imposes stricter constraints on token quality, leading to unnecessary delays. We plan to further explore the impact of \u03b3 in our future work."}, {"title": "E ADDITIONAL IMPLEMENTATION DETAILS", "content": "The hyper-parameters of the Transformer-based SiMT models used in our experiments are shown in Table 7. The hyper-parameters used by SimulPL during the MSFT and SimulDPO phases are listed in Table 8. In the training process, we share the LoRA in the MSFT and SimulDPO phases. We use the instruction-following format to guide the LLM in completing the SiMT task. Our used prompt"}, {"title": "F NUMERICAL RESULTS", "content": "Tables 9, 10, and 11 respectively present the numerical results of different SiMT models on the Zh\u2192En, De\u2192En, and En\u2192Zh SiMT tasks. In addition to LAAL, we also recorded other common latency metrics such as AL (Ma et al., 2019), DAL (Ma et al., 2020b), and AP (Cho, 2016)."}, {"title": "GADDITIONAL ANALYSES ON DE\u2192EN SIMT TASK", "content": "In this section, we conduct our analysis experiments on De\u2192En task. The results are respectively shown in Figure 11, 12, and 13. Through these experiments, we further validate our findings: Both SimulDPO and MSFT improve model performance, with a more pronounced effect at low latency levels; although the effect is less pronounced in the De\u2192En task compared to the Zh\u2192En task, SimulPL's performance is still influenced by \u03b1; SimulPL also generalizes well to other preference optimization methods on the De\u2192En task."}]}