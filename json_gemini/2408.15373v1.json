{"title": "Handling Geometric Domain Shifts in Semantic Segmentation of Surgical RGB and Hyperspectral Images", "authors": ["Silvia Seidlitz", "Jan Sellner", "Alexander Studier-Fischer", "Alessandro Motta", "Berkin \u00d6zdemir", "Beat P. M\u00fcller-Stich", "Felix Nickel", "Lena Maier-Hein"], "abstract": "Robust semantic segmentation of intraoperative image data holds promise for enabling automatic surgical scene understanding and autonomous robotic surgery. While model development and validation are primarily conducted on idealistic scenes, geometric domain shifts, such as occlusions of the situs, are common in real-world open surgeries. To close this gap, we (1) present the first analysis of state-of-the-art (SOA) semantic segmentation models when faced with geometric out-of-distribution (OOD) data, and (2) propose an augmentation technique called \u201cOrgan Transplantation\u201d, to enhance generalizability. Our comprehensive validation on six different OOD datasets, comprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs, each annotated with 19 classes, reveals a large performance drop in SOA organ segmentation models on geometric OOD data. This performance decline is observed not only in conventional RGB data (with a dice similarity coefficient (DSC) drop of 46%) but also in HSI data (with a DSC drop of 45%), despite the richer spectral information content. The performance decline increases with the spatial granularity of the input data. Our augmentation technique improves SOA model performance by up to 67% for RGB data and 90% for HSI data, achieving performance at the level of in-distribution performance on real OOD test data. Given the simplicity and effectiveness of our augmentation method, it is a valuable tool for addressing geometric domain shifts in surgical scene segmentation, regardless of the underlying model. Our code and pre-trained models are publicly available at https://github.com/IMSY-DKFZ/htc.", "sections": [{"title": "1. Introduction", "content": "To achieve context-aware assistance and autonomous robotic surgery, automated surgical scene segmentation is crucial. Recent studies have demonstrated that deep learning-based surgical scene segmentation can achieve high accuracy (Kadkhodamohammadi et al., 2019; Scheikl et al., 2020) up to the level of human performance when using hyperspectral imaging (HSI) instead of RGB data. Additionally, HSI provides the advantage of offering functional tissue information (Seidlitz et al., 2022). However, the important issue of geometric domain shifts, commonly encountered in real-world surgical scenes due to variations in procedures or situs occlusions (cf. Figure 1), has not been addressed in the literature. It remains uncertain whether the state-of-the-art (SOA) image-based segmentation networks presented in (Seidlitz et al., 2022) can generalize to out-of-distribution (OOD) geometries. It has recently been shown that algorithms for surgical instrument segmentation fail to generalize to unseen surgery types involving known instruments in unfamiliar contexts (Kitaguchi et al., 2022). Furthermore, image-based segmentation models demonstrated inferior generalization towards unseen surgeries than pixel-based segmentation (Seidlitz et al., 2022). However, to our knowledge, there has been no investigation or methodological contribution addressing geometric domain shifts in the context of surgical scene segmentation.\nDomain shifts are being intensively studied in the general deep learning community, with data augmentation emerging as a simple yet powerful technique to improve generalizability. Traditional augmentation approaches can be categorized into geometric (e.g., rotating), photometric (e.g., color jittering), noise (e.g., Random Erasing), kernel (e.g., Gaussian blur) and image mixing (e.g., CutMix) transformations (Shorten and Khoshgoftaar, 2019; Alomar et al., 2023). Geometric transformations are the most common augmentation method in deep learning-based semantic image segmentation (Kar et al., 2021). Surgical applications are no exception. Our analysis of the state of the art (35 publications on tissue or instrument segmentation) predominantly identified the use of geometric, photometric, and kernel transformations. Only a single publication (Ro\u00df et al., 2021) mentioned the use of elastic transformations and Random Erasing, where a rectangular area of the image is blacked out (Zhong et al., 2020). Likewise, augmentation schemes for tissue classification using HSI data have so far been limited to geometric augmentations. Topology-altering transformations such as Hide-and-Seek (divides the image into a grid of patches that are randomly blacked out) (Singh and Lee, 2017), Jigsaw (divides images into grids of patches that are randomly exchanged across images) (Chen et al., 2019), CutMix (copies a random patch from one image to another) (Yun et al., 2019) and CutPas (copies an object onto a random background image) (Dwibedi et al., 2017) (cf. Figure 1 for illustrations) have been proposed for image classification and object detection. However, to the best of our knowledge, their potential for surgical scene segmentation has not yet been explored.\nTo address these gaps in the literature, this paper investigates the following research questions:\nRQ1: What is the impact of geometric domain shifts on the performance of state-of-the-art RGB and HSI surgical scene segmentation models?\nRQ2: How does the spatial granularity of the input data affect the degree of performance degradation?\nRQ3: Can topology-altering augmentation techniques compensate for geometric domain shifts?\nAlthough portions of this work were previously published in (Sellner et al., 2023b), this paper introduces several new contributions:\n1. Additional experiments: We conducted new experiments to investigate performance degradation in the presence of geometric domain shifts as a function of the input spatial granularity (pixels, superpixels, patches and images).\n2. In-depth results: We added an analysis of common organ neighborhood relations from a set of 166 semantically annotated images. This new analysis contributes to the understanding of which organ classes are most affected by geometric domain shifts. Furthermore, we included exemplary segmentations of our models.\n3. Comprehensive description of our work: We substantially expanded our Materials and Methods (Section 2), Experiments (Section 3), Results (Section 4), and Discussion (Section 5) sections by including additional information, thereby improving the comprehensibility and reproducibility of our work."}, {"title": "2. Materials and Methods", "content": "The following sections describe the hardware and data used in this work (Section 2.1) and the deep learning-based semantic scene segmentation pipeline (Section 2.2). Code and pretrained models are available in our GitHub repository\u00b9 (Sellner and Seidlitz, 2023).\n2.1. Image Acquisition and Annotation\nThe data was collected at Heidelberg University Hospital following approval by the Committee on Animal Experimentation of the regional council of Baden-W\u00fcrttemberg in Karlsruhe, Germany (G-161/18 and G-262/19). The medical device-graded HSI camera system Tivita\u00ae Tissue (Diaspective Vision GmbH, Am Salzhaff, Germany) was used, capturing 100 spectral channels in the range between 500 nm and 1000 nm at a spectral resolution of 5 nm and spatial resolution of 640 \u00d7 480 pixels. RGB images were reconstructed from the HSI cubes by aggregating spectral channels in the blue, green, and red ranges (Holmer et al., 2018). The camera system captures an area of approximately 20\u00d730 cm. An integrated distance calibration system, comprising two light marks that overlap when the correct measurement distance of about 50 cm is maintained, ensured consistent imaging distances. To prevent spectral distortion from stray light, all light sources except the integrated halogen lighting unit were turned off during image acquisition, and the window blinds were closed. The acquisition of one HSI cube took approximately seven seconds (Holmer et al., 2018; Kulcke et al., 2018).\nA total of 600 intraoperative HSI cubes from 33 pigs were collected and semantically annotated with 19 classes. Two annotators performed the semantic annotations using vector annotation tools on the SuperAnnotate platform (SuperAnnotate, Sunnyvale, USA)2. To ensure correctness and consistency, a third medical expert subsequently reviewed all annotations. The annotations include two thoracic organs (heart, lung), eight abdominal organs (stomach, small bowel, colon, liver, gallbladder, pancreas, kidney, spleen), and one pelvic organ (bladder). For the kidney, images were taken before and after the removal of Gerota's fascia, and labeled \u201ckidney with Gerota's fascia\u201d and \u201ckidney", "background.": "subsections"}, {"title": "2.2. Deep Learning-Based Semantic Scene Segmentation", "content": "Our contribution is based on the hypothesis that application-specific data augmentation can effectively address geometric domain shifts. Instead of altering the network architecture of previously successful segmentation methods, we thus focused on combining SOA segmentation models with a topology-altering data augmentation.\nSurgery-inspired data augmentation: Similar to how a donor organ is received during transplantation, our Organ Transplantation augmentation technique copies all pixels of a specific object class (e.g., an organ or background) onto a different surgical scene. As shown in Figure 2, the corresponding object segmentation is copied and pasted accordingly. This approach places the organ in an unusual geometric context while preserving its shape and texture. The augmentation was inspired by the CutPas image-mixing augmentation initially proposed for object detection (Dwibedi et al., 2017). It has since been adapted for instance segmentation (Ghiasi et al., 2021) and for generating low-cost datasets through image synthesis from a few real-world images in surgical instrument segmentation (Wang et al., 2022).\nFurther data augmentation methods: Several other topology-altering augmentation methods (cf. Figure 2) could potentially improve generalizability under geometric domain shifts. Hide-and-Seek and Random Erasing are noise augmentations that obscure all pixels within rectangular regions of an image, thereby creating artificial situs occlusions. In contrast, the image-mixing techniques Jigsaw and CutMix transfer all pixels from rectangular regions of one image onto a different surgical scene. For our segmentation task, we modified these augmentations by also copying and pasting the corresponding segmentations. As a result, the adapted Jigsaw and CutMix augmentations not only occlude parts of the original scene but also place image segments in unusual contexts.\nThe SOA models and pipeline are detailed in (Seidlitz et al., 2022). Only a brief summary of model architectures, data pre-processing and training setup is given here.\nSOA model architectures: While the same model architectures were utilized for both HSI and RGB-based segmentation, different architectures were proposed for various spatial granularities. For pixel-based segmentation, each spectrum was processed individually using a one-dimensional convolutional classification network. Superpixels, which were precomputed from the RGB image and resized to a fixed shape, were input into an EfficientNet-B5 encoder (Tan and Le, 2019) that assigned a class label to each superpixel. For patch-based segmentation, two models were suggested: the patch_32 model, which uses 32 x 32 x c (width \u00d7 height \u00d7 number of spectral channels c) patches, and the patch_64 model, which uses 64 \u00d7 64 \u00d7 c patches. Both the patch_32 and patch_64 models use a U-Net (Ronneberger et al., 2015) with an EfficientNet-B5 encoder for segmentation. For all pixels, superpixels and patches composing an image, predictions were collected to obtain the image segmentation. For image-based segmentation, the entire image was processed by a model using a U-Net with an EfficientNet-B5 encoder, which directly provided the segmentation output. In all instances where the EfficientNet-B5 encoder was used, weights were pre-trained on the ImageNet data.\nHSI pre-processing: To eliminate sensor noise and convert the acquired HSI data from radiance to reflectance, the raw HSI cubes were automatically corrected using a pre-recorded white and dark reference cube, as described in Holmer et al. (2018). Each pixel spectrum was $l^1$-normalized to account for multiplicative illumination changes.\nTraining setup: To avoid biases from differences in the training setup and enable a fair comparison of modalities, spatial granularities and augmentation methods, the training setup was made as comparable as possible across all models. The same model hyperparameter settings as described in the SOA were used (Seidlitz et al., 2022): Before extracting pixels, superpixels, or patches, images were augmented using the geometric transformations shift (limit of 0.0625), scale (limit of 0.1), and rotate (limit of 45\u00b0), each applied with a probability of 0.5. The Adam optimization algorithm (Kingma and Ba, 2017) was used in conjunction with an exponential learning rate schedule (initial learning rate: 0.001, decay rate $\\gamma$: 0.99, Adam decay rates $\\beta_1$: 0.9 and $\\beta_2$: 0.999). The Dice loss and cross-entropy loss were weighted equally to calculate the loss function. Training was conducted for 100 epochs with an epoch size of 500 images, and stochastic weight averaging (Izmailov et al., 2018) was applied during the final 10 epochs. Dropout regularization was applied with a probability of 0.1."}, {"title": "3. Experiments", "content": "To examine the performance of SOA surgical scene segmentation models under geometric domain shifts as a function of input modality (RQ1) and spatial granularity (RQ2), and assess the generalizability improvements provided by topology-altering augmentations (RQ3), we addressed the following OOD scenarios:\n1. Organs in isolation: Abdominal linens are routinely used to control excessive bleeding, absorb blood and secretions and protect soft tissue and organs. Certain surgeries (e.g., enteroenterostomy) necessitate covering all organs except one, creating the need to robustly identify an organ without any information about neighboring organs.\n2. Organ resections: In surgical resections, organ parts or even the entire organ are removed, necessitating the identification of surrounding organs despite the absence of a common neighboring organ.\n3. Situs occlusions: Large portions of the surgical site can be obscured by the procedure itself, introducing OOD neighbors such as gloved hands, which challenge the correct segmentation of the visible parts of the site.\nReal-world datasets: We acquired a total of 600 intraoperative HSI cubes from 33 pigs. Out of these, 94 images from 25 pigs compose the dataset isolation_real. In these images, all organs except a specific one were covered by abdominal linen, providing isolation images for all 18 organ classes. To examine the impact of occlusions, we collected 142 images of 20 pigs with real-world situs occlusions by gloved hands (dataset occlusion) and 364 images without such occlusions (dataset no-occlusion). The dataset original is the combination of the datasets occlusion and no-occlusion. Example images for all datasets are illustrated in Figure 2.\nManipulated datasets: We enhanced our real-world datasets with four manipulated datasets. For a simulated organs in isolation scenario, we iterated over all images $I$ and labels $l$ in the dataset original and replaced all pixels in $I$ except those corresponding to class $l$ with either zeros to create the dataset isolation_zero, or with spectra from a background image to form the dataset isolation_bgr. Similarly, we generated the resection datasets removal_zero and removal_bgr by replacing all pixels in $I$ corresponding to class $l$ with zeros and background spectra, respectively. Example images for our manipulated datasets are illustrated in Figure 2.\nTrain-test-split: The dataset original was previously used for developing the SOA models in (Seidlitz et al., 2022). The same split as described in the publication was employed, which includes a hold-out test split of 166 images from 5 pigs and a training split of 340 images from 15 pigs. To ensure a fair comparison across models and OOD scenarios, the same train-test-split at the pig level was consistently applied: The test splits of isolation_zero, isolation_bgr, removal_zero, and removal_bgr were created by manipulating the images in the test split of original. In the occlusion scenario, models were trained on the subset of images in the train split of original that do not contain occlusions, and testing was performed on the subset of the test split of original without occlusions (no-occlusion) and with occlusions (occlusion). As described in Section 2.2, most model hyperparameters were set according to the SOA. Only hyperparameters related to the topology-altering augmentations, specifically the probability p of applying the augmentation, were optimized using a grid search with p \u2208 {0.2, 0.4, 0.6, 0.8, 1}. The optimal p-value was determined using five-fold-cross-validation on the train splits of the datasets original, isolation_zero, and isolation_bgr.\nValidation strategy: To overcome limitations of individual metrics (Reinke et al., 2024), we assessed the performance for each class label $l$ using both the overlap-based metric dice similarity coefficient (DSC) (Dice, 1945) and the boundary-based metric normalized surface dice (NSD) (Nikolov et al., 2021), as recommended by the Metrics Reloaded framework (Maier-Hein et al., 2023). For the NSD, we used the class-specific thresholds reported in the SOA (Seidlitz et al., 2022). Metric aggregation adhered to the hierarchical structure of the data by first macro-averaging the class-level metric value $M_l$ ($M$ \u2208 {DSC, NSD}) across all images of a single pig, and then averaging across pigs. In the organ removal scenario, for each class label $l$ in an image $I$, a set of metric scores {$M_l(I')$} was obtained for the one-by-one removal of all other classes $I'$ in $I$. To evaluate the effect of removing the most important neighbor of l, we selected the smallest score in {$M_l(I')$} prior to proceeding with the hierarchical aggregation.\nPerformance ranking: To compare the performance of our Organ Transplantation augmentation with other topology-altering augmentations, we computed performance rankings and assessed ranking stability following the guidelines in (Wiesenfarth et al., 2021): We calculated the rank for each augmentation method across 1000 bootstrap samples. For each class label $l$ in each bootstrap sample, we randomly selected $N_l$ subject-level scores without replacement, where $N_l$ is the total number of subjects with images available for class 1. Hierarchical aggregation of the metric values was performed for each bootstrap sample, resulting in a set of 1000 class-averaged scores that capture the variability across subjects."}, {"title": "4. Results", "content": "A primary purpose of our study was to examine how geometric domain shifts affect the performance of SOA segmentation models as a function of (1) the modalities HSI and RGB, and (2) various input spatial granularities such as pixels, superpixels, patches and images. The results of this analysis are presented in Section 4.1. Section 4.2 presents the performance analysis of our Organ Transplantation model, and a comparison to topology-altering augmentations adapted from the general computer vision community is given in Section 4.3.", "subsections": []}, {"title": "4.1. Effects of Geometric Domain Shifts on State-Of-The-Art Surgical Scene Segmentation", "content": "Figure 3 provides an overview of the segmentation performance, measured by the DSC, for all investigated models on the two in-distribution datasets (original and no-occlusion) and six OOD datasets (isolation_zero, isolation_bgr, isolation_real, removal_zero,removal_bgr, and occlusion).\nPerformance drop as a function of modality: The average performance drop on OOD data is smaller for HSI (23%) compared to RGB (30%).\nPerformance drop as a function of spatial granularity: For both modalities, the in-distribution performance is highest for image-based segmentation (RBG: DSC of 0.83 (standard deviation (SD) 0.10); HSI: DSC of 0.86 (SD 0.10)) and decreases with reduced input spatial granularity. While pixel-based segmentation models show the lowest overall performance, they do not experience a decline in performance on OOD data for the isolation and removal scenarios. As the input spatial granularity increases, the decrease in segmentation performance when encountering organs in isolation and removals becomes more pronounced across both modalities. For image-based segmentation, the performance drop is highest, ranging from 10%-46% for RGB and 5%-45% for HSI, depending on the OOD scenario.\nFigure 4 offers a detailed analysis of which organ classes are most impacted in the organ removal scenario for the image#HSI model. The performance drop is highest for the gallbladder when the liver is removed, and second highest for the major vein when the peritoneum is removed. In both cases, the removed organ and the organ under investigation are frequent neighbors, with the liver comprising 83.9% of the gallbladder's neighborhood, and the peritoneum accounting for 60.1% of the major vein's neighborhood, on average."}, {"title": "4.2. Performance of Our Organ Transplantation Augmentation", "content": "Figure 5 and Figure A.1 demonstrate that our Organ Transplantation augmentation effectively addresses performance drops of image-based surgical scene segmentation models under geometric domain shifts for both the RGB and HSI modalities. The performance improvement over the baseline ranges from 9%-67% (DSC) and 15%-79% (NSD) for RGB, and from 9%-90% (DSC) and 16%-96% (NSD) for HSI. Even on in-distribution data, slight performance enhancements can be observed. The largest performance boost on OOD data is obtained in the isolation scenario.\nFigure 6 presents example predictions from the image#HSI model with and without the Organ Transplantation augmentation for each OOD dataset. These images were selected based on the maximum difference in DSC performance between the baseline model and the Organ Transplantation augmentation model, highlighting cases where the benefit from the augmentation is most significant. In all six examples, the performance drop in the baseline model is primarily due to mispredicting entire organ classes, such as mispredicting the gallbladder and stomach after liver removal, or failing to recognize the stomach and large parts of the omentum obstructed by a gloved hand in the occlusion scenario."}, {"title": "4.3. Comparison to Other Topology-Altering Augmentations", "content": "Figure 7 displays the DSC-based ranking of our Organ Transplantation augmentation compared to six other topology-altering augmentations on our geometric OOD test datasets. The NSD-based ranking is presented in Figure A.2. Our Organ Transplantation augmentation consistently ranks first, whereas the baseline augmentations generally rank last in most OOD scenarios. Although the other ranks vary across different geometric OOD datasets, in the overall ranking, the image-mixing augmentations CutMix and Jigsaw outperform the noise augmentations Random Erasing and Hide-and-Seek."}, {"title": "5. Discussion", "content": "In this study, we are the first to demonstrate that SOA surgical scene segmentation networks fail under geometric domain shifts. Through comprehensive validation on six geometric OOD datasets, comprising 600 RGB and HSI cubes from 33 pigs, each annotated with 19 classes, we found that performance degradation was generally higher for RGB compared to HSI. Additionally, the decline was more pronounced with increased input spatial granularity, such as images and patches, compared to smaller granularity like pixels and superpixels. To enhance the generalization of SOA models towards OOD geometries, we adapted previously unexplored topology-altering data augmentation methods to surgical scene segmentation. Our proposed Organ Transplantation augmentation outperformed all other topology-altering augmentations and achieved performance on par with in-distribution performance.\nThe following sections offer a detailed discussion of our experimental results (Section 5.1), limitations and future work (Section 5.2), and our conclusions (Section 5.3)."}, {"title": "5.1. Interpretation of Results", "content": "Performance degradation across spatial granularities: We observed that performance degradation was more pronounced, the higher the input spatial granularity. However, some performance scores reported in Figure 3 should be interpreted with care, as there are artifacts due to limitations in our experiments: In the manipulated isolation scenarios, superpixel-based segmentation seemed to have improved over in-distribution data. This is due to our manipulation strategy. As demonstrated in (Seidlitz et al., 2022), superpixel boundaries on real data do not perfectly align with annotation boundaries, especially between different organ classes. However, on the manipulated data, we used the reference boundary annotation of the target organ to replace non-target pixels with zeros or background spectra. This resulted in superpixel boundaries that closely follow the annotations, leading to higher segmentation scores. Pixel-based segmentation also showed performance boosts on simulated and real isolation data. As demonstrated in (Seidlitz et al., 2022), pixel-based segmentation has incomplete and scattered boundaries between organs, whereas it accurately discriminates between tissue and background. In the simulated isolation scenarios, pixels outside the target organ annotation were replaced with zeros and background pixels, and in the isolation_real dataset, the entire scene except for the target organ was covered by abdominal linen. This prevented the mispredictions of the target class outside the annotated area that typically occur in multi-organ images.\nWhich is the optimal spatial granularity? The optimal input spatial granularity was previously examined in (Seidlitz et al., 2022) using in-distribution data from the original dataset, revealing that performance improves with larger spatial granularity. However, we demonstrated that SOA models with larger input spatial granularity were less robust to geometric domain shifts. Despite this, we believe image-based segmentation models are the best choice due to their training efficiency (e.g., augmentations can be computed on the GPU) (Sellner et al., 2023a), their superior segmentation results that could be restored through our Organ Transplantation augmentation, and their ability to apply batch-based augmentations that benefit from larger spatial context (cf. Section 5.2).\nHSI vs RGB: Previous work (Seidlitz et al., 2022) showed that HSI- outperforms RGB-based segmentation with in-distribution data. We found that HSI also showed a smaller overall performance drop on geometric OOD data, underscoring the value of additional spectral information for handling geometric domain shifts.\nGeometric OOD performance is class-dependent: Geometric OOD scenarios impacted different class labels unevenly, as demonstrated by the significant variability in class-level scores for OOD scenarios in Figure 5. For instance, in the removal scenario, performance dropped most notably for gallbladder and major vein when their frequent neighbor was removed, whereas liver, muscle, and background showed no performance decline with the removal of other classes.\nTopology-altering augmentations: Our experiments were based on the hypothesis that topology-altering augmentations can introduce simulated geometric domain shifts during model training, thus enabling the models to generalize better on geometric OOD data. This hypothesis was confirmed, as in most scenarios, topology-altering augmentations outperformed the baseline augmentations. Overall, the image-mixing augmentations Organ Transplantation, CutMix, and Jigsaw outperformed the noise augmentations Random Erasing and Hide-and-Seek. This could be because image-mixing augmentations create unusual neighborhood relations by copying parts of one surgical scene onto another, whereas noise augmentations only obscure parts of the scene without altering neighborhood relations. Additionally, noise augmentations that black out parts of the scene may train models to handle this specific kind of obscuration, but not to deal with image parts being obscured by a known class. This might explain why Random Erasing ranked best on the isolation_zero and removal_zero datasets, where parts of the image are blacked out. We further observed that our Organ Transplantation augmentation performed best, and that CutMix and Random Erasing, which alter rectangular patches, outperformed their grid-based counterparts. This may be due to the level of unnatural boundaries introduced: Our Organ Transplantation augmentation preserves organ boundaries in unusual contexts, while CutMix and Random Erasing create unnatural boundaries at the edges of rectangles, and Jigsaw and Hide-and-Seek generate even more unnatural boundaries due to their grid-based modifications.\nValue of manipulated data: We developed and optimized our augmentation models solely on the validation splits of the in-distribution dataset original and the manipulated OOD datasets isolation_zero and isolation_bgr, keeping all real-world OOD datasets as untouched test sets. Despite this, our proposed augmentation was effective on all datasets, demonstrating that image manipulations are a powerful tool for assessing geometric OOD performance. This is particularly valuable in scenarios with limited real-world data, such as organ resection, which would require an impractical number of animals."}, {"title": "5.2. Limitations and Future Work", "content": "A general limitation of image-mixing augmentations like our Organ Transplantation method is that they require a minimum batch size of two images to transplant an organ from one scene to another. For image-based segmentation, this is not an issue, as batch sizes are typically larger (e.g., five images in our case). However, for other spatial granularities, implementing image-mixing augmentations would result in significant computational overhead and increased memory requirements: Since we believe that the preservation of organ boundaries is a key strength of our Organ Transplantation augmentation, it should be applied before extracting pixels, superpixels, or patches. As the latter must be done on the CPU due to memory and efficiency constraints, our augmentation would have to be run on the CPU as well. However, it has been shown that for efficient training, HSI models should perform augmentations on the GPU instead (Sellner et al., 2023a). Additionally, the need to load at least two images simultaneously would substantially increase memory and load demands for lower spatial granularities, making batch-level augmentations impractical. For this reason, we only validated the topology-altering augmentations on image-based segmentation models. Given that image-based segmentation generally outperformed smaller spatial granularities and our method achieved geometric OOD performance comparable to in-distribution performance, using smaller spatial granularities would not provide additional benefits.\nIn this paper, we studied geometric OOD scenarios common in real-world open surgeries. Similar scenarios, such as instrument occlusions and organ removals, occur in minimally invasive surgeries. However, there are significant differences, including more close-up views of organs, fewer neighboring organs visible in an image, tissue deformations, and substantial changes in imaging perspectives. With the recent availability of medical device-graded HSI systems for minimally invasive surgery and the steady increase in the number of such procedures over the past decades (John et al., 2020), investigating and compensating for geometric domain shifts in this context is a promising future direction."}, {"title": "5.3. Conclusion", "content": "To the best of our knowledge, we are the first to address surgical scene segmentation under geometric domain shifts. We demonstrated significant performance drops in SOA segmentation models when faced with geometric OOD scenarios, but also showed that in-distribution performance could be restored using our Organ Transplantation augmentation. Our augmentation method stands out for being computationally efficient, effective, and model-independent, making it suitable for image-based surgical scene segmentation of both HSI and RGB data across various architectures. Our code repository and pretrained models are publicly available in GitHub\u00b3 (Sellner and Seidlitz, 2023), and we have also integrated our Organ Transplantation augmentation into the Kornia library (Riba et al., 2020) for easy access by the broader computer vision community."}, {"title": "Acronyms", "content": "DSC dice similarity coefficient\nHSI hyperspectral imaging\nNSD normalized surface dice\nOOD out-of-distribution\nSD standard deviation\nSOA state-of-the-art"}, {"title": "Declaration of Competing Interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. The Authors declare that there is no conflict of interest."}, {"title": "Declaration of generative AI and AI-assisted technologies in the writing process", "content": "During the preparation of this work, the authors used ChatGPT in order to improve language. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article."}]}