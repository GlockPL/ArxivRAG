{"title": "CorMulT: A Semi-supervised Modality Correlation-aware Multimodal Transformer for Sentiment Analysis", "authors": ["YANGMIN LI", "RUIQI ZHU", "WENGEN LI"], "abstract": "Multimodal sentiment analysis is an active research area that combines multiple data modalities, e.g., text, image and audio, to analyze human emotions and benefits a variety of applications. Existing multimodal sentiment analysis methods can be classified as modality interaction-based methods, modality transformation-based methods and modality similarity-based methods. However, most of these methods highly rely on the strong correlations between modalities, and cannot fully uncover and utilize the correlations between modalities to enhance sentiment analysis. Therefore, these methods usually achieve bad performance for identifying the sentiment of multimodal data with weak correlations. To address this issue, we proposed a two-stage semi-supervised model termed Correlation-aware Multimodal Transformer (CorMulT) which consists pre-training stage and prediction stage. At the pre-training stage, a modality correlation contrastive learning module is designed to efficiently learn modality correlation coefficients between different modalities. At the prediction stage, the learned correlation coefficients are fused with modality representations to make the sentiment prediction. According to the experiments on the popular multimodal dataset CMU-MOSEI, CorMulT obviously surpasses state-of-the-art multimodal sentiment analysis methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal sentiment analysis combines multiple perceptual modalities such as text, image, audio and video to analyze and understand human emotions. Currently, multimodal sentiment analysis has been widely applied in various domains, including predicting movie box office performance [2], predicting stock market performance [1], and predicting political election results [25].\nThe general paradigm of multimodal sentiment analysis is to extract emotional information from multiple modalities and fuse them together to achieve a more comprehensive and accurate understanding of emotions. Existing methods for multimodal sentiment analysis can be roughly clas-sified into three categories, i.e., modality interaction-based methods, modality transformation-based methods, and modality similarity-based methods. Modality interaction-based methods emphasize on learning the interactions and dependencies between different modalities to combine them together. The representative methods are MultiModal InfoMax [5] and MMLatch [20]. Modality transformation-based methods select one modality as the primary modality, and extract information from other modalities to obtain cross-modal representation. The representative methods include BAFN [23] and RAVEN [28]. Modality similarity-based methods compare the features between different modalities for deep understanding of sentiment, and the representative methods include Misa [6] and MUET [15].\nHowever, most existing methods for multimoal sentiment analysis heavily rely on the assumption of strong correlations between modalities, and have not yet fully explored and examined the correlations between modalites. In this case, they may achieve bad performance for identifying the sentiment for multimodal data with weak correlations. In general, strong modality correlations are characterized by a well-aligned, consistent, and strongly related representation of objects across visual, textual, and audio aspects, with minimal noise interference such as irrelevant visual elements and background sounds. Conversely, weak modality correlations are often plagued with different discrepancies as illustrated in Fig. 1. Voice discrepancy arises when non-primary sounds overshadow the main audio subjects. Content discrepancy occurs when there is an inconsistency between the objects described in text and those depicted in images. Alignment discrepancy is the inability to achieve a complete synchronization across modalities. Clarity discrepancy refers to the presence of excessive noises that muddle the primary signals. In practice, it is difficult for existing multimodal sentiment analysis methods to accurately decide the sentiment of data samples with such discrepancies.\nTo address the above issue, this work proposed a two-stage semi-supervised model termed Correlation-aware Multimodal Transformer (CorMulT). CorMulT develops a novel modality corre-lation constractive learning module to learn modality correlation coefficients between different modalities at the pre-training stage. Then, the learned correlation coefficients are fused with the modality representations to produce the sentiment classification results.\nIn sum, the contributions of this work are threefold as highlighted below.\n\u2022 We identified the issue of weak modality correlations in existing multimodal sentiment analysis methods, and proposed the CorMulT model to address this issue by accurately learning the correlations between modalities and fusing the learned correlations with modality representation to enhance sentiment analysis."}, {"title": "2 RELATED WORK", "content": "In this section, we provide more details about the three categories of multimodal sentiment analysis methods, i.e., modality interaction-based methods, modality transformation-based methods, and modality similarity-based methods."}, {"title": "2.1 Modality Interaction-based Methods", "content": "These methods focus on capturing the dynamic interactions and dependencies between different modalities during the process of combining information from multiple modalities. Wei et al. [5] proposed MultiModal InfoMax (MMIM) to maximize the mutual information between unimodal input pairs, and that between the fusion results and low-level unimodal input representations to preserve task-relevant information during multimodal fusion. Paraskevopoulos et al. [20] utilized a forward feedback mechanism to capture the top-down cross-modal interactions and achieved multi-modal fusion during network training by masking sensory inputs with higher-level representations extracted from each modality. Kumar et al. [13] utilized self-attention to capture the long-term context and employed a gating mechanism to selectively learn weak contributing features. Sun et al. [22] proposed a general debiasing framework based on Inverse Probability Weighting (IPW), which adaptively assigns small weights to the samples with larger bias to avoid fitting the spurious correlations between multimodal features and sentiment labels. Xue et al. [29] proposed a multi-level attention map network (MAMN) to filter noise before multimodal fusion and capture the consistent and heterogeneous correlations among multi-granularity features. Quan et al. [21] proposed a multimodal comparative learning interaction module to better focus on the characteristics of different modalities. He et al. [9] proposed a multimodal mutual attention-based sentiment analysis (MMSA) framework that uses three levels of subtasks to preserve the unimodal unique semantics and enhance the common semantics of multimodal data. Wang et al. [26] introduced supervised contrastive learning to learn effective and comprehensive multimodal data representation. Liu et al. [16] proposed the Scanning, Attention and Reasoning (SAR) model for multimodal sentiment analysis, where a perceptual scanning model is designed to perceive the image and text content, as well as the intrinsic correlations between them. Wang et al. [27] proposed a model to assist in learning modality representations with multitask learning and contrastive learning. Tsai et al. [24] repeatedly reinforce the features of the target modality using low-level features of the source modality, thus ensuring cross-modal interactions and addressing the alignment issue between different modalities. Yu et al. [31] proposed a cyclic memory enhancement network for capturing the long-term dependencies in multimodal data. Ping et al. [10] proposed a novel temporal position prediction task for speech-text alignment to deal with the cross-modal alignment issue. Cheng et al. [3] and Ye et al. [11] proposed Attentional Temporal Convolutional Network (ATCN) and Multimodal Temporal Attention (MMTA) to extract unimodal temporal features for adaptive inter-modal balance. Lin et al. [14] and He et al. [8] narrowed the gap among different modalities by transferring the sentiment-related knowledge."}, {"title": "2.2 Modality Transformation-based Methods", "content": "In this category of methods, one modality serves as the primary modality while the others serve as supplementary modalities. For example, Khan et al. [12] developed a two-stream model that converts image modality to textual representations as auxiliary text, thereby providing more information for the text modality. Wang et al. [28] obtained the text drift vectors using non-textual data, e.g., audio and image modalities, thus achieving drifted word embedding. Majumder et al. [19] mapped the personality traits from textual documents to audio and visual modalities to improve the effectiveness of sentiment analysis. Tang et al. [23] introduced a more discriminating text modality to guide the dynamic learning of emotional context within modalities, thus reducing the redundant information in auxiliary modalities during the modality transformation process. He et al. [7] fused nonverbal embedding with language embedding before inputting them into Bert to obtain multimodal representations. However, according to Gan et al. [4], pure modality transformation methods often underperform the methods that consider the interaction between modalities, which could be because modality transformation methods overlook the inherent correlations between modalities and result in the loss of information."}, {"title": "2.3 Modality Similarity-based Methods", "content": "These methods compare the features across different modalities to facilitate the analysis of their sentiment. For instance, Yu et al. [32] jointly trained multimodal and unimodal tasks to learn the consistency and differences in modality representations. Hazarika et al. [6] projected each modal-ity into two subspaces, i.e., modality-invariant subspace and modality-specific subspace, where modality-invariant subspace facilitates cross-modal representation learning to reduce modality discrepancies, and modality-specific subspace captures the distinctive features of each modality. Mai et al. [18] proposed curriculum learning for weakly supervised modality correlations, which leverage limited or noisy labeled data to align and correlate features across different modalities, to learn a more discriminative embedding space for multimodal data. However, they often struggle with the ambiguity and sparsity of weak labels, which can lead to suboptimal learning of true modality correlations. In summary, although these methods are capable of learning certain cor-relations between modalities, they often fall short in further refining and integrating the learned correlations for sentiment analysis, and are thus unable to achieve outstanding performance."}, {"title": "2.4 Discussion", "content": "As described above, most existing multimodal sentiment analysis models rely on the assumption of strong correlations between modalities. However, a large amount of multimodal data may not exhibit strong correlations. As a result, the correlations between modalities have not been fully explored and utilized in most models. Besides, it is labor-intensive and impractical to manually filter all the data samples with low modality correlations, and existing methods like cross-modal alignment also cannot solve the problem. Therefore, this work proposed the two-stage model CorMulT. At the pre-training stage, CorMulT learns the modality correlation coefficients via a modality correlation contrastive learning module. At the prediction stage, the learned modality correlation coefficients are fused with the modality representations to enhance the sentiment prediction."}, {"title": "3 METHODOLOGY", "content": "Fig. 2 illustrates the architecture of Modality Correlation-aware Multimodal Transformer (Cor-MulT) which consists of modality correlation contrastive learning module and correlation-aware multimodal transformer module. Different modalities are first fed into the modality correlation con-trastive learning module to obtain modality correlation coefficients which are then fused with the modality representations in the correlation-aware multimodal transformer module for predicting the sentiment."}, {"title": "3.1 Overview", "content": "Fig. 2 illustrates the architecture of Modality Correlation-aware Multimodal Transformer (Cor-MulT) which consists of modality correlation contrastive learning module and correlation-aware multimodal transformer module. Different modalities are first fed into the modality correlation con-trastive learning module to obtain modality correlation coefficients which are then fused with the modality representations in the correlation-aware multimodal transformer module for predicting the sentiment."}, {"title": "3.2 Modality Correlation Contrastive Learning", "content": "Modality correlation contrastive learning module utilizes the contrastive learning framework to learn the correlation coefficients between different modalities. This module consists of two sub-modules, i.e., modality-specific feature extraction and modality correlation evaluation. The modality-specific feature extraction module is responsible for extracting unimodal features from the three modalities, i.e., audio, image and text. The modality correlation evaluation module analyzes the correlations between different modalities."}, {"title": "3.2.1 Modality-Specific Feature Extraction", "content": "To encode different modalities into unimodal represen-tations, different feature extraction methods are employed according to the characteristics of the modalities.\nFor audio encoding, we employ the Mel spectrogram [30], a widely used visual representation in audio processing that transforms the audio signals into an image-like format. Initially, we capture the spectral characteristics of the audio signals using the Short-Time Fourier Transform (STFT) which decomposes the audio signals into frequency components over short time intervals and produces a time-frequency representation, i.e.,"}, {"title": null, "content": "Xstft(n,k) = \\sum_{m=-\\infty}^{\\infty} x(m)w(m \u2013 n)e^{-j2\\pi km/N} \\qquad(1)"}, {"title": null, "content": "where $X_{stft}(n, k)$ represents the coefficient value of the STFT matrix for the $k^{th}$ frequency at the $n^{th}$ frame, $x(m)$ denotes the value of input audio signal at time $m$, $w(m \u2013 n)$ denotes the value of window function at time $n$, $k$ denotes the sampling point on the frequency axis, and $N$ represents the number of points in the Fast Fourier Transform (FFT).\nNext, we convert the STFT representations into the Mel spectrogram which offers a more perceptually relevant representation of the audio. This conversion involves a set of Mel filters that mimic the human auditory system's response to different frequencies. These filters capture the energy within specific frequency ranges, i.e.,"}, {"title": null, "content": "H_m(k) =\\begin{cases}0 & \\text{if } f_k < f_{m-1} \\\\\n      \\frac{f_k-f_{m-1}}{f_m-f_{m-1}} & \\text{if } f_{m-1} \\leq f_k < f_m \\\\\n      \\frac{f_{m+1}-f_k}{f_{m+1}-f_m} & \\text{if } f_m \\leq f_k < f_{m+1} \\\\\n      0 & \\text{otherwise}\\end{cases}\\qquad(2)"}, {"title": null, "content": "where $f_k$ represents the frequency value of the $k^{th}$ sampling point on the frequency axis, $f_m$ represents the center frequency of the $m^{th}$ Mel frequency band, and $H_m(k)$ denotes the output of the Mel filter bank for the $k^{th}$ frequency. The Mel filter bank $H_m(k)$ applies a set of triangular filters to map the STFT coefficients to the Mel scale. The dimension of $H_m(k)$ is $M \\times K$, where $M$ denotes the number of Mel frequency bands, and $K$ denotes the number of STFT coefficients.\nTo further enhance the audio signal representation, we calculate the logarithm of the Mel spectrogram. This process transforms the power spectrum obtained from the STFT into a logarithmic scale, which more closely mirrors human auditory perception. The logarithmic Mel spectrogram is represented as follows:"}, {"title": null, "content": "X_{mel} (n, m) = \\log \\left( \\sum_{k=0}^{N-1} |X_{stft}(n,k)|^2H_m (k) \\right) \\qquad(3)"}, {"title": null, "content": "where $X_{mel}(n, m)$ denotes the log-transformed Mel spectrogram value for the $m^{th}$ Mel frequency band at the $n^{th}$ time frame. The Mel spectrogram matrix is defined in the context of $\\mathcal{F}_a \\subseteq \\mathbb{R}^{b \\times t \\times m}$, where $b$ represents the batch size, $t$ represents the number of time frames, and $m$ represents the number of Mel frequency bins. This logarithmic scaling of the Mel spectrogram not only enhances the representation of the audio signals by emphasizing the perceptually important aspects of the sound, but also stabilizes the numerical range, thus facilitating more effective model training.\nBy employing the Mel spectrogram and its logarithm, we obtain a compact and informative visual representation of the audio signal, enabling our model to well learn the patterns in the frequency content of the audio."}, {"title": "3.2.2 Sample Generation for Contrastive Learning.", "content": "Most existing methods for multimodal sentiment analysis heavily rely on the manually annotated datasets. However, few datasets have explicit annotation for modality correlations. Therefore, we introduce contrastive learning to unify the unimodal data representations. To this end, the critical task is to generate effective positive and negative data samples.\nSince most video data in our daily life exhibits inter-modality correlations, we thus employ multi-ple enhancement techniques, including temporal shift, cross-sample mixing, and data perturbation, to generate negative samples. Temporal shift slightly shift the time alignment of different modalities within a sample to create negative samples. For example, for the audio and image modalities of a video, they can be shifted slightly out of sync to create negative samples. Cross-sample mixing mix modalities across different samples to create negative samples. For example, the audio from one video can be mixed with the visual component of another video to create negative samples. Data perturbation introduces perturbations, e.g., noise and image transformations, to produce negative samples. For example, adding noise to an image can create a negative sample that is similar to the original image."}, {"title": "3.2.3 Modality Correlation Evaluation", "content": "The unimodal representations are then fed into modality correlation evaluation (MCE) module to evaluate the correlations between different modalities. Fig. 3 shows the structure of MCE module. which consists of modality transformation layer, cross-attention layer, feedforward layers, similarity analysis layer, and loss calculation layer. The modality transformation layer aims to transform the input data into higher-level representations with dimensions equal to the hidden layer dimension. The cross-attention layer computes the attention scores across modalities to reveal the interrelations among text, audio, and images. The feedforward layers linearly transform the attention results, i.e., mapping them to the pre-defined output dimension. The goal of MCE is to minimize the triplet loss function which calculates the disparity between the predicted outputs and labels.\nModality representation. For audio modality, its unimodal representation $\\mathcal{F}_A \\in \\mathbb{R}^{b \\times t \\times m}$ is converted to a representation of dimension $\\mathcal{F}' \\in \\mathbb{R}^{b \\times t \\times d}$ after positional encoding, where $d$ is the hidden layer dimension. For text modality, after the positional encoding and textual embedding, the unimodal representation $\\mathcal{F}_T \\in \\mathbb{R}^{b \\times s}$ is converted to $\\mathcal{F}_T \\in \\mathbb{R}^{b \\times s \\times d}$, where $s$ denotes the text sequence length. For image modality, after positional encoding and visual embedding, its unimodal representation $\\mathcal{F}_v \\in \\mathbb{R}^{b \\times f \\times i}$ is transformed to $\\mathcal{F}'_v \\in \\mathbb{R}^{b \\times f \\times d}$."}, {"title": "Transformer Encoding", "content": "The encoder consists for three layers, and takes as inputs the $\\mathcal{F}_A$, $\\mathcal{F}_T$, and $\\mathcal{F}_V$ to generate compact and consistent representation for each modality. The implementation details of the encoder are discussed as follows.\nFirst, the input $X$ is normalized to obtain $X_1$ using the LayerNorm function, i.e.,"}, {"title": null, "content": "X_1 = LayerNorm(X) \\qquad(4)"}, {"title": null, "content": "where $X$ denotes the unimodal representation of certain modality. Next, $X_1$ is passed through the Multihead Attention to produce $X_2$, i.e.,"}, {"title": null, "content": "X_2 = MultiheadAttention(Q, K, V : X_1) \\qquad(5)"}, {"title": null, "content": "where $X_1$ is used as query $Q$, key $K$, and value $V$.\nTo well preserve the original information, a residual connection is established between $X_2$ and $X_1$, and the results are normalized using LayerNorm, i.e.,"}, {"title": null, "content": "X_3 = LayerNorm(X_1 + X_2) \\qquad(6)"}, {"title": null, "content": "The normalized $X_3$ is then fed into the FeedForward layer, and the results are combined with $X_3$ to produce the final output of tranformer encoding, i.e.,"}, {"title": null, "content": "X_{out} = X_3 + FeedForward(X_3) \\qquad(7)"}, {"title": "Transformer encoding effectively encodes the inputs $\\mathcal{F}_A$, $\\mathcal{F}_V$, and $\\mathcal{F}_T$ for different modalities to learn their compact representations for further processing.", "content": "Similarity Analysis. After the features are encoded, linear transformation is implemented to project $X_{out}$ for each modality to a shared correlation space $\\mathbb{R}^{b \\times t \\times o}$ for similarity comparison. Thus, we obtain $\\mathcal{F}''_A$, $\\mathcal{F}''_V$, and $\\mathcal{F}''_T$ for audio, vision, and text modalities, respectively. In this space, the features from different modalities help determine the inter-modal correlations. For audio, vision and text, the mapped feature matrices in the space of $\\mathbb{R}^{b \\times t \\times o}$, where $t$ is the temporal output mapping parameter and $o$ is the feature dimension output mapping parameter. $t$ adjusts the temporal dimension mapping of features, while $o$ defines the dimensionality of features in the output space. Together, these parameters enable the mapping of outputs from different modalities into a unified feature space, facilitating operations such as similarity comparison.\nThen, we use cosine similarity to measure the correlations between different modalities, and obtain the pair-wised cross-modal correlation coefficients, i.e., $Cor_{L,A}$, $Cor_{L,V}$ and $Cor_{V,A}$."}, {"title": "3.2.4 Loss calculation.", "content": "We designed a triple-loss function (TL) to minimize the disparity between each modality's feature matrix in space $\\mathbb{R}^{b \\times t \\times o}$ and those of the other modalities, thus enhancing the interaction among modalities to improve joint representation learning.\nConcretely, the TL function measures and optimizes the relationship between the feature matrix of each modality ($\\mathcal{F}'$) against positive ($\\mathcal{F}_p$) and negative ($\\mathcal{F}_n$) feature matrices of another modality, where \"positive\" refers to the highly correlated modalities, and \"negative\" indicates the uncorrelated ones, aiming to foster closer feature vectors among related modalities while enlarging the distance between unrelated ones.\nFor three modalities, the corresponding losses ($\\mathcal{LOSS}_A$, $\\mathcal{LOSS}_T$, $\\mathcal{LOSS}_V$) are derived from their similarity with the features of the other two modalities, i.e.,"}, {"title": null, "content": "\\begin{aligned}\n\\mathcal{LOSS}_A &= TL(\\mathcal{F}'_A, \\mathcal{F}_{Tp}, \\mathcal{F}_{Tn}) + TL(\\mathcal{F}'_A, \\mathcal{F}_{Vp}, \\mathcal{F}_{Vn}) \\\\\n\\mathcal{LOSS}_T &= TL(\\mathcal{F}'_T, \\mathcal{F}_{Ap}, \\mathcal{F}_{An}) + TL(\\mathcal{F}'_T, \\mathcal{F}_{Vp}, \\mathcal{F}_{Vn}) \\\\\n\\mathcal{LOSS}_V &= TL(\\mathcal{F}'_V, \\mathcal{F}_{Tp}, \\mathcal{F}_{Tn}) + TL(\\mathcal{F}'_V, \\mathcal{F}_{Ap}, \\mathcal{F}_{An}) \\end{aligned} \\qquad(8)"}, {"title": null, "content": "The final loss for modality correlation evaluation is $\\mathcal{LOSS} = \\frac{\\mathcal{LOSS}_A+\\mathcal{LOSS}_T+\\mathcal{LOSS}_V}{3}$, which could enhance the interactions among different modalities and promote an equitable contribution from each modality towards the overall learning objective."}, {"title": "3.3 Correlation-aware Multimodal Transformer", "content": "Correlation-aware multimodal transformer receives as inputs the features extracted from the audio, image, and text modalities by the feature extraction module, and merges them with the modality correlations learned by the MCE module for sentiment analysis."}, {"title": "3.3.1 Multimodal Feature Learning", "content": "For each modality $m \\in \\{T, A, V\\}$, where $T, A$, and $V$ denote the text, audio, and visual data, respectively, we project the extracted feature $\\mathcal{F}_m$ via a 1D convolution layer to a common feature dimension to obtain $\\hat{\\mathcal{F}_m}$, i.e.,\n$\\hat{\\mathcal{F}_m} = Conv1D(\\mathcal{F}_m)$\nAfter projection, we apply two successive crossmodal transformer layers to align unmodal representations. Each layer takes the projected features of one modality as the query, and the features of another modality as the key and value to achieve crossmodal interactions, i.e.,"}, {"title": null, "content": "\\begin{aligned}\nh_{T\\rightarrow V} &= CrossmodalTransformer_{T\\rightarrow V} (\\hat{\\mathcal{F}_T}, \\hat{\\mathcal{F}_V}) \\\\\nh_{T\\rightarrow A} &= CrossmodalTransformer_{T\\rightarrow A} (\\hat{\\mathcal{F}_T}, \\hat{\\mathcal{F}_A}) \\\\\nh_{V\\rightarrow T} &= CrossmodalTransformer_{V\\rightarrow T} (\\hat{\\mathcal{F}_V}, \\hat{\\mathcal{F}_T}) \\\\\nh_{V\\rightarrow A} &= CrossmodalTransformer_{V\\rightarrow A} (\\hat{\\mathcal{F}_V}, \\hat{\\mathcal{F}_A}) \\\\\nh_{A\\rightarrow T} &= CrossmodalTransformer_{A\\rightarrow T} (\\hat{\\mathcal{F}_A}, \\hat{\\mathcal{F}_T}) \\\\\nh_{A\\rightarrow V} &= CrossmodalTransformer_{A\\rightarrow V} (\\hat{\\mathcal{F}_A}, \\hat{\\mathcal{F}_V}) \\\n\\end{aligned}"}, {"title": null, "content": "where $\\mathbf{h}_{M\\rightarrow N}$ represents the output of the crossmodal transformer layer, and modality $M$ is enriched with the information from modality $N$."}, {"title": "3.3.2 Modality Correlation-enhanced Multimodal Fusion", "content": "Both inter and intra-modality information are incorporated with the results of modality correlation analysis. First, we perform correlation analysis using the pre-trained MCE model, i.e.,\n$Cor_{T,A}, Cor_{T, V}, Cor_{A,V} = MCE(\\hat{\\mathcal{F}_A}, \\hat{\\mathcal{F}_T}, \\hat{\\mathcal{F}_V})$\nwhere $Cor_{T,A}$, $Cor_{T,V}$ and $Cor_{A,V}$ represent the pair-wised modality correlation coefficients.\nThen, we obtain the feature representation $\\mathbf{h}_{m_i\\rightarrow m_j}$ for each pair of modality $m_i, m_j \\in \\{T, A, V\\}$ using the cross-modal attention and self-attention layers. These features are then integrated with modality correlation coefficients to produce comprehensive representations that encompass both intra-modal and inter-modal information, i.e.,"}, {"title": null, "content": "\\begin{aligned}\n&\\mathbf{h}'_{T\\rightarrow A} = \\mathbf{h}_{T\\rightarrow A} \\times Cor_{T,A} \\\\\n&\\mathbf{h}'_{T\\rightarrow V} = \\mathbf{h}_{T\\rightarrow V} \\times Cor_{T,V} \\\\\n&\\mathbf{h}'_{A\\rightarrow T} = \\mathbf{h}_{A\\rightarrow T} \\times Cor_{T,A} \\\\\n&\\mathbf{h}'_{A\\rightarrow V} = \\mathbf{h}_{A\\rightarrow V} \\times Cor_{A,V} \\\\\n&\\mathbf{h}'_{V\\rightarrow T} = \\mathbf{h}_{V\\rightarrow T} \\times Cor_{T,V} \\\\\n&\\mathbf{h}'_{V\\rightarrow A} = \\mathbf{h}_{V\\rightarrow A} \\times Cor_{A,V}\n\\end{aligned}"}, {"title": null, "content": "where $\\mathbf{h}'_{T\\rightarrow A}, \\mathbf{h}'_{T\\rightarrow V}, \\mathbf{h}'_{A\\rightarrow T}, \\mathbf{h}'_{A\\rightarrow V}, \\mathbf{h}'_{V\\rightarrow T}, \\mathbf{h}'_{V\\rightarrow A}$ are the modality-specific feature representations en-hanced by their corresponding modality correlation coefficients, signifying the weighted importance of one modality's features in the context of another.\nSubsequently, we utilize the self-attention to further refine the fused features for culminating in a holistic representation, i.e.,"}, {"title": null, "content": "\\begin{aligned}\n&H'_{T} = Transformer_{Tmem} (Concat(\\mathbf{h}'_{A\\rightarrow T}, \\mathbf{h}'_{V\\rightarrow T})) \\\\\n&H'_{A} = Transformer_{Amem} (Concat(\\mathbf{h}'_{T\\rightarrow A}, \\mathbf{h}'_{V\\rightarrow A})) \\\\\n&H'_{V} = Transformer_{Vmem} (Concat(\\mathbf{h}'_{T\\rightarrow V}, \\mathbf{h}'_{A\\rightarrow V}))\n\\end{aligned}"}, {"title": null, "content": "where $\\mathbf{H}'_T$, $\\mathbf{H}'_A$, $\\mathbf{H}'_V$ are the final feature representations for text, audio, and visual modalities, respectively. Then, the learned representations for three modalities are concatenated together to obtain the comprehensive representation about the multimodal data sample, i.e., $\\mathbf{H}_{concat} = Concat(\\mathbf{H}'_T, \\mathbf{H}'_A, \\mathbf{H}'_V)$.\nFinally, the sentiment prediction is attained through a linear projection layer and a Softmax layer. The most probable sentiment class is determined by the index of the maximum value in the Softmax output, i.e.,"}, {"title": null, "content": "y = Softmax(Linear(\\mathbf{H}_{concat}))"}, {"title": null, "content": "where $y$ is the probability distribution over sentiment classes, and the predicted sentiment class $\\hat{c}$ is given by:\n$\\hat{c} = arg \\max(y_i)$ where $\\hat{c}$ is the index of the highest probability in the vector $y$, corresponding to the most likely sentiment class."}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments over the CMU_MOSEI dataset which is a widely used benchmark for multimodal sentiment analysis. This dataset contains 23,453 distinct video clips that cover diverse domains, including reviews, debates and consults. Each video clip has three modalities, i.e., video, audio and text, and is labelled with seven classes of fundamental emotions with an intensity scale from -3 to 3. Table 1 provides the statistical details of CMU_MOSEI dataset. For our experiments, we allocate 70% of the dataset(16,417 clips) to the training set, 15% to the validation set(3,518 clips), and 15% to the test set(3,518 clips)."}, {"title": "4.1 Dataset and Settings", "content": "We conduct experiments over the CMU_MOSEI dataset which is a widely used benchmark for multimodal sentiment analysis. This dataset contains 23,453 distinct video clips that cover diverse domains, including reviews, debates and consults. Each video clip has three modalities, i.e., video, audio and text, and is labelled with seven classes of fundamental emotions with an intensity scale from -3 to 3. Table 1 provides the statistical details of CMU_MOSEI dataset. For our experiments, we allocate 70% of the dataset(16,417 clips) to the training set, 15% to the validation set(3,518 clips), and 15% to the test set(3,518 clips)."}, {"title": "4.1.1 Dataset", "content": "We conduct experiments over the CMU_MOSEI dataset which is a widely used benchmark for multimodal sentiment analysis. This dataset contains 23,453 distinct video clips that cover diverse domains, including reviews, debates and consults. Each video clip has three modalities, i.e., video, audio and text, and is labelled with seven classes of fundamental emotions with an intensity scale from -3 to 3. Table 1 provides the statistical details of CMU_MOSEI dataset. For our experiments, we allocate 70% of the dataset(16,417 clips) to the training set, 15% to the validation set(3,518 clips), and 15% to the test set(3,518 clips)."}, {"title": "4.1.2 Experiment Settings", "content": "All experiments were conducted on a server equipped with an Intel SkyLake-E 12-core processor and a GeForce GTX 1180 graphics card. We run each experiment five times, and report the average results. Multiple evaluation metrics as listed below are introduced to evaluate the performance of CorMult model and the baseline multimodal sentiment analysis methods."}, {"title": "4.2 Effects of Perturbation Strategy", "content": "The following four negative sample generation strategies are considered for contrastive learning.\nStrategy A: For each data sample, we randomly choose one modality and replace it with the corresponding modality from another randomly selected data sample in the same batch.\nStrategy B: For audio and video modalities, we introduce a time offset, e.g., one second, between them. For text, an offset of one word is introduced.\nStrategy C: For audio and video modalities, a Gaussian noise is added. For text, words or characters are randomly replaced.\nStrategy D: A combination of strategies A, B, and C, i.e., randomly selecting one of the three strategies above.\nEach strategy is applied across the entire training dataset. Training parameters and model architectures are consistent across strategies to ensure a fair comparison. Table 2 presents the sentiment analysis results while using different data perturbation strategies. Among the four strategies, Strategy D achieves the highest values in terms of the four performance evaluation metrics, indicating the advantage of combining multiple data perturbation strategies."}, {"title": "4.3 Effects of Distance Measurement", "content": "The MCE module in CorMult model maps the learned audio, image, and text to a unified shared space termed the Correlation Joint Space. In this space, the semantic correlations between different data modalities can be more easily evaluated. We seek to identify the most appropriate distance metric to measure modality correlations by evaluating different similarity measures. Concretely, given two vectors A and B, the metrics under consideration include Euclidean distance"}]}