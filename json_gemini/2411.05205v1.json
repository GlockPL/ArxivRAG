{"title": "Maximizing User Connectivity in AI-Enabled Multi-UAV Networks: A Distributed Strategy Generalized to Arbitrary User Distributions", "authors": ["Bowei Li", "Yang Xu", "Ran Zhang", "Jiang (Linda) Xie", "Miao Wang"], "abstract": "Deep reinforcement learning (DRL) has been extensively applied to Multi-Unmanned Aerial Vehicle (UAV) network (MUN) to effectively enable real-time adaptation to complex, time-varying environments. Nevertheless, most of the existing works assume a stationary user distribution (UD) or a dynamic one with predicted patterns. Such considerations may make the UD-specific strategies insufficient when a MUN is deployed in unknown environments. To this end, this paper investigates distributed user connectivity maximization problem in a MUN with generalization to arbitrary UDs. Specifically, the problem is first formulated into a time-coupled combinatorial nonlinear non-convex optimization with arbitrary underlying UDs. To make the optimization tractable, a multi-agent CNN-enhanced deep Q learning (MA-CDQL) algorithm is proposed. The algorithm integrates a ResNet-based CNN to the policy network to analyze the input UD in real time and obtain optimal decisions based on the extracted high-level UD features. To improve the learning efficiency and avoid local optimums, a heatmap algorithm is developed to transform the raw UD to a continuous density map. The map will be part of the true input to the policy network. Simulations are conducted to demonstrate the efficacy of UD heatmaps and the proposed algorithm in maximizing user connectivity as compared to K-means methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-Unmanned Aerial Vehicle (UAV) networks (MUNs) have emerged as a powerful paradigm, with coordinated oper-ations that significantly enhance the versatility, scalability, and robustness of aerial missions [1]. As a key component in be-yond 5G communications, a MUN maneuverably enhances the service provisioning of cellular networks on-demand and fills the communication vacuum where terrestrial networks are not available [2]. However, attributed to UAVs' ad-hoc mobility, supporting ground communications with MUNs poses signif-icant challenges, particularly for time-serial decision making over a long time horizon in dynamic, uncertain environments.\nRecent advancements in deep reinforcement learning (DRL) have opened new avenues for addressing these challenges. With DRL, UAVs learn optimal collaborative strategies through trial-and-error interactions with the environments, enabling real-time adaptation to complex, time-varying condi-tions with minimal human intervention [3]. This is particularly crucial when UAVs must operate in unpredictable environ-ments, such as disaster zones or hostile territories, where predefined strategies may be insufficient.\nMUNs have been extensively studied in literature with cen-tralized or distributed DRL. In centralized DRL, a master agent collects complete network information and makes decisions for all the UAVs [4]\u2013[6]. For instance, Khairy et al. [4], Luong et al. [5] and Zhang et al. [6] studied joint altitude control and channel access management, joint UAV positioning and radio resource allocation, and user connectivity maximization in MUNs, using Proximal Policy Optimization (PPO), deep Q learning (DQL), and deep deterministic policy gradient (DDPG) algorithms, respectively. Distributed DRL, or multi-agent DRL, distributes the learning load across UAVs such that each UAV learns its own policy in a coordinated way to achieve overall objectives [7], [8]. For instance, Hu et al. [7] investigated trajectory design for a MUN to maximize ground user coverage. Multi-agent value decomposition based DRL was exploited. Park et al. [8] developed a collaborative multi-UAV positioning algorithm to achieve energy-efficient and reliable mobile access to cellular-vehicle communications. Multi-agent DQL was leveraged to tackle the optimization.\nNevertheless, most of the existing works on DRL-based MUN management assume a stationary user distribution (UD) [4] [8] [9] or a dynamic one with predicted patterns [6] [10]. In other words, the model of UD is a priori knowledge in the training, leading to UD-specific policies. When a MUN is deployed to unknown environments in occasions such as post-disaster management, ad-hoc activities, environment investigation, etc, such strategies may not perform well or require considerably extra training time to re-optimize. This makes optimal operations hardly available in a short time. Therefore, an adaptive MUN management strategy that can provide satisfying control for arbitrary UDs is desired.\nTo this end, we study the design of DRL algorithms which can be generalized to arbitrary UDs after the training stage. A user connectivity maximization problem is considered where optimal UAV positioning is needed to maximize the number of"}, {"title": "II. SYSTEM MODEL", "content": "We consider a L-by-L target area, across which a set of users, denoted as U, are randomly distributed. Most of the users are clustered around hot spots while the remaining are scattered outside. To provide downlink communication services to the ground users, a group of UAVs, denoted as I, are deployed, flying at a fixed altitude H over the target area. Each UAV is equipped with a high-gain directional antenna that focuses most of its transmission power within a downward angle \u03b8, forming a circular communication footprint on the ground. The coverage radius can be expressed as\n$r = H \\cdot tan(\\theta/2)$.\nUAVs communicate with each other for coordination via wireless hubs (e.g., cellular base stations or a satellite). The UAV-hub links and UAV-user links occupy disjoint spectrum, thus causing no co-channel interference. Free of remote or centralized controllers, each UAV decides its own moves based on its local observations and mutually exchanged information."}, {"title": "B. Spectrum Access", "content": "UAVs offer the same radio spectrum to ground users. Users access the shared spectrum via Orthogonal Frequency Division Multiple Access (OFDMA) [11]. With OFDMA, spectrum of each UAV is divided into orthogonal resource blocks (RBs), with a total number $N_{rb}$. Each UAV assigns different RBs to its users. Thus one user does not interfere with other users of the same UAV, but may suffer from interference from other UAVs that also cover it using overlapping RBs. We consider that each user has a minimum throughput requirement $r_{min}$. Therefore, if a user $u$ is connected to a UAV $i$, it will be randomly assigned with $N_{i,u} \\le N_{rb}$ RBs satisfying\n$\\sum_{k=1}^{N_{i,u}} W_{RB}log_2(1+SINR_{i,u,m_k}) \\geq r_{min}$,\nwhere $W_{RB}$ denotes the bandwidth of one RB, $SINR_{i,u,m_k}$ denotes the signal-to-interference-and-noise ratio of user $u$ from UAV $i$ at RB $m_k$, and $m_k$ is the index of the kth random RB assigned to user $u$. $SINR_{i,u,m_k}$ is expressed as\n$SINR_{i,u,m_k} = \\frac{P_iG_{i,u,m_k}}{n_o + \\sum_{j\\neq i, j \\in Z_{u,m_k}} P_jG_{j,u,m_k}}$\nwhere $G_{i,u,m_k} = 10^{-PL_{i,u,m_k}/10}$.\nwhere $P_i$ and $P_j$ denote the transmit power spectrum density (PSD) of UAV $i$ and $j$, respectively, $G_{i,u,m_k}$ denotes the channel gain from UAV $i$ to user $u$ at RB $m_k$, $n_o$ denotes the PSD of noise, and $PL_{i,u,m_k}$ denotes path loss in dB from UAV $i$ to user $u$ at RB $m_k$. $Z_{u,m_k}$ denotes the set of UAVs that cover user u and transmit over RB $m_k$. It may change with the UAV positions. A commonly adopted air-to-ground channel model [12] is exploited to estimate $PL_{i,u,m_k}$ as follows.\n$PL_{i,u,m_k} = 20 log_{10} d_{i,u} + 20 log_{10} f_{m_k} - 27.55 + \\eta$ (dB),\nwhere $f_{m_k}$ is the center frequency of RB $m_k$ in MHz, $d_{i,u}$ is 3D distance between UAV $i$ and user $u$, and $\\eta$ represents excessive path loss related to Line-of-Sight (LoS) availability and propagation environments (e.g., urban or suburban)."}, {"title": "C. User Association", "content": "We consider a two-stage user association policy. In the first stage, each user determines the UAV that provides the best average SINR, and sends it a connection request. Once requests are collected, each UAV admits users according to a descending order of the reported SINR in the requests. Each UAV only admits a user when its available RBs satisfy Eq. (1). Thus users making requests are not always guaranteed with admission. In the second stage, any unassociated user"}, {"title": "D. Problem Formulation", "content": "Our objective is to develop a trajectory control strategy to guide a set of UAVs to maximize the number of served users over a time horizon given arbitrary UDs. The decision variables are each UAV's movements in each time step, con-strained by the UAV spectrum availability and user association. The problem formulation is as follows.\n$\\begin{aligned} &\\underset{a_{i,t}, \\forall i \\in I}{\\text{max}} \\sum_{t=1}^{T} [\\sum_{i \\in I} \\sum_{u \\in U} X_{u,i}(t)] & \\tag{P} \\\\ &\\text { s.t. } \\quad X_{u,i}(t) \\in\\{0,1\\}, \\forall i \\in I \\text { and } \\forall u \\in U & \\tag{C1} \\\\ &\\qquad \\sum_{i \\in I} X_{u,i}(t) \\le 1, u \\in U & \\tag{C2} \\\\ &\\qquad \\sum_{u \\in U} N_{i,u} \\le N_{r b}, \\forall i \\in I & \\tag{C3} \\\\ &\\qquad 0 \\le X_{i,t}, Y_{i,t} \\le L, \\forall i \\in I & \\tag{C4} \\\\ \\end{aligned}$\nIn problem P, timing of the network is slotted into T steps, indexed by t. At step t, $a_{i,t}$ denotes the moving direction and distance of UAV i, and $(x_{i,t}, Y_{i,t})$ denote its horizontal position. $X_{u,i}(t)$ is a binary indicator taking 1 when user u is associated with UAV i at step t, and 0 otherwise. ${X_{u,i}(t)}$ are jointly determined by UAV positions, UAV RB allocation and user association. Constraint C2 requires each user associated to at most one UAV at each step. Constraint C3 ensures that the available RBs of each UAV are not over assigned. Constraint C4 limits the UAVs within the target region.\nWhen ${a_{i,t}}$ are discrete and finite, problem Pis a time-coupled combinatorial nonlinear non-convex optimiza-tion problem. What is more challenging is that for different UDs, the optimization problem needs to be retackled to obtain the new optimum. This is prohibitively complicated for onsite operation where computing resources are limited and UAVs need to be settled in a short time. Instead of solving the optimization directly, our proposed method utilizes CNN to extract the high-level features of the detected UD and outputs via offline trainings an adaptive strategy which can infer optimal UAV positions for arbitrary UDs with low computing complexity."}, {"title": "III. GENERATION OF USER DISTRIBUTION HEATMAP", "content": "The motivation of generating UD heatmap is illustrated in Fig. 2. When a UAV is at a position with sparsely distributed users (e.g., the black coverage), its movement in the middle way towards hot spots (e.g., to the green coverage) may bring no change or even a decrease in the number of served users. Such situations act negatively in helping the UAV find the optimal position. Instead of using the discrete distribution, a heatmap can be generated reflecting the DENSITY of UD in a continuous manner. When the UAV moves towards the hot spots, its covered density increases even if its covered number of users reduces. This will effectively increase the exploration efficiency and reduce the chance of being trapped in the local optimum."}, {"title": "IV. DESIGN OF MULTI-AGENT CNN-ENHANCED DEEP Q-LEARNING (MA-CDQL) ALGORITHMS", "content": "In this section, we present the design details of the MA-CDQL algorithm based on the obtained UD heatmap. The multi-agent framework requires the UAVs to exchange infor-mation for coordination. In our design, each UAV will share its position and number of served users at each step."}, {"title": "A. State Definition", "content": "The target area is discretized into an MxM grid, with UAVs taking positions only at the grid intersections. The individual state space of each UAV needs to include i) information of"}, {"title": "B. Action Definition", "content": "When UAV positions are limited only to the grid in-tersections, each UAV has only 5 possible moves at each intersection, i.e., move forward, backward, to the left, to the right, and hover still. When it moves, it will move by a constant distance from one intersection to an adjacent one. Denote the individual action space of UAV i as A\u2081, which can be expressed as\n$A_i = \\{0, 1, 2, 3, 4\\}$,\nwhere 0,1,2,3,4 corresponds to the above 5 actions."}, {"title": "C. Reward Function", "content": "The reward function for one individual UAV is composed of four parts: a) the number of its served users, b) the total number of served users by all the UAVs, c) the total heatmap density covered by all the UAVs, and d) the distance penalty for coverage overlapping with other UAVs.\nPart a) of the reward is defined in Eq. (8). It guides each UAV to serve as many users as possible, directing the UAV toward areas with denser UDs.\n$R_{local\\_num}(t) = \\sum_{u \\in U} X_{u,i}(t), \\forall i \\in I$.\nHowever, relying solely on a) may lead to uncoordinated UAV positioning, where some UAVs occupy dense regions early and casually, preventing others from finding good positions. To tackle this, UAVs may exchange information of their respective number of served users to collaboratively maximize the total number of served users, i.e., part b). Part b) is defined as\n$R_{glb\\_num}(t) = \\sum_{u \\in U, j \\in I} X_{u,j}(t) / I, \\forall i \\in I$.\nPart c) is to give the UAV a hint on if they are potentially heading towards hot spots at one step. This is to compensate the negative effect illustrated in Fig. 2 that sometimes a good move may result in decreased part a) and b) but increased part c). The part c) reward is calculated as follows. Define an MxM coverage matrix $H_C(t)$ at step t, with each element corresponding to a grid intersection and calculated as\n$H_{C_{i,j}}(t) =\n\\begin{cases}\n1, & \\text{if any UAV covers intersection (i,j)}; \\\\\n0, & \\text{otherwise}\n\\end{cases}$\nThen part c), i.e., the global density reward, is obtained as:\n$R_{glb\\_dens}(t) = \\sum_{i=1}^{M} \\sum_{j=1}^{M} (H_2)_{i,j}(t) \\cdot H_{C_{i,j}}(t)$\nLast but not least, part d) is to punish coverage overlapping between UAVs such that UAVs are well scattered to reduce contention. It is calculated according to the following equation.\n$R_{ovg\\_pnt}(t) = \\sum_{j \\in I, j\\neq i} max\\Big(0, (1-\\frac{d_{i,j}(t)}{2r}) \\cdot P_{max}\\Big)$,\nwhere $P_{max} = \\frac{d_p}{I}$.\nIn Eq. (12), r, $d_{i,j}(t)$, $P_{max}$ denotes the ground coverage radius of a UAV, the distance between UAV i and j, and the maximum distance-based penalty, respectively. The value $d_p$ is an adjustable weighting factor that tunes the impact of $R_{ovg\\_pnt}(t)$ on the total individual reward $R_i(t)$. When one UAV is at least 2r distance away from all the other UAVs, it will not get penalized, i.e., $R_{ovg\\_pnt}(t) = 0$.\nThe total individual reward $R_i(t)$ is calculated as a weighted summation of all the parts defined above, as given in Eq. (13), where $\u03b1_1, \u03b1_2$ and $\u03b1_3$ are weighting constants.\n$R_i(t) = \u03b1_1 * R_{local\\_num}(t) + \u03b1_2 * R_{glb\\_num}(t) + \u03b1_3 * R_{glb\\_dens}(t) + R_{ovg\\_pnt}(t)$"}, {"title": "D. Neural Network Design", "content": "The architecture of the CDQN is based on ResNet [13]. The input state matrix is first saved, then processed through con-volution, batch normalization and ReLu activation into a 64-channel feature map. The feature map is then passed through 3 residual layers. Each residual layer is composed of 6 residual blocks which further consists of 2 convolutional layers and a residual connection. The results are then flattened into a one-dimension vector, with the action output generated through a fully connected layer."}, {"title": "E. Implementation", "content": "The MA-CDQL algorithm consists of three phases: UD heatmap generation, model training, and algorithm execution. In the first phase, a pool of UDs are randomly generated where most users are clustered in random hot spots and the remaining are randomly distributed across the target area. Each UD is transformed into a UD heatmap according to Algorithm I and downsampled. During the training phase, in each episode, the environment is initialized with a UD from the pool and the corresponding downsampled UD heatmap. Throughout the training, UAVs interact with the environment, exchange information, and update their respec-tive Q networks until final convergence. During the execution phase, when deployed in a scene with an unknown UD, UAVs will first cooperatively sweep the area to get the scattered-point UD, from which downsampled UD heatmap is obtained. Based on the downsampled heatmap and the real-time positions of all the UAVs, each UAV is guided by the distributed policy to its optimal position that collectively maximizes the total number of served users."}, {"title": "V. EXPERIMENT", "content": "The target area is discretized into a 10 \u00d7 10 grid. Each grid cell has a side length of 100 meters. The training was conducted in Windows 10 with a 16-core vCPU and an RTX 4090 GPU. The entire experiment was implemented in Python using the PyTorch library. Training was performed for up to 30,000 episodes, each having 30 time steps. Three different settings were considered in the experiment. All the settings have 100 users. Setting 1 has 3 UAVs and 3 hot spots, with two hot spots having significantly more users than the third; 10 users are outside the hot spots. Setting 2 has 3 UAVs and 4 hot spots, with each hot spot being clearly separated; 10 users are outside the hot spots. Setting 3 has 5 UAVs and 4 hot spots, with all the users in the hot spots."}, {"title": "B. Simulation Results", "content": "We first test the convergence performance of the MA-CDQL algorithm, and present the results in Fig. 4. 150 randomly generated UDs (75 for Setting 1 and 2, respectively) are used, each of which is initialized into the environment every 150 episodes. Thus in Fig. 4, we group every 150 episodes into an epoch. We randomly selected several distributions to display the individual convergence, along with the average convergence over all distributions. It can be seen that the overall training converges well.\nWe then compare user connectivity performance between three methods: K-means clustering [14], the MA-CDQL w/o heatmap, and MA-CDQL w/ heatmap. Algorithms in K-means class can quickly identify user clusters with low computing complexity, but it is challenging to integrate more compli-cated constraints (e.g., user throughput requirement, spectrum availability) for better performance. For MA-CDQL without heatmap, it calculates the number of users in each grid and shapes the numbers into an MxM matrix as part of the state space. Its reward function considers both individual and overall number of served users. The results are shown in table II.\nIn the table, trS and teC refers to the training and testing set of a specific setting, respectively. The table shows that MA-CDQL yields considerably better user connectivity in Setting 1 and 2 than K-means, and the heatmap version outperforms the non-heatmap version in all settings. Yet in Setting 3, K-means achieves better performance than the proposed method.\nTherefore, it can be concluded that i) compared to K-means, our proposed method does not always outperform, especially in cases when the number of UAVs exceeds the number of hot spots; ii) however, the proposed method yields better generalization across various types of clustering distributions with considerably better or comparable user connectivity per-formance; iii) integration of heatmap can effectively improve the user connectivity performance in all settings."}, {"title": "VI. CONCLUSIONS", "content": "This paper has studied distributed user connectivity maxi-mization problem with generalization to arbitrary user distri-butions (UDs). To make the problem tractable, the MA-CDQL algorithm has been proposed. The algorithm has integrated a ResNet-based CNN into the deep Q network to extract the high-level features of the input UD and infer optimal guidance to UAVs to maximize user connectivity. A UD heatmap algo-rithm has been developed to smooth the UD into continuous density map in order to improve the learning efficiency and avoid local optimums. Simulations have demonstrated that i) compared to K-means methods, the proposed method yields better generalization across various UD clustering settings with considerably better or comparable user connectivity perfor-mance; and ii) integration of UD heatmap can effectively further improve the performance in all settings."}]}