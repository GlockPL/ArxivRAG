{"title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025", "authors": ["Nantheera Anantrasirichai", "Fan Zhang", "David Bull"], "abstract": "The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries by enabling innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores the significant technological shifts since our previous review in 2022, highlighting how these developments have expanded creative opportunities and efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss AI integration into post-production workflows, which has significantly accelerated and refined traditional processes. Despite these innovations, challenges remain-particularly for the media industry-due to the demands on communication traffic from creative content. We therefore include data compression and quality assessment in this paper. Furthermore, we highlight the trend toward unified AI frameworks capable of addressing multiple creative tasks and underscore the importance of human oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges to maximize its benefits while addressing associated risks.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) has grown dramatically over the past few years, particularly due to the rise of generative AI and large language models (LLMs). These advancements have been widely regarded as beneficial in many countries, such as the UK, as highlighted in a report published by the Authority of the House of Lords [1]. These AI advancements have also had significant direct and indirect impacts on the creative industries, influencing the direction of their growth. Generative AI, for instance, primarily focuses on generating new data that is not identical to the training data but shares similarities. However, the training data can be very large and have a much broader range than what humans have encountered themselves. The resulting output may present entirely new work to an individual, serving as inspiration. Indeed, AI tools have also opened up opportunities to a wider range of users with different skill sets, enabling creative work to be done faster, more effectively, and with greater creativity.\n\nThe major breakthrough in generative AI has come from OpenAI\u00b9, an AI research and deployment company, with the introduction of Generative Pre-trained Transformer (GPT) models for LLMs. LLMs are specifically designed to understand and generate human language. They are characterized by their vast size in terms of parameters and the amount of training data. This breakthrough was particularly significant with the release of ChatGPT in 2022, which was fine-tuned from a model in the GPT-3.5 series. ChatGPT is a conversational model that includes advancements in safety features to mitigate the generation of inappropriate content. Several other LLM platforms were developed around the same time, such as LaMDA and PaLM by Google AI, Ernie Bot by Baidu, and BLOOM by BigScience. Additionally, Anthropic launched Claude, the LLM trained specifically to be harmless and honest, leveraging reinforcement learning from human feedback (RLHF) a technique used to train AI systems to appear more human [2]. However, ChatGPT stands out as the most renowned, thanks to its quick and efficient responses, and notably, it is available for free. Another breakthrough in the same year was in text-to-image models. OpenAI achieved a significant milestone with DALLE 2, producing impressive artworks and photorealistic images despite its limited language understanding. Midjourney by Midjourney, Inc., another well-known text-to-image generation, supports higher resolution images, which can go up to 4096\u00d74096 pixels. Stable Diffusion by Stability AI, for which the code and model weights are publicly available2, allows developers and artists to further adapt it to their own applications.\n\nThe next breakthrough happened in 2023 when OpenAI unveiled GPT-4, a significantly larger model with more parameters and improved performance compared to its predecessors [3]. GPT-4 is a multimodal large language model that can generate responses to both text and images. It incorporates DALLE 3, enabling it to comprehend a much broader range of nuances and details than earlier versions. In March 2024, Claude 3 Opus by Anthropic has been released, boasting multimodal capabilities in generating images, tables, graphs and diagrams. Moreover, Anthropic claims that Claude 3 Opus outperforms GPT-4 in generating human-like dialog and contextually aware responses. These AI advancements however make the media industry faces challenges. With the impact of these technologies, DMG Media, the Financial Times, and the Guardian Media Group have highlighted concerns about the potential impact on print journalism, particularly if AI tools reduce the need for users to click through to news websites, affecting advertising and subscription revenues [1].\nGenerating videos is significantly more challenging than generating images. In February 2024, Google announced Gemini 1.5 with a capability of processing a large data (approximately 8 times more than GPT-4), like video footage and audio recordings\u00b3. In the same month, OpenAI provided the first preview of Sora, a model capable of generating impressive realistic videos up to 1 minute long. As of November 2024, Sora remains unreleased and unavailable to the public. Based on the videos released by OpenAI, Sora appears to outperform other text-to-video models. A month later, Gemini 1.5 has announced its support native audio understanding in 180+ countries. With the emergence of Gemini 1.5 and Sora, alongside other forthcoming AI tools, it is obvious that video content creation will significantly benefit, opening up the media landscape for creativity and providing more opportunities for diverse storytellers, while also saving time to produce. A recent example is the AI-generated Christmas commercial by Coca-Cola. These advertisements overcome the limitations of current technologies by using very short videos with quick scene transitions, ensuring that any artifacts, such as unnatural fingers, are not immediately noticeable.\n\nFor post-production workflow, generative AI may not have a direct impact, but the neural networks originally proposed for generative AI have been widely adapted to serve this purpose, leading to significant improvements in both output quality and computational speed. Moreover, in recent years, there has been a noticeable trend in technology towards adopting a unified framework rather than addressing individual challenges, as it better reflects real-world scenarios. For instance, natural history filmmaking involves challenging acquisition environments and high production standards. It is often filmed in low light, underwater, heat haze, or adverse weather conditions. This can lead to increased noise levels, focus issues, low contrast, color balance problems, and blurriness in the footage. Moreover, the unified models offer advantages in generalizing to diverse tasks and providing flexibility. Take Painter by BAAI Vision [4] as an example, which employs an image pair as a task prompt (similar to a text prompt in LLMs), their model transfers the input image to produce a similar output as the task prompt, enabling it to undertake various tasks such as segmentation, low-light enhancement, rain removal, etc.\n\nWhile generative AI makes creating and post-processing digital content easier and faster, more efficient performance in video content streaming is consequently required. Although AI-based solutions have been proposed both for conventional coding tool enhancement and for new compression frameworks, they have not been fully adopted in practical applications due to hardware constraints and complexity issues. However, the latest learning-based video codecs have already demonstrated their potential to compete with conventional standard video codecs, in particular, with the latest advances in implicit neural representations. The techniques could emerge in the next few years and contribute to the future video coding standards.\n\nMoreover, in the past few years, we have also seen the impact of AI in the research area of visual quality assessment. The advances include new model architectures based on different attention mechanisms and the application of LLMs, which evidently improves model generalization. New training methodologies have also been proposed based on weakly/unsupervised learning, which address the issue with limited training content.\n\nOne of the exciting aspects of using LLMs in the creative sector is that 'The human in the loop' [5] is simplified through text prompts, as language enables artists to convey complex emotions and narratives. This is important because generative AI could produce mistakes, known as hallucinations; hence, we, as humans, correct this through reinforcement learning with human feedback [6].\n\nIn this paper, the objective is to investigate the latest advancements in technology that have emerged since our previous review paper on AI in the creative industries before 2022 [7]. Compared to the earlier paper, where most technologies were developed as support tools, this updated review paper reveals significant shifts in the technologies after 2022 due to generative AI and most recent AI-based technologies. We explore the new direction of how AI could be involved in the creative sector. Similar to [7], we first provide a high-level overview of current advanced AI technologies (Section 2), followed by a selection of creative domain applications (Section 3), where current technologies enabling AI in creative scenarios are described. Finally, we discuss challenges and the future potential of AI associated with the creative industries (Section 4)."}, {"title": "2 Current Advanced AI Technologies", "content": "Foundation models (FMs) have been termed by The Stanford Institute for Human-Centered Artificial Intelligence in 20215 which mean \u201cany model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\". LLMs is one kind of the FMs.\n\nAs this paper provides updates from our previous review [7], an introduction to AI, basic neurons, convolutional neural networks (CNNs), generative adversarial networks (GANs), recurrent neural networks (RNNs), and deep reinforcement learning (DRL) can be found there. Here, we will emphasize four technologies that are rapidly growing and largely affecting creative industries, particularly during the year around 2023. This includes Transformers, Large language models (LLMs), Diffusion Models (DMs), and Implicit Neural Representations (INRs). It is important to note that while these newer technologies are gaining prominence, those from previous generations are still widely used, often in conjunction with the newer ones, as they offer distinct advantages. For instance, CNNs effectively capture local features and semantic meaning, while the attention mechanism in transformers offers global dependencies."}, {"title": "2.1 Transformers", "content": "No objection, the usage of AI has become widely accessible to everyone due to the launch of ChatGPT by OpenAI since November, 2022. ChatGPT has become the fastest-growing consumer software application in history6, marking a significant advancement in the performance of large language models (LLMs). However, the turning point was actually back in 2017 when Google AI introduced \u2018Transformer' architectures in their publication 'Attention Is All You Need' [8]; since then, the performance of language models has dramatically improved. The applications have been extended, including vision understanding [9], and even learning multiple tasks across multiple modalities simultaneously (e.g., Gato [10]). Here, we will provide a background of transformers.\n\nBefore the advent of transformers, natural language processing (NLP) was done using recurrent neural networks (RNNs), processing data sequences sequentially. In contrast, the transformers capture long-range dependencies through self-attention mechanisms, that go across all words in the sequence, weigh the importance of different words and find relationships between words regardless of their positions. This learns context-aware representation and enables parallel processing of the entire sequence, making the transformers computationally efficient. A set of several attention layers running in parallel is called Multi-Head Attention. In the technical overview level, Transformer architecture, shown in Fig. 1 (a), comprises Encoder and Decoder, similar to many CNN-based generators. But the encoder is a stack of identical layers, concatenating a multi-head self-attention mechanism and a fully connected feed-forward network. The decoder is also a stack of identical layers, of which each layer has additional sub-layer to perform multi-head attention over the output of the encoder stack.\n\nMathematically, the attention function is computed from inputs: query Q, keys K, and values V. The matrix of outputs of attention function is\n\\[Attention(Q, K, V)=\\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}})V,\\]\nwhere $d_k$ is a dimension of K. The term $Q K^T$ is Dot-Product Attention, which yields a high similarity value when the two words are closely related. If Q and K are from the same sentence, Eq. 1 refers to self-attention, but if Q and K are from different sentences, it is referred to as cross-attention. Within the network, multi-head attention is actually employed to concurrently process attention and enable the model to collectively focus on information from distinct representation subspaces at various positions through the learnable parameters $W_s$.\n\\[\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O, \\\\ \\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i).\\]\nAttention modules have not only been using in the transformers, but they have also been integrated with other deep learning architectures, including with CNNs in particular, used for image classification [11], object detection [12], and other computer vision tasks [13] as providing performance improvement.\n\nIn 2020, the first successful training of a transformer encoder for image recognition was published in [9], known as Vision Transformer (ViT). ViT divides an input image into patches, similar to words in a sentence, and processes them through multi-head attention. Additionally, a Multilayer Perceptron (MLP) is employed as the feedforward network. The later work uses a hierarchical division of image inputs and the shifted window approach, called Swin Transformer [14] by Microsoft. It reported outperforming ViT by 2.4% in ImageNet-22K classification (21,841 different categories). Its version 2 [15] applies cosine function in the attention module enabling the scaling up of capacity and resolution. More detail on transformer-based object detection is discussed in Section 3.4.2. To date, Swin Transformer has been widely adopted in various applications, including image restoration [16], where it is integrated with UNet. The application of transformers to video recognition has done by input as a sequence of frames [17] or dividing the video into space-time blocks [18]. Comprehensive surveys on transformers for images and videos can be found in [19] and [20], respectively.\n\nTransformers have been widely used and offer better performance across many tasks. One reason for this is the availability of the open-source Transformers library through Hugging Face7, a platform that assists developers in building their own applications across various tasks,"}, {"title": "2.2 Large language models", "content": "LLMs are based on transformer models with self-attention mechanisms as core modules. Training LLMs comprises two steps: i) pre-training in unsupervised learning manner, and ii) fine-tuning to a specific task or prompt-tuning (or prompt engineering) for better user inputs. The models are first 'pre-trained' with a large amount of unlabelled text data to learn the meaning of words, and the relationships between words before using it to adapt to a downstream task. This is why OpenAI calls their models generative pre-trained transformers (GPT).\n\nFine-tuning involves training the model on new datasets. The drawback is that these data need to be large enough to generalize a new task. Prompt-tuning and prompt engineering are relatively new disciplines for developing and optimizing prompts to efficiently use language models. Prompts is a component that guides the way AI models interpret and respond to user queries. Prompt engineering is the process of structuring text or phrasing that guide the model towards generating the desired output. This relies heavily on trial and error, and an understanding of how the model responds. Prompt-tuning, on the other hand, involves training a small set of parameters before utilizing the LLM, thus requiring a relatively small amount of new data. This approach essentially converts text inputs into task-specific virtual inputs, referred to as tokens, while the pre-trained LLM remains unchanged [25]. As it is AI-based design, the main drawback of prompt-tuning is lack of interpretability. This paradigm has however extended to other domains, such as visual prompt tuning [26]. For a comprehensive survey of LLMs, please refer to [27].\n\nTo date, there are many LLM platforms as Fig. 2 (a) shows their timeline. Many publications on surveys and evaluations of LLMs are available [27-29]. This includes FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets) [30] evaluating LLMs based on 12 fine-grained skills for comprehensive language model evaluation: logical correctness, logical robustness, logical efficiency, factuality, commonsense understanding, comprehension, insightfulness, completeness, metacognition, conciseness, readability, and harmlessness. Their evaluation results are shown in Fig. 2 (b)."}, {"title": "2.3 Diffusion Models", "content": "A generative model, in the context of generative AI, exploits machine learning to learn a probability distribution of the training data to generate new data samples. The very first models are based on Autoencoders (AEs) that learn to encode input data into a lower-dimensional representation (latent space) and then decode it back to its original form. The specific type of AEs, Variational Autoencoders (VAEs) [31], learns the latent space as statistic parameters of probabilistic distributions, leading to significant improvement of the generated results. Concurrently, in the same year, Goodfellow et al. [32] introduced an alternative architecture known as a Generative Adversarial Network (GAN). GANs comprise two competing AI modules: the generator, which creates a sample, and the discriminator, which determines whether the received sample is real or generated. When comparing VAEs to GANS, VAEs exhibit greater stability during training, whereas GANs excel at producing realistic images. More details about AEs and GANs for creative technologies can be found in our previous review [7].\n\nThe important factor driving the rapid growth of generative AI is the development of diffusion probabilistic models, or in short, diffusion models (DMs). The first DM was introduced in 2015 by Sohl-Dickstein et al. [33], using Nonequilibrium Thermodynamics. However, it took 5 years for DMs to generate desirable results. The era of DMs began with Denoising Diffusion Probabilistic Models (DDPMs) proposed by Ho et al. [34] in 2020 and Score-based diffusion models proposed by Song et al. [35] in 2021. These are a simplified process using a denoising autoencoder to approximate Bayesian inference. In brief, the models leverage a diffusion process to learn a probability distribution of the input data. As the name suggests, the data is diffused by gradually adding noise at each iteration step as shown in Fig. 1 (b). A deep neural network (DNN) is then trained to remove this noise, called the denoising process or reverse process. Consequently, the trained model uses random noise to generate data with characteristics similar to those of the training samples. Comparing to GANs, the DMs provide higher diversity samples [36] and the training process is much more stable and do not suffer from mode collapse; however, the DMs are computationally intensive and require longer training times compared to GANs. The complexity is significantly reduced by training the DMs in latent space. Latent Diffusion Models (LDM) [24] use pretrained networks to convert images to feature maps, and perform training on a low-dimensional space. The diagram of LDM is shown in Fig. 1 (c).\n\nGenerating a synthesized sample at random might not be particularly useful, especially for creative industries. Therefore, conditional diffusion models have been proposed, with a wide range of applications such as text-to-sound, text-to-images, and image-to-videos. For DMs, the conditional distributions are modelled using a conditional denoising autoencoder. Classifier guidance was introduced in [36] to improve a diffusion generator in generating images of a desired class. For example, when we provide the model with more information, such as 'a flower', the DM will synthesize a variety of flower images, as the word 'flower' guides the model toward the latent distribution that is formed by various images of flowers. The work in [37] simply refines the latent space of well-trained unconditional DDPM so that the higher-level semantics of the synthetic samples are similar to the reference (conditioning). The LDM [24] offers more flexible conditional image generators by adding cross-attention layer (referred to Transformers in Section 2.1) to the denoising autoencoder. A survey on the methods and applications of DMs prior to 2024 can be found in [38]."}, {"title": "2.4 Implicit Neural Representations", "content": "Implicit Neural Representations (INR), also called neural fields, neural implicits, and coordinate-based neural networks, represent input content implicitly through learned functions F, as shown in Eq. 3. They can be seen as fields x (represented by a scalar, vector, or tensor with a value, such as magnetic field in physics) that are fully or partially parameterized by a neural network \u03a6, typically an MLP [39].\n\\[F(x, \\Phi, \\nabla_x \\Phi, \\nabla^2\\Phi,...) = 0, \\quad \\Phi : x \\leftrightarrow \\Phi(x).\\]\nThe concept might sound complex, but the process is actually very straightforward. For example, in the case of an image, the coordinate of each pixel (x, y) contains colors (r, g, b). The INR inputs (x, y) to the MLP and learns to give the output (r,g,b). The weights and biases of the MLP now represent such an image. Usually, the number of parameters of the MLP is smaller than the total number of pixels multiplied by 3, accounting for the 3 color channels. Hence, one of its applications is data compression [40]. Moreover, the INR can handle complex and high-dimensional data efficiently, bringing attention in visual computing, such as 3D scene reconstruction.\n\nTraditional MLPs employ ReLU (rectified linear unit) for non-linear activation due to its simplicity. However, Sitzmann et al. [41] demonstrate that using periodic functions, such as sinusoids, is more suitable for representing complex natural signals, offering a better fit to the first- and second-order derivatives of the signals. However, this activation could cause ringing artifacts. Saragadam et al. instead proposed using complex Gabor wavelets [42], which learn to represent high frequencies better and simultaneously are robust to noise.\n\nOne of the fastest-growing areas that exploits INRs is Neural Radiance Fields (NeRF), evidenced by 57 papers presented at CVPR, the largest annual conference in computer vision, in 2022 and which grew to 175 papers in 20238. First introduced in 2020 by Mildenhall et al. [43], NeRF is a form of neural rendering, a subset of generative AI, that generates novel views of a scene based on a partial set of 2D images. It achieves this by learning a mapping from 3D spatial coordinates and view directions (x, y, z, \u03b8, \u03c6) to colors and density (r, g, b, \u03c3). This implicit representation allows NeRF to handle complex scenes with varying geometry and appearance, resulting in highly realistic renderings that include accurate lighting, shadows, and reflections. More detail can be found in Section 3.5.2."}, {"title": "3 Advanced AI for the creative industries", "content": "Similar to our previous paper reviewing AI for the creative industries, we categorize the applications and the corresponding AI-based solutions as shown in Table 1. However, the subsections may differ as generative AI affects each application differently."}, {"title": "3.1 Content creation", "content": "Content creation stands as a fundamental activity of artists and designers and the term 'AI art' refers to either created with the assistance of AI algorithms or entirely by AI systems. It is in various digital forms, including images, texts, audio, and videos. The roots of AI art can be traced back to before the 2000s, exemplified by AARON, a computer program initiated in 1972 to autonomously produce paintings and drawings [229]. The practicality of AI art has actually begun with advancements in deep learning, particularly GAN from 2014 and, more recently, transformers, DMs and INRs. Note that when referring to real camera content acquisition using AI, commonly known as intelligent cinematography, we direct the reader to the review in [230]."}, {"title": "3.1.1 Text generation, script and journalism", "content": "In the era of LLMs, AI writing tools have been widely used to help with various writing tasks, including generate written content of articles, blog posts, essays, and reports. These tools go beyond mere grammar and spelling checks; they boast advancements enabling them to analyze the style and tone of written material, adding images, videos and tables, offering suggestions to enhance clarity, coherence, and overall readability [231]. Moreover, AI tools extend their utility beyond content generation by automating tasks like keywords, meta tags, and descriptions, thereby increasing search rankings using search engine optimization (SEO). Additionally, they support the process of publishing across multiple online platforms. Transformers are used to generate image captions by combining information of the images with a word prefix or questions [44].\n\nAI script generators serve as beneficial aids for writers, filmmakers, and game developers, offering inspiration, idea generation, and assistance in crafting entire scripts. Human-AI brainstorming is helpful and saves time [232]. Presently, there are numerous software and websites providing both free and paid script generation services. However, many of these tools are still constrained when it comes to longform creative writing. Dramatron, developed by Google [233], introduces hierarchical language generation, enabling the creation of cohesive scripts and screenplays spanning long ranges. This includes elements such as titles, characters, story beats, location descriptions, and dialogue.\n\nAs discussed earlier in this paper, chatbots are now powered by LLMs, effectively simulating human conversation. These fundamental LLMs are specialized for specific tasks, for instance, journalist AI and blog AI writers, which generate content with layouts suitable for print or online publication. Additionally, there are AI tools designed to detect AI-generated content (e.g., for checking for copyrights), AI-writing styles and content originality to ensure the natural sound of articles. Undoubtedly, generative AI is reshaping the way artists and journalists operate. For an in-depth exploration of the impact and implications of these technological advancements on news organizations, refer to the survey conducted by Beckett et al. [234].\n\nGenerating text and scripts automatically can also be done through image and video inputs without text prompts (e.g., image captioning [235]) and with text prompts. The technical term of these approaches is called Vision Language Models (VLMs), which are multimodal models that learn from images and text. The most common and prominent models often consist of an image encoder, an embedding projector to align image and text representation, often via a dense neural network, and a text decoder stacked in this order. The most well-known technique is Contrastive Language-Image Pre-training (CLIP) [236]. More recent work in [45] scales up the vision vocabulary by incorporating new image features into the existing CLIP model, resulting in improved content understanding. The comprehensive survey of VLMs for vision tasks can be found in [237]."}, {"title": "3.1.2 Audio and Music generation", "content": "Similar to language models, AI-based music generation has rapidly advanced due to unsupervised learning on large datasets and the use of transformers (see Section 2.2). Examples of such software include MuseNet10, Magenta Studio11, and Musicfy12. These tools can assist in composing music by learning complex musical patterns, predicting the next word or music note in a sequence, and mixing specified instruments. Moreover, the AI tools can convert one type of sound into another, such as from whistling to violin or from flute to saxophone13. This capability is invaluable for artists who may not be proficient in playing all the instruments they wish to incorporate, saving both time and costs. March 2024, Suno has released their first model capable of producing radio-quality music that can be created in 2 minutes14. Later, Udio 15, was launched. It offers a prompt to create lyrics and music with a maximum duration of 90 seconds, and also appears to have at least some awareness of copyright.\n\nAI voice software changes voice from one person to another and enable users to train the model to convert other people's voices into their own, e.g. lalals16, Kits17, Media.io18, etc. Certain software, such as Voice.ai19, even offer real-time voice changing capabilities. The technologies behind this use the transformer to learn voice features and patterns in mel-spectrogram. For example, the framework proposed in [49] uses a method based on DM with transformer backbone to turn text input into a mel-spectrogram using the vector quantized variational autoencoder (VQ-VAE) [238]. Next, this mel-spectrogram is transformed into a sound wave. Unlike a regular spectrogram, the mel-spectrogram is based on the mel-frequency scale, which offers higher resolution for lower frequencies. Voice style transfer often use zero-shot learning (a model is trained to recognize classes or categories it has never seen during training) [47] or few-shot learning (a model is trained with only one or a few examples per class) [239].\n\nAnother emerging AI technology is in the field of spatial audio. In 2022, Apple Music revealed that, in just over a year, more than 80% of its worldwide subscribers were enjoying the spatial audio experience, with monthly plays in spatial audio increasing by over 1,000%20. With head tracking, this technology significantly enhances the immersive experience. Masterchannel has launched SpatialAI21, claiming it to be the world's first spatial mastering AI. It processes audio files and returns an optimized track for streaming platforms, along with an individually optimized stereo version for traditional distribution. All these advancements leverage transformer-based technologies."}, {"title": "3.1.3 Image generation", "content": "As described in Section 2.3, recent advances in AI technologies for image generation are based on Diffusion Models (DMs). Well-known and highly competitive text-to-image models include Stable Diffusion22, Midjourney23, DALL\u00b7E24, and Ideogram25. Released in March 2024, the latest version, Stable Diffusion 3 (SD3) [50], reports outperforming state-of-the-art text-to-image generation systems such as DALL\u00b7E 3 (released August 2023), Midjourney v6 (released December 2023), and Ideogram v1 (released February 2024) in terms of typography and prompt adherence, based on human preference evaluations. These open-source tools are built with the Multimodal Diffusion Transformer (MM-DiT) architecture, which integrates attention from both text and image. Examples of text-to-image generation are shown in Fig. 3 (a) comparing the performance of four models, i.e. Ideogram v1, DALL\u00b7E 3, Photoshop 2024, and sdxy-turbo by Nvidia. It is obvious that hands are the most difficult to generate, e.g., one hand has six fingers.\n\nDALL-E 3, available on ChatGPT 4, also provides an inpainting tool, allowing the user to manually select the area to edit. However, as of April 2024, the performance is still limited. As illustrated in Fig. 3 (b), the selected area is the white car, and with the follow-up request to change the white car to the red car, DALL\u00b7E 3 generates correctly. However, if asked to replace it with a bicycle, it does not work. Recently, LLM-grounded Diffusion [54] enables instruction-based scene specification with multiple rounds of user requests without a need of manual selection on the image. This is achieved by generating layout-grounded images first using stable diffusion, then masking the latent variables as priors for the next round of generation26.\n\nSimilar to DALL\u00b7E 3, Photoshop features a Generative Fill tool27 designed to generate new images or assist with photo editing. It accepts a text prompt and provides several generation choices. After defining the editing area, users can remove and add new objects (more inpainting tasks are discussed in Section 3.3.5), transfer to new styles, and expand content within images. Recently, InstructPix2Pix [51] proposed a conditional diffusion model to generate image editing examples without predefined editing areas. It combines GPT-3 and Stable Diffusion. This clearly demonstrates that the model effectively captures and matches the semantic meaning of the content in both text and image. Sometimes, style and context are not easy to describe in words. Textual Inversion [52] personalizes large pre-trained text-to-image diffusion models based on specific objects and styles, using 3-5 images of a user-provided concept. Recently, ByteDance announces Hyper-SD [55] proposed trajectory segmented consistency distillation that provides real-time high-resolution image generation from drawing with a control text prompt."}, {"title": "3.1.4 Video generation and animation", "content": "Despite the success of text-to-image generation, text-to-video generation has just started to grow rapidly in 2024, largely due to its computational expense and content complexity. However, several major companies and private platforms have released their progress in this area, including Gemini 1.5 by Google, Make-A-Video by Meta, and Sora by OpenAI. Make-A-Video [67", "62": "which turns a single static image and a speech audio clip into a video clip of realistic talking faces mimicking human facial expressions and head movements, shown in Fig. 4 (right). The overall quality of the generated videos is better than VLOGGER by Google [63", "66": "that offers listening and speaking states during multi-turn conversations. This framework is based on a conditional diffusion transformer. The main technologies of text-to-video and image-to-video tasks are based on DMs with a combination of 3D convolutions (or separately spatial and temporal convolutions), and spatial and temporal attention modules [69", "70": "modifies the style of an input video using a text prompt. The method leverages pretrained text-to-image models and introduces attention tuning to ensure temporal consistency. Early video generation methods often exhibit flickering, as observed in the CVPR2023 competition on text-guided video editing, where all results suffered from temporal inconsistency. Dreamix [68", "56": "employs VQ-VAE to convert input frames to tokens, which are then fused with text tokens to produce a new video. Phenaki [57", "60": ".", "64": "."}]}