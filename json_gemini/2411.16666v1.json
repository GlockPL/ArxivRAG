{"title": "CatNet: Effective FDR Control in LSTM with Gaussian Mirrors and SHAP Feature Importance", "authors": ["Jiaan Han", "Junxiao Chen", "Yanzhe Fu"], "abstract": "We introduce CatNet, an algorithm that effectively controls False Discovery Rate (FDR) and selects significant features in LSTM with the Gaussian Mirror (GM) method. To evaluate the feature importance of LSTM in time series, we introduce a vector of the derivative of the SHapley Additive exPlanations (SHAP) to measure feature importance. We also propose a new kernel-based dependence measure to avoid multicollinearity in the GM algorithm, to make a robust feature selection with controlled FDR. We use simulated data to evaluate CatNet's performance in both linear models and LSTM models with different link functions. The algorithm effectively controls the FDR while maintaining a high statistical power in all cases. We also evaluate the algorithm's performance in different low-dimensional and high-dimensional cases, demonstrating its robustness in various input dimensions. To evaluate CatNet's performance in real world applications, we construct a multi-factor investment portfolio to forecast the prices of S&P 500 index components. The results demonstrate that our model achieves superior predictive accuracy compared to traditional LSTM models without feature selection and FDR control. Additionally, CatNet effectively captures common market-driving features, which helps informed decision-making in financial markets by enhancing the interpretability of predictions. Our study integrates of the Gaussian Mirror algorithm with LSTM models for the first time, and introduces SHAP values as a new feature importance metric for FDR control methods, marking a significant advancement in feature selection and error control for neural networks.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks have been widely proven as an accurate and generalizable method for prediction in many fields including image processing, biochemistry, and finance. However, as the DNN model become complex-structured, interpreting the model's prediction and underlying mechanism becomes extremely difficult. Model interpretation is important in many practical fields, since understanding the mechanism guides people to make rational real-world decisions. In such a setting, interpreting the influence of input features is essential for understanding the real significant factors. For example, in financial market predictions, we first need to discover the type of factors that make the largest contribution to the final result, and thus we can concentrate on analyzing these factors to make more rational and informed market predictions. It is also essential to extract the unimportant features in such a scenario, since these features may interact with other predictors and perplex the overall model, causing unexpected failures in the final prediction.\nThere are many previous works in measuring feature importance and selecting important features in neural networks. Hechtlinger [19] proposed a simple method to quantify feature importance by taking partial derivatives on each input feature. Verikas et al. [38] quantifies feature importance in Neural Networks by the changes in cross-validation error rate after removal of one feature. These methods are direct and rational, but fail to consider the inherent statistical errors of the model, which may cause unimportant features to have significant influences to final prediction. Such an error is a common phenomena which can be explained by a simple example in multiple linear regression: if a predictor xj has no linear correlation with the label y, we may still falsely predict a coefficient \\( \\beta_j \\neq 0 \\) with xj due to multiple reasons. This is characterized as \"False Discovery\" or \"Type I error\" in statistical tests, and the expected value of the proportion of false discoveries relative to all discoveries (i.e. reject the null) is defined as the False Discovery Rate (FDR).\nTo construct a feature selection method which effectively controls the statistical errors, we need an effective approach to estimate and thus control the FDR. The method to control FDR was first proposed by Benjamini and Hochberg [5] by comparing p-values of multiple testing, which has been extensively studied and proven to be effective on linear models. However, in nonlinear models it is difficult to construct valid p-values for testing, since the distribution of the test statistics is usually unclear. Using Bootstrap or other methods to approximate p-values is valid but often impractical because of complex model and data structures or high computational costs.\nTo conquer such problems, Ke et al.[23] introduced a new generalized framework for effective FDR control algorithm, which can be summarized to three parts:\n(1) Construct an importance metric for each input feature and rank each feature by its importance\n(2) Create a tampered design matrix by adding fake variables."}, {"title": "2 Methods", "content": "(3) Calculate the feature importance of each fake variable in the design matrix which corresponds to each input feature, and convert the results into a symmetric test statistic which represents each input feature. The test statistic should be symmetric about zero for null variables (\\(\\beta_j = 0\\)) and take a large positive value for non-null variables (\\(\\beta_j \\neq 0\\)). In other words, the input feature should be robust to perturbations added in the fake variables and remain significant under the test statistic to be considered a significant feature.\nThere are several methods that effectively controls the FDR under this framework. Barber et al. [2] introduced the \"Knockoff Filter\", which creates a \"fake copy\" of the feature matrix which maintains the original covariance structure but is independent of the responses. Comparing the inference results of the original features and the knockoff provides a valid selection of features while controlling the FDR. Candes et al. [8] proposed the Model-X Knockoff to extend this method to more complex models, and Lu et al. [26] used this model to control FDR in Deep Neural Networks. The Model-X Knockoff method is valid, but requires the distribution of inputs to be either exact or be within a distribution family that possesses simple sufficient statistics. Such assumption is not sufficient in many practical applications including medicine and finance. To propose a more general solution, Xing et al. [42] introduced the Gaussian Mirror algorithm, which measures the validity of each feature by adding a Gaussian noise to each input feature and analyzing its disturbance to the importance of each feature. The features are considered to be \"Really Significant\" if the change in its importance is smaller than a given threshold.\nThe Gaussian Mirror method is more reasonable and generalizable, but there still exist several problems. The first problem appears in the feature importance metric. The method they proposed to calculate feature importance is to track all weights in each neuron and sum over all path weights that connect the neurons. This method is a natural extension of the partial derivative method as proposed by Hechtlinger. These methods assume that we can approximate the contribution of the feature xj to the output y by holding other variables constant and adding a small perturbation \\(\\Delta x_j\\) to calculate the changes in output \\(\\Delta y\\). However, the assumption that \"all other input features remain constant\" is inappropriate because it assumes all other features are uncorrelated and orthogonal to xj and thus do not change with xj, which nearly impossible in real data.\nTo make such importance metric a practical and global measure, we introduce SHAP [28], which measures the input feature importance based on Shapley Values from game theory. It measures the influence of each feature to the output by computing the weighted average of the \"Contribution Margin\" of the feature to every possible combination of other features. This value accounts for the changes in all input variables and effectively considers their correlation. We take the derivative of the SHAP values of each feature as the measure of its importance, which can be interpreted as a \"path derivative\" which orients the direction xj changes with other variables. This value provides a interpretable and global measure of the input feature importance, and is essentially useful in complex neural networks where the fitted function may differ significantly in different directions, such as CNN, LSTM, and"}, {"title": "2.1 LSTM", "content": "Transformers. In addition, we represent the importance of each feature xj as a vector with each entry corresponding to each value of xj, to quantify the change in the model's response to the feature following the change of feature values.\nAnother problem of the Neural Gaussian Mirror method appears in its construction of tampered design matrix. The construction of fake variables in this method aims to ensure that the distribution of theses variables are uncorrelated, to avoid the instability of weight matrices in training due to multicollinearity, which will cause the feature importance measures to be very unstable and useless. To capture the possible non-linear correlations between variables which may lead to \"Multicollinearity\" in nonlinear models, Xing et al. proposed a kernel-based dependence metric, which maps the variables into high-dimensional space through kernel functions and capture its high-dimensional correlation. This method works well in MLP models, but it fails to consider the possible time-series correlation which may cause instability in training of time-series model or similar model that requires capturing the relationships between current and past observations, including models in the RNN-category (RNN, LSTM, etc.) and models with Attention mechanisms (Transformers, etc). To conquer such possible failure, we extend the Kernel-based Dependence Measure to consider the time-series cross-correlation of input features, to ensure the feature importance metric is robust in the training process.\nIn this paper, we introduce CatNet, an algorithm which modifies the Neural Gaussian Mirror method for FDR control in LSTM models for effective time-series prediction. We use SHAP values to measure feature importance and extended kernel method to construct the tampered design matrix. In addition, we use an extended version of the \"signed max\" of feature importance vector as a symmetric mirror statistic for effective FDR control. To test our algorithm's effectiveness and robustness, we experiment our algorithm with simulated data in both linear models and LSTM models. In linear models, we test CatNet's performance in both low-dimensional (\\(p < n\\)) and high-dimensional (\\(p > n\\)) cases and compare it with the original Gaussian Mirror method. The algorithm effectively controls the FDR under the preset level and maintains a very high statistical power. We then test CatNet's performance in different LSTM models by using different link functions demonstrating different nonlinear relations of the feature and the response. The algorithm still maintains a high power and controls the FDR under different dimensions and link functions. This displays that our algorithm is robust and generalizable in different time-series prediction models. To evaluate our algorithm's performance in real data, we construct a multi-factor portfolio for predicting the price of S&P 500 component stocks. We classify the factors into different categories and run the algorithm to select the significant factors. The simulation results demonstrate that the combined model with selected features make more accurate prediction compared to LSTM without feature selection and other common methods. Our method can detect abnormal macro-economic fluctuations and can generalize to other stocks not in our training set. We also conduct analysis of our selected factors to interpret the drivers of the stock market, showing that the method improves our understanding of the financial market and helps us make better informed future predictions."}, {"title": "2.2 Gaussian Mirror for FDR Control", "content": "Long Short-Term Memory (LSTM), first proposed by Hochreiter and Schmidhuber [20], is an advanced Recurrent Neural Network aiming to tackle the gradient vanishing or exploding problem in RNNs. It has been widely proven to be effective in time-series predictions, as its structure is designed to manage time-series memory.\nConsider a linear regression model \\(y = X\\beta + \\epsilon\\) where \\( \\beta = {\\beta_1, \\beta_2,\\cdots, \\beta_p} \\) and X is the input matrix with n \u00d7 p dimensions. We want to test p hypotheses; \\(H_j : \\beta_j = 0\\) for j = 1,...,p and find a rule to determine which hypotheses should be rejected. Let \\(S_0 \\subset {1,2,\\dots,p}\\) be the set of predictors with \\(\\beta_i = 0\\) and \\(S_1\\) be the set of effective predictors. We set \\(\\hat{S_1}\\) be the set of selected effective predictors (i.e. reject the null hypothesis) by our statistical model. The FDR is defined as the expected value of the proportion of Type I error:\n\\[FDR = E[FDP], \\text{ where } FDP = \\frac{\\#\\{i | i \\in S_0, i \\in \\hat{S}_1\\}}{\\#\\{i | i \\in \\hat{S}_1\\} \\vee 1}\\]\nXing et al. [42] proposed the Gaussian Mirror method for controlling the FDR in linear regression models. For a given model \\(y = X\\beta + \\epsilon\\), for each feature xj, we construct"}, {"title": "2.3 Feature Importance Vector for Mirror Statistic", "content": "it = \u03c3(Wiixt + bii + Whiht\u22121 + bhi)\nft = \u03c3(Wifxt + bif + Whfht\u22121 + bhf)\ngt = tanh(Wigxt + big + Whght-1+bhg)\not = \u03c3(Wioxt + bio + Whoht\u22121 + bho)\ngt\nCt = ft \u00a9 Ct\u22121 + it\nht = ot tanh(ct)\nwhere ht is the hidden state at time t, ct is the cell state at time t, xt is the input at time t, ht-1 is the hidden state of the layer at time t \u2013 1 or the initial hidden state at time 0. it, ft, gt, ot are the input, forget, cell, and output gates. o is the sigmoid function, and \u00a9 is the Hadamard product.\nThe memory cell ct is the core of LSTM. It is the \"memory\" of previous input information stored in the model weights. The forget gate controls whether to retain or update the memory. The input gate controls whether to add current input into memory, and the output gate controls whether to add the output into hidden state. The output, or the predicted label of i at time t is denoted as:\n\\[\\hat{y_i} = Softmax(h_tW_{fc})\\]\nwhere Wfc is the matrix transforming the hidden state into the output.\nLSTM has been widely used in time-series prediction tasks such as stock price predictions. In this paper, we construct the FDR control method which is designed for LSTMs but can also be generalized in more complex time-series prediction models.\nThe above method provides a basic framework of the Gaussian Mirror algorithm: (a) Add uncorrelated perturbation features for evaluating robustness, (b) Use a feature importance metric to rank features, (c) Construct the mirror statistic for feature selection. In later literature, we will examine each part and make proper improvements to extend it to general neural network models.\nWe first start with the discussion for Part (c). In the previous literature, the feature importance metric, such as \\(\\beta_j\\) in linear models, is represented by a scalar. It is under the assumption that the contribution of the j-th feature to the model outcome remains constant, and the specific values of the j-th feature do not alter its influence to the output. This assumption holds true for linear models, where the relationship between features and model output does not change relative to feature values. However, for more complex nonlinear models such as neural networks, the importance of a feature may vary depending on its input value, and a scalar value may no longer be sufficient to measure the global contribution of a feature. By such intuition, we introduce a vector-formed importance metric to accurately capture the change in feature importance through the input space."}, {"title": "2.4 SHAP Derivatives for Feature Importance", "content": "the mirror variables \\(x_j^{+} = x_j + c_jz_j\\) and \\(x_j^{-} = x_j - c_jz_j\\), where \\(z_j \\sim N(0, I_n)\\) is an independently and identically distributed standard Gaussian random vector.\nA key factor in constructing the Gaussian Mirror is to compute \\(c_j\\) to make the correlation of \\(x_j^{+}\\) and \\(x_j^{-}\\) closest to zero (given the other variables \\(X_{-j}\\), which represents the feature matrix X subtracted by the the j-th column \\(x_j\\)). If we did not set the correlation equal or close to zero, \\(x_j^{+}\\) and \\(x_j^{-}\\) will follow very similar distributions and their coefficients will be very unstable due to multi-collinearity. However, we hope that the regression coefficients of \\(x_j^{+}\\) and \\(x_j^{-}\\) are independent and thus can reflect the robustness of the original feature \\(x_j\\) subject to perturbations.\nIn a low-dimensional linear model (p < n) and use ordinary least squares (OLS) for estimation, we can get an explicit expression of \\(c_j\\) to make the correlation equal to zero:\n\\[c_j = \\frac{x_j^T(I_n \u2013 X_{-j}(X_{-j}^TX_{-j})^{-1}X_{-j}^T)z_j}{\\sqrt{z_j^T(I_n \u2013 X_{-j}(X_{-j}^TX_{-j})^{-1}X_{-j}^T)z_j}}\\]\nThen we construct the mirror statistic\n\\[M_j = |\\hat{\\beta_j^+} + \\hat{\\beta_j^-}| \u2013 |\\hat{\\beta_j^+} - \\hat{\\beta_j^-}|\\]\nWe construct Mj so that, under the null hypothesis \\(\\beta_j = 0\\), by choosing the appropriate \\(c_j\\), it is proven [42] that the distribution of Mj is symmetric about zero, that is,\n\\[\\#\\{j \\in S_0 | M_j > t\\} \\approx \\#\\{j \\in S_0 | M_j \\leq -t\\}\\]\nBased on this property, the false discovery proportion (FDP) can be estimated as\n\\[FDP(t) = \\frac{\\#\\{j : M_j \\leq -t\\}}{\\#\\{j : M_j \\geq t\\} \\vee 1}\\]\nIn addition, [8] explained that the above formula is a slightly biased estimate of the FDP. For a more accurate estimate, we need to add the numerator by 1. Thus we estimate the FDP as\n\\[FDP(t) = \\frac{\\#\\{j : M_j \\leq -t\\} + 1}{\\#\\{j : M_j \\geq t\\}}\\]\n, and then a data-adaptive threshold\n\\[\\tau_q = min\\{t > 0 : FDP(t) \\leq q\\}\\]\nis chosen to control the FDR at a preset level q."}, {"title": "2.4.1 SHAP Values", "content": "\u04d8\u03a6\nj\nj\nIt can be easily proved that in linear regression models, the expected value of \\( \\frac{\\partial \\Phi_j}{\\partial x_j} \\) across all t equals the regression coefficient Bj (See Appendix B). So this measure is also a natural extension of regression coefficients.\nIn real scenarios, the SHAP values \u00dej of xj in all times t will be noised due to the intrinsic random noise in xj. So we need to use a smooth function to fit the scatterplot of \u03a6\u2081 relative to xj and take the slope of the fitted function in each input value x, to minimize the impact of random noises. In practice, we find Lowess smoothing sufficient for fitting this function and get the desired result.\nThen we can construct the feature importance vector and the mirror statistic as noted before:\nL(xj)1xn = [\\(\\phi_j^{1-},  \\phi_j^{2-}, ..., \\phi_j^{n-}\\)], L(x+j)1xn = [\\(\\phi_j^{1+},  \\phi_j^{2+}, ..., \\phi_j^{n+}\\)]\n\\[M_j^{sgn} = \\frac{(L(x_j^+), L(x_j^-))}{||L(x_j^+)||_2 * ||L(x_j^-)||_2} (||L(x_j^+)||_1 \\vee ||L(x_j^-)||_1)\\]\nFor a null feature j, the expectation of the SHAP value in all values of xj is zero (See Property 2 in Appendix A). So the derivatives of the smoothed function SHAP values should be a small value randomly distributed around zero, and the mirror statistic should be equally distributed around zero. For non-null features, the absolute value of the derivatives should be large and the corresponding mirror statistic will take a large positive value. This fulfills the requirement for the mirror statistic in the Gaussian Mirror algorithm for an effective FDR control."}, {"title": "2.4.2 Feature Importance Metric Based on SHAP", "content": "SHAP (SHapley Additive exPlanations), first introduced by Lundberg et al. [28] is a unified framework for interpreting machine learning model predictions by leveraging concepts from cooperative game theory. SHAP is rooted in the foundational work of Lloyd Shapley, who introduced the Shapley value in 1953 as a solution concept for the fair distribution of payoffs among players in a cooperative game. The Shapley value for a feature j is defined as:\n\\[\\Phi_j = \\sum_{S \\subset N\\{j\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} [v(S\\cup \\{j\\}) \u2013 v(S)]\\]\nwhere N is the set of all features. S is a subset of features not containing j. v(S) is the value function representing the model's prediction using the feature subset S. The term \\(\\frac{|S|! (|N|-|S|-1)!}{|N|!}\\) serves as the weight factor for each coalition S.\nSHAP extends the concept of Shapley values to machine learning by treating the prediction task as a cooperative game where each feature is a contributing player. Its value for each feature quantifies its contribution to the difference between the actual prediction and the average prediction. The SHAP value for a feature j is defined to be the same as its Shapley value, where the value function v(S) is given by:\n\\[v(S) = E_{X\\backslash S} [f (x_s \\cup X \\backslash S)] \u2013 E [f(x)]\\]\nHere, xs represents the feature values in subset S for instance i, and X \\ S denotes the random variables for the features not in S.\nFormally, the value function is defined using integral:\n\\[\u03c5_{f,x^{(i)}} (S) = \\int f (x_S \\cup X_C) dP(X_C) \u2013 E [f(x)]\\]\nFor the coalition S, the feature values are set to those of the instance being explained, while the features not in S are treated as random variables sampled from their distribution P(Xc).\nSubstitute the value function v(S), we get the expression for the SHAP value:\n\\[\\Phi_j = \\sum_{S \\subset \\{1,...,p\\} \\backslash \\{j\\}} \\frac{|S|! (p \u2013 |S| \u2212 1)!}{p!} (v(S\\cup \\{j\\}) \u2013 v(S))\\]\nwhere the marginal contribution of feature j to coalition S is defined as:\n\\[v(S\\cup\\{j\\}) - v(S) = \\int \\int (x^{(i)}_{S\\cup\\{j\\}} \\cup X_{C\\backslash\\{j\\}}) dP (X_{C\\backslash\\{j\\}}) - \\int f (x_{X_{S}}) \\cup X_{C}) dP(X_{C})\\]"}, {"title": "2.5 Kernel-Based Dependence Measure", "content": "\\frac{\\partial\\hat{y}}{\\partial x_j};\nWe now continue with Part (b). A common method to measure the contribution of the input features to the output is to take the partial derivative of the predicted output \\(\\hat{y}\\) relative to the feature xj: \\(\\phi_j = \\) as proposed by Hechtlinger [19]. Specifically for MLP models, Xing et al. [41] introduced a feature importance metric by tracking all paths that links the input feature to the output in the model and then summing over all path weights, which is a natural extension of the partial derivatives method. The problem that appears in both methods is that they assume that all variables are orthogonal to each other (i.e. completely uncorrelated), which is not true in nearly all real scenarios. Therefore, we need a measure that accounts for the change of other variables following the change on one input variable. In other words, we want to find the \"path\" that each input x; moves in the feature space and take the derivative of the output on such a path. By this intuition, we come with SHAP values.\nIn this part, we consider the construction of the mirror variables \\(x_j^{+}\\) and \\(x_j^{-}\\) in Part (a). The reason that we need the distribution of these two variables uncorrelated can be simply explained by a linear regression example. Consider a simple linear regression model \\(y = \\beta_0x_0 + \\epsilon\\). If we take two variables x1 and x2 to be exactly from the same distribution as x0 and regress y on these two variables: \\(y = \\beta_1x_1 + \\beta_2x_2 + \\epsilon\\), then theoretically \\(\\beta_1 + \\beta_2 = \\beta_0\\), and \\(\\beta_1, \\beta_2\\) can take any value that sum to \\(\\beta_0\\) since they are totally correlated. In other words, high multicollinearity of the two variables causes the regression coefficients to be highly correlated and unstable. In the Gaussian Mirror algorithm, if we make \\(x_j^{+}\\) and \\(x_j^{-}\\) highly correlated, the weights of the fitted model will be highly unstable and may be totally different. Since we want to test the robustness of the input feature in one stable model, we need to avoid the model's intrinsic instability due to such multicollinearity issues.\nIn linear models, we can simply minimize the Pearson correlation coefficient of \\(x_j^{+}\\) and \\(x_j^{-}\\). However, non-linear models may capture the non-linear correlations between the variables"}, {"title": "2.6 CatNet", "content": "which the Pearson correlation coefficient cannot quantify. For example, if two variables x1 and x2 has x2 = sin(x1), their Pearson correlation coefficient may be very small, but Neural Networks are highly likely to capture such a correlation, causing highly unstable gradients in back-propagation and thus unstable weight matrices in the model. As a result, we need a dependence measure that can effectively capture the non-linear correlation between two variables.\nA natural thought is to map the feature values into high-dimensional space through a kernel function to capture the high-dimensional correlation, which corresponds to the non-linear correlations in the original feature space. Gretton et al. [17] proposed the Hilbert-Schmidt Independence Criterion (HSIC) which effectively quantifies this kernel-based independence measure:\n\\[HSIC(X, Y) = ||C_{XY} ||_{HS}^2\\]\nwhere Cxy is the covariance operator of X and Y in the Reduced Kernel Hilbert Space (RKHS) that the kernel function maps to, and \\(|| \u00b7 ||_{HS}\\) is the Hilbert-Schmidt norm.\nFor a sample \\(\\{(x_i, Y_i)\\}_{i=1}^n\\), the unbiased estimator of HSIC is:\n\\[HSIC_n = \\frac{1}{(n - 1)^2}tr(K \\tilde{L})\\\\ \\tilde{K} = HKH, \\tilde{L} = HLH\\]\nwhere K, L are the kernel matrices \\(K_{ij} = k(x_i, x_j)\\), \\(L_{ij} = l(y_i, y_j)\\), H is the centering matrix \\(H = I - \\frac{1}{n} 11^T \\) to standardize the kernel matrices and tr is the trace of the matrix.\nFor linear kernels, we can prove that HSIC is proportional to the square of Pearson correlation coefficient (See Appendix C). For non-linear kernels, HSIC can be viewed as extending the Pearson Correlation Coefficient into high-dimensional feature space to capture correlations unobserved in low-dimensional space.\nIn the Gaussian Mirror method, we need the mirror variables \\(x_j^{+}\\) and \\(x_j^{-}\\) to be independent conditional on x_j. To measure the conditional independence and save computational costs, Xing et al. [41] transformed the calculation of HSIC in Gaussian Mirrors to the following equivalent form:\n\\[HSIC = \\frac{1}{n^2} [(HK^U H) \\circ (HK^V H) \\circ ) \\circ K^W]_{++}\\]\nwhere \\([A]_{++} = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}\\) and \\(\\circ\\) is the Hadamard product. KU, Kv, Kw are the kernel matrices for \\(x_j^+, x_j^-, x_{-j}\\), respectively. The denominator \\(n^2\\) and \\((n - 1)^2\\) does not matter since we only need to find the minimum of HSIC in this case. This approach has an approximately O(n) complexity with parallel computing compared to the O(n\u00b2) complexity of the original formula for HSIC."}, {"title": "3 Numerical Simulations", "content": "Now we can summarize our CatNet algorithm, which efficiently selects the significant features in the LSTM model and controls the FDR under the preset level:\nto construct the mirror variables \\(x_j^{+} = x_j + c_jz_j\\) and \\(x_j^{-} = x_j - c_jz_j\\).\nIn practice, we compute the optimal cj by taking a uniform range of values of c within a relatively large interval, and calculate the corresponding values of I; to fit the function. Then we subtract the range where the minimum value of Ij(c) approximately exists, and use Cubic Spline Interpolation to fit the function of I;(c) within this range and find the minimum. We find that I; (c) is always a convex smooth function, allowing us to use interpolation to approximate the value of cj within the error bound of the fourth derivative of Ij(c).\n\n\nThough this algorithm is designed for LSTM model, we suppose it can be easily extended to other time-series prediction models such as RNN and GRU. It may also be generalized to Attention models with minor changes, since they both emphasizes the relation of past observations with current observation.\nTo increase computational efficiency, we also construct an algorithm which construct the mirror variable of the p input features at the same time. Denoted as Simultaneous-CatNet (S-CatNet). In later simulations, we demonstrate that S-CatNet also effectively controls the FDR and maintains a high power."}, {"title": "3.1 Linear Models", "content": "However, to extend the Gaussian Mirror algorithm into time-series models, such an HSIC value is not sufficient. The main problem is that this measure only considers the correlation between \\(x_1^t\\) and \\(x_2^t\\) for two variables x1,x2, but fails to capture their time-series cross-correlation. Avoiding such cross-correlation is essential in time-series models. Take a simple example in LSTM, if we set the look-back time of LSTM to be 60, then the model will calculate the weights that the past 60 inputs contribute to the current input. In this case, if we set \\(x_1^t = x_1^{t-1}\\), then suppose the weight matrices for all look-back times are:\n\\[W_1 = (w_{-60}^1, w_{-59}^1, ..., w_{-2}^1, w_{-1}^1) = (0, 0, ..., 0, 1)\\]\n\\[W_2 = (w_{-60}^2, w_{-59}^2, ..., w_{-2}^2, w_{-1}^2) = (0, 0, ..., 1, 0)\\]\nThen W\u2081 and W2 contributes the same to the model output. And training this model a second time may lead to completely different weights in W\u2081 and W2, similar to the multicollinearity issue in linear regression. To capture this correlation, we need to compute the HSIC value for a time-lapse \u0442:\n\\[HSIC_\\tau (X, Y) = \\frac{1}{(n - 1)^2}tr(K\\tilde{L}^{t-\\tau})\\]\nwhere \\(K_{ij} = k(x_i^t, x_j^t)\\), \\(L_{ij} = k(y_i^t, y_j^t)\\).\nLSTM considers the feature value of k lookback times, so we need to consider the HSIC value for all \u03c4 = {1,...,k}. We take the weighted sum of all HSIC values as the final Dependence Measure:\n\\[I = \\sum_{\\tau=0}^k w_\\tau HSIC_\\tau (X, Y) = \\sum_{\\tau=0}^k \\frac{w_\\tau}{(n - 1)^2}tr(K\\tilde{L}^{t-\\tau})\\]\nwhere \\(\\sum_{\\tau=0}^k w_\\tau = 1\\). The value with \u30f6 = 0 represents the normal non-time-series correlation between X and Y. We take the weighted sum because LSTM tends to consider values closer to current time over earlier values, so the correlation with earlier values are not so important. We can take the empirical formula \\(w_\\tau = a \\cdot exp(-\\frac{\\tau}{\\lambda})\\), where a is the factor that scales the sum to 1.\nThen we can transform this measure into the conditional dependence measure required for constructing the mirror variables in the Gaussian Mirror algorithm:\n\\[I_j (c) = \\sum_{\\tau=0}^k w_\\tau HSIC_\\tau (X, Y) = \\sum_{\\tau=0}^k \\frac{1}{n^2} w_\\tau [(HK^U H) \\circ (HK^V H) \\circ K^W]_{++}\\]\nAnd then we take cj to be the the argmin of the dependence measure:"}, {"title": "3.2 LSTM Models", "content": "To evaluate the effectiveness and robustness of CatNet, we consider its effect in both Linear Model and LSTM.\nFor linear models, we generate all input variables X from a standard Gaussian distribution N(0,1). We assume the input variables X and the response y follow a linear model: \\(y_i = \\beta^Tx_i + \\epsilon_i, \\epsilon_i \\stackrel{iid.}{\\sim} N(0, 1)\\). We randomly set k elements in \\(\\beta\\) to be nonzero and generate \\(\\beta\\) from \\(N(0, (20log(p)/n)^2)\\). The k variables with nonzero \u03b2 values are considered to be relevant features. By running our algorithm, we want to select the relevant features we set and reject all other features as null features since they can be seen as random noises.\nWe run the CatNet algorithm in different numbers of input features p and data length n. For the low-dimensional cases where p < n, we keep the proportion of relevant features and null features to be 1 : 4. For high-dimensional cases where p > n, we keep the proportion of relevant features and the data length n to be 1 : 10 to mimic the sparsity of significant features in high-dimensional regression. In cases where p > n, we first use LASSO to select the features as part of our algorithm, and then run the simple linear regression to construct the mirror statistic. We set the FDR level q = 0.1 and test the FDR and Power of our algorithm in p = 125 ~ 1500 and n = 250 ~ 3000. For each test, we repeat 30 times and take the mean value."}, {"title": "4 Application in Real World Stock Data", "content": "cj = arg min Ij (c)\nc\nto construct the mirror variables \\(x_j^{+} = x_j + c_jz_j\\) and \\(x_j^{-} = x_j - c_jz_j\\).\nIn practice, we compute the optimal cj"}]}