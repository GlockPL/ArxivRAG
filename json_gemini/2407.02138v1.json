{"title": "Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks", "authors": ["Wataru Hashimoto", "Hidetaka Kamigaito", "Taro Watanabe"], "abstract": "Trustworthy prediction in Deep Neural Networks (DNNs), including Pre-trained Language Models (PLMs) is important for safety-critical applications in the real world. However, DNNs often suffer from uncertainty estimation, such as miscalibration. In particular, approaches that require multiple stochastic inference can mitigate this problem, but the expensive cost of inference makes them impractical. In this study, we propose k-Nearest Neighbor Uncertainty Estimation (kNN-UE), which is an uncertainty estimation method that uses the distances from the neighbors and label-existence ratio of neighbors. Experiments on sentiment analysis, natural language inference, and named entity recognition show that our proposed method outperforms the baselines or recent density-based methods in confidence calibration, selective prediction, and out-of-distribution detection. Moreover, our analyses indicate that introducing dimension reduction or approximate nearest neighbor search inspired by recent kNN-LM studies reduces the inference overhead without significantly degrading estimation performance when combined them appropriately.", "sections": [{"title": "1 Introduction", "content": "In order to use Deep Neural Networks (DNNs) including Pre-trained Language Models in safety-critical regions, uncertainty estimation is important. By improving the predictive uncertainty, the prediction will be calibrated (Guo et al., 2017), or improve selective prediction performance, which is predictive performance when there is a choice to abstain from model prediction (Galil et al., 2023). On the other hand, DNNs often fail to quantify the predictive uncertainty, for example, causing miscalibrated prediction (Guo et al., 2017). Such uncertainty estimation performance problems can be mitigated by the PLMs, such as BERT (Devlin et al., 2019) or DeBERTa (He et al., 2021b), that are self-trained on large amounts of data (Ulmer et al., 2022), although, there is still a need for improvement (Desai and Durrett, 2020).\nTo solve the problem of uncertainty estimation, multiple probabilistic inferences such as MC Dropout (Gal and Ghahramani, 2016) and Deep Ensembles (Lakshminarayanan et al., 2017) are generally effective. On the other hand, these methods require multiple stochastic inferences for a single data instance, which leads to high computational cost, and makes them impractical for real world application. To obtain reasonable predictive uncertainty without multiple inferences, Temperature Scaling (Guo et al., 2017) is generally used, which scales logits with a temperature parameter. Furthermore, density-based methods such as Density Softmax (Bui and Liu, 2024a) and Density Aware Calibration (DAC) (Tomani et al., 2023), which correct the model outputs based on estimated density, have achieved promising very recent years in terms of uncertainty estimation performance and in-"}, {"title": "2 Related Work", "content": "Uncertainty Estimation Many prior studies on uncertainty estimation focus primarily on image classification. One approach for improved uncertainty estimation is the sampling-based approach of performing multiple stochastic predictions for one data instance. The approach includes MC Dropout (Gal and Ghahramani, 2016) and Deep Ensembles (Lakshminarayanan et al., 2017). However, their practical application is limited due to the computationally expensive inference. As an alternative to improve uncertainty, the sampling-free approaches do not demand multiple stochastic inferences, e.g., Temperature Scaling (Guo et al., 2017), Deep Deterministic Uncertainty (Mukhoti et al., 2023), and Posterior Network (Charpentier et al., 2020). Very recently, the density-based methods (Bui and Liu, 2024a; Tomani et al., 2023; Bui and Liu, 2024b) show good performance on latency and uncertainty estimation.\nk-Nearest Neighbor Language Models / Machine Translation k-Nearest Neighbor Language Model (kNN-LM) (Khandelwal et al., 2020) has been proposed, which performs linear interpolation of kNN probability based on distance from neighbors and base model probability, in the language modeling task. k-Nearest Neighbor Machine Translation (kNN-MT) applied the kNN-LM framework to machine translation (Khandelwal et al., 2021). kNN-LM and kNN-MT have been successful because they enhance predictive performance through the memorization and use of rich token representations of pre-trained language models and mitigate problems such as a sparsity comes from low-frequency tokens (Zhu et al., 2023). The main issue on kNN-LM and kNN-MT is the inference overhead, and there are several studies to solve this problem. He et al. (2021a) employs datastore compression, adaptive retrieval, and dimension reduction to reduce computational overhead with retaining perplexity. Deguchi et al. (2023) dramatically improves decoding speed by dynamically narrowing down the search area based on the source sentence.\nUncertainty Estimation for Natural Language Processing Tasks Studies about uncertainty estimation for NLP tasks are limited when compared with those for image datasets. Kotelevskii et al. (2022) has shown excellent performance in classification with rejection tasks and out-of-distribution detection tasks using uncertainty scores using density estimation results. Vazhentsev et al. (2022) performed misclassification detection using Determinantal point processes, spectral normalization, and loss regularization in text classification and NER. Vazhentsev et al. (2023) proposed the new uncertainty method which combined epistemic and aleatoric uncertainty estimation methods for text classification tasks. In this study, we comprehensively investigate the density-based methods such as Density Softmax and DAC in NLP tasks, inspired by the success of memorization and use in"}, {"title": "3 Base Methods", "content": "In this section, we describe the density-based methods for uncertainty estimation. We use the following notations: $z_i$ denotes the logits for class $i$, $p_i$ denotes the calibrated probability for class $i$, $D$ means the dimension of the final output layer, and $J$ denotes the number of classes."}, {"title": "3.1 Density Softmax", "content": "Density Softmax (Bui and Liu, 2024a) obtains confidence by weighting logits with normalized log-likelihood from a trained density estimator. In this study, we use RealNVP (Dinh et al., 2017) as the density estimator (details for the density estimator are in Appendix A). $\\beta$ is the parameters of the density estimator; $f(X)$ is the representation obtained from the input example $X$; $p(f(X); \\beta)$ is the normalized log-likelihood from the density estimator, then the corrected confidence is written as\n$$p(y|x) = \\frac{\\exp (p(f(X); \\beta)\\cdot z_i)}{\\Sigma_{j=1}^{J} \\exp (p(f(X); \\beta)\\cdot z_j)}.$$\nIn Density Softmax, the closer the normalized log-likelihood to zero, the closer the prediction to Uniform distribution (Bui and Liu, 2024a). Density Softmax achieves reasonable latency and competitive uncertainty performance with state-of-the-art methods at the cost of demanding the density estimator training and multiple base model training."}, {"title": "3.2 Density Aware Calibration (DAC)", "content": "DAC (Tomani et al., 2023) scales the logits by using sample-dependent temperature $\\Phi(X, w)$,\n$$P(y|x) = \\frac{\\exp (z_i/\\Phi(X, w))}{\\Sigma_{j=1}^{J} \\exp (z_j/\\Phi(X, w))}$$\nwhere\n$$\\Phi(X, \\omega) = \\Sigma_{l=1}^{L} w_l s_l + w_0.$$\n$W_1...W_L$ are the weights for every layer of the base model, $s_i$ is the averaged distance from kNN search on l-th layer, and $w_0$ is the bias term. $w_0...W_L$ are optimized using the L-BFGS-B method (Liu and Nocedal, 1989) based on the loss in the validation set. In the original DAC paper, the uncertainty estimation performance tends to improve with the increase in the number of layer's representation (Tomani et al., 2023). Therefore, we report the performance when all the hidden representations in each layer of the base PLMs are used.\nDAC avoids the problem of the high training cost in Density Softmax. However, both DAC and Density Softmax use only the density of the training data representations for uncertainty estimation. Considering that the uncertainty is mainly composed of epistemic uncertainty and aleatoric uncertainty, DAC and Density Softmax only represent the epistemic uncertainty because they use only the distance from neighbors or the data likelihood. Therefore, the uncertainty estimates could be further improved by introducing the concept of aleatoric uncertainty."}, {"title": "4 Proposed Method: k-Nearest Neighbor Uncertainty Estimation (kNN-UE)", "content": "Our proposed method, kNN-UE, is based on the intuition that a model should give high confidence only when the predicted instance exists in the region with high density of training data and a high proportion of the model's predicted label exists in the neighbors sharing the same label, as illustrated in Figure 1. To represent this intuition, we introduce a scoring method based on kNN and labels of the neighbors in the following formula:\n$$P(y_i|x) = \\frac{\\exp(W_{kNN}(\\hat{y}) \\cdot z_i)}{\\Sigma_{j=1}^{J} \\exp(W_{kNN}(\\hat{y}) \\cdot z_j)}$$\nwhere\n$$W_{kNN}(\\hat{y}) = \\alpha \\cdot \\exp(-\\frac{d_k}{\\tau})$$\n$$\\frac{distance term}{+\\frac{S(\\hat{y})}{K} + b}.$$\nlabel term\nK is the number of neighbors from kNN search, S(y) is the number of examples where the predicted label $\\hat{y}$ and the label of the neighbors are same, $d_k$ is the distance between the k-th $f(X)$ representations obtained by kNN search and the representations of training data. The parameters $\\alpha$, $\\tau$, and"}, {"title": "5 Experimental Settings", "content": "We measure the uncertainty estimation performance on Sentiment Analysis (SA), Natural Language Inference (NLI), and Named Entity Recognition (NER) in In-domain (ID) and Out-of-Domain settings. Dataset statistics are described in Table 1.\nSentiment Analysis (SA) is a task to classify whether the text sentiment is positive or negative. The IMDb movie review dataset (Maas et al., 2011) is treated as ID, and the Yelp restaurant review dataset (Zhang et al., 2015) is treated as OOD.\nNatural Language Inference (NLI) classifies the relationship between a hypothesis sentence and a premise sentence. We treat the Multi-Genre Natural Language Inference (MNLI) dataset (Williams et al., 2018) as ID and the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) as OOD.\nNamed Entity Recognition (NER) extracts the named entities, such as a person, organization, or location. The NER task was carried out in the framework of sequence labeling. We regard the OntoNotes 5.0 dataset (Pradhan et al., 2013) broadcast news (bn) domain as ID, and newswire (nw) and telephone conversation (tc) domains as OOD."}, {"title": "5.2 Existing Methods", "content": "Softmax Response (SR) is a trivial baseline, which treats the maximum score from output of the base model's softmax layer as the confidence (Cordella et al., 1995).\nTemperature Scaling (TS) is a calibration technique by which the logits (i.e., the outputs of the final layer before the softmax) are divided by a temperature parameter $T$ before applying the softmax function (Guo et al., 2017). We optimized $T$ by L-BFGS on dev set loss.\nLabel Smoothing (LS) is the calibration and generalization technique by introducing a small degree of uncertainty $\\epsilon$ in the target labels during training (Miller et al., 1996; Pereyra et al., 2017). In LS, we optimized $\\epsilon \\in \\{0.01, 0.05, 0.1, 0.2, 0.3\\}$ by using dev set accuracy when SA and NLI, and dev set F1 when NER.\nMonte-Carlo Dropout (MC Dropout) is a regularization technique that be used for estimating uncertainty in neural networks (Gal and Ghahramani, 2016). This approach involves executing the model $M$ times with varying dropout masks and computing the mean softmax output across these runs. In our experiments, we set $M = 20$ for all evaluations, and the dropout rate is 0.1."}, {"title": "5.3 Training Setting", "content": "In all experiments, we train and evaluate the models on a single NVIDIA A100 GPU with 40GB of memory. We used DeBERTaV3BASE\u00b3 and mDeBERTaV3BASE4 (He et al., 2023), as the transformer encoder from transformers (Wolf et al., 2020) pre-trained model checkpoints. Cross-entropy loss is minimized by AdamW (Loshchilov and Hutter, 2019) with a linear scheduler (Goyal et al., 2017). The batch size is 32, and gradient clipping is applied with the maximum norm of 1. The initial learning rate was set to 1e-5. All experiments are run five times, and we report the mean and standard deviation of the scores.\nDatastore Construction It is necessary to preserve the representation of the data for training a density estimator in Density Softmax and kNN search in DAC and KNN-UE. We maintain final layer representations corresponding to CLS tokens"}, {"title": "6 Results", "content": "We present the uncertainty estimation results for sentiment analysis. KNN-UE consistently outperforms existing methods in terms of ECE, MCE, and E-AURC. In AUROC, LS outperforms in OOD setting, but KNN-UE outperforms existing methods in ID setting. Furthermore, the proposed method clearly outperforms DAC that uses an ensemble of neighbor search results for each hidden representation, by adding the label term."}, {"title": "6.2 Natural Language Inference", "content": "In NLI, we evaluate the uncertainty estimation performance (calibration and selective prediction) and the out-of-distribution detection performance."}, {"title": "6.2.1 Uncertainty Estimation", "content": "First, we show the results of in-distribution and out-of-distribution uncertainty estimation in the NLI task using the DeBERTaV3 model. Similar to Section 6.1, kNN-UE shows a significant improvement in ECE, MCE and E-AURC. Furthermore, in AUROC, kNN-UE outperforms existing methods in both ID and OOD settings. Galil et al. (2023) have reported that improving calibration performance does not necessarily lead to improving selective prediction performance, but our proposed method improves both. On the other hand, the degree of improvement is greater for calibration performance. Specifically, the largest improvement is obtained on SNLI, where kNN-UE reduces MCE by more than 31.49% pt compared to SR."}, {"title": "6.2.2 Out-of-Distribution Detection", "content": "Following the previous study (Tomani et al., 2023), we carried out the experiments in the out-of-distribution detection task. Out-of-distribution de-"}, {"title": "6.3 Named Entity Recognition", "content": "In order to evaluate NLP tasks other than simple multi-class classification, we evaluate the effectiveness of the proposed method for uncertainty estimation in NER. Since NER focuses on entities, it is necessary to obtain the confidence of the entity. In this research, we use the product of the confidence of the tokens that construct the entity as the confidence level of the entity."}, {"title": "6.4 Case Study: Effects of the Label Term in kNN-UE for a Misclassified Example", "content": "Table 6 shows SR and kNN-UE confidences, and $S(\\hat{y})$ in kNN-UE for a misclassified example. In this case, SR and kNN-UE make incorrect prediction even though the true label is negative. However, the confidence is appropriately reduced by including the distances from the neighbors in kNN-UE, compared to SR. Moreover, by using the information that there are only 11 examples in K = 32 neighbors with the same label as the predicted label among the neighbors obtained by kNN search, our KNN-UE shows that the confidence is further reduced."}, {"title": "7 Analyzes", "content": "To understand the behavior of kNN-UE, we evaluated the performance in uncertainty estimation when changing the number of neighbors K$\\in$ {8, 16, 32, 64, 128} during kNN execution.\nNER, the smaller K, the better uncertainty estimation tends to be. Since our method averages the distance to the top K examples, logits are scaled"}, {"title": "7.2 Impact of Efficient Nearest Neighbor Search Techniques", "content": "In this subsection, we investigate the inference time and uncertainty estimation performance when applying approximate nearest neighbor search techniques and dimension reduction when executing kNN search in kNN-UE. As shown in Table 7, in the sequence labeling based NER that requires the kNN search execution per token, it takes twice as much inference time as SR. On the other hand, in kNN-LM (Khandelwal et al., 2020), dimension reduction and approximate kNN search techniques are effective to improve inference speed while maintaining perplexity (He et al., 2021a; Xu et al., 2023). Therefore, inspired by these works for faster kNN-LM, we investigate how the approximate nearest neighbor search techniques, such as Product Quantization (J\u00e9gou et al., 2011) or clustering, and dimension reduction affect the uncertainty estimation and inference speed of our proposed method: kNN-UE.\nProduct Quantization Product Quantization (PQ) (J\u00e9gou et al., 2011) is a data compression technique based on vector quantization. In PQ, a D-dimensional representation is divided into $N_{sub}$ subvectors and quantized by performing k-means clustering on the vectors in each subspace. Vector quantization can significantly reduce the amount of memory occupied by vectors. In addition, by calculating the distance between compressed PQ codes, we can efficiently calculate the estimated value of the original Euclidean distance.\nClustering The original kNN-LM uses an inverted file index (IVF) technique that speeds up the search by dividing the representation into $N_{list}$ clusters by k-means and searching for neighbors based on $N_{probe}$ centroids. In this study, we evaluate the uncertainty estimation performance and inference speed when the number of clusters $N_{list}$ = 100.\nDimension Reduction In general, Transformer-based models such as PLM have high-dimensional token representations. In high-dimensional spaces, nearest neighbor search often suffer from the curse of dimensionality. To reduce this problem, we apply dimension reduction to kNN-UE similar to He et al. (2021a). In this study, we use Principal Component Analysis (PCA) as a dimension reduction algorithm to reduce the dimension of the datastore"}, {"title": "8 Conclusion", "content": "In this paper, we proposed kNN-UE, which estimates uncertainty by using the distance to neighbors and labels of neighbors. The experimental results showed that our method showed higher uncertainty estimation performance than existing uncertainty estimation methods in SA, NLI and NER. Furthermore, we showed that the uncertainty estimation performance can be further improved by adjusting the number of neighbors $K$ in kNN-UE. Our method can greatly improve uncertainty estimation performance, especially in text classification tasks, with little degrading in inference speed. On the other hand, to address the degradation of the inference speed in token-level tasks such as NER, we investigated the effects of efficient neighbor search techniques in kNN-UE. As a result, we found that product quantization, clustering, or dimension reduction improves inference speed without degrading the uncertainty estimation much more, unless combining all of them simultaneously."}, {"title": "9 Limitations", "content": "In this study, we focused only on the classification-based tasks. On the other hand, taking advantage of the recent growth of Large Language Models, uncertainty estimation in text generation is also attracting attention (Fadeeva et al., 2023; Lin et al., 2024). Therefore, to investigate the effectiveness of kNN-UE in text generation tasks is an interesting direction for future research. Furthermore, although kNN-UE only used the representation of the last layer of the base model, exploring for an appropriate representation for uncertainty estimation is a future challenge."}, {"title": "Ethical Considerations", "content": "In this study, we used existing datasets that have cleared ethical issues following policies of published conferences. Therefore, they do not introduce any ethical problems. On the other hand, we have an ethical consideration about uncertainty estimation. Specifically, decision support systems with machine learning algorithms do not necessarily have a positive effect on performance. Jacobs et al. (2021) showed that collaboration with machine learning models does not significantly improve clinician's treatment selection performance, and that performance is significantly degraded due to the presentation of incorrect recommendations. This problem is expected to remain even if uncertainty estimation methods are applied to machine learning models. In addition, introducing uncertainty estimation methods could conversely lead humans to give overconfidence in machine learning models, resulting in performance degradation."}, {"title": "A Training Settings for Density Estimator in Density Softmax", "content": "In Density Softmax (Bui and Liu, 2024a), we use RealNVP (Dinh et al., 2017) which has two coupling structures. shows the hyperparameters for training RealNVP as the density estimator in Density Softmax."}, {"title": "B Details of Evaluation Metrics", "content": "Expected Calibration Error (ECE) ECE (Naeini et al., 2015) quantifies the difference between the accuracy and confidence of a model. Formally, ECE is expressed as:\n$$ECE = \\Sigma_{b=1}^{B} \\frac{|D_b|}{n} |acc(D_b) - conf(D_b)|$$\nwhere B is the number of confidence interval bins, $D_b$ denotes the set of examples with predicted confidence scores in the b-th bin, n is the total number of examples, acc($D_b$) is the accuracy of the model on the examples in $D_b$, and conf($D_b$) is the average confidence of the model on the examples in $D_b$. In this study, we use B = 10.\nMaximum Calibration Error (MCE) MCE, as detailed by Naeini et al. (2015) measures the maximum difference between the model's accuracy and the confidence across variouts confidence levels. MCE is defined as:\n$$MCE = max_{b=1}^{B} |acc(D_b) - conf(D_b)|,$$\nA lower MCE means that there is a small risk that the confidence of the model's prediction will deviate greatly from the actual correct answer. In this study, we use B = 10, same as ECE.\nArea Under the Risk-Coverage curve (AURC) The AURC is the area of the risk-coverage curve when the confidence levels of the forecasts corresponding to the n data points are sorted in descending order. The larger the area, the lower the error rate corresponding to a higher confidence level, which means that the output confidence level is more appropriate. Formally, AURC is defined as:\n$$AURC = \\Sigma_{i=1}^{n} \\frac{\\Sigma_{i=1}^{n} g(x_i)}{ixn}$$\nwhere g(x) returns 1 if the prediction is wrong and 0 otherwise.\nExcess-Area Under the Risk-Coverage curve (E-AURC) E-AURC (Geifman et al., 2019) is a measure of the AURC score normalized by the smallest risk-coverage curve area $\\text{AURC}^* \\approx \\hat{\\gamma} + (1 - \\hat{\\gamma}) ln(1 - \\hat{\\gamma})$, where $\\hat{\\gamma}$ is the error rate of the model. The reason for normalizing the AURC is that the AURC depends on the predictive performance of the model and allows for performance comparisons of confidence across different models and training methods. E-AURC is defined as:\n$$E-AURC = AURC - AURC^*$$\nE-AURC scores are reported with multiplying by 1,000 due to visibility."}, {"title": "C Each Result of Product Quantization, Clustering, and Dimension Reduction", "content": "We evaluated uncertainty estimation performance and inference time when the number of clusters in the codebook was fixed at 32, and the number of subvectors was changed to $N_{sub}$ $ \\in$ {16, 32, 64}.\nWe evaluate the uncertainty estimation performance and inference speed when the number of clusters $N_{list}$ = 100 is fixed and number of cluster centroids to search changes$N_{probe}$ $ \\in$ {8, 16, 32, 64}."}, {"title": "C.3 Dimension Reduction", "content": "As shown in Table 13, the uncertainty estimation performance depends on the number of target dimension, and the performance degrades when $D_{pca}$ = 64 or $D_{pca}$ = 128. On the other hand, the performance in $D_{pca}$ = 256 is almost the same as default kNN-UE. This is because while dimension reduction mitigates the problem of high dimensionality, the failure to extract adjacent examples by kNN search when applying PCA is less likely compared to PQ and clustering if the target dimension in dimension reduction is not reduced too much."}, {"title": "D Distance Recomputation for kNN-UE", "content": "When using efficient kNN search techniques in Section 7.2, we use approximate distances to compute Eq. 5. Although we can get raw vectors by using the example indices obtained from approximate nearest neighbor search and compute accurate distance, in kNN-LM this has been shown to lead to performance gains and latency degradation (He et al., 2021a). We measure the uncertainty estima-"}]}