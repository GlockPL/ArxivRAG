{"title": "Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks", "authors": ["Wataru Hashimoto", "Hidetaka Kamigaito", "Taro Watanabe"], "abstract": "Trustworthy prediction in Deep Neural Networks (DNNs), including Pre-trained Language Models (PLMs) is important for safety-critical applications in the real world. However, DNNs often suffer from uncertainty estimation, such as miscalibration. In particular, approaches that require multiple stochastic inference can mitigate this problem, but the expensive cost of inference makes them impractical. In this study, we propose k-Nearest Neighbor Uncertainty Estimation (kNN-UE), which is an uncertainty estimation method that uses the distances from the neighbors and label-existence ratio of neighbors. Experiments on sentiment analysis, natural language inference, and named entity recognition show that our proposed method outperforms the baselines or recent density-based methods in confidence calibration, selective prediction, and out-of-distribution detection. Moreover, our analyses indicate that introducing dimension reduction or approximate nearest neighbor search inspired by recent kNN-LM studies reduces the inference overhead without significantly degrading estimation performance when combined them appropriately.", "sections": [{"title": "1 Introduction", "content": "In order to use Deep Neural Networks (DNNs) including Pre-trained Language Models in safety-critical regions, uncertainty estimation is important. By improving the predictive uncertainty, the prediction will be calibrated (Guo et al., 2017), or improve selective prediction performance, which is predictive performance when there is a choice to abstain from model prediction (Galil et al., 2023). On the other hand, DNNs often fail to quantify the predictive uncertainty, for example, causing miscalibrated prediction (Guo et al., 2017). Such uncertainty estimation performance problems can be mitigated by the PLMs, such as BERT (Devlin"}, {"title": "2 Related Work", "content": "Many prior studies on uncertainty estimation focus primarily on image classification. One approach for improved uncertainty estimation is the sampling-based approach of performing multiple stochastic predictions for one data instance. The approach includes MC Dropout (Gal and Ghahramani, 2016) and Deep Ensembles (Lakshminarayanan et al., 2017). However, their practical application is limited due to the computationally expensive inference. As an alternative to improve uncertainty, the sampling-free approaches do not demand multiple stochastic inferences, e.g., Temperature Scaling (Guo et al., 2017), Deep Deterministic Uncertainty (Mukhoti et al., 2023), and Posterior Network (Charpentier et al., 2020). Very recently, the density-based methods (Bui and Liu, 2024a; Tomani et al., 2023; Bui and Liu, 2024b) show good performance on latency and uncertainty estimation.\nk-Nearest Neighbor Language Models / Machine Translation k-Nearest Neighbor Language Model (kNN-LM) (Khandelwal et al., 2020) has been proposed, which performs linear interpolation of kNN probability based on distance from neighbors and base model probability, in the language modeling task. k-Nearest Neighbor Machine Translation (kNN-MT) applied the kNN-LM framework to machine translation (Khandelwal et al., 2021). kNN-LM and kNN-MT have been successful because they enhance predictive performance through the memorization and use of rich token representations of pre-trained language models and mitigate problems such as a sparsity comes from low-frequency tokens (Zhu et al., 2023). The main issue on kNN-LM and kNN-MT is the inference overhead, and there are several studies to solve this problem. He et al. (2021a) employs datastore compression, adaptive retrieval, and dimension reduction to reduce computational overhead with retaining perplexity. Deguchi et al. (2023) dramatically improves decoding speed by dynamically narrowing down the search area based on the source sentence.\nStudies about uncertainty estimation for NLP tasks are limited when compared with those for image datasets. Kotelevskii et al. (2022) has shown excellent performance in classification with rejection tasks and out-of-distribution detection tasks using uncertainty scores using density estimation results. Vazhentsev et al. (2022) performed misclassification detection using Determinantal point processes, spectral normalization, and loss regularization in text classification and NER. Vazhentsev et al. (2023) proposed the new uncertainty method which combined epistemic and aleatoric uncertainty estimation methods for text classification tasks. In this study, we comprehensively investigate the density-based methods such as Density Softmax and DAC in NLP tasks, inspired by the success of memorization and use in"}, {"title": "3 Base Methods", "content": "In this section, we describe the density-based methods for uncertainty estimation. We use the following notations: zi denotes the logits for class i, pi denotes the calibrated probability for class i, D means the dimension of the final output layer, and J denotes the number of classes."}, {"title": "3.1 Density Softmax", "content": "Density Softmax (Bui and Liu, 2024a) obtains confidence by weighting logits with normalized log-likelihood from a trained density estimator. In this study, we use RealNVP (Dinh et al., 2017) as the density estimator (details for the density estimator are in Appendix A). \u03b2 is the parameters of the density estimator; f(X) is the representation obtained from the input example X; p(f(X); \u03b2) is the normalized log-likelihood from the density estimator, then the corrected confidence is written as\n$p(y|x) = \\frac{exp (p(f(X); \\beta) \\cdot z_i)}{\\Sigma_{j=1}^J exp (p(f(X); \\beta) \\cdot z_j)} \\qquad(1)$\nIn Density Softmax, the closer the normalized log-likelihood to zero, the closer the prediction to Uniform distribution (Bui and Liu, 2024a). Density Softmax achieves reasonable latency and competitive uncertainty performance with state-of-the-art methods at the cost of demanding the density estimator training and multiple base model training."}, {"title": "3.2 Density Aware Calibration (DAC)", "content": "DAC (Tomani et al., 2023) scales the logits by using sample-dependent temperature \u03a6(\u03a7, w)\n$P(Y_i| x) = \\frac{exp (z_i/\\Phi(X, w))}{\\Sigma_{j=1}^J exp (z_j/\\Phi(X, w))}\\qquad(2)$\nwhere\n$\\Phi(X, w) = \\Sigma_{l=1}^L w_l s_l + w_0. \\qquad(3)$\nW\u2081...WL are the weights for every layer of the base model, si is the averaged distance from kNN search on l-th layer, and wo is the bias term. wo...WL"}, {"title": "4 Proposed Method: k-Nearest Neighbor Uncertainty Estimation (kNN-UE)", "content": "Our proposed method, kNN-UE, is based on the intuition that a model should give high confidence only when the predicted instance exists in the region with high density of training data and a high proportion of the model's predicted label exists in the neighbors sharing the same label, as illustrated in Figure 1. To represent this intuition, we introduce a scoring method based on kNN and labels of the neighbors in the following formula:\n$P(Y_i| x) = \\frac{exp(W_{kNN}(\\hat{y}) \\cdot z_i)}{\\Sigma_{j=1}^J exp(W_{kNN} (\\hat{y}) \\cdot z_j)}\\qquad(4)$\nwhere\n$W_{kNN}(\\hat{y}) = exp(-\\frac{d_k}{\\alpha}) \\qquad distance term$\n$+ \\tau (\\frac{S(\\hat{y})}{K} + \\beta) \\qquad label term\\qquad(5)$\nK is the number of neighbors from kNN search, S(y) is the number of examples where the predicted label \u0177 and the label of the neighbors are same, dk is the distance between the k-th f(X) representations obtained by kNN search and the representations of training data. The parameters \u03b1, \u03c4,"}, {"title": "5 Experimental Settings", "content": "We measure the uncertainty estimation performance on Sentiment Analysis (SA), Natural Language Inference (NLI), and Named Entity Recognition (NER) in In-domain (ID) and Out-of-Domain settings. Dataset statistics are described in Table 1.\nis a task to classify whether the text sentiment is positive or negative. The IMDb movie review dataset (Maas et al., 2011) is treated as ID, and the Yelp restaurant review dataset (Zhang et al., 2015) is treated as OOD.\nclassifies the relationship between a hypothesis sentence and a premise sentence. We treat the Multi-Genre Natural Language Inference (MNLI) dataset (Williams et al., 2018) as ID and the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) as OOD.\nextracts the named entities, such as a person, organization, or location. The NER task was carried out in the framework of sequence labeling. We regard the OntoNotes 5.0 dataset (Pradhan et al., 2013) broadcast news (bn) domain as ID, and newswire (nw) and telephone conversation (tc) domains as OOD."}, {"title": "5.2 Existing Methods", "content": "is a trivial baseline, which treats the maximum score from output of the base model's softmax layer as the confidence (Cordella et al., 1995).\nis a calibration technique by which the logits (i.e., the outputs of the final layer before the softmax) are divided by a temperature parameter T before applying the softmax function (Guo et al., 2017). We optimized T by L-BFGS on dev set loss.\nis the calibration and generalization technique by introducing a small degree of uncertainty \u20ac in the target labels during training (Miller et al., 1996; Pereyra et al., 2017). In LS, we optimized \u20ac \u2208 {0.01, 0.05, 0.1, 0.2, 0.3} by using dev set accuracy when SA and NLI, and dev set F1 when NER.\n is a regularization technique that be used for estimating uncertainty in neural networks (Gal and Ghahramani, 2016). This approach involves executing the model M times with varying dropout masks and computing the mean softmax output across these runs. In our experiments, we set M = 20 for all evaluations, and the dropout rate is 0.1."}, {"title": "5.3 Training Setting", "content": "In all experiments, we train and evaluate the models on a single NVIDIA A100 GPU with 40GB of memory. We used DeBERTaV3BASE and mDeBERTaV3BASE (He et al., 2023), as the transformer encoder from transformers (Wolf et al., 2020) pre-trained model checkpoints. Cross-entropy loss is minimized by AdamW (Loshchilov and Hutter, 2019) with a linear scheduler (Goyal et al., 2017). The batch size is 32, and gradient clipping is applied with the maximum norm of 1. The initial learning rate was set to le-5. All experiments are run five times, and we report the mean and standard deviation of the scores.\nIt is necessary to preserve the representation of the data for training a density estimator in Density Softmax and kNN search in DAC and KNN-UE. We maintain final layer representations corresponding to CLS tokens"}, {"title": "6 Results", "content": "We present the uncertainty estimation results for sentiment analysis. Table 2 shows the results of in-domain and out-of-domain uncertainty estimation. kNN-UE consistently outperforms existing methods in terms of ECE, MCE, and E-AURC. In AUROC, LS outperforms in OOD setting, but KNN-UE outperforms existing methods in ID setting. Furthermore, the proposed method clearly outperforms DAC that uses an ensemble of neighbor search results for each hidden representation, by adding the label term."}, {"title": "6.2 Natural Language Inference", "content": "In NLI, we evaluate the uncertainty estimation performance (calibration and selective prediction) and the out-of-distribution detection performance."}, {"title": "6.2.1 Uncertainty Estimation", "content": "First, we show the results of in-distribution and out-of-distribution uncertainty estimation in the NLI task using the DeBERTaV3 model in Table 3. Similar to Section 6.1, kNN-UE shows a significant improvement in ECE, MCE and E-AURC. Furthermore, in AUROC, kNN-UE outperforms existing methods in both ID and OOD settings. Galil et al. (2023) have reported that improving calibration performance does not necessarily lead to improving selective prediction performance, but our proposed method improves both. On the other hand, the degree of improvement is greater for calibration performance. Specifically, the largest improvement is obtained on SNLI, where kNN-UE reduces MCE by more than 31.49% pt compared to SR."}, {"title": "6.2.2 Out-of-Distribution Detection", "content": "Following the previous study (Tomani et al., 2023), we carried out the experiments in the out-of-distribution detection task. Out-of-distribution de-"}, {"title": "6.3 Named Entity Recognition", "content": "In order to evaluate NLP tasks other than simple multi-class classification, we evaluate the effectiveness of the proposed method for uncertainty estimation in NER. Since NER focuses on entities, it is necessary to obtain the confidence of the entity. In this research, we use the product of the confidence of the tokens that construct the entity as the confidence level of the entity.\nThe proposed method improves the uncertainty estimation performance in all cases except ECE of DAC in the bn and nw domains, often resulting in large improvements compared to the SR. Moreover, we can see that TS and MC Dropout, which are often effective in multi-class classification, are not effective in evaluating entities like NER. On the other hand, E-AURC in NER is consistently better without using the KNN-UE label term."}, {"title": "6.4 Case Study: Effects of the Label Term in kNN-UE for a Misclassified Example", "content": "Table 6 shows SR and kNN-UE confidences, and S(\u0177) in kNN-UE for a misclassified example. In this case, SR and kNN-UE make incorrect prediction even though the true label is negative. However, the confidence is appropriately reduced by including the distances from the neighbors in kNN-UE, compared to SR. Moreover, by using the information that there are only 11 examples in K = 32 neighbors with the same label as the predicted label among the neighbors obtained by kNN search, our KNN-UE shows that the confidence is further reduced."}, {"title": "7 Analyzes", "content": "To understand the behavior of kNN-UE, we evaluated the performance in uncertainty estimation when changing the number of neighbors K\u2208 8, 16, 32, 64, 128} during kNN execution.\ntends to be. Since our method averages the distance to the top K examples, logits are scaled"}, {"title": "7.2 Impact of Efficient Nearest Neighbor Search Techniques", "content": "In this subsection, we investigate the inference time and uncertainty estimation performance when applying approximate nearest neighbor search techniques and dimension reduction when executing kNN search in kNN-UE. As shown in Table 7, in the sequence labeling based NER that requires the kNN search execution per token, it takes twice as much inference time as SR. On the other hand, in kNN-LM (Khandelwal et al., 2020), dimension reduction and approximate kNN search techniques are effective to improve inference speed while maintaining perplexity (He et al., 2021a; Xu et al., 2023). Therefore, inspired by these works for faster kNN-LM, we investigate how the approximate nearest neighbor search techniques, such as Product Quantization (J\u00e9gou et al., 2011) or clustering, and dimension reduction affect the uncertainty estimation and inference speed of our proposed method: kNN-UE.\nProduct Quantization Product Quantization (PQ) (J\u00e9gou et al., 2011) is a data compression technique based on vector quantization. In PQ, a D-dimensional representation is divided into N sub subvectors and quantized by performing k-means clustering on the vectors in each subspace. Vector quantization can significantly reduce the amount of memory occupied by vectors. In addition, by calculating the distance between compressed PQ codes, we can efficiently calculate the estimated value of the original Euclidean distance.\nClustering The original kNN-LM uses an inverted file index (IVF) technique that speeds up the search by dividing the representation into Nlist clusters by k-means and searching for neighbors based on Nprobe centroids. In this study, we evaluate the uncertainty estimation performance and inference speed when the number of clusters Nlist = 100.\nIn general, Transformer-based models such as PLM have high-dimensional token representations. In high-dimensional spaces, nearest neighbor search often suffer from the curse of dimensionality. To reduce this problem, we apply dimension reduction to kNN-UE similar to He et al. (2021a). In this study, we use Principal Component Analysis (PCA) as a dimension reduction algorithm to reduce the dimension of the datastore"}, {"title": "8 Conclusion", "content": "In this paper, we proposed kNN-UE, which estimates uncertainty by using the distance to neighbors and labels of neighbors. The experimental results showed that our method showed higher uncertainty estimation performance than existing uncertainty estimation methods in SA, NLI and NER. Furthermore, we showed that the uncertainty estimation performance can be further improved by adjusting the number of neighbors K in kNN-UE. Our method can greatly improve uncertainty estimation performance, especially in text classification tasks, with little degrading in inference speed. On the other hand, to address the degradation of the inference speed in token-level tasks such as NER, we investigated the effects of efficient neighbor search techniques in kNN-UE. As a result, we found that product quantization, clustering, or dimension reduction improves inference speed without degrading the uncertainty estimation much more, unless combining all of them simultaneously."}, {"title": "9 Limitations", "content": "In this study, we focused only on the classification-based tasks. On the other hand, taking advantage of the recent growth of Large Language Models, uncertainty estimation in text generation is also attracting attention (Fadeeva et al., 2023; Lin et al., 2024). Therefore, to investigate the effectiveness of kNN-UE in text generation tasks is an interesting direction for future research. Furthermore, although kNN-UE only used the representation of the last layer of the base model, exploring for an appropriate representation for uncertainty estimation is a future challenge."}, {"title": "Ethical Considerations", "content": "In this study, we used existing datasets that have cleared ethical issues following policies of published conferences. Therefore, they do not introduce any ethical problems. On the other hand, we have an ethical consideration about uncertainty estimation. Specifically, decision support systems with machine learning algorithms do not necessarily have a positive effect on performance. Jacobs et al. (2021) showed that collaboration with machine learning models does not significantly improve clinician's treatment selection performance, and that performance is significantly degraded due to the presentation of incorrect recommendations. This problem is expected to remain even if uncertainty estimation methods are applied to machine learning models. In addition, introducing uncertainty estimation methods could conversely lead humans to give overconfidence in machine learning models, resulting in performance degradation."}, {"title": "A Training Settings for Density Estimator in Density Softmax", "content": "In Density Softmax (Bui and Liu, 2024a), we use RealNVP (Dinh et al., 2017) which has two coupling structures."}, {"title": "B Details of Evaluation Metrics", "content": "ECE\n(Naeini et al., 2015) quantifies the difference between the accuracy and confidence of a model. Formally, ECE is expressed as:\n$ECE = \\Sigma_{b=1}^B \\frac{|D_b|}{n} |acc(D_b) - conf(D_b)| \\qquad(6)$"}, {"title": "C Each Result of Product Quantization, Clustering, and Dimension Reduction", "content": "We evaluated uncertainty estimation performance and inference time when the number of clusters in the codebook was fixed at 32, and the number of subvectors was changed to Nsub \u2208 {16, 32, 64}.\nIn ECE and E-AURC, there are almost no degradation in uncertainty estimation performance due to PQ. On the other hand, in MCE in ID setting, the uncertainty estimation performance consistently degrades. Furthermore, compared to kNN-UE among different Nsub, the larger Nsub, the better the uncertainty estimation performance tends to improve, but the inference time increases.\nIn this study, we evaluate the uncertainty estimation performance and inference speed when the number of clusters Nlist = 100 is fixed and number of cluster centroids to search changesNprobe \u2208 {8, 16, 32, 64}.\nWe evaluate the uncertainty performance and inference speed when applying PQ, clustering, and dimension reduction are applied sequentially.\nthat while applying PQ and clustering simultaneously or applying PCA individually covers a certain percentage of the example indices obtained from kNN-UE, which uses exhaustive search by default, applying PQ, clustering, and PCA simultaneously hardly covers any of the indices from the default kNN-UE. It is assumed that applying PQ and PCA leads to coarse distance computation in a single subvector, which would correspondingly degrade the uncertainty estimation performance in kNN-UE. On the other hand, if they are not applied simultaneously, the degradation in the uncertainty performance is limited."}, {"title": "C.3 Dimension Reduction", "content": "As shown the uncertainty estimation performance depends on the number of target dimension, and the performance degrades when Dpca = 64 or Dpca = 128. On the other hand, the performance in Dpca = 256 is almost the same as default kNN-UE. This is because while dimension reduction mitigates the problem of high dimensionality, the failure to extract adjacent examples by kNN search when applying PCA is less likely compared to PQ and clustering if the target dimension in dimension reduction is not reduced too much."}, {"title": "D Distance Recomputation for kNN-UE", "content": "When using efficient kNN search techniques in Section 7.2, we use approximate distances to compute Eq. 5. Although we can get raw vectors by using the example indices obtained from approximate nearest neighbor search and compute accurate distance, in kNN-LM this has been shown to lead to performance gains and latency degradation (He et al., 2021a). We measure the uncertainty estima-"}, {"title": "E Licenses of Datasets, Tools and Models", "content": "Datasets IMDb movie dataset can be used for research purpose as described in https://developer.imdb.com/non-commercial-datasets/. Yelp Polarity dataset can be used for academic purpose as described in https://s3-media0.fl.yelpcdn.com/assets/srv0/engineering_pages/f64cb2d3efcc/assets/vendor/Dataset_User_Agreement.pdf. MNLI dataset is licensed for research purpose as described in Williams et al. (2018). SNLI dataset can be used for research purpose as described in https://nlp.stanford.edu/projects/snli/. OntoNotes 5.0 dataset can be used for research purpose as described in https://catalog.ldc.upenn.edu/LDC2013T19.\nTools transformers is licensed by Apache-2.0. faiss is MIT-licensed.\nand mDeBERTaV3BASE from Huggingface model checkpoints are MIT-licensed."}]}