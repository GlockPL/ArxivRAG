{"title": "STEERING MASKED DISCRETE DIFFUSION MODELS VIA DISCRETE DENOISING POSTERIOR PREDICTION", "authors": ["Jarrid Rector-Brooks", "Mohsin Hasan", "Zhangzhi Peng", "Zachary Quinn", "Chenghao Liu", "Sarthak Mittal", "Nouha Dziri", "Michael Bronstein", "Yoshua Bengio", "Pranam Chatterjee", "Alexander Tong", "Avishek Joey Bose"], "abstract": "Generative modeling of discrete data underlies important applications spanning text-based agents like ChatGPT to the design of the very building blocks of life in protein sequences. However, application domains need to exert control over the generated data by steering the generative process-typically via RLHF-to satisfy a specified property, reward, or affinity metric. In this paper, we study the problem of steering Masked Diffusion Models (MDMs), a recent class of discrete diffusion models that offer a compelling alternative to traditional autoregressive models. We introduce DISCRETE DENOISING POSTERIOR PREDICTION (DDPP), a novel framework that casts the task of steering pre-trained MDMs as a problem of probabilistic inference by learning to sample from a target Bayesian posterior. Our DDPP framework leads to a family of three novel objectives that are all simulation-free, and thus scalable while applying to general non-differentiable reward functions. Empirically, we instantiate DDPP by steering MDMs to perform class-conditional pixel-level image modeling, RLHF-based alignment of MDMs using text-based rewards, and finetuning protein language models to generate more diverse secondary structures and shorter proteins. We substantiate our designs via wet-lab validation, where we observe transient expression of reward-optimized protein sequences.", "sections": [{"title": "1 INTRODUCTION", "content": "The success of diffusion models in continuous spaces, leading to state-of-the-art foundation models for image (Stability AI, 2023; Midjourney, 2023) and video synthesis (Villegas et al., 2022; Brooks et al., 2024), has spurned several attempts to translate these approaches for the generative modeling of discrete structures. The most performant approaches squarely fall under the scalable framework of absorbing state discrete diffusion (Austin et al., 2021), with new simplified training recipes that result in Masked Diffusion Models (MDMs) (Sahoo et al., 2024; Shi et al., 2024; Gat et al., 2024; Zhao et al., 2024a). Indeed, recent MDMs now rival autoregressive models of a similar scale to GPT-2 (Radford et al., 2019) for language modeling, with the potential for further progress through scaling. Furthermore, MDM style models are not constrained to generating data sequentially unlike autoregressive models\u2014which invites a more straightforward application to domains without a natural causal ordering, e.g. molecule generation (Vignac et al., 2022), discrete modeling of images (Salimans et al., 2017), and modeling protein sequences (Lin et al., 2022; Wang et al., 2024).\nCritical to the successful deployment of discrete generative models in practical applications\u2014beyond simply producing high-quality samples\u2014is the ability to steer the generated samples to optimize a pre-specified downstream metric. For instance, in Language Modeling (LM) it is desirable to bias the model's generations to be sanitized from harmful responses (Zou et al., 2023; Perez et al., 2022), or aiming to generate protein sequences that are highly likely to be successfully synthesized and expressed in real wet lab settings (Verkuil et al., 2022; Dauparas et al., 2022). Put succinctly, highly performant discrete generative models are required to be aligned in a manner that fine-tuning against downstream reward models has the intended effect of controllable generation, wherein the model post fine-tuning selects high-scoring samples from the universe of possible high-fidelity generations."}, {"title": "2 BACKGROUND AND PRELIMINARIES", "content": "Notation and convention. A discrete data sample x is identified by its realization over a vocabulary set $V = \\{1, ..., d \u2013 1\\}$, over d \u2013 1 possible categories. Of particular interest is the setting of masked diffusion models that include an additional d-th category of a masked token m to the vocabulary V which serves as an absorbing state for the diffusion process. A discrete token is represented by the one-hot vector $e^i \\in \\mathbb{R}^d$ in the d-dimensional probability simplex and corresponds to placing a 1 on the i-th index and 0 on all the other d \u2013 1 indices. In this paper, by convention, we set $e_m = e_d$ as the one hot vector associated with the masked state m. A categorical distribution over d-categories, $Cat(x; p)$, is constructed by placing a Dirac \u03b4 with weight $p^i$, with the constraint $\\sum_{i=1}^d p^i = 1$ and the density of a discrete sample is written as $p(X = x) = \\sum_{i=0}^d p^i \\delta(x \u2013 e^i)$, where X is the discrete random variable.\nA sequence $x = (x_1, ..., x_n)$ of n tokens is defined over the product space $V^n = \\{1, ..., m\\}^n$, and its corresponding probability mass function is given by $p(X = x) = \\prod_{i=1}^n \\sum_{i=0}^d \\delta(x^i \u2013 e^i)$. To reduce notational clutter, we make use of the shorthand \u03b4(y) to denote a Dirac measure on a discrete sample y and interchangeably write p(X = x) = p(x) to denote the probability mass function. A dataset of sequences is designated as samples from the data distribution $P_{data}$ to be learned by a"}, {"title": "2.1 SIMPLIFIED MASKED DISCRETE DIFFUSION", "content": "We are interested in developing a discrete diffusion model directly on discrete data\u2014i.e. without embeddings or continuous reparameterizations\u2014whose approach mirrors the construction of diffusion models for continuous spaces. Consequently, we require the specification of a forward process that converts discrete data $x_0 \\sim p_0$ at time t = 0 to an unstructured prior, $p_1$ at the terminal time t = 1. The specification of a forward process via the transition kernel $p_t(x_t|x_0)$ implies a unique time reversal of this forward process, termed the \u201creverse process\u201d, such that simulating from this reverse process results in samples from the desired target data distribution $p_0(x_0)$.\nWe restrict our attention to the recent performant \u201csimplified masked\u201d forward process (Sahoo et al., 2024; Shi et al., 2024; Gat et al., 2024; Zhao et al., 2024a) which hits a terminal distribution of all mask tokens in a sequence $P_1 = [\\delta(m)]^n$. Given a non-masked token in a sequence, $x \\in x$ the simplified masked forward process increases the likelihood of transition to the mask state as time increases. Moreover, the masked forward process is simplified by design since the transition probabilities of a token unmasking ($x_{t+1}^i \\neq m$ when $x_t^i = m$) is set to zero\u2014i.e. the token remains a masked token for the remainder of the trajectory. The design of the simplified forward process is also independent across each dimension of the sequence, conditioned on $x_0$, which allows us to model the transitions of each discrete token in a sequence separately. In totality, the forward process for a sequence $x_0$ can be summarized using the following expression for the transition kernel $p_t(x|x_0)$:\n$\\begin{equation}\nP_t(X_t|X_0) = \\prod_{i=1}^n P_t(x_t^i|x_0^i) = \\prod_{i=1}^n Cat(x^i; \\alpha_t\\delta(x^i) + (1 - \\alpha_t)\\delta(m)),\n\\end{equation}$\nwhere $\\alpha_t$ is an invertible reparameterization of time such that $\\alpha_0 = 1$ and $\\alpha_1 = 0$. Effectively, $\\alpha_t$ corresponds to the noise schedule which corrupts the discrete data to $p_1$. The corresponding marginal density induced by the forward process at time t can written as $p_t(x_t) = \\sum_{x_0} P_t(x_t|x_0)p_0(x_0)$.\nThe reverse process which denoises a sample from $t\\rightarrow t-1$, and is the time reversal of the simplified masked forward process, also factorizes over each dimension of a sequence x. The probability"}, {"title": "3 POSTERIOR SAMPLING VIA DISCRETE DENOISING POSTERIOR PREDICTION", "content": "Given access to a pretrained masked discrete diffusion model $p_{\\theta}^{pre}(x_0)$ we wish to sample from the reward-induced Bayesian posterior distribution $\\pi_0(x_0) \\propto p_{\\theta}^{pre}(x_0) R(x_0)$. We solve this sampling problem by first defining a time-dependent forward masking process that progressively adds noise to $\\pi_0$ yielding the noisy reward-induced posterior $\\pi_t(x_t) = \\sum_{x_0} \\pi_t(x_t|x_0)\\pi_0(x_0)$, where we set $\\pi_t(X_t|X_0) = P_t(X_t|x_0)$ as it is the same masking process for the pre-trained MDM. Unfortunately, since $p_{\\theta}^{pre}(x_0)$ is an MDM it does not easily provide an exact likelihood. Undeterred we seek to approximate the reverse process $\\pi_t(x_{t-1}|x_t)$ tied to the masking forward process by using another parametrized model $q_{t,0}(x_0|x_t) = Cat(x_0; \\mu_{\\theta\u2019}(x_t, t))$ which we take to be another MDM.\nMatching sub-trajectories. To approximate the reverse process using an MDM we require matching the denoising trajectory $\\pi_t(X_{0,..., X_{t-1}}|x_t)$ of the reward-induced posterior across all masking levels. Assisted in this endeavor, we recall the fact that since $p_{\\theta}^{pre}(x_0)$ is also an MDM, we have direct access to the pre-trained model\u2019s denoiser. Thus, we can compute any transition density starting from $p_{\\theta}(x_{t-1}|x_t, p_{\\theta}(x, t))$ to the posterior over the endpoint $p_{\\theta}^{pre}(x_0|x_t)$, conditioned on a partially masked sample $x_t$. We form the sub-trajectory matching problem as an instantiation of a detailed balance constraint starting from a partially masked sequence $x_t$ of a clean starting point $x_0$:\n$\\begin{equation}\nq_{\\theta\u2019}(X_{0,..., X_{t-1}}|X_t, \\theta\u2019)p_t(x_t) = \\pi_t(X_{0,..., X_{t-1}}|X_t)p_t(x_t).\n\\end{equation}$\nSetting $x_0 = \\mu_{\\theta}(x_t, t)$ as the MDM's denoised sample, then $\\pi_t(x_0, ..., x_{t-1}|x_t)$ is defined as,\n$\\begin{equation}\n\\pi_t(X_{0,..., X_{t-1}}|X_t) = \\frac{p_{\\theta}^{pre}(x_{0,..., x_{t-1}}|x_t) R(x_0)}{\\sum_{X_0,..., X_{t-1}} p_{\\theta}^{pre}(X_{0,..., X_{t-1}}|X_t) R(X_0)} = \\frac{\\prod_{j=1}^t P_t^{pre}(x_{j-1}|x_j, x_{\\theta}) R(x_0)}{Z_{\\pi_\\theta}(X_t)}\n\\end{equation}$\nThe detailed balance constraint over sub-trajectories suggests a natural discrete denoising posterior predictive (DDPP) objective that minimizes the mean squared error of a log-ratio between the denoising sub-trajectories of the amortized MDM sampler and the reward-induced target posterior,\n$\\begin{equation}\n\\mathcal{L}^{CPP} = \\mathbb{E}_{t, x_t} [\\mathbb{E}_{\\tau(x_{0:t})} [|| \\log q_{\\theta\u2019}(x_{0:t-1}|x_t, x_{\\theta})) - \\log p_{\\theta}^{pre}(x_{0:t-1}|x_t) + \\kappa ||^2]],\n\\end{equation}$\nwhere reward and the log partition function are captured in the constant $\\kappa = \\log Z_{\\pi_\\theta}(X_t) - \\log R(x_0)$. Interestingly, we can form an equivalent expression for the sub-trajectory loss $\\mathcal{L}^{CPP}$ above by sampling two intermediate points $x_s, x_{s-\\gamma}$ in the sub-trajectory $\\tau(x_{0:t})$, such that $0 < s - \\gamma < s < t$:\n$\\begin{equation}\n\\mathcal{L}^{CPP} = \\mathbb{E}_{t, x_t} [\\mathbb{E}_{\\tau(x_{0:t})} [|| \\mathbb{E}_{s, x_s, x_{s-\\gamma}} [\\log q_{\\theta}(x_{s-\\gamma}|x_s, x_{\\theta}) - \\log p_{\\theta}^{pre}(x_{s-\\gamma}, x_s, x_t) + \\kappa ] ||^2].\n\\end{equation}$"}, {"title": "3.1 ESTIMATING THE LOG PARTITION FUNCTION", "content": "Inspecting the posterior predictive objective in Eq. 8 we remark that it is a simulation-free stochastic regression objective which does not require a differentiable reward as the loss computes R(x0) and not a gradient of the reward. Consequently, this makes the posterior predictive objective both a scalable and efficient objective for fine-tuning large pre-trained MDMs as long the reward model is easy to eval-uate. Moreover, the posterior predictive objective is also an off-policy objective as it can be evaluated using any partially masked samples $x_t \\sim p(x_t|x_0)$. Practically, this means that fine-tuning can be per-formed using a replay buffer of samples from a biased dataset, e.g. the original training set for $p_0$, or even partially masked sequences that arrive from a different model altogether. Despite its simple form the posterior predictive objective requires the computation of the log partition function of a partially masked sequence $\\log Z_{\\pi_\\theta}(X_t)$ which does not have a closed-form expression and must be estimated.\nMonte Carlo Estimate of $\\log Z_{\\pi_\\theta}$ with DDPP-IS. A numerical estimate of the log normalization constant can be obtained by utilizing the trick of using the pre-trained model\u2019s denoising posterior $p_{\\theta}^{pre}(x_0|x_t)$. Specifically, given $x_t \\sim p_t(x_t)$ we obtain a Monte Carlo estimate of $\\log Z_{\\pi_\\theta}(X_t)$ that uses M additional samples from $x_0 \\sim p_{\\theta}^{pre}(x_0|x_t)$ to estimate the log partition function,\n$\\begin{equation}\n\\log Z_{\\pi_\\theta}(X_t) = \\log(\\sum_{X_0,..., X_{t-1}} P_{\\theta}^{pre} (x_{0,..., X_{t-1}}|X_t)R(x_0))\n\\approx \\log (\\mathbb{E}_{x_0 \\sim p_{\\theta}^{pre}(x_0|x_t)} [R(x_0)]).\n\\end{equation}$\nWhere in the second equality in the first line we used the fact that we can approximately jump to the endpoint of the reverse process directly by using the pretrained model's denoiser to sample x0. Conveniently, this MC estimate solely requires obtaining a denoised sample from the pre-trained MDM which can be efficiently done as each sample requires a single step as due to the denoising posterior parametrization of an MDM (Eq. 4). We can further improve the estimation of this log normalization constant by leveraging importance sampling (IS) with a proposal distribution w(x0):\n$\\begin{equation}\n\\log Z_{\\pi_\\theta}(X_t) = \\log \\mathbb{E}_{x_0 \\sim w(x_0)} [\\frac{p_{\\theta}^{pre}(x_0|x_t) R(x_0)}{w(x_0)}] = \\log (\\frac{1}{M} \\sum_{j=1}^M \\frac{p_{\\theta}^{pre}(x_0|x_t) R(x_0)}{w(x_0)} ]\n\\end{equation}$\nFor the IS estimator above it is easy to verify that the optimal proposal distribution for variance reduction is proportional to the denoising reward-induced target posterior $w^*(x_0) \\propto \\pi_t(X_0|x_t)$. Fortunately, this is precisely the distribution that is approximated by $q_{t,\\theta\u2019}$ using the posterior predictive objective which motivates the reuse of the finetuned model as a suitable proposal, i.e. $w(x_0) = q_{t,0}(X_0|X_t)$.\nLearning $\\log Z_{\\pi_t}$ with DDPP-LB. An alternative to using an MC-based estimate for $\\log Z_{\\pi_t}$ is to parameterize the log partition function itself $\\log \\tilde{Z}_{\\pi_t,\\theta\u2019}^{LB}$, jointly with the $q_{t,\\theta\u2019}$ and optimize both using the same posterior predictive objective as first defined in Eq. 11. Operationally, this amounts to including another prediction head for the finetuned MDM model and is cheaper to compute than using an MC-based estimate as we do not require M evaluations of the pre-trained model as in $\\log Z_{\\pi_t}^{IS} (X_t)$."}, {"title": "3.2 SINGLE-STEP POSTERIOR SAMPLING WITH ENDPOINT PREDICTION", "content": "The sub-trajectory matching objective used by DDPP-IS and DDPP-LB can be simplified to a faster single-step objective at the cost of paying a discretization error by not using finer-grained trajectory information. Specifically, we note that for MDMs the denoising posterior over end-points $q_{t,0}(X_0|x_t) \\approx Cat(x_0; \\mu_{\\theta\u2019}(x_t, t))$ can be approximately computed without unrolling the sub-trajectory. This fact also holds for the pre-trained MDM as the model parametrization implies $p_{\\theta}^{pre}(x_0|x_t) \\approx Cat(x_0; \\mu(x_t, t))$. For the single-step objective we assume the parameterized denoisers exactly match the posteriors. Leveraging this enables us to express the denoising reward-induced target posterior using a simple expression that directly uses the pre-trained model's denoising posterior $p(x_0|x_t)$ as follows:\n$\\begin{equation}\n\\pi_t (X_0| X_t) = \\frac{P_t(X_t|X_0)p_{\\theta}^{pre}(x_0)R(x_0)}{P_t(x_t)} = \\frac{p_{\\theta}^{pre}(x_0|x_t)R(x_0)}{Z_{\\pi_\\theta}(X_t)}.\n\\end{equation}$\nThe choice of parameterizing $q_{t,0}(x_0|x_t)$ as another MDM offers a prescriptive strategy for sampling from the desired target $\\pi_0$ by learning to match the denoising reward-induced posterior at the pre-dicted endpoint $\\pi_t(x_0|x_t)$. This simplifies the expression of DDPP defined over trajectories in Eq. 8 to a single point, namely the predicted endpoint x0 of each MDM. This objective is presented below:\n$\\begin{equation}\n\\mathcal{L}^{PP} \\triangleq \\mathbb{E}_{t, x_0, x_t} [|| \\log q_{\\theta\u2019}(x_0|x_t) - \\log p_{\\theta}^{pre}(x_0|x_t) - \\log R(x_0) + \\log Z_{\\pi_\\theta}(X_t)||^2].\n\\end{equation}$"}, {"title": "3.3 DDPP-KL: POSTERIOR PREDICTION VIA REVERSE KL MINIMIZATION", "content": "The single-step posterior prediction objective as defined using the loss function $\\mathcal{L}^{PP}$ in Eq. 11 requires the estimation of $\\log Z_{\\pi_t,\\theta\u2019}^{LB} Z_{ure}$ which introduces a source of variance in loss estimates that may sub-optimally influence learning dynamics of the fine-tuned model. In settings, where the reward model is differentiable we can bypass computing $\\log Z_{\\pi_t,\\theta\u2019}^{LB} Z_{ure}$ altogether by learning to match the denoising reward-induced posterior under the reverse KL divergence. To see this, we define a variational posterior matching problem using the reverse KL divergence that takes the following form:\n$\\begin{equation}\n\\mathcal{L}^{KL} := D_{KL}(q_{t,0}(X_0|X_t)p_t(x_t)|||pi_t(X_0|X_t)p_t(x_t)).\n\\end{equation}$\nUnlike conventional generative modeling using the reverse KL divergence which solely matches distributions at t = 0 the problem definition in Eq. 12 defines a series of reverse KL minimization problems through time. In this manner, the reverse KL matches distributions annealed through time and can be used to derive a stochastic regression objective for fine-tuning,\n$\\begin{equation}\n\\mathcal{L}^{KL} \\triangleq \\mathbb{E}_{t, x_0, x_t} [\\log q_{t,0}(x_0|x_t) - \\log p_{\\theta}^{pre}(x_0|x_t) - \\log R(x_0)] + C.\n\\end{equation}$\nThe expectation in Eq. 13, like DDPP-IS and DDPP-LB is taken uniformly with respect to time t ~ U[0, 1]. However, unlike the previous estimators, clean data needed to compute $\\mathcal{L}^{KL}$ is drawn purely on-policy by simulating the fine-tuning model $x_0 \\sim q_{\\theta\u2019}(x_0)$, which then also allows us to craft a noisy sample using the masking forward process $x_t \\sim p_t(x_t|x_0)$. Additionally, in Eq. 13 the constant $C = \\mathbb{E}_{t, x_0, x_t} [\\log Z_{\\pi_t}(x_t)]$ does not depend on the $\\theta\u2019$\u2014and as a result is also independent of the sample $x_0 \\sim q_{t,0}(x_0)$. This results in the constant C being zero when computing the gradient of the loss $\\nabla \\mathcal{L}^{KL}$ and as a result we can safely disregard computing $\\log Z_{\\pi_t}$ entirely.\nAs samples $x_0$ are procured on-policy to compute the gradient of the loss $\\nabla \\mathcal{L}^{KL}$ we require backpropagating through the stochastic sampling of $x_0$ which comes from simulating the fine-tuning MDM $q_{t,0}(x_0)$. Fortunately, we can make use of modern discrete gradient estimators which provide a biased but low variance gradient estimate enabling us to compute $\\mathcal{L}^{KL}$. Specifically, we opt to use the scalable 2nd order REINMAX estimator (Liu et al., 2024) which estimates the discrete gradient up to second-order terms in a Taylor approximation of the actual gradient. We note that unlike DDPP-IS and DDPP-LB this new loss that minimizes the reverse KL divergence $\\mathcal{L}^{KL}$ requires the reward model R to be differentiable and as a result is less broadly applicable than computing $\\mathcal{L}^{PP}$. However, in practice, learning can be faster as we make use of the information afforded to us by the gradient $\\nabla R$ as well as the fact that the objective does not need to estimate the log partition function."}, {"title": "4 EXPERIMENTS", "content": "We investigate the application of DDPP to a variety of discrete generative modeling settings. We provide the full experimental details in \u00a7D and present our main experimental results next."}, {"title": "5 RELATED WORKS", "content": "Discrete diffusion. The prevailing paradigms for diffusion over discrete spaces can be broadly categorized into 1.) continuous diffusion in a latent or reparametrized space by first transforming the initial discrete data (Li et al., 2022; Chen et al., 2022; Davis et al., 2024; Cheng et al., 2024), and 2.) defining diffusion using discrete analogs of score approximation (Meng et al., 2022; Lou et al., 2023). The latter approach can also be described using the theoretical framework of Continuous-time Markov Chains (CTMC) (Austin et al., 2021; Campbell et al., 2022; 2024). Closest to our setting we consider a specific instantiation of discrete diffusion that simplifies the CTMC framework by using a masked forward process (Sahoo et al., 2024; Shi et al., 2024; Zhao et al., 2024a; Gat et al., 2024).\nFinetuning as sampling. The task of fine-tuning generative models under reward models can be viewed as a sampling problem and encompasses conventional RLHF (Uehara et al., 2024a; Black et al., 2023; Fan et al., 2024; Dong et al., 2023). A simple but expensive method to sample from the reward-induced Bayesian posterior distribution is best of N sampling (Stiennon et al., 2020; Nakano et al.,"}, {"title": "A BROADER IMPACT", "content": "Our proposed DISCRETE DENOISING POSTERIOR PREDICTION is a tailored approach to steering and fine-tuning Masked Diffusion Models. At present, MDMs are an emergent category of discrete generative models that have general-purpose modeling capabilities in a variety of domains including language modeling, sequence-based drug design, and discrete modeling of graphs. Consequently, we believe DDPP has potential use in various practical use cases. For instance, like current RLHF tech-niques applied to modern autoregressive LLMs, future scaled MDMs on text datasets might be tuned to promote harmful behavior and toxic content. Moreover, applying DISCRETE DENOISING POSTERIOR PREDICTION in drug design use cases has the potential to create in-silico sample of protein sequences that may have biologically potent negative externalities. We do, however, make the distinction that such a risk is speculative at this stage given the large complexities of translating in-silico designs to actual synthesized biomolecules. As a result, we encourage practitioners who seek to fine-tune MDMS using DDPP to exercise due caution when applying our proposed techniques to actual use cases.\nEthical statement. As part of qualitatively evaluating DDPP, this paper includes generated samples of text. We highlight that the set of examples may contain potentially disturbing, harmful, or upsetting examples, covering a variety of sensitive topics like discriminatory language, descriptions of harm, and misinformation, among other high-risk categories. Its primary purpose is to advance research in under-standing the impact of DDPP from a more interpretable lens. It is not advised to train future MDMs on such generated samples in order to prevent further propagation of undesirable content and behaviors."}, {"title": "B ADDITIONAL RELATED WORK", "content": "Sampling proportional to energy. Our approach can be closely linked to learning to sample proportional to a target probability, as in our setup we aim to approximate sampling proportional to the energy $p_{\\theta}^{pre} (x_t)R(x)$ for any point $x_t$ at any time t. This has been an avenue of research for a number of works in continuous time (Bengio et al., 2021; 2023; Malkin et al., 2022; Lahlou et al., 2023; Akhound-Sadegh et al., 2024; Sendera et al., 2024; De Bortoli et al., 2024), in Bayesian posterior inference where the energy is defined by the product of likelihood and prior (Mittal et al., 2023), as well as posterior inference in settings where we even do not have access to energy function but only to a simulator (Radev et al., 2020; Wildberger et al., 2024; Geffner et al., 2023)."}, {"title": "C THEORETICAL RESULTS", "content": "Before proving proposition 1 we first prove a useful Lemma that states the optimal log partition function $\\log Z_{\\pi_\\theta}(X_t)$ which is the learning goal for a parameterized approach $\\log \\tilde{Z}_{\\pi_t,\\theta\u2019}(X_t)$.\nLemma 1. Given a sample $x_t \\sim p_t(x_t|x_0)$ and the denoising posterior distribution $q_{t,0}(X_0|x_t)$, a local minimizer for estimate for the log partition function $\\log \\tilde{Z}_{\\pi_t}$ using N samples from $x_0 \\sim q_{t,0}(X_0|x_t)$ is given by:\n$\\begin{equation}\n\\log \\tilde{Z}_{\\pi_t} = \\frac{1}{N} \\sum_{i=1}^N \\log (\\frac{P_t(x_0|x_t) R(x)}{q_{t,0}(X|X_t)} ).\n\\end{equation}$\nProof. By definition the log partition function is a constant, let that constant be $\\log Z_{\\pi_t}(X_t) = C$. Then the loss in Eq. 11 is a quadratic in C,\n$\\begin{equation}\n\\mathcal{L} = \\mathbb{E}_{x_0 \\sim \\tau(x_0)} [|| \\log q_{t,0}(X_0|X_t) + C - \\log p_t(x_0|x_t) - \\log R(x_0)||^2]\n\\end{equation}"}, {"title": "C.1 PROOF OF PROPOSITION 1", "content": "Before proving proposition 1 we first prove a useful Lemma that states the optimal log partition function $\\log Z_{\\pi_\\theta}(X_t)$ which is the learning goal for a parameterized approach $\\log \\tilde{Z}_{\\pi_t,\\theta\u2019}(X_t)$.\nLemma 1. Given a sample $x_t \\sim p_t(x_t|x_0)$ and the denoising posterior distribution $q_{t,0}(X_0|x_t)$, a local minimizer for estimate for the log partition function $\\log \\tilde{Z}_{\\pi_t}$ using N samples from $x_0 \\sim q_{t,0}(X_0|x_t)$ is given by:\n$\\begin{equation}\n\\log \\tilde{Z}_{\\pi_t} = \\frac{1}{N} \\sum_{i=1}^N \\log (\\frac{P_t(x_0|x_t) R(x)}{q_{t,0}(X|X_t)} ).\n\\end{equation}$\nProof. By definition the log partition function is a constant, let that constant be $\\log Z_{\\pi_t}(X_t) = C$. Then the loss in Eq. 11 is a quadratic in C,\n$\\begin{equation}\n\\mathcal{L} = \\mathbb{E}_{x_0 \\sim \\tau(x_0)} [|| \\log q_{t,0}(X_0|X_t) + C - \\log p_t(x_0|x_t) - \\log R(x_0)||^2]\n\\end{equation}"}, {"title": "C.2 ESTIMATING DDPP-KL WITH REINMAX", "content": "We first provide an algorithmic description below of training using DDPP-KL. We first highlight how the reverse KL objective can be applied to a more general setting beyond just fine tuning before turning to the exact setting of the main paper."}, {"title": "C.3 EQUIVALENCE OF SUB-TRAJECTORY OBJECTIVES", "content": "In this appendix, we detail how to compute an efficient approximation of the loss function that is inspired by the KL divergence between sub-trajectories as found in the GFlowNet literature but adapted for MDMs.\nConsider the trajectory of a sequence: $\\tau(x_{0:t}) := X_1 \\rightarrow \u2026 \\rightarrow X_t \\rightarrow X_{t-1} \\rightarrow \u2026 x_0$. We seek to minimize the joint distribution over the (sub)-trajectories conditioned on a partially masked sample $x_t$:\n$\\begin{equation}\nq_{\\theta\u2019}(X_{0,..., X_{t-1}}|X_t, \\mu_{\\theta}(x_t, t))p_t(x_t) = \\pi_t(X_{0,..., X_{t-1}}|X_t)p(x_t).\n\\end{equation}$\nHere $\\pi_t(X_{1,..., x_{t-1}}|X_t, x_0)$ is defined as,\n$\\begin{equation}\n\\pi_t(X_{0,..., X_{t-1}}|X_t, \\mu_{\\theta}(x_t, t)) = \\frac{p_{\\theta}^{pre}(x_{0,..., x_{t-1}}|x_t)R(x_0)}{Z_{\\pi_\\theta}(X_t)}\n\\end{equation}$\n$\\begin{equation}\nZ_{\\pi_\\theta}(X_t) = \\sum_{X_0,..., X_{t-1}} \\prod_{j=1}^t P_t^{pre}(x_{j-1}|x_j, x_{\\theta})R(x_0)\n\\end{equation}$\nWe minimize the following KL divergence,\n$\\begin{equation}\nD_{KL}(q_{\\theta\u2019}(X_{0,..., X_{t-1}}|X_t, x_0)p_t(x_t)||\\pi_t(X_{0,..., X_{t-1}}|X_t)p(x_t)).\n\\end{equation}"}, {"title": "D ADDITIONAL EXPERIMENTAL DETAILS", "content": "All experiments were performed on a shared"}]}