{"title": "Enterprise Benchmarks for Large Language Model Evaluation", "authors": ["Bing Zhang", "Mikio Takeuchi", "Ryo Kawahara", "Shubhi Asthana", "Md. Maruf Hossain", "Guang-Jie Ren", "Kate Soule", "Yada Zhu"], "abstract": "The advancement of large language models (LLMs) has led to a greater challenge of having a rigorous and systematic evaluation of complex tasks performed, especially in enterprise applications. Therefore, LLMs need to be able to benchmark enterprise datasets for various tasks. This work presents a systematic exploration of benchmarking strategies tailored to LLM evaluation, focusing on the utilization of domain-specific datasets and consisting of a variety of NLP tasks. The proposed evaluation framework encompasses 25 publicly available datasets from diverse enterprise domains like financial services, legal, cyber security, and climate and sustainability. The diverse performance of 13 models across different enterprise tasks highlights the importance of selecting the right model based on the specific requirements of each task. Code and prompts are available on GitHub.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), or foundation models, have garnered significant attention and adoption across various domains due to their remarkable capabilities in natural language understanding and generation. To align with the new era of LLMs, new benchmarks have been proposed recently to probe a diverse set of LLM abilities. For example, BIG-bench (Beyond the Imitation Game benchmark) (Srivastava et al., 2022) and HELM (Holistic Evaluation of Language Models) (Liang et al., 2022) attempt to aggregate a wide range of natural language processing (NLP) tasks for holistic evaluation, but often lack domain-specific benchmarks, particularly for enterprise fields such as finance, legal, climate, and cybersecurity. This gap poses challenges for practitioners seeking to assess LLM performance tailored to their needs.\nEnterprise datasets, though potentially useful as benchmarks, often face accessibility or regulatory issues. Evaluating LLMs with these datasets can be difficult due to sophisticated concepts or techniques needed to convert use case-based inputs to the standard input format of evaluation harness (e.g., BIG-bench or HELM), which indicates the need for standardized metrics and clear performance benchmarks. This highlights the necessity for robust evaluation frameworks that measure LLM performance in specialized domains.\nEmerging enterprise-focused or domain-specific LLMs, such as Snowflake Arctic\u00b9 and BloombergGPT (Wu et al., 2023), are evaluated with limited enterprise application scope and volume. For textual inputs, Snowflake Arctic is assessed on world knowledge, common sense reasoning, and math. However, academic benchmarks often fail to address the complexities of enterprise applications, such as financial Named Entity Recognition (NER), which requires precise domain language understanding. BloombergGPT is evaluated with several finance datasets, mostly proprietary, and does not include the summarization task.\nTo narrow the gap between LLM development and evaluation in enterprises, we present a framework in Figure 1 by augmenting Stanford's HELM that emphasizes the use of enterprise benchmarks that cater specifically to domains such as finance, legal, climate and sustainability, and cyber security. This framework aims to create and adopt standardized benchmarks reflecting real-world application requirements. By enhancing evaluation metrics and providing tailored enterprise-specific prompts, we seek to empower practitioners to assess LLMs more"}, {"title": "2 Related Work", "content": "Recently, researchers have developed several frameworks to assess various capabilities of LLMs. Examples include HELM (Bommasani et al., 2023), MMLU (Hendrycks et al., 2020), Big-Bench (Lewkowycz et al., 2022), EleutherAI (Phang et al., 2022), and MMCU (Zeng, 2023), which are widely used to evaluate LLMs on multiple NLP tasks. Specifically, HELM categorizes potential scenarios and metrics of interest for LLMs. However, these frameworks lack benchmarks and metrics for evaluating LLM performance in enterprise-focused problems. This work leverages the HELM platform, extending its benchmark scenarios and metrics to include task-specific and domain-specific evaluations for LLMs.\nResearchers are developing enterprise LLM's benchmarks in areas such as finance, legal, and sustainability. In FinBen (Xie et al., 2024), Xie et al. provided finance-based benchmark on 24 financial tasks including information extraction, question answering, risk management, etc. The drawback of this benchmark is that its tasks were designed for the Chinese language, and hence its applicability is limited in American market data and English texts. To counter this, Xu et al.(Xu et al., 2024) provided fine-granted analysis on diverse financial tasks and applications for six domains and twenty-five specialized tasks in the Chinese language. However, this work cannot be extended to English and other languages and hence is not widely usable.\nEnterprise benchmarks in legal are upcoming with works like Legalbench (Guha et al., 2024), Lawbench (Fei et al., 2023), and LAiW (Dai et al., 2023). Lawbench is evaluated on multilingual and Chinese-oriented LLMs while LAiW is Chinese legal LLMs benchmark. Legalbench provides a benchmark on reasoning while the others are evaluating legal foundation inference and complex legal application tasks.\nLastly, in the cybersecurity domain, researchers have contributed to benchmarks like SEvenLLM (Ji et al., 2024), CyberBench (Liu et al., 2024), Cyberseceval 2 (Bhatt et al., 2024) and CyberMetric (Tihanyi et al., 2024). These benchmarks provide analysis on tasks like cyber advisory and reasoning, question-answering, and cybersecurity incident analysis. Compared to existing benchmarks,"}, {"title": "3 Enterprise Benchmarks", "content": "This work introduces benchmark datasets from four specific domains: finance, legal, climate and sustainability, and cyber security, where natural language understanding is crucial for productivity and decision-making. All datasets are curated from open data sources to cover a broad range of natural language tasks and diverse industry use cases within these domains. Datasets without reference answers or with fewer than 100 test cases were excluded from the benchmarks. Although the collected tasks are mostly conventional (See also Section 6.1), these tasks are still important in the context of LLM applications. In addition, the combination of such tasks and domain-specific datasets are still rare and understudied. The focus of this paper is in catering a means for practitioners to evaluate the performance of processing domain-specific datasets. This is because it is known that a general domain LLM might suffer from the degradation of performance when it processes domain-specific data because of the unique terminology and knowledge that are only used in a specific industry.\nAs summarized in Table 1, the finance benchmarks include 10 datasets collected from important use cases such as market prediction based on earnings call transcripts, entity recognition for retrieving information from U.S. Securities and Exchange Commission (SEC) filings\u00b2, and understanding news and reports. The tasks range from classification and NER to QA and long document summarization. NER is crucial for many applications in digital finance, and numerical NER is a particularly challenging task for language models. ConvFinQA provides multi-turn conversational financial QA data involving information extraction from tables and numerical reasoning, offering a critical lens for evaluating LLMs' numerical reasoning capabilities.\nSimilarly, the seven legal benchmarks in Appendix/Table 3 contain rich NLP tasks and important use cases, such as legal opinion classification, legal judgment prediction, and legal contract summarization. Climate and sustainability is an emerging domain for LLM applications, including summarizing claims and understanding human concerns like wildfires and climate change. Due to limited open-source datasets with quality labels, three benchmarks are curated, as shown in Appendix/Table 4. Security-related tasks, including classification and summarization of textual documents such as network protocol specifications, malware reports, vulnerability, and threat reports, are curated and shown in Appendix/Table 5."}, {"title": "4 Benchmark Development", "content": "Recent LLMs, primarily based on the decoder-only Transformer architecture, have unique capabilities and limitations, such as in-context learning (few-shot learning) and input token length constraints. Domain-specific benchmark datasets are often designed for conventional machine learning models or different architectures (such as BERT), necessitating adaptations in datasets and task implementations.\nIn HELM, a scenario represents an evaluation task with a specific dataset and corresponding metrics. These adaptations are incorporated into the development of the scenarios."}, {"title": "4.1 Classification Task", "content": "In a classification task, a model is asked to generate the name of a class of the input sample directly as an output. It is better to use natural language words as the class names (e.g., positive/neutral/negative) than to use symbolic names (see the discussion in Section 4.2). One usually needs to provide few-shot examples to ensure that a model does not generate tokens other than the class names.\nFor multi-class classification tasks with more than 20 classes, defining all classes in a prompt and covering them in in-context learning examples is challenging due to input token length limits. This work simplifies the task by selecting the top-k classes based on their distributions, where k is typically less than 10.\nIn addition to HELM's built-in micro- and macro-F1 scores, the Weighted F1 score as implemented in (scikit-learn developers, 2024) is added as a performance metric."}, {"title": "4.2 Named Entity Recognition Task", "content": "A conventional NER task is formalized as a sequence-to-sequence task, where the input is a sequence of tokens. A system classifies whether each token is a part of a named entity and identifies its category (e.g., person, location, organization, etc.). Then the system generates a sequence of corresponding tags (so-called BIO tags) in the same order as the input tokens(Cui et al., 2021). However, in our preliminary experiments, this approach did not work well with LLMs. This seems to be because BIO tags are unknown to pre-trained LLMs. In addition, few-shot examples consume many tokens if the inputs and the tags are provided in a seq-to-seq manner. The former issue can be overcome by formalizing the task as a translation to an augmented natural language(Paolini et al., 2021), but the latter issue still exists.\nDue to these difficulties, a simplified approach used in the evaluation of BloombergGPT(Wu et al., 2023) for NER tasks is employed in this work. Instead of generating a sequence of BIO tags, a model extracts only named entities and their categories, and answers those in a list of paired natural language phrases (e.g., New York (location), etc.). There is another report that such use of natural language words improves NER performance in a low-resource domain because of a transfer of pre-trained knowledge from the general language domain(Cui et al., 2021). In some scenarios, the number of categories is reduced, as explained in the previous section. Even with these simplifications, a NER task requires more in-context learning examples than a classification task.\nTo support the above task of extracting named entities, a new metric called Entity F1 is added. For each test sample, predicted named entities and the categories of those are compared with those in the ground-truth, to compute true positives, false positives, and false negatives. Those are aggregated"}, {"title": "4.3 Question and Answering Task", "content": "There are several types of QA tasks, some of which overlap with information retrieval tasks. In many business applications, one is requested to answer a question based on a given set of documents (e.g., product manuals, FAQs, medical papers, regulations, etc.). This involves a ranking of answer candidates with respect to their relevance to the user's question. However, LLMs struggle with these operations because handling multiple answer candidates in a single prompt consumes many tokens.\nAlternatively, the \"point-wise\" approach provided in HELM is adopted(Liang et al., 2022). For a question qi, there are k pre-defined answer candidates {aij|j = 1,\u2026k} and one prompts the following question to a model: \"Does aij answer the question qi? Answer in yes or no.\" From this prompt, one can obtain a pair of the output text bij \u2208 {\"yes\", \"no\"} and its log probability Cij from the model. Note that the log probability of a token is available in APIs of the most of public LLM services, and plays a role of the confidence of the output. As a result, one collects k output pairs {(bij, Cij)} for one question qi. Then, one can rate an answer candidate that has bij = \"yes\" with higher Cij to be higher and that has bij = \"no\" with higher Cij to be lower, which defines the ranking."}, {"title": "4.4 Summarization Task", "content": "In a summarization task, one needs to handle a long document as an input. Therefore, the input token length limit becomes a severe issue. In this study, this issue is handled by selecting relatively shorter samples and truncating the end of the samples to preserve the original context as much as possible.\nIn this study, conventional ROUGE scores are used as performance metrics."}, {"title": "5 Experiments and Results", "content": "This evaluation is conducted by augmenting HELM's framework to encompass 25 publicly available task datasets from multiple domains, namely financial, legal, climate and cyber security. For each benchmark, the evaluation is done on a specific configuration. The intention of this section is to demonstrate the usefulness for practitioners of our benchmarks in evaluating candidate models with their own settings."}, {"title": "5.1 Evaluated Models", "content": "Here, the evaluation models are selected from the best-performing open-sourced models under 70 billion parameters based on model size, type of training data, accessibility, and model tuning method. Specifically 1) LLaMA2 (Touvron et al., 2023) is a series of advanced LLM ranging from 7 billion to 70 billion parameters. It is designed to perform well across various tasks and is notable for its versatility and robustness in natural language understanding and generation. 2) GPT-NeoX-20B (Black et al., 2022) is particularly known for its impressive performance in text generation tasks, providing a strong alternative to proprietary models due to its open-source nature and large model size. 3) FLAN-UL2 (Tay et al., 2023) is another state-of-the-art model that has been fine-tuned with a focus on instruction understanding and task completion. It excels in few-shot and zero-shot learning scenarios, making it highly effective for various practical applications. 4) Phi-3.5 (Abdin et al., 2024) is a family of powerful, small language models (SLMs) with groundbreaking performance at low cost and low latency. 5) Mistral 7B (Jiang et al., 2023) is a series of 7-billion-parameter language models engineered for superior performance and efficiency. 6) The granite.13b models are set of the latest open-sourced enterprise-focused models. The datasets used in this second tranche of training tokens includes some finance and legal datasets, such as FDIC, Finance Text Books, EDGAR Filings, etc. The base model serves as the foundation for two variants: granite.13b.instruct.v2 and granite.13b.chat.v2.1. Granite.13b.instruct has undergone supervised fine-tuning to enable better instruction following (Wei et al., 2021), making it suitable for completing enterprise tasks via prompt engineering. Granite.13b.chat uses novel alignment methods to improve generation quality, reduce harm, and ensure outputs are helpful and follow social norms (Conover et al., 2023; Bai et al., 2022; Kim et al., 2022).\nAll the 13 models are evaluated in our benchmarks, regardless of the purposes of the models (i.e., for chat, etc.). As we will see in the following sections, the relation between the performance of a task and the intended purpose of a model is not straightforward."}, {"title": "5.2 Evaluation Setup", "content": "In this study, the data source-provided train and test splits are used whenever possible. Model performance is reported based on test or validation examples, depending on the availability of test labels. If the train and test splits do not exist, a task-specific ratio of the data is selected as the test split, with the remainder used as the train split. In-context learning examples are sampled from the train split. The number of few-shot examples provided to the model varies by task and is detailed in Appendix/Tables 1, 3, 4, and 5. Note that, by default in HELM, only one set of randomly sampled examples is used across all test cases of a given benchmark. If the training context examples are unfortunately not good, the performance of all the models will be affected. A contextual sampling strategy based on nearness to the test sample through some similarity measure is future work.\nFor in-context learning, this work adopts HELM's sampling strategy, which includes samples from minority classes. This is different from a conventional uniformly random sampling, where samples in a minority class tend to be ignored in the case of a few-shot sampling. This sampling strategy is especially important when the distribution of the labels is imbalanced and its application needs a capability to detect the minority class samples (e.g., anomaly detection). However, this strategy has less impact when the focus is on accurately classifying majority class samples.\nFor the current evaluation, all the models use the same parameters and the same context examples. Standard prompts (see the techniques of few-shot-prompting and zero-shot-prompting and examples of prompts), without chain-of-thought prompting (Wei et al., 2023), or system prompts are used. For News Headline and FiQA SA, the prompts are taken from BloombergGPT (Wu et al., 2023). The prompts for each scenario are shown in Appendix A.3. To ensure reproducibility, a fixed random seed and the greedy decoding method without repetition penalty are used. After generating the output, standard text normalization (i.e., moving articles, extra white spaces, and punctuations followed by lowering cases) is applied before matching texts. In ConvFinQA, a regular expression is used to match the floating-point numbers."}, {"title": "5.3 Evaluation Results", "content": "Finance Benchmark The results presented in Table 2 provide the evaluation results of 13 models across a range of financial NLP tasks, including classification, NER, QA, and summarization. Each task was assessed using the best-fitted metrics to determine the performance of different models.\nFor classification tasks, the highest Weighted F1 score was achieved by the granite.13b.instruct.v2 model in the Earnings Call Transcripts classification, demonstrating its strong performance in extracting relevant information from earnings calls. For News Headline classification, the llama2.70b.chat model achieved the highest Weighted F1 score, indicating its effectiveness in handling short text classification tasks.\nNER was evaluated using three different tasks: Credit Risk Assessment, KPI-Edgar, and FiNER-139. The llama2.70b.chat model outperformed others in Credit Risk Assessment with the highest Entity F1 score. In KPI-Edgar, the llama2.70b model achieved the best Modified Adjusted F1 (Adj F1) score, while gpt-neox-20b led in FiNER-139 with the highest Entity F1 score, showcasing its capability in identifying financial entities accurately.\nAmong the diverse QA tasks, the flan-ul2 model excelled in FiQA-Opinion and Insurance QA with the highest RR scores, highlighting its proficiency in answering complex questions with limited context. For FiQA SA, the llama2.70b.chat model obtained the highest Weighted F1 score. The granite.13b.chat.v2.1 model stood out in ConvFinQA with the highest accuracy, indicating its robustness in handling multi-turn financial QA tasks involving numerical reasoning.\nFor Text Summarization, the flan-ul2 model achieved the highest Rouge-L score, demonstrating its ability to generate concise and relevant summaries from financial texts. Compared to other domains, the financial benchmark evaluation extensively encompasses all kinds of LLM tasks.\nLegal Benchmark The results in Appendix/Table 6 highlight the performance of various models across legal tasks. For classification, the mistral-7b-instruct-v0-3 model excelled in Legal Sentiment Analysis (0.727 Weighted F1) and UNFAIR-ToS (0.720 Weighted F1), while granite.13b.instruct.v2 led in Legal Judgement Prediction (0.863 Weighted F1). In QA, granite.13b.instruct.v2 achieved the highest F1 score (0.790) in the CaseHOLD task. For"}, {"title": "6 Conclusion and Future Work", "content": "In summary, this work advances the evaluation of LLMs in domain-specific contexts by consolidating benchmark datasets and incorporating unique performance metrics into Stanford's HELM framework. This enables researchers and industry practitioners to assess and optimize LLMs for specific domains. We demonstrated the effectiveness of these evaluations on widely used 13 LLMs through extensive experiments on 25 publicly available benchmarks in financial, legal, climate, and cyber security domains, providing practical prompts for practitioners. Our analysis offers valuable insights and highlights future needs for benchmarking"}, {"title": "6.1 Limitations", "content": "In this study, the same prompts are used for evaluating all models and scenarios. Recent LLMs suggest model-specific prompts with meta-level information. However, HELM's model-agnostic architecture complicates applying different prompts to different models. Future work will explore emerging applications such as Retrieval-Augmented-Generation (RAG) (Yu et al., 2024) and Chain-of-Thought (CoT) question answering (Suzgun et al., 2023) to optimize prompts. Evaluation of different aspects of industry needs on LLMs such as instruction following, multi-lingual capabilities including machine translation, and a case study are also included in future work."}, {"title": "7 Acknowledgments", "content": "This work is funded by IBM Research and MIT-IBM Watson AI Lab. We would like to thank David Cox, Rameswar Panda, Maruf Hossain, Naoto Satoh, Futoshi Iwama and Alisa Arno for their guidance and constructive comments. The views and conclusions are those of the authors and should not be interpreted as representing that of IBM or the government."}, {"title": "A Appendix", "content": "Prompts that are used in the experiments are shown in this section. Figures 2 to 5 shows the prompts for finance, legal, climate and sustainability, and cybersecurity scenarios, respectively.\nA prompt consists of an \"instruction\" block, which is shown above a dotted line, and an \"input-output\" block, which is shown below the dotted line. The instruction block contains an instruction, which is placed at the beginning of a prompt. Some scenarios may not have the instruction block. The input-output block contains a pair of the input and output of each sample. This is located after the instruction block. Within a block, a text enclosed with curly brackets { ... } is replaced with an input text of each sample. A text enclosed with square brackets [...] is a placeholder of the generated text by an LLM as an output. In the case of a few-shot learning setting, the input-output block can be used to show a training example for in-context learning. In that case, the placeholder of the output is filled with the ground truth label of the sample. Such instances of input-output blocks that correspond to the few-shot examples are iterated after the instruction block for n times, where n is the number of the shots of the in-context learning. After the in-context learning examples, another input-output block is placed without filling the output with a ground truth label."}, {"title": "A.1 Benchmarks Overview", "content": "The data tables summarize key benchmarking information. Each table includes the Task, which specifies the problem, and the Task Description, explaining its nature. The Dataset column names the data used, with the Dataset Description detailing its characteristics. The N-shot Prompt indicates the number of examples used for few-shot learning, which is provided to ensure consistency and replicability. Lastly, the Metric column outlines the evaluation metrics used to measure model performance. Table 1, and 3 to 5 present the overview of benchmarks in the domain of finance, legal, climate and sustainability, and cybersecurity, respectively."}, {"title": "A.2 Evaluation Results", "content": "Table 6 to 8 shows the LLMs evaluation results of legal, climate and cyber security benchmarks. We have discussed these results in section5.3."}, {"title": "A.3 Prompts", "content": "Prompts that are used in the experiments are shown in this section. Figures 2 to 5 shows the prompts for finance, legal, climate and sustainability, and cybersecurity scenarios, respectively.\nA prompt consists of an \"instruction\" block, which is shown above a dotted line, and an \"input-output\" block, which is shown below the dotted line. The instruction block contains an instruction, which is placed at the beginning of a prompt. Some scenarios may not have the instruction block. The input-output block contains a pair of the input and output of each sample. This is located after the instruction block. Within a block, a text enclosed with curly brackets { ... } is replaced with an input text of each sample. A text enclosed with square brackets [...] is a placeholder of the generated text by an LLM as an output. In the case of a few-shot learning setting, the input-output block can be used to show a training example for in-context learning. In that case, the placeholder of the output is filled with the ground truth label of the sample. Such instances of input-output blocks that correspond"}]}