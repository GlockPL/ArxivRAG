{"title": "Privacy-hardened and hallucination-resistant synthetic data generation with logic-solvers", "authors": ["Mark A. Burgess", "Brendan Hosking", "Roc Reguant", "Anubhav Kaphle", "Mitchell J. O'Brien", "Letitia M.F. Sng", "Yatish Jain", "Denis C. Bauer"], "abstract": "Machine-generated data is a valuable resource for training Artificial Intelligence algorithms, evaluating rare workflows, and sharing data under stricter data legislations. The challenge is to generate data that is accurate and private. Current statistical and deep learning methods struggle with large data volumes, are prone to hallucinating scenarios incompatible with reality, and seldom quantify privacy meaningfully. Here we introduce Genomator, a logic solving approach (SAT solving), which efficiently produces private and realistic representations of the original data. We demonstrate the method on genomic data, which arguably is the most complex and private information. Synthetic genomes hold great potential for balancing underrepresented populations in medical research and advancing global data exchange. We benchmark Genomator against state-of-the-art methodologies (Markov generation, Restricted Boltzmann Machine, Generative Adversarial Network and Conditional Restricted Boltzmann Machines), demonstrating an 84-93% accuracy improvement and 95-98% higher privacy. Genomator is also 1000-1600 times more efficient, making it the only tested method that scales to whole genomes. We show the universal trade-off between privacy and accuracy, and use Genomator's tuning capability to cater to all applications along the spectrum, from provable private representations of sensitive cohorts, to datasets with indistinguishable pharmacogenomic profiles. Demonstrating the production-scale generation of tuneable synthetic data can increase trust and pave the way into the clinic.", "sections": [{"title": "Introduction", "content": "Synthetic data is emerging as a valuable resource in the medical sector \u00b9, driven by stricter data sharing and privacy regulations\u00b2, as well as the need for bioinformatics standardisation through characterized reference samples\u00b3. Synthetic genomes are of particular interest due to the high cost of obtaining data through sequencing and the risk of exposing personally identifiable or sensitive information when sharing real genomic data. Use cases for synthetic genomes include (a) \"digital twin\" dataset\u00b9 where population-specific genomic information is reproduced without replicating any one individual specifically, (b) \u201cTruth Challenges\u201d, where synthetic genomes are spiked with difficult to identify variants for grading pathology providers, and (c) methodology benchmark for evaluation, or for capturing the \u201clong tail\u201d of human diversity and disease states\u00b9.\nGartner expects synthetic data to \"completely overshadow real data in Al models by 2030\"8, illustrating the need for creating high quality synthetic data to ensure the subsequently trained models have fidelity, generalisability, diversity, and compliance. We discuss three key considerations for this. Firstly, the risk of producing skewed, incomplete, or discriminatory synthetic genomes exists"}, {"title": "Methods", "content": ""}, {"title": "2.1 Datasets", "content": "For evaluating population-structure we used the 805 SNPs from across the genome as selected by Yelmen et al. 15 to represent the population structure of the 2504 samples in the 1000 Genomes"}, {"title": "2.2 Software tools", "content": "Genomator constructs and solves a SAT problem to produce synthetic genomic data using real genotypic information captured in a Variant Call Format (VCF) file. The central rationale of Genomator's algorithm is to ensure that pairs of features that do not occur in the input data will also not occur in the synthetic data (see Figure 1).\nThe task is performed by constructing a set of constraints from the real genomes for the SAT solver to resolve into synthetic genomes. Genomator hence does not require extensive training but instead creates synthetic genomes iteratively from subsets of input data. This subset is picked at random from a set of clusters of size N created based on Hamming distance between the pairwise genotype vectors. This process can be repeated and is efficient enough to create tens of thousands of synthetic genomes, with the randomness in the clustering process creating overlapping clusters, which ensures that no sample is under-represented and diverse population-scale cohorts can be created.\nThere are two parameters, N and Z, controlling the size of the input clusters, and the strength of the attenuation of rare feature pairs, respectively. Increasing N makes the formed constraints less stringent as there are likely less feature pairs that none of the input data possess. The parameter Z can be randomised with each feature pair considered. Increasing Z hence stochastically reduces the contribution of rare (and potentially identifying) combinations of SNPs in the constraint creation and thus enhances privacy.\nSupplemental Section 1 explains the workflow in full, including pseudocode and detailed diagrams. Genomator is implemented using PySAT library\u00b3\u00b9 and, unless otherwise stated, is run with a cluster size of N = 10 and Z = 0.\nReverse Genomator complements Genomator and deductively determines possible combinations of input genomes (of size N) which could have been used by Genomator to construct the produced synthetic data (Figure 1). Reverse Genomator reverses Genomator's logic by constructing a SAT problem with constraints that eliminate input combinations from which Genomator could not have generated the synthetic data with.\nMarkov Chain method as implemented by Yelmen et al.15 was used with a window size parameter of 10 variants.\nRestricted Boltzmann machine (RBM) as implemented by Yelmen et al. 16 was used with a hidden layer size of 500, 50 Gibbs steps and 500 parallel chains to compute the negative term of the gradient in the training process, with a minibatch size of 500 and a learning rate of 0.01, data was trained till 1200 epochs.\nConditional Restricted Boltzmann machine (CRBM) implementation from Yelmen et al. 16 was used with a hidden layer size of 500, 50 Gibbs steps and 500 parallel chains to compute the negative term of the gradient in the training process, with a minibatch size of 500 and a learning rate of 0.005, data was trained till 1200 epochs with a window overlap of 300 (reduced to from the originally published value of 5000 to fit our dataset).\nGenerative adversarial network (GAN) implementation by Yelmen et al. from 202115 where they used a latent size of 600, generator learning rate of 0.0001, discriminator learning rate of 0.0008, with 2 intermediate layer dense neural networks for generator and discriminator with sizes a factor 1.2, 1.1 and 2, 3 smaller than the data's dimension respectively, with leakyReLU neural activation function with an alpha value of 0.01, GANs were trained for the suggested 20000 epochs - for investigation of the variability in this method we also optionally added a multiplier on the size of all"}, {"title": "2.3 Accuracy calculation", "content": "Wasserstein distance is used for analysing the consistency between datasets. We employ the Sliced Wasserstein distance algorithm in the Python Optimal Transport library30. In Section 3.1, to quantify the differences in the PCA of the synthetic versus real data on the Yelmen 805 SNP dataset, we calculated the Wasserstein distance on the first two principal components between synthetic and real. In Section 3.4 we evaluate the data itself, hence calculated the Wasserstein distance across all dimensions between input and synthetic data and reported percentage error as proportion of the maximal of 800 haplotypes (400 SNPs of ploidy 2).\nLD correlation error used for Figure 2b was calculated from Pairwise LD and evaluated using the Rogers-Huff r-squared method29, which is part of the Scikit-Allel Python package. LD reproducibility between a synthetic and real dataset is quantified by average LD square error, obtained by subtracting the pairwise synthetic LD values from the true LD relationship and squaring the result and averaging. We evaluated synthetic data LD reproducibility generated from a training set against a test set - produced by equal split partition of datasets. We also calculated the difference in LD between synthetic and real data for all SNP pairs in a window size averaged across the tested genome. We systematically test window size ranges from 5000 to the maximal gene length in 5000 increments."}, {"title": "2.4 Privacy calculation", "content": "Attribute inference-based privacy evaluation emulates an inference attack scenario. We split the original genome dataset into two random subsets and generate synthetic data from each. Next, we compute the median Hamming distance between the input sequences and to the synthetic dataset created from the subset containing the input sequence (in-data) and the subset that does not contain it (out-data). The difference between the in-data and out-data distances reflects the additional likelihood that an attacker correctly identifies SNP attributes. This approach reflects Giomi et al 32 (inference attack) except baselining performance against synthetic data from a hold-out set, instead of random imputations.\nPrivate and fictitious quadruplet revelation measures privacy by how likely rare combinations of SNPs are reproduced in the output. To do this, we randomly select SNP combinations in groups of four (quadruplets) and assess if they are private - meaning the combination occurs at most once in the original data - or fictitious, meaning it does not appear at all in the original data. We continue sampling until we have 100K fictitious and private combinations each. Next, we generate 1K versions of synthetic data and determine how many of the 100K SNP combinations occur across these 1K datasets to compute the likelihood for each category, i.e., private or fictitious. This reflects Giomi et al 32 ('singling out') except identifying unique combinations among the input data for comparison to the synthetic data, instead of vice versa.\nExposure risk deduces privacy directly and is unique to our logic-based approach. We randomly select G SNPs and N individuals from a real dataset (1KG project containing 3.2K individuals). We then generate 500 distinct plausible input combinations (of same size N) for a synthetic genome"}, {"title": "Results", "content": ""}, {"title": "3.1 Accuracy: SAT-solvers replicate both genome-wide and local genomic structure.", "content": "First, we evaluate the ability of the methods to produce synthetic data that captures the higher- order complexities of genomic data. We ran a principal component analysis (PCA) on the synthetic data generated from the Yelmen 805 SNP dataset (see Section 2.1) to measure how well each reproduces the well-known \u201cV\u201d shape of the underlying population structure. At this time synthetic samples were generated on a training set and plotted against a holdout set of individuals from the same cohort.  shows that Genomator reproduces the population structure accurately (average Wasserstein Score of 1382\u00b1326) whereas RBM (1517\u00b1624), GAN (2010\u00b1497) produce synthetic data that are dispersed and slightly shifted from the real data. Markov Chain (7249\u00b153) and CRBM (9751\u00b1109) perform poorly, showing clustering of synthetic samples and loss of the characteristic V shape. Supplemental Tables 1a-e illustrates the stability of the methods over 10 runs, the PCA of the median run is plotted in  and PCA on larger datasets are in Supplemental Section 11.\nWe evaluate the methods' abilities to replicate local interactions as these are of particular importance to medical and research applications33. We produced 1000 synthetic genomes for each method, focussing on the largest genes reproducible by all models (RBFOX1, FHIT, AGBL4 and CCSER1), and compared the linkage disequilibrium (LD) patterns constructed to the original dataset.  shows that for AGBL4 gene, Genomator captures both short distance LD as well as long distance LD. Markov chains only reproduce short distance LD, while RBM, CRBM and GAN capture both short- and long-range interactions but with visibly lower accuracy than Genomator. We quantify this for all genes in Figure 2c by calculating the average square error between the real and reproduced LD with increasing window sizes across the gene, i.e. small windows evaluate the capability to capture local SNP-SNP effects, while larger window sizes cover local as well as long- ranged interactions. Genomator has an average square error percentage of 2% (std 0.13%) over all genes compared to RBM of 11.4% (3.6%), CRBM of 17.1% (9.4%), Markov Chain of 21.6% (15.3%), and GAN of 19% (14%); for a detailed breakdown see Supplemental Table 2a and b with correlation measures provided in Supplemental Table 3."}, {"title": "3.2 Efficiency: Runtime Statistics and trends", "content": "We evaluate the runtime of generating a synthetic genome from each of the methods. We included all training and processing, but excluded input and output file operations, as these are dominated by the initial loading of the VCF file and final writing of the synthetic data, which is common to all methods and increases with file size. We report the runtime for creating a synthetic genome from increasing input file sizes, up to the full 11 million SNPs, constituent of the full human genome (excluding sex chromosomes) from the 1KGP project. For this experiment we produced different VCF files of 400 human individuals with increasing genome segments across chromosomes 1-22 with MAF > 0.01 filtering. We ran all tests with Intel Xeon(R) CPU Platinum 8452Y, 2.00GHz, 36 cores, 80 GB of ram, with GPU Nvidia H100 94GB graphics card and two days of compute time.\n shows that Genomator is the only method scalable to the largest file size \u2013 producing synthetic genome across all human chromosomes in 18 seconds. RBM&CRBM ran out of graphics card VRAM above 1.6M SNPs, GAN ran out of VRAM above 6.4K SNPs, and Markov method ran out of RAM above 6.5M SNPs. On the 1638K SNPs completed by all but GANs, Genomator is 1067 to 1693 times (5 seconds vs 5283 \u2013 8383 seconds) faster than the other tested methods - see Supplemental Table 5."}, {"title": "3.3. Privacy", "content": "Many approaches use similarity tests, including Yelmen et al. 16 who uses nearest neighbour adversarial accuracy for privacy evaluation. This is based on the balanced accuracy of an adversarial classifier that attempts to determine if a given data point is closer to the real data cluster than to the synthetic cluster34. However, Stadler et al.35 showed that these methods underestimate the privacy risks. We hence considered three approaches (based on attribute inference, private quadruplet revelation, and a direct deductive privacy inspection of the output) as introduced over the next sections.\nFirstly, we simulate an attribute inference attack, where an attacker knows a subset of a target's genome and uses the nearest neighbour in the synthetic cohort to infer the remaining SNPs, similar to Giomi et al 32. We calculated how accurate such an attacker would be on the datasets generated over the Yelmen 805 SNP dataset, as the down-scaled representation of the genome.\nWe split the data into two similar subsets by random selection. We calculated the \u201cin-data\" distance between the real data and the synthetic data generated from the subset containing the target individual, while the \"out-data\" distance was calculated on the cohort without the target individual using Hamming distance (Supplemental Figure 9). Ideally, in-data distance is small indicating a good fit, while the difference between in-data and out-data distance is zero. The difference between in-data and out-data distances can be interpreted as a measure of privacy, as the additional likelihood that an attacker could make a correct inference about a SNP of a target sequence owing due to the synthetic data being generated from a dataset including that target sequence."}, {"title": "3.4 Genomator can customise the level for accuracy-privacy trade-off", "content": "Currently, privacy is estimated through proxy-based methods as it is impractical for iterated learning methods like GANs and RBMs to deductively infer whose data was used in the synthetic data generation process. Since Genomator is a logic-based approach, reverse logic is possible. We hence developed Reverse Genomator, which identifies for any synthetic genome, the logical space of all possible combinations of subsets of input data that could have been used to generate it (given the full information about Genomator and a dataset from which Genomator's cluster input is selected from). Individuals who appear in all these subsets are deduced to have been used as input and are considered 'privacy exposed' (see Methods 2.4)."}, {"title": "3.5 Utility: Reproduction of well-known pharmacogenetic SNPs in large synthetic datasets", "content": "Genomator was used to create a synthetic dataset of 1000 individuals across the 1.11M SNPs of chromosomes 10 and 16. Creating whole chromosomes was feasible with the available resources for Genomator but would have been impossible for the other methods as shown in Section 3.2. We hence omitted them from this analysis. Chromosomes 10 and 16 were chosen as they include three"}, {"title": "4. Discussion", "content": "While all methods were able to produce genomes that broadly reflect the input data, Genomator more effectively captures secondary statistics, such as gene-gene correlations (i.e. LD) because it reasons directly over all pairwise associations in its generation process. By contrast, GAN and RBM are limited in the amount of information that they can reproduce by the size of their network and"}, {"title": "5. Conclusion", "content": "In this study we presented Genomator and demonstrated that it is possible to generate synthetic data using a SAT solver to reproduce genomic information. Our technique is accurate, scalable, and computationally efficient as well as configurable to retain the genomic privacy of the individuals in the source dataset. Additionally, we have developed Reverse Genomator to deductively and logically inspect the output from Genomator and calculate the absolute privacy afforded by Genomator.\nThe use of private synthetic data in lieu of real data may allow institutions and biobanks to share genomic information more liberally and advance health applications and knowledge."}]}