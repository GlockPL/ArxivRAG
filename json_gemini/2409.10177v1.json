{"title": "AUGMENTING AUTOMATIC SPEECH RECOGNITION MODELS WITH DISFLUENCY DETECTION", "authors": ["Robin Amann", "Zhaolin Li", "Barbara Bruno", "Jan Niehues"], "abstract": "Speech disfluency commonly occurs in conversational and spontaneous speech. However, standard Automatic Speech Recognition (ASR) models struggle to accurately recognize these disfluencies because they are typically trained on fluent transcripts. Current research mainly focuses on detecting disfluencies within transcripts, overlooking their exact location and duration in the speech. Additionally, previous work often requires model fine-tuning and addresses limited types of disfluencies.\nIn this work, we present an inference-only approach to augment any ASR model with the ability to detect open-set disfluencies. We first demonstrate that ASR models have difficulty transcribing speech disfluencies. Next, this work proposes a modified Connectionist Temporal Classification(CTC)-based forced alignment algorithm from [1] to predict word-level timestamps while effectively capturing disfluent speech. Additionally, we develop a model to classify alignment gaps between timestamps as either containing disfluent speech or silence. This model achieves an accuracy of 81.62% and an F1-score of 80.07%. We test the augmentation pipeline of alignment gap detection and classification on a disfluent dataset. Our results show that we captured 74.13% of the words that were initially missed by the transcription, demonstrating the potential of this pipeline for downstream tasks.\nIndex Terms- Automatic speech recognition, speech disfluency, forced alignment", "sections": [{"title": "1. INTRODUCTION", "content": "Speech disfluency refers to interruptions in the flow of speech, such as repetitions, interjections, and revisions. It is a natural part of conversational and spontaneous speech but can be particularly pronounced and frequent in certain speech disorders, such as stuttering [2, 3, 4]. Analyzing speech disfluency can aid in diagnosing speech disorders. It can also help in understanding language proficiency that can be applied, for example, in interviews and children's education.\nManually annotating speech disfluency for analysis is costly, and Automatic Speech Recognition (ASR) can support the annotation process. The ASR systems transcribe the speech into readable text, which can then be passed to the evaluator or automatic evaluation pipelines for analysis. However, ASR models show performance degradation in disfluent speech, because the models are developed to generate fluent transcripts to enhance readability [2, 5].\nTo detect speech disfluency with ASR models, one popular approach is post-processing the ASR predictions as a sequence labelling problem [6, 7, 8, 9, 10]. Alternatively, [11, 12, 13, 14, 15, 16] focus on jointly predicting transcriptions and disfluencies with end-to-end speech recognition. In addition, [17] explores adapting the ASR foundation models, which are robust to recognize unfinished words, to detect disfluencies. However, those approaches only detect disfluency within the transcript while neglecting the location and duration of the speech disfluency, which plays an important role in the disfluency analysis, e.g. for the assessment of interlocutors' alignment in collaborative activities [18].\nRecent work focuses on detecting speech disfluencies at"}, {"title": "2. DISFLUENCY DETECTION", "content": "The inference-only pipeline to augment ASR models with open-set disfluency detection consists of three hierarchical steps (Figure 1). In the first step, the ASR system generates an estimated transcript, and a feature extractor model produces the frame-wise probability from speech. The ASR model could be the same as the frame-wise feature extractor models, such as Wav2Vec2 [23]. But it can be any ASR model as the augmentation pipeline only needs its transcript.\nAfter that, the pipeline applies a modified (CTC)-based forced alignment algorithm, that is based on [1] with the above generations. The algorithm generates word-level timing information and the signal gaps between the word timesteps are recognized as potential instances of disfluent speech. In the end, a developed classification model is applied to identify alignment gaps containing disfluent speech or only silence. The classification results can be utilized for downstream tasks like second-step transcription and identifying disfluency types."}, {"title": "2.1. Augmentation pipeline", "content": "The inference-only pipeline to augment ASR models with open-set disfluency detection consists of three hierarchical steps (Figure 1). In the first step, the ASR system generates an estimated transcript, and a feature extractor model produces the frame-wise probability from speech. The ASR model could be the same as the frame-wise feature extractor models, such as Wav2Vec2 [23]. But it can be any ASR model as the augmentation pipeline only needs its transcript.\nAfter that, the pipeline applies a modified (CTC)-based forced alignment algorithm, that is based on [1] with the above generations. The algorithm generates word-level timing information and the signal gaps between the word timesteps are recognized as potential instances of disfluent speech. In the end, a developed classification model is applied to identify alignment gaps containing disfluent speech or only silence. The classification results can be utilized for downstream tasks like second-step transcription and identifying disfluency types."}, {"title": "2.2. Forced alignment", "content": "As the key step to extract timing information, three forced alignment approaches are employed: the standard CTC forced alignment, a modified CTC forced alignment, and the cross-attention approach of Whisper, one of the SOTA ASR models, whose attention value exhibits a high degree of correlation with timestamps."}, {"title": "2.2.1. Standard CTC Alignment", "content": "CTC-based forced alignment is a popular approach to extracting timing information used in many speech recognition packages, such as ESPnet [24], SpeechBrain [25] and Flashlight [26]. The alignment is calculated in three steps 2:\n1. The audio is fed into a feature extraction model that is pre-trained with CTC to generate frame-wise label probability over the whole alphabet.\n2. From the probability, a trellis matrix is generated to represent the probability of labels occurring at each time frame. The trellis at point $(t, j)$ (where $0 \\leq t \\leq T-1$ and $0 \\leq j \\leq U \u2013 1$) represents the maximal probability that the first $j-1$ labels of the transcripts are aligned to the first $t-1$ timeframes of the audio. The trellis is calculated in the log domain to avoid numerical instability.\nThe maximum probability that the first j labels of the transcript are aligned at the timeframe t is the maximum of 1) Stay on the label, which means the first j labels of the transcript are already aligned at time"}, {"title": "2.2.2. Modified CTC Alignment", "content": "Our preliminary experiments show that the standard CTC alignment struggles to generate correct information when the automated transcription does not include the disfluency. As Figure 2 shows, the manual transcript of this example contains the disfluency, which is removed by the ASR model for better readability. The standard CTC alignment extends the alignment around incomplete words, leading to inaccurate alignment and missing disfluency detection. This occurs because the standard CTC algorithm tends to align a word for a longer duration rather than to align silence where something is being said. In trellis generation (refer to Equation 1), the emissions for a blank token in this part of the audio are very low, as something was spoken there. As a consequence, gaps in the alignment of the transcript may not occur where they should, which is undesirable for this application.\nTo counteract this issue, we proposed the modified CTC alignment alignment to enable the model to detect the alignment gaps containing the speech of untranscribed disfluency. In trellis generation, the modified Equation 2 is applied if the current label j is a space token. The space token infers a special label used to represent gaps or spaces between characters or tokens in the output space. With modification, the modified probability of staying on this label is the maximum of the probability acquired through the emissions and a predefined probability c. This modification incentivises the algorithm to extend silence between words to some extent.\n$\n\\text{trellis}[j, t] = \\max(\\text{trellis}[j, t \u2013 1] .\\\n\\max(\\text{prob}[t, e], c), \\text{trellis}[j - 1,t \u2013 1] \u00b7 \\text{prob}[t, j])\n$\nAs for the previous examples shown in Figure 2, the modified CTC alignment correctly aligns the speech signals to the ASR prediction while capturing gaps that correspond to untranscribed disfluencies. However, it is also important to note that the modified alignment for, e.g., \"that\" has also become shorter. This is because this modification encourages the algorithm to keep words short since an alignment containing much silence gives a better score. Setting the probability c too high may result in words being too short overall. Therefore, we experiment with different predefined probabilities to choose the value carefully."}, {"title": "2.2.3. Cross-attention Alignment", "content": "Whisper is trained in such a way that there exists a correlation between the cross-attention weights and the audio input. Consequently, the cross-attention weights and the output transcript allow for the calculation of alignment to the input audio through Dynamic Time Warping (DTW) [27]. As implemented in the HuggingFace library 3, token-level timesteps are calculated using the encoder-decoder cross-attentions and DTW to map each output token to a position in the input audio."}, {"title": "2.3. Alignment Comparison Metric", "content": "Comparing the alignment approaches requires an automatic metric, which is not available. This work proposes a metric that evaluates the alignment between the manual and automatic transcripts by considering the position and length of aligned words. The correctly transcribed words are extracted using Levenshtein Alignment, and we denote $(s_1, e_1)$ and $(s_2, e_2)$ as the manual timing annotation and automatically aligned timing information.\nAfter forced alignment, timing information of the same word from manual annotation $A1$ and automated transcription A2. For each pair of words $(w_1, w_2)$, the length, position and"}, {"title": "2.4. Gap classification", "content": "The forced alignment detects the alignment gaps, while the gaps can contain speech or only silence. Accordingly, we propose a classification step to identify alignment gaps where disfluent speech may occur.\nSince the timing information of the disfluent speech is not available in the dataset, we define an alignment gap containing disfluent speech as one that covers at least one word. We define the coverage as the duration of the word has more than 50% overlapping with the duration of the alignment gap. The 50% overlap criterion strikes a balance: Considering only words completely within the gap would result in many gaps being deemed empty, despite there being speech intuitively present. Conversely, if a gap is considered non-empty as soon as a word is even partially within it, the transcription model may find it challenging to transcribe this word in the subsequent step.\nClassification with all gaps is inefficient and might involve too small gaps due to alignment inaccuracy. Therefore, this work performs classification on gaps that exceed a minimum threshold length. The chosen minimum gap size should not be too small, as minimal inconsistencies in the alignment would become too noticeable. This would result in previously transcribed speech being transcribed again. On the other hand, the minimal gap should not be too large, as this could lead to overlooking too many gaps where untranscribed speech may be present. Based on preliminary experimental results, a gap size of 0.3 is selected as optimal."}, {"title": "3. DATASET", "content": "As this work aims to detect the location and duration of speech disfluencies, the dataset must be composed of spontaneous speech with word-level timing annotation. Besides,"}, {"title": "3.1. Dataset Preparation", "content": "As this work aims to detect the location and duration of speech disfluencies, the dataset must be composed of spontaneous speech with word-level timing annotation. Besides, the dataset must be large enough to act as training data for the classification model. Therefore, we use the Switchboard-1 Release 24 and Treebank 35 datasets. The Switchboard dataset consists of approximately 260 hours of telephone conversations with word-level timing information, and the Treebank 3 dataset adds the corresponding word-level disfluency annotation to the transcripts of the Switchboard dataset."}, {"title": "3.2. Segmenting Audio", "content": "This dataset consists of recordings that are several minutes long. The long recordings hinder forced alignment performance, and the segmentation pre-processing is applied.\nFirst, the audio file is segmented at all points where there is more than 5 seconds of silence. These points provide good places to divide the transcript without interrupting the speaker's flow of speech. However, there are still segments that remain several minutes long.\nIn the next step, as long as the segment is longer than 30 seconds, the largest gap between two words in the middle of the transcript is sought, ensuring that it is at least 10 seconds away from the beginning and end of the segment. This ensures that the resulting segments are between 10 and 30 seconds long. Of course, it is also possible that shorter segments are created when splitting by 5-second pauses."}, {"title": "4. EXPERIMENTS AND RESULTS", "content": ""}, {"title": "4.1. Experimental setups", "content": "This work experiments with the SOTA ASR model Whisper 6 to augment disfluency detection ability. As for frame-wise feature extractor, this work selects Wav2Vec27 which is fine-tuned on English ASR. The ASR model can be the same as the feature extractor model in pipeline design, but we select Whisper as the ASR model because this pipeline supports augmenting any ASR model, and Whispers is a stronger ASR model in terms of accuracy and robustness for this dataset. We believe our approach is effective for ASR models providing more accurate predictions than Whisper. Besides, we acknowledge that our approach may suffer performance degradation with ASR models yielding less accurate predictions."}, {"title": "4.2. Are ASR models good at disfluency recognition?", "content": "For the initial transcription of the audio, we employ the Whisper model on the dataset and achieve 22.54 WER points. To assess the percentage of fluent and not fluent speech transcribed, the manual and the automatic transcript are aligned using the operations obtained during the calculation of the"}, {"title": "4.3. What parameter to use for modified CTC algorithm?", "content": "This work proposes a modified CTC alignment algorithm to overcome the problems of the standard CTC alignment algorithm on disfluency recognition (Section 2.2.2). The modified algorithm involves a fixed probability c as to incentive gap recognition in alignment. The probability c experimented with probability values -5, -4, -3, -2, -1, -0.5, -0.1, -0.01 on a par with the log probability scale in frame-wise acoustic probability. Setting as -0.01 is essentially 0 and the value above 0 makes no sense as for comparison with log probability. A higher value of probability e indicates a higher chance to stay with the space token, corresponding to more and longer alignment gaps.\nThis work counts the number of words that are covered by the alignment gaps to evaluate alignment performance. We evaluate the alignment approaches on all words in the manual transcription to show the general alignment performance, and we also evaluate them on only the words next to the untranscribed words to show the performance specifically on disfluent speech. Note that the untranscribed words are determined with Levenshtein Alignment, same as Section 2.3. As Figure 3 shows, significantly more untranscribed words are reachable with a default probability of -0.01 than -5.\nBesides, we evaluate using the proposed alignment scores. As shown in Table 2, the combined alignment scores for all words show no significant difference, but the score for words around the untranscribed words improves clearly with decreasing the probability value of c, which is consistent to Figure 3. Therefore, the -0.001 is chosen for the modified CTC algorithm in later experiments."}, {"title": "4.4. Which forced alignment algorithm works better?", "content": "The alignments calculated by the standard CTC alignment, the modified CTC algorithm and cross-attention are compared"}, {"title": "4.5. How to build disfluency classification model?", "content": "The forced alignment brings gaps between the words of transcription, and the next question comes as to how to classify the gaps as containing speech or empty. This work proposes to build a classification model and train it with a dataset tailored for the pipeline with the gaps.\nWith the combined dataset, we build the datasets consisting of extracted gaps. The gap is labelled as \"gap contains speech\" if at least one word from the manual transcript falls into this gap. Otherwise, they were labelled as \"gap is empty\". We use the modified CTC algorithm for alignment. After shuffling, we select 80% of the gaps for training data, and the rest for test data (Table 4).\nWith the above dataset, this work builds a classification model by fine-tuning a wav2wec2 model in conjunction with a classification head. The classification head consists of a linear layer projecting the output of Wav2Vec2 onto the predefined classes: an empty gap and a gap with speech. With the evaluation of the test split, the classification model achieved an accuracy of 81.62%, a precision of 86.14%, a recall of 74.80%, and an F1-score of 80.07%."}, {"title": "4.6. How effective is the disfluency detection pipeline?", "content": "Knowing the proportion of all gaps successfully classified does not provide information about the proportion of untranscribed words successfully classified. Therefore, we count the number of transcribed and untranscribed words that are classified and covered in the gaps, classified but not covered in gaps and not classified by the gap classification model.\nAs Table 5 shows, 15,478 out of a total of 20,880 untranscribed words are covered by the detected alignment gaps, leading to a detection rate of 74.13%. However, 13,202 out of 168,256 already transcribed words are also labelled as predicted in the gaps, counting to a false detection rate of 8.6%. The false detection rate indicates the risk of double transcription if a follow-up re-transcription was carried on."}, {"title": "5. CONCLUSION", "content": "In this work, we propose an inference-only pipeline to augment any ASR model with open-set disfluency detection. We reveal the current ASR models struggle to transcribe speech disfluency. To tackle this issue, we propose a modified CTC forced alignment algorithm to recognize the location and duration of speech disfluencies. We show the effectiveness of this approach by comparing it with popular forced alignment approaches in disfluency recognition. Additionally, we build a pipeline for disfluency detection and show that the approach captures 74.13% of the words that are not transcribed by the initial transcription.\nHowever, the disfluency detection performance is dependent on the ASR model performance. This is because that transcribed disfluencies will not be identified as disfluencies, as they are already aligned with the transcribed words."}]}