{"title": "ON THE BENEFITS OF PIXEL-BASED HIERARCHICAL POLICIES FOR TASK GENERALIZATION", "authors": ["Tudor Cristea-Platon", "Bogdan Mazoure", "Josh Susskind", "Walter Talbott"], "abstract": "Reinforcement learning practitioners often avoid hierarchical policies, especially in image-based observation spaces. Typically, the single-task performance improvement over flat-policy counterparts does not justify the additional complexity associated with implementing a hierarchy. However, by introducing multiple decision-making levels, hierarchical policies can compose lower-level policies to more effectively generalize between tasks, highlighting the need for multi-task evaluations. We analyze the benefits of hierarchy through simulated multi-task robotic control experiments from pixels. Our results show that hierarchical policies trained with task conditioning can (1) increase performance on training tasks, (2) lead to improved reward and state-space generalizations in similar tasks, and (3) decrease the complexity of fine tuning required to solve novel tasks. Thus, we believe that hierarchical policies should be considered when building reinforcement learning architectures capable of generalizing between tasks.", "sections": [{"title": "INTRODUCTION", "content": "General artificial agents aim to solve varied complex tasks and to quickly learn new skills. Two difficulties that typically arise in these settings are long-horizon challenges and generalization to novel environments. The Hierarchical Reinforcement Learning (HRL) framework Dayan & Hinton (1992); Parr & Russell (1997); Sutton et al. (1999) can help address long horizons by discretizing larger tasks into easier sub-tasks. The hierarchy is imposed on the policy: higher levels operate at more abstract time scales and provide sub-goals to lower-level policies, which select the primitive actions. The second problem can be tackled using task conditioning Tenenbaum (1999); Fei-Fei et al. (2003); Rakelly et al. (2019); Zhao et al. (2020). Agents are trained to solve multiple related tasks while receiving characteristics of the current task as input. Developing agents that are able to overcome both aforementioned difficulties is an active area of research.\nA recent HRL architecture, called Director Hafner et al. (2022), learns hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. The high-level policy maximizes both task and exploration rewards by selecting latent goals, while the low-level policy learns to achieve the goals. Despite promising results in some domains, most notably solving mazes from an egocentric view, Director does not outperform baselines in other domains, such as quadruped run in the DeepMind Control Suite (DMC) Tassa et al. (2018). The difference between the successful and unsuccessful domains is primarily in the ability of a successful policy to leverage composition of lower-level behaviors. In locomotion tasks in DMC, the agent needs only to learn to walk, where with maze solving tasks, the agent must learn to walk as well as use that walking ability to navigate.\nIn addition to single-task composition, a related promise of HRL is transfer of lower-level policies from learned tasks to novel tasks that share a common underlying structure. Recent task conditioned RL work, for example MELD Zhao et al. (2020), has shown that flat (non-hierarchical) policies can learn to generalize. MELD directly encodes the extrinsic reward signal into the latent space model, thus allowing the agent to infer the current task and adapt the policy to it. This method of task conditioning has been successful on training and evaluation tasks with distinct task parameters, which were selected from the same underlying distribution."}, {"title": "RELATED WORK", "content": "Few previous works showcased convincing results for learning hierarchical policies directly from pixels. Indeed, Director Hafner et al. (2022) was outperformed across a multitude of single tasks by its flat counterpart, DreamerV2 Hafner et al. (2021). For the direct comparisons, see Figures J.1 and K.1 in Hafner et al. (2022).\nOther algorithms have shown good performance of hierarchical agents on single tasks when provided with global state information, in the form of XY coordinates of the goal and robot position Nachum et al. (2018) or of a top-down view of the environment Nachum et al. (2019a), when evaluated on quadruped maze tasks.\nPrevious work on multi-level hierarchical policies in a multi-task setup has typically required training on a diverse distribution of tasks, which were manually specified Frans et al. (2017); Tessler et al. (2017); Veeriah et al. (2021); Rao et al. (2022). Hierarchical Imitation Learning has shown good performance in the multi-task setup Yu et al. (2018); Fox et al. (2019); Gao et al. (2022); Chen et al. (2023), if the expert data quality is high.\nHierarchical structures represent just one approach in tackling compositionality, which is important across other domains as well, such as language modeling and reasoning. Modular computations, based on specialised and reusable sub-NNs, Lepikhin et al. (2021); Fedus et al. (2022) are another approach that have recently shown potential for improving generalization Goyal et al. (2021).\nHowever, the generalizability potential of the hierarchical algorithms mentioned above has remained unclear. To the authors' collective knowledge, the present analysis is the first of its kind dedicated to evaluating the generalizability of task conditioned Hierarchical Reinforcement Learning."}, {"title": "MOTIVATION", "content": "Hierarchical reinforcement learning adds an additional layer of complexity to classical (flat) RL approaches, by grouping sequences of atomic actions into a more structured action space. So what is there to gain from introducing a hierarchy in the action space? We posit that hierarchical RL has three potential advantages, due to the top-down nature of its learning signal: (a) HRL reduces the effective task horizon, (b) HRL learns generalizable and composable skills and (c) HRL allows for more sample-efficient few-shot adaptation.\nConsider the task of bipedal locomotion outlined in Figure 2, where a bipedal agent has to navigate towards a goal region. We will use it to showcase the key insights about HRL presented in this work. To evaluate the task conditioned hierarchical agent architecture, we consider a series of multi-task settings."}, {"title": "Shortening the effective task horizon", "content": "The bipedal locomotion example outlined in Figure 2 illustrates how decomposing a long horizon problem of length $H$ into $k$ smaller sub-problem of length $H/k$ can accelerate learning Nachum et al. (2019b). Intuitively, the locomotion task require the hierarchical agent to learn strategies for the higher level policy, such as setting goals of different body poses and speeds, while the lower level policy needs to learn high-frequency controls over the joints. Instead of training a single flat policy to produce $H$ actions, it is instead possible to learn a single high-level policy over $H/k$ composite actions (i.e. skills), together with multiple short-horizon low-level policies. Under the assumption that low-level policies are composable and transferable, it is possible to further amortize the training cost by learning the low-level controls on diverse data coming from a multitude of similar tasks, which we present in Section 4.1. To support the claim that the hierarchical structure contributes to performance improvements, rather than an increase in parameters or other confounding cause, we empirically investigate the effects of varying the goal horizon of the higher level policy in Section 4.2. In the limit of short horizons, the hierarchical agent reduces to a flat one, as the higher-level policies outputs new goals for every step of the lower-level one. Similarly, in the limit of long horizons, the hierarchical agent once again reduces to a flat one, as the higher-level policies outputs a single goal for the duration of the entire task, effectively leaving the solution entirely up to lower level policy."}, {"title": "Zero-shot generalization through compositionality", "content": "Figure 2 highlights how the seemingly simple task of bipedal locomotion actually involves a sequence of complex movements. The periodicity of these movements suggests that they could be temporally interchangeable, e.g. the left leg motion could be executed before the motion of the right leg, and vice-versa. Intuitively, composable motions can be recombined in multiple ways, and applied to solve groups of similar tasks - leading to better performance on unseen tasks, i.e. generalization. If the learned low-level policies are indeed composable, they can be used to solve tasks with different reward structures and topologies of the state space. Since the bipedal locomotion tasks provide a limited testbed for our hypotheses, we extend them with an additional challenging domain:"}, {"title": "EXPERIMENTS", "content": "In this study of HRL and flat RL, two categories of tasks are considered: (1) bipedal and quadruped locomotion and (2) quadruped maze navigation.\nWe select two published agents, which are trained in imagination using actor-critic, share the world model design, have many overlapping NN architecture choices, and their key difference represents that one is a hierarchical policy, Director Hafner et al. (2022), while the other is flat, DreamerV3 Hafner et al. (2023). The default hyper-parameters of these two algorithms are used, apart from increasing the weight associated with the reward loss of the world model. We thus restrict the scope of the paper to comparing HRL and flat RL agents with world models from pixels, but we see no reason why similar analysis could not be performed on other types of HRL and flat RL."}, {"title": "IN-DOMAIN PERFORMANCE", "content": "The first series of experiments focuses on the locomotion of bipedal and quadrupedal walkers. To provide the agent with task information, we condition on reward. Specifically, we pass the reward as input to the world model, as a way for the agent to infer the task Zhao et al. (2020). For the walker and quadruped walking tasks, the reward is given by $r(s_t,a_t) = 1 - |v(s_t) - v_{\text{target}} |$, which is a measure of how closely the agent manages to match the target speed. $s_t$ is the location of the walker at time step $t$, $v$ is the walker's velocity, $v_{\text{target}}$ is the target velocity, and $a_t$ is the action of the agent at time step $t$.\nWe evaluate the hierarchical and flat policies, by requiring the policies to train the bipedal and quadruped walkers from DMC suite to walk at various speeds, uniformly sampled between 1.0 and 3.0, and between 0.5 and 1.5 respectively. In DMC suite, the typical bipedal walker tasks are walking (speed 1.0), and running (speed 8.0), while for the quadruped walker they are walking (speed 0.5), and running (speed 5.0). Figure 3 shows the training curves for these task. Notice that both the hierarchical and the flat policy are able to solve the bipedal tasks in a comparable number of steps, however the hierarchy is twice as fast as the flat policy for the quadruped tasks.\nWhen trained on multiple related tasks, the hierarchical agent shows a boost in performance over its flat counterpart. This can likely be attributed to multi-task setting having transferable worker policies across tasks, which can more naturally be leveraged by a hierarchical structure."}, {"title": "Navigation", "content": "Next, we consider quadruped navigation experiments, specifically the task of reaching a target inside a box. The inputs represent proprioception and egocentric view. We train both the hierarchical and flat policies in a small box, 5\u00d75, with different colored walls, Figure 4a. The objective is to reach the green ball and whenever this is accomplished, the target is re-spawned somewhere in the area, and the task repeats until the end of the episode. At the start of each training episode, the quadruped begins in the center of the arena with random orientation, however the target initial position is randomly spawned. Note that the task conditioning in this set-up does not depend on the reward and instead is inferred from the pixel inputs, i.e. the image of the target. Nevertheless, in order to speed up the training, the agent is provided with a reward of the form: $r(s_t,a_t) = - |s_t - s_{\text{target}} |$, which is a measure of how close the quadruped is to the current target location. $s_{\text{target}}$ is the location of the target."}, {"title": "ZERO-SHOT GENERALIZATION PERFORMANCE", "content": "Next, we evaluate these trained policies on a broader spectrum of walking speeds, ranging from 0.0 to 8.0 for the bipedal walker, Figure 5, and from 0.0 to 5.0 for the quadruped walker, or in"}, {"title": "Locomotion", "content": "These results show the increased robustness and reward generalizability of the hierarchical policy compared to the flat benchmark, when evaluated on similar tasks that share a common underlying structure. We believe this is due to the higher level policy being able to leverage goals, which the worker is already able to achieve, in order to provide a better policy for the novel task."}, {"title": "Navigation", "content": "During training (Section 4.1), we provide the reward as input to the model to speed up the training and not for task-conditioning purposes. During evaluation, to ensure that the reward is not task-conditioning and that the quadruped does not just follow this guiding reward to the target, we change the reward function. Unless the quadruped touches the target, i.e. $||s_t-s_{\text{target}}||_2<8$, the system receives a large and constant negative reward (which in our case is -8.0), regardless of the distance between the quadruped and the target. Hence, the reward cannot guide the quadruped or condition the task. If the quadruped touches the target, the agent is rewarded with the negative radius of the target (which in our case is 0.5). Nevertheless, changing the reward formulation could negatively impact the performance of an agent, if that agent has learned to blindly follow the guiding reward instead of looking for the target and then moving towards its.\nWe start by reporting the evaluation results for the 5\u00d75 arena that was also used in training. For an episode of set length of 3000 steps, the hierarchical policy manages to reach the re-spawning target an average of 29.5 times per episode, while the flat one obtains an average of 12. The results are summarised in Table 1 column 2. This difference might be surprising, since Figure 4b shows comparable training curves. One can attribute this to the constant reward signal used at evaluation, which can no longer serve as a guide"}, {"title": "Ablation of goal selection frequency", "content": "A key hyper-parameter of the hierarchical structure is the goal horizon length of the higher-level policy. That is the duration for which a goal is kept constant and the lower-level policy is attempting to reach it. This horizon defines the scale of the temporal abstraction. Throughout our analyses, we have used the same default horizon length as in the Director paper, specifically 8. As the flat policy benchmark has an action repeat of two, while the hierarchical one has an action repeat of one, the former resembles most closely the hierarchy with a goal horizon of length two."}, {"title": "FEW-SHOT GENERALIZATION PERFORMANCE", "content": "We investigate the adaptability of a pre-trained hierarchy via fine-tuning subsets of the policy's components. We choose the quadruped maze as a fine-tuning task, since the learned lower level policies are sufficient to move the agent reliably. Specifically, the lower level policies have already learned how to walk and achieve goals set by higher level ones."}, {"title": "DISCUSSION", "content": "One limitation of the original Director Hafner et al. (2022) paper was the evaluation of the algorithm. Across most of the DMC suite tasks, it was outperformed by a flat policy approach Hafner et al. (2021). The benefits of the hierarchy were shown when training and evaluating on single maze tasks, in particular when the geometry was complex (for example multiple corners and increased distance to the target). These suggested that hierarchical agents benefit from tasks that display compositionality, yet this angle was not discussed and further analysis was needed to better understand it.\nWe are able to extend the task conditioning paradigm to the HRL architecture and benchmark it against a flat policy. In the process, we uncovered that, while flat policies have been known to learn and adapt to new in-distribution task parameters Zhao et al. (2020), they tend to rapidly falter as these parameters extend out-of distribution. Our findings show that hierarchical policies can not only increase performance on training tasks, but also lead to improved reward and state-space generalizations in similar tasks, allowing agents to solve them with a higher degree of success. Furthermore, hierarchical policies can decrease the complexity of fine-tuning required to solve novel tasks, by only focusing on training the higher level policies.\nFuture work is needed to continue the exploration of the generalization potential of hierarchical structures. One such direction would be improving the goal conditioning of the worker, possibly using a LLM, so that it learns abstract commands, e.g. go left, go forward, as opposed to learning to match a specific latent state. This would allow agents to further abstract details of the environment that are not shared across similar tasks (for example changing the color of the floor for the walker or quadruped between training and evaluation tasks).\nHence, we believe it worthwhile to further pursue research into hierarchical reinforcement architectures and invite practitioners to consider hierarchy when requiring agents capable of generalization."}]}