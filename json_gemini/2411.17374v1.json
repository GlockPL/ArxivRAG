{"title": "Fairness And Performance In Harmony: Data Debiasing Is All You Need", "authors": ["Junhua Liu", "Wendy Wan Yee Hui", "Roy Ka-Wei Lee", "Kwan Hui Lim"], "abstract": "Fairness in both machine learning (ML) predictions and human decisions is critical, with ML models prone to algorithmic and data bias, and human decisions affected by subjectivity and cognitive bias. This study investigates fairness using a real-world university admission dataset with 870 profiles, leveraging three ML models: XGB, Bi-LSTM, and KNN. Textual features are encoded with BERT embeddings. For individual fairness, we assess decision consistency among experts with varied backgrounds and ML models, using a consistency score. Results show ML models outperform humans in fairness by 14.08% to 18.79%. For group fairness, we propose a gender-debiasing pipeline and demonstrate its efficacy in removing gender-specific language without compromising prediction performance. Post-debiasing, all models maintain or improve their classification accuracy, validating the hypothesis that fairness and performance can coexist. Our findings highlight ML's potential to enhance fairness in admissions while maintaining high accuracy, advocating a hybrid approach combining human judgement and ML models.", "sections": [{"title": "I. INTRODUCTION", "content": "Fairness has emerged as a critical concern in terms of both the predictive outputs of Machine Learning (ML) models and the decisions made by human experts [1]-[4]. In the context of ML models, challenges in fairness often stem from data biases and algorithmic limitations, which can propagate or exacerbate existing inequities. Conversely, fairness in human decision-making is inherently influenced by subjective judgment and cognitive biases, making it vulnerable to inconsistency and error. These two sources of unfairness necessitate rigorous investigation into both computational and human-centric frameworks to mitigate bias and ensure equitable outcomes.\nTo investigate fairness, this work utilizes a real-world university admission dataset with 870 unique profiles and three effective ML models, namely, Extreme Gradient Boosting (XGB), Bi-directional Long Short-Term Memory (Bi-LSTM) and K-Nearest Neighbours (KNN). The textual features of the profiles are encoded into high-dimensional embeddings using the Bidirectional Encoder Representations from Transformers (BERT), enabling the extraction of rich contextual representations for downstream predictive tasks. In our case, we focus on the important but challenging task of admission offer decision making.\nWe begin by examining individual fairness in offer decisions for a heterogeneous group of applicants, characterized by diverse backgrounds, leadership qualities, and academic achievements. Using a standardized set of features for comparison, we assess individual fairness by measuring decision consistency through a consistency score [5], which quantifies how similarly applicants with comparable profiles are treated by human experts or ML models. Our results reveal that both XGBoost and Bi-LSTM outperform human experts significantly, achieving higher consistency scores by a margin of 14.08% to 18.79%. These findings suggest that ML models demonstrate superior reliability in ensuring fair treatment of similar applications.\nFurthermore, we assess group fairness by analyzing the effects of gender debiasing on classification performance and consistency across different models and decision-making stages. To address potential biases, we propose a debiasing pipeline designed to eliminate gender-specific language from the input features. The effectiveness of this approach is demonstrated through a statistical analysis, confirming its capability to mitigate gender bias while preserving the integrity of classification outcomes.\nSubsequently, we examine the effects of data debiasing on prediction performance by employing three empirically effective machine learning models to perform classification tasks on the dataset, both before and after debiasing. Experimental results across all three models validate our hypothesis, demonstrating that predictions either maintain or improve in accuracy following the debiasing process."}, {"title": "II. RELATED WORK", "content": "The notion of individual fairness, introduced [6], emphasizes treating similar individuals similarly. This definition of individual fairness is closely related to differential privacy [7]. Specifically, for every pair of individuals, the Lipschitz condition holds. Individual fairness, however, does not imply group fairness, which focuses on ensuring that different groups (often defined by protected attributes such as race, gender, or age) are treated fairly. What is fair or not depends on the worldview held [8]. Fairness measures consistent with the worldview can often be derived from confusion matrices.\nAn interesting development in the field of computer science is the increasing use of large language models (LLMs) to automate decision tasks. As discussed in [9], human-like biases may be picked up by LLMs. It is important to investigate whether these biases exist in real life applications and whether or not these biases may be mitigated.\nPrior works also investigated methods to mitigate bias to enhance fairness in various aspects. For instance, [10] proposed strategies through robust data preprocessing to achieve fair and accurate predictive modelling in the education domain. [5] proposed debiasing algorithms for fair representation learning. [11] provided a comprehensive study on auditing techniques for human expertise.\nThere has been numerous works studying various aspects of fairness in admissions but many of these are based on smaller-scale qualitative studies [12], general admissions policies [13] or specific aspects like standardized tests [14]. While these studies provide interesting insights, their methodologies often require significant effort to replicate. Furthermore, the small-scale user groups or the limited geographic focus of these works constrain the generalizability of their findings to broader contexts. Other studies, such as [15], focus on comparing human perceptions of fairness in AI-driven decision-making with those of human decision-making.\nPuranik et al. [16] proposed the Fair-Greedy selection policy where participants selection from under-represented groups reinforces their participation in future rounds. Fair-Greedy balances between two objectives of greedy (maximize scores of selected applicants) and fairness (minimize deviations of the proportion of under-represented group).\nBhattacharya et al. [17] conducted an empirical investigation into biases in university admissions arising from applicants' socioeconomic factors. Their study utilized a threshold-crossing model, integrated with admission data, to analyze disparities in admission thresholds across different demographic groups and identify the directionality of these differences.\nWhile existing literature offers valuable insights into algorithmic fairness and fairness in education, limited attention has been given to addressing biases present in text-derived data, which are inherently prone to subjectivity. To the best of our knowledge, this study is the first to demonstrate an effective approach for mitigating such biases to enhance individual fairness. Furthermore, we experimentally prove that debiasing, counterintuitively, not only preserves but often improves model prediction performance."}, {"title": "III. DATA PROCESSING", "content": "We use the university admission dataset introduced in [18], which consists of applicant profiles from the 2024 admission cycle. We concatenate all content and form a Combined document.\nThe debiasing pipeline focuses on removing gender-specific language from the PIQ and Leadership features while preserving other meaningful content. We maintained a comprehensive list of gender-specific terms, including pronouns (e.g., \"he\", \"she\"), titles (e.g., \"Mr.\", \"Mrs.\"), and gender-specific roles (e.g., \"son\", \"daughter\"). These terms were systematically removed using a combination of spaCy-based tokenization and regular expression pattern matching.\nTo assess the effectiveness of debiasing, we perform a binary classification using a pretrained BERT model (bert-base-uncased) and compare the distributions before and after debiasing. The profiles are classified as male or female with a threshold of 0.8, otherwise as unknown.\nThese results strongly suggest that the debiasing procedure successfully reduces the system's reliance on gender-based features while maintaining balanced treatment across gender categories.\nTo prepare the data for fairness assessment, we generate contextual embeddings using BERT (bert-base-uncased). For each feature, we generated BERT embeddings using the following process: the text was tokenized and padded to a maximum length of 512 tokens, then processed through BERT to obtain contextual embeddings. We extracted the [CLS] token embedding (768-dimensional vector) as the representation for each feature. The individual embeddings were then concatenated to create a combined representation (3840-dimensional vector) for each profile, preserving the contextual information. For both original and debiased data, the final output included individual embeddings for each feature and embeddings for the concatenated feature.\nWe use three human decision points, such as Shortlisting (SL), Admission Recommendation (AR) and Offer (OF), as targets to assess prediction accuracy and consistency. Specifically, we mapped the categorical labels to binary values, where \"Shortlisted\", \"Recommended\" and \"Offered\" corresponds to 1, and 0 otherwise, respectively."}, {"title": "IV. INDIVIDUAL FAIRNESS", "content": "Individual fairness is grounded in the principle that similar individuals should receive similar treatment in decision-making processes. Following [5], we operationalize this concept through a consistency metric that evaluates how similarly a classifier treats applicants with comparable profiles.\nFormally, given a set of N applicants, we define the consistency score C of a classifier as:\n$C=1-\\frac{1}{N} \\sum_{i=1} \\frac{1}{k} \\sum_{j\\in knn(i)} |\\hat{y}_i - \\hat{y}_j|$"}, {"title": "V. GROUP FAIRNESS", "content": "Group fairness in machine learning systems addresses systematic biases that can disadvantage certain demographic groups in model predictions. We formulate group fairness using accuracy parity, which ensures equal accuracy among groups. In other words, assuming a binary protected attribute a, accuracy parity requires:\n$\\\\Delta_{Acc} = |P(\\hat{Y} = Y|a = 0) \u2013 P(\\hat{Y} = Y|a = 1)| = 0$"}, {"title": "VII. CONCLUSION", "content": "Fairness in decision-making is critical, whether conducted by machine learning models or human experts. Our work first investigates individual fairness, using a consistency score to evaluate decision-making among a diverse group of subjects. Experimental results reveals that ML models such as XGBoost and BiLSTM achieve significantly higher consistency scores than human experts. This finding suggests that ML models can provide more reliable and fair treatment of similar applications, addressing the subjectivity and cognitive biases inherent in human decision-making."}]}