{"title": "Energy & Force Regression on DFT Trajectories is Not Enough for Universal Machine Learning Interatomic Potentials", "authors": ["Santiago Miret", "Kin Long Kelvin Lee", "Carmelo Gonzales", "Sajid Mannan", "N. M. Anoop Krishnan"], "abstract": "Universal Machine Learning Interactomic Potentials (MLIPs) enable accelerated simulations for materials discovery. However, current research efforts fail to impactfully utilize MLIPs due to: 1. Overreliance on Density Functional Theory (DFT) for MLIP training data creation; 2. MLIPs' inability to reliably and accurately perform large-scale molecular dynamics (MD) simulations for diverse materials; 3. Limited understanding of MLIPs' underlying capabilities. To address these shortcomings, we argue that MLIP research efforts should prioritize: 1. Employing more accurate simulation methods for large-scale MLIP training data creation (e.g. Coupled Cluster Theory) that cover a wide range of materials design spaces; 2. Creating MLIP metrology tools that leverage large-scale benchmarking, visualization, and interpretability analyses to provide a deeper understanding of MLIPs' inner workings; 3. Developing computationally efficient MLIPs to execute MD simulations that accurately model a broad set of materials properties. Together, these inter-disciplinary research directions can help further the real-world application of MLIPs to accurately model complex materials at device scale.", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) opens up the possibility to greatly accelerate materials discovery for a vast range of applications, including the development of energy technologies to mitigate climate change, ubiquitous computing technologies, and sustainable agriculture and manufacturing. The complexity of materials systems in modern applications continues to make empirical testing of devices, and their underlying materials, more and more challenging. As such, insilico materials simulation methods have become increasingly important to understand the properties and behavior of an growing set of diverse materials systems, ranging from solid-state crystals, molecules, and protein structures to name a few (Zeni et al., 2025; Miret et al., 2024; Zitnick et al., 2020; Lee et al., 2023b; Terwilliger et al., 2024). To fully realize the potential of insilico evaluation, it is necessary to consider how complex materials, which are comprised of large numbers of atoms governed by quantum mechanical laws, behave in diverse application conditions. Concretely, material systems are often part of multi-material devices that contain defects and imperfections while operating under various temperatures, pressures and other exogenous conditions. Therefore, similarly to how biological applications motivate the building of a virtual cell (Bunne et al., 2024), materials applications require the building of virtual devices (e.g. transistors, battery cells, nano-sized air filters) to accurately capture atomic interactions with quantum mechanical accuracy in complex environments.\nTo date, the computational evaluation of materials systems with quantum accurate methods has mostly focused on applying approximations, such as Density Functional Theory (DFT), to idealized bulk structure materials (Jain et al., 2011; Yang et al., 2019; Saal et al., 2013; Horton et al., 2019; Doerr et al., 2016; Miret et al., 2023). While these efforts have significantly improved the understanding of diverse materials, their applicability has been hindered by prohibitively high computational costs that scale exponentially with the number of atoms in the system. Modern ML methods for atomistic modeling, mainly based on geometric deep learning (Duval et al., 2023), have emerged as a promising alternative to traditional computational chemistry pipelines. These ML methods boast the ability to model large material systems with quantum accuracy at constant compute cost following model training. This has led to the development of machine learning interatomic potentials (MLIPs), which are trained on large amounts of quantum simulation data. MLIPs aim to approximate the potential energy of diverse materials in a fast and accurate manner. In recent years, an increasing number of datasets (Chanussot et al., 2021; Tran et al., 2023; Barroso-Luque et al., 2024; Lee et al., 2023a; Schmidt et al., 2024; Wang et al., 2024a; Fuemmeler et al., 2024) and models (Batatia et al., 2023; Neumann et al.,"}, {"title": "2. Background & Current Work", "content": "Most atomistic ML methods are based on geometric deep learning architectures (Duval et al., 2023), which provide a useful framework for encoding inductive biases, such as invariance, equivariance, and other symmetries. Here, we provide an overview of relevant quantum mechanical simulation methods for training data generation, which are graphically depicted in Figure 2 with Jacob's ladder of modeling accuracy. Appendix A details how MLIP inference is coupled with MD simulation for materials modeling."}, {"title": "Density Functional Theory (DFT):", "content": "DFT is a widely used quantum mechanical framework to model the behavior of electrons in atomistic systems. Conventionally, DFT aims to solve the non-relativistic electronic Schr\u00f6dinger equation by modeling the electron density from which all other electronic properties can be derived (e.g. potential energy, multipole moments). A hypothetical functional F, of the electron density, is estimated by iteratively minimizing its corresponding electronic energy, thereby producing a variational solution. Jones (2015) provides a comprehensive review of DFT fundamentals, which primarily rely on the assumption that an exact functional can perfectly describe the electron density for all and any point cloud of atoms in 3D space. This exact functional is unknown-some regard it as unknowable (Schuch & Verstraete, 2009)-and in practice is approximated using an exchange-correlation term (Kohn & Sham, 1965). Two of the most commonly used functionals include the Perdew-Burke-Ernzerhof functional (PBE) (Perdew et al., 1996) and Becke, 3-parameter, Lee-Yang-Parr functional (B3LYP) (Lee et al., 1988; Becke, 1988). These functionals have been applied in many popular ML materials datasets, such as Materials Project (Jain et al., 2013), Alexandria (Schmidt et al., 2024), and OpenCatalyst (Chanussot et al., 2021). Different approximations have subsequently led to families of density functionals for diverse cases. As such, the choice of approximations and parameterizations ultimately affects the performance of the functional on specific elements of the periodic table and/or classes of molecules and materials (Apra et al., 2020; K\u00fchne et al., 2020; Giannozzi et al., 2009). While DFT, which scales $O(N^3 \u2013 N^5)$ with N electrons, has been popular for training current MLIPs, it has known inaccuracies, inconsistent results, and other shortcomings as described in Section 3."}, {"title": "Coupled Cluster (CC) Theory:", "content": "CC methods (\u010c\u00ed\u017eek, 1966) employ highly accurate \u201cwavefunction\u201d or ab initio quantum mechanical formulations that yield systematically improvable results. CCSD(T) (Purvis & Bartlett, 1982), which applies single, double, and perturbative triple excitations is regarded as the \"gold standard\" of quantum chemical methods. The hallmarks of CC-based methods is the ability to truncate a Taylor series of excitation operators to systematically choose between computational cost and accuracy-this series converges to the exact solution to the non-relativistic Schr\u00f6dinger equation (Figure 2) for fixed nuclei, as opposed to DFT methods that involve semi-empirical tuning. Notably, MLIP evaluation today is often done with DFT references while new DFT functionals are benchmarked against CCSD(T), which in turn is usually compared against experimental observables. The computational cost of CCSD(T), which scales $O(N^7)$ with N electrons, has limited its application to small molecules until very recently (Tang et al., 2024), where hybrid methods with MLIPs have enabled large scale, finite-temperature simulations of periodic systems with CC quality (Herzog et al., 2024)."}, {"title": "3. MLIPs for Materials Science Simulations", "content": "Current gaps in MLIP research include the reliance on DFT for training data generation (Section 3.1) and the underrepresentation of diverse materials in MLIP datasets and evaluation methods (Section 3.2). Section 3.2 also describes the modeling of materials under realistic conditions, including how applying MLIPs jointly with MD simulations enables benchmarking against experimentally measured properties. Section 3.3 outlines recommendations for new research directions towards future MLIP training and development."}, {"title": "3.1. Limitations of DFT for Training Data Generation", "content": "DFT Methods Have Limited Accuracy: A critical aspect for dataset quality is the quality of the method used to obtain ground truth labels. While DFT methods are by far the most common source of data for both ML research and materials scientists alike, the implementations used rely on a hierarchy of approximations. The accuracy of DFT rests primary on the quality of the functional form (see Section 2), and its shortcomings in the description of many key chemical and physical phenomena are well-documented throughout the literature (Schuch & Verstraete, 2009). The weaknesses of DFT manifest themselves in multiple forms, such as inaccuracies in calculating band gaps (Perdew, 1985; Bystrom et al., 2024), fractional charges (Cohen et al., 2012), and general systems that demonstate static/strong electron correlation (Cohen et al., 2008; Su et al., 2018). Recent DFT benchmarking work by Araujo et al. (2022) shows that, without intricate corrections, the commonly applied PBE+D3 functional results in errors on the order of tens of kcal/mol (~0.5 eV) for adsorption energies on transition metal surfaces, making it impossible to model catalytic activity with uniform accuracy across the periodic table. Given the parametric approximations required for DFT, it is also easy to significantly overfit when modeling energy (Medvedev et al., 2017), resulting in poor generalization to other material properties. These errors can then further propagate to MLIP-based property prediction models.\nDFT Introduces Inconsistencies and Reproducibility Issues Across Different Codes: The reproducibility of DFT calculations across different electronic structure codes presents a significant challenge for MLIP data generation. Even when using identical exchange-correlation functionals, variations in implementation details, basis sets, pseudopotentials, and numerical parameters can lead to discrepancies in computed energies and forces (Schuch & Verstraete, 2009; Bootsma & Wheeler, 2019). These inconsistencies, as prominently displayed in Lejaeghere et al. (2016); Bosoni et al. (2024), become particularly problematic when training MLIPs, as the resulting models inherently encode code-specific biases\u2014not necessarily physical behavior. Another challenge is the DFT data generated by open-source code viz-a-viz closed source packages. Most public DFT datasets (and workflows to generate those), such as MPTrj, OMat24 (Barroso-Luque et al., 2024), and Alexandria, rely on Vienna Ab-initio Simulation Package (VASP) (Kresse & Hafner, 1994), a closed-source commercial package. We encourage the use of open-source codes, such as Quantum Espresso (Giannozzi et al., 2009) and CP2K (K\u00fchne et al., 2020), to make data generation more accessible and reproducible. Even though these methods still suffer from implementation-specific variance, they enable greater scientific transparency.\nData Generation with Higher Accuracy Methods: Overall, we argue that the utility of continuing to apply DFT for large-scale data generation has diminishing returns due to known limitations and inaccuracies of the method. As such, we advocate for changing data generation methods to higher accuracy methods to avoid running into previously encountered obstacles with ML for atomistic modeling. Applying DFT for data generation and MLIP training may still be useful in targeted cases that further the understanding of specialized systems (Wang et al., 2024a; Lou et al., 2012) or novel scientific vantage points for materials science. However, MLIP performance is ultimately gated by the availability of high quality data, which necessitates more accurate simulation methods and targeted real-world experiments. Otherwise, many of the machine learning discoveries may be prone to hacking (Ghugare et al., 2024; Govindarajan et al., 2024) or materials with limited experimental utility (Cheetham & Seshadri, 2024). In our opinion, perhaps the strongest advantage of MLIPs is the ability to improve physical accuracy at constant computational inference cost: if an MLIP architecture approximates DFT [$O(N^3 \u2013 N^5)$], the true benefit lies in approximating a higher quality, higher cost function with the same number of floating point operations such as CCSD(T) [$O(N^7)$], or even full configuration interaction [$O(N!)$] as exact solutions to the non-relativistic electronic Schr\u00f6dinger equation. Given the computational cost of generating these labels, more research is required to effectively bootstrap high volume, low quality data (DFT) into low volume, high quality data (CCSD(T))."}, {"title": "Hybrid ML+QM Approaches:", "content": "One interesting related research direction consists of \"hybrid\" solutions that aim to improve how quantum mechanical models are solved with machine learning: parameterized models are used as a basis or ans\u00e4tze for the solution of classical methods; i.e. bounded solutions to the exact Schr\u00f6dinger equation. A strong example of this approach is the variational Monte Carlo approach from Pfau et al. (2024) built on top of Psi-former/FermiNet (Pfau et al., 2020; von Glehn et al., 2023). These methods show significant promise for yielding accurate models for electronic and nuclear properties beyond simple energy and force regression targets. The difficulty faced by these approaches mainly lies in domain adaptation (e.g. periodic boundary conditions for solid-state structures) and scaling (i.e. larger and more diverse atomic systems). An important distinction to emphasize is that one quality target noted in Pfau et al. (2024) is based on experimental measurements. When benchmarking MLIPs, experimental"}, {"title": "3.2. Exploring Broader Ranges of Materials Under Realistic Application Conditions", "content": "Current large datasets, such as MPtrj (Deng et al., 2023), which several universal potentials have been trained on, have primarily focused on a limited set of materials. They sometimes even neglect broad classes of materials present in real-world applications, such as metallic glasses, disordered materials, metal organic frameworks, polymers, alloys, and doped semiconductors (Burner et al., 2023; Wang et al., 2024a; Vita et al., 2023; Downs & Hall-Wallace, 2003). Moreover, the current datasets like MPtrj are biased towards specific families of materials and elements. Figure 3 shows that certain elements such as H, Li, Mg, Si, P, and O, along with their possible compounds, are overrepresented, with 89 elements completely missing. This highlights a significant gap in data availability for many materials systems, many of which are relevant to real-world applications.\nIn addition to the aforementioned limitations on DFT accuracy, the limited set of systems studied imposes additional constraints on MLIP models generalizing to new designs. This is further compounded by many DFT calculations only being evaluated in ideal conditions, meaning zero temperature and pressure, which does not properly approximate most application conditions. Some initial work has shown promise in ML models generalizing across different temper-"}, {"title": "Interface and Multi-Material Interactions:", "content": "Most DFT datasets and benchmarks to date, with a notable exception of OpenCatalyst (Chanussot et al., 2021; Tran et al., 2023), have focused on modeling the properties of structures that represent a single bulk crystal or small molecules. While valuable to bootstrap the development of MLIPs, this is not sufficient to enable MLIP-based device scale simulation. Materials in modern devices interact with other materials around them to fulfill various complex performance requirements. As shown in Figure 1, a modern transistor requires the intricate combination of multiple materials (Reddy & Panda, 2022), each of which provide essential functions. The need to model multiple materials and their interfaces reliably and accurately introduces a significantly more complex task than what is available in current training datasets."}, {"title": "Aligning MLIP-Based Simulation Evaluation Towards Experimental Properties:", "content": "Given the vast set of applications for different materials, it is important to be able to model diverse sets of properties. One of the main advantages of MLIP-based simulations is the ability to model experimentally measurable properties, such as elastic moduli, thermal expansion, and thermal conductivity. Moreover, while considering the experimental properties, aligning the corresponding synthesis and testing conditions are important to make a meaningful comparison with the corresponding simulations. Thus, the measurements should be aligned to real-world conditions and properly documented. Such a database could potentially serve two purposes: i) Benchmarking MLIPs based on realistic scenarios encountered during applications; ii) Applying experimental data to train MLIPs for more accurate property prediction. Differentiable simulation frameworks provide an interesting framework to further the development of MLIPs by back-propagating directly through simulation trajectories to update MLIP and simulation parameters. Given the nascense of differentiable simulation frameworks, more research is needed to develop performant and rigorous simulation and theoretical frameworks (Gangan et al., 2024; Metz et al., 2021)."}, {"title": "3.3. MLIP Training & Representation Learning", "content": "Most of today's MLIPs are based on message passing graph neural networks (GNNs) with geometric inductive biases that infuse different types of symmetries (Duval et al., 2023). Many of these models have been trained and evaluated using regression objectives for energy and forces or other materials properties (Riebesell et al., 2023; Chanussot et al., 2021; Choudhary et al., 2020; Lee et al., 2023a). Recently, new models have emerged that have made use of multiple datasets in their training pipeline (Barroso-Luque et al., 2024; Neumann et al., 2024) to achieve better overall performance and generalization. Compared to other fields, few methods have been proposed for self-supervised MLIP pre-training (Liao et al., 2024). The success of denoising-based pretraining for effective representation learning in adjacent fields, such as proteins (Abramson et al., 2024; Zhang et al., 2023) and small molecules (Zaidi et al., 2023), as well as the increasing data diversity related to MLIPs create the need to explore effective representation learning methods.\nAs the scale of datasets increases, the computational requirements of pretraining will also increase, thereby prompting further investigation into scaling laws for MLIPs (Frey et al., 2023). As both training and inference scale requirements increase, the utility of inductive biases will continue to remain a pertinent question. Recent work indicates that GNNs provide useful inductive biases (Alampara et al., 2024) even though conflicting evidence has emerged in adjacent fields. Further, in the limit of large data regimes, invariant GNNS may perform as well as equivariant ones at lower cost (Qu & Krishnapriyan, 2024; Brehmer et al., 2024; Duval et al., 2024). Thus, a critical analysis of the inductive biases required while considering scalability and accuracy is needed to identify optimal architectures. Message passing in GNNs, for example, can become a bottleneck when deployed in simulations given the iterative nature of the inference pass needed to cover large graphs. This has prompted architectures without message passing that provide similar performance as GNNs (Bochkarev et al., 2024). Additionally, given the tight coupling between MLIPs and data generation methods, the field could also benefit from data-centric ML research related to designing datasets and continued model improvement (Oala et al., 2024), as well as knowledge distillation described in Appendix C."}, {"title": "4. MLIP Metrology: Testing & Interpretability", "content": "Since MLIPs serve as materials design tools for scientific end-users, they can benefit from being intuitive, reliable, and easy to test and analyze. As such, the development of MLIP metrology to reliably analyze properties, behavior, and limitations for MLIPs becomes important. Based on current research, we suggest a preliminary start of MLIP metrology techniques, which help build towards a greater understanding of the capabilities and limitations of MLIPs, focused on: 1. large-scale benchmarking across materials and conditions as described in detail in Appendix B; 2. MLIP visualization and analysis, such as energy landscape visualization (Bihani et al., 2024b); 3. stability based analysis methods to enhance simulation reliability (Brandstetter et al., 2022; Raja et al., 2024; Ibayashi et al., 2023); 4. interpretability studies to understand MLIP inner workings (Lee et al., 2024b).\nIn the case of interpretability, for example, recent work has shown a lack of understanding and transparency in the learned representations of equivariant models using spherical harmonics and tensor products commonly used in current MLIPs. Lee et al. (2024b) showed that latent embeddings of an equivariant model projected and visualized with manifold structure preserving methods like PHATE (Moon et al., 2019) showed no discernable structure, highlighting a strong need for both new projection methods and control over training dynamics for these methods. One potential architectural remedy relates to \"white-box\" or \"glass-box\" models, named so due to their intrinsic transparency and comprehensibility (i.e. intentionally simple structure) or through post-hoc explanation (Esders et al., 2025; Goethals et al., 2022; Esders et al., 2024; Wang et al., 2024b). While there have been traditional modelling efforts dedicated to comprehensible models, they receive significantly less attention to their black-box counterparts. Pfau et al. (2024) provides a framework for showing state-of-the-art modelling whilst remaining highly intuitive to computational chemists by infusing chemical first principles into the neural network.\nAn important consequence of the paradigm proposed by Pfau et al. (2024) is the ability for learned representations and intermediate solutions to pass property tests, in contrast to black-box MLIPs: eigenvalues and vectors from derived solutions behave as physical models do (e.g. electron spin), and in the context of comprensiblilty, the ability to obtain variational bounds on results means that quantitative behavior is well-understood. Intermediate approaches towards comprehensible models for materials modeling could borrow ideas from interpretable modeling choices, such as generalized additive models (GAM) (Hastie & Tibshirani, 1986; Wood, 2024) and derivatives like explainable boosting machines (Lou et al., 2012). An example hybrid approach that marries existing approaches with interpretable methods"}, {"title": "5. Computational & Modeling Considerations", "content": "While tools exist today to integrate MLIPs into materials simulation codes (Gupta et al., 2024), such as ASE (Hjorth Larsen et al., 2017) and LAMMPS (Thompson et al., 2022), the evaluation of MLIPs on simulation benchmarks remains limited in large part due to inefficient computation (Gonzales et al., 2024). Some architectures, such as MACE (Batatia et al., 2023), have achieved additional acceleration by implementing the model architecture using the Kokkos language (Trott et al., 2022). While re-implemtation might be feasible in isolated cases, this approach may not be scalable given the dominance of PyTorch for MLIP development and training. One promising approach for scaling the training and deployment of MLIPs is the emergence of performant, cross-hardware programming language like JAX (Bradbury et al., 2018) and Triton (Tillet et al., 2019) that encourage community driven development. These langauges have already enabled the development of MLIP-based simulation frameworks (Schoenholz & Cubuk, 2020; Hu et al., 2020; Helal & Fitzgibbon, 2024; Doerr et al., 2021) along with faster training and inference of MLIP architectures based on scalable primitives (Lee et al., 2024a).\nAs shown in Figure 4, hardware and software advancements have reached new levels of maturity, thereby enabling the deployment of higher accuracy methods described in Section 3. While these advancements show promise for better data generation methods, a significant gap remains for effective MLIP deployment. Like many other computational fields, there are physical effects and interactions that can only be modeled over large spatial and temporal scales-in the case of materials science, logarithmic in the number of atoms and the time range. To date, pragmatic modeling choices are made to enable computationally tractable but not necessarily realistic simulations to be conducted.\nThe majority of the field currently focuses on pure crystalline structures that take full advantage of small unit cells, but fall short of the device-scale regime that requires the explicit treatment of billions of atoms composed into functional components (e.g. transistors), with timescales spanning from femtoseconds (e.g. electric field response) to microseconds or more (e.g. thermal dissipation, diffusion"}, {"title": "6. Alternative Viewpoints & Approaches", "content": "AV1: DFT is Good Enough & MLIPs Are Already Universal. One reasonable alternative viewpoint is that current methods that aim to replicate DFT-level accuracy for property prediction provide enough functional accuracy to perform large-scale materials screening leading to a downselection mechanism for further analysis. From that perspective, the main goal of MLIPs is to provide an accelerated way of filtering material candidates through reasonable property prediction and reduce the number of required downstream experiments. Additionally, MLIPs and DFT validation have already shown their effectiveness in validating the predicting of diverse materials generative models (Merchant et al., 2023; Levy et al., 2024; Jiao et al., 2024; Zeni et al., 2025; Miller et al., 2024; Ding et al., 2024; Gruver et al., 2024) that could provide valuable input to materials designers. As described in Section 3.1, DFT generally provides a reasonable trade-off between accuracy and computational cost, which largely underlies its popularity for high-throughput simulation and data generation. On top of that, there are cases where DFT is known to simulate properties well, informing both materials discovery and understanding. We believe that DFT, as well as MLIPs trained on DFT, will continue to have a role in future research at the intersection of ML and materials science but argue that we can utilize MLIPS for more ambitious purposes. Furthermore, recent research works claim and provide reasonable generalization of MLIPs to a set of cases not stricly observed in the DFT-based training data (Yang et al., 2024; Merchant et al., 2023; Batatia et al., 2023). These results are encouraging that generalizeable MLIPs can be built, yet the evidence they provide is far from conclusive given conflicting results in other works (Bihani et al., 2024a; Gonzales et al., 2024), indicating the need for further research work. While such challenges may be improved with additional data generation, eventually the limitations of DFT as the source of the data will become the limiation for the underlying MLIP.\nAV2: Large-Scale Experimental Data is Required to Train ML Models for Materials Discovery. One of the primary reasons for the success of AlphaFold (Jumper et al., 2021) was the availability of high-quality sequence-structure data obtained by experimental measurements (wwPDB consortium, 2019). Given the importance of aligning to experimental measurements as the most pertinent ground truth as described in Section 3.2, the approach of developing large,"}, {"title": "7. Conclusion", "content": "For MLIPs to disruptively accelerate materials discovery, the community requires new research priorities that ultimately lead to fully atomistic device scale simulations with quantum mechanical accuracy. These types of simulations will unlock new realms of insight for how the composition of materials affects device performance. Using this guiding principle, we propose a number of interdisciplinary research challenges related to higher quality first-principles data generation, architecture and workflow design, proper inferencing, and algorithmic improvements for computational performance; all of which will require expertise from materials scientists, ML researchers, and software engineers."}, {"title": "A. MLIP Evaluation in Molecular Dynamics Simulation", "content": "Ab-Initio Molecular Dynamics (AIMD): AIMD combines quantum mechanical calculations with classical molecular dynamics to model the dynamics of atoms and molecules. AIMD generally applies a quantum mechanical method, such as DFT or Car-Parrinello Molecular Dynamics (CPMD) (Car & Parrinello, 1985), to compute the electronic energy and atomic forces (the negative gradient of the energy with respect to atom coordinates) which are used to propagate atoms according to classical mechanics. The ability of AIMD toward quantum accurate computation of energy and forces makes it a natural choice for MLIPs deployment shown in a couple of early studies that have indicated limited success (Fu et al., 2023; Bihani et al., 2024a). AIMD is also a useful method for obtaining correlated training data for MLIPs for real-world applications of materials system under physical conditions, thereby lending itself in enabling device-scale simulations."}, {"title": "MLIP Inference and Training Time Comparison:", "content": "Figure 5 based on Bihani et al. (2024a) illustrates the inference and training times for several geometric deep learning architectures developed for molecular dynamics (MD) simulations. It is important to note that while these models often have a large number of parameters, leading to higher inference times, certain architectures-such as transformers\u2014are relatively fast during inference. However, transformer-based models require more epochs for training, which increases the overall training cost."}, {"title": "B. MLIP Evaluation at Large Scale for Diverse Materials", "content": "Our analysis in Section 3.2 shows the limited space of coverage of common DFT datasets, such as Mptrj (Deng et al., 2023). The shortage of complex materials can further be augmented by comparing Mptrj to experimental databases, such as real-world experimentally measured minerals from the AMCSD database (Downs & Hall-Wallace, 2003). One observation is that real minerals consist of far more components than just binary and ternary crystals as shown in Figure 6. While the maximum number of elements in Mptrj is 9, AMCSD contains meaningful representation with much larger number of elements, with some containing up to 23 components. This highlights that the interactions between such complex compositions might not be captured by the universal potential trained only on the MPtrj dataset, suggesting that these materials may be considered out of distribution."}, {"title": "B.1. Considerations for Large-Scale MD Bechmarking", "content": "The utility of MLIPs span a broad application space, from single system inference, to large scale material screening. In the way that computer vision models are just as easily applied to one image as they are to a collection of real time video feeds, and single point weather forecasting is transferable to global scale weather modeling, we would like to be able to"}, {"title": "C. Model & Data Distillation Methods", "content": "Data distillation: Data distillation in MLIPs represents a critical, yet underaddressed, challenge, wherein strategic configuration selection from large-scale datasets (such as MPTrj, OMat24, Alexandria) can substantially reduce computational training costs. Existing active learning approaches insufficiently address the complex multi-dimensional challenge of identifying representative, information-rich configurations that capture the configurational space's essential physics. Moreover, in active learning approaches such as the full retraining approach in (Gonzales et al., 2023), the computational cost is fundamentally\nModel distillation: Model distillation is an emerging strategy for scaling MLIPs to device-level simulations through knowledge transfer from large \u201cteacher\" models to compact \u201cstudent\u201d models exploiting, for instance, energy hessians (Amin et al., 2025). This approach enables the development of parameterically efficient models that can potentially surpass their large-scale predecessors in system-specific performance. The core methodology involves systematically compressing the knowledge representation of complex MLIPs into smaller neural network architectures. These distilled models can be further refined on targeted, minimal datasets, leveraging their reduced parameter space to achieve computational efficiency comparable to classical potentials while maintaining high-fidelity predictive capabilities. However, further investigation is required on the distilling strategy, the stability and performance of the consequent MLIP, including its generalizability."}]}