{"title": "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation", "authors": ["Rithvik Prakki"], "abstract": "This paper introduces a novel approach to creating adaptive language agents by integrating active inference with large language models (LLMs). While LLMs demonstrate remarkable capabilities, their reliance on static prompts limits adaptation to new information and changing environments. We address this by implementing an active inference framework that acts as a cognitive layer above an LLM-based agent, dynamically adjusting prompts and search strategies through principled information-seeking behavior. Our framework models the environment using three state factors (prompt, search, and information states) with seven observation modalities capturing quality metrics. By framing the agent's learning through the free energy principle, we enable systematic exploration of prompt combinations and search strategies. Experimental results demonstrate the effectiveness of this approach, with the agent developing accurate models of environment dynamics evidenced by emergent structure in observation matrices. Action selection patterns reveal sophisticated exploration-exploitation behavior, transitioning from initial information-gathering to targeted prompt testing. The integration of thermodynamic principles with language model capabilities provides a principled framework for creating robust, adaptable agents, extending active inference beyond traditional low-dimensional control problems to high-dimensional, language-driven environments.", "sections": [{"title": "Introduction", "content": "Recent advancements in artificial intelligence have witnessed the emergence of large language models (LLMs) that demonstrate remarkable capabilities in natural language understanding and generation. These models have been instrumental in various applications, ranging from chatbots to complex problem-solving agents. However, a significant limitation of current LLM-based systems is their reliance on static prompts, which do not adapt dynamically to new information or changing environments. This rigidity hampers the ability of AI agents to perform self-improvement and adapt their interactions based on past experiences.\nActive inference, grounded in the Free Energy Principle (FEP), offers a promising framework for modeling adaptive and autonomous behavior in cognitive agents. The FEP implies a classical thermodynamics through its foundation in Bayesian mechanics, where belief updating incurs specific thermodynamic costs (Fields et al., 2023). Just as biological systems must balance the thermodynamic free energy required for metabolic maintenance, cognitive systems can be understood as minimizing their variational free energy - a mathematical construct that bounds the entropy of their sensory exchanges with the environment. By treating perception and action as processes aimed at minimizing this variational free energy, active inference agents can update their beliefs and make decisions that optimize their interactions with the environment while managing the inherent costs of information processing.\nIn this paper, we introduce a novel approach that integrates an active inference generative model as a cognitive layer atop a research agent powered by multiple LLMs. Our active inference agent acts as a 'brain' that dynamically adjusts the prompts provided to the LLMs, facilitating a learning process that evolves with each interaction. Through the lens of the FEP, the agent maintains state factors for prompts, search terms, and information, allowing it to systematically explore various prompt combinations and assess their efficacy while managing the trade-off between information gain and energetic costs.\nTo evaluate and refine its strategies, the agent receives observations in the form of metrics related to the responses of the research agent specifically, accuracy, relevance, and comprehensiveness. These metrics inform the agent's posterior beliefs about which prompt combinations yield optimal results, guiding future decisions through principled belief updating that respects thermodynamic constraints. When conducting searches using predetermined search terms, the agent observes additional modalities such as information relevance, information usefulness, source quality, and information state. These observations help the agent assess the quality of external information sources and incorporate them into its learning process, effectively reducing its informational entropy through active exploration.\nThe agent operates by alternating between prompt-changing and searching states, with each belief update incurring thermodynamic costs as described by the Jarzynski equality. Actions taken in these states produce observations that update the agent's beliefs, enabling it to adapt its strategies over time. This dynamic interplay allows the agent to achieve continuous self-improvement by minimizing both its instantaneous variational free energy (through accurate perception) and expected free energy (through adaptive action selection). The expected free energy serves as a principled objective function that guides the system toward preferred future states while accounting for both utility and information gain. This foundation in the FEP provides theoretical guarantees about the agent's ability to maintain stability while adapting to new information, with explicit consideration of the thermodynamic costs associated with belief updating."}, {"title": "Related Works", "content": "The field of improving agent behavior with large language models (LLMs) has seen extensive research, which can be broadly categorized into the following related themes: improving LLM performance, leveraging LLMs as adaptive agents, and the integration of active inference principles for adaptive decision-making. Below, we review each of these themes, highlighting their contributions and limitations, and distinguishing our approach from existing work."}, {"title": "Improving LLM Performance", "content": "Efforts to enhance the performance of LLMs have primarily focused on improving the models themselves through better training data curation and self-improvement loops. For instance, Bowman et al. (2022) and Sun et al. (2023) explored methods for aligning LLMs with human preferences by using heuristics and self-generated principles to filter high-quality training data. Similarly, Bai et al. (2022) investigated using self-improvement via critique-based fine-tuning, while Gou et al. (2023a) proposed leveraging external tools for more granular feedback.\nApproaches such as Re-ReST (Guo et al., 2024) curate better training data for LLMs by generating and validating data with ground truth feedback, enhancing the model's reasoning capabilities. SELF (Lu et al., 2023) and Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing (Sun et al., 2023) use reflective mechanisms to iteratively improve LLMs. While these techniques improve LLM capabilities, they primarily target the model's intrinsic quality rather than enhancing the decision-making or adaptability of agents leveraging LLMs.\nOur approach diverges by focusing on the agent's structure, policies, and environmental interactions rather than improving the LLM itself. Specifically, we use active inference to guide an agent in adapting prompts dynamically, enabling systematic exploration of complex policy spaces that are not addressed by intrinsic LLM improvements."}, {"title": "Leveraging LLMs as Adaptive Agents", "content": "Adaptive agent frameworks often use LLMs as the core reasoning component. WEBRL (Wang et al., 2023b) and CLIN (Nascimento et al., 2024) illustrate LLMs functioning as components within multi-agent systems that use reinforcement learning to adapt based on environmental feedback. In such systems, the LLM serves as the vector of adaptation, leveraging its context window for environmental understanding.\nIn open-world exploration tasks, such as Minecraft-based environments, LLMs are used to drive agent behavior based on environmental cues (Wortsman et al., 2019; Wang et al., 2023; Liu et al., 2023). Frameworks like Voyager (Wang et al., 2023) and Odyssey (Liu et al., 2023) demonstrate how large language models can guide agents in acquiring open-world skills. However, these systems rely on using the LLM itself as the vector of adaptation, which limits the agent's ability to learn structurally from its environment beyond immediate context.\nIn contrast, our model integrates active inference to go beyond context-driven adaptation. By modeling the environment through state factors and observing structured feedback metrics (e.g., accuracy, relevance, comprehensiveness), our framework actively updates beliefs about prompts and search actions. This allows the agent to dynamically adapt not just its behavior but its structural knowledge about effective strategies. Our agent is able to take advantage of a computational framework based on the human brain, in selecting which LLMs to use and for what purpose going beyond simply using LLMs directly for all aspects of exploration and exploitation."}, {"title": "Active Inference in AI", "content": "Active inference has emerged as a theoretical framework for modeling adaptive behavior, rooted in the free energy principle (Friston et al., 2017). Most applications in AI focus on low-dimensional problems, such as robotic control or navigation (Schwartenbeck et al., 2019; Parr et al., 2022), where actions aim to reduce uncertainty about the environment.\nThe use of active inference in high-dimensional, language-driven environments remains underexplored. Existing research has not addressed its application as a \"brain\" for LLM-based agents to systematically explore complex policy spaces. Our work addresses this gap by employing active inference to balance exploration and exploitation in an LLM-driven research agent. By explicitly modeling prompt combinations, search strategies, and the costs of actions, our framework introduces a practical approach to incorporating active inference for structured exploration and learning."}, {"title": "Integrated Frameworks for Multi-Objective Learning", "content": "Some frameworks aim to combine various optimization strategies into unified models. For example, SELF-Evolutionary Systems (Zhong et al., 2023) apply reinforcement learning for internet-based exploration tasks, and Generative AI for Self-Adaptive Systems (Nascimento et al., 2024) provide research roadmaps for improving agent adaptability. However, these methods often treat optimization, information retrieval, and adaptation as distinct processes.\nOur approach unifies these elements within a single active inference model. By framing decisions through Expected Free Energy (EFE) minimization, we enable the agent to dynamically select between exploration (e.g., testing new prompts) and exploitation (e.g., retrieving information). This integration provides a coherent mechanism for multi-objective learning, allowing for systematic decision-making that accounts for both environmental feedback and resource constraints."}, {"title": "Background", "content": null}, {"title": "Theoretical Foundations", "content": "Active inference rests on the principle that biological systems minimize variational free energy both through perception and action. We begin with a rigorous derivation of this framework from first principles."}, {"title": "Derivation of Variational Free Energy", "content": "Starting with the definition of surprise (negative log model evidence):\n$\\- Inp(o) = \\- In \\sum_s p(o, s)$\nWe can introduce an arbitrary distribution $q(s)$ by multiplying and dividing by it:\n$\\- Inp(o) = \\-ln \\frac{\\sum_s p(o, s)q(s)}{q(s)}$\nBy Jensen's inequality, since ln is a concave function:\n$\\- In p(o) \\le \\- \\sum_s q(s) In \\frac{p(o, s)}{q(s)} = F$\nThis upper bound $F$ is the variational free energy. We can decompose it:\n$F = \\sum_s q(s) In \\frac{p(o, s)}{q(s)}$\n$= \\sum_s q(s) Inq(s) \\- \\sum_s q(s) Inp(o, s)$\n$= \\sum_s q(s) Inq(s) \\- \\sum_s q(s) Inp(s) \\- \\sum_s q(s) lnp(os)$\n$= D_{KL}[q(s)||p(s)] \\- E_{q(s)} [lnp(o|s)]$"}, {"title": "Information Gain and Pragmatic Value Formulation of Expected Free Energy", "content": "The expected free energy in its conceptual form from Smith et al. is given by:\n$G_{\\pi} = \\-E_{q(o|\\pi)}[D_{KL}[q(s|o, \\pi)||q(s|\\pi)]] \\- E_{q(o|\\pi)} [ln p(o|\\pi)]$\ninformation gain\npragmatic value\nThis is the form used in the experiments. However, this formulation is a conceptual variety of the physical formulation, involving entropy. As shown in Champion et al., this formulation can be derived through a series of steps. The derivation relies on the following equality:\n$q(s|\\pi)q(s|o, \\pi) = q(o|\\pi)q(o|s)$"}, {"title": "Message Passing Implementation", "content": "To implement state inference through gradient descent on VFE:\n$\\frac{JF}{dq(s)} = ln q(s) \\- In p(s) \\- In p(o|s) + 1$\nFor factorized variational inference with multiple factors f, the solution becomes:\n$q(sf) = \\sigma(\\sum_m lnp(om|sf, s\\-f) + lnp(sf))$\nwhere s-f represents all other factors except f.\nFor temporal models with control states \u03c0:\n$q(s_{\\tau+1,f}|\\pi) = B_{\\pi,f}q(s_{\\tau}|\\pi)$\nwhere $B_{\\pi,f}$ is the transition matrix for factor f under policy \u03c0.\nIn matrix notation this becomes:\n$s_{\\pi,\\tau+1,f} = B_{\\pi}s_{\\pi,\\tau}$"}, {"title": "Learning Through Parameter Updates", "content": "The learning rules follow from minimizing VFE with respect to model parameters. For the A matrix with multiple modalities m:\n$\\frac{JF}{JA_m} = \\frac{\\partial}{\\partial A_m} \\-E_{q(s)} [ln p(o|s)]$\nFor Dirichlet priors over parameters:\n$p(A_m) = Dir(a_m)$\nGiven observation $o_m$ and beliefs $q(s)$ over states that modality m depends on, the update uses the outer product:\n$O_m \\otimes q(s)$\nThis yields the learning rule:\n$a^{t+1}_m = a_m + n (o_m \\otimes q(s)) \\cdot (A_m > 0)$\nwhere n is the learning rate and $\\cdot$ represents element-wise multiplication."}, {"title": "Policy Selection", "content": "The posterior over policies follows from minimizing expected free energy:\n$G_{\\pi} = E_{q(o|\\pi)} [lnp(o)] + E_{q(s|\\pi)} [lnq(s|\\pi)] + G_{param}$\nwhere $G_{param}$ captures parameter information gain.\nThe policy posterior is computed via softmax:\n$q(\\pi) = \\sigma(\\gamma G + In E)$\nwhere y is the precision parameter and E represents prior policy preferences (\"habits\").\nThe selected policy is then:\n$\\pi^* = arg \\underset{\\pi}{max} q(\\pi)$"}, {"title": "Setup", "content": null}, {"title": "Agent Architecture", "content": "The research agent's architecture is implemented through an active inference framework with three key state factors: prompt states (33 possible combinations), search states (11 possible states), and information states (3 possible states: no information, basic information, detailed information). The agent observes seven modalities: three prompt-dependent quality metrics (accuracy, relevance, comprehensiveness), three search-dependent quality metrics (information relevance, information usefulness, source quality), and one information state observation."}, {"title": "Generative Model", "content": "The generative model is defined by the following components:"}, {"title": "Observation Model (A Matrices)", "content": "The observation model consists of a set of likelihood mappings between hidden states and observations, organized into a tensor with different slices for each modality type (see Figure 1).\nThe prompt-dependent modalities (A[0,1,2]) map 33 possible prompt states to 11 quality levels (0-10), capturing how different prompt combinations influence output quality. The search-dependent modalities (A[3,4,5]) map 11 search states to quality observations, modeling how search strategies affect information gathering. The information state modality (A[6]) provides a direct mapping between hidden and observed information states."}, {"title": "Transition Model (B Matrices)", "content": "The transition model is structured as three matrices corresponding to each state factor. The prompt transitions are modeled by a 33 \u00d7 33 \u00d7 33 tensor (Bprompt) handling transitions between prompt states. Search transitions utilize an 11 \u00d7 11 \u00d7 11 tensor (Bsearch) for modeling transitions between search states. Information state progression is captured by a 3 \u00d7 3 \u00d7 1 tensor (Binfo). Transition probabilities are initially configured to maintain current states when no action is taken, with controlled transitions possible through specific actions."}, {"title": "Prior Preferences (C Matrix)", "content": "The preference distribution is structured to drive both information-seeking and quality-maximizing behavior. For quality metrics (modalities 0-5), the model implements a strong negative preference (-16.0) for low quality observations, with quadratically increasing preferences for higher quality levels scaled by 2.0. Information states (modality 6) are assigned highly structured preferences with values of -32.0, 8.0, and 64.0 for no_info, basic_info, and detailed_info states respectively."}, {"title": "Initial State Priors (D Matrix)", "content": "Initial state beliefs are configured as uniform distributions across all state factors. Prompt states are initialized with 1/33 probability for each state, search states with 1/11 probability for each state, and information states with 1/3 probability for each state."}, {"title": "Learning Parameters", "content": "The agent employs Dirichlet distributions for learning the observation and transition models:"}, {"title": "Observation Learning (PA)", "content": "Minimal concentration parameters (base concentration = 1.0) are used for all modalities to maximize learning flexibility. The prompt modalities use 11 \u00d7 33 matrices, search modalities employ 11 \u00d7 11 matrices, and the information state modality utilizes a 3 \u00d7 3 matrix."}, {"title": "Transition Learning (pB)", "content": "The model implements minimal structured priors with a base concentration of 1.0 and small biases (0.1) for specific transition types. These include state persistence under no action for prompt transitions, decay to no-search state for search transitions, and forward progression for information state transitions."}, {"title": "Action Selection", "content": "The agent employs a sophisticated policy selection mechanism with a policy horizon of 2 steps and an inference horizon of 1 step. Both state-information gain and parameter-information gain are enabled, and the model uses deterministic action selection. Valid policies are constructed to allow three types of actions: no action [0 0 0], prompt changes only [\u00bf0 0 0], and search actions only [0 0 0]."}, {"title": "Control Parameters", "content": "The model implements several key control parameters. A learning rate (n) of 50.0 is used for both observation and transition learning. Policy precision (\u03b3) is set to 8.0 to balance exploration and exploitation. Action precision (a) is configured at 16.0 to control action selection determinism."}, {"title": "State Factor Dependencies", "content": "The model employs structured dependencies between state factors and observations. The prompt factor influences accuracy, relevance, and comprehensiveness metrics. The search factor affects information relevance, usefulness, and source quality observations. The information factor determines information state observations. These dependencies are encoded in A_factor_list and B_factor list specifications to ensure proper message passing during inference."}, {"title": "Implementation", "content": "The active inference algorithm can be expressed as:\nThe algorithm iterates until convergence, performing state estimation, policy evaluation, action selection and parameter learning at each step. The key equations governing each update have been derived in previous sections."}, {"title": "Results", "content": null}, {"title": "Learning Environment Dynamics", "content": "Through active exploration and learning, the agent successfully developed an accurate model of the environment dynamics, particularly the relationships between states and observations. Figure 2 shows the final learned observation mappings after multiple interactions with the environment. Compared to the initial uniform distributions (Figure 1), these matrices show clear structure, indicating the agent has learned meaningful relationships between states and observations. The prompt quality matrices (A[0,1,2]) developed distinct patterns showing which prompt combinations lead to higher quality outputs. The search quality matrices (A[3,4,5]) reveal learned associations between search actions and information quality, while the information state matrix (A[6]) captures the reliable mapping between hidden and observed information states."}, {"title": "Strategic Action Selection", "content": "The agent's action selection strategy evolved over time, demonstrating increasingly sophisticated decision-making. Figure 3 shows the expected free energy of different policies at four time points during the agent's operation. This progression reveals how the agent learned to value different action combinations based on their information-gathering and goal-achieving potential."}, {"title": "Action Selection Patterns", "content": "The agent's actual behavior patterns provide strong evidence for intelligent exploration and exploitation. Figure 4 shows the frequency of different action combinations throughout the experimental runs. The distribution of actions reveals a strategic approach to environment exploration."}, {"title": "Information-Driven Exploration", "content": "A key finding is the agent's strategic use of search actions to minimize entropy and guide prompt exploration. The action selection patterns show that the agent learned to prioritize information-gathering search actions when uncertainty was high, using the resulting knowledge to inform subsequent prompt testing. This behavior emerged naturally from the free energy minimization framework, with the agent recognizing that search actions could reduce uncertainty about the environment's structure.\nEarly in the learning process, the agent showed a higher proportion of search actions, as evidenced by the concentration of activity in the search action dimension of Figure 4. As the agent gathered more information and reduced uncertainty about the environment, it transitioned to more focused prompt testing, demonstrating an effective balance between exploration and exploitation.\nThe progression of policy EFE values (Figure 3) shows how the agent's evaluation of different action combinations became more refined over time. Initially, the differences in EFE values were relatively small, reflecting high uncertainty about action outcomes. As learning progressed, the EFE landscape developed clear structure, indicating the agent had learned which action combinations were most effective for achieving its goals.\nThe final learned observation matrices (Figure 2) provide clear evidence that this exploration strategy was successful in discovering the underlying structure of the environment. The emergence of distinct patterns in these matrices, particularly in the prompt-quality relationships, demonstrates that the agent effectively learned which prompt combinations lead to better outcomes."}]}