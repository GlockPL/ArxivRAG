{"title": "Neural Networks Learn Distance Metrics", "authors": ["Alan Oursland"], "abstract": "Neural networks may naturally favor distance-based representations, where smaller activations indicate closer proximity to learned prototypes. This contrasts with intensity-based approaches, which rely on activation magnitudes. To test this hypothesis, we conducted experiments with six MNIST architectural variants constrained to learn either distance or intensity representations. Our results reveal that the underlying representation affects model performance. We develop a novel geometric framework that explains these findings and introduce OffsetL2, a new architecture based on Mahalanobis distance equations, to further validate this framework. This work highlights the importance of considering distance-based learning in neural network design.", "sections": [{"title": "Introduction", "content": "Neural networks have transformed machine learning through their ability to learn complex, hierarchical representations, traditionally interpreted through intensity-based representations where larger activation values signify stronger feature presence. This interpretation, originating from the McCulloch-Pitts neuron McCulloch and Pitts [1943] and Rosenblatt's perceptron Rosenblatt [1958], underlies modern deep learning LeCun et al. [2015]. However, our theoretical understanding of neural networks' internal mechanisms remains limited Lipton [2018], Montavon et al. [2018], particularly regarding how they represent and process features.\nIn our prior work Oursland [2024a], we argued that networks may naturally learn distance-based representations, where smaller activations indicate proximity to learned prototypes. This reinterpretation challenges the traditional intensity-based paradigm and provides a statistical foundation rooted in principles like the Mahalanobis distance Mahalanobis [1936], McLachlan [2019]. Empirical evidence supports this theory. In our work Oursland [2024b], we demonstrated that distance-based metrics play a crucial role in how networks learn and utilize features, suggesting the need to rethink the fundamental nature of representations.\nCore Questions. This paper investigates: (1) whether neural networks naturally prefer distance-based or intensity-based representations, (2) how architectural choices shape these representational biases, and (3) what geometric and statistical principles underlie these preferences.\nContributions. We examine neural network behavior through distance and intensity representations via: (1) A theoretical framework formalizing the distinction between these representations, (2) empirical analysis of six architectural variants, revealing mechanisms behind dead node creation and geometric performance limitations, and (3) introduction of OffsetL2, a novel architecture validating our framework through strong, stable performance.\nThe remainder of this paper reviews related work (Section 2), establishes theoretical foundations (Section 3), presents our experimental design (Section 4) and findings (Section 5), and discusses implications (Section 6)."}, {"title": "Related Work", "content": "Our work builds upon recent advances in understanding neural networks through statistical distance metrics. Our prior work Oursland [2024a] demonstrated how linear layers with Absolute Value (Abs) activations approximate the Mahalanobis distance Mahalanobis [1936]. We established a mathematical foundation for distance-based representations, showing that such layers naturally encode distance relationships rather than intensity-based activations. Empirical studies further reinforce this perspective. In Oursland [2024b], we demonstrated that networks with ReLU and Abs activations exhibit particular sensitivity to perturbations affecting distance relationships in the feature space, suggesting that distance metrics play a fundamental role in how networks process information.\nAlternative approaches to incorporating distance metrics in neural networks include Radial Basis Function (RBF) networks Broomhead and Lowe [1988], Park and Sandberg [1991], which use distances from learned centers for classification, Siamese networks Bromley et al. [1994], Hadsell et al. [2006], which learn embeddings where distances represent similarity, and Learning Vector Quantization (LVQ) Kohonen [1995], which explicitly models class prototypes and uses distance-based classification. Contrastive learning Chen et al. [2020], He et al. [2020] emphasizes the importance of learning representations where distances reflect semantic similarity, although those methods typically rely on carefully constructed training objectives rather than inherent architectural biases. While specialized architectures like RBF and LVQ demonstrate the effectiveness of explicitly encoding distances, they remain largely confined to specific applications rather than general-purpose deep learning.\nComplementing these distance-centric views, geometric interpretations of neural computation offer valuable insights for understanding internal representations Montavon et al. [2018], Olah et al. [2017]. These approaches analyze hyperplanes and decision boundaries to explain how networks partition and represent data Lipton [2018], Erhan et al. [2009], though they typically focus on networks trained under standard intensity-based assumptions.\nThis work bridges the gap between distance-based and geometric interpretations by investigating how architectural choices influence the emergence of distance-based representations."}, {"title": "Background", "content": "This section introduces the theoretical framework for understanding how neural networks represent data through either distance-based or intensity-based methods, providing context for our experimental analysis."}, {"title": "Features, Representations, and Distance Metrics", "content": "Neural networks learn features through their internal representations. For the purposes of this paper, we define features as inherent properties of the data that can be quantified statistically, and representations are the compositions of these features learned by a node to generate its output. Traditional intensity-based representations encode features through activation magnitudes, while distance-based representations encode features through proximity to learned prototypes. In our discussion, we primarily consider prototypes as the features of interest, as they provide a structured way to quantify similarity and influence learned representations.\nWe argue that neural networks fundamentally learn distance features that quantify similarity between data points and learned prototypes. What appear to be intensity-based representations can be reinterpreted as disjunctive sets of distance features, corresponding to Disjunctive Normal Form (DNF) in Boolean algebra Post [1921]. For example, an intensity representation for class a implicitly learns \u00ab(bVc) = \u00abb^\u00abc, where b and c represent distances to other classes. This logical interpretation has connections to work exploring the relationship between neural networks and Boolean formulas Anthony [2003]."}, {"title": "Distance vs. Intensity Representations", "content": "Intensity-based interpretations, while lacking precise mathematical definition, are deeply embedded in the foundations of deep learning Goodfellow et al. [2016]. These interpretations treat larger activations as indicating stronger feature presence, fundamentally shaping how we train and interpret networks. The use of one-hot encoded labels for classification directly encodes this assumption - the correct class should produce the highest activation while all others should be suppressed. This intensity-based paradigm appears throughout modern practice: cross-entropy loss encourages larger activations for correct classes, feature visualization Erhan et al. [2009], Olah et al. [2017] assumes maximum activations represent learned features, and even basic image processing interprets pixel intensities as direct measures of feature strength. However, this pervasive intensity-based interpretation remains an assumption imposed on neural networks rather than an inherent property.\nDistance-based representations offer an alternative framework, interpreting smaller activations as indicating similarity to learned prototypes. This aligns with statistical metrics like the Mahalanobis distance Mahalanobis [1936], where the activation f(x) is inversely proportional to the distance between input x and prototypes \u03bc:\n$f(x) = |W(x \u2212 \u03bc)|_p$,"}, {"title": "Theoretical Foundations: Mahalanobis Distance", "content": "The Mahalanobis distance provides a statistical foundation for distance-based representations, capturing feature correlations and scales through the covariance matrix \u03a3:\n$D_M(x) = \\sqrt{(x \u2212 \u03bc)^T\u03a3^{-1}(x \u2013 \u03bc)}$,\nThrough eigendecomposition \u2211 = VAVT, this distance can be expressed as:\n$D_M(x) = ||A^{-1/2}V^T (x \u2212 \u03bc) ||_2$,"}, {"title": "Geometric Interpretations of Representations", "content": "Neural networks project input data into a latent space structured by hyperplanes; under a distance-based interpretation, these hyperplanes act as axes of a manifold, with distances to prototypes along these axes defining the representation Montavon et al. [2018].\nFor a hyperplane defined by y = Wx + b, where x is N-dimensional and y is scalar, its decision boundary occurs where the activation function equals zero (e.g., 0 = Wx + b). This boundary is uniquely described by N linearly independent points: N - 1 points on the decision boundary plus one point at x = 0, y = b. These boundary points serve as feature prototypes, encoding relationships between inputs and their latent representations. These prototypes don't necessarily reflect human-perceived similarity, but rather capture relationships defined by the network's learned distance metric.\nWhen b = 0, hyperplanes must pass through the origin, making it a fixed prototype. This constraint has minimal impact in high dimensions since each hyperplane still intersects N - 2 independent points on its decision boundary, plus points at the origin and x! = 0, y! = 0. While these prototypes are theoretically significant, recovering them becomes computationally intractable as dimensionality increases Donoho [2000]."}, {"title": "Activation Functions and Representational Preferences", "content": "Activation functions shape how networks encode representations:\n\u2022 ReLU: f(x) = max(0,x) is traditionally associated with intensity-based interpretations but can be viewed through a distance lens Nair and Hinton [2010], Glorot et al. [2011]. The zero region corresponds to one side of a decision boundary, with positive activations encoding prototype proximity. However, ReLU's tendency to produce \"dead neurons\" when inputs remain negative can hinder learning. This issue has motivated research into alternative activation functions He et al. [2015], Ramachandran et al. [2018], Misra [2019].\n\u2022 Absolute Value (Abs): f(x) = |x| directly represents distance-based relationships by preserving magnitude information regardless of sign. This symmetry ensures neurons remain active and maintain complete information about distance from decision boundaries.\n\u2022 Neg Layers: f(x) = -x transforms positive distance representations into negative intensity representations by inverting activation order.\nThis theoretical foundation frames our experimental analysis, where we systematically investigate how architectural choices, such as activation functions and bias terms, affect representational biases. By probing these factors, we aim to elucidate the fundamental principles driving neural network learning and provide a unified framework for understanding distance-based and intensity-based representations."}, {"title": "Experimental Design", "content": "We explore whether networks exhibit a preference for distance-based or intensity-based representations. We employ simple two-layer networks and systematically architectural components to force either distance or intensity representations in the final linear layer. We aim to reveal potential training biases towards specific representation types."}, {"title": "Objectives", "content": "1. When neural networks are constrained to produce either distance-based or intensity-based outputs, how does this affect their performance?\n2. How do different activation functions influence the network's ability to learn under these different representational constraints?\n3. By analyzing the performance and behavior of networks under these constraints, what can we infer about the nature of the representations learned in the output layer and the geometry of the feature space?"}, {"title": "Model Design", "content": "We constrained six neural network designs to force specific internal representations. These architectures are intentionally kept simple to isolate the specific behaviors under study. We utilize two-layer networks. All of the hidden layers have 128 nodes. ReLU is studied as a standard activation function in deep learning. Abs is studied for its theoretical connection to the Mahalanobis distance.\nThe core representational constraint is CrossEntropy Loss, which enforces intensity-based outputs. The combination of activations and a negation operator controls how the output layer must represent features internally. The activation function forces positive values (more precisely, non-negative). The negation reverses the order and signs of the activations. Models with the negation learn a positive distance representation which is converted by the negation layer into a negative intensity representation.\nThe six primary architectures exclude a bias term in the final linear layer. This choice prevents the network from trivially learning the opposite representation (because - (Wx) = (-W)x) and then simply shifting it back to the positive side by using the bias.\nTo establish a baseline for comparison, we include two control architectures ReLU and Abs.\nThe four experimental architectures are Abs2, Abs2-Neg, ReLU2, and ReLU2-Neg."}, {"title": "Experimental Setup", "content": "We use the MNIST dataset LeCun et al. [1998] for its well-understood features and relatively low dimensionality (28x28 pixels), making it suitable for analyzing representational preferences in a controlled setting. As is standard practice, images are normalized to zero mean and unit variance across the dataset.\nTo minimize confounding factors, we choose a simple training protocol with minimal hyperparameters. We use SGD optimization with learning rate 0.001 and train for 5000 epochs with full-batch updates. Each experiment is repeated 20 times. The loss function is CrossEntropyLoss (which includes LogSoftmax) applied to the final layer's logits.\nWe evaluate each architecture's performance using three metrics: test accuracy on MNIST, stability (variance across 20 training runs), and statistical significance via paired t-tests between architectures. These metrics enable us to compare both absolute performance and the consistency of learning across different random initializations.\nSource code and additional resources can be found in our GitHub repository\u00b9.\nThe performance of each architecture, under the described experimental conditions, is analyzed in the following section."}, {"title": "Experimental Results", "content": "We conducted extensive experiments comparing baseline architectures and architectural variants. All experiments were run for 5,000 epochs with 20 independent trials to ensure statistical robustness."}, {"title": "Baseline Performance", "content": "The baseline ReLU and Abs architectures showed strong performance on MNIST, with no statistically significant difference between them (t(38) = 1.14, p = 0.26, Cohen's d = 0.37). This comparable performance suggests that the choice of activation function alone does not significantly impact model effectiveness under standard conditions. These results provide a robust foundation for evaluating our architectural modifications."}, {"title": "Intensity Learning Models", "content": "The models constrained to learn intensity representations through the addition of a second activation function, ReLU2 and Abs2, exhibited markedly different behaviors.\nReLU2's performance degraded catastrophically, showing a substantial drop from the baseline ReLU model (t(38) = -17.33, p < 0.001, Cohen's d = 5.56). This dramatic failure aligns with our hypothesis that neural networks may exhibit a bias towards learning distance-based representations.\nIn contrast, Abs2 maintained performance statistically indistinguishable from the baseline Abs model (t(38) = 1.4967, p = 0.1427, Cohen's d = 0.47). This finding complicates our initial hypothesis, suggesting that the relationship between activation functions and representational biases may be more nuanced than initially theorized."}, {"title": "Distance Learning Models", "content": "The models designed to learn distance representations through the Negation layer, ReLU2-Neg and Abs2-Neg, showed contrasting behaviors.\nReLU2-Neg exhibited a remarkable recovery from ReLU2's catastrophic failure (t(38) = -17.33, p\u00a1 0.001, Cohen's d = 5.48), achieving performance statistically comparable to the baseline ReLU (t(38) = -12.78, p \u00a1 0.001, Cohen's d = 4.04). This recovery supports our hypothesis that neural networks may be biased towards learning distance-based representations, with the Neg transformation enabling ReLU2-Neg to leverage this bias effectively.\nSurprisingly, Abs2-Neg showed significant performance degradation compared to both the baseline Abs (t(38) = -8.81, p\u00a1 0.001, Cohen's d = 2.79) and its intensity counterpart, Abs2 (t(38) = 8.97, pj 0.001, Cohen's d = 2.84). The markedly higher variability in Abs2-Neg's performance (SD = 2.56% vs. 0.17% for Abs2) further suggests that enforcing distance-based learning through negation may fundamentally interfere with the Abs activation function's learning dynamics."}, {"title": "Impact of Bias Exclusion", "content": "We excluded the bias term from the second linear layer to enforce learning through the origin, effectively reducing the dimensionality of the solution space by one. For completeness, we conducted parallel experiments with the bias term included (Table 3)."}, {"title": "Summary of Findings", "content": "The experiments revealed that seemingly minor architectural changes can significantly impact model performance, yielding both expected and surprising results. The catastrophic failure of ReLU2 under intensity constraints aligned with our predictions about distance-based representational bias. However, Abs2's resilience to these same constraints complicated this narrative. The distance-constrained models further nuanced our understanding: ReLU2-Neg's recovery to baseline performance supported our distance-bias hypothesis, while Abs2-Neg's significant underperformance revealed unexpected limitations.\nThese contrasting behaviors suggest that neural networks' representational capabilities are more nuanced than our initial hypothesis predicted. While networks can adopt both distance- and intensity-based approaches, their success appears highly dependent on the specific architectural configuration, particularly the choice of activation function. This interplay between architecture and representation forms the focus of our subsequent geometric analysis in the Discussion section."}, {"title": "Discussion", "content": "Our experiments revealed unexpected behaviors in how neural networks learn representations. While we hypothesized a preference for distance-based representations, the results paint a more complex picture: ReLU2 failed catastrophically when constrained to learn intensity representations, yet Abs2 showed surprising resilience to these same constraints. Meanwhile, Abs2-Neg underperformed despite being designed for distance-based learning. These counterintuitive findings suggest that the relationship between network architecture and representational capacity is more nuanced than initially theorized. This aligns with research highlighting the complex interplay between architecture, optimization, and generalization in deep learning Bengio et al. [2013], Jacot et al. [2018], Lee et al. [2019]."}, {"title": "Feature Distributions in Latent Spaces", "content": "To help visualize our geometric analysis, we analyze how data points are distributed relative to the hyperplanes defined by the first linear layer, using preactivation values to measure distances from decision boundaries. Since precise feature identification is challenging in deep networks, we use MNIST class labels as proxies to understand how different classes cluster in the latent space."}, {"title": "Analysis of ReLU-based Architectures", "content": "ReLU2 failed catastrophically (47.20% \u00b1 12.00%) due to widespread node death in its output layer: 33.00% of nodes were permanently inactive and 53.50% activated for less than 5% of inputs. This failure stems from attempting to learn a disjunctive distance representation while constrained by intensity-based learning. Since non-target classes comprise 90% of the data for any classification decision, the network drives most pre-activations negative to minimize non-target activations. This issue of inactive or dying ReLUs has been widely studied in the context of rectifier activations He et al. [2015].\nThe dead node collapse emerges from a compounding effect: when classes overlap in the latent space, minimizing activations for non-target classes (\u00acc) inevitably affects the target class (c) as well. Since each class comprises only 10% of the data, the optimization overwhelmingly prioritizes minimizing non-target activations (90% of cases) over preserving activations for the target class (10% of cases). As a result, pre-activations for both target and non-target classes are driven negative, which the second ReLU then zeros out, leading to widespread dead nodes.\nIn contrast, ReLU2-Neg achieves near-baseline performance (94.93% \u00b1 0.15%) by building a conjunctive distance representation. It positions hyperplanes so that class c points have negative pre-activations (centered around ze), which ReLU converts to zero. Crucially, this also applies to all classes with even smaller pre-activations (i.e., those positioned to the left of ze in the projected space, as shown in Figure 1). Since ReLU zeros out everything below the target class, the network must rely on the decorrelation of these classes across different hyperplane projections to prevent them from overlapping with the target class. The diverse projections in the latent space Bengio et al. [2013], Cogswell et al. [2016] ensure this decorrelation, allowing the target class to maintain minimal activation while maximizing \u00acc."}, {"title": "Analysis of Abs-based Architectures", "content": "Unlike ReLU, Abs networks cannot produce dead nodes. Instead of zeroing out negative values, Abs folds them to the positive side, ensuring all nodes remain active. Under our distance metric theory-where zero activation signifies maximum feature membership this architectural difference leads to distinct feature representations.\nIn ReLU networks, maximum feature membership extends to all input regions producing negative pre-activations, creating broader feature sets. In contrast, Abs networks achieve maximum membership only at exact decision boundary points, resulting in more focused feature sets. Since the minimum activation can correspond to either ze or z\u00acc, Abs networks provide a more direct encoding of distances to learned prototypes or anti-prototypes.\nWhile Abs2 performed well (95.35% \u00b1 0.17%), its distance-learning counterpart, Abs2-Neg, suffered a notable accuracy drop (90.08% \u00b1 2.56%) with significantly higher variance. What explains this unexpected performance gap?\nWe theorize that Abs2-Neg underperformance may be related to the clustered nature of MNIST Deng [2012], Perez and Wang [2017]. This dataset's distinct clusters might lead to the existence of a single, highly optimal prototype point, zc, for each class. An output hyperplane in Abs2-Neg must pass through that optimal prototype zc and 127 additional linearly independent points. If a single, dominant ze exists, the remaining points must be suboptimal, potentially lying closer to non-target class distributions. This constraint could lead to misclassifications and higher variance. The development of OffsetL2, which explicitly models a single prototype per class, was motivated by this hypothesis.\nIn contrast, Abs2 constructs its hyperplane by selecting z\u00acc, the centers of non-target classes, for each latent dimension. Since each dimension can be aligned with any of the nine non-target classes, Abs2 has a vast combinatorial space of possible hyperplane configurations (9128 choices). This flexibility allows it to compensate for suboptimal points in some dimensions by making better choices in others. As a result, Abs2 can achieve robust class separation, explaining its higher accuracy and lower variance compared to Abs2-Neg."}, {"title": "Validation Through Additional Experiments", "content": "Our theory about the Abs2-Neg performance drop suggests that a layer designed to explicitly represent the distance to a single optimal point might correct the performance difference. To address the limitations of Abs2-Neg, where the need for multiple non-optimal intersection points hampered performance, we propose a layer called OffsetL2 that computes the weighted L2 distance from a single learned reference point \u03bc:\n$Yi = ||a_i \u2299 (x \u2212 \u03bc_i)||_2$\nOffsetL2 directly implements our geometric intuition by explicitly learning a single optimal reference point, \u00b5i, for each class, corresponding to the hypothesized ideal prototype zc. The learnable weight vector ai modulates the importance of each dimension in the distance calculation, providing greater flexibility. This approach contrasts with Abs2-Neg, which implicitly discovers prototypes through hyperplane positioning. This is conceptually similar to the learned prototypes in Radial Basis Function (RBF) networks, where classification is determined by distance to a set of learned reference points Moody and Darken [1989], and to the Mahalanobis distance formulation discussed in the Background.\nWhen preceded by a linear layer, OffsetL2 becomes functionally equivalent to the PCA-based Mahalanobis distance, where the linear layer learns principal components (V) and OffsetL2 learns the scaling (\u039b-1/2) and mean (\u03bc). This strong connection between OffsetL2, RBF networks, and Mahalanobis distance further reinforces its theoretical grounding.\nTo evaluate OffsetL2, we introduced four new models: ReLU-L2, ReLU-L2-Neg, Abs-L2, and Abs-L2-Neg. Training was extended to 50,000 epochs after observing that models had not fully converged at 5,000 epochs."}, {"title": "Conclusion", "content": "Our analysis reveals fundamental insights into the geometric principles underlying neural network representations through the lens of statistical distances. We demonstrate that while networks can adopt either representation, ReLU-based architectures exhibit a natural bias towards distance-based learning. The catastrophic failure of ReLU2 under intensity constraints illustrates how architectural choices can create untenable optimization landscapes, particularly when inputs must cluster near decision boundaries. The performance gap between Abs2 and Abs2-Neg further illuminates this geometric perspective: while Abs2 leverages combinatorial flexibility to select optimal separation points in high-dimensional space, Abs2-Neg's restricted prototype selection leads to degraded performance. This distinction underscores the crucial role of architectural flexibility in learning effective distance-based representations.\nThese findings suggest that network behavior is driven by geometric interactions in the feature space rather than intrinsic properties of activation functions. The success of our OffsetL2 architecture, which directly models statistically-motivated geometric relationships through Mahalanobis distance calculations, validates this framework by achieving superior performance (97.61% accuracy) with remarkable stability (\u00b1 0.07% standard deviation). By learning single prototypes representing either optimal ze or z\u00acc for each class, OffsetL2 avoids the pitfalls of implicit prototype discovery through constrained hyperplanes, demonstrating the advantages of explicitly modeling distance-based relationships.\nThis research opens new avenues for neural network design by demonstrating the importance of explicitly modeling geometric relationships in feature space. Future work should explore how these principles extend to deeper architectures and diverse tasks, potentially leading to networks that are not only more powerful but also more interpretable and aligned with the underlying statistical structure of the data. By viewing neural computation through the lens of statistical distances rather than activation intensities, we can develop more principled approaches to architecture design that bridge the gap between theoretical understanding and practical application."}]}