{"title": "Detecting Memorization in Large Language Models", "authors": ["Eduardo Slonski"], "abstract": "Large language models (LLMs) have achieved impressive results in natural language\nprocessing but are prone to memorizing portions of their training data, which can com-\npromise evaluation metrics, raise privacy concerns, and limit generalization. Traditional\nmethods for detecting memorization rely on output probabilities or loss functions, of-\nten lacking precision due to confounding factors like common language patterns. In\nthis paper, we introduce an analytical method that precisely detects memorization by\nexamining neuron activations within the LLM. By identifying specific activation patterns\nthat differentiate between memorized and not memorized tokens, we train classification\nprobes that achieve near-perfect accuracy. The approach can also be applied to other\nmechanisms, such as repetition, as demonstrated in this study, highlighting its versatility.\nIntervening on these activations allows us to suppress memorization without degrading\noverall performance, enhancing evaluation integrity by ensuring metrics reflect genuine\ngeneralization. Additionally, our method supports large-scale labeling of tokens and\nsequences, crucial for next-generation AI models, improving training efficiency and\nresults. Our findings contribute to model interpretability and offer practical tools for\nanalyzing and controlling internal mechanisms in LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing by demonstrating\nunprecedented abilities in text generation, comprehension, and a variety of applications. These models\nexcel in tasks ranging from machine translation and summarization to creative writing and complex\nproblem-solving, largely due to their extensive training on vast and diverse datasets. However,\nalongside these impressive capabilities, LLMs exhibit a propensity to memorize segments of their\ntraining data verbatim, which can lead to overfitting, privacy concerns, and challenges in evaluation.\nMemorization within LLMs presents a double-edged sword. While it allows models to recall specific\nfacts or phrases essential for certain tasks, excessive memorization can damage their ability to\ngeneralize to new challenges and may result in the unintended disclosure of sensitive information\nfrom the training data. Moreover, verbatim reproduction complicates the evaluation of models, as it\ncan inflate performance metrics without reflecting genuine understanding or reasoning capabilities.\nPrevious approaches to detecting memorization often rely on examining output probabilities or loss\nfunctions (Carlini et al., 2020), under the assumption that memorized tokens lead to highly confident\npredictions with near-zero loss. While intuitive, these methods struggle with confounding factors\nlike common phrases and predictable patterns that generate similar outputs. They also often lack the\nprecision and interpretability needed for in-depth analysis and intervention."}, {"title": "2 Related Work", "content": "Understanding and detecting memorization in large language models have garnered significant\nattention due to its implications for privacy, generalization, and model interpretability. Previous\nstudies have primarily focused on quantifying and extracting memorized data, as well as exploring\nthe factors that contribute to memorization in neural networks.\nCarlini et al. (2020) introduced practical attacks to extract verbatim memorized training data from\nLLMs using black-box query access. They formalized the notion of k-eidetic memorization and\ndeveloped methods for generating and ranking potential memorized samples. Their work highlighted\nthat larger models are more prone to memorization and discussed mitigation strategies such as\ndifferential privacy and data de-duplication.\nFurther exploring the quantification of memorization, Carlini et al. (2023) established that memoriza-\ntion scales log-linearly with model size and data duplication. They demonstrated that longer prompts\nincrease the discoverability of memorized data, emphasizing challenges in auditing and mitigating\nmemorization in large models.\nHuang et al. (2024) conducted controlled experiments by injecting specific sequences into training\ndata to study verbatim memorization. They observed that non-trivial repetition is required for\nmemorization and that later training checkpoints memorize more effectively. Their findings suggest\nthat verbatim memorization is intertwined with general language modeling capabilities, making it\ndifficult to suppress without degrading model quality.\nBiderman et al. (2023a) investigated the predictability of memorization behavior in LLMs. They found\nthat smaller or partially trained models are unreliable predictors of memorization in larger models,\nidentifying emergent properties not predictable from smaller scales. Their scaling laws provide\ninsights into forecasting memorization behavior but also highlight the limitations of extrapolation\nmethods.\nEfforts to localize memorization within models have been explored by Maini et al. (2023) and Chang\net al. (2023). Maini et al. (2023) demonstrated that memorization is not confined to specific layers but\ndistributed across neurons scattered throughout the network. They introduced \"example-tied dropout\"\nto localize memorization to predetermined neurons, effectively mitigating memorization with minimal\nimpact on generalization. Chang et al. (2023) proposed benchmarks to evaluate localization methods,\nfinding that precise localization of memorization remains challenging due to shared neurons among\nrelated sequences."}, {"title": "3 Detecting Memorization", "content": "Previous studies have attempted to detect when a language model uses memorization to predict the\nnext token by examining the loss function (e.g., Carlini et al., 2020). This approach is intuitive\nbecause memorized tokens usually narrow the output to a single confident prediction, causing the\nloss to approach zero. However, this method faces challenges since other mechanisms can produce\nsimilar effects on the output. For instance, some studies (e.g., Meng et al., 2022) aim to identify\nwhere information is stored in large language models (LLMs) by precisely intervening in the forward\ncomputation to determine which components affect the result.\nIn contrast, we propose an analytical method that is highly precise, achieving an accuracy close to\n100%, and interpretable."}, {"title": "3.1 Methodology", "content": "Our approach involves first collecting samples that are memorized by the LLM and comparing\ntheir activations with similar, not memorized samples. We then identify neuron activations that\nbest distinguish between the two groups and use these activations to label a larger dataset, which is\nsubsequently used to train classification probes. We use the Pythia 1B model (Biderman et al., 2023b)\nfor this study."}, {"title": "Part 1: Identifying Neuron Activations", "content": "We identified several sources likely to be memorized by the\nLLM, including famous quotes, speeches, Bible passages, legal texts, manuals, poems, pledges,\nlicenses, nursery rhymes, anthems, passages from famous novels, song lyrics, common disclaimers,\nand more. We manually tested each sample on the LLM and retained those that were memorized,\nindicated by a very high confidence level on the correct predictions. It is essential to have a small yet\nsufficiently diverse corpus of samples; our corpus comprised 100 memorized samples.\nThis step is crucial and can lead to incorrect results if not\nconducted properly. We balanced the not memorized samples with the memorized ones by including\na very similar but not memorized sample for each memorized sample. For instance, for a memorized\nspeech, we included a not memorized speech of a similar style and length. It is important to avoid\nusing random samples, as this can introduce bias due to differences in distribution; memorized\nsamples are more likely to originate from the sources cited above."}, {"title": "3.  Labeling the Tokens", "content": "We manually labeled the tokens of both the memorized and not memorized\ngroups, paying attention to three key considerations:\nSometimes, not the entire sample is memorized. It is important to\ninclude only the tokens that are actually memorized.\nDue to tokenization, some words are split into multiple tokens (e.g.,\n\"gira\" and \"ffe\" for \"giraffe\"). We only use the last token of a word because the preceding\ntokens use different mechanisms to predict the next token.\nTo avoid biasing the labeled token dataset toward specific texts, we limit\ntokens from each sample to 100. For example, using all tokens from the U.S. Constitution\nwould overrepresent that single text.\nOur final corpus contains 10,000 tokens, evenly split between memorized and not memorized tokens."}, {"title": "4.  Detecting Neuron Activations", "content": "We recorded activations for all samples and analyzed them\nstatistically, comparing labeled memorized versus not memorized tokens. Features were ranked using\nCohen's d (1), which measures group separation using pooled standard deviation. Other methods\n(ROC AUC, Wilcoxon, t-test, Kolmogorov-Smirnov, Jensen-Shannon, Wasserstein, energy statistics,\nLevene's test, kurtosis) yielded similar results for this task. Distribution-focused methods revealed an\ninteresting finding discussed in Section 9.\nCohen's d = \\frac{M_1 - M_2}{SD_{pooled}} where SD_{pooled} = \\sqrt{\\frac{(n_1-1)SD_1^2 + (n_2 - 1)SD_2^2}{n_1 + n_2 - 2}} (1)\nMi, SDi, and ni are the means, standard deviations, and sample sizes of groups i = 1,2.\nOur analysis revealed that many neuron activations are related to memorization and can effectively\nseparate the two groups. We consider a Cohen's d value of 1 or greater to be indicative of an effective\nseparation.\nIn Figure 2, we show the Cohen's d distribution for output activations across layers. The distributions\nbecome taller in later layers, indicating more activations with larger Cohen's d values and greater\nseparation. Notably, there is one specific activation consistently at the top across all layers, activation\n1668, which we discuss further in Section 8."}, {"title": "5.  Feature Extraction:", "content": "The MLP serves as a strong feature extractor by default.\nKnowledge Storage: It has been credited as the primary location of factual knowledge in\nTransformers (Meng et al., 2022).\nComputational Freedom: It is not part of a skip connection, giving it more freedom to create\nand utilize features, as it is not directly added to future computations."}, {"title": "Part 2: Training Classification Probes", "content": "We first selected the activations that best separated the two groups, as\nexplained in the previous section. We performed both manual and automated tests on each activation\nto ensure they were suitable as labelers. We identified several activations capable of performing this\ntask and ultimately chose the one that yielded the most reliable results, activation 1857 in the MLP at\nlayer 10.\nUsing the selected activation, we labeled memorized tokens in a larger dataset.\nWe utilized the SlimPajama dataset (Soboleva et al., 2023) and randomly selected a subset of 200,000\nsamples (approximately 400 million tokens), excluding the GitHub portion due to the distinct patterns\nfound in code compared to general text. We processed the samples through the model and selected\nmemorized and not memorized tokens using the following procedure:\nWe employed a window size of at least 10 tokens, where all tokens had activation\nvalues within the memorized threshold.\nWe ignored completion tokens, same procedure mentioned in\nSection 3.1 Tokenization Issues.\nWe applied the same procedure to select not memorized\ntokens by inverting the threshold.\nWe separately saved sequences that met the memorization criteria\nbut had an average cross-entropy loss greater than 2, which is atypical for memorized sequences.\nWe saved all activations for each token and ultimately obtained one million memorized and one\nmillion not memorized tokens.\nWe trained probe models on each activation type. Specifically, we trained\nlinear probes and two-layer probes with a ReLU non-linear activation in the hidden layer. The probes\nwere trained to classify tokens as memorized (1) or not memorized (0)."}, {"title": "4 Results", "content": "The probes were able to classify memorization with 99.9% accuracy. Figure 7 shows the classification\naccuracy of memorized and not memorized tokens across the layers in the test set. The accuracy\nreaches a very high level as early as layer 3 and continues to approach 100% in subsequent layers.\nIt is important to note that the plots in Figures 7 and 8 show accuracy on a validation set derived\nfrom the same dataset used for training the probe. This dataset comprises memorized sentences\nand randomly selected not memorized sentences. However, this may not be an entirely unbiased\ntest because the memorized sentences originate from specific distributions (e.g., books, speeches,\nlicenses, disclaimers), while the not memorized sentences are randomly selected and can belong to\nany distribution. Consequently, the probe might exploit distributional differences rather than focusing\nsolely on the memorization feature, potentially lowering its loss by learning these distributional cues.\nThe plots accurately reflect the probe's ability to identify memorized sentences within common LLM\ntraining data but may not necessarily indicate its effectiveness in distinguishing a memorized versus a\nnot memorized poem, for example, when both belong to the same distribution. To address this, we\nevaluated the probe on a curated dataset comprising memorized and not memorized samples from the\nsame distribution."}, {"title": "5 Repetition", "content": "To demonstrate the applicability of our method to another mechanism, we applied the same approach\ndescribed in Section 3.1 to train probes for detecting repetition. Repetition occurs when the model\nsimply copies a sequence that has previously appeared in the text. As shown in Figure 11, we present\nthe distribution of activations that effectively separate repeated from not repeated text.\nThe classification accuracy using the activations with the best Cohen's d at each layer is presented in\nFigure 12, while Figure 13 shows the accuracy achieved using our two-layer probe.\nSimilar to our findings with memorization, we achieved nearly 100% accuracy both with individual\nactivations that effectively separate the two groups and with our trained probes.\nThese findings demonstrate that our methodology extends beyond memorization detection. This sug-\ngests our approach could be valuable for studying other language model mechanisms like reasoning,\nknowledge retrieval, pattern matching, translation, physical world understanding, and more."}, {"title": "6 Evaluation", "content": "During the evaluation of our probes, we identified sequences that were classified as memorized by\nthe probe but exhibited an average cross-entropy loss greater than 2, as discussed in Section 3.1. To\nunderstand the reasons behind this discrepancy, we manually analyzed 1,000 of these sequences. The\nresults of this analysis are summarized in Table 1.\nThe \u201cFew large losses\u201d category comprises samples that are generally memorized but contain a small\nnumber of tokens with high loss values, which increases the overall average loss of the sequence.\nThis situation often arises when the sequence follows a specific format that is reused frequently but\nincludes variable elements. For instance, websites might use a standardized template for different\nentities, altering only the name and specifications automatically. Although these sequences were\nflagged as potential misclassifications due to our loss threshold, they are, in fact, memorized.\nThe \u201cCalls to action\u201d (e.g., \u201cClick here to...", "Disclaimers": "e.g., \"By clicking next, you\nagree that we...", "Others": "ategory includes a small number of sequences with unique patterns for which we\ncould not ascertain why the model employs the memorization mechanism.\nIt is important to highlight that in all these cases, the activations that distinguish memorized sequences,\nas discussed in Section 3.1, also classified them as memorized. Therefore, the issue does not lie with\nthe probes themselves.\nTo mitigate this problem, we trained additional probes using the cross-entropy loss as labels. Instead\nof training the model to predict a binary label, memorized (1) or not memorized (0), we trained\nthe probes to predict a continuous value inversely related to the loss of the token. Specifically, we\nassigned a label of 0 for not memorized tokens and used the loss value for memorized tokens, which\ntypically ranges from 0 to 10. We clipped the loss at a maximum of 2 because beyond this point, the\ntoken is clearly not memorized. To accentuate the difference between memorized and not memorized\ntokens, we squared the result. The labeling formula is as follows:\nlabel = (1 - \\frac{min(loss, 2)}{2})^2 (2)\nWith this formula, losses close to 0 are labeled close to 1 (memorized), while losses close to 2 are\nlabeled close to 0 (not memorized).\nThe results demonstrated that the new probe could effectively differentiate between sequences that\nare fully memorized and those that utilize the memorization mechanism but fail to make accurate\npredictions. Specifically, the average probe value for misclassified tokens decreased from 0.91 to 0.27,\nwhile the average for correctly memorized tokens slightly decreased from 0.94 to 0.82. This indicates\nthat the probe maintains its ability to detect memorized tokens while reducing misclassifications. It is\nimportant to note that we are reporting average values; for most memorized tokens, the probe still\noutputs a very high memorization score. The new probe is simply more cautious in its predictions\nand accounts for degrees of memorization.\nAs mentioned, the new probe also aids in detecting varying degrees of memorization. We observed\nthat many sequences are not fully memorized (i.e., with a cross-entropy loss very close to 0) but are"}, {"title": "7 Intervening", "content": "We leverage our trained probes to intervene in the model's activations to alter its behavior. Specifically,\nwe demonstrate that we can suppress the memorization and repetition mechanisms, compelling the\nmodel to utilize alternative internal mechanisms for next-token prediction.\nTo attenuate memorization, we subtract the intervention from the activation set during the forward\npass. The intervention is computed by projecting the activation vector onto the direction of the\nnormalized probe weights, scaling the projection by a hyperparameter \u03b1, and reconstructing the\nvector in that direction to create the final intervention. To better preserve not memorized tokens while\neffectively targeting memorized ones, we square the projection. We then subtract this computed\nintervention from the original activations. Mathematically, the intervention is defined as:\nresult = activations - \\alpha ( \\frac{activations \\cdot W_{probe}}{\\| W_{probe}\\|} )^2 \\frac{W_{probe}}{\\| W_{probe}\\|} (3)\nThe hyperparameter \u03b1 can vary by activation type and layer. For instance, we may require a larger \u03b1\nfor earlier layers and smaller values for later layers, as well as different magnitudes for the output layer\ncompared to the dense attention computations. As observed by Maini et al. (2023), memorization is\ndistributed across many layers. Given the vast continuous search space this introduces, we employed a\ncustom genetic algorithm. The optimization objective was to elevate the loss of memorized sequences\nto match that of not memorized ones, while keeping the loss of not memorized sequences unchanged."}, {"title": "8 Certainty", "content": "During our research, we discovered that the model has a robust mechanism for encoding certainty,\nreflected in the unique, high-magnitude activation of neuron 1668 in the residual stream from layer 4\nonward. Tokens where activation 1668 has smaller values than its peers exhibit greater certainty about\nthe next token. This includes mechanisms like memorization, repetition, completion, knowledge\nretrieval, and other categories where the model demonstrates certainty. Interestingly, we have found\nthat the level of certainty can sometimes reveal the model's knowledge about specific content.\nTo illustrate this, we present a plot comparing the values of activation 1668 at the output layer\nwith the softmax probabilities of the top-1 predictions (see Figure 19). There is a strong negative\nPearson correlation of approximately -0.7 at layer 13, indicating that lower values of activation 1668\ncorrespond to higher probabilities (closer to 100%) of the top-1 prediction. This reflects a strong\ncorrelation between the activation value and the model's certainty in its predictions."}, {"title": "9 Interpretability", "content": "As observed in Figure 11 in the Repetition section, more than 50% of the activations at the output\nof Layer 11 effectively distinguish between tokens associated with repetition and those that are not.\nWe consider it unlikely that such a large proportion of the token representation is dedicated solely to\nhandling repetition. It is crucial to acknowledge that many activations are not exclusively utilized\nfor the specific features we are measuring. For example, activation 1668 represents certainty and\ndifferentiates between repetition, memorization, and other mechanisms.\nEven when accounting for this overlap, the fact that over half of the activations represent these\nkinds of mechanisms is significant. This behavior suggests the diverse ways in which large language\nmodels (LLMs) employ neurons as features. In this context, it is more plausible that the model uses\nthese features as relative values rather than absolute ones. We observed this phenomenon in specific\nsamples during our research, but were unable to find an aggregate measure.\nEventually, we identified a particularly interesting phenomenon in the intermediate activations of\nthe multilayer perceptron (MLP). These activations not only separate memorization but also exhibit\nanother distinct distribution exclusively for the token \"the,\" which, intriguingly, also separates\nmemorization. This pattern occurs in several other activations for different tokens, such as \u201can,\u201d \u201cto,\u201d\n\"for,\" etc.\nFigure 23 illustrates the distribution of activation values for neuron 5422 in MLP Layer 11, comparing\nmemorized and not memorized tokens. The figure shows that the activation values for the token\n\"the\" form a distinct distribution, separate from both memorized and not memorized tokens. This\nindicates that certain neurons are sensitive to specific tokens, in addition to their role in mechanisms\nlike memorization."}, {"title": "10 Discussion", "content": "We believe that the methodology presented in this paper\u2014beginning with a small, diverse dataset of\nsamples that can be distinguished by their activations, and then using these samples to label a larger,\ngeneral dataset for training classifiers\u2014can be effectively applied to other mechanisms within large\nlanguage models (LLMs). While we have demonstrated its applicability to repetition, we anticipate\nthat each mechanism will introduce its own nuances and challenges that must be addressed.\nPerhaps the most impactful aspect of our approach is the ability to classify tokens based on the\nmodel's internal representations. This capability is extremely useful when selecting data for training\nmodels. For example, we can attenuate memorization mechanisms in mathematical problems, thereby\nencouraging the model to rely more on its mathematical reasoning processes.\nWe strongly advocate for more specialized training of LLMs to align them with specific use cases\nrather than solely focusing on downstream tasks. This approach is exemplified by practices such as\nfine-tuning or reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon\net al., 2020; Ouyang et al., 2022) applied to pre-trained models. Instead of merely predicting the\nnext token in a general dataset, the model is trained to be more useful for its intended applications.\nTechniques that enable token labeling can be particularly powerful in achieving this objective.\nWe expect that the methods demonstrated in this paper will be effective for other types of mechanisms,\nsuch as factual retrieval, logical reasoning, mathematics, and so on. This is especially true when\ncombined with other mechanisms like certainty, which can be traced back to the fundamental instances\nwhere they arise at the token level."}, {"title": "11 Limitations", "content": "While our method achieves high accuracy in detecting memorization within LLMs, several limitations\nshould be acknowledged. First, the probes we trained are specific to the model architecture and dataset\nused in our experiments. Although we anticipate that the underlying principles are applicable to other\nmodels, the probes may require adjustments when applied to different architectures or datasets.\nSecond, our method is primarily effective in detecting verbatim memorization. Identifying more\nnuanced forms of memorization, such as format-based or knowledge memorization, may be more\nchallenging and require additional refinement of our techniques.\nLastly, despite extensive evaluations and tests, we recognize that the mechanisms identified by our\nprobes might encompass more than just memorization or repetition. It is possible that these probes\nare capturing additional internal processes within the model, and further research is needed to fully\ndisentangle and understand these underlying mechanisms."}, {"title": "12 Conclusion", "content": "In this paper, we introduced an analytical method for detecting memorization in large language models\nby focusing on their internal neuron activations. By identifying specific activations that effectively\ndistinguish between memorized and not memorized tokens, we trained classification probes that\nachieved near-perfect accuracy in detecting memorization. Our approach not only provides a precise\ndetection mechanism but also enhances interpretability by revealing how memorization manifests\nwithin the model's architecture.\nWe extended our methodology to detect other mechanisms, such as repetition, demonstrating the\nversatility of our approach in probing various internal processes of language models. Furthermore,\nwe showed that it is possible to intervene in the model's activations to suppress specific mechanisms\nsuch as memorization and repetition, effectively altering the model's behavior without compromising\nits overall performance.\nOur findings have significant implications for the development and evaluation of large language"}, {"title": "Appendix", "content": null}, {"title": "A Cohen's d Distribution Plots for Memorization", "content": null}, {"title": "B Classification Accuracy Using Best Activation (Cohen's d) for Memorization", "content": null}]}