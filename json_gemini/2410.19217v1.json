{"title": "NO FREE LUNCH: FUNDAMENTAL LIMITS OF LEARNING NON-HALLUCINATING GENERATIVE MODELS", "authors": ["Changlong Wu", "Ananth Grama", "Wojciech Szpankowski"], "abstract": "Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of \u201challucinations\", where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze the learnability of non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statistically impossible when relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporating inductive biases aligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the facts set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative models have emerged as powerful tools with applications in virtually all socio-economic enterprises. At a high level, these techniques integrate large amounts of data to provide good statistical concentration and build on the resulting mixture of embeddings to produce incredible generative models of text, images, video, mechanical drawings, computer programs, mathematical proofs, and others. However, there is increasing recognition of \"hallucinations\" in generative models, that ultimately limit their utility in critical application settings, such as those that have correctness, accuracy, or safety constraints. Hallucinations correspond to plausible but invalid, incorrect, or misleading outputs. The key challenge in mitigating hallucinations is that there are often no characterizations of the space of \"valid\u201d, \u201ccorrect\u201d, or \u201clogical\u201d assertions, making it difficult to assess hallucinations. A common retort is that generative models should generate hypotheses that are grounded in training data. However, this assertion limits the rich potential of generative systems to that of powerful information retrieval (IR) systems, rather than those capable of new insight not grounded in training data. This tension between the desired ability of AI models to generate new hypothesis, while not generating \"falsifiable\" artifacts, without a well characterized notion of \u201ctrue\u201d artifacts represents the key underlying challenge of mitigating hallucinations.\nAlthough many methods have been proposed for addressing hallucinations, such as factual data enhancement (Dziri et al., 2022), hallucination detection (Manakul et al., 2023), and fine-tuning with human feedback (Ouyang et al., 2022), their performance has primarily been validated through empirical studies, and their suitability for critical applications remains unresolved. This strongly motivates the study of hallucination and its relation to model performance from a general mathematical perspective. This paper advances the state of the art by developing a theoretical framework to understand hallucinations in generative models from a learning-theoretic perspective. We establish a rigorous theoretical foundation for analyzing the learnability of non-hallucinating generative models under various learning paradigms. Specifically, we characterize the conditions under which non-hallucinating learning is possible, identify the appropriate learning rules, and determine the corresponding sample complexity required to achieve learnability."}, {"title": "2 PRELIMINARIES", "content": "We review in this section some notations and concepts from the classical distribution learning literature, and highlight some immediate connections with our non-hallucinating learning paradigm. Let X be an instance space, and P \\subset \\Delta(X) be a hypothesis class. There exists some (unknown) ground truth distribution q \u2208 \\Delta(X) (not necessarily in P) that generates a sample x^n i.i.d. from q. The distribution PAC learning problem aims to find a learning rule \\Phi so that the produced distribution \\hat{p}_n := \\Phi(x^n) satisfies (for some a, \\epsilon > 0):\n$\\left\\|q-\\hat{p}_{n}\\right\\|_{T V} \\leq \\alpha \\inf _{p \\in P}\\|q-p\\|_{T V}+\\epsilon$,\nwith probability > 1 - \\delta over x^n \\sim q.\nA hypothesis class P is said to be a-\\textit{agnostic PAC learnable} if there exists a learning rule \\Phi such that for any \\epsilon, \\delta > 0 there exists a number n (sample size) such that, for any distribution q, equation (3) holds. It can be shown (see e.g. Bousquet et al. (2022)) that any finite hypothesis class P is 3-agnostic learnable for proper learners. Intuitively, agnostic (proper) PAC learning ensures that the"}, {"title": "3 IMPOSSIBILITY RESULTS", "content": "In this section, we establish several impossibility results for certain natural formulations of agnostic (proper) non-hallucinating learning. Analogous to the a-agnostic (distribution) PAC learning formulation, one may consider the following definition:\nDefinition 1 (a-agnostic non-hallucinating learning). A hypothesis class P \u2282 \u0394(X) is a-agnostic non-hallucinating learnable if for some a > 0 there exists a proper learner \\Phi such that for any given \u03b5, \u03b4 > 0, there exists a number n, such that for any facts set T and faithful-demonstrator q w.r.t. T\n$\\operatorname{Pr}_{x^{n} \\stackrel{i . i . d .}{\\sim} q}\\left[\\operatorname{hall}\\left(\\Phi\\left(x^{n}\\right), T\\right)-\\alpha \\inf _{p \\in P} \\operatorname{hall}(p, T) \\geq \\epsilon\\right]<\\delta$.\nIntuitively, a-agnostic non-hallucinating learnability requires that w.h.p., the learner produce a distribution within the class P that has hallucination rate not much higher than the minimal achievable hallucination rate of P, regardless of how the facts set T and demonstrator q are chosen. It is crucial to note that we do not require the learned distribution to be close to the demonstrator q under total variation. Although this may appear to be a much weaker condition than learning the distribution q itself, the following example demonstrates that it is impossible even for a hypothesis class with two elements and the minimal achievable hallucination rate is 0."}, {"title": "4 NON-HALLUCINATING LEARNING VIA KNOWLEDGE OF T", "content": "As we demonstrated by Theorem 1, agnostic (proper) non-hallucinating learning is impossible by restricting the hypothesis class alone, even measured competitively. This is partially due to the"}, {"title": "4.1 THE IMPROPER LEARNING CASE", "content": "Clearly, if the leaner is improper, then a na\u00efve non-hallucinating learner that simply generates the empirical distribution over the training sample x^n never hallucinates. This is clearly not satisfactory, as the learned model is completely non-generalizable. To mitigate this restriction, we introduce certain constrains on the facts set T that allow our learned model to \u201cgeneralize\". We assume that T\u2208 C, where C \u2282 2X is a concept class of all possible sets that T can be chosen from. Here, the concept class would be determined by prior knowledge of the learner on T.\nTo quantify the \u201cgeneralizability\u201d of the learned model, we introduce an additional notion of information measure I that quantifies the amount of \u201cinformation\u201d of distributions in \u0394(\u03a7):\n$\\mathcal{I}: \\Delta(X) \\times X^{n} \\rightarrow \\mathbb{R}_{\\geq 0}$.\nNote that, here we allow the information measure I (p, x^n) to be dependent on the training set x^n as well. For any given information measure function I, we consider the following definition:\nDefinition 3 (Improper non-hallucinating learnable). For any \u03b3 < 1, a concept class C is said to be \u03b3-approximately improper non-hallucinating learnable w.r.t. information measure I, if there exists an (improper) learner \\Phi such that for any \u03b5, \u03b4 > 0, there exists a number n, so that for any T\u2208 C and any faithful-demonstrator q over T, w.p. \u2265 1 \u2212 \u03b4 over xn \u223c q the following holds:\n1. The hallucination rate $\\operatorname{hall}(\\mathcal{P}(x^{n}), T) \\leq \\epsilon$ ;\n2. $\\mathcal{I}(\\Phi(x^{n}), x^{n}) \\geq (1-\\gamma)\\mathcal{I}(q, x^{n})$.\nObserve that the first part of Definition 3 ensures that the learned model does not hallucinate, while the second part ensures that the model is as informative as the demonstrator so that generalizability is possible. Moreover, it is crucial to note that, although the facts set T is restricted to the concept class C, the demonstrator q is unconstrained, as long as it is faithful w.r.t. T.\nThere are many natural information measures. We mention a few here:\n1. For discrete X, Shannon entropy is defined as H(p) := \u03a3x\u2208X p[x] log p[x] which measures the (absolute) information contained in p independent of the training set xn;\n2. The a-R\u00e9nyi entropy is defined as $H_{\\alpha}(p)=\\frac{1}{1-\\alpha} \\log \\left(\\sum_{x \\in X} p[x]^{\\alpha}\\right)$, where a > 0 and a \u2260 1, which subsumes the Shannon entropy for a \u2192 1;\n3. The out-of-sample mass is defined as (for n \u2265 1): $I(p, x^{n}) := p[X \\setminus\\{x_{1}, \\ldots, x_{n}\\}]$, which measures the amount of probability mass of p assigned outside of the training set.\nThe specific choice of information measure typically depends on the task at hand. We show in the following theorem, perhaps surprisingly, that improper non-hallucinating learning is possible if the concept class C has finite VC-dimension, regardless of what information measure is selected.\nTheorem 2. If a concept class C has finite VC-dimension, then C is 0-approximately (improper) non- hallucinating learnable w.r.t. any information measure function I. Moreover, the sample complexity is upper bounded by $n<\\mathcal{O}\\left(\\frac{V C(C) \\log (1 / \\epsilon)+\\log (1 / \\delta)}{\\epsilon}\\right)$\nProof. Let T* \u2208 C be a ground truth facts set and q* be a faithful-demonstrator w.r.t. T*. Let xn ~ q* be a set of samples. We define the version space:\n$C_{n}=\\{T \\in C: \\forall i<n, x_{i} \\in T\\}$"}, {"title": "4.2 THE PROPER LEARNING CASE", "content": "We demonstrated in Theorem 2 that non-hallucinating learning is possible for improper learners if the facts set lies in a concept class of finite VC-dimension. A natural questions is whether such an approach can be extended to the proper learning case, i.e., instead of allowing the learner to output an arbitrary distribution in \u2206(X), we restrict the output to some hypothesis class PC \u2206(X).\nWe again assume that the facts set T is selected from some concept class C. Let q be any faithful demonstrator w.r.t. T and xn \u223c q. A natural learning rule that is inspired from (7) is as follows:\n$\\Phi(x^{n})=\\arg \\max _{p \\in P}\\left\\{\\mathcal{I}(p, x^{n}): \\operatorname{hall}(p, T) \\leq \\epsilon, \\forall T \\in C_{n}\\right\\}$"}, {"title": "5 CONCLUSION AND DISCUSSION", "content": "In this paper, we investigated the learnability of non-hallucinating generative models from a learning-theoretic perspective. We showed that agnostic non-hallucinating learning is statistically"}, {"title": "A ADDITIONAL LOWER BOUNDS", "content": "In this appendix, we provide a lower bound for improper non-hallucinating learning when the Shannon entropy is chosen as the information measure\nTheorem 5. For any number d \u2208 N, there exists a concept class C of VC-dimension d such that the sample complexity of 0-approximately (improper) non-hallucinate learning C w.r.t. the Shannon entropy function H is lower bounded by \u03a9(d) for all sufficiently small \u20ac, \u03b4 > 0.\nProof. We now take X = [d] and define C to be a collection of subsets of X such that \u2200T \u2208 C, |T| = d/2 and\n$\\forall T_{1} \\neq T_{2} \\in C, \\left|T_{1} \\cap T_{2}\\right| \\leq \\frac{d}{4}$.\nIt can be shown that such a collection exists and |C| \u2265 \u221aed/16 see e.g., (Wu et al., 2023, Thm. D.1) for a proof. Moreover, the VC-dimension of C is at most d. For any T\u2208 C, we denote qT as the uniform distribution over T. Now, in order for the Definition 3 to hold, for any (T, qT) the learner must produce a distribution p such that: (1) p[T] \u2265 1 - \u20ac; and (2) the Shannon entropy H(p) \u2265 log(d/2). We claim that for any other T' \u2208 C that differs from T, one must have p[T'] < 1 - \u0454, for any \u20ac < 0.07. To see this, we split the support of p into 3 parts A1 = T\\T', A2 = T \u2229 T' and A3 = [d]\\T. It is clear that p[A3] < \u20ac. Assume now that p[T'] > 1 - \u20ac, then one must have p[A2] \u2265 1 - 2\u20ac. By expressing the entropy of p conditioning on the A1, A2 and A3 we have\n$H(\\hat{p})=\\sum_{i \\in\\{1,2,3\\}} \\hat{p}[A_{i}] \\log \\frac{1}{\\hat{p}[A_{i}]}+\\hat{p}[A_{i}] \\cdot H(\\hat{p} \\mid A_{i}),$\nwhere H (p | Ai) is the entropy of the conditional distribution of p on Ai. Observe that H(p | A1) \u2264 log d/2, H(p | A2) \u2264 logd/4 and H(p | A3) \u2264 log d, since uniform distribution maximizes entropy. Moreover, our previous discussion yields that p[A1],p[A3] < \u20ac and p[A2] \u2265 1 - 2\u20ac."}]}