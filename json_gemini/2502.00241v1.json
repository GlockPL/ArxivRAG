{"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "authors": ["Shiqi He", "Insu Jang", "Mosharaf Chowdhury"], "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category of multimodal models because of their many practical use cases, including in healthcare, robotics, and accessibility. Unfortunately, even though different VLMs in the literature demonstrate impressive visual capabilities in different benchmarks, they are handcrafted by human experts; there is no automated framework to create task-specific multimodal models.\nWe introduce Mordal, an automated multimodal model search framework that efficiently finds the best VLM for a user-defined task without manual intervention. Mordal achieves this both by reducing the number of candidates to consider during the search process and by minimizing the time required to evaluate each remaining candidate. Our evaluation shows that Mordal can find the best VLM for a given problem using up to 8.9\u00d7-11.6\u00d7 lower GPU hours than grid search. In the process of our evaluation, we have also discovered new VLMs that outperform their state-of-the-art counterparts.", "sections": [{"title": "1 Introduction", "content": "Vision Language Models (VLMs) bridge the gap between visual and language understanding, rising as the dominant approach to solving visual information-based tasks. Notably, GPT-4V (OpenAI, 2023), a large-scale multimodal language model, demonstrates impressive vision reasoning capabilities by taking images as input and generating detailed natural language descriptions. Although the technical details behind GPT-4V remain undisclosed, researchers have proposed a number of publicly available VLMs (e.g., MiniGPT-4 (Chen et al., 2023), LLaVA (Liu et al., 2023a) and Qwen-VL (Bai et al., 2023)) that aim to match GPT-4's capabilities. Many of these open-source VLMs share a similar architecture, in which a feature projector converts the image embeddings generated by a vision encoder and feeds it to a large language model (LLM) along with the text embeddings.\nTo construct and train a VLM, the common approach starts from selecting an appropriate pretrained vision encoder and language model. Thanks to the ever-growing ecosystem like HuggingFace (Face, 2025), developers are able to choose from countless pretrained models for their own VLMs. Unfortunately, despite the great number of available models, it is difficult to determine which pretrained models (i.e., vision encoders and language models) are the most appropriate ones. Given a user-specific downstream task, it is unclear which pretrained models can form the VLM that will meet the user's needs most effectively. Unfortunately, as shown in Figure 1, no single VLM consistently outperforms the others in accuracy across all dimensions. Their capabilities vary significantly depending on their pretrained components (Liu et al., 2023c; Xu et al., 2023).\nWith significant constraints on available time and computation cost, it is unrealistic to try every pretrained model combination and train corresponding VLM candidates. Training a single VLM with vision-text alignment data could take"}, {"title": "2 Background and Motivation", "content": "more than 100 GPU hours (Liu et al., 2023a; Karamcheti et al., 2024). It is also unreliable and unpredictable to rely on human \"intuitions\" to select pretrained models for the given downstream task, such as selecting the latest one or the most well-known one (Liu et al., 2023c). In addition, existing model selection methods (Brown et al., 2020; You et al., 2021; Lin et al., 2024), like evaluating the highest zero-shot performance, will fail in VLM scenario since they were designed for vision-only and LLM-only tasks. With an untrained feature projector, the LLM will not understand the image embeddings and can produce random outputs. This motivates the key question of this work: How to effectively find the best pretrained models in a VLM given a downstream task?\nTo address this question, we formulate the pretrained model selection problem for VLMs for the first time and model it as a resource-constrained task to predict the alignment performance of a VLM; i.e., the performance on the downstream task after training the feature projector with the vision-text alignment data. We empirically show that existing VLMs fail to dominate all downstream tasks and a naive approach like grid search is infeasible in practice.\nWe present Mordal, a novel pretrained model selection framework, which automatically and efficiently explores different pretrained model combinations in VLM. Mordal builds on our observation that efficiently solving this problem needs jointly considering two optimization directions: (1) minimizing the number of VLM candidates, where each candidate has different pretrained vision encoders and LLMs; and (2) reducing the evaluation time for each candidate. Overall, we make the following contributions in this work:\n\u2022 We introduce and define the pretrained model selection problem in the context of VLMs for the first time and demonstrate that off-the-shelf VLMs often do not contain the best pretrained components for a given downstream task.\n\u2022 We propose Mordal, an efficient pretrained model search framework, to find the best VLM for a given downstream task. Mordal clusters VLM candidates by their representation similarities while employing an early stopping mechanism and an observational scaling law to reduce evaluation time.\n\u2022 We implement Mordal and provide a flexible interface. Our extensive evaluations show that Mordal efficiently finds near-optimal VLMs with 8.9\u00d7-11.6\u00d7 less computation time than grid search for a wide range of tasks.\nWe start by outlining the architecture of typical VLMs. Following this, we highlight the challenges in pretrained model"}, {"title": "2.1 Vision Language Model", "content": "VLMs are compound AI models with three key components:\n\u2022 Vision Encoder (VE). The vision encoder is responsible for processing input images and extracting relevant features. Potential encoder options are CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023), InternViT (Chen et al., 2024) and DINOv2 (Oquab et al., 2023), etc.\n\u2022 Feature Projector (FP). The vision-language cross-modal feature projector aims to align the encoded image features to the text token embedding space. The feature projector can be achieved directly by a Linear Projector or Multi-Layer Perceptron (MLP), i.e., several linear projectors interleaved with non-linear activation functions (Liu et al., 2023b).\n\u2022 Language Model (LM). The language model processes mixed embeddings generated from both user instructions in text as well as image inputs. The commonly used LMs in VLMs are decoder-only LLMs, which include Llama (Touvron et al., 2023a), Vicuna (Chiang et al., 2023) and Mistral (Jiang et al., 2023).\nWhen serving a request, a VLM first processes an input image with an image processor and passes it to a vision encoder. The vision encoder outputs a sequence of raw vision embeddings (or patches). The feature projector will then map the raw vision embeddings to aligned vision embeddings, which will append to text prompt embedding and feed into the LM decoder to generate output text."}, {"title": "2.2 Pretrained Model Selection: No Silver Bullet", "content": "VLMs are versatile and powerful because most vision tasks can be formulated as next-token prediction. To train a VLM for a specific downstream task, developers usually cherry-pick pretrained vision encoders and language models for alignment. However, different pretrained models have varying capacities, which affect VLM performance. To investigate the impact of different pretrained models, we conduct grid search on 49 VLM candidates (i.e., seven vision encoders and seven language models) and train each candidate with the same alignment data. While complete evaluation is described in Section 4, we present the performance of four representative VLM candidates on six datasets, across three dimensions: Visual QA (GQA (Hudson & Manning, 2019) and VizWiz (Gurari et al., 2018)), Doc QA (ChartQA (Masry et al., 2022) and DocVQA (Mathew et al., 2021)) and Knowledge (ScienceQA (Lu et al., 2022) and"}, {"title": "2.3 Exhaustive Search is Prohibitively Expensive", "content": "Solving the pretrained model selection problem by relying on empirical experiences or intuitions, such as choosing the newest, largest or most well-known model, is unstable and unpredictable. For example, in Table 1, a VLM candidate with Llama-3-8B does not have a better performance than a VLM candidate with Vicuna-1.5-7B. Additionally, since the VLM is not aligned, it is impossible to directly evaluate its zero-shot performance (Brown et al., 2020). Most existing model selection methods are designed for classification, regression, or LLM-only generation tasks, and do not account for the vision encoder and the alignment process (Vu et al., 2020; You et al., 2021; Lin et al., 2024). This makes the selection problem particularly challenging, especially in resource-constrained environments.\nWhile an exhaustive search strategy is feasible within a limited search space, it becomes impractical when dealing with a large number of pretrained models \u2013 for reference, there are more than 150,000 LLMs on the HuggingFace platform as of Jan 2025. Each candidate requires training the projector and potentially finetuning pretrained components, making it unrealistic to train all candidates before making a selection.\nEven using a pretrained 7B LLM as the backbone, training a single VLM can take over 100 GPU hours with the LLaVA-1.5 dataset. Indeed, manually generating suitable results for"}, {"title": "3 Mordal Design", "content": "Table 1 cost us 1000+ GPU hours (many inferior combinations are not shown for brevity). Furthermore, researchers are continuously developing new neural architectures and improving datasets to enhance VLM performance. This means that whenever a new encoder or LLM is released, developers must manually re-train VLMs with the new components to evaluate performance.\nTo address the inefficiencies of exhaustive search in pretrained model selection, we must reduce the search cost along two key dimensions: (1) reducing the number of candidates and (2) minimizing the time required to evaluate each candidate. By optimizing these two dimensions, the exhaustive search process can be replaced with a more efficient pipeline that balances time consumption and selection accuracy.\nThis section details the core components of Mordal's design. The exhaustive search is expensive because it (1) needs to evaluate every candidate (large search space), and (2) needs to train each candidate with a full dataset to see its performance (high evaluation cost). Mordal reduces search space by clustering the candidates based on their similarity and by introducing a two-step inter- and intra-cluster evaluation (\u00a73.1), and reduces evaluation cost of each candidate with an early stopping mechanism and observational scaling laws (\u00a73.2)."}, {"title": "3.1 Candidate Clustering", "content": "With the rapid increase in the number of pretrained models in popular model zoos (e.g., HuggingFace (Face, 2025)), evaluating every candidate combination is expensive. Based on prior observations that similar models tend to have similar performance (Hu et al., 2023; Yu et al., 2024; Lai et al., 2023), Mordal clusters candidates and evaluates them in two steps: inter-cluster and intra-cluster, to reduce the search space.\nMeasuring similarity. Measuring the similarity of VLM candidates - without training projectors between vision encoders and language models - is challenging. Parameter similarity (Lai et al., 2023), which has been used to measure"}, {"title": "3.2 Efficient Evaluation", "content": "similarity between model architectures, does not fully consider the data distribution pattern in the target task. Models with high parameter similarity could still show different performance on different tasks. Therefore, in Mordal, we consider representation similarity between VLM candidates, which depends on the target task.\nMordal employs centered kernel alignment (CKA) (Kornblith et al., 2019) to evaluate the similarity of representations between two VLM model structures. CKA has been proven to be an effective tool for understanding and comparing the information encoded across different layers of neural networks. Formally, CKA operates on two datasets by analyzing their corresponding activation matrices. The CKA score is defined as:\n$CKA(K, L) = \\frac{HSIC(K, L)}{\\sqrt{HSIC(K, K) HSIC(L, L)}}$ (1)\nwhere K and L are the kernel matrices of activations of two models. HSIC is the Hilbert-Schmidt Independence Criterion (HSIC) defined as:\n$HSIC(K, L) = Tr(KHLH)$ (2)\nwhere Tr() is the trace of a matrix and H is the centering matrix $H = I \u2013 \\frac{1}{n}11^T$. CKA is particularly useful in this context for two reasons. First, CKA can compare representations with differing shapes generated by different pretrained models, a task where traditional metrics such as cosine similarity fail. Second, as vision representations are commonly projected through MLP layers, this transformation does not compromise CKA's properties, making it robust and well-suited for such evaluations.\nTwo-step clustering. Computing the CKA score between each pair of candidates can be expensive, since K and L are activations from batched data input. To reduce the amount of pair-wise CKA computation, Mordal introduces a two-step VLM clustering strategy: (1) clustering vision encoders, (2) clustering language models based on a fixed vision representation. We detail the clustering process as follows:\n\u2022 Vision encoder clustering. Mordal computes the representation similarity between vision encoders using"}, {"title": "3 Mordal Design", "content": "CKA. A distance matrix $Dist_{ve}$ is then constructed based on the dissimilarity values. The clustering function will take an input threshold $t_{ve}$ and output the vision encoder clusters $C_{ve}$.\n\u2022 Language model clustering. As shown in Figure 3, Mordal constructs different language model clusters based on each vision encoder cluster. Using the medoid vision encoder from each cluster, Mordal generates a fixed image embedding for the dataset and a warmed-up feature projector transforms the shape of image embeddings to match the LLM input shape. A distance matrix $Dist_{lm}$ will record the dissimilarity and language models are then clustered based on an input threshold $t_{lm}$.\nFor each vision encoder cluster and the corresponding language model clusters, Mordal generates the candidate clusters by conducting the Cartesian product of the two clusters. The two-step clustering process reduces the computation costs by avoiding computing the similarity between candidates that have dissimilar vision encoders, which we show in Section 4.3 that usually yield distinct performance.\nInter- and intra-cluster evaluation. After grouping the candidates into clusters, Mordal finds the best candidate with two-step evaluation: inter-cluster evaluation and intra-cluster evaluation. The detailed process is shown in Figure 4. Given that candidates in the same cluster have similar performance, inter-cluster evaluation first picks medoid from each cluster as the representative candidate and compares"}, {"title": "3.2 Efficient Evaluation", "content": "performance of each cluster to eliminate poorly performing clusters. The remaining Top-K clusters will be evaluated in the second step, where K is a user-defined parameter.\nWith the inter-cluster evaluation, many fewer candidates remain in consideration. Intra-cluster evaluation goes back to candidate-granularity evaluation by aggregating candidates from the remaining Top-K clusters. It trains all of them on the given alignment dataset, and returns the one with the best performance to the user.\nSection 3.1 reduces the search space; however, evaluating each candidate in both stages (i.e., inter- and intra-) still requires training each candidate with a full dataset to assess its performance. Mordal introduces an early stopping mechanism that eliminates the clusters more aggressively but with high fidelity for inter-cluster exploration and an efficient scaling prediction algorithm based on an observational scaling law for intra-cluster exploration.\nEarly stopping mechanism. Some obviously poor-performing candidates can be eliminated at an early stage of inter-cluster evaluation to allocate resources to more promising candidates. Mordal applies a simple but efficient Successive Halving Algorithm (SHA) (Jamieson & Talwalkar, 2016) to roughly filter out the candidates before observing the scaling law. It ensures that computational resources are focused on the most promising clusters.\nIn Figure 5a, SHA is conducted during inter-cluster evaluation, where all representative candidates are evaluated with the maximum data sample ratio R. It consists of multiple rounds, also known as rung. In each round, SHA allocates a budget b to each candidate, evaluates all of them, and keeps the top 1/\u03b7 candidates. In the next round, SHA increases"}, {"title": "4 Evaluation", "content": "the budget to b \u00d7 \u03b7 per candidate. This repeats until representative candidates are converged or Top-K candidates are determined. In cases where the number of remaining candidates is large, it is also possible to apply SHA again to intra-cluster evaluation. With SHA as the early stopping mechanism, Mordal eliminates poor-performing candidates earlier. It optimizes evaluation time for each candidate, which is an orthogonal dimension to search space, and therefore, speeds up the evaluation process.\nObservational scaling law. The scaling law has been explored in previous works (Lin et al., 2024; Ruan et al., 2024), but limited to LLM selection. In standard scaling laws (Kaplan et al., 2020), the \"scale\" is defined by the compute resources allocated to training LLMs, such as the number of training FLOPs C, model parameters N, and training samples D. Scaling laws are typically formulated as a power-law relationship between LLMs' cross-entropy loss L and their compute scale measures. A common functional form for transformer architecture (Hoffmann et al., 2022) is shown as:\n$L(N, D) = \\frac{a}{N^\u03b1} + \\frac{b}{D^\u03b2} + e$ (3)\nwhere parameters a, \u00df, a, b, e are fitted by training LLMs across different compute scales, varying N and/or D, and measuring their loss. With this formula, a hypothesized power-law relationship between D and loss L is that they are linearly correlated under the log-log scale. In Mordal, our focus differs from previous scaling laws in our goals existing scaling laws aim to understand the scaling properties of pretraining and finetuning LLMs. In contrast, Mordal is interested in scaling laws of VLM alignment performance for a fixed model parameter size. We find that an observational scaling law exists for VLMs and present the results in Section 4.3. By fixing model parameters N and changing training sample D, we may construct log-linear relationship"}, {"title": "Algorithm 1: Scaling Prediction", "content": "for each candidate and predict its performance.\nWith the observational scaling law, Mordal employs a scaling prediction algorithm that automatically detects the log-linear scaling with sampled alignment dataset to conserve resources. As shown in Algorithm 1, for each candidate c in remaining candidate list C, the algorithm starts from an initial data sample ratio R (e.g., 1). It evaluates a checkpoint trained on randomly sampled data with ratio R. The corresponding performance point (Log(r), Log(Err)) will be recorded in list P. After that, the algorithm reduces data sample ratio and repeats the above process until enough performance points (i.e., p) are collected and the log-linear relationship is observed for a given candidate c. Since data sample ratio r is decreasing, we may effectively start the evaluation of r/u from existing intermediate checkpoints to save computation costs.\nFor each candidate c, Mordal observes a distinct log-linear relationship and fits a linear regression model fc based on P. It uses the fitted model f to predict the candidate performance with full data alignment (i.e., f (1)) and chooses the best candidate. Note that scaling prediction is orthogonal to the two-stage evaluation. Although it cannot be applied to inter-cluster evaluation speculatively since observing the log-linear relationship requires training the candidate with some portion of alignment data, it can be used during intra-cluster evaluation and save evaluation time for promising candidates."}, {"title": "4.1 Experimental Setup", "content": "We conducted extensive experiments to thoroughly evaluate Mordal's performance. These experiments assessed its effectiveness in pretrained model selection and performed an ablation study. The key findings are summarized as follows:\n1. Mordal identifies the optimal combination of vision encoder and LLM for the target task 8.9\u00d7-11.6\u00d7 faster than the grid search baseline, as shown in Section 4.2. For each target task, the best candidate surpasses the LLaVA-1.5-7B equivalent structure in accuracy while using the same alignment dataset.\n2. We further validate the effectiveness of model similarity computation and observational scaling law in Section 4.3. By conducting the ablation studies, we show that each component in Mordal helps reduce the total search time while ensuring that it can still identify the top-performing candidates.\nThe experiments are conducted on six mainstream datasets across three domains with seven vision encoders and seven LLMs. We deployed Mordal on a set of VMs on a cluster with a total of 16 NVIDIA A40 GPUs. Each GPU has 48 GB GDDR6 memory. More training and hyperparameter settings are described in Appendix C.\nDataset. In experiments, we use LLaVA-1.5-Instruction datasets (Liu et al., 2023a) for alignment. In practice, users may have their own alignment datasets. For target datasets, we pick six representative datasets, which cover three domains: Visual QA, Doc QA and Knowledge. These datasets are commonly used as benchmarks to assess a model's performance (Zhang et al., 2024).\nModel zoo. We select seven vision encoders and seven language models based on popularity and performance. The selected vision models include both language-supervised models (e.g., OpenAI CLIP) and self-supervised models (e.g., DinoV2). For LLMs, all models follow the decoder-only structure, and we pick the most used 7B LLMs from HuggingFace. The complete list of our model zoo is shown in Table 4.\nBaseline and metric. We consider grid search as the baseline, which fully trains and evaluates every VLM candidate. We measure the total evaluation time for each method to get the top-1 model for the given task. Additionally, to provide a comprehensive assessment, we also pay attention to the rank of candidates. In grid search, candidates are ranked based on their performance, whereas in Mordal, candidates are ranked by their elimination order. Ideally, if Ti represents the rank of candidate i in grid search and Si represents the rank of candidate i in Mordal, we expect Si is better than Sj if Ti is better than Tj. This can be captured by Kendall's"}, {"title": "4.2 Performance Results", "content": "T coefficient defined as:\n$\u03c4= \\frac{2}{M(M-1)} \\sum_{1<i<j<M} sgn(T_i-T_j)sgn(S_i-S_j)$ (4)\nwhere M is the total number of candidates and sgn() is the sign function. A perfect ranking match results in T = 1. To further focus on top-performing candidates, we adopt the weighted Kendall's coefficient Tw, which is previously used in (You et al., 2021; Vigna, 2015). The details for implementing Tw can be found in SciPy (Virtanen et al., 2020).\nThis section examines the total training time required by Mordal to identify the best VLM candidate compared to grid search. Using the LLaVA-1.5-Instruction dataset for alignment, we also evaluate candidate accuracy across six datasets in comparison with the VLM structure (i.e., CLIP-Vicuna) used for LLaVA-1.5-7B. Additionally, we compute Kendall's T coefficient for Top-10 ground-truth candidates found by grid search and their order in Mordal.\nMordal is significantly faster than grid search. Table 3 presents a comprehensive comparison between Mordal and grid search across six datasets. In each task, grid search exhaustively evaluates 49 candidates, requiring 5439 GPU hours. Since the same alignment dataset is used across all six tasks, the total search time for grid search remains constant. Notably, for most tasks, the best VLM candidates have different pretrained model combinations and all of them surpass the performance of the LLaVA-1.5-7B equivalent structure, highlighting the importance of pretrained model selection. Compared to grid search, Mordal identifies"}, {"title": "4.3 Ablation Studies", "content": "fies the top-performing VLM candidates significantly faster and successfully selects the best candidate for five out of six tasks. For example, on VizWiz, Mordal completes the search in just 469 training hours, identifying SigLIP-Mistral as the top-1 model with 46.9% accuracy. The search time for Mordal varies depending on factors such as the number of clusters formed and the convergence speed of candidates across tasks. Nevertheless, it consistently achieves substantial speedups ranging from 8.9\u00d7 to 11.6\u00d7 compared to grid search.\nMordal maintains a good performance in finding top-performing models. Beyond selecting the Top-1 model, we assess Mordal's capability to identify the Top-5 and Top-10 VLM candidates. As shown in Figure 6 and Figure 7, Mordal consistently identifies four out of the Top-5 performing candidates when compared to grid search. However, some candidates (e.g., DINOv2-Vicuna) are overlooked due to two reasons: (1) these candidates belong to poorly performing clusters; or (2) they demonstrate suboptimal performance and are early stopped. In both scenarios, resources are reallocated to better-performing models. We further compute Kendall's T values based on Top-10 candidates identified via grid search. Mordal achieves high T values across all six datasets. For instance, Mordal achieves \u0430\u0442 value of 0.96 on ScienceQA, indicating that it correctly ranks eight out of ten models.\nIn this section, we first validate the model similarity method and observational scaling law in Mordal. Then we conduct a comprehensive ablation study to evaluate the individual components of Mordal and their contributions to overall performance. Specifically, we analyze the effects of candidate"}, {"title": "4.3 Ablation Studies", "content": "clustering, early stopping and scaling prediction.\nModel similarity validation. To validate the effectiveness of CKA, we train four VLM candidates with four vision encoders and the same LLM backend Vicuna-1.5-7B. The trained VLMs are evaluated on two different datasets: ScienceQA and VizWiz. As shown in Figure 8, the image representation (i.e., outputs of projection heads) generated by different vision encoders show different levels of similarity to each other. For example, with input images from ScienceQA, CLIP, SigLIP, and DFN-CLIP generate similar representations. Meanwhile, DFN-CLIP and InternViT generate similar representations in VizWiz. From the results in Table 3, the vision encoders with similar representations will have similar performance with the same language model. Although this observation does not allow us to directly predict the target task's performance, it helps reduce the number of VLM candidates by eliminating similar pretrained models.\nObservation scaling law validation. To validate the existence of scaling laws in VLM alignment, we train multiple VLM combinations with different vision encoders and the same language model background on a sampled alignment dataset. The trained VLMs are evaluated on two different datasets: GQA and AI2D. As shown in Figure 9, we observe log-linear scaling across different VLMs, which supports the design of scaling prediction. However, the log-linear scaling will only appear after a certain number of training samples, which is consistent with the conclusion in previous work (Lin et al., 2024; Ruan et al., 2024).\nEffect of candidate clustering. Candidate clustering plays a vital role in Mordal as it enables inter- and intra-cluster evaluation. As illustrated in Figure 10a, inter- and"}, {"title": "5 Conclusion", "content": "intra-cluster evaluation (i.e., Mordal without efficient evaluation) significantly reduces training time while maintaining a high \u315c value. By grouping candidates with similar characteristics into clusters, Mordal evaluates representative candidates from each cluster first and eliminates candidates in poor-performed clusters.\nEffect of efficient exploration. Early stopping mechanism prunes candidates during the early stage of training. While it significantly reduces the search time, applying it during the entire evaluation (i.e., Mordal without scaling prediction) will eliminate some promising candidates (e.g., SigLIP-Qwen on AI2D shown in Figure 9). It evaluates candidates based on intermediate performance and leads to a low - value. Mordal limits the usage of early stopping and introduces scaling prediction instead to predict the performance of promising candidates. As shown in Figure 10b, this leads to a significant improvement in the \u315c value while further reducing the total training time.\nIn this work, we introduced Mordal, an automated pretrained model selection framework designed to address the challenges of efficiently identifying the best vision-language model (VLM) for a given downstream task. Mordal reduces the complexity of candidate selection by minimizing both the number of VLM candidates and the evaluation time for each candidate. Through extensive experiments, we demonstrate that Mordal achieves 8.9\u00d7-11.6\u00d7 reduction in search time compared to traditional grid search while achieving robust and near-optimal results."}, {"title": "A Algorithm", "content": "Detailed two-step clustering algorithm for candidate clustering as described in Section 3.1. We adopt the MinibatchCKA for computation efficiency, which is introduced in (Nguyen et al., 2020) and later used in (Raghu et al., 2021). In LLM clustering, we use the last hidden state from LLM as the sentence representation for CKA computation since it produces the best clustering performance. We leverage the hierarchical clustering from scipy.cluster.hierarchy library in SciPy (Virtanen et al., 2020). It is possible to adopt other clustering methods. Overall, this approach significantly reduces the number of candidate to explore."}, {"title": "B Implementation", "content": "This section describes Mordal's implementation details and configurations to ensure efficient and scalable pretrained model selection. We highlight key design choices that optimize resource utilization without compromising performance. As shown in code snippet Listing 1, to submit a job, users need to provide the alignment data and data for the target task. Users are also allowed to submit a list of available pretrained models. In vlm_kwargs, users may specify the projector's architecture and whether to free pretrained components. In mordal_kwargs,"}, {"title": "C Additional Experiments", "content": "When unfree pretrained components, instead of performing expensive full finetuning, we uses Low-Rank Adaptation (LoRA) (Hu et al., 2021) implemented by Parameter-Efficient Fine-Tuning (PEFT) (Mangrulkar et al., 2022) and manage LoRA configurations for each pretrained model. LoRA injects task-specific adaptations into the pretrained model by learning low-rank updates for certain layers while keeping the core parameters frozen. This significantly reduces computational and memory overhead, making finetuning feasible under resource-constrained settings. We incorporate Flash Attention (Dao, 2023) for scalable attention computation, which is a memory-efficient implementation of scaled dot-product attention that avoids redundant operations and reduces memory overhead. All models are trained with torch bfloat16 precision, which balances computational efficiency and numerical stability. Mordal also automatically allocates idle GPU resources to candidates that are not converged to speed up the exploration process."}, {"title": "C.1 Evaluation Time Breakdown", "content": "Mordal hyperparameter settings. For pretrained model clustering, the threshold for vision encoder and LLMs are set to tve = 0.7 and tum = 0.8, respectively. And the warmup round for the feature projector is 10. When performing an efficient evaluation, we set both topkinter and topkintra to 3, which means that the Top-3 clusters will be selected in inter-cluster evaluation and Top-3 candidates will be selected in intra-cluster evaluation with the early stopping mechanism. We use \u03b7 = 2 as the default reduction factor, which is consistent with typical SHA settings. We further set p = 3 and d = 5e - 5 by default for scaling prediction. We will discuss the effect of hyperparameters in Appendix C.2.\nAs shown in Table 2, When evaluating Mordal on different tasks, the total evaluation time required is different. To investigate the differences in time consumption, we analyze the breakdown time for each component of Mordal and present the result in 11. As expected, the early stopping stage consists of most of the evaluation time, and the prediction only takes a small part of time. This is because the scaling prediction is only performed for the candidates left after early stopping. The time varies depending on when the model is converged and scaling is observed. The time for inter-cluster and intra-cluster early stopping depends on the number of clusters, controlled by tve and tum. The cluster is generally less obvious for difficult tasks (e.g., ChartQA and DocVQA), leading to many clusters with only one candidate. As fewer candidates are eliminated, the total evaluation time increases."}, {"title": "C.2 Sensitivity Analysis", "content": "Sensitivity analysis explores how key hyperparameters affect Mordal's performance and efficiency, focusing on clustering thresholds tve and tlum, exploration parameters Sinter and Sintra, and scaling prediction p. Careful tuning of these parameters ensures efficient operation without compromising Mordal's ability to select top-performing models. Generally, Mordal is robust and consistently identifies the best-performing model across most hyperparameter settings.\nEffect of clustering hyperparameters. The clustering threshold tve and tum significantly affects Mordal's performance and efficiency. As shown in Table 5, a smaller threshold tve = 0.5 creates fewer, larger clusters based on the LLM's general characteristics, which will lower the 7 value by missing finer distinctions and discarding strong candidates with other LLM backend. On the other hand, a larger threshold tve = 0.9 results in more, smaller clusters, capturing subtle differences and improving the 7 value but increasing the number of candidates to evaluate during scaling prediction, leading to longer searching time. Balancing the threshold is crucial to ensure diverse clusters while keeping the computational cost reasonable.\nEffect of inter- and intra-cluster hyperparameters. Based on Table 5, inter-cluster exploration generally has a greater impact on Mordal's performance than intra-cluster exploration. A smaller topkinter reduces the number of clusters evaluated, speeding up the search but lowering the T value by excluding promising clusters aggressively. A larger topkinter explores more clusters, increasing search time but improving the T value by retaining diverse clusters. Intra-cluster early stopping affects candidate selection within clusters, with smaller topkintra focusing on fewer candidates and larger topkintra exploring more candidates for scaling prediction. However, its influence is smaller, as clusters already limit diversity. Properly balancing these topkinter and topkintra ensures efficient exploration and strong performance.\nEffect of scaling prediction hyperparameters. The scaling prediction parameter p has minimal impact on Mordal's overall performance but affects search time. Increasing p beyond appropriate values adds to the computational cost without improving results. In practice, p = 3 is sufficient for constructing the linear regression model used in scaling prediction."}, {"title": "D Discussion", "content": "While Mordal demonstrates promising results in efficiently selecting pretrained models fo VLM with small vision encoders and 7B LLMs under a single-request, certain limitations must be addressed to extend its utility and effectiveness. Below, we discuss two key areas for improvement: scaling to different model sizes and optimizing across similar user requests.\nVLM alignment. One must go through an alignment process to ensure that the individual components of the VLM are well integrated before using it. Developers integrate pretrained LLMs and visual encoders, training the projector from"}, {"title": "E Related Work", "content": "scratch using visual alignment datasets like VQA (Antol et al., 2015). During the alignment process, pretrained components may remain"}]}