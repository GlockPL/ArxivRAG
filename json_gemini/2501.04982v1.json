{"title": "CuRLA: Curriculum Learning Based Deep Reinforcement Learning For Autonomous Driving", "authors": ["Bhargava Uppuluri", "Anjel Patel", "Neil Mehta", "Sridhar Kamath", "Pratyush Chakraborty"], "abstract": "In autonomous driving, traditional Computer Vision (CV) agents often struggle in unfamiliar situations due to biases in the training data. Deep Reinforcement Learning (DRL) agents address this by learning from experience and maximizing rewards, which helps them adapt to dynamic environments. However, ensuring their generalization remains challenging, especially with static training environments. Additionally, DRL models lack transparency, making it difficult to guarantee safety in all scenarios, particularly those not seen during training. To tackle these issues, we propose a method that combines DRL with Curriculum Learning for autonomous driving. Our approach uses a Proximal Policy Optimization (PPO) agent and a Variational Autoencoder (VAE) to learn safe driving in the CARLA simulator. The agent is trained using two-fold curriculum learning, progressively increasing environment difficulty and incorporating a collision penalty in the reward function to promote safety. This method improves the agent's adaptability and reliability in complex environments, and understand the nuances of balancing multiple reward components from different feedback signals in a single scalar reward function.", "sections": [{"title": "INTRODUCTION", "content": "The quest for safer, more efficient, and accessible transportation has made autonomous vehicles (AVs) a key technological innovation. Rule-based systems showed some promise (Moravec, 1990) but lacked the adaptability and robustness to tackle real-world scenarios. In the late 1900s, neural network-based supervised machine learning algorithms were trained on labeled datasets to predict steering angles, braking, and other control actions based on sensor data (e.g., ALVINN (Pomerleau, 1988)). In the early 2000s SLAM techniques such as LiDAR and Radar improved understanding of vehicle position and surroundings, improving vehicle navigation accuracy (Thrun et al., 2005), even in dynamic lighting and weather conditions - (Li and Ibanez-Guzman, 2020). The DARPA Grand Challenge in 2005 (Wikipedia contributors, 2024) which saw Stanley emerge as the winner, marked significant milestones in autonomous driving. Stanley utilized machine learning techniques to navigate the unstructured environment, thus recognizing machine learning and artificial intelligence as essential components of autonomous driving technology. (Grigorescu et al., 2019)\nEventually, with the advancement in camera technology, vision-based navigation became an area of research, with algorithms processing visual data to identify lanes, obstacles, and traffic signals. Although this approach worked very well in controlled environments, it faced challenges in dynamic scenarios and varying light conditions (Dickmanns and Zapp, 1987). Combining the advancements in Deep Learning and Computer Vision, CNNs were used to extract feature maps from visual data from the camera mounted on the vehicle. This led to breakthroughs in object detection, lane understanding, and road sign recognition (e.g., ImageNet) (Krizhevsky et al., 2012). Despite progress, fully automating decision-making and vehicle control in dynamic environments remains challenging, requiring significant manual feature engineering in ML and DL methods. Here is where reinforcement learning comes into play, showing great potential for decision-making and controlling tasks. Combining DL with RL, where agents learn through trial and error in simulated environments, has significantly improved AV decision-making - (Mnih et al., 2013). Unlike rule-based systems, DRL agents can learn to navigate through diverse scenarios by trial and error behavior. This kind of learning also allows the agents to master intricate maneuvers and handle unexpected situations. (Mnih et al., 2013) used DQN to showcase driving policy learning from raw pixel inputs in austere video game environments. However, a simple DQN (Mnih et al., 2013) will not work well in real-life applications, such as driving, as the action space is not discrete but continuous.\nDeep Deterministic Policy Gradient (DDPG) is an on-policy actor-critic algorithm specifically designed to handle continuous action spaces by directly parameterizing the action-value function (Lillicrap et al., 2019). The authors of (Kendall et al., 2018) used this algorithm to drive a full-sized autonomous vehicle. The system was first trained in simulation before being introduced in real-time using onboard computers. It used an actor-critic model to output the steering angle and speed control with the help of an input image. They suggested having a less sparse reward function and using Variational Auto Encoders (VAEs) (Kingma and Welling, 2022) for better state representation. DDPG also requires many interactions with the environment, making the training slow, especially in a high-dimensional and complex environment. DDPG training can also be unstable, especially in sparse reward or non-stationary environments. This instability can manifest as oscillations or divergence during training (Mnih et al., 2016). Proximal Policy Algorithm (PPO) (Schulman et al., 2017b) was developed to address these issues.\nIn our work, we use PPO (Schulman et al., 2017b) and Curriculum Learning (Bengio et al., 2009) for the self-driving task. To obtain a better representation of the state, we have used Variational Auto Encoders (VAE) (Kingma and Welling, 2022) to encode the current scene from CARLA (Dosovitskiy et al., 2017), the urban driving simulator (Town 7). Our paper builds upon the foundational work about accelerated training of DRL-based autonomous driving agents presented in (Vergara, 2019). Salient features of our work:\n\u2022 Introduction of Curriculum learning in the training process allows the agent to learn the easier tasks like moving forward initially and as difficulty increases to make it learn more difficult tasks, like maneuvering in traffic or avoiding high-speed collisions.\n\u2022 We have introduced a refined reward function that gives a higher reward to the agent to travel at higher speeds. This is important to increase the average speed, reducing travel time.\n\u2022 Unlike our base paper, our reward function takes into account the collision penalty as well as other rewards like angle, centering, and speed reward. This is crucial to make the reward function less sparse and aid a smoother driving experience.\n\u2022 The combination of the curriculum learning approach, involving increasing traffic density and augmenting the reward function, as well as the modified reward function, helps in the faster training process and better average speed of the agent.\nWe name this method CuRLA - Curriculum Learning Based Reinforcement Learning for Autonomous Driving, as curriculum learning is integral to the features in our work. These features and the improvements they bring about will be discussed in the paper in further detail."}, {"title": "PRELIMINARIES", "content": "In this section, we provide the foundational tools, concepts, and definitions necessary to understand the subsequent content of this paper. We begin by introducing the environment used for training, followed by a brief introduction to Policy Gradient RL algorithms. Next, we discuss the concept of curriculum learning, and finally, we detail the encoder used in our approach."}, {"title": "CARLA Driving Simulator", "content": "The rise of autonomous driving systems in recent years owes much to the emergence of sophisticated simulation environments. CARLA (Dosovitskiy et al., 2017) is a critical resource for researchers and developers in autonomous driving, providing an open-source, high-fidelity simulator. Its capabilities include realistic vehicle dynamics, sensor emulation (such as LiDAR, radar, and cameras), and dynamic weather and lighting conditions. Moreover, CARLA's scalability enables us to simulate large-scale scenarios involving multiple vehicles and pedestrians interacting in complex urban environments. Its standardized metrics and scenarios facilitate fair comparisons between different self-driving approaches in the field. We particularly chose the CARLA simulator as it also provides a collision intensity whenever a vehicle collides with another object in the environment, and we use this in the reward function design."}, {"title": "Policy Gradient Methods", "content": "Policy gradient methods (Sutton et al., 1999) are pivotal in reinforcement learning for continuous action spaces, directly parameterizing policies, enabling the learning of complex behaviors. Unlike value-based approaches that estimate state/action values, these methods learn a probabilistic mapping from states to actions, enhancing adaptability in stochastic environments. These methods optimize policy parameters \"0\" in continuous action spaces through on-policy gradient ascent on the performance objective J(\u03c0\u04e9).\nTrust Region Policy Optimization (TRPO) (Schulman et al., 2017a) is a type of policy gradient method that stabilizes policy updates by imposing a KL divergence constraint (Kullback and Leibler, 1951), preventing large updates. However, TRPO's complex implementation and incompatibility with models sharing parameters or containing noise are drawbacks.\nProximal Policy Optimization (PPO) (Schulman et al., 2017b) is an on-policy algorithm suited for complex environments with continuous action and state spaces. It builds upon TRPO (Schulman et al., 2017a) by using a clipped objective function for gradient descent, simplifying implementation with first-order optimization while maintaining data efficiency and reliable performance. In later sections, we will see how this has been implemented in our work."}, {"title": "Curriculum Learning", "content": "Curriculum Learning (Bengio et al., 2009) is a strategy aimed at enhancing the efficiency of an agent's learning process by optimizing the sequence in which it gains experience. By strategically organizing the learning trajectory, either performance or training speed on a predefined set of ultimate tasks can be improved. By quickly acquiring knowledge in simpler tasks, the agent can leverage this understanding to reduce the need for extensive exploration to tackle more complex tasks. (Narvekar et al., 2020)"}, {"title": "Variational Autoencoder", "content": "Autoencoders (Rumelhart et al., 1986) are essentially generative neural networks that comprise an encoder followed by a decoder, whose objective is to transform input to output with the least possible distortions (Baldi, 2011).\nVAEs (Kingma and Welling, 2022) excel in reinforcement learning by producing varied and structured latent representations, enhancing exploration strategies, and adapting to novel states. Moreover, their probabilistic framework enhances resilience to uncertainty, which is crucial for adept decision-making in dynamic settings."}, {"title": "EXPERIMENTAL STUDY AND RESULT ANALYSIS", "content": "In this section, we present the experimental setup, methodology, and results of our study. We start by describing the experimental environment, followed by a detailed explanation of the evaluation metrics and the baseline methods for comparison. We then present the results of our experiments and finally discuss the implications of our findings and compare our results"}, {"title": "Proposed Model and Experimental Study", "content": "Our model is named as CuRLA and the model used in the base paper is named as Self-Centering Agent (SCA) for further reference. We also perform experiments using only curriculum learning for the optimized reward function to compare it's performance to the two-fold curriculum learning method that is implemented in CuRLA. This model is named One-Fold CL. We use Town 7 from the list of Towns in CARLA Lap Environment (Dosovitskiy et al., 2017) as shown in figure (2). The reason for using this specific town is that it provides a highway environment without any exits, thus making our job of end-to-end driving easier. In our experiments, curriculum learning is employed by gradually increasing traffic volume and introducing functionalities to the agent in it's reward function in stages, allowing it to quickly grasp the basics of the environment and efficiently tackle more complex tasks.\nThe VAE (Kingma and Welling, 2022) we have used serves as feature extractors by compressing high-dimensional observations into a lower-dimensional latent space. This aids the learning process by providing a more manageable representation for the agent. Variational Autoencoder (VAE) is chosen as it is used for learning probabilistic representations of the input in the latent space, unlike regular autoencoders (Rumelhart et al., 1986), which learn deterministic representations. We use the VAE architecture from (Vergara, 2019) to encode the state of the environment, utilizing the pre-trained VAE from the same study to replicate the encodings. The dataset used to train the VAE contains 9000 training and 1000 validation examples (total of 10,000 160 \u00d7 80 RGB images), all collected manually from the driving simulator environment.\nWe have made use of the state-of-the-art PPO-Clip (Schulman et al., 2017b) algorithm to optimize our agent's policy and to arrive at an optimal policy that maximizes the return. PPO-Clip clips the policy to ensure that the new policy does not diverge far away from the old policy. The policy update equation of PPO-Clip is given as:\n$\n0k+1 = arg max Es,a~\u04e9k [L(s,a, \u0472k, 0)]\n$\nHere, L is given as:\n$\nL(s, a, 0k, 0) =\nmin\n(\n\u03c0\u03b8(as)\n\u03c0\u03bf (as)\n\u0391\u03c0\u03b8\u03ba, clip\n\u03c0\u03b8(as), 1+8,1-8) \u0391\u03c0\u03bf\u03ba\n\u03c0\u03bf\u03b9 (as)'\n)\n$\nHere, 0 refers to the policy parameters being updated, and Ok is the policy parameters currently being used in the kth iteration to get the next iterations parameters, 0k+1. Also, & is the clipping hyperparameter defining how much the new policy can diverge from the old one. The min in the objective computes the minimum of the un-clipped and clipped objectives.\nThe overall graphical representation of the PPO+VAE training process we have used in our paper is shown in the figure (3). In the figure (3), the external variables are acceleration, steering angle, and speed from top to bottom.\nThe models are trained with the same parameters mentioned in (Vergara, 2019). Table 2 shows the complete list of hyperparameters used in the experiments. All models are trained for 3500 episodes, and the evaluation is done once every 10 episodes. Curriculum learning is implemented in both CuRLA and One-Fold CL. After 1500 episodes, traffic and a collision penalty are introduced in CuRLA. In One-Fold CL, only the collision penalty is introduced after 1500 episodes, and traffic is present from the first episode itself. An episode ends when the agent finishes three laps, drifts off the center of the lane by more than 3 metres, or has a speed of less than 1 km/hr for 5 seconds or more. A collision will not lead to the end of"}, {"title": "Improvements Made", "content": "Keeping the architecture of the VAE and the RL algorithm the same as the original paper, we chose to change the original reward function. The original reward function (3) has 3 components: the angle reward, the centering reward, and the speed reward. Our revised reward function (4) has 4 components, the three original components, and we have also added a collision penalty, to encourage the agent against unsafe driving behaviour.\n$\nr = r_ar_d.r_v\n$\n$\nr = r_a + r_d + r_v + r_c\n$\nAngle Reward: The angle reward component ensures that the agent is aligned with the road. Angle a, determines the angle difference between the forward vector of the vehicle (the direction the vehicle is heading in) and the forward vector of the vehicle waypoint (the direction the waypoint is aligned with). Using a, the angle reward ra is defined by\n$\nra = max \\left(1 - \\frac{|a|}{amax}, 0\\right)\n$\nwhere amax = 20\u00b0 (\u03c0/9 radians).\nThis ensures that the angle reward ra \u2208 [0, 1], and that the angle reward linearly decreases from 1 (perfect alignment) to 0 (when the deviation is 20\u00b0 or more).\nCentering Reward: The centering reward factor ensures that the driving agent stays to the center of the lane. The usage of a high-fidelity simulator like CARLA enables us to have a precise measurement of distance between objects in the environment. To reward the agent for staying within the center of the lane, we use the distance d between the center of the car and the center of the lane to define the centering reward by\n$\nra=(1-\\frac{d}{dmax})\n$\nwhere dmax = 3.\nThis ensures that the centering reward component rd \u2208 [0,1] (as the episode terminates when the agent goes off center by more than 3 metres) and that ra is inversely proportional to d, encouraging more centering for the agent while driving.\nSpeed Reward: While keeping the angle and centering reward the same for all three agents, we change the speed reward component. The minimum speed vmin, maximum speed vmax, and target speed Vtarget are taken as 15 Km/hr, 105 Km/hr and 60 Km/hr respectively, and are kept as the same for both the original and updated speed reward components. The original speed reward component r, and the new speed reward component ry are then defined by\n$\nrv =\n\\begin{cases}\n0.5 , & V< Vmin, \\\\\n\\frac{V-Vmin}{Vtarget-Vmin}, & Vmin v\u2264 Vtarget, \\\\\n1, & Vtarget <V \u2264 Vmax\n\\end{cases}\n$\n$\nrv =\\begin{cases}\n0.5 , & V< Vmin, \\\\\n\\frac{Vtarget-V}{Vtarget-Vmin}, & Vmin \u2264 v \u2264 Vtarget, \\\\\n\\frac{Vmax-V}{Vmax-Vtarget}, & Vtarget <V \u2264 Vmax\n\\end{cases}\n$\nwhere v is the current speed of the agent.\nAs seen in the graphs (Fig.4 & Fig.5), in the original speed reward function (Fig.4), the graph is constant in the range [Vmin, Vtarget] and receives the highest reward of 1. This is misleading as the agent can get confused, deciphering the minimum speed as the target speed itself, as it is getting a constant reward of 1 at either speed. This ensures that the agent will drive slower, as ensuring a higher centering and angle reward at a lower speed is much easier than at a higher speed. To rectify this, we have replaced the constant graph in [Vmin, Vtarget] with an increasing function (see Fig.5) to prioritize getting as close to the target speed as possible without losing too much performance on the angle and centering reward components. Both CuRLA and One-Fold CL use this revised reward function, whereas SCA uses the original reward function."}, {"title": "Collision Penalty", "content": "A collision penalty factor was introduced for both One-Fold CL and CuRLA to ensure the agent explicitly learns the behaviour of safe driving, avoiding collisions with other objects and vehicles in the environment. The advantage of using a simulator like CARLA also enables us to get a collision intensity (Ic) value between the agent and other objects in the environment, which we use to devise the collision penalty. The collision penalty re is defined by\n$\nrc = max(-1,-log10(max(1,Ic)))\n$\nThis penalty ensures rc \u2208 [-1,0], thus ensuring r\u2208 [-1,1]."}, {"title": "Curriculum Learning", "content": "In CuRLA, curriculum learning is implemented in a two-fold manner. Firstly, we gradually increase the traffic volume of our simulation environment as the number of episodes increases. The agent gets to focus on learning to drive in a traffic-free environment initially and then slowly navigate through traffic in the later epochs. Secondly, the functionalities of the agent are gradually added. As the agent learns to drive in a traffic-free environment, we introduce a collision penalty while increasing the volume of traffic to teach it to avoid collisions with other vehicles. The reasoning behind adding this collision penalty is that while the current reward function accounts for smooth driving, it does not punish the agent enough for colliding with the other vehicles. All agents have been trained with traffic included in the roads, making a collision penalty extremely important. This method of training helped the agent to learn the basics of the environment quickly and enabled it to learn harder tasks efficiently while also keeping safety as a factor."}, {"title": "Metrics and Result Analysis", "content": "The models are compared on two metrics - distance travelled and average speed. These metrics are chosen as they capture an accurate representation of the trade-off between efficiency and performance in autonomous driving scenarios. The distance traveled metric emphasizes the model's capability to maximize the length of the path it traverses, reflecting its ability to sustain long journeys without interruption. Conversely, the average speed metric accounts for both the distance covered and the time taken to complete the journey, offering a thorough assessment of the model's performance in terms of both speed and efficiency. Evaluating these metrics provides a detailed understanding of how each model balances speed and distance, which are essential factors in assessing the overall effectiveness of autonomous driving systems. The evaluation of our models against the base model provided insightful findings. (All graphs have been considered with a smoothing factor of 0.999)\nIn our assessment, for the metric of Distance Traveled, we measure it by the number of laps traveled by the vehicle, where 100% is considered as one lap finished. As it can be seen in Fig.6 & Fig.8, CuRLA and SCA have similar performance, completing approximately 1.5 laps on average in training and 0.3 laps in evaluation. Meanwhile, One-Fold CL did worse than both CuRLA and SCA, completing approximately 1.25 laps in training and 0.25 laps in evaluation. CuRLA does perform slightly better in training compared to SCA, which performs slightly better in evaluation. However, from the graph it can be seen that CuRLA had a consistent learning curve while training compared to SCA, which dropped in performance and then picked back up. One-Fold CL also performed comparably to both models, albeit slightly worse. This attests to the improvement in performance that two-fold curriculum learning shows over simple curriculum learning during training and training without curriculum learning.\nThe assessment of the metric of Average Speed highlighted the strengths of our assumption in changing the underlying speed reward component. During training, as can be seen in Fig.7, both CuRLA and One-Fold CL significantly outperform SCA reaching average speeds of 22 Km/hr and 20 Km/hr in training, compared to SCA's 14 Km/hr. The difference is not as much during evaluation (Fig.9), but CurLA and One-Fold CL still outperform SCA here as well, with CuRLA reaching an average speed of 6 Km/hr and One-Fold CL reaching an average speed of 5 Km/hr, compared to SCA's 4 Km/hr. This superior performance of CuRLA and One-Fold CL (on the revised reward function) compared to the SCA agent (using the original reward function) underscores our reward function's efficiency in optimizing speed-related aspects."}, {"title": "CONCLUSIONS", "content": "In this paper, we presented a model (CuRLA) that used a PPO+VAE architecture and two-fold curriculum learning along with a reward function tuned to accelerate the training process and achieve higher average speeds in autonomous driving scenarios. We show the performance of two-fold curriculum learning against simple curriculum learning (One-Fold CL agent), as well as the performance of agents on the revised reward function compared to the base reward function. While CuRLA and One-Fold CL perform comparably to the base agent (SCA) in the distance traveled metric (with CuRLA performing slightly better and One-Fold CL being slightly worse), a significant improvement in average speed is observed. This prioritization of speed was a deliberate design choice. The distance traveled metric solely optimizes for maximizing the traversed path length. Conversely, the average speed metric inherently optimizes for both distance and the time taken to complete the journey, effectively accounting for two performance factors within a single measure. The performance of CuRLA and One-Fold CL agents, when compared to SCA, also attest to the benefits of using curriculum learning while training, and how decomposing the tasks in the autonomous driving problem helps agents to learn better and faster. Integrating multiple objectives into a single scalar reward function often leads to suboptimal agent performance. However, by employing curriculum learning during training, we can enable agents to master the nuances of each reward component more effectively. This approach facilitates better understanding of the environment and objectives, and ultimately enhances overall performance.\nFuture research will focus on enhancing performance by updating the architecture and algorithms."}]}