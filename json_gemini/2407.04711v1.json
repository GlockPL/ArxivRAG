{"title": "MetaFruit Meets Foundation Models: Leveraging a Comprehensive Multi-Fruit Dataset for Advancing Agricultural Foundation Models", "authors": ["Jiajia Li", "Kyle Lammers", "Xunyuan Yin", "Xiang Ying", "Long He", "Renfu Lu", "Zhaojian Li"], "abstract": "Fruit harvesting poses a significant labor and financial burden for the industry, highlighting the critical need for advancements in robotic harvesting solutions. Machine vision-based fruit detection has been recognized as a crucial component for robust identification of fruits to guide robotic manipulation. Despite considerable progress in leveraging deep learning and machine learning techniques for fruit detection, a common shortfall is the inability to swiftly extend the developed models across different orchards and/or various fruit species. Additionally, the limited availability of pertinent data further compounds these challenges. In this work, we introduce MetaFruit, the largest publicly available multi-class fruit dataset, comprising 4,248 images and 248,015 manually labeled instances across diverse U.S. orchards. Furthermore, this study proposes an innovative open-set fruit detection system leveraging advanced Vision Foundation Models (VFMs) for fruit detection that can adeptly identify a wide array of fruit types under varying orchard conditions. This system not only demonstrates remarkable adaptability in learning from minimal data through few-shot learning but also shows the ability to interpret human instructions for subtle detection tasks. The performance of the developed foundation model is comprehensively evaluated using several metrics, which outperforms the existing state-of-the-art algorithms in both our MetaFruit dataset and other open-sourced fruit datasets, thereby setting a new benchmark in the field of agricultural technology and robotic harvesting. The MetaFruit dataset (https://www.kaggle.com/datasets/jiajiali/metafruit) and detection framework (https://github.com/JiajiaLi04/FMFruit) are open-sourced to foster future research in vision-based fruit harvesting, marking a significant stride toward addressing the urgent needs of the agricultural sector.", "sections": [{"title": "1. Introduction", "content": "Farm work is inherently labor-intensive and represents a significant burden. According to a report by the Economic Research Service of the U.S. Department of Agriculture\u00b9, farm labor in the U.S. relies heavily on immigrants, par- ticularly those of Hispanic or Mexican origin. The H-2A Temporary Agricultural Program serves as a vital resource for crop farmers to address seasonal labor demands. Over the past 17 years, the number of H-2A positions requested and approved has increased more than sevenfold. Additionally, for all farms, labor costs averaged 10.4% of gross cash income during 2018-2020, with the figure reaching about 30% for fruits and tree nuts. The mechanical harvesting system for fruits is an efficient and profitable approach but has the problem of excessive mechanical damage on fruits. Therefore, there is a critical need for the innovation of robotic harvesting technologies to mitigate labor shortages, minimize human injury risks, and boost the efficiency and economic viability for the fruit industry. The perception system is essential in harvesting robots, as it enables the identification of fruits within the target area and guides the robot in executing subsequent tasks. Recent significant advancements in the affordability of cameras and computer vision (CV) technology have made image-based fruit detec- tion systems increasingly popular in robotic fruit harvesting. For instance, in , the minimum Euclidean distance-based segmentation method is proposed for segmenting the fruit region from the input image to identify and count the number of fruits on trees. In conventional Machine Learning (ML)/CV techniques such as interest point feature extraction, support vector machines, and interest region extraction are developed for detecting green fruits on plants based on texture analysis. However, these methods depend heavily on manually designed features and can be negatively impacted by variations in lighting conditions and occlusions. More recently, deep learning (DL) based approaches have rapidly evolved and attracted significant attention in various agricultural sectors, such as plant disease identifi- cation, weed detection, plant counting, and plant breeding. These DL methods have also been proven effective in fruit de- tection. For instance, Faster-RCNN has been successfully applied for apple, kiwifruit, and multiple fruits detection (mangoes, almonds and apples). In addition, YOLO models are also applied for fruit detection and recognition such as apple, mango, orange, and cherry. In our previous research, state-of-the-art DL techniques based on Mask-RCNN and Faster RCNN are developed for accurate apple detection for dense orchard settings. Despite the aforementioned successes, developing DL models from scratch faces several challenges. Firstly, it relies heavily on large, accurately annotated image datasets, which are generally costly to obtain. Secondly, the training phase is remarkably time-intensive and demands significant computational resources. Moreover, while these specialized models excel in their designated tasks, they often encounter difficulties when applied to novel scenarios, such as different orchard condi- tions or fruit species, demonstrating limited capabilities in generalization. It is widely acknowledged that a comprehensive set of annotated images is essential for the development of high- performing DL models in visual fruit detection tasks. In , the authors have pro- vide an overview of various publicly accessible fruit image datasets aimed at robotic harvesting. For instance, mango- related datasets, such as MangoNet and Mango YOLO contain 49 and 1730 images for mango segmentation and detection, respectively. There are specialized apple datasets for apple detection, including KFuji RGB-DS WSUApple , LFuji-air dataset , and MinneApple , along with two apple datasets from our previous studies. Additionally, DeepBlueberry is a dataset including 294 images for blueberry detection. However, most of these datasets are species-specific and not transferable to different fruit types. Recently, there has been an increasing interest in multi-fruit datasets. For instance, FruitNet and Fruit360 feature 19,500 and 41,322 images across 5 and 80 fruit species, respectively, catering to fruit classification tasks. In terms of fruit detection, OrchardFruit and DeepFruits provide open-source access to 3,232 and 587 images for 3 and 7 fruit species, respectively. Yet, these datasets are typically designed for specific orchard environments with less dense fruit clusters. Table 1 summarizes these datasets, providing an overview of the resources available for advancing research in fruit detection. Lately, the rise of large pre-trained models, commonly known as foundation models (FMs), such as ChatGPT-4 , Segment Anything Model (SAM)"}, {"title": "2. Materials and Methods", "content": "In this section, we first present our collected dataset, MetaFruit, and the VFMs used for multi-class fruit de- tection. We then detail the few-shot learning, evaluation metrics, and experimental setups employed in our study."}, {"title": "2.1. MetaFruit dataset", "content": "The multi-class fruit dataset, MetaFruit, introduced in this study is collected utilizing advanced imaging technol- ogy, comprising both a high-definition camera and a sophis- ticated LiDAR system (with a resolution of 1920 \u00d7 1080), from commercial orchards in North Michigan and Califor- nia, USA. To guarantee a diverse and varied collection of images that enhances model robustness , the dataset includes images taken under natural field lighting conditions across various weather conditions (e.g., sunny, cloudy, and overcast) during the peak harvest season of the fruit growth stage. The dataset contains 4,247 images, featuring five distinct fruit types: apples, oranges, lemons, grapefruits, and tangerines. Figure 1 shows representative samples for each fruit category. Unlike existing datasets, MetaFruit is characterized by more realistic/complex or- chard environments with fruits frequently appearing in clus- ters, presenting a challenging yet realistic scenario for model training and evaluation. Notably, the dataset also includes multiple varieties within each fruit category. For example, the apple class includes both red and green species, adding another layer of diversity and complexity to the dataset. The images acquired for the MetaFruit dataset are metic- ulously labeled by trained personnel. These annotators uti- lized the Labelme tool to accurately draw bounding boxes around individual fruit instances in the images. This meticulous process results in the acquisition of 248,015 manually labeled bounding boxes. The distribution of the MetaFruit dataset is detailed in Table 2. Overall, the dataset exhibits an even distribution among apples, oranges, lemons, and tangerines, each with a similar number of images, whereas grapefruits are represented with slightly fewer images, totaling 490. Tangerines are particularly well- represented in the dataset with 1,063 images and 85,785 labeled instances, averaging 81 bounding boxes per image. The average number of bounding boxes per image sheds light on the density of fruits captured in the images, whereas the average size of these instances provides insight into the physical size of the objects. Notably, the smaller the size of the instances, the greater the challenge in detecting them accurately. Interestingly, while the lemon class does not have the highest average number of bounding boxes per image, it features the smallest average size of instances (823 pixels per instance), indicating lemons' smaller physical presence within the images, which presents its unique detection chal- lenges. The MetaFruit dataset, in terms of instance numbers, significantly surpasses previous collections, being more than 10 times larger than the dataset for multi-class fruit species featured in OrchardFruit (as shown in Table 1). To the best of our knowledge, it represents"}, {"title": "2.2. VFMs for fruit detection", "content": "In recent years, DL approaches have made significant strides in advancing fruit detection models. Prominent among the object detectors employed are FCOS, Faster-RCNN, and YOLO series , all of which are designed as closed-set detectors. Such models operate under the assumption that the categories of objects to be detected are predefined and known during both the training and testing phases, thereby limiting their capacity to recognize previ- ously unseen categories. Furthermore, these approaches de- pend on extensive, meticulously labeled image datasets-a process that is both labor-intensive and demands significant resources. In contrast, recent focus has shifted towards open- set object detection and the exploration of LLMs and FMs . These open-set detectors are capable of not only precisely detecting the known classes but also efficiently handling the unknown ones. Therefore, the language data needs to be added for model training to solve the situation that a testing sample comes from some unknown classes. Similarly, LLMs and FMs, which are trained on extensive datasets covering a wide range of domains and modalities, demonstrate a remarkable ability to perform a variety of open-set tasks after training, which is achieved with minimal fine-tuning and reduced reliance on extensive, task-specific labeled data. To facilitate open-set fruit detection across a diverse array of fruit categories, this study employs a vision foun- dation model (VFM), specifically the Grounding DINO"}, {"title": "2.3. Few-shot learning", "content": "Contemporary fruit detection algorithms, while yielding promising results, often struggle to generalize across varying data distributions, such as different fruit classes and orchard settings, especially when faced with a lack of extensive data. The scarcity of data can be attributed not only to the inherent challenges of the task or privacy issues but also to the significant costs associated with data prepa- ration, including collection, preprocessing, and labeling. In response to these challenges, few-shot learning has gained recognition as a promising learning method, demonstrating the significant potential for quickly learning underlying pat- terns from merely a few or even zero samples. Zero-shot transfer learning refers to scenarios where no training samples are utilized, and models are directly deployed on testing images, aiming to make accurate predic- tions based solely on their pre-existing knowledge and capa- bilities. On the other hand, few-shot learning involves using a minimal number of samples to refine and adjust the models. For example, in 5-shot learning, precisely five samples are employed for model fine-tuning. It is important to note that while few-shot learning allows models to adapt to new tasks with limited data, the performance of such models, when only a few samples are used for fine-tuning, can sometimes be constrained. The effectiveness of the fine-tuning process is heavily dependent on the quality and representativeness of the selected samples, their alignment with the task at hand, and the model's inherent ability to generalize from minimal information. This delicate balance between sample selection and model adaptability is critical for maximizing the potential of few- shot learning approaches in diverse application scenarios, including those within the domain of fruit detection where variability across classes and environments is high. In this study, we employ few-shot learning frameworks to evaluate the generalizability of the FMFruit model across various fruit categories. Specifically, the zero-shot learning scenario is utilized by deploying the FMFruit model on new fruit classes without any model fine-tuning. Concurrently, for the few-shot learning experiments, a minimal number of samples are randomly selected from these new fruit categories to slightly adjust the model."}, {"title": "2.4. Evaluation metrics", "content": "The performance of DL models in fruit detection tasks is rigorously evaluated using key detection accuracy met- rics, such as Average Precision (AP), mean Average Recall (mAR), and mean Average Precision (mAP). These metrics collectively offer a detailed assessment of a model's proficiency in both identifying and precisely locating fruits within images. AP, with a specific focus on precision at a 50% overlap threshold (AP50), and mAP, which calculates the average precision across a range of overlap thresholds (from 0.5 to 0.95, in increments of 0.05), together provide insights into the precision aspects of model performance. Meanwhile, mAR evaluates the model's recall capabilities over a spectrum of Intersection over Union (IoU) ranging from 0.5 to 0.95, thereby gauging the model's ef- fectiveness in capturing the true positive detections across various conditions."}, {"title": "2.5. Experimental setups", "content": "Extensive experiments are conducted based on the fol- lowing four settings:\n\u2022 Zero-shot transfer, few-shot learning, and fine-tuning on our MetaFruits.\n\u2022 Cross-class generalization ability evaluation by fine- tuning with four kinds of fruits and evaluating on the remaining novel one.\n\u2022 Zero-shot evaluation, few-shot learning, and fine- tuning on some of the existing fruit data.\n\u2022 Case study of language-referring object detection.\nWe have two model variants, FMFruit-T with Swin-T , and FMFruit-L with Swin-L as the image backbone, respectively. Following BERT-base , Hugging Face is used as the text backbone. All the models are trained for 100 epochs with the AdamW optimizer. The learning rate is set to be 1e-4 with the weight decay as 0.0001, but the learning rate for the image and text backbone is set to be 1e-5. To expedite the model training process, we leverage transfer learning based on pre-trained DINO and pre-trained Grounding DINO. The fine-tuning pro- cedure involves using a batch size of 4 over 100 epochs, and we utilize the PyTorch framework (version 1.10.1) . The MetaFruit dataset is divided into training and test sets, with a distribution ratio of 60% for training and 40% for testing. Both the training and testing phases of the models take place on a server running Ubuntu 20.04. This server is equipped with two GeForce RTX 2080Ti GPUs, each offering 12GB of GDDR6X memory."}, {"title": "3. Results", "content": "In this section, we first evaluate the zero-shot and few- shot transfer learning performance of FMFruit in compari- son with leading fruit detection algorithms on our MetaFruit dataset. Then, we examine its ability of cross-class gen- eralization and evaluate its effectiveness on other publicly available fruit datasets. Lastly, we present initial findings on its capability to integrate text inputs and comprehend referring expressions."}, {"title": "3.1. Few-shot fruit detection performance", "content": "In this subsection, we examine the zero-shot and few- shot transfer learning capabilities of our proposed model across five distinct fruit types from our MetaFruits data. We compare our model's performance with that of leading object detection models, including Fully Convolutional One- Stage (FCOS) object detector , Faster- RCNN, RetinaNet , and RTMDet . The performance comparison is presented in Table 3. It is noteworthy that traditional CNN-based models such as FCOS, despite being trained on the comprehensive COCO dataset , which encompasses 80 categories including apples and oranges, fail to achieve any positive mAP and mAR scores in fruit detection tasks across all fruit classes. This highlights a crit- ical limitation of conventional object detection algorithms, which struggle with generalization across diverse datasets and are typically fine-tuned for narrow, specific detection scenarios. Among the baseline models, RTMDet emerges as one of the best-performing models following comprehensive training across all evaluated fruit types in terms of mAP and mAR metrics, while RetinaNet is observed to lag behind the rest of the baseline models in performance. Conversely, our foundation model-based fruit detection model, FMFruit, demonstrates exceptional zero-shot trans- fer performance across all evaluated fruit classes. Notably, for FMFruit-T, two out of the five fruit classes achieve a mAP score exceeding 36.0, alongside an AP50 score and a mAR surpassing 64.0 and 52.0 across all types of fruits, respectively. The zero-shot experimental results underline its impressive capability to accurately detect and identify a wide range of fruits without specific prior training in those classes. FMFruit-T's performance on apples shows a specific chal- lenge, achieving a zero-shot 24.1 mAP score. This perfor- mance can be attributed to the presence of densely clustered fruits, with an average of 76 apples per image, as detailed in Table 2. Similarly, the model's detection capability for lemons, which achieves a 29.4 mAP score, highlights the difficulty in accurately identifying fruits that occupy small areas within images, with the average size being only 823 pixels per lemon, as also indicated in Table 2."}, {"title": "3.2. Performance of cross-class generalization", "content": "In this subsection, we evaluate the cross-class general- ization capability of FMFruit to assess the impact of training on existing fruit classes on the detection performance of an unseen fruit class. Specifically, in this evaluation, the model is first trained on four fruit classes and subsequently tested on the fifth, unseen class. For instance, to test the model's generalization capability to detect lemons with cross-variety training data of other fruits, the model is first fine-tuned using data from oranges, apples, grapefruits, and tangerines, and then tested for its ability to detect lemons, a class not seen during training. This assessment helps us understand FMFruit's adaptability and effectiveness in recognizing new fruit types based on learned features from other fruit classes. summarizes the performance of FMFruit across three distinct training settings: zero-shot, where the model receives no training on any of the five fruit classes; cross- class, where the model is trained on four fruit classes and evaluated on the fifth, unseen class; and fine-tuning, where the model undergoes training on the specific fruit classes. The data results clearly demonstrate the efficacy of cross-class training in enhancing fruit detection capabil- ities. Specifically, cross-class training significantly boosts detection performance by 98.9, with an AP50 improvement from 45.7 to 90.9, nearly matching the performance in the fine-tuning setting, which achieves an AP50 of 92.7. This"}, {"title": "3.3. Performance on other fruit datasets", "content": "In this subsection, we evaluate the performance of our model, FMFruit-T, across several established fruit datasets, specifically targeting the DeepBlueberry, StrawDI_Db1 , and MinneApple. These datasets contain 125, 300, and 331 test images, respectively, with corresponding training sets comprising 184, 2800, and 670 images. This evaluation allows us to rigorously assess FMFruit-T's effec- tiveness across diverse fruit classes and scenarios. illustrates FMFruit-T's exemplary performance across all tested datasets. Remarkably, after pretraining on the MetaFruit dataset, FMFruit-T achieves mAP rates of 30.8, 47.6, and 20.7 on the DeepBlueberry, StrawDI_Db1, and MinniApple datasets, respectively. Despite the absence of strawberry and blueberry classes in the MetaFruit dataset, FMFruit-T demonstrates a notable ability to generalize and adapt to these fruit types, even though it was not directly trained on them. The Minneapple dataset's inclusion of com- plex orchard environments similar to those in the MetaFruit dataset, along with numerous apples fallen on the ground , presents unique challenges, re- sulting in the lower initial detection accuracy in zero-shot settings. However, with an increased number of training samples, FMFruit-T adeptly distinguishes and focuses on apples located on trees, effectively ignoring those on the ground. Upon fine-tuning setting, FMFruit-T impressively attains AP scores of 69.4, 87.7, and 51.8 for the DeepBlue- berry, StrawDI_Db1, and MinniApple datasets, respectively, demonstrating its robustness and adaptability across diverse fruit detection scenarios."}, {"title": "3.4. Performance of referring expression comprehension (REC)", "content": "In this subsection, we present an initial evaluation of our FMFruit model's ability in terms of REC. The model is tasked with processing human instructions provided in natural language, identifying the critical elements of these instructions, and selecting features that accurately corre- spond to the described text. shows the REC results. The first illustrative set involves the model detecting apples with minimal oc- clusion, guided by the specific instruction \u201capple with less occlusion\u201d. FMFruit demonstrates proficiency in accurately isolating and excluding apples that are heavily occluded by leaves, adhering closely to the given instructions. The second example demonstrates the model's ability to filter out apples occluded by branches, following the instruction \u201cap- ple without occlusion by branch\u201d. Unsurprisingly, FMFruit exhibits exceptional adaptability by focusing detection on apples without branch occlusion. These scenarios highlight FMFruit's precise interpretation and execution based on spe- cific linguistic instructions, underscoring its sophisticated ability to utilize referring expressions for enhanced fruit detection accuracy."}, {"title": "4. Discussion", "content": "Fruit detection is a widely studied research topic but is still a practical challenge. Traditional DL methods have shown considerable success, yet they tend to be specialized for certain fruit types and specific scenarios, limiting their applicability to new orchard environments and different fruit classes. In response to this limitation, our study delves into the potential of VFMs to tackle a wider range of fruit de- tection challenges. Additionally, we introduce the MetaFruit dataset, encompassing 248,015 labeled instances across five fruit classes, to support and enhance the development and evaluation of advanced fruit detection models. Despite its contributions, this study acknowledges certain limitations, as elaborated below."}, {"title": "4.1. Challenges in real-world implementation", "content": "Implementing FMs in agricultural applications comes with some challenges, particularly regarding inference speed and model size which often require significant computing resources. As shown in Table 7, our proposed FMFruit models have the largest inference time, which could limit the deployment of FMs in many on-field agricultural settings, as the downstream tasks often require immediate action based on the model's outputs. For example, after outputting the fruit location, the fruit-picking system needs to implement other actions immediately, such as decision-making and path planning. In addition, the com- plexity and size of FMs demand large computing resources and memory bandwidth, which may not be practical for real- world deployment. To overcome these challenges, recent research has fo- cused on the model optimization techniques and made great progress. For instance, model com- pression can significantly reduce the model size and speed up the infer- ence without compromising performance. These techniques include quantization, knowledge distillation, and pruning, among others, each contributing to more efficient deploy- ment of FMs in resource-constrained environments like agri- culture. For example, SqueezeLLM proposed a post-training quantization framework to enable loss- less compression and achieve higher quantization perfor- mance under the same memory constraint. Furthermore, adopting edge computing strategies can accelerate the inference process by facilitating data process- ing near its point of origin. In the context of agriculture, edge devices such as drones or field sensors can process data on-site, allowing for immediate decision-making without dependence on remote servers. An illustrative example of this approach is MobileSAM , which is designed by distilling the knowledge from the heavy image encoder (ViT-H in the original SAM ) to a lightweight image encoder, enable the model implementation in mobile devices. Specifically, the MobileSAM achieves an inference time of 12ms and a model parameter of 9.66M, compared with the original SAM with an inference time of 456ms and a model parameter of 615M."}, {"title": "4.2. Integration of LLMs", "content": "The realm of Large Language Models (LLMs) and Foun- dation Models (FMs) has seen remarkable advancements, finding applications in diverse fields including ChatGPT , robotics , and agriculture. The preliminary investiga- tions into the use of LLMs and FMs within agricultural contexts reveal significant promise for their integration into farming technologies, suggesting a fruitful avenue for en- hancing agricultural practices through advanced compu- tational models. In Section 3.4, we explore the efficacy of Referring Expression Comprehension by leveraging hu- man instructions to refine detection outcomes. To seam- lessly integrate language and visual modalities, we employ a language-guided query selection method , which selects features that closely align with the input text, utilizing the principles of the grounded language-image pre-training (GLIP) model. This method promises more precise and contextually relevant detection capabilities. However, it necessitates the preparation of well-organized and labeled (image, text) pairs for training, a pro- cess that is both time-intensive and complex. Looking ahead, the exploration of integrating mature LLM and FM devel- oper Application Programming Interfaces (APIs) presents an exciting avenue. For instance, OpenAI has made their ChatGPT API available to the public, enabling researchers to develop their own applications and tools leveraging these advanced platforms. Human-robot interaction (HRI), a multidisciplinary field that studies how humans and robots interact, presents an- other opportunity for integrating LLMs and FMs into fruit- harvesting robots. With the power of LLMs and FMs, fruit-harvesting robots can be endowed with enhanced comprehension abilities, enabling them to understand and execute complex instructions provided by humans in natural language. This integration not only fa- cilitates smoother and more intuitive communication be- tween humans and robots but also significantly improves the robots' adaptability and decision-making capabilities in dynamic orchard environments."}, {"title": "5. Summary", "content": "Fruit detection is a pivotal component in the develop- ment of robotic fruit harvesting systems. Central to success- ful fruit detection is the assembly of a substantial, accurately labeled fruit dataset and the subsequent development of ro- bust DL models. This paper introduces, to date, the most ex- tensive fruit detection dataset pertinent to U.S. commercial orchards, encompassing 4,248 images across 5 fruit classes, annotated with a total of 248,015 bounding boxes, gathered under diverse natural field lighting conditions and differ- ent geographic locations. Moreover, we have developed an innovative open-set fruit detection system that utilizes the advanced capabilities of VFMs to identify a wide range of fruits. This model shows superior capability of detecting unknown fruits and can achieve fine performance under zero-shot and few-shot learning scenarios. Furthermore, the model demonstrates cross-class generalization capabilities by being trained on known fruit classes and then tested on novel classes, showcasing its exceptional open-set detection ability. Lastly, the model shows superior ability in language referring expression comprehension, thus providing oppor- tunities for human-robot interactions. The fruit detection dataset and source codes for model development and evalua- tion are now publicly accessible to the research community."}]}