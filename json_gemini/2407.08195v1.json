{"title": "A Text-to-Game Engine for UGC-Based Role-Playing Games", "authors": ["Lei Zhang", "Xuezheng Peng", "Shuyi Yang", "Feiyang Wang"], "abstract": "The shift from professionally generated content (PGC) to\nuser-generated content (UGC) has revolutionized various\nmedia formats, from text to video. With the rapid advance-\nments in generative AI, a similar shift is set to transform the\ngame industry, particularly in the realm of role-playing\ngames (RPGs).\nThis paper introduces a new framework for a text-to-game\nengine that utilizes foundation models to convert simple tex-\ntual inputs into complex, interactive RPG experiences. The\nengine dynamically renders the game story in a multi-modal\nformat and adjusts the game character, environment, and me-\nchanics in real-time in response to player actions. Using this\nframework, we developed the 'Zagii' game engine, which has\nsuccessfully supported hundreds of RPG games across a di-\nverse range of genres and facilitated tens of thousands of\nonline user gameplay instances. This validates the effective-\nness of our framework. Our work showcases the potential for\na more open and democratized gaming paradigm, highlight-\ning the transformative impact of generative AI on the game\nlife cycle.", "sections": [{"title": "1. Introduction", "content": "The creation of traditional RPGs, typically undertaken by\nprofessional and large development teams, requires a di-\nverse set of skills. These include screenwriting, character\ndesign, game mechanics, coding, and graphical design,\namong others. This multidisciplinary requirement makes the\ndevelopment process costly, time-consuming, and relatively\ninflexible, as changes in one area often trigger cascading\nchanges across the game. This complexity slows the intro-\nduction of new games to the market, restricts the variety of\navailable narrative and gameplay styles, limits player free-\ndom and control over narrative paths, and affects content ex-\npansion depth. Traditional RPGs, reliant on rigid game en-\ngines, often struggle to dynamically adapt to player choices,\ntypically offering a linear or branching path that doesn't\nevolve based on player interaction, or networked narratives\nthat aren't replicable for broader use.\n illustrates the major life cycle of a game, struc-\ntured into several phases: Concept Planning, Game Design,\nGame Development, and Game Rendering. Developers me-\nticulously handcraft these phases, requiring substantial man-\nual effort and creativity. Asset generation involves profes-\nsional artists and designers creating high-quality characters,\nenvironments, and props. Despite the detailed craftsmanship,\nthe user experience often features an accumulation of artifi-\ncial elements, limited game paths, and finite experiences, re-\nsulting in a predictable and repetitive set of dialogues, end-\nings, and player interactions.\nIn contrast, AI-native RPGs represent a significant para-\ndigm shift in game development. These games utilize gen-\nerative AI to evolve, generate, and maintain game content\nfrom a certain initial condition, thereby eliminating the need\nfor human intervention. This AI-driven approach allows for\nthe synthesis of various game elements, such as storylines,\ncharacters, and worlds, from simple textual inputs provided\nby users. Unlike traditional methods, this approach signifi-\ncantly reduces the need for technical skills, enabling indi-\nvidual creators to produce complex games. Furthermore,\nAI-native RPGs are inherently dynamic, with the unique ca-\npability to dynamically generate and adjust game content in\nreal-time based on player decisions. This adaptability results\nin a more personalized gaming experience, as the game\nworld evolves uniquely for each player, reflecting their ac-\ntions and choices in a narrative that continuously unfolds."}, {"title": "2. Related Works", "content": "There are numerous works at the intersection of LLMs and\ngame development, the role of LLMs in games can be cate-\ngorized into several key areas: Player, Non-Player Character\n(NPC), Game Master (GM), Player Assistant, Commentator\n/Reteller, Game Mechanic, Automated Designer, and De-\nsign Assistant (Gallotta et al. 2024). LLMs can play games\nby converting game states and actions into token sequences,\nhandling both text-based and visual-based game states. They\nhave been applied in board games like Chess (Toshniwal et\nal. 2022), Go (Ciolino, Kalin, and Noever 2020), and\nOthello (Li et al. 2022), as well as in text adventure games\nwhere they generate character response based on environ-\nment descriptions (Yao et al. 2020, Tsai et al. 2023). LLMs\nalso play Atari games by predicting actions from visual in-\nputs, as demonstrated by the GATO agent (Reed et al. 2022).\nIn enhancing NPC dialogue and behavior, LLMs create\nimmersive interactions by adapting responses to game con-\ntexts (Shanahan, McDonell, and Reynolds 2023). They are\nused for both foreground NPCs, which require contextual\ninteractions (Warpefelt and Verhagen 2017, Xu et al. 2023,\nMaas, Wheeler, and Billington 2023), and background\nNPCs, which maintain ambient dialogue (Mehta et al. 2022).\nAs Game Masters in tabletop role-playing games (TTRPGs),\nLLMs generate plots, characters, and narratives. Applica-\ntions like AI Dungeon (Hua and Reley 2020) use LLMs for\ninteractive storytelling. Tools like CALYPSO (Zhu et al.\n2023) assist human GMs with encounter generation, and\nShoelace (Acharya et al. 2023) aids in monitoring and re-\nsponding to in-game conversations.\nAdditionally, LLMs can narrate game events for players\nor spectators, enhancing engagement by summarizing inter-\nactions and providing automated commentary, which helps\nstreamers manage audience interactions effectively (Ranella\nand Eger 2023).\nFocusing on role-playing games (RPGs), the applications\nof LLMs have garnered significant attention in both aca-\ndemic research and industry. Existing studies highlight the\nsynergy between LLMs and RPGs. For instance, Generative\nAgent (Park et al. 2023) introduced computational agents\nthat simulate human behavior and described an architecture\nthat utilizes memories and reflections to dynamic plan agent\nbehaviors. LLMGA (Hu et al. 2024) provides a broad per-\nspective on the architecture and functionality of LLM-based\ngame agents, highlighting their application across various\ngame genres. In study (Shanahan, McDonell, and Reynolds\n2023), they examine the nuances of role-playing, particu-\nlarly the conversational agent's capabilities in deception and\nself-awareness, providing insights into achieving more hu-\nman-like interactions in RPGs. Character-LLM (Shao et al.\n2023) introduces a novel approach to enhancing role-play-\ning scenarios through fine-tuning on role-play datasets, em-\nphasizing the importance of character consistency and im-\nprovisation. Role LLM (Wang et al. 2023) presents a sys-\ntematic evaluation of LLMs in role-playing, identifying key\nareas for improvement and suggesting iterative enhance-\nments based on user feedback.\nThese studies have significantly advanced the field, al-\nlowing us to view the possibilities of AI-native games from\na more transformative perspective, which is this paper going\nto present in following chapters.\nThe text-to-game RPG shares the same life cycle as tradi-\ntional game but has brand-new definition of each stage as\nfigure 2.\n\u2022\nChapter #3 introduces the details of Game Building Co-\npilot, not like the traditional RPG to build a completed\ngame, Al engine only builds the starting point, like world\nview, characters, where the game should start from.\n\u2022\nChapter #4 presents how the Game Rendering happens\nin an Al engine, which is far more powerful and intelli-\ngent than traditional game engine. It becomes the brain of\nthe game and generates the game from the starting point\nand based on the user interaction for a personalized end-\ning without human intervention.\nAnd in Chapter #5 introduce the system implementation\nand experiment. In the end, this paper introduces the poten-\ntial key area in the future."}, {"title": "3. Copilot for Game Building", "content": "Envision Copilot as a virtual studio, composed of multiple\nAl agents. Each agent specializes in a different aspect of\ngame development, collaboratively transforming a user's\nbrief description into a comprehensive game. This multi-\nagent system can elaborate on the initial input to create a\nfully realized game setting, complete with detailed world-\nbuilding, character creation, and an engaging initial story-\nline, laying the foundation for a complete game experience.\nA UGC creator can incorporate IP-based novels to con-\nstruct worlds they are familiar with, or create a world with a\nbrand-new setting that is entirely original. The creator deter-\nmines the starting point of the world, and the Al engine col-\nlaborates with players to shape a personalized world during\nthe game's progress.\nIn this virtual studio, the process commences with the\nuser's input, such as \"A post-apocalyptic world where ro-\nbots have taken over, and a lone human survivor fights to\nreclaim their home.\" The AI agents then collaborate to flesh\nout this concept:\n1. World-Building Agent: Constructs the game's environ-"}, {"title": "4. AI Engine for Game Rendering", "content": "As an engine tasked with revolutionizing the next gener-\nation game experience, it should embody the following five\nkey characteristics:\n\u25c6 Zealous, reflecting the enthusiasm and creativity that\nthe engine brings to game creation.\n\u25c6 Adaptability, adjusting and responding to user inputs\nand preferences.\n\u25c6 Generativity, creating content and assets through AI\nin real-time.\n\u25c6 Interactivity, employing real-time, multimedia, and\nrealistic world simulation communication methods to\nenhance game immersion.\n\u25c6 Iteration, as Al is not about mechanically executing\ntasks, but rather driving the evolution of the game\nfrom a god-like perspective.\nOur \"ZAGII\u201d Engine serves as the foundational system\nthat drives the game-playing experience, integrating multi-\nple advanced subsystems to create a dynamic and immersive\nenvironment while controlling the game progress, for a\n\"Multi-Players, Multi-NPCs\" scenario.\nAs illustrated in Figure 4, all the modules communicate\nand share information through a centralized Message Bus,\nensuring data consistency and coordination across the entire\nsystem. This integration allows these modules to function\nharmoniously, much like a team of agents working collabo-\nratively towards the common goal of delivering a ground-\nbreaking gaming experience. By maintaining a unified and\nconsistent flow of information, the Zagii Engine ensures that\nevery aspect of the game works in concert, providing players\nwith a seamless and innovative gaming environment.\nThe Role-playing System leverages the capabilities of\nLarge Language Models (LLMs) to endow NPCs with so-\nphisticated cognitive abilities. These characters can observe\ntheir surroundings, understand complex scenarios, think\ncritically, plan their actions, make informed decisions, and\ninteract naturally with their environment. This advanced\nlevel of NPC autonomy and intelligence enriches the game-\nplay experience, making interactions with NPCs more real-\nistic and engaging. By simulating human-like behaviors and\nresponses, the role-playing system creates a more believable\nand immersive game world."}, {"title": "4.1. Role-Playing System", "content": "Role-playing Games (RPGs) have emerged as a significant\ngenre in the gaming industry, offering a unique gaming ex-\nperience that hinges on the collaboration of players and\nNon-Player Characters (NPCs). These games allow partici-\npants to deeply engage in diverse roles within the game,\ncrafting a distinctive narrative that is shaped by their collec-\ntive actions. Consequently, the need for an intelligent Role-\nplaying System that can enhance this experience by enabling\nNPCs to authentically embody their roles and collectively\npropel the game narrative forward, is paramount.\nHowever, a significant challenge in the current RPG land-\nscape is the predominantly passive nature and limited re-\nsponse capabilities of characters. While substantial work has\nbeen done to develop systems that enable characters to\nfreely respond to player interactions (Urbanek et al., 2019;\nShanahan et al., 2023; Shao et al., 2023; Wang et al., 2023),\nempowering NPCs to take proactive actions based on their\nobjectives and the current game scenario remains a chal-\nlenging and open-ended task. This task demands high stand-\nards from the Role-playing System, requiring NPCs to au-\ntonomously initiate suitable actions that are not pre-de-\nsigned in the game.\nTo address this challenge, we draw inspiration from the\nLLM Powered Autonomous Agents (Lilian 2023), a frame-\nwork that has demonstrated remarkable capabilities in con-\nducting human-like decision-making in complex environ-\nments, and Large Language Model Game Agent (LLMGA)\n(Hu et al. 2024), which is an survey on application of auton-\nomous agent architecture in game NPCs. We propose a\nRole-playing System Architecture that comprises four core\ncomponents: Perception, Memory, Thinking, and Action\n(referred to as PMTA), as illustrated in Figure 5.\nThe Perception module serves as the character's sensory\norgan, perceiving all changes in the game world. This in-\ncludes the character's external behaviors, alterations in the\ngame world state, and the progression of the game. These\ninputs are processed by the Perception module and trans-\nformed into data that can be handled by Large Language\nModels (LLMs) or Multimodal Large Language Models\n(MLLMs), serving as input for character decision-making.\nMemory, another pivotal module, stores a series of crucial\ninformation for Role-playing. This encompasses the charac-\nter's role setting, goals in the game, as well as self-aware-\nness and memory fragments generated during role-play in-\nteractions. In decision-making for Role-Play, Memory dy-\nnamically retrievals relevant historical memories (present in\nthe form of natural language text) based on the information"}, {"title": "4.2. Player Assistant System", "content": "Maximizing player creativity and reducing barriers during\ngameplay are critical to the success of any game. Our system\nincludes a meticulously designed Player Assistant module\nto enhance the experience of next-generation Al games.\nThis module leverages advanced multimodal large language\nmodels (MIIMs) to allow users to customize their characters\nmore freely, provide intelligent and effective game guidance,\nand, when necessary, delegate player actions to an Al model.\nThis delegation is particularly important in multiplayer\ngames. These enhancements significantly improve the con-\nvenience of game interactions and offer increased playabil-\nity.\nCharacter creation and customization are fundamental as-\npects of many games. Historically, technical constraints\nhave limited the degree of customization available to users.\nHowever, the maturation of AI-generated content (AIGC)\ntechnology now makes it feasible for players to define their\ndesired characters with unprecedented freedom. With the\nsupport of generative AI technology, users can swiftly cus-\ntomize their character's appearance, background, skills, and\npreferred game items. This capability provides a sense of\nfreedom where players' imaginations can be directly trans-\nlated into their in-game personas, enhancing their immer-\nsion and personal connection to the game world.\nNumerous studies have demonstrated that providing users\nwith effective in-game tips is crucial for enhancing the depth\nand enjoyment of their gaming experience. To address this,\nwe have integrated a gameplay copilot module for players,\nbased on sophisticated large model technology. This copilot\ncan instantly analyze the current game situation, offer ac-\ntionable recommendations, and provide natural language\nexplanations of the game's progress. The copilot's role in\nenhancing player experience is a significant research focus\nwithin the gaming industry (Gallotta et al. 2024), with nota-\nble contributions from companies such as Microsoft setting\nbenchmarks in this domain.\nAdvancing beyond the copilot mode, AI-based Autonomous\nGame Playing becomes crucial in certain scenarios. For in-\nstance, in multiplayer games, Autonomous Play can seam-\nlessly fill in when there are insufficient players, ensuring a"}, {"title": "4.3. Game Status Manager", "content": "In our framework, the Game Status Manager module is re-\nsponsible for tracking the progression of the game and facil-\nitating the advancement of new plots. This module is pivotal\nto gameplay as it determines when to assign new tasks to\nplayers, introduce new plots or clues, and transition to the\nsubsequent game chapter.\nThe Game Status Manager performs three primary func-\ntions:\n1. It analyzes the most recent interactions of all game char-\nacters and their impact on the game environment, tracks\nessential game states, and presents the updated states via\nthe UI for immediate player feedback.\n2. It verifies whether any goals have been accomplished\nbased on the current game status.\n3. Signifies the need for the advancement of new game plots\nbased on achievement detection. The module then assigns\nnew tasks to players or NPCs, issues new clues or plot\ninformation, or even concludes the current chapter and\ntransitions to the next one to make the game more playa-\nble.\nDuring the game creation phase, the Game Building Co-\npilot aids creators in identifying critical game statuses for\nmonitoring. Creators establish game goals and their corre-\nsponding achievement criteria. The Copilot identifies key\nperformance indicators aligned with these objectives for\ncontinuous tracking during gameplay.\nWe use numerical values or concise text to record key sta-\ntus details. For instance, in emotional companion games, we\ntrack player and character emotions using intimacy metrics.\nIn Dungeons & Dragons, we monitor the health of player\nand monsters. In adventure games, we track the player's cur-\nrent location in the overall map.\nTo manage complex goals, the Copilot decomposes the\ngoal into multiple sub-goals based on logical judgments or\ndependencies, presenting these relationships in a structured\nformat. We provide an example to ensure clear understand-\ning and inference of each sub-goal by the LLM in Figure-6.\nThe diversity of game goals calls for a flexible goal check\nmodule capable of adapting its prompt template to each\nunique game scenario. This is vital as the module operates\ncontinuously throughout game dialogues, necessitating both\nspeed and accuracy to ensure smooth gameplay. However,\nthe limited reasoning capabilities of lightweight LLMs can\ncompromise the effectiveness of goal assessments. To ad-\ndress this, we employ two modules leveraging state-of-the-\nart (SOTA) models:\n\u2022\nCold Start: Before gameplay, the SOTA model processes\nand understands the full scope of the game's information\nand goals. It generates crucial considerations for goal val-\nidation, which are then integrated into the goal check\nmodule's prompt template, guiding the lightweight LLM.\n\u2022\nReal-Time Assessment: During gameplay, the evalua-\ntions of the lightweight LLM are periodically sampled"}, {"title": "4.4. Emergent Narrative System", "content": "Our goal is to revolutionize gaming narratives by generating\ndynamic, real-time narratives that adapt to player actions\nand game status. We aim for a \"thousand different endings\"\neffect, providing a unique gaming experience for each\ngameplay.\nOur approach stands out from traditional methods that use\nstatic scripts or predefined narratives. We ensure the game-\nplay experience aligns with the unfolding narrative and pro-\ngression. The Emergent Narrative Generation System is pri-\nmarily distinguished by two features: Real-time Narrative\nGeneration and Interactive Narrative Consumption.\nWe generate narratives in real-time, aligning narrative de-\nvelopment with game progress and the creator's design. This\ndynamic generation keeps the narrative relevant and engag-\ning.\nA key challenge in game narrative design is detailing\ncharacter interactions and stories. While designers excel at\ncreating expansive worlds and frameworks, nuanced narra-\ntive development often requires additional finesse. This gap\nhas spurred research in automated story generation.\nOur system builds on principles established by Yang et al.\n(2022), who enhanced long story coherence through struc-\ntured prompts and detailed outlines. We adopt a similar ap-\nproach, progressing from game world and character design\nto chapter and goal formulation. The system enriches in-\nchapter narratives with multiple goals and twists, ensuring\ndynamic gameplay. In the future, it will also support dy-\nnamic additions and deletions to accommodate open-world\ngames.\nThe system leverages context from the game building co-\npilot and incorporates a material recall mechanism, drawing\nfrom large language model knowledge bases and Retrieval-\nAugmented Generation systems. This enhances the narra-\ntive's factual and contextual accuracy.\nAs showed in Figure 7, our workflow integrates player,\nenvironment, and NPC states as factors influencing the nar-\nrative. During generation, information from the game build-\ning copilot, current states, and incomplete goals are struc-\ntured into prompts. Integration with Game Status Manager\nensures the narrative reflects changes in player stats, envi-\nronment conditions, and NPC statuses."}, {"title": "4.4.2. Interactive Narrative Consumption", "content": "As players primarily experience the narrative through inter-\nactions with NPCs, our system dynamically updates NPC's\nrole-playing prompts to reflect the evolving narrative. This\napproach makes narrative consumption interactive, reflect-\ning player decisions and the dynamic game world, and ad-\ndresses the limitations of static prompts.\nNPC's role-playing prompts are categorized into static in-\nformation, task-related information, and current narrative\ncontext. By dynamically adjusting the narrative context and\nNPC tasks, interactions remain fresh and relevant.\nWe evaluate this system using metrics including align-\nment of NPC responses with character traits, accuracy of\ntask execution and information provided, consistency with\nbackgrounds and narratives, and relevance of NPC tasks to\ncurrent chapter goals.\nIn conclusion, the system's capacity to dynamically gen-\nerate and render narrative elements, maintain character con-\nsistency, accommodate open-world dynamics, and incorpo-\nrate a Game Status Manager significantly enhances game\nimmersion. This comprehensive approach sustains player\nengagement by delivering continuously evolving narratives\nthat are responsive to both player actions and the game en-\nvironment."}, {"title": "4.5. Multi-Modal Rendering System", "content": "A complete gaming experience is composed of a combina-\ntion of multi-modal content including visuals, sound, back-\nground music, and sound effects. Building on the foundation\nof the text-based rendering capabilities, however, unfolding\nan evolving multi-modal content expression that dynami-\ncally responds to player interactions and narrative progress\\ion throughout the game poses significant challenges on the\noutput consistency and continuous coherent evolution.\nThe Multi-modal Rendering System utilizes large lan-\nguage models (LLMs) for memory retrieval, status manage-\nment, and information orchestration. Through various\nadapters, it transforms the RPG gaming experience into cor-\nresponding multi-modal descriptions to trigger real-time\ncontent generation by large multi-modal models. The pro-\nduced multi-modal content serves as part of the session\nmemory, ensuring consistency throughout the game's evolu-\ntionary process.\nEntities are any objects that can act or interact independently,\neach possessing its own description, attributes, and multi-\nmodal assets, which are part of the session memory and can\nrepresent NPCs, scenes, key items, or even players. In AI-\nnative games, when users initiate a game, it merely marks\nthe beginning of an expandable world, initially featuring a\nlimited and incomplete number of entities. Entities are cre-\nated and updated based on chapter changes and special\nevents within the game. When the multi-modal rendering\nsystem renders a scene involving an entity, it reads the enti-\nty's multi-modal assets; if these assets are absent, initial as-\nsets are generated. If multi-modal assets already exist, they\nare used as reference information for subsequent generation\nto maintain consistency. The lifecycle of an entity is deter-\nmined by the game's progress, while the lifecycle of its\nmulti-modal assets is determined by that of the entity.\nThe perception module is responsible for preprocessing\nplayers' gaming experiences into concise plot summaries,\ngenerating information to retrieve entity IDs, and assisting\nthe status manager in updating the status of entity assets.\nBased on the current round of dialogue, the perception mod-\nule interprets structured historical dialogue data from the\nplayer's first-person perspective, understanding player in-\ntentions and actions, and outputs plot themes and narratives.\nThrough the interpretation of player behavior by the per-\nception module, the system can retrieve specific entity IDs\nfrom session memory storage, representing which entities\nthe user interacted with from their perspective in the current\nround, and the depth of these interactions. Game Status\nManager evaluates changes in entities within the dialogue\nhistory based on the current round of dialogue. If there is a\nsignificant change in an entity's textual metadata, its multi-\nmodal assets are updated to reflect the latest game progress.\nThe status of entity assets not retrieved remains unchanged;\nonly the retrieved are used and updated in the current round.\nTaking image generation as an example, diffusion models\nguided solely by text struggle to maintain consistency of\nspecific objects across multiple inference processes. La-\nbeled prompts provide a vast representational space, yet\nthey also introduce randomness in the generated content.\nThis inconsistency induced by text-guided conditions poses\nsignificant obstacles for rendering real-time RPG games.\nYang et al. (2024) proposed a training-free framework that\nuses language models for Recaptioning, Planning, and Gen-\nerating to guide regional conditional diffusion. The Omost\nmethod, proposed by Lvmin Zhang's team (LLlyasviel\n2024), further summarized the approaches for regional con-\nditional combination diffusion using LLMs, achieving prac-\ntically significant results through uniquely designed block\nimage representation symbols that fine-tune LLMs.\nOur multi-modal processor references these studies, im-\nplementing regional conditional control of the image canvas\nthrough attention decomposition. We first arrange prompts\ninto global and local sub-prompts. For global prompts, cor-\nresponding to the canvas background, we use Plot themes\nand narratives from the perception module as textual\nprompts to guide the overall image semantics and composi-\ntion. For local sub-prompts, we employ the Cot method to\nguide LLMs in regional partitioning of entity locations and\nintroduce image prompts through the IP-Adapter, consider"}, {"title": "5. System Implementation and Experiments", "content": "To validate our proposed text-to-game framework, we de-\nveloped an experimental system using Game Building Co-\npilot and Zagii Engine. Game Building Copilot facilitates\ngame creation, rapidly transforming innovative ideas into\nplayable RPGs. Conversely, Zagii Engine handles real-time\ngame rendering, capable of supporting online gameplay\nwith significant concurrency.\nWhile the design principles and solutions for each subsys-\ntem have been discussed in previous sections, we will focus\nhere on the aspects related to Large Language Model (LLM)\napplications. The ability to harness the power of large mod-\nels effectively and controllably is of paramount importance\nto us, as we leverage LLM across almost all subsystems to\nachieve the desired outcomes."}, {"title": "5.1. Dynamic Prompt Generation", "content": "The utilization of Large Language Models (LLMs) has be-\ncome increasingly prevalent across a myriad of applications,\nowing to their ability to generate human-like text. The effi-\ncacy of these models, however, is contingent upon the qual-\nity of the prompts provided to them. A well-crafted prompt,\nmeticulously tailored to the task at hand, can elicit superior\nresults from LLMs. This principle holds true across a broad\nspectrum of scenarios involving the use of these large mod-\nels, underscoring the importance of investing time and effort\nin the creation of effective prompts.\nNevertheless, the task of creating universally effective\nprompts is not without its challenges. Certain applications\npresent complex scenarios that necessitate a nuanced ap-\nproach to prompt design. A prime example of such a sce-\nnario is our role-playing system, where the design of role-\nplaying prompts for all characters across various game gen-\nres can be a complex undertaking.\nOur role-playing system supports a wide array of Role-\nPlaying Games, encompassing genres such as detective, ad-\nventure, simulator, communication, and more. The diversity\nof these genres introduces a level of complexity in prompt\ndesign, as each genre has unique characteristics and require-\nments. Moreover, within a single game, there exists a mult-"}, {"title": "5.2 Model Finetuning and Management", "content": "Large Language Models (LLMs) can address many applica-\ntion requirements, especially when enhanced with prompt\nengineering or dynamic prompt generation. These tech-\nniques guide the model's responses and improve perfor-\nmance across tasks. However, fine-tuning, which adjusts a\npre-trained model's parameters for a specific task or dataset,\ncan be more effective in certain scenarios. We fine-tune the\nLLM on a custom dataset that includes various game sce-\nnarios, dialogues, and narratives. This ensures that the LLM\ncan generate responses that are not only grammatically cor-\nrect and coherent but also creative and engaging, enhancing\nthe overall gaming experience.\nAs an application layer based on Large Language Models\n(LLMs), it is essential to continually enhance the founda-\ntional capabilities of LLMs in the gaming domain. Concur-\nrently, it is also necessary to tailor the approach by incorpo-\nrating the unique characteristics of different game genres,\ntraining the models with specific data to endow them with\nrelevant generative abilities. Thus, the structure involves a\ntwo-level training process.\nFirstly, the base of dialogue and game-related data is con-\ntinuously expanded to fine-tune the foundational models,\nensuring they are well-suited for general outputs in the gam-\ning domain. This involves the collection and integration of\na wide range of dialogue interactions and game scenarios,\nallowing the models to learn from diverse contexts and im-\nprove their adaptability and coherence in generating game-\nrelated content.\nSecondly, genre-specific dialogue and game data are con-\nstructed, allowing for secondary training of the models for\ndistinct game genres. This process involves curating de-\ntailed datasets that reflect the unique elements, themes, and\nmechanics of various game genres, such as role-playing\ngames, strategy games, and adventure games. By focusing\non genre-specific data, the models can develop a deeper un-\nderstanding and more nuanced generative capabilities tai-\nlored to the specific requirements and expectations of each\ngenre."}, {"title": "5.3. Experiment Results", "content": "For comprehensive experimental validation, we have made\nour Game Building Copilot and Zagii Engine accessible on\nour website. We have also recruited a group of individual\ngame creators to create their own games and conducted\ngame testing on a scale of tens of thousands of players.\nDuring the experiments, game creators used the Game\nBuilding Copilot to create games. We offered eight different\ntemplates for game creation, allowing creators to either\nbuild continuously on these templates or create entirely new\ngames. The games covered six categories: Adventure\n(where the game master controls the game), Role-playing,\nMystery, Simulation, Strategy, and Choice-based Adventure.\nThe game creators successfully published a total of 803\ngames. Of these, 746 games (approximately 93%) were cre-\nated and published within 24 hours, demonstrating the effi-\nciency of our framework in enhancing game development\nproductivity.\nFrom the published 746 games, we selected 168 games\nfor player testing. During the experiment period, these\ngames amassed a total of 60,301 gameplay sessions. A\ngameplay session is defined as a scenario where a player\nstarts a game and continues until the game ends, or they exit\nmid-way. Notably, a meticulously crafted game achieved a\ntotal of 35,407 gameplay sessions, indicating significant\nsuccess and validating the potential of the text-to-game con-\ncept to produce engaging games.\nFor the sake of data analysis consistency, we excluded\nthis exceptional game from subsequent analysis, focusing on\nthe 167 games which accumulated 24,894 gameplay ses-\nsions.\n shows the distribution of gameplay sessions\namong the 167 games. As can be seen, a small number of\ngames have garnered the majority of gameplay, which is in\nline with the 80/20 rule of traffic distribution. Twenty-nine\ngames garnered more than 100 gameplay sessions each,\nwith six games receiving over 500 sessions.\n11 presents the distribution of User-NPC interac-\ntion rounds across the 24,894 gameplay sessions. Most\ngame play sessions had interaction rounds ranging between\n5 and 30, with over 200 sessions exceeding 50 interaction\nrounds. A significant portion of gameplay sessions had\nfewer than 5 interaction rounds, which includes instances\nwhere players opened the game but exited quickly. The rea-\nsons for these early exits vary, including unsuccessful game\nrendering on the front end or players finding the game did\nnot meet their expectations."}, {"title": "6. Conclusion and Future Work", "content": "This paper presents an innovative text-to-game engine that\ncreates and renders RPG games in real-time. Utilizing gen-\nerative AI, our goal is to provide a framework that enables\nanyone to create any game, simplifying game development\nto typing in a text box. The current implementation supports\nlimited scale 'Multi-Player Multi-NPCs' RPG scenarios. As\ngenerative AI continues to evolve, we anticipate the creation\nof large-scale open worlds for UGC-based RPGs, akin to the\nnext generation of Roblox. However, several issues need to\nbe addressed to achieve this goal."}, {"title": "6.1. 2D & 3D asset generation", "content": "Generative Al has led to significant progress in the genera-\ntion of 2D and 3D game assets. Tools like NVIDIA's Gau-\nGAN and Art-breeder are used in the industry to create de-\ntailed and diverse 2D textures and characters. Innovations in\n3D asset generation have been seen with models like\nNVIDIA's DLSS and Unreal Engine's MetaHuman Creator,\nenabling the creation of lifelike character models and envi-\nronments. Academic research has also contributed, with\nstudies showing the potential of Generative Al to produce\nhigh-quality assets that integrate seamlessly into game\nworlds. However, a major challenge remains: the lack of\nreal-time rendering and optimization algorithms to ensure\nthat generated assets are not only of high quality but also\nperform well within game environments."}, {"title": "6.2. AB Testing Framework", "content": "Another crucial area for future work is the development of\na robust A/B testing framework. This framework would as-\nsess the impact of different model versions and game fea-\ntures on a large scale and capture a variety of detailed data\npoints from player interactions. The process involves creat-\ning parallel test environments where players are randomly\nassigned to different versions of the game engine or specific\nfeatures. By comparing player responses and performance\nacross these environments, we can identify which changes\nenhance the game experience. The A/B testing framework\nshould be flexible and scalable, ensuring that model itera-\ntions are guided by data-driven insights, leading to continu-\nous improvements."}]}