{"title": "A Text-to-Game Engine for UGC-Based Role-Playing Games", "authors": ["Lei Zhang", "Xuezheng Peng", "Shuyi Yang", "Feiyang Wang"], "abstract": "The shift from professionally generated content (PGC) to\nuser-generated content (UGC) has revolutionized various\nmedia formats, from text to video. With the rapid advance-\nments in generative AI, a similar shift is set to transform the\ngame industry, particularly in the realm of role-playing\ngames (RPGs).\nThis paper introduces a new framework for a text-to-game\nengine that utilizes foundation models to convert simple tex-\ntual inputs into complex, interactive RPG experiences. The\nengine dynamically renders the game story in a multi-modal\nformat and adjusts the game character, environment, and me-\nchanics in real-time in response to player actions. Using this\nframework, we developed the 'Zagii' game engine, which has\nsuccessfully supported hundreds of RPG games across a di-\nverse range of genres and facilitated tens of thousands of\nonline user gameplay instances. This validates the effective-\nness of our framework. Our work showcases the potential for\na more open and democratized gaming paradigm, highlight-\ning the transformative impact of generative AI on the game\nlife cycle.", "sections": [{"title": "1. Introduction", "content": "The creation of traditional RPGs, typically undertaken by\nprofessional and large development teams, requires a di-\nverse set of skills. These include screenwriting, character\ndesign, game mechanics, coding, and graphical design,\namong others. This multidisciplinary requirement makes the\ndevelopment process costly, time-consuming, and relatively\ninflexible, as changes in one area often trigger cascading\nchanges across the game. This complexity slows the intro-\nduction of new games to the market, restricts the variety of\navailable narrative and gameplay styles, limits player free-\ndom and control over narrative paths, and affects content ex-\npansion depth. Traditional RPGs, reliant on rigid game en-\ngines, often struggle to dynamically adapt to player choices,\ntypically offering a linear or branching path that doesn't\nevolve based on player interaction, or networked narratives\nthat aren't replicable for broader use.\nillustrates the major life cycle of a game, struc-\ntured into several phases: Concept Planning, Game Design,\nGame Development, and Game Rendering. Developers me-\nticulously handcraft these phases, requiring substantial man-\nual effort and creativity. Asset generation involves profes-\nsional artists and designers creating high-quality characters,\nenvironments, and props. Despite the detailed craftsmanship,\nthe user experience often features an accumulation of artifi-\ncial elements, limited game paths, and finite experiences, re-\nsulting in a predictable and repetitive set of dialogues, end-\nings, and player interactions.\nIn contrast, AI-native RPGs represent a significant para-\ndigm shift in game development. These games utilize gen-\nerative AI to evolve, generate, and maintain game content\nfrom a certain initial condition, thereby eliminating the need\nfor human intervention. This AI-driven approach allows for\nthe synthesis of various game elements, such as storylines,\ncharacters, and worlds, from simple textual inputs provided\nby users. Unlike traditional methods, this approach signifi-\ncantly reduces the need for technical skills, enabling indi-\nvidual creators to produce complex games. Furthermore,\nAI-native RPGs are inherently dynamic, with the unique ca-\npability to dynamically generate and adjust game content in\nreal-time based on player decisions. This adaptability results\nin a more personalized gaming experience, as the game\nworld evolves uniquely for each player, reflecting their ac-\ntions and choices in a narrative that continuously unfolds."}, {"title": "2. Related Works", "content": "There are numerous works at the intersection of LLMs and\ngame development, the role of LLMs in games can be cate-\ngorized into several key areas: Player, Non-Player Character\n(NPC), Game Master (GM), Player Assistant, Commentator\n/Reteller, Game Mechanic, Automated Designer, and De-\nsign Assistant (Gallotta et al. 2024). LLMs can play games\nby converting game states and actions into token sequences,\nhandling both text-based and visual-based game states. They\nhave been applied in board games like Chess (Toshniwal et\nal. 2022), Go (Ciolino, Kalin, and Noever 2020), and\nOthello (Li et al. 2022), as well as in text adventure games\nwhere they generate character response based on environ-\nment descriptions (Yao et al. 2020, Tsai et al. 2023). LLMs\nalso play Atari games by predicting actions from visual in-\nputs, as demonstrated by the GATO agent (Reed et al. 2022).\nIn enhancing NPC dialogue and behavior, LLMs create\nimmersive interactions by adapting responses to game con-\ntexts (Shanahan, McDonell, and Reynolds 2023). They are\nused for both foreground NPCs, which require contextual\ninteractions (Warpefelt and Verhagen 2017, Xu et al. 2023,\nMaas, Wheeler, and Billington 2023), and background\nNPCs, which maintain ambient dialogue (Mehta et al. 2022).\nAs Game Masters in tabletop role-playing games (TTRPGs),\nLLMs generate plots, characters, and narratives. Applica-\ntions like AI Dungeon (Hua and Reley 2020) use LLMs for\ninteractive storytelling. Tools like CALYPSO (Zhu et al.\n2023) assist human GMs with encounter generation, and\nShoelace (Acharya et al. 2023) aids in monitoring and re-\nsponding to in-game conversations.\nAdditionally, LLMs can narrate game events for players\nor spectators, enhancing engagement by summarizing inter-\nactions and providing automated commentary, which helps\nstreamers manage audience interactions effectively (Ranella\nand Eger 2023).\nFocusing on role-playing games (RPGs), the applications\nof LLMs have garnered significant attention in both aca-\ndemic research and industry. Existing studies highlight the\nsynergy between LLMs and RPGs. For instance, Generative\nAgent (Park et al. 2023) introduced computational agents\nthat simulate human behavior and described an architecture\nthat utilizes memories and reflections to dynamic plan agent\nbehaviors. LLMGA (Hu et al. 2024) provides a broad per-\nspective on the architecture and functionality of LLM-based\ngame agents, highlighting their application across various\ngame genres. In study (Shanahan, McDonell, and Reynolds\n2023), they examine the nuances of role-playing, particu-\nlarly the conversational agent's capabilities in deception and\nself-awareness, providing insights into achieving more hu-\nman-like interactions in RPGs. Character-LLM (Shao et al.\n2023) introduces a novel approach to enhancing role-play-\ning scenarios through fine-tuning on role-play datasets, em-\nphasizing the importance of character consistency and im-\nprovisation. Role LLM (Wang et al. 2023) presents a sys-\ntematic evaluation of LLMs in role-playing, identifying key\nareas for improvement and suggesting iterative enhance-\nments based on user feedback.\nThese studies have significantly advanced the field, al-\nlowing us to view the possibilities of AI-native games from\na more transformative perspective, which is this paper going\nto present in following chapters.\nThe text-to-game RPG shares the same life cycle as tradi-\ntional game but has brand-new definition of each stage as\nfigure 2.\n\u2022\nChapter #3 introduces the details of Game Building Co-\npilot, not like the traditional RPG to build a completed\ngame, Al engine only builds the starting point, like world\nview, characters, where the game should start from.\n\u2022 Chapter #4 presents how the Game Rendering happens\nin an Al engine, which is far more powerful and intelli-\ngent than traditional game engine. It becomes the brain of\nthe game and generates the game from the starting point\nand based on the user interaction for a personalized end-\ning without human intervention.\nAnd in Chapter #5 introduce the system implementation\nand experiment. In the end, this paper introduces the poten-\ntial key area in the future."}, {"title": "3. Copilot for Game Building", "content": "Envision Copilot as a virtual studio, composed of multiple\nAl agents. Each agent specializes in a different aspect of\ngame development, collaboratively transforming a user's\nbrief description into a comprehensive game. This multi-\nagent system can elaborate on the initial input to create a\nfully realized game setting, complete with detailed world-\nbuilding, character creation, and an engaging initial story-\nline, laying the foundation for a complete game experience.\nA UGC creator can incorporate IP-based novels to con-\nstruct worlds they are familiar with, or create a world with a\nbrand-new setting that is entirely original. The creator deter-\nmines the starting point of the world, and the Al engine col-\nlaborates with players to shape a personalized world during\nthe game's progress.\nIn this virtual studio, the process commences with the\nuser's input, such as \"A post-apocalyptic world where ro-\nbots have taken over, and a lone human survivor fights to\nreclaim their home.\" The AI agents then collaborate to flesh\nout this concept:\n1. World-Building Agent: Constructs the game's environ-"}, {"title": "4. AI Engine for Game Rendering", "content": "As an engine tasked with revolutionizing the next gener-\nation game experience, it should embody the following five\nkey characteristics:\n\u25c6 Zealous, reflecting the enthusiasm and creativity that\nthe engine brings to game creation.\n\u25c6 Adaptability, adjusting and responding to user inputs\nand preferences.\n\u25c6 Generativity, creating content and assets through AI\nin real-time.\n\u25c6 Interactivity, employing real-time, multimedia, and\nrealistic world simulation communication methods to\nenhance game immersion.\n\u25c6 Iteration, as Al is not about mechanically executing\ntasks, but rather driving the evolution of the game\nfrom a god-like perspective.\nOur \"ZAGII\u201d Engine serves as the foundational system\nthat drives the game-playing experience, integrating multi-\nple advanced subsystems to create a dynamic and immersive\nenvironment while controlling the game progress, for a\n\"Multi-Players, Multi-NPCs\" scenario.\nAs illustrated in Figure 4, all the modules communicate\nand share information through a centralized Message Bus,\nensuring data consistency and coordination across the entire\nsystem. This integration allows these modules to function\nharmoniously, much like a team of agents working collabo-\nratively towards the common goal of delivering a ground-\nbreaking gaming experience. By maintaining a unified and\nconsistent flow of information, the Zagii Engine ensures that\nevery aspect of the game works in concert, providing players\nwith a seamless and innovative gaming environment.\nThe Role-playing System leverages the capabilities of\nLarge Language Models (LLMs) to endow NPCs with so-\nphisticated cognitive abilities. These characters can observe\ntheir surroundings, understand complex scenarios, think\ncritically, plan their actions, make informed decisions, and\ninteract naturally with their environment. This advanced\nlevel of NPC autonomy and intelligence enriches the game-\nplay experience, making interactions with NPCs more real-\nistic and engaging. By simulating human-like behaviors and\nresponses, the role-playing system creates a more believable\nand immersive game world."}, {"title": "4.1. Role-Playing System", "content": "Role-playing Games (RPGs) have emerged as a significant\ngenre in the gaming industry, offering a unique gaming ex-\nperience that hinges on the collaboration of players and\nNon-Player Characters (NPCs). These games allow partici-\npants to deeply engage in diverse roles within the game,\ncrafting a distinctive narrative that is shaped by their collec-\ntive actions. Consequently, the need for an intelligent Role-\nplaying System that can enhance this experience by enabling\nNPCs to authentically embody their roles and collectively\npropel the game narrative forward, is paramount.\nHowever, a significant challenge in the current RPG land-\nscape is the predominantly passive nature and limited re-\nsponse capabilities of characters. While substantial work has\nbeen done to develop systems that enable characters to\nfreely respond to player interactions (Urbanek et al., 2019;\nShanahan et al., 2023; Shao et al., 2023; Wang et al., 2023),\nempowering NPCs to take proactive actions based on their\nobjectives and the current game scenario remains a chal-\nlenging and open-ended task. This task demands high stand-\nards from the Role-playing System, requiring NPCs to au-\ntonomously initiate suitable actions that are not pre-de-\nsigned in the game.\nTo address this challenge, we draw inspiration from the\nLLM Powered Autonomous Agents (Lilian 2023), a frame-\nwork that has demonstrated remarkable capabilities in con-\nducting human-like decision-making in complex environ-\nments, and Large Language Model Game Agent (LLMGA)\n(Hu et al. 2024), which is an survey on application of auton-\nomous agent architecture in game NPCs. We propose a\nRole-playing System Architecture that comprises four core\ncomponents: Perception, Memory, Thinking, and Action\n(referred to as PMTA), as illustrated in Figure 5.\nThe Perception module serves as the character's sensory\norgan, perceiving all changes in the game world. This in-\ncludes the character's external behaviors, alterations in the\ngame world state, and the progression of the game. These\ninputs are processed by the Perception module and trans-\nformed into data that can be handled by Large Language\nModels (LLMs) or Multimodal Large Language Models\n(MLLMs), serving as input for character decision-making.\nMemory, another pivotal module, stores a series of crucial\ninformation for Role-playing. This encompasses the charac-\nter's role setting, goals in the game, as well as self-aware-\nness and memory fragments generated during role-play in-\nteractions. In decision-making for Role-Play, Memory dy-\nnamically retrievals relevant historical memories (present in\nthe form of natural language text) based on the information"}, {"title": "4.2. Player Assistant System", "content": "Maximizing player creativity and reducing barriers during\ngameplay are critical to the success of any game. Our system\nincludes a meticulously designed Player Assistant module\nto enhance the experience of next-generation Al games.\nThis module leverages advanced multimodal large language\nmodels (MIIMs) to allow users to customize their characters\nmore freely, provide intelligent and effective game guidance,\nand, when necessary, delegate player actions to an Al model.\nThis delegation is particularly important in multiplayer\ngames. These enhancements significantly improve the con-\nvenience of game interactions and offer increased playabil-\nity.\nCharacter creation and customization are fundamental as-\npects of many games. Historically, technical constraints\nhave limited the degree of customization available to users.\nHowever, the maturation of AI-generated content (AIGC)\ntechnology now makes it feasible for players to define their\ndesired characters with unprecedented freedom. With the\nsupport of generative AI technology, users can swiftly cus-\ntomize their character's appearance, background, skills, and\npreferred game items. This capability provides a sense of\nfreedom where players' imaginations can be directly trans-\nlated into their in-game personas, enhancing their immer-\nsion and personal connection to the game world.\nNumerous studies have demonstrated that providing users\nwith effective in-game tips is crucial for enhancing the depth\nand enjoyment of their gaming experience. To address this,\nwe have integrated a gameplay copilot module for players,\nbased on sophisticated large model technology. This copilot\ncan instantly analyze the current game situation, offer ac-\ntionable recommendations, and provide natural language\nexplanations of the game's progress. The copilot's role in\nenhancing player experience is a significant research focus\nwithin the gaming industry (Gallotta et al. 2024), with nota-\nble contributions from companies such as Microsoft setting\nbenchmarks in this domain.\nAdvancing beyond the copilot mode, AI-based Autonomous\nGame Playing becomes crucial in certain scenarios. For in-\nstance, in multiplayer games, Autonomous Play can seam-\nlessly fill in when there are insufficient players, ensuring a"}, {"title": "4.3. Game Status Manager", "content": "In our framework, the Game Status Manager module is re-\nsponsible for tracking the progression of the game and facil-\nitating the advancement of new plots. This module is pivotal\nto gameplay as it determines when to assign new tasks to\nplayers, introduce new plots or clues, and transition to the\nsubsequent game chapter.\nThe Game Status Manager performs three primary func-\ntions:\n1. It analyzes the most recent interactions of all game char-\nacters and their impact on the game environment, tracks\nessential game states, and presents the updated states via\nthe UI for immediate player feedback.\n2. It verifies whether any goals have been accomplished\nbased on the current game status.\n3. Signifies the need for the advancement of new game plots\nbased on achievement detection. The module then assigns\nnew tasks to players or NPCs, issues new clues or plot\ninformation, or even concludes the current chapter and\ntransitions to the next one to make the game more playa-\nble.\nDuring the game creation phase, the Game Building Co-\npilot aids creators in identifying critical game statuses for\nmonitoring. Creators establish game goals and their corre-\nsponding achievement criteria. The Copilot identifies key\nperformance indicators aligned with these objectives for\ncontinuous tracking during gameplay.\nWe use numerical values or concise text to record key sta-\ntus details. For instance, in emotional companion games, we\ntrack player and character emotions using intimacy metrics.\nIn Dungeons & Dragons, we monitor the health of player\nand monsters. In adventure games, we track the player's cur-\nrent location in the overall map.\nTo manage complex goals, the Copilot decomposes the\ngoal into multiple sub-goals based on logical judgments or\ndependencies, presenting these relationships in a structured\nformat. We provide an example to ensure clear understand-\ning and inference of each sub-goal by the LLM in Figure-6.\nThe diversity of game goals calls for a flexible goal check\nmodule capable of adapting its prompt template to each\nunique game scenario. This is vital as the module operates\ncontinuously throughout game dialogues, necessitating both\nspeed and accuracy to ensure smooth gameplay. However,\nthe limited reasoning capabilities of lightweight LLMs can\ncompromise the effectiveness of goal assessments. To ad-\ndress this, we employ two modules leveraging state-of-the-\nart (SOTA) models:\n\u2022\n\u2022 Cold Start: Before gameplay, the SOTA model processes\nand understands the full scope of the game's information\nand goals. It generates crucial considerations for goal val-\nidation, which are then integrated into the goal check\nmodule's prompt template, guiding the lightweight LLM.\nReal-Time Assessment: During gameplay, the evalua-\ntions of the lightweight LLM are periodically sampled"}, {"title": "4.4. Emergent Narrative System", "content": "Our goal is to revolutionize gaming narratives by generating\ndynamic, real-time narratives that adapt to player actions\nand game status. We aim for a \"thousand different endings\"\neffect, providing a unique gaming experience for each\ngameplay.\nOur approach stands out from traditional methods that use\nstatic scripts or predefined narratives. We ensure the game-\nplay experience aligns with the unfolding narrative and pro-\ngression. The Emergent Narrative Generation System is pri-\nmarily distinguished by two features: Real-time Narrative\nGeneration and Interactive Narrative Consumption.\nWe generate narratives in real-time, aligning narrative de-\nvelopment with game progress and the creator's design. This\ndynamic generation keeps the narrative relevant and engag-\ning.\nA key challenge in game narrative design is detailing\ncharacter interactions and stories. While designers excel at\ncreating expansive worlds and frameworks, nuanced narra-\ntive development often requires additional finesse. This gap\nhas spurred research in automated story generation.\nOur system builds on principles established by Yang et al.\n(2022), who enhanced long story coherence through struc-\ntured prompts and detailed outlines. We adopt a similar ap-\nproach, progressing from game world and character design\nto chapter and goal formulation. The system enriches in-\nchapter narratives with multiple goals and twists, ensuring\ndynamic gameplay. In the future, it will also support dy-\nnamic additions and deletions to accommodate open-world\ngames.\nThe system leverages context from the game building co-\npilot and incorporates a material recall mechanism, drawing\nfrom large language model knowledge bases and Retrieval-\nAugmented Generation systems. This enhances the narra-\ntive's factual and contextual accuracy.\nAs showed in Figure 7, our workflow integrates player,\nenvironment, and NPC states as factors influencing the nar-\nrative. During generation, information from the game build-\ning copilot, current states, and incomplete goals are struc-\ntured into prompts. Integration with Game Status Manager\nensures the narrative reflects changes in player stats, envi-\nronment conditions, and NPC statuses."}, {"title": "4.5. Multi-Modal Rendering System", "content": "A complete gaming experience is composed of a combina-\ntion of multi-modal content including visuals, sound, back-\nground music, and sound effects. Building on the foundation\nof the text-based rendering capabilities, however, unfolding\nan evolving multi-modal content expression that dynami-\ncally responds to player interactions and narrative progress\\ion throughout the game poses significant challenges on the\noutput consistency and continuous coherent evolution.\nThe Multi-modal Rendering System utilizes large lan-\nguage models (LLMs) for memory retrieval, status manage-\nment, and information orchestration. Through various\nadapters, it transforms the RPG gaming experience into cor-\nresponding multi-modal descriptions to trigger real-time\ncontent generation by large multi-modal models. The pro-\nduced multi-modal content serves as part of the session\nmemory, ensuring consistency throughout the game's evolu-\ntionary process.\nEntities are any objects that can act or interact independently,\neach possessing its own description, attributes, and multi-\nmodal assets, which are part of the session memory and can\nrepresent NPCs, scenes, key items, or even players. In AI-\nnative games, when users initiate a game, it merely marks\nthe beginning of an expandable world, initially featuring a\nlimited and incomplete number of entities. Entities are cre-\nated and updated based on chapter changes and special\nevents within the game. When the multi-modal rendering\nsystem renders a scene involving an entity, it reads the enti-\nty's multi-modal assets; if these assets are absent, initial as-\nsets are generated. If multi-modal assets already exist, they\nare used as reference information for subsequent generation\nto maintain consistency. The lifecycle of an entity is deter-\nmined by the game's progress, while the lifecycle of its\nmulti-modal assets is determined by that of the entity.\nThe perception module is responsible for preprocessing\nplayers' gaming experiences into concise plot summaries,\ngenerating information to retrieve entity IDs, and assisting\nthe status manager in updating the status of entity assets.\nBased on the current round of dialogue, the perception mod-\nule interprets structured historical dialogue data from the\nplayer's first-person perspective, understanding player in-\ntentions and actions, and outputs plot themes and narratives.\nThrough the interpretation of player behavior by the per-"}]}