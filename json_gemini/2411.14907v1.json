{"title": "DAIRHuM: A Platform for Directly Aligning AI Representations with Human Musical Judgments applied to Carnatic Music", "authors": ["Prashanth Thattai Ravikumar"], "abstract": "Quantifying and aligning music AI model representations with human behavior is an important challenge in the field of MIR. This paper presents a platform for exploring the Direct alignment between Al music model Representations and Human Musical judgments (DAIRHuM). It is designed to enable musicians and experimentalists to label similarities in a dataset of music recordings, and examine a pre-trained model's alignment with their labels using quantitative scores and visual plots. DAIRHUM is applied to analyze alignment between NSynth representations, and a rhythmic duet between two percussionists in a Carnatic quartet ensemble, an example of a genre where annotated data is scarce and assessing alignment is non-trivial. The results demonstrate significant findings on model alignment with human judgments of rhythmic harmony, while highlighting key differences in rhythm perception and music similarity judgments specific to Carnatic music. This work is among the first efforts to enable users to explore human-AI model alignment in Carnatic music and advance MIR research in Indian music while dealing with data scarcity and cultural specificity. The development of this platform provides greater accessibility to music AI tools for under-represented genres.", "sections": [{"title": "I. INTRODUCTION", "content": "Aligning AI models to human judgments is crucial for music information retrieval (MIR) tasks involving AI-generated music, as well as musical analysis. As AI systems are increasingly employed to generate, classify, and interpret music, ensuring their outputs resonate with human perception and preferences, is an important and non-trivial challenge.\nResearch on aligning AI models with human perception in music has shown promising advancements in the fields of music expectation and surprise prediction. Masclef et al. demonstrated the utility of deep generative models, including diffusion models, in estimating surprise or \"surprisal\" values that align with human enjoyment. Their work observed the Wundt effect, where moderate levels of surprise tend to increase listener enjoyment, indicating that generative models can replicate aspects of human anticipatory responses to music [1]. Similarly, Hansen et al. analyzed how high-entropy tones-those with greater uncertainty attract listeners' attention for extended periods, impacting how musical phrases are perceived in terms of completeness. This work highlights the role of anticipatory processing in the brain, showing that certain predictive models can capture the segmentation processes involved in auditory sequence perception [2]. Research in other genre-specific contexts has illustrated the challenges faced when models trained on Western data are applied to indigenous music forms with diverse renditions across performers [3]. There have also been studies on Indian music investigating the computational analysis of ragas and rhythms [4]. However, exploring the direct alignment between AI music system representations and human music judgments remains an open challenge.\nThe paper proposes to tackle this specific problem of creating an accessible platform to assess an Al model's alignment to human judgments. The platform was designed to address the specific use-case of enabling musicians and experimentalists to rate similarities in a dataset of music-tracks, upload a model, and examine the model's alignment with their ratings. The results of using the platform to analyze Carnatic percussion duets are presented, and discussed within emerging trends at the intersections of MIR, cognitive science, and cultural music studies."}, {"title": "II. CARNATIC PERCUSSION MUSIC", "content": "Carnatic music, one of the two primary classical music traditions of India, alongside Hindustani music, has complex rhythmic structures that are central to both musical and allied South Asian dance forms like Bharatanatyam. Developing a platform to demonstrate AI alignment through this genre serves as an excellent example for illustrating accessibility in Music Information Retrieval (MIR) for culturally intricate genres.\nIn a typical Carnatic quartet performance, there are four improvising performers on the stage, the vocalist, the violinist, the lead percussionist (Mridangam) and one or more secondary percussionists (Kanjira/Ghatam/Morsing). The vocalist performs the main melody and the violinist plays the accompaniment melody. The lead percussionist improvises relative to the melody (Vocal and the Violin) and the secondary percussionist typically provides accompaniment to the lead percussionist.\nDespite the rich improvisational interactions in Carnatic quartet performances, there has been limited computational work focused on modeling alignment between different improvisers in Carnatic music. Some efforts have been made to capture Indian percussion patterns, with datasets like the Mridangam Stroke Dataset [5] and the Tabla Dataset [6] which provide isolated strokes and rhythmic patterns for these percussion instruments. Computational techniques, such as those by Guedes et al. [7] have focused on generating patterns of Carnatic rhythms. There have also been studies on rhythm analysis for extracting musically meaningful rhythm related information from improvised solo recordings of Mridangam playing [8]. However, these do not address the adaptive interaction between the lead and the secondary percussionists in an ensemble performance.\nThis paper focuses on studying model alignment using the concept of rhythmic harmony between the secondary percussionist (Kanjirist), lead (Mridangist) and the rest of the quartet ensemble performance. In particular, the DAIRHuM platform will be used to evaluate the alignment of AI-generated rhythmic embeddings with human expert judgments of rhythmic harmony between the lead (Mridangist) and the secondary (Kanjirist) percussionists. The rest of the paper describes the tools offered by the system and the procedure for evaluation."}, {"title": "III. THE DAIRHUM SYSTEM", "content": "The DAIRHUM system is a Python package designed to support users in exploring and interpreting human-model alignment in musical tasks. Available on GitHub\u00b9, its source code provides tools for creating recordings and their variations, analyzing embeddings to explore semantic similarities, and evaluating model performance against human judgments. The system enables users to systematically assess and interpret alignment using the following procedure.\n\u2022 Labeling source and variations: In this step, users compile a set of audio tracks, designating some as source tracks and others as variations. The source-variation labels are assigned based on some musically meaningful attributes of each track perceived by the user, e.g., the degree of melodic harmony or rhythmic congruence or similarity.\n\u2022 Analysing embeddings: In the second stage, pre-trained models are used to generate embeddings for each music track. These embeddings are analyzed to test for statistically significant differences, such as differences in aspects like rhythm or harmony, between variations of the same musical excerpt as interpreted by the AI model.\n\u2022 Performance evaluation against human judgments: Finally, the AI model's predictions are compared against human expert ratings from the dataset to determine the degree of alignment, measuring how closely the model's outputs reflected the expert musical judgments."}, {"title": "A. Labeling source and variations", "content": "Typically, when users utilize this system, they will first gather a collection of audio tracks and assign them musically meaningful labels. Some of these audio tracks are identified as sources, and the other tracks are identified as variations of the source track. Users will assign source-variation labels based on a semantically significant attribute of the music they perceived within the tracks, e.g., degree of rhythmic or melodic harmony or any music similarity. The audio files are organized as a source-variation numbering scheme and placed in \"Recordings\u201d folder under \"Data\u201d.\nIn this specific example, the dataset contains Carnatic percussion accompaniment, featuring Mridangam and Kanjira, for three distinct melodic excerpts. Each melodic excerpt includes an original recording and five synthetically generated variations. The synthetic variations differ in the degree of rhythmic harmony in the accompaniment played on the Kanjira. This data is sourced from an empirical study on gathering musical harmony ratings from Carnatic experts for synthetically generated Kanjira tracks using a hand-crafted generative model [9].\nThe tracks are systematically labeled, with the original track for song 1 designated as \"R1-V0\" and the variations sequentially named from \"R1-V1\" to \"R1-V5\u201d. This labeling scheme was consistently applied across all three songs (R1, R2, and R3) to ensure uniformity in the analysis."}, {"title": "B. Analysing embeddings", "content": "To proceed with assessing alignments between the AI model predictions and human judgments, users will present the labeled audio tracks to pre-trained models, and gather judgments of \"sameness\" between the tracks based on the similarity between their musical embeddings. In this study, the NSynth model, a widely recognized neural network for audio signal processing was used to generate embeddings from the dataset created in the previous step.\nHere's an example of a procedure offered by the system to examine embeddings of audio tracks in the Kanjira dataset:\n\u2022 Generate embeddings: First, for each track in the dataset, the NSynth model is used to generate audio embeddings. E.g., suppose there are audio tracks labeled as R1-V0 (original recording for song 1) and R1-V1 (first variation of that song), the system generates embeddings for each using the NSynth model.\n\u2022 Experiment with similarity metrics: Users will experiment with a selection of distance metrics to compare embeddings and find the one that aligns most closely with their chosen labeling scheme or musical characteristic of interest, such as rhythmic harmony.\n\u2022 Choose statistical tests: Users choose a permutation test to evaluate whether the observed differences in embeddings are statistically significant. E.g., suppose there are audio tracks labeled as R1-V0 (original recording for song 1) and R1-V1 (first variation of that song), the permutation test compares whether R1-V0 to R1-V1 were generated from the same distribution.\n\u2022 Pairwise comparisons: Pairwise statistical comparisons are conducted between all variations of the same song to detect any statistically significant differences.\n\u2022 Assessing 'sameness': The p-values from pairwise comparisons are used to interpret whether the model labels two variations of a musical track as same or different.\nTracks with p-values < 0.05 are labeled \"distinguishable(D)\" indicating that the model interprets these tracks as being generated from different distributions.\nTracks with p-values >= 0.05 are labeled \"indistinguishable(I)\" indicating that the model interprets these tracks as being generated from the same underlying distribution.\nTo make this procedure accessible, users have the option to utilize two preset similarity metrics - the Maximum Mean Discrepancy (MMD), and the Wasserstein distance. These presets offer an accessible way to explore commonly-used similarity metrics through libraries like SciPy and other Python packages. For users seeking more control, the platform also allows detailed customization of distance metrics for optimal alignment. Adjustable settings include kernel functions (e.g., Radial Basis Function (RBF) kernel) and specific values, such as gamma ($\\gamma$) set to $\\frac{1}{\\text{median distance between embeddings}}$, which enhances sensitivity to subtle rhythmic nuances."}, {"title": "C. Performance evaluation against human judgments", "content": "The system is designed to measure the degree of alignment between a model's representations and human user representations. Specifically, it compares the notion of sameness/distinguishability derived from embeddings with the notion of sameness or distinguishability as used by humans in musical judgment. Thus, a high degree of alignment means that the model represents these concepts in the embedding space comparable to how humans would perceive them.\nThe simplest measure of alignment involves finding the number of matches between the AI-generated labels with the expert-assigned labels. The matches are used to quantify the degree of alignment through an alignment score or percentage. For example, Table III shows matches (M) and non-matches (NM) between human and model judgments, indicating an alignment score of 66.66%. For datasets with multiple sources and variations, the alignment scores for each source can be averaged to find the average alignment score for a given dataset.\nAdditionally, the system generates plots offering a qualitative overview of the distribution of ratings within the representational space. Figure 1 shows the overall trend indicating some similarities between the human and model judgments. Of particular interest are the regions of transition from high to low ratings, as these suggest plausible musical variations that induce categorical shifts in human perception or model interpretation. For instance, the plot of human ratings suggests that different musically varying accompaniments can have the same degree of harmony (3 Diamond shapes at Rating 2). On the other hand, for the same recordings, the model provides different ratings (Diamond shape at x-axis 4 is farther away). This suggests that there are qualitative differences between the expert's and the model's judgments, which could be explained through the subjectivity in the perception of certain rhythmic groupings."}, {"title": "IV. DISCUSSION", "content": "This paper presents a novel tool for Music Information Retrieval (MIR) researchers, musicians, and experimentalists to explore the direct alignment between AI-generated representations and human perceptual judgments. As a proof-of-concept, the platform was applied to analyze Carnatic percussion music, using embeddings generated by the NSynth model. Quantitative and qualitative results highlight similarities and differences in the human and the model's organization of rhythmic spaces, which presents opportunities to improve the context-sensitivity of the model.\nBy providing a framework for evaluating how Al models interpret musical content in relation to expert human assessments, this platform offers a crucial advancement in the field of Al music alignment. While alignment has been studied in other domains, such as neural mechanisms and behavioral sciences, this work introduces a new avenue by directly addressing how AI representations of music align with human judgment in the context of musical genres, particularly those outside of the Western classical or popular domains. Platforms like Brain-score [10], which allow users to assess the alignment of neural models with human perception, have influenced this work. However, to the authors' knowledge, this is the first platform designed specifically to examine how AI models represent music in culturally nuanced genres, such as Carnatic percussion music, and compares these representations against expert human ratings.\nA key strength of the proposed system lies in its simplicity, as it only requires audio tracks, embeddings generated by a pre-trained model, and human labels. This bypasses the need for genre-specific representations and relies on a more universal approach through embeddings, which can be used to analyze any musical genre with human labels. This also makes it easier to apply the system to under-represented or low-resource musical genres, such as those from indigenous or non-Western cultures, such as Carnatic percussion music, which was used as an example in this paper. In this regard, DAIRHUM offers a significant step forward in creating more inclusive, culture-driven MIR systems, and enables researchers and practitioners to assess Al models' representations across a diverse array of musical genres.\nMore specific to the context of Carnatic percussion music, the results reveal interesting differences in how AI models and human experts perceive rhythmic harmony. The clustering patterns observed between the human and model ratings suggest that while human experts group variations of similar rhythmic motifs together, the AI model tends to segregate these variations into more distinct, fine-grained categories. These findings prompt an important question concerning whether models, when trained on more culturally appropriate datasets, can evolve to organize musical variations in a manner that mirrors human experts' perceptions. If so, it would represent a significant step toward creating AI systems for MIR, that not only align with human judgment but also develop a more nuanced understanding of cultural variations in music."}, {"title": "V. CONCLUSION", "content": "This paper presents a first-of-its-kind platform for directly assessing the alignment of AI models with human judgments. By analyzing a dataset of rhythmic duets in Carnatic percussion music with embedding techniques and pre-trained models, this provides a foundation for future research in culturally specific MIR. Furthermore, the insights gained from this platform underscore the complexities involved in modeling music similarity in a way that resonates with culturally informed human perception. This platform facilitates new possibilities for MIR applications in classification, generation, and music cognition studies within Indian as well other music genres."}]}