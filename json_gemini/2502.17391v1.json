{"title": "THE EMPIRICAL IMPACT OF REDUCING SYMMETRIES ON\nTHE PERFORMANCE OF DEEP ENSEMBLES AND MOE", "authors": ["Andrei Chernov", "Oleg Novitskij"], "abstract": "Recent studies have shown that reducing symmetries in neural networks enhances lin-\near mode connectivity between networks without requiring parameter space alignment,\nleading to improved performance in linearly interpolated neural networks. However, in\npractical applications, neural network interpolation is rarely used; instead, ensembles of\nnetworks are more common. In this paper, we empirically investigate the impact of re-\nducing symmetries on the performance of deep ensembles and Mixture of Experts (MoE)\nacross five datasets. Additionally, to explore deeper linear mode connectivity, we intro-\nduce the Mixture of Interpolated Experts (MoIE). Our results show that deep ensembles\nbuilt on asymmetric neural networks achieve significantly better performance as ensemble\nsize increases compared to their symmetric counterparts. In contrast, our experiments do\nnot provide conclusive evidence on whether reducing symmetries affects both MoE and\nMOIE architectures.", "sections": [{"title": "INTRODUCTION", "content": "In the last decade, neural networks have proven to be one of the most important algorithms in the field of\nmachine learning. Despite their undeniable empirical success, many fundamental questions remain unan-\nswered. One such question concerns parameter space symmetries: for any given set of neural network\nparameters, there exist numerous 'twins' that produce exactly the same output for every input while having\ndifferent parameter values.\nThere are multiple sources of symmetry in neural network architectures. One prominent example is permu-\ntation symmetry in fully connected layers. Consider a standard multi-layer perceptron (MLP). If we swap\ntwo neurons in a hidden layer along with their incoming and outgoing weights, the network will produce\nthe exact same output. As a result, any hidden layer of size n has n! different sets of parameters that yield\nidentical outputs. Activation functions such as ReLU Nair & Hinton (2010) can also produce symmetries\nWiese et al. (2023).\nThe effects of parameter symmetries have been studied in various areas, including neuron interpretability\nGodfrey et al. (2022), optimization Neyshabur et al. (2015), and Bayesian deep learning Kurle et al. (2022).\nIn this work, we primarily focus on the impact of symmetries on model accuracy. It has been shown in Lim\net al. (2024) that eliminating redundant parameters in neural networks improves linear mode connectivity,\nthereby enhancing the performance of networks whose parameters are obtained by interpolating between\ntwo trained models. In this study, we present the Mixture of Interpolated Experts model (see Section 4.3 for\ndetails) and investigate how parameter symmetries influence its performance.\nHowever, the practical application of interpolated neural networks remains controversial, as such architec-\ntures usually do not provide a performance boost. The most common approach to leveraging multiple neural"}, {"title": "RELATED WORK", "content": ""}, {"title": "W-ASYMMETRIC MLP", "content": "Various methods for breaking parameter symmetries in neural networks have been studied, including ap-\nproaches to removing permutation symmetries Pourzanjani et al. (2017); Pittorino et al. (2022), scaling\nsymmetries Badrinarayanan et al. (2015), and sign symmetries Wiese et al. (2023). However, in most of\nthese approaches, the neural network architectures or training processes deviate from standard practices,\nmaking them difficult to apply in practice. In this work, we fully adopt the approach from Lim et al. (2024)\nto break symmetries in neural networks. This method randomly freezes a portion of the neural network's\nweights before training, keeping them unchanged throughout training (see Section 4.1 for details). Notably,\nit does not require any special modifications to the training process. Authors of Lim et al. (2024) showed\nthat breaking symmetries improves linear mode connectivity between two independently trained neural net-\nworks. In this paper, we investigate the empirical impact of reducing symmetries on the performance of\nDeep Ensembles and Mixture of Experts."}, {"title": "NEURAL NETWORK ENSEMBLES", "content": "In this study, we employ two different approaches for ensembling neural networks. The first approach,\nknown as Deep Ensembles Lakshminarayanan et al. (2017), trains k neural networks independently and\naverages their outputs to obtain the final prediction.\nThe second approach is the Mixture of Experts (MoE) Yuksel et al. (2012), which consists of two main\ncomponents: experts and a gating network. Each expert generates an output, but unlike Deep Ensembles,\nthe final prediction is obtained through a weighted average of the experts' outputs. The weights for each\nexpert are dynamically predicted by the gating network rather than being fixed. Recently, MoE architectures\nutilizing MLP models as experts have gained popularity Fedus et al. (2022) especially in NLP Du et al.\n(2022) and CV domains Puigcerver et al. (2023); Riquelme et al. (2021). In this work, we adapt MoE\narchitectures for tabular data from Chernov (2025). We cover it in detail in Section 4.2."}, {"title": "DATASETS", "content": "For our work, we selected five datasets to cover different problems:\n\u2022 Regression: California Housing Prices dataset Pace & Barry (1997).\n\u2022 Binary classification: Churn Modeling\u00b9 and Adult Income Kohavi et al. (1996).\n\u2022 Multi-class classification: MNIST Deng (2012) and Otto Group Product\u00b2.\nAppendix A.1 summarizes the key attributes of these datasets. To ensure consistency, we applied a standard-\nized preprocessing pipeline. Each dataset was split into training, validation, and testing sets with an overall\npartitioning of 64% for training, 16% for validation, and 20% for testing. Real-valued features were scaled"}, {"title": "MODELS", "content": ""}, {"title": "W-ASYMMETRIC MLP", "content": "In this paper, we fully adopt the implementation of W-Asymmetric MLP (WMLP) from Lim et al. (2024),\nwhere it was theoretically proven that this approach significantly reduces parameter symmetries. This is\nachieved by freezing a small portion of the weights, approximately O(n^{1/4}) for details see Algorithm 1\nIt is important to emphasize that in ensemble networks utilizing different WMLP models, the frozen neu-\nrons-both in value and position\u2014remain identical across all instances. For the hidden layers, we use the\nGeLU activation function from Hendrycks & Gimpel (2016) in both MLP and WMLP."}, {"title": "MIXTURE OF EXPERTS", "content": "In Chernov (2025), it was shown that MoE performs at least as well as a vanilla MLP on tabular data while\nrequiring significantly fewer parameters. In this paper, we compare the performance of MoE with MLP as\nexperts against MoE with WMLP experts.\nFrom Chernov (2025), we utilize both the vanilla MoE, where logistic regression is used as a gating neural\nnetwork, and the Gumbel Gating MoE (GG MoE), which employs the Gumbel-softmax function instead of\nthe standard Softmax activation for logistic regression. Following the original paper, we use 10 samples\nfrom the Gumbel-softmax distribution during inference."}, {"title": "MIXTURE OF INTERPOLATED EXPERTS", "content": "Since Lim et al. (2024) demonstrated that reducing symmetries improves the performance of linearly inter-\npolated neural networks, we evaluate the performance of the Mixture of Interpolated Experts (MOIE). MOIE\nuses the same gating function as MoE but, instead of computing a weighted average of the final outputs, it\nlinearly interpolates the weights of the experts to produce an output:\n\\begin{equation}\n\\hat{y} = \\text{Expert architecture} \\left(\\sum_{i=1}^{k} \\alpha_i(\\mathbf{x}) W_i(\\Theta)\\right),\n\\end{equation}"}, {"title": "EXPERIMENTS", "content": ""}, {"title": "SETUP", "content": "In this section, we describe the details of the training and evaluation procedures applied to Deep Ensembles\n(Section 5.1.1), MoE and MoIE (Section 5.1.2)."}, {"title": "DEEP ENSEMBLE", "content": "We trained models with a batch size of 256. For constructing Deep Ensembles, we trained 64 instances of\nboth the MLP and WMLP models, each initialized with a different random seed to ensure variability in the\nfree weights.\nFor WMLP, a fixed number of random weights per row, denoted as nfix, was selected and frozen in each layer.\nThese frozen weights were sampled from a N(0, I) distribution. To reduce variance in the final metrics, we\nrepeated training and evaluation 10 times independently and reported the average evaluation metrics on the\ntest sets.\nFor Deep Ensembles, we utilized MLP and WMLP blocks. Their structure consisted of an input layer that\nmapped the number of dataset features to a hidden dimension (hidden_dim), followed by two hidden layers\nof size hidden_dim \u00d7 hidden_dim, and an output layer of size hidden_dim \u00d7 out features, where out features\nwas set to 1 for regression and to the number of classes for classification. Experiments for Deep Ensembles\nwere conducted for hidden_dim values of 64, 128, and 256.\nLoss functions were selected based on the task: MSELoss for regression and CrossEntropyLoss for\nclassification, with RMSE and accuracy serving as the evaluation metrics, respectively. Optimization was\ncarried out using the AdamW optimizer with a learning rate of 1 \u00d7 10^{-3} and a weight decay of 3 \u00d7 10^{-2}.\nEach network was trained for up to 1000 epochs, with batch_size = 256 and early stopping triggered if the\nvalidation loss did not improve for 16 consecutive epochs. Training was performed in parallel on 64 CPUs.\nAfter each training iteration, we logged the training time, the number of epochs executed, and the perfor-\nmance metric for each of the 64 MLP and WMLP models. Finally, the individual models were aggregated\ninto Deep Ensembles of 2, 4, 8, 16, 32, and 64 networks. For regression tasks, ensemble predictions were\ncomputed as the mean of the individual outputs, while for classification tasks the logits were averaged and\nthe final prediction was determined via the argmax function. For each ensemble, both the ensemble perfor-\nmance metric and an interpolation metric-derived from averaging the model weights\u2014were recorded."}, {"title": "MOE AND MO\u0399\u0395", "content": "In experiments with MoE and MoIE, we used both MLP and WMLP architectures, along with the same loss\nfunctions, evaluation metrics, training procedures, and optimizer parameters as described in Section 5.1.1.\nFor these experiments, the expert hidden dimension was fixed at 64. In the WMLP architecture, the number\nof fixed weights per output unit, nfix, was set to 2 for the input layer and 3 for subsequent layers. The number\nof experts was varied among [2, 4, 8, 16, 32, 64]. We conducted experiments for all models described in\nSections 4.2 and 4.3."}, {"title": "RESULTS", "content": "Figure 1 presents the experimental results, showing the average performance improvements in test metrics\nacross different random seeds. Specifically, we report accuracy for classification tasks and RMSE for re-\ngression tasks, measuring improvement relative to the average performance of a single neural network. The\nresults are presented for each dataset and hidden dimension configuration and indicate that Deep Ensembles\nwith WMLP models improve significantly more than with MLP models and this improvement increases as\nthe ensemble size increases.\nA possible explanation for this behavior could be that WMLP deep ensembles perform worse than MLP\nensembles in terms of absolute test metric values. However, this is not the case, as demonstrated in Appendix\nA.2. Given that WMLP models retain the universal approximation property, as shown in Lim et al. (2024),\nwe believe this is a promising finding that could encourage the adoption of asymmetric neural networks in\nensembles for practical applications."}, {"title": "CONCLUSION", "content": "In this paper, we empirically demonstrated that the performance of Deep Ensembles improves significantly\nwith increasing ensemble size when using W-Asymmetric MLP models compared to vanilla MLP models.\nThis result may serve as a first step toward understanding the practical impact of reducing symmetry in\nneural networks.\nHowever, based on our experiments, we cannot conclude that W-Asymmetric MLP improves the perfor-\nmance of either the Mixture of Experts (MoE) or the Mixture of Interpolated Experts (MoIE) models. As\ndiscussed in Section 5.2, the experimental setup for MoE should be refined in future work"}]}