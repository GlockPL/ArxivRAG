{"title": "Robust Multiple Description Neural Video Codec with Masked Transformer for Dynamic and Noisy Networks", "authors": ["Xinyue Hu", "Wei Ye", "Jiaxiang Tang", "Eman Ramadan", "Zhi-Li Zhang"], "abstract": "Multiple Description Coding (MDC) is a promising error-resilient source coding method that is particularly suitable for dynamic networks with multiple (yet noisy and unreliable) paths. However, conventional MDC video codecs suffer from cumbersome architectures, poor scalability, limited loss resilience, and lower compression efficiency. As a result, MDC has never been widely adopted. Inspired by the potential of neural video codecs, this paper rethinks MDC design. We propose a novel MDC video codec, NeuralMDC, demonstrating how bidirectional transformers trained for masked token prediction can vastly simplify the design of MDC video codec. To compress a video, NeuralMDC starts by tokenizing each frame into its latent representation and then splits the latent tokens to create multiple descriptions containing correlated information. Instead of using motion prediction and warping operations, NeuralMDC trains a bidirectional masked transformer to model the spatial-temporal dependencies of latent representations and predict the distribution of the current representation based on the past. The predicted distribution is used to independently entropy code each description and infer any potentially lost tokens. Extensive experiments demonstrate NeuralMDC achieves state-of-the-art loss resilience with minimal sacrifices in compression efficiency, significantly outperforming the best existing residual-coding-based error-resilient neural video codec.", "sections": [{"title": "Introduction", "content": "Video delivery is integral to many popular Internet applications and has dominated the Internet traffic. Emerging 5G networks are designed to enable new applications such as augmented/virtual/extended reality, tele-operated robots, and remote driving - all of which rely on streaming pre-recorded or real-time videos. 5G networks are capable of delivering beyond 1 Gbps of peak throughput by leveraging multiple radio signal paths and/or multiple radio channels (Rochman et al. 2023; Li et al. 2023a; Ye et al. 2024). However, 5G throughput is known to suffer from wild fluctuations due to noisy radio environments and dynamic changes in availability in MIMO (multi-input, multi-output) layers or radio channel conditions (Narayanan et al. 2020a,b, 2021; Ye et al. 2024).\nRecent studies (Narayanan et al. 2021; Ramadan et al. 2021) reveal that video streaming applications under-perform in 5G networks. Despite achieving high bitrates in 5G networks, existing streaming systems experience significantly higher stall times due to delays in receiving the necessary packets for successful video decoding, since conventional video codecs are highly sensitive to packet loss\u00b9. While forward error correction (FEC) (Wicker and Bhargava 1999; Badr et al. 2017) and retransmission (RTX) are implemented to mitigate packet loss, their effectiveness is limited. This limitation arises from difficulties in determining optimal FEC redundancy parameters for dynamic networks in advance, and the significant delays introduced by RTX. Furthermore, bitrate adaptation algorithms (Hu et al. 2023) have been utilized to adjust codec bitrates in response to throughput fluctuations, yet the significant variability in 5G network throughput poses substantial challenges to their accuracy. Moreover, the complexity of distributing packets across heterogeneous network paths adds another layer of complication to video streaming in 5G networks. These observations raise the question: Can we design a \"proactive\" video codec that is inherently robust to noisy networks and that can more effectively utilize multiple network paths, rather than relying solely on the streaming techniques previously mentioned?\nConventional video codecs such as AVC and HEVC as well as the more recent neural codecs (Lu et al. 2019; Mentzer et al. 2022; Lin et al. 2022) compress a video into a single bitstream for network delivery. In contrast, a Multiple Description Coding (MDC) (Kazemi, Shirmohammadi, and Sadeghi 2014) video codec compresses a video into multiple independently-decodable and mutually-refinable streams (also called descriptions). Hence, MDC provides a promising alternative paradigm for video delivery over noisy and dynamic networks such as 5G networks, as it makes it possible to dynamically exploit the availability of multiple noisy radio paths or channels. For instance, each video stream can be transmitted separately over different network paths/channels\nIn this study, packet loss refers to both packets dropped in transit and packets not received by the decoding deadline. Video streaming can experience a loss rate ranging from 0% to 80% (Cheng et al. 2024)."}, {"title": "", "content": "using various networking mechanisms. If one or more path/channel suffer significant impairments or become unavailable, as long as one or more (even partial) descriptions are received, the video can be successfully decoded, albeit with lower fidelity.\nDespite such advantages, designing an efficient MDC video codec is nontrivial. Existing MDC video codecs (Franchi et al. 2005; Le et al. 2023) are largely extensions of AVC/HEVC. They suffer from cumbersome architectures, requiring different side decoders for each description and a central decoder for combined descriptions. This results in poor scalability when generating more than 2 MDC streams. They also exhibit limited loss resilience due to the de-correlated nature of DCT transforms and extremely complicated encoder-decoder state synchronization. To improve loss resilience, existing MDC techniques often oversample or duplicate source information, resulting in lower compression efficiency. Consequently, MDC codecs have never been widely adopted in practice.\nInspired by the rapid advances in neural video compression, which outperforms AVC and HEVC in rate distortion performance (Lu et al. 2019; Mentzer et al. 2022; Lin et al. 2022), in this paper, we rethink the design of MDC through the lens of neural codecs. We find that bidirectional transformers (Chang et al. 2022) trained for masked token prediction simplify and enhance MDC design. Our neural MDC codec compresses videos in three steps (see Fig. 1). First, we use a lossy AutoEncoder transform to independently map each frame $x_t$ to a quantized representation $y_t$. Second, we split $Y_t$ into multiple non-overlapping parts to form multiple descriptions. Third, a masked transformer extracts spatial and temporal redundancies to model the distributions of $Y_t$ conditioned on previous representations. We use these predicted distributions and entropy coding to compress each description independently into a bitstream. At the receiver side, the received descriptions are merged into $Y_t$ and decoded to reconstruct the frame $x_t$. If any part of $y_t$ is lost during transmission or fails to arrive before the decoding deadline, the predicted distributions infer the missing part by leveraging spatial-temporal dependencies among representations.\nTo the best of our knowledge, this paper is the first to utilize neural compression to design MDC video codec, making video compression more robust and adaptive to network dynamics. Our NeuralMDC video codec is elegantly simple yet powerful, leveraging a masked transformer to capture spatial-temporal correlations and leverage arbitrary relationships between frames. Our approach avoids complex state synchronization or warping operations, achieving state-of-the-art loss resilience performance and outperforming the best existing loss-resilient neural video codec, Grace (Cheng et al. 2024), by 2 to 8 times in terms of PSNR and MS-SSIM of reconstructed videos with packet losses. Additionally, our NeuralMDC achieves 76.88% bitrate savings over Grace. It is particularly suited for 5G networks where one can intelligently and adaptively leverage multiple"}, {"title": "", "content": "radio paths/channels when available, while combating the challenges posed by highly noisy and dynamic radio environments."}, {"title": "Related Work", "content": "MDC codecs. The earliest works (Fleming and Effros 1999) on MDC design focused on developing various quantizers to ensure each description contains the full source information at different levels of coarseness. This line of research primarily focused on rate-distortion optimization of MDC through theoretical analysis and was mainly pursued within the information theory community. Later, the design of MDC shifted towards splitting source information into multiple descriptions, each containing a portion of the source data. Depending on the type of source information used, descriptions are generated by partitioning either the pixels (Shirani 2006; Yap\u0131c\u0131 et al. 2008) in the spatial domain, or frames (Tillo and Olmo 2004; Radulovic et al. 2009) in the temporal domain, or transformed data (Wang et al. 2001; Conci and De Natale 2007) in the frequency domain. In recent years, neural networks have been used to enhance MDC design. Techniques such as CNNs (Zhao et al. 2018), AutoEncoders (Zhao et al. 2022), and Implicit Neural Representations (Le, Pic, and Antonini 2023) have been utilized to create MDC image codecs. However, little attention has been given to MDC video codecs, with the only work (Hu et al. 2021) which proposed a GNN-based super-resolution method to improve the reconstruction quality of a traditional MDC video codec.\nNeural video codecs. Numerous research papers have emerged on neural video codecs. The authors in (Lu et al. 2019) introduce the first end-to-end deep learning model that jointly optimizes all components of the video codec. This model uses learning-based optical flow for motion estimation and frame reconstruction. Subsequent work focuses on simplifying module complexity and training schemes, as well as improving the accuracy of motion estimation. For example, (Lin et al. 2022) decomposes the motion information to better model it; (Agustsson et al. 2020) proposes the generalized warping operator and scale-space flow; and (Hu, Lu, and Xu 2021) utilizes the feature space video coding network. At the same time, (Li, Li, and Lu 2021a) proposes a context-based conditional coding framework, aiming to achieve higher compression rates than the aforementioned predictive coding framework. Following it, (Mentzer et al. 2022) uses Transformer to predict the distribution of future frames. Inspired by the success of implicit neural representations, another research direction (Chen et al. 2021; Kwan et al. 2024) represents videos as neural networks with frame indices as inputs, significantly improving decoding speed and video quality."}, {"title": "Method", "content": "Overview\nIn general, the design of an MDC video codec involves three main challenges: 1) splitting the source informa-"}, {"title": "", "content": "tion into multiple descriptions that contain correlated (i.e., redundant) information, 2) exploiting redundancy among these descriptions to estimate any potentially lost from those received; and 3) handling error propagation due to the mismatch of encoder and decoder states.\nWe use neural compression techniques to design MDC and address the above challenges judiciously. A high-level overview of our approach is shown in Fig. 1. We generate multiple descriptions in the latent domain using a CNN-based AutoEncoder to tokenize each frame $x_t$ into a quantized latent representation $y_t$. Unlike DCT transforms, which de-correlates the coefficients, AutoEncoder transforms retain spatial-temporal correlations in the latent domain (He et al. 2022; Li et al. 2023b). Thus, we split representation $y_t$ into different descriptions containing correlated information.\nTo transmit each description with fewer bits as well as to exploit temporal and spatial correlations among descriptions, we use a bidirectional masked transformer to parameterize the distribution of representation $P(Y_t|Y_{t-1}, Y_{t-2})$. We then use the predicted distribution and entropy coding (EC) to independently convert each description to a bitstream with $\\approx \\sum_{i} - \\log_2 P(y_i)$ bits (Minnen and Singh 2020). If any descriptions are lost, we use the predicted distribution to infer the lost parts. Better distribution prediction results in fewer bits for $y_t$ and more accurate loss inference. Since each description is entropy encoded independently, each one is independently decodable. Any combination of received descriptions enhances the decoded latent representation's accuracy and improves the decoded frame's visual quality. By avoiding the use of motion vector or warping operations and limiting conditioning to the previous two representations, the impact of temporal error propaga-"}, {"title": "", "content": "tion caused by loss is confined to a few local frames.\nAutoEncoder Transform\nWe use an existing CNN-based ELIC AutoEncoder (He et al. 2022) to independently convert each input frame from the pixel domain to the latent domain. Given an $H \\times W$ frame $x_t$, the CNN-based image encoder $E$ maps it to a latent representation of shape $(h, w,c)$, where $h, w$ are $16\\times$ smaller than the input resolution and $c$ is set to be 192 throughout the paper. Following existing works (Lu et al. 2019; Mentzer et al. 2022), we quantize the latent representation element-wise using scalar quantization and get the quantized representation $Y_t = [E(x_t)]_Q$. From $y_t$, the decoder $D$ reconstructs the input frame $\\hat{x}_t = D(y_t)$.\nSource Information Splitting\nThe source information considered by NeuralMDC codec is the latent representation of each frame. The representation generated by AutoEncoder transform exhibits spatial-temporal correlations (Li et al. 2023b; Yu et al. 2023) and an information compaction property (He et al. 2022): a few channels exhibit significantly higher average energy (see channel map visualization example in Fig. 2). Since channels with higher energy are more important, we split the latent representation by masking out portions of channel maps to ensure resilience to description loss. Instead of treating each of the $h \\times W \\times c$ elements in the representation as a token, we group each $1 \\times 1 \\times c$ column into a token and split these hw tokens into different descriptions. This approach maintains a similar energy level in each description and avoids creating infeasibly long sequences for transformers. Fig. 3 shows an example of forming 4 descriptions. We split the latent"}, {"title": "", "content": "representation by masking out it with a special learnable mask token in an interleaving way\u00b2, forming multiple masked latent representations whose combination equals the original representation.\nNote that we do not utilize other types of source information, such as motion, optical flow, or residuals (Lu et al. 2019; Xiang, Tian, and Zhang 2022; Li, Li, and Lu 2023), as they carry differently important source information and lack strong correlations with each other. Consequently, the loss of one type (e.g., motion) cannot be efficiently estimated from the received other types (e.g., residual). The distinct impacts of loss on motion vectors and residuals on reconstructed video quality are shown in Fig. 4. It is evident that motion information is more critical than residuals, and the loss of motion cannot be effectively compensated for, even if the residuals are fully received. Therefore, our NeuralMDC video codec exclusively uses the latent representation as source information, letting the transformer extract diverse contexts from representations for compression.\nMasked Spatial-Temporal Transformer Entropy Coding\nWe independently entropy encode each description into an MDC stream, allowing each stream to be decoded independently. To reduce the bit length of each stream, we propose a masked spatial-temporal transformer entropy model. Given a sequence of video frames $\\{x_t\\}_{t=1}^T$ and the corresponding latent representation sequence $\\{Y_t\\}_{t=1}^T$, where each representation is split into S descriptions\n\u00b2Random splitting also works as long as it is reversible at the receiver side. We use interleaving splitting for its simplicity and similar performance to random splitting."}, {"title": "", "content": "Y_t = [Y_{t,s}]_{s=1}^S$, we use the masked transformer to predict $P{Y_{t,s}|Y_{t-1}, Y_{t-2}}$ and entropy code each description $Y_{t,s}$ to a bitstream. The transformer runs independently on each description, trading reduced spatial context for parallel execution. To compress the full video, we simply apply this procedure iteratively, letting the transformer predict the conditional distribution for each latent representation and padding with zeros when predicting distributions for the first two frames.\nEntropy Model Fig. 5 shows the overview of our masked transformer entropy model. It extends MaskGIT-like transformer (Chang et al. 2022; Mentzer, Agustson, and Tschannen 2023) to extract both spatial and temporal dependencies among latent tokens. Let $y_t = [y_i]_1^N$ denote the latent tokens of the current frame, where N is the length of the reshaped token matrix, and M= $[m_i]_1^N$ is the corresponding binary mask. To simulate an arbitrary number of descriptions during training, we randomly sample a subset (from 0% to 100%) of tokens and replace them with a special learnable mask token [M]. $m_i$ = 1 indicates that the token $y_i$ is replaced with [M].\nDenote $Y_{t.M}$ as the masked representation after applying mask M to $y_t$. We train the masked transformer to minimize the cross entropy of the predicted distributions P with respect to the true distribution Q, i.e., the average bit rate:\nR(y_t) = E_{yt \\sim Q}[\\sum_{m_i=1}- \\log_2P(y_i|Y_{t,M}, Y_{t-1}, Y_{t-2})] \\qquad (1)\nHere, previous representations $Y_{t-1}, Y_{t-2}$, and the current masked representation $Y_{t.M}$ provide both temporal and spatial context for predicting the distribution of masked tokens. We model this conditional distribution through a mixture of Gaussians (GMM) with $N_M$ = 3 mixtures, each parameterized by a mean \u03bc, scale \u03c3, and weight w.\nIterative Encoding and Decoding One approach is to use the above masked transformer entropy model"}, {"title": "", "content": "to encode and decode a description $y_{t,s}$ in one step by masking out all latent tokens in the description. However, this is inefficient because the spatial context information in the description is totally ignored and hence increases the bitrate cost. Instead, we apply the entropy model L times following the QLDS masking schedule (Mentzer, Agustson, and Tschannen 2023) $\\{M_1,..., M_L\\}$, where $M_i[j]$ = 1 indicates that the j-th token is predicted and uncovered at step i and the number of tokens uncovered monotonically increases during iteration. At the first iteration, we start with all tokens in $y_{t,s}$ are [M], then we only entropy code the tokens corresponding to $M_1$, uncover them as input for the next iteration. The process repeats until all tokens in $y_{t,s}$ have been entropy coded and uncovered.\nNote that, unlike VCT (Mentzer et al. 2022), which uses transformers to model the distribution autoregressively and sequentially, the masked bi-directional transformer predicts the distribution with richer contexts by attending to all tokens in the provided representations. To mitigate the impact of temporal error propagation caused by corrupted previous frames, in contrast to MIMT (Xiang, Tian, and Zhang 2022), NeuralMDC utilizes only the two most correlated latent representations from the past. Furthermore, to ensure each description is independently decodable and robust to potential loss, NeuralMDC avoids conditioning on any side information, such as hyper-prior and optical flow used by MIMT, since their loss cannot be efficiently estimated.\nLoss and Training Process\nWe decompose the training into three stages. In stage I, we train the per-frame encoder and decoder by minimizing the rate-distortion trade-off r(y) + \u03bbd(x,x):\n$\\mathcal{L}_1 = \\mathbb{E}_{x \\sim p_x, \\mu \\sim U_{\\pm 0.5}}[- \\log p(\\hat{y} + \\mu) + \\lambda MSE(x,\\hat{x})]$ \\qquad (2)\nwhere $x \\sim p_x$ are frames drawn from the training set, \u0177 refers to the unquantized representation, and we use additive i.i.d. noise from a uniform distriubiton in [-0.5,0.5] to simulate quantization during training (Theis et al. 2022). We use mean squared error (MSE) as the distortion loss and employ the mean-scale hyperprior (Minnen, Ball\u00e9, and Toderici 2018) approach to estimate p (i.e., bitrate) temporally, which we discard in later stages. To get gradients through the quantization operation, we rely on straight-through estimation (STE) (Minnen and Singh 2020; Theis et al. 2022). After stage I, we obtain the lossy ELIC encoder and decoder transformers reaching nearly any desired distortion MSE(x, \u00ee) by varying how large the range of each element in y is. Basically, the wider the value range of y, the higher the quality of frame reconstruction and the larger the bitrate tends to be.\nIn stage II, we train the masked temporal transformer to obtain p and only minimize the bitrate:\n$\\mathcal{L}_{II} = \\mathbb{E}_{(x_1, x_2, x_3) \\sim P_{X_{1..3}}, \\mu \\sim U}[\n    \\sum_{M[i]=1} - \\log_2 P(y_3 + \\mu | Y_{3,M}, Y_1, Y_2)\\] \\qquad (3)"}, {"title": "", "content": "where $(X_1,X_2,X_3) \\sim P_{X_{1..3}}$ are three adjacent video frames. Given the representation y, we randomly sample a mask M, where 0-100% of the entries are 1. The corresponding entries in y are masked, which means we replace them with a special mask token, which is a learned c-dimensional vector. Together with the previous representation $Y_1, Y_2$ the resulting masked representation $y_{3,M}$, which simulates the description after any arbitrary source splitting, are fed to the masked temporal transformer, which predicts the distribution of the tokens. We assume the distribution of each token is a mixture of Gaussian and let the transformer predict the mean, scale, and weight per token. When computing the bitrate loss, we only consider the distributions corresponding to the masked tokens.\nIn stage III, we finetune the ELIC Autoencoder and the masked transformer jointly by replacing the mean-sale hyperprior entropy model with the masked transformer entropy model (i.e., replacing p() in Eq. 2 with Eq. 3).\nInference of Lost Tokens\nThe inference of lost latent tokens involves predicting and sampling. After entropy decoding the received streams and merging available tokens, to predict the lost tokens caused by transmission loss, we apply the masked transformer to predict the probabilities, denoted as $p(y_t,M|Y_{t,M},Y_{t-1}, Y_{t-2})$, for all the masked tokens in parallel. Here, the reconstructed tokens of current frame $Y_{t, M}$ and previous representations provide temporal and spatial contexts for the transformer to predict the distributions of the missing tokens. At each masked location j, we sample a token y based on its maximal probabilities\n$\\tilde{y} = \\underset{y}{argmax} \\quad p(y_t,M|Y_{t,M},Y_{t-1}, Y_{t-2})\\qquad (4)"}, {"title": "Experiments", "content": "Datasets. We train NeuralMDC on the Vimeo-90K dataset (Xue et al. 2019). During training, we randomly sample 256 \u00d7 256 crops from the original frames. The training batches are made up of randomly selected triplets of adjacent frames. We evaluate on two common benchmark datasets: UVG (Mercat, Viitanen, and Vanne 2020) and MCL-JCV (Wang et al. 2016), both containing raw videos with a resolution of 1920 \u00d7 1080.\nBaselines. We compare NeuralMDC against the following video codecs for evaluating loss resilience and rate-distortion performance. The implementation details of NeuralMDC are elaborated in the Appendix. For loss resilience evaluation, we randomly corrupt bitstreams of H.264 using the FFmpeg x264 codec based on the bitstream corruption framework in (Liu et al. 2024). We run Grace (Cheng et al. 2024), a loss-resilient residual-coding video codec, using their public checkpoints. Grace is an extension of DVC and trains a variational autoencoder where the latent representation is sampled from a specific loss distribution. We also run DCVC-DC (Li,"}, {"title": "Results", "content": "Loss Resilience Performance\nIn the real world, video streaming over communication networks can experience packet loss (referring to both packets dropped\u00b3 in transit and those not received by the decoding deadline) ranging from 0% to over 80% (Cheng et al. 2024). In this section, we compare NeuralMDC with other video codecs that operate without retransmission and with DCVC-DC, which operates with retransmission.\nBaselines without Retransimission Fig. 6 compares the decoded video quality of NeuralMDC with the baselines under varying loss rates on the UVG and MCL-JCV datasets. For a fair comparison, we ensure that NeuralMDC and all baselines have the similar bpp performance and none of them retransmit lost packets. We see that our NeuralMDC outperforms all the baselines in both PSNR and MS-SSIM. The loss resilience performance of our NeuralMDC surpasses the best baseline by 1.78 to 8.66 times. Although DCVC-DC achieves higher visual quality in the absence of packet loss, it is highly sensitive to packet loss, causing the reconstructed\n\u00b3See Appendix Fig. 12 for packet drop rates of a real-world 5G trace example."}, {"title": "", "content": "video quality to degrade more rapidly compared to NeuralMDC. This verifies the effectiveness of our masked transformer entropy model in exploiting the spatial and temporal redundancies in received descriptions to infer the lost tokens. Since both Grace and DCVC-DC utilize motion information and DCVC-DC propagates extracted features along frames, their poor performance indicates that lost motion cannot be efficiently estimated and the temporal error caused by encoder-decoder state mismatch propagates due to feature propagation. The visualization of reconstruction samples under 50% loss rate is shown in Appendix Fig. 13.\nBaselines with Retransimission Since DCVC-DC has a better rate-distortion performance than NeuralMDC, we further compare NeuralMDC with DCVC-DV that operates additionally with the retransmission (RTX) scheme. In this experiment, both NeuralMDC and DCVC-DC are evaluated under the same network bandwidth and transmission time. This means that as the loss ratio increases, the effective bpp of DCVC-DC with RTX decreases. We also consider the impact of network round-trip time (RTT). Typically, modern transport protocols wait 1.5 times the RTT to retransmit a lost packet (Stevens 1997). Consequently, to transmit videos within the same time, a higher RTT further reduces the effective bpp of DCVC-DC with RTX. Fig. 7 shows that NeuralMDC outperforms DCVC-DC with RTX when RTT exceeds 10 ms. As RTT increases, the loss resilience performance advantage of NeuralMDC over DCVC-DC further improves. This highlights the superior performance of NeuralMDC in achieving high visual quality and low latency video streaming, even when compared to state-of-the-art neural codecs protected by the RTX scheme.\n\u2074Here the retransmission scheme refers to both the retransmission of packets dropped in transit and the reinjection of packets from low paths to fast paths (Zheng et al. 2021)."}, {"title": "Rate-Distortion Performance", "content": "Fig. 8 shows the rate-distortion performance in scenarios where no packet loss occurs. In this case, we set the number of descriptions to 1 and evaluate the overhead of source splitting later. Except for DCVC-DC, our NeuralMDC outperforms VCT and other baselines. This demonstrates the effectiveness of the bidirectional transformer in extracting richer contexts to improve compression efficiency. Our NeuralMDC has a worse rate-distortion trade-off compared to DCVC-DC because DCVC-DC additionally utilizes motion vectors to extract more contexts and propagates extracted features along frames. However, as shown above, this makes DCVC-DC more sensitive to packet loss. We note that Grace sacrifices a significant amount of compression efficiency to make DVC robust to packet loss.\nNeuralMDC BPP Overhead\nSince NeuralMDC splits the source information into distinct descriptions, the bit costs increase because the correlation of source information within each description decreases, thereby reducing compression efficiency. Fig. 9 shows the bpp overhead caused by source splitting with respect to the anchor of a single description. As the number of descriptions increases, the bpp overhead also increases. However, our NeuralMDC codec exhibits an upper limit on the bpp overhead increase. This is because, as the number of descriptions grows, the previous frame information primarily provides the context for compression, even though the decreased correlation within the current frame information offers little context."}, {"title": "Runtime", "content": "We conduct a detailed breakdown of the time costs associated with NeuralMDC. The video codec is tested with 1080p videos. The masked transformer entropy model is applied 12 times to iteratively encode and decode"}, {"title": "", "content": "each description, following the QLDS masking schedules. We run the transformer in parallel for 4 descriptions. Fig. 10 shows the runtime of the encoding and decoding processes. Note that running the transformer at 1080p once only takes about 27.29 ms, but we run it 12 times for iteratively entropy encoding and decoding.\nFig. 11 shows the inference time for predicting lost tokens due to packet loss. We only report the runtime of inferring tokens from the predicted representation distributions, as we can reuse the distribution prediction results in the entropy decoding stage. As the packet loss ratio increases, the inference time also increases. This is reasonable because higher loss means more tokens needs to be inferred from the predicted distribution, which requires more computational resources and time."}, {"title": "Conclusions", "content": "We have designed a novel error-resilient source coding method, NeuralMDC, for video delivery over dynamic and noisy networks. It is designed in particular to take advantage of dynamically available, albeit noisy, multiple network paths or radio channels that have become prevalent in today's high-speed networks such as 5G. NeuralMDC first tokenizes each input frame into its latent representation. It then splits the tokens on the channel-axis to evenly distribute energy among different descriptions for redundancy allocation. NeuralMDC finally trains a spatial-temporal masked transformer to capture the spatial-temporal correlations of tokens. Furthermore, NeuralMDC performs token entropy coding based on distributions derived from the trained transformer to achieve efficient compression. For error-resilient decoding, NeuralMDC infers missing tokens using received current and past tokens and reconstructs frames using both received and inferred tokens. We show that NeuralMDC exhibits a superior 2 to 8 times improvement in loss resilience while achieving compression efficiency comparable to the state-of-the-art."}]}