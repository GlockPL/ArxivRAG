{"title": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation", "authors": ["Qiliang Chen", "Alireza (Sepehr) Ilami", "Nunzio Lore", "Babak Heydari"], "abstract": "This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior. Through validation in iterative games, including the prisoner's dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations. The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates. This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings.", "sections": [{"title": "1 Introduction", "content": "Interactions within social and sociotechnical systems are frequently characterized by a delicate balance of cooperation and competition, often leading to complex social dilemmas [1, 2, 3, 4, 5]. The challenge of governing these systems effectively hinges on the ability to foster and sustain prosocial behavior, thereby enhancing both system-level efficiency and fairness. The importance"}, {"title": "2 Related Work", "content": "Mechanisms to Promote Cooperation under Social Dilemma A significant body of literature has examined mechanisms that influence cooperation in strategic interactions involving social dilemmas. Reputation and reciprocity are well-established factors, with interventions that establish and reinforce them consistently promoting cooperative behavior [21, 22]. Punishment and reward systems have also proven effective, deterring defection while incentivizing prosocial actions [23, 24]. Additionally, the structure of interaction networks plays a crucial role, either by identifying network characteristics that support cooperation [25, 26] or by examining how changes in the network structure over time can influence cooperation [27, 28].\nWhile these factors are effective in fostering cooperation, many require extended periods of interaction to yield results (e.g., promoting cooperation through the emergence of new norms or conventions) or rely on interventions that contradict agents' autonomy (e.g., top-down approaches that alter network structure [14, 29]). Another influential factor is the extent to which agents can observe others' behaviors, and recall historical information [30, 31]. In this work, we focus on this factor as our intervention mechanism, where the RL agent dynamically modulates the level of information available to LLM agents\u2014specifically through adjustments in observation and recall, in order to increase the overall cooperation rate.\nLLMs and Strategic Decision Making Recent studies reveal that large language models (LLMs) are capable of handling basic economic and game-theoretic scenarios [32, 33, 34, 35], yet their decision-making processes are often unclear and they seem to struggle with belief refinement [36]. When these models are evaluated as substitutes for human agents, the results frequently diverge from the predictions of both rational choice theory and behavioral economics, raising questions about their cognitive fidelity [37, 38, 39, 40]. This has sparked a debate on the most appropriate methods for evaluating these models [41, 42], particularly in terms of their alignment with human-like reasoning. Nonetheless, there is growing optimism about their potential to simulate human thought and behavior, as ongoing advancements continue to enhance their ability to replicate complex cognitive processes. [43, 44, 45, 40, 46].\nLLMs and Multi-Agent Systems Research on LLM-empowered multi-agent systems can be broadly categorized into two primary domains: simulations and implementations. Implementations refer to algorithms or platforms that leverage the interactive capabilities of LLMs to generate concrete, end-to-end solutions or finished products [47, 48, 49]. These systems focus on harnessing the synergistic interactions among multiple LLMs to achieve specific, often practical, outcomes. In contrast, simulations aim to explore and demonstrate the potential of LLM-powered agents to emulate"}, {"title": "3 Methods", "content": "3.1 Overview of the general framework\nFigure 1 illustrates the overall framework used in this paper. We begin by developing interactive strategic LLM agents (SLAs) and a dynamic prompt structure, which is then employed by a reinforcement learning (RL) agent to encourage prosocial behavior in social dilemma games, such as the prisoner's dilemma (See the SI document, section A, for more details). A key step in this process is what we call micro-level validation, where we design the prompting template to ensure that agents' cooperative behavior shifts appropriately in response to governance signals intended for use by the RL agent. The SLAs are then placed as nodes of a network, where pairs of neighbors engage in a multi-period strategic game (prisoner's dilemma (PD)). In each period, they can choose to cooperate or defect in interactions with their network neighbors, with each SLA capable of selecting different actions for different neighbors at any given time.\nIn each period, the RL agent sends a vector of signals to the network (one per SLA), aiming to maximize the discounted sum of scores for all agents, which, in the case of the PD game, requires a high rate of cooperation across the network. To preserve the autonomy of the SLAs, the steering signals only modify the level of information each SLA has about the past cooperative behavior of other agents, with variations in aggregation levels and conditions (e.g., average of past mutual history). To assess whether RL intervention has increased the rate of cooperation and overall average pay-off, we compare these trends against a set of benchmarks. We now go over the key parts of the framework.\n3.2 Strategic LLM Agents (SLA) and Micro-level Validation\nThe Strategic LLM Agent (SLA) is central to our framework, designed either as a digital proxy for human agents in complex multi-agent scenarios or for task-specific multi-agent LLMs. We restrict SLA interactions to pairwise social dilemma games, such as prisoners' dilemma. In every period, SLAs receive messages describing the nature of their pairwise strategic games, conveyed solely through the payoff matrix and objectives, omitting explicit references to game names (e.g., Prisoner's"}, {"title": "3.3 Pro-social Promoting Agent (PPA)", "content": "The pro-social promoting agent (PPA) serves as the governing entity within our framework and is trained using reinforcement learning. Its role is to enhance network-level social welfare (the sum of all SLA scores) by dynamically adjusting the information levels provided to each agent.\nThe PPA is a standard Actor-Critic RL agent, whose environment is modeled as a Partially Observable Markov Decision Process (POMDP) (See the SI document, section C, for more details). The PPA strategically tailors the information disclosed to each SLA, providing different signals to different SLAs during each interaction period. The action set of the RL includes the four tiers of information access: (1) no information about past interactions; (2) the last action pair between the SLA and its co-players; (3) the last action pair combined with the long-term cooperation ratios of both the SLA and its co-players; and (4) the last action pair along with the long-term cooperation ratios of the SLA and all neighboring agents. The scenario of having no information is often unrealistic, as agents are generally expected to recall the history of their last interaction with other players. Therefore, in our experiments, we limit the use of this action. The RL agent is rewarded based on the discounted sum of social welfare of the network (the sum of all SLA pay-offs), which for the case of the Prisoners' Dilemma game, is expected to be highly correlated with the overall rate of cooperation in the network."}, {"title": "4 Experiment results and analysis", "content": "In this section, we begin by describing the settings of our experimental environment. We will then discuss the outcomes of micro-level validation for LLM agents, including how we crafted the prompts for subsequent experiments. Lastly, we will present the results of system governance conducted by the RL manager across multiple LLM agents and analyze the evolution of the environment throughout the experiments.\n4.1 Environment settings\nWe outline the key features and settings of the environment used to evaluate the proposed framework. The strategic interactions among agents are simulated using the prisoner's dilemma, a choice we make to test our framework under a high level of social dilemma. The game's payoff matrix is as follows: mutual cooperation (CC) yields 3 points for each player, unilateral cooperation with defection by"}, {"title": "4.2 Results from micro-level validation of LLM agent", "content": "LLM agents possess the capability to make decisions that are akin to human decisions across various tasks. To ensure that the experimental results in our project are both reasonable and meaningful, we conducted several micro-level validations of LLM agent behavior to address the questions outlined in the previous section. Based on these findings, we crafted prompts incorporating different types of information for subsequent experiments.\nFirst, we design prompts that describe the prisoner's dilemma and set objectives for the LLM agents to guide their decisions. While it remains uncertain whether LLMs fully understand different games, we avoid explicitly mentioning \"prisoner's dilemma\" to minimize bias in the LLM's behavior. Instead, we present the payoff matrix without labeling the game. For the objective, we instruct the SLA to maximize its rewards while noting that it may interact with the same co-player multiple times. This is intended to encourage SLAs to take actions that balance short-term and long-term payoffs, much like how humans make strategic decisions based partly on the likelihood of future interactions. Additionally, we incorporate chain-of-thought (CoT) prompting [56] to encourage more strategic reasoning. By default, SLA agents always have access to the most recent action pairs from their interactions with co-players. Figure 2 shows the prompts used when SLA agents have access to the latest action pairs.\nAt the initial step of the game, because there is no history of the last action pairs, we will first delete the red part. Besides, we will add \"Consider the proposed scenario and act as if you were taking part in it.\" at the beginning of the prompt as the no prior information prompt. Through extensive"}, {"title": "4.3 PPA Effect on System Performance and Cooperation Rate", "content": "In the previous section, we designed prompts to integrate game structure and information access. Building on this, SLAs are then constructed, and the PPA learns to modulate information access for each SLA, aiming to optimize the social welfare of all agents. It is important to note that, in many cases, the PPA can only modulate or expose information from more distant interactions, while the history of recent interactions between an SLA and its co-player remains is expected to be retained in most applications. Thus, we used the no information action, only for the first period within each round, when SLAs have no prior interactions with each other. We established several baselines for comparison, each using specific information within the same round. We conducted ten rounds for each method (more rounds are expected to be added), and the comparison of results can be found in Table 3.\nHere We present the normalized social welfare and the system's cooperation rate, averaged across all rounds, with each round involving a different random network instance. Additionally, we report both the time-averaged welfare and cooperation rate across rounds, as well as the outcomes from the final step of the game. We can make the following observations:"}, {"title": "5 Conclusion", "content": "In this paper, we propose a framework comprising multiple Strategic LLM Agents (SLAs) positioned in a random network, interacting with their neighbors, and a Pro-social Promoting Agent (PPA) that dynamically provides information to SLAs to foster pro-social behavior and maximize social welfare. Each SLA receives prompts, including descriptions of pairwise strategic games, objectives, and additional information from the PPA, to make decisions such as cooperation or defection. The information set and prompts are refined through micro-level validation to ensure SLAs' behaviors are consistent and reasonable. The PPA, trained via reinforcement learning, observes relevant information from both SLAs in each interaction and determines the optimal level of information to provide, aiming to maximize social welfare. The evaluation results demonstrate that PPA with RL outperforms other baseline methods in various aspects. Furthermore, the analysis of the learned behavior of PPA generates meaningful insights.\nThis work has a few limitations. First, the limited sample size of the evaluation (number of rounds) may introduce fluctuations in the observed trends, primarily due to constraints in computational"}]}