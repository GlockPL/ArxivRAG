{"title": "Pixel-Aligned Multi-View Generation with Depth Guided Decoder", "authors": ["Zhenggang Tang", "Peiye Zhuang", "Chaoyang Wang", "Aliaksandr Siarohin", "Yash Kant", "Alexander Schwing", "Sergey Tulyakov", "Hsin-Ying Lee"], "abstract": "The task of image-to-multi-view generation refers to generating novel views of an instance from a single image. Recent methods achieve this by extending text-to-image latent diffusion models to multi-view version, which contains an VAE image encoder and a U-Net diffusion model. Specifically, these generation methods usually fix VAE and fine-tune the U-Net only. However, the significant downscaling of the latent vectors computed from the input images and independent decoding leads to notable pixel-level misalignment across multiple views. To address this, we propose a novel method for pixel-level image-to-multi-view generation. Unlike prior work, we incorporate attention layers across multi-view images in the VAE decoder of a latent video diffusion model. Specifically, we introduce a depth-truncated epipolar attention, enabling the model to focus on spatially adjacent regions while remaining memory efficient. Applying depth-truncated attn is challenging during inference as the ground-truth depth is usually difficult to obtain and pre-trained depth estimation models is hard to provide accurate depth. Thus, to enhance the generalization to inaccurate depth when ground truth depth is missing, we perturb depth inputs during training. During inference, we employ a rapid multi-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the depth-truncated epipolar attention. Our model enables better pixel alignment across multi-view images. Moreover, we demonstrate the efficacy of our approach in improving downstream multi-view to 3D reconstruction tasks.", "sections": [{"title": "1. Introduction", "content": "Multi-view images that show an object from a small number of different viewpoints, have emerged as a commonly used auxiliary representation in 3D generation. To obtain multi-view images, multi-view diffusion models [22, 23, 33] are fine-tuned from a large-scale 2D diffusion model [30] and inherit their generalizability since the model is trained on huge data. The resulting multi-view diffusion models then serve downstream tasks, e.g., a subsequent 3D reconstruction in a 3D generation pipeline [18, 35, 46], or act as approximate 3D priors for distillation-based 3D [29, 45] or 4D generation [2, 21, 49].\nDespite promising results, it is challenging for current multi-view generation methods to achieve pixel-level image alignment across views. The coarse alignment achieved with current methods introduces ambiguity in subsequently employed reconstruction methods. as shown in Fig. 1, irrespective of whether per-instance optimization [40] or feed-forward methods [18] are used for 3D generation, they get blurred results due to pixel-level misalignment. Pixel-level alignment issues arise because existing multi-view diffusion models are mostly fine-tuned from an image diffusion model with additional multi-view attention [24, 33] or an intermediate implicit 3D representation [23, 46]. Notably, the diffusion process occurs in a latent space with limited resolution, and decoding is performed independently for each frame without cross-view communication, making pixel-level alignment difficult. To improve, some multi-view diffusion models are fine-tuned from video diffusion models with camera trajectory control [8, 37]. Although the multi-view latents are jointly decoded using a video decoder, achieving pixel-level alignment remains challenging due to the sparsity of adjacent multi-view frames.\nIn this work, we propose to address the pixel-level alignment issue by improving existing VAE decoders. Following prior multi-view generation works, We adopt the VAE decoder from Stable Video Diffusion [3] as our backbone. Differently, to enable cross-view attention at higher-resolution and and achieve better pixel-level multiview alignment, we modify the VAE decoder in two ways: First, we propose a depth-truncated epipolar attention mechanism applied to high-resolution layers. This attention mechanism extracts cross view features that are crucial for better feature alignment. Different from conventional epipolar attention, the depth-truncated epipolar attention not only helps models focus on critical regions, but also enables information aggregation at high resolution. However, the depth information is not available during inference. Moreover, the multi-view latents are often not accurately aligned. Second, to solve this, we augment data with structured-noise depth to mitigate the"}, {"title": "2. Related work", "content": "3D generation. Conventional 3D generative models are train on 3D data and have employed various representations, including point clouds [1, 27], voxels [20, 34], meshes [51], implicit functions [9, 10, 16, 26], etc. However, the scarcity of 3D data limits the quality and diversity of these methods. In contrast, image diffusion models have witnessed superior quality and generalizability due to available large-scale data. To utilize pre-trained image diffusion models for 3D generation, Score Distillation Sampling (SDS) [28] and its variants [7, 19, 39, 43, 52] have been proposed to distill knowledge from 2D models in a per-instance optimization manner, taking minutes to hours for each generation. Recently, to circumvent time-consuming optimization, feed-forward methods [15, 18, 35, 46] have emerged. They use multi-view images as an auxiliary representation followed by 3D reconstruction. In this work, we focus on pixel-aligned multi-view image generation to facilitate better 3D reconstruction, ultimately leading to better 3D generation.\nMulti-view image generation. To inherit the generalizability, multi-view diffusion models are mostly fine-tuned from large-scale image diffusion models [30] using synthetic 3D object data [11]. To adapt from image diffusion models, multi-view diffusion models incorporate multi-view cross attention [24, 32] or adopt intermediate 3D representations like voxels [23] or a triplane-based neural radiance field (NeRF) [46]. With recent advances in video diffusion models, some methods propose to fine-tune from video diffusion models with camera trajectory control [37] or by treating multi-view generation as a form of image-to-video translation. However, irrespective of the approach, existing efforts still struggle to synthesize pixel-level aligned multi-view images.\nEpipolar attention in multi-view stereo and multi-view generation. Multi-view Stereo (MVS) is a classic task aiming to reconstruct 3D scenes from multiple views that are assumed to be given. With the advent of deep learning, learning-based MVS methods [14, 25, 38, 44, 48] have dominated the field, often improving upon traditional approaches [4, 13, 31, 36]. Learning-based methods formulate cost volumes by incorporating 2D semantics and 3D spatial associations. Our work is related to a stream of methods [5, 25, 41, 47] employing epipolar attention to help aggregate information from multi-view images. Epipolar attention mechanisms help in aligning and combining multi-view features more effectively, enhancing the accuracy and"}, {"title": "3. Method", "content": "We aim to generate multi-view images with better pixel-level alignment. For this, we focus on improving the decoder of a latent diffusion model. Specifically, the proposed method is based on the decoder from Stable Video Diffusion (SVD) [3].\nTo improve pixel-level alignment, we propose a depth-truncated epipolar attention mechanism. It aggregates features from multi-view latents by making use of depth information. To further mitigate the domain gap between the ground-truth depth used in training and the predicted depth used in inference, we propose a structured-noise depth augmentation strategy. The strategy can also help handle the imperfect generated multi-view latents during inference.\nIn the following, we first provide an overview of the proposed approach in Section 3.1. We then introduce the depth truncated epipolar attention mechanism in Section 3.2, the structured-noise depth augmentation strategy in Section 3.3, and the implementation details in Section 3.4."}, {"title": "3.1. Overview", "content": "To generate pixel-level aligned multi-view images, we propose a cross-view decoder with depth-truncated epipolar attention (Fig. 2). The decoder takes image latents as input and outputs multi-view RGBs. Latents are generated from an image-conditioned multi-view diffusion model, which typically uses multi-view self-attention to learn low-resolution consistency. In contrast, we propose a truncated epipolar attention module which is employed in each Up-block of the decoder. Moreover, we find that additionally conditioning the decoder on the available front view improves the results."}, {"title": "3.2. Depth-truncated epipolar attention", "content": "To generate pixel-level aligned multi-view images, the decoder needs to gather and process information from multi-view latents. An epipolar attention mechanism is an excellent candidate for this task, because it permits to combine information from corresponding points across views. Importantly, to attain a more accurate pixel-level information exchange, the attention is preferably applied to any resolution, particularly also on higher resolution latents. However, a vanilla epipolar attention mechanism often spreads too much attention on irrelevant parts, which makes it difficult for the network to learn to extract the correct adjacent features. Moreover, it also consumes a lot of memory and easily leads to out-of-memory errors given current hardware memory constraints, even on high-end equipment. To address this, we propose a depth-truncated epipolar attention mechanism. This approach not only aggregates multi-view information at higher resolutions, but also further improves the quality by enabling the model to focus on crucial regions.\nConcretely, consider a feature map from a referenced view $F_{ref}$, and feature maps from $N_v$ other views $\\{F_i\\}_{i\\neq ref}$. For a source point $s$ on $F_{ref}$, we can get the epipolar lines $\\{l_i\\}_{i\\neq ref}$ on the other views. Instead of using all points on the epipolar lines, we only sample points around the regions of interest. Specifically, given a known or estimated depth value $z$ and the camera intrinsics, we can unproject the point to 3D space $s_{3D}$. We then sample $N_p$ points $\\{p_i\\}$ around $s_{3D}$ in range $[-r, r]$ in a stratified way along the 3D line formed by $s_{3D}$ and $T_{c2w}^i$, where $T_{c2w}^i$ is a camera-to-world transformation. We then project these points to the epiplor lines $\\{l_i\\}$ on the other views to extract features.\nTo compute the cross attention among views, we first aggregate features across views. For the $N_p$ sampled points, we get features $\\{f_i\\}_{i=1, ..., N_p}^{j=1, ..., N_v}$ after projecting the points onto the epipoloar line on the $j^{th}$ feature map. For each point, we aggregate these features by a concatenation operation followed by a 2-layer MLP,\n$f_{mv} = MLP(concat(\\{f_i\\}_i)),$\n$MLP: \\mathbb{R}^{N_v \\times dim} \\rightarrow \\mathbb{R}^{dim}, f_{mv} \\in \\mathbb{R}^d.$\nThen, we aggregate the features across $N_p$ points by stacking along the feature dimension and get $F_{mv} \\in \\mathbb{R}^{N_p \\times d}$.\nWe use this feature map to compute the keys and values of classic attention while the queries are computed from the reference view, i.e.,\n$Q = W_Q F_{ref}, W_Q \\in \\mathbb{R}^{d \\times H \\times W}$\n$K = W_K F_{mv}, W_K \\in \\mathbb{R}^{d \\times N_p}$\n$V = W_V F_{mv}, W_V \\in \\mathbb{R}^{d \\times N_p}$\nWe apply the depth truncated epipolar attention on all latent resolutions (from 32 to 256) in all Up-blocks of the decoder."}, {"title": "3.3. Structured-noise depth augmentation", "content": "The proposed depth-truncated epipolar attention mechanism requires access to depth information. During training, we leverage 3D data to obtain ground-truth depth. During inference, we can predict depth using off-the-shelf depth predictors. However, the predicted depth is usually imperfect. Furthermore, the multi-view latents are encoded from ground-truth 3D assets during training, but are generated by the diffusion process during inference. That is, the multi-view latents we are decoding might not be accurately aligned. Therefore, we need a strategy to mitigate the domain gap.\nRather than warping the ground truth latents to simulate the misalignment of generated latents, we warp the ground truth depth, which can be regarded as equivalently warping the latents. For this, we propose structured-noise depth augmentation as the noising process. During training, we uniformly sample noise in lower resolution (3, 64, 128) hierarchically. We then upsample these noises to the 256"}, {"title": "3.4. Implementation details", "content": "We train our model on a subset of the Objaverse [11] dataset, which includes around 23k objects with high-quality geometry and texture. To render the dataset, our procedure is similar to wonder3D [24]: we render RGB images from six fixed views-front, front right, right, back, left, and front left. Then the images are encoded by the VAE encoder of Stable Diffusion [30]. During inference, the image latents as the decoder's input are generated by wonder3D [24] given the input image. Note that our method is compatible to all other latent multi-view diffusion models. We choose wonder3D due to its SOTA performance. The depth map of all views are calculated from a NeuS [40] model, which is optimized in the same way as wonder3D [24], i.e., from six normal and RGB maps decoded from the default decoder of Stable Diffusion [30].\nWe base our model on the SVD [3] decoder VAE and finetune. We use the SVD decoder rather than a Stable Diffusion [30] decoder as the starting point because the spatio-temporal CNN in the former improves the multi-view pixel-level consistency. For the truncated epipolar attention, we sample 7, 7, 7, 2 points around the noised depth and the range is set to r = 0.1. For the noise scale, we have 0.1 = $s_3= s_{64}\\cdot4 = s_{128}\\cdot8$. Besides low resolution latents, we also condition our model on the front view of the input resolution, which is necessary to maintain detailed texture."}, {"title": "4. Experiments", "content": "We conduct extensive experiments to answer the following questions: a) Can our method generate high-quality multi-view images that are consistent and pixel-aligned with the input image and amongst each other? b) Does better pixel-"}, {"title": "4.1. Multi-view consistency", "content": "Following prior works, we evaluate baselines and our method on a subset of the Google Scanned Object (GSO) dataset [12], which includes a variety of objects in common life. The subset matches what is used in SyncDreamer [23] and wonder3D [24], including 30 objects from humans and animals to everyday objects. For each object, we render its front view in a 256 resolution and use it as the input to all methods. Moreover, we use the photometric PSNR, SSIM [42], and LPIPS [50] as evaluation metrics. The quantitative results are summarized in Tab. 1. Note that Wonder3D's performance in our evaluation is lower than reported in the original paper. We tried our best to re-implement their evaluation. Results still improve upon those of other methods. Our method performs favorably to Wonder3D in PSNR and SSIM.\nQualitatively, the multi-view images generated by our method are more consistent, as shown in Fig. 4. We provide zoomed-in illustrations to highlight complex textures. Notably, our method generates textures that are more faithful to the input view, while Wonder3D and SyncDreamer both yield blurred textures. This is due to their diffusion process occurring in latent space with limited resolution. Moreover, their decoder doesn't consider other views.\nNext, we quantitatively and explicitly assess the consistency among images to further showcase the necessity of pixel-level alignment. We measure the number of correspondences between adjacent views using the off-the-shelf dense matching method AspanFormer [6]. As shown in Tab. 2, the proposed method outperforms Wonder3D by 40%. This result highlights the improved pixel-alignment."}, {"title": "4.2. Rerendering from 3D generation", "content": "Next, we show that consistent multi-view generation is beneficial for 3D asset generation. As shown in Fig. 5, we optimize NeuS [40] again using the images decoded by our method, and re-render the NeuS results from the fixed views. The Wonder3D [24] baseline reconstruction follows its procedure. We observe the baseline's re-rendering to be much"}, {"title": "4.3. Ablation study", "content": "We illustrate how the truncated epipolar attention module and the structured-noise depth augmentation improve consistency. Results are shown in Fig. 6. We study several baselines: w/o epi. only adds the front view condition to the SVD decoder. We observe that its output is very blurred. This highlights that learning the consistency via solely the decoder's original CNN and without epipolar attention is difficult. Then we assess Full epi., using conventional epipolar attention that queries all adjacent features on the whole epipolar line. Note that due to limited GPU memory, full epipolar attention can only be applied on resolutions < 128. The results are worse than the truncated version for two reasons. First, excessive irrelevant information is processed by the attention mechanism, making it difficult for models to focus on the critical information. Second, the attention is not applied to the high resolution, resulting in a less direct impact to the output.\nNext, we study the necessity of the structured-noise depth augmentation. The method abbreviated with w/o depth aug. uses ground-truth depth without any augmentation, and the method referred to with Indep. depth aug. augments data by adding independent noise to each pixel. We find that the output of w/o depth aug. has severe artifacts due to the large distribution gap between depths used in training and inference. Meanwhile, Indep. depth aug. struggles to learn the correct consistency given too much high-frequency noise. These results emphasize that a proper balance is desirable to improve the quality of the results.\nFinally, we also report in Tab. 2 the pixel consistency of all baseline methods. Both depth-truncated epipolar attention and the structured-noise depth augmentation play a crucial role in improving consistency. Removing any of our contributions yields a lower number of matching correspondences."}, {"title": "5. Conclusion", "content": "We study pixel-aligned multi-view generation. Multi-view images obtained from multi-view generation are emerging as an important auxiliary representation for 3D generation."}, {"title": "Limitations:", "content": "The back view information is often not clearly shown in the provided front view and only finetuning a reconstruction decoder can not complete the texture of unseen views. We leave replacing the decoder with a pixel-level upsampling diffusion model as future work."}, {"title": "Broader impacts.", "content": "Content generation in general may have positive and negative societal impacts. On the positive side, generating a desired look, even if abstract, is easier than ever. However, on the negative side, potentially malicious or deceiving content can also be generated easily."}]}