{"title": "Imputation Matters: A Deeper Look into an Overlooked Step in Longitudinal Health and Behavior Sensing Research", "authors": ["AKSHAT CHOUBE", "RAHUL MAJETHIA", "SOHINI BHATTACHARYA", "VEDANT DAS SWAIN", "JIACHEN LI", "VARUN MISHRA"], "abstract": "Longitudinal passive sensing studies for health and behavior outcomes often have missing and incomplete data. Handling missing data effectively is thus a critical data processing and modeling step. Our formative interviews with researchers working in longitudinal health and behavior passive sensing revealed a recurring theme: most researchers consider imputation a low-priority step in their analysis and inference pipeline, opting to use simple and off-the-shelf imputation strategies without comprehensively evaluating its impact on study outcomes. Through this paper, we call attention to the importance of imputation. Using publicly available passive sensing datasets for depression, we show that prioritizing imputation can significantly impact the study outcomes - with our proposed imputation strategies resulting in up to 31% improvement in AUROC to predict depression over the original imputation strategy. We conclude by discussing the challenges and opportunities with effective imputation in longitudinal sensing studies.", "sections": [{"title": "1 INTRODUCTION", "content": "The ubiquitous presence of smartphones and wearables has enabled researchers to collect rich and high-resolution data on individuals' daily activities, interactions, and physiological responses over extended periods [15]. This widespread adoption has profoundly influenced the landscape of mobile health research, leading to a surge in longitudinal studies passively examining participants' behavior. These studies capture multiple dimensions of a person's life - mental health [47, 67, 68, 72], academic performance [17, 28, 41], workplaces [18, 19, 45], medical symptoms tracking [22, 56], sleep wellness [49, 74], and physical activity [35]. Data from longitudinal studies can reveal patterns, fluctuations, and trends that might not be apparent in cross-sectional studies, where data is collected at a single point in time. Despite the potential of passive sensing studies in monitoring longitudinal trajectories, ensuring data completeness remains a persistent challenge [5, 36].\nData missingness in longitudinal studies can occur due to several factors, including participant's behaviors leading to low compliance and technical problems with the sensing apps/devices [48, 66]. Many longitudinal studies incentivize participants to be compliant through monetary rewards [9] or altruistic motives [33]. While incentives can attract participants, they do not guarantee full compliance [9]. Participants may become disengaged over time or fail to adhere to data collection protocols, leading to gaps in the collected data [73]. For instance, participants may forget to charge their smartphones or wearables regularly, resulting in periods of data loss when devices power off due to low battery levels. Similarly, participants may inadvertently switch off sensors or disable data collection features, leading to gaps in the recorded data. Moreover, the appeal of incentives may attract participants who are less motivated to contribute accurate or reliable data, further exacerbating data completeness issues [29]. Even when participants adhere to data collection protocols, technical errors or bugs in data collection processes can compromise data completeness. For example, software bugs or compatibility issues may cause data loss or corruption during transmission or storage. Furthermore, these studies are often conducted in the wild, which entails many unforeseen circumstances that can add to data missingness.\nMissing data in longitudinal studies, whether due to technical challenges or participant-related factors, could present an incomplete view of behavioral patterns and researchers may fail to capture the longitudinal shifts or developmental trajectories. Thus, compromising the reliability and accuracy of the scientific insights researchers may seek to derive from the data [4, 51]. For instance, consider a study tracking physical activity patterns over a span of several months and suggesting interventions to promote a healthy lifestyle. Periodically missing physical activity data, however, could lead to an underestimation of activity levels, potentially impacting conclusions about the effectiveness of interventions or lifestyle changes. The presence of missing data may also introduce bias into the study results, as the observed sample may not be representative of a person's behavior. Thus, missing data, if not handled properly, can not only impact behavioral data analysis but can also be potentially detrimental to participants if these analyses are used for outcome predictions and interventions.\nResearchers are increasingly using sophisticated deep learning and machine learning methods to model and predict various health outcomes with passive sensing data. These approaches, particularly the ones with neural networks (e.g., LSTMs, FNNs, etc.) require complete data and necessitate researchers to handle missingness before feeding the data into such models. To reduce data missingness, researchers often employ protocols to monitor data collection constantly and check with participants through emails and phone calls if they see missing data from the participant [13, 63]. Despite this manual and labor-intensive process, data missingness is unavoidable in longitudinal research [9], with recent studies reporting missing data rates from 20% of over 50% for certain features and data types [50, 55, 71, 72].\nAddressing data missingness is a complex process that involves making decisions about whether to drop missing data, use imputation techniques, or apply a combination of both approaches. Researchers' decisions can directly affect the data they use for subsequent analyses and model building. Despite the importance of this process, it appears to be often overlooked in passive sensing research. This observation stems from the fact that prior works in passive sensing studies often tend to focus on novel outcomes or novel machine learning and modeling methods, with relatively limited discussion on handling missing data or imputation strategies. As a first step, we wanted to understand whether data missingness remains understudied or simply underreported. Thus, to gain a deeper insight into researchers' decision- making processes, practices, and challenges with handling data missingness, we conducted formative semi-structured interviews with thirteen researchers with experience in working with longitudinal passive sensing health data.\nThe researchers we interviewed revealed that they often considered addressing data missingness and imputation as a lower priority than other aspects of the study, such as model or algorithm development and subsequent post-study analyses. When imputing data, researchers tend to use simple or off-the-shelf strategies without comprehensively evaluating the validity of the imputed data or its impact on the study outcomes. Through this paper, we draw attention to the critical role of selecting appropriate and robust imputation strategies in health and behavioral longitudinal studies. To this end, we present a case study using the GLOBEM platform and its associated publicly available datasets for"}, {"title": "2 RELATED WORK", "content": "Longitudinal studies provide a comprehensive understanding of the underlying behavioral, affective, and mental processes by continuously tracking changes in behavior over an extended period. Researchers in the past have conducted longitudinal studies to track a wide spectrum of human behaviors [16, 25, 41, 50, 68, 69, 72, 74]. However, conducting long-running longitudinal studies is a demanding task requiring significant time, resources, and funding, with ensuring data completeness being a big challenge."}, {"title": "2.1 Missing Data in Longitudinal Studies", "content": "Missing data in longitudinal studies has been attributed to low participant compliance, attrition, and technical issues. Participants may drop out of the study over time for various reasons, such as relocation, loss of interest, or inability to continue participation [39, 54, 70]. Studies based in schools and universities have reflected attrition due to students moving schools, being absent, or refusing consent [7, 43]. The demographics of participants can play a crucial role in attrition rates. Young et al. [73] showed that younger individuals have a much higher tendency to drop out of studies as compared to middle-aged or older individuals. Past works have tried simulating attrition to account for it in designing longitudinal studies [31, 34]. Researchers have also outlined guidelines to reduce attrition rates [21, 37]. Even in studies with low attrition rates, data missingness still remains a persistent problem as participants may choose not to respond to surveys or adhere to data collection protocols. Longitudinal studies collecting information on sensitive topics have shown higher rates of non-response or missing data due to participant discomfort or reluctance to disclose [24, 40]. Passive sensing longitudinal studies rely on mobile phones, wearables, and sensors to collect data, which leads to missing data stemming from technical and connectivity issues with devices [3, 12, 68, 72].\nTypically, to reduce data missingness researchers continuously monitor data collection streams and reach out to participants with missing data. Triplet et al. [63] and Christensen et al. [13] sent reminders to participants who did not"}, {"title": "2.2 Dropping Incomplete Data", "content": "Researchers often drop features, days, or participants' entire data to ensure data quality. For the StudentLife study, Wang et al. [68] removed a student's data if their phone data was missing due to power issues or the phone being left at the dorm. Similarly, Obuchi et al. [52] removed participants who had less than 18 hours per day and less than 14 days of data during the term. Likewise, Wang et al. [69] removed participants with less than 19 hrs of data on a day and less than 7 days of usable data. They mentioned that only 159 out of the 646 participants satisfied their data inclusion criteria and were included in the analysis. In a multimodal smartphone sensing study [2] on complex daily activity recognition, the authors dropped features from sensors that were missing more than 70% of times. A study on drinking behavior sensing [60] used three criteria to select user data - the participant had responded to either the drink or the forgotten drink survey at least once during a given night; b) the participant had at least one data sample for any sensor data type during the night; and c) the participant had at least one stay-point based on location sensor logs. Huckins et al. [30] chose a threshold of at least 20 days of available data to keep participants in a study for depression assessment using smart phone data. Similarly, In a study by Grunerbl et al., [25] on passive monitoring of bipolar disorder, data from only four out of ten patients was considered appropriate for analysis due to attrition and data inconsistencies. Some studies in the past had to remove data due to health conditions [58], privacy [57], and even the death of the participants [62]. Even after excluding participants' data with high levels of missing information, data missingness still persists in the datasets for participants' data below the data removal thresholds. To address this, researchers use data imputation strategies to estimate missing data using available data of the participants."}, {"title": "2.3 Imputing Incomplete Data", "content": "Collecting passive sensing data on health and behavior is a task that requires significant effort and time [26]. Discarding all the data records with missing values often leads to datasets that are too small to be useful, making this approach an impractical solution. Moreover, most of these studies build Machine Learning and Deep Learning models that often require complete input data feature vectors for training and inference, thus necessitating the imputation of missing data. For mobile and wearable sensing data, researchers have used simple statistical imputation strategies (like mean, median, mode, and LOCF) and statistical interpolation methods. Xu et al. [71] used mean and median imputation strategies for filling up missing data in the GLOBEM Platform. Similarly, Chow et al. [12] used LOCF to estimate missing values in global positioning system (GPS) data, whereas Burns et al. [8] used mixed models to get a best-fit line for \"estimated GPS\" coordinates when the phone cannot obtain GPS readings due to connectivity. In cStress [27] study, authors used cubic Hermite splines to interpolate high-frequency Electrocardiogram and respiration data. Likewise, DaSilva et al. [20] imputed the missing MPSM (Mobile Photographic Stress Meter) scores using Kalman smoothing [1]. Researchers have also used kNN-based strategies for sensor data [2, 61] and have also built machine-learning models to impute missing"}, {"title": "3 FORMATIVE STUDY: HANDLING DATA MISSINGNESS IN PRACTICE", "content": "We conducted qualitative interviews with researchers to gain insights into their decision-making process and challenges they encounter while handling missing data and selecting an imputation strategy in their studies."}, {"title": "3.1 Study Method", "content": "We chose a convenience sampling strategy and reached out to 13 researchers (faculty members (3), postdoctoral scholars (2), research scientists (2), and doctoral students (6)) experienced in working with longitudinal passive sensing data (\u00b5 = 6.23 years, \u03c3 = 3.72 years) for health and behavior outcomes. Eleven researchers expressed that they are professional in dealing with longitudinal health and behavioral datasets, while two researchers said they are somewhat professional. Some of these researchers are also authors of popular and well-received datasets and publications in this field. We conducted 30-minute interview sessions with participants over Zoom. After receiving their verbal consent, we recorded the meeting. We used transcripts from the meeting to generate our insights. Participants also completed a short survey on demographic information at the end of the interview session. To thank participants for their time, we compensated them with a $15 Amazon gift card. Our interview protocol was approved by our university's Institutional Review Board (IRB).\nWe used MS Word's automatic transcribing tool to transcribe our interviews. Two members of our research team independently coded four interviews to develop the initial codes. Both the coders carefully reviewed the transcripts and employed the Open Coding approach [14]. After independently coding four interviews, the coders met to discuss their codes - merging codes and resolving conflicts through consensus. After this, the first author coded the rest of the interviews."}, {"title": "3.2 Interview Insights", "content": "Our interviews with researchers revealed interesting insights into potential causes of missingness in studies, their decision-making process in handling missingness, and the associated challenges.\nThe researchers confirmed that data missingness is a highly prevalent issue, especially in free-living studies. As P11 said, \"I have come to accept that it's part of life and most of the pipelines that we have developed take this into account and are built on the premise that there will be missing data\". Researchers attributed technical issues with passive sensing devices and data collection applications (or servers) as a prominent cause. P3 mentioned, \"A reason (for missingness) is network issues or the mobile app that you have developed has a bug (like) memory out of bounds or app is going crazy.\" Researchers also mentioned participants' low compliance as a major reason, highlighting that participants may switch off their phones or stop wearing smartwatches due to privacy concerns, personal preferences, or discomfort. P4 noted that \"A lot of times users may turn off their location for privacy reasons or maybe for battery saver reasons.\u201d Some researchers studying specialized populations, schizophrenia patients, opioid recovery patients, and older adults mentioned that low motivation or technical skills in specialized populations might lead to lower compliance. P5 explained, \"One reason that is not highlighted as often is that sometimes people don't even know how to operate phones, like people who are socially less functioning or on severe spectrum. It's hard to educate them and maintain the quality of incoming data.\" Researchers also explained that understanding the reasons for missing data might be challenging due to a lack of context about participants' lives, not having research coordinators available to regularly reach out to participants for clarification, and/or not having collected the datasets themselves. P6 mentioned that \"Missingness is a large issue, and I think we spend a lot of time trying to understand why missingness occurs because often it's not quite clear to us, especially because a lot of times if the focus is on the machine learning end, we might not have been a part of the larger clinical study that would collect the data.\"\nTo deal with data missingness, the first decision researchers must make is determining which portion of the data to drop and which portion to impute. Data missingness percentage plays an important role in this decision. When the percentage of missing data is low, researchers often prefer to drop the records with missing data, as the remaining dataset would still be large enough to yield meaningful conclusions. P1 noted, \"If the missingness isn't too much, I'll drop the data and move on. But if more than 25% is missing, I'll try to impute it.\" Researchers also discard parts of participants' data that do not meet specific thresholds. These thresholds vary among researchers some treat them as hyperparameters to experiment with, while others select values they deem appropriate. P2 remarked, \"Sometimes I have a somewhat random threshold, like, if more than half of data is missing, then it doesn't make sense for us to impute ... It is a hyperparameter that can be tuned, but I have never experimented with it.\u201d Researchers also expressed that they are more comfortable in discarding data that is easier to collect. P9 noted, \u201cFor datasets that get continuously generated, it's okay to not invest time in imputation, and you can drop data without much information loss, but in some cases removing rows can lead to information loss, for example, experience sampling data, the imputation would make sense.\" Additionally, researchers highlighted that phone-sensing data is generally more difficult to collect. They also felt more at ease in dropping data from in-the-lab studies compared to in-the-wild studies.\nSome researchers expressed skepticism about adding synthetic data to the collected ground truth data. P7 explained, \u201cI am gently cautious of simulating or imputing data. Not that I'm morally opposed to it, but I'm like a lot of things need to be validated before I do it... I think I'm actually more on the side of if you know that data is missing from just a portion of it, I would just not use that portion.\" Other researchers were more comfortable imputing input sensor data but were hesitant to do so for target outcome data, particularly self-reported data. For instance, P6 mentioned, \u201cI have a lot of\""}, {"title": "4 GLOBEM DATASETS AND PLATFORM", "content": "As part of GLOBEM, Xu et al. [72] have made their four datasets publicly available. They collected these four datasets (INS-W_1, INS-W_2, INS-W_3, and INS-W_4) across four years, from 2018 to 2021, at a university. In addition to these datasets, Xu et al. also compared their approaches with two additional datasets (INS-D_1 and INS-D_2), collected at a second university from 2018 to 2019 [71], which they graciously shared with us for our work. For the purposes of this paper, we refer to these six datasets collectively as GLOBEM datasets. Each dataset included data from the Spring quarter (approximately 10 weeks) for each participant to control for seasonal effects. These datasets include a broad spectrum of features that the authors collected continuously in the background, including call, sleep, location, steps, phone screen activity, and Bluetooth. Participants also completed two comprehensive surveys on health behaviors, social well-being, emotional states, and mental health during the start and end of the study. In addition to pre- and post-study surveys, participants answered Ecological Momentary Assessment (EMA) surveys every week (PHQ-4, PSS-4, PANAS) measuring depression and stress. In the GLOBEM datasets, each day is split into four epochs (time intervals of 6 hours each) - morning, afternoon, evening, and night. The feature data collected over a particular epoch is aggregated as the cumulative data of the entire epoch. Thus, GLOBEM datasets contain features aggregated over the entire day and four epochs of the day. For additional details on the different datasets, please refer to the original GLOBEM papers [71, 72].\nThe authors also provided an open-source benchmarking platform, \"GLOBEM Platform\" [71] with 19 depression detection algorithms and multiple evaluation tasks. The GLOBEM platform supports the development and evaluation of different longitudinal behavior modeling methods. For our analyses and evaluations, we chose best-performing machine learning algorithm \"Chikersal\" [10] and deep learning-based algorithm \"Reorder\" [71] available in GLOBEM Platform."}, {"title": "4.1 Data Availability in GLOBEM", "content": "The GLOBEM datasets are a tremendous resource for researchers to advance the field of behavioral modeling and depression prediction with passive sensing data. These datasets, however, suffer from considerably high data missingness. We report the overall data availability metrics for features used by Reorder and Chikersal algorithms. We calculate data availability as the percentage of non-empty cells in the dataset. The feature sets for Chikersal and Reorder Algorithms are quite different, with just eight features in common. The Chikersal feature set has considerably less missingness than the Reorder feature set. The open-source code of the GLOBEM Platform and the data missingness in"}, {"title": "4.2 Variability in GLOBEM Datasets", "content": "We examined the distributions of the key features across the six GLOBEM datasets. We visualized the distribution of feature values using kernel density plots to identify differences in the shape, skewness, and tail behavior of feature distributions. We present density plots of some of the representative features from different categories in Figure 1. The density plots show significant variations in feature distributions across datasets. For instance, location features like the number of locations visited per day, as well as time spent in locations, i.e., work, home, etc., were significantly different in W_3 and W_4 as compared to data collected in W_1 and W_2, possibly due to enforced restrictions on outdoor movement during COVID-19 years.\nThe observed differences in feature data dispersion and distribution suggest that datasets are diverse, and so are individual participants. Thus, although sensor data features collected as part of the GLOBEM framework are consistent across all six datasets, the diversity of datasets adds to the generalizability as well as the robustness of the analyses and results in subsequent sections."}, {"title": "5 METHODOLOGY", "content": "In our formative interviews, researchers mentioned focusing on different study outcomes, i.e, researchers conduct longitudinal passive sensing studies with broadly two objectives: first, to observe patterns, describing associations, and clarifying/discovering phenomena in longitudinal data using statistical methods without involving model construction or outcome prediction (Reconstruction Task). Second, to track and predict certain outcomes related to human activity, behavior, or health. In these studies, researchers collect longitudinal data and then build machine learning, deep learning, and statistical models to predict outcomes (Prediction Task). Hence, in this work, we chose to analyze imputation strategies for both study outcomes."}, {"title": "5.1 Reconstruction Task", "content": "If the goal of a longitudinal study is to assess correlations between various behavioral features, then imputation should be able to fill in the values of a feature close to its original distribution. To compare different imputation strategies, we can remove some portion of available data and then use the strategies to impute the removed data. We can then assess the closeness of imputed values with actual values using metrics like root mean squared error (RMSE) or mean absolute error (MAE).\nConsider, we remove n values from the dataset D using some amputation strategy (A). Let the removed indices be {(i_m, j_m), m = 1...n}. Then, we use the imputation strategy 'I' to impute and get the imputed dataset D. we can compute reconstruction RMSE (r-RMSE).\n$r-RMSE_I = \\sqrt{\\frac{1}{n} \\sum_{m=1}^{n} (D_{(i_m,j_m)} - \\hat{D}_{(i_m,j_m)})^2}$\nWe explore following amputation Strategies (A):\n\u2022 MCAR: This strategy amputes data completely at random. To simulate empirical imputation error for r% missingness, we iterate through each participant's data and remove r% of feature data per participant per feature randomly\n\u2022 MNAR: This strategy removes data assuming the data is missing not at random. For MNAR amputation, we try two ways of data removal:\n  MNAR (i): Higher than (50 \u2013 k_i/2)th percentile and lower than (50+k_i/2)th percentile of the target feature variable.\n  MNAR (ii): Lower than (k_{ii}i/2)th percentile and higher than [100 \u2013 (k_{ii}i/2)]th percentile of the target feature variable [32].\nFor a fair comparison with the MCAR method, we chose k_i and k_{ii} respectively such that it accounts for an overall r% missing rate."}, {"title": "5.2 Prediction Task", "content": "If the goal is to build systems and models to predict health and behavioral outcomes, then imputation should help ensure high predictive performance of the outcomes. Researchers often measure predictive performance with metrics like F1 score, balanced accuracy, and area under the receiver operating characteristic curve (AUC). Therefore, to compare the success of different imputation strategies, we need to compare the relative boost in predictive performance by these strategies. In this study, for prediction task, we focus on the task of predicting depression labels for future weeks of participants using sensor and survey data from their past weeks. This task was first defined in the original paper [72] and is available as the \"within-user\" task in the GLOBEM platform. We used two prediction model algorithms from the original paper - Reorder and Chikersal. The prediction models were trained on data from 80% of the weeks and then used to predict depression labels for the remaining 20% of weeks. We use the same evaluation metrics (balanced AUROC and balanced accuracy) as used by the original study.\nAdditionally, as longitudinal studies can run for extended periods spanning months or even years, researchers might want to analyze data and build models during multiple checkpoints in the study. Thus, we wanted to explore data imputation trends and performances of imputation strategies in real-time settings. We simulated a real-time inductive imputation scenario where we start predicting depression labels starting at the end of week 3 until the end of week 10. We use n - 1 weeks data to impute and then predict depression labels for the nth week."}, {"title": "5.3 Imputation Strategies", "content": "For the tasks mentioned above, we evaluate various commonly employed and off-the-shelf imputation strategies such as Median, Multiple Imputations by Chained Equations (MICE) [65], k-Nearest Neighbour (kNN) [64], and Matrix Completion [42]. We used the sklearn library's Iterative Imputer and KNNImputer for MICE and KNN strategies, respectively. For Matrix Completion we used fancyimpute library's Soft Impute function for our experiments. We also developed two novel algorithms:"}, {"title": "5.3.1 Bounded kNN", "content": "This variant of kNN allows a choice of a lower bound (1) on a number of neighbors to be present for imputation to happen. If there are less than I neighbors present, we do not impute the missing value. We also constrain existing feature values within the 5th and 95th percentiles of their respective distributions while calculating the distance between neighbors to ensure that outlier values do not impact our neighbor selection. For our experiments, we set the lower bound (1) as 2 and the upper bound (u) as 6 based on empirical evaluations. Throughout the rest of the paper, we refer to this variant of kNN as bounded kNN, while the standard off-the-shelf kNN is called Simple kNN."}, {"title": "5.3.2 Autoencoder based Algorithm", "content": "Autoencoders are a type of artificial neural network that learns to encode input data into a lower-dimensional representation and then decode it back to its original form. It consists of an encoder network that compresses the input data into a latent space representation and a decoder network that reconstructs the original input from this representation.\nMathematically,\nf_{w,b}(x) \\approx x"}, {"title": "5.3.3 GLOBEM's Imputation Strategies", "content": "In the GLOBEM Platform, imputation happens after the data is divided into input vectors for training models. For the Reorder algorithm, the imputation happens after the training data preparation step, where the participants' data is already divided into 28-day sized input vectors for model training. GLOBEM imputes missing data with the column median across the 28-day input vector, resorting to zeroes if the entire column is empty. On the other hand, the implementation of the Chikersal algorithm uses mean across the entire dataset as an imputation strategy after it has conservatively removed rows and columns having more than 50% missingness. We refer to the default imputation strategies of Reorder and Chikersal algorithms as the \"GLOBEM-R\u201d and \u201cGLOBEM-C\", respectively, and use them as a baseline for their respective comparisons.\nFor clarity, we want to emphasize that we refer to methods for imputation (e.g., kNN, MICE, etc.) as imputation strategies while referring to methods for depression detection in the GLOBEM platform as algorithms (Chikeral and Reorder)."}, {"title": "6 RESULTS AND EVALUATION", "content": "In this section, we present the results of our comparative analysis of different data imputation algorithms, focusing on their impact on data availability, feature-data reconstruction, and prediction performance for the behavioral variable, i.e., depression."}, {"title": "6.1 Reconstruction Task", "content": "We first present the relationship between feature data availability and reconstruction RMSE (r-RMSE) for synthetically imputed data per participant. We used one MCAR and two MNAR strategies (MNAR (i) and MNAR (ii)) amputation strategies. We present r-RMSE vs data availability for the Reorder feature set for INS-W_1 dataset. The trends with other datasets are similar and are included in supplementary materials.\nFor all three amputation strategies in the Reorder feature set, the GLOBEM-R strategy performed the worst as it replaces feature values with zero if the median of the column is not available in the past 28-day data, most of the time, making feature values out of bounds of the original distribution of the feature. Similarly, MICE performed worse than other strategies as MICE builds regression models, and sometimes, these models predict values out of the bounds of the feature column. For Matrix Completion, Autoencoder-Median, Simple kNN, and Bounded kNN for the Reorder, we observed that the choice of the amputation strategy impacted the performance of the imputation strategies for reconstruction. In the MAR and MNAR (i) amputation strategy, bounded-kNN and Matrix Completion performed the best, whereas, for the MNAR (ii) strategy, all imputation strategies (except MICE and GLOBEM-R) had similar performance. For the Chikersal feature set, this effect is even more prominent. For example, Autoencoder-Median performs better for MNAR (i) amputation strategy than Simple KNN but performs worse for MNAR (ii) amputation strategy. The reconstruction graphs for other datasets for Chikersal feature sets are also included in the supplementary materials.\nIn GLOBEM datasets, we checked that the missingness was not completely at random (MCAR); thus, the missing not at random (MNAR) amputation strategies used might be a better way of amputation. Inducing the missingness pattern similar to that found in the dataset using an amputation strategy is inherently challenging and a limitation in this research field. Researchers, however, should be aware that using incorrect amputation strategies can lead to biased and incorrect conclusions."}, {"title": "6.2 Prediction Task", "content": "For the Prediction Task, we considered two scenarios: (1) Depression prediction for future weeks using past weeks' data (outlined as within-person task in GLOBEM), and (2) Real-time weekly Depression prediction using past weeks."}, {"title": "6.2.1 Within-Person Depression Prediction", "content": "We compared the performances of the Reorder and Chikersal models trained on data imputed using different strategies for within-person depression prediction: using the first 80% of weeks (the past) to predict the last 20% of weeks (as the future). Tables 2 and Table 3 compare the success of different imputation strategies in improving the predictive performance of Reorder and Chikersal models, respectively. For Reorder models, Autoencoder-based strategies (Autoencoder-Median and Autoencoder-kNN) consistently showed the best or comparable to best performances across all datasets. Autoencoder-Median imputation increased AUC performance over the baseline approach (GLOBEM-R) used in original work by 25.2%, 31.1%, 27.1%, 22%, 19.2%, and 17.1% for W_1, W_2, W_3, W_4, D_1, and D_2 datasets respectively. This notable increase in the predictive performance of models emphasizes the importance of choosing the right imputation strategy for datasets. The default GLOBEM-R strategy (median for 28 days features) consistently showed inferior performance compared to all imputation algorithms, including using the median across entire columns (Median). This suggests that relying solely on the median of 28 days for imputation might"}, {"title": "6.2.2 Real Time Depression Prediction", "content": "To understand how these algorithms would perform in real-world systems that might require on-the-fly imputation for real-world predictions, we evaluated the various imputation strategies in a real-time simulations. To evaluate the prediction objectives in a real-time scenario, we reframed our objective to predict depression at the end of each week in the study starting week 3. In Figure 5, we compared the balanced accuracies of the model trained with data imputed using different imputation accuracies. Despite slight fluctuations, we observed that the Autoencoder-Median consistently outperformed all other strategies for the majority of the weeks, suggesting it could be a reliable strategy even in real-time settings."}, {"title": "7 DISCUSSIONS", "content": "In this section, we discuss insights from our formative study involving thirteen researchers experienced in working with health and behavioral datasets. We tie together these insights to the findings from our case study on the GLOBEM datasets."}, {"title": "7.1 Underreported and Understudied", "content": "Our formative interviews revealed that handling data missingness and imputation is not only under-reported in papers on passive health and behavioral sensing but is also understudied. Researchers often consider imputation a low-priority task compared to other aspects of study like developing sophisticated machine-learning models and other post-study analyses. They often use \"ad-hoc\u201d and \u201chit and trial\" methods to choose imputation strategies, mostly relying on simple and sometimes on off-the-shelf strategies. This lack of a systematic evaluation of imputation strategies is due to time constraints and a greater focus on other aspects of the study. One way to encourage researchers to perform this systematic evaluation is to build a platform offering multiple imputation strategies where researchers can just plug in their datasets and explore different imputation strategies. As a first step, we make our implementations of imputation strategies publicly available on GitHub. In the long term, we can take inspiration from the GLOBEM Platform [71], where researchers can experiment with different depression detection algorithms and build a similar platform for imputation strategies. In our interviews, some researchers explicitly expressed this need. As P9 mentioned, \"I always wanted to build a little tool where I can select the imputation method and quickly see the results, rather than to code things from scratch. With the limitation of time, the more complex implementation (of imputation strategies) gets deprioritized\u201d. This kind of platform can save researchers effort and time. As P12, who has mostly relied on simple imputation strategies, argued, \"one of the ways to get me to use imputation strategies is to make it easy for me to use if somebody has a strategy and it's easy to use, and it's definitively shown to improve the model. Sure, I would use (it).\u201d"}, {"title": "7.2 Impact on Depression Prediction in GLOBEM Datasets", "content": "In our case study with the GLOBEM datasets, our proposed Autoencoder-based imputation strategy achieved up to 31% improvements in AUROC for future depression prediction tasks. Although our Autoencoder-based strategy comes out as a promising and viable strategy, we do not claim it to be the best or the only imputation strategy for longitudinal health and behavior datasets as it warrants more investigation. As P11 noted, there is \u201cno universal pattern\" with imputation strategies, and mentioned \"It is very tricky to say one technique works better than others.\". Our aim in the case study was to demonstrate that investing time in the development and evaluation of imputation algorithms can lead to performance improvements in the outcomes of the study (in this case, depression prediction). We hope our findings can encourage researchers to consider data imputation as a crucial part of the overall model development process and carefully explore multiple strategies to complete the datasets accurately.\""}, {"title": "7.3 Reconstruction in GLOBEM Datasets"}]}