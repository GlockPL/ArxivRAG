{"title": "Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models", "authors": ["Xin Li", "Weize Chen", "Qizhi Chu", "Haopeng Li", "Zhaojun Sun", "Ran Li", "Chen Qian", "Yiwei Wei", "Zhiyuan Liu", "Chuan Shi", "Maosong Sun", "Cheng Yang"], "abstract": "The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph analysis. The benchmark, datasets and enhanced open-source models are available at https://github.com/BUPT-GAMMA/ProGraph.", "sections": [{"title": "1 Introduction", "content": "Background. Large language models (LLMs) [6, 51, 3] are parameter-rich neural networks trained on a vast amount of text data to understand and generate human language. LLMs can not only handle classical natural language processing tasks like translation, but also benefit task solving in various domains such as code generation [41], logical reasoning [39], and mathematical calculation [43].\nPrevious LLM Benchmarks for Graph Analysis. Recently, many researchers have proposed extending LLMs to scenarios that require graph understanding and analysis [25, 48]. As graph is a very commonly used data structure in real-world services (e.g., social networks [30, 35] and urban computing [63, 59]), enabling the ability of LLMs to process graphs is an important step toward more advanced general intelligence. To this end, several benchmarks have been developed to evaluate such ability. For example, NLGraph [52] and GraphInstruct [33] investigate whether LLMs can understand"}, {"title": "Limitations of Existing Benchmarks.", "content": "However, from the perspective of practicality, we argue that previous benchmarks have three major drawbacks. Firstly, the problems in these work require LLMs to read through the adjacency lists of graphs from prompts before answering specific questions. Consequently, the graph sizes in their benchmarks are rather small (typically with a few dozens of nodes), due to the length limitation of LLMs. Being able to compute the shortest path on a small graph does not mean that the same can be done on a real graph with millions of nodes. Secondly, the desired solving process in these work requires step-by-step reasoning fully based on LLMs. But even with the help of Chain-of-Thought (CoT) [55, 14], the reasoning depths of current LLMs are still shallow [29, 27]. Consequently, LLMs might be able to count triangles one by one in a small graph with 10 nodes, and will inevitably fail for large graphs. Thirdly, the problem descriptions in these work are abstract and monotonous in form, lacking context from real-world application scenarios."}, {"title": "Inspirations from Human Experts.", "content": "Consider the scenario that a human expert is asked to find the shortest path between two nodes in a million-scale graph, she will probably write a few lines of Python codes based on NetworkX[23], instead of directly reasoning over the raw inputs. To this end, a question naturally arises: can LLMs analyze graphs like professionals? Fortunately, most popular LLMs have shown the ability to write codes and utilize various application programming inferfaces (APIs), making it possible to analyze graphs via API calling as human experts will do. Compared with direct reasoning in previous benchmarks, generating a few lines of codes requires much shallower reasoning depths for LLMs, but can solve more complex problems."}, {"title": "Benchmark.", "content": "In this paper, we propose ProGraph benchmark to evaluate the capability of LLMs in leveraging external APIs for graph analysis. The benchmark includes 512 problems with hand-crafted questions and answers (QA pairs). The problems cover three categories of tasks: basic graph theory, graph statistical learning, and graph embedding, and can be solved based on six popular Python libraries. In the questions, graphs can be either described by natural language or stored in files, and thus can scale to 106 in our benchmark. To improve the diversity of problem descriptions and align with real-world scenarios, the questions are rephrased in a role-play manner based on GPT-4 turbo [37]. In the answers, we provide reference code, key APIs and execution results. We also design an automated evaluation process that aligns well with human judgement."}, {"title": "Datasets and Models.", "content": "To facilitate LLMs to solve these problems via programming, we construct the LLM4Graph datasets with both document and code data. The document dataset contains API information crawled from the official documents of the six Python libraries. The code dataset includes 29,260 QA pairs automatically generated by back-instructing [53] GPT-4 turbo. To enable CoT learning, we further introduce the thought on the document information of relevant APIs to the answers of the code dataset as prefixes. To demonstrate the value of our datasets, we enhance closed-source LLMs by extracting relevant information from the document dataset as RAG (retrieval- augmented generation), and improve open-source ones by instruction tuning over the code dataset. Besides the datasets, the improved open-source LLMs are also released for future researches."}, {"title": "Key Results.", "content": "The accuracies of closed-source models (Claude, GPT and Gemini) on ProGraph are 25-36%, and can be improved to 37-46% with RAG using LLM4Graph as the retrieval pool. The accuracies of open-source models (Llama3 [1] and Deepseek Coder [19]) are only 12-24%, but can be improved to 45-47% through instruction-tuning on LLM4Graph. These results show that ProGraph is challenging for current LLMs, and LLM4Graph can significantly enhance their performance."}, {"title": "Contributions.", "content": "(1) To the best of our knowledge, we are the first work exploring the ability of LLMs to analyze graphs with external APIs. The utilization of external APIs is practical and powerful in real-world scenarios. (2) To evaluate such ability, we propose a novel and challenging ProGraph"}, {"title": "3 Benchmark", "content": "To evaluate the ability of LLMs in graph analysis, we introduce the ProGraph benchmark, featuring 512 problems in three categories. These hand-crafted problems include questions and answers with two difficulty levels. To enhance diversity and realism, we leverage GPT-4 turbo to rephrase the questions in a role-playing manner, followed by manual verification for correctness. Finally, given the high consistency of answer judgments between humans and GPT-4o, we employ GPT-4o to automate"}, {"title": "3.1 Tasks", "content": "The proposed ProGraph benchmark considers three categories of tasks:\nCategory 1: Basic Graph Theory. Graph theory primarily studies the fundamental concepts of graphs, including types, properties, classical algorithms and many other basic operations. For example, some problems in this category aim to check whether a graph is acyclic, compute the centrality of nodes, or find the maximum cardinality matching.\nCategory 2: Graph Statistical Learning. Graph statistical learning utilizes a probabilistic model to extract useful information from nodes, edges, or the entire topology for various tasks. In this work, we mainly focus on graph clustering and sampling techniques. For example, detecting communities in a graph with Louvain algorithm [5], or sampling a subgraph based on random walk.\nCategory 3: Graph Embedding. Graph embedding technique aims to learn real-valued vectors for nodes in a graph, where similar nodes are expected to have similar vectors. The learned vectors can be used as features to enhance the performance of downstream tasks. For example, a typical problem in this category will require the learned embeddings of DeepWalk algorithm [40] for a given graph."}, {"title": "3.2 Human Annotation", "content": "To create high-quality problems for benchmarking, we invite human annotators to develop questions and answers based on the following guidelines, and the annotation manual is in Appendix I.\nPython Libraries. In this work, we consider six popular libraries to support the above three task categories, i.e., NetworkX [23] and igraph [13] for basic graph theory, CDlib [44], graspologic [11] and Little Ball of Fur [46] for graph statistical learning, and Karate Club [45] for graph embedding.\nQuestion Design. First of all, human annotators need to either manually design or use some random graph generators to obtain a graph. Then, based on the API documents of the six libraries, the annotators are asked to design questions with one of the four types: true/false, calculation, drawing, and hybrid questions. Here hybrid questions are composed of two or more of the first three types. Here we present a calculation question from the basic graph theory category as an example:"}, {"title": "3.3 Role-Play Rephrasing", "content": "To make questions more realistic, we rephrase them in a role-play manner. First, GPT-4 turbo generates hundreds of job titles and descriptions. Then, we randomly select a job for a given problem. Using GPT-4 turbo, we mimic the profession's characteristics, knowledge, and style to integrate the question into a real-world context. We manually review the modified questions to ensure they maintain the same semantics and graph structures as the original ones. A complete example of rephrasing is in Appendix B, and here we present the rephrased question of the above problem:"}, {"title": "3.4 Automated Evaluation", "content": "Metrics. To evaluate the ability of LLMs to solve these problems, we consider two metrics: pass rate and accuracy. Pass rate measures the ratio of executable code generated by an LLM, while accuracy measures the ratio of correct answers from the executable code. Accuracy is always no higher than the pass rate and is considered the more important metric as it evaluates final performance.\nProcess. Evaluating diverse answer formats with rule-based matching is challenging, and human evaluation is too labor-intensive. Thus, we automate evaluation using GPT-4o. First, we extract code snippets from LLM-generated answers using regular expressions. GPT-4o is then asked to check the correctness given the execution result. For problems with certain answers, such as true/false or calculation questions, GPT-4o assigns 1 point if the execution result matches the reference code's result, and 0 otherwise. For other problems, like drawing questions, GPT-4o matches key API usage: if the generated code contains m out of n key APIs, the accuracy point is m/\u043f."}, {"title": "4 Datasets and Models", "content": "To enhance the ability of LLMs to solve graph problems with Python APIs, we construct LLM4Graph datasets. Based on the LLM4Graph, we enhance both closed-source and open-source models."}, {"title": "4.1 Datasets", "content": "Document dataset. The document dataset is crawled from the official documents of the corresponding Python libraries. These documents can be directly used to enhance closed-source models via RAG. We also use these documents for generating code datasets. Specifically, each API corresponds to a specific entry of the document dataset, including the description, parameter list, return value list, code examples, and other contents. An example of API entry is shown in Appendix C."}, {"title": "4.2 Models", "content": "We use the above datasets to improve various LLMs in handling graph analysis tasks. For closed- source models, we enhance them by retrieving relevant information from the document dataset as RAG. For open-source models, we fine-tune them using code datasets as instruction tuning.\nClosed-source Models. Due to the high difficulty of our ProGraph benchmark, mainstream LLMs (including Claude, GPT and Gemini) are not particularly strong in directly solving these problems. Therefore, before closed-source LLMs answer these questions, we retrieve the document information of top-K relevant APIs based on Llamaindex [31], allowing the models to learn through the context and enhance their performance. We explore K = 3, 5, 7 to investigate the impact of RAG, and the models will be given the corresponding suffix as No RAG, RAG 3, RAG 5 or RAG 7.\nOpen-source Models. We use the two code datasets (i.e., Code and Doc+Code) to fine-tune Llama-3- 8B-Instruct [1] and Deepseek-Coder-7B-instruct [19], and consider the following model variants: (1) Code-only: LLMs are fine-tuned with the Code dataset. (2) Code + RAG 3/5/7: Code-only models"}, {"title": "5 Experiment", "content": "In this section, we present a comprehensive evaluation of our proposed benchmark, ProGraph, and the accompanying dataset, LLM4Graph. Our experiments assess the performance of both closed-source and open-source LLMs on graph analysis tasks. We demonstrate the limitations of current LLMs in handling structured graph data, and showcase the potential improvements achievable through the use of our datasets via RAG or instruction tuning. More results are shown in Appendix G and H."}, {"title": "5.1 Benchmarking LLMs on ProGraph", "content": "We evaluate a variety of mainstream closed-source and open-source LLMs on the ProGraph, including GPT [36], Claude [3], Gemini [42], Llama 3 [1] and Deepseek Coder [18].\nThe results, presented in Table 5, indicate that none of the tested LLMs could effectively solve the problems in ProGraph. While GPT-4 turbo and its variant GPT-4o demonstrate relatively higher performance with an overall accuracy of 31.2% and 36.3% respectively, they still fall short in delivering satisfying accuracy across different categories. For instance, GPT-4 turbo achieves an accuracy of 42.1% in Category 1 but only 14.8% and 12.0% in Categories 2 and 3, respectively. Similar trends are observed in other models. These results highlight the challenges faced by current LLMs in addressing structured graph data as human experts. This necessitates the development of specialized datasets and fine-tuning approaches to bridge this performance gap."}, {"title": "5.2 Enhancing Closed-Source LLMs with Document Dataset", "content": "To investigate the potential of enhancing LLMs' performance on graph analysis tasks, we utilize RAG techniques to extract relevant API information from the documents in LLM4Graph. This augmented context is then provided to the LLMs to assist in generating more accurate and effective solutions.\nFigure 3 shows the performance gains for four top closed-source LLMs: GPT-4 turbo, GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro. All models show significant boosts in pass rate and accuracy, with more than a 5% improvement in accuracy. Gemini 1.5 Pro and Claude 3 Opus achieve over 10% accuracy improvement with five retrieved APIs. However, performance improvements plateau with additional API information, which may be attributed to the saturation of relevant information, where additional API details no longer contribute to further understanding and instead introduce redundancy or noise. This observation aligns with the findings of previous studies on RAG [28]."}, {"title": "5.3 Enhancing Open-Source LLMs with Code Dataset and Doc+Code Dataset", "content": "We investigate the potential of enhancing open-source LLMs' performance on graph analysis tasks by fine-tuning them using LLM4Graph and augmenting the models with RAG. Experiments are conducted on a general-purpose model, Llama-3-8B-Instruct [1], and a model specifically designed for code generation, Deepseek-Coder-7B-Instruct-v1.5 [18]. The results, presented in Figure 4, demonstrate that our LLM4Graph can significantly improve the performance of different types of open-source small models. The accuracy of both models, after being fine-tuned merely on the code data within LLM4Graph, substantially surpasses the best result from closed-source models without RAG. The Doc+Code setting further enhances the models' performance, leading to comparable or even better accuracy than the best result from closed-source model with RAG.\nHowever, augmenting the open-source models fine-tuned on the code with RAG mechanism does not further improve the performance, and even leads to degraded performance. We hypothesize that this discrepancy may be attributed to the limited ability of these small models to process long text, hindering their utilization of the document information. The additional information provided by RAG may introduce confusion in understanding the problem statement and arriving at the correct solution. Overall, our proposed Doc+Code setting proves to be an effective means of integrating document information from LLM4Graph, significantly enhancing the accuracy of open-source models. We show that LLM4Graph can serve as effective data to enhance the model's code-generation capabilities and lead to better utilization of various graph libraries."}, {"title": "6 Analysis", "content": "In this section, we present a comprehensive analysis of the performance of different models on the ProGraph. By grouping the benchmark based on categories, question types, and answer difficulty, we aim to provide a granular exploration of the strengths and limitations of these models in handling graph analysis tasks. We also present the types of compilation errors made by different models."}, {"title": "6.1 Performance Analysis on Different Benchmark Groupings", "content": "To gain a deeper understanding of the capabilities of LLMs and fine-tuned smaller models presented in Section 5.3, we analyze their performances on ProGraph from three different perspectives: task category, question type and answer difficulty.\nTask Category. We analyze the model performance based on different categories in the ProGraph, as shown in Figure 5a. Mainstream LLMs and fine-tuned smaller models exhibit similar performance on graph theory and graph statistical learning tasks. However, a significant disparity is observed in their performance on graph embedding tasks, where fine-tuned smaller models substantially outperform RAG-enhanced large models. This observation suggests that not all graph analysis tasks can be easily handled by closed-source LLMs without further fine-tuning. More complex and challenging tasks still require fine-tuning for effective learning.\nQuestion Type. We further examine the model performance based on different question types, i.e., true/false, calculation, drawing, and hybrid. In Figure 5b, we plot the performance of different models on these question types separately. Mainstream LLMs excel in true/false and drawing types but struggle with calculation and hybrid ones. Fine-tuned smaller models demonstrate improvements across various question types, especially on the complex hybrid type, indicating the effectiveness of our proposed enhancement strategies.\nAnswer Difficulty. Lastly, we divide the ProGraph into two levels of difficulty: easy (involving only one API) and hard (involving multiple APIs). As shown in Figure 5c, mainstream closed-source large models perform well on easy-level problems, with accuracies generally approaching or exceeding 50%. However, their performance significantly deteriorates on hard-level ones, with the highest accuracy reaching only 32.5%. This observation suggests that mainstream LLMs have limitations when the number of required APIs increases. In contrast, our fine-tuned models demonstrate significantly higher accuracy on hard-level problems, approaching or exceeding 40%, yielding approximately an improvement of 8% compared to the best closed-source LLM. Note that LLM4Graph only contains data involving one API. Still, the models fine-tuned on LLM4Graph show strong generalizability on problems requiring multiple APIs."}, {"title": "6.2 Compilation Error Analysis", "content": "To gain insights into the types of compilation errors made by different models, we conduct an error analysis on the best-performing closed-source models (GPT-4 turbo RAG 5 and Claude 3 Opus RAG 7) and fine-tuned open-source small models (DeepSeek Coder Doc+Code and Llama 3 Doc+Code). As shown in Figure 6, we categorize the errors into ten distinct types to identify patterns and differences in the error distributions among these models.\nOur analysis reveals that closed-source models exhibit a low similarity in their error cases, suggesting that they possess varying coding capabilities. For instance, GPT-4 turbo often makes SyntaxError, but rarely ImportError, which is contrary to Claude 3 Opus. The fine-tuned open-source small models exhibit a high similarity in their error distributions, with AttributeError being the most dominant.\nThe error analysis also highlights some common challenges faced by all models, such as AttributeError and TypeError, suggesting that models may have difficulty in memorizing and understanding the attribute of class objects from various python libraries, and the type of returned results from different functions. Interestingly, the fine-tuned models have a notably lower percentage of SyntaxError"}, {"title": "7 Conclusion", "content": "In this paper, we introduce ProGraph, a novel and challenging benchmark for evaluating LLMs in graph analysis using external APIs. Current LLMs achieve only 36% accuracy, revealing their limitations. To bridge this gap, we further construct LLM4Graph, a dataset with crawled documents and auto-generated codes based on popular graph libraries. The datasets can help improve the accuracy of both closed-source and open-source LLMs by 11-32% through RAG and instruction tuning. Our work highlights the potential of enhancing LLMs with our LLM4Graph, offering valuable resources for advancing LLM capabilities in structured data analysis. Discussions about limitations and boarder impacts can be found in Appendix A."}, {"title": "A Limitation and Boarder Impact", "content": "Limitation. Due to the version iterations of GPT, the results based on automated evaluation might show slight differences compared to those presented in this paper. The proposed ProGraph benchmark contains 512 hand-crafted problems, which can only reflect a core subset of graph analysis tasks. Note that our dataset includes nearly 30,000 auto-generated instances. Thus, in future work, we can expand the benchmark by manually selecting high-quality cases from the dataset. Admittedly, the benchmark still presents certain differences from real-world scenarios, e.g., we did not consider a multi-turn task-solving process.\nBoarder Impact. The inherent complexity of graph data in real-world scenarios demands professional coding capabilities, making it a compelling testbed for evaluating the structured data handling abilities of LLMs. We believe this work can shed light on further researches toward more powerful general intelligence with structured data understanding."}, {"title": "B Role-play Rephrasing Example", "content": "Original Question\nI have a graph with an edge set [(1, 2), (1, 3), (2, 3), (2, 4), (3, 5), (4, 5)], can you help me compute node connectivity for all pairs of nodes and print the node connectivity for each pair?\nRole-Play Rephrasing Prompt\nThe above question is too abstract. Please role-play based on the following professional information, mimic their everyday language and tone, and give a background from real life (more than 4 sentences). Restate the problem, ensuring that the semantics remain unchanged. Notes: If the original question has a file, you must give the file name in your new question.\nProfessional Information\nEcologist: Studies the relationships between organisms and their environments, including ecosystems, habitats, and biodiversity.\nRephrased Question\nImagine we're examining a simplified model of an ecosystem where the nodes represent different species, and the edges indicate direct ecological interactions between them, such as predator-prey or symbiotic relationships. For our current study, we've mapped out a series of interactions as follows: [(1, 2), (1, 3), (2, 3), (2, 4), (3, 5), (4, 5)].\nTo fully understand the resilience of our ecosystem, we need to determine the species connectivity, which reflects how each pair of species is interconnected through these interactions. Can we analyze our network to reveal the minimum number of species that would need to be removed to disrupt the direct connection between any two species in this web? I'd like to have a measurable indication of node connectivity for each unique pair of species within our ecosystem based on the interactions detailed above. This will provide us with valuable insights into the robustness of their relationships and, by extension, the stability of our ecosystem."}, {"title": "C Document Dataset Example", "content": "Section ID triadic_census\nDescription\nDetermines the triadic census of a directed graph. The triadic census is a count of how many\nField List\nParameters: G: digraph; nodelist: list\nReturns: census : dict\nRaises: ValueError\nRubrics\nNotes: This algorithm has complexity O(m) where m is the number of edges in the graph"}, {"title": "D Prompt for GPT Back-Instruction", "content": "Back Instruction Prompt\nGiven an API doc, you need to respond like this: you should give me an example of Python code to show how to use it, and give me a question that describes the data that code uses, and the problem that code solves. Now I will give you an API document and you need to complete my request."}, {"title": "E ProGraph Benchmark Example", "content": "Conponent Example\nTable 6: Benchmark Example\nAnnotated Question Given a graph with edge set [(1, 2), (1, 3), (2, 4), (3, 4), (4, 5), (5, 6), (5, 7), (6, 7)], can you Color a graph using largest_first coloring strategy of greedy graph coloring?\nNotes: You need to print the result like this.\nfor node, color in coloring.items():\nprint (f\"Node {node}: Color {color}\")\nRephrased Question As a Project Coordinator, you're tasked with the organization of a network of collaborative tasks, each represented by connections between different points, or 'nodes,' in a particular sequence. This sequence is comprised of pairs indicating task interdependencies: [(1, 2), (1, 3), (2, 4), (3, 4), (4, 5), (5, 6), (5, 7), (6, 7)]. The challenge lies in assigning 'colors,' or categories, to these tasks in a manner that adjacent tasks (those directly connected) are not categorized the same - this ensures a clear division of responsibilities and resources.\nYour goal is to utilize the \"largest_first\" strategy within the greedy color- ing algorithm for optimal organization. This strategic approach selects nodes based on their descending degree, that is, beginning with the node with the most connections. By categorizing in this method, you're likely to minimize the total number of categories used.\nUpon completion of this process, we need to visualize our task cate- gorization. Could you please format the output as a sequence of print statements in Python, displaying the nodes alongside their assigned cate- gories? The output should follow this structure for clarity:\nfor node, color in coloring.items():\nprint (f\"Node {node}: Color {color}\")\nThis simplification will effectively translate the abstract graph coloring problem into a practical scenario suitable for organizing our network of tasks.\nAnswer Node 4: Color 0 Node 5: Color 1 Node 1: Color 0 Node 2: Color 1 Node 3: Color 1 Node 6: Color 0 Node 7: Color 2\nReference Code import networkx as nx\nfrom networkx.algorithms.coloring import greedy_color\n# Create a sample graph\nG = nx.Graph()\nedges = [(1, 2), (1, 3), (2, 4), (3, 4), \n (4,5), (5,6), (5, 7), (6, 7)]\nG.add_edges_from(edges)\n# Apply the largest first coloring strategy\ncoloring = greedy_color(G, strategy=\"largest_first\")\nfor node, color in coloring.items():\n print (f\"Node {node}: Color {color}\")\nKey APIs greedy_color"}, {"title": "F Doc+Code Dataset Example", "content": "Question\nGiven the karate club graph provided by NetworkX, how do we find its communities using the greedy modularity maximization method and then print out the sorted list of nodes for each community?\nAnswer\nTo solve the given question, we can leverage greedy_modularity_communities function.\nIt is a function that \"Find communities in G using greedy modularity maximization. This function uses Clauset-Newman-Moore greedy modularity maximization [2] to find the community partition with the largest modularity. Greedy modularity maximization begins with each node in its own community and repeatedly joins the pair of communities that lead to the largest modularity until no further increase in modularity is possible (a maximum). Two keyword arguments adjust the stopping condition. cutoff is a lower limit on the number of communities so you can stop the process before reaching a maximum (used to save computation time). best_n is an upper limit on the number of communities so you can make the process continue until at most n communities remain even if the maximum modularity occurs for more. To obtain exactly n communities, set both cutoff and best_n to n. This function maximizes the generalized modularity, where resolution is the resolution parameter, often expressed as (gamma). See modularity() .\".\nThe function accepts parameters:\n\"Parameters:\": {\n\"G: NetworkX graph\": \"\",\n\"weight: string or None, optional (default=None)\": \"The name of an edge attribute that holds the numerical value used as a weight. If None, then each edge has weight 1. The degree is the sum of the edge weights adjacent to the node.", "resolution": "float, optional (default=1)\": \"If resolution is less than 1, modularity favors larger communities. Greater than 1 favors smaller communities.", "cutoff": "int, optional (default=1)\": \"A minimum number of communities below which the merging process stops. The process stops at this number of communities even if modularity is not maximized. The goal is to let the user stop the process early. The process stops before the cutoff if it finds a maximum of modularity.", "best_n": "int or None, optional (default=None)\": \"A maximum number of communities above which the merging process will not stop. This forces community merging to continue after modularity starts to decrease untilbest_ncommunities remain. If None, force it to continue beyond a maximum.\"}\nThe function returns: \"Returns:\": {\n\"communities: list\": \"A list of frozensets of nodes, one for each community. Sorted by length with largest communities first.\" }\nThe function's path is \"networkx.community.greedy_modularity_communities\".\nBy using the info above, we can write a Python code to solve this problem.\nHere is the Python code.\nimport networkx as nx\n# Create a graph\nG = nx.karate_club_graph()\n# Find communities in the graph\ncommunities = nx.community.greedy_modularity_communities(G,\n weight='weight', resolution=1, cutoff=1, best_n=None)\n# Print the sorted list of nodes in each community\nfor community in communities:\n print(sorted(community))"}, {"title": "A Annotation Manual for ProGraph Benchmark", "content": "Annotation This manual provides detailed guidelines for annotating a benchmark dataset by creating and documenting tasks related to graph-related Python packages such as NetworkX and graph. Each annotator will document a function, propose a question, create a graph, and provide a reference code with execution results.\n1. Objective\nTo create a high-quality benchmark dataset for evaluating the performance of graph- related functions in Python packages.\nTo document function usage, practical questions, graph creation, corresponding results, and key APIs.\n2. Annotation Tools\nGraph Libraries: NetworkX, igraph, CDlib, graspologic, Karate Club, Little Ball of Fur\nCode Editor: Any Python IDE (e.g., PyCharm, VS Code, Jupyter Notebook)\nDocumentation Resources: Official documentation for the 6 Python libraries.\n3. Data Types\nFunction Documentation: Description and usage of a specific function.\nGraph: Manually crafted or randomly generated graphs.\nCode: Python code snippets for graph creation and function execution.\nResults: Output from executing the provided code.\nKey APIs: APIs that are crucial and indispensable for solving a problem.\n4. Annotation Guidelines\nFunction Documentation:\nSelect an API from the provided graph-related packages.\nDocument the API name, parameters, return type, and a brief description.\nProposing a Question:\nFormulate a clear, practical question that the API can solve.\nEnsure the question is specific and relevant to the API's capabilities.\nCreating a Graph:\nManual Crafting: Draw a graph that fits the proposed question.\nRandom Generation: Use code to generate a random graph appropriate for the function.\nGraph Description: Provide a brief description of the graph structure.\nReference Code:\nWrite Python code to create the graph.\nInclude the API call with appropriate parameters.\nEnsure the code is well-commented and readable.\nExecution Result:\nExecute the code and record the output.\nProvide a detailed explanation of the result.\n5. Annotation Process\nStep-by-Step Procedure:\n(a) API Selection: Choose an API from the provided documentation.\n(b) Question Formulation: Develop a practical question for the API.\n(c) Graph Creation: Create a graph manually or using code.\n(d) Code Writing: Write reference code to demonstrate the API.\n(e) Result Recording: Execute the code and document the output.\n(f) Review and Submission: Review the annotation for accuracy and clarity, then submit.\nExamples:"}, {"title": "6. Quality Control", "content": "Provide examples for each step to guide annotators.\nReview Process:\nConduct peer reviews of annotations to ensure consistency and accuracy.\nProvide feedback and request revisions if necessary.\nConsistency Checks:\nEnsure all annotations follow the same structure and guidelines.\nVerify the correctness of code and results.\n7. Common Issues\nAmbiguity in Questions: Ensure questions are specific and clear.\nErrors in Code: Double-check code for syntax and logical errors.\nInconsistent Results: Verify that results match the expected output.\n8. Annotation Tips\nBe precise and consistent in the document.\nUse the provided examples as references.\nClarify any doubts with the project coordinator.\n9. Frequently Asked Questions (FAQs)\nQ: What if I encounter an error in the API execution?\nA: Check the document and debug the code. If the error persists, seek help from the coordinator.\nQ: How detailed should the graph description be?\nA: Provide enough detail to understand the graph structure and its relevance to the API."}]}