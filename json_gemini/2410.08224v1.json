{"title": "A Survey of Spatio-Temporal EEG data Analysis: from Models to Applications", "authors": ["Pengfei Wang", "Huanran Zheng", "Silong Dai", "Yiqiao Wang", "Xiaotian Gu", "Yuanbin Wu", "Xiaoling Wang"], "abstract": "In recent years, the field of electroencephalography (EEG) analysis has witnessed remarkable advancements, driven by the integration of machine learning and artificial intelligence. This survey aims to encapsulate the latest developments, focusing on emerging methods and technologies that are poised to transform our comprehension and interpretation of brain activity. We delve into self-supervised learning methods that enable the robust representation of brain signals, which are fundamental for a variety of downstream applications. We also explore emerging discriminative methods, including graph neural networks (GNN), foundation models, and large language models (LLMs)-based approaches. Furthermore, we examine generative technologies that harness EEG data to produce images or text, offering novel perspectives on brain activity visualization and interpretation. The survey provides an extensive overview of these cutting-edge techniques, their current applications, and the profound implications they hold for future research and clinical practice. The relevant literature and open-source materials have been compiled and are consistently being refreshed at https://github.com/wpf535236337/LLMs4TS", "sections": [{"title": "1 Introduction", "content": "Electroencephalography (EEG) has long been a cornerstone in the study of brain function, offering a non-invasive means to monitor electrical activity within the brain. Non-invasive are easier to implement without surgery, but they lack simultaneous consideration of temporal and spatial resolution, as well as the ability to capture deep brain information. In contrast, invasive methods like Stereoelectroencephalography (SEEG) [35] can measure these brain signals more precise with higher signal-to-noise data [16], albeit requiring surgical procedures to insert recording devices. Overall, non-invasive signals are relatively safer, more portable, have greater potential for use, and are applicable to a wider population, reflecting voltage fluctuations caused by ion currents in neurons.\nWhile our understanding of the brain deepens and computational methods advance[30, 56], the field of EEG analysis faces many challenges. The first challenge is the effective capture of representations in EEG data, particularly in the absence of labels. The second challenge involves the identification and classification of complex and subtle patterns within brain activity, requiring advanced discriminative methods that can accurately interpret the nuanced differences indicative of various brain states or conditions. Lastly, the challenge of creating meaningful visualizations or interpretations from EEG data calls for generative methods that can transform the abstract EEG signals into more tangible and comprehensible forms, such as images or text, thereby enhancing our understanding of the brain's intricate workings. Addressing these challenges collectively advances the field of EEG analysis, making it more robust, insightful, and applicable to a wider range of scientific and clinical applications."}, {"title": "2 Related survey", "content": "In response to aforementioned challenges, recent developments in deep learning and artificial intelligence have paved the way for more robust and nuanced EEGEEG analysis strategies. This paper surveys three key areas of advancement that are reshaping the field of EEG analysis:\n\u2022 Representation Learning in EEG Analysis: Representation learning is the first fundamental step in EEG analysis, concentrate on automatically extracting useful features from EEG signals. Self-supervised learning methods are being employed to develop robust signal representations that enhance the precision and interpretability of downstream tasks. These unsupervised learning methods are naturally suited for the vast amounts of brain signal data and mimic human learning processes.\n\u2022 Discriminative EEG Analysis: Discriminative methods focus on distinguishing between different categories or patterns in EEG signals. Advanced architectures such as Graph Neural Networks (GNNs), Foundation Models, and LLMs-based Methods are being utilized to gain deeper insights into brain activity. These architectures efficiently capture discriminative patterns, which are crucial for understanding complex neural processes.\n\u2022 Generative EEG Analysis: Generative methods aim to generate new modalities or signal data from EEG signals. Innovative approaches such as diffusion produce images or text from EEG data are providing novel approaches to the understanding and visualization of brain activity. These generative techniques are also important applications for AI-generated content (AIGC).\nThis paper aims to provide a comprehensive overview of these cutting-edge techniques, discuss their details, and consider the significant implications they hold for future research and clinical practice in EEG analysis. The remainder of this paper is organized as follows: Section 2 summarizes the background and related surveys of our work. Section 3 discusses the robust representation learning strategy and its significance in EEG data analysis. Section 4 explores the emergent discriminative architecture, detailing the role of GNNs (4.1), Foundation Models (4.2), and LLMs-based Methods (4.3). Section 5 examines the innovative generative applications of EEG data. Section 6 provides an introduction of the most widely used datasets and the key metrics employed to assess the performance of various EEG analysis models. Finally, Section 7 concludes the paper and discusses potential future directions for EEGEEG analysis."}, {"title": "2.1 Existing Surveys on EEG Analysis", "content": "In the domain of EEG-related concepts and research, numerous review studies have provided comprehensive summaries. Hosseini et al. [56] introduced the application of machine learning in EEG signal processing, covering traditional methods such as Support Vector Machines (SVM), k-Nearest Neighbors (kNN), and Naive Bayes in classification scenarios. However, this review did not consider the extensive discussion of deep learning algorithms that have demonstrated superior performance. Jiang et al. [65] discussed the removal of artifacts from EEG signals, making their review more detailed in technical aspects. Nevertheless, their work did not cover deep learning algorithms and did not consider a broader range of EEG downstream tasks. In contrast, Zhang et al. [161] provided a more comprehensive perspective, introducing the origins and applications of Brain-Computer Interface (BCI) and discussing the integration of mainstream deep learning algorithms such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Generative Adversarial Networks (GAN) with EEG tasks. With the continuous innovation in artificial intelligence community, EEG research based on foundational models and large language models has begun to emerge. However, to the best of our knowledge, there is currently no literature that reviews EEG analysis from a more holistic frontier technology perspective, which is the gap this paper aims to fill."}, {"title": "2.2 Emerging Surveys on General Time-Series Analysis", "content": "In the general time series domain, a substantial body of work has summarized the application of the latest technologies in various downstream tasks. Zhang et al.[159] categorized existing self-supervised learning-based time series analysis methods into three types: generative, contrastive, and adversarial, and discussed their key intuitions and main frameworks in detail. Jin et al.[68] provided an overview of the application of graph neural networks in time series tasks such as forecasting, classification, imputation, and anomaly detection. Liang et al.[89] reviewed foundational models in time series analysis from the perspectives of model architectures, pre-training techniques, adaptation methods, and data modalities. Similarly, [64, 70, 160] systematically outlined methods and procedures for time series analysis based on large language models. Yang et al. [149] reviewed the application of diffusion models in time series and spatio-temporal data. Additionally, there are some works focusing on more specific model architectures or downstream tasks[144, 163]. We refer the reader to the corresponding publication for a more in-depth understanding.\nAlthough numerous reviews exist within the broader time series field, few surveys concentrate exclusively on EEG data. Moreover, EEG data possesses unique characteristics, and a substantial body of related work has emerged recently. Thus necessitating a comprehensive review and synthesis, this paper seeks to offer an in-depth examination of state-of-the-art techniques, elaborate on their intricacies, and explore their profound implications for future EEG research and clinical applications."}, {"title": "3 Representation Learning in EEG Analysis", "content": "In recent years, deep learning has excelled in extracting hidden patterns and features of the data. Typically, feature extraction models based on deep learning rely heavily on large volumes of labeled data, a method commonly referred to as supervised learning. However, in certain practical applications, particularly in time-series data such as Electroencephalograms (EEG), acquiring extensive labeled data is both time-consuming and costly. As an alternative, Self-Supervised Learning (SSL) has garnered increasing attention due to its label efficiency and generalization capabilities. SSL, a subset of unsupervised learning, extracts supervisory signals by solving tasks automatically generated from unlabeled data, thereby creating valuable representations for downstream tasks.\nWith the significant success of SSL in fields such as computer Vision(CV)[91] and Natural Language Processing(NLP)[43], its application to time-series data appears particularly promising. However, directly applying tasks designed for visual or linguistic processing to time-series data is challenging and often yields limited effectiveness. The primary reasons include:\n\u2022 Time-series data possess unique attributes such as seasonality, trends, and frequency domain information, which are typically not considered in tasks designed for images or language.\n\u2022 Common data augmentation techniques in computer vision, such as rotation, flipping, and cropping, can disrupt the temporal dependencies and integrity of time-series data, such as EEG signals. For instance, rotating or flipping the time points in an EEG signal could completely lose physiological significance and contextual information.\n\u2022 Many time-series datasets are multidimensional, with each dimension potentially representing a different measurement channel. This contrasts with handling single images or text data, requiring synchronous analysis and processing across multiple dimensions."}, {"title": "3.1 Contrastive Learning", "content": "Contrastive learning is a self-supervised learning method that acquires invariant representations of data by learning the similarities and differences between samples. This approach maps similar samples to proximate representation spaces and dissimilar samples to distant ones, thereby enabling the learning of generalized feature representations without the need for explicit label information. Formally, given a set of samples X = {x1,x2,...,xN}, contrastive learning aims to learn a mapping function f that maximizes the similarity between positive sample pairs of the same class and minimizes the similarity between negative sample pairs of different classes. For positive sample pairs (x, x+) and negative sample pairs (x, x\u00af), the objective of contrastive learning is to optimize the following loss function:\nL (x,x+,x\u00ae) = \u2212 log( ef(x,x+)/T / ef(x,x+)/\u315c + ef(x,x\u00ae)/\u0442 ) (1)\nwhere f(x, x+) denotes the similarity of feature representations for positive pairs, f(x, x\u00af) for negative pairs, and 7 is a temperature parameter that adjusts the scale of similarity. The intuitive interpretation of this loss function is that by maximizing the similarity of positive pairs while minimizing that of negative pairs, the model learns high-level semantic relationships between samples, resulting in more distinctive representations.\nContrastive learning offers significant advantages in the analysis of multivariate medical signals. Firstly, as an unsupervised learning method, it does not require explicit labeling, thus providing an efficient training approach for the domain of multivariate medical signals where labeling is difficult and costly. Secondly, by learning the similarities and differences between positive and negative sample pairs, contrastive learning can capture more generalizable feature representations. This is particularly applicable to multivariate medical signals collected from different subjects or devices, enabling the learning of core discriminative representations and reducing the impact of domain shift."}, {"title": "3.1.1 Based on Data Augmentation", "content": "Data augmentation is an indispensable component of contrastive learning. It generates different views of input samples using data augmentation techniques, and then learns representations by maximizing the similarity between views of the same sample while minimizing the similarity between views of different samples. SeqCLR[97] introduces a set of data augmentation techniques specifically for EEG and extends the SimCLR [22] framework to extract channel-level features from EEG data.\nTS-TCC [41] generates different views of input data using both strong and weak augmentation methods. Weak augmentation employs jittering and scaling strategies, while strong augmentation uses permutation and jittering strategies, applying them to the temporal contrast module of EEG signals for temporal representation learning. This method maximizes the similarity between contexts of the same sample while minimizing the similarity between contexts of different samples. Jiang et al.[66] applies transformations such as horizontal flipping and adding Gaussian noise to EEG signals, then learns the correlation between signals by measuring the feature similarity of these transformed signal pairs. Additionally, the authors explore the impact of transformation combinations on the network's representation capability to find the optimal combination for downstream tasks. mulEEG [81] proposes a novel multi-view self-supervised method. By designing EEG augmentation strategies and introducing a diversity loss function, mulEEG effectively leverages complementary information from multiple views to learn better representations. However, these EEG data augmentation methods often lead to sampling bias[28], especially for noisy EEG data, which can significantly affect performance[115]. To address these limitations, ContraWR [148] constructs positive sample pairs using data augmentation and employs global average representations as negative samples to provide contrastive information, thereby learning robust EEG representations without labels. Additionally, ContraWR assigns greater weight to closer samples when calculating the global average.\nExisting contrastive learning methods primarily focus on a single data level and fail to fully exploit the complexity of EEG signals. Therefore, COMET [140] leverages all data levels of medical time-series, including patient, trial, sample, and observation levels, to design a hierarchical contrastive representation learning framework. Its advantage lies in fully utilizing the hierarchical structure of medical time-series, enabling"}, {"title": "3.1.2 Combined with Expert Knowledge", "content": "Expert knowledge contrastive learning is a relatively new representation learning framework. Generally, this modeling framework incorporates expert prior knowledge or information into deep neural networks to guide model training. In a contrastive learning framework, prior knowledge can help the model select the correct positive and negative samples during training.SleepPriorCL [158] was proposed to mitigate the sampling bias problem in data augmentation-based contrastive learning. It is well known that each sleep stage occupies a certain frequency range. The authors utilized this fact to calculate the energy of these frequency bands and used it as prior knowledge for training. Specifically, the authors calculated the rhythm energy vector E = [\u0395(\u03b4), E(0), E(\u03b1), E(\u03b2)] for each EEG segment x, referred to as prior features, and then defined the dissimilarity di,j between the anchor xi and the sample xj as follows:\ndi,j = log (||Ei - Ej ||2) (2)\nSamples are ranked by dissimilarity, with the top K samples selected as positive samples and the rest as negative samples. Additionally, SleepPriorCL introduces a mechanism to adjust the gradient penalty strength of each sample based on its confidence as a positive or negative sample. To achieve this, each sample is assigned a customized temperature. The multi-positive contrastive loss is modified as follows:\nL(xi)=P(1) \u03a3\u03c1\u03b5P(i) log (exp (Si,p /Tp) /\u03a3\u03b7\u2208N(i) exp (Sin /\u03a4\u03b7) (3)\nwhere xi is the sleep epoch, si,j is the cosine similarity between zi and zj, and zi and zj are the vectors of Xi after encoding and projection. The index i is referred to as the anchor, the index p as the positive sample, N(i) is the set of all negative samples in the batch, and the index n as the negative sample. P(i) is the set of positive samples containing all true positive samples of x in the batch.\nKDC2 [145] is based on the neural theory of EEG generation, which states that EEG signals are produced by synchronized synaptic activity that stimulates neuronal excitation, generating a negative extracellular"}, {"title": "3.2 Mask Autoencoder Approaches", "content": "voltage that transforms neurons into dipoles. The voltage generated by the dipoles is transmitted to the scalp via capacitive and volume conduction and is captured by electrodes as EEG signals. Therefore, the authors constructed scalp and neural views to describe the external and internal information of brain activity, respectively, and designed a knowledge-driven cross-view contrastive loss to extract neural knowledge by contrasting the same augmented samples between views. Positive sample pairs are composed of representations of the same augmented samples in different views, while negative sample pairs are composed of representations of different augmented samples in different views. By minimizing the distance between positive sample pairs and maximizing the distance between negative sample pairs, the model learns complementary features that describe the internal and external manifestations of brain activity. The designed cross-view contrastive loss can be calculated as follows:\nLcross = 1/pair+ -log( pair+ / pair+ + pair ) (4)\npair+ = \u03a3bEB \u03a3i=0^m exp(s(rsa,b, ria,b)/T) (5)\npair- = \u03a3b\u2208B \u03a3i=0^m \u03a3j=i+1^m exp(s(rosa,b,rta,b)/T) (6)\nwhere pair+ and pair\u2212 represent the cross-view positive and negative pairs, respectively, B is the sample batch, and 7 is the temperature parameter. The function s() represents the cosine similarity. The representation generated from the scalp view is denoted as rs, and the representation generated from the inner neural topology view is denoted as rt. rsa and rta represent the corresponding augmented samples, and b indexes the samples contained in the batch.\nMasked language modeling is a widely adopted method for pre-training in NLP. BERT [38] retains a portion of the input sequence and predicts the missing content during the training phase, which generates effective representations for various downstream tasks. MAE can be represented as:\nxm = M(x), z = E(xm), x = D(z), (7)\nL = M(||x - X||2) (8)"}, {"title": "4 Discriminative-based EEG Analysis", "content": "where M(\u00b7) denotes the masking operation, xm represents the masked input, E(\u00b7) and D(\u00b7) represent the encoder and decoder.\nInspired by this, BENDR [79] follows the wav2vec2.0 [9] architecture. It first encodes EEG data into temporal embeddings using 1D convolutions, then creates a mask vector to randomly mask these embeddings. A transformer-based module [133] is then used to extract temporal correlations and output the reconstructed embeddings. The contrastive loss function aims to make the reconstructed embeddings as similar as possible to the original unmasked embeddings while making them as different as possible from the remaining embeddings. It can be calculated as follows:\nL = -log( exp(cossim(ct, bt))/\u043a / \u03a3bEBD exp(cossim(ct,bi))/\u043a ) (9)\nwhere ct represents the output of the transformer module at position t, bi represents the original vector at some offset i, BD is a set of 20 negative samples uniformly selected from the same sequence, along with bt, cossim denotes the cosine similarity, and is a temperature parameter controlling the contrastive loss.\nMAEEG [27] has a similar structure to BENDR but includes two additional layers to map the output of the transformer module back to the original EEG dimensions. The reconstruction loss is calculated by comparing the reconstructed EEG (x) with the input EEG (x) signal, using the formula 1 \u2013 x/x. The key difference between BENDR and MAEEG is that MAEEG learns representations by minimizing the reconstruction loss rather than using contrastive learning.\nUnlike the above two methods that mask temporal embeddings, WAVELET2VEC [107] performs masking and reconstruction tasks in different frequency bands to capture time-frequency information. Specifically, the authors apply low-pass and high-pass filtering to the raw EEG signal, recursively calculate the coefficients of each level of decomposition, and obtain wavelets in different frequency bands. They then design an encoder consisting of six parallel ViT [39] units, each corresponding to a frequency band wavelet. Each wavelet is flattened and divided into patches, and 10% of the input patches are randomly masked. The decoder reconstructs the missing patch sequences, and self-supervised pre-training is performed by minimizing the Euclidean distance between the patch sequences of the original signal and the reconstructed patch sequences. This method forces the model to learn the time-frequency information and understand its correlations by masking the frequency patch sequences of the EEG.\nFor a more profound comprehension of brain activity, this survey examines advanced architectures, including: Graph Neural Networks (GNNs) in section 4.1: These networks capitalize on the structural information inherent in brain connectivity to offer deeper insights. Foundation Models in section 4.2: Models pre-trained on extensive datasets and adaptable for specific EEG analysis tasks through fine-tuning. LLMs-based Methods in section 4.3: Leveraging the power of large language models to improve the interpretability of EEG data."}, {"title": "4.1 Graph Neural Networks", "content": "EEG data is a type of multi-channel time series data, in which multiple channels (brain regions) are related to each other, with structural and functional connectivity[104]. Due to brain regions are in non-Euclidean space, graph is the most appropriate data structure to indicate brain connection[63]. In recent years, graph neural networks(GNN), represented by graph convolutional networks(GCN)[36], have developed rapidly and become a powerful tool for learning non-Euclidean data representations. They are able to capture intricate relationships inter-variable and inter-temporal, therefore emerging as one of the mainstream frameworks for modeling multivariate time series. Motivated by the success of graph representation learning, a line of studies has utilized GNNs to perform multivariate time series analysis and demonstrate promising results in many downstream tasks such as classification[142], forecasting[17], and anomaly detection[37]. The survey by Jin et al. [68] has summarized the application of GNNs in time series analysis, but it does not specifically concentrate on EEG data and only briefly outlines the application in the field of healthcare. In contrast, this paper mainly focuses on EEG data, reviews the recent advances in mainstream EEG analysis tasks with GNNs. It covers a wide range of tasks such as epilepsy detection, sleep staging, and emotion recognition, and sorts out related works from the perspective of EEG graph construction and dependency modeling. All of the methods are summarized in table 2."}, {"title": "4.1.1 EEG Graph Construction", "content": "In general, each channel in the EEG signal is considered as a node in the graph. Referring to structural connectivity and functional connectivity, the methods for calculating adjacency matrix can be roughly divided into two categories. One is based on the geometry of EEG channels, the other is based on functional connectivity between brain regions. Based on the geometry between the channels, i.e., the anatomical connections between brain regions, previous studies have presented that adjacent brain regions affect each other and the strength of the impact is inversely proportional to the actual physical distance[116]. Thus, the adjacency matrix of the graph is constructed from the Euclidean distance between the electrodes, and it is worth noting that this matrix is the same for all EEG clips. The other is based on functional connectivity between brain regions, which captures dynamic brain connections that vary between different EEG clips. It is often calculated based on correlations or dependencies among signals, and the most common methods are Pearson Correlation Coefficient(PCC)[105], Mutual Information(MI)[33], and Phase Locking Value(PLV) [8].\nTang et al. [125] utilizes the above two methods to construct EEGs as graphs and only uses one type of graph as input at a time. Experimental results on the TUSZ v1.5.2 dataset show that the correlation-based graph structure can better localizes focal seizures than the distance-based graph. For a given EEG clip, Ho et al. [54] employs four different metrics to construct graphs, including nodes Euclidean distance, randomly connection of nodes, node features correlations, and directed transfer function. The first two are meant to capture the geometry of EEG channels and the last two are for capturing connectivity of brain regions.\nAlthough the correlation-based graph can be used even when the physical locations of electrodes are unknown, the adjacency matrix is still fixed, which limits its performance to a certain extent. To solve this problem, a lot of research has explored adaptive graph learning strategies. For example, GraphSleepNet[63] learns the connection relationship between two nodes based on their input features. Specifically, it is implemented through a layer neural network. If the distance between the features of the two nodes is larger, the connection of the two in the adjacency matrix is smaller. And the loss function is defined to be optimized towards this direction. The superiority of adaptive (learnable) adjacency matrix is demonstrated by comparing it with fixed adjacency matrices in the experiment. MSTGCN[62] uses the adaptive graph learning method proposed by GraphSleepNet[63], and also computes the spatial distance-based brain graph. Both views serve as the input of the model to extract features and a concatenate operation is employed to perform feature fusion on the two views. The results of the ablation experiment show that multi-view fusion is more effective than using only one single view. MD-AGCN[88] constructs temporal domain functional brain connectivity and frequency domain functional brain connectivity, respectively. Pearson's correlation coefficient is used as the connectivity index in the temporal domain. The frequency-domain adjacency matrix is divided into public part and private part. Public part is shared by all of the samples and is set to be trainable parameters, which illustrates the general functional brain connectivity patterns for emotional recognition. Private part is obtained by computing the dot product between two vertexes, and is unique to each sample. Before performing classification, functional brain connections in the two domains are combined together. By visualization of the learned graphs, the results indicate that the model can process global connectivities with the deep layers. BayesEEGNet[138] considers an electrical impulse between two nodes in the brain as a Poisson process, the countless electrical impulses generated by the brain in a period are represented as an infinite number of connection probability graphs. Then, the countless graphs are coupled into a summary graph by superposition of Poisson distributions, and the summary graph is subsequently transformed into the functional connectivity graph through two three-layer MLPs. By comparing with the adaptive learning strategy proposed by GraphSleepNet[63], the connectivity graph obtained in this paper has the best performance in downstream tasks."}, {"title": "4.1.2 Dependency Modeling and Graph Representation Learning", "content": "Once the EEG graph is constructed, it is often necessary to model the dependencies in the graph to learn the representation that is more discriminative for the downstream task. For example, Tang et al. [125] models the spatial dependency in the EEG signals by graph diffusion convolution. And to model the temporal dependency in EEGs, Gated Recurrent Units(GRUs) is employed. Also, in order to learn task-agnostic representations, a self-supervised pretraining method that predicts preprocessed signals for the next time period is proposed. For GraphSleepNet[63], a spatial-temporal convolution is designed, which consists of graph convolutions for capturing spatial features and temporal convolutions for capturing temporal context information. Moreover, the attention mechanism is applied in the spatial dimension and the temporal dimension respectively to extract valuable information. BayesEEGNet[138] also employs the spatial-based graph convolution to aggregate neighbor information directly in the spatial domain. For the emotion recognition task based on multi-modal signals, HetEmotionNet[61] first combines the temporal domain feature vector and the mutual information based adjacency matrix to form a heterogeneous spatial-temporal graph at the current moment, and then stacks the heterogeneous graphs of all time steps to form a heterogeneous graph sequence. Next, the Graph Transformer Network(GTN) is used to model the heterogeneity of multi-modal signals by automatically extracting the meta-paths from the adjacency matrix set. GCN is used to capture the correlation between multi-modal signals, and GRU is applied to extract temporal domain features from the graph sequence obtained after GCN. BrainNet[20] utilizes GCN to model two types of brain wave diffusion processes. Concretely, cross-time diffusion models the propagation of longer epileptic waves between two consecutive time segments. Meanwhile, fast signal spreading within the same time segments of each channel are captured by inner-time diffusion. The experimental results show that both diffusion processes can promote the performance of seizure detection.\nThere are also methods to mine patterns in a graph by designing self-supervised learning tasks. To capture the correlation patterns in space and time, MBrain[16] proposes two self-supervised tasks. Instantaneous time shift that is based on multi-channel Contrastive Predictive Coding(CPC) aims to capture the short-term correlations focusing on spatial patterns and delayed time shift is used for temporal patterns in broader time scales. In addition, replace discriminative learning is designed to preserve the unique characteristics of each channel so as to achieve accurate channel-wise seizure prediction. Ho et al.[54] leverages a random walk with restart(RWR) technique to create two positive and one negative sub-graphs for every node in every constructed EEG graph, and employs them to perform contrastive learning. Also, a generative learning module is proposed to learn the contextual information hidden in the graph through reconstructing the target node anonymized in the positive sub-graphs, using the other node features and edges of the sub-graph. To promote spatial consistency in multiple sensors, GCC[142] proposes novel graph augmentations including node augmentations and edge augmentations, to augment sensors and their correlations respectively. Next, a graph contrasting method is designed. Node-level Contrasting is achieved by contrasting sensors in different views within each sample while Graph-level Contrasting is achieved by contrasting the samples within each training batch. Through these two contrasting procedures, robust sensor-level features and global-level features can be learned."}, {"title": "4.2 Foundation Models", "content": "Foundation models (FMs)[14], often known as large-scale pretrained models, are advanced neural networks trained on extensive datasets. These models possess a vast range of general knowledge and can recognize numerous patterns. As a result, they offer a flexible and comprehensive foundation for addressing various tasks across multiple domains. ChatGPT[15] is the most famous textural foundation model that has a powerful ability to understand and generate natural language texts, and can perform a variety of natural language processing tasks, including text classification, sentiment analysis, machine translation, etc., showing extremely high flexibility and generalization capabilities. CLIP [110] and SAM [77] are representative visual foundation models, which exhibit robust general understanding and reasoning performance. Foundation models consistently demonstrate high performance in diverse domains, from natural language processing to computer vision, showcasing their versatility and the potential to revolutionize the way AI systems interact with and understand the world.\nIn the field of EEG data processing, researchers usually proposed specially designed methods or models for specific data or tasks. However, data annotation in the medical field is more difficult and expensive than"}, {"title": "4.2.1 Model Structure", "content": "in other fields. As a result, the size of EEG medical data sets is usually small, which greatly restricts the capabilities of the model [21, 135]. The emergence of large language models provides a new solution for the processing of biological signal data such as EEG. Recently, a lot of work has begun to draw on the ideas of large language models, using a large amount of unlabeled data and unsupervised pre-training methods to build foundation models for EEG or biological signal data [3, 23, 31, 64, 153, 155, 156]. These foundation models have learned a lot of knowledge about time series signals, can well represent EEG data, have generalization capabilities that previous models did not have, and can achieve excellent performance on different downstream tasks. Below, we outline the existing work related to foundation models in the field of EEG signals, considering the three important elements: data, model structure, and training methods. While the datasets themselves are thoroughly described in section 6, this chapter will focus on how they are used in the process of EEG foundation models established.\nWhile the datasets are crucial and will be extensively discussed, this chapter is dedicated to the presentation of the models and training methodologies. The summary of existing foundation models is shown as table 3.\nWith the rapid development of deep learning, many model structures have emerged, such as Convolutional Neural Network (CNN) [83], Recurrent Neural Network (RNN) [154], Transformers[134], Mamba [47], etc. How to design a model structure suitable for processing time series signals is the top priority in building a foundation model. A good structure can allow the foundation model to better understand and learn the information and knowledge in time series signals. Most of the existing EEG foundation models construct the main model by stacking Transformer layers or convolutional blocks. Because both structures have strong scalability and are suitable for mining information in time series signals."}, {"title": "4.2.2 Training Methods", "content": "Brant [156] has two encoders, temporal encoder and spatial encoder. The temporal encoder contains a 12-layer Transformer encoder and the spatial encoder contains a 5-layer Transformer encoder. They are used to capture the time correlation and channel correlation in time series signals, respectively. Salar et al. [3] built the foundation model based on an EfficientNet-style 1D convolutional neural network. Neuro-GPT [31] and LaBraM [64] use both convolutional layers and Transformers layers. They first use a small number of convolutional layers to preliminarily extract the features of time series signals and transform their dimensions, and then use a large number of Transformers layers to further capture the correlation between different sequence patches and better represent time series signals.\nSince the input of the Transformer layer is tokens, and the time series data is a continuous value, the foundation model needs to convert the time series data into patches before subsequent calculations can be performed. A common approach is to split the original data by a fixed window size and a fixed strides. Specifically, given a neural signal x \u2208 RN\u00d7C, where N is the number of timestamps and C is the number of electrode channels, we divide x with window size M and stride S to generate a set of patches p \u2208 RNp\u00d7C\u00d7M, where N = [N-M / S] is the number of patches in each channel. After obtaining the segmented patches, additional position or frequency encoding information is usually added to them to help the model learn better. Some researchers [64] also map each patch to a fixed codebook in order to make the foundation model have a fixed vocabulary like a large language model. Specifically, it first represents the patch and then utilizes quantizer to quantize all the patch representations into the neural codebook embeddings. The codebook looks up the nearest neighbor of each patch in the neural codebook.\nThe parameter size of the existing foundation models in the EEG field is usually between tens and hundreds of millions, which is still relatively small compared to the parameters of large language models. This may be because the amount of EEG data is still much smaller than text data. However, we believe that with the continuous development of the field, the scale of the foundation model will continue to increase, and its capabilities will continue to increase.\nIn order for the model to learn useful knowledge from massive amounts of unlabeled data, it is essential to design an effective training method. A good training method is like a good teacher, which can make the learning process more efficient.\nExisting foundation models are all pre-trained using self-supervised methods. One of the mainstream approaches is to use masked autoencoder (MAE) as a pre-training task [64, 137, 156]. MAE has been proven to be a simple and effective method in many fields, which trains model to reconstruct the whole input given its partial observation. In this way, the foundation model can be forced to infer the whole from partial information, so that the model can learn powerful representation capabilities.\nThere is another pre-training method that is similar to MAE, which can be understood as masking only the latter part of the input. During the training process, the model predicts the future situation based on the historical content of the time series data [31]. Its goal is actually the same as the short-term or long-term prediction in the downstream task. Therefore, the foundation model pre-trained by this method usually has strong predictive ability, which can capture regularities from historical time series data.\nAnother type of work uses contrastive learning to train the foundation model. The core idea is to learn how to effectively distinguish similar (positive) and dissimilar (negative) data points by comparing data samples, so as to optimize the data representation or feature vector. This method can help the model capture the intrinsic structure and relationship between data, thereby improving its generalization ability on downstream tasks. For example, Salar et al. [3] constructed positive and negative pairs at the participant level. Specifically, the positive pairs are selected as augmented views of two"}, {"title": "4.3 LLMs-based Methods", "content": "different segments from the same participant, while the segments from different subjects are regarded as negative samples. Through this training method, the model can not only acquire strong representation capabilities, but also enhance its generalization ability on different subjects.\nUsing various pre-training methods, the foundation model can acquire enough knowledge from a large amount of unlabeled data. Therefore, it only needs to be fine-tuned with a small amount of data to be well adapted to various downstream tasks. It can even have zero-shot capabilities like a large language model. This makes it possible to build a universal EEG foundation model.\nLarge Language Models (LLMs) [4, 128, 129"}]}