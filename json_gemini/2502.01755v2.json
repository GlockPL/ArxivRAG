{"title": "Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA", "authors": ["Shuangyi Chen", "Yuanxin Guo", "Yue Ju", "Hardik Dalal", "Ashish Khisti"], "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) optimize federated training by reducing computational and communication costs. We propose RoLoRA, a federated framework using alternating optimization to fine-tune LoRA adapters. Our approach emphasizes the importance of learning up and down projection matrices to enhance expressiveness and robustness. We use both theoretical analysis and extensive experiments to demonstrate the advantages of RoLoRA over prior approaches that either generate imperfect model updates or limit expressiveness of the model. We present theoretical analysis on a simplified linear model to demonstrate the importance of learning both down-projection and up-projection matrices in LORA. We provide extensive experimental evaluations on a toy neural network on MNIST as well as large language models including RoBERTa-Large, Llama-2-7B on diverse tasks to demonstrate the advantages of RoLORA over other methods.", "sections": [{"title": "1. Introduction", "content": "The remarkable performance of large language models (LLMs) stems from their ability to learn at scale. With their broad adaptability and extensive scope, LLMs depend on vast and diverse datasets to effectively generalize across a wide range of tasks and domains. Federated learning (McMahan et al., 2017) offers a promising solution for leveraging data from multiple sources, which could be particularly advantageous for LLMs.\nRecently, Parameter-Efficient Fine-Tuning (PEFT) has emerged as an innovative training strategy that updates only a small subset of model parameters, substantially reducing computational and memory demands. A notable method in this category is LoRA (Hu et al., 2021), which utilizes low-rank matrices to approximate weight changes during fine-tuning. These matrices are integrated with pre-trained weights for inference, facilitating reduced data transfer in scenarios such as federated learning, where update size directly impacts communication efficiency. Many works integrate LORA into federated setting (Zhang et al., 2023b; Babakniya et al., 2023; Kuang et al., 2023; Chen et al., 2024; Sun et al., 2024). FedPETuning (Zhang et al., 2023b) compares various PEFT methods in a federated setting. SLORA (Babakniya et al., 2023) presents a hybrid approach that combines sparse fine-tuning with LoRA to address data heterogeneity in federated settings. Furthermore, FS-LLM (Kuang et al., 2023) presents a framework for fine-tuning LLMs in federated environments. However, these studies typically apply the FedAVG algorithm directly to LoRA modules, resulting in in-exact model updates, as we will discuss later in the paper.\nTo address the issue of in-exact model updates, a few recent works have proposed modifications to the down-projection and up-projection components in LoRA. In FlexLoRA (Bai et al., 2024), the authors propose updating these projections with matrix multiplication followed by truncated SVD. A related method is also considered in (Wang et al., 2024). Another approach, by Sun et al., introduces a federated finetuning framework named FFA-LORA (Sun et al., 2024), which builds on LoRA by freezing the down-projection matrices across all clients and updating only the up-projection matrices. They apply differential privacy (Dwork et al., 2006) to provide privacy guarantees for clients' data. With a sufficient number of finetuning parameters, FFA-LORA, using a larger learning rate, can achieve performance comparable to FedAVG of LoRA while reducing communication costs by half. However, we observe that with fewer finetuning parameters, FFA-LORA is less robust than FedAVG of LoRA, primarily due to its reduced expressiveness from freezing down-projections. In this work, we explore the necessity of learning down-projection matrices and propose a federated fine-tuning framework with computational and communication advantages.\nWe connect the objective of learning down-projection matrices in a federated setting to multitask linear representation learning (MLRL), an approach in which a shared low-rank representation is jointly learned across multiple tasks. While, to the best of our knowledge, the alternating optimization of down- and up-projection matrices has not been explored within the context of LoRA, prior works on MLRL (Collins et al., 2021; Thekumparampil et al., 2021) have demonstrated the importance of alternately updating low-rank representations and task-specific heads, demonstrating the necessity of learning a shared representation. Inspired by MLRL, we tackle this challenge by employing alternating optimization for LoRA adapters. We theoretically establish that alternating updates to the two components of LORA, while maintaining a common global model, enable effective optimization of down-projections and ensure convergence to the global minimizer in a tractable setting."}, {"title": "1.1. Main Contributions", "content": "\u2022 RoLoRA framework. We propose RoLoRA, a robust federated fine-tuning framework based on the alternating optimization of LoRA as shown in Figure 1. RoLoRA fully leverages the expressiveness of LORA adapters while keeping the computational and communication advantages.\n\u2022 Theoretical Insights. We show that in a tractable setting involving a local linear model, RoLoRA converges exponentially to the global minimizer when clients solve linear regression problems, using rank-1 LoRA adapters. In this case, RoLoRA is reduced to an alternating minimization-descent approach, outperforming FFA-LORA, whose fixed down-projection limits performance. This highlights the importance of training the down-projection in LoRA for improved federated learning performance.\n\u2022 Empirical results. Through evaluations on a two-layer neural network with MNIST and on large language models (ROBERTa-Large, Llama-2-7B) across various tasks (GLUE, HumanEval, MMLU, Commonsense reasoning tasks), we demonstrate that RoLoRA maintains robustness against reductions in fine-tuning parameters and increases in client numbers compared to prior approaches."}, {"title": "1.2. Notations", "content": "We adopts the notation that lower-case letters represent scalar variables, lower-case bold-face letters denote column vectors, and upper-case bold-face letters denote matrices. The $d \\times d$ identity matrix is represented by $I_d$. Depending on the context, $||.||$ denotes the $l_2$ norm of a vector or the Frobenius norm of a matrix, $||.||_{op}$ denotes the operator norm of a matrix, $|.|$ denotes the absolute value of a scalar, $T$ denotes matrix or vector transpose. For a number $N$, $[N] = \\{1, ..., N\\}$."}, {"title": "2. Related Works", "content": "Parameter Efficient Fine Tuning (PEFT): LoRA and Its Enhancements Parameter efficient finetuning (PEFT) allows for updates to a smaller subset of parameters, significantly reducing the computational and memory requirements. One of the most well-known methods is LoRA(Hu et al., 2021). LoRA uses low-rank matrices to approximate changes in weights during fine-tuning, allowing them to be integrated with pre-trained weights before inference. In (Zhang et al., 2023a), the authors propose a memory-efficient fine-tuning method, LoRA-FA, which keeps the projection-down weight fixed and updates the projection-up weight during fine-tuning. In (Zhu et al., 2024), the authors highlight the asymmetry between the projection-up and projection-down matrices and focus solely on comparing the effects of freezing either the projection-up or projection-down matrices. (Hao et al., 2024) introduces the idea of resampling the projection-down matrices, aligning with our observation that freezing projection-down matrices negatively impacts a model's expressiveness. Furthermore, (Hayou et al., 2024) explore the distinct roles of projection-up and projection-down matrices, enhancing performance by assigning different learning rates to each.\nPEFT in Federated Setting PEFT adjusts only a few lightweight or a small portion of the total parameters for specific tasks, keeping most foundational model parameters unchanged. This feature can help reduce data transfer in federated learning, where communication depends on the size of updates. Zhang et al. (Zhang et al., 2023b) compares multiple PEFT methods in federated setting, including Adapter(Houlsby et al., 2019), LoRA(Hu et al., 2021), Prompt tuning(Liu et al., 2022) and Bit-Fit(Zaken et al., 2022). SLORA(Babakniya et al., 2023), which combines sparse finetuning and LoRA, is proposed to address the data heterogeneity in federated setting. As discussed before,"}, {"title": "3. Preliminaries", "content": "3.1. Low-Rank Adaptation: LoRA\nLow-Rank Adaptation (LoRA) (Hu et al., 2021) fine-tunes large language models efficiently by maintaining the original model weights fixed and adding small, trainable matrices in each layer. These matrices perform low-rank decompositions of updates, reducing the number of trainable parameters. This approach is based on the finding that updates to model weights during task-specific tuning are usually of low rank, which allows for fewer parameters to be adjusted. For example, for a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times d}$, the update is a low-rank product $AB$, where the down-projection $A \\in \\mathbb{R}^{d \\times r}$ and the up-projection $B \\in \\mathbb{R}^{r \\times d}$, with $r < d$. Only $A$ and $B$ are trainable, allowing $W = W_0 + \\alpha AB$, with $\\alpha$ adjusting the update's impact.\nApplying LoRA in a federated setting is a practical choice. By using LORA adapters, clients can fine-tune foundation models efficiently with limited resources. Since only these specific matrices need to be transmitted to a central server, this approach significantly reduces communication costs. This makes LoRA an advantageous solution for enhancing model performance in collaborative scenario comparing to full parameter finetuning in the federated setting.\n3.2. FedAVG of LORA Introduces Interference\nIntegrating LoRA within a federated setting presents challenges. In such a setup, each of the $N$ clients is provided with the pretrained model weights $W_0$, which remain fixed during finetuning. Clients are required only to send the updated matrices $B_i$ and $A_i$ to a central server for aggregation. While most current studies, such as SLORA (Babakniya et al., 2023) and FedPETuning (Zhang et al., 2023b), commonly apply FedAVG directly to these matrices as shown in (2), this approach might not be optimal. The precise update for each client's model, $\\Delta W$, should be calculated as the product of the low-rank matrices $A_i$ and $B_i$. Consequently, aggregation on the individual matrices leads to inaccurate model aggregation.\n$\\begin{aligned}\n& \\frac{1}{N} \\sum_{i=1}^{N} \\Delta W_i  \\\\ & \\neq \\frac{1}{N} \\sum_{i=1}^{N} A_i B_i  \\\\ & = (A_1B_1 + A_2B_2 + ... + A_NB_N)  \\\\ & \\neq (\\frac{1}{N} A_1 + A_2 + ... + A_N ) (B_1 + B_2 + ... + B_N)\n\\end{aligned}$\nThere are a few options to avoid it."}, {"title": "4. RoLORA Framework", "content": "In this section, we describe the framework design of RoLoRA and discuss its practical advantages.\nAlternating Optimization and Corresponding Aggregation Motivated by the observations discussed in Section 3.2, we propose applying alternating optimization to the local LORA adapters of each client in a setting with $N$ clients. Unlike the approach in FFA-LORA, where $A$ is consistently frozen, we suggest an alternating update strategy. There are alternating odd and even communication rounds designated for updating, aggregating $A$ and $B$, respectively.\n$\\begin{aligned}\n&\\text { In the odd comm. round: } \\frac{1}{N} \\sum_{i=1}^{N} \\Delta W_i^{t+1} \\\\\n& = \\frac{1}{N} \\sum_{i=1}^{N} A^t (B_i^{t+1} + B_i^{t+1} +...+ B_i^{t+1}) \\\\\n& = A^*(B_1^{t+1} + B_2^{t+1} + ... + B_N^{t+1}) \\\\\n\\end{aligned}$\n$\\begin{aligned}\n&\\text { In the even comm. round: } \\frac{1}{N} \\sum_{i=1}^{N} \\Delta W_i^{t+2} \\\\\n& = \\frac{1}{N} \\sum_{i=1}^{N} (A_i^{t+1}B_i^{t+1} + A_i^{t+1}B_i^{t+1} +...+ A_i^{t+1}B_i^{t+1}) \\\\\n& = (A_1^{t+1} + A_2^{t+1} + ... + A_N^{t+1})B^{t+1} \\\\\n\\end{aligned}$\nAs in Algorithm 1, all clients freeze $A^t$ and update $B_i^t$ in the odd communication round. The central server then aggregates these updates to compute $B^{t+1} = \\frac{1}{N} \\sum_{i=1}^{N} B_i^{t+1}$ and distributes $B^{t+1}$ back to the clients. In the subsequent communication round, clients freeze $B^{t+1}$ and update $A_i^t$. The server aggregates these to obtain $A^{t+1} = \\frac{1}{N} \\sum_{i=1}^{N} A_i^{t+1}$ and returns $A^{t+1}$ to the clients. It is important to note that in round $2t + 1$, the frozen $A$ are identical across all clients, as they are synchronized with $A^t$ from the central server at the beginning of the round. This strategy ensures that the update and aggregation method introduces no interference, as demonstrated in (3) and (4)."}, {"title": "Computation and Communication Cost", "content": "Computation and Communication Cost The parameter-freezing nature of RoLoRA enhances computational and communication efficiency. In each communication round, the number of trainable parameters in the model is effectively halved compared to FedAVG of LoRA. The only additional cost for RoLoRA compared to FFA-LORA is the alternating freezing of the corresponding parameters. We remark this additional cost is negligible because it is applied to the clients' models and can be executed concurrently during the server's aggregation."}, {"title": "5. Analysis", "content": "In this section, we provide an intuitive analysis of the necessity of training down-projection of LoRA module in a federated setting. We first theoretically compare RoLoRA and FFA-LORA on a linear model. Then we empirically verify the effectiveness of the theoretical analysis on a two-layer neural network."}, {"title": "5.1. Federated LoRA on a Toy Model", "content": "Consider a federated setting with $N$ clients, each with the following local linear model\n$f_i(X_i) = X_i a b^T$\nwhere $Y_i \\in \\mathbb{R}^{m \\times d}, X_i \\in \\mathbb{R}^{m \\times d}$ with the sample size $m$, $a \\in \\mathbb{R}^d$ (a unit vector) and $b \\in \\mathbb{R}^d$ are the LoRA weights corresponding to rank $r = 1$. In this setting, we model the local data of i-th client such that\n$Y_i = X_i a^* b^{*T}$\nfor some ground truth LoRA weights $a^* \\in \\mathbb{R}^d$ (a unit vector) and $b^* \\in \\mathbb{R}^d$. We consider the following objective\n$\\min_{a, b} \\frac{1}{N} \\sum_{i=1}^{N} l_i(a, b)$\nwhere the local loss is $l_i(a, b) = ||X_i a^* b^{*T} - X_i a b^T||_2^2$. Each $X$ is assumed to be a Gaussian random matrix, where each entry is independently and identically distributed according to a standard Gaussian distribution.\nWe remind the reader that Section 1.2 provides a summary of mathematical notations and also point to Table 3 in Appendix A.1 for a summary of the symbols used throughout the theoretical analysis.\nResults. In this section, we assume homogeneous clients where there is a single target model as in (6). In the special case with the model as in (5) and the objective in (7), we modify RoLoRA from Algorithm 1 to Algorithm 2, employing alternating minimization for b (line 5) and gradient descent for a (line 9). Details are described in Algorithm 2. We note that the analysis of the alternating minimization-gradient descent algorithm is inspired by (Collins et al., 2021; Seyedehsara et al., 2022; Vaswani, 2024) for a different setting of MLRL. We aim to show that the training"}, {"title": "5.2. Verifying Results On a Two-Layer NN", "content": "The previous analysis considers a simple linear model for each client. To assess the validity in a non-linear model, we consider a two-layer neural network model on each client given by\n$f_i(x_i) = ReLU(X_i A B) W_{out}$\nwhere $W_{out} \\in \\mathbb{R}^{d \\times c}, A \\in \\mathbb{R}^{d \\times r}$ and $B \\in \\mathbb{R}^{r \\times d}$ are weights. We train the model on MNIST (Deng, 2012) with 60,000 training images. We consider two different ways to distribute training images to clients. The first is to distribute the images to 5 clients and each client gets access to training images of two specific labels, while the second is to distribute the images to 10 clients and each client only has training images of one specific label. There is no overlap in the training samples each client can access. Only weights matrices $B$ and $A$ are tunable, while $W_{out}$ are fixed. We apply the typical initialization, where $A$ is initialized to a random Gaussian matrix, and $B$ is initialized to zero. We use $c = 10, d = 784, r = 16$ and make each client train 5 epochs locally with batch-size 64 and aggregate clients' update following three methods: FedAVG of LORA, referred as LORA; FFA-LORA (Sun et al., 2024), which freezes $A$ during training, and RoLoRA, which alternately update $B$ and $A$. We experiment with multiple learning rates, display the best performance of each method in Figure 2.\nAs shown in Figure 2, we evaluate the performance of the model in each iteration on the test set with 10,000 images. We observe that the accuracy of FFA-LORA plateaus around 55% in both settings, which aligns with our theoretical analysis. The decline in LoRA's performance with an increasing number of clients is most likely due to less accurate model aggregation, as demonstrated in (1) and (2). Notably, RoLoRA demonstrates greater robustness in these settings."}, {"title": "6. Experiments on LLMs", "content": "In this section, we evaluate the performance of ROLORA in various federated settings. Considering all clients will participate in each round, we will explore the following methods based on FedAVG.\n\u2022 LoRA means LoRA adapter and its finetuning algorithm are directly applied to local finetuning of clients in the federated system. Specifically, in iteration $t$, the server receives A and B from client $i$ and aggregates by $A^t = Avg(A)$ and $B^t = Avg(B)$.\n\u2022 LORA-FFA (Sun et al., 2024) is a baseline that enable the clients to finetune B and keep A frozen locally. Thus, in iteration $t$, the server aggregates by $B^t = Avg(B)$.\n\u2022 RoLoRA enables clients to alternate updating A and B as described in Section 4.\nFlexLoRA (Bai et al., 2024) fine-tunes large models in a federated setting by aggregating the matrix products of LORA components and compressing them using truncated-SVD. However, due to its significant memory and computation overheads it is not directly comparable with other schemes. Nonetheless, we include its performance in Table 6 in Appendix.\nImplementation & Configurations. We implement all the methods based on FederatedScope-LLM (Kuang et al., 2023). We use NVIDIA GeForce RTX 4090 or NVIDIA A40 for all the experiments. To make a fair comparison, for each dataset, we obtain the best performance on test set and report the average over multiple seeds. Specifically, the learning rate is chosen from the set $\\{5e \u2013 4, 1e \u2013 3, 2e \u2013 3, 5e \u2013 3, 1e \u2013 2, 2e \u2013 2, 5e \u2013 2, 1e \u2013 1\\}$. Other hyper-parameters for experiments are specified in Table 4 in Appendix B.1. Please note that in all tasks, we compare the performance of the three methods under the same number of communication rounds."}, {"title": "6.1. Language Understanding Tasks", "content": "Model and Datasets. We take the pre-trained ROBERTa-Large (355M) (Liu et al., 2019) models from the Hugging-Face Transformers library, and evaluate the performance of three federated finetuning methods on 5 datasets (SST-2, QNLI, MNLI, QQP, RTE) from the GLUE (Wang et al., 2019). GLUE benchmark is a comprehensive set of tasks for evaluating the performance of language models on a variety of natural language understanding tasks. Due to the limitation of the unpublished test set in GLUE, we follow the previous studies (Zhang et al., 2023b) and use the original validation set as the new test set and split a part of the training set as the validation set."}, {"title": "6.2. Commonsense Reasoning Tasks", "content": "Model, Datasets. We evaluate RoLoRA against FFALORA and LoRA on Llama-2-7B(Touvron et al., 2023) for commonsense reasoning tasks. The commonsense reasoning tasks include 8 sub-tasks, each provided with predefined training and testing datasets. Following the setting in (Hu et al., 2021), we merge the training datasets from all 8 sub-tasks to create a unified training dataset, which is then evenly distributed among the clients. Evaluations are conducted individually on the testing dataset for each sub-task.\nResults In Table 2, we compare the results of three methods with Llama-2-7B models on 8 commonsense reasoning tasks. The configurations are presented in Appendix B.2.4. The performance is reported as the mean accuracy with standard deviations across 3 trials. RoLoRA consistently achieves the highest accuracy across all tasks, demonstrating significant improvements over both LoRA and FFA-LORA. We also highlights that FFA-LORA exhibits large performance variances across trials, such as a standard deviation of 9.55 for PIQA and 8.44 for SIQA, respectively. This significant variability is likely due to the initialization quality of parameter A, as different initializations could lead to varying optimization trajectories and final performance outcomes as discussed in Section 5. Additional results on this task are presented in Table 10 in Appendix B.2.4."}, {"title": "7. Conclusion", "content": "In this work, we introduce RoLoRA, a federated framework that leverages alternating optimization to finetune LoRA adapters. Our approach highlights the role of learning down-projection matrices to enhance both expressiveness and robustness. Through theoretical analysis on a simplified linear model, and comprehensive experiments on a toy neural network and large language models like ROBERTa-Large and Llama-2-7B, we show that RoLORA outperforms existing methods that limit model updates or expressiveness."}, {"title": "A. Theoretical Analysis", "content": "A.1. Notation\nTable 3 provides a summary of the symbols used throughout this theoretical analysis.\nA.2. Auxiliary\nDefinition A.1 (Sub-Gaussian Norm). The sub-Gaussian norm of a random variable $\\xi$, denoted as $|\\|\\xi\\|\\|_{\\psi_2}$, is defined as:\n$\\|\\xi\\|\\|_{\\psi_2} = \\inf\\{t > 0: E[\\exp(\\xi^2/t^2)] \\leq 2\\}.$\nA random variable is said to be sub-Gaussian if its sub-Gaussian norm is finite. Gaussian random variables are sub-Gaussian. The sub-Gaussian norm of a standard Gaussian random variable $\\xi \\sim N(0, 1)$ is $\\|\\xi\\|\\|_{\\psi_2} = \\sqrt{8/3}$.\nDefinition A.2 (Sub-Exponential Norm). The sub-exponential norm of a random variable $\\xi$, denoted as $|\\|\\xi\\|\\|_{\\psi_1}$, is defined as:\n$\\|\\xi\\|\\|_{\\psi_1} = \\inf\\{t > 0: E[\\exp(|\\xi|/t)] \\leq 2\\}.$\nA random variable is said to be sub-exponential if its sub-exponential norm is finite."}, {"title": "A.3. Vector-vector case with heterogeneous clients", "content": "Consider a federated setting with $N$ clients, each with the following local linear model\n$f_i(X) = X_i a b$\nwhere a \u2208 Rd is a unit vector and b \u2208 Rd are the LoRA weights corresponding to rank r = 1. In this setting, we model the local data of i-th client such that Y\u2081 = X\u00bfa*b* for some ground truth LoRA weights a* \u2208 Rd, which is a unit vector, and local b \u2208 Rd. We consider the following objective\n$\\,\\,\\,\\text{examined} (a, b)$\nWe consider the local population loss $l_i(a, b) = \\|a^* b - ab \\|_2^2$.\nWe aim to learn a shared model (a, b) for all the clients. It is straightforward to observe that (a', b') is a global minimizer of if and only if a'b' = a*b*, where b* = * =1 b. The solution is unique and satisfies a' = a* and b\u2032 = b*. With this global minimizer, we obtain the corresponding minimum global error of\u22111||a* (b - b*) ||2."}]}