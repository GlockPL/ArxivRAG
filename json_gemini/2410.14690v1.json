{"title": "Rethinking VLMs and LLMs for Image Classification", "authors": ["Avi Cooper", "Keizo Kato", "Chia-Hsien Shih", "Hiroaki Yamane", "Kasper Vinken", "Kentaro Takemoto", "Taro Sunagawa", "Hao-Wei Yeh", "Jin Yamanaka", "lan Mason", "Xavier Boix"], "abstract": "Visual Language Models (VLMs) are now increasingly being merged with Large Language Models (LLMs) to enable new capabilities, particularly in terms of improved interactivity and open-ended responsiveness. While these are remarkable capabilities, the contribution of LLMs to enhancing the longstanding key problem of classifying an image among a set of choices remains unclear. Through extensive experiments involving seven models, ten visual understanding datasets, and multiple prompt variations per dataset, we find that, for object and scene recognition, VLMs that do not leverage LLMs can achieve better performance than VLMs that do. Yet at the same time, leveraging LLMs can improve performance on tasks requiring reasoning and outside knowledge. In response to these challenges, we propose a pragmatic solution: a lightweight fix involving a relatively small LLM that efficiently routes visual tasks to the most suitable model for the task. The LLM router undergoes training using a dataset constructed from more than 2.5 million examples of pairs of visual task and model accuracy. Our results reveal that this lightweight fix surpasses or matches the accuracy of state-of-the-art alternatives, including GPT-4V and HuggingGPT, while improving cost-effectiveness.", "sections": [{"title": "1 Introduction", "content": "Many of the best methods for visual representation learning now make use of Visual Language Models (VLMs)1\u20133. These models combine vision and language (e.g. with contrastive training) in order to learn general vision-language representations. Recently, several new VLMs have been developed that combine VLMs with pre-trained Large Language Models (VLM+LLMs)\u20204\u20136.\nSince LLMs are trained on large corpora of text and computer code, combining pre-trained LLMs with VLMs has brought many benefits to VLMs, including the addition of outside knowledge and reasoning, iterative interaction, and an ability to provide open-ended responses.\nWhile these new abilities are impressive, image classification tasks in which the image is categorized into a closed set of possible choices, such as in classic object recognition, constitute one of the primary goals of VLMs and computer vision in general. At the moment, it is unclear whether the additional capabilities of VLM+LLMs also contribute to an overall improvement to image classification. In the literature on VLM+LLMs, the evaluation focus has primarily centered on assessing the new abilities introduced by LLMs, i.e. open-ended answering and interactivity, with limited attention given to evaluating their impact on classifying the image into a given set of possible choices as it was traditionally done in image classification.\nThus, in this work we re-evaluate VLM+LLMs on a diverse set of visual tasks with the goal of answering: when does leveraging LLMs improve model performance in traditional image classification setup?\nWe evaluate the performance of existing open source VLMs and VLM+LLMs across a range of benchmarks designed to test object and scene recognition as well as visual reasoning and outside knowledge. There are three possible outcomes to this experiment that a priori are not possible to discern:\n1. VLM+LLMs are superior in all the tasks for image classification. This result may be the most expected, as VLM+LLMs currently dominate state-of-the-art accuracy in most vision tasks.\n2. VLMs outperform VLM+LLMs depending on datasets. If we observe this, it is worth investigating when VLMs are better than VLM+LLMs."}, {"title": "2 Do VLM+LLMs Always Beat VLMs?", "content": "In this section we analyze the capabilities of VLM+LLMs across multiple image classification tasks. We first introduce the VLMs and VLM+LLMs that we analyze and then introduce the evaluation methodology, which includes an overview of the datasets and prompting strategies used. Finally, we present the findings derived from our analysis."}, {"title": "2.1 Comparing VLM+LLMs with VLMs", "content": "To fairly compare the effect of leveraging LLMs for vision-language processing, we compare VLM+LLMs and VLMs that have exactly the same visual representations. That is, that use identical vision encoders. In this way we assess the effectiveness of different strategies for handling text, one that uses LLMs and the other that does not.\nNote that including an LLM in a VLM necessitates the inclusion of additional machinery around the LLM to integrate it with the vision encoder (fusion modules, training objective, additional data, etc.). Two prevalent strategies exist in designing VLM+LLM models, namely that visual information can be incorporated into the LLM via embeddings (e.g. Flamingo) or via text descriptions of the image (e.g. PNP-VQA\u2074). We investigate models from each approach."}, {"title": "2.2 Returning to Closed-Ended Evaluation", "content": "In language modelling tasks, such as translation and image captioning, evaluation is typically open-ended where models engage in open-ended text generation. On the other hand, image classification is inherently a closed-ended task, requiring a selection from a set of options rather than general description. In the open-ended case, evaluation is unclear because the degree of detail required is somewhat ambiguous. It is not sufficient to say that there is a tree in the image \u2014 the model must decide whether it is an elm, oak or willow tree if the task requires so. Thus, we adopt closed-ended evaluation for all experiments.\nThe methods we employ to compute closed-ended predictions differ across model types. For VLMs, the prediction of the model is the closed-ended option whose representation is best aligned with the image representation through a dot product. For VLM+LLMs, like PNP-VQA and Flamingo, the logits of each of the closed-ended options are used to compute a sequence probability. The sequence of logits with the highest average probability is the prediction of the model. As we show below in the results, differences in how models handle closed-ended prediction does not affect the overall conclusions."}, {"title": "2.3 Zero-Shot Visual Tasks", "content": "LLMs excel at zero-shot tasks13 and can capture nuanced relationships and contextual information that might not be explicitly present in visual data. Conceivably, the extensive knowledge encapsulated by LLMs could be harnessed to address visual tasks when there is a lack of training data.\nZero-shot visual tasks allow us to assess the effectiveness of the synergy between VLMs and LLMs in a more controlled manner than if there would be task-specific training samples for fine-tuning. To avoid adding confounding factors arising from fine-tuning design choices (like deciding which layers to unfreeze) all experiments in this paper revolve around zero-shot"}, {"title": "2.4 Prompt Choice", "content": "It is well known that prompt choice can have a significant impact on model performance23\u201328, and work has been conducted to specifically improve LLM's reasoning capabilities, through Chain-of-Thought29, Tree of Thought30 and similar processes.\nDuring evaluation we observed that varying the prompt dramatically affected the evaluation performance of the various networks. For example, when evaluating CLIP on CIFAR-100, using prompts of the form \"What is this? This is aquarium_fish\" or \"What is this? This is beaver\" (with class names inserted as-is from the CIFAR-100 dataset), resulted in an accuracy of 44.6%. We modified some of the class names, separating words and using simpler forms where appropriate (for example, \"aquarium_fish\" to \"aquarium fish\" and \"aeroplane\" to \"plane\"). We also prepended the appropriate article (a/an) in object classification tasks. The result was prompts of the form \"What is this? This is an aquarium fish\" and \"What is this? This is a beaver.\" These changes alone increased accuracy of CLIP on CIFAR-100 up from 44.7% to 69.1%.\nWe also found that alternative phrasing of the question could impact performance. For example, on the Abstract VQA dataset, prompting with the form \"Is the dog asleep?\" yielded an accuracy of 8.3% in one Flamingo model, while prompting with the form \"Using the image, the answer to Is the dog asleep? is most likely\" yielded an accuracy of 29.3% for the same model.\nFor this reason, we evaluated all models on a range of prompt formulations appropriate for each dataset. A full description of the class name alterations and prompt variations can be found the in Appendix B."}, {"title": "2.5 Results", "content": "We analyze results on 462 experiments, including ten datasets, seven models and between two to nine prompt strategies per dataset. Results are displayed in Table 1 and Figure 2. Table 1 reports the average accuracy among prompts and also the best accuracy among prompts, for each model and dataset. We highlight in boldface the best model per dataset, and underline the best model among each group of VLM+LLM and corresponding VLM. In order to ensure that the winning models displayed in Table 1 are largely prompt-independent, in Figure 2 we depict the dependency of our results on the prompting strategy. Concretely, in Figure 2a, we display the distribution of accuracy across prompt variations, indicating with different colors whether the model was a VLM or a VLM+LLM. As expected, there is substantial variability in accuracy among models across different prompt variations, sometimes more than 50%. Despite this high variability, Figure 2b shows the proportion of times a VLM or VLM+LLM obtains higher accuracy for each prompt variation. We observe consistent model type across datasets, which highlight that the trends are prompt-independent.\nThe key trends that we observe by analyzing the results in Table 1 and Figure 2 are twofold:\n1. For most datasets that require visual reasoning and outside knowledge, VLM+LLMs achieves superior accuracy (see Figure 2b and last six columns in Table 1). VLM+LLMs showcase superior capabilities in extracting high-level semantic information and adapting to conceptual understanding, which may be an expected result. Note that the superior performance of VLM+LLMs cannot be attributed to the fact that the VLM+LLMs contain more parameters to process text. Despite the fact that LiT leverages a text encoder with many more parameters than CLIP's, CLIP achieves superior results to LiT on visual tasks that require reasoning. Thus, parameter count alone does not explain the superior performance of Flamingo over CLIP.\n2. For most datasets of object and scene recognition, VLMs consistently outperform their corresponding VLM+LLMs (see Figure 2b and the first four result column in Table 1), i.e. BLIP outperforms PNP-VQA and CLIP outperforms Flamingo. Note that this result cannot be attributed to the possibility that VLM+LLMs might struggle with closed-ended evaluation, as VLM+LLMs still excel in closed-ended visual tasks that require reasoning, as discussed before in point 1. We also observe that comparing the two versions of Flamingo, i.e. with the LLMs with 3B and 9B parameters, provides some improvements in some cases, but it is not sufficient to outperform VLMs. All this suggests that the reduction in accuracy may be attributed to the way the LLM is integrated with the vision encoder.\nIt is notable that all models struggle in some of these benchmarks, with accuracies close to chance, specifically for the Hateful Memes and Skin Cancer datasets. This highlights the need for further improvement in zero-shot visual tasks, despite the significant strides in recent years.\nRegarding the best-performing model among all, there is no clear winner. For recognition datasets, CLIP and LiT dominate. In datasets that require a higher degree of reasoning and outside knowledge, Flamingo and PNP-VQA excel depending on the dataset. Thus, exploring the integration of these models into a unified system becomes a key research direction, allowing us to leverage the strengths of each model for optimal performance in all scenarios. In the next section, we pursue this idea by introducing a computationally efficient solution that combines the strengths of all the models into one system."}, {"title": "3 A Fix: Routing the Task to the Best Model", "content": "Consider the following set up: a user provides a machine learning system with an image and a question about the image, and the response options. In Section 2 we have seen that VLMs and VLM+LLMs complement each other in the tasks they excel at. Since there is no single best model, we aim to design a system that can select the right model based on the user input. We propose to pass the user input to an LLM which then learns to route the input to the best model for the user's task."}, {"title": "3.1 Training an LLM to Act as a Router", "content": "Previous LLM-based model selection systems make use of human written descriptions of the model capabilities or synthetically generated data31. In contrast, we use real model execution results calculated during our experiments (i.e. the experiments done to create Table 1). Importantly, this means our router learns from experience with actual data rather than some possibly inaccurate proxy as in previous works.\nAfter running experiments over all models, datasets and prompts we create a dataset of more than 2.5 million samples containing (i) an input prompt and image, (ii) a set of options to respond with, (iii) model performance. All of the input is represented in natural language. For the input image, we calculate simple image metadata in the form of image resolution and per channel mean and standard deviations which are prepended to the prompt. The set of response options is a list of possible answers to the visual task. We find that the router performs best when the list of response options are not included as input to the LLM router, even though these are necessary to execute the predicted model for closed-ended evaluation. Per sample model"}, {"title": "3.2 Evaluating on Held-Out Datasets", "content": "To test the generalization abilities of the router on new tasks, we test on unseen datasets in a cross-validation style. We train ten separate models, each time holding out one of the ten datasets which is then used to evaluate the performance of the router. Thus, by testing the LLM router on out-of-distribution datasets, we can properly evaluate its generalization capabilities.\nWe initially compare the performance of our LLM router against basic model selection techniques both in terms of accuracy on unseen data and computational cost, expressed as the number of models that must be run in order to use the technique. These model selection baselines are the following:\nChance-no models are run and a random label is assigned to each image. Average-we report the average performance over all models, note that in the limit this is the same as randomly sampling a model each time a new input is received. Voting-all models are run and the modal output is selected, this can be seen as a form of model ensemble. Oracle-requires knowing the average accuracies for all models on the held-out dataset and then selecting the best performing model for a given input. Upper bound-per image oracle that shows how often at least one of the available models correctly classifies a given input.\nIn Table 2 we see that our router outperforms all baselines, achieving the best performance on the largest number of datasets (seven of ten) as well as the best average performance. Most notably our method outperforms the strongest baseline of ensemble voting, whilst being more computationally efficient at inference time. Ensemble voting requires executing every model on the inputs, making it linear in the number of models. On the other hand, our method only executes the single model chosen by the router, so is of constant order. Compared to the best baseline with the same computational cost (average), our method achieves almost a ten-percentage point improvement on average. Regarding the baselines that require access to ground truth results on the held-out dataset (oracle and upper bound), our method is very close to the oracle performance, suggesting our router often picks the best model despite not having access to ground truth data. Finally, the upper bound column shows that by collating only a relatively small number of models we can theoretically achieve extremely high performance if the perfect model could be picked for every individual image, which indicates that there is still room for future improvements."}, {"title": "3.3 Comparison with State-of-the-art", "content": "We additionally compare against two modern state-of-the-art solutions that are applicable to our use case. Firstly HuggingGPT which uses prompt design and in-context learning to get GPT-313 to provide a model, or set of models, from the HuggingFace Library34 to execute the task and then summarize the results. Secondly, we consider the newly released GPT-4V(ision)8,35. We use GPT-4V to evaluate a very recent approach which, motivated by scaling laws36,37, focuses on massive compute and data as a solution for improving visual understanding. Whilst technical details about GPT-4V are relatively limited we do know that this system is substantially more compute heavy than our relatively small LLM router and open-source models, any one of which can run on a single V100 GPU. Details of our implementations of these methods can be found in Appendix C.\nTable 3 compares our LLM routing approach against these baselines. We note that due to rate restrictions and high monetary costs (Appendix C) this table evaluates a total of 365 samples randomly selected (40 per dataset, except Weather and Skin Cancer, which have 24 and 21, respectively). Whilst this limited number of samples means we must be careful not to read too much into small differences in performance for individual datasets, some interesting trends emerge overall.\nWhen comparing to HuggingGPT, our approach outperforms HuggingGPT on nine of ten datasets and by more than ten percent absolute accuracy on average. Although HuggingGPT has access to many more models than our system, its reliance on text descriptions of the models and in-context learning means it fails to perform as well as our approach which is trained on actual model accuracy. What's more, due to the careful prompt engineering and in-context samples, queries to HuggingGPT run for several thousand tokens which at current pricing for GPT-3 (text-davinci-003) means we faced an inference cost of approximately $0.05 per sample.\nWhen comparing to GPT-4V our method performs competitively with the average performance difference being less than one percent, this is despite our method requiring vastly less computational resources.\nWe also evaluated an LLM router with LLaMA as the LLM. However, we did not observe an improvement of accuracy, possibly because more training data is needed given LLaMA has more parameters than GPT-2. In Appendix E we show further results with the LLaMA router.\nThe success of our LLM router approach can be attributed to its specific training on a dataset that includes pairs of visual tasks and model accuracy. In contrast, other approaches rely on proxies like human-generated descriptions of the models."}, {"title": "3.4 Analyses and Ablations", "content": "In this section we further analyze our results, first by exploring how the router makes decisions on held-out datasets and then by conducting an ablation study where we experiment with the input information provided to the router."}, {"title": "4 Related Work", "content": "Foundation Models: VLMs & LLMs. Foundation models38 have changed the landscape of vision and language, with large image39-41, language13,42,43 and explicitly multimodal5,6,35,44\u201346 transformer based models47 creating new capabilities in terms of generalization and low-shot performance48. Of these various models, our work is most concerned with Vision Language Models 1-3 and methods that combine VLMs with LLMs4,5,49.\nSome recent intriguing findings highlight the nuanced relationship between LLMs and VLMs5,50,51. In the study by Roth et al.50, CLIP evaluated on images with randomly generated query texts demonstrates comparable performance with CLIP evaluated on images with LLM-generated query texts. This is further evidence, complementary to ours, that how best to use LLMs to improve VLMs is remains an open question. Meanwhile, a careful examination of the Flamingo paper, reveals that it lags behind state-of-the-art VLM models in ImageNet and Kinetics700 classification tasks as shown in Appendix B in. This trend can also be seen by comparing Tables 5 and 19 in the results of PaLI. Our findings makes this observation explicit and general. By focusing on the difference in performance of VLM+LLMs and their corresponding VLMs on classification tasks we reveal VLM+LLMs' limitations across multiple datasets and models and then, we propose a solution to it.\nRouting, Model Selection & Augmented LLMs. Model selection is a standard problem in machine learning52 but is most commonly examined in the case where validation data is used to select the best single model by estimating test error for future i.i.d. test data. In our case we instead use other datasets to learn a model which can dynamically select the best model for a given task, which we refer to as routing. Most similar to our work is concurrent work on routing for language benchmarks53, which has interesting links to meta-learning54. Yet, the results of Shnitzer et al.53 are limited to only natural language tasks.\nMany tool augmented LLMs55\u201359, that is LLMs that can reference external tools, can be viewed as performing routing to enable tasks beyond their initial training data. Several systems have been proposed to allow LLMs to reference external machine learning models however, these works are either aspirational in nature60, use synthetic data itself generated by an LLM31, or rely on prompt engineering and in-context learning7,61. In contrast, our method for routing learns cost-effective strategies from accuracies obtained by running and evaluating models, which serve as training data for the LLM router."}, {"title": "5 Conclusions", "content": "We found that VLM+LLMs were better at advanced reasoning and knowledge tasks but were worse on object and scene recognition tasks when compared with VLMs with the same vision encoder. To address this issue, we proposed the use of a low-resource LLM that serves as a router, intelligently selecting the most suitable model among VLMs and VLM+LLMs for each visual task query. This LLM router strategy achieved higher accuracy than other model selection and ensemble baselines, and is superior or on par with several other state-of-the-art approaches, like HuggingGPT and GPT-4Vision, which use far more computational resources. Our LLM router's effectiveness is rooted in its specialized training on a corpus of examples, consisting of pairs of visual tasks and model accuracy. In contrast, competing approaches use proxies such as human-generated descriptions of the models."}, {"title": "B.1 Prompt Grammar", "content": "In order to methodically explore a range of different prompts, we outlined a small grammar to compose prompts. The grammar is as follows:\n\u2022 key: Datasets have several text fields, including the class name and additional text-context. One value that is used across datasets is class_name. When this value is used, n prompts are generated, where n is the number of classes in the dataset (for ex, 100 in the case of CIFAR-100) and each prompt is set to one of the class names. Some datasets have dataset-specific keys\n\u2022 a_an: An argument that adds an a/an appropriately to the key\n\u2022 rename_classes: If the key text has a defined substitution, the key will be swapped for its rename\n\u2022 arg: Can be one of [a_an, rename_classes]\n\u2022 {[arg]* + key}: The tag is fully described by zero or multiple args, a | character (which can be omitted if no args are specified) and the key\nAdditional text can surround the tags.\nFurther, each dataset defines a set of questions and options. If a set of options is not defined, this means that the options are provided by the dataset for each sample. A prompt is formed by concatenating a question to an option formulation. Sometimes we leave the question empty to test whether the class name alone is the best prompt. The cartesian product between all question and option formulations defines the full set of prompts. Below we outline the prompts, options and additional text fields for each dataset."}, {"title": "B.2 Prompts for each dataset", "content": "CIFAR-100\n\u2022 Question Formulations\n6611\n\"This is \"\n\"What is this? This is \"\n\u2022 Option Formulations\n\"{class_name}\"\n\u201c{rename_classes | class_name}\"\n\u2013 \u201c{a_an, rename_classes | class_name}\"\n\u2022 Class Renames\naquarium_fish: aquarium fish\npickup_truck: pickup truck\nlawn_mower: lawn mower\nsweet_pepper: pepper\nmaple_tree: maple\noak_tree: oak\npalm_tree: palm\npine_tree: pine\nwillow_tree: willow\nOOD-CV Has the same question and option formulations as CIFAR-100\n\u2022 Class Renames\naeroplane: plane\ndiningtable: table\nWeather\n\u2022 Question Formulations\n6611\n\"It is \"\n\"The weather is \"\n\u2022 Option Formulations\n\"{class_name}\"\n\u201c{rename_classes | class_name}\"\n\u2022 Class Renames\nSunrise: sunrise\nCloudy: cloudy\nShine: sunny\nRain: rainy"}, {"title": "Skin Cancer", "content": "\u2022 Question Formulations\n6611\n\"This is \"\n\"This skin is \"\n\u2022 Option Formulations\n\"{class_name}\"\n\u201c{rename_classes | class_name}\"\n\u2022 Class Renames\nmelanoma: cancerous\nnotmelanoma: healthy\nHateful Memes Below, the key text is provided by the dataset for each sample. It is the text-string of the text written on the meme.\n\u2022 Question Formulations\n6611\n\"{text}. \"\n\"{text}. This meme is \"\n\"This is an image of a meme. It contains the text: {text}. The meme is \"\n\u2022 Option Formulations\n\"{class_name}\"\n\u201c{rename_classes | class_name}\"\n\u2022 Class Renames\nnot mean: nice\nmean: mean\nScienceQA Below, the keys class_question and class_hint are provided by the dataset for each sample. class_question is the question for each sample, while class_hint is some additional (unnecessary but helpful) context for each sample provided by the dataset.\n\u2022 Question Formulations\n6611\n\"{class_question} \"\n\"{class_hint} {class_question} \"\n\"Question: {class_hint} {class_question} \"\n- \"{class_hint} Question: {class_question} \""}, {"title": "Visual Genome Attribution & Relation", "content": "Visual Genome Attribution & Relation Below, the key class_question is provided by the dataset for each sample. It is the question for each sample.\n\u2022 Question Formulations\n6611\n\"{class_question} \"\n\"This best describes the image: \"\nAbstract Scenes VQA and Binary Abstract Scenes Below, the key class_question is provided by the dataset for each sample. It is the question for each sample.\n\u2022 Question Formulations\n6611\n\"Question: {class_question} Answer: \"\n\"Using the image, the answer to {class_question} is most likely \""}, {"title": "B.3 Prompting PNP-VQA", "content": "The implementation of PNP-VQA requires a question to serve as the prompt, separately from the options. However for several datasets, we tried prompts that have an empty question, denoted with \"\". In these cases, we provide the empty string to PNP-VQA, to ensure that evaluation is the same across models. However PNP-VQA cannot provide a response in these cases. For the experiments in Table 1 we use all the prompts described in this section. When training the planner, to ensure all models have the same number of prompts and to avoid prompt/model dependence, we remove this question from further analysis for all models."}, {"title": "C Baseline Implementation", "content": "This section describes how the baselines GPT-4V and HuggingGPT were implemented to create a fair comparison with our LLM router."}, {"title": "C.1 GPT-4V(ision)", "content": "GPT-4V(ision) was released for developer access on 11/6/23. Our experiments took place between 11/13/23 and 11/16/23. During this period GPT-4V was in preview, meaning organizations could only make 100 API requests per day, which limited the number of images we were able to test with it.\nFurther, though the API documentation provides for options that might be used to approximate close-ended evaluation, we were unable to make these parts of the API perform as documented for GPT4-V. We therefore took an alternative approach to simulate closed-ended evaluation. We provided the following system prompt:\n\"Complete the prompt from the list called OPTIONS, enclosed by [ and ]. Your response can only include a single element from this list\"\nand in the user message, we provided the prompt and the closed set of response options within brackets. For each dataset we pick the prompt that is valid for all models and achieved the highest accuracy in the experiments to create Table 1.\nTo evaluate GPT-4V each completion was determined to be one of:\n\u2022 Correct Prediction: if the response of the model contained only one element from OPTIONS and it was the correct label.\n\u2022 Incorrect Prediction: if the response of the model contained only one element from OPTIONS and it was an incorrect label.\n\u2022 No Guess: if the response contained no elements from OPTIONS\n\u2022 Multiple Guesses: if the response contained multiple elements from OPTIONS"}, {"title": "C.2 HuggingGPT", "content": "To run HuggingGPT we use the code made publicly available at https://github.com/microsoft/JARVIS, the code was pulled on 10/19/23, HuggingGPT uses text-davinci-003 as the LLM. Due to making multiple requests with long context prompts, we found HuggingGPT cost approximately $0.05 per-sample, or around $20 to calculate results on the small subset of data in Table 3 (excluding experimentation and development requests). We evaluated HuggingGPT in the same manner as GPT-4V, but calculated only one completion per sample due to these costs.\nFor the most part we used the default settings provided in the repository, making the following changes to the prompting to encourage the system to select from the set of valid response options.\n\u2022 System Prompt:\nOriginal: \"#4 Response Generation Stage: With the task execution logs, the AI assistant needs to describe the process and inference results.\"\nOurs: \"#4 Response Generation Stage: With the task execution logs, the AI assistant needs to provide the best completion for the prompt provided by the user.\"\n\u2022 User Message:\nOriginal: \"Yes. Please first think carefully and directly answer my request based on the inference results. Some of the inferences may not always turn out to be correct and require you to make careful consideration in making decisions. Then please detail your workflow including the used models and inference results for my request in your friendly tone. Please filter out information that is not relevant to my request. Tell me the complete path or urls of files in inference results. If there is nothing in the results, please tell me you can't make it.\"\nOurs: \"Yes. Please complete the prompt from the list called OPTIONS, enclosed by [ and ]. Your response can only include a single element from this list.\"\nThe user message is then additionally given the instruction:\n\"Using the image and the options provided please complete the following prompt. {prompt}... OPTIONS: {response options}\", where {prompt} and {response options} are replaced with the relevant prompt and response options for the given sample. As with GPT-4V, for each dataset we pick the prompt that is valid for all models and achieved the highest accuracy in the experiments to create Table 1.\nWe also found HuggingGPT was rarely able to actually access and run models from the Hugging Face Hub. Due to this limitation and to improve reproducibility we ran HuggingGPT in local mode, downloading all relevant implemented models. The models available locally to HuggingGPT were downloaded on 11/11/23. The full list of models we made available to HuggingGPT is:\n\u2022 nlpconnect/vit-gpt2-image-captioning\n\u2022 Illyasviel/ControlNet\n\u2022 Illyasviel/sd-controlnet-canny\n\u2022 Illyasviel/sd-controlnet-depth\n\u2022 Illyasviel/sd-controlnet-hed\n\u2022 Illyasviel/sd-controlnet-mlsd"}, {"title": "D.1 Closed-Ended Evaluation", "content": "In Section 2.2, we motivated our use of closed-ended evaluation. As mentioned, the implementation of closed-ended evaluation differed across models. Here we outline the implementation in various model architectures. This evaluation is computed for each sample in the dataset, where each sample is an image and the corresponding set of text prompts (0 \u2208 0): question and response options.\nConstrastive Models This group includes CLIP, BLIP, and LiT. The model has two different encoders, one for images and one for text. For each sample, we extract the N-dimensional image embedding, v; \u2208 RN, and the text embedding matrix, Vo \u2208 R|O|\u00d7N, where rows of Vo are the N-dimensional embedding of the prompts o \u2208 O. We then take the prediction to be the option indexed by argmax(v \u00b7 Vo).\nLanguage Models This group includes PNP-VQA, and Flamingo. Models of this type can return a log-probability for the next token, across all tokens in the vocabulary, given the preceding tokens. These probabilities are generally used for generating new text, but they can also be used for closed-ended evaluation. For each prompt, o \u2208 O, we sum the log probabilities for each of the tokens in the prompt. The prediction is then the response option used to create the prompt which has the highest probability."}, {"title": "D.2 Data Format", "content": "Section 3.1 describes the creation of training data for the LLM router. After this process a single sample in the dataset looks like\n[img]dim::(270,317,3)ave::(23.1,31.8,\n46.2)std::(15.1,11.5,10.9)[prompt] What\nis this? This is ;;; [ `a car', 'a sofa',\n'a train', 'a table', 'a chair',\n'a boat', 'a plane', `a motorbike',\n'a bus', 'a bicycle' (SEP) model::\nclip[response]correct::True;;;\navg_accuracy::0.88238\nThe values following the [img] marker are the resolution of the image along with per-channel mean and standard deviation of the pixel values. Then the prompt that should be completed is provided along with the set of response options (a car, a sofa etc.). These components are the possible inputs to the router that we experiment with. After the [SEP] token is a label for the model (CLIP). The final part is the boolean that marks whether or not the model gave the correct response for the image in questions as well as the average accuracy for the model on the dataset the image is from. The boolean and the average accuracy are used to select the best model for each image which is then used to train the LLM router as described in Section 3.1."}, {"title": "D.3 GPT-2 Implementation", "content": "For GPT-2", "https": "huggingface.co/gpt2 The optimizer was Adam with an initial learning rate of 2 \u00d7 10\u20134. The batch size was 1, we trained for 5000 iterations, after every 1000 iterations, an evaluation"}]}