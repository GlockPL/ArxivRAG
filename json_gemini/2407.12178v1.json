{"title": "Exploration Unbound", "authors": ["Dilip Arumugam", "Wanqiao Xu", "Benjamin Van Roy"], "abstract": "A sequential decision-making agent balances between exploring to gain new knowledge about an environment and exploiting current knowledge to maximize immediate reward. For environments studied in the traditional literature, optimal decisions gravitate over time toward exploitation as the agent accumulates sufficient knowledge and the benefits of further exploration vanish. What if, however, the environment offers an unlimited amount of useful knowledge and there is large benefit to further exploration no matter how much the agent has learned? We offer a simple, quintessential example of such a complex environment. In this environment, rewards are unbounded and an agent can always increase the rate at which rewards accumulate by exploring to learn more. Consequently, an optimal agent forever maintains a propensity to explore.", "sections": [{"title": "1 Introduction", "content": "Consider an unscrupulous geometry teacher and a persistent student eager to learn the digits of the mathematical constant \u03c0\u2248 3.1415926535. On each day, the student approaches the teacher and may provide any arbitrarily-long sequence of digits. For any k-digit sequence they provide, the teacher simply checks to see if these exactly match the first k digits of \u03c0or not. If there is such an exact k-digit match, the teacher awards the student r(k) dollars; otherwise, for anything less than a perfect match, the teacher charges the student c(k) dollars. The teacher sets r(k) and c(k) to be increasing functions of the number of digits and, therefore, guessing longer digit sequences offers more potential upside but also increased risk from erroneous guesses.\nSuppose rewards are bounded and the student wishes to maximize expected discounted return. Then, if on any day, it is optimal to exploit current knowledge by choosing k digits of the student has learned thus far, it will be optimal to choose those same k digits of on every subsequent day. This is because the theory of sequential decision-making problems establishes that, when rewards are bounded, there is an optimal policy that maps each possible state of knowledge to a single action. In our example, the student's state of knowledge remains unchanged by the exploitative choice. This leads to indefinite daily repetition of the same knowledge state and same optimal action.\nOn the other hand, if rewards are unbounded, there may always be substantial value in exploring to learn more. In this circumstance, the aforementioned behavior, where on some date the student exploits and continues to do so indefinitely, can fall far short of optimal. Instead, an optimal policy may have to randomize"}, {"title": "2 Problem Formulation", "content": "We consider a bandit environment (Lattimore & Szepesv\u00e1ri, 2020) with a (possibly infinite) action set A and stochastic reward process {Rt}t\u2208Z>0. Here, {Rt}t\u2208Z>0 is an exchangeable sequence of random vectors, each with one component per action; each vector Rt assigns a scalar reward Rt,a to each action a. At each time t \u2208 Z>0, an action At is executed and generates a scalar reward Rt+1,A\u2081. If there exists an R\u2208 R such that E[Rt,a] < R for each time t and action a, we say that rewards are bounded. Otherwise, we say that rewards are unbounded.\nBy de Finetti's Theorem (de Finetti, 1937), exchangeability immediately implies the existence of a random variable \u03b8, representing the unknown bandit environment, conditioned upon which the sequence {Rt}t\u2208Z>0 is iid. A common example may clarify our formulation; consider the Bernoulli bandit with unknown success probabilities \u03b8 \u2208 [0, 1]A and observe that, conditioned on \u03b8, the rewards {Rt}t\u2208Z>0 are iid. Hence, {Rt}t\u2208Z>0 is exchangeable. The initial distribution of \u03b8 expresses prior beliefs. At time t, there is a history Ht = (A0, R1, A0, A2,..., At-1, Rt, At\u22121) of actions and realized rewards, and posterior beliefs are expressed by the distribution of \u03b8 conditioned on Ht.\nA (stationary) policy \u03c0 is a mapping from histories to action probabilities. In particular, for any history h and action \u03b1, \u03c0(\u03b1 \u2758h) is the probability assigned to executing action a upon observing history h. Hence, if an agent applies a policy \u03c0, each action At is sampled from \u03c0(\u00b7 | Ht). To frame a notion of optimality across policies, we first define the expected finite-horizon discounted return of a policy \u03c0:\n\\(V^{\\pi}_0 = E\\left[\\sum_{t=0}^{T-1} \\gamma^t R_{t+1, A_t}\right].\\)\nHere, \u03b3\u2208 [0,1] is a discount factor and T\u2208 Z>0 is a time horizon. Note that this expectation integrates over uncertainty not only in @ but also in rewards conditioned on \u03b8.\nA policy \u03c0 is said to be discounted-overtaking optimal if, for all policies \u03c0',\n\\(\\liminf_{T\to\\infty}(V^{\\pi}_0 - V^{\\pi'}_0) \\geq 0.\\)"}, {"title": "3 Necessity of Randomized Exploration", "content": "In this section, we define and analyze a complex environment that will be our main object of study for the remainder of the paper.\nExample 1. We define a bandit with an action set \\(A = \bigcup_{k=0}^{\\infty} \\mathbb{Z}^k\\). Hence, each action \\(a \\in A\\) is a positive-integer-valued tuple \\((a_1,...,a_k)\\) of some arbitrary length \\(k \\geq 0\\). The stochastic process of reward vectors \\(\\{R_{t+1}\\}_{t\\in\\mathbb{Z}_{>0}}\\) is parameterized by fixed, known scalars \\(a > 1\\) and \\(\\tau\\in (1, \\frac{\\alpha}{\\alpha-1})\\), and an unknown positive-integer sequence \\(a^* = (a_1^*, a_2^*, a_3^*, ...)\\) such that, for each \\(a \\in \\mathbb{Z}^k\\),\n\\(R_{t+1,a} = \\begin{cases}a^k, & \\text{ if } a = a^*_{1:k} \\\\\\\\-a^{k-1}, & \\text{ otherwise.} \\end{cases}\\)\nNote that the action set includes the length 0 vector, which we will denote by \\(\u00d8 = a_{1:0}\\) and yields reward \\(R_{t+1,0} = -1\\).\nThe reward process \\(\\{R_{t+1}\\}_{t\\in\\mathbb{Z}_{>0}}\\) is exchangeable since \\(R_1 = R_2 = ...\\). As \\(\\{R_{t+1}\\}_{t\\in\\mathbb{Z}_{>0}}\\) is determined by a*, we define its prior distribution in terms of a prior distribution of a*. In particular, for each \\(k \\in \\mathbb{Z}_{>0}\\), let the distribution \\(p_k(\\cdot|a_{1:k-1})\\) of \\(a^*_k\\) conditioned on \\(a^*_{k-1}\\) be geometric with mean \u03c4. Then, let the prior probability assigned to \\(a^*_{1:K}\\) be \\(\\prod_{k=1}^{K} p_k(a^*_k|a_{1:k-1})\\).\nWe say that an agent is exploiting at timestep t if its chosen action At is a previously selected action known to achieve highest reward, given history Ht. Otherwise, we say that an agent is exploring. Example 1 offers a representative instance of how the presence of infinitely-many actions and unbounded rewards demands an infinite amount of exploration in order to synthesize optimal behavior. An initial thought may be to purely explore in perpetuity and continually uncover higher tiers of reward. Unfortunately, the cost structure associated with incorrect actions (that do not match the goal sequence a*) is calibrated such that this policy is provably not optimal.\nTheorem 1. In Example 1, an agent that always explores is never discounted-overtaking optimal.\nUpon further reflection, it is perhaps not terribly surprising that a strategy of pure exploration is sub-optimal in Example 1, as is often the case for many sequential decision-making problems studied in the literature. However, unlike these latter commonly-studied problems, we may also obtain an analogous theoretical result establishing that any policy which ceases exploration in any time period is also provably not-optimal. This represents a more substantial departure from the traditional literature, where the presence of bounded rewards (even with infinitely-many actions) guarantees the existence of an optimal policy which eventually exploits with probability 1; we defer a review of this prior work to Section 4.\nTheorem 2. In Example 1, an agent that stops exploring is never discounted-overtaking optimal."}, {"title": "4 Discussion", "content": "Two key facets of the complex environment studied in this work are the presence of infinitely-many actions and unbounded rewards. In this section, we begin with an overview of prior work, which largely focuses on the former condition in the absence of the latter, as well as a small handful of papers from outside the machine-learning literature which consider both conditions together. We conclude with a discussion of how one might begin to approach the design of practical agents for such complex environments and offer a simple computational experiment to corroborate our proposal."}, {"title": "4.2 Towards Practical Agent Design", "content": "While we offer theoretical results which underscore the importance of randomization for a discounted-overtaking optimal policy in a complex environment, it is perhaps not immediately apparent how to go about the practical implementation of a computational agent that could learn or approximate this optimal behavior from interaction data. We anticipate that the concept of a learning target (Lu et al., 2023) is essential to the design and practical implementation of such an agent. When, at any given time, optimal behavior is so complicated that it requires too much information to learn, it behooves the agent to have a mechanism for prioritizing some other modest corpus of information that, while capable of facilitating behavioral improvement, is itself insufficient to enable near-optimal performance; broadly speaking, a learning target is such a mechanism. As the agent gains competency through its prolonged interaction with the environment, one might envision that this learning target could adapt in kind to reflect updated knowledge and reorient exploration towards new, feasible discoveries.\nA recent line of work (Arumugam & Van Roy, 2021a;b; 2022) has studied the design, analysis, and implementation of decision-making agents endowed with the ability to compute such learning targets and autonomously decide what to learn. We strongly suspect that deciding what to learn and striking a desired trade-off between information requirements and performance is a critical capability for an agent coping with complex environments where eternal exploration is the only path to optimal behavior."}, {"title": "A Computational Experiment", "content": "In this section, we provide additional details on the concluding computational experiment of Section 4. The environment can be seen as a restricted version of Example 1 where the action set A = {0,1,..., 99} consists of all two-digit sequences and the agent aims to learn the first two digits of with a = 2.\nGiven the agent's current (posterior) beliefs about the underlying environment P(\u03b8\u2208 Ht), a Thompson Sampling agent proceeds via the probability-matching principle to select an action At such that P(At = a | Ht) = P(A* = a | Ht), where A* \u2208 arg maxa\u2208AE [R1,a | \u03b8]. Broadly speaking, one might consider an alternative learning target x \u2208 A such that an agent may employ a variant of Thompson Sampling by probability matching with respect to x: P(At = a | Ht) = P(x = a | Ht).\nA line of work (Arumugam & Van Roy, 2021a;b; Arumugam et al., 2024) studies how to compute such a learning target via information theory (Shannon, 1948; Cover & Thomas, 2012). More specifically, when targeting an optimal action A*, the mutual information between the environment and target It (\u03b8; A*) given the current (random) history Ht quantifies the amount of information an agent must obtain through prudent exploration in order to identify optimal behavior. In the context of this work, a complex environment is one for which this amount of information is near-infinite or intractably large It (\u03b8; A*) \u2191\u221e across all time periods. Consequently, an agent may instead find it fruitful to orient exploration around an alternative target X that is easier to learn, in the sense that It (\u03b8; x) < It(\u03b8; A*). Of course, as the agent still aims to be productive with respect to the task at hand and optimize reward, this should be done carefully so as to incur bounded expected regret in each time period: Et [Rt,A* - Rt,x] < D, for some threshold D\u2208 R\u22650.\nStriking a desired balance between information and utility is a hallmark characteristic of lossy compression problems studied by the information theory community within the sub-area of rate-distortion theory (Shannon, 1959; Berger, 1971). The fundamental limit for the lossy compression faced by an agent is given by the rate-distortion function\n\\(R_{\\lambda}(D) = \\inf_{A} I_t(\\theta; A) \\text{ such that } E_t\\left[\\frac{\\left(R_{t,A^*} \u2013 R_{t,A}\\right)^2}{\\lambda}\\right] \\leq D.\\)\nThe Rate-Distortion Thompson Sampling (RDTS) algorithm of Arumugam et al. (2024) provides a theoretical analysis outlining the benefits of computing and probability matching with respect to the target At that achieves the rate-distortion limit in each time period. While their study pertains to a fixed distortion threshold D \u2208 R\u22650, our computational experiments employ an adaptive version where the dynamic threshold Dt is chosen to ensure the agent focuses its efforts on identifying the digits of \u03c0 in sequence, rather than pursuing a guess for all digits at once like Thompson Sampling (corresponding to D\u2081 = 0 for all time periods). While previous work (Arumugam & Van Roy, 2021a) has avoided dealing directly with the rate-distortion function computationally by appealing to the classic Blahut-Arimoto algorithm (Blahut, 1972; Arimoto, 1972), our experiment leverages the fact that the rate-distortion function constitutes a convex optimization problem (Csisz\u00e1r, 1974; Chiang & Boyd, 2004) which can be solved in each time period via CVXPY (Diamond & Boyd, 2016)."}, {"title": "B Analysis", "content": "In this section, we provide all proofs for theoretical results presented in the main paper.\nB.1 Proof of Theorem 2\nProof. For ease of exposition, we refer to each component in an action tuple (a1,..., ak) as one digit. The analogy is made for the illustrative \u03c0-guessing game in Section 1. A history Ht = (A0, R1, A1, ..., At, Rt+1) can be completely characterized by an agent state St made up of digits tried and failed so far as well as"}, {"title": "B.4 Proof Roadmap for Conjecture 1", "content": "NSmProof roadmap. Recall the expression for Vin Equation (1). Consider the first summand in the expectedvalue. We letfn(m; T) = E1\u00b5 + nm \u2264 T (m + 1)an\u22121 \u2212 (\u03bc\u03b7 \u2212 1)TTT\u22121n)]To decouple the dependence between the indicator random variable 1\u00b5 + nm \u2264 T and the multiplier((m + 1)an\u22121 \u2212 (n \u2212 1)an\u22121), we define an independent copy in of n and analyzefn(m;T)= E1\u00b5 + \u00b5n + nm \u2264 T (m + 1)an\u22121 \u2212 (\u03bc\u03b7 \u2212 1)nT\u22121n)]= P\u00b5 + \u00b5n + nm \u2264TE (m + 1)an\u22121 \u2212 (\u03bc\u03b7 \u2212 1)an\u22121)] = P\u00b5 + \u00b5n + nm \u2264T(m\u2212a)an\u22121.We may then proceed to lower bound PP \u2211\u00b5j +\u00b5n + nm \u2264T using Kolmogorov\u2019s inequality. An upper bound on fn(m;T) can be obtained by taking each \u00b5j = 1 in the indicator. Finally, we account forthe difference fn(m;T) \u2212 fn(m; T). A similar analysis can be carried out for the second and third summandin Equation (1). Optimizing the upper and lower bounds of V over m gives two sequences mr and mr,respectively, which both converge to a. Thus, the corresponding exploitation probabilities should convergeto \u03b1+1\u03b1+\u03c4."}]}