{"title": "RUMI: Rummaging Using Mutual Information", "authors": ["Sheng Zhong", "Nima Fazeli", "Dmitry Berenson"], "abstract": "This paper presents Rummaging Using Mutual In-formation (RUMI), a method for online generation of robot action sequences to gather information about the pose of a known movable object in visually-occluded environments. Focusing on contact-rich rummaging, our approach leverages mutual information between the object pose distribution and robot trajectory for action planning. From an observed partial point cloud, RUMI deduces the compatible object pose distribution and approximates the mutual information of it with workspace occupancy in real time. Based on this, we develop an information gain cost function and a reachability cost function to keep the object within the robot's reach. These are integrated into a model predictive control (MPC) framework with a stochastic dynamics model, updating the pose distribution in a closed loop. Key contributions include a new belief framework for object pose estimation, an efficient information gain computation strategy, and a robust MPC-based control scheme. RUMI demonstrates superior performance in both simulated and real tasks compared to baseline methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Active exploration, the process of autonomously planning actions to gather more information about a target quantity, is a core problem in robotics, particularly when dealing with unknown environments [2]. This problem encompasses a range of scenarios, differentiated by the type of robot (e.g., mobile vs. stationary), the primary sensor modality (often vision), and the specific quantity to be estimated.\nAs robotics applications have transitioned from known, structured environments like factories to the unknown, dynamic environments of homes, new challenges have emerged. One critical application area is object manipulation, where visual perception is often hindered by occlusions caused by both the environment and the objects themselves [54]. To address these challenges, we focus on actively exploring to estimate the pose of a movable object with a known shape through contact-rich interactions, commonly referred to as rummaging.\nOcclusions of the target object, both from itself and from other objects, motivate the need to use contact to determine the object's pose. Our prior work has investigated how to track the position of contact points during rummaging with an unknown number of objects [53], and how to estimate the plausible set of object poses given observed contact and free space points [54]. However, the problem of how to plan information-gathering trajectories to estimate a movable object's pose is still under-explored. A primary challenge is the object's mobility, coupled with the requirement for contact-based information collection. Without careful planning, making contact can inadvertently push the object out of the robot's workspace, as evidenced in our experiments.\nActive exploration is often framed from an information-theoretic perspective, where the quantity to be estimated is treated as a random variable, and actions are selected to minimize its uncertainty. This approach can be computationally expensive, necessitating a trade-off between accuracy and speed or limiting the exploration to a single next best action. Additionally, some methods restrict the action space to movements along the object's surface [47], [13]. While this restriction simplifies the problem, it also limits the robot's capabilities. Instead, we aim to enable robots to make and break contact dynamically throughout the rummaging process, enhancing their exploratory capabilities.\nTo address the above challenges, we present Rummaging Using Mutual Information (RUMI), an active exploration method. Specifically, our contributions include:\n1) a framework for creating and updating a belief over poses given observed point clouds, augmented with volumetric semantics such as whether each point is in free space or on the surface of the object, based on the discrepancy formulated in CHSEL [54]\n2) a measure of information gain based on the mutual information between the object pose and volumetric semantics at the positions that the robot trajectory will cover, and show that it can be efficiently computed in parallel for dense workspace points in real time\n3) a closed loop MPC planning framework using cost functions based on the information gain and maintaining object reachability, and a stochastic object dynamics model\nIn our experiments, we show that RUMI is the only method to achieve consistent success in simulated and real robot rummaging tasks across various objects."}, {"title": "II. RELATED WORK", "content": "In a broad sense, we focus on the problem of actively exploring an unknown environment to reduce the uncertainty of some quantity. There are many variants and names for the problem, including active sensing [42], sensor path planning [5], active perception [2], and interactive perception [3]. The variants differ primarily by the robot type (mobile vs stationary base), sensing modality, and by the quantity to be estimated; e.g. the map of the environment [29], [38], [18], the shape of an object [13], [51], the pose of an object (object localization) [10], [1], or the pose of effective grasps for objects [20], [37]. In the case of unknown object shape, or reconstruction from a set of objects, the problem is also known as active shape completion [41]. This paper focuses on estimating the pose of a movable rigid object with a known shape.\nIn general, active exploration is the iterative process of:\n1) forming a belief over state given observations\n2) computing expected information gain over a workspace\n3) planning an action sequence\n4) executing some of the action sequence and collecting observations"}, {"title": "A. Representing Belief", "content": "Representations suitable for active exploration have been studied extensively. In many cases, parametric filters like the extended Kalman Filter (EKF) [43], [27] may be used when the posterior of the quantity of measure should be approximately Gaussian. Otherwise, non-parametric methods like particle filters [11], [21] are often used. Occupancy grids have also been popular, e.g. used in the simultaneous localization and mapping (SLAM) variant of active exploration [34], [49], [8]. In particular, when assuming each grid cell is independent, information gain based on the entropy of all the cells may be efficiently computed on an occupancy grid. We make a similar assumption that enables efficient computation of our information gain.\nRecently, Gaussian processes (GPs) [18] have also been used for estimating object shape. GP implicit surfaces (GPIS) have shown strong representation power [12], [13]. GPIS uses a GP to output a field in which the 0-level set represents the surface of the object. In our method, we do not need the full representation power of a GP since we have a known object shape. Instead, we use a particle filter to represent the pose distribution, and present a novel way to evaluate the particle probabilities given an observed point cloud."}, {"title": "B. Information Gain", "content": "The information gain can be formulated in many ways, often depending on the belief representation. For GPIS the variance of the GP [13], or the differential entropy of the GP for adding a new data point [14] can be evaluated directly and used. However, despite work on geometric shape priors for GPIS [31], there remains no satisfactory way to condition a GP on a known shape with unknown pose. We implement a GPIS baseline and condition it on the shape by augmenting the input data. Mutual information between observations and the estimated quantity is also common [18], [30], which measures the reduction in uncertainty of the estimated quantity given the observations. Thus, we formulate our information gain function based on the mutual information between the object pose and the occupancy at points a robot trajectory would sweep out."}, {"title": "C. Planning", "content": "Searching for an optimally-informative trajectory is usually computationally intensive. GP-based methods in particular are limited by inference times that grow rapidly with increasing number of data points, often addressed by using sparse GPs or downsampling to trade off accuracy [45]. Some methods greedily selects the optimal next configuration, and additionally constrain the action space to slide along the surface of the object [47], [13]. Our formulation of the information gain allows us to efficiently evaluate it for many query trajectories in parallel, enabling us to use longer-horizon planning methods such as sampling-based model predictive control in a closed loop. We consider difficult tasks which necessitate long horizon planning.\nActive exploration problems also differs by sensing modality. In the context of object shape and pose estimation, the most common modality is visual perception, with the common framing of the problem as finding the next best view [22]. Tactile approaches have also demonstrated success [51], [13], as well as hybrid approaches [41], [44]. Tightly coupled with sensing modality is the distinction of whether the robot is passively observing the environment or actively interacting with and changing the environment as in the interactive perception problem [3]. RUMI is a hybrid approach for interactive perception, primarily relying on contact-rich interactions using tactile sensors, but also leveraging visual perception to initialize pose estimates. Visual perception in our case is weakened by environmental occlusion and object self-occlusion. Unlike most other methods for object pose or shape estimation, we do not assume the object is stationary, which accounts for a large part of the difficulty. The closest method to ours is Act-VH [41], which trains an implicit surface neural network to output hypothesis voxel grids of seen objects given a partially observed point cloud and selects the best point to probe next. One major weakness of this method is the need to either retrain their network on all candidate objects whenever there is a new target object, or to train a network per object and assume object identity is known. Our method can be applied to new known objects without any training. Additionally, their object is in between the robot and the camera, meaning that the visually-occluded region is highly reachable, bypassing a major challenge that we address. Lastly, we consider the information gain from full robot trajectories rather than a single next point to probe."}, {"title": "III. PROBLEM STATEMENT", "content": "Let \\(q \\in \\mathbb{R}^{N_q}\\) denote the robot configuration, and \\(u \\in \\mathbb{R}^{N_u}\\) denote control. We study a single robot exploring an unmodeled environment, using limited visual perception and contact-heavy rummaging to estimate the pose of a single movable rigid target object of known shape. A rigid object's configuration is defined by its pose, a transform \\(T \\in SE(3)\\). Every \\(T\\) can be identified with a \\(\\mathbb{R}^{4 \\times 4}\\) homogeneous transformation matrix, and for convenience, we use \\(x = Tx\\) to denote the homogeneous transform of point \\(x \\in \\mathbb{R}^3\\) from world frame coordinates to the object frame of \\(T\\) (homogeneous coordinates have 1 appended). There is an underlying dynamics function \\(f: \\mathbb{R}^{N_q} \\times \\mathbb{R}^{N_u} \\rightarrow \\mathbb{R}^{N_q}\\) that we do not know, but are given the free space dynamics function \\(f_f : \\mathbb{R}^{N_q} \\times \\mathbb{R}^{N_u} \\rightarrow \\mathbb{R}^{N_q}\\). The difference in dynamics is primarily due to contact between the robot and the target object. We are interested in generating a fixed length trajectory of \\(T\\) actions, \\(u_1,..., u_t\\) to actively explore and estimate the target object's pose.\nSpecifically, we have the target object's precomputed object frame signed distance function (SDF) derived from its 3D model, \\(\\text{sdf} : \\mathbb{R}^3 \\rightarrow \\mathbb{R}\\). After each action, sensors observe a set of points at time \\(t\\) : \\(X_t = \\{(x_1, s_1), ..., (x_N, s_N)\\}\\) with observed world positions \\(x_n \\in \\mathbb{R}^3\\) and semantics \\(s_n\\) (described below). For convenience, we refer to a pair of position and semantics as a geometric feature. Let \\(X_t\\) denote the accumulated set of geometric features up to and including time \\(t\\). Sensors may include but are not limited to robot proprioception, end-effector mounted tactile sensors, and external cameras.\nWe treat the pose of the target object as a random variable and define \\(p(T|X_t)\\) as the posterior probability distribution over poses given \\(X_t\\). Observation noise, object symmetry, and the partial nature of \\(X_t\\) results in pose uncertainty.\nLet \\(T^*\\) be the true object transform, then the observed semantics are"}, {"title": null, "content": "\\(S_n = \\begin{cases} \\text{free} & \\text{implies } \\text{sdf}(T^*x_n) > 0 \\\\ \\text{occupied} & \\text{implies } \\text{sdf}(T^*x_n) < 0 \\\\ \\text{surface} & \\text{implies } \\text{sdf}(T^*x_n) = 0 \\end{cases}\\)\nFor a workspace point \\(x\\) that we have not observed, its semantics is a discrete random variable \\(S_x\\) with the shorthand \\(p(S_x) = p(s|x)\\). We are given a sensor model \\(p(S_x|T) = p(S_x | \\text{sdf}(Tx))\\) such as in Fig. 2 that gives the probability of observing each \\(s\\) value given a SDF value. The sensor model does not consider uncertainty over the position, and we assume we are given exact positions with only uncertainty over semantics \\(S_x\\).\nGiven a prior \\(p(T)\\), and starting at \\(q_1\\), our goal is to estimate the pose of the object by maximizing the expected information gain after \\(T\\) actions:"}, {"title": null, "content": "\\(\\mathop{\\text{arg max}}_{u_1,..., u_T} E_{X_T} [D_{KL}(p(T|X_T)||p(T))]\\)\ns.t. \\(q_{t+1} = f(q_t, u_t), t = 1, ..., T\\)"}, {"title": null, "content": "The expectation is over the semantics of each position in \\(X_T\\). Note that this is equivalent to the mutual information between \\(T\\) and \\(X_T\\), \\(I(T; X_T)\\) [36].\nThe challenge of this problem comes from the need for contact-based perception due to limited sensing capabilities, coupled with the fact that the target object is movable. Moreover, an ineffective action sequence can result in undesirable contacts, potentially pushing the object out of the robot's reachable workspace.\nWe evaluate the quality of the estimated pose distribution by evaluating the likelihood of the ground truth pose \\(L(T^*|X_t)\\), or equivalently its negative log likelihood (NLL). Low NLL indicates both certainty and correctness of the pose distribution. We do so by sampling a set of surface points in the object frame and transforming them by \\(T^*\\) to produce world positions \\(X\\). We then evaluate the NLL of all of the points having surface semantics:"}, {"title": null, "content": "\\(\\text{nll}(X) = - \\log p(S_x = \\text{surface}|x)\\)"}, {"title": null, "content": "We use this metric as well as computational efficiency to evaluate our method against baselines and ablations."}, {"title": "IV. METHOD", "content": "Our high level approach to addressing the problem in Eq. 1 is depicted in Fig. 3. We represent the pose posterior \\(p(T|X)\\) with a particle filter and describe how to evaluate \\(p(T|X)\\).\nNext, we present a tractable surrogate for information gain that we develop into a cost function for model predictive control (MPC). To discourage trajectories that move the target object out of the robot's reachable area, we develop an additional reachability cost function. Furthermore, to estimate the displacement of the target object given an action trajectory, we implement a stochastic dynamics model \\(f\\). We use the cost functions and the dynamics function inside MPC, which executes in a closed loop for \\(T\\) steps. During this process, we detail how to merge current observations with previous ones and update the pose posterior \\(p(T|X)\\)."}, {"title": "A. Representing Pose Posterior", "content": "We maintain a belief over the pose posterior \\(p(T|X_t)\\) using a particle filter, where each particle is a pose. We have \\(P\\) particles \\(T_{1..P}\\), with weights \\(w_{1..P}\\) such that \\(\\Sigma_{i=1}^P w_i = 1\\).\nOur choice of a particle filter over alternative representations is motivated by the potential multi-modality of the posterior and the ability to process each particle in parallel.\nA major obstacle to the tractability of solving Eq. 1 is the information correlation between geometric features. Observing one decreases the information gain from others in a non-trivial manner, and it is a common long-standing assumption to consider the information gain from each independently [7], [46]. Thus, we assume the conditional mutual independence of \\(S_x\\) for all query positions \\(x\\) given observed \\(X_t\\).\nCritical to our method is a way to evaluate the posterior \\(p(T|X)\\). Our prior work CHSEL [54] formulated a differentiable cost function \\(\\hat{C}(X, T)\\) that evaluates the discrepancy between \\(X\\) and \\(T\\). It bears similarity to hydroelastic, or pressure field contact modelling [15], [32], except in addition to the pressure field penalizing object penetration, there are pressure fields that penalize semantics violation, such as observed free space geometric features being inside objects.\nWe simplify the third semantics class from CHSEL, which represented known SDF of any value. We restrict it to \\(s = 0\\), which refers to surface points. The cost is formulated by first partitioning the observed \\(X\\) into \\(X_f = \\{(x, s) | s = \\text{free}\\}\\), \\(X_o = \\{(x, s) | s = \\text{occupied}\\}\\), and \\(X_s = \\{(x, s) | s = \\text{surface}\\}\\)."}, {"title": null, "content": "\\(\\hat{C}(X, T) = \\sum_{x,s \\in X_f} \\hat{c}_f(x) + \\sum_{x,s \\in X_o} \\hat{c}_o(x) + \\sum_{x,s \\in X_s} \\hat{c}_k(x, s)\\)\n\\(\\hat{c}_f(x) = C \\cdot \\max(0, a - \\text{sdf}(x))\\)\n\\(\\hat{c}_o(x) = C \\cdot \\max(0, a + \\text{sdf}(x))\\)\n\\(\\hat{c}_k(x, s) = |\\text{sdf}(x) - s|\\)"}, {"title": null, "content": "where \\(C > 0\\) is a scaling parameter and \\(a > 0\\) allows for small degrees of violation due to uncertainty in the positions."}, {"title": null, "content": "Their gradients are defined as"}, {"title": null, "content": "\\(\\nabla \\hat{c}_f(x) = C \\cdot \\max(0, a - \\text{sdf}(x))(-\\nabla \\text{sdf}(x))\\)\n\\(\\nabla \\hat{c}_o(x) = C \\cdot \\max(0, a + \\text{sdf}(x))\\nabla \\text{sdf}(x)\\)\n\\(\\nabla \\hat{c}_k(x, s) = (\\text{sdf}(x) - s) \\nabla \\text{sdf}(x)\\)"}, {"title": null, "content": "where \\(\\nabla \\text{sdf}(x)\\) is the object SDF gradient with respect to an object-frame position \\(x\\) and normalized such that \\(\\vert\\nabla \\text{sdf}(x)\\vert\\vert_2= 1\\).\nSimilar to energy-based methods, we use the Boltzmann distribution [17], [48] to interpret Eq. 3 as the posterior pose probability:"}, {"title": null, "content": "\\(p(T|X) = \\eta e^{-\\lambda \\hat{C}(X, T)}\\)"}, {"title": null, "content": "where \\(\\lambda > 0\\) selects how peaky the distribution should be and \\(\\eta\\) is the normalization constant such that \\(\\int \\eta e^{-\\lambda \\hat{C}(X, T)} dT = 1\\).\nWe observe that the cost in Eq. 3 is additive in the sense"}, {"title": null, "content": "\\(\\hat{C}(X \\cup (x, s), T) = \\hat{C}(X, T) + \\hat{C}((x, s), T)\\)"}, {"title": null, "content": "This is an important property that enables us to efficiently evaluate information gain of all workspace positions in parallel."}, {"title": "B. Mutual Information Surrogate", "content": "Our conditional mutual independence assumption of \\(p(S_x|X)\\) lets us consider the information gain from knowing the semantics at a single new position, which we denote the information gain field \\(\\hat{I}(x|X)\\). This is much simpler than considering the information gain of a robot trajectory directly because there is no time component or correlation between the semantics of neighbouring geometric features. Suppose we have observed \\(X\\) and want to evaluate the information gain from observing some new geometric feature \\((x, s)\\). Note that here we are querying a specific given value of \\(x\\), but \\(S_x\\) is still a random variable, so the expectation is over \\(p(S_x|X)\\):"}, {"title": null, "content": "\\(\\hat{I}_f(x | X) = E_{s \\sim p(S_x | X)} [D_{KL}(p(T | X \\cup (x, s))||p(T|X))]\\)\n\\(= E_{s} E_{T \\sim p(T|X \\cup (x,s))} [\\log \\frac{p(T|X \\cup (x, s))}{p(T|X)}]\\)"}, {"title": null, "content": "The forward KL divergence results in an expectation over \\(p(T|X \\cup (x, s))\\). Since we need to evaluate the information gain for many positions in the workspace, this becomes intractable.\nTo address this challenge, we use the reverse KL divergence, since the expectation is then over \\(p(T|X)\\) for all queried positions. In general, KL divergence is not symmetric. However, when two distributions are close together the KL divergence is approximately symmetric [52], [23]. In our case the KL divergence is between \\(p(T|X)\\) and \\(p(T|X \\cup (x, s))\\) with all having SE(3) support, avoiding infinite divergences.\nAs we increase \\(X\\) during exploration, we expect the two distributions to become closer and the reverse KL to better approximate the forward KL divergence. Intuitively, a geometric feature has high reverse KL divergence if it has high \\(p(T|X)\\) and low \\(p(T|X \\cup (x, s))\\). These correspond to geometric features that would invalidate currently high-probability poses i.e. these are positions we would like to explore.\nUsing reverse KL, We now have"}, {"title": null, "content": "\\(\\hat{I}_r(x | X) = E_{s} [D_{KL}(p(T|X)||p(T|X \\cup (x, s)))]\\)\n\\(= E_{s} E_{T \\sim p(T|X)} [\\log \\frac{p(T|X)}{p(T|X \\cup (x, s))}]\\)"}, {"title": null, "content": "Substituting Eq. 10 in"}, {"title": null, "content": "\\(\\hat{I}_r(x | X) = E_{s} E_{T \\sim p(T|X)} [\\log \\frac{\\eta_1 e^{-\\lambda \\hat{C}(X, T)}}{\\eta_2 e^{-\\lambda \\hat{C}(X \\cup (x, s), T))}}]\\)\n\\(= E_{s} E_{T \\sim p(T|X)} [\\log \\frac{\\eta_1 e^{-\\lambda \\hat{C}(X, T)}}{\\eta_2 e^{-\\lambda (\\hat{C}(X, T) + \\hat{C}((x, s), T))}}]\\)\n\\(= E_{s} E_{T \\sim p(T|X)} [\\log e^{\\lambda \\hat{C}((x, s), T)}] + \\log \\frac{\\eta_1}{\\eta_2}\\)\n\\(= \\lambda E_{s} E_{T \\sim p(T|X)} [-\\hat{C}((x, s), T)] + \\log \\frac{\\eta_1}{\\eta_2}\\)"}, {"title": null, "content": "where \\(\\eta_1\\) and \\(\\eta_2\\) are the normalizing constants for \\(p(T|X)\\) and \\(p(T|X \\cup (x, s))\\), respectively. First we simplify using the additive property of \\(\\hat{C}\\) (Eq. 11) then consider the normalizing constants,"}, {"title": null, "content": "\\(\\hat{I}_r(x | X) = \\lambda E_{s} E_{T \\sim p(T|X)} [\\hat{C}((x, s), T)] + \\log \\frac{\\eta_1}{\\eta_2}\\)"}, {"title": null, "content": "We note that \\(\\eta_2\\) depends on the querying position \\(x\\) because each \\(x\\) induces a different \\(p(T|X \\cup (x, s))\\). This normalizing constant is intractable to compute because it involves an integral over \\(T\\), so we instead optimize the approximation"}, {"title": null, "content": "\\(\\hat{I}(x | X) = \\lambda E_{s} E_{T \\sim p(T|X)} [\\hat{C}((x, s), T)]\\)\n\\(= \\lambda \\sum_{s} p(S_x = s|X) E_{T \\sim p(T|X)} [\\hat{C}((x, s), T)]\\)"}, {"title": null, "content": "Selecting \\(\\lambda\\) too high leads to the pose particle weights dominated by a few, causing particle degeneracy."}, {"title": null, "content": "We approximate the expectation over the posterior by taking the weighted sum over the pose particles"}, {"title": null, "content": "\\(\\hat{I}(x | X) \\approx \\lambda \\sum_{s} \\sum_{i=1}^P p(S_x = s|x)w_i\\hat{C}((x, s), T_i)\\)"}, {"title": null, "content": "Finally, we consider how we can approximate the conditional semantics distribution \\(p(S_x|X)\\) which is the last term required for fully computing \\(\\hat{I}(x | X)\\). We use the law of total probability"}, {"title": null, "content": "\\(p(S_x | X) = \\int p(S_x | T, X) p(T | X) dT\\)"}, {"title": null, "content": "Here again we approximate the expectation over the posterior by taking the weighted sum over the pose particles"}, {"title": null, "content": "\\(p(S_x | X) \\approx \\sum_{i=1}^P w_i p(S_x | T_i, X)\\)"}, {"title": null, "content": "we assume the conditional independence of \\(S_x\\) and \\(X\\) when given \\(T\\), so"}, {"title": null, "content": "\\(p(S_x | X) \\approx \\sum_{i=1}^P w_i p(S_x | T_i)\\)"}, {"title": null, "content": "where \\(p(S_x | T_i)\\) is given by the sensor model."}, {"title": null, "content": "Note that all the terms in Eq. 23 only query \\(x\\) and \\(T_{1..P}\\), without needing to directly consider \\(X \\cup (x, s)\\). This enables us to evaluate \\(\\hat{I}(x | X)\\) for all positions inside a workspace \\(x \\in W \\subset \\mathbb{R}^3\\) in parallel."}, {"title": "D. Posterior Update Process", "content": "So far", "16": [9], "54": "to produce the initial pose particles in Algorithm 1 line 2. CHSEL performs Quality Diversity (QD) optimization [39", "19": "or measure slip (such as in [33", "40": "to estimate \\(\\Delta T\\) directly. Not all robots have these sensors"}, 0, ".", 0, "1..", {"D": "mathbb{R}^3 \\times \\mathbb{R}^+ \\rightarrow \\mathbb{R}^3\\).\nWith updated observations \\(X_t\\), we can update the weights of the pose particles. Importantly, we update even when not making contact because observing \\(s = \\text{free}\\) geometric features provides information about where the object is not. This process is described in Algorithm 2, where Eq. 3 is applied to get discrepancies \\(l_{1..P}\\). We then apply Eq. 10 to convert it to an unnormalized probability. For numerical stability, we subtract the minimum \\(l\\) from all of them to get relative discrepancy. This is without"}]}