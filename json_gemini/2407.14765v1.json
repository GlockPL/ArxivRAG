{"title": "Data Augmentation in Graph Neural Networks: The Role of Generated Synthetic Graphs", "authors": ["S\u00fcmeyye Ba\u015f", "K\u0131ymet Kaya", "Resul Tugay", "\u015eule G\u00fcnd\u00fcz \u00d6\u011f\u00fcd\u00fcc\u00fc"], "abstract": "Graphs are crucial for representing interrelated data and aiding predictive modeling by capturing complex relationships. Achieving high-quality graph representation is important for identifying linked patterns, leading to improvements in Graph Neural Networks (GNNs) to better capture data structures. However, challenges such as data scarcity, high collection costs, and ethical concerns limit progress. As a result, generative models and data augmentation have become more and more popular. This study explores using generated graphs for data augmentation, comparing the performance of combining generated graphs with real graphs, and examining the effect of different quantities of generated graphs on graph classification tasks. The experiments show that balancing scalability and quality requires different generators based on graph size. Our results introduce a new approach to graph data augmentation, ensuring consistent labels and enhancing classification performance.", "sections": [{"title": "1 Introduction", "content": "Graphs serve as important representations of interrelated data in various fields, including social networks and chemical sciences, due to their ability to encapsulate complex relationships and facilitate critical tasks such as predictive modeling. Obtaining high-quality graph representations is vital for revealing underlying interrelated patterns and phenomena in fields that rely on graph-related data [1].\nHowever, the progress in this area is being hampered by the lack of useful big datasets and consistent evaluation processes. Despite significant progress in data availability in recent years, this development remains limited in various application domains for privacy and security reasons [2,3]. This indicates that future models may intentionally or unintentionally resort to generated synthetic data."}, {"title": "2 Literature Review", "content": "Data augmentation has recently drawn significant attention in the field of machine learning, especially for deep learning models, [8-10], thanks to its ability to enhance model performance and generalization by incorporating additional training data. Data augmentation is commonly used in computer vision and text applications. Data augmentation with images is relatively simpler due to the Euclidean nature and well-organized structure of image instances. Pixel matrices can be easily transformed using common rule-based techniques like rotation, scaling, and flipping preserving the labels [11]. Similarly, for text-level data augmentation rule-based data augmentation approaches including random insertion, random deletion, and synonym substitution are quite effective [12]. AugGPT rephrases sentences in the training data through rule-based approaches to increase the training data size and ensure accurate labeling in the generated text data [13].\nHowever, compared to text and image data types graphs are irregular and non-Euclidean. Therefore, even small changes in the structure may result in a loss of inter-related information, making the augmentation process more complex for graph data. Graph Data Augmentation methods can be broadly examined under two headings: rule-based and learned (AI-based) approaches. Among these, rule-based methods apply predefined rules to modify graph data, such as random edge removal and graph clipping while learned methods, such as graph structure learning and graph rationalization, exploit learnable parameters for data augmentation that can be trained independently or with downstream tasks.\nThe conventional rule-based graph augmentation techniques widely applied in the literature, such as arbitrary node removal, edge modification, or the occlusion of node characteristics, rely on random alterations of network structures or attributes. However, these arbitrary changes often compromise label invariance by inadvertently damaging significant label-related information, thus failing to generate appropriate graph data and improve graph prediction models' performance in practical applications. To address these limitations, GraphAug is offered as a solution by computing label-invariant augmentations through an automated approach, thereby safeguarding crucial label-related data within graph datasets [14]. Furthermore, Sui et al. proposed Adversarial Invariant Augmentation (AIA), a strategy aimed at mitigating covariate shifts in graph representation learning [15]. Yue et al. advanced a label-invariant augmentation method for graph-structured data in graph contrastive learning, generating augmented samples in challenging directions within the representation space while maintaining the original sample labels [16].\nDespite advances in data augmentation, studies on using generated data for prediction in graph neural networks are limited in the literature [17]. Zero-shot image classification was performed using data generated with models that had been extensively trained. The performance of prediction models using synthetic data and real data for image classification is compared and similar accuracy values are observed [18]. The effect of synthetic graphs on the model performance of node classification algorithms was examined, and small improvements in performance were obtained through pre-training using graphs with similar characteristics. Sun et al. presented the MoCL framework for learning molecular representations, utilizing both local and global domain expertise to guide the augmentation procedure and guarantee variation without changing graph semantics, as shown on several molecular datasets [19]. Using social network graph datasets, Tang et al. performed cosine similarity-based cross-operation on the initial characteristics to produce additional graph features for node classification tasks [20]. In this study, we investigate the impact of graph data augmentation on the graph classification task. Generator methods proposed in the literature for data augmentation offer different advantages. Our work differs from its contemporaries in that it appropriately combines two state-of-the-art generator models, according to their advantages in terms of graph size."}, {"title": "3 Methodology", "content": "The overall structure of our proposed method is depicted in Fig. 1. First, for the graph classification task, the data G = G1, ..., Gs with E edges and V nodes, is grouped as small or large according to the average number of nodes and edges of the graphs it contains. In this study, the average number of nodes is fifty and above, or an edge number of thousand two hundred twenty-five (edge numbers for a fully connected simple graph with fifty nodes), and above is determined as large, otherwise small. In Step 2 of Fig. 1, the appropriate generation model is selected according to whether the graphs in the data are small or large, and for each graph class Ci, the desired number of synthetic graphs for that class Cc, is generated for data augmentation. Here we recommend GRAN for data consisting of large graphs and GraphRNN for small ones. GRAN, with its focus on balancing efficiency and quality particularly through its stride parameter-may experience a reduction in generation quality for smaller graphs when compared to GraphRNN. On the other hand, GRAN's methodology is optimized for handling larger graphs, whereas GraphRNN's architecture and training process are better suited for generating smaller-scale graphs [6] and mostly give out of memory error for larger graphs despite efforts to enhance scalability through techniques like bread-first-search (BFS) node ordering scheme. Graph generation methods GraphRNN, GRAN, and all the classification methods used during the experiments are detailed in the subheadings, and the code repository is also available here6."}, {"title": "3.1 Graph Data Generation", "content": "Learning a distribution Pmodel(G) over graphs is the aim of generative model learning. This is achieved by sampling a collection of observed graphs G = G1,..., Gs from the data distribution p(G), where each graph in G may vary in the number of nodes and edges [7]. Instead of directly acquiring knowledge about the probability distribution p(G), which is difficult to define precisely the representation of the sample space, an auxiliary random variable is sampled to represent node ordering as sequences. This transforms the graph generation process into the generation of node and edge sequences, where nodes and edges are generated autoregressively. An adjacency matrix with a node ordering \u03c0 maps nodes to rows and columns of the matrix enabling each graph in G to be depicted by the adjacency matrix $A^{\\pi} \\in R^{n \\times n}$.\n$A_{ij} = \\mathbb{1} [(\\pi(v_i), \\pi(v_j)) \\in E]$.\nThe aforementioned generator models were designed to work with simple graphs G = (V, E). Initially, graph nodes and edges are represented as sequences and sequences of sequences using a mapping fs, respectively. For a graph G sampled from p(G) with n nodes under a node ordering \u03c0, the sequence S is obtained as in Equation 2, where S is an adjacency vector representing the edges between node \u03c0(vi) and the preceding nodes \u03c0(vj), j\u2208 {1, ..., \u0456 \u2212 1}, already present in the graph 3.\n$f_s(G, \\pi) = (S_1^{\\pi}, ..., S_n^{\\pi})$\n$S_i^{\\pi} = (A_{i,1}^{\\pi}, ..., A_{i,i-1}^{\\pi}), v_i \\in \\{2,..., n\\}$ \n$p(G) = \\sum_{\\pi} p(S^{\\pi}) \\mathbb{1} [f_G(S^{\\pi}) = G]$ \nIn the case of undirected graphs, S\u03c0 uniquely determines a graph G, denoted by the mapping fG(\u00b7), where fG(S\u03c0) = G. This sequentialization process allows generators to observe S\u03c0 and learn about its probability distribution, p(S\u03c0), which can be analyzed sequentially as S\u03c0 exhibits a sequential nature. During inference time, generators can derive samples of G without explicitly calculating p(G) by sampling S\u03c0, which corresponds to G through the function fg. With these concepts, p(G) can be expressed as a marginal probability distribution of the joint distribution p(G, S\u03c0) in Equation 4, where p(S\u03c0) is the distribution that the generator aims to learn."}, {"title": "Small Graphs Generation: GraphRNN [7]", "content": "In GraphRNN, the graph sequentialization is followed by constructing a scalable auto-regressive model that is suitable for small-medium size of graphs and can benefit from graph structure. Its generation process is shown in the Step-2.b of Fig. 1. GraphRNN can be viewed as a hierarchical model where new nodes are constructed by a graph-level RNN and the edges of each newly formed node are generated by an edge-level RNN, all while maintaining the state of the graph.\n$h_i = f_{trans}(h_{i-1}, S_{i-1}^{\\pi})$\n$o_i = f_{out}(h_i)$ \n$S^{\\pi} = f_s(G, BFS(G, \\pi))$ \nThe probability distribution p(Si\u03c0|S<in) for each i is intricate, requiring an understanding of how node \u03c0(vi) connects to preceding nodes based on the previously added nodes. GraphRNN suggests parameterizing p(Si\u03c0|S<i\u03c0) with a neural networks model ensuring scalability with share weights across all time steps. GraphRNN employs an RNN comprising a state-transition function ftrans and an output function fout as in Equations 8 and 9, where hi in Rd represents the state encoding of the generated graph up to this point, Si-1 is the adjacency vector for the most recently generated node i-1, and i denotes the distribution of the adjacency vector for the next node (i.e., Si follows distribution P\u2082). Generally, ftrans and fout can be any neural network, and Po can be any distribution over binary vectors.\nA key finding of the GraphRNN technique is that, without sacrificing generality, it learns to produce graphs using breadth-first-search (BFS) node orderings instead of learning to generate graphs under any conceivable node permutation. BFS also provide a unique representation of graphs. Therewith, Equation 2 is changed to Equation 10, with the deterministic BFS function represented by BFS(.). Specifically, this BFS function takes a random permutation i as its input, selects node vl as the starting point, and appends each node's neighbors to the BFS queue following the order given by the permutation. It should be noted that the BFS function is many-to-one, meaning that multiple permutations can result in the same ordering once the BFS function is executed."}, {"title": "3.2 Graph Classification", "content": "GraphSAGE (Graph Sample and Aggregation) [22] creates node embeddings by sampling and aggregating data from each node's neighborhood. Graph-SAGE generates robust graph categorization representations by combining information from several layers of the graph.\nGINO: GIN (Graph Isomorphism Network) generates node embeddings by using message-passing procedures and a learnable set function through a multi-layer perceptron network. GIN is suitable for graph classification applications since it can capture higher-order graph structures. GINO sets the learnable \u025b parameter as 0 and depends rather on a set aggregation algorithm for updating node features. Therefore, it is computationally cheaper but less flexible [23].\nGINWithJK [23] adds the concept of jumping knowledge (JK) concept to the GIN model. This concept combines representations from several layers to improve the final node embeddings. Therefore, better information flow between layers can be achieved.\nGCNWithJK [24] Graph Convolutional Networks (GCN) iteratively aggregate information from neighboring nodes. GCN with Jumping Knowledge (GCNWithJK) directly merges node representations from several GCN layers. So, capturing both local and global graph structures becomes easier.\nEdgePool [25] is an edge-level graph pooling technique utilized in Graph Neural Networks (GNNs). It efficiently reduces the size of the graph while maintaining important structural information by selectively aggregating edges. Along with this, every EdgePool layer outputs the mapping between every node in the old graph and every node in the newly-pooled graph. An inverse mapping from pooled nodes to unpooled nodes is produced during unpooling. It is possible to link this mapping via many pooling layers because each node is assigned to exactly one merged node."}, {"title": "4 Experimental Results", "content": "We observed the effect of synthetic graph data by GNNs on graph classification on six public datasets from TU7 - three chemical compounds (MUTAGENICITY, ENZYMES, MUTAG), two social networks (COLLAB, TWITCH EGOS), and one protein interactions (DD). The descriptive statistics of these benchmark datasets are given in Table 1.\nTo investigate the impact of generated graphs on the graph classification task, we partitioned each dataset into three, ensuring the class distributions remained consistent: raw-data (80%), sub-real data (10%), and test data (10%). The raw-data serves as the baseline for comparison, representing the initial data available to researchers. In comparison, the sub-real data (R) imitates the supplementary data that researchers might acquire through additional time and resource investment in real-world scenarios. Lastly, the test data is designated to evaluate the performance of graph classification. Created with real-world application in mind, this strategic data partitioning enables the analysis of how generated graphs affect the accuracy of graph classification models.\nLeveraging the initial raw-data available to researchers, we generated datasets comparable in size to the R, ensuring class distributions remained consistent across all generated sets as in all other sets. This approach allowed us to assess the impact of the real and generated data of the same size on graph classification performance. We further extended these experiments by generating data sets twice and three times the size of the R, to examine how change in the volume of generated graphs affects the model performance. Given the diversity in graph sizes within our datasets (see. varying sizes of avg. nodes, avg. edges in Table 1), we employed GRAN, known for its adaptability to large graphs, as our preliminary study indicated that GraphRNN encountered Out of Memory (OOM) errors with bigger graphs. Table 2 presents the prediction results of graph classifier models from various backgrounds for raw-data, with R added to the raw-data (w/ Real), and with the aforementioned generated data sets added to the raw-data namely w/Gen.1, w/Gen.2, w/Gen.3. According to the results in Table 2, the most accurate predictions for each dataset, as highlighted in the table, were achieved by incorporating the generated graphs and mostly with w/Gen.2.\nIn the second part of the experiments, we generated one thousand twenty-four graphs from each class in the datasets and drastically increased the proportion of the generated data size. Here, we aim to explore the feasibility of obtaining a more balanced dataset with a large number of samples with the proposed graph data generation method, avoiding the imbalanced data and data scarcity problems that affect the prediction performance of many learning algorithms. The results of these experiments, which we obtained by data generation with GraphRNN and GRAN on datasets MUTAGENICITY, ENZYMES, and MUTAG, which consist of small graphs and contain a small number of samples, are presented in Table 3. According to the results in Table 3, the most accurate predictions for each dataset, as highlighted in the table, were achieved with w/Gen.(GraphRNN).\nThe overall summary of the results we obtained with the proposed textit-Graph classification with graph size-aware data augmentation framework is presented in Fig. 2. The results here reflect the raw-data, w/R and w/G accuracy values of the most accurate graph classifier for the relevant dataset (DD, COLLAB (C), TWITCH EGOS (TE), MUTAGENICITY (ME), ENZYMES (E), MUTAG (M)). Mirroring the observations of Touat et al. [6], our study reveals that particularly when working with medium to large-sized graphs, the graphs produced by GRAN are more analogous to the original graphs, however, GraphRNN has scalability problems not applicable to large graphs. However, while working with smaller graphs GRAN tends to overfit and its generation quality drops, hence GrapRNN is better."}, {"title": "5 Conclusion and Future Work", "content": "To conclude that, our study demonstrates the substantial impact of synthetic graph data on the performance of graph classification tasks across diverse datasets. The proposed Graph classification with graph size-aware data augmentation framework offers a flexible graph data generation process that is applicable for small, medium, and large-sized graphs. Moreover, the experiments involving a significant increase in the proportion of generated data further highlighted the potential of our graph generation framework to mitigate issues of data imbalance and scarcity, especially in smaller datasets. This balance was crucial for achieving higher prediction accuracy, as evidenced by the superior performance of models utilizing balanced, generated datasets. Overall, our findings underscore the effectiveness of AI-driven data generation in enhancing graph classification tasks, paving the way for more accurate and reliable machine learning models in diverse applications.\nFor future work, we plan to investigate the reasons for the differences in the performance of the generated data with explainable AI methods and also to work on generating synthetic graphs for dynamic graphs."}]}