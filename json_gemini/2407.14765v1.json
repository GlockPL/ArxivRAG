{"title": "Data Augmentation in Graph Neural Networks:\nThe Role of Generated Synthetic Graphs", "authors": ["S\u00fcmeyye Ba\u015f", "K\u0131ymet Kaya", "Resul Tugay", "\u015eule G\u00fcnd\u00fcz \u00d6\u011f\u00fcd\u00fcc\u00fc"], "abstract": "Graphs are crucial for representing interrelated data and aid-\ning predictive modeling by capturing complex relationships. Achieving\nhigh-quality graph representation is important for identifying linked pat-\nterns, leading to improvements in Graph Neural Networks (GNNs) to\nbetter capture data structures. However, challenges such as data scarcity,\nhigh collection costs, and ethical concerns limit progress. As a result,\ngenerative models and data augmentation have become more and more\npopular. This study explores using generated graphs for data augmen-\ntation, comparing the performance of combining generated graphs with\nreal graphs, and examining the effect of different quantities of generated\ngraphs on graph classification tasks. The experiments show that balanc-\ning scalability and quality requires different generators based on graph\nsize. Our results introduce a new approach to graph data augmentation,\nensuring consistent labels and enhancing classification performance.", "sections": [{"title": "1 Introduction", "content": "Graphs serve as important representations of interrelated data in various fields,\nincluding social networks and chemical sciences, due to their ability to encap-\nsulate complex relationships and facilitate critical tasks such as predictive mod-\neling. Obtaining high-quality graph representations is vital for revealing under-\nlying interrelated patterns and phenomena in fields that rely on graph-related\ndata [1].\nHowever, the progress in this area is being hampered by the lack of useful\nbig datasets and consistent evaluation processes. Despite significant progress in\ndata availability in recent years, this development remains limited in various\napplication domains for privacy and security reasons [2,3]. This indicates that\nfuture models may intentionally or unintentionally resort to generated synthetic\ndata."}, {"title": "2 Literature Review", "content": "Data augmentation has recently drawn significant attention in the field of ma-\nchine learning, especially for deep learning models, [8-10], thanks to its ability\nto enhance model performance and generalization by incorporating additional\ntraining data. Data augmentation is commonly used in computer vision and text\napplications. Data augmentation with images is relatively simpler due to the Eu-\nclidean nature and well-organized structure of image instances. Pixel matrices\ncan be easily transformed using common rule-based techniques like rotation,\nscaling, and flipping preserving the labels [11]. Similarly, for text-level data aug-\nmentation rule-based data augmentation approaches including random insertion,\nrandom deletion, and synonym substitution are quite effective [12]. AugGPT\nrephrases sentences in the training data through rule-based approaches to in-\ncrease the training data size and ensure accurate labeling in the generated text\ndata [13].\nHowever, compared to text and image data types graphs are irregular and\nnon-Euclidean. Therefore, even small changes in the structure may result in a loss\nof inter-related information, making the augmentation process more complex for\ngraph data. Graph Data Augmentation methods can be broadly examined un-\nder two headings: rule-based and learned (AI-based) approaches. Among these,\nrule-based methods apply predefined rules to modify graph data, such as random\nedge removal and graph clipping while learned methods, such as graph struc-\nture learning and graph rationalization, exploit learnable parameters for data\naugmentation that can be trained independently or with downstream tasks.\nThe conventional rule-based graph augmentation techniques widely applied\nin the literature, such as arbitrary node removal, edge modification, or the oc-\nclusion of node characteristics, rely on random alterations of network structures\nor attributes. However, these arbitrary changes often compromise label invari-\nance by inadvertently damaging significant label-related information, thus fail-\ning to generate appropriate graph data and improve graph prediction models'\nperformance in practical applications. To address these limitations, GraphAug is\noffered as a solution by computing label-invariant augmentations through an au-\ntomated approach, thereby safeguarding crucial label-related data within graph\ndatasets [14]. Furthermore, Sui et al. proposed Adversarial Invariant Augmenta-\ntion (AIA), a strategy aimed at mitigating covariate shifts in graph representa-\ntion learning [15]. Yue et al. advanced a label-invariant augmentation method for\ngraph-structured data in graph contrastive learning, generating augmented sam-\nples in challenging directions within the representation space while maintaining\nthe original sample labels [16].\nDespite advances in data augmentation, studies on using generated data for\nprediction in graph neural networks are limited in the literature [17]. Zero-shot\nimage classification was performed using data generated with models that had\nbeen extensively trained. The performance of prediction models using synthetic\ndata and real data for image classification is compared and similar accuracy\nvalues are observed [18]. The effect of synthetic graphs on the model perfor-\nmance of node classification algorithms was examined, and small improvements\nin performance were obtained through pre-training using graphs with similar\ncharacteristics. Sun et al. presented the MoCL framework for learning molec-\nular representations, utilizing both local and global domain expertise to guide\nthe augmentation procedure and guarantee variation without changing graph se-\nmantics, as shown on several molecular datasets [19]. Using social network graph\ndatasets, Tang et al. performed cosine similarity-based cross-operation on the ini-\ntial characteristics to produce additional graph features for node classification\ntasks [20]. In this study, we investigate the impact of graph data augmentation\non the graph classification task. Generator methods proposed in the literature for\ndata augmentation offer different advantages. Our work differs from its contem-\nporaries in that it appropriately combines two state-of-the-art generator models,\naccording to their advantages in terms of graph size."}, {"title": "3 Methodology", "content": "The overall structure of our proposed method is depicted in Fig. 1. First, for the\ngraph classification task, the data $G = G_1, ..., G_s$ with $E$ edges and $V$ nodes, is\ngrouped as small or large according to the average number of nodes and edges\nof the graphs it contains. In this study, the average number of nodes is fifty and\nabove, or an edge number of thousand two hundred twenty-five (edge numbers\nfor a fully connected simple graph with fifty nodes), and above is determined as\nlarge, otherwise small. In Step 2 of Fig. 1, the appropriate generation model is\nselected according to whether the graphs in the data are small or large, and for\neach graph class $C_i$, the desired number of synthetic graphs for that class $C_c$, is\ngenerated for data augmentation. Here we recommend GRAN for data consisting\nof large graphs and GraphRNN for small ones. GRAN, with its focus on balanc-\ning efficiency and quality particularly through its stride parameter-may ex-\nperience a reduction in generation quality for smaller graphs when compared to\nGraphRNN. On the other hand, GRAN's methodology is optimized for handling\nlarger graphs, whereas GraphRNN's architecture and training process are better\nsuited for generating smaller-scale graphs [6] and mostly give out of memory\nerror for larger graphs despite efforts to enhance scalability through techniques\nlike bread-first-search (BFS) node ordering scheme. Graph generation methods\nGraphRNN, GRAN, and all the classification methods used during the experi-\nments are detailed in the subheadings, and the code repository is also available\nhere6."}, {"title": "3.1 Graph Data Generation", "content": "Learning a distribution $P_{model}(G)$ over graphs is the aim of generative model\nlearning. This is achieved by sampling a collection of observed graphs $G =$\n$G_1,..., G_s$ from the data distribution $p(G)$, where each graph in $G$ may vary\nin the number of nodes and edges [7]. Instead of directly acquiring knowledge\nabout the probability distribution $p(G)$, which is difficult to define precisely the\nrepresentation of the sample space, an auxiliary random variable is sampled\nto represent node ordering as sequences. This transforms the graph generation\nprocess into the generation of node and edge sequences, where nodes and edges\nare generated autoregressively. An adjacency matrix with a node ordering $\\pi$\nmaps nodes to rows and columns of the matrix enabling each graph in $G$\nto be depicted by the adjacency matrix $A^{\\pi} \\in R^{n \\times n} 1$.\n$A_{ij} = 1 [(\\pi (v_i), \\pi (v_j)) \\in E]$.\n(1)\nThe aforementioned generator models were designed to work with simple\ngraphs $G = (V, E)$. Initially, graph nodes and edges are represented as sequences\nand sequences of sequences using a mapping $f_s$, respectively. For a graph $G$\nsampled from $p(G)$ with $n$ nodes under a node ordering $\\pi$, the sequence $S^{\\pi}$ is\nobtained as in Equation 2, where $S^{\\pi}$ is an adjacency vector representing the\nedges between node $\\pi(v_i)$ and the preceding nodes $\\pi(v_j)$, $j\\in \\{1, ..., i - 1\\}$,\nalready present in the graph 3.\n$f_s(G, \\pi) = (S_1^{\\pi}, ..., S_n^{\\pi})$\n(2)\n$S_i^{\\pi} = (A_{1,i}^{\\pi},..., A_{i-1,i}^{\\pi}), \\forall v_i \\in \\{2,..., n\\}$\n(3)\n$p(G) = \\sum_{\\pi} p (S^{\\pi}) 1 [f_G (S^{\\pi}) = G]$\n(4)\nIn the case of undirected graphs, $S^{\\pi}$ uniquely determines a graph $G$, denoted\nby the mapping $f_G(\\cdot)$, where $f_G(S^{\\pi}) = G$. This sequentialization process allows\ngenerators to observe $S^{\\pi}$ and learn about its probability distribution, $p(S^{\\pi})$,\nwhich can be analyzed sequentially as $S^{\\pi}$ exhibits a sequential nature. During\ninference time, generators can derive samples of $G$ without explicitly calculating\n$p(G)$ by sampling $S^{\\pi}$, which corresponds to $G$ through the function $f_G$. With\nthese concepts, $p(G)$ can be expressed as a marginal probability distribution of\nthe joint distribution $p(G, S^{\\pi})$ in Equation 4, where $p(S^{\\pi})$ is the distribution\nthat the generator aims to learn."}, {"title": "Large Graphs Generation: GRAN [21]", "content": "The overall procedure of a gener-\nation phase with GRAN is illustrated in Step-2.a of Fig. 1. GRAN promises to\nprovide a strong autoregressive conditioning between the graph's generated and\nto-be-generated portions as attention-based GNN helps better distinguish multi-\nple newly added nodes. While expressing networks as adjacency matrices, some\nmatrices remain unchanged under certain permutations, resulting in symmetry.\nTo solve this, GRAN constructs a set of symmetric permutations as in Equation\n5 and develops a surjective function u that maps permutations to symmetric\npermutations. Thus, for a graph G, different adjacency matrices for all permuta-\ntions are modeled. However, for undirected graphs, it is enough to just model the\nlower triangular portion of the adjacency matrix $L^{\\pi}$. GRAN generates the lower\ntriangular component $L^{\\pi}$ block by block, adding one block of nodes and asso-\nciated edges at a time $t$. This procedure considerably decreases auto-regressive\ngraph creation decisions by a factor of O(N), where N = |V|.\n$\\triangle(A^{\\pi}) = \\{\\tilde{\\pi} | A^{\\tilde{\\pi}} = A^{\\pi}\\}$\n(5)\n$b_t = \\{B(t - 1) + 1, ..., B_t\\}.$\n(6)\n$p(L^{\\pi}) = \\prod_{t=1}^T P(L^{\\pi}_{b_t} | L^{\\pi}_{< b_t})$\n(7)\nGRAN creates one block of B rows of $L^{\\pi}$ at a time. The t-th block consists\nof rows with indices as in Equation 6. The number of steps required to create a\ngraph is therefore $T = O(N/B)$. The conditional probability in Equation 7 de-\ntermines the likelihood of producing the current block. The probability function\nit needs to learn becomes a long conditional probability as it uses previous blocks\nto infer the next block. To avoid long-term bottlenecks and use the structural\nfeatures of graphs, GRAN prefers GNNs over RNNs."}, {"title": "Small Graphs Generation: GraphRNN [7]", "content": "In GraphRNN, the graph se-\nquentialization is followed by constructing a scalable auto-regressive model that\nis suitable for small-medium size of graphs and can benefit from graph struc-\nture. Its generation process is shown in the Step-2.b of Fig. 1. GraphRNN can be\nviewed as a hierarchical model where new nodes are constructed by a graph-level\nRNN and the edges of each newly formed node are generated by an edge-level\nRNN, all while maintaining the state of the graph.\n$h_i = f_{trans}(h_{i-1}, S_{i-1}^{\\pi})$\n(8)\n$o_i = f_{out} (h_i)$\n(9)\n$S^{\\pi} = f_s(G, BFS(G, \\pi))$\n(10)\nThe probability distribution $p(S_i^{\\pi}|S_{<i}^{\\pi})$ for each $i$ is intricate, requiring\nan understanding of how node $\\pi(v_i)$ connects to preceding nodes based on the\npreviously added nodes. GraphRNN suggests parameterizing $p(S_i^{\\pi}|S_{<i}^{\\pi})$ with\na neural networks model ensuring scalability with share weights across all time\nsteps. GraphRNN employs an RNN comprising a state-transition function $f_{trans}$\nand an output function $f_{out}$ as in Equations 8 and 9, where $h_i$ in $R^d$ represents\nthe state encoding of the generated graph up to this point, $S_{i-1}$ is the adjacency\nvector for the most recently generated node i-1, and $o_i$ denotes the distribution of\nthe adjacency vector for the next node (i.e., $S_i$ follows distribution $P_\\theta$). Generally,\n$f_{trans}$ and $f_{out}$ can be any neural network, and $P_\\theta$ can be any distribution over\nbinary vectors.\nA key finding of the GraphRNN technique is that, without sacrificing gener-\nality, it learns to produce graphs using breadth-first-search (BFS) node orderings\ninstead of learning to generate graphs under any conceivable node permutation.\nBFS also provide a unique representation of graphs. Therewith, Equation 2 is\nchanged to Equation 10, with the deterministic BFS function represented by\nBFS(.). Specifically, this BFS function takes a random permutation i as its in-\nput, selects node $v_l$ as the starting point, and appends each node's neighbors to\nthe BFS queue following the order given by the permutation. It should be noted\nthat the BFS function is many-to-one, meaning that multiple permutations can\nresult in the same ordering once the BFS function is executed."}, {"title": "3.2 Graph Classification", "content": "GraphSAGE (Graph Sample and Aggregation) [22] creates node embed-\ndings by sampling and aggregating data from each node's neighborhood. Graph-\nSAGE generates robust graph categorization representations by combining in-\nformation from several layers of the graph.\nGINO: GIN (Graph Isomorphism Network) generates node embeddings by\nusing message-passing procedures and a learnable set function through a multi-\nlayer perceptron network. GIN is suitable for graph classification applications\nsince it can capture higher-order graph structures. GINO sets the learnable $\\varepsilon$\nparameter as 0 and depends rather on a set aggregation algorithm for updating\nnode features. Therefore, it is computationally cheaper but less flexible [23].\nGINWithJK [23] adds the concept of jumping knowledge (JK) concept to\nthe GIN model. This concept combines representations from several layers to\nimprove the final node embeddings. Therefore, better information flow between\nlayers can be achieved.\nGCNWithJK [24] Graph Convolutional Networks (GCN) iteratively ag-\ngregate information from neighboring nodes. GCN with Jumping Knowledge\n(GCNWithJK) directly merges node representations from several GCN layers.\nSo, capturing both local and global graph structures becomes easier.\nEdgePool [25] is an edge-level graph pooling technique utilized in Graph\nNeural Networks (GNNs). It efficiently reduces the size of the graph while main-\ntaining important structural information by selectively aggregating edges. Along\nwith this, every EdgePool layer outputs the mapping between every node in the\nold graph and every node in the newly-pooled graph. An inverse mapping from\npooled nodes to unpooled nodes is produced during unpooling. It is possible\nto link this mapping via many pooling layers because each node is assigned to\nexactly one merged node."}, {"title": "4 Experimental Results", "content": "We observed the effect of synthetic graph data by GNNs on graph classification\non six public datasets from TU7 - three chemical compounds (MUTAGENICITY,\nENZYMES, MUTAG), two social networks (COLLAB, TWITCH EGOS), and\none protein interactions (DD). The descriptive statistics of these benchmark\ndatasets are given in Table 1.\nTo investigate the impact of generated graphs on the graph classification\ntask, we partitioned each dataset into three, ensuring the class distributions re-\nmained consistent: raw-data (80%), sub-real data (10%), and test data (10%).\nThe raw-data serves as the baseline for comparison, representing the initial data\navailable to researchers. In comparison, the sub-real data (R) imitates the sup-\nplementary data that researchers might acquire through additional time and\nresource investment in real-world scenarios. Lastly, the test data is designated\nto evaluate the performance of graph classification. Created with real-world ap-\nplication in mind, this strategic data partitioning enables the analysis of how\ngenerated graphs affect the accuracy of graph classification models.\nLeveraging the initial raw-data available to researchers, we generated datasets\ncomparable in size to the R, ensuring class distributions remained consistent\nacross all generated sets as in all other sets. This approach allowed us to assess\nthe impact of the real and generated data of the same size on graph classifi-\ncation performance. We further extended these experiments by generating data\nsets twice and three times the size of the R, to examine how change in the\nvolume of generated graphs affects the model performance. Given the diversity\nin graph sizes within our datasets (see. varying sizes of avg. nodes, avg. edges\nin Table 1), we employed GRAN, known for its adaptability to large graphs,\nas our preliminary study indicated that GraphRNN encountered Out of Mem-\nory (OOM) errors with bigger graphs. Table 2 presents the prediction results\nof graph classifier models from various backgrounds for raw-data, with R added\nto the raw-data (w/ Real), and with the aforementioned generated data sets\nadded to the raw-data namely w/Gen.1, w/Gen.2, w/Gen.3. According to the\nresults in Table 2, the most accurate predictions for each dataset, as highlighted\nin the table, were achieved by incorporating the generated graphs and mostly\nwith w/Gen.2.\nIn the second part of the experiments, we generated one thousand twenty-four\ngraphs from each class in the datasets and drastically increased the proportion\nof the generated data size. Here, we aim to explore the feasibility of obtain-\ning a more balanced dataset with a large number of samples with the proposed\ngraph data generation method, avoiding the imbalanced data and data scarcity\nproblems that affect the prediction performance of many learning algorithms.\nThe results of these experiments, which we obtained by data generation with\nGraphRNN and GRAN on datasets MUTAGENICITY, ENZYMES, and MU-\nTAG, which consist of small graphs and contain a small number of samples,\nare presented in Table 3. According to the results in Table 3, the most accu-\nrate predictions for each dataset, as highlighted in the table, were achieved with\nw/Gen.(GraphRNN).\nThe overall summary of the results we obtained with the proposed textit-\nGraph classification with graph size-aware data augmentation framework is pre-"}, {"title": "5 Conclusion and Future Work", "content": "To conclude that, our study demonstrates the substantial impact of synthetic\ngraph data on the performance of graph classification tasks across diverse datasets.\nThe proposed Graph classification with graph size-aware data augmentation frame-\nwork offers a flexible graph data generation process that is applicable for small,\nmedium, and large-sized graphs. Moreover, the experiments involving a signifi-"}]}