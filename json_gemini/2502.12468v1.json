{"title": "MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation", "authors": ["Yutong Wang", "Pengliang Ji", "Chaoqun Yang", "Kaixin Li", "Ming Hu", "Jiaoyang Li", "Guillaume Sartoretti"], "abstract": "The LLM-as-a-Judge paradigm shows promise for evaluating generative content but lacks reliability in reasoning-intensive scenarios, such as programming. Inspired by recent advances in reasoning models and shifts in scaling laws, we pioneer bringing test-time computation into LLM-as-a-Judge, proposing MCTS-Judge, a resource-efficient, System-2 thinking framework for code correctness evaluation. MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluations. Through a node-selection strategy that combines self-assessment based on historical actions in the current trajectory and the Upper Confidence Bound for Trees based on prior rollouts, MCTS-Judge balances global optimization and refinement of the current trajectory. We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis. Extensive experiments on three benchmarks and five LLMs demonstrate the effectiveness of MCTS-Judge, which improves the base model's accuracy from 41.0% to 80.0%, surpassing the o1-series models with 3\u00d7 fewer tokens. Further evaluations validate the superiority of its reasoning trajectory in logic, analytics, thoroughness, and overall quality, while revealing the test-time scaling law of the LLM-as-a-Judge paradigm.", "sections": [{"title": "1 Introduction", "content": "LLM-as-a-Judge, wherein Large Language Models (LLMs) serve as the golden rule for evaluation criteria (Gu et al., 2024), has been proposed for applications such as generative content assessment (Li et al., 2024b), and data captioning (Chen et al., 2024), serving as a cost-effective solution compared to human expert evaluators. Among those, LLM-as-a-Judge has revolutionized code evaluation by automating judgment (Yang et al., 2024), repair (Liu et al., 2024), and explanation (Weyssow et al., 2024), replacing inaccurate similarity-based execution-free methods (Ren et al., 2020; Tran et al., 2019), and expensive execution-based methods reliant on manually-crafted test cases (Zheng et al., 2023; Zhuo et al., 2024).\nDespite its growing adoption, recent studies highlight critical challenges in the LLM-as-a-Judge paradigm, including bias (Gu et al., 2024), misalignment (Ye et al., 2024), and fairness concerns (Li et al., 2024a), questioning its reliability for accurate, human-like judgments. To address these issues, researchers have focused on pretraining (Hui et al., 2024), fine-tuning (Wang et al., 2024a), and in-context learning (Wei et al., 2022) to improve reasoning capabilities, which are highly demanded in programming scenarios. Unfortunately, as LLMs near the upper bounds imposed by scaling laws, further advancements involve increasing costs in training with diminishing returns (Snell et al., 2024).\nTo address these limitations, inspired by the shift of scaling laws from training to test time (Xu et al., 2025) and recent breakthroughs in Reasoning LLMs, such as OpenAI's o-series (Jaech et al., 2024), we introduce the first framework that integrates test-time computation into the LLM-as-a-Judge paradigm. We target code correctness evaluation and propose MCTS-Judge, a resource-efficient LLM-as-a-Judge framework with System-2 thinking, offering human-like reasoning for more reliable evaluations. It achieves State-Of-The-Art (SOTA) performance compared to prior LLM-as-a-Judge methods, which rely on rapid and superficial System-1 thinking (Tong and Zhang, 2024; Zhuo, 2023). MCTS-Judge leverages a tailored Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluation tasks. In the selection phase of MCTS, we introduce a global-local node selection strategy that combines self-assessment based on historical actions in the current trajectory, and the Upper Confidence Bound for Trees (UCT) algorithm, guided by prior rollouts, to balance the optimization of high-value regions in the global search space with local reasoning trajectories. We further designed a high-precision simulated execution reward mechanism. This mechanism involves cost-effective automatic test case synthesis, followed by the use of LLMs as an interpreter to simulate the execution of test cases, which encourages the LLM to perform line-by-line, in-depth analysis and achieve unit-test-level reliability.\nExtensive experiments on five LLMS across three challenging code benchmarks-BigCodeBench (Zhuo et al., 2024), HumanEval-X (Zheng et al., 2023), and APPS (Hendrycks et al., 2021)\u2014with varying code complexity and languages, highlight the reliability of MCTS-Judge powered by test-time computation. As shown in Fig. 1, our approach elevates the accuracy of DeepSeek-Coder-Lite-16B (Zhu et al., 2024) from 41.0% to 80.0%, surpassing o1-series models (Jaech et al., 2024) and open-source Qwen-QwQ-32B (Qwen, 2024), while using only 3\u00d7 fewer tokens and a smaller model. Furthermore, we achieve SOTA performance on all experiments compared to previous System-1 thinking-based LLM-as-a-judge frameworks, with up to 32% improvement on APPS, and demonstrate strong robustness in generalizable scenarios without code references. Case studies on HumanEval-X further showcase MCTS-Judge's superior reasoning across four fine-grained dimensions, such as logic and analytics, achieving a higher win rate over o1-series models. Finally, we validated that scaling test-time computation, including tree depth and rollouts, further enhances MCTS-Judge's accuracy, shedding light into the test-time scaling law for LLM-as-a-judge paradigms."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Code Correctness Evaluation", "content": "Code correctness evaluation can be broadly broken down into two paradigms. Execution-free methods, such as BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), METEOR (Denkowski and Lavie, 2014), ChrF (Popovi\u0107, 2015), RUBY (Tran et al., 2019), and CodeBLEU (Ren et al., 2020), assess code based on textual or code-specific feature similarity to reference code. In this paper, we refer to them as similarity-based evaluation methods. However, reference code is often unavailable in practice, and these methods struggle to distinguish semantically equivalent but syntactically different code, leading to low accuracy, as shown in Appendix B. In contrast, execution-based methods, commonly used in code generation benchmarks (Zheng et al., 2023; Zhuo et al., 2024), assess code correctness by executing it against test cases. However, this approach demands comprehensive handcrafted test cases and isolated environments, making it costly and operationally complex (Zhang et al., 2022). To address these limitations, recent efforts have explored LLM-as-a-Judge paradigms with in-context learning. ICE-Score (Zhuo, 2023) integrates evaluation criteria into prompts, while CODEJUDGE (Tong and Zhang, 2024) employs a two-stage prompting approach. However, these methods rely on System-1 thinking (Kahneman, 2011), leading to rapid, superficial decisions that are constrained by the inherent uncertainties of LLMs, resulting in limited reliability."}, {"title": "2.2 Test-time Computation Boost Reasoning", "content": "Recent studies highlight a shift in scaling laws from train-time to test-time (Ji et al., 2025; Xu et al., 2025), as pretrained models approach data scale limits (Snell et al., 2024), while reasoning models leverage test-time computation, demonstrating remarkable performance improvements, exemplified by OpenAI's o-series models (Jaech et al., 2024). To advance human-like System-2 thinking, key innovations include chain-of-thought data curation (Wang et al., 2022, 2024b), reinforcement learning (DeepSeek-AI, 2025; Qwen, 2024), and reward models (Guan et al., 2025; Yu et al., 2024). As a core support, search paradigms like beam search and MCTS dynamically select diverse reasoning trajectories, significantly enhancing accuracy in large search spaces. Examples include ReST-MCTS (Zhang et al., 2024a), rStar (Qi et al., 2024), MCTSr (Zhang et al., 2024b), and (Xie et al., 2024), which integrate MCTS with reinforced selftraining, self-play mutual reasoning, and preference optimization, driving advancements in reasoning tasks such as math and code problem-solving. Building on this remarkable improvement in reliability, we pioneeringly integrate test-time computation into the LLM-as-a-Judge paradigm, proposing a novel framework, MCTS-Judge, which leverages System-2 thinking to generate reliable, human-like reasoning trajectories for comprehensive, multi-perspective code correctness evaluation."}, {"title": "3 MCTS-Judge", "content": "In this section, we first introduce the overview of MCTS-Judge for code evaluation (Sec.3.1), then detail its MCTS architecture (Sec.3.2) and reward mechanism (Sec. 3.3)."}, {"title": "3.1 Overview", "content": "The code correctness evaluation task determines whether a code snippet $c$ correctly implements the functionality described in a problem statement $p$, expressed as $x = (c,p)$. In MCTS-Judge, we decompose this task into multiple subtasks, each requiring the LLM to determine whether the code satisfies a specific type of requirement. The action space of our MCTS consists of these subtasks and a null action representing no evaluation. The subaction space at each level of the search tree includes one non-repeating subtask and the null action. Each action in MCTS produces an output $s_i \\in S$ with state transitions defined as $s_i = L(x, s_1, ..., s_{i-1})$, where $L$ represents an LLM. This forms a reasoning trajectory $t = x \\oplus s_1 \\oplus ... \\oplus s_k$, where $k$ is the maximum depth of the search tree. The prediction for a trajectory is computed as $f (t, g)$, where $f$ aggregates the subtask outcomes in $t$ along with a global evaluation $g$. A task-specific terminal reward is assigned based on the agreement between $f(t, g)$ and the result of simulated execution. We perform multiple rollouts, yielding a set of reasoning trajectories $T = {t_1, t_2, ..., t_j}$. The cumulative rewards $R(t_i) = \\sum_{s \\in t_i}r(s)$ for these trajectories are used for weighted sampling to select the optimal trajectory $t_b$. The final prediction for $x$ is given by $f(t_b, g)$."}, {"title": "3.2 Architecture Design", "content": "We chose MCTS to implement System-2 thinking essential for code evaluation for two reasons: First, MCTS breaks down the overall code evaluation task into simpler subtasks, reducing the task complexity compared to other System-2 methods like Best-of-N (Brown et al., 2024) and self-consistency (Wang et al., 2022), which require generating complete solutions in a single inference. Second, our MCTS introduces rewards to guide the search and select the optimal trajectory, further improving the reliability of the LLM-as-a-Judge paradigm. As shown in Fig. 2, our tailored MCTS follows four key stages: selection, expansion, simulation, and backpropagation.\n1) Selection. The selection process begins at the root node and progresses hierarchically until it reaches a node that has not been fully expanded yet. We propose a selection strategy that combines global and local information to balance the optimization of high-value regions in the search space with the current trajectory, resulting in a more coherent evaluation. Specifically, we employ a two-level approach: a global-level UCT algorithm (Kocsis and Szepesv\u00e1ri, 2006), leveraging insights from previous rollouts, and a local-level LLM-driven self-assessment, which evaluates historical actions within the current trajectory. The final selection is obtained through weighted sampling, with the UCT result weighted by $w_u$ and the self-assessment result weighted by $w_r$. The UCT algorithm selects the node with the highest UCT value, computed as:\n$UCT(s) = \\frac{Q(s)}{N(s)} + \\alpha \\cdot \\sqrt{\\frac{ln N_{parent}(s)}{N(s)}}$ (1)\nwhere $Q(s)$ represents the cumulative reward of node $s$, $N(s)$ is the visit count of $s$, $N_{parent}(s)$ the visit count of $s's$ parent node, and $\\alpha$ is a constant that helps balance exploration and exploitation. The LLM self-assessment result is obtained by prompting the LLM whether including this subtask enhances code evaluation completeness based on the completed subtasks in the current trajectory.\n2) Expansion. If the maximum depth has not been reached, a new child node is added to the selected node by randomly sampling an unused action and executing it. If the action is not null, a subtask outcome is obtained by prompting the LLM to carefully analyze $c$ and $p$ (optionally with reference code) from a specific perspective and then summarize the analysis into a binary decision.\n3) Simulation. During the simulation process,"}, {"title": "3.3 Reward Mechanism", "content": "Reward is crucial in MCTS to guide the search toward promising paths while minimizing suboptimal exploration. Moreover, cumulative rewards directly determine the final answer in MCTS-Judge, further underscoring the importance of reward accuracy. However, verifying the correctness of predictions without ground truth labels is challenging. Approaches like M* (Kang et al., 2024) and LLaMA-Berry (Zhang et al., 2024c) attempted to address this issue by training a reward model, but these methods often struggle with data collection and risk overfitting. RAP (Hao et al., 2023) introduced a self-evaluation mechanism where rewards are derived by asking the LLM to identify errors in its reasoning within a single completion. However, this mechanism may perform close to random if the LLM's capabilities are limited (Qi et al., 2024).\nTherefore, inspired by execution-based evaluation methods (Liu et al., 2024; Xia et al., 2024; Zhang et al., 2024d), we propose a fully LLM-powered simulated execution reward mechanism that requires no training and enhances reliability through cross-checking and step-by-step, in-depth analysis. As shown in Fig.3, our reward mechanism consists of two main phases: test case generation and test case execution simulation. The test case generation phase is completed prior to the start of the MCTS process and requires only the problem statement $p$. This phase leverages GPT4o (OpenAI, 2024) to construct, validate, and store test cases. The test case execution simulation phase takes place when the MCTS search reaches its maximum depth, and is executed using the same LLM utilized during the MCTS process. The test cases used in this paper are all in the form of input-output pairs, meaning that given an input, the code being evaluated should produce the corresponding output.\n1) Recipe for Test Case Generation. In the test case generation phase, we first instruct GPT-4o to carefully analyze the requirements of the problem, systematically identifying constraints, boundary conditions, and special cases, and then generate multiple low-complexity test cases covering various scenarios with brief explanations for each. Next, we employ GPT-4o to organize these test cases into structured pairs. Finally, we validate these test case pairs by inputting them into GPT-4o one at a time and asking it to perform $\\beta$ self-evaluations to determine whether the relationship between the input and output aligns with the expected behavior described in the problem statement. Test cases that pass this validation are stored, while those that fail are discarded.\n2) LLM-driven Execution Simulation. When the MCTS search reaches its maximum depth, the test case execution simulation phase begins. We randomly select $\\gamma$ stored test cases, mask their outputs, and provide the inputs to the LLM one by one. We instruct the LLM to simulate a code interpreter, executing the code line by line while tracking variable changes, and then determining the expected output for the given input based on this execution trace. This process repeats $\\delta$ times per test case, and the generated outputs are compared with the originally stored outputs. The majority vote from the $\\delta$ repetitions finally determines whether a test case passes. The reward mechanism predicts that the code is correct only if all sampled test cases pass, and the result is expressed by $h(x)$. This design mirrors practical test case evaluation: if a code passes all test cases, it may be correct; however, if it fails any test case, it is definitively incorrect. Finally, if the trajectory's prediction $f (t, g)$ matches $h(x)$, the trajectory receives a terminal reward $e$.\nIn doing so, our reward mechanism is both cautious and reliable, leveraging the characteristics of the code evaluation task to establish a systematic, cross-checking evaluation process that effectively minimizes errors. Additionally, by simulating an interpreter that executes the code line by line, our approach encourages LLMs to perform fine-grained deductive reasoning, considering code flow, variable updates, and logical branches. This detailed analysis helps uncover potential errors that might otherwise go unnoticed with a superficial \"general impression\", ensuring that final conclusions are grounded in concrete and verifiable evidence."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Setup", "content": "Following previous work (Tong and Zhang, 2024), we evaluated MCTS-Judge on three challenging benchmarks: HumanEval-X (Zheng et al., 2023), APPS (Hendrycks et al., 2021), and BigCodeBench (Zhuo et al., 2024). HumanEval-X includes 164 introductory coding tasks across five programming languages. APPS consists of Python coding tasks of three different difficulty levels, from which we randomly selected 100 competitionlevel tasks. BigCodeBench contains 1,140 practical and challenging Python programming tasks, covering 723 function calls from 139 libraries. For tasks in BigCodeBench that lack meaningful inputoutput pairs, such as drawing or compressing, we shift the reward mechanism from simulated execution to simulated discussions, granting rewards only when all reasoning steps yield positive signals that enhance generalizability.\nMCTS-Judge is a general framework applicable to various LLMs. To evaluate the effectiveness and generalizability of our approach, we employed five different LLMs as base models, including code-specialized LLMs: Qwen2.5-Coder14B (Hui et al., 2024), DeepSeek-Coder-V2-16BInstruct (Zhu et al., 2024) and Mistralai-Codestral22B (team, 2024), as well as general LLMs: Llama3.1-8B-Instruct (Touvron et al., 2023) and GPT-4o-mini (Achiam et al., 2023). We compare MCTSJudge with three System 2 thinking LLMs, including OpenAI ol-preview (Jaech et al., 2024), ol-mini (Jaech et al., 2024), and Qwen-QwQ32B (Qwen, 2024), as well as two LLM-as-a-Judge paradigms designed for code evaluation with System 1 thinking: CodeJudge (Tong and Zhang, 2024) and ICE-Score (Zhuo, 2023). Additionally, we introduce a baseline named Vanilla, which directly prompts the LLM to assess the code correctness, showcasing the inherent code evaluation capability of the base model. More details such as hyperparameters and prompts are included in Appendix A."}, {"title": "4.2 Main Results", "content": "Table 1 presents the comparison results between MCTS-Judge and baselines. We highlight three key observations: (1) MCTS-Judge significantly enhances the code evaluation capabilities of all base models. When using open-source LLMs with substantially smaller model sizes, its performance can match or even surpass o1-series models. This phenomenon is illustrated more clearly in Fig. 4. On average, MCTS-Judge achieves a 14.34% accuracy improvement across five different base models on three benchmarks. In particular, DeepSeekCoder-V2-16B-Instruct, originally at 41% accuracy on the APPS benchmark, improved dramatically to 80% with MCTS-Judge, surpassing both 01preview and o1-mini. (2) Any base model we evaluated, when powered by MCTS-Judge, outperforms the open-source reasoning model Qwen-QwQ-32B on most tasks. For instance, MCTS-Judge based on Llama-3.1-8B-Instruct, with a model size only a quarter of Qwen-QwQ-32B, outperforms it in all tasks except those using the Go language, achieving up to a 20.88% higher accuracy. (3) Compared to previous LLM-as-a-Judge paradigms with System 1 thinking, MCTS-Judge demonstrates significantly superior performance in all tasks. For example, MCTS-Judge with DeepSeek-Coder-V216B-Instruct achieved 18% higher accuracy than CodeJudge and 32% higher than ICE-Score on the APPS benchmark."}, {"title": "4.3 Inference Efficiency", "content": "To evaluate the test-time computational efficiency of MCTS-Judge, we analyzed the average number of tokens generated on the APPS benchmark. As presented in Table 2, when using DeepSeek-CoderV2-16B-Instruct as the base model, MCTS-Judge outperforms ol-preview in accuracy, while only consuming one third as many of the reasoning tokens and maintaining a model size that is 19 times smaller."}, {"title": "4.4 Fine-grained Quality Assessment", "content": "MCTS-Judge demonstrated superior code correctness evaluation ability, while simultaneously generating multi-perspective analyses during reasoning trajectory construction. We believe that this may offer developers deeper insights into the code, providing a distinct advantage over both similaritybased and execution-based evaluation methods. To evaluate the quality of meta-analysis and reasoning capabilities, we compared the reasoning trajectories generated by MCTS-Judge with three reasoning models-01-preview, 01-mini, and Qwen-QwQ32B across four critical dimensions: thoroughness, logic, analysis, and overall reasoning quality, with GPT-4o assessing the win rate. As shown in Table 3, MCTS-Judge with Deepseek-Coder-V2-16BInstruct and Qwen2.5-Coder-14B-Instruct consistently achieves higher win rates, particularly excelling at thoroughness and depth of analysis."}, {"title": "4.5 Extensions to General Scenarios", "content": "Reference code is crucial for similarity-based evaluation but is often unavailable in practice. While its inclusion enhances LLMs' problem understanding, LLM-as-a-Judge methods can adapt to more generalizable scenarios without reference code. We evaluated MCTS-Judge and baselines on three benchmarks without reference code (full results in Appendix C). As shown in Table 4, the absence of reference code significantly degrades the performance of existing LLM-as-a-Judge frameworks. In contrast, our MCTS-Judge demonstrates exceptional robustness, with only minimal performance drop, highlighting its promising generalization capabilities."}, {"title": "4.6 Scaling Test-time Computation", "content": "We explore the relationship between test-time computational scale and performance gains under our LLM-as-a-Judge framework. MCTS-Judge relies on simulated execution of test cases to determine the terminal reward, thereby providing more accurate guidance for MCTS and final prediction selection. Intuitively, increasing the number of test cases (a) reduces the likelihood of misjudging incorrect code as correct, while increasing the execution times per test case ($\\delta$) further enhances accuracy. Furthermore, extending the maximum tree depth provides a more comprehensive evaluation, and more rollouts enable broader exploration. Fig. 5 demonstrates the impact of these key hyperparameters on the APPS benchmark using DeepSeek-Coder-V2-16B-Instruct and Qwen2.5-Coder-14B-Instruct as base models. As expected, MCTS-Judge benefits from extending the test-time computation, although the trends vary depending on the specific hyperparameters and models. These findings align with similar observations by OpenAI (Openai, 2024), demonstrating the potential of test-time scaling for LLM-as-a-judge paradigms."}, {"title": "4.7 Ablation Studies", "content": "Table 5 presents the results of ablation studies assessing the core components of MCTS-Judge. Under System-1 thinking, the Vanilla approach reflects the base model's inherent evaluation capability, while Majority Vote executes all subtasks and selects the most frequent answer. Majority Vote achieves a 13% accuracy improvement over Vanilla, demonstrating the benefit of integrating multi-perspective evaluation into the framework. Under System-2 thinking, driven by MCTS, reward mechanisms are further analyzed. RMsc assigns rewards based on self-consistency majority voting (Qi et al., 2024), while RMSE incorporates self-evaluation rewards (Hao et al., 2023). Our proposed simulated execution reward, closely aligned with ground truth, surpasses RMSC and RMSE by 13% in accuracy. Additionally, the variant that selects nodes purely based on UCT is outperformed by the complete MCTS-Judge, demonstrating the effectiveness of our global-local aware node selection strategy."}, {"title": "5 Conclusion", "content": "In this work, we propose MCTS-Judge, a novel resource-efficient, test-time computation LLM-as-aJudge framework with System-2 thinking for code correctness evaluation. Powered by a fully LLM-driven MCTS, MCTS-Judge decomposes problems into simpler, multi-perspective evaluations. Through our global-local node selection strategy, along with guidance from a simulated execution reward mechanism, MCTS-Judge performs line-by-line deep analysis. Experiments on five LLMs and three benchmarks show that MCTSJudge significantly improves base model accuracy, surpassing o1-series models and Qwen-QwQ32B with one-third of the tokens and a smaller model size. Compared to existing LLM-as-a-Judge frameworks with System-1 thinking, MCTS-Judge achieves SOTA performance while reducing dependence on reference code. Moreover, its reasoning trajectory shows superiority in logic, analytics, thoroughness, and overall quality. We further reveal the test-time scaling law of MCTS-Judge, marking an important first step in integrating test-time computation with the LLM-as-a-Judge paradigm."}, {"title": "A Experiment Settings", "content": "Table 6 details the hyperparameter settings of MCTS-Judge employed to generate the results presented in this paper. These settings encompass various aspects of the MCTS architecture, including maximum tree depth, number of rollouts, exploration constant $\\alpha$, LLM sample weight $\\omega_l$, and UCT sample weight $\\omega_u$. Additionally, the reward mechanism parameters include the number of test case validations $\\beta$, number of test cases used $\\gamma$, number of test case simulations $\\delta$, and reward scaling factor $e$. Hyperparameters related to the LLM configuration, such as temperature, top_p, top_k, and maximum output tokens, are also specified. All experiments were executed on a single H100 GPU with 80GB of memory, ensuring consistency and reproducibility in computational performance."}, {"title": "B Limitations of Execution-Free Methods", "content": "In this experiment, we evaluate whether similaritybased execution-free metrics, which do not require test cases and isolated environments, can be used to accurately assess code correctness. We evaluated six representative metrics on the APPS benchmark: BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, and RUBY. Fig. 6 shows that the score distributions for incorrect and correct code differ only slightly for these metrics. This further highlights the importance of developing high-precision execution-free methods for assessing code correctness."}, {"title": "C Evaluation without References", "content": "As discussed in Sec.4.5, we present the complete results for extending to more general scenarios,"}, {"title": "D Case Study", "content": "We demonstrated the superiority of analysis metadata from reasoning chains generated by MCTSJudge across four key dimensions in Section 4.4. To further highlight MCTS-Judge's advantages over the System-1 thinking approach in terms of accuracy and comprehensive analysis, we present a case study comparing the vanilla Deepseek-Coder-V216B-Instruct model with the one integrated with MCTS-Judge.\nAs shown in Fig. 7, at each step, MCTS-Judge provides evaluations from diverse perspectives, such as verifying whether requirements are met (Step 2) and assessing the correctness of the code logic (Step 5). In contrast, the vanilla model generates an incorrect answer with only superficial analysis, demonstrating a lack of deeper understanding."}, {"title": "E Prompts", "content": "We present the key prompt designs utilized in MCTS-Judge, including the vanilla baseline (Fig. 8), the LLM-driven self-assessment (Fig. 9), the logic assessment action (Fig. 10), the test case generation and validation (Fig. 11), and the simulated execution (Fig. 12)."}]}