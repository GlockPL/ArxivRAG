{"title": "QUANTIFYING CALIBRATION ERROR IN MODERN NEURAL NETWORKS THROUGH EVIDENCE BASED THEORY", "authors": ["Koffi Ismael OUATTARA"], "abstract": "Trustworthiness in neural networks is crucial for their deployment in critical applications, where reliability, confidence, and uncertainty play pivotal roles in decision-making. Traditional performance metrics such as accuracy and precision fail to capture these aspects, particularly in cases where models exhibit overconfidence. To address these limitations, this paper introduces a novel framework for quantifying the trustworthiness of neural networks by incorporating subjective logic into the evaluation of Expected Calibration Error (ECE). This method provides a comprehensive measure of trust, disbelief, and uncertainty by clustering predicted probabilities and fusing opinions using appropriate fusion operators. We demonstrate the effectiveness of this approach through experiments on MNIST and CIFAR-10 datasets, where post-calibration results indicate improved trustworthiness. The proposed framework offers a more interpretable and nuanced assessment of AI models, with potential applications in sensitive domains such as healthcare and autonomous systems.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) systems, particularly neural networks, are increasingly employed in critical applications such as healthcare, finance, and autonomous systems. These systems play an integral role in decision-making, where the trustworthiness of their predictions becomes paramount. Trustworthiness in AI encompasses attributes like reliability, robustness, fairness, and transparency, yet these qualities are often difficult to evaluate, particularly in neural networks, which are typically viewed as \"black-box\" models. This opacity raises significant concerns about their trustworthiness, especially in sensitive domains where incorrect decisions can lead to severe consequences.\nTraditional performance metrics like accuracy, precision, and recall measure only the correctness of the model's predictions but fail to capture the confidence and uncertainty associated with those predictions. Confidence calibration, which aligns predicted probabilities with actual outcomes, has emerged as an important tool to address these shortcomings. Well-calibrated models provide predictions where the predicted probability corresponds to the actual likelihood of the event, ensuring that a 70% confidence means the event occurs approximately 70% of the time. However, despite its utility, calibration alone does not fully address the issue of trustworthiness, as it does not account for subjective uncertainty or provide an interpretable way to assess trust across a range of predictions.\nTo address this, we propose the use of subjective logic for trustworthiness quantification in neural networks. Subjective logic extends probabilistic logic by incorporating degrees of belief, disbelief, and uncertainty, making it well-suited for"}, {"title": "2 Related Work", "content": "The quantification of AI trustworthiness is an increasingly critical area of research, driven by the imperative to ensure that Al systems are reliable, fair, and interpretable. This section reviews seminal works that contribute to the development of methodologies and frameworks for evaluating and quantifying trust in AI systems, particularly focusing on deep neural networks (DNNs) and neural networks (NNs).\nInterpretable Trust Quantification Metrics for DNNs Wong et al. [2021]: Wong, Wang, and Hryniowski propose a novel set of interpretable metrics aimed at quantifying the trustworthiness of DNNs based on their performance in a series of questions. Their research, outlined in \"How Much Can We Really Trust You? Towards Simple Interpretable Trust Quantification Metrics for Deep Neural Networks,\" Wong et al. [2021] introduces metrics such as Question-Answer Trust, Trust Density, Trust Spectrum, and NetTrustScore. These metrics focus on the accuracy and confidence of AI outputs, emphasizing the importance of output confidence in the broader context of AI reliability without delving into other aspects like privacy or security. Their approach is static, as it establishes trust in the AI system through a one-time evaluation. It's a black box method, focusing on the AI system's outputs without requiring insight into its internal processes.\nComprehensive Framework for Trustworthiness Evaluation Celdran et al. [2023]: Contrasting with the singular focus on output confidence, Huertas Celdran and colleagues in \"A Framework Quantifying Trustworthiness of Supervised Machine and Deep Learning Models,\" Celdran et al. [2023] offer a broader evaluation encompassing fairness, explainability, robustness, and accountability. This framework provides a structured methodology for a holistic measure of trust in AI systems, underlining the criticality of these four pillars in establishing a comprehensive understanding of AI trustworthiness. Their approach is static and does not explicitly align with the white or black box paradigms but offers a structured evaluation that could incorporate elements of both.\nSurvey on Uncertainty in DNNs Gawlikowski et al. [2022]: Gawlikowski et al. in \"A Survey of Uncertainty in Deep Neural Networks,\" Gawlikowski et al. [2022] focus on the quantification of uncertainty inherent in DNN predictions. By categorizing uncertainty into model, data, and distributional types, their work highlights the importance of distinguishing these uncertainties to enhance the reliability and trustworthiness of AI systems in high-stakes applications. The survey underscores the challenge of establishing trust based solely on output accuracy/confidence. The paper delves into \"Single Deterministic Methods,\" exploring a dynamic and white box approach for assessing uncertainty in neural network (NN) predictions. This method involves utilizing single deterministic network methods to quantify uncertainty"}, {"title": "3 Background", "content": ""}, {"title": "3.1 Subjective Logic", "content": ""}, {"title": "3.1.1 Subjective Opinion", "content": "Subjective logic J\u00f8 sang [2016] is an extension of probabilistic logic to consider uncertainty. By adding uncertainty,\nsubjective logic allows random variable to take a value such as \"I don't know\". Subjective opinion notation is w, the\nsubscript X indicates the target variable or proposition to which the opinion applies, and the superscript A indicates\nthe subject agent who holds the opinion, i.e., the belief owner. In Subjective Logic we can distinguish three types of\nopinions (we consider that the domain of X is X):\n1. binomial:\n\u2022 card(X) = 2(X = {x, x}), w = (bx, dx, Ux, ax),\n\u2022\n\u2022 b belief mass in support of X being TRUE (i.e. X = x ),\n\u2022 d disbelief mass in support of X being FALSE (i.e. X = X ),\nUx, uncertainty mass representing the vacuity of evidence,"}, {"title": "3.1.2 Subjective Logic Fusion Operators", "content": "Subjective Logic can be used to analyze trust. Josang defined trust as a subjective binomial opinion (t, d, u): (trust,\ndistrust, untrust). He also introduced fusion operators that can be used to fuse information derived using different ways.\nThese fusion operators are essential for merging opinions in various scenarios, such as trust analysis, decision-making,\nand expert systems.\nSeveral fusion operators are used depending on the nature of the information and the relationship between sources.\nBelief Constraint Fusion (BCF) applies when no compromise is possible between opinions, meaning no conclusion is\ndrawn if there is total disagreement. Cumulative Belief Fusion (CBF), in its aleatory and epistemic forms, assumes\nthat adding more evidence reduces uncertainty, especially in statistical processes (A-CBF) or subjective knowledge\n(E-CBF). Averaging Belief Fusion (ABF) is used when opinions are dependent but equally valid, averaging them without\nassuming more evidence increases certainty. Weighted Belief Fusion (WBF) gives more confident opinions greater\nweight, ideal for expert input where confidence varies. Finally, Consensus & Compromise Fusion (CCF) preserves\nshared beliefs while turning conflicting opinions into vague beliefs, reflecting uncertainty and fostering consensus.\nChoosing the appropriate fusion operator depends on the specific situation. For example, BCF is useful when strict\nagreement is required, while CCF is suited for cases where compromise is possible. By understanding the nature of the\nopinions and their relationships, analysts can select the most effective fusion operator to ensure accurate and meaningful\nresults."}, {"title": "3.1.3 Binomial Opinion from evidence", "content": "In subjective logic, a binomial opinion is a probabilistic framework that expresses belief, disbelief, and uncertainty\nregarding a binary event or proposition. This opinion is particularly useful for decision-making under uncertainty, as it\nmodels not only the likelihood of an event but also the uncertainty due to limited or unreliable evidence.\nThe binomial opinion is characterized by four parameters:\n\u2022 bx: The belief mass in support of proposition X = x being true.\n\u2022\n\u2022 dx: The disbelief mass, indicating the belief that X = x is false.\nUx: The uncertainty mass, representing the vacuity of evidence for X.\nax: The base rate, which is a prior probability expressing an agent's bias toward X in the absence of evidence.\nThese parameters follow the constraint bx + dx + ux = 1, ensuring the total belief, disbelief, and uncertainty add up to\nunity. The projected probability of X is then defined as:\n$P(x) = bx + ux \\cdot Ax$\nThis form allows for a flexible representation of both the evidence and the lack of evidence for a particular hypothesis."}, {"title": "3.2 Neural Network Calibration", "content": "Neural networks have become the backbone of many modern AI applications due to their ability to learn and generalize\nfrom large datasets. However, an important aspect of their performance that has garnered attention is calibration.\nCalibration refers to the degree to which a model's predicted probabilities align with the true likelihood of outcomes. A\nwell-calibrated model provides predicted probabilities that match the actual frequencies of events. For example, among\nall instances where a model predicts a 70% probability of an event, that event should occur approximately 70% of the\ntime.\nDespite their high accuracy, many state-of-the-art neural networks, such as deep convolutional networks used in\nimage classification or recurrent networks in natural language processing, often suffer from poor calibration. This\nmisalignment can lead to overconfident or underconfident predictions, which is problematic in applications where\nuncertainty estimation is crucial, such as medical diagnosis or autonomous driving."}, {"title": "3.2.1 Calibration Error", "content": "Expected Calibration Error (ECE) is one of the most common metrics used to quantify calibration quality. ECE\ncompares the predicted confidence of a model with the actual outcomes by grouping predictions into bins based\non confidence levels. Each bin represents a range of predicted probabilities (e.g., 0.0 to 0.1, 0.1 to 0.2, etc.). The\ndifference between the model's average accuracy in each bin and its average predicted confidence gives a measure of\nthe miscalibration. Formally, ECE is calculated as:\n$ECE = \\sum_{m=1}^{M} \\frac{|B_m|}{n} |acc(B_m) - conf(B_m)|$\nwhere M is the number of bins, Bm represents the set of predictions in bin m, and n is the total number of samples.\nPerfect calibration occurs when the predicted confidence exactly matches the actual accuracy in every bin, resulting in\nan ECE of 0. A higher ECE indicates miscalibration, which could manifest as overconfidence (the predicted probability\nis higher than the actual outcome) or underconfidence (the predicted probability is lower than the actual outcome).\nWhile useful, ECE does not offer a full view of a model's trustworthiness because it focuses solely on probability\nalignment without considering the underlying uncertainty in predictions."}, {"title": "3.2.2 Calibration Techniques", "content": "Several methods have been developed to improve neural network calibration. One of the most effective and widely-used\ntechniques is Temperature Scaling, a simple post-processing method introduced by Guo et al. Guo et al. [2017]. In this\nmethod, the logits (the raw outputs of the neural network before applying the softmax function) are divided by a scalar\ntemperature parameter T, which is optimized using a validation set. The modified logits are then fed into the softmax\nfunction to generate calibrated probabilities. The temperature parameter T is tuned to reduce the difference between\npredicted probabilities and actual outcomes, as measured by metrics like the Negative Log-Likelihood (NLL).\nTemperature scaling does not affect the model's classification accuracy but reduces its confidence in predictions,\nresulting in better-calibrated probabilities. This technique is particularly advantageous because it is easy to implement,\ncomputationally inexpensive, and does not require retraining the model. By adjusting the temperature, it makes\noverconfident models more realistic in their probability estimates, thus improving their trustworthiness."}, {"title": "4 Problem statement", "content": "The increasing deployment of neural networks in critical applications such as healthcare, finance, and autonomous\nsystems necessitates the need for reliable and ethical decision-making. Despite their high performance in terms of\naccuracy, these neural networks are often perceived as black-box systems due to their complex and opaque nature.\nThis opacity leads to significant challenges in evaluating the trustworthiness of these models, which is crucial for their\nadoption in sensitive domains.\nTraditional metrics like accuracy, precision, and recall focus solely on the performance of neural networks in terms of\ntheir output but, fail to capture the uncertainties and subjective aspects associated with their predictions. These metrics\ndo not provide insights into how much confidence one can place in the model's predictions, particularly in situations\nwhere the model might be overconfident or underconfident.\nOne approach to improving trustworthiness is through confidence calibration, where methods like the Expected\nCalibration Error (ECE) are used to align predicted probabilities with actual outcomes. However, while ECE provides a\nuseful measure of calibration, it is not easily interpretable and does not offer a comprehensive view of an AI system's\ntrustworthiness. ECE focuses on the difference between predicted probabilities and actual outcomes but lacks the\ncapability to represent the degrees of belief, disbelief, and uncertainty that are crucial for a more nuanced trust\nassessment.\nFurthermore, existing black-box approaches for quantifying trustworthiness often require access to ground truth data\nduring the operational phase, which can be difficult or even impossible to obtain in real-world scenarios. In some cases,\nthese methods rely on the assumption of an oracle to provide ground truth, but this oracle itself may be unreliable or\nuntrustworthy, further complicating the assessment process. This reliance on unavailable or uncertain ground truth limits\nthe effectiveness of these approaches, highlighting the need for more robust methods that can evaluate trustworthiness\nwithout depending on such assumptions."}, {"title": "5 Quantification Framework", "content": "This paper tackles the challenges of quantifying the trustworthiness of neural networks by introducing a novel framework\ngrounded in subjective logic. Our primary contribution lies in developing a comprehensive trustworthiness quantification\nmethod that extends the traditional Expected Calibration Error (ECE). This framework incorporates subjective measures\nof belief, disbelief, and uncertainty, offering a more nuanced and interpretable evaluation of model trustworthiness.\nThe framework operates through the following steps summarized in Algorithm 1:"}, {"title": "5.1 Clustering Predicted Probabilities", "content": "For each class in the output layer of a neural network, we create M clusters of predicted probability values within the\nrange [0, 1]. Each cluster represents a specific range of predicted probabilities and has a representative value, which can\nbe the mean probability of the cluster. This clustering allows us to group predictions with similar confidence levels\ntogether, facilitating a more granular analysis of trustworthiness."}, {"title": "5.2 Computing Trustworthiness Opinions", "content": "For each cluster i, we calculate the number of classifications ni that fall into this cluster. We then compute the following:\n\u2022 Positive Evidence: The representative probability value of the cluster.\n\u2022 Negative Evidence: This quantity is based on the difference between:\n(1) the ratio of the number of true classifications in the cluster (tc) to the total number of predictions in the\ncluster (nc) and\n(2) the representative probability (RP).\nThe full equation is: altc>nc\u00d7RP;(tc - nc \u00d7 RPi) + \u1e9e1tc<nc\u00d7RP; (nc \u00d7 RPi - tc) a and \u1e9e choose based on\nhow we want to penalize over and under-confidence. For simplicity, we use a = \u03b2 = 1, thus: |tc-nc\u00d7RPi|=\n|\\frac{t_c}{n_c} - RP|.\n\u2022 The total number of evidence in our framework is then: $t_c + |t_c - n_c \u00d7 RP| = t_c(1 + |1 \u2013 \\frac{t_c}{n_c} \u00d7 RP|)$,\nwhich mostly depends on how far the the predictive probability is from actual probability.\nUsing these evidences, we compute a subjective logic opinion for the trustworthiness of each probability cluster. This\nopinion incorporates degrees of belief, disbelief, and uncertainty, providing a nuanced measure of trustworthiness."}, {"title": "5.3 Fusing Trust Opinions", "content": "The trustworthiness opinions for each probability cluster are combined using appropriate fusion operators. This\nstep allows us to synthesize the trust opinions into a single, comprehensive trust opinion that reflects the overall\ntrustworthiness of the neural network's predictions. The fusion process accounts for the varying levels of trustworthiness\nacross different clusters, ensuring that the final trust opinion is balanced and representative.\nAfter fusing trust opinion for each clusters, we obtain a trust opinion for the class. Then we have to fuse all these trust\nopinion (for each class) using another appropriate fusion operator to get the final trust opinion on the Neural Network."}, {"title": "5.4 From Static to Dynamic", "content": "To extend our framework from a static evaluation to a dynamic quantification of trustworthiness, we propose maintaining\nindividual opinions for each probability cluster. During the operational phase, after inference, the predicted probabilities\ncan be recorded and mapped to their corresponding clusters. Trustworthiness is then derived for each probability\nprediction within its respective cluster and label. These trustworthiness scores are subsequently fused using an\nappropriate fusion operator, dynamically evaluating the trustworthiness of the output probabilities in real-time as new\npredictions are made."}, {"title": "6 Experiments", "content": "We evaluate two neural networks, one trained on the MNIST dataset and the other on CIFAR-10. The evaluation consists\nof assessing the trustworthiness of the models before and after calibration using temperature scaling. To quantify\ntrustworthiness, we applied our algorithm (based on subjective logic) to calculate belief, disbelief, and uncertainty for\nindividual probability clusters, and we used cumulative fusion to combine these opinions. We chose cumulative fusion\nbecause of its associativity and commutativity which allows us to generalize this fusion operators to more than two\ntrustworthiness opinion 2."}, {"title": "6.1 Datasets", "content": "We evaluated the models on two well-known datasets:\n\u2022 MNIST: The MNIST dataset consists of 70,000 grayscale images of handwritten digits (0-9), each of size\n28x28 pixels. We split the dataset into 60,000 images for training and 10,000 for testing. The task is to classify\neach image into one of the 10 digit classes.\n\u2022 CIFAR-10: The CIFAR-10 dataset contains 60,000 color images of size 32\u00d732, divided into 10 classes, with\n6,000 images per class. The dataset is split into 50,000 training images and 10,000 test images. The classes\nrepresent various objects, such as airplanes, automobiles, birds, cats, and more. Each image is classified into\none of the 10 object categories."}, {"title": "6.2 Neural Network Architectures", "content": "We designed two distinct neural network architectures, optimized for each dataset:\n\u2022 MNIST Network: A fully connected neural network was employed for MNIST classification. The architecture\nconsists of:\nThe input is a 28\u00d728 grayscale image. This layer flattens the 2D image into a 1D array of 784 elements to\nserve as the input for the subsequent fully connected layers.\nA fully connected layer with 128 units, followed by the ReLU activation function. This layer learns 128\nfeatures from the input data, allowing the network to model complex relationships in the data.\nThe final fully connected layer with 10 units (one for each digit class). This layer outputs raw scores\n(logits) for each of the 10 classes. These logits are then converted into probabilities using the softmax\nfunction during evaluation and prediction.\nThis straightforward architecture is sufficient for classifying MNIST digits due to the simplicity of the dataset.\n\u2022 CIFAR-10 Network: A convolutional neural network (CNN) was employed for CIFAR-10 classification. The\narchitecture consists of:\nConvolutional Layers: The model includes three convolutional layers with ReLU activation. The first\nlayer uses 32 filters, while the subsequent two layers each use 64 filters, all with a 3\u00d73 kernel size. These\nlayers progressively extract low- to high-level features from the input images.\nMax Pooling Layers: After the first and second convolutional layers, 2\u00d72 max pooling layers are applied\nto reduce the spatial dimensions of the feature maps, improving computational efficiency.\nFlatten Layer: The feature maps from the final convolutional layer are flattened into a 1D vector, preparing\nthem for the fully connected layers.\nDense Layer: A fully connected layer with 64 units, followed by ReLU activation, enables the model to\nlearn complex patterns from the extracted features.\nOutput Layer: The final layer is a softmax layer with 10 units, providing the probability distribution\nacross the 10 CIFAR-10 classes.\nThis architecture is well-suited for the complex task of object classification in CIFAR-10, handling the\nvariations in color, texture, and shapes present in the dataset."}, {"title": "6.3 Calibration: Temperature Scaling", "content": "After training, we calibrated both models using temperature scaling, a post-processing technique designed to align\npredicted probabilities with actual outcomes. Temperature scaling modifies the model's output logits without changing\nits classification accuracy, thereby improving the reliability of the predicted probabilities."}, {"title": "7 Results and Discussion", "content": "This section compares the trustworthiness evaluation results before and after calibration (using temperature scaling) of\nthe MNIST and CIFAR-10 neural networks. The metrics\u2014trust, disbelief, and uncertainty were assessed over 100\nepochs for all labels (0 to 9). The color bar in the figures represents the label corresponding to each curve, allowing us\nto observe the impact of calibration on individual labels. The results reveal distinct differences between the pre- and\npost-calibration performance, particularly in how the metrics evolve over time.\nWhile the trustworthiness curves (trust/belief, distrust/disbelief, and uncertainty) start at similar points both before and\nafter calibration, the post-calibration curves exhibit greater stability. In contrast, the pre-calibration performance tends\nto worsen, with a decrease in trust alongside an increase in distrust and uncertainty."}, {"title": "7.1 MNIST Results", "content": "The results for MNIST model are depicted in Figure 1"}, {"title": "7.2 CIFAR10 Results", "content": "For the CIFAR-10 dataset, a similar trend was observed, where the post-calibration model exhibited stable trustworthi-\nness, while the pre-calibration model's performance deteriorated over time. The results are illustrated in Figure 2\n\u2022 Trust: Both pre- and post-calibration trust curves began at similar values (at around 0.7) the increase a little\nbit for some labels to reach around 0.75 for pre calibration and 0.82 for post calibration. after that without\ncalibration, as training continued, trust decreased to reach 0.48 after 20 epochs and remains constant. After\ncalibration, trust decreases a little to come back to the initial value 0.7 after around 20 epochs and then remained\nconstant. This reflects the impact of calibration in maintaining high confidence in probability predictions.\n\u2022 Disbelief: Both pre- and post-calibration disbelief curves began at similar values, around 0.4. Before calibration,\ndisbelief gradually increased as training progressed, reaching a peak of 0.58 after approximately 20 epochs,\nwhere it remained relatively constant for the rest of the training. After calibration, disbelief initially increased\nslightly, but then decreased steadily to return to the initial value of 0.4 around the 20th epoch. It remained\nstable thereafter. This shows that calibration was effective in controlling the network's overconfidence in\nincorrect predictions, leading to more reliable performance over time."}, {"title": "7.3 Discussion", "content": "The behavior of the metrics for the CIFAR-10 model can be explained by the fact that the model requires around 10\nepochs to be well-trained. After this point, as shown in Figure 3b, the model begins to overfit, as indicated by the\nincreasing gap between the training and validation losses. This overfitting leads to poor generalization and explains why\nthe metrics-trust, disbelief, and uncertainty\u2014are optimal for both pre- and post-calibration models during the early\nepochs, but deteriorate sharply for the pre-calibration model after about 10 epochs. Specifically, trust decreases, and\ndisbelief and uncertainty increase for the pre-calibration model as overfitting sets in. In contrast, the post-calibration\nmodel experiences a slight decline in performance but stabilizes quickly, maintaining higher trustworthiness throughout\nthe training.\n\u2022 Overfitting in CIFAR-10: The overfitting observed in the CIFAR-10 model results in a more overconfident\nnetwork. This overconfidence leads to poor probability predictions, which causes trust to decrease and disbelief\nto increase, as evidenced by the worsening of these metrics in the pre-calibration model. After around 20\nepochs, trust in the pre-calibration model drops to around 0.48, and disbelief peaks at 0.58, remaining relatively\nconstant thereafter (as shown in the results).\n\u2022 Post-Calibration Stability: Temperature scaling mitigates the effects of overfitting by adjusting the model's\nconfidence levels. This explains why the post-calibration model remains stable after the initial performance\ndip and avoids the severe deterioration seen in the pre-calibration model. After calibration, trust decreases\nonly slightly and stabilizes around 0.7 after 20 epochs, while disbelief and uncertainty also stabilize at much\nlower levels compared to the pre-calibration model.\nFor the MNIST model, the dynamics are different. As shown in Figure 3a, both training and validation losses remain\nlow and close to each other throughout the training process, indicating little to no overfitting. This explains why we do\nnot observe the same pattern of improvement followed by deterioration as seen in CIFAR-10.\n\u2022 MNIST Training Behavior: The MNIST model reaches its best performance relatively early in training, and\nthere is little difference in behavior between the pre- and post-calibration models. The absence of significant\noverfitting means that trust remains relatively constant throughout training for both models, and disbelief and\nuncertainty are minimal. The difference between the pre- and post-calibration models is less pronounced\nbecause the MNIST task is simpler and requires fewer epochs to reach optimal performance.\n\u2022 Impact of Overfitting on Trustworthiness: Overfitting leads to overconfident neural networks, which in turn\nproduce poorly calibrated probability predictions. This decreases belief (or trust) in the network's predictions\nand increases disbelief and uncertainty. In the CIFAR-10 model, these effects are evident, with trust decreasing\nand disbelief increasing as overfitting sets in. However, temperature scaling corrects for this by recalibrating\nthe model's confidence, which helps the post-calibration model remain trustworthy even after the base model\nbegins to overfit.\nIn conclusion, the key to understanding the results is the role of overfitting in the CIFAR-10 model, which leads to a\nsteep decline in performance for the pre-calibration model. Temperature scaling helps mitigate this, resulting in a more\nstable and trustworthy model. In contrast, the MNIST model does not experience significant overfitting, and thus the\ndifference between pre- and post-calibration models is less marked."}, {"title": "8 Conclusion", "content": "In this paper, we presented a novel framework for quantifying the trustworthiness of neural networks, particularly\nin classification tasks, by leveraging subjective logic. Our approach extends traditional Expected Calibration Error\n(ECE) by incorporating belief, disbelief, and uncertainty, providing a more interpretable and nuanced evaluation of"}]}