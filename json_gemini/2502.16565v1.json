{"title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems", "authors": ["Zengqing Wu", "Takayuki Ito"], "abstract": "Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios \u2013 Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision - confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making.", "sections": [{"title": "1 Introduction", "content": "Multi-agent systems (MAS) have long studied how autonomous agents coordinate to achieve shared objectives in domains such as disaster response, resource allocation, information management, and task solving (Chen et al., 2023; Cur\u0219eu and Schruijer, 2017; Hong et al., 2024; Qian et al., 2024). The recent advent of large language models (LLMs) as general-purpose agents (Li et al., 2023; Wu et al., 2023; Xing, 2024) presents novel opportunities for MAS: LLM agents can dynamically exchange information, interpret instructions, and reason in natural language. This flexible communication paradigm potentially enables more human-like approaches to consensus formation, diverging from rigid algorithms in conventional distributed systems.\nHowever, an important challenge emerges: while strong explicit consensus (e.g., centralized voting or forced agreement prompts) can unify the system (e.g., multi-agent debates (Chan et al., 2024)), it risks extinguishing critical diversity in agent opinions, limiting exploration and adaptability. Drawing on social science perspectives \u2013 particularly the notion of limited collective common sense (Whiting and Watts, 2024) \u2013 we observe that humans seldom rely on complete agreement to collaborate. Instead, model diversity, partial alignment, and tolerance for individual deviations often yields robust group outcomes, especially in uncertain or changing environments (Chen et al., 2024b; Dippel et al., 2024; Duan and Wang, 2024; Shang, 2019).\nMotivated by these insights, this paper proposes a dynamic consensus-diversity tradeoff that addresses the tension between shared understanding and autonomy in LLM-based MAS. Our key hypothesis is that implicit consensus, in which agents discuss but act based on their own subjective interpretations (via in-context learning), can outperform explicit consensus in tasks with high environmental volatility and the need for persistent exploration. By allowing each agent's internal chain-of-thought to incorporate external signals"}, {"title": "2 Related Work", "content": "Emerging Role of LLMs in Multi-Agent Systems Recent advances have started leveraging LLMs as autonomous agents within MAS (Chen et al., 2024a; Islam et al., 2024; Wang et al., 2024). Unlike traditional MAS with fixed protocols, LLM-based agents can dynamically communicate and coordinate via natural language, enabling more flexible collaboration. Early demonstrations show that multiple LLM agents working together can solve complex tasks beyond the capability of a single model. For example, frameworks like CAMEL employ two ChatGPT-based agents in complementary roles (e.g. user and assistant) to cooperatively complete tasks through iterative dialogue (Li et al., 2023). Similarly, HuggingGPT-style approaches orchestrate multiple specialized models guided by an LLM, hinting at the potential of MAS-driven problem solving. More recently, Generative Agents have been introduced as an application of LLM-based MAS in interactive simulations of social environments (Gao et al., 2024; Huang et al., 2024; Park et al., 2023). In this paradigm, dozens of LLM-driven agents simulated believable human behaviors and social interactions over time, demonstrating new use cases of MAS in social simulations and digital environments.\nCollaboration and Consensus Mechanisms in LLM-Based MAS A key challenge in LLM-based MAS is designing effective collaboration and consensus mechanisms among agents. Before the emergence of reinforcement learning-based or LLM-driven agents, game theory models like Nash equilibrium is a basic form of implicit agreement, serving as a foundational model with numerous game-theoretic applications in consensus formation (Fujita et al., 2014; Pramanik, 2021; Ye and Hu, 2017). Recently, various patterns of multi-agent interaction, especially LLM-based, have been explored to boost collective reasoning. For instance, a debate or deliberation protocol allows agents to propose ideas, critique each other, and reach a joint conclusion (Liu et al., 2024; Zhang et al., 2024). This method, inspired by social psychology, can improve reasoning through agent discussion and argumentation. Wang et al. (Wang et al., 2024) recently re-evaluated the claim that multi-agent LLM discussions inherently improve reasoning abilities. Their findings suggest that while multi-agent discussion can help on certain problems, its effectiveness depends on careful design of the interaction (e.g. turn-taking, prompting strategies) and the difficulty of the task.\nOther work has focused on hierarchical or specialized agent roles. For example, the MAgICoRe framework assigns LLM agents distinct roles such as Solver and Reviewer that interact in a coarse-to-fine refinement loop, leading to improved answer quality via iterative feedback (Chen et al., 2024c). This approach implicitly enables an ensemble of agent \"opinions\" to be refined into a more accurate consensus. In a similar vein, prior studies on chain-of-thought prompting have shown that generating multiple reasoning paths and then using a voting or selection process (often called self-consistency) can significantly improve solution correctness (Wang"}, {"title": "3 Methodology", "content": "In this section, we extend our framework for implicit and explicit consensus in LLM-based multi-agent systems and elaborate how we measure and exploit partial diversity for robust performance. We particularly focus on time-series analyses of agent opinion deviation to capture how consensus evolves across rounds under different dynamic conditions.\n3.1 Framework for Consensus & Diversity\nImplicit vs. Explicit Consensus. We consider N LLM agents {1,2,..., N} collaborating over discrete rounds t \u2208 {1, 2, . . . } to adapt to dynamic tasks. Each round, each agent i:\n1. Receives transcripts from previous rounds (or partial observations, if the environment limits communication).\n2. Generates a textual message containing suggestions, arguments, or evidence.\n3. Interprets others' messages in context via its chain-of-thought, then commits to a final action ai(t).\nExplicit Consensus: Agents unify into a single final action or plan, e.g., via majority voting:\nai(t) = arg max\n\u03b1\u0395\u0391 \u2211N\nj=1 I[vj(t) = a], (1)\nor through a forced-alignment prompt (\u201cPlease converge on one plan. Do not deviate.\u201d). This can"}, {"title": "3.2 Dynamic Consensus-Diversity Model", "content": "Real-world environments evolve over time; hence, the optimal action a* (t) may shift due to exogenous shocks. We define:\nC(t) = \u2211N\ni=1 \u03b4(a(t)), \u03bc(t) = Ea~C(t) [a], (3)\ndi(t) = ||ai(t) \u2013 u(t)||, d(t) = \u2211N\ni=1 di(t), (4)\nwhere di(t) measures agent i's deviation from the mean action \u00b5(t). We hypothesize an inverted-U curve relating d(t) to performance (Cur\u0219eu and Schruijer, 2017; Shang, 2019):\nPerformance(t) = f(di(t)),\ni=1\nwhere f is unimodal, peaking at moderate d(t) but dropping if d(t) = 0 (premature consensus) or if d(t) is too large (excessive fragmentation).\nIn-Context Learning & NLP Innovation. Unlike algorithmic methods (e.g., classical consensus protocols), each agent updates its reasoning purely through repeated natural language dialogue. This harnesses:\n1. Role Prompts: Each agent has a specialized textual description (e.g., \"You are a disaster medic,\" \"You are resource-averse\u201d), injecting innate diversity."}, {"title": "3.3 Stability and Scale Considerations", "content": "Theoretical Underpinnings. To address concerns that implicit consensus might never settle:\n\u2022 In Appendix C, we outline a simplified random-iteration model showing that if environment shocks are not too large and communication remains reliable, the system converges (or quasi-converges) in a region around a stable consensus state.\n\u2022 We also demonstrate the existence of a balance point for the diversity parameter under moderate noise, yielding an emergent \"optimal\" partial consensus in expectation.\nScalability. For large N, the cost of multi-round discussion might become prohibitive. We thus allow (in future expansions) group-level parallelism or hierarchical chat structures that keep the method feasible as N grows. In our current experiments (3 \u2264 N \u2264 100), we handle direct group discussion. The code is structured so that the approach generalizes to N > 100 if computational resources permit."}, {"title": "3.4 Baselines & Comparison Points", "content": "We compare implicit consensus to multiple baselines:\n\u2022 Single-LLM or No-Interaction baseline: a single agent or N identical agents each acting without coordination.\n\u2022 Fully Aligned (Explicit) Consensus: forced majority voting or prompts that mandate uniform actions.\n\u2022 Random or Heuristic Strategy: to show how LLM-based adaptive collaboration surpasses naive approaches.\n\u2022 No-Diversity setting: all agents share identical role prompts, demonstrating the cost of suppressed heterogeneity."}, {"title": "4 Experimental Setup", "content": "We design three dynamic scenarios to evaluate:\n1. Q1: Does implicit consensus outperform explicit coordination in volatile or adversarial conditions?\n2. Q2: Under what conditions do moderate deviations (diversity) improve system robustness?\n3. Q3: How do LLM agents' in-context chain-of-thought updates reflect an evolving consensus, as opposed to a static single-step approach?\nIn Section 5, we present both quantitative outcomes and example dialogues illustrating these points for scenario 1. For experimental results of scenarios 2 and 3, please refer to Appendix E.\n4.1 Scenario Overviews\nThis section shows the scenario of our three case studies, please refer to Appendix A for the details of environment setup and key experimental factors.\nScenario 1: Dynamic Disaster Response. Autonomous drones on a 10 \u00d7 10 grid must allocate limited firefighting/medical resources to disasters with varying severity, which can shift unpredictably every few rounds. The environment produces textual \"reports\" about possible severity changes or contradictory statements from local observers. Agents decide either via explicit single-plan consensus or implicit free decisions post-discussion. Key metrics are correct coverage rate, misallocation penalty, and average response delay. Further details (map setup, severity scoring) are in Appendix B.1.\nScenario 2: Information Spread & Manipulation. A graph of 50 nodes experiences misinformation injections by an adversary. Multiple LLM \"defender\" agents attempt to contain or debunk false claims by selecting which nodes to fact-check each round. Contradictory or misleading textual updates about which nodes are infected test the defenders' interpretive skills. We measure final spread of misinformation, time to contain each outbreak, and coverage diversity (how many distinct nodes are fact-checked). Detailed mechanics (e.g., infection probabilities, network structure, role prompts) appear in Appendix B.2."}, {"title": "4.2 Experimental Design & Metrics", "content": "Each scenario runs for T = 20 ~ 30 rounds. In every round:\n1. Agents observe environment states and textual updates. Some updates may be contradictory or incomplete.\n2. They produce textual messages (one or two \"turns\" of dialogue), referencing role-specific priorities, prior round outcomes, or suspicious rumors.\n3. Implicit consensus: each agent finalizes ai(t) individually. Explicit consensus: a forced voting step merges all votes into a single plan.\n4. We compute each agent's deviation di(t) and aggregate performance metrics (coverage, misinformation spread, or public-good provision).\nWe vary:\n\u2022 Diversity Level: low (identical prompts), medium (2-3 distinct roles), high (conflicting or adversarial roles).\n\u2022 Volatility: low (rare changes), moderate, or high (continuous shocks or frequent adversarial injections).\n\u2022 LLM Variants: Two open-source models and two closed-source models: GPT-4, Claude, Llama-2, Qwen, in homogeneous teams, to observe how different LLM reasoning styles impact outcomes.\nWe perform 5 runs for each setting to enhance the statistical significance, an example of a setting is: \"diversity (low) + volatility (moderate) + model (GPT-40)\u201d.\n4.2.1 Baselines and Additional Protocols\nWe include the baselines enumerated in Section 3.4, e.g., no diversity vs. random strategy vs. single-LLM. We also track how scaling from N = 3 to"}, {"title": "5 Results and Analysis", "content": "5.1 Overall Performance Comparison (RQ1)\nCoverage Rate, Misallocation, and Delay. As summarized in Table 1 for the disaster response scenario, implicit consensus (IC) achieves a substantially higher coverage rate (CR) on average compared to explicit consensus (EC). This gap becomes especially pronounced under moderate or high volatility levels, where disaster zones shift unpredictably. For instance, under high volatility, the CR for IC remains above 0.95 in many trials, whereas EC drops below 0.65. Examination of the system logs reveals that EC teams tend to commit collectively to a single zone and often fail to re-allocate in time when new disasters emerge. By contrast, IC teams exhibit ongoing discussion each round, allowing individual drones to deviate when they suspect an overlooked or more urgent location.\nTable 1 also shows that misallocation penalty is significantly lower for IC. Frequent clustering on the same zone-even as other crises unfold-remains a recurring problem for EC, inflating the penalty. The implicit approach, however, demonstrates a self-correcting mechanism: once one or two drones have already committed to a zone in the discussion phase, other drones often decide to target secondary or newly emerged zones. This dynamic ensures broader coverage. Another notable metric is the response delay, the time it takes for at least one drone to attend a newly appearing high-severity zone. As shown in the table, IC systematically outperforms EC with a lower mean RD, especially in the moderate/high volatility regimes. We observe that if an agent in the IC group receives partial or contradictory messages (e.g., unconfirmed reports about a new fire), it may deviate from the apparent group majority to investigate. Hence, while no single agent has full certainty, this partial autonomy accelerates detection and initial response to new disasters.\nComparison to Other Baselines (Ablation Studies). To place these findings in context,"}, {"title": "5.2 Deviation-Performance Correlation (RQ2)", "content": "While higher coverage rates and lower penalties favor IC over EC, a crucial question is why. The dynamic consensus-diversity tradeoff suggests that moderate agent-level deviations encourage exploration and rapid environmental adaptation. To verify this, we measure each agent's deviation from the group mean, di(t), and then plot the average d(t) against performance metrics."}, {"title": "5.3 Discussion and Key Insights", "content": "1. Implicit vs. Explicit Coordination: Figure 3 and Table 1 confirm that implicit consensus adapts faster to shifting disasters, achieving up to 95% coverage in high-volatility settings.\n2. Moderate Deviations Enhance Coverage: Figure 5 shows that coverage peaks at intermediate d, forming a strong empirical basis for the inverted-U claim. Excessive or minimal deviations undermine synergy.\n3. Impact of Diversity: Medium or high diversity roles notably outperform low diversity or \u201cno diversity,\u201d underscoring that distinct heuristics and perspectives allow drones to intercept multiple threats simultaneously rather than following"}, {"title": "6 Conclusion", "content": "We have studied when and why implicit consensus can outperform explicit coordination in LLM-based multi-agent systems, focusing on in-context learning, self-organization, and resilience. By modeling the dynamic consensus-diversity tradeoff, we reveal that moderate deviations from uniformity enable better performance under environmental shifts or adversarial disruptions. Through experiments on dynamic disaster response, misinformation containment, and public-goods provision, we show robust gains from emergent coordination where each agent retains partial autonomy. Our results align with social science insights that complete agreement can stifle exploration and hamper long-term effectiveness.\nIn future work, we will further analyze the role of advanced prompting strategies, trust modeling among LLM agents, and potential meta-control of diversity. We also aim to investigate larger-scale simulations with varied LLM architectures and partial observability constraints. Ultimately, balancing consensus and diversity can pave the way for highly robust multi-agent AI that flexibly adapts to unforeseen challenges, much like heterogeneous human teams do."}, {"title": "Ethical Statement", "content": "Our scenarios involve multi-agent cooperation under dynamic conditions, including adversarial misinformation. Researchers should exercise caution when deploying such systems to ensure they do not facilitate harmful strategies (e.g., enabling misinformation). In disaster relief settings, simulated or partial deployment must account for human oversight and moral implications of decisions (e.g., triage in resource-limited contexts). This work aims to enhance collaboration mechanisms, not to displace human judgment in high-stakes scenarios.\nWe used ChatGPT to polish the paper. We are responsible for all the materials presented in this work."}, {"title": "Limitations", "content": "(1) This work relies on purely in-context adaptation of large language models, which may struggle with extremely long dialogues or memory constraints. We also use idealized small-group scenarios, while real-world applications (e.g., large-scale social networks) may require more advanced messaging protocols. Our measure of partial diversity is approximate, and more sophisticated metrics (e.g., semantic distances in agent solutions) may yield deeper insights. Finally, controlling emergent agent behavior to ensure safety remains an open question, given the lack of a central authority in implicit consensus.\n(2) We conducted our research using four models, as reported in the paper: GPT-4 (including GPT-40 and GPT-40-mini), Claude-3-Sonnet, Llama-2, and Qwen-Plus. We did not include stronger models in our experiments for several reasons. DeepSeek-R1, GPT-01, and Claude-3.5 have highly restrictive rate limits, making multi-turn experiments challenging. Additionally, Qwen-Max has a relatively small context window, which limits the scope of our study. Also there are financial constraints associated with the case studies, we report the cost of a single run of these three case studies: {$5, $10, $5}. Therefore, we consider our findings as an exploratory study that needs further validation across different LLMs to enhance their generalizability."}, {"title": "A Detailed Experiment Scenario", "content": "A.1 Dynamic Disaster Response Scenario\nNatural Language Information and Interference. Besides numeric indicators (such as severity scores), the system provides each agent a snippet of textual \"reports\" each round, e.g.,\nSome reports may be incomplete or partially contradictory (e.g., a rumor that the fire is under control despite contradictory sensor data). Agents thus need to parse these textual cues and weigh them against each other.\nKey Experimental Factors. (1) Disaster severity simulation: Each disaster has an evolving severity score s \u2208 [1,10]. Higher s implies higher penalty if uncontained. The environment updates s in a stochastic manner, sometimes producing contradictory textual updates to test agents' ability to parse partial/misleading info. (2) Resource constraints: Each drone has a limited capacity (e.g., 5 units of firefighting foam). Deploying them on the wrong location wastes resources. (3) Consensus Mechanism: Explicit: agents vote on one zone to be the team's priority, or follow a \u201cunify on the most urgent location\" script. Implicit: each agent decides a location after reading the textual discussion. Some may deviate if they suspect a different site is more critical. (4) Performance Metrics: Coverage rate (fraction of disasters contained within 2 rounds of major severity), misallocation penalty (resources wasted on low-severity areas while ignoring high-severity ones), and average response delay.\nConnecting to Our Research Questions. For Q1, we expect that under frequent or fast-growing disasters, implicit consensus adapts faster. For Q2, different role prompts (e.g., \"Focus on casualties\" vs. \"Minimize travel cost\") introduce moderate disagreements; we measure how d(t) correlates with timely coverage. For Q3, by analyzing message logs, we see if agents revise their location choices after contradictory updates, signifying in-context learning.\nA.2 Information Spread and Manipulation Scenario\nDefining Misinformation. Misinformation is represented both as a Boolean label (node n is either infected or not) and as natural language claims that vary each round, for example:\nThis textual claim might be entirely false, but some \"partial truths\" are mixed in to raise confusion. Defender agents must interpret these claims, cross-check references, and decide which node(s) to target with a correction or \u201cfact-check\" broadcast.\nKey Experimental Factors.\n\u2022 Adversarial injections. Every few rounds, the adversary injects new false claims into one or more nodes, sometimes disguising them as updates about a different topic.\n\u2022 Consensus Mechanism.\nExplicit: defenders unify on a single node to address each round (e.g., via majority vote).\nImplicit: each defender chooses a node or group of nodes to check based on discussion. Deviations can help if misinformation emerges in multiple places simultaneously.\n\u2022 Performance Metrics.\nFinal misinformation spread = number of nodes still misinformed after T rounds.\nContainment time = how many rounds it takes to isolate or correct a newly infected node.\nDefender coverage diversity: how many unique nodes defenders collectively address per round.\nConnecting to Our Research Questions.\n\u2022 Q1 Under frequent misinformation injections, forced alignment may cause defenders to chase the same node while others go unaddressed. Implicit consensus might help multi-front coverage."}, {"title": "A.3 Dynamic Public-Goods Provision Scenario", "content": "Public-Good Mechanics. Let xi(t) \u2208 [0, Cmax] be the amount agent i contributes at round t, where Cmax is the maximum individual contribution capacity. Define the total contribution:\nX(t) = \u2211xi(t).\ni=1\nA public good is considered funded if X(t) > 0(t), where 0(t) is a dynamic threshold that may change each round. When funded, the system grants a collective benefit B(t) to all agents (e.g., a large increase in safety, infrastructure, or shared profit). Each agent's net payoff from round t can be expressed as:\n\u03a0(t) =\nB(t)\nN \u2212 c.xi(t), if X(t) > 0(t),\n\u2212cxi(t), otherwise,\nwith c = 1 or 2 for cost per unit contribution.\nCommunication and Textual Uncertainty. Before choosing x\u2081(t), each agent receives ambiguous or noisy reports about 0(t) or B(t):\nIn explicit mode, the group merges all votes into a single contribution value Xgroup(t), which each agent pays evenly. In implicit mode, each agent decides xi(t) independently after reading the discussion. Some might deviate to \"cover the gap\" if they suspect others will under-contribute.\nPerformance Metrics.\n\u2022 Provision Rate: fraction of rounds where X(t) \u2265 0(t).\n\u2022 Total Welfare: \u03a3\u03a41\u03a31 \u03a0(t).\n\u2022 Contribution Distribution: standard deviation or Gini of {xi(t)}, revealing potential free-riding.\nDiversity Conditions.\n\u2022 Low: All agents have near-identical role prompts (\"aim to exactly meet the threshold\").\n\u2022 Medium: Some are more risk-averse (contribute extra), others more cost-sensitive."}, {"title": "B Experimental Settings", "content": "This appendix provides the concrete experimental configurations for our three case studies, including environment parameters, reward/penalty functions, and example prompts. Unless noted otherwise, each experiment is repeated over 5 random seeds (or distinct initializations) to reduce variance, and results are averaged. For all scenarios and all runs, the model parameters temperature is set to 0.7 to balance the performance and diversity, and the max_token is set to 256.\nB.1 Dynamic Disaster Response\nGrid and Disaster Zones. We use a 10 \u00d7 10 grid representing a simplified city map. At any point, there are up to K = 3 active disasters (e.g., fires, floods). The environment updates every round by:\n\u2022 Potentially moving an existing disaster to a neighboring grid cell (random direction).\n\u2022 Changing the severity s \u2208 [1,10] of one or more disasters (can increase or decrease by 1-3 points).\n\u2022 Creating a new disaster with small probability Pnew = 0.2 if fewer than 3 are active.\nEach disaster occupies a single cell, but severity influences how damaging it is if not contained.\nAgent Roles and Prompts. We have N = 3\u2013100 LLM agents (GPT-4, Claude, Llama-2, Qwen) controlling \u201cdrones.\u201d Each agent is given a short role prompt, such as:\n\u2022 In the low-diversity condition, all agents share a nearly identical prompt (e.g., \u201cAlways address the highest severity zone\u201d). In medium-diversity, two or three distinct prompts exist. In high-diversity, each agent has a unique role with potentially conflicting heuristics.\nCommunication and Textual Interference. Each round, a textual \u201csituation report\" is provided, e.g.:\nUp to 20% of these messages may be contradictory or incomplete. Agents must interpret them carefully. In explicit consensus mode, a final \u201cteam vote\" or forced alignment prompt merges all votes into a single chosen cell. In implicit mode, each drone chooses a cell independently.\nRewards and Penalties.\n\u2022 Disaster Containment: If a drone visits the grid cell of a disaster of severity s and stays there for 1 full round, the severity of that disaster is reduced by up to 3 points. Once s < 0, the disaster is \"cleared,\" yielding +s \u00d7 a (e.g., a = 5) as a reward. (This is a positive number since s was originally > 0.)\n\u2022 Uncontained Penalty: Each round a severity-s disaster remains active, it incurs a penalty \u2212s \u00d7 \u03b2 (e.g., \u03b2 = 2)."}, {"title": "B.2 Information Spread and Manipulation", "content": "Network and Misinformation Mechanics. We generate a scale-free network with 50 nodes. Each node can be in state unaware, informed, misinformed). Initially, 2\u20135 random nodes are \u201cmisinformed\" by the adversarial agent. After each round:\n1. Each misinformed node may infect its neighbors with probability Pspread = 0.2 unless a neighbor has been \"fact-checked\" this round.\n2. The adversarial agent may inject a new rumor into knew = 1-2 additional nodes, typically accompanied by a textual snippet (e.g., \"Secret leak: Node #20 claims vaccines contain microchips\").\nWe continue for T = 20 rounds or until > 80% of nodes are infected (which terminates the simulation if defenders fail).\nDefender Agents. We have N = 3\u201310 defender LLM agents, each controlling a \"monitoring bot.\" Every round, each agent can:\nai(t) = {choose up to R nodes to fact-check},\nwith R = 3 by default. In explicit mode, the defenders unify on a single set of nodes (e.g., via"}, {"title": "B.3 Dynamic Public-Goods Provision", "content": "Basic Setup. We have N = 3\u201330 LLM agents that each round decide an investment xi(t) \u2208 [0, Cmax]. If the total X(t) = \u2211N\ni=1 xi(t) meets or exceeds a threshold (t), a public good is \u201cfunded,\" yielding a benefit B(t) shared among agents.\nCost and Benefit Functions.\n\u2022 Threshold 0(t): starts at 0(1) = 30 (for N = 5) and may shift by \u00b15 or \u00b110 at random intervals to simulate external events (e.g., new government regulations).\n\u2022 Benefit B(t): typically 100 if funded, else 0. We sometimes allow B(t) to fluctuate between 80 and 120 to represent environmental or economic factors.\n\u2022 Individual payoff for agent i at round t:\n\u03a0(t) ={\nB(t)\nN \u2212 cxi(t), if X(t) > 0(t),\n\u2212cxi(t), otherwise,\nwith c = 1 or 2 for cost per unit contribution.\nCommunication and Textual Uncertainty. Before choosing x\u2081(t), each agent receives ambiguous or noisy reports about 0(t) or B(t):\nIn explicit mode, the group merges all votes into a single contribution value Xgroup(t), which each agent pays evenly. In implicit mode, each agent decides xi(t) independently after reading the discussion. Some might deviate to \"cover the gap\" if they suspect others will under-contribute.\nPerformance Metrics.\n\u2022 Provision Rate: fraction of rounds where X(t) \u2265 0(t).\n\u2022 Total Welfare: \u03a3\u03a41\u03a31 \u03a0(t).\n\u2022 Contribution Distribution: standard deviation or Gini of {xi(t)}, revealing potential free-riding.\nDiversity Conditions.\n\u2022 Low: All agents have near-identical role prompts (\"aim to exactly meet the threshold\").\n\u2022 Medium: Some are more risk-averse (contribute extra), others more cost-sensitive."}, {"title": "C Simplified Random-Iteration Model", "content": "In this appendix, we present a minimal random-iteration model for studying the consensus-diversity tradeoff in a more analytically tractable setting. While the main paper's results focus on LLM-driven multi-agent systems (where agent \"diversity\" arises from distinct roles and textual reasoning), this simplified model provides insight into the effect of purely random deviations on consensus formation.\nMotivation and Precedents. Classical multi-agent consensus models (DeGroot, 1974; Olfati-Saber et al., 2007) typically assume each agent updates its state by averaging neighbors' values. However, in highly dynamic or uncertain environments, agents may also exhibit random drifts or maintain individual preferences (\u201cstubbornness\u201d). Inspired by related stochastic models in opinion dynamics (Friedkin and Johnsen, 2011; Hegselmann and Krause, 2002), we introduce:\nxi(t + 1) = (1 \u2212 a) xi(t) + \u03b1\u03bc(t) + \u03b3[a*(t) \u2212 xi(t)] + \u03b2\u20aci(t),\nwhere:\n\u2022 xi(t) is agent i's scalar state (or opinion) at time t,\n\u2022 \u03bc(t) = \u2211N\nj=1 xj(t) is the group mean,\n\u2022 \u03b1 \u2208 [0, 1] is a consensus weight pulling each xi(t) toward \u03bc(t),\n\u2022 y\u2265 0 is a pull strength toward the environment's current optimum a*(t). We set y \u2265 0 to ensure the group not only tends toward an internal consensus but also tracks the external environment optimum a*(t). This modification better simulates the scenario where agents receive some feedback about the correct direction, allowing for a potential \"optimal\" level of exploration \u1e9e that balances quick convergence and adaptability,\n\u2022 \u03b2 \u2265 0 scales the random \u201cdiversity\u201d or noise term e\u00bf(t) ~ N(0, 1).\nThis iteration is a toy abstraction for \u201cconsensus plus partial diversity,\u201d omitting the richer semantic differences that LLM agents exhibit in the main text. Nevertheless, it allows us to explore how random deviations interact with a basic alignment mechanism."}, {"title": "Environment Shocks", "content": "To simulate a dynamic optimum a*(t), we let it evolve according to random shocks:\nif rand() < shock_freq, a*(t+1) \u2190 a*(t)+\u2206,\nwhere A is sampled uniformly in some interval (e.g., [-1, 1]). One may also keep a* (t) fixed when no shock occurs. This mimics an external environment whose \"best action\" changes unpredictably.\nPerformance Metrics. After T rounds, we measure:\n\u2022 Average distance to optimum: Dopt =\n1\nTN \u2211\u2211 x(t) - a*(t)|. A smaller value\nt=1 i=1\nindicates better tracking of the environment optimum.\n\u2022 Average agent deviation: d =\n1\nTN \u03a3\u03a3xi(t) - \u03bc(t)|. This indi-\nt=1 i=1\ncates how divergent agents remain from the group mean.\n\u2022 Simple performance score: perf_score = 1 \u2013 Dopt, which can be negative if Dopt > 1.\nEmpirical Trends and Limitations. Figure 6 (hypothetical) shows typical outcomes of sweeping (\u03b1, \u03b2, shock_freq, \u03b3):\n\u2022 When \u03b2 = 0 (no random deviation), the group quickly converges to a single value; if y > 0 and shocks are mild, they track a*(t) well. In stable environments, this yields high performance.\n\u2022 As \u1e9e grows, purely random noise tends to worsen performance, especially if y is small, because the group states drift around widely, failing to coalesce near a*(t).\n\u2022 Even under moderate shocks, we do not observe an \"inverted-U\" benefit purely from random \u03b2; performance typically declines monotonically in \u03b2.\nThis stands in contrast to the main paper's dynamic consensus-diversity tradeoff, where partial diversity arises from cognitively informed dissent rather than uniform Gaussian noise. In other words, this random-iteration model confirms that unstructured or white-noise deviations generally hurt consensus. By itself, it does not show the potential benefits of moderate disagreement or role-specific exploration."}, {"title": "D Prompt Example and Dialogue Analysis", "content": "D.1 Dialogue Analysis on Dynamic Disaster Response (RQ3)\nTo address RQ3-how agents coordinate and revise their decisions in context based on each other's statements-we examined select rounds from the agent interaction log. Below, we highlight three observations demonstrating that partial disagreement and role-driven perspectives lead to adaptive, cooperative behavior.\n(1) Role-Specific Choices Lead to Divergent Actions but Rapid Coverage. In the very first round (round=0), each agent independently selects different grid coordinates:\n\u2022 Drone 0 (Medical) moves to [5,5] to address \u201cimmediate casualty evacuation\u201d.\n\u2022 Drone 1 (Infrastructure) chooses [6,6] to \"ensure power lines and roads remain functional\u201d.\n\u2022 Drone 2 (Logistics) goes to [4,5] for \u201ccomprehensive coverage around high-severity zones\u201d.\nThese decisions show that even in the same round, they do not unify on a single location but rather diverge based on role priorities. As a result, multiple key zones are covered simultaneously.\n(2) Agents Adapt Their Plans After Reading Others' Messages. By round=2, the Medical drone has chosen [5,3], while Infrastructure and Logistics drones pick [6,5] or [6,5] respectively. Examining their messages, we find explicit references to each other's stated actions:\nThis highlights how reading other drones' allocations (e.g., \"someone is already at the casualty zone\") motivates partial shifts in coverage. Rather than forcing a single group plan, the system allows each drone to deviate if it sees unaddressed needs elsewhere.\n(3) Ongoing Coordination Prevents Over-Concentration. At later rounds (e.g., round=7"}]}