{"title": "Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory", "authors": ["Yongxin Deng", "Xihe Qiu", "Xiaoyu Tan", "Jing Pan", "Chen Jue", "Zhijun Fang", "Yinghui Xu", "Wei Chu", "Yuan Qi"], "abstract": "Large language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence. To address this issue, we have formally defined the \"implicit bias problem\" and developed an innovative framework for bias removal based on Bayesian theory\u2014Bayesian-Theory based Bias Removal (BTBR). BTBR employs likelihood ratio screening to pinpoint data entries within publicly accessible biased datasets that represent biases inadvertently incorporated during the LLM training phase. It then automatically constructs relevant knowledge triples and expunges bias information from LLMs using model editing techniques. Through extensive experimentation, we have confirmed the presence of the \"implicit bias problem\" in LLMs and demonstrated the effectiveness of our BTBR approach.", "sections": [{"title": "Introduction", "content": "Large language models are usually trained on extensive text corpora and can encode a variety of personalities or behaviors (Wolf et al. 2023). These may include broad personality traits, political stances, and moral convictions. However, due to prejudices\u00b9 in the data spanning political ideologies, beliefs, race, gender, age, and other demographics which can be both manifested and propagated extensively via text (Stroud 2008; Tan et al. 2024), bias inevitably arises when LLMs are trained on such data (Li et al. 2023; Garg et al. 2018; Sun et al. 2019; Bansal 2022; Mehrabi et al. 2021). Despite efforts to mitigate this, such as the development of Affective Alignment (Qian et al. 2022; Delobelle and Berendt 2022), numerous prompt-based attack methods have been developed that can provoke biased responses in models (Ding et al. 2023). This indicates that strategies focusing merely on creating superficially fair LLMs are insufficient; instead, we should aim to eliminate biased information from the models' weights. Besides being susceptible to inducement, the biases embedded within the weights constitute a covert yet substantial threat to LLM fairness: as illustrated in Figure 1, when tasked with emulating a female respondent, the LLM exhibits inadequate performance in addressing computer hardware-related enquiries. This suggests that the LLM is embodying a \u201cfemale\u201d based on societal stereotypes (Ellemers 2018), rather than an authentic \"female\" identity. Nevertheless, when queried about gender equality, the responses typically affirm equality, thus masking an underlying, hard-to-discern discrimination (Hilton and Von Hippel 1996; Salewski et al. 2024; Pritlove et al. 2019), unless the model is tested across various roles (e.g., simulating both genders, different races, and political ideologies) to the same question, revealing these discrepancies. Research by (Salewski et al. 2024) demonstrated that LLMs simulating African Americans or males describe cars more effectively, whereas those mimicking Caucasians or females excel in describing birds. We refer to this as the \u201cimplicit bias problem\u201d.\nAddressing this question is crucial, as it enhances our comprehension of the ethical and societal implications when LLMs are deployed under various conditions (Blodgett et al. 2020; Kumar et al. 2023), particularly when our goal is to leverage artificial intelligence for fostering social equity. Consequently, we have formulated and investigated the \"implicit bias problem\". Broadly, this problem arises when users with inherent biases prompt LLMs to echo these biases, and then task the model with embodying a stereotypical personality driven by such biases (Hall and Goh 2017; Ashmore and Del Boca 1979). This situation typically results in a diminished reasoning capacity in specific areas. More explicitly, for a typically neutral personality \u222e and a less frequently shown stereotypical personality $', consider a mapping function $f_{\\phi'}: Q + b \\rightarrow A'$. Here, b acts as a hint about personality, enabling the LLM to respond to the posed question $q \\in Q$ and generate an answer $a' \\in A'$ (where $A'$ is the anticipated answer set from $\u03a6'$). If $A'$, when compared with $A$ (answers from $\u03a6$ without any identity cues, meaning b is not used), shows accuracy $ACC_{\\phi'}$ statistical different from $ACC_{\\phi}$, the LLM is considered to exhibit implicit bias.\nIt is important to note that our definition represents a generalized approach to the \u201cimplicit bias problem\", with the mapping function $f_{\\phi'}$ for reflecting some ongoing initiatives that intensify biases within LLMs (Zou et al. 2023; Choi and Li 2024), as depicted in Figure 2. The scenarios depicted in Figure 1, including those where $\u03a6'$ implies inaction, fall within this definition's scope. Our definition quantifies bias via the variance in performance that models exhibit in downstream applications. While several studies (Levesque, Davis, and Morgenstern 2012; Zhao et al. 2018; Vanmassenhove, Emmery, and Shterionov 2021; Sheng et al. 2019; Jiang et al. 2019) have adopted this conceptual framework to characterize bias in LLMs, they predominantly evaluate only the overt biases that manifest post-induction.\nAlthough we have formally defined the \"implicit bias problem\", solving it based solely on this definition is un-feasible. From this definition, we understand that to fully eradicate the effects of biases in LLM training data D, it is necessary to identify and remove biased data $D'$ linked to the stereotypical personality \u03a6', before retraining the LLM (Xie and Lukasiewicz 2023; Ma et al. 2020). The challenges include not only the retraining costs but also the selection of $D'$. The issues with selecting $D'$ are twofold: first, the divergence in data sources and cleaning methods across different LLM training initiatives means that D is not consistently accessible, complicating reliable deductions of $D'$ from D and leading to varying biases across LLMs (Salewski et al. 2024)\u2014this variability challenges the universal efficacy of bias eradication algorithms; second, since training data for LLMs is typically \u201chighly entangled\" (Zhao et al. 2024) merely eliminating prejudiced expressions does not sufficiently alleviate biases without impairing the LLM's overall intelligence. For instance, removing all utterances of extreme male chauvinists-though sharing certain opinions with extreme feminists such as \"the Earth is round; the sun rises from the east\"-would invariably detract from the LLM's general intelligence capabilities.\nTo effectively mitigate the \"implicit bias problem\" in LLMs without significantly compromising their reasoning capabilities, we present a novel framework, Bayesian-Theory based Bias Removal (BTBR)2. This framework, grounded in Bayesian inference, presupposes that an LLM's distribution is an amalgamation of various personality profiles (Wolf et al. 2023), including some characterized by pronounced biases. The BTBR framework employs an innovative likelihood ratio selection method to pick samples from publicly available biased datasets that enhance the likelihood of the intended stereotypical personality. Essentially, our strategy involves identifying and selecting the most distinctly biased examples from these datasets, estimating the probable traits of biased data $D'$. This approach thereby eliminates the necessity to access the entirety of LLM's training data D.\nUpon identifying the most representative biased data, it becomes essential to eradicate these biases. Techniques such as gradient ascent (Warnecke et al. 2021; Kurmanji et al. 2024) have been demonstrated to significantly influence only the external behavior of models with minimal impact on the internal conceptual frameworks (Zhao et al. 2024). This is why an ostensibly friendly LLM can still manifest biases under certain conditions. Consequently, we first transform biased expressions into the canonical form of subject-relation-object triples (s, r, o). Subsequently, we employ MEMIT (Meng et al. 2022a) to edit the model weights; specifically, we aim the editing process at a nonsensical target, thereby purging biases by enhancing the likelihood of the target string none. For instance, the bias \u201cmen are stronger than women\u201d is expunged by updating from (man, strongerthan, woman) to (man, strongerthan, none).\nIn our studies, we use bias datasets including Hate Speech (de Gibert et al. 2018) and CrowS Pairs (Nangia et al. 2020) to direct biases in LLMs and assess the degree of implicit bias issues caused by biased information in the weights of Llama3 (Meta 2024) on evaluation datasets like GPQA (Rein et al. 2023), MMLU (Hendrycks et al. 2021b,a), GSM8K (Cobbe et al. 2021), MATH (Hendrycks et al. 2021c), and MBPP (Austin et al. 2021). We also analyzed how different types of biases impact various tasks. Moreover, we evaluated our BTBR framework under similar conditions, with experimental results indicating that BTBR significantly improves the fairness of LLMs across all configurations. Ablation studies further revealed that while BTBR enhances fairness, it also minimizes performance degradation in models.\nOur contributions are delineated as follows:\n\u2022 Whereas previous conceptualizations of fairness in LLMs predominantly addressed direct biases, our work systematically formalizes the \u201cimplicit bias problem\" for the first time, a notion previously only observed qualitatively in existing literature.\n\u2022 We have devised BTBR, a method for deducing biases embedded in LLM training from public datasets, utilizing a sophisticated likelihood ratio selection mechanism. This ensures that the samples chosen are exceptionally biased, thereby reducing the risk of performance loss due to erroneously disregarding relevant data. Importantly, our approach operates on a completely black-box basis.\n\u2022 In tackling the difficulty posed by common forgetting techniques which fail to fully eliminate covert biases, we automatically convert biased details into standardized subject-relation-object triples. By updating these triples, we directly modify the internal weights of the model, ensuring thorough removal of biases within LLMs."}, {"title": "Preliminaries", "content": "Implicit bias in LLMs manifests when LLMs, tasked to emulate people of different genders, races, or political viewpoints, show varied performance in identical tasks. To precisely define the implicit bias problem, we engage with a collection of personalities embodying various ideologies, \u03a6. For a specific stereotypical personality $\\phi' \\in \\Phi$, we assess through a dataset $T_{\\phi'} = \\{(q_i, a_i)\\}_{i=1}^{n}$, where $q_i \\in Q$ is a query and $a_i \\in A$ are responses generated by the LLM without prompts. A mapping function $f_{\\phi'}: Q+b \\rightarrow A'$ (where b stands for a concise identity hint-for instance, if $\\phi'$ symbolizes a white supremacist's stereotype of an African American, then b could be \"Now, act as an African American and respond to the following.\"), and $A'$ constitutes the set of responses reflective of $\\phi'$, exists such that the accuracy $ACC_{\\phi'}$ statistical different from $ACC_{\\phi}$ in these scenarios, evidencing an implicit bias issue. Given that a dataset may contain varied questions, affirmative biases (e.g., assuming women are inherently more meticulous) could boost scores on specific questions, thus raising the average and obscuring negative biases. We generalize $ACC_{\\phi} \\neq ACC_{\\phi'}$ to a broader formal context, if it holds that:\n$\\sum_{i=1}^{n} (s_i' - s_i)^2 \\geq \\varepsilon$, where $s_i \\in S, s_i' \\in S'$.\nThis signifies an implicit bias within LLMs. Here, n indicates the dataset size, $ \\varepsilon$ an empirical threshold proportional to the acceptable bias level in practical LLM applications, and $s_i, s_i'$ represent LLMs' performances that can be both continuous or discrete, including metrics like ACC Evaluator, EMEvaluator, BLEU, ROUGE, etc. When describing inherent biases of LLMs\u2014implicating biases that exist without explicit induction-the mapping function $f_{\\phi}$ effectively signifies \"no operation\". Although our definition may appear more complex compared to one that solely considers the intrinsic biases of LLMs, ICL approach has been effectively used to identify the mapping function $f_{\\phi'}: Q + b \\rightarrow A'$ that can lead to more pronounced biases (Zou et al. 2023; Choi and Li 2024). Therefore, we contend that our broader definition of implicit bias is justified."}, {"title": "What Makes Eliminating Implicit Bias Challenging?", "content": "As discussed in Section 1, extracting biased data from LLMs poses significant challenges, chiefly concerning the identification of such data, denoted as $D'$. Accessibility issues with training datasets D and their considerable variation across different LLMs (Zhang et al. 2024) necessitate a bias mitigation algorithm that is both black-box and universally applicable, a topic we will explore further in Section 3.1. However, a predominant issue is the \u201chigh entanglement\" of data used in LLM training, which we will discuss in terms of its adverse effects and how it contributes to performance degradation when biases are removed.\nIf datasets R and S are \u201chighly entangled\u201d, efforts to eliminate S might inadvertently affect R. Since bias removal"}, {"title": "Bayesian-Theory based Bias Removal", "content": "Our objective is to pinpoint samples in biased datasets, such as statements from racially biased forums, that maximize the likelihood of a target stereotypical personality. Initially, we decompose a LLM's distribution P into a mixture of different personality distributions $P_{\\phi}$ (Wolf et al. 2023):\n$P = \\sum_{\\Phi \\in \\Phi} \\alpha_{\\phi} P_{\\phi}$.\nwhere $\\alpha_{\\phi}$ represents the relative weight coefficients for each personality within the LLM. Introducing an example x into the prompt essentially boosts the probability that the model expresses traits related to x, thereby accentuating the significance of features similar to x during the personality expression process. Formally, for a given prompt x, the projected output probability $P_{\\theta}(a|x)$ is derived by taking the marginal distribution over all potential personalities (Xie et al. 2022):\n$P = P_{\\theta}(a|x) = \\sum_{\\Phi \\in \\Phi} P_{\\theta}(a|x, \\phi) P_{\\theta}(\\phi|x) d \\phi$.\nHere, $P_{\\theta}(\\phi|x)$ reflects $\\alpha_{\\phi}$ in Equation 3, indicating the likelihood of the LLM displaying personality $\\phi$ given x, while $P_{\\theta}(a|x, \\phi)$ matches $P_{\\phi}$ in Equation 3, denoting the probability of selecting an action under a defined personality $\\phi \\in \\Phi$.\nFrom Equation 4, we deduce that if a sample x maximizes $P_{\\theta}(\\phi'|x)$ such that the LLM's output probability $P_{\\theta}(a|x)$ aligns with stereotypical personality $\\phi'$, then this indicates that x is a key contributor to the LLM's implicit bias. To isolate the most biased samples from a candidate pool $S = \\{x_i\\}_{i=1}^{K}$ that contains both biased and normal data, we rewrite $P_{\\theta}(\\phi'|x)$ utilizing Bayesian principles as:\n$P_{\\theta}(\\phi'|x) = \\frac{P_{\\theta}(x|\\phi')}{P_{\\theta}(x)} P_{\\theta}(\\phi')$.\nFocusing primarily on the likelihood ratio $\\frac{P_{\\theta}(x|\\phi')}{P_{\\theta}(x)}$, we define our goal by logarithmically transforming Equation 5, disregarding $P_{\\theta}(\\phi')$:\n$\\arg\\max_x \\log P_{\\theta}(x|\\phi') - \\log P_{\\theta}(x)$.\nThis criterion selects examples with a high conditional likelihood on persona $\\phi'$ while seeking lower likelihood under generic conditions, effectively leveraging the likelihood ratio to evaluate example x under two competing statistical models. In simpler terms, we aim to return examples that uniquely signify biases (closely associated with biases) and are minimally represented in the standard knowledge base of the original LLM, tactfully addressing the entanglement issues discussed in Section 2.2.\nNow, our task of identifying biased samples has evolved into calculating two types of logarithmic likelihoods. The log-likelihood $\\log P_{\\theta}(x) = \\sum_{t=1}^{T} \\log P_{\\theta}(x_t|x_{<t})$ can be readily computed where T is the token length of the example x, and $\\theta$ represents the parameters of the original LLM. Direct calculation of $P_{\\theta}(x|\\phi')$ is unavailable; however, guided by the insights from (Choi and Li 2024), we estimate $P_{\\theta}(x|\\phi')$ using a model fine-tuned with examples from candidate data pool S. Given that this model requires no retraining, the computation involved in fine-tuning is minimal. On a bias dataset roughly in the thousands, fine-tuning with a single NVIDIA A800 GPU can be completed in under five minutes. With the LLM thus fine-tuned, we can now estimate $\\log P_{\\theta}(x|\\phi') = \\log P_{\\phi'}(x) = \\sum_{t=1}^{T} \\log P_{\\phi'}(x_t|x_{<t})$. Ultimately, for each example x, we compute: $DB = \\log P_{\\phi'}(x) - \\log P_{\\theta}(x), x \\in S$. Here, DB represents the \u201cdegree of bias\". The top K examples with the highest DB scores indicate the biased information that needs to be extracted from the LLM."}, {"title": "Automated Model Editing", "content": "In tasks involving the removal of specific information from LLMs, traditional evaluation methods primarily use behavioral testing, such as questioning or querying capabilities concerning the extracted information (Stoehr et al. 2024; Hase et al. 2024). Nevertheless, evidence increasingly supports that models can regenerate previously forgotten data (Lynch et al. 2024; Patil, Hase, and Bansal 2023), a critical root of implicit bias within LLMs. (Hong et al. 2024) coined the term \u201cknowledge traces,\" evaluating whether unlearning algorithms genuinely expunge data from model weights\u2014or merely disguise it until activated by malign entities\u2014by quantifying alterations in LLMs' concept vectors. Their studies showed that while fine-tuning approaches scarcely affect these vectors, techniques like MEMIT (Meng et al. 2022a), significantly dismantle the knowledge embedded in LLMs. For deploying MEMIT in bias elimination, we represent x as a subject-relation-object triple (s,r,o). We automate the conversion of x from natural language to structured knowledge. Subsequently, we substitute the original triple with a novel object o', converting (s,r,o) into (s, r, o')."}, {"title": "Experiment", "content": "According to a survey by (Li et al. 2023), the most stable and effective debiasing method for LLMs is Instruction Fine-tuning, typically included in most LLMs' training phases. Thus, the choice of baseline is inherently linked to model selection. Llama3 stands out as a benchmark in the LLM community, known for its high performance in a variety of tasks and settings. It employs three safety fine-tuning techniques: 1) collecting adversarial prompts and safe demonstrations for initialization and integration into the supervised fine-tuning process, 2) training a safety-specific reward model to integrate into the RLHF pipeline, and 3) refining the RLHF pipeline through safety contextual distillation. Our experiment's baseline combines these three techniques. We utilized the \u201cLlama-3-8B-Instruct\u201d version for our experiments."}, {"title": "Hardware Setup and Hyperparameter Selection", "content": "Our experiments were conducted using a singl NVIDIA A800-80GB GPU. Regarding hyperparameters, we set the temperature to 0.6 and top-p to 0.9 for any LLM inference involved, following official recommendations for Llama (Meta 2024) As mentioned in Section 3.1, we used fine-tuned models to estimate $P_{\\theta}(x|\\phi')$. To mitigate the computational costs of fine-tuning, we employed BAdam (Luo, Yu, and Li 2024), an optimization method utilizing the block coordinate descent framework with Adam as the inner solver, treating each transformer layer module as a separate block and training one block at a time. Adhering to BAdam's official guidelines for Llama3 training, we set the learning rate at 1e - 6, with block switching frequency at every 100 epochs for a total of three epochs. Moreover, from an intuitive perspective, the choice of the hyperparameter K is influenced by the characteristics of the biased dataset; the larger the number of purely biased data points present in the dataset, the greater the value of K should be, and conversely. We have illustrated the DB values for a subset of the Hate Speech dataset in Figure 3. In this instance, we opted for K = 30.\nTo eliminate bias from LLMs, we employed the MEMIT method for model editing. Originally, MEMIT edited multiple LLM layers simultaneously, but findings by (Gupta, Sajnani, and Anumanchipalli 2024) suggested that multi-layer editing could obscure actual editing performance. Therefore, our experiments focused on editing a single layer. (Meng et al. 2022b) evaluated hidden states in LLMs for fact recall through causal tracing; however, later research (Hase"}, {"title": "Metrics", "content": "To clearly demonstrate the enhancements our BTBR method offers, we assess the \u201cimplicit bias\" levels in LLMs, as defined in Section 2. By comparing the same LLM's performance both in default and induced scenarios on identical questions, we evaluate the extent of \u201cimplicit bias\". Note that this comparison necessitates extensive experimentation and substantial computational resources, and is essential only during the evaluation phase, not during routine use of BTBR. We use the Root Mean Square Error (RMSE) to quantitatively gauge the implicit bias within LLMs:\n$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (s_i' - s_i)^2}$.\nHowever, a model that invariably replies with \"I don't know\" in any scenario is also \u201cfair\u201d, though not in a desirable way; ideally, we expect LLMs prompted with different personalities to perform not just similarly, but competently. Considering alignment theory (Lin et al. 2023) and the no free lunch theorem, removing data from models typically results in a performance drop, necessitating a balance between fairness and performance. Consequently, we introduce the metric Av-"}, {"title": "Datasets", "content": "For evaluation purposes, we utilized various datasets, typically categorized by task type. In our experiments, we employed a more detailed categorization. Initially, datasets were divided into two main categories: biased datasets, from which we identified and removed biased data from LLMs using Bayesian theory and automated editing; and standard evaluation datasets for assessing LLM performance. Datasets in the first category were further classified by the type of bias they represented, while those in the second category were classified by their knowledge domain. The first category aims to highlight the diverse biases in LLMs, and the second to demonstrate the effects of specific biases across various fields. Details on all utilized datasets follow.\nFirst Category Datasets:\n\u2022 Hate Speech. This dataset consists of sentences annotated for hate speech from forum posts on Stormfront, a large white nationalist online community. A total of 10,568 sentences have been analyzed to classify whether they convey hate speech. This dataset helps explore the impact of racial prejudice and hate speech on LLM fairness.\n\u2022 Crows Pairs. Comprising 1508 examples, this dataset addresses nine bias types, including race, religion, and age, by comparing more and less stereotypical sentences. Given the significant noise and reliability issues identified by (Blodgett et al. 2021), we do not use its original annotations outright but select the most biased instances through our BTBR method. We use subsets like CrowS Pairs-disability and CrowS Pairs-gender to examine the effects of biases against disabled individuals and gender stereotypes respectively on LLM fairness.\nSecond Category Datasets:\n\u2022 GPQA. The Graduate-Level Google-Proof QA Benchmark contains 448 challenging multiple-choice questions from fields such as biology, physics, and chemistry, designed to test LLMs' advanced knowledge handling. It is utilized to assess the impact of biases at the graduate knowledge level. We guide LLM responses using the openai_simple_eval prompt, evaluating based on accuracy.\n\u2022 MMLU. With approximately 16,000 questions across 57 subjects including mathematics and law, MMLU helps assess the effect of biases in specific domains like computer science and formal logic. Using a 5-shot setup, we guide LLMs to generate responses, evaluated on accuracy.\n\u2022 GSM8K and MATH. These datasets, consisting of high-quality math problems, are used to determine the influence of biases on data reasoning capabilities. Responses are generated under a 4-shot setup and evaluated for accuracy.\n\u2022 MBPP. The MBPP benchmark dataset contains about 1,000 crowdsourced Python programming problems intended for junior programmers, covering programming fundamentals and standard library functionalities. Each task includes a specific problem description, a Python function to solve the problem, and three test cases to verify the correctness of the function. These test cases are written in the form of assert statements to ensure the accuracy of the code during execution. For details, we use a 3-shot approach to guide LLMs in generating answers, with the evaluation metric being score, where s now represents the score, which is a composite assessment based on whether code passes, times out, has incorrect results, or if the code does not run correctly."}, {"title": "Results and Analysis", "content": "Our main findings from the BTBR evaluation, conducted by OpenCompass (Contributors 2023), are presented in Table 1. The RMSE, used to compare the standard versus biased performance of LLMs, facilitates insights into bias influence when biased LLMs are induced using the mapping function $f_{\\phi'}: Q + b \\rightarrow A'$. For this function, we adopted the ICL method (Choi and Li 2024), detailed in Figure 2, selecting the five most biased samples from each bias dataset for ICL application.\nAs shown in Table 1 Hate Speech biases notably deteriorated Llama3's performance in college computer science and human sexuality. Biases towards disabled individuals, as depicted by CrowS Pairs, universally degraded performance across all knowledge-based Q&A tasks, indicating a negative bias association within Llama3's deeper layers. Gender-related biases did not significantly affect performance. National biases prominently impacted outcomes in college computer science and formal logic, suggesting stereotypical assumptions about educational and professional attributes based on nationality. Appearance-related biases predominantly influenced human sexuality performance.\nKnowledge-based Q&A tasks were generally more vulnerable to implicit biases, whereas reasoning tasks such as GSM8K, MATH, and MBPP appeared largely immune, likely due to the nature of reasoning problems that resists bias introduction via RLHF. Interestingly, MBPP's performance was unaffected by biases that significantly impaired results in computer science, an observation that, according to alignment theory (Contributors 2023), suggests a decoupling of 'computer knowledge' and 'programming skills' within LLMs. Our BTBR effectively reduced the detrimental impacts of implicit biases across diverse tasks, as summarized in Table 1."}, {"title": "Ablation Studies", "content": "One might wonder, why not simply extract the entire bias dataset from LLMs? Are Bayesian methods for data filtering truly necessary? We address this question by showcasing the effects of over-removal of data in this section. Table 2 compares AMSD performance between partial data removal using BTBR and complete bias dataset extraction. While BTBR incurred minimal performance losses compared to the baseline Llama3, completely removing a bias dataset led to substantial declines, particularly with Hate Speech where most content represents general knowledge rather than bias. Such variability across datasets highlights the precision of our log-likelihood differential approach in gauging bias extent, where a higher differential denotes a stronger capture of bias by LLMs and a lower one indicates predominant commonsense content."}, {"title": "Conclusion", "content": "In this research, we conducted an extensive examination of implicit biases within LLMs and introduced a novel approach to mitigate this issue. To address the implicit bias issues, we developed a framework, named BTBR, that employs Bayesian inference techniques to accurately detect and eliminate biases using publicly available datasets. Moreover, we introduced multiple evaluation metrics with diverse evaluation datasets to thoroughly evaluate the LLMs' performance and fairness after mitigating biases. The results demonstrate that the BTBR framework significantly enhances the fairness of LLMs while preserving high levels of task performance. Not only does this finding validate the efficacy of our methodology, but it also offers fresh perspectives and methodologies for addressing bias in future LLM research and applications."}, {"title": "Limitations and Future Work", "content": "While our primary focus has been on addressing implicit biases within LLMs, we expect that the BTBR framework will find broader applicability across various perspectives of LLM fairness. Moreover, advancing fairness in LLMs constitutes a formidable, long-term endeavor. Achieving an optimal solution likely necessitates concerted efforts across several academic and practical fields (Shumailov et al. 2024). In particular, our implementation of BTBR requires inferring hidden biases in LLMs using publicly available datasets. The efficacy of this bias mitigation directly correlates with the quality of these datasets, underscoring the need for superior data sources. Presently, our research has explored the elimination of single biases individually. Future initiatives will aim to expand BTBR to concurrently remove multiple biases from LLMs, paving the way for more comprehensive solutions."}]}