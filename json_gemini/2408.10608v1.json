{"title": "Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory", "authors": ["Yongxin Deng", "Xihe Qiu", "Xiaoyu Tan", "Jing Pan", "Chen Jue", "Zhijun Fang", "Yinghui Xu", "Wei Chu", "Yuan Qi"], "abstract": "Large language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence. To address this issue, we have formally defined the \"implicit bias problem\" and developed an innovative framework for bias removal based on Bayesian theory\u2014Bayesian-Theory based Bias Removal (BTBR). BTBR employs likelihood ratio screening to pinpoint data entries within publicly accessible biased datasets that represent biases inadvertently incorporated during the LLM training phase. It then automatically constructs relevant knowledge triples and expunges bias information from LLMs using model editing techniques. Through extensive experimentation, we have confirmed the presence of the \"implicit bias problem\" in LLMs and demonstrated the effectiveness of our BTBR approach.", "sections": [{"title": "Introduction", "content": "Large language models are usually trained on extensive text corpora and can encode a variety of personalities or behaviors (Wolf et al. 2023). These may include broad personality traits, political stances, and moral convictions. However, due to prejudices\u00b9 in the data spanning political ideologies, beliefs, race, gender, age, and other demographics which can be both manifested and propagated extensively via text (Stroud 2008; Tan et al. 2024), bias inevitably arises when LLMs are trained on such data (Li et al. 2023; Garg et al. 2018; Sun et al. 2019; Bansal 2022; Mehrabi et al. 2021). Despite efforts to mitigate this, such as the development of Affective Alignment (Qian et al. 2022; Delobelle and Berendt 2022), numerous prompt-based attack methods have been developed that can provoke biased responses in models (Ding et al. 2023). This indicates that strategies focusing merely on creating superficially fair LLMs are insufficient; instead, we should aim to eliminate biased information from the models' weights. Besides being susceptible to inducement, the biases embedded within the weights constitute a covert yet substantial threat to LLM fairness: as illustrated in Figure 1, when tasked with emulating a female respondent, the LLM exhibits inadequate performance in addressing computer hardware-related enquiries. This suggests that the LLM is embodying a \u201cfemale\u201d based on societal stereotypes (Ellemers 2018), rather than an authentic \"female\" identity. Nevertheless, when queried about gender equality, the responses typically affirm equality, thus masking an underlying, hard-to-discern discrimination (Hilton and Von Hippel 1996; Salewski et al. 2024; Pritlove et al. 2019), unless the model is tested across various roles (e.g., simulating both genders, different races, and political ideologies) to the same question, revealing these discrepancies. Research by (Salewski et al. 2024) demonstrated that LLMs simulating African Americans or males describe cars more effectively, whereas those mimicking Caucasians or females excel in describing birds. We refer to this as the \u201cimplicit bias problem\u201d."}, {"title": "Preliminaries", "content": ""}, {"title": "How to Define the Implicit Bias Problem?", "content": "Implicit bias in LLMs manifests when LLMs, tasked to emulate people of different genders, races, or political viewpoints, show varied performance in identical tasks. To precisely define the implicit bias problem, we engage with a collection of personalities embodying various ideologies, \u03a6. For a specific stereotypical personality \u03c6' \u2208 \u03a6, we assess through a dataset T\u03c6' = {(qi, ai)}i=1n, where qi \u2208 Q is a query and ai \u2208 A are responses generated by the LLM without prompts. A mapping function f\u03c6' : Q+b \u2192 A' (where b stands for a concise identity hint-for instance, if \u03c6' symbolizes a white supremacist's stereotype of an African American, then b could be \"Now, act as an African American and respond to the following.\"), and A' constitutes the set of responses reflective of \u03c6', exists such that the accuracy ACC\u03c6' statistical different from ACC\u03c6 in these scenarios, evidencing an implicit bias issue. Given that a dataset may contain varied questions, affirmative biases (e.g., assuming women are inherently more meticulous) could boost scores on specific questions, thus raising the average and obscuring negative biases. We generalize ACC\u03c6 \u2260 ACC\u03c6 to a broader formal context, if it holds that:\n\u2211i=1n(si\u2032\u2212si)2>\u03f5, where si \u2208 S, si\u2032\u2208 S\u2032.\nThis signifies an implicit bias within LLMs. Here, n indicates the dataset size, \u03f5 an empirical threshold proportional to the acceptable bias level in practical LLM applications, and si\u2032, si represent LLMs' performances that can be both continuous or discrete, including metrics like ACC Evaluator, EMEvaluator, BLEU, ROUGE, etc. When describing inherent biases of LLMs\u2014implicating biases that exist without explicit induction-the mapping function f\u03c6 effectively signifies \"no operation\". Although our definition may appear more complex compared to one that solely considers the intrinsic biases of LLMs, ICL approach has been effectively used to identify the mapping function f\u03c6': Q + b \u2192 A' that can lead to more pronounced biases (Zou et al. 2023; Choi and Li 2024). Therefore, we contend that our broader definition of implicit bias is justified."}, {"title": "What Makes Eliminating Implicit Bias Challenging?", "content": "As discussed in Section 1, extracting biased data from LLMs poses significant challenges, chiefly concerning the identification of such data, denoted as D'. Accessibility issues with training datasets D and their considerable variation across different LLMs (Zhang et al. 2024) necessitate a bias mitigation algorithm that is both black-box and universally applicable, a topic we will explore further in Section 3.1. However, a predominant issue is the \u201chigh entanglement\" of data used in LLM training, which we will discuss in terms of its adverse effects and how it contributes to performance degradation when biases are removed.\nIf datasets R and S are \u201chighly entangled\", efforts to eliminate S might inadvertently affect R. Since bias removal"}, {"title": "Bayesian-Theory based Bias Removal", "content": ""}, {"title": "Likelihood Ratio-based Selection Mechanism", "content": "Our objective is to pinpoint samples in biased datasets, such as statements from racially biased forums, that maximize the likelihood of a target stereotypical personality. Initially, we decompose a LLM's distribution P into a mixture of different personality distributions P\u03c6 (Wolf et al. 2023):\nP=\u2211\u03c6\u2208\u03a6\u03b1\u03c6P\u03c6\u03b1\u03c6.\nwhere \u03b1\u03c6 represents the relative weight coefficients for each personality within the LLM. Introducing an example x into the prompt essentially boosts the probability that the model expresses traits related to x, thereby accentuating the significance of features similar to x during the personality expression process. Formally, for a given prompt x, the projected output probability p\u03b8(a|x) is derived by taking the marginal distribution over all potential personalities (Xie et al. 2022):\nP=p\u03b8(a|x)=\u222b\u03a6\u2208\u03a6p\u03b8(a|x,\u03c6)p\u03b8(\u03c6|x)d\u03c6.\nHere, p\u03b8(\u03c6|x) reflects \u03b1\u03c6 in Equation 3, indicating the likelihood of the LLM displaying personality \u03c6 given x, while p\u03b8(a|x, \u03c6) matches P\u03c6 in Equation 3, denoting the probability of selecting an action under a defined personality \u03c6 \u2208 \u03a6. From Equation 4, we deduce that if a sample x maximizes p\u03b8'(\u03c6'|x) such that the LLM's output probability p\u03b8(a|x) aligns with stereotypical personality \u03c6', then this indicates that x is a key contributor to the LLM's implicit bias. To isolate the most biased samples from a candidate pool S = {xi}i=1n that contains both biased and normal data, we rewrite p\u03b8'(\u03c6'|x) utilizing Bayesian principles as:\np\u03b8\u2032(\u03c6\u2032|x)=p\u03b8(x|\u03c6\u2032)p\u03b8(\u03c6\u2032)p\u03b8(x).\nFocusing primarily on the likelihood ratio p\u03b8(x|\u03c6')/p\u03b8(x), we define our goal by logarithmically transforming Equation 5, disregarding p\u03b8(\u03c6'):\nargmaxxlogp\u03b8(x|\u03c6\u2032)\u2212logp\u03b8(x).\nThis criterion selects examples with a high conditional likelihood on persona \u03c6' while seeking lower likelihood under generic conditions, effectively leveraging the likelihood ratio to evaluate example x under two competing statistical models. In simpler terms, we aim to return examples that uniquely signify biases (closely associated with biases) and are minimally represented in the standard knowledge base of the original LLM, tactfully addressing the entanglement issues discussed in Section 2.2.\nNow, our task of identifying biased samples has evolved into calculating two types of logarithmic likelihoods. The log-likelihood log p\u03b8 (x) =\u2211t=1Tlog p\u03b8 (xt|x<t) can be readily computed where T is the token length of the example x, and \u03b8 represents the parameters of the original LLM. Direct calculation of p\u03b8'(x|\u03c6') is unavailable; however, guided by the insights from (Choi and Li 2024), we estimate p\u03b8(x|\u03c6') using a model fine-tuned with examples from candidate data pool S. Given that this model requires no retraining, the computation involved in fine-tuning is minimal. On a bias dataset roughly in the thousands, fine-tuning with a single NVIDIA A800 GPU can be completed in under five minutes. With the LLM thus fine-tuned, we can now estimate log p\u03b8(x|\u03c6') = log p\u03c6'(x) =\u2211t=1Tlog p\u03c6\u2032 (xt|x<t). Ultimately, for each example x, we compute: DB = log p\u03c6\u2032 (x) \u2212 log p\u03b8(x), x \u2208 S. Here, DB represents the \u201cdegree of bias\". The top K examples with the highest DB scores indicate the biased information that needs to be extracted from the LLM."}, {"title": "Automated Model Editing", "content": "In tasks involving the removal of specific information from LLMs, traditional evaluation methods primarily use behavioral testing, such as questioning or querying capabilities concerning the extracted information (Stoehr et al. 2024; Hase et al. 2024). Nevertheless, evidence increasingly supports that models can regenerate previously forgotten data (Lynch et al. 2024; Patil, Hase, and Bansal 2023), a critical root of implicit bias within LLMs. (Hong et al. 2024) coined the term \u201cknowledge traces,\" evaluating whether unlearning algorithms genuinely expunge data from model weights\u2014or merely disguise it until activated by malign entities\u2014by quantifying alterations in LLMs' concept vectors. Their studies showed that while fine-tuning approaches scarcely affect these vectors, techniques like MEMIT (Meng et al. 2022a), significantly dismantle the knowledge embedded in LLMs. For deploying MEMIT in bias elimination, we represent x as a subject-relation-object triple (s,r,o). We automate the conversion of x from natural language to structured knowledge. Subsequently, we substitute the original triple with a novel object o', converting (s,r,o) into (s, r, o')."}, {"title": "Experiment", "content": ""}, {"title": "Baseline and Model Selection", "content": "According to a survey by (Li et al. 2023), the most stable and effective debiasing method for LLMs is Instruction Fine-tuning, typically included in most LLMs' training phases. Thus, the choice of baseline is inherently linked to model selection. Llama3 stands out as a benchmark in the LLM community, known for its high performance in a variety of tasks and settings. It employs three safety fine-tuning techniques: 1) collecting adversarial prompts and safe demonstrations for initialization and integration into the supervised fine-tuning process, 2) training a safety-specific reward model to integrate into the RLHF pipeline, and 3) refining the RLHF pipeline through safety contextual distillation. Our experiment's baseline combines these three techniques. We utilized the \u201cLlama-3-8B-Instruct\" version for our experiments."}, {"title": "Hardware Setup and Hyperparameter Selection", "content": "Our experiments were conducted using a singl NVIDIA A800-80GB GPU. Regarding hyperparameters, we set the temperature to 0.6 and top-p to 0.9 for any LLM inference involved, following official recommendations for Llama (Meta 2024) As mentioned in Section 3.1, we used fine-tuned models to estimate p\u03b8(x|\u03c6'). To mitigate the computational costs of fine-tuning, we employed BAdam (Luo, Yu, and Li 2024), an optimization method utilizing the block coordinate descent framework with Adam as the inner solver, treating each transformer layer module as a separate block and training one block at a time. Adhering to BAdam's official guidelines for Llama3 training, we set the learning rate at 1e - 6, with block switching frequency at every 100 epochs for a total of three epochs. Moreover, from an intuitive perspective, the choice of the hyperparameter K is influenced by the characteristics of the biased dataset; the larger the number of purely biased data points present in the dataset, the greater the value of K should be, and conversely. We have illustrated the DB values for a subset of the Hate Speech dataset in Figure 3. In this instance, we opted for K = 30.\nTo eliminate bias from LLMs, we employed the MEMIT method for model editing. Originally, MEMIT edited multiple LLM layers simultaneously, but findings by (Gupta, Sajnani, and Anumanchipalli 2024) suggested that multi-layer editing could obscure actual editing performance. Therefore, our experiments focused on editing a single layer. (Meng et al. 2022b) evaluated hidden states in LLMs for fact recall through causal tracing; however, later research (Hase"}, {"title": "Metrics", "content": "To clearly demonstrate the enhancements our BTBR method offers, we assess the \u201cimplicit bias\" levels in LLMs, as defined in Section 2. By comparing the same LLM's performance both in default and induced scenarios on identical questions, we evaluate the extent of \u201cimplicit bias\". Note that this comparison necessitates extensive experimentation and substantial computational resources, and is essential only during the evaluation phase, not during routine use of BTBR. We use the Root Mean Square Error (RMSE) to quantitatively gauge the implicit bias within LLMs:\nRMSE=1n\u2211i=1n(Si\u2212si)2.\nHowever, a model that invariably replies with \"I don't know\" in any scenario is also \u201cfair\u201d, though not in a desirable way; ideally, we expect LLMs prompted with different personalities to perform not just similarly, but competently. Considering alignment theory (Lin et al. 2023) and the no free lunch theorem, removing data from models typically results in a performance drop, necessitating a balance between fairness and performance. Consequently, we introduce the metric Average Maximum Score Drawdown (AMSD):\nAMSD=1n\u2211i=1nmax((si\u2212\u015di),(s\u2032i\u2212\u015d\u2032i)).\nHere, \u015di denotes the performance score of LLMs post-bias removal via BTBR, and \u015d\u2032i the performance post-induction. Typically, the term si \u2212 \u015di is negative, as the model becomes less biased and thus performs better. Nonetheless, potential performance declines from data removal must be considered. The AMSD metric represents the maximum performance trade-off we accept in enhancing LLM fairness, aiming for as low a value as possible."}, {"title": "Datasets", "content": "For evaluation purposes, we utilized various datasets, typically categorized by task type. In our experiments, we employed a more detailed categorization. Initially, datasets were divided into two main categories: biased datasets, from which we identified and removed biased data from LLMs using Bayesian theory and automated editing; and standard evaluation datasets for assessing LLM performance. Datasets in the first category were further classified by the type of bias they represented, while those in the second category were classified by their knowledge domain. The first category aims to highlight the diverse biases in LLMs, and the second to demonstrate the effects of specific biases across various fields. Details on all utilized datasets follow.\nFirst Category Datasets:\n\u2022 Hate Speech. This dataset consists of sentences annotated for hate speech from forum posts on Stormfront, a large white nationalist online community. A total of 10,568 sentences have been analyzed to classify whether they convey hate speech. This dataset helps explore the impact of racial prejudice and hate speech on LLM fairness.\n\u2022 Crows Pairs. Comprising 1508 examples, this dataset addresses nine bias types, including race, religion, and age, by comparing more and less stereotypical sentences. Given the significant noise and reliability issues identified by (Blodgett et al. 2021), we do not use its original annotations outright but select the most biased instances through our BTBR method. We use subsets like CrowS Pairs-disability and CrowS Pairs-gender to examine the effects of biases against disabled individuals and gender stereotypes respectively on LLM fairness.\nSecond Category Datasets:\n\u2022 GPQA. The Graduate-Level Google-Proof QA Benchmark contains 448 challenging multiple-choice questions from fields such as biology, physics, and chemistry, designed to test LLMs' advanced knowledge handling. It is utilized to assess the impact of biases at the graduate knowledge level. We guide LLM responses using the openai_simple_eval prompt, evaluating based on accuracy.\n\u2022 MMLU. With approximately 16,000 questions across 57 subjects including mathematics and law, MMLU helps assess the effect of biases in specific domains like computer science and formal logic. Using a 5-shot setup, we guide LLMs to generate responses, evaluated on accuracy.\n\u2022 GSM8K and MATH. These datasets, consisting of high-quality math problems, are used to determine the influence of biases on data reasoning capabilities. Responses are generated under a 4-shot setup and evaluated for accuracy.\n\u2022 MBPP. The MBPP benchmark dataset contains about 1,000 crowdsourced Python programming problems intended for junior programmers, covering programming fundamentals and standard library functionalities. Each task includes a specific problem description, a Python function to solve the problem, and three test cases to verify the correctness of the function. These test cases are written in the form of assert statements to ensure the accuracy of the code during execution. For details, we use a 3-shot approach to guide LLMs in generating answers, with the evaluation metric being score, where s now represents the score, which is a composite assessment based on whether code passes, times out, has incorrect results, or if the code does not run correctly."}, {"title": "Results and Analysis", "content": "Our main findings from the BTBR evaluation, conducted by OpenCompass (Contributors 2023), are presented in Table 1. The RMSE, used to compare the standard versus biased performance of LLMs, facilitates insights into bias influence when biased LLMs are induced using the mapping function f\u03c6': Q + b \u2192 A'. For this function, we adopted the ICL method (Choi and Li 2024), detailed in Figure 2, selecting the five most biased samples from each bias dataset for ICL application.\nAs shown in Table 1 Hate Speech biases notably deteriorated Llama3's performance in college computer science and human sexuality. Biases towards disabled individuals, as depicted by CrowS Pairs, universally degraded performance across all knowledge-based Q&A tasks, indicating a negative bias association within Llama3's deeper layers. Gender-related biases did not significantly affect performance. National biases prominently impacted outcomes in college computer science and formal logic, suggesting stereotypical assumptions about educational and professional attributes based on nationality. Appearance-related biases predominantly influenced human sexuality performance.\nKnowledge-based Q&A tasks were generally more vulnerable to implicit biases, whereas reasoning tasks such as GSM8K, MATH, and MBPP appeared largely immune, likely due to the nature of reasoning problems that resists bias introduction via RLHF. Interestingly, MBPP's performance was unaffected by biases that significantly impaired results in computer science, an observation that, according to alignment theory (Contributors 2023), suggests a decoupling of 'computer knowledge' and 'programming skills' within LLMs. Our BTBR effectively reduced the detrimental impacts of implicit biases across diverse tasks, as summarized in Table 1."}, {"title": "Ablation Studies", "content": "One might wonder, why not simply extract the entire bias dataset from LLMs? Are Bayesian methods for data filtering truly necessary? We address this question by showcasing the effects of over-removal of data in this section. Table 2 compares AMSD performance between partial data removal using BTBR and complete bias dataset extraction. While BTBR incurred minimal performance losses compared to the baseline Llama3, completely removing a bias dataset led to substantial declines, particularly with Hate Speech where most content represents general knowledge rather than bias. Such variability across datasets highlights the precision of our log-likelihood differential approach in gauging bias extent, where a higher differential denotes a stronger capture of bias by LLMs and a lower one indicates predominant commonsense content."}, {"title": "Conclusion", "content": "In this research, we conducted an extensive examination of implicit biases within LLMs and introduced a novel approach to mitigate this issue. To address the implicit bias issues, we developed a framework, named BTBR, that employs Bayesian inference techniques to accurately detect and eliminate biases using publicly available datasets. Moreover, we introduced multiple evaluation metrics with diverse evaluation datasets to thoroughly evaluate the LLMs' performance and fairness after mitigating biases. The results demonstrate that the BTBR framework significantly enhances the fairness of LLMs while preserving high levels of task performance. Not only does this finding validate the efficacy of our methodology, but it also offers fresh perspectives and methodologies for addressing bias in future LLM research and applications."}, {"title": "Limitations and Future Work", "content": "While our primary focus has been on addressing implicit biases within LLMs, we expect that the BTBR framework will find broader applicability across various perspectives of LLM fairness. Moreover, advancing fairness in LLMs constitutes a formidable, long-term endeavor. Achieving an optimal solution likely necessitates concerted efforts across several academic and practical fields (Shumailov et al. 2024). In particular, our implementation of BTBR requires inferring hidden biases in LLMs using publicly available datasets. The efficacy of this bias mitigation directly correlates with the quality of these datasets, underscoring the need for superior data sources. Presently, our research has explored the elimination of single biases individually. Future initiatives will aim to expand BTBR to concurrently remove multiple biases from LLMs, paving the way for more comprehensive solutions."}]}