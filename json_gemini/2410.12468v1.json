{"title": "Evaluating Software Development Agents:\nPatch Patterns, Code Quality, and Issue\nComplexity in Real-World GitHub Scenarios", "authors": ["Zhi Chen", "Lingxiao Jiang"], "abstract": "In recent years, AI-based software engineer-\ning has progressed from pre-trained models to advanced\nagentic workflows, with Software Development Agents\nrepresenting the next major leap. These agents, capable of\nreasoning, planning, and interacting with external environ-\nments, offer promising solutions to complex software engi-\nneering tasks. However, while much research has evaluated\ncode generated by large language models (LLMs), compre-\nhensive studies on agent-generated patches, particularly\nin real-world settings, are lacking. This study addresses\nthat gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench\nVerified, focusing on their impact on code quality. Our\nanalysis shows no single agent dominated, with 170 issues\nunresolved, indicating room for improvement. Even for\npatches that passed unit tests and resolved issues, agents\nmade different file and function modifications compared\nto the gold patches from repository developers, revealing\nlimitations in the benchmark's test case coverage. Most\nagents maintained code reliability and security, avoiding\nnew bugs or vulnerabilities; while some agents increased\ncode complexity, many reduced code duplication and min-\nimized code smells. Finally, agents performed better on\nsimpler codebases, suggesting that breaking complex tasks\ninto smaller sub-tasks could improve effectiveness. This\nstudy provides the first comprehensive evaluation of agent-\ngenerated patches on real-world GitHub issues, offering\ninsights to advance AI-driven software development.", "sections": [{"title": "I. INTRODUCTION", "content": "Background: Agents Are The Future Of AI. [1]. AI-\nbased software engineering has evolved rapidly, moving\nfrom pre-trained models [2] to fine-tuned large language\nmodels (LLMs) [3], in-context learning [4], and further\nadvancing with techniques like chain-of-thought [5] and\nagentic workflows [6], [7]. Software Development Agents\nrepresent the next step in Al development [6]-[8], inte-\ngrating reasoning, planning, and interaction with external\nenvironments to perform autonomous tasks and make\ndecisions which enables them to tackle complex software\nengineering challenges beyond simple function genera-\ntion. Emerging agents, such as Amazon Q Developer and\nEPAM AI/Run Developer Agent, highlight Al's potential\nto address more complex development tasks, marking a\nnew direction where agentic workflows drive the creation\nof sophisticated, real-world applications.\nMotivation: While software development agents have\nadvanced rapidly, comprehensive evaluations of the code\nthey generate in real-world tasks are still lacking. Agents\ndiffer from large language models (LLMs) that gener-\nate code from static prompts by incorporating reason-\ning, planning, and interactions with external environ-\nments, which requires a distinct evaluation approach.\nAlthough many studies have evaluated LLM-generated\ncode, focusing on aspects like security vulnerabilities\nand reliability [9]-[13], these findings may not directly\napply to agents due to their more complex workflows\nand autonomous decision-making. Furthermore, much\nof the existing research is based on simpler tasks like\ngenerating Python functions or solving algorithmic chal-\nlenges [10], [14], or on controlled vulnerability scenar-\nios [9], [15], which do not capture the complexity of\nreal-world software development. Our study addresses\nthis gap by evaluating agent-generated patches on real\nGitHub issues, providing insights that are more relevant\nto real-world software development."}, {"title": "II. STUDY DESIGN", "content": "A. Choice of Benchmark\nWe aim to evaluate the quality of agent-generated\npaches in real-world scenarios. For this, we use the\nSWE-Bench Verified dataset [17], which contains 500\nIssue-Pull Request pairs from 12 open-source Python\nrepositories. Validated by software engineers with sup-\nport from OpenAI's Preparedness Team, it offers a\nreliable benchmark for assessing agents on real GitHub\nissues. Each issue is tied to a PR containing solution code\nand unit tests based on gold patches from the repos-\nitory developers. These tests include FAIL_TO_PASS\ntests, which fail before the PR and pass after, verifying\nthat the issue is resolved, and PASS_TO_PASS tests,\nwhich pass both before and after, ensuring that unrelated\nfunctionality remains intact. Agents are given the issue\ntext (problem statement) and access to the codebase but\nnot the tests. An issue is considered RESOLVED if the\nagent's code passes both test types, ensuring the solution\nis correct and does not break existing functionality. This\nevaluation framework, coupled with a public leaderboard\ntracking agent performance, provides a robust bench-\nmark for assessing agent-generated patches [16].\nB. Choice of Agents\nWe selected the top 10 agents from the SWE-Bench\nVerified public leaderboard as of August 25, 2024,\nrepresenting both industry and academia. These agents,\nrecognized for their high performance in resolving\nGitHub issues, represent the state-of-the-arts in the latest\nadvancements in AI-driven software development. By\nevaluating this diverse group, we provide a comprehen-sive assessment of the quality of their generated patches.\nThe rankings and data reflect the most current results,\nensuring the relevance and accuracy of our evaluation.\nTable I presents the details and reported issue resolution\nrates for these agents.\nC. Research Questions\nRQ1:What patch patterns do current Software De-\nvelopment Agents use when solving real-world GitHub\nissues?\nMotivation: The SWE-Bench Verified dataset presents\ncomplex tasks where agents must analyze problem state-\nments, identify relevant files in large codebases, and gen-\nerate patches to resolve issues. While the current leader-\nboard only measures the percentage of issues resolved,\nit lacks deeper analysis into how agents generate these\npatches. Our goal is to go beyond this basic metric by\ncomparing agent-generated patches to human-developed\ngold patches, exploring whether agents modify similar\nfiles and functions or make alternative modifications.\nThis will help uncover how closely agent solutions align\nwith human solutions and reveal the nuances of their\npatch generation.\nRQ2: How do patches generated by Software De-\nvelopment Agents impact the reliability, security, and\nmaintainability of the codebase?"}, {"title": "III. DATA COLLECTION AND CONSTRUCTION", "content": "Our evaluation data consists of mainly three por-\ntions: the official SWE-Bench Verified dataset, agent-\ngenerated patch solutions, and the code files associated\nwith each patch. The following subsections introduce\nthese datasets, which are the bases for our later analyses.\nA. SWE-Bench Verified Dataset\nWe downloaded the SWE-Bench Verified dataset from\nHugging Face\u00b3. It includes 500 human-validated samples\nfrom the larger SWE-Bench dataset of 2,200 samples.\nEach sample in the dataset has been reviewed for quality\nby OpenAI's Preparedness Team [17]. The key com-\nponents in this dataset are: 1) repo The repository\nowner/name identifier from GitHub; 2) base_commit\nThe commit before the solution patch is applied;\n3) problem_statement The issue title and body\ndescribing the problem; 4) patch The gold patch\ncreated by repository developers to resolve the issue.\nB. Agent-Generated Patches\nFor each agent, we collected the agent's patch solu-\ntions from the SWE-Bench Verified public leaderboard\nby extracting from its logs and prediction files (e.g.,\nall_preds.jsonl) for the 500 GitHub issues. The\ngenerated patch.diff files represent the agent's at-\ntempts to resolve these issues.\nC. Patch-Associated Code Files\nTo assess the impact of the patches, we retrieved the\npre-patch relevant files from the base commit of each\nrepository and applied the corresponding patch.diff\nfiles to generate the post-patch files. This processing\nenabled us to track changes in code quality, the number\nof files, and modifications between the pre-patch and\npost-patch states. Table II summarizes the number of\npatches generated by each agent, the number of patches\nthat were applied without errors (regardless of whether\nthey resolved the issue), and the total number of files\nbefore and after applying the patches."}, {"title": "IV. PATCH ANALYSIS", "content": "A. Experimental Setup\nTo answer RQ1: What patch patterns do current Soft-\nware Development Agents use when solving real-world\nGitHub issues? we designed a multi-level analysis to\nevaluate agent-generated patches. This approach begins\nat the issue level, where we assess overall problem-\nsolving success, and drills down to file, function, and\nline-level modifications. The purpose of this structure is\nto progressively reveal how agents handle increasingly\ngranular aspects of software development, allowing us\nto understand both high-level patching performance and\ndetailed patch patterns.\nIssue Level: At the issue level, our goal is to un-\nderstand how effectively agents are resolving real-world\nGitHub issues. We categorize issues based on how many\nagents successfully resolved them\u2014starting from those\nsolved by all 10 agents to those solved by only one\nor none. This allows us to identify common challenges\nthat agents consistently resolve and issues that remain\nunsolved by all agents. Additionally, we perform an over-\nlap analysis to explore whether certain agents dominate\nspecific issues or if there is complementary performance\nbetween agents. This helps reveal patterns of strength\nand weakness among the top-performing agents, shed-\nding light on whether individual agents specialize in\ncertain types of issues or if there is significant overlap\nin their success.\nFile and Function Level: We analyze which files and\nfunctions each agent modifies compared to the repository\ndevelopers. Although a solution may pass all test cases,\nthese are based on gold patches, and if agents modify\ndifferent parts of the code, the tests may not fully\ncover their changes. We assess whether agents target\nthe same files and functions as developers, using pre-\ncision, recall, and Fl-score to quantify alignment with\ngold patches [23]. This helps determine how effectively\nagents identify relevant code areas and make targeted\nmodifications.\nLine Level: At the line level, we assess the patterns\nof how agents modify code through metrics such as\nadded lines, deleted lines, total edits (sum of additions\nand deletions), and net code size change (difference\nbetween added and deleted lines). These patterns are key\nfor understanding how agents manage code complexity\nand maintainability. Excessive changes can introduce\ncomplexity, while minimal edits may leave issues un-\nresolved [24]. To quantify differences between agent\nand gold patch modifications, we apply the Wilcoxon\nsigned-rank test [25] to identify statistically significant\nvariations in these line-level patterns.\nB. Experimental Result\nIssue-Level Analysis: In this section, we explore the\nperformance of agents on the issue level, focusing on\nhow many issues were resolved and the overlaps between\nagents."}, {"title": "V. CODE QUALITY ANALYSIS", "content": "A. Experimental Setup\nTo answer RQ2: How do patches generated by Soft-\nware Development Agents impact the reliability, secu-\nrity, and maintainability of the codebase? we focus on\neach agent's patches that have successfully RESOLVED\nissues\u2014those most likely to be accepted and merged\ninto the codebase. Given their potential to impact the\ncodebase long-term, it is crucial to evaluate these patches\nbeyond functional correctness. For each agent's group of\nresolved patches, we assess their impact across three key\nnon-functional aspects: reliability, security, and main-\ntainability. These aspects are essential for understanding\nthe broader effects on code quality and sustainability.\nWe use SonarQube to perform static analysis for these\nmetrics, as it is widely recognized in both academic\nresearch and industry [10], [20], [26]. Its free community\nversion ensures the reproducibility of our experiments,\nwhile sharing the same set of detection rules as the\ncommercial version used in industry. This alignment\nguarantees that our findings are both accessible and\nrelevant to real-world software quality concerns.\na) Reliability: The primary goal of assessing re-\nliability is to determine whether the agent-generated\npatches introduce new bugs or fix existing ones without\nbreaking other parts of the codebase [27]. This is crucial\nbecause a patch, while solving one problem, could\ninadvertently destabilize other parts of the system. To\nevaluate this, we measure the number of bugs in the pre-patch files and compare them to the post-patch files using\nSonarQube's bug detection capabilities. This approach\nprovides insight into whether agents maintain or degrade\nthe overall stability of the code.\nb) Security: Security is crucial in software devel-\nopment, as introducing vulnerabilities can lead to severe\nconsequences [28]. To assess whether agent-generated\npaches improve or weaken the security of the code-\nbase, we use SonarQube to calculate the number of\nvulnerabilities in the pre-patch files and then re-evaluate\nthem after the patch is applied. This analysis helps\ndetermine whether the agent patches not only address\nthe issue but also avoid creating new security risks.\nGiven the increasing importance of secure software,\nthis step ensures that agent-generated patches contribute\npositively to the overall security posture of the software.\nc) Maintainability: Maintainability measures how\neasily the code can be understood, modified, and ex-\ntended in the future, which is essential for the long-term\nsustainability of software [29]. We use SonarQube to\nevaluate three key metrics: code smells, code complexity,\nand code duplication.\n\u2022\n\u2022\nCode complexity: We measure code cyclomatic com-\nplexity, which quantifies the number of independent\npaths through a function's control flow [30]. Since\nhighly complex code is harder to modify and maintain,\nwe normalize this value by dividing the cyclomatic\ncomplexity by the number of lines of code to give a\nbalanced view of code complexity relative to its size.\nCode duplication: Duplicated code increases mainte-\nnance costs, as changes must be applied in multiple\nplaces. We assess the percentage of duplication by\ncalculating the ratio of duplicate lines to the total lines\nof code, providing insight into the risk of code bloat\nand unnecessary repetition.\nCode smells: Indicators of potential design flaws that\ncan make the code harder to maintain. We calculate\nthe total number of code smells in the pre-patch and\npost-patch files and normalize them by lines of code,\ngiving a clearer picture of their prevalence relative to\nthe size of the codebase.\nBy comparing these metrics before and after the patch\nis applied, we can determine whether the agent patches\nimprove or worsen the maintainability of the code. Since\nthe data for these metrics does not follow a normal\ndistribution, as verified by the Shapiro-Wilk test [31], we\nuse the non-parametric Wilcoxon signed-rank test [25] to\ndetermine whether there are statistically significant im-provements or declines in code quality after the patches\nare applied. Additionally, we compute the Rank-Biserial\nCorrelation [32] to quantify the magnitude of changes,\ninterpreting effect sizes using Cohen's guidelines [33]."}, {"title": "VI. GITHUB ISSUE ANALYSIS", "content": "To address RQ3: What differentiates resolved and\nunresolved GitHub issues, and how can these differences\nbe used to improve the Issue Resolved Rate of Soft-\nware Development Agents? we systematically compare\nRESOLVED GitHub issues (those successfully addressed\nby at least one agent) with UNRESOLVED issues. This\ncomparison is conducted through three key perspectives:\n(1) the complexity of the issue's problem statement, (2)\nthe source code files associated with these issues (derived\nfrom gold patches), and (3) the gold patch solutions pro-\nvided by repository developers. These perspectives are\nchosen to comprehensively understand the multifaceted\nchallenges that agents encounter when resolving issues.\n1) Analysis of GitHub Problem Statements: Under-\nstanding the complexity of problem statements is crucial\nas it directly impacts an agent's ability to comprehend\nand address issues effectively. Therefore, we evaluate the\nproblem statements using the following metrics:\nReadability and Length: We assess problem statement\ncomplexity using Flesch Reading Ease [38], where\nhigher scores indicate easier text, and Flesch-Kincaid\nGrade Level, which estimates the required education\nlevel. These metrics have been applied in previous\nwork [22], [39], [40]. Additionally, we consider Sen-\ntence Count and Word Count, assuming that lower\nreadability and longer content may hinder agents'\nunderstanding of the GitHub issue, making it more\ndifficult to generate accurate solutions [41].\nCode Relevance: Code snippets in the problem state-ment can provide valuable context for agents, helping\nthem locate and solve issues. However, they may\nalso increase the difficulty for agents to process [42].\nWe measure metrics such as Contains Code snippets,\nNumber of Code Blocks, Lines of Code, and the\nCode-to-Text Ratio to compare these factors between\nresolved and unresolved GitHub issue groups.\n2) Analysis of Associated Source Code Files: Nav-igating, understanding, and modifying relevant source\ncode files are crucial for resolving issues. We analyze\nthe source code files linked to each issue, using the gold\npatch-the repository developers' solution as the base\nfor comparison:"}, {"title": "VII. DISCUSSION", "content": "A. Suggestions\n1) Suggestions for SWE-Bench Verified Maintain-ers: Enhance the evaluation framework by expanding\ntest coverage to detect potential side effects from agent-\ngenerated patches that diverge from gold patches, even\nif unit tests pass. Incorporate code quality metrics such\nas complexity, duplication, and code smells into the as-\nsessment criteria to ensure agents produce maintainable\ncode [44], [45].\n2) Suggestions for AI Agent Developers: Enhance\nagents' ability to handle complex tasks by breaking\ndown issues into manageable sub-tasks. While improv-\ning functional correctness, focus on the non-functional\naspects of the generated solutions, such as avoiding over-\nmodifications, improving maintainability, reducing code\ncomplexity and duplication, and ensuring no new bugs\nor vulnerabilities are introduced. Consider integrating\nadditional safeguards, like Code Shield, to promote\nsecure software development [46], [47].\n3) Suggestions for Users of Al Agents: Utilize mul-tiple agents to leverage their varied strengths, increasing\nthe chances of successful issue resolution. Carefully\nreview agent-generated patches for over-modifications\nand potential unintended effects, even if they pass unit\ntests. For complex issues, consider decomposing them\ninto smaller, manageable tasks to align with agents'\ncurrent capabilities.\nB. Threats to Validity\n1) Threats to Internal Validity: Our study may be\naffected by data collection bias and measurement reli-ability. Since we relied on data from the SWE-Bench\nVerified dataset and agent-generated patches, any errors\nin data extraction or processing could influence the\nresults. To mitigate this, we automated data collection\nand performed manual checks for accuracy. Additionally,\nusing SonarQube for code quality analysis could intro-duce measurement errors. We addressed this by using\nthe widely accepted community version of SonarQube\nand ensuring consistent analysis conditions.\n2) Threats to External Validity: Our findings may\nhave limited generalizability, as the study focuses on\n500 GitHub issues, which may not represent other pro-gramming languages or project types. However, SWE-Bench Verified, with 12 diverse and widely-used Python\nrepositories, strengthens the relevance of our results. As\nfuture benchmarks expand to more languages and project\nscenarios, we plan to extend our study accordingly.\nWhile software development agents evolve rapidly, the\ndata from the public leaderboard reflects the most recent\nrankings at the time of data collection, ensuring the\ntimeliness of our analysis.\n3) Threats to Construct Validity: The validity of our\nmetrics and comparisons may pose a threat. Metrics like\ncode smells, cyclomatic complexity, and our statistical\ntests may not capture all aspects of code quality or agent\nperformance. To mitigate this, we used well-established\nmetrics and multiple measures for a comprehensive\nassessment. Comparing agent patches to gold patches\nassumes the gold patches are optimal, which may not\nalways be the case. We addressed this by also evaluating\nthe impact of agent patches on code quality, acknowl-edging that alternative solutions can be acceptable if they\nmaintain or improve quality."}, {"title": "VIII. RELATED WORK", "content": "Recent studies have explored the security and quality\nof code generated by large language models (LLMs) like\nGitHub Copilot and ChatGPT. Pearce et al. [9] found\nthat approximately 40% of Copilot-generated code was\nvulnerable to CWE Top 25 weaknesses, while a replica-tion by Majdinasab et al. [15] reduced this to 27.25%,\nhighlighting ongoing security concerns. Asare et al. [34]\nand Hamer et al. [35] compared LLM-generated code\nwith human-written code and StackOverflow snippets,\nnoting that while LLMs can introduce vulnerabilities,\nthey sometimes perform comparably or better than hu-man developers. Nguyen et al. [26] analyzed Copilot's\ncode suggestions using LeetCode problems, revealing\nvariations across languages, along with issues like high\ncomplexity and reliance on undefined methods. Sim-ilarly, Liu et al. [10] and Liu et al. [14] assessed\nChatGPT's performance on algorithmic tasks and found\nissues in code correctness and maintainability. Rabbi et\nal. [48] and Siddiq et al. [12] further emphasized the\nchallenges in using ChatGPT-generated code, identifying\nlimitations in quality and maintainability.\nOur research offers two notable contributions that\ndifferentiate it from related work. First, we evalu-ate the quality of code produced by software devel-opment agents like Amazon-Q Developer Agent and\nAppMap Navie + GPT 40-that enhance LLM capa-bilities through agentic workflows beyond standalone\nor base LLMs. Second, unlike prior work focusing on\nsimplified scenarios like isolated algorithmic challenges\nor vulnerability-prone prompts, we assess code quality\nwith real-world GitHub issues, which involve complex\ncodebases and require modifications across multiple\nfiles. This provides a more realistic evaluation of agent-generated code, bridging a critical gap in the literature."}, {"title": "IX. CONCLUSION", "content": "This study analyzed 4,892 patches generated by 10\nsoftware development agents on 500 real-world GitHub\nissues from SWE-Bench Verified, focusing on their\nimpact on code quality. No single agent dominated,\nwith 170 issues unresolved, highlighting areas for im-provement. Even for patches that passed unit tests and\nresolved issues, their divergence from \"gold patches\"\nrevealed risks not captured by current tests. While some\nagents like Gru demonstrated more balanced modifi-cations, and the others like HoneyComb over-modified\nthe code, impacting maintainability. Most agents main-tained code reliability and security, avoiding new bugs\nor vulnerabilities, and performed comparably to human\npatches in reducing code smells and duplication. How-ever, some agents need improvement in minimizing code\ncomplexity and duplication. Lastly, agents were more\nsuccessful with simpler tasks, suggesting that breaking\ndown complex issues could enhance their effectiveness.\nFuture work should focus on improving agents' ability\nto handle more complex scenarios, as well as expanding\nthe benchmarks to include vulnerability-prone issues\nfor a deeper evaluation of agent performance in secure\nsoftware development."}]}