{"title": "A large collection of bioinformatics question-query pairs over federated knowledge graphs: methodology and applications", "authors": ["Jerven Bolleman", "Vincent Emonet", "Adrian Altenhoff", "Amos Bairoch", "Marie-Claude Blatter", "Alan Bridge", "Severine Duvaud", "Elisabeth Gasteiger", "Dmitry Kuznetsov", "S\u00e9bastien Moretti", "Pierre-Andre Michel", "Anne Morgat", "Marco Pagni", "Nicole Redaschi", "Monique Zahn-Zabal", "Tarcisio Mendes de Farias", "Ana Claudia Sima"], "abstract": "In the last decades, several life science resources have structured data using the same framework and made these accessible using the same query language to facilitate interoperability. Knowledge graphs have seen increased adoption in bioinformatics due to their advantages for representing data in a generic graph format. For example, yummydata.org catalogs more than 60 knowledge graphs accessible through SPARQL, a technical query language. Although SPARQL allows powerful, expressive queries, even across physically distributed knowledge graphs, formulating such queries is a challenge for most users. Therefore, to guide users in retrieving the relevant data, many of these resources provide representative examples. These examples can also be an important source of information for machine learning, if a sufficiently large number of examples are provided and published in a common, machine-readable and standardized format across different resources.\nWe introduce a large collection of human-written natural language questions and their corresponding SPARQL queries over federated bioinformatics knowledge graphs (KGs) collected for several years across different research groups at the SIB Swiss Institute of Bioinformatics. The collection comprises more than 1000 example questions and queries, including 65 federated queries. We propose a methodology to uniformly represent the examples with minimal metadata, based on existing standards. Furthermore, we introduce an extensive set of open-source applications, including query graph visualizations and smart query editors, easily reusable by KG maintainers who adopt the proposed methodology.\nWe encourage the community to adopt and extend the proposed methodology, towards richer KG metadata and improved Semantic Web services.", "sections": [{"title": "Introduction", "content": "The accuracy and real-world applicability of question answering systems over structured data crucially depend on the availability of high-quality, representative question-query pairs over diverse datasets. Resources such as UniProt (UniProt Consortium, 2023), have been used in bioinformatics for many years, where they facilitate data integration and exploration through (potentially federated) SPARQL queries. However, a long-standing challenge is making this wealth of data usable by a wider range of users, including researchers without technical training. Writing SPARQL queries is outside the competence of most life scientists, as it requires both knowledge of the query language itself and understanding of the knowledge graph schema, for example, a Resource Description Framework (RDF) graph data model. Many solutions have thus been investigated, including Natural Language Interfaces (NLI) to knowledge graphs (KG). The recent progress in Large Language Models (LLMs) has enabled to boost performance across a wide range of Natural Language Processing tasks, including promising results in LLM-based Knowledge Graph Question Answering (KGQA) systems (A.-C. Sima & de Farias, 2023). A recent study highlights that unified models, trained across diverse datasets stemming from distinct knowledge graphs, eliminate the need for separate models for each KG and can therefore significantly reduce the overall costs of fine-tuning and deploying such models (Zahera et al., n.d.). Therefore, a key requirement to the development of novel, LLM-based KGQA systems, is the availability of high-quality, representative public datasets of questions and corresponding SPARQL queries across a wealth of diverse KGs.\nCapturing user intent and translating questions becomes significantly more complex when considering multiple, distributed scientific KGs such as those comprising the SIB Swiss Institute of Bioinformatics Semantic Web of Data. Although federated SPARQL queries are very powerful because they enable joint querying of physically distributed knowledge graphs, the challenge lies in formulating such queries. Federated queries are inherently complex because they require users to not only understand the data models of the individual sources involved in the query, but also to be aware of how these sources interconnect (i.e., how they can be joined). Hence, the availability of examples to guide users - or of a system supporting the automatic translation of user questions to federated queries - are critical. However, federated queries have been mostly neglected so far in existing collections of natural language questions and equivalent SPARQL queries.\nThe SIB hosts a number of high-quality, curated and interoperable life science knowledge graphs\u00b9 that are accessible via public SPARQL endpoints, i.e. web addresses that can receive and process SPARQL queries. Each of these KGs is independently managed by different research groups within the institute. However, a common point across all resources is that they each maintain a set of representative questions and their equivalent SPARQL queries, collected over time. These are made available to inspire and help users formulate new queries over the respective KG data. Importantly, the resources also include examples of federated queries, which allow for joining information from their KG with those of other, complementary KGs, from"}, {"title": "Sources", "content": "At the time of writing, we have collected contributions from 10 large-scale KGs across different SIB groups. Additionally, we integrated contributions from one external KG, \u201cdbgi\u201d, with examples contributed by external collaborators in the Digital Botanical Gardens Initiative\u00b2.\nFor details on the SPARQL endpoints maintained by the SIB, we refer the reader to the SIB Semantic Web of Data (SIB Swiss Institute of Bioinformatics RDF Group Members, 2024). Among the contributing KGs, we can mention UniProt - currently the largest publicly available\nKG - with over 168B triples and a total of 61 contributed example queries. To our knowledge, the SIB collection is the most comprehensive collection of real-world SPARQL queries examples to-date in bioinformatics.\nAs an estimate of the complexity of the queries in this collection, we show the average number of triple patterns (TP) per endpoint included in the collection in Figure 1. The number of TPs is a good approximation for the complexity of a query, because it indicates the number of hops (joins) required to compute an answer over the graph. Most queries have around 6 TPs on average, with the notable exception of the DBGI, for which queries have a much higher complexity, owing to the complexity of the data model, but also due to federation with Wikidata, which is present in most examples. On the other end, HAMAP includes only a few very simple examples, as it is a small KG (only 7 classes) storing annotation rules. We note that most existing large scale collections focus on simple queries, i.e. queries with at most 3 TPs (for examples, see Related Work)."}, {"title": "Methodology", "content": "Standardization is at the core of our approach. To structure and describe the question-query pairs, we rely on the SHapes Constraint Language (SHACL) which is RDF based\u00b3. In addition to SHACL and RDF Schema (RDFS)\u2074, we describe and annotate the examples with Schema.org. Standardizing how we represent the examples is crucial since they can target different and independent data sources managed by distinct groups and organizations. A standardized, machine-readable representation, enables both users and services to discover and smoothly use the question-query descriptions that are defined in RDF and stored in distributed data stores accessible with SPARQL. More precisely, example question-query pairs can be stored with their associated metadata in the same RDF store, consequently, making them accessible\nvia the same SPARQL endpoint. As a result, this standardization and infrastructure facilitate the automatic testing of queries, as well as the deployment of services with minimal code modifications across endpoints, such as the generation of user-friendly Web pages with query descriptions and graphical visualizations (see Section Applications)."}, {"title": "Implementation", "content": "We collaborate in an open GitHub repository (https://github.com/sib-swiss/sparql-examples).\nThe repository is the \"single source of truth\u201d, where all SPARQL examples are collected via contributions from data store maintainers. Each example is stored in a separate file using the Terse RDF Triple Language (Turtle, i.e. \u201c. ttl\u201d) syntax and file format, within a folder named according to the resource that contributed the example. For example, all Bgee queries can be found in https://github.com/sib-swiss/sparql-examples/tree/master/examples/Bgee .\nEach Turtle file stores a single example, which is represented by:\na) The type of the query (e.g., Ask, Select, Describe).\nb) The description of the query (e.g., the question in plain text), including a language tag.\nc) The SPARQL query itself.\nd) The target SPARQL endpoint(s) where the query can be executed.\ne) Additional keywords (tags).\nEach question-query pair is defined as a SPARQLExecutable SHACL concept. We also specify the SPARQLExecutable subtype, such as SPARQLAskExecutable and SPARQLSelectExecutable, that is executable queries based on an ASK and SELECT SPARQL query, respectively. Furthermore, all of the fields b) to e) above are represented in a structured, standardized and machine-readable format. Namely, the plain English question or description of the query can be described via an rdfs:comment, the query itself via sh:select, sh:ask, sh:update, sh:construct or sparql-examples:describe' (according to the type of the SPARQL query, i.e. ASK, SELECT etc.). For multilingual support in the examples, all questions are annotated with a corresponding language tag (e.g. \u201c@en\u201d for English). The set of queries in this work only includes Ask, Select and Describe queries. As DESCRIBE queries are defined in SPARQL 1.1 and can not be used in SHACL, we introduce a new property and concept to represent this type of query, that is, sparql-examples:describe where the sparql-examples: prefix corresponds to https://purl.expasy.org/sparql-examples/ontology#. The target SPARQL endpoint(s) are described with schema:target. Note that there can be several such endpoints where the query is directly executable. The query reusability (without any modifications) stems from the adoption of the FAIR principles in the design of KGs across the entire SIB Semantic Web of Data, which makes them uniformly queryable when requesting the same type of information. For example, the query defined at https://www.bgee.org/sparql/.well-known/sparql- examples/1 is usable as-is in at least three SPARQL endpoints, namely Bgee, OMA and UniProt. In the case of this example, it retrieves the list of species in each KG. The schema:target shows where users can run these queries and also demonstrates the benefit of sharing knowledge representation of common concepts and values. Finally, example queries can optionally be annotated with additional keywords (tags) using schema:keywords property.\nTo further illustrate how question-query pairs are structured\nAfter describing the SPARQL examples, we also store them in a standardized location directly as a subgraph within the KG that they target - that is, as part of the same SPARQL endpoint as the data they act on - so as to enhance their findability. This allows both users and services to uniformly retrieve examples of interest, or to aggregate information about the collection of examples available in each resource, by directly querying the SPARQL endpoints themselves. In addition, it enables reusability of services across KGs with minimal modifications and without reliance on a centralized repository. We provide a script to merge the examples targeting a given endpoint as a single .ttl file that can be uploaded to a dedicated subgraph (e.g., as a named graph) within each KG. Across SIB, we have chosen to upload examples to a named graph composed of the SPARQL endpoint web address and the \u201c.well-known/sparql-examples\u201d suffix. Examples can then be assigned a unique identifier, more precisely, an Internationalized Resource Identifier (IRI), which may also be resolvable, i.e. allow access to the query using standard HTTP(S) for resource maintainers that want to associate specific webpages to the queries. For example, the query that retrieves all taxa over the UniProt SPARQL endpoint can be accessed at https://sparql.uniprot.org/.well-known/sparql-examples/1.\nOnce uploaded in the SPARQL endpoint, the examples themselves can be processed (for example, to search for relevant examples on a given topic) through simple SPARQL queries. Listing 2 shows how to retrieve example question-query pairs where the question contains the keyword \"species\u201d from a given endpoint. The following query can be directly executed, for example, over the UniProt or the Bgee\u00ba SPARQL endpoint:"}, {"title": "Applications", "content": "In this section, we provide several applications facilitated by the collection of example queries, such as:\nAutomated query testing across SIB endpoints.\n\u2022\n\u2022 Uniform query search and editing with a reusable SPARQL editor.\n\u2022 Automatically generating graph visualizations of queries.\n\u2022 Searching for scientific questions in the Bio-Query template interface.\nAll the applications can be deployed across the entire collection of questions and queries, owing to the standardized representation of examples that we have designed. The applications can be reused over any question-query collections that follow the approach described in the \"Methodology\" Section."}, {"title": "Tooling to help maintain SPARQL query examples", "content": "To help teams maintain their own SPARQL query examples, we provide a set of tools in a separate GitHub repository\u00b9\u00b3. This includes testing for continuous integration, visualizations to\nhelp understand the queries, and a set of automated fixes that can remove common errors. We provide details on each of these components in the following sections."}, {"title": "Automated testing", "content": "Each SPARQL query is tested for syntactical correctness using two different parsers: the first provided by the Apache Jena\u00b9\u2074 project, and a second by the eclipse RDF4j\u00b9\u2075 project. The metadata about the query is tested using a set of SHACL rules with the RDF4j implementation of SHACL. These rules check if the structure of the metadata around the query matches the expected format, avoiding errors in the contributed examples. For example, the presence of all mandatory fields (query ID, query type, comment, target) is checked. Furthermore, the rules check if the query matches the metadata-for example, that all queries declared to be federated queries actually contain a SERVICE clause in the query text.\nOptionally, we can also execute all queries on their remote target endpoints. We chose to only check that the queries retrieve at least one result, by programmatically changing the queries. More specifically, we add a limit which restricts the queries to only retrieve one result (i.e., a LIMIT\u00b9\u2076 clause with a value of 1). This avoids introducing non-essential load on the tested SPARQL servers, since some queries would result in long-running times or high computational load. Finally, all SPARQL endpoints used in federated queries are tested to be alive and responsive to SPARQL queries. These tests are written in Java using JUnit 5 Jupiter\u00b9\u2077 as the test framework. Tests that do not require remote SPARQL endpoints are deployed as GitHub actions that run on each push to the shared git repository, ensuring the validity of newly contributed queries. Tests that require network or that access the remote SPARQL endpoints are only run on demand by the query writers and can be used as regression tests when changes are made to the target repositories (i.e., to ensure that existing queries are still valid). Many of the query examples are relatively complex and would therefore add significant traffic to the remote endpoints if executed frequently. We thus only run these tests on demand."}, {"title": "Enabling query visualization", "content": "Complex SPARQL queries are difficult to follow for the majority of users. To assist in understanding the existing question-query examples, we automatically generate Markdown\u00b9\u2078 files with visual graphs and descriptions for each example query. To visualize these, we use Mermaid\u00b9\u2079, a diagramming and charting tool inspired on the Markdown text syntax. Namely, we automatically generate for each example query a corresponding visual graph in Mermaid. The generated markdown files can be displayed as GitHub pages\u00b2\u2070 or as Web pages in general (i.e., rendered HTML\u00b2\u00b9). These Web pages can be then accessible, for example, via the web address used as their IRI. To facilitate the findability and understanding of the examples, we make the full collection, including Mermaid visualizations for each query, available online in GitHub Pages 22.\nFixing query examples to be fully compliant with SPARQL 1.1\nThere are a number of query engines that have extended SPARQL 1.1 in non-standard ways. We implemented a module that attempts to \u201cfix\u201d these queries to be fully compliant with the SPARQL 1.1 specification. For example, Anzo and Blazegraph have implemented an extension for named subqueries. This named subquery extension is widely used in the Wikidata community. Nevertheless, these queries can often be semantically rewritten by following the\nSPARQL 1.1 specification. More precisely, our \u201cfixer\u201d approach parses the queries with the Blazegraph SPARQL parser and browses specific abstract syntax trees to look for such named subqueries and replace them with SPARQL 1.1 standard-compliant subqueries.\nAnother common mistake is that query examples might miss the prefix declaration required to make a query run. We store the common prefixes used in our resources, with an option for project specific prefixes, we can easily add missing ones to a query without further user interaction."}, {"title": "Query editing and searching", "content": "We extend the widely used Yasgui SPARQL environment (Rietveld & Hoekstra, 2013) with a new section showing the queries in use. The code for this extension is available as a package at the npm software registry\u00b2\u00b3 and can be easily deployed by SPARQL endpoint maintainers using a single custom HTML element <sparql-editor/>. We welcome contributions to develop this further at https://github.com/sib-swiss/sparql-editor. The editor\u00b2\u2074 includes several useful features. It automatically pulls query examples from the endpoint and presents them to the user alongside the editor, so users can easily reuse these queries to write their own by editing or extending them. It also provides a precise autocomplete based on the VoID descriptions that summarize an endpoint content. Whenever available, these descriptions are retrieved by the editor that queries the endpoint itself. The main information of interest processed by the editor from the VoID descriptions are the classes and properties that connect them. The VoID description is directly extracted from the SPARQL endpoint that is used by our extended Yasgui environment, therefore accurately reflecting the actual data. This ensures that only properties that are indeed stated, applicable according to the schema and the known type of a variable are suggested in the autocomplete dropdown menu. For further details on the VoID description generator used, see the corresponding GitHub repository\u00b2\u2075."}, {"title": "Integration in Bio-Query", "content": "The standardized representation of example queries facilitates their use in applications downstream. As an example, we have also incorporated the collection as-is in the Bio-Query (A. C. Sima et al., 2019) interface available online\u00b2\u2076. Figure 4 shows an example question and query from the collection. The questions are automatically assigned to categories named according to the resource that contributed them. The collection can be found under the \u201cSIB Example SPARQL queries\u201d category in the public Bio-Query interface."}, {"title": "Challenges and Lessons Learned", "content": "Efforts towards standardization are always faced with challenges at multiple levels. We have found that, even with community agreement on a common metadata format, simple details, such as non-uniform HTTP response headers expected across the different endpoints, can still hamper the uniform processing of the metadata. Moreover, even the queries themselves\u2014although in principle portable across different triplestores\u2014are not always reusable across different implementations. This is because, as mentioned previously, some query engines have extended SPARQL 1.1 in non-standard ways. An example is the way that \u201cmagic triples\u201d are used to provide query execution hints\u00b2\u2077 in Blazegraph or AWS Neptune. These \"magic triples\u201d need to be removed when moving to a different SPARQL endpoint, as they would otherwise refer to triples that do not exist.\nAt a higher-level, one of the biggest challenges we still face is that the collection of questions and queries were designed as generic guidelines for users, and not to be consumed by machines directly. This poses a set of unique challenges also in reusing them for the purpose of training and evaluating machine learning algorithms to automatically translate questions into equivalent SPARQL queries. End-user questions are often more abstract than their corresponding SPARQL queries, because the information searched by the user is not explicitly defined in the question. For example, let us consider the question: \"What are the species available?\", in this question we do not know what is the exact information the user is looking for.\nIs the user searching for common names or scientific names of species, or both? Is the user looking for a specific list of species identifiers of a given taxonomy? On the other hand, in a SPARQL query we often define the exact information to be retrieved (i.e., query projection). This impacts the retrieved results and potentially their amount. Therefore, while for an end-user the exact projected variables do not seem to be an essential point in the translation of a question to a query, for automatically evaluating an ML system, the set of projected variables must be precisely defined in the question (e.g., \u201creturn only the species scientific names\u201d, \u201creturn only proteins and their associated gene names\u201d) as opposed to being loosely defined, which would hamper a precise evaluation. Some questions in the catalog have also been designed in sequence, such that the text of one question is a follow-up to a previous one (e.g., \u201cSame as previous but with P0ABU7 as a query\u201d). Unless consistently done so throughout the collection, such follow-up questions would need to be discarded or rewritten, given that training a system to understand follow-ups would require a completely different approach.\nFinally, an added challenge (common to all collections of question-query pairs) is that the SPARQL endpoints the queries target might change or not even be available anymore, rendering the queries harder to execute or reproduce. We acknowledge that ensuring long-term preservation of the corresponding data is a difficult task, however precisely due to this reason we cannot maintain dedicated copies of the datasets targeted by the collection of queries. Instead, we rely on the resource providers to provide a long-term solution, which can be, for example, automatically redirect the existing SPARQL endpoint link to a site where the data is available at least in an archived format, from where users can deploy it locally.\nNevertheless, in spite of these challenges, the portfolio of reusable services that we have developed and deployed across independently managed SIB endpoints attests to the fact that even small steps towards standardization can have an important impact.\""}, {"title": "Related Work", "content": "Many existing works have proposed collections of Natural Language questions and corresponding SPARQL queries, usually designed as benchmarks to facilitate the task of automatically translating questions to queries. However, existing collections suffer from shortcomings, which we mention below:\n1) Non-federated: to the best of our knowledge, existing peer-reviewed published benchmarks, either large or small scale, do not consider federated queries and are usually centered around a single endpoint. We can mention here the examples of (Dubey et al., 2019; Trivedi et al., 2017). Even multi-dataset collections usually effectively address one single endpoint at a time, such as (Dubey et al., 2019; Kosten et al., 2023; Trivedi et al., 2017). Consequently, existing KGQA systems also focus on simple, non-federated queries (Zafar et al., 2018), often solely over DBPedia or Wikidata (Diefenbach et al., 2018). The existing collections of federated SPARQL queries, such as (Aimonier-Davat et al., 2024; Saleem et al., 2018) are designed to merely evaluate SPARQL federation engines and do not include corresponding natural language questions.\n2) Non-representative: crafting a large-scale benchmark manually is an extremely costly endeavor, therefore large collections (see (Dubey et al., 2019)) have been generated semi-automatically, which does not ensure that the questions accurately represent the information needs of real users. In contrast, our collection has been collected over time by different KG maintainers across the SIB, where most examples reflect questions asked by real users of the data.\n3) Small scale: the higher-complexity, human-written question-query collections, such as the QALD series (Question Answering over Linked Data challenge updated each year, e.g. (Usbeck et al., 2023)), only include a small number of queries. In contrast, our dataset includes over 1,000 question-query pairs, all of which have been human-written and collected over time from the different KG maintainers.\n4) HAMAP as SPARQL (Bolleman et al., 2020) is also a collection of more than 4,000 queries for a specific goal, annotating proteins. However, these are not aimed at educating users to the capabilities of a system. Nor are they annotated with a text for what their purpose is.\n5) SPIN\u00b2\u2078, SPARQL Inference Notation, is a W3C member submission that informed some of the design decisions of SHACL. SPIN provides for SPARQL templates and has logic for describing arguments to queries. However, its focus on constraints lead its modeling logic to ignore the concept of using these queries to mark up examples.\n6) The Wikidata project has a media wiki template\u00b2\u2079 for SPARQL queries. However, this does not standardize the metadata about the queries, nor does it provide the necessary utilities to help users maintain their queries."}, {"title": "Conclusion", "content": "In this paper, we introduce a large collection of example questions and their corresponding SPARQL queries collected over time across the catalog of SIB KGs and beyond. Moreover, we have proposed a methodology to standardize the representation of these examples, facilitating their uniform processing by both users and services. Finally, we have presented a comprehensive portfolio of services that leverage the standardization of metadata across the collection of examples, including services for the automated testing, editing and visualizing of example queries. All of these are available open-source and can easily be adapted by other KG maintainers for their own purposes, provided that maintainers adopt our recommended approach.\nIn the future, we plan to extend the current methodology to support defining templates in the example queries, in other words, fields that can be dynamically filled by querying a dedicated API (e.g., a SPARQL query). This would be a generalization of our current format, avoiding hard-coding literals in the example queries. We also plan to deploy an LLM-based solution to\nautomatically translate user questions in natural language to machine-readable SPARQL queries, leveraging the collection presented here.\nWe encourage the community to contribute to our proposed metadata specification, towards FAIRer Knowledge Graphs, which will pave the way for improved Semantic Web Tools. The advent of new technologies, such as Large Language Models, provide an opportunity for enabling a wider audience, including researchers and the public at large, to fully benefit from the wealth of interconnected data available in distributed KGs. We believe that a set of representative metadata standards for KGs, including the SPARQL examples metadata proposed here, represent an important and pragmatic step forward in this direction."}]}