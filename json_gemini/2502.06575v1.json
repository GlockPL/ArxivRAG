{"title": "Predictive Red Teaming: Breaking Policies Without Breaking Robots", "authors": ["Anirudha Majumdar", "Mohit Sharma", "Dmitry Kalashnikov", "Sumeet Singh", "Pierre Sermanet", "Vikas Sindhwani"], "abstract": "Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations. These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations. We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios. In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations. Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates). We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2-7x.", "sections": [{"title": "1. Introduction", "content": "Is it possible to expose the vulnerabilities of a given robot policy with respect to changes in environmental factors such as lighting, visual distractors, and object placement without performing hardware evaluations in these scenarios? As we seek to deploy robots in environments with ever-increasing complexity, it becomes imperative to develop scalable methods for predicting how well they will generalize when faced with unseen scenarios. Performing hardware evaluations to discover vulnerabilities - which can depend in surprising ways on the specifics of policy training and architecture - is often prohibitively expensive to set up and execute, especially when the goal is to test the limits of safe deployment in a sufficiently diverse set of scenarios.\nAs an example, consider a visuomotor diffusion policy [1] trained to perform pick-and-place tasks via behavior cloning (Fig. 1). The policy is trained with a large dataset: over 3K+ demonstrations with varied objects, locations, and visual distractors. Will the policy generalize well to a change in the height of the table by a few centimeters (as one may plausibly predict due to the variations in 2D object locations in the training dataset) compared to when a human is standing closer to the table than seen during training? If so, what is the absolute degradation of the success rate in each case? As it turns out, the above prediction is incorrect: the success rate of the policy degrades from ~ 65% under nominal conditions to ~ 10% by changing the table height, and remains roughly constant with a human close to the table. Predicting the relative and absolute impact of other factors (e.g., lighting, table backgrounds, object distractors; Fig. 2) can be even more challenging.\nContribution 1 (Predictive Red Teaming). We introduce and formalize the problem of predictive red teaming: discovering vulnerabilities of a given policy with respect to changes in environmental factors, and predicting the (relative or absolute) degradation in performance without performing hardware evaluations in off-nominal scenarios.\nThe ability to perform predictive red teaming has a number of important consequences. First, it enables targeted deployment: by understanding the envelope of conditions that will yield satisfactory performance, we can choose where the policy is deployed. Second, it enables policy comparison: knowing the relative vulnerabilities of different policies allows us to select one that is more likely to meet deployment needs. Third, it enables targeted data collection: if we know that certain environmental conditions degrade performance more than others, we can re-train the policy with additional data from the adverse conditions in order to help patch vulnerabilities.\nContribution 2 (RoboART). We introduce RoboART\u2014 robotics automated red teaming (ART) - an approach to predictive red teaming for visuomotor policies based on generative image editing and anomaly detection.\nThe pipeline for RoboART has two main steps: edit and predict (Fig. 1). First, we use automated image editing tools [2\u20135] to modify a set of nominal RGB observations by varying different factors of interest (e.g., lighting, distractors, object locations) in a fine-grained and realistic manner via language instructions (e.g., \u201cadd a person close to the table\"; Fig. 1). The second step is to predict the degradation in performance induced by each environmental factor using anomaly detection. Specifically, we find that a simple anomaly detector that computes distances in policy embedding space between edited observations and a set of nominal observations (with an anomaly threshold computed using conformal prediction [6]) is surprisingly predictive of both relative and absolute performance degradation.\nContribution 3 (Demonstration for visuomotor diffusion policies). We evaluate RoboART using 500+ hardware experiments that vary twelve environmental factors for two visuomotor diffusion policies with significantly different architectures.\""}, {"title": "2. Related Work", "content": "Red teaming. The concept of red teaming originated in the military realm, where a team posing as the enemy tries to find vulnerabilities of a military plan [7]. In recent years, the practice of red teaming has been adopted for finding vulnerabilities of large language models (LLMs) in terms of bias, misuse, and other harmful behavior [8\u201312]. While red teaming for LLMs was initially performed by human evaluators, this limits the coverage of possible issues that can be discovered. As a result, recent work has sought to partially automate the process of red teaming [13\u201318], e.g., by using LLMs themselves to discover vulnerabilities.\nWhile there is a growing literature on red teaming for vision-language models [19, 20] and text-to-image generative models [21, 22], red teaming for robotics is still nascent. Recent work has considered embodied red teaming for finding flaws in language-conditioned robotic foundation models [23]. Specifically, [23] focuses on instruction generalization: how well does a policy perform when faced with novel language instructions? As such, all evaluations in [23] are performed in simulation. Related work has also considered jailbreaking LLM-powered robots [24], i.e., finding adversarial prompts that override safety guardrails and cause robots to perform harmful actions. In contrast to [23, 24], our focus is on finding environmental factors (e.g., background colors, lighting, object locations) that degrade the performance of a given policy without performing hardware evaluations in off-nominal scenarios. The work in [25] uses simulation to assess the generalization"}, {"title": "3. Problem: Predictive Red Teaming", "content": "We formally introduce the problem of predictive red teaming: exposing vulnerabilities of a given policy with respect to environmental factors such as lighting, visual distractors, and object locations, and predicting their impact on performance without performing any hardware evaluations in these off-nominal scenarios.\nNominal scenarios. In each episode, the robot is deployed in a scenario \u03be, which is defined as a partially observable Markov decision process (POMDP) initialized in a particular state. Let Dnom be a distribution over scenarios that captures nominal variations in all environmental factors (e.g., objects that the robot may encounter, lighting conditions, background colors, etc.) and tasks (via the reward function). We do not assume knowledge of Dnom, except a dataset Snom of observations collected from nominal scenarios \u03be ~ Dnom.\nInputs to the red team. The red team is provided a deterministic or stochastic policy \u03c0 that maps observations o \u2208 O to actions at \u2208 A, along with the dataset Snom of nominal observations. Our focus"}, {"title": "Goal: predictive red teaming.", "content": "The red team's goal is to expose vulnerabilities of \u03c0 with respect to various environmental factors f \u2208 F chosen by the red team. These factors may be arbitrarily fine-grained, e.g., the introduction of a particular distractor or a specific change to the table color. Formally, let Df be a distribution of scenarios where a factor \u0192 has changed relative to the nominal distribution Dnom. Let Rom be the expected reward of \u03c0 for scenarios \u03be ~ Dnom, and let R be the expected reward for Df. For simplicity, we will assume henceforth that rewards are bounded in [0, 1]. Knowing Rom, we consider two problems: (1) rank the factors \u0192 \u2208 F by performance degradation, and (2) predict the absolute performance R, Vf \u2208 F. The former problem is important for targeted data collection, while the latter helps understand the envelope of acceptable performance."}, {"title": "4. RoboART: Predictive Red Teaming via Image Editing and Anomaly Detection", "content": "We introduce RoboART (Robotics Auto-Red-Teaming): a method for predictive red teaming using generative image editing and anomaly detection. We focus on visuomotor policies that rely primarily on RGB image observations. Our approach has two main steps, which are illustrated in Fig. 1. First, we use generative image editing tools to modify the nominal observations in Sval (Sec. 3) to reflect changes in various factors of interest (e.g., background, lighting, distractor objects). For each factor, we then predict the performance degradation of the policy using anomaly detection. We describe each of these steps below."}, {"title": "4.1. Generative Image Editing", "content": "Selection of environmental factors. The red team first selects a set F of environmental factors that have the potential to degrade the performance of the given policy \u03c0. This set can be arbitrarily fine-grained in its contents (e.g., specific lighting conditions, distractor objects, background colors, etc.). The specific factors of interest will depend on the deployment needs of the policy and plausible environmental changes that the robot may encounter.\nGenerating edited observations. For each factor \u0192 \u2208 F, we modify observations in the nominal set Sval to reflect a change in f. We leverage state-of-the-art generative image editing tools, which have the capacity to take detailed language instructions as input in order to produce realistic and globally consistent edits. In this work, we specifically utilize the Imagen 3 diffusion model [2], which has been trained to perform language-prompted image editing tasks such as inpainting, outpainting, and colorization.\nAs an example, consider an edit that adds a novel object to the scene. Fig. 3 illustrates the prompt used for this edit, along with examples of the original and edited images. For robots with multiple cameras (e.g., a wrist camera in addition to an overhead camera), we edit each observation independently with the same prompt. Fig. 3 shows the original and edited wrist camera images for the manipulator from Fig. 1. The image editing model is able to render the desired object in a realistic manner that maintains per-view global consistency in lighting, shadows, and overall composition of the scene (see Sec. 6.1 for a discussion of multi-view consistency).\nIn addition to adding novel objects to the scene, state-of-the-art image editing models allow us to"}, {"title": "VLM critic.", "content": "Diffusion-based image editing models can generate multiple variations of edited images given the same input image and prompt. These variations often differ in terms of their quality and adherence to the prompt. In order to ensure that the edited observations accurately reflect the desired change in the environmental factor f, we generate a batch of four edited images per input, and utilize a vision-language model (VLM) as a critic. As shown in Fig. 4, we prompt the VLM with the original and edited images, and ask it to judge if any of the options accurately reflect the desired change; if so, the VLM is tasked with choosing the best one (if not, we simply discard the observation from our set). The full prompt for the VLM \u2014 which involves chain-of-thought reasoning \u2014 is provided in Appendix B. We use the Gemini Pro 1.5 VLM [9] for our experiments."}, {"title": "4.2. Predicting Performance via Anomaly Detection", "content": "At the end of the image editing process, the red team has a set Sf of edited observations corresponding to each environmental factor f \u2208 F. The second key component of RoboART (Fig. 1) uses Sf to predict the performance degradation induced by each factor f. Our key idea is to utilize techniques from anomaly detection: for each observation in Sf, we quantify how \u201cclose\" it is to nominal observations in Snom from the perspective of the policy \u03c0. If this distance is above a threshold computed using conformal prediction [6], the observation is flagged as an anomaly. The primary hypothesis is that one can define such a policy-specific anomaly detector that predicts performance degradation:\n$R\\approx 1-\\alpha_f,$\nwhere R is the expected reward under factor f (Sec. 3) and \u03b1 is the anomaly rate for f, i.e., the proportion of edited observations in Sf flagged as anomalous according to a threshold chosen to ensure Rom \u2248 1 \u2212 \u03b1nom (where \u03b1nom is the proportion of nominal observations flagged as anomalous)."}, {"title": "Anomaly detection.", "content": "Next, we further describe how to compute the anomaly rate \u03b1 for each factor f using the edited observations Sf. In this work, we utilize policy embedding distances as a method for quantifying how far from nominal a given observation is. This choice is motivated by the prior success of embedding-based methods in anomaly detection (see, e.g., [38, 66]) and the simplicity of implementation. Let \u03a6\u03c0(o) be a latent representation produced by the policy n for a given observation o. For policies directly parameterized using a neural network, a common choice is to use the output of an intermediate layer of the network. In our experiments in Sec. 5, we utilize policies parameterized using diffusion models. In this setting, we utilize the context vector provided to the denoising process as our latent representation; see Appendix C for more details. Using \u03a6\u03c0, we can define a policy-specific anomaly score s\u03c0(o, Snom) that quantifies how far from nominal the observation o is. A simple choice is to define s\u012b as the nearest-neighbor cosine distance between the embedding \u03a6\u03c0(o) and the embeddings computed for the nominal observations in Snom:\n$\\Sigma_\\pi (o, S_{nom}):= min\\lbrace 1 - \\frac{\\Phi_{\\pi}(o)\\cdot \\Phi_{\\pi}(o_{nom})}{\\| \\Phi_{\\pi}(o)\\|\\| \\Phi_{\\pi}(o_{nom}) \\|} | o_i \\in S_{nom} \\rbrace.$\nA more general variant that we use in our experiments is to compute the mean of the k-nearest neighbor cosine distances. Intuitively, this anomaly score quantifies how dissimilar a given observation is compared to similar training observations from the perspective of the policy."}, {"title": "Anomaly threshold.", "content": "For each factor f \u2208 F, we compute the anomaly score for all edited observations o \u2208 Sf. The anomaly rate for a factor f is then defined as the proportion of observations flagged as anomalous according to a threshold \u03c4:\n$\\alpha_f:= \\frac{\\{ o \\in S_f | S_{\\pi}(o, S_{nom}) > \\tau \\}}{\\|S_f\\|} $\nThe anomaly threshold \u03c4 is chosen to ensure that \u03b1nom (the anomaly rate for nominal observations) predicts the nominal success rate Rom of the policy: Rom \u2248 1 - \u03b1nom. Given access to a validation set Sval with nval nominal observations, one can simply choose t such that the proportion of these flagged as anomalous is 1 \u2013 Rom. A more sophisticated approach uses conformal prediction [6]:\n$\\tau := quantile_{\\frac{\\lfloor (n_{val}+1)R_{om} \\rfloor }{n_{val}}}(\\{ S_{\\pi}(o, S_{nom}) | o \\in S_{val} \\}),$\nwhich chooses t as the [(nval + 1)Rom]/nval empirical quantile of the set of anomaly scores for the"}, {"title": "5. Experiments", "content": "We evaluate our framework using 500+ hardware trials that vary twelve environmental factors (Fig. 2) for two visuomotor diffusion policies with significantly different architectures. These experiments seek to investigate the following questions:\n1. How well does RoboART identify vulnerabilities and predict policy performance when relevant environmental factors are varied?\n2. How effective is RoboART in enabling policy improvement via targeted data collection?\n3. How good of a proxy is anomaly detection for performance degradation in different environmental conditions?\nHardware setup. Fig. 1 visualizes our hardware platform: a bimanual manipulator with two Kuka IIWA robotic arms (our experiments only utilize the left arm). We use a Weiss gripper to interact with the objects in the environment. For sensing, we use a dual camera setup with a fixed overhead camera and another camera mounted on the left wrist.\nTraining data. We use a trajectory optimization-based motion planner to automatically collect a set of training data consisting of 3K+ demonstrations for grasping objects. These demonstrations are collected in nominal conditions, i.e., with fixed lighting, with a fixed pink background on a table, and an object set that consists of blocks, plush toys, small cans, and artificial fruits. Additional details on the data collection pipeline are provided in Appendix C. We highlight that the chosen task (grasping) is relatively easy to learn, and hence makes the problem of red teaming more challenging; trained policies demonstrate a nontrivial degree of generalization, but are also vulnerable in ways that are hard to intuit.\nPolicies. We consider two policies that vary significantly in their overall architecture. The first policy, \u03c0hyb, uses a hybrid policy architecture inspired by [68], which aims to combine the benefits of"}, {"title": "Environmental factors.", "content": "We choose a set F of twelve environmental factors that reflect common vulnerabilities of visuomotor policies trained via behavior cloning. These are shown in Fig. 2, and include: (1-3) three changes to the lighting conditions (red, green, blue), (4\u20136) three changes to the color (red, green, blue) of the table background on which objects are placed, (7\u201310) four distractor objects (black and white trash can, laptop, candle) that partially occlude other objects, (11) a distractor in the form of a person close to the table, and (12) a change to the height of the table (which changes the location of objects relative to the overhead camera). In order to evaluate the predictions made by RoboART, we execute both policies in 20+ episodes for each of the twelve factors; this allows us to estimate the corresponding success rates Rhy, Rhyb and Rdfn, Vf \u2208 F. The subsequent results thus include 500+ hardware trials."}, {"title": "5.1. How accurately does RoboART identify vulnerabilities and predict policy performance?", "content": "We first evaluate how well RoboART predicts the performance degradation induced by each of the twelve environmental factors for the different policies. We utilize two metrics to evaluate RoboART, which correspond to the two versions of predictive red teaming described in Sec. 3:\n1. Spearman rank correlation [69]: this is a value p \u2208 [-1,1] which measures how accurately RoboART ranks the different factors by performance degradation.\n2. Average prediction error: measures how accurately RoboART predicts the absolute success rates for the different factors by computing \\frac{1}{\\|F\\|} \\sum_{f \\in F} |R_f^h - R_{f,pred}^h|.$\nImplementation. In order to make predictions using RoboART, we generate a set Sf of 100 edited observations for each factor f using first time-step observations from a held-out portion of training episodes. Examples of edits and complete prompts are provided in Fig. 1 and Appendix A. We compute the resulting anomaly rates \u03b1 using S\u0192 for each policy as described in Alg. 1. We take the anomaly score s (0, Snom) to be the mean of the k-nearest neighbor cosine distances (in the respective policy embedding space) to a set Snom, which is chosen to be a subset of first-time-step observations from the training episodes."}, {"title": "5.2. How effective is RoboART in enabling policy improvement via targeted data collection?", "content": "Our second set of experiments seeks to evaluate how well predictions from RoboART enable policy improvement via targeted data collection. To this end, we collect additional demonstration data for hyb with the three environmental factors that RoboART predicts the highest performance degradation for: blue lighting, change in the table height, and blue table background. We collect around 1 hour of training data (\u2248 100 trajectories) under each of these off-nominal settings. We then co-finetune our initial learned policy thyb on the older data combined with the new small amount of off-nominal data. During co-finetuning, we ensure that each mini-batch consists of 80% of the original data mixture and 20% from the new off-nominal data. We co-finetune the policy with a reduced learning rate (5e-6) for a total of 20K steps.\nThe fine-tuned policy \u03c0hyftb is evaluated in nominal conditions, the three conditions for which we collected data, and also the other background and lighting conditions. Videos of \u03c0hyftb are available on the project website. Fig. 6 shows the results. We observe improved performance in nominal conditions and a 2\u20137x improvement in off-nominal conditions under which training data was collected. Interestingly, the targeted data collection also yields cross-domain generalization: the performance of the policy is improved by 2\u20135x even for background and lighting conditions where we did not collect additional data. This highlights the benefits of targeting data collection towards adverse scenarios via predictive red teaming."}, {"title": "5.3. How accurately does anomaly detection predict performance degradation?", "content": "Our final set of experiments seeks to evaluate the anomaly detection component of RoboART in isolation from the image editing pipeline. To this end, instead of executing the embedding-based anomaly detector on the set Sf of edited observations (as described in Algorithm 1), we execute the detector on the set sreal composed of real robot observations collected from the first time step of the 20+ episodes where the factor f is varied. We then compute the corresponding anomaly rates"}, {"title": "6. Conclusions", "content": "Summary. We have introduced the problem of predictive red teaming: given access to observations from nominal scenarios, discovering vulnerabilities of a policy with respect to unseen changes in environmental factors and predicting the resulting performance degradation. Our approach to predictive red teaming - Robotics Auto Red Teaming (RoboART) - modifies nominal observations via generative image editing to reflect changes in environmental factors of interest, and then uses a policy embedding-based anomaly detector to predict performance degradation. Experiments across 500+ trials for visuomotor diffusion policies demonstrate RoboART's ability to (i) identify environmental factors that significantly impact performance, (ii) predict the relative and absolute impact of factors, and (iii) enable policy improvement via targeted data collection."}, {"title": "6.1. Limitations and Future Work", "content": "We discuss limitations of our approach and promising future directions that may address them.\nEdit-to-real gap. While state-of-the-art image editing tools are capable of producing realistic edits (especially with careful prompting), there are still gaps in realism for certain environmental factors. For example, edits that reflect lighting changes (Fig. 1) do not modify the shadows of objects as real lighting changes do. We expect that our method will benefit from the significant investments in improving image editing models for commercial applications. Beyond single-view realism, a more challenging limitation is ensuring multi-view consistency. As seen in Fig. 3, edited observations from the overhead and wrist cameras do not represent a consistent geometry for the new object. One exciting possibility is to utilize recent 3D scene editing tools based on Gaussian Splatting [70, 71] that allow for edits with multi-view consistency. Scene editing may also allow us to go beyond RGB observations and edit depth channels.\nAnomaly-to-failure gap. Our approach utilizes the anomaly rate as a predictor for performance degradation. However, as seen in Sec. 5.3, these predictions are not perfectly accurate. One avenue for future work is to perform edits on observations from multiple time-steps within each episode, and to compute anomaly rates based on these sequences (rather than only utilizing the first time-step from episodes, as we currently do). We are also interested in exploring other methods from the vast literature on anomaly detection to identify techniques that may serve as better predictors of performance degradation (see, e.g., [72] for a recent empirical study)."}, {"title": "Hidden environmental factors.", "content": "An important limitation of RoboART is that it requires changes in environmental factors to be reflected in visual observations of the robot. As such, RoboART will not identify vulnerabilities with changes that are completely hidden (e.g., changing the mass of objects without changing their visual appearance). In such cases, predictive red teaming via simulation is a promising avenue, but requires bridging the sim-to-real gap, which is typically very significant for RGB observations and may be even more pronounced when simulating unseen off-nominal scenarios."}, {"title": "Multi-round predictive red teaming.", "content": "RoboART currently chooses a single set F of environmental factors at the beginning of predictive red teaming. A more sophisticated approach could iteratively explore the space of environmental factors: choose an initial set F, make predictions for these, and expand the set iteratively to include factors that are similar to ones that were predicted to yield poor performance."}, {"title": "4.1. Generative Image Editing", "content": "As we seek to deploy robots in challenging real-world applications, it is essential that we develop scalable methods for predicting the limits of their performance. We hope that formalizing the problem of predictive red teaming and providing a baseline in the form of RoboART spurs further research along this underexplored direction."}, {"title": "A. Image Editing: Examples and Prompts", "content": "Examples of different edits applied to both the overhead camera and the wrist camera are shown in Figure 7. Below, we provide complete prompts used to generate the edited observations for each environmental condition."}, {"title": "Full prompts for edits", "content": "Person:\nAdd a person to the image. Specifically, add a person behind the blue platform, realistically interacting with the platform and fitting seamlessly into the existing environment. Preserve all other aspects of the image, including the different objects on the mat, other background elements, and the overall composition. The lighting should remain consistent. The new person should be realistically rendered with all details of the person including their face, clothing, and any other"}, {"title": "Large distractor (e.g., trash cans):", "content": "Add a large <target color><target object> at the edge of the pink mat, so that it doesn't modify or occlude any of the objects on the pink mat. Specifically, add a <target color> <target object > that is larger than any objects at the edge of the pink mat, fitting realistically and seamlessly into the existing scene. Preserve all details of the objects on the mat, their poses, and the overall composition of the image. The <target color> <target object> should be realistically and exquisitely rendered and should not occlude any of the objects on the pink mat. The lighting should remain consistent. Only the <target color> <target object> at the edge of the pink mat should be added."}, {"title": "Small distractor (e.g., candle):", "content": "Modify image <image> as described below: Add a small scented candle on the pink mat, so that it doesn't modify or occlude any of the objects on the mat. Specifically, add a scented candle with roughly the same size as the objects on the pink mat, fitting realistically and seamlessly into the existing scene. Preserve all details of the composition of the image. The scented candle should be realistically and exquisitely rendered and should not occlude any of the objects on the pink mat. The lighting should remain consistent. Only the scented candle should be added."}, {"title": "Background color:", "content": "Modify image <image> as described below: change the color of the pink mat that objects are on to <target color>.Preserve the different objects on the mat, and all other aspects of the image including the lighting and the overall composition."}, {"title": "Lighting (overhead camera):", "content": "Modify image <image> as described below: Colorize the bottom half of the image with an extremely intense <target color> hue. Preserve the existing composition, details, and textures of the objects in the scene, including the ones on the pink mat and the background. Only the shadows and color palette should be altered to reflect an extremely intense <target color> light, maintaining the style of the original image. The overall lighting should remain consistent, with shadows and highlights adjusted to match the new color palette. Make sure that the hue for the bottom half of the image is changed to intense <target color>, including for the objects on the table."}, {"title": "Lighting (wrist camera):", "content": "Modify image <image> as described below: Colorize the entire image with an extremely intense <target color> color tone. Preserve the existing composition, details, and textures of the objects in the scene, including the ones on the pink mat and the background. Only the shadows and color palette should be altered to reflect an extremely intense <target color> light, maintaining the style of the original image. The overall lighting should remain consistent, with shadows and highlights adjusted to match the new color palette. Make sure that the color for the entire image is changed to intense <target color>."}, {"title": "Table height:", "content": "Change the color of the pink mat to <target color>. Preserve all other aspects of the image, including the different objects on the mat, the lighting, and the overall composition. Only the color of the pink mat should be altered to <target color>, maintaining its shape, size, and position. [We then apply a zoom to the portion of the image that contains the table in order to simulate a change in the height of the table."}, {"title": "B. Filtering Edits with a Vision-Language Model", "content": "For each nominal observation, we generate a batch of four candidate edited observations via the image editing model. We then use a vision-language model (VLM) in order to judge if any of the options accurately reflect the desired change; if so, the VLM is tasked with choosing the best one (if not, we simply discard the observation from our set). The full prompt for the VLM \u2014 which involves chain-of-thought reasoning \u2014 is provided below. We use the Gemini Pro 1.5 VLM [9] for our experiments."}, {"title": "Prompt for filtering edits with a VLM", "content": "Here is the original image I have: <original image>. Do any of Image 0: <Image 0>, Image 1: <Image 1>, Image 2: <Image 2>, or Image 3 <Image 3> accurately reflect an edited version of the original image with the instruction \u201c<short edit instruction>\"? Give your reasoning and then answer with a True or False. If True, provide the index (0,1,2,3) of the best image.\nThe variable <short edit instruction> contains a shortened version (e.g., \u201cChange the color of the pink mat to <target color>\") of the full prompt provided to the image editing model. We find that providing the full prompt (instead of a shortened version) can lead the VLM to be overly critical and filter out many acceptable edits."}, {"title": "C. Training and Policy Details", "content": "Training data collection. For training our policies, we collect 3K+ demonstrations for grasping tasks on the hardware. Specifically, we use trajectory optimization-based motion planners to automatically collect a large set of training data. Our data collection pipeline uses the overhead camera to obtain a 3D point cloud of the scene. We segment the point cloud into multiple objects and randomly choose different objects to pick using the left arm. We use automated success detection to segment these trajectories. For each episode, we further automatically annotate keypoints for the object the policy should grasp; these are used as additional context for the robot policy in addition to camera and proprioceptive observations. All demonstrations are collected in nominal conditions, i.e., with fixed lighting, with a fixed pink background on a table, and an object set that consists of blocks, plush toys, small cans, and artificial fruits."}, {"title": "C.2. Hybrid Policy Architecture", "content": "Hybrid policy. We consider two policies that vary significantly in their overall architecture. The first policy rhyb uses a hybrid policy architecture inspired by [68], which aims to utilize the benefits of trajectory optimization-based approaches for free space planning together with the reactive nature of closed-loop visuomotor diffusion policies. We achieve this by using two separate heads in our policy architecture (see Fig. 8), where each head represents an action mode. These two different action modes correspond to:\n1. a waypoint action mode which outputs a single waypoint (w \u2208 SE(3)), and\n2. a trajectory action mode which outputs a dense sequence of robot joint angles (qi \u2208 R14).\nIn addition to these policy heads we also output a mode selection scalar which defines which action"}, {"title": "Vision encoder.", "content": "Our policy architecture uses pre-trained ViT [73] encoders to encode the image observations from each image. We use separate models for each camera observation (overhead and wrist). We reduce the number of tokens from each ViT using a TokenLearner layer [74]. We encode proprioceptive features using a multi-layer perceptron (MLP) with a single hidden layer."}, {"title": "Instruction encoder.", "content": "The robot is instructed to grasp a target object using semantic keypoints. Specifically, we extract a small patch (64\u00d764) from the overhead camera view around a keypoint that is selected by the robot operator. We encode this patch using a small coordinate convolution-based neural network. Since we train a multi-skill policy we encode the skill that the robot needs to perform using a continuous embedding. The semantic keypoint representation is concatenated with the skill embedding to form the instruction tokens."}, {"title": "Context Fuser.", "content": "The observation tokens, the instruction tokens and proprioceptive tokens are fused together using a context fuser which uses a stack of self-attention based transformer layers. We also additionally add a readout token, which we refer to as the waypoint-mode token. At the end of the context fuser we get a set of fused observation-instruction embeddings as well as the embedding for the readout token. The observation-instruction embeddings are used to predict the trajectory and"}, {"title": "C.3. Diffusion Policy.", "content": "Our diffusion policy architecture adfn uses a standard diffusion policy [1, 76] to directly output the joint angles to control the robot. Our base architecture is similar to hyb (described above) wherein we only use the trajectory mode, i.e., only the trajectory diffusion head is used to predict robot trajectories. The rest of the architecture including the vision encoders and the multi-modal instruction encoder are common between adfn and rhyb. However, unlike thyb, \u03c0dfn does not include a readout token (waypoint/mode token) within the context fuser."}, {"title": "C.4. Training and Inference Details", "content": "Table 3 shows the common set of hyper-parameters used to train each of our policies. We use a batch size of 256 during training. As shown in Figure 8 for the high dimensional image observations we use an additional token learner to reduce the number of image tokens. We use 128 tokens for each image observation. For our diffusion model we use a continuous time implementation based on [77", "77": "we use a second order Heun solver to solve the flow ODE. We use 30 ODE steps during inference. As shown in Table 3, we use an action horizon of 10. Since we collect our training data at 10Hz this corresponds to 1 second of robot motion. During inference, we open-loop rollout entire 10 steps before querying the policy again. During evaluation we use a maximum of 30 policy steps before we stop policy evaluation. For our targeted data collection experiment Section 5.2, we use a much smaller learning rate of 5e \u2013 6 and a linear warmup of 4"}]}