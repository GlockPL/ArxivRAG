{"title": "GENERATIVE MODELS FOR FINANCIAL TIME SERIES DATA: ENHANCING SIGNAL-TO-NOISE RATIO AND ADDRESSING DATA SCARCITY IN A-SHARE MARKET", "authors": ["Guangming CHE"], "abstract": "The financial industry is increasingly seeking robust methods to address the challenges posed by data scarcity and low signal-to-noise ratios, which limit the application of deep learning techniques in stock market analysis. This paper presents two innovative generative model-based approaches to synthesize stock data, specifically tailored for different scenarios within the A-share market in China. The first method, a sector-based synthesis approach, enhances the signal-to-noise ratio of stock data by classifying the characteristics of stocks from various sectors in China's A-share market. This method employs an Approximate Non-Local Total Variation algorithm to smooth the generated data, a bandpass filtering method based on Fourier Transform to eliminate noise, and Denoising Diffusion Implicit Models to accelerate sampling speed. The second method, a recursive stock data synthesis approach based on pattern recognition, is designed to synthesize data for stocks with short listing periods and limited comparable companies. It leverages pattern recognition techniques and Markov models to learn and generate variable-length stock sequences, while introducing a sub-time-level data augmentation method to alleviate data scarcity issues. We validate the effectiveness of these methods through extensive experiments on various datasets, including those from the main board, STAR Market, Growth Enterprise Market Board, Beijing Stock Exchange, NASDAQ, NYSE, and AMEX. The results demonstrate that our synthesized data not only improve the performance of predictive models but also enhance the signal-to-noise ratio of individual stock signals in price trading strategies. Furthermore, the introduction of sub-time-level data significantly improves the quality of synthesized data, particularly in scenarios with limited comparable companies and short listing periods. This research contributes to the financial data synthesis domain by providing new tools and techniques that can support financial analysis and high-frequency trading, offering valuable insights into the complex dynamics of the A-share market.", "sections": [{"title": "1 Introduction", "content": "Introduction\nThe realm of finance is rife with complexities and uncertainties that make the accurate prediction of financial instrument prices and returns a formidable task. Such predictions are essential for guiding investment decisions, optimizing asset allocation, and managing risk effectively. The accuracy of these predictions, however, is often hindered by the limitations in data quality and quantity, particularly in the stock market where issues like low signal-to-noise ratios and data homogeneity pose significant challenges to constructing high-precision predictive models [1].\nFinancial data is not only sensitive but also of high value, and its mishandling can lead to severe security risks for both companies and users [1]. The implementation of data privacy regulations has further exacerbated the challenges faced by the financial industry in acquiring and sharing data, resulting in information asymmetry and isolated data silos [1]. In response to these challenges, the advent of artificial intelligence and deep learning models offers innovative"}, {"title": "2 Background and Related Work", "content": "2.1 Financial Market Dynamics and Data Scarcity\nThe financial markets are intricate systems where the accurate prediction of asset prices and returns is crucial for investment strategies and risk management. However, the inherent volatility and unpredictability of these markets make accurate forecasting a significant challenge. A key issue in this context is the scarcity and low quality of financial data, which limits the application of deep learning techniques within the financial industry. Particularly in stock markets, the low signal-to-noise ratio and high data homogeneity hinder the construction of precise predictive models [1].\nThe sensitivity and high value of financial data also mean that any leakage or malicious manipulation can pose serious security risks to both financial institutions and their customers [1]. With the enforcement of data privacy regulations, the financial sector is facing increased difficulties in data acquisition and sharing, leading to information asymmetry and the creation of \"data silos\" [1]. These challenges have spurred the exploration of artificial intelligence techniques to generate synthetic financial data that can maintain the characteristics of original data, increase data diversity, protect privacy, and enhance the effectiveness of model training and prediction accuracy [2].\n2.2 Generative Models in Finance\nGenerative models, such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models, have shown great potential in generating synthetic data across various domains, including finance. These models have been used to create synthetic financial data that mimics the statistical properties of real-world market behavior and customer trading patterns [5, 6, 7].\nVAEs, introduced by Kingma et al. in 2013 [5], have been particularly successful in generating new data instances similar to the training data by learning to encode and decode data in a probabilistic manner. GANs, proposed by Goodfellow et al. in 2014 [6], have revolutionized the field of image generation by employing a competitive approach between a generator and a discriminator. Diffusion models, initially proposed by Sohl-Dickstein et al. in 2015 [8] and later improved by Ho and Jain in 2020 [7], have demonstrated their ability to generate high-quality samples by gradually reversing the noise injection process.\n2.3 Challenges in Financial Data Generation\nDespite the advancements in generative models, their application in financial data generation is still in its nascent stages. The complexity and dynamism of financial markets present numerous challenges. Financial data generation requires a deep understanding of market dynamics and the exploration of how to effectively integrate deep generative models with the characteristics of financial data to produce high-quality, practical synthetic data [4].\nMoreover, these models are often developed and trained on data from Western markets, such as the U.S. stock market, and may not fully account for the unique rules and characteristics of markets like China's A-share market. For instance, the distribution and regulatory differences between these markets can lead to synthetic data that does not conform to the rules and patterns of specific financial markets [9, 10]."}, {"title": "2.4 Financial Data Synthesis Approaches", "content": "To address these challenges, various financial institutions and scholars have proposed frameworks for generating synthetic financial data. These frameworks aim to create datasets that are statistically similar to real data but without revealing any entity information, thus supporting financial analysis and research while maintaining privacy constraints [9]. Academic research has also explored different variations of GANs to improve performance in financial data synthesis, including architectural variants and loss function variants [11, 12, 13].\nIn conclusion, the background and related work highlight the importance of addressing data scarcity and the unique challenges of financial data generation. The exploration of generative models in finance is still an active area of research, with significant potential for developing more sophisticated methods to generate synthetic data that can enhance the predictive power of financial models and strategies."}, {"title": "3 Score-based Generative Models", "content": "Score-based Generative Models (SGMs) are a type of self-supervised machine learning method used for generating new data samples. The core idea of SGMs is to learn the score function of an unknown data distribution Pdata(x), which is the gradient of the log-probability density function with respect to the data x,  x log p(x), to guide the generation of new samples. Once we have an estimate of this score function, we can start from a random data point and move it to a position with higher probability density using gradient ascent.\nIn practice, a neural network se is used to approximate the score  \u2207 log p(x), and the goal of training this neural network is to minimize the following objective function:\nL(0) = Epa pdata(x) [||Se(X) \u2013 \u2207 log Pdata(x)||2] = Epdata(x) tr (as(x)) + 1/2||se(x)||2.\t\t(1)\nAccording to Score Matching, L(0) can be written in the following form[14], and the derivation is detailed in Appendix A:\nL(0) = Epdata (x) [tx tr (Vase(x)) + 1/2||se(x)||2]\t(2)\nHowever, directly computing the tr(\u2207\u00e6se(x)) in the objective function requires a large amount of computational resources. Therefore, Vincent et al. proposed Denoising Score Matching (DSM) as a solution to avoid this computation[15]. DSM perturbs the data x with a given noise distribution qr(xx) and then attempts to estimate the score of the perturbed data distribution qr(x). At this point, the training goal becomes minimizing the following objective function:\n1/2Eqo (x) Pdata(x) [||Se(x) \u2013 \u2207 log qo (xx)||2].\t\t(3)\nIn some cases, the training goal of DSM is equivalent to the training goal of Denoising Autoencoders (DAE), which also follows a simple noise-adding and denoising process.\nThese models and methods can be unified through Stochastic Differential Equations (SDEs), which provide a framework for understanding how to gradually add noise from a known initial distribution and how to remove noise from a perturbed distribution to recover the original data distribution. This offers a powerful tool for designing and analyzing score-based generative models and enables different variants and extensions."}, {"title": "3.1 Score Matching with Langevin Dynamics", "content": "The process of Score Matching with Langevin Dynamics (SMLD) includes two parts. On the one hand, it uses denoising score matching to estimate the score of the perturbed data distribution; on the other hand, it uses Langevin dynamics to iteratively sample from the prior distribution.\nSong et al. proposed perturbing the data with multiple levels of noise and training a Noise Conditioned Score Network (NCSN)[16] to estimate the score corresponding to all noise levels. The perturbation method is defined as:\nqo(x|x) = N(x|x, \u03c3\u00b2\u0399),\nwhere I is the identity matrix. The probability distribution of the perturbed data is:\nqo(x) = [Pdata(x)N(\u00e6\\x, o\u00b2I)d\u00e6."}, {"title": "3.2 Denoising Diffusion Probabilistic Model", "content": "The Denoising Diffusion Probabilistic Model (DDPM)[7] can be regarded as a type of hierarchical Markov variational autoencoder[17]. Consider a noise sequence 0 < \u03b2\u2081 < \u03b22 < ... < \u03b2N < 1 and the forward noise process P(Xi|Xi\u22121) = N(xi|\u221a1 \u2212 \u03b2ixi\u22121, \u03b2iI). Define \u03b1\u2081 = \u03a0\u00b2=1(1 \u2013 Bj), then we have:\nPai (Xixo) = N(xi|\u221aaixo, (1 \u2013 \u03b1\u0390)\u0399).\t\t(4)\nSimilar to Score Matching with Langevin Dynamics (SMLD), the perturbed data distribution can be expressed as:\nPai (x) = [Pdata(x)po; (xx)dx.\t\t(5)\nHere, the noise scale is preset to satisfy Pan (x) ~ N(0, I), meaning that the final perturbed data is close to a standard Gaussian distribution. According to the reparameterization method[7], we can obtain the conditional distribution q(XtX0):\nq(xt|x0) = N (xt; \u221aatxo, (1 \u2212 at)I),\t\t(6)\nwhere at = \u03a0\u22121 ai and at 1 - Bt. Then, xt can be written as:\nxt = \u221aatxo + \u221a1 \u2013 \u03b1\u03c4\u03b5,\t\t(7)\nwhere \u20ac ~ N(0, I). The reverse denoising process can be written as:\nPo(Xi-1|xi) = N (Xi-1; 1/ \u221a1 - \u03b2i (Xi + \u03b2tsi(xi)), \u03b2iI)\t\t(8)\nThe training objective of DDPM is the sum of the weighted Evidence Lower Bound (ELBO), that is, to find the optimal parameters 0* to minimize the following loss function:\nN\n0* = arg min (1 - ai) Epdata(x) Epa; (x) [|[s(x, i) \u2013 V\u0101 log pa\u2081 (\u00c3|x) ||2] .\t\t(9)\ni=1\nwhere ai \u03a0\u00b2=1(1 \u2212 \u03b2;) is the cumulative product, representing the total noise influence from the initial data to the i-th step; pa\u2081 (xx) = N(x|\u221a\u221aa\u00bfx, (1 \u2013 a\u2081)I) is the conditional distribution of the perturbed data a given the original data x."}, {"title": "3.3 Unifying SMLD and DDPM from the Perspective of Stochastic Differential Equations", "content": "Based on the research by Song et al. [18], Score Matching with Langevin Dynamics (SMLD) and Denoising Diffusion Probabilistic Models (DDPM) can be unified under the perspective of Stochastic Differential Equations (SDEs). Let x(t)o be a stochastic diffusion process indexed by a continuous time variable t \u2208 [0,T]. Let po represent the true data distribution, and pr be a tractable prior distribution such that xo ~ po and x\u0442 ~ \u0440\u0442.\nUsing pt(x) to denote the probability density function of x(t), and pst (x(t) | x(s)) to represent the transition kernel from x(s) to x(t), where 0 < s < t < T. A stochastic differential equation can be used to represent this forward diffusion process:\ndx = f(x,t)dt + g(t)dw,\t\t(10)\nwhere f(x,t)dt is the drift term, representing the average trend of the system's change; g(t)dw is the diffusion term, representing the strength of random fluctuations; here, w is a standard Wiener process, and dw ~ N(0,I), meaning that dw follows a Gaussian distribution with mean zero and covariance matrix equal to the identity matrix I, representing unpredictable random noise. The synthetic data generation process is the reverse process of (3.11), which is also a stochastic differential equation:\ndx = [f(x,t) - g\u00b2(t)\u2207x log pt(x)] dt + g(t)d\u016b,\t\t(11)\nwhere w is the reverse-time Wiener process; \u2207\u00e6 log pt (x) is the score corresponding to the marginal distribution pt(x) at each t. This process starts from the initial data point xT ~ pr, gradually denoises, and finally generates a sample x0 close to the true data distribution po. In theory, if dt is small enough, we can gradually obtain x0 ~ po through (3.12). To estimate V\u00e6 log pt(x), the score of the marginal distribution, a score network se(x, t) is trained, with the objective function being:\nx(t)EtEx0Ext | x0 [|s0(x(t), t) \u2013 \u2207x(t) log p0t(x(t) | x(0))|2],\t\t(12)\nwhere \u5165 : [0, T] \u2192 R+ is a positive weight function; t ~ U[0, T], indicating that t is drawn from a uniform distribution U[0, T].\nDDPM can be seen as a discrete form of continuous-time SDEs. Typically, the discrete forward process for SMLD is:\nxi = xi \u2212 1 + \u221ao? \u2013 o?_\u2081zi \u2212 1, i = 1, \u2026, N.\t(13)\nAs N \u2192 \u221e, the discrete form xii = 1 becomes the continuous form\u00e6(t)t = 0\u00b9, and the continuous forward process can be written as:\ndx = d[0\u00b2 (t)]/ dt dw, \t\t(14)\nwhere f(x, t) = 0 and g(t) = [02]. This is called the Variance Exploding SDE (VE-SDE).\nFor DDPM, the discrete forward process is:\nxi = \u221a1 \u2013 \u03b2\u00bfx\u0456 \u2013 1 + \u221aBizi\u22121, i = 1,\u2026, N.\t\t(15)\nAs N\u2192\u221e, the continuous form of the DDPM forward process can be written as:\ndx = \u03b2(t)/2 xdt + \u221a\u03b2(t)dw, \t\t(16)\n2\nwhere f(x,t) = - \u03b2(t)/2x and g(t) = \u221a\u03b2(t). This is called the Variance Preserving SDE (VP-SDE).\nBy replacing f(x, t) and g(t) in (3.12) with those in (3.15) or (3.17), we can obtain the SDE form of the reverse process corresponding to SMLD or DDPM."}, {"title": "4 Methodology Design", "content": "The task of stock prediction is challenging, and the scarcity of data is a significant reason. To fully leverage the potential of machine learning models, ample and high-quality data is crucial. However, obtaining high-quality stock data within a specific target domain is often very difficult. In this study, we utilize diffusion models (DMs) to propose a new method for synthesizing A-share data-CS-Diffusion. This method generates additional data points to augment stock data, thereby overcoming the problem of data scarcity and enabling us to more accurately predict the potential return rates (RR) of stocks in the real world."}, {"title": "4.1 Training Method Based on A-Share Market Plate Type", "content": "4.1.1 Problem Definition\nLet \u00e6 and c be two time series of length L, where x \u2208 RL is the input to the diffusion model, and c\u2208 RM is the condition for the model. The denoised time series \u00e2 is also of size RL.\n4.1.2 Training Phase\nDuring the training phase, following the approach of Song et al. [18], we add random noise levels conforming to a uniform distribution t ~ U(0, T) to the input data x through the forward diffusion process shown in equation (3.11). Here, we control the maximum noise level through T, where T = 1 represents the maximum noise level. Then, we update the network parameters @ according to equation (3.13) until convergence.\nTo train the conditional diffusion model, we employ a classifier-free guidance method[19], which is based on classifier guidance[20]. The conditional score  \u2207 log p(xc) is calculated using the following formula:\nVx log p(xc) = \u2207x log p(x) + w\u2207x log p(cx),\nwhere x log p(x) is the unconditional score, Vlog p(c|x) is the gradient of the classifier; w is a hyperparameter controlling the guidance strength. The classifier guidance method requires separately training a classifier for the noisy data, although it does not require retraining the original generative model. However, given that existing generative models are trained on U.S. stock or factor data, to synthesize data that conforms to the characteristics of the A-share market, the model needs to be retrained to adapt to the new data distribution, and we decide not to use the classifier guidance method.\nThe classifier-free guidance method combines the conditional model and the unconditional model, avoiding the training of additional classifiers:\nV\u00e6 log p(xc) = w\u2207x log p(x|c) + (1 \u2212 w)\u2207\u00e6 log p(x),\t\t(17)\nwhere x log p(x|c) represents the sampling direction of the conditional model, and \u2207 log p(x) represents the sampling direction of the unconditional model. When w = 0, equation (15) degenerates into the score of the unconditional model; when w = 1, it becomes the score of the conditional model. By adjusting the hyperparameter w, the model can be made more flexible, thus better balancing the influence of conditional and unconditional information during the generation process.\nTo enhance the model's conditional generation capability, we introduce two types of orthogonal conditional data: the stock's corresponding Shenwan secondary industry and its plate information. These two types of conditional data provide industry information and plate fluctuation characteristic information, respectively, solving the problem that existing methods do not consider the fluctuation rules of China's stock market.\nAs of 2024, there are 124 mutually exclusive categories in the Shenwan secondary industry, and the A-share market is mainly divided into the following 5 plates: Main Board, STAR Market, Growth Enterprise Market, Beijing Stock Exchange, and ST stocks. We splice the two types of conditional data to form a comprehensive conditional vector c. For the Shenwan secondary industry, an embedding layer is used to map the 124 categories into a lower-dimensional space, denoted as Cindustry. For A-share plate information: a one-hot encoding is used to generate a 5-dimensional vector Cboard, and Cindustry and Cboard are spliced together to form the final conditional vector c."}, {"title": "4.2 Improvements to Existing Sampling Methods", "content": "4.2.1 Denoising Process\nIn the denoising process, we subtract noise from \u00e6t to recover the corresponding 20 ~ q(x0). As described in 3.2.1, we parameterize po(xt-1|xt) using a neural network to estimate q(xt-1|Xt, xo). Specifically, we have:\nPo(Xt-1|Xt) = N (xt\u22121; \u03bc\u03b8(xt, t), \u03a3q(t)I)  (18)\nwhere,\n1/ Vat  (19)\n\u03a3(t) = (1 \u2212 at\u22121)\u03b2t/ 1-at / Vat\t(20)\nHere, 60(xt, t) is a trainable noise term used to predict e during the diffusion process.\n4.2.2 Accelerated Sampling\nTraditional diffusion probability models excel at generating high-quality samples, but their main drawback is the slow sampling speed. The reverse process of DDPM requires a large number of time steps (typically T reaches thousands of steps), making the time cost of generating samples very high. To overcome this issue, Denoising Diffusion Implicit Models (DDIM) propose a new method to accelerate the sampling process [21]. The core idea of DDIM is to achieve more efficient denoising by implicitly modeling conditional distributions. Specifically, DDIM introduces a combination of determinism and randomness in the reverse process, allowing acceleration of sampling by reducing the number of diffusion steps T while maintaining the quality of generated samples. Unlike DDPM, DDIM can make the sampling process more deterministic by adjusting parameters, thus significantly reducing the number of required sampling steps.\nSpecifically, DDIM modifies the forward process to be non-Markovian to accelerate sampling, that is:\nT\nqo (X1:T|X0) = qo (XT|Xo) qo (xt-1|Xt, X0), t=2 (21)\nwhere qo (xt-1|Xt, x0) is controlled by the parameter \u03c3, representing the magnitude of the random process. When \u03c3\u03c4 = \u2211q(t), the forward process degenerates into a Markov process, and the denoising process is the same as shown in equation (3.10). Particularly, when \u03c3\u03c4 = 0, the corresponding denoising process becomes deterministic, thus allowing acceleration along a deterministic path. Technically, we follow a deterministic sampling design and create a subsequence {Ti}, where i = 1,\u2026\u2026,T' is a subset of {t = 1,2,\u2026\u2026,T}, and T' is the length of the subsequence. With the help of DDIM sampling, the denoising process can be completed in only T' \u226aT steps.\n4.2.3 Approximate Nonlocal Total Variation Loss\nWhen dealing with time series data, especially involving generative models, we often face a challenge: how to effectively control the variance of the generated sequence while maintaining the overall trend and features of the data. Traditional local methods, such as Total Variation (TV) regularization, can provide good results in some cases, but they often ignore the global dependency relationships between data points, leading to unnatural block effects or over-smoothing in the generated sequences.\nTo overcome these limitations, we propose an Approximate Nonlocal Total Variation (ANTV) loss based on Liu et al.'s Nonlocal Total Variation (NTV) method[22]. The NTV algorithm is a mathematical model used in image processing for tasks such as image denoising, image reconstruction, and other related tasks. This method extends the traditional local Total Variation (TV) method by introducing nonlocal information to overcome the staircasing effect that occurs when processing images with local TV methods. Our ANTV method effectively captures the global structure of time series by considering the nonlocal dependencies between data points within a local window, while reducing computational complexity.\nSpecifically, the Approximate Nonlocal Total Variation (ANTV) loss function can be expressed as:\nn\nLANTV(x) = \u03b1 (xj \u2013 Xi)w(i, j) / jew(i) (22)"}, {"title": "4.2 Improvements to Existing Sampling Methods", "content": "where a is a regularization parameter controlling the strength of the ANTV term. x = (x1,x2,...,xn) is the time series data. w(i) is a local window centered at xi, considering only the points within this window. w(i, j) is a weight function measuring the similarity between xi and xj. The weight function w(i, j) can be represented by a Gaussian kernel as:\nw(i, j) = exp(-(Xi - Xj) 2/2\u03c32)  (23)\nwhere o is the standard deviation of the Gaussian kernel, controlling the width of the weight function. We summarize the steps of this algorithm in Algorithm 2. After each denoising step in the sampling phase, we perform the following operations on x\u2081: for each time point xi, we calculate its nonlocal gradient within the local window and update xi to reduce variance and maintain sequence coherence. This design aims to reduce significant fluctuations in the generated stock sequences, as rapid and large trend reversals are less common in the A-share market.\n4.2.4 Application Scenarios for Fourier Transform Filtering\nIn recent years, with the development of China's capital market, some emerging stock plates, such as the Beijing Stock Exchange (BSE), have attracted increasing attention. However, due to the short establishment time of the BSE, its historical data is relatively limited, posing challenges for model training. To overcome this issue, we can adopt the method of transfer learning, using the mature market with a wealth of historical data (such as the Shanghai Stock Exchange or Shenzhen Stock Exchange) as the source domain, and the BSE as the target domain, to achieve effective data augmentation. Specifically, in the transfer learning framework, we first use a large amount of historical data from the source domain for pre-training, and then start from the limited historical data of the BSE, gradually add noise, and apply bandpass filter loss to generate new synthetic data. When data augmentation is needed from the target domain data, we use the bandpass filter method. In this case, we perturb the original data \u00e6o to x', rather than sampling noise directly from the Gaussian distribution and then generating stock data. Therefore, this is an optional method, mainly applied in scenarios similar to transfer learning.\nHere, our second loss function is the bandpass filter loss (Band-Pass Filter Loss), defined as follows:\nLBP(xt, x) = ||F(xt) \u2013 BandPassFilter(F(x), flow, fhigh) ||2.\t(24)\nThe bandpass filter loss aims to make the frequency domain characteristics of the given signal consistent with the expected target. Here, \u00e6t is the noisy data, and \u00e6 is the original data. F(\u00b7) represents the Fast Fourier Transform (FFT), which transforms data from the time domain to the frequency domain. We assume that noise mainly exists in low-amplitude frequency components, while useful information is concentrated in a specific frequency range, so we use BandPassFilter(\u00b7) to retain components within the frequency range of flow and fhigh.\n4.2.5 Summary of Sampling Algorithms\nAs shown in Algorithm ??, during the sampling process of the conditional diffusion model, we use a method that combines DDIM sampling and various loss function optimizations to generate high-quality data. Specifically, the algorithm first initializes the number of data to be generated m, sampling steps T', and the condition vector c, and prepares an empty list list for storing the generated results.\nFor each generated data point, we randomly sample the initial noise \u00e6, from the Gaussian distribution N (0, I). Then, starting from the time step t = T' and gradually retreating to t = 0, we use the DDIM sampling method to calculate"}, {"title": "4.2 Improvements to Existing Sampling Methods", "content": "the updated xt-1 at each step. To further optimize the generated data, we apply two regularization losses at each time step: the Approximate Nonlocal Total Variation loss and the Band-Pass Filter loss. These two losses are used to enhance the structural consistency of the time series data and ensure that the frequency characteristics of the generated data are consistent with the target domain data.\nAfter each complete sampling process from T' to 0, we add the final generated xo to the result list list. When the number of generated data points reaches the preset number m, we calculate the mean of all generated samples as the final output: 2 \u2190 Mean(list). This sampling algorithm can not only efficiently generate high-quality time series data but also control specific conditions by introducing the condition vector c, thereby better adapting to the characteristics of the target domain. In addition, the combination of Approximate Nonlocal Total Variation and Band-Pass Filter losses can significantly improve the quality and stability of the generated data, ensuring that the generated data not only conforms to the expected structural characteristics but also retains important frequency information\n5 Experimental Validation\n5.1 Introduction to Experimental Setup\n5.1.1 Experimental Objectives\nIn the experimental section, we primarily aim to verify the following issues:\n\u2022 Question 1: Whether the stock time series data synthesized based on diffusion models improves the signal-to-noise ratio;\n\u2022 Question 2: Whether the stock time series data synthesized based on diffusion models can be used for trading and generate more profit;\n\u2022 Question 3: Whether our proposed sector-based stock data synthesis method can alleviate the problem of data scarcity.\n5.1.2 Dataset Introduction\nThe data we used is the daily frequency A-share data provided by the quantitative data provider RiceQuant, spanning from January 1, 2014, to June 1, 2024, covering all A-share listed companies and excluding ST and delisted companies. During this period, there were 3,173 companies on the main board, 1,363 companies on the ChiNext board, 581 companies on the STAR Market, and 253 companies on the Beijing Stock Exchange. To clarify the plate to which each stock belongs, we classified them based on stock codes: for example, stocks starting with 30 belong to the ChiNext board, those starting with 002 belong to the SME board, those starting with 60 belong to the Shanghai main board, those starting with 000 belong to the Shenzhen main board, and those starting with 688 belong to the STAR Market. Stocks on the Beijing Stock Exchange start with 83, 87, or 88. In addition, we used the Shenwan second-level industry classification from the Wind terminal database to classify stocks, ensuring the accuracy and consistency of industry attributes. For each stock in each plate, we used a sliding window of size 60 with a step size of 20 to obtain the stock closing price time series, making each data sample in the dataset 60 in length, and the dataset is divided into training and test sets in a 4:1 ratio.\nFurthermore, considering the existence of stock market suspensions, stock price rights issues, and large fluctuations in new stock listings on the first day, we have the following coping methods. Specifically, for situations where stocks are suspended, we use linear interpolation to fill in the missing closing prices during short-term suspensions to maintain the continuity of the time series. For stocks that are suspended for more than 5 trading days, we use forward filling to maintain the consistency of the time series; if a stock is frequently suspended or suspended for too long a period, considering the quality of the data, we remove the data for that period from the training set.\nFor newly listed stocks, in their initial period (usually the first few trading days), due to unrestricted price fluctuations, the stock price fluctuates greatly and is not representative, so we choose not to use the data for these days to avoid the impact of abnormal fluctuations on the model.\nTo eliminate the impact of corporate actions such as dividends and bonus shares on stock prices, we perform forward rights processing for all stocks to ensure the comparability and continuity of historical prices, avoiding false fluctua-tions caused by changes in share capital. In addition, since some companies may change their main business or be acquired and merged, causing changes in their Shenwan second-level industry, when the Shenwan industry changes, we uniformly use the later industry information to reflect the latest business situation."}, {"title": "5.1.3 Performance Evaluation Methods", "content": "Return Ratio (RR)\nThe main goal of stock prediction is to achieve significant profits. Most previous studies have used the return ratio (RR) as a measure of model performance[23]. The return ratio is a key indicator for assessing whether a stock prediction model can achieve profitable investment results. We define the return ratio as follows:\nRR(i) = Closet+i Closet/ Closet,\t\t(25)\nwhere t represents the current time, i represents the time interval in days. Closet represents the closing price of the stock at the current time t, and Closet+i represents the closing price of the same stock i days later.\nLog Return (LR)\nLog return is a commonly used indicator in financial data analysis, reflecting the proportion of price changes and having the properties of additivity and approximate normal distribution. We define the log return as follows:\nLog Return(i) = log (Closet+i/Closet)  (26)\nHere, we calculate the log return daily, and usually set i to 1 day to capture daily price changes.\nInformation Coefficient (IC)\nThe information coefficient (IC) is a commonly used indicator to assess the linear correlation between predicted values and true labels. Specifically, IC represents the Pearson correlation coefficient between predicted values and true labels, defined as follows:\nIC = \u03a31(P-P)(Ri \u2013 R)/  \u03a31(P \u2013 P)21(Ri \u2013 R)2 (27)\nwhere Pi represents the predicted score of the i-th stock, Ri represents the actual return of the i-th stock, P and R represent the average predicted score and average actual return of all stocks, respectively, and N is the number of stocks.\nRanked Information Coefficient (Rank IC)\nThe ranked information coefficient (Rank IC) is used to assess the rank correlation between predicted values and true labels. Specifically, Rank IC represents the Spearman rank correlation coefficient between predicted values and true labels, defined as follows:\nRank IC 1- 6\u03a31(R(P) \u2013 R(R\u2081))2 /N(N2 - 1), (28)\nwhere R(Pi) and R(Ri) represent the rank of the predicted score and actual return of the i-th stock, respectively, and N is the number of stocks. The Spearman rank correlation coefficient measures the linear relationship between the ranks of two variables, which can better reflect nonlinear correlations.\n5.1.4 Implementation Details\nWe follow Gao et al.'s proposed DiffsFormer[24], using a neural network se(x, t, c) to estimate the noise in the noisy data distribution, which is a Transformer-based neural network for generating time series data. The network takes the time series x as input and uses conditional information c and sine-embedded time t as conditions. To enhance reproducibility, we provide the specific details of the network below. For the conditional embedding network, we use a 3-layer multilayer perceptron (MLP) with a hidden layer dimension of 128 and adopt SiLU as the activation function. For the diffusion model network, we use a transformer-based architecture with 64 output channels in the convolutional layer, 8 attention heads, 4 residual blocks, and adopt ReLU as the activation function. The total number of denoising steps is set to 400 steps. For the approximate non-local total variation coefficient XANTV, we set it to 0.03, for the Fourier transform filtering coefficient ABP, we set it to 0.03, and the classifier-free guidance scale w is set to 7.5."}, {"title": "5.1.5 Algorithms Involved in the Experiment", "content": "The following is a detailed description of various models used for stock price prediction:\n\u2022 MLP: We use a 2-layer multilayer perceptron (MLP) with 256 units in each layer.\n\u2022 LSTM[25", "GRU[26": "A stock price prediction method based on the Gated Recurrent"}]}