{"title": "Dopamin: Transformer-based Comment Classifiers through Domain Post-Training and Multi-level Layer Aggregation", "authors": ["Nam Le Hai", "Nghi D. Q. Bui"], "abstract": "Code comments provide important information for understanding the source code. They can help developers understand the overall purpose of a function or class, as well as identify bugs and technical debt. However, an overabundance of comments is meaningless and counterproductive. As a result, it is critical to automatically filter out these comments for specific purposes. In this paper, we present Dopamin, a Transformer-based tool for dealing with this issue. Our model excels not only in presenting knowledge sharing of common categories across multiple languages, but also in achieving robust performance in comment classification by improving comment representation. As a result, it outperforms the STACC baseline by 3% on the NLBSE'24 Tool Competition dataset in terms of average F1-score, while maintaining a comparable inference time for practical use. The source code is publicity available at https://github.com/FSoft-AI4Code/Dopamin.", "sections": [{"title": "INTRODUCTION", "content": "In the intricate world of software development, source code comments play a crucial role, serving as the backbone of applications by elucidating the functionality and intent behind code segments. These comments vary widely in their utility, ranging from summarizing the purpose of functions or classes, aiding in code maintenance, to identifying instances of technical debt, as highlighted in studies like [7, 9, 12]. However, not all code comments are equally beneficial, or different information in comments can be used for different development tasks. With the increasing complexity of software projects, the task of discerning valuable comments from the less pertinent ones has become more critical.\nCurrent tools for comment classification, as discussed in references like [1, 10, 11], predominantly focus on categorizing comments based on their apparent utility in the coding workflow. Despite their effectiveness, these tools often lack the nuanced understanding needed to differentiate between subtly different types of comments, limiting their practical utility. Recognizing this gap, this paper introduces Dopamin - a Transformer-based Comment Classifier that utilizes a code language model for an enhanced classification process.\nDopamin takes a novel approach, relying on domain post-training on diverse comment types across various coding environments, and adopting a multi-level layer aggregation strategy inspired by Karimi et al. [5]. The post-training procedure incorporates data from all programming languages, facilitating knowledge transfer across different languages and leveraging the high-resource language (Java) to improve the less resource-intensive languages (Python). Meanwhile, layer aggregation methodology enables Dopamin to not only classify comments but also understand the nuanced semantic information they carry, as the higher layers in BERT are adept at capturing intricate semantic features, a concept supported by Jawahar et al. [3]. By doing so, Dopamin significantly improves the relevance and accuracy of comment classification, catering to the evolving complexity of software development.\nThe efficacy of Dopamin is evident in our experimental results, where it achieves an F1-score of 0.74, surpassing the 0.71 F1-score of the existing STACC [1] baseline. Through Dopamin, we aim to redefine the standards in code comment classification, providing a tool that is effective in distinguishing various types of comments on multiple programming languages."}, {"title": "DATA PREPARATION", "content": "In this section, we illustrate the NLBSE'24 Tool Competition dataset introduced by Kallis et al. [4], detailing our approach to processing comments and splitting the training data for model selection."}, {"title": "Dataset statistic", "content": "The competition provided binary comment classification data of three languages (Python, Java, and Pharo). In total, there are 19 categories corresponding to 19 classifiers required to build. Overall"}, {"title": "Data preprocess", "content": "Input feature: Follow Al-Kaswan et al. [1], we concatenate the class name and comment sentence to serve as the input to the model, employing \"</s>\" as the separator between them.\nData spliting: The training data is divided into a training set and a validation set for refining the optimal model. Instead of employing the validation set for hyperparameter tuning, it is utilized to choose the best checkpoint during the training process. Due to the modest amount of data, we select only 10% of the training set of each category to serve as the validation set. We employ stratified sampling to maintain the label distribution in both sets."}, {"title": "METHODOLOGY", "content": "This section describes the Dopamin methodology, including model selection, the methodology for obtaining the optimal checkpoint, domain post-training procedures, and layer aggregation techniques."}, {"title": "Model selection", "content": "We investigate several candidates as the backbone model. Since the primary source of classification information comes from comments, which are in natural language form, we choose RoBERTa [8] and ALBERT [6] as candidates. Additionally, the comments primarily contain syntax related to the coding domain. Therefore, we are also considering CodeBERT [2], which is a language model pretrained on large code corpus, as an option. We opt for these architectures because they are based on Transformer encoders, commonly employed for classification tasks. Additionally, the base versions of these models share the same size as the baseline (STACC), resulting in fair comparison and no additional overhead in inference time, which is a key consideration for evaluation score. As a result, we select CodeBERT - the model that achieves the best F1 score on the validation set, as the backbone model."}, {"title": "Domain post-training", "content": "Table 1 shows that Python and Pharo have much fewer examples compared to Java. Besides, some categories are contained in both Java and Python languages such as Expand, Summary, and Usage. Therefore, we combined the data of all languages to finetune the CodeBERT backbone model in the data domain (post-training), facilitating the transfer of knowledge across languages before individual training of models for each category in the target domain.\nDuring the post-training procedure, we did not concatenate the class name and comment sentence as input to the model as mentioned in Section 2.2. Instead, we concatenate the category to the comment sentence since the model is required to predict for all categories, necessitating the inclusion of category information in the input.\nFollowing the post-training of the model in the target domain, we utilize it as the initial state for the model to be trained individually for each category using the procedure in Section 3.4 and input feature in Section 2.2."}, {"title": "Multi-level layer aggregation", "content": "Jawahar et al. [3] previously showed that the upper layers in BERT yield rich semantic features of linguistic information and distinct layers can exhibit distinct capabilities in encoding semantic information. Hence, combining these layers can obtain a more comprehensive representation for the input text. Therefore, we adopt the Hierarchical aggregation (HSUM) introduced by Karimi et al. [5] to enrich the comment representation. The illustration of HSUM is shown in Figure 1. Specifically, we combined the top four layers of the model to obtain the final representation for comment."}, {"title": "Optimal checkpoint", "content": "Given the high cost of hyperparameter search for 19 categories, we choose to keep the hyperparameters constant throughout the training process for each category. Instead, we use the validation set to determine the best checkpoint step. This strategy is similar to early stopping, which aims to prevent the model from overfitting. We employ it as a heuristic to determine the optimal step for the stage of training the model on the original training set without validation. Specifically, there are two stages in the training process.\n\u2022 Stage 1: the model is trained on the training set and the best checkpoint (optimal_step) is obtained based on the F1-score of the validation set.\n\u2022 Stage 2: Considering the limited amount of data, training the model on the entire original training set is essential. After obtaining the optimal_step in stage 1, we train the model on the original training set and acquire the final model at step optimal_step + extra_steps. The extra_steps represents the additional steps due to the incorporation of more data during training.\nFor example, after stage 1 the model for the Java - Deprecation category attains its highest F1 score at step 200. The extra_steps is set to 100, thus after training on the full original dataset in stage"}, {"title": "EXPERIMENT SETUP", "content": "we obtain the checkpoint at step 300 as the final model for the Deprecation category of Java."}, {"title": "Training hyperparameters", "content": "For reproducibility, we set the random seed as 0. The hyperparameters are selected based on references from prior studies [2, 6, 8] that involved fine-tuning models on downstream tasks.\n\u2022 Post-training stage: In this stage, we train the backbone model during 10 epochs with the learning rate of 2e - 5, batch size of 64, and evaluation step of 500.\n\u2022 Invidual classifier training stage: Each model is trained for 10 epochs for Java categories and 20 epochs for Python or Pharo categories. We assign a lower number of epochs to Java due to its higher volume of training examples, resulting in a greater number of training steps within a single epoch. The learning rate is set to 1e-5, the batch size is 64, and the evaluation step is 50. We use the extra_steps of 100 for all categories."}, {"title": "Implementation", "content": "We use the HuggingFace transformers\u00b2 and PyTorch packages\u00b3 to implement Dopamin. All experiments are conducted using two Nvidia A100 GPUs with 80GB of VRAM. During the evaluation on the provided test set, we utilize Google Colab T4, adhering to the competition's specifications, to acquire the inference time."}, {"title": "Metrics", "content": "For evaluation, we employ the metrics outlined by the competition, considering a category c. Specifically, we calculate the recall $R_c$, precision $P_c$, and F1 score $F1_c$ for each category. These metrics are defined as follows:\n$P_c = \\frac{TP_c}{TP_c + FP_c}$\n$R_c = \\frac{TP_c}{TP_c + FN_c}$\n$F1_c = 2 \\cdot \\frac{P_c \\cdot R_c}{P_c + R_c}$\nin which, $TP_c$, $FP_c$, and $FN_c$ are the true positives, false positives, and false negatives for a category c, correspondingly.\nFinally, the submission score of the competition using both the average F1-score and the inference time is defined as:\nsubmission_score\n$= 0.75(avg.F1) + 0.25 \\frac{max\\_avg\\_runtime \u2013 measured\\_avg\\_runtime}{max\\_avg\\_runtime}$"}, {"title": "EXPERIMENTAL RESULTS", "content": "In this section, we present the performance comparison of Dopamin against the STACC baseline. Table 4 shows the performance comparison between Dopamin and STACC across various categories and languages demonstrates Dopamin's enhanced capabilities in comment classification. Below is a summary of the key findings from the table."}, {"title": "Overall Performance", "content": "Dopamin attains a comprehensive F1-score of 0.74, balancing Precision at 0.73 and Recall at 0.75. This marks an enhancement compared to STACC's overall F1-score of"}, {"title": "Performance by Language", "content": "Java: Dopamin performs better in categories like Pointer, Summary, Ownership, Rational, and Usage, with notable improvements in F1-scores. For instance, Dopamin achieves F1-score at 0.90 compared to STACC's 0.78 in the Summary category.\n\u2022 Pharo: Dopamin shows improved performance in categories like Classreferences, Collaborators, and Example. For example, in the Classreferences category, Dopamin's F1-score is 0.68 compared to STACC's 0.52. However, Dopamin exhibits a weakness in 4 out of 7 categories when compared to STACC in this language. The explanation might stem from Pharo being the language with the lowest data volume per category (under 2000). Given the limited data, the few-shot approach (STACC) appears more effective than fine-tuning with classification loss (Dopamin). Moreover, the absence of overlap categories between Pharo and other languages could restrict the advantages of the Post-training stage.\n\u2022 Python: Dopamin also outperforms STACC in the Python category, particularly in Parameters, Summary, and Usage categories. These categories overlap with those in Java, suggesting the effectiveness of knowledge transfer during our Post-training process."}, {"title": "Category-Specific Performance", "content": "Dopamin excels particularly in categories where understanding the context and semantics is crucial, like Summary, Usage, and Ownership. In some categories, like Java - Expand and Pharo - Keyimplementationpoints, Dopamin's performance is lower than STACC. This suggests room for further optimization in certain specific categories.\nIn summary, Dopamin generally exhibits a balanced improvement in both precision ($P_c$) and recall ($R_c$) across various categories. For example, in the Java - Usage category, Dopamin shows a precision of 0.87 and recall of 0.95, compared to STACC's 0.64 and 0.92, respectively. Dopamin also demonstrates a notable improvement over STACC in most categories and languages, reflecting its advanced capability in understanding and classifying code comments."}, {"title": "ABLATION STUDY", "content": "In this section, we present the experimental outcomes obtained by systematically trimming individual components within Dopamin,"}, {"title": "CONCLUSION", "content": "We proposed Dopamin, a novel approach for code comment classification, which demonstrates notable advancements in the benchmark. By selecting CodeBERT as the backbone model and implementing an optimal checkpoint process, we have optimized the classification accuracy across various programming languages. The domain post-training procedure significantly enhances performance for low-resource languages, illustrating the model's adaptability. Additionally, the use of multi-level layer aggregation, specifically the Hierarchical aggregation (HSUM) technique, enriches the semantic representation of comments, contributing to Dopamin's superior performance over the STACC baseline in most categories. Overall, Dopamin's methodology and results mark a significant improvement in the efficiency and precision of code comment classification, offering valuable insights and tools for software communities."}]}