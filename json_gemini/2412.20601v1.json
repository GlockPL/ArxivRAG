{"title": "MATEY: MULTISCALE ADAPTIVE FOUNDATION MODELS FOR SPATIOTEMPORAL PHYSICAL SYSTEMS", "authors": ["Pei Zhang", "M. Paul Laiu", "Matthew Norman", "Doug Stefanski", "John Gounley"], "abstract": "Accurate representation of the multiscale features in spatiotemporal physical systems using vision transformer (ViT) architectures requires extremely long, computationally prohibitive token sequences. To address this issue, we propose two adaptive tokenization schemes that dynamically adjust patch sizes based on local features: one ensures convergent behavior to uniform patch refinement, while the other offers better computational efficiency. Moreover, we present a set of spatiotemporal attention schemes, where the temporal or axial spatial dimensions are decoupled, and evaluate their computational and data efficiencies. We assess the performance of the proposed multiscale adaptive model, MATEY, in a sequence of experiments. The results show that adaptive tokenization schemes achieve improved accuracy without significantly increasing the length of the token sequence. Compared to a full spatiotemporal attention scheme or a scheme that decouples only the temporal dimension, we find that fully decoupled axial attention is less efficient and expressive, requiring more training time and model weights to achieve the same accuracy. Finally, we demonstrate in two fine-tuning tasks featuring different physics that models pretrained on PDEBench data outperform the ones trained from scratch, especially in the low data regime with frozen attention.", "sections": [{"title": "1 INTRODUCTION", "content": "Developing foundation models for physical systems is vital for energy generation, earth sciences, and power and propulsion systems. These models offer faster solutions than physics-based simulations and can generalize better across multiple systems than single-purpose AI approaches. However, their application to physical systems, often characterized by multiple sub-processes at different scales, is still in the early stages. For instance, fluid flowing around a cylinder creates a von K\u00e1rm\u00e1n vortex street, a highly dynamic flow with rapidly evolving vortices. Accurate solutions of such multiscale systems require a very high resolution representation to capture the most complex features across space and time. However, for scientific machine learning as for modeling and simulation, using very high resolutions to achieve accurate solutions incurs significant computational cost. This is particularly true for developing foundation models using vision transformer (ViT)-based architectures, as using the standard self-attention mechanism for extremely long spatiotemporal sequences can become prohibitively computationally expensive.\nEfficient representation of multiscale features in high-resolution inputs has been an active research topic in computer vision. Three broad approaches can be characterized. First, multiscale models like Swin Transformer (Liu et al., 2021) and MViTv2 (Li et al., 2022) introduce multiple stages with decreasing resolution and increasing feature dimension for efficient hierarchical representations. Second, computational techniques have been developed that facilitate training on long sequences (e.g., sequence parallelism across GPUs (Jacobs et al., 2023)) or reduce the effective sequence length in the attention kernel (e.g., decomposing attention along axial directions (Ho et al., 2019)). Third, the actual sequence length can be directly shortened by pruning and merging tokens ((Haurum et al.,"}, {"title": "2 RELATED WORK", "content": "Scientific foundation models. Several research directions have been explored for building foundation models for physical systems, including multiple physics pretraining (McCabe et al., 2023) with PDEBench data, input augmentation with PDE system configurations (Hang et al., 2024), robust pretraining schemes (Hao et al., 2024), fine-tuning effectiveness investigations (Subramanian et al., 2024), and data-efficient multiscale ViT architectures (Herde et al., 2024). While these work made remarkable progress, they do not directly address the issue of token sequence length, which becomes a computation bottleneck when applying ViTs to high dimension or high resolution data.\nMultiscale ViTs. While most multiscale ViTs achieve hierarchical representations via multi-stage attention blocks at different resolutions (e.g., MViTv2 (Li et al., 2022) and Swin Transformer (Liu et al., 2021)), there are a few focusing on tokenization schemes, such as(Yin et al., 2022; Fan et al., 2024; Zhang et al., 2024; Havtorn et al., 2023). Among these, the single-stage MSViT with dynamic mixed-scale tokenization (Havtorn et al., 2023), which leverages a learnable gating neural network for selecting the token refinement, is most related to our work. This approach requires a tailored gate loss function and an adaptive trimming scheme to handle the high overhead cost, which in return hurts gate training accuracy. In contrast, the tokenization scheme in MATEY adaptively adjusts the patch sizes directly based on local feature scales, which is simpler and more direct.\nAxial attentions. The quadratic scaling nature of attention makes it computationally prohibitive for extremely long token sequences from multidimensional systems. To address this challenge, (Ho et al., 2019) proposed the axial attention, which decomposes the full attention into a sequence of attention operations along each axis. It reduces the attention cost from $O(N^2d)$ to $O(Nd+1)$, for a given d-dimensional system with Nd tokens. ViViT (Arnab et al., 2021) factorized the spatiotemporal attention into spatial- and temporal-dimensions for video classification. (McCabe et al., 2023) applied the axial attention in the Axial ViT (AViT) for spatiotemporal solutions of physical systems. While these spatiotemporal attention schemes can reduce the sequence length and hence the attention cost, their impact on accuracy in physical systems is unclear."}, {"title": "3 MATEY, EXPLAINED", "content": "We propose multiscale adaptive foundation models, MATEY, to predict two-dimensional spatiotemporal solutions of multiple physical systems. The architecture of MATEY is illustrated in Figure 1.\nGiven a sequence of T past solutions of some physical system leading up to time t, MATEY predicts"}, {"title": "Multi-physics preprocessor, postprocessor, and training.", "content": "To accommodate multiple physical systems with different sets of variables at different spatial resolutions, we adopt the multi-physics preprocessor and postprocessor used in MPP (McCabe et al., 2023). For system k with $C_k$ variables, the preprocessor first encodes solutions $u_t(x,y) \\in \\mathbb{R}^{C_k}$ to a latent space $\\mathbb{R}^{C_{uni}}$, where $C_{uni} \\gg C_k$ is shared among all systems. Specifically, letting H and W denote the resolution in the x and y directions, respectively, the preprocessor encodes the solution $U_k \\in \\mathbb{R}^{T\\times H \\times W\\times C_k}$ of system k into the unified latent representation $U \\in \\mathbb{R}^{T\\times H \\times W\\times C_{uni}}$. U is then tokenized into sequences $Z^0 \\in \\mathbb{R}^{n_t\\times n_{px}\\times n_{py}\\times C_{emb}}$ in the tokenization module, which consists of convolutional blocks. Here $n_t = T/p_t$, $n_{px} = H/p_x$, and $n_{py} = W/p_y$ are the number of patches in each dimension with prescribed patch size $[p_t, p_x, p_y]$. After passing through L attention blocks, the input token sequence $Z^0$ leads to the attention output $Z^L \\in [\\mathbb{R}^{n_t\\times n_{px}\\times n_{py}\\times C_{emb}}$. The last temporal snapshot of $Z^L$ is then decoded in the postprocessor into the prediction $u_{pred} \\in \\mathbb{R}^{H\\times W \\times C_k}$. In this work, the preprocessor is a linear map, the tokenization module is implemented as a convolutional neural network (CNN), and the final decoding postprocessor uses 2D transposed convolutional blocks. To train the model from solutions with different resolutions, we follow the approach in MPP by performing system-based sampling in the training process and fusing information from samples across different systems via multi-GPU training with PyTorch Distributed Data Parallelism (DDP) and gradient accumulation."}, {"title": "Attention mechanisms \u2014 AVIT, SVIT, and ViT.", "content": "The standard ViT attention mechanism takes into account the attention across the entire set of spatiotemporal dimensions, which results in a high attention cost when extremely long spatiotemporal token sequences (e.g., from high-resolution spatiotemporal data) are considered. To address this issue, various factorized attention mechanisms have been proposed, such as AViT (Ho et al., 2019; McCabe et al., 2023) and a spatio-temporal decoupled attention (Arnab et al., 2021), referred to as SViT here. These attention mechanisms mainly consist of the same multihead self-attention (MHSA) and feed forward multi-layer percep-"}, {"title": "Adaptive tokenization.", "content": "Smaller patch sizes are preferred for better representation accuracy, as ViTs can capture long-range correlations between patches well but lack inductive biases within patches. However, features in physical systems often cross multiple length scales and exhibit strong spatiotemporal inhomogeneities. Consequently, constant patch sizes that are small enough to provide good accuracy in the necessary regions of such systems result in impractically long token sequence lengths over the entire domain. To address this issue, we propose an adaptive ViT that dynamically adjusts the tokenization patch sizes according to local physical features. To maximize expressiveness, we start with coarse patching and identify the most complex patches in each sample based on a simple metric, such as the variance of local features. The identified patches are further refined to the sub-token-scale (STS) to improve representation accuracy. Adaptive patch size leads to patches of varying length across samples, which are handled with padding masks. Patch position and patch area bias are represented following the embedding method in (Bodnar et al., 2024).\nFor a given solution field $u_t \\in \\mathbb{R}^{H\\times W\\times C}$, tokenization at a constant patch size $[p_x, p_y]$ is achieved through a CNN block and leads to a patch grid of size $(n_{px}, n_{py}) = (H/p_x, W/p_y)$. For adaptive tokenization, we apply varying patch sizes in space based on local complexity represented by the patch variance. For a solution $u_t \\in \\mathbb{R}^{H\\times W\\times C}$ and an initial coarse patch size $[P_{x1}, P_{y1}]$, a variance tensor $v_t \\in \\mathbb{R}^{n_{px1}\\times n_{py1}}$ ($n_{px1} = H/P_{x1}$ and $n_{py1} = W/P_{y1}$) is calculated from solutions inside"}, {"title": "After spatiotemporal attention, the decoding of adaptive patch sequences into solution fields within\nthe multi-physics postprocessor is performed using transposed convolutional blocks, tailored to each\ncorresponding scale. For Adap_Mul, the patches at different resolutions/sizes are deconvoluted sep-\narately and then summed to the final output, $\\hat{u}_t$. Specifically, for a coarse attention output $Z_{coarse}^L =$\n$[z_1, z_2,..., z_{n_{px1}\\times n_{py1}}^L]$ and STS attention outputs $Z_{sts,i}^L = [z_{sts,1}^L, z_{sts,2}^L,..., z_{sts, P_x1/P_{xsts} P_y1/P_{ysts}}^L]$\n$(i = 1,..., N_{sts})$, \"Adap_Mul\" performs the following operations:", "content": "Reconstruction from coarse patches: $\\hat{u}_{t} = ConvTranspose2d_1(Z_{coarse}^L)$,\nReconstruction from STS patches: $\\hat{u}_{t,sts,i} = ConvTranspose2d_2(Z_{sts, i}^L)$\n$\\hat{u}_{t,sts} = [\\hat{u}_{t,sts,1},..., \\hat{u}_{t,sts, N_{sts}}]$\nFusion of multi-resolution solutions: $\\hat{u}_t [STS\\text{-}IDs] = \\hat{u}_t [STS\\text{-}IDs] + \\hat{u}_{t,sts}.$\n(5)\nOn the other hand, the Adap_Mix approach fuses the coarse and STS patch sequences into the full sequences at the coarse and fine STS scales, respectively, reconstructs the solutions via transposed convolutions at corresponding resolutions separately, and then merges them to achieve multi-resolution solutions. This approach guarantees consistency with the coarse patch solution when"}, {"title": "Pretraining and fine-tuning.", "content": "We pretrain the models on PDEBench data, which includes five basic 2D systems: incompressible flows, compressible flows, turbulent flows, reaction-diffusion systems, and shallow water equations. We consider two fine-tuning cases: 1) colliding thermals between a cold and a warm bubbles from MiniWeather simulations (Norman, 2020) and 2) lid-driven cavity MHD flows (Fambri et al., 2023). As discussed in detail in Appendix A.1, these fine-tuning datasets were selected to be meaningfully out-of-distribution, not only in flow regime but also in including thermal and electromagnetic components that are not represented at all in the pretraining data. Training was performed on the Frontier and Perlmutter supercomputers at the Oak Ridge Leadership Computing Facility (OLCF) and National Energy Research Scientific Computing Center (NERSC), respectively."}, {"title": "4 EXPERIMENTS", "content": "We design three experiments to evaluate 1) the performance of three spatiotemporal attention schemes, AVIT, SViT, and ViT, 2) the impact of adaptive tokenization, and 3) the effectiveness of pretrained models on two fine-tuning tasks that feature physics different from the pretraining data. In these experiments, we set $p_t = 1$ and $C_{uni} = C_{emb}/4$, and employ square patches (i.e., $p_x = p_y$, $P_{x1} = P_{y1}$, and $p_{xsts} = p_{ysts}$) by default."}, {"title": "4.1 SPATIOTEMPORAL ATTENTION SCHEMES", "content": "We evaluate AViT, SViT, and ViT for three model sizes: Tiny (Ti), Small (S), and Base (B) with 3, 6, 12 heads and hidden dimension $C_{emb} = 192, 384, and 768$, respectively (Touvron et al., 2022), as"}, {"title": "4.2 ADAPTIVE TOKENIZATION", "content": "We start the evaluation of our adaptive tokenization methods in a single collision trajectory between two thermal bubbles. Figure 4 compares the temperature contours of the true solution at t = 590 with the predicted solutions from Ti-SViT models at constant patch sizes, ps=16\u00d716 and ps=32\u00d732, and adaptive tokenization (Adap_Mul with $P_{x1} = P_{y1} = 32$, $p_{xsts} = p_{ysts} = 16$, and $\\gamma_{sts} = 0.2$). The predicted solution from ps=32 \u00d7 32 exhibits abrupt changes with clear edges for the local structures inside the patches, while the finer resolution model at ps=16 \u00d7 16 captures smoother, finer structures but requires many more patches. In contrast, our adaptive tokenization methods capture smooth, fine structures comparable to ps=16 \u00d7 16 while requiring much shorter sequences.\nAdap Mix in ViT and SVIT Adap_Mix with $(P_{x1}, p_{xsts}, \\gamma_{sts})$ is designed to ensure convergence in $\\gamma_{sts}$ values. When $\\gamma_{sts} \\rightarrow 1$, no refinement is conducted and the output is converged to $ps=P_{x1} \\times P_{x1}$."}, {"title": "or quadratically for attention with sequence length in various model components. To represent the\ncost, we define the linear and quadratic indices for ViT and SViT as in", "content": "$L_{lin} = \\frac{1}{T} \\sum_{t=1}^{T} L_{t} \\frac{1}{n_{px_1} n_{py_1} + N_{sts,t}} \\left( \\frac{P_{x_1}}{p_{x_{sts}}} \\frac{P_{y_1}}{p_{y_{sts}}} \\right)$,\n$L_{quad} = \\frac{1}{T} \\sum_{t=1}^{T} L_{t} \\left( \\frac{(n_{px_1} \\cdot n_{py_1})^2} {(n_{px_1} \\cdot n_{py_1})^2 + N_{sts,t}} \\left( \\frac{P_{x_1}}{p_{x_{sts}}} \\frac{P_{y_1}}{p_{y_{sts}}} \\right)^2\\right)$.\n(11)\nFor AViT, the index $L_{quad,t}$ needs to be adjusted as"}, {"title": "For AViT, the index $L_{quad,t}$ needs to be adjusted as", "content": "$L_{quad,t} = (n_{px_1}^2 \\cdot n_{py_1} + n_{px_1} \\cdot n_{py_1}^2) + N_{sts,t} \\left(\\frac{P_{x_1}}{p_{x_{sts}}}\\right)^2 + \\left(\\frac{P_{y_1}}{p_{x_{1}}}\\right)^2\\left(\\frac{P_{y_1}}{p_{y_{sts}}}\\right)^2$.\\n(12)\nFigure 6 shows the final NRMSE test loss against the two cost estimation indices $L_{lin}$ (left) and $L_{quad}$ (right) of Adap_Mul in ViT (top) and SViT (bottom) at varying values of $(p_{x_1}, p_{x_{sts}}, \\gamma_{sts})$. Clearly, as $\\gamma_{sts}$ decreases for ViT and SViT, predictive errors are significantly reduced. Simultaneously, the linear cost increases progressively from coarse to refined and beyond, while the increase in quadratic cost remains negligible. Moreover, the accuracy does not converge to the refined case when $\\gamma_{sts} = 0$, despite the higher linear cost. This indicates that Adap_Mul is better suited for scenarios involving attention on long sequences. Figure 7 presents similar results for Adap_Mul in AViT. Compared with ViT and SViT, the accuracy improvement in AViT is less pronounced in our experiments with $(p_{x_1}, p_{sts}) = (32, 16)$ and (16,8) (top), possibly due to the extremely short sequence lengths of 2 in AViT. Notably, significant accuracy gains are observed when $p_{sts}$ is reduced from 16 to 8 for $p_{x_1} = 32$ (bottom)."}, {"title": "Comparing the two approaches for adaptive tokenization, we find that Adap_Mix provides better predictive accuracy, likely due to considering cross-scale correlations, and guarantees convergence toward the solution with uniformly refined tokens. In contrast, Adap_Mul is dramatically more cost effective for attention operations with quadratic complexity and easier to implement than Adap_Mix.\nAViT does not interact well with adaptive tokenization approaches when the STS sequence length $P_{x_1}/p_{x_{sts}}$ is too short.", "content": ""}, {"title": "4.3 EFFECTIVENESS OF PRETRAINING IN COLLIDING THERMALS AND MHD FINE-TUNING\nTASKS", "content": "We examine the transferrability of pretrained models to fine-tuning systems with distinct physics and different set of variables, as in Table A1. Specifically, we aim to address three broad questions:\n1. Is pretraining effective when the downstream tasks have a distinct set of physical variables?\n2. How does limited fine-tuning of non-attention blocks compare to full fine-tuning?\n3. How does the amount of fine-tuning data affect convergence?\nTo address these three questions, we design a sets of experiments, starting from models pretrained on PDEBench or randomly initialized models (\u201c*_INIT\u201d), and fine-tune them on colliding thermals and MHD datasets with distinct physical variables. For fine-tuning each model, we either allow all model parameters to be tunable (\u201cALL\u201d) or freeze the attention blocks and limit training to the preprocessor, the tokenization module, and the postprocessor (\u201cPREPOST\u201d). Finally, for each initial model and fine-tuning configuration, we train four models with increasing amounts of fine-tuning data.\nFor the colliding thermals dataset, Figure 8 compares the test loss with full and limited fine-tuning using pretrained and randomly initialized models. The different training data sizes ranging from one set of colliding thermals time-trajectory to 24 sets of trajectories. The fine-tuning task is to predict the solution of the physical system at a lead time of $t_{lead}$ uniformly sampled between 1 and 50 steps. An example of the true and predicted solutions in these four training configurations is illustrated in Figure 9.\nFor the limited fine-tuning test with the colliding thermals dataset, the pretrained models achieve significantly lower error than starting from scratch with randomly initialized parameters. Moreover, while this advantage persists as the number of fine-tuning data increases, it is most pronounced in the low data configuration of learning from a single trajectory. Indeed, we find that limited fine-tuning with the pretrained models generalizes well even when learning from one trajectory, seeing only moderate improvements when run on the largest dataset size considered. Overall, the lower converged error from pretrained models suggests the frozen attention blocks clearly learned transferable knowledge during pretraining. For full fine-tuning, the accuracy is much better than"}, {"title": "contrast, full fine-tuning leads to more expressive models that can capture all training data informa-\ntion when trained on limited data but often show high test errors; as more training data is provided,\nthey generalize well and lead to a convergent improved test error. In our fine-tuning, the randomly\ninitialized models perform well in testing even with a single data configuration, likely due to the\nsimilarity between training and testing tasks. Future work will explore more challenging scenarios\nwith increased heterogeneity within the fine-tuning data.", "content": "While studies like McCabe et al. (2023) have demonstrated impressive outperformance from fine-tuning of pretrained models versus randomly initialized models, these fine-tuning tests were performed on data that, while distinct, was fully governed by physical equations and characterized by physical variables that were represented in the training data. Yet for a model that aims to be foundational for multiphysical systems, we argue that assessing model performance in more realistic settings, where equations like Navier-Stokes are coupled with those from other domains of physics, is a more informative test of the effectiveness of pretraining. Accordingly, we assess fine-tuning performance on physical systems that incorporate fluid flows, which are well-represented in PDEBench, with thermodynamics and electromagnetism, which are not. As reasonably anticipated, we find that advantages of pretraining are reduced in this more complex setting."}, {"title": "5 DISCUSSION", "content": "In this paper, we make three contributions that will advance the development of foundation models for multiscale physical systems. First, we find that while some data efficiency is lost in a fully decoupled spatiotemporal attention scheme such as AViT, SViT provides an intriguing balance of computational and data efficiency versus the standard ViT approach. Yet using SViT alone does not sufficiently address the computational challenges associated with attention for high spatial resolutions. Second, we instead suggest that our adaptive tokenization scheme provides a promising approach for working with high resolution data. This sort of adaptivity has the potential to be both flexible and expressive enough to deal with the dynamic and sparse nature of the multiscale features in physical data. Third, we suggest an alternative path to evaluate foundation models for multiscale physical systems that focuses on fine-tuning problems involving out-of-distribution physics governed by different equations with distinct sets of physical variables. In two such settings, colliding thermals and magnetohydrodynamics, we find that while pretraining does provide an advantage, its impact is much more muted compared to fine-tuning on the same set of variables, suggesting additional effort is required to obtain truly foundational models in this space."}, {"title": "DATA AND SOFTWARE AVAILABILITY", "content": "We will publicly release the data, code, and trained models upon the publication of this paper."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DATASETS", "content": "Three datasets were used in the work: PDEBench (Takamoto et al., 2022), colliding thermals (Norman, 2024), and lid-driven cavity MHD flows.\n\u2022 PDEBench (https://github.com/pdebench/PDEBench) consists of diverse 1D, 2D, and 3D diverse benchmark datasets. We used the 2D cases incompressible flows, compressible flows, turbulent flows, reaction diffusion, and shallow water for model pre-training in Section 4.3. The govern equations are summarized below."}, {"title": "Shallow water equations [swe]:", "content": "$\\partial_t h + \\nabla \\cdot (hv) = 0$,\n$\\partial_t (hv) + \\nabla \\cdot \\left( \\frac{1}{hv} \\left( \\frac{1}{2}hv^2 \\right) + \\frac{1}{2}gh^2 \\right) = -g \\tau_b h \\nabla b$"}, {"title": "Diffusion-reaction equations [diffre2d]:", "content": "$\\partial_t c = D\\nabla^2 c + R(c)$,\nwhere $\\xi$ and $\\delta$ in $c = [\\xi, \\phi]$ are the activator and the inhibitor, respectively."}, {"title": "Incompressible NS [Incomp]:", "content": "$\\nabla \\cdot v = 0$,\n$\\rho (\\partial_t v + v \\cdot \\nabla v) = - \\nabla p + \\eta \\nabla^2 v + f$\nCompressible NS [compNS] with random and turbulent initial conditions:"}, {"title": "Compressible NS [compNS] with random and turbulent initial conditions:", "content": "$\\partial_t \\rho + \\nabla \\cdot (\\rho v) = 0$,\n$\\rho (\\partial_t v + v \\cdot \\nabla v) = - \\nabla p + \\eta \\nabla^2 v + (\\zeta + \\eta/3)\\nabla(\\nabla\\cdot v)$\n$\\partial_t \\epsilon + \\nabla \\cdot v\\left( \\frac{\\rho u^2}{2} \\right) = -\\nabla \\cdot \\left(p v \\right) + \\nabla \\cdot v - v \\cdot \\sigma\u2019$\nwith $\\epsilon = p/(\\Gamma-1)$ and $\\Gamma = 5/3$."}, {"title": "A.1.1 COLLIDING THERMALS", "content": "Thermal collision datasets contains multiple time history trajectories of the mixing of two bubbles-one cold bubble at the top colliding with a warm bubble at the bottom. Details about the governing equations can be found in Norman (2024). These trajectories start from different initial temperature conditions as\n$T_0(x, z) = 300.0 + T_{10}(x, z) + T_{20}(x, z)$,\nwith one hot $T_{10}$ and cold $T_{20}$ thermals being\n$T_{10}(x, z) = \\begin{cases}\nT_{c1} \\cos ^2 (d_1(x, z))^2, \\text{ if } d_1(x, z) \\leq 1 \\\\\n0, \\text{ otherwise}\n\\end{cases}$\nand\n$T_{20}(x, z) = \\begin{cases}\n-T_{c2} \\cos ^2 (d_2(x, z))^2, \\text{ if } d_2(x, z) \\leq 1 \\\\\n0, \\text{ otherwise}\n\\end{cases}$\nwhere $T_{ci}$ is the center temperature amplitude and $d_i (x, z) = \\sqrt{\\frac{(x-x_{ci})^2}{r_{x_i}^2} + \\frac{(z-z_{ci})^2}{r_{z_i}^2}}$ is the distance from thermal center $(x_{ci}, z_{ci})$ for $i = 1, 2$. The thermals are elliptical in shape with the radius, $r_{x_i}$ and $r_{z_i}$, in x and z directions, respectively."}, {"title": "Configurations", "content": "We sample 4096 configurations with the thermals (i = 1, 2) at different locations following uniform distribution,\n$x_{ci} \\sim U[0.2L, 0.8L]$, $z_{c1} \\sim U[0.2L, 0.3L]$, and $z_{c2} \\sim U[0.7L, 0.8L]$,\n(16)\nwith different elliptical shapes also following uniform distribution,\n$r_{xi} \\sim U[0.1L, 0.2L]$ and $r_{zi} \\sim U[0.1L, 0.2L]$,\n(17)\nand with temperature amplitudes equally sampled from,\n$T_{ci} \\sim C\\{10, 15, 20, 25\\}$.\n(18)\nThe equations are solved by using a finite volume method with $n_x = 256, n_y = 256$ grid points in x and z directions, respectively. The simulations are advanced in time for 500 seconds and solutions are saved every 0.5 second. In total, we have 4096 trajectories, each with data at size (nt = 1001, nx = 256, ny = 256)."}, {"title": "A.1.2 LID-DRIVEN CAVITY MAGNETOHYDRODYNAMICS (MHD) FLOWS", "content": "The MHD dataset contains solution trajectories from initial conditions to steady states for a benchmark lid-driven cavity MHD flow problem in two dimensions with varying configurations. The MHD flow is governed by an incompressible Navier-Stokes equation with Lorentz force coupled with an induction equation with divergence cleaning. The detail formulation of the governing equations and problem setting for the lid-driven MHD cavity problem are given in Fambri et al. (2023).\nConfigurations In this dataset, we include solution trajectories of the lid-driven cavity problem at three magnetic Reynolds numbers $R_{em} = 100, 200$, and 500, each with ten external horizontal magnetic field magnitude $B_x = 0.05, 0.10, ..., 0.50$. This gives 30 different problem configurations. For each problem configuration, the fluid velocity field v and the magnetic field B are recorded on a 128x128 uniform spatial mesh for 2,000 time steps."}, {"title": "A.2 MORE ON SPATIOTEMPORAL ATTENTIONS AND ADAPTIVE TOKENIZATION", "content": "Training setting We randomly sampled a subset with 512 trajectories for training and 64 trajectories for testing for the results in Sections 4.1 and 4.2. During training, we use the AdamW optimizer with a learning rate equal to $10^{-4}$. Batch size was set to be 128 and accumulate gradient step was set to be 1. Models were trained for 20,000 steps. For cases with constant patch size, the value was set to be 32 \u00d7 32.\nFor the experiment on spatotemporal attention schemes in Section 4.1, we ran 9 cases with AViT, SViT, and ViT attention blocks at three sizes (Ti, S, and B). Figure A1 shows the loss history during training of the models for both training and test sets, and Figure A2 shows the training time cost."}, {"title": "A.3 PRETRAINING AND FINE-TUNING", "content": ""}, {"title": "A.3.1 PRETRAINING", "content": "Five 2D datasets from PDEBench Takamoto et al. (2022) were used for pretraining, including shallow water, diffusion reaction, incompressible flows, compressible flows, and turbulent flows. The details of these datasets including physical variables, spatiotemporal resolutions, and number of trajectories are summarized in Table A1.\nDuring training, we used the Adamw optimizer with DAdaptAdam for learning rate scheduling. Batch size was set to be 1472 and patch size was 32 \u00d7 32. Training/testing/validating split was 0.8/0.1/0.1. Gradient accumulation was set to be 1. We trained the model for 30,000 steps to predict the next step solution given a history of T = 16."}, {"title": "A.3.2 FINE-TUNING", "content": "For fine-tuning, we evaluate the transferrability of pretrained models to systems with distinct physics and different sets of variables. Table A1 summarizes the two fine-tuning cases: colliding thermals and lid-driven cavity MHD flows. In the two cases, pretrained models were fine-tuned to predict the solution at a future time t + tlead given a history of solutions from t - T + 1 to t. In our experiments, T was set to be 10 while tlead was set to 50 for the colliding thermals and 100 for the lid-driven cavity MHD flows. The fine-tuned models were evaluated on a held-out test set for all runs in each case. We used the Adamw optimizer with a learning rate equal to $10^{-4}$. Batch size was set to be 256. Models were fine-tuned for 600 epochs for colliding thermals and 1000 epochs for lid=drive cavity MHD flows.\nColliding thermals We sampled 1, 6, 12, and 24 trajectories for training. The results in Section 4.3 are shown for a fixed test set with 24 trajectories.\nLid-driven cavity MHD flows Among the 30 cases, we kept 6 for testing. From the remaining 24 cases, we sampled 1, 3, 6, and 12 cases to assess the impact of the amount of fine-tuning data."}]}