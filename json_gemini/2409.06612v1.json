{"title": "Label-free Monitoring of Self-Supervised Learning Progress", "authors": ["Isaac Xu", "Scott Lowe", "Thomas Trappenberg"], "abstract": "Abstract-Self-supervised learning (SSL) is an effective method\nfor exploiting unlabelled data to learn a high-level embedding\nspace that can be used for various downstream tasks. However,\nexisting methods to monitor the quality of the encoder\nduring training for one model or to compare several trained\nmodels still rely on access to annotated data. When SSL\nmethodologies are applied to new data domains, a sufficiently\nlarge labelled dataset may not always be available. In this study,\nwe propose several evaluation metrics which can be applied\non the embeddings of unlabelled data and investigate their\nviability by comparing them to linear probe accuracy (a common\nmetric which utilizes an annotated dataset). In particular, we\napply k-means clustering and measure the clustering quality\nwith the silhouette score and clustering agreement. We also\nmeasure the entropy of the embedding distribution. We find\nthat while the clusters did correspond better to the ground\ntruth annotations as training of the network progressed, label-\nfree clustering metrics correlated with the linear probe accuracy\nonly when training with SSL methods SimCLR and MoCo-v2,\nbut not with SimSiam. Additionally, although entropy did not\nalways have strong correlations with LP accuracy, this appears\nto be due to instability arising from early training, with the\nmetric stabilizing and becoming more reliable at later stages\nof learning. Furthermore, while entropy generally decreases\nas learning progresses, this trend reverses for SimSiam. More\nresearch is required to establish the cause for this unexpected\nbehaviour. Lastly, we find that while clustering based approaches\nare likely only viable for same-architecture comparisons, entropy\nmay be architecture-independent.", "sections": [{"title": "I. INTRODUCTION", "content": "For many specialized fields seeking to deploy deep learning\nmodels, generating labels to produce viable datasets for su-\npervised machine learning can be a costly process in terms of\ntime and expertise. Taking advantage of unlabelled data, recent\nself-supervised learning (SSL) methods have achieved state\nof the art performance as a means for extracting high-level\nfeatures from complex inputs such as imagery [1]-[4]. These\nSSL methods have mainly been evaluated on labelled data. In\nthis work, we propose and evaluate metrics to monitor learning\nand the performance of the SSL models without annotations.\nIn computer vision, an encoder model maps an input image\nfrom pixel-space to a lower dimensional representational space\nas an embedding vector which captures high-level contextual\nand semantic information in the image. With supervised learn-\ning, the encoder is trained as part of the classification model.\nThen, through transfer learning, additional neural network\n\"head\" layers can be appended onto the encoder to interpret\nvectors in this embedding space for the purpose of other\ndownstream tasks.\nIn SSL, the encoder can be trained with labels generated\nfrom the data itself via a \"pretext task\", meaning expensive\nhuman-annotation is not required. An encoder trained with\nSSL can learn a more robust and generalizable embedding\nspace than those generated from a supervised learning process\n[5], [6]. Recently, instance learning has been demonstrated to\nbe a successful SSL pretext task. In this method, the represen-\ntational distance between independently augmented views of\nthe same sample is minimized [3], [4]. A subset of instance\nlearning known as contrastive learning, also maximizes the\ndistance between views of different samples [1], [7].\nA common evaluation method for models prepared with\nSSL is to train a linear read-out layer on top of the encoder\non a supervised classification task [1]\u2013[4], [7]. This process is\nreferred to as a linear probe (LP). Another increasingly popular\nmethod is to pass a dataset through the encoder and use a k-\nnearest neighbours (kNN) classifier [2], [8]. Since the distance\nbetween sample embeddings in the representational space\ncarries semantic meaning, a sample's class can be predicted\nusing the classes of its top k nearest neighbours. Although\nthese methods only measure the performance on one down-\nstream task (whole-frame classification), their performance is\nindicative of the utility of the embedding space on other tasks.\nHowever, these methods still require labelled test datasets,\nwhich can be challenging to acquire.\nOur proposal for monitoring learning progress without\nlabels is based on the conjecture that due to the semantic\nmeaning in learned embedding space, similar samples should\nincreasingly group together over learning. Hence, a core part\nof our evaluation approach relies upon employing k-means\nclustering on sample embeddings and characterization of the\nclusters using traditional clustering metrics. We also look to\nthe agreement between two independent k-means clustering\nattempts to provide an intuition for clustering consistency. We\nexpect that the more well-formed ground truth clusters are,"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Instance learning\nThe SSL methods we use employ strong augmentations to\ncreate a pretext task. Each sample from a particular dataset\nis augmented twice independently, producing two \"views\" of\nthe sample. The encoder is motivated to generate embeddings\nwhich are robust against the augmentation operation, such\nthat the independent views for the same sample have similar\nembeddings.\n1) SimSiam: With the SimSiam training configuration [3],\nboth views (i and j) pass through the encoder, plus a multi-\nlayer perceptron (MLP) section dubbed the projector. One\nview passes through another MLP dubbed the predictor. The\nnetwork is trained to minimise the distance between the\nprojected representation of one view, u\u2081, and the prediction\ngenerated from the second view, \u00fbj, as given by\n\nLi,j =   \\frac{1}{2} \\cdot (d_{cos}(u_i, \\hat{u}_j) + d_{cos}(u_j, \\hat{u}_i)),\n\nwhere dcos(,) is the cosine similarity distance measure. In\norder to prevent collapse to a trivial solution, updates to\nminimize this loss propagate back only through the prediction\nbranch. We can interpret this process as tasking the model to\npredict the average embedding (in the projector space) over\nall views of a sample, when presented with any one view of\nthat sample. The SimSiam method is displayed in Figure 1a.\n2) SimCLR: Meanwhile, the SimCLR approach uses con-\ntrastive learning, as seen in Figure 1b. Views of other same\nbatch samples are used as negative views and the network must\nlearn embeddings such that two views of the same sample\n(positive views) are close together, whilst negative views are\nfar apart. In the SimCLR formulation, the predictor head is\nnot used and we only need consider the projected view, u.\nFor a given pair of positive views, i and j, in a minibatch of\nN samples, the loss is\n\nL_{i,j} = - \\log \\frac{\\exp \\left(\\text{d}_{\\cos }\\left(u_{i}, u_{j}\\right) / \\tau\\right)}{\\sum_{k=1}^{2 N}\\left(1-\\delta_{i k}\\right) \\exp \\left(\\text{d}_{\\cos }\\left(u_{i}, u_{k}\\right) / \\tau\\right)},\n\nwhere \u03b4 is the Kronecker delta function and \u03c4 is a temper-\nature scaling factor [1]. This loss is also referred to as the\nnormalized temperature-scaled cross entropy loss [1] or as\nInfoNCE [15].\n3) MoCo-v2: The final method MoCo-v2, seen in Fig-\nure 1c, shares the InfoNCE loss function with SimCLR. The\nmain difference between the two methods lies in the use of\na momentum or \"key\" encoder and a memory queue for the\nprojections of negative views, referred to as negative keys.\nHere, the main encoder is referred to as a query encoder.\nThe key encoder parameters are updated as the exponential\nmoving average (EMA) of the query encoder's parameters at\nevery batch. The views processed by the key encoder are then"}, {"title": "III. METHODS", "content": "A. Datasets\nOur networks were trained on either the CIFAR-10 or\nCIFAR-100 dataset [9]. These datasets consist of 32 \u00d7 32 pixel\nnatural RGB images, with either 10 or 100 classes, respec-\ntively. All training was performed on the training partition\nand representations were evaluated using the test partition of\nthese datasets.\nB. Self-supervised learning\nWe used the modified ResNet-18 [11] backbone from\nSimCLR [1] for CIFAR-10 images, with a 512 dimensional\nrepresentation space. During SSL, we augmented the images to\ncreate pairs of views using the CIFAR-10 augmentation stack\nfrom SimCLR [1]. When training SimSiam, we added a three\nlayer projector and a two layer predictor, with a width of 2048\nthroughout except for the bottleneck layer of the predictor,\nwhich had a width of 512. For SimCLR, we instead added a\ntwo-layer projector, with hidden dimension 2048 and output\ndimension 128. These are also the same dimensions we use\nfor the MoCo architectures.\nThe networks were trained with SSL using stochastic gra-\ndient descent (SGD) using a one-cycle learning rate schedule\n[20] with cyclic momentum from 0.85 to 0.95. For SimSiam,\nthe peak learning rate \u03b7 = 0.06, weight decay \u03bb = 5 \u00d7 10\u22124,\nand we trained the network for 800 epochs. For SimCLR,\n\u03b7 = 0.5, \u03bb = 1 \u00d7 10-4, temperature \u03c4 = 0.5, and we\ntrained the network for 1000 epochs. Lastly, for MoCo-v2,\n\u03b7 = 0.06, \u03bb = 5 \u00d7 10\u22124, \u03c4 = 0.1, queue length was 4096,\nEMA multiplier m = 0.99, and the network was trained for\n800 epochs.\nC. Representation evaluation\nEvery 20 epochs of the SSL process, we created a check-\npoint of the model, referred to as a \"milestone\". For each\nmilestone, we passed the test partition samples through the\nencoder backbone to generate a set of 512-d embedding\nvectors, Z512. Typical clustering methods do not work well\non large representation spaces [21], so we reduced Z512 down\nto a 3-d space, using uniform manifold approximation and\nprojection (UMAP) [22] with n = 50 neighbours. We then\nclustered the 3-d UMAP projections of the embedding vectors\nZ3, using k-means with the cosine distance metric and k\u2081 = 10\nor k\u2081 = 100 as per the number of annotated classes in the\ndataset. The cluster labels generated from this clustering are\nreferred to as C1.\nWe evaluated the quality of C\u2081 by measuring the amount of\ninformation about the ground truth labels CGT, contained in\nC1 using AMI(C1; CGT). This serves as a baseline to compare\nour other metrics against.\nWe defined S\u2081 as the silhouette score for the clusters C1,\nevaluated by Euclidean distance for the original embedding\nvectors Z512. We similarly defined SGT as the silhouette score\nof CGT, also for Z512. This measurement provides an upper-\nbound on the utility of S1 that which could be obtained\nwith \"perfect\" cluster assignments."}, {"title": "B. Extrinsic metrics", "content": "Extrinsic metrics are measures of clustering quality based\non an external reference. In this work, we use the mutual infor-\nmation [16] between the cluster labels generated by k-means\nand the ground truth class labels, acting as a benchmark to\ncompare label-free metrics against. We can imagine the class\nand cluster labels to be two discrete random variables. For\ndiscrete random variables X and Y, their mutual information\nis defined as\n\nI(X; Y) = \\sum_{Y \\in \\mathcal{Y}} \\sum_{X \\in \\mathcal{X}} P(x, y) \\log \\left(\\frac{P(x, y)}{P(x)P(y)}\\right)\n\nwhere P(x, y) denotes the joint probability distribution for X\nand Y, while P(x) and P(y) denotes their respective marginal\ndistributions. Intuitively, we can conceptualize mutual infor-\nmation as a measure of how much information we can obtain\nabout Y upon observing X and vice versa.\nFor implementation, we use the adjusted mutual information\n(AMI) score from the scikit-learn library [17]. The AMI\ncorrects for the chance level of mutual information which\nwould be measured given a certain finite number of samples.\nThe AMI between two discrete random variables X and Y is\ndefined as\n\nAMI(X; Y) = \\frac{I(X; Y) - E[I(X^*;Y^*)]}{(H(X) + H(Y))/2 - E[I(X^*; Y^*)]},\n\nwhere H is entropy and the expected information is taken over\na hypergeometric model of X and Y."}, {"title": "C. Intrinsic measures", "content": "Intrinsic measures of clustering quality consider properties\nof the clusters, such as inter-cluster and intra-cluster distances.\nThe silhouette score is one such measure [18]. For each\nsample, i, the silhouette score is defined as\n\nS_i = \\frac{b_i - a_i}{\\max(a_i, b_i)},\n\nwhere b\u2081 is the average inter-cluster distance between sample\ni and the nearest neighbouring cluster's samples, while ai is\nthe average intra-cluster distance from sample i to other same\ncluster samples. The individual silhouette scores are averaged\nover all samples to provide an overall silhouette score.\nOur work differs from deep clustering [19] in that rather\nthan incorporating clustering into the training process, we are\nlooking to evaluate a trained model (of varying degrees) using\nclustering. Furthermore, a major goal is to dissociate model\nevaluation from label schemes and therefore we cannot rely\nupon the use of extrinsic measures which take into account\nground truths. We use such extrinsic measures only as a means\nof comparison against investigated label-free methods."}, {"title": "D. Pre-trained models", "content": "We loaded pre-trained models provided in torchvision [14],\nwhich had been trained on ImageNet-1k. We used models with\nResNet, DenseNet, and EfficientNet architectures of varying\nsizes: ResNet 18, 34, 50, 101, and 152; DenseNet 121, 161,\n169, and 201; EfficientNet (v1) sizes bo through b7. The\nsame methodology as described above was applied, using the\npre-trained model as a (frozen) encoder, with the following\nchanges. (1) CIFAR-10 and -100 images were upscaled to the\nsame resolution as that which the model was trained on. (2)\nDue to the significantly larger image resolutions for the bigger\nEfficientNet models, we reduced all batch sizes to 48. (3) The\nmaximum learning rate was reduced to 0.003 for LPs. (4) The\nentropy bin width was increased to li = 0.80i, because the\ndistribution in the representation space was found to be up to\ntwice as large for SL pre-trained models when compared with\nSSL."}, {"title": "E. Linear probe", "content": "For each milestone, we extracted the encoder from the\nnetwork and froze its weights. We added a linear layer on\ntop of the encoder and used the training partition to learn a\nlinear mapping from the embedding space to the target labels.\nThe linear layer was trained with the Adam optimizer [23], for\n20 epochs, using a one-cycle learning rate schedule with peak\nlearning rate of 0.08, cyclic momentum from 0.85 to 0.95,"}, {"title": "IV. RESULTS", "content": "We measured the Pearson correlation over training mile-\nstones between our metrics and the LP accuracy, the results\nof which are shown in Table I. Similarly, we measure the\ncorrelation for the torchvision pre-trained models separately\nfor each architecture type and in a general \u201coverall\u201d case,\npresented in Table II. The overall case is intended to be a\nmeasure of how feasible a metric may be for cross-architecture\ncomparison. The notations used are as in the previous section,\nwith subscripts GT indicating ground truth labels and 1\nindicating clusters C1. Only metrics pertaining to C\u2081 require\nk-means clustering to be performed.\nWe observe that AMI(C1, CGT) correlates strongly with LP\naccuracy throughout all combinations of methods and datasets.\nThe metric also performs well across the pre-trained models\nbut could not be extended to cross-architecture evaluation, as\nevidenced by its weak correlation in the overall case. This\nresult demonstrates that clustering is indeed able to progres-\nsively pick out ground truth classes as clusters, reflective of\nlearning progression with the likely caveat of being limited to\nsame-architecture comparisons. Although the silhouette score\nSGT remains near zero (|SGT| < 0.05) and assigns a poor\nscore to the ground truth interpreted as clusters, we find it is\nwell correlated with the LP measurements in SimSiam and\nSimCLR cases. However, it appears to be inconsistent for\nMoCo-v2 as well as potentially for EfficientNet and DenseNet\narchitectures, suggesting that the embedding clustering shapes\neven when representing ground truth, may not progress in a\nmanner reflective of learning progression.\nOur results show that the label-free metrics we investigated\nonly present weak correlations with the label-based scores\non average. More specifically, the silhouette score S\u2081 did\nnot correlate well with LP accuracy. As is the case with\nits ground truth counterpart, it also assigned poor scores to\nminimally separated clusters. We also found that there could\nbe a large change in the correlation if the initial network state\nis dropped from the correlates. Due to the non-linearity of\nlearning progression, equally spacing out milestones in terms\nof epochs can lead to a relatively lower number of data points\nreflective of early training. As such, early training progression\nwhich may differ in nature from later training progression,\nmay not be as sufficiently captured.\nAlthough clustering agreement AMI(C1, C2), was corre-\nlated with LP accuracy for SimCLR and to a lesser extent,\nMoCo-v2, it was either not correlated or inversely correlated\nwhen training with SimSiam. This observation may be tied\nto the unexpected result that entropy increases for SimSiam\nbut decreases for SimCLR. We hypothesis that this may be\nbecause SimSiam is more susceptible to dimensional collapse\n[25], [26]. In future work, this could be confirmed by observ-\ning the singular value spectra of the projector and predictor\nembedding spaces."}, {"title": "V. CONCLUSION", "content": "The label-free metrics we propose in this paper are generally\nindicative of the quality of the embedding space as measured\nwith downstream classification when training the network with\nSimCLR or MoCo-v2. However, none of our metrics were able\nto consistently measure the utility of the embedding space\nlearned with SimSiam. If the cause for why entropy reverses\ndirection depending on methodology can be identified, entropy\nmay be a viable means of label-free learning monitoring or\npotentially a means to compare different models. Further work\nis needed to resolve why the metrics work relatively well only\nwith some SSL methodologies and to test them on additional\nmethods.\nAs cross-architecture measures of model quality, clustering-\nbased metrics appear to be insufficient as even with ground\ntruth, we were unable to obtain strong results in an overall\nscenario encompassing ResNet, DenseNet, and EfficientNet\narchitectures. However, there is some evidence that entropy\nmay be architecture-independent given the significant result\nfor CIFAR-100 in this overall case and by visual examination\nof entropy behaviour compared to LP accuracy. Additional\narchitecture examples would help establish greater confidence\nin our results."}]}