{"title": "AstroM\u00b3: A self-supervised multimodal model for astronomy", "authors": ["M. Rizhko", "J. S. Bloom"], "abstract": "While machine-learned models are now routinely employed to facilitate astronomical inquiry, model inputs tend to be limited to a primary data source (namely images or time series) and, in the more advanced approaches, some metadata. Yet with the growing use of wide-field, multiplexed observational resources, individual sources of interest often have a broad range of observational modes available. Here we construct an astronomical multimodal dataset and propose AstroM\u00b3, a self-supervised pre-training approach that enables a model to learn from multiple modalities simultaneously. Specifically, we extend the CLIP (Contrastive Language-Image Pretraining) model to a trimodal setting, allowing the integration of time-series photometry data, spectra, and astrophysical metadata. In a fine-tuning supervised setting, our results demonstrate that CLIP pre-training improves classification performance for time-series photometry, where accuracy increases from 84.6% to 91.5%. Furthermore, CLIP boosts classification accuracy by up to 12.6% when the availability of labeled data is limited, showing the effectiveness of leveraging larger corpora of unlabeled data. In addition to fine-tuned classification, we can use the trained model in other downstream tasks that are not explicitly contemplated during the construction of the self-supervised model. In particular we show the efficacy of using the learned embeddings for misclassifications identification, similarity search, and anomaly detection. One surprising highlight is the \"rediscovery\" of Mira subtypes and two Rotational variable subclasses using manifold learning and dimension reduction algorithm. To our knowledge this is the first construction of an n > 2 mode model in astronomy. Extensions to n > 3 modes is naturally anticipated with this approach.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the vast volumes of publicly available raw astronomical data, with a few notable subfield exceptions, the application of machine learning to discovery and inference has yet to broadly permeate the field. One impediment stems from the challenge of fusing data across heterogeneous modes of collection. Off-the-shelf architectures do not easily accommodate a mixture of irregularly sampled multi- spectral multi-scale heteroskedatic time-series data, images, spectra, and metadata. Another issue, arising in the classification context, is that very few ground-truth labels exist. This \"small label\" problem arose, for example, in Richards et al. (2012), who sought to proba- bilistically classify 50,124 variable stars using only 810 labels over 28 classes. Last, models learned on a dataset from one survey do not eas- ily transfer to other data collected on the same objects from different surveys (e.g., Long et al. 2012; Kim et al. 2021). Our self-supervised multimodal architecture addresses the first two challenges, establish- ing methods and milestones for a more generalized foundation model applicable to inference tasks on unseen survey data.\nOur work builds upon the Contrastive Language-Image Pretraining (CLIP) framework, originally introduced by Radford et al. (2021);\nCLIP demonstrated the power of contrastive learning on large-scale image and text datasets to learn joint representations. Since its in- troduction, CLIP has been extensively researched and improved in various ways. For example, Li et al. (2021) enhanced data efficiency through supervision, while Yao et al. (2021) focused on improving semantic alignment. Cherti et al. (2023) introduced scaling laws, and Sun et al. (2023) optimized the model for faster training. Addition- ally, CLIP has been combined with other pretraining objectives: Mu et al. (2022) incorporated image self-supervision, and Singh et al. (2022) along with Li et al. (2022) added masked multimodal, image, and language modeling. Furthermore, CLIP has been extended to other modalities: audio-text (Wu et al. 2023), video-text (Luo et al.\n2021; Xu et al. 2021; Ma et al. 2022), and point cloud-text (Zhang et al. 2022). In the astronomical context, Parker et al. (2024) used dual-mode CLIP on static-sky galaxy images and spectra. Closest to the approach of our work outside of astronomy, Guzhov et al. (2022) adapted CLIP for use with three modalities: audio, image, and text. Given the proven versatility and success of CLIP in differ- ent domains, we build upon it herein. We extend CLIP to work on three modalities: time-series photometry, spectra, and metadata (see Figure 1). Our work, and a recent preprint from Zhang et al. (2024), are the first efforts to incorporate time-series data with CLIP, and our"}, {"title": "2 RELATED WORK", "content": "Early classification-focused research used hand-crafted features of time-series photometry and metadata with decision forests in a su- pervised context (Debosscher et al. 2007; Richards et al. 2011; Du- bath et al. 2011; Palaversa et al. 2013). Neural network approaches to learn representations of time-series photometry (both in super- vised and self-supervised contexts) then achieved state of the art, first with flavors of RNNs (e.g., LSTMs: Naul et al. 2018, GRUs: Muthukrishna et al. 2019; Becker et al. 2020) and more recently with convolution (Jamal & Bloom 2020; Boone 2021) and Transformers (Donoso-Oliva et al. 2023; Leung & Bovy 2024). CNNs have been used to achieve state of the art classification on galaxy spectra (e.g., GalSpecNet: Wu et al. 2024a). Hayat et al. (2021) use CNN autoen- coders with contrastive learning for self-supervised embedding of galaxy images.\nAstroCLIP (Parker et al. 2024) fused pre-trained embeddings of galaxy spectra and images with constrastive learning and showed the trained model to be competitive with purpose-built classification models. Our work differs from AstroCLIP in that 1) our primary objects are individual sources that vary in time (i.e. not static like"}, {"title": "3 DATASET ASSEMBLY", "content": "The basis of our observational dataset is the variable star catalog (Jayasinghe et al. 2019) observed and curated by the All-Sky Au- tomated Survey for SuperNovae (ASAS-SN) project (Shappee et al. 2014). We downloaded the lightcurve data from the 2021 assem- bly of the 687,695 v-band variables and the 2022 assembly of the 378,861 g-band variables, along with the associated metadata cata- logs. These catalogs contain cross-matched photometry information for each source from WISE (Wright et al. 2010), GALEX (Morris-"}, {"title": "4 METHOD", "content": "Our objective is to develop a self-supervised multimodal model that can learn from astronomical data across three distinct modal- ities: time-series photometry, spectra, and astrophysical metadata. To achieve this, we extend the Contrastive Language-Image Pretrain- ing (CLIP) framework (Radford et al. 2021) to a trimodal setting, enabling simultaneous learning from multiple data types. In this sec-"}, {"title": "4.1 Photometric Time-Series Model", "content": "Photometric time-series data are flux measurements of astronomical objects over time. To effectively capture the temporal dependencies and handle sequences of varying lengths, we employ the Encoder component from the Informer model (Zhou et al. 2021).\nModel Architecture. The photometric time-series encoder con- sists of:\n\u2022 Input Embedding Layer: Projects the input features to a higher- dimensional space.\n\u2022 Informer Encoder Layers: Eight encoder layers with a hidden dimension of 128, four attention heads, and a feedforward dimension of 512.\n\u2022 Output Layer: Produces a fixed-length embedding representing the input time-series data.\nData Preprocessing. Each light curve is a sequence of flux measurements f = {f\u2081, f\u2082,..., f\u209c} and flux errors of {\u03c3f\u2081, \u03c3f\u2082,..., \u03c3f\u209c} at corresponding times t = {t\u2081, t\u2082,..., t\u209c}. We normalize the flux by subtracting the mean \u03bcf and dividing by the median absolute deviation MADf: fi =  f\u1d62-\u00b5f/MADf. Flux errors are nor- malized by the flux median absolute deviation division: \u03c3fi = \u03c3f\u1d62/MADf . Time is scaled between 0 and 1 for each light curve:  dt = tmax - tmin; ti = t\u1d62/dt. Auxiliary features such as amplitude, period, Lafler- Kinmann string length statistic (Lafler & Kinman 1965), peak-to- peak variability, delta time 35 and logarithm of median absolute deviation log MADf are included as additional inputs.\nHandling Variable Sequence Lengths. We set a maximum se- quence length of L = 200. Sequences longer than this are randomly cropped during training and center-cropped during validation and testing. Shorter sequences are padded with zeros, and an attention mask is used to differentiate between valid data and padding."}, {"title": "4.2 Spectra Model", "content": "Spectral data provides detailed information about the composi- tion and physical properties of astronomical objects. We adapt the GalSpecNet architecture (Wu et al. 2024b), which is specifically designed for processing one-dimensional astronomical spectra.\nModel Architecture. The spectra encoder consists of:\n\u2022 Convolutional Layers: Four layers (64, 64, 32, 32 channels) followed by ReLU activations.\n\u2022 Pooling Layers: Max-pooling layers after each convolutional layer except for the last one.\n\u2022 Dropout Layer: Applied after the last convolutional layer for regularization.\n\u2022 Output Layer: Generates a fixed-length embedding of the spec- tral data.\nModifications. We reduce the last three fully connected layers to a single one for classification or omit it entirely when using the model as a feature extractor. We also add additional input channels for spectra errors and auxiliary data."}, {"title": "4.3 Metadata Model", "content": "The metadata modality consists of astrophysical parameters and ob- servational data not included in the other two modalities. This in- cludes features like absolute magnitudes in various bands, astromet- ric information, and other cross-matched catalog data. A full list of features and their descriptions is provided in Table A1.\nModel Architecture. The metadata encoder is a Multilayer Per- ceptron consisting of:\n\u2022 Input Layer: Accepts the 34 preprocessed features.\n\u2022 Hidden Layers: Two hidden layers with 512 units each followed by ReLU activations.\n\u2022 Dropout Layers: Applied after hidden layers for regularization.\n\u2022 Output Layer: Provides a fixed-length metadata embedding.\nData Preprocessing. Except for the steps already mentioned dur- ing the dataset assembly (see 3), we apply logarithm to period and then standardize each feature to have zero mean and unit variance."}, {"title": "4.4 AstroM\u00b3: Multi-modal CLIP Model", "content": "To integrate the three modalities we extend the CLIP model to a trimodal setting and name the entire architectural approach as AstroM\u00b3. The goal is to learn a shared embedding space where representations from different modalities corresponding to the same astronomical object are close together (see Figure 1).\nProjection Heads. Each modality has its own architecture, pro- ducing embeddings of different sizes. To bring these embeddings into a shared space, we apply a projection head to each modality. The projection head is a fully connected layer that maps the embeddings to a fixed size of 512. Let the original embeddings of photometry, spectra, and metadata be denoted as Pi, Si, and Mi, where i denotes the i-th sample in a batch of size N. The projection heads transform these original embeddings as follows:\nPi = WpPi + bp (1)\nSi = WsSi + bs (2)\nMi = WM Mi+bM, (3)\nwhere Wp, Ws, and WM are the weight matrices, and bp, bs, and bM are the bias terms for the projection head of each modality. After applying these transformations, the projected embeddings Pi, Si, and Mi all have a fixed size of 512, making them suitable for comparison in the shared embedding space.\nPairwise Similarity Matrices. For each pair of modalities (photometry-spectra, spectra-metadata, metadata-photometry) we"}, {"title": "5 RESULTS", "content": "We evaluated the models on downstream classification across four modes: photometry only, spectra only, metadata only, and all modal- ities combined. For single modalities, we added a fully connected layer on top of the respective encoders for classification. In the mul- timodal setting, we averaged the embeddings from all three modali- ties and then applied a fully connected layer for classification. Each model was trained in two ways: with CLIP pre-training, where the model was initially trained using the CLIP framework and then fine- tuned for the downstream task, and without CLIP pre-training, where models were trained directly on the task with randomly initialized weights. Importantly, model architecture and setup were identical across all conditions, differing only in the initialization of weights. The training setup and hyperparameter search process are detailed in Appendix A. All models were cross-validated using 5 random seeds and data splits for robust evaluation."}, {"title": "5.1 CLIP Evaluation", "content": "The results in Table 2 show that while there is no statistically signif- icant difference between using CLIP and not using CLIP for spectra,"}, {"title": "5.2 Limited Labeled Data", "content": "To evaluate the effectiveness of CLIP pre-training when the availabil- ity of labeled data is limited, we conducted experiments on smaller subsets of the original dataset. Specifically, we created subsets con- taining 10%, 25%, and 50% of the data by downsampling the most common classes, ensuring a balanced class distribution. Table 3 pro- vides details on the class distribution across these subsets. Note that we choose to downsample the overrepresented sources at random. An interesting alternative to this, to approximate the ways in which brighter sources preferentially are easier to label on new survey data, would be to select only the brightest (or highest signal-to-noise) sources to include in the training data.\nModels. For each subset, we retrained all models, with and without CLIP pre-training, using the same optimization settings and hyper- parameter search as previously applied. It is important to note that the CLIP model used for these experiments was the same as before: pre-trained on the full dataset without using any labels. This setup is designed (for future applications) to leverage large amounts of unla- beled data for pre-training and then fine-tuning the model on smaller labeled datasets.\nResults. The results in Table 4 demonstrate that CLIP pre-training"}, {"title": "5.3 UMAP Analysis", "content": "We use Uniform Manifold Approximation and Projection (UMAP) method (McInnes et al. 2018) to visualize how well our model dis- tinguishes among classes in the embedding space. UMAP is fit on the averaged embeddings across all modalities from the training set, and projections are generated for both the training (Figure 2a) and the test (Figure 2b) sets. The results show that:\n\u2022 Most classes are well separated, though Detached Algol-type binaries (EA), \u03b2 Lyrae-type binaries (EB) and W Ursae Majoris type binaries (EW) partially overlap. This is expected on a physical basis, as these are all types of binary stars and share similar characteristics\u00b9.\n\u2022 As expected, the test set follows the same UMAP projection structure as the training set. For instance, Spotted Variables with"}, {"title": "5.4 Modalities Importance", "content": "To evaluate the importance of each modality in our CLIP classifi- cation model, we exploit the ability to utilize any combination of available modalities during testing. This flexibility is achieved by av- eraging the embeddings before the fully connected layer-rather than concatenating them and by learning a shared embedding space. We calculate the class-wise accuracy percentages for each modality in- dividually, for every pairwise combination, and for all modalities combined."}, {"title": "5.5 Similarity Search", "content": "An additional strength of our approach is the ability to perform sim- ilarity or dissimilarity searches within the embedding space. This expands the utility of the CLIP-based model beyond classification to serve as a versatile tool for exploratory data analysis, anomaly detection, and multimodal inference. This capability holds promise for aiding the discovery of rare or unexpected phenomena in astro- nomical data.\nModality-Specific Similarity Search. Our model allows to find similar objects based on a chosen modality. For example, if we want to find objects with spectral features similar to those in Figure 4a, we can embed the spectrum of that object and compute the cosine similarity with other objects in the dataset (where a cosine similarity of 1 indicates maximum similarity). Figure 8 shows the two most similar objects based solely on spectral similarity, with cosine similarities of"}, {"title": "Outlier Detection.", "content": "Beyond UMAP-based analysis, we can identify outliers using all 512 features of the embedding space. This allows us to detect (1) misclassifications, (2) in-class outliers, and (3) complete outliers that do not belong to any known class. To identify (1) and (2), we can calculate class centroids by averaging all embeddings for each class. We then build a cosine distance distribution for each class and set a threshold, such as the 99th percentile. Any object with a cosine distance from its class centroid exceeding this threshold can be labeled as an outlier. This process can be performed separately for each modality, and the results can be further refined by marking only those objects that are identified as outliers in more than one modality. For (3), we can apply DBSCAN clustering on the entire set of embeddings without using explicit labels, marking any object that falls outside the main clusters as a complete outlier."}, {"title": "6 CONCLUSION", "content": "We present the curation of a large labeled dataset suitable for build- ing and testing next-generation multi-modal self-supervised mod- els. This includes 21,440 objects with time-series photometry, spec- tra, and metadata. We also introduce AstroM\u00b3 self-supervised pre- training framework that leverages all three data modalities. By ex- tending the Contrastive Language-Image Pretraining model to handle a trimodal setting, our approach effectively learns joint representa- tions across diverse astronomical data types, enhances classification accuracy, and leverages unlabeled data to improve performance when labeled data is limited. Beyond classification, AstroM\u00b3 demonstrates versatility in tasks such as misclassification detection and in-class outlier identification. Additionally, it shows promise for scientific discovery by \"rediscovering\" different Mira types and Rotational variables subclasses, and enables efficient searches by identifying similar objects, cross-modality contrasts, or cross-modality similar- ities-facilitating targeted exploration of specific sources.\nFuture Work. To be clear, while our approach outperforms clas- sification tasks on the dataset we have curated, we are not claiming that AstroM\u00b3 has been shown to achieve state-of-the-art on classifica- tion of time-variable sources in general-the application of AstroM\u00b3 to existing photometric benchmark datasets from other surveys is a clear next step. There are several other directions for extending our framework beyond AstroM3. Given the abundance of photometry and metadata compared to spectra, one key area is to develop an algorithm capable of handling missing modalities during training, allowing us to leverage all available photometry and metadata. Additional di- rections include expanding the framework to integrate even more modalities, such as photometry from other bands and human com- ments on sources; learning to manage varying and missing metadata; and incorporating new classes, including non-periodic ones. Build- ing a larger, more diverse dataset and applying the models to tasks like prediction and anomaly detection are essential next steps toward creating a truly foundational multimodal model for astronomy."}, {"title": "DATA AVAILABILITY", "content": "All code, model weights, parameters, and data will be made available upon acceptance. This includes the code for defining the models, as well as the code used for training and evaluation, the hyperparame- ters applied during training, the trained model weights, and the code"}, {"title": "APPENDIX A: TRAINING SETUP AND HYPERPARAMETERS", "content": "In this work, we used Optuna (Akiba et al. 2019) to perform hyperpa- rameter optimization for our models. Our goal was to minimize the validation loss across multiple architectures and pre-training strate- gies. We tuned CLIP itself, as well as models for photometry, spectra, metadata, and multimodal data, with two initialization options: ran- dom initialization or pre-trained CLIP weights.\nFor each model type, the hyperparameters we explored included:\n\u2022 Learning rate (lr): Sampled from a logarithmic scale between 1 \u00d7 10\u207b\u2075 and 1 \u00d7 10\u207b\u00b2\n\u2022 Dropout rates for photometry (p_dropout), spectra"}]}