{"title": "Large Language Models for Multi-Robot Systems: A Survey", "authors": ["Peihan Li", "Zijian An", "Shams Abrar", "Lifeng Zhou"], "abstract": "The rapid advancement of Large Language Models (LLMs) has opened new possibilities in Multi-Robot Systems (MRS), enabling enhanced communication, task planning, and human-robot interaction. Unlike traditional single-robot and multi-agent systems, MRS poses unique challenges, including coordination, scalability, and real-world adaptability. This survey provides the first comprehensive exploration of LLM integration into MRS. It systematically categorizes their applications across high-level task allocation, mid-level motion planning, low-level action generation, and human intervention. We highlight key applications in diverse domains, such as household robotics, construction, formation control, target tracking, and robot games, showcasing the versatility and transformative potential of LLMs in MRS. Furthermore, we examine the challenges that limit adapting LLMs in MRS, including mathematical reasoning limitations, hallucination, latency issues, and the need for robust benchmarking systems. Finally, we outline opportunities for future research, emphasizing advancements in fine-tuning, reasoning techniques, and task-specific models. This survey aims to guide researchers in the intelligence and real-world deployment of MRS powered by LLMs. Based on the fast-evolving nature of research in the field, we keep updating the papers in the open-source Github repository.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Large Language Models (LLMs) has significantly impacted various fields, including natural language processing and robotics. Initially designed for text generation and completion tasks, LLMs have evolved to demonstrate problem-understanding and problem-solving capabilities [95, 108]. This evolution is particularly vital for enhancing robot intelligence by enabling robots to process information and make decisions on coordination and action accordingly [38, 42]. With these capabilities, robots can more effectively interpret complex instructions, interact with humans, collaborate with robotic teammates, and adapt to dynamic environments [91]. As robotic systems evolve toward more sophisticated applications, integrating LLMs has become a transformative step, bridging the gap between high-level reasoning and real-world robotic tasks.\nOn the other hand, Multi-Robot Systems (MRS), which consist of multiple autonomous robots working collaboratively [8, 73], have shown great potential in applications such as environmental monitoring [20, 64, 86], warehouse automation [52, 76, 87], and large-scale exploration [10, 22]. Unlike single-robot systems, MRS leverages collective intelligence to achieve high scalability, resilience, and efficiency [73]. The distributed nature of tasks across multiple robots allows these systems to be cost-effective by relying on simpler, specialized robots instead of a single highly versatile one. Moreover, MRS provides increased robustness, as the redundancy and adaptability of the collective can often mitigate the failures of individual robots [56, 116]. These features make MRS indispensable in scenarios where the scale, complexity, or risk is beyond the capabilities of a single robot.\nDespite their importance, MRS introduces unique challenges, such as ensuring robot communication, maintaining coordination in dynamic and uncertain environments, and making collective decisions that adapt to real-time conditions [6, 25]. Researchers are working to integrate LLMs into MRS to address the unique challenges associated with deploying and coordinating MRS [15, 65]. For example, effective communication is essential for the MRS to share knowledge, coordinate tasks, and maintain cohesion in the dynamic environment among individual robots [25]. LLMs can provide a natural language interface for inter-robot communication, allowing robots to exchange high-level information more intuitively and efficiently instead of predefined communication structures and protocols [65]. Furthermore, the problem-understanding and problem-solving abilities of LLM can enhance the adaptability of MRS when given a particular goal without specific instructions. The LLMs can understand the mission, divide it into sub-tasks, and assign them to individual robots within the team based on their capabilities [12, 57]. The generalization ability across different contexts of LLMs can also allow MRS to adapt to new scenarios without extensive reprogramming, making them highly flexible during the deployment [94, 104].\nThe application of LLMs in MRS also aligns with the growing need for human-robot collaboration [37]. As the operators often do not have expertise in robot systems, using LLMs as a shared interface can enable operators using natural languages to communicate and command the robots to make decisions and complete complex real-world missions [2]. These capabilities enhance the efficiency of MRS and broaden their applicability to domains requiring close human-robot collaboration."}, {"title": "2 Backgrounds", "content": "This section provides background knowledge on MRS and LLMs. While several other research papers have discussed on the applications of LLMs in robotic systems, they do not specifically focus on the MRS. We will summarize their contributions and discuss why our survey on facilitating LLMs with MRS is necessary and impactful.\n2.1 Multi-Robot Systems\nA MRS consists of multiple robots that collaborate to complete specific tasks. Unlike single-robot systems, MRS leverages the combined capabilities of multiple robots to perform complex tasks more efficiently, reliably, and flexibly [5, 75, 114]. These systems are commonly employed in applications such as search and rescue [8, 44, 61, 73], target tracking [51, 54, 106, 111, 113], environmental monitoring [28, 78], coverage and exploration [10, 11, 58, 80, 81], and warehouse automation [4, 98], where the task's scale or complexity exceeds a single robot's capabilities. When all robots in the team are identical and share the same functionality, the team is called a homogeneous multi-robot team. In contrast, a heterogeneous multi-robot team consists of different types of robots [11, 71, 79]. The advantages of MRS include enhanced scalability, as tasks can be distributed among robots [13, 55, 110], and increased resilience, as the failure of one robot can often be mitigated by the others [50, 56, 66, 74, 82, 109, 112, 115\u2013117]. In contrast to designing a single, highly versatile robot, MRS usually relies on more uncomplicated, task-specific robots, reducing the cost and complexity of individual units while benefiting from collective intelligence [39]. However, these systems also present unique challenges, particularly in communication, coordination, and decision-making, as robots must operate cohesively in dynamic and uncertain environments [73]. Two primary control paradigms are commonly employed to manage the interaction and task distribution within an MRS: centralized and decentralized controllers [16, 100]. In a centralized controller, a single controller receives all the information and directs the actions of all robots in the system, allowing for optimized coordination and global planning. However, centralized systems can become a bottleneck when the group size increases and are vulnerable to single points of failure [60]. On the other hand, a decentralized controller distributes decision-making among the roots, enabling the robots to operate resiliently [75]. This approach enhances scalability and resilience but often introduces additional complexity to ensure seamless communication and coordination between robots. The choice between centralized and decentralized control depends on the specific application requirements, environmental conditions, and the desired balance between efficiency and robustness [100].\n2.2 Large Language Models\nLLMs are deep learning models with millions to billions of parameters [107]. Initially, the application of the LLMs is for text completion based on the context or text generation from the user's instruction [108]. LLMs are trained using an extensive collection of text from books, articles, websites, and other written sources. During this training process, LLMs learn to predict the next word in a sentence or fill in missing information using the attention mechanism [88]. This pre-training phase enables"}, {"title": "2.2.1 Fine-tuning and RAG", "content": "While LLMs are pre-trained on a diverse dataset for general tasks, the performance in specialized cases can be unideal since the training dataset might not fully cover the special usages [18, 119]. People can prepare a dataset dedicated to the specialized tasks and retrain the model. However, retraining the entire model is often challenging due to the limited computing resources and numerous parameters within the model [18]. One solution to address this issue is to use techniques like low-rank adaptation (LoRA) to fine-tune the LLMs with limited computational resources [33]. LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture [88], significantly reducing the number of trainable parameters for the downstream tasks.\nOn the other hand, retrieval-augmented generation (RAG) is an alternative technique that integrates external knowledge sources to increase the zero-shot accuracy of the LLMs on specialized tasks [34, 47]. RAG addresses a key limitation of LLMs' reliance on pre-trained, static knowledge, which may not include domain-specific or up-to-date information. By combining a retrieval mechanism with the generative capabilities of LLMS, RAG allows the model to query external databases or knowledge repositories to retrieve relevant information during runtime [23]. This retrieved data is then used to guide the model's response, enhancing its accuracy and applicability in specialized contexts. For instance, RAG can provide real-time access to task-specific knowledge or environmental updates for robots, enabling better decision-making in dynamic scenarios [118]. Although RAG introduces additional complexity, such as managing retrieval latency and ensuring data relevance, it offers a powerful method for bridging the gap between static pre-trained knowledge and the dynamic requirements of real-world applications."}, {"title": "2.2.2 Multimodal LLMS", "content": "Traditional LLMs excel at processing and generating text but fall short in scenarios where understanding multiple data types is essential. Recent progress on multimodal LLMs addresses this limitation by incorporating diverse modalities, enabling them to combine textual inputs with visual, auditory, or other sensory data [102]. These models align information from different modalities into a shared semantic space, allowing for seamless integration and contextual understanding. For instance, a multimodal LLM can process visual data from a robot's camera alongside textual commands to identify objects, navigate environments, or perform complex tasks [42, 92]. This ability to synthesize information across modalities significantly enhances their applicability, especially in robotics, where real-world interactions demand integrating various data types. By leveraging multimodal capabilities, these models push the boundaries of what LLMs can achieve, offering a new level of flexibility and adaptability."}, {"title": "2.3 Related Survey Papers", "content": "Several survey papers have applied LLMs in the robotics and multi-agent field. Firoozi et al. [21], Zeng et al. [107], and Kim et al. [42] all explored how LLMs and foundation models could enhance robotics in areas like perception, decision-making, and control. While they share this focus, their approaches and scopes differ. Firoozi et al. [21] provided a broad overview of foundation models in robotics, emphasizing their adaptability across various tasks but without specific attention to MRS. Zeng et al. [107] focused on the applications of LLMs in robotics, categorizing their impact on single-robot systems in areas like control and interaction without exploring collaborative systems. Wang et al. [92] concentrated on summarizing the applications of LLMs for manipulation tasks for a single robot. Kim et al. [42] divided LLM applications into communication, perception, planning, and control, offering practical guidelines for integration, but their work is also centered on single-robot applications. Hunt et al. [37] explored the use of language-based communication in robotics, categorizing applications of LLMs based on their roles in robotic systems, such as tasking robots, inter-robot communication, and human-robot interaction. Their focus is primarily on language as a medium for interaction without addressing the unique challenges of MRS. Guo et al. [30] reviewed LLM-based multi-agent systems, exploring their applications in problem-solving and world simulation. Although their work included embodied agents, their emphasis is on general multi-agent frameworks, which focus on abstract roles and interactions within systems that may not require physical embodiment or real-world interaction. Kawaharazuka et al. [41] examined real-world applications of foundation models in robotics, focusing on replacing components within robotic systems but without addressing inter-robot collaboration or the collective intelligence of MRS.\nNone of these surveys address the challenges and opportunities of integrating LLMs into MRS. While multi-agent systems provide a generalized framework for understanding roles and interactions, they are often abstract and virtual, lacking the physical embodiment and real-world constraints that characterize MRS [30]. MRS requires actual physical robots to collectively perceive, decide, and act within dynamic and uncertain environments, posing unique challenges in communication, coordination, and decision-making that go beyond the scope of virtual agents [90]. Moreover, MRS uniquely benefits from improved scalability, failure resilience, and cost-effective collective operations, making them fundamentally different from single-robot systems or general multi-agent frameworks. This gap highlights the need for a dedicated survey that explores how LLMs can facilitate communication, coordination, and collaborative task execution in MRS, providing critical insights into this emerging and impactful area of research."}, {"title": "3 Communication Types for LLMs in Multi-robot Systems", "content": "LLMs demonstrate remarkable abilities in understanding and reasoning over complex information. However, their performance can significantly vary depending on"}, {"title": "4 LLMs for Multi-robot Systems", "content": "In this section, we categorize the applications of LLMs in MRS into high-level task allocation, mid-level motion planning, low-level action generation, and human intervention scenarios. High-level task planning involves tasks that demand a higher degree of intelligence, such as task allocation and planning among multiple robots, where the LLM is required to exhibit logical reasoning and decision-making capabilities. Mid-level motion planning refers to navigation or path-planning scenarios. Low-level action generation uses LLMs to generate and directly control robots' posture or motion. On the other hand, human intervention involves using LLMs to interact with human operators and guide task planning and execution. Table 1 shows the list of papers based on those four categories.\n4.1 High-Level Task Allocation and Planning\nHigh-level task planning leverages LLMs' advanced reasoning and decision-making capabilities to handle complex and strategic tasks. This scenario often requires allocating tasks among robot teams, developing comprehensive task plans, or solving problems requiring contextual understanding and logic. Here, we explore studies illustrating LLMs' capability in these sophisticated domains.\nRecent work has demonstrated that LLMs are capable of allocating tasks among multiple robots. Wu et al. [97] proposed a hierarchical LLMs framework consisting of two layers to solve the multi-robot multi-target tracking problem. In this scenario,"}, {"title": "4.1.1 Multi-Robot Multi-Task", "content": "In the multi-robot multi-task scenarios, a team of robots is tasked with completing multiple objectives simultaneously. LLMs play a critical role in devising actionable and efficient task distribution strategies in such settings. By interpreting high-level instructions and understanding the context of each task, LLMs can dynamically allocate tasks among robots, ensuring optimal utilization of resources and effective collaboration. This capability enables multi-robot teams to handle complex, multi-faceted operations with increased precision and adaptability.\nLakhnati et al. [45] proposed a framework where three heterogeneous robots aim to accomplish complex tasks instructed by human operators in VR simulation. First, each robot LLM is given an initial prompt to clarify its role and abilities. A central controller LLM analyzes human descriptions of the task and distributes them to the respective robots. Instructions from human operators can either directly specify what each robot should do (e.g., \u201cJupiter needs to move to the dumbbell and pick it up, Neptune and Pluto have to move to the fridge.\") or describe the tasks without assigning to specific robots (e.g., \"Three dinner plates have to be put into the trash, and all agents need to end up next to the garbage bin.\"). Following this line, Chen et al. [12] proposed a centralized framework where an LLM controller distributes the human instructions to a multi-robot team. They aim to make a heterogeneous multi-robot team accomplish multiple heterogeneous household tasks. However, the task distribution process they introduced is in the form of a discussion between the \"Central Planner\" LLM and the robot-dedicated agent LLM on each robot. The original task information is a geometric representation from a SLAM system. It is constructed into a scene context to prompt LLM. The \"Central Planner\" LLM first assigns each task to each robot according to its analysis. Then, each robot-dedicated agent LLM provides feedback according to the assigned task, and its robot resume is generated from the robot's URDF code by the robot-dedicated agent LLM. If the task does not match the robot's resume, it prompts the \"Central Planner\" for a reassignment. This discussion between LLMs continues until no reassignments are required. Chen et al. [15] took a step further to investigate the scalability of an LLM-based heterogeneous multi-task planning system. The efficiency and accuracy of four different communication architectures are compared, as shown in Fig. 3 under four distinct environments, including BoxNet, warehouse, and BoxLift. The results demonstrate that the HMAS-2 structure achieves the highest success rate while CMAS is the most token-efficient. On the other hand, Gupte et al. [31] proposed an LLM-based framework to solve Initial Task Allocation for a multi-robot multi-human system. In this centralized framework, LLM first generates prescriptive rules for each user's objective and then generates experiences based on those rules for each objective. After acquiring a practical knowledge of the rules generated, the LLM's performance is evaluated by inferencing, where the user provides instructions, and the LLM allocates the task according to the rules and experiences. Two distinct RAG workflows are leveraged in the inferencing stage to use the acquired knowledge fully. Moreover, Huang et al. [35] tested the ability of LLMs to solve the multi-robot Traveling Salesman Problem (TSP). By providing appropriate prompts, the LLM plans the optimal paths for multiple robots and generates Python code to control their movements. The study set up three frameworks: single"}, {"title": "4.1.2 Complex Task Decomposition", "content": "Task decomposition refers to scenarios where MRS must collaborate to complete one or more complex tasks that require careful planning and division of labor. In such cases, LLM can be leveraged to break down the overall task into smaller, manageable subtasks that align with the capabilities of each robot in the team. By designing effective prompts, LLMs can generate logical and actionable task decompositions, ensuring that the workload is distributed efficiently and that the robots cooperate seamlessly to achieve the overarching objective.\nKannan et al. [40] introduced SMART-LLM, a framework that utilizes LLMs to decompose high-level human instruction into subtasks and allocate them to heterogeneous robots based on their predefined skill sets. Unlike Chen et al. [12], where robot capabilities are inferred from their URDF code using LLMs, SMART-LLM adopts a more conventional approach by explicitly defining each robot's skill set for heterogeneous task allocation. The process involves decomposing instructions into sub-tasks, analyzing the required skills for each sub-task to form coalitions, and distributing robots accordingly to ensure efficient task execution. Wang et al. [94] propose Dependency-Aware Multi-Robot Task Decomposition and Execution LLMS (DART-LLM), a system designed to address complex task dependencies and parallel execution problems for MRS, as shown in Fig. 5. The framework utilizes LLMs to parse high-level natural language instructions, decompose them into interconnected subtasks, and define their dependencies using a Directed Acyclic Graph (DAG). DART-LLM facilitates logical task allocation and coordination by establishing dependency-aware task sequences, enabling efficient collaboration among robots. Notably, the system demonstrates robustness even with smaller models, such as Llama 3.1 8B, while excelling in handling long-horizon and collaborative tasks. This capability enhances the intelligence and efficiency of MRS in managing complex composite problems. Xu et al. [99] proposed a two-step framework that leverages LLMs to translate complex natural language instructions into a hierarchical linear temporal logic (LTL) representation for MRS. In the first step, the LLM decomposes the instruction into a hierarchical task tree, capturing logical and temporal dependencies between subtasks to avoid errors in sequence. In the second step, a fine-tuned LLM translates each subtask into flat LTL formulas, enabling precise execution using off-the-shelf planners. This framework emphasizes the importance of temporal reasoning in decomposing complex instructions, ensuring accurate task allocation and execution for long-horizon and interdependent multi-robot tasks. In contrast to the abovementioned approaches, Obata et al. [70] adopted a slightly different approach and proposed LiP-LLM, a framework integrating LLMs with linear programming for multi-robot task planning. Instead of providing end-to-end task allocation and execution, LiP-LLM utilizes LLMs to generate a skill set and a dependency graph that maps relationships and sequential"}, {"title": "4.2 Mid-Level Motion Planning", "content": "Mid-level motion planning in MRS encompasses tasks such as navigation and path planning, where the focus lies on enabling robots to traverse or coordinate within an environment efficiently. These scenarios are more direct and practical than high-level applications but critical for multi-robot teams' seamless operation. LLMs contribute significantly to this domain by leveraging their contextual understanding and learned patterns to generate robust and adaptive solutions. By interpreting environmental data and dynamically adapting to changes, LLMs enable robots to collaboratively plan paths, avoid obstacles, and optimize movement within shared spaces. Integrating LLMs into mid-level motion planning enhances efficiency and resilience, making MRS more capable in dynamic and unpredictable environments.\nYu et al. [104] proposed Co-NavGPT framework to integrate LLMs as a global planner for multi-robot cooperative visual semantic navigation, as shown in Fig. 6. Each robot captures RGB-D vision data, which is converted into semantic maps. These maps are merged and combined with the task instructions and robot states to construct prompts for the LLMs. The LLMs then assign unexplored frontiers to individual robots for efficient target exploration. By leveraging semantic representations, Co-NavGPT enhances the comprehension of the environment and guides collaborative exploration. In this framework, the LLMs are limited to allocating unexplored fron-"}, {"title": "4.3 Low-Level Action Generation", "content": "Low-level action generation focuses on controlling robot motion or posture at the hardware level, translating high-level goals into precise control commands. These tasks are critical for ensuring smooth and efficient operations in dynamic environments. While LLMs offer contextual reasoning and adaptability, their performance in low-level tasks, which demand high precision and real-time responsiveness, is often limited"}, {"title": "4.4 Human Intervention", "content": "In MRS, LLMs typically focus on executing tasks based on human-provided instructions, emphasizing the interpretation of instructions and autonomous task completion. Once the instructions are delivered, human involvement is often minimized. However, emerging research explores scenarios that require continuous interaction between LLMs and humans, emphasizing cooperation, decision-making, or external observation throughout task execution. These studies highlight the potential for dynamic human intervention to address unexpected challenges, refine task strategies, or ensure safety in critical applications. By enabling iterative human-robot collaboration, such approaches enhance the adaptability and reliability of LLM-driven MRS. The simplest form of human-robot interaction is demonstrated by Lakhnati et al. [45], where robots operate in a straightforward cycle: receiving a human command, executing the corresponding task, reporting the completion status, and awaiting the next instruction. Building on this, Lykov et al. [62] introduced the LLM-MARS framework, which enables humans to inquire about each robot's current state and task progress at any time. In this system, both response generation and task execution are handled by a single LLM, enhanced with distinct LoRA adapters for efficiency. Hunt et al. [36] proposed a more interactive approach, requiring human approval before executing any plan generated through LLM-driven discussions. If the proposed plan is deemed unreasonable, the human supervisor can provide feedback, prompting the LLMs to refine their approach through further dialogue. Ahn et al. [2] introduced the VADER system, further enhancing human involvement. When a robot encounters a task-related issue, it posts a request for assistance on the Human-Robot Fleet Orchestration Service (HRFS), a shared platform accessible to both human operators and robotic agents. Any agent or human can respond to the request, and once the issue is resolved, the robot resumes its task. These examples illustrate the varying degrees of human involvement in LLM-driven MRS, ranging from simple command execution to active collaboration and dynamic problem-solving."}, {"title": "5 Application", "content": "The integration of LLMs into MRS has enabled advancements across a variety of application domains, each with unique challenges and opportunities. These applications leverage LLMs' capabilities in understanding, planning, and coordinating tasks, offering solutions ranging from indoor to outdoor scenarios. The adaptability of LLMs has driven innovation in tasks requiring precise navigation, task allocation, and dynamic decision-making, demonstrating their potential to address both structured and unstructured environments.\nIn this section, we categorize studies based on their application scenarios, focusing on two primary domains. First, the household domain highlights MRS addressing indoor challenges such as navigation, task decomposition, and object manipulation. These systems often emphasize collaboration among heterogeneous robots to execute intricate tasks, from identifying targets in multi-room settings to organizing household appliances. Second, applications in construction, formation, target tracking, and game illustrate the versatility of LLMs in specialized fields. These studies showcase MRS solving complex problems in outdoor or competitive environments, such as drone formations for search-and-rescue missions, robotic soccer strategies, and navigation in hazardous areas. Together, these domains underscore the growing impact of LLMs in advancing MRS capabilities across diverse real-world contexts."}, {"title": "6 LLMs, Simulations, and Benchmarks", "content": "6.1 LLMs and VLMS\nLLMs and VLMs play an increasingly significant role in MRS by enabling advanced decision-making, communication, and perception-driven collaboration. Different models offer unique strengths, making them suitable for specific MRS applications. Table 1 provides a comparative summary of LLMs and VLMs used in the studies discussed earlier, highlighting their contributions to multi-robot coordination, planning, and perception. GPT is one of the most extensively used language models, as demonstrated in Table 1, where it forms the backbone of many referenced studies. Its general-purpose reasoning and adaptability allow it to be integrated into multi-robot"}, {"title": "6.2 Simulation Environments", "content": "We have summarized the simulation platforms used in related works, highlighting their contributions to evaluating and advancing the field. AI2-THOR has been adapted for MRS in [15, 40, 93, 99] to evaluate embodied AI agents operating in complex indoor environments [43]. While originally designed for single-agent tasks such as object manipulation and scene understanding, recent research extends its use to MRS scenarios, including cooperative object retrieval, shared perception, and collaborative planning in constrained environments. The physics-enabled interactions allow researchers to test LLM-driven coordination strategies in dynamic and physically grounded environments, where multiple agents must navigate, manipulate objects, and resolve conflicts dynamically. PyBullet is an open-source physics engine widely used for simulating robotic systems, including articulated manipulators, wheeled robots, and multi-agent interactions [17]. It provides real-time physics simulations, supporting tasks like collision detection, rigid body dynamics, and reinforcement learning in robotics. In the context of MRS, PyBullet enables accurate modeling of decentralized collaboration, object manipulation, and dynamic environment interactions [105]. BEHAVIOR-1K, utilized by Liu et al. [57], serves as the foundation for the COHERENT framework, which focuses on large-scale, heterogeneous multi-robot collaboration [48]. This platform facilitates training and evaluation in complex household-like environments where different types of robots (e.g., manipulators, mobile bases) must coordinate to accomplish everyday tasks such as table setting, object handoff, and multi-step assembly processes. The benchmark ensures that LLM-enhanced systems can handle dynamic task dependencies and ambiguous role assignments. The Pygame platform is a cross-platform Python module set designed for video game writing. Robots are modeled as point-mass entities, focusing on formation control, decentralized consensus algorithms, and motion coordination without obstacle avoidance. This platform is particularly useful for analyzing emergent behaviors in swarms, where LLM-based controllers guide self-organized formations through simple local interactions [89]. Habitat-MAS, an extension of Habitat, introduces explicit multi-agent communication for indoor navigation and exploration [72, 77, 85]. Unlike the single-agent focus of its predecessor, Habitat-MAS enables studies on cooperative search, simultaneous localization and mapping (SLAM), and inter-agent strategy adaptation, crucial for deploying multi-robot exploration teams in disaster response and service robotics [12]. ROS-based simulation is a middleware framework widely used for MRS, enabling inter-robot communication [46], decentralized control, and real-time data sharing. It provides essential tools for swarm coordination, collaborative mapping, and distributed task allocation. With built-in simulation environments like Gazebo and RViz, ROS allows researchers to develop and test MRS strategies for exploration, target tracking, and cooperative manipulation [97]. VR platform introduces immersive simulations for human-robot collaboration and reinforcement learning. These environments are used to test human-in-the-loop control strategies for heterogeneous robot teams, such as coordinating robotic arms and mobile robots in warehouse logistics through natural language instructions [45]. GAMA offers a multi-agent modeling environment suited for large-scale robot interactions [31]. It supports evaluations of distributed swarm intelligence, multi-agent task negotiation,"}, {"title": "6.3 Benchmarks", "content": "Benchmarks are essential for evaluating LLM-driven MRS, providing standardized environments to measure coordination, adaptability, and performance across diverse scenarios. They enable consistent comparisons, helping identify strengths, limitations, and the effectiveness of MRS in real-world applications. RoCoBench, introduced by Mandi et al. [65], focuses on human-robot collaboration in fine-grained manipulation tasks. While primarily designed for single-robot control, RoCoBench also provides insights into multi-robot collaboration, particularly in tasks requiring shared manipulation, coordinated actions, and real-time adjustments to changing conditions. The benchmark provides detailed metrics on precision, task success rates, and robustness under unpredictable physical conditions, making it valuable for evaluating LLM-assisted multi-robot cooperation in human-shared workspaces. ALFRED, utilized in Xu et al. [99], integrates language and vision benchmarks to test agents' ability to follow natural language instructions and execute multi-step tasks in household environments. Though originally focused on single-agent evaluations, ALFRED's framework can be extended to multi-robot task coordination, testing MRS on collaborative planning, sequential action execution, and efficient division of labor in domestic or service robotics applications. BOLAA, proposed by Liu et al. [59], introduces a multi-agent orchestration benchmark specifically designed for LLM-augmented autonomous agents (LAAs). Unlike conventional evaluations that focus on individual agents, BOLAA assesses how LLMs manage multi-agent interactions, optimizing task distribution, decision-making, and real-time adaptability. This makes it a useful benchmark for LLM-driven MRS, where autonomous robots benefit from effective communication and collaboration to tackle complex, long-horizon objectives. COHERENT-Benchmark, developed by Liu et al. [57], is specifically designed for heterogeneous multi-robot collaboration in dynamic and realistic scenarios. Built on the BEHAVIOR-1K platform, this benchmark evaluates MRS across diverse environments, requiring coordinated task execution among robots with distinct capa-"}, {"title": "7 Challenges, and Opportunities", "content": "Despite the progress in integrating LLMs into MRS, significant challenges that limit their broader adoption and effectiveness remain. These challenges span areas such as reasoning capabilities, real-time performance, and adaptability to dynamic environments. Addressing these issues is critical to unlocking the full potential of LLMs in MRS. This section identifies key challenges the field faces and outlines promising opportunities for future research, offering a roadmap for enhancing the utility and robustness of LLM-driven MRS.\n7.1 Challenges\nInsufficient Mathematical Capability. LLMs struggle with tasks requiring precise calculations or logical reasoning, such as multi-robot path planning or trajectory optimization. This limitation reduces their effectiveness in scenarios where quantitative accuracy is critical. Mirzadeh et al. [67] performed a detailed comparison and investigation on the mathematical understanding and problem-solving ability of several state-of-art LLMs. Specifically, LLMs exhibit noticeable variance when responding to different variations of the same question, with performance declining significantly when only the numerical values are altered. Furthermore, their reasoning capabilities are fragile; they often mimic patterns observed in training data rather than performing genuine logical deduction. This fragility is exacerbated by an increase in the number of clauses within a question, even when the added clauses are irrelevant to the reasoning chain, leading to performance drops of up to 65% in state-of-the-art models. These vulnerabilities present serious challenges for MRS, where precise calculations and robust reasoning are essential for collision-free trajectories, spatial planning, and efficient task execution. Addressing these limitations is critical for deploying LLMs reliably in mathematically intensive applications.\nHallucination. LLMs are prone to generating content that appears plausible but lacks factual accuracy, a phenomenon known as hallucination. This issue is particularly concerning in MRS, where precise and reliable output is crucial for effective collaboration and operation. According to a comprehensive survey on hallucination in LLMs by Huang et al. [34], hallucination can be categorized into two main types: actuality hallucinations and faithfulness hallucinations. Factuality hallucinations involve discrepancies between generated content and verifiable real-world facts, leading to incorrect outputs. Faithful hallucinations occur when the generated content diverges from the user's instructions or the provided context, resulting in outputs that do not accurately reflect the intended information. In the context of MRS, such hallucinations can lead to misinterpretations, faulty decision-making, and coordination errors among robots, potentially compromising mission success and safety. Addressing these challenges requires developing methods to detect and mitigate"}, {"title": "7.2 Opportunities", "content": "Fine-tuning and RAG. Fine-tuning LLMs on domain-specific datasets and incorporating RAG techniques are promising avenues for improving their performance in multi-robot applications. Fine-tuning allows researchers to adapt pre-trained LLMs to specific tasks, enhancing their contextual understanding and reducing issues like hallucination. RAG complements this by integrating external knowledge retrieval mechanisms, enabling LLMs to access relevant information dynamically during runtime. Together, these techniques can significantly improve LLMs' accuracy, reliability, and adaptability in diverse and complex multi-robot scenarios.\nHigh-quality Task-specific Datasets. Creating high-quality and task-specific datasets is essential for advancing LLM capabilities in MRS. Leveraging more capable models, such as the latest LLMs, to generate synthetic datasets can accelerate the development of training materials tailored to specific tasks or environments. These datasets should include diverse scenarios, reasoning-focused labels, and context-specific knowledge to improve LLMs' problem-solving and decision-making capabilities. Task-specific datasets are particularly important for preparing MRS to operate in unstructured or open-world environments.\nAdvanced Reasoning Techniques. Improving the reasoning capabilities of LLMs is critical for addressing their current limitations in logical and mathematical tasks. Techniques such as Chain of Thought (CoT) prompting, fine-tuning with explicit reasoning labels, integrating symbolic reasoning, and training with RL can enhance the ability of LLMs to handle complex multi-step problems. By advancing reasoning methods, LLMs can better support tasks that require precision and logical deduction, such as multi-robot path planning and coordination.\nTask-specific and Lightweight Models. While large-scale LLMs offer superior performance, they are often impractical for resource-constrained environments. Developing task-specific and lightweight models tailored for multi-robot applications can mitigate this issue. Models like SmolVLM, Moondream 2B, PaliGemma 3B, and Qwen2-VL 2B demonstrate how smaller architectures can reduce computational demands and latency while maintaining adequate performance for specific tasks. Model distillation is another approach to make small models more capable by distilling knowledge from a more capable LLM, like DeepSeek-R1-Distill-Qwen-1.5B, where the knowledge from DeepSeek R1 is distilled into a small Qwen2.5-Math-1.5B model. Balancing efficiency and effectiveness is key to enabling scalable deployments of LLMs in field robotics.\nExpanding to Unstructured Environments. Most current applications and benchmarks focus on indoor or structured environments, leaving significant gaps in outdoor and unstructured scenarios. Research should prioritize expanding MRS capabilities to include operations in open-world contexts, such as agricultural fields, disaster zones, and remote exploration sites. Addressing the unique challenges of these environments, including variability, noise, and unpredictable dynamics, will"}, {"title": "8 Conclusion", "content": "This survey provides the first comprehensive exploration of integrating LLMs into MRS, a topic at the intersection of robotics and artificial intelligence that is rapidly gaining prominence. Unlike general robotics or multi-agent systems, MRS pose unique challenges and opportunities due to their reliance on physical embodiment and real-world interaction. This paper highlights how LLMs can address these challenges, offering novel possibilities for collective intelligence and collaboration"}]}