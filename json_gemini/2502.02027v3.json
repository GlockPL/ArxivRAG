{"title": "FROM FOG TO FAILURE: HOW DEHAZING CAN HARM CLEAR IMAGE OBJECT DETECTION", "authors": ["Ashutosh Kumar", "Aman Chadha"], "abstract": "This study explores the challenges of integrating human visual cue-based dehazing into object detection, given the selective nature of human perception. While human vision adapts dynamically to environmental conditions, computational dehazing does not always enhance detection uniformly. We propose a multi-stage framework where a lightweight detector identifies regions of interest (RoIs), which are then enhanced via spatial attention-based dehazing before final detection by a heavier model. Though effective in foggy conditions, this approach unexpectedly degrades the performance on clear images. We analyze this phenomenon, investigate possible causes, and offer insights for designing hybrid pipelines that balance enhancement and detection. Our findings highlight the need for selective preprocessing and challenge assumptions about universal benefits from cascading transformations. The implementation of the framework is available\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Low-visibility conditions, such as rain, snow, fog, smoke, and haze, pose significant challenges for deep learning applications in autonomous vehicles, security and surveillance, maritime navigation, and agricultural robotics. Under these conditions, object detection models struggle due to reduced contrast and obscured features, leading to performance degradation. This study proposes a deep learning framework inspired by human visual perception to enhance object recognition in adverse visibility scenarios, particularly in foggy environments.\nA key motivation for this work comes from the impact of poor visibility on airport operations, where disruptions in taxiing and docking cause delays and increase reliance on ground support. Limited staff availability exacerbates these issues, sometimes resulting in flight cancellations. While initially driven by airport challenges, the proposed framework applies broadly to various low-visibility scenarios across multiple domains.\nTo address these challenges, we introduce Selective Region Enhancement, a method that focuses on specific regions of interest rather than applying uniform dehazing. This approach reduces processing overhead and prevents unintended degradations that may introduce false positives. Additionally, we propose Integration with Object Detection, bridging image enhancement with object detection in a unified pipeline. This integration leverages the strengths of both techniques, overcoming limitations of traditional independent processing models. Our approach draws inspiration from human visual mechanisms, including selective attention, foveal and peripheral vision, adaptive eye responses, bottom-up sensory cues, and top-down goal-driven processing (see Appendix B).\nThe paper is structured as follows: Section 2 reviews prior work on low-visibility object detection and integration of human visual cues in deep learning applications. Section 3 presents our framework, including its vision-inspired design, data set selection, and experimental setup, followed by results and observed anomalies in Section 4."}, {"title": "2 RELATED WORK", "content": "Advancements in navigation and detection under low-visibility conditions have leveraged sensor fusion, visual cue integration, and computational techniques. Aircraft landing studies have explored sensor fusion of visible and virtual imagery (Liu et al., 2014) and visual-inertial navigation using runway features (Zhang et al., 2018). Multi-sensor fusion algorithms have improved odometry in GPS-denied environments (Khattak et al., 2019), while research on depth visualization has enhanced navigation and obstacle avoidance (Lieby et al., 2011). Synthetic Vision Systems and full-windshield Head-Up Displays aid drivers and pilots in low visibility (Kramer et al., 2014; Charissis & Papanastasiou, 2010). Image enhancement techniques for low-light conditions (Atom et al., 2020) and the fusion of visual cues with wireless communication improve road safety (Boban et al., 2012). Studies have also emphasized the role of geometrical shapes and colors in driving perception via Head-Up Displays (Zhan et al., 2023).\nDespite these advances, challenges persist, including computational complexity (Zhang et al., 2018; Atom et al., 2020; Tang et al., 2022), performance issues under extreme conditions (Khattak et al., 2019; Boban et al., 2012), overfitting due to limited datasets (Zhang et al., 2018; Khattak et al., 2019), and insufficient real-world validation (Liu et al., 2014; Boban et al., 2012; Tang et al., 2022). Some works lack clarity in explanation or rigorous validation (Kramer et al., 2014; Zhan et al., 2023).\nIn visual recognition, research has explored human-like processing in computational models. Studies on brain mechanisms highlight hierarchical, feedforward object recognition (DiCarlo et al., 2012), while comparisons with deep neural networks (DNNs) reveal human superiority in handling distortions and attention mechanisms (Dodge & Karam, 2017; van Dyck et al., 2021). Eye-tracking data has been used to guide DNN attention with limited success (van Dyck et al., 2022). Approaches such as adversarial learning for feature discrimination (Yang et al., 2023a), biologically inspired top-down and bottom-up models (Malowany & Guterman, 2020), and retina-mimicking models for dehazing (Zhang et al., 2015) have been proposed. Foveal-peripheral dynamics have also been explored to balance computational efficiency and high-resolution perception (Lukanov et al., 2021).\nRecent research has tackled low-visibility challenges like fog, low light, and sandstorms. The YOLOv5s FMG algorithm improves small-target detection with enhanced modules (Zheng et al., 2023), while novel MLP-based networks refine image clarity in hazy and sandstorm conditions (Gao et al., 2023). The PKAL approach integrates adversarial learning and feature priors for robust recognition (Yang et al., 2023b). Deformable convolutions and attention mechanisms enhance pedestrian and vehicle detection in poor visibility (Wu & Gao, 2023). Reviews highlight the limitations of non-learning and meta-heuristic dehazing methods in real-time applications (V et al., 2023), emphasizing the need for integrated low-level and high-level vision techniques (Yang et al., 2020). Innovations such as spatiotemporal attention for video sequences (Zhai & Shah, 2006), the PDE framework for simultaneous detection and enhancement (Li et al., 2022), spatial priors for saliency detection (Jian et al., 2021), and early visual cues for object boundary detection (M\u00e9ly et al., 2016) further contribute to the field.\nDespite advances, existing methods struggle with joint optimization of object detection and image enhancement, detection of low-contrast objects, and adaptation to dynamic visibility changes. This paper addresses these challenges by integrating human visual cues, such as attention mechanisms and contextual understanding, into object detection, enhancing both robustness and efficiency. Traditional approaches process entire images uniformly, increasing computational load, and sometimes degrading clear regions. Our method selectively enhances regions of interest, reducing unnecessary computations and improving responsiveness under varying conditions."}, {"title": "3 METHODOLOGY", "content": "The proposed methodology, illustrated in Figure 1, presents a deep learning framework that enhances object detection in low-visibility conditions by leveraging the atmospheric scattering model and human visual cortex principles. It integrates adaptive image enhancement with object detection, optimizing performance through different integration strategies. The pipeline starts with a lightweight detection model to identify regions of interest, guiding spatial attention in the dehaz-"}, {"title": "4 RESULTS AND OBSERVED ANOMALIES", "content": "For in-distribution performance on the Foggy Cityscapes dataset, see Appendix F.1, while OOD evaluation on RESIDE-\u1e9e (RTTS) and OTS datasets is detailed in Appendix F.2, demonstrating the pipeline's robustness across diverse hazy conditions. Evaluation metrics include SSIM and PSNR for dehazing and mAP for object detection (Appendix G). Figure 4 illustrates visibility improvement with AOD-Net, Figure 5 shows its impact on object detection after dehazing with AOD-NetX on Foggy Cityscapes, and Figure 6 presents performance on the RESIDE dataset."}, {"title": "5 DISCUSSIONS", "content": "The unexpected performance improvement of AOD-NetX-based models under foggy conditions (Table 1) raises critical questions about feature adaptation in hybrid object detection pipelines. Typically, object detectors exhibit a decline in accuracy when transitioning from clear to foggy conditions, as seen in conventional YOLOv5x and YOLOv8x models. However, our proposed AOD-NetX integration leads to an inverse trend\u2014where object detection improves in foggy conditions relative to clear ones. This suggests that AOD-NetX introduces an implicit domain adaptation effect, making the detection network more attuned to foggy environments at the cost of generalization to clear images.\nOne possible explanation lies in dataset bias and overprocessing. Since AOD-NetX is trained primarily on foggy images, its learned feature space is optimized for haze removal but lacks the necessary constraints to preserve features in clear conditions. Consequently, when applied to clear images, the model introduces distortions instead of enhancements, disrupting feature consistency for the object detector. This emphasizes the need for context-aware enhancement, where image processing techniques are selectively applied based on scene conditions rather than indiscriminately.\nFurthermore, the results challenge the assumption that cascading pipelines\u2014where lightweight detection informs region-specific enhancement before a final robust detection\u2014always improve performance. While effective in foggy settings, this multi-stage approach appears to introduce trade-offs, potentially harming accuracy in clear conditions. Future designs must strike a balance between specialization for adverse weather conditions and adaptability to diverse environments. Another consideration is the real-time feasibility of this approach. RoI-specific dehazing adds computational overhead, which could limit deployment in time-sensitive applications such as autonomous driving. Optimizing processing efficiency while retaining performance gains remains an open challenge."}, {"title": "6 LIMITATIONS & FUTURE WORK", "content": "A fundamental limitation of integrating dehazing into object detection is the feature space misalignment between foggy and clear images. Models trained primarily on foggy conditions lack the ability to preserve the natural characteristics of clear images, leading to unintended alterations that degrade detection performance. This highlights the importance of adaptive enhancement techniques that can determine when dehazing is necessary, rather than applying it universally. A potential solution is the integration of a haze-level estimation module, which could prevent unnecessary processing by triggering dehazing only when haze exceeds a certain threshold (Mao & Phommasak, 2014).\nAnother challenge is pretraining for scene differentiation. Since the AOD-NetX-enhanced models perform better in foggy conditions, their feature representations may be overfitting to haze-specific characteristics. Introducing joint training on both foggy and clear images could help mitigate this issue by aligning the feature space across different visibility conditions (Huang et al., 2024).\nAdditionally, unifying dehazing and object detection into a single model rather than having multi-stage framework may yield mutual benefits. For instance, detection-aware dehazing\u2014where dehazing prioritizes regions of interest could help the model preserve essential features for object detection, enhancing accuracy in both clear and foggy conditions (Fan et al., 2024).\nFinally, computational efficiency remains a key concern for real-time applications. While the current pipeline enhances detection performance in low-visibility conditions, its multi-stage nature introduces latency. Future work should focus on optimizing inference speed, exploring lightweight architectures, and developing efficient knowledge distillation techniques to maintain accuracy while reducing processing overhead.\nBy implementing adaptive processing strategies, improved pretraining, and joint optimization, future object detection pipelines can become more resilient across diverse visibility conditions, ensuring robust performance without compromising clarity in optimal conditions."}, {"title": "B HUMAN VISUAL CUES", "content": "Selective Attention and Foveation: The human eye does not perceive all areas of the visual field with equal clarity. Foveal vision, which corresponds to central vision, is highly detailed and is essential for tasks such as reading and object recognition. In contrast, peripheral vision is less detailed but more sensitive to motion. The visual system initially scans the entire scene using peripheral vision, akin to the preliminary detection phase in our approach. This broad scanning process helps identify regions requiring closer inspection, enabling a more detailed analysis through foveal vision. Similarly, the proposed method does not process every detail uniformly but prioritizes key areas of interest.\nAdaptation to Environmental Conditions: The human visual system dynamically adjusts to varying lighting conditions and levels of visibility, such as adapting from bright sunlight to a dark room. Similarly, the adaptive dehazing method modulates its processing intensity and focus based on detection feedback and environmental context. This mechanism ensures optimal perception, mirroring the way human vision adapts to maintain clarity under diverse conditions.\nEye Tracking and Gaze-Directed Processing: Eye-tracking technology monitors gaze direction and identifies focal points of attention. This concept translates to strategically allocating resources toward regions of interest in computational visual processing. The proposed method follows a similar principle by directing dehazing and detailed object detection efforts to areas where objects are likely to be present. Just as human vision selectively fixates on specific regions when searching for an object, the system prioritizes certain parts of the image to enhance clarity and detection performance.\nIntegration of Bottom-Up and Top-Down Processes: Human vision combines bottom-up processing, driven by sensory input, with top-down processing, guided by prior knowledge, expectations, and goals. The proposed model adopts a similar dual approach: it first employs a bottom-up strategy, where object detection algorithms identify potential areas of interest. This is followed by a top-down refinement process, where dehazing efforts are concentrated on flagged areas, leveraging previous learning. This interplay between data-driven signals and cognitive insights aligns with the way human perception integrates sensory input with contextual understanding."}, {"title": "C METHODOLOGY", "content": "Preliminary Detection: A lightweight and fast object detection algorithm, such as YOLOv5s or YOLOv8n, is employed to rapidly scan the image and identify potential regions of interest or active regions. These models flag image patches with a high probability of containing objects. While the smaller variants of YOLO models offer lower accuracy compared to their full-sized counterparts, they are significantly faster, making them well-suited for this initial detection phase.\nRegion-Based Dehazing: Dehazing algorithms are selectively applied to the active regions identified during the preliminary detection phase. The approach dynamically adjusts based on the depth or severity of haze within the detected regions, ensuring an adaptive and efficient dehazing process.\nThe proposed architecture, AOD-NetX, illustrated in Figure 2, builds upon the transmission map generated by the standard AOD-Net (Li et al., 2017a). This transmission map is integrated into a spatial attention map module, producing an attention-enhanced transmission map. The spatial attention map is derived from the bounding boxes or Regions of Interest (ROIs) detected by the lightweight object detection model (YOLOv5s/YOLOv8n) within the proposed framework. A sigmoid layer is applied to map the output probabilities to a range between 0 and 1. Unlike softmax, which normalizes outputs across multiple regions, sigmoid is preferred in this context since each bounding box holds independent significance."}, {"title": "G.1 STRUCTURAL SIMILARITY INDEX MEASURE (SSIM)", "content": "The performance of dehazing methods is evaluated using the Structural Similarity Index Measure (SSIM), which quantifies the similarity between two images based on luminance, contrast, and structural components. It is defined as:\n$SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + C_1) (2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}$\nwhere:\n\u2022 $\\mu_x$ and $\\mu_y$ are the mean intensities of images x and y, respectively.\n\u2022 $\\sigma_x^2$ and $\\sigma_y^2$ denote the variances of x and y.\n\u2022 $\\sigma_{xy}$ represents the covariance between x and y.\n\u2022 $C_1 = (k_1L)^2$ and $c_2 = (k_2L)^2$ are stabilizing constants to prevent division by zero, where L is the dynamic range of pixel values (e.g., 255 for 8-bit images), and default values are $k_1 = 0.01$ and $k_2 = 0.03$."}, {"title": "G.2 PEAK SIGNAL-TO-NOISE RATIO (PSNR)", "content": "Peak Signal-to-Noise Ratio (PSNR) is a widely used metric to assess image reconstruction quality by comparing the original and processed images. It is expressed in decibels (dB) and is calculated as:\n$PSNR = 10 \\cdot log_{10} \\left( \\frac{MAX^2}{MSE} \\right),$\nwhere MAX is the maximum possible pixel value (e.g., 255 for 8-bit images), and MSE is the Mean Squared Error:\n$MSE = \\frac{1}{mn} \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} (I(i, j) - K(i, j))^2.$\nHere, $I(i, j)$ and $K(i, j)$ represent pixel values at position $(i, j)$ in the original and reconstructed images, respectively. A higher PSNR value indicates better image quality, as it corresponds to lower distortion. PSNR is extensively used in evaluating dehazing, denoising, and image compression methods."}, {"title": "G.3 MEAN AVERAGE PRECISION (MAP)", "content": "For object detection performance, we use mean Average Precision (mAP), which evaluates the precision-recall tradeoff. The Average Precision (AP) is computed as:\n$AP = \\frac{\\sum_{k=1}^{n}(P(k) \\times rel(k))}{number\\ of\\ relevant\\ objects}$ \nwhere:\n\u2022 P(k) is the precision at rank k.\n\u2022 rel(k) is an indicator function, which is 1 if the object at rank k is relevant, and 0 otherwise.\n\u2022n is the total number of retrieved objects.\nThe mean Average Precision is computed as:\n$MAP = \\frac{\\sum_{q=1}^{Q} AP_q}{Q}$ \nwhere APq is the Average Precision for the qth query, and Q is the total number of queries. Higher mAP values indicate better object detection performance across different classes."}]}