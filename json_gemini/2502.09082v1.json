{"title": "COSER: Coordinating LLM-Based Persona Simulation of Established Roles", "authors": ["Xintao Wang", "Heng Wang", "Yifei Zhang", "Xinfeng Yuan", "Rui Xu", "Jen-tse Huang", "Siyu Yuan", "Haoran Guo", "Jiangjie Chen", "Wei Wang", "Yanghua Xiao", "Shuchang Zhou"], "abstract": "Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present COSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the COSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-40 on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively. Our code, dataset and models are available at https://github.com/Neph0s/CoSER.", "sections": [{"title": "1. Introduction", "content": "Recent advances in large language models (LLMs) have facilitated the emergence of anthropomorphic cognition in AI (Kosinski, 2023; Shanahan et al., 2023). Role-playing language agents (RPLAs), i.e., LLMs that simulate specific personas based on relevant data, have hence been popular (Park et al., 2023). RPLAs have been adopted to simulate personas of various types, including demographics, characters, or daily individuals (Chen et al., 2024a). They have inspired extensive applications including character chatbots, agents in video games, and digital clones for humans. This paper studies RPLAs for established characters, which represent a crucial yet challenging task beyond the naive portrayal of individual traits or stereotypes. Specifically, RPLAs should faithfully align with their characters' complex backgrounds and capture their nuanced personalities.\nTowards effective RPLAs, two major challenges persist in: 1) Data: High-quality datasets are lacking. Existing datasets are limited to dialogues between two characters, and lack necessary dialogue contexts and knowledge in other forms. Moreover, many datasets are synthesized by LLMs, compromising authenticity and fidelity to the origins (Wang et al., 2024a; Lu et al., 2024); 2) Evaluation: Current methods fall short in assessing complex character portrayals of LLMs. They typically focus on single-turn interactions with pre-defined question sets, and rely on either LLM-based judges or multi-choice questions. The former lack nuanced discrimination and suffer from bias issues (Li et al., 2024), while the latter only assess specialized aspects (Xu et al., 2024). Overall, there is a lack of authentic character data and appropriate evaluation methods based on such data.\nIn this paper, we introduce CoSER, a collection of authentic character data, along with open state-of-the-art models and evaluation protocol based on such data, for Coordinating LLM-Based Persona Simulation of Established Roles. The CoSER dataset is sourced from narratives and dialogues in 771 renowned books, processed via our LLM-based pipeline. CoSER differs from existing datasets in two"}, {"title": "3. COSER Dataset", "content": "In this section, we introduce the CoSER dataset, which covers authentic data of 17,966 characters from 771 renowned books. CoSER features its authentic, non-synthesized dialogues with real-world intricacies, and comprehensive data representations supporting various usages. In Table 1, we provide a comprehensive comparison with existing datasets. We illustrate our dataset's design principles in \u00a73.1, curation pipeline in \u00a73.2, and statistical analysis in \u00a7A.1."}, {"title": "3.1. Design Principles", "content": "As shown in Table 1, COSER differs from previous RPLA datasets mainly in its: 1) rich data types, 2) internal thoughts and physical actions in messages, 3) environment as a role.\nRich Types of Data The persona data $D_e$ can represent a character $c$ from fictional works in diverse forms, e.g., narratives, profiles, dialogues, experiences, e.t.c.. Previous work focuses primarily on profiles and dialogues, which represent limited knowledge. Hence, we propose a more comprehensive set of data types that are: 1) Comprehensive: covering extensive knowledge about characters and plots from the books; 2) Orthogonal: carrying distinct, complementary information with little redundancy; 3) Contextual-rich: providing sufficient context to enable $\\pi_e$ to faithfully reproduce $c$'s behaviors and responses in given scenarios.\nSpecifically, we organizes knowledge from books hierarchically via three interconnected elements: plots, conversations and characters. Each plot comprises its raw text, summary, conversations in this plot, and key characters' current states and experiences in this plot. A conversation contains not only the dialogue transcripts, but also rich contextual settings including scenario descriptions and characters' motivations. Characters are associated with their conversations and plots, based on which we craft their profiles."}, {"title": "3.2. Dataset Curation", "content": "We curate the CoSER dataset through a systematic LLM-based pipeline that transforms book content into high-quality data for RPLAs 2. The details are as follows.\nSource Selection Our dataset is sourced from most acclaimed literary works to ensure data quality and character depth. We identify the top 1,000 books on Goodreads's Best Books Ever list 3, and obtain the content for 771 books. As shown in Table 6, these books offer characters and narratives with literary significance and widespread recognition across diverse genres, time periods, and cultural backgrounds.\nChunking We segment book contents into chunks to fit in LLMs' context window. We employs both static, chapter-based strategy and dynamic, plot-based strategy. Initially, we use regular expressions to identify chapter titles as natural chunk boundaries. Then, we merge adjacent small chunks and split large chunks to ensure moderate chunk sizes. However, static chunking neglects the storyline and truncates important plots or conversations. To address this, we implement dynamic plot-based chunking, i.e., during data extraction, we also prompt LLMs to identify truncated plots or trailing content in the current chunk, and concatenate them with the subsequent chunk to ensure plot integrity.\nData Extraction We employ LLMs to extract plot and conversation data from book chunks, including (1) contents, summaries and character experiences of plots, and (2) dialogues and background settings of conversations. The extracted data representations are illustrated in Fig. 1 and introduced in \u00a73.1. In the messages, speeches are always extracted from the original dialogues, while actions and thoughts can either be extracted or inferred by LLMs based on the context. For evaluation purposes, we hold out data from the final 10% plots in each book.\nOrganizing Character Data\nBased on the extracted data, we form the knowledge bases for characters in three steps. First, we unify character references by establishing name mappings between aliases and canonical names using LLMs, e.g., mapping Lord Snow to an unified identifier Jon Snow. Second, we aggregate relevant plots and conversations for each character. Finally, we leverage LLMs to generate character profiles based on their extracted data, describing them from multiple perspectives including background, experiences, physical characteristics, personality traits, core motivations, relationships, character arcs, e.t.c..\nFor technical details, including our prompts, engineering implementation, and handling mechanisms for exception caused by LLMs, please refer to \u00a7A."}, {"title": "4. Training and Evaluation via GCA", "content": "In this section, we introduce given-circumstance acting (GCA) for training and evaluating LLMs' role-playing abilities using the CoSER dataset, as shown in Fig. 2."}, {"title": "4.1. Given-Circumstance Acting", "content": "In Konstantin Stanislavski's acting methodologies, given-circumstance acting is a fundamental ap-proach where actors are trained and judged through performance within specified conditions including environmental context, historical events and personal conditions (Stanislavski, 2008).\nWe propose to adapt this approach to a framework that trains and evaluates LLMs' role-playing skills,"}, {"title": "4.2. GCA Training and COSER Models", "content": "We fine-tune LLMs' role-playing abilities through GCA. Each training sample is derived from a con-versation and one of its character $c$ in CoSER dataset, and LLMs are trained on $c$'s utterances $M_c$. Specifically, we first compose a role-playing instruction $i_c$ comprising the scenario description, the character's profile $p_c$ and motivation, and profiles of other involved characters, which provide com-prehensive context for role-playing. The original dialogue messages are denoted as $M = [m_1, \u2026, m_T]$, where $T$ is the number of turns. Then, the training sample $[i_c, m_1, ..., m_T]$ is a concatenation of the instruction and messages, where the character's messages $M_c \\subset M$ are treated as outputs for optimization, and the other parts serve as inputs.\nWe train CoSER 8B and CoSER 70B based on LLaMA 3.1 Instruct models (Dubey et al., 2024), using 90% books in our dataset. To effectively support diverse use cases of RPLAs, our training samples cover extensive settings: 1) The CoSER dataset contains massive characters and conversation settings from extensive books. We train models on all characters in each conversation, ranging from major characters with detailed profiles to minor roles driven only by the context; 2) To simulate real use cases, we incorporate role-playing instructions in diverse formats through instruction templates of varying formats. Besides, we consider different combinations of available data by including or excluding: profiles of other characters, plot summaries, and characters' motivations; 3) We train models both with and without characters' internal thoughts in the extracted dialogues.\nWe extend CoSER's training beyond character role-playing to develop complementary capabilities in environment modeling and next speaker prediction (NSP), which facilitates RPLA applications. To maintain models' general abilities, we augment our training data with the Tulu-3 dataset (Lambert et al., 2024). Please refer to \u00a7B for more details."}, {"title": "4.3. GCA Evaluation", "content": "Evaluating role-playing LLMs remains a significant challenge, primarily in two aspects: 1) providing appropriate scenarios to elicit role-playing performance, and 2) properly assess the performance. Towards these challenges, we propose GCA evaluation for actor LLMs' role-playing abilities, comprising two stages: multi-agent simulation and penalty-based LLM judging, as illustrated in Fig. 2.\nMulti-agent Simulation For a test conversation $M$, we build a multi-agent system to simulate a conversation $M$, in the same setting as $M$. We create an RPLA $\\pi_c$ for each character $c \\in C$ using the actor LLM. We provide RPLAs with comprehensive data as described in \u00a74.2: scenario descriptions and involved character profiles offer crucial context, and character motivations promote RPLA proactiveness and a natural conversation flow. Following \u00a73.1, RPLAs are instructed to output in the speech-action-thought format. Each RPLA's motivations and inner thoughts are inaccessible to other RPLAs. We adopt an NSP model to select the speaker of each turn from $C \\cup \\{e\\}$, and another LLM as the environment model $\\pi_e$ to provide environmental feedback. The simulation ends upon an <END> signal from NSP, or reaching the maximum of 20 turns. In this way, we obtain a multi-turn, multi-character simulation that comprehensively reflects the actor LLMs' role-playing abilities.\nIn addition, we introduce a continue-from parameter $k$, where the simulation starts from the first $k$ original messages in $M$. Setting $k > 0$ controls the story direction and language style, similar to in-context learning. Hence, it enables more controlled evaluation and reduces the influence of"}, {"title": "different language styles of LLMs.", "content": "Penalty-based LLM Judging In this stage, we assess the simulated dialogue $M$ via LLM critics. Different from previous LLM-as-a-judge methods for RPLA evaluation, our LLM critics: 1) apply penalty-based scoring by identifying role-playing flaws following detailed rubrics, and 2) leverage the original conversation $M$ as reference.\nSpecifically, we employ LLM critics 4 to identify flaw instances $F$ in $M$ of specific rubrics, such as \u201cdeviate from the original conversation\u201d or \u201clack initiative and goals\u201d, instead of directly outputting a score in previous work (Wang et al., 2024a; Tu et al., 2024). Each flaw $f$ is assigned a severity $v_f$ from 1 (minor) to 5 (severe). The initial score for each dimension is calculated as $s = 100 - \\Sigma_{f \\in F} v_f$.\nThe rubrics are derived from human-annotated issues in extensive human-RPLA conversations from real users and our multi-agent simulations. We also consider evaluation dimensions from previous work (Shanahan et al., 2023; Shao et al., 2023; Chen et al., 2024a). For more informed evaluation, LLM critics are provided with additional materials, i.e., the original conversation $M$ and plot summary, besides data available to actor LLMs. Each dimension is assessed in independent LLM requests. The evaluation dimensions and their rubrics are summarized as follows:\n1. Anthropomorphism: Evaluates whether RPLAs behave in a human-like manner, with rubrics covering self-identity, emotional depth, persona coherence, and social interaction.\n2. Character Fidelity: Assesses whether \u0158PLAs faithfully portray their characters, with rubrics examining language style, knowledge and background, personality and behavior, and social relationships.\n3. Storyline Quality: Evaluates whether the simulated conversation develops naturally, with rubrics focusing on narrative flow and logical consistency.\n4. Storyline Consistency: Measures alignment between the simulated conversation $M$ and original dialogue $M$, i.e., whether RPLAs' reactions (emotions, attitudes, behaviors) remain consistent with the original.\nAs longer simulations naturally make more flaws, we implement length correction to reduce bias in LLM judging following Dubois et al. (2024). Specifically, we obtain the length corrected score as $s = 100 - \\Sigma_{f \\in F} v_f + \\lambda|M|$, where $\\lambda$ is set to 1.5 based on statistical analysis in \u00a7C.3. For detailed prompts and rubrics, please refer to \u00a7F."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Settings", "content": "Evaluation Protocol We evaluate LLMs' role-playing abilities through GCA on CoSER Test, a test set of held-out conversations from the final 10% of each book. CoSER Test contains 200 conversations, with 100 from books used in CoSER training and 100 otherwise. We employ GPT-40 as the critic LLM and environment model, and CoSER 70B for NSP. We exclude characters' inner thoughts for LLM critics, and set the continue-from parameter $k = 0$. The details are in \u00a7C.\nMetrics We report LLM-judged scores for each dimension, and their average as the overall score. For analysis, we also evaluate two traditional metrics based on N-gram, i.e., BLEU (Papineni et al., 2002) and ROUGE-L(Lin and Och, 2004) compared against original dialogues. Additionally, we report win rates versus GPT-3.5 and GPT-40 in \u00a7E.\nModels Our experiments cover numerous LLMs: 1) Close models, including Minimax Abab7-"}, {"title": "5.2. Main Results", "content": "Performance of Various LLMs on CoSER Test We apply CoSER Test to evaluate extensive LLMs. The results shown in Table 2 are averaged across three runs, from which we observe that: 1) CoSER 70B achieves state-of-the-art performance across both LLM-judged and N-gram-based metrics. For"}, {"title": "Authentic Conversations from High-quality Novels Improve LLMs' Role-playing Ability", "content": "According to Table 2, COSER models demonstrate significant improvements over their LLaMA 3.1 baselines. In contrast, Higgs-LLaMA-3 70B, fine-tuned on synthesized dialogues, performs below LLaMA-3.1 70B, These results highlight the importance of high-quality, authentic role-playing data for LLM training."}, {"title": "Conversation Continuation Enables More Controlled Evaluation", "content": "Table 15 shows experiment results when multi-agent systems start from the first $k = 3$ original messages. In this setting, model obtain higher scores compared to simulations from scratch ($k = 0$), with reduced performance gaps between different models, especially for BLEU and ROUGE-L results. For example, the average score gap between Qwen-2 72B and 7B decreases from 11.5% ($k = 0$) to 8.8% ($k = 3$). This improvement occurs because the $k = 3$ original messages guide the story direction and language style, particularly benefiting smaller models that typically struggle with complex role-playing instructions."}, {"title": "Results on Other Benchmarks", "content": "We evaluate CoSER and other models on existing benchmarks for RPLAs based on multi-choice questions. As shown in Table 3, COSER 70B achieves state-of-the-art performance across these benchmarks. Notably, COSER 70B achieves 93.47% accuracy on LifeChoice, surpassing GPT-40 by 23%. These results exhibit CoSER models' strong capability in nuanced portrayal of characters' personalities and behaviors."}, {"title": "5.3. Ablation Studies", "content": "Inner Thoughts and Motivations Enhance RPLAs at Test Time Table 4 compares LLMs' overall scores on CoSER Test with or without inner thoughts and motivations. The results show consistent performance improvements across all models when inner thoughts and motivations are included."}, {"title": "Inner Thoughts Benefit Role-Playing Training", "content": "We train COSER model variants without inner thoughts, and evaluate them on various benchmarks. Results in Tables 3 and 4 show that models trained without inner thoughts consistently underperform regular CoSER models, exhibiting the value of inner thoughts for role-playing training."}, {"title": "5.4. COSER Dataset for Retrieval Augmentation", "content": "We evaluate the value of our comprehensive data types for retrieval augmentation on CoSER Test. We explore three retrieval sources for a specific character: dialogues in related conversations (Conv.), as well as experiences (Expr.) and raw text in related plots. The retrieval system is based on FAISS (Douze et al., 2024) with BGE-M3 (Chen et al., 2024b) for text embeddings. As shown in Fig. 17, we observe: 1) Models consistently benefit from characters' retrieved experiences and conversations, especially for COSER 70B; 2) However, raw text retrieval barely enhances LLMs' performance. Detailed experimental settings and results are provided in \u00a7E and Table 17."}, {"title": "5.5. Case studies", "content": "We conduct case studies to analyze LLMs' performance in GCA simulation. Several cases are presented in Tables 7 to 12, from which we observe that: 1) CoSER models, trained on authentic dialogues, communicate more naturally, closely aligning with human speech patterns. 2) CoSER models better recall character-related knowledge, such as the iconic line \u201cGrown enough to be wed, wed enough to be bedded\" by Lysa Arryn in Table 7. This is consistent with their high BLEU and ROUGE-L scores."}, {"title": "6. Conclusion", "content": "Towards effective RPLAs for established characters, this paper introduces CoSER, a collection of an authentic dataset, along with models and evaluation protocol based on such data. The CoSER dataset offers high-quality data from 771 renowned books, and includes comprehensive data types such as authentic dialogues, plot summaries, character experiences, inner thoughts, e.t.c.. Then, we propose given-circumstance acting (GCA) for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters within authentic book scenarios. Applying GCA training to LLaMA-3.1 models using our dataset, we develop COSER 8B and COSER 70B, advanced open LLMs for role-playing. For evaluation, GCA combines multi-agent simulation and penalty-based LLM critics. Extensive experiments exhibit CoSER dataset's value for RPLA training, evaluation, and retrieval. Moreover, CoSER models achieve state-of-the-art performance on both our evaluation and three existing RPLA benchmarks."}, {"title": "Impact Statement", "content": "COSER aims to advance RPLA research by providing effective dataset, models and evaluation protocol. We will release our dataset, models, and evaluation scripts to foster innovation in RPLAs. The dataset is intended for research purposes only. For copyright policies, we will not distribute raw novel content. We require that anyone using our work must adhere to copyright policies and obtain proper permissions for any derivative works. The CoSER dataset is derived from literary works may involve ethical considerations, and the content involved does not represent the authors' viewpoints. Our methods can potentially be applied to develop agents for real-world individuals. However, such applications must strictly respect personal data privacy and obtain necessary consent. We hope our research will benefit RPLA researchers and developers, but emphasize the importance of responsible development. Any applications must respect copyright policies, personal data privacy, and be developed with proper licensing."}, {"title": "A. Dataset", "content": null}, {"title": "A.1. Statistics and Analysis", "content": "As shown in Table 5, CoSER dataset is extensive and comprehensive, encompassing dialogue data from 771 books and 17,966 distinct characters. The dataset includes 30,069 unique plots and 29,798 conversations. On average, each conversation consists of approximately 13.2 utterances, with the entire dataset comprising a total of 392,900 utterances."}, {"title": "A.2. Data Splits", "content": "For evaluation purposes, we held out the last 10% of data from each book; that is, they are not included in our prompts or datasets for training or retrieval purposes. Additionally, we trained the COSER models on only 90% of the books."}, {"title": "A.3. Implementation Details for Construction", "content": "Extracting Raw Text LLMs often struggle to extract verbatim original content, especially with punctuation marks like quotation marks, making it difficult to extract raw text directly. Therefore, instead of asking LLMs to extract the complete text of a plot, we prompt LLMs to extract the first and last sentences of each plot. Since the extracted sentences may still contain typos, we apply lexical similarity to match them with the exact sentences from the raw text. Finally, we identify the complete raw text based on these first and last sentences.\nParsing Structured Data During extraction, we instruct LLMs to output extracted data in JSON format. However, LLM-generated JSON strings may sometimes be unparseable or do not conform to the specified format (e.g., missing required keys).\nTowards this challenge, we adopt a repair-and-retry strategy to improve extraction success rate. For each chunk to be extracted, we invoke LLMs and attempt to parse a valid JSON object. If parsing fails, we employ LLMs to repair the invalid JSON string and retry. Some failures occur because the LLM attempts to output JSON that exceeds the maximum token limit, resulting in truncation. In such cases, we prompt the repairing LLM to truncate the JSON at an appropriate point. If it still fail, we restart the entire process from the beginning, making up to 5 attempts.\nRefining Conversation Settings During data extraction, we observe that the initially extracted conversation settings, including scenarios and character motivations, often fail to provide a comprehensive context. We attribute this to the LLM\u2019s tendency to distribute information across different data fields when extracting multiple kinds of information simultaneously, rather than repeating it in different data fields. For instance, if certain information is already mentioned in the plot summary, it might be omitted from the scenario description.\nTherefore, to provide a complete context for given-circumstance acting, we implement an additional LLM call to refine the conversation settings based on the extracted data. We instructed the refining LLM to provide a comprehensive conversation setting, while carefully avoiding any disclosure of subsequent dialogue content or plot developments.\nFor additional details, such as the regular expressions used for identifying chapter titles, please refer to our code."}, {"title": "A.4. Comparison with Existing Methods for Character Profiling", "content": "Previous character profiling methods, including hierarchical updating (Wu et al., 2021), incremental updating (Chang et al., 2023), and one-shot summarization (Yuan et al., 2024), typically only generate the profile of a single character at a time. Morevoer, Papoudakis et al. (2024) shows that these methods, particularly hierarchical updating, perform suboptimally when generating multiple character profiles simultaneously.\nCOSER's multi-stage, extract-then-aggregate pipeline addresses these limitations. It ensures compre-hensive character profiles with high precision and recall of character knowledge, capturing evolving character arcs, and significantly improving processing efficiency."}, {"title": "B. Training", "content": null}, {"title": "B.1. Training Samples", "content": "We transform conversations from the CoSER dataset into training samples in the Sharegpt format. We utilize 90% of the books in the dataset for training, while the remaining 10% are set aside to evaluate our models' ability to generalize to out-of-domain characters and books. We construct one training sample for each character in every CoSER conversation, encompassing both main characters and minor roles. When training on a character $c$, we designate $c$'s messages as targets for optimization, while using the system prompt and messages from other characters as inputs. Adjacent inputs are concatenated.\nTowards general role-playing capabilities across diverse scenarios and applications, we dynamically generate role-playing instructions (system prompts) using varied phrasings, formats, and data types, as shown in Tables 21 and 22. We consider instructions entirely in natural language, as well as those formatted with special symbols (such as ###, ===), randomly sampling different formats and various expressions for the same semantics. We consider various configurations of the available data. Each sample may include (50%) or exclude (50%) the following elements : 1) Profiles of other characters in this conversation; 2) Summaries of the relevant plot; 3) Inner thoughts within the messages.\nBesides character role-playing, we train CoSER models for environment modeling and next speaker prediction (NSP) for multi-agent simulation. For environment modeling, we train LLMs $\\pi_e$ to play the environment role $e$ in the same approach, leveraging environment messages in our dataset. For NSP, given setting $S$ and messages \\{m\u2081, ..., m\u209c\\}, we train LLMs to predict the speaker of m\u209c\u208a\u2081 (or ending the conversation).\nOur role-playing dataset comprises approximately 0.1B tokens, as measured using the LLaMA 3.1 tokenizer. To maintain general intelligence and instruction-following capabilities, we augment this with an equivalent volume (0.1B tokens) of general-purpose supervised fine-tuning data from Tulu 3 (Lambert et al., 2024) 12. This balanced mixture ensures that the model retains broad language understanding while developing specialized role-playing abilities. If more role-playing data are expected, our data curation pipeline can be easily applied to additional books or other fictional works, thereby acquiring data on a much larger scale.\nFor more details, please refer to our code."}, {"title": "B.2. Hyperparameters", "content": "We fine-tune the LLaMA 3.1 models using the following hyperparameters: a learning rate of 1 \u00d7 10\u207b\u2075, a sequence length of 16,384, training for 8 epochs, and a global batch size of 48."}, {"title": "C. Experiment Settings", "content": null}, {"title": "C.1. Test Set Sampling", "content": "The CoSER Test set contains 200 samples: 100 from books used in CoSER training (in-domain) and 100 from books not used in training (out-of-domain). We employ a weighted sampling strategy to prioritize well-established characters with more persona data. The sampling process consists of the following steps: First, for each book, we assign character weights as the square root of the number of"}, {"title": "C.2. Prompting Strategies for Exisitng RPLA Benchmarks", "content": "For existing RPLA benchmarks, including InCharacter (Wang et al., 2024b), LifeChoice (Xu et al., 2024), and Cross-MR (Yuan et al., 2024), we adapt or refine their prompting strategies as follows:\n1. For InCharacter, we add \u201cYou're consulting with a personality assessment expert who will ask you some questions. Please provide honest and detailed responses to help with the analysis. Please think carefully and state your reasons when answering the questions.\u201d after the character profile. This adaptation aims to ensure that RPLAs honestly express their true thoughts. After being trained on authentic character dialogues, the CoSER models, unlike general LLMs, tend to produce brief, conversational-style answers that may be too short or may decline to answer questions, thus failing to provide sufficient information for personality assessment.\n2. For LifeChoice and Cross-MR, we reverse the order of their reasoning and answering processes. Specifically, we have them think before providing their choices, thus enabling RPLAs to make well-considered decisions."}, {"title": "C.3. Length Correction", "content": "In our evaluation, we use a penalty-based scoring mechanism that counts the flaws in RPLAs' perfor-mance. However, since longer simulations naturally accumulate more flaws, we need to implement length correction to reduce length bias in LLM judges, following previous work (Li et al., 2024).\nWe analyze the phenomenon of length bias in penalty-based scoring. The initial score is defined as $s = 100 - \\Sigma_{f \\in F} v_f$, where $F$ represents the set of flaws and $v_f$ is their severity ranging from 1 to 5. Our analysis is conducted on the initial scores from simulations of three models on the CoSER Test set: COSER 70B, LLaMA-3.1 70B, and GPT-40, with or without retrieval augmentation (three Experience and one Conversation), totaling 1,200 cases. We examine the relationship between the number of rounds and the vanilla scores in these 1,200 cases. As shown in Figure 5, we plot the data points for these cases and perform linear regression to fit these points. The fitted linear function is score = -1.5909 \u00d7 rounds + 59.0617, which means that for each additional round in the simulation, the score decreases by approximately 1.6 points.\nTo mitigate this bias, we implement length correction by compensating for the points deducted due to increased rounds. Specifically, we compute the length-corrected score as $s = 100 - \\Sigma_{f \\in F} v_f + \\lambda|M|$, where $\\lambda$ is set to 1.5 based on the analysis above."}, {"title": "D. Examples and Case Study", "content": "We present several examples of our extracted conversations, as well as corresponding simulations in this given circumstance by LLMs.\nTables 7 to 8 illustrate a classic conversation extracted from A Storm of Swords (A Song of Ice and Fire, #3) and the corresponding simulation by CoSER 70B. In CoSER 70B's simulation, when confronted with Sansa Stark, Lysa Arryn utters her iconic line, \u201cGrown enough to be wed, wed enough to be bedded\u201d, reflecting her personality and worldview. This indicates that CoSER models excellently recall and apply character-related knowledge from their pretrained data."}, {"title": "E. Additional Results", "content": "Win Rates against GPT-40 and GPT-3.5 To further evaluate existing LLMs in given-circumstance acting, we present their win rates against GPT-3.5 (OpenAI, 2022) and GPT-40 (OpenAI, 2023) in Table 13, in addition to the results shown in Table 2.\nGeneralization of CoSER Models to New Characters Table 14 separately presents the performance of LLMs on test splits from books included in and excluded from CoSER training. The results demonstrate consistent trends across both splits, confirming that CoSER models maintain strong performance even on out-of-domain characters.\nConversation Continuation Table 15 shows experiment results when multi-agent systems continue conversations from the first $k = 3$ original messages.\nTables 15 presents detailed evaluation results where our multi-agent simulations start from the first $k = 3$ original messages.\nCOSER Dataset for Retrieval Augmentation We validate the effectiveness of COSER 's comprehen-sive data types for retrieval augmentation on the CoSER Test set. We explore three retrieval sources related to specific characters: dialogues from conversations, experiences from plots, and raw text from plots. We compare several combinations of these sources, including: 1) None (Base) 2) Raw"}, {"title": "F. Prompts", "content": "In this section, we list the detailed prompts for: 1) dataset curation in Tables 18 to 20; 2) RPLA and multi-agent simulation in Tables 21 to 22, which have been carefully optimized based on our experience in multi-agent simulation; 3) Penalty-based LLM Judging in Tables 23 to 24."}, {"title": "G. Limitations", "content": "There are several limitations to this study:\nFirst, evaluation via given-circumstance acting still faces challenges related to LLM judges. While the simulation stage effectively elicits RPLA performance, the judging stage still relies on LLM judges. Despite our penalty-based scoring mechanism and detailed rubrics, problems such as length bias persist (). Moreover, LLM Judges may lack the necessary knowledge to accurately evaluate character fidelity.\nSecond, while the dialogues extracted from novels are authentic, their corresponding thoughts remain to be optimized by future work. Character thoughts"}]}