{"title": "More than Marketing? On the Information Value of Al Benchmarks for Practitioners", "authors": ["AMELIA HARDY", "ANKA REUEL", "KIANA JAFARI MEIMANDI", "LISA SODER", "ALLIE GRIFFITH", "DYLAN M. ASMAR", "SANMI KOYEJO", "MICHAEL S. BERNSTEIN", "MYKEL J. KOCHENDERFER"], "abstract": "Public AI benchmark results are widely broadcast by model developers as indicators of model quality within a growing and competitive market. However, these advertised scores do not necessarily reflect the traits of interest to those who will ultimately apply AI models. In this paper, we seek to understand if and how AI benchmarks are used to inform decision-making. Based on the analyses of interviews with 19 individuals who have used, or decided against using, benchmarks in their day-to-day work, we find that across these settings, participants use benchmarks as a signal of relative performance difference between models. However, whether this signal was considered a definitive sign of model superiority, sufficient for downstream decisions, varied. In academia, public benchmarks were generally viewed as suitable measures for capturing research progress. By contrast, in both product and policy, benchmarks even those developed internally for specific tasks were often found to be inadequate for informing substantive decisions. Of the benchmarks deemed unsatisfactory, respondents reported that their goals were neither well-defined nor reflective of real-world use. Based on the study results, we conclude that effective benchmarks should provide meaningful, real-world evaluations, incorporate domain expertise, and maintain transparency in scope and goals. They must capture diverse, task-relevant capabilities, be challenging enough to avoid quick saturation, and account for trade-offs in model performance rather than relying on a single score. Additionally, proprietary data collection and contamination prevention are critical for producing reliable and actionable results. By adhering to these criteria, benchmarks can move beyond mere marketing tricks into robust evaluative frameworks that accurately reflect AI progress and guide informed decision-making in both research and practical domains.", "sections": [{"title": "1 Introduction", "content": "When choosing between different AI models, users are faced with a growing number of increasingly capable options [30]. Such decisions may be informed by model evaluations, which commonly include manual assessment, red-teaming, and the use of AI benchmarks Chen et al. [13]. Benchmarks, which following Raji et al. [36] we define \"as a particular combination of a dataset or sets of datasets [...], and a metric, conceptualized as representing one or more specific tasks or sets of abilities, picked up by a community of researchers as a shared framework for the comparison of methods\", are frequently cited in publications and reports from model developer [7, 39]. Although such reports frame scores on public benchmarks as the basis one should have for understanding a model's capabilities, it is not evident that this framing is consistent with how benchmarks are used in practice. Despite what developers who publish their scores may suggest, benchmark results may not give model users enough information to make substantive, final choices about which model is adequate for their use case.\nIn this work, we seek to understand how AI benchmarks inform decisions between models. To build our theory, we perform and analyze semi-structured interviews with 19 practitioners in academia, policy, and industry, including research scientists, PhD students, and machine learning engineers. The study aims to address the following core research questions:\n\u2022 How are Al benchmarks developed, assessed, and used, across different application areas?\n\u2022 What challenges do stakeholders face in using and interpreting these benchmarks?\n\u2022 To what extent do AI benchmarks impact decision-making and progress in research, product, and policy?\nOur analysis reveals that in the domains of research, product, and policy, individuals are comfortable using benchmarks to make relative comparisons between models. These comparisons more frequently serve negative decisions more than positive ones: a low benchmark score can stop a model from being deployed, but a high score does not guarantee deployment. When it comes to real-world usage decisions, product and policy practitioners often find existing benchmarks to be insufficient. The ability of existing benchmarks to facilitate comparison does not obviate these practitioners' need for further assessment, leading them to either develop internal benchmarks or to use alternate forms of evaluation. This contrasts with how benchmarks function in research settings, where the mere existence of a benchmark can drive growth within a sub-field. Thus, the question remains: what do benchmarks measure progress towards?\nWe propose that the openness of this question is a key factor impacting the adoption of AI benchmarks. Drawing from theories of information technology acceptance [18], we find that although benchmarks meet the majority of the criteria for technology adoption, they fall short on the factor of performance expectancy, i.e., a technology's perceived ability to perform the task it is assigned. A recurring theme among participants in our study was that of a gap between the task a benchmark tests a model on and how that model will ultimately be used. While there are"}, {"title": "2 Related Work", "content": "Our literature review considers three broad domains as the basis for understanding how AI benchmarks are (and are not) used to support decisions. First, to position our findings, we review the literature on the recognized issues with current Al evaluation practices. Although some issues with model assessment are benchmark-specific, others remain open questions in the field. Second, to contextualize current AI benchmarks, we review literature on the history of benchmarking and analyze the motivations that have driven past benchmark use in computing and other fields. Finally, to map the circumstances under which benchmarks may be adopted, we consider models for the acceptance of information technology."}, {"title": "2.1 Evaluating Al", "content": "Evaluating Al systems is necessary for understanding their capabilities, limitations, and potential for positive and negative impacts [38]. However, this process is complex, requiring careful consideration of which evaluation methods are used and how their results are interpreted.\nOne of the primary challenges in AI evaluation is ensuring the closeness of an evaluation to a system's anticipated use [6, 36]. This issue is exemplified by a study conducted by [1], which examined five different AI tools deployed in a U.S. hospital. The study found that none of these tools met expectations created by accuracy measured using expert-labeled ground truth data. This highlights the risk of treating constructed quality measures as objective markers of knowledge and underscores the importance of diligent, use-case specific expert evaluation [1]. This phenomenon of metrics used to evaluate AI models lacking correlation with users' subjective experiences has been documented across applications. Gordon et al. [21] note that models which score highly on standardized tests have been observed and criticized for their obvious mistakes. While bridging the gap between assessment and reality is an inherent issue with AI evaluation [24], we note that it is possible to mitigate the issue to some extent through careful, case-specific design [22, 37].\nAnother challenge in AI evaluation is the inherent difficulty of clearly defining the capabilities AI systems are sup-posed to exhibit and, subsequently, ensuring construct validity [17] with respect to the test tasks. While performance on certain tasks like image classification or speech recognition can be clearly-defined, more complex tasks - such as decision-making under uncertainty or ethical reasoning - are much harder to pin down [8]. Since AI systems often operate in environments that are unpredictable and involve nuanced human behaviors, it is difficult to set clear, mea-surable objectives for evaluation [19]. In a study of the challenges faced by software engineers developing AI systems, Patel et al. [35] find that evaluating performance is a significant challenge, in part due to the trade offs between differ-ent attributes, e.g., accuracy and the protection of privacy. That the former is more easily captured than the latter does not make it a better optimization objective.\nFurther complicating the issue of assessment, Al systems tend to excel in narrow, known tasks, while struggling to generalize to new, unseen scenarios. Evaluations struggle to capture this property, as they are typically based on datasets that represent a small slice of reality. As a result, evaluations can suffer from overfitting, wherein systems"}, {"title": "2.2 History of Benchmark Use", "content": "To understand how benchmarks became a popular approach to Al system evaluation, we examine their history. Bench-marks originated as measures of computer hardware performance, intended to facilitate selection between an increas-ingly varied field of products [26]. Lewis and Crews [26] asserts that there is an important difference between bench-marking in the public and private sectors: the former is required to fairly and thoroughly compare potential vendors, thus placing a high importance on standardized benchmark assessments, while the latter prioritizes profitability and may favor cheaper and less rigorous approaches. As chronicled by [15, 27, 36], both the public and private sectors drove the development of benchmarking for Natural Language Processing (NLP) tasks in the 1980s: DARPA funded the development of a well-defined, efficient evaluation for speech-to-text systems [15]. This had a significant impact on the development of the field. As Church [15] writes, a key benefit of this system was that it enabled hill climbingno longer limited to infrequent evaluations, researchers could measure and pursue more rapid advancements. IBM similarly worked to develop the Common Task Framework (CTF), which would become a precursor to modern AI benchmarking [27]. By providing well-defined goals and corresponding tasks, the efforts of IBM and DARPA lowered barriers to entry, driving growth of a field [27]."}, {"title": "2.3 Benchmarking in Other Fields", "content": "We draw further insights about benchmark use from a review of benchmarking literature from other fields. [14] highlight the need for standardized benchmarks in the evaluation of field-effect transistors (FETs) to enable consistent and fair comparisons across different studies, given the complexity and variability of device structures and materials. According to the authors, the purpose of a benchmark is to provide a reliable framework for assessing key performance metrics, such as current, mobility, and contact resistance, to help guide the development and optimization of FET technologies. They note that benchmarks can also aid in decision-making by offering a clear basis for identifying the most promising technologies and guiding research efforts toward scalable and high-performance solutions.\n[12] discusses the purpose and limitations of environmental quality benchmarks (EQBs). The primary function of EQBs is to serve as a tool to assess potential harm from chemicals and other stressors in aquatic ecosystems, though the author emphasizes they are not perfect and should not be used alone for final decision-making but that \"they can provide understandable scientific input to decision makers\". According to the author, the benchmarks are designed to"}, {"title": "2.4 Theories of Information Technology Adoption", "content": "Theories of information technology (IT) adoption study how new IT \u2013 which broadly encompasses computers, software, data and information processing, and artificial intelligence - becomes accepted and used. AI benchmarks themselves commonly combine software and datasets; as such, we consider them to be a part of IT and draw upon these theories to provide a framework for understanding when and how Al benchmarks are used.\nDavis [18] introduces the Technology Acceptance Model (TAM), which identifies two primary factors that im-pact the acceptance of information technology: perceived usefulness and perceived ease of use. The former refers to the perception of a given tool's ability to enhance an individual's job performance, while the latter refers to the per-ceived effort that the use of the tool requires. Integrating Davis [18]'s model with several others, Venkatesh et al. [40] presents the Unified Theory of Acceptance and Use of Technology (UTAUT). According to this framework, four constructs determine acceptance and usage: performance expectancy, effort expectancy, social influence, and facilitat-ing conditions. Performance and effort correspond to Davis [18]'s usefulness and ease of use, respectively. Venkatesh et al. [40] defines social influence as an individual's perception that important others believe they should adopt the tech-nology in question. Finally, facilitating conditions are the degree to which an individual believes that organizational and technical infrastructure supports the technology's adoption.\nApplying this theory to AI benchmarks, we predict that Venkatesh et al. [40]'s four criteria must be met in order for benchmarks to be adopted. Since benchmarks aim to provide information about a model, we expect practitioners must determine this information to be useful in order for the criteria of performance expectancy to be met. The widespread use of benchmark scores in publications and marketing materials [7, 39] suggests that social influence is fulfilled. However performance expectancy, effort expectancy, and facilitating conditions remain open questions that we explore in our interview study. We note that even when all factors are present, the UTAUT model does not provide a prediction about absolute levels of usage. Instead, it offers an interpretation for why benchmarks are or are not employed by practitioners."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Research Design", "content": "This study utilizes a grounded theory approach to investigate the development, use, and impact of AI benchmarks [3]. Grounded theory is employed due to its suitability for exploratory research, enabling the derivation of theory based on empirical data [25]. This method is particularly appropriate for examining dynamic and multifaceted subjects such as AI benchmarks, where established theoretical frameworks are limited or underdeveloped [23]. A semi-structured"}, {"title": "3.2 Data Collection", "content": "In-depth interviews were conducted with participants representing four categories of stakeholders: policy analysts, researchers in academia, researchers in industry, and benchmark users in product development and management. Interviews took place over the course of a month, September of 2024, via the online platform Zoom. The interviews were conducted in English and all interviewees were located in the United States.\nParticipant selection for this study followed a combination of intentional sampling and snowball recruitment. Ini-tially, individuals were identified based on their recognized expertise and relevance to the subject matter, ensuring the inclusion of participants with diverse professional backgrounds. In addition, snowball sampling was employed, wherein participants were asked to recommend other potential informants who could offer insights. This was comple-mented by posting invitations on LinkedIn and X, which allowed reaching a broader audience of professionals. Finally, the recruitment process involved directly contacting experts through professional networks. Our work intentionally varies participants' backgrounds to allow for a broad perspective on the development, application, and impact of AI benchmarks across different domains (see Table 1). We note that our sampling approaches resulted in a population that was heavily skewed in terms of gender, thus limiting the generalizability of our conclusions.\nAll participants provided informed consent, and their identities were anonymized in the final dataset to protect their privacy. IRB approval was obtained prior to data collection, ensuring the study adhered to relevant ethical guidelines for research with human subjects (IRB-76636)."}, {"title": "3.3 Interview Design", "content": "The interview design for this study employed a semi-structured format, allowing for both consistency in data collection and flexibility for exploring emerging themes in greater depth. Interview questions were crafted to align with the study's objectives of examining the development, application, and impact of AI benchmarks across different domains. This approach enabled the participants to share their experiences while allowing the interviewer to follow up on specific insights as they came up. The interview guide (see Appendix A.2) was structured into multiple key sections, with questions designed to gather comprehensive data on participants' interactions with Al benchmarks and their perspectives on their effectiveness, limitations, and improvement suggestions.\nWe began our interviews with introductory questions such as, \"Please describe your current role\" and \"How do you interact with Al models in this role?\" These questions were included to establish a baseline understanding of the participant's professional background and their direct engagement with Al systems. By gaining insight into each participant's unique context, the interviewer could better interpret the interviewee's responses to subsequent questions about benchmarks.\nNext, the interviews moved to questions like \"In the context of your work, what is the purpose of AI benchmarks?\" and \"How satisfied are you with the ability of existing benchmarks to fulfill this purpose?\" which were designed to probe participants' views on the functionality and relevance of benchmarks in their professional tasks. These questions were critical for assessing the alignment between current benchmarks and their intended goals. Additionally, questions"}, {"title": "4 Findings", "content": "Through qualitative analysis of our interview data, we found that although all participants used benchmarks for relative comparisons of models, few interpreted these scores as an absolute signal, relying on supplemental measures to make downstream decisions. In this section, we present our detailed findings, highlighting how the strengths and weaknesses of existing benchmarks lead to the observed use patterns."}, {"title": "4.1 Relative Measures to Assess Progress", "content": "Across all three settings considered - research, product, and policy - participants reported that benchmarks were highly useful as standardized measures of relative progress towards a benchmark's target metric.\nI-11 describes the ability of benchmarks to evaluate whether a new method is an improvement over previous ones:\n\"So to make progress, when you're designing a method... you need to have a sense of if you're actually heading in the right direction. And I think benchmarks are often nice if they actually do encapsulate the problem you care about.\"\nThe relative assessments benchmarks provide can guide future development by indicating the efficacy of current approaches. I-15 describes benchmarks as a directional measure: \"We're using them as sort of a vector, right? Are we headed in the right direction?\" This type of insight is also beneficial in a product setting. I-6 lists questions benchmarks can answer about a new model:\n\"How do we do [with a] previous version of a model compared to a new version, or... if you're doing a feature improvement, how does the addition of a feature improve or decrease the performance?\"\nPractically, these signals guided model developers and users in decisions such as whether to adapt the model (e.g., if benchmark scores dropped precipitously) or to continue development (e.g., if it outperformed some baseline). These relative scores were more often used as negative indicators than positive ones. In other words, poor benchmark per-formance was more likely to disqualify a model from further development or use than good benchmark performance was to qualify it. I-8 describes the impact of benchmarks on model development:\n\"\"We'll see regressions [in score] and then that's led to some debugging. For instance, people say 'why did numbers get worse on this dimension?' So they go back, 'Oh, actually, we made a mistake' . . . Or, actually, you know, we had tried a more aggressive approach for this thing... and we need to roll things back.... Yeah, it's called some late nights for someone on our team too, where they find out, like, . . oh, actually, the results weren't as good as we expected, so we need to try to slip in another checkpoint before things get released."}, {"title": "4.1.1 Research: Is Our Model Better?", "content": "When used in research settings, benchmarks measured comparative performance between old and new methods. I-18 underscores the importance of the standardized measurement these benchmarks provide:\n\"Without it, you won't even have any kind of feedback loop, right? So we won't know how we are doing against... the state of the art models, even if we come up with a new technique.\"\nThese evaluations were accepted to the extent that I-17 described benchmark performance as essential to publication: \"If you're trying to publish an academic paper and it doesn't beat the benchmark, it won't get published.\" Within academia, these scores could be viewed as definitive signs of function and progress, as reflected in I-15's statement that \"the goal in academia is to just push the number higher.\" In research, product, and policy, benchmarks were considered markers of relative improvement. A model was good if it outperformed baselines on benchmark assessments. This was a key point that distinguished research from product and policy: in research, improvement on a benchmark alone made a model good."}, {"title": "4.1.2 Product and Policy: Is Better Good Enough?", "content": "Confronted with the consequences of real-world deployments, prod-uct and policy practitioners in our interviews required a more rigorous standard. I-1 explained this need:\n\"If you want to build benchmarks that inform policy or inform a societal response to Al systems, that requires having an... understanding of the absolute capabilities, rather than the relative capabilities.\"\nGiven the lack of absolute signal available, I-7 observed that benchmarks were not used as the basis of significant decisions: \"People look at them. They make minor code changes based on them. I've never seen someone launch or not launch, like, or do anything super critical based on a benchmark.\" When industry benchmarks were used to inform deployment, this was cause for concern: \"I get very nervous when, when people say 'based on this benchmark, we think this is launchable'\" (I-8). The most notable gap, in I-8's perspective, arose because \"it's impossible to know how people are going to use the [model being evaluated]\". To mitigate this, he suggested that model developers draw from the release processes used for other products: \"you have test users come in, or you do user case studies.\" This need for substantial additional assessment contrasts starkly with the research setting, where benchmark improvement alone was considered a success."}, {"title": "4.2 General Assessments, Specific Applications: When Benchmarks Lack Relevance", "content": "The observed discrepancy in benchmark interpretation may arise from the fact that unlike decisions between models made in research settings, those made in policy and product often require absolute signal due to their potential for direct impact on people. Many of the participants we spoke to found publicly available benchmarks lacking and were actively developing benchmarks suited to the specific deployments they were concerned with. Based on our analyses, we identify the gap between what existing public benchmarks assess and how models will be practically used as a key factor that limits their meaningful adoption. I-9, who was building benchmarks tailored to specific industrial"}, {"title": "4.2.1 The specific case of safety-critical systems", "content": "Such evaluation was not considered sufficient by all practitioners. When issues of safety were at stake, policy and product workers needed assessments that were both standardized and grounded in evidence and downstream use cases. A goal of I-8, who developed benchmarks for safety-relevant features, was \"trying to anticipate the real user behavior.\" I-1 described the challenge of evaluating risks that arise from a model's downstream use:\n\"Proxies are like doable, and... relative comparisons are doable, but absolute sort of assessment of a model's risk is a lot harder because... there's a difference between the things you can do in a benchmark and how a model will actually be used.\"\nGeneral public benchmarks were not considered suitable for such applications. I-14, who was researching benchmarks to evaluate Al models for medical applications, described the gap between performance on such benchmarks and performance on downstream tasks:\n\"Large language models are benchmarked on some multiple choice questions, but we found that those don't actually reflect well when applied to clinical settings, because ... the proxy that was used to create the benchmark as a multiple choice question was not... reflective of how things function in the clinic\""}, {"title": "4.2.2 Informative Benchmarks Fit Real-World Use Cases", "content": "Outside of safety-critical use cases, the most informative benchmarks were also considered to be those where the ability being measured was tied as closely as possible to the real-world use case of the model being evaluated. Without such grounding, benchmark scores were difficult to interpret. I-10 compared assessing models' ability based on a single quantitative benchmark score to assessing people's health based on weight:"}, {"title": "4.3 Benchmarks Cannot Replace Human Evaluation", "content": "Even when satisfied with available automated metrics, participants in all three groups agreed that benchmarks should be complemented by, rather than substituted for, human evaluation. I-7, who found benchmarks to be highly suited to his use case of ASR, said\n\"That would be the ideal scenario where you get someone speaking or each language, which we cover, and asking them to spend, like, a couple of hours giving them some instructions where they just absolutely go ballistic at the model, do all kinds of things and see that you will get, like, a good trend.\"\nWhen assessing the medical models they were developing, I-14 found that \"keeping humans in the loop is necessary.\" Human feedback could supplement both benchmarks and approaches such as model-based red-teaming. A robust system, I-8 reported, would \"have... red teaming, and also just... people [to] test out the models, like, use it in whatever way, as additions to benchmarks.\" Participants agreed that there were insights which could only be gained through direct, manual evaluation of the models.\nAn additional benefit of such human evaluation is that it is durable in the face of rapid model advancement. While benchmarks frequently saturate, human evaluation remains relevant. I-2, whose work concerned the assessment of future model capabilities, said\n\"I place a high value on, like, the human interaction, vibes-based evaluation technique, because it's... more or less the only thing that doesn't get invalidated by a new paradigm or type of model. We don't really know what models will be like in like five years or something, and it would be very surprising if many of our benchmarks were still used then.\"\nI-11 named human-centered evaluation as the most significant gap in existing benchmarks:\n\"Benchmarks... measuring like, how LLMs or just generally, models interact with humans are missing. It's hard and there's not really a clear setup. I think people probably need to partner more with... psychologists and social scientists on how to like do these benchmarks. But those would be the most valuable.\"\nParticipants agreed that task-specific evaluations, designed with a model's use case in mind, were more helpful than generic ones. Consistent with this belief, they expressed that for models directly used by humans, human evalua-tion is irreplaceable. Although some participants validated their benchmark results with extensive human evaluation, such testing was most commonly performed by the developers themselves, who interacted with their own models during the development process. Many noted the expense of human annotation as a significant deterrent for collecting comprehensive manual evaluations."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Al Benchmark Adoption Through the Lens of UTAUT: Performance, Effort, Social Influence, and Facilitation", "content": "Our work seeks to understand how and when AI benchmarks are used by practitioners. Deciding to use a benchmark can be considered the acceptance of a technology; thus, we return to the UTAUT framework and discuss which of its criteria were or were not met for our participants. Applying this framework, we see that although public benchmarks were generally considered easy to use (effort expectancy), were widely accepted (social influence), and generally came with infrastructure to support their use (facilitating conditions), individuals working in policy and product did not"}, {"title": "5.2 Improving Benchmarks is Necessary \u2013 and Hard", "content": "Despite benchmarks' widespread use and well-known limitations, participants who developed benchmarks in both research and product settings found that the impact and difficulty of their work was underestimated. I-8 described his colleagues' perception of benchmarking: \"I don't think it's appreciated enough. I think people say, like, just come up with the data set. How hard can it be?\" In research, I-15 expressed that such attitudes led some students away from benchmark development:\n\"My biggest gripe... is going to be that our field only exists and progresses because of benchmarks. Like, if you don't publish experimental results on a benchmark, you don't publish like, you literally don't publish. You will get rejected. Yet, if you try to publish a benchmark data set at a conference, it'll more likely than not be rejected because you didn't present a method... So if you propose that let's build a new benchmark, everyone will be like, well, let's hold on a second. Maybe that's not worth the time. There's no ROI on that for a PhD student.\"\nUnderstanding the difficulty of developing benchmarks is critical, since the problems with existing methods cannot be solved without the investment of substantial time and resources. Practices like the inclusion of community members and domain experts in the benchmark design process are costly, but are essential to evaluating model performance in a way that is truly meaningful to those that models most directly impact."}, {"title": "5.3 What Is A Better Benchmark?", "content": "Many of the participants in our study were working to mitigate the outlined issues with existing benchmarks by developing new, internal ones. When asked about how they addressed these challenges, participants emphasized that a good benchmark should offer a meaningful evaluation of an Al system's capabilities, aligning closely with real-world applications to ensure robust, meaningful assessments. I-2 mentioned that benchmarks \"should be actually informative about [a] particular capability\" and I-7 stressed the importance of \"making sure the data is realistic.\" I-7 emphasized that \"there's a ton of little domain-specific things that matter a lot to getting a good benchmark,\" highlighting the need for benchmarks to address the specific contexts in which Al models will be deployed, and the necessity of involving domain experts in the design process.\nAnother trait that participants emphasized was that of transparency in benchmark scope and goals as a step towards preventing misinterpretation. One way to make scores easier to interpret, which participants frequently mentioned,"}, {"title": "6 Limitations", "content": "The study's scope is limited by its reliance on self-reporting methods, such as interviews, which may introduce bias due to participants' desire to appear competent or answer in socially desirable ways [33]. The presence of the researcher and"}, {"title": "7 Conclusion", "content": "AI benchmarks can act as markers of relative progress, especially in academia, but fail to allow new models to demonstrate actual increased performance on relevant capabilities. Marketing materials may assign substantial meaning to benchmark scores; however, most benchmarks provide limited information regarding practical deployment decisions. Although a score of 80 is relatively better than a score of 70, figures such as these are only meaningful in the context of a benchmark designed to inform downstream use. Given the high-stakes settings in which Al models are increasingly used, it is critical that practitioners have informative comparative measures available to them. Through measures such as the incorporation of community members and domain experts in the benchmark design process, more relevant benchmarks can be developed. Based on the interviews we conducted, we propose that the difficulty of creating such benchmarks must be appreciated so that resources can be allocated appropriately and that even ideal benchmarks cannot obviate the need for human evaluation."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Interview Code Data", "content": ""}, {"title": "A.2 Interview Question Guide", "content": "Introduction and Background\n(1) Please describe your current role.\n\u2022 How do you interact with Al models in this role?\n\u2022 How did you come to this role?\n\u2022 Can you tell me about your relationship to AI and machine learning technology more broadly?\n\u2022 How does your educational background inform the way that you think about AI?\nBenchmark Usage and Purpose\n(1) In the context of your work, what is the purpose of AI benchmarks?\n\u2022 How satisfied are you with the ability of existing benchmarks to fulfill this purpose?\n(2) What are you using benchmarks in your job for?\n\u2022 Can you describe your process for doing this?\n\u2022 Of the use cases you mentioned, which is the most prevalent?\n\u2022 What is your goal in using benchmarks for these purposes?\n(3) What information do you hope to get from benchmarks?\n\u2022 How would this information be helpful?\n\u2022 What choices does this information inform?\n(4) How often do you use benchmarks?\n\u2022 Would you like to be using benchmarks more or less often? Why?\nBenchmark Selection and Evaluation\n(1) Can you describe your process for finding a suitable benchmark?\n\u2022 What factors are you considering when choosing a benchmark?\n\u2022 When do you consider having looked at 'enough' benchmarks to make an informed decision?\n\u2022 Where do you find these benchmarks?"}]}