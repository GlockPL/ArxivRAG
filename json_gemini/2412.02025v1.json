{"title": "PKRD-CoT: A Unified Chain-of-thought Prompting for Multi-Modal Large Language Models in Autonomous Driving", "authors": ["Xuewen Luo", "Fan Ding", "Yinsheng Song", "Xiaofeng Zhang", "Junnyong Loo"], "abstract": "There are growing interest in leveraging the capabilities of robust Multi-Modal Large Language Models (MLLMs) directly within autonomous driving contexts. However, the high costs and complexity of designing and training end-to-end autonomous driving models make them difficult for many enterprises and research entities. To address this, our study explores a seamless integration of MLLMs into autonomous driving systems, by proposing a Zero-Shot Chain-of-Thought (Zero-shot-CoT) prompt design named PKRD-CoT. PKRD-CoT is constructed based on the four fundamental capabilities of autonomous driving-perception, knowledge, reasoning, and decision-making-making it particularly suitable for understanding and responding to dynamic driving environments by mimicking human thought processes step by step to enhance decision-making in real-time scenarios. Our design enables MLLMs to tackle problems without prior experience, thus enhancing its utility within unstructured autonomous driving environments. In experiments, we demonstrate the exceptional performance of GPT 4.0 with PKRD-COT across autonomous driving tasks, highlighting its effectiveness for application in autonomous driving scenarios. Additionally, our benchmark analysis reveals promising viability of PKRD-CoT for other MLLMs such as Claude, LLava1.6, and Qwen-VL-Plus. Overall, this study contributes a novel and unified prompt designing framework for GPT 4.0 and other MLLMs in autonomous driving, while also evaluating the efficacy of these widely recognized MLLMs in the autonomous driving domain through rigorous comparisons.", "sections": [{"title": "1 Introduction", "content": "Previously, autonomous driving technologies were dominated by data-driven modelling [4], where deep learning advancements have allowed cars to \"learn\" how to drive from large amounts of driving data. However, such an approach require us to train the car on a large amount of data, giving rise to dilemmas such as dataset bias, overfitting, non-interpretability, and poor generalization ability [3].\nOn the contrary, the nature of human driving skills is knowledge-driven rather than data-driven through the exploration of several studies [33]. For analogy, humans can always drive a car correctly by learning driving skills and traffic rules for a few months. In other words, learning how to drive is accomplished via prior knowledge transfer without requiring a long training time. Motivated by this, knowledge-driven autonomous driving techniques [21] is introduced as an attempt to address the existing difficulties of data-driven driving techniques.\nIn particular, recent approaches [33,8,9] have integrated Large Language Models(LLMs) and Vision-Language Models (VLMs) into autonomous driving, emphasizing language reasoning, decision-making, and vision-text fusion. Additionally, Multimodal Large Language Models(MLLMs) [31,10] merge natural language processing with computer vision to efficiently interpret complex traffic scenarios, including pedestrian movements, traffic signs, and vehicle behavior prediction.\nMLLMs excel in processing multimodal data for autonomous driving [34], achieving minimal human intervention by automating decision-making and operations [7]. Their integration of visual, linguistic, and sensor data enables precise navigation and real-time decisions in complex environments [14], thus achieving accurate model performances in tasks such as object detection, scene understanding, and trajectory prediction [29].\nThe above findings have motivated the question: How can we apply the capabilities of MLLMs to autonomous driving systems?\nTraining MLLMs is often complex and costly. Even with fine-tuning techniques like LORA [13], substantial computational resources and data are required [20]. In terms of developing MLLM capabilities in autonomous driving more cost-effectively, the prompt techniques has shown immense potential [5,12].Research [17] on motivating language models, it has been found that Zero-shot-CoT(Chain-of-Thought) can be used to generate instances of thought process at each traffic scenario through \"Let's think step by step\", together with an answer-directed prompt to motivate the model in generating answers [27]. This through process of Zero-shot-CoTresembles the human thought process [16], thus better capitalizing the language-driven ability of MLLMs.\nTherefore, we have designed a Zero-shot-CoT prompt framework, named PKRD-COT(Perception, Knowledge, Reasoning, and Decision-making Chain-of-Thought) as Figure 1, derived from four fundamental capabilities of autonomous driving, i.e., perception, knowledge, reasoning, and decision-making. Autonomous driving systems require \"perception\" and \"knowledge\" to understand their environment, while \"reasoning\" and \"decision-making\" are in respond to dynamic traffic conditions, which is crucial for their safety and efficiency, and \"memory\" can effectively resolve contextual continuity problems in LLM. Two experiments on the real-world Nuscene dataset and a highway simulation dataset are used to thoroughly validate the the understanding and reasoning ability of MLLMs in real driving scenarios. Our results show that MLLMs with PKRD-CoT improve end-to-end autonomous driving."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Multi-Modal Large Language Models", "content": "The rapid evolution of Language Large Models (LLMs) has continuously been improving their abilities to understand the world [38]. Multimodal Language Large Models (MLLMs), as an extension of LLMs, represent a more advanced level, enabling language understanding that is no longer confined to text, but capable of comprehensively understanding and generating multiple types of data, including non-textual data such as images, audio and video [10,37].\nRecent advances in computational power and data availability have spurred the development of multimodal macro models, enhancing complex content understanding and task precision. Notable models like GPT4.0 [18], which is known for its broad application success and data handling; MiniGPT-4[40], a streamlined version of GPT4.0 with efficient linguistic processing and lower resource needs; InstructBLIP [11] and LLaVA [25] for rapid image recognition and linguistic output; Qwen-VL [2] for image localization and mathematical tasks; CogVLM [30] for image comprehension and dialogue; and Anthropic's Claude AI [1], excelling in reasoning, math, and code generation.\nCurrently, more studies are using MLLMs for autonomous driving [33,24,36,35,28]. A notable example of the autonomous driving systems is DriveMLM [31], which combines the MLLM's decision-making outputs with the behavioral planning phase of autonomous driving, enabling the driving system to understand complex driving scenarios and make smart decisions.\nA MLLM-integrated autonomous driving system will be more accurate, interpretable and safe. On this account, applying these various MLLM capabilities, such as sensing, recognizing, reasoning, and decision-making, is an essential pressing issue of autonomous driving."}, {"title": "2.2 Language Prompt", "content": "Prompt engineering has emerged as a critical technique in adapting MLLMs to specific tasks and environments[22]. Prompt engineering techniques take many forms, including zero-sample prompts, few-sample prompts, role-playing prompts, and chain-of-thought prompts[19][39].\nAmong the various types of prompts, Zero-Shot Chain of Thought (CoT) prompts have shown remarkable effectiveness in guiding MLLMs. Kojima et al. [15] highlighted the Zero-Shot CoT prompting method, showing its potential to improve reasoning and decision-making without requiring additional task-specific training data [32].\nZero-shot Chain of Thought (Zero-shot-CoT) enables complex question answering across domains without task-specific model training by simulating human problem-solving thought processes in language models [17]. It improves the model's adaptability, flexibility, and explanatory power [23], especially when dealing with abstract and multi-step reasoning tasks.\nAccording to the studies, the models applying Zero-shot-CoT demonstrated significant performance improvement on multiple standard datasets [26]. Using Zero-shot-CoT, models significantly improved multi-step reasoning accuracy [27] and demonstrated enhanced understanding and answer generation in complex reading comprehension and question-answering tasks [16].\nThe Zero-shot-CoT is crucial for fully realizing the potential of MLLMs for end-to-end autonomous driving. Through these unique advantages, Zero-shot-CoT guides the model to find the optimal solution in its thought process [17] to facilitate a smooth end-to-end process from perception to decision-making for autonomous driving."}, {"title": "3 PKRD-CoT", "content": ""}, {"title": "3.1 PKRD-CoT framework", "content": "PKRD-CoT is a specialized prompt framework designed for autonomous driving tasks. It derives its capabilities from four fundamental aspects: perception, knowledge, reasoning, and decision-making, ensuring safety and efficiency in dynamic traffic conditions. The process is carried out step by step through Observation, Identification, Memory, and Decision, which we detail in the following.\nThe first step is to observe the environment and form a current understanding of the surroundings. It ensures that the system accurately perceives relevant details and dynamics of the driving environment.\nThe second step is to identify and predict specific targets within the environment. It ensures that the system can focus on and track important elements, such as other vehicles and pedestrians, crucial for safe navigation.\nThe third step memory stores the environmental understanding in a structured JSON format, following a designed template. This module addresses the limitations of language models in maintaining context over extended interactions, enabling the system to retain and utilize past information effectively.\nThe last step is to make informed decisions based on the current understanding of the present information and the stored information from the memory. It ensures that the system can respond appropriately to dynamic traffic conditions."}, {"title": "3.2 PKRD-COT Performance in GPT4.0", "content": "This section explores and validates whether GPT-4.0 can perform autonomous driving tasks within the PKRD-CoT framework.\nEnvironment Construction: In the environment construction, we select a subset of data from a public autonomous driving datasets, Nuscenes [6], which comprises complex and diverse driving scenarios to conduct tests and experiments.\nIn particular, we merge the images from six cameras in Nuscenes, right-front, front, and left-front images are merged into the front image; right-back, back, and left-back are merged into the back image, to form a panoramic environment around the car. This panoramic view not only enhances GPT4.0's overall awareness of the surrounding context, but also effectively reduces the long tail problem (weak understanding and generation for rare or uncommon scenarios) existing in GPT4.0.\nInput and Output: We input the panoramic view of the car's environment (front and back view formed by 6 images) alongside the PKRD-CoT prompt. In doing this, we expect GPT-4.0 to utilize its Scene Perception, Object Detection, Localization, and Semantic Understanding modules to execute the task and ultimately provide the analysis and the answer to the task."}, {"title": "3.3 PKRD-CoT Ablation Study", "content": "To validate the PKRD-COT prompt framework, we designed and implemented a series of ablation experiments. These experiments are conducted on images of real driving scenes, and GPT4.0's final decision is used subjected to evaluation. The accuracy of the this ablation study is evaluated as the number of correct decisions divided by the total number of samples. These experiments were designed to assess the effects of different prompt engineering methods: zero-shot, role-playing, and PKRD-CoT on the performance of the MLLM model, i.e., GPT4.0 in our case.\nPrompt Zero-Shot: we defined a baseline experimental condition in which the model was allowed to make driving decisions without any cues, relying solely on its built-in capabilities for decision-making and task execution without additional auxiliary information or structural prompts.\nPrompt Role-Playing: we introduced role-playing prompts that simulate specific scenarios and roles (intelligent drivers of self-driving cars) to stimulate the model's ability to respond more naturally and intelligently in the simulated environment.\nPrompt PKRD-CoT: By explicitly structuring the chain of thought, and based on the four important capabilities required for autonomous driving, PRKD-CoT helps the model to decompose complex tasks step by step, ensuring that each step of the reasoning process is interpretable, thus ultimately improves the accuracy and reliability of task execution."}, {"title": "4 MLLMs Capabilities Evaluation Experiments", "content": "In the previous section, we has demonstrated that by incorporating the PKRD-CoT prompt, GPT-4.0 is able to achieve high-fidelity decision making for self-driving. Building on these experimental insights, we further explored the integration of other MLLMs with our PKRD-CoT prompt framework and assessed their performance in autonomous driving.\nGiven that the essence of our PKRD-CoT lies in four essential capabilities for autonomous driving, we conducted experiments to evaluate various models based on these key abilities: perception, knowledge, reasoning, and decision-making. Here, we selected six representative state-of-the-art MLLMs: GPT4.0 [18], claude [1], LLaval.6 [25], Qwen-VL-Plus [2], CogVLM chat [30], and Minigpt4 [40], for comparisons."}, {"title": "4.1 Perception Ability", "content": "We obtained real images from public datasets and tested them on the following five target species: Car, People, Traffic Lights, Pedestrian Crossings, and Current Scene."}, {"title": "4.2 Knowledge Ability", "content": "We further tested these MLLMs on the panoramic view of the car, each MLLM will act as an intelligent driver and attempt to accomplish identified tasks. The models scrutinize and analyze the driving scenarios based on the given driving scenarios information and their own knowledge. Figure 5 shows an example of the models' decision-making a given scenario.\nIn conclusion, CogVLM chat, GPT4.0, Claude, and LLava 1.6 show emphasis on safety by tending towards stopping or remaining the current speed at red lights and crosswalks. This suggests a cognitive ability that includes understanding and adhering to traffic rules. On the contrary, Minigpt4 is inclined towards utilizing information of upcoming signal changes for making decisions, thus showing an ability to predict the environment but putting less emphasis on safety, which is essential in real-world traffic environments. GPT4.0, on the other hand, takes into account the dynamics of the incoming red light and the vehicles present at the moment, to make a more accurate and informed driving decision. Both of GPT4.0 and Qwen-VL-Plus show the model's ability to adapt to a dynamic environment. Qwen-VL-Plus considers not only traffic rules but also pedestrians, showing its complexity and comprehensiveness in cognitive processing."}, {"title": "4.3 Mathematical Reasoning Ability", "content": "In addition, the ability to perform mathematical calculations is crucial for scene understanding in autonomous driving, such as calculating relative speed and maintaining a safe distance between vehicles. Therefore, we assessed mathematical computation as a measure of reasoning ability.\nTo test the mathematical ability of the model, we conducted a computational experiment during driving, using the safe vehicle distance as the reference object. MLLMs calculate the vehicle distance using the coordinates of the vehicle and the Pythagorean theorem formula. The calculated distance is then compared with the ground-truth distance, allowing for calculating the error percentage, which will be used to verify the model's computing ability.\nThis experiment proved that the model can automatically evaluate the Pythagorean theorem for distance calculation with varying parameters, even without additional prompts. A model was counted as correct if it could correctly compute the distance with an absolute error of not more than 0.5m. Then, the percentage of correct model calculations is computed to evaluate the model's computational ability. The results are shown in the Table 3."}, {"title": "4.4 Decision-Making Ability", "content": "In autonomous driving, perception, knowledge, and reasoning are used to inform the final decision. Decision-making capability integrates the vehicle's perception, knowledge, environmental changes, and predicted future trends to make optimal choices. The driving agent needs to quickly process large amounts of data, while demonstrating high adaptability and flexibility to respond to the complex and changing driving environment.\nHere, we choose highway as the experimental scenario and used the BEV images to assess the decision-making ability of the MLLMs on the highway environment. To achieve this, we first convert driving scenario data to text for informed input, and then input our PKRD-CoT prompts to guide the output decisions of these models. Outputs of the MLLMs are shown in the Figure 6."}, {"title": "5 Conclusion", "content": "Our research presents an innovative PKRD-CoT framework that combines chain-of-mind cues with the essential capabilities (perception, knowledge, reasoning, and decision-making) essential for autonomous driving. This marks the first application of prompt engineering to multimodal large language models (MLLMS) for an autonomous driving task. The seamless integration of these capabilities within the PKRD-CoT framework allows MLLMs to perform knowledge-driven driving without the need for pre-training.\nIn addition, we present a set of comprehensive evaluation tests to assess the suitability of various MLLMs for autonomous driving tasks, focusing on the four key driving capabilities. Our results show that GPT-4 outperforms the other models overall, with Claude and LLava 1.6 following in second place. In particular, CogVLM excels in target localization and perception tasks, while MiniGPT-4 shows weaker performance in mathematical reasoning. These tests offer valuable insights into the strengths and limitations of the different MLLMS, crucial for future advancements in LLM-based autonomous driving systems.\nIn conclusion, our study introduces a novel and effective framework that leverages PKRD-CoT in conjunction with MLLMs for autonomous driving. This unified approach enhances real-time decision-making and fosters further innovation in the field. Potential future work in this area includes refining knowledge integration and exploring advanced reasoning capabilities to further improve the safety and efficiency of autonomous driving systems."}]}