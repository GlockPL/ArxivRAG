{"title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles", "authors": ["Joshua Ashkinaze", "Emily Fry", "Narendra Edara", "Eric Gilbert", "Ceren Budak"], "abstract": "Recent debates raised concerns that language models may favor certain viewpoints. But what if the solution is not to aim for a \"view from nowhere\" but rather to leverage different viewpoints? We introduce Plurals, a system and Python library for pluralistic AI deliberation. Plurals consists of Agents (LLMs, optionally with personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is a generator of simulated social ensembles. Plurals integrates with government datasets to create nationally representative personas, includes deliberation templates inspired by democratic deliberation theory, and allows users to customize both information-sharing structures and deliberation behavior within Structures. Six case studies demonstrate fidelity to theoretical constructs and efficacy. Three randomized experiments show simulated focus groups produced output resonant with an online sample of the relevant audiences (chosen over zero-shot generation in 75% of trials). Plurals is both a paradigm and a concrete system for pluralistic AI.", "sections": [{"title": "1 Introduction", "content": "There is a fundamental tension between how generative AI models are built and how they are used. Companies typically build a small number of foundation or \"generalist\" models that dominate the market [90]. However, these generalist models are used by a diverse base of users-with varying preferences and values. Invariably, this tension sparked allegations of bias, with supposedly neutral models accused of favoring certain viewpoints [12, 25, 28].\nWhile a tempting solution is to aim for models that have \"no bias\" and hold a \"view from nowhere\" [36], truly neutral models are likely infeasible. Some scholars argue that all knowledge is situated [36]. But with open-ended text generation, defining some unbiased ground truth is especially difficult. For many use cases, there is no unbiased ground truth. This difficulty is compounded by the fact that users can ask models a large variety of questions. Any bias benchmark can only capture an infinitesimal slice of the query space [68].\nAs a motivating example, imagine a company preparing to launch a new work-from-home policy. The CEO seeks to determine which aspects of the policy memo will raise concerns for employees. Or suppose a housing justice group aims to identify the most effective messaging for a homeless shelter proposal. LLMs can theoretically be deployed for both cases. But what viewpoint should the LLM adopt? Different employees and residents have different perspectives. The standard approach of prompting a single model is unlikely to represent diverse viewpoints. We propose an alternative approach: A system of LLMs engage in controlled deliberation, simulating distinct viewpoints. The CEO could create a network of simulated employees to provide feedback, upweighting the voices of the most affected groups. The housing justice group could create a sequence of LLMs with demographically weighted personas to provide iterative feedback based on preceding concerns.\nAs an alternative to \"bias-free\" models, we introduce a new pluralistic Al system [81], Plurals, that can accomplish these tasks. It is a public-facing Python library (Figure 1 for system overview, Figure 2 for code snippets, see here\u00b9 for library). Plurals consists of Agents (optionally integrated with government datasets for nationally representative personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is an end-to-end generator of customizable \"simulated social ensembles\u201d. We incorporate interaction templates inspired by democratic deliberation theory and integration with government datasets for nationally representative personas. For example, to create an Agent representing a male California resident, our system samples a statistically representative citizen from American National Election Studies, and then uses the citizen's demographics and political stances as an LLM prompt. We draw on democratic deliberation theory, which emphasizes dialogue between different views [14, 55], as a blueprint. Our work builds on research in deliberation [13, 14, 27, 35, 55, 59, 80], pluralistic sociotechnical systems [4, 33, 50, 97], and multi-agent AI"}, {"title": "1.1 Brief System Overview", "content": "Plurals allows users to create simulated social ensembles with Agents, Structures, and Moderators: Agents complete tasks within Structures, which define how information is shared between Agents."}, {"title": "2 System Grounding", "content": "Plurals is grounded in deliberation literature, sociotechnical systems that broaden technological perspectives, and multi-agent systems for Al alignment. The result is an end-to-end generator of simulated social ensembles-groups that engage in deliberation. We integrate deliberative theory into our system by incorporating templates of first- and second-generation deliberative ideals and using deliberative theory to inform the structure of AI deliberation. We extend previous work on broadening technological perspectives, such as Argyle et al.'s dataset-based personas [4], Gordon et al.'s \"juries\" [33], and Zhang et al.'s PolicyKit [97]. Our system encompasses individual, group, and governance-level simulations, unlike previous approaches that focused on flexibility at only one of these three levels. By drawing on the concept of deliberative \"mini-publics\" (groups who engage in deliberation [80]), we evolve from aggregative methods (like juries) to a more deliberative approach. Additionally, we contribute to multi-agent Al research by offering a flexible system for creating diverse interaction structures and providing reusable code and shared infrastructure for experiments."}, {"title": "2.1 Deliberation", "content": "Deliberation is defined as \"mutual communication that involves weighing and reflecting on preferences, values, and interests regarding matters of common concern\" [14]. As B\u00e4chtiger et al. distinguish [14], deliberative democracy differs from aggregative democracy. The former centers talking and the latter centers voting-though they can co-occur (e.g., talking before voting [26, 41]). Deliberation can occur in many different forms, in many different ways, and have many different outcome measures. In what follows, we clarify the aspects of deliberation literature that inform our system.\nPractice of Deliberation. The abstractions of Plurals map to the practice of deliberation. Ryfe breaks deliberative practice into three phases [71]: (1) The organization of the encounter, (2) the deliberation within the encounter, and (3) the final product of deliberation. Agents are the building blocks of deliberation. As such, Agent initialization corresponds to Phase 1. The deliberation within the encounter is governed by Structures and combination instructions, corresponding to Phase 2. Finally, Moderators can amend the final product of deliberation, corresponding to Phase 3. Separate from Ryfe, Morrell [59] considers three factors of deliberation that affect outcomes: individual dispositions, institutional structures, and facilitators/moderators. Again, these correspond almost directly to our abstractions of Agents, Structures, and Moderators.\nMore generally, formal deliberation nowadays often occurs in \"mini-publics\" [80]. These are groups of citizens who come together to deliberate, often in an advisory role. Plurals is an end-to-end generator of simulated social ensembles. This is analogous to reproducing the process of mini-public deliberation.\nDeliberative Ideals. While the ideals of deliberation are not universally agreed upon, we adopt the dichotomy of \"first-generation\" and \"second-generation\" ideals articulated by B\u00e4chtiger et al. [14]. According to B\u00e4chtiger et al., the first generation of deliberative theorists (e.g., Habermas [35]) emphasized rationality, achieving a universal consensus, and reason-giving. The second generation of deliberative theorists took a more expansive view of deliberation, beyond rationality and universalism [14]. For example, second-wave deliberation also valued more emotional forms of communication [61], lived experience, testimony, and storytelling [14]. Furthermore, to second-wave theorists, the goal was not necessarily a universal consensus (since legitimate disagreement may still exist after perfect deliberation [55]), but rather a clarifying of understanding [14, 27].\nWe incorporate these ideals into our system as both persona templates (how LLMs should enact personas) and combination instructions (how LLMs should combine information with others). To do this, we started with the taxonomy of first-generation and second-generation principles from [14]. Two authors then engaged in an iterative, two-step process where we first decided whether each dimension was relevant to Al agents, and then how to operationalize this dimension for both generations of deliberation thought. Appendix Table 2 lists how we operationalized each ideal. Some, but not all, ideals or benefits of human deliberation may apply to AI deliberation. Deliberative mini-publics can be useful for the outcomes that they produce [14, 80, 92] or the process that produces these outcomes. Regarding the latter, deliberation proponents argue deliberation has certain epistemic (outcome-independent) benefits-such as increased perceived legitimacy for decisions when the sequence of thought leading to them is made public [23]. It is the former-outcome-oriented benefits-that is relevant to AI deliberation.\nTo be clear, our system is inspired by human deliberation; it is not meant to substitute for it. An analogy we offer is how architects and engineers often draw on the natural world to create artifacts. For example, Velcro was inspired by burrs sticking to the inventor's dog [52]. Likewise, multi-agent LLM systems may draw inspiration from the social world-but we do not view LLM deliberation as a substitute for human deliberation. The limits of this metaphor are discussed in section 7."}, {"title": "2.2 Pluralistic Sociotechnical Systems", "content": "Other projects have sought to broaden the representation of technology, what we term \"pluralistic sociotechnical systems\" for short-hand. These approaches usually focus exclusively on individuals [4], groups [33, 50], or governance structures [97]. As an end-to-end generator of simulated social ensembles, Plurals does all three."}, {"title": "2.3 Multi-Agent Systems for AI Alignment", "content": "Multi-agent systems have a long history in artificial intelligence [62, 89]. Now there is substantial interest in multi-agent LLM systems [16, 38, 39, 45, 54, 64, 84, 87]. Our system incorporates aspects of these systems such as debate [40] and the idea of role-based communication [38, 54, 64, 100].\nLike our system, several multi-agent systems are explicitly designed with the goal of alignment [38, 40, 54, 64]. Broadly, these systems typically center interactions between agents or agent roles. For example, several projects have explored the role of AI alignment through debate [40, 45]). Khan et al. [45] propose a system where a weaker model picks an answer after seeing stronger models debate, and this process of debate leads to more truthful answers for the weaker model. Other multi-agent systems center agent roles [38, 54, 64, 88, 100]-the idea being that agents playing distinct parts can aid human decision-makers [88].\nTo this body of research, we offer several contributions. More theoretically, our abstractions are specifically grounded in the theory and practice of deliberation. More practically, because our system has support for Agents, Structures, and Moderators, it effectively enables users to customize both information-sharing (as in AI debate literature) and Agent roles (as in the Al role literature). We extend the debate paradigm by allowing for arbitrary information structures. A back-and-forth debate is of course just one of many possible informational structures. Our system contributes to the role-based literature by integrating with ANES, enabling users to quickly draw up nationally representative roles. We also design around deliberation-the space in between roles and information-sharing. For example, users can ablate the role of an Agent (i.e: their system instructions) and the combination instructions of an Agent. Finally, Plurals is a fully functioning Python package and not a one-off study. Hence, Plurals can operate as shared infrastructure. It makes multi-agent systems faster to set up and more accessible for researchers."}, {"title": "3 System Principles", "content": ""}, {"title": "3.1 Interactional Pluralism", "content": "Plurals uses metaphors from human deliberation to make existing artificial intelligence systems more pluralistic. Thus, a core principle is pluralism through deliberation, or what we term \u201cinteractional pluralism\".\nSorensen et al.'s typology of pluralistic Al systems is a useful starting point [81]. They distinguish between models that (1) present a spectrum of reasonable responses, (2) can be steered to reflect certain perspectives, and (3) are well-calibrated to a given population. The ability to craft custom personas aligns with the second type and our use of government datasets like ANES to generate nationally representative personas aligns with the third type.\nPlurals extends this typology by allowing users to define the rules of engagement between agents: Structures shape the dynamics of information sharing and aggregation; Combination instructions"}, {"title": "3.2 Modularity", "content": "The system is modular. The same Agent can be deployed in different Structures and Agents can also be used outside of Structures, increasing the system's versatility. Hence, the separation of Agents and Structures allows researchers to ablate these abstractions, facilitating more precise experiments and analyses.\nApart from the practical utility, this separation between Agents and Structures aligns with well-established social science frameworks. This conceptualization is most explicitly articulated in Structuration Theory by Anthony Giddens [32], which explores the interplay between \"agents\" and larger \"structures\" agents exist in. Giddens aimed to transcend theories of behavior that centered exclusively on either individuals or societal structures. Similar distinctions appear across disciplines: individuals and environments in development psychology [67], person and situation in social psychology [29], individual and field in sociology [85], and agent and enviornment in artificial intelligence [70]. By using Agents and Structures as core abstractions\u00b2, we create a modularity that resonates with different disciplines."}, {"title": "3.3 Grounded in Deliberation Practice", "content": "As described in subsection 2.1, our abstractions (Agents, Structures, Moderators) map to the practice of deliberation. Ryfe [71] breaks deliberation into (1) The organization of the encounter, (2) the deliberation within the encounter, and (3) the final product. These map onto Agent initialization (Phase 1), Structures and combination instructions (Phase 2), and moderation (Phase 3). By mirroring the components of deliberation, we ground our system in it. Of course, the utility of these abstractions in simulated agent space is less clear than with humans. However, incorporating these foundations can help build realistic simulations and test whether strategies developed in the literature can be used to improve LLM outputs.\nThe addition of Moderators provides practical benefits. Just as in human deliberation, it is helpful to have some summary of what transpired. In many multi-agent systems, one Agent aggregates the communications of others [16, 39, 87, 100]. The motivation for adding auto-moderators-a feature where Moderators come up with their own moderation instructions based on the task-is based on the paradigm of \"auto-prompting\" in DSPy [45]."}, {"title": "3.4 Balancing Autonomy and Usability", "content": "Our system offers users substantial autonomy. First, we ensured that Agents can be used outside of Structures so users are not wedded to Structures. Second, both Agents and Structures are highly customizable. Agents can (as some examples) be over 100 LLMs, integrate with ANES, contain a different task than other Agents in a Structure, have custom combination instructions, different model parameters, etc. Likewise, Structures span a range of use cases and information-sharing protocols (e.g.: debates, ensembles, graphs) and have tuneable parameters. Advanced users can create their own Structures.\nBut we tried to balance this autonomy with usability. At a high level, we tried to build abstractions that are intuitive to use. Figure 2 shows code snippets. As discussed in subsection 3.2 and subsection 3.3, our core abstractions (Agents, Structures, Moderators) correspond to both common terminology and the basic process of deliberation. At a documentation level, our repository provides extensive examples of how to use each component. At an instantiation level, we made the decision for most of the package to be usable with very few custom arguments, leveraging defaults and templates. The drawback of defaults is that \"artifacts have politics\" [94], and so this imposes certain principles on users. For example, many of the templates (apart from debate) are deliberative rather than agonistic-emphasizing building on outputs rather than arguing. By extracting our default templates to a single human-readable file on GitHub, we make these defaults more legible to users-providing some informational autonomy."}, {"title": "4 System Details and Implementation", "content": "See Figure 1 for a full system diagram and Figure 2 for specific examples. At a high level, Plurals consists of three core abstractions. Agents complete tasks within Structures, which define how information is shared between Agents. Multi-agent communication can be summarized by Moderators. We now describe these abstractions in more detail."}, {"title": "4.1 Agents", "content": ""}, {"title": "4.1.1 Component Description.", "content": "Agents are large language models who complete tasks. We consider an Agent to have the following properties:\n\u2022 Profile: System instructions describe the Agent's \"profile\" at a high level. These system instructions can be left blank (for default model behavior), set manually, or constructed via various persona-based methods described below. See Figure 2 for examples. We provide different persona templates as part of the package.\n\u2022 Task: This is the user prompt Agents are responding to. Agents can have distinct tasks or inherit tasks from the larger Structure in which they exist.\n\u2022 Combination Instructions: Combination instructions define how Agents combine information from other Agents to complete the task. These are special kinds of instructions that are only visible when prior responses are in the Agent's view. Users can rely on templates or create their own. We provide, and empirically test, templates inspired by deliberative democracy-spanning first-wave (reason-giving) and"}, {"title": "4.1.2 Implementation.", "content": "System instructions can be instantiated directly by the user or by using our persona-based methods. When using persona-based methods, the full system instructions are a combination of a specific persona and a persona template which gives more instructions on how to enact that persona. See Figure 2a for an example. In that example, there is a specific persona from ANES \"You are a...\" and then a template from second-wave deliberation that formats the persona. (Users can make their own persona templates, too-it is a string with a ${persona} placeholder.) The logic for bracketing out a specific persona from a persona template is to facilitate the ablation of an Agent's identity versus additional instructions for how to apply that identity.\nSpecific personas can be inputted by the user (e.g: \"A graphic designer\") or drawn from American National Election Studies (ANES)4, as in Argyle et al. [4]. When using ANES, our system finds a real individual satisfying some criteria and then creates a persona based on the totality of this individual's attributes. Sampling is always probability-weighted, so the probability of a citizen being simulated matches their national sample probability weight. Because ANES is nationally representative, the marginal distribution of Plurals-generated personas matches that of the general population. Code snippet Figure 2d (top panel), shows initializing Agents based on specific criteria (e.g: California resident below the age of 40) using the query_str method, which searches ANES through a Pandas string, For convenience, we also support an ideology method (ideology='liberal') and initializing randomly selected ANES citizens (persona='random', Figure 2a). The latter can be used to quickly draw up nationally-representative \"citizen assemblies\" (Figure 2b).\nANES is just one possible generator of data-driven personas, and in future iterations, we aim to provide additional persona-generation methods. We chose ANES as our initial dataset for the following reasons. First, it has been used in prior work-most notably, Argyle et al. [4]. Second, ANES has data on political ideologies,"}, {"title": "4.2 Structures", "content": ""}, {"title": "4.2.1 Component Description.", "content": "Structures (Figure 3) govern how information is shared between Agents completing a task. Structures differ in the following attributes:\n\u2022 Amount of information shared: Chains, Debates, and DAGs have a parameter called last_n that controls how many prior responses each Agent can see. For DAGs, the density of the network can be thought of as an amount of information sharing as well. Ensembles are a basic structure where no information is shared; Agents process tasks in isolation.\n\u2022 Directionality of information shared: A \"Chain\" of Agents is a linear chain of the form Agent1->Agent2->... where the direction of sharing only goes one way. A debate involves two agents (Agent1Agent2) sharing information for a given number of cycles. In DAGs, Agents may have both predecessors and successors.\n\u2022 Randomness: Chains support a shuffle parameter that if True will rewire the order of Agents on each cycle. This affords a degree of randomness in information-sharing.\n\u2022 Repetition: Chains, Debates, and Ensembles support a cycle parameter which will repeat the process."}, {"title": "4.2.2 Implementation.", "content": "Existing structures we have include Chains, Graphs, Debates, and Ensembles. In an \"Ensemble\" no information is shared and Agents process requests in parallel. A \u201cChain\u201d is a highly flexible Structure where agents build upon each other's answers with deliberation optionally rewired on each cycle (Figure 2d, bottom panel). There, three Agents will build on each other's output for three cycles. The initial order is agent1->agent2->-agent3 but because shuffle=True, the order will change each cycle. Debates involve a back-and-forth between two agents (Figure 2d, top panel).\nThe Graph structure enables users to create directed acyclic graphs (DAGs) of Agents, processing tasks via Kahn's algorithm for topological ordering. DAGs allow \"upweighting\u201d certain voices by increasing their connectedness. In Figure 2c, Agents critique and revise a company memo using the combination_instructions = 'critique_revise' template. A woman ANES Agent's output is fed forward to all of the other Agents (so they see that Agent's responses when answering). Then a Moderator summarizes all responses.\nThe possibility space of potential structures is vast. Our existing structures provide a lot of customizability. But some users will want a structure that has a different behavior than what can be accomplished via existing structures. Consequently, we built the package so that advanced users can easily create their own custom structures, leveraging the polymorphic design of the structure classes (more details in Appendix B)."}, {"title": "4.3 Moderators", "content": ""}, {"title": "4.3.1 Component Description.", "content": "Moderators are a subclass of Agents who summarize multi-agent deliberation. Any Structure supports an optional Moderator. Moderators are defined by:\n\u2022 Profile: Like Agents, Moderators have a distinct \u201cprofile\" which we operationalize as system instructions. System instructions can be set directly or via persona methods. We have a special class of Moderators called \"Auto-Moderators\" who generate their own system instructions based on a task.\n\u2022 Combination Instructions: Here, combination instructions define how Moderators aggregate the responses that they see.\n\u2022 Task: Moderators can have a distinct task from Agents, or inherit the task from the Structure they are moderating.\n\u2022 Model: Moderators are initialized to be a particular LLM."}, {"title": "4.3.2 Implementation.", "content": "Moderators can be useful when users want an Agent who will not participate in deliberation but merely summarize it. For example, users may want to have a chain or ensemble of liberals with an independent Moderator summarizing responses at the end. As with other components, we offer pre-defined templates for Moderators. We support various pre-defined moderator instructions such as \"information aggregators\u201d or \u201csynthesizers\u201d. Inspired by auto-prompting libraries such as DSPy [46], we also support Auto-Moderators. Given a task, an Auto-Moderator will ask itself what the system instructions of a Moderator should be for the task it was assigned. Auto-Moderators are initialized through system_instructions='auto' (bottom panel of Figure 2d)."}, {"title": "5 Case Studies", "content": "We provide several preliminary empirical results (Table 1). Case Studies 1 and 2 are meant as mechanistic fidelity checks. We show that the system does what we are claiming it does. Case Studies 3-5 are efficacy tests. We show that our system outperforms a standard zero-shot (and zero-shot chain-of-thought) LLM approach. Case Study 6 is a preliminary analysis of how this system can be used for ethical guardrails. All human subject experiments received prior IRB approval from our university and met power requirements.\nIn Case Study 1, we show that using intersectional ANES personas (i.e: combining ideology with demographic variables) results in more response diversity than prompting with only-ideology personas (\"You are a liberal\"), suggesting this multi-attribute persona method can reduce homogenization. In Case Study 2, we show that Agents are correctly applying a subset of combination instructions from our deliberation templates-suggesting that combination instructions can steer LLM deliberation.\nIn Case Studies 3-5, we used zero-shot and Plurals simulated social ensembles to create output aimed at resonating with specific audiences. Plurals output was chosen as more compelling by the relevant Prolific audience for both conservatives (Study 3) and liberals (Study 4, Study 5). See Appendix Table 3 for multilevel models and a pooled analysis across studies. In Case Study 6, we discuss how Plurals can facilitate custom ethical guardrails with a preliminary case study. We discuss the ethical implications of studies 3-6 in more detail in section 7."}, {"title": "5.1 Mechanistic Fidelity: Adding demographics to ideology personas diversifies responses.", "content": "Summary. We discussed how intersectional personas from government datasets should lead to less homogenizing output than single-attribute personas. Responses for a set of prompts corresponding to different liberals (\"You are a liberal and X = x and Y = y...\") should logically have more diversity than applying the same single-ideology prompt (\"You are a liberal.\"). Here we show this empirically. Our ANES persona method for political ideologies generates more diverse responses than prompting an LLM with only ideology instructions in 100% of Claude Sonnet comparisons and 95% of GPT-40 comparisons. This is almost true by definition, so methodology and analysis are in Appendix C."}, {"title": "5.2 Mechanistic Fidelity: LLM deliberation instructions yield faithful deliberation protocols.", "content": "Summary. We evaluated Agents' adherence to combination instructions by creating two-turn debates on ballot initiatives under rational and emotional conditions. These correspond to first- and second-generation differences in the \"Reasons\u201d dimension (Appendix Table 2). Crowdworkers guessed which instructions yielded which output, with an annotation accuracy of 89%.\nGeneration. We first collected 2024 ballot initiatives from the website Ballotpedia. We then randomly sampled 30 of the 137 ballot measures for which we could scrape both a short description and a more detailed explanation to turn into a prompt (Appendix D). We then generated two-cycle debates for each ballot initiative under rational and emotional conditions, differing only in one line of combination instructions. We used the final response from each debate for annotation, with agents randomly assigned to be GPT-40, GPT-4 Turbo, or Claude Sonnet. See Appendix D for full combination instructions.\nHuman Evaluation. We recruited 20 participants from Prolific who completed more 100 tasks and had a 98%+ approval rating. Participants were paid $2, based on an anticipated study duration of 7 minutes ($17/hr). After providing informed consent, each participant viewed 10 pairs of responses (rational, emotional) for different ballot measures. We randomly assigned participants to identify either the rational or emotional condition across their 10 trials. We randomized both the order of condition presentation within each pair and the sequence of ballot measures. See Appendix D for task wording.\nMeasures. We calculated annotation accuracy by condition, defining an accurate response as one where the participant's judgment matched the generation condition.\nResults. Overall accuracy was 0.89, (95% CI = [0.84, 0.93]). Accuracy for the rational condition was 0.93, (95% CI = [0.88, 0.98]),"}, {"title": "5.3 Efficacy: Conservatives preferred solar panel ideas from a simulated focus group of conservatives over zero-shot.", "content": "Summary. Combining ANES personas, ensembles, and Moderators, we tested whether a \"simulated focus group\" yields ideas resonant with the relevant Prolific audience. Specifically, the aim of this study was to generate descriptions of solar panel companies that conservatives would buy from. We generated descriptions under two conditions-zero-shot, or a simulated focus group of ANES conservatives. In the latter, we queried 10 simulated ANES conservatives on what they would want in a solar panel company, and then a Moderator proposed an idea based on this simulated feedback. We then had conservatives on Prolific evaluate solar panel company ideas and found those generated from the simulated focus group were preferred over zero-shot ideas in 88% of cases. This experiment used GPT-40. See Appendix F for code.\nGeneration. In the zero-shot condition, we set the system instructions of GPT-40 to \"You are an expert copywriter for an ad agency\" and the user prompt was \"Come up with a specific product for a solar panel company that would resonate with conservatives. Be very specific. Answer in 50 words only.\" In the Plurals condition, the Moderator had the same instructions. However, that Moderator oversaw an ensemble of 10 simulated ANES conservatives (initialized using our ideology persona method and anes persona template) who were asked what features they personally would want in a solar panel company. The Moderator then came up with a 50-word solar panel idea after exposure to these simulated discussions. For 15 trials, we generated a solar panel company idea with zero-shot and Plurals.\nIntuition for Efficacy. In earlier pilots, we found that simply prompting LLMs to generate ideas for a solar panel company for conservatives resulted in outputs that were highly ideological (e.g., emphasizing being founded by a veteran). This was despite instructions like \"be very specific\" that we maintained for this study. However, when LLMs simulated specific conservatives who were asked what product details they would want in a solar panel company, few of the product details were ideological. Hence, our intuition was that this focus group would surface concerns relevant to actual conservatives (e.g: rural weather) as a function of the non-ideological aspects of the conservative ANES personas. More generally, personalization (incorporating details about a user into messaging) increases the persuasiveness of LLM generations [79]. Querying simulated personas can be thought of as a synthetic kind of \"personalization\".\nHuman Experiment. We recruited 20 conservative participants from Prolific using Prolific's screening tools. We applied additional filters to ensure participants lived in the United States, were above 18, and had a 98% approval rating. Participants were paid $1.50 for an expected duration of 6 minutes ($15/hr). After providing"}, {"title": "5.4 Efficacy: Liberals preferred charter school ideas from a simulated focus group of liberals over zero-shot.", "content": "Summary. We conducted a follow-up experiment to the solar panel experiment. Here, the goal was to generate descriptions of charter schools that liberal parents would send a child to. Using a similar setup-and evaluations from liberals with children-we found those descriptions generated from the simulated focus group were preferred over zero-shot chain of thought (CoT) ideas in 69% of cases. This experiment used Claude Sonnet. See Appendix G for materials and code.\nGeneration. In the zero-shot condition, we generated a charter school idea using a CoT prompt. In the Plurals (DAG) condition, we also started with a CoT idea. But then this initial idea was fed to three simulated liberal parents, who offered separate critiques of the idea. Then a default Agent executed a variant of the initial CoT prompt, taking into account critiques of the initial idea. We generated 15 pairs of zero-shot ideas and DAG ideas. See Appendix G for code. This experiment differed from the previous experiment in two ways. We used a CoT prompt for the zero-shot generation since this may be a more difficult baseline. We also employed a \"critique and revise\" setup similar to the idea behind constitutional AI (CAI) [8].\nHuman Experiment. We recruited 20 liberal parents from Prolific, using Prolific's screening tool. We applied additional filters to ensure participants lived in the United States, were above 18, and had a 98% approval rating. Participants earned $1.75 for an expected duration of 7 minutes ($15/hr). After providing informed consent, participants answered a commitment check [66]. We then presented a brief passage on charter schools adapted from Wikipedia [1], followed by a comprehension check (Appendix G) of this passage. For 15 trials, participants chose between pairs of charter school ideas generated under zero-shot and simulated focus group conditions, answering, \"Supposing you were sending a child to a charter school, which would you choose?\u201d We randomized the presentation order"}, {"title": "5.5 Efficacy: Liberals preferred homeless shelter ideas from a simulated focus group of liberals over zero-shot", "content": "Summary. We conducted a third efficacy experiment that was motivated by \"NIMBYism\" (Not in My Backyard)-the phenomena of citizens supporting policies in the abstract but not in their specific neighborhoods [19, 53, 96]. Here, the goal was to generate proposals for homeless shelters-which are a frequent target of NIMBYism [53, 96]-that liberals would find compelling. Using a similar setup to the previous experiment, we find that ideas generated from the simulated focus group were preferred over zero-shot ideas in 66% of trials. This experiment used Claude Sonnet. See Appendix H for code.\nGeneration. In the default condition, we used a zero-shot chain of thought (CoT) prompt. In the Plurals condition, we created a DAG with the following structure: A zero-shot CoT model proposed a homeless shelter idea description. Then, three simulated liberals (using ANES personas) were instructed to state how the proposal could be made more compelling to them, in particular. A third Agent then integrated these critiques to come up with a final idea.\nHuman Experiment. We recruited 20 liberals from Prolific who lived in the United States, were above 18, and had a 98% approval rating. Participants were paid $1.75 for an expected duration of 7 minutes ($15/hr). After providing informed consent, participants answered a commitment check [66] and then engaged in 10 trials. In each trial, participants were shown pairs of homeless shelter proposals generated under both zero-shot and the simulated focus group and were asked, \"Consider two proposals for a homeless shelter in your neighborhood. Which of these proposals would be more compelling to you?\u201d We randomized the presentation order of condition responses and sequence of idea pairs.\nMeasures. We conducted exact two-tailed binomial tests on whether the proportion of times the simulated focus group option was chosen differed from chance.\nResults. Plurals output was chosen over zero-shot in 66% of cases, (95% CI = [60%, 73%]), binomial p < 0.001, Figure 4."}, {"title": "5.6 Moderation: Using Plurals for LLM Guardrails", "content": "Summary. Case studies 3-5 demonstrate Plurals' ability to create output that resonates with audiences more than zero-shot approaches. However, depending on the use, this capability raises ethical concerns-which we discuss more extensively in section 7. Here, we present a case study on steerable Moderators as an initial exploration of how Plurals abstractions can create ethical guardrails. We show that Moderators can be steered"}]}