{"title": "On the use of neurosymbolic AI for defending against cyber attacks", "authors": ["Gudmund Grov", "Jonas Halvorsen", "Magnus Wiik Eckhoff", "Bj\u00f8rn Jervell Hansen", "Martin Eian", "Vasileios Mavroeidis"], "abstract": "It is generally accepted that all cyber attacks cannot be pre- vented, creating a need for the ability to detect and respond to cyber attacks. Both connectionist and symbolic AI are currently being used to support such detection and response. In this paper, we make the case for combining them using neurosymbolic AI. We identify a set of chal- lenges when using AI today and propose a set of neurosymbolic use cases we believe are both interesting research directions for the neurosymbolic AI community and can have an impact on the cyber security field. We demonstrate feasibility through two proof-of-concept experiments.", "sections": [{"title": "Introduction", "content": "Protecting assets in the cyber domain requires a combination of preventive measures, such as access control and firewalls, and the ability to defend against cyber operations when the preventive measures were not sufficient. \nOur focus in this paper is on defending against offensive cyber operations, and before going into details, some concepts and terminology need to be in place:\nTerminology & concepts The focus of this paper is to defend assets against threats in cyberspace. An asset can be anything from information or physical in- frastructure to the internal processes of an enterprise. Threats manifest themselves in the form of cyber operations (or cyber attacks) conducted by an adversary (or threat actor). In our context of defending, the term incident is used for a potential attack that is deemed to have an impact on assets, and the process of defending comes under the area of incident management [19]. This is typically conducted in a security operations centre (SOC), which consists of people, processes and tools. One of the objectives of a SOC is to detect and respond to threats and at- tacks, where security analysts play a key role. Knowledge/intelligence of threats and threat actors in the cyber domain is called cyber threat intelligence (CTI)."}, {"title": "Challenges faced when using AI in a SOC", "content": "MAPE-K (Monitor-Analyse-Plan-Execute over a shared Knowledge) [51] is a common reference model to structure the different phases when managing an incident. For each phase of MAPE-K, we below discuss the common use of AI, including underlying representations, and identify key challenges security practitioners face when using A\u0399. \nMonitor In the monitor phase, systems and networks are monitored and the telemetry is represented as sequences of events. An event could, for instance, be a network packet, a file update, a user that logs on to a service, or a process being executed. Events are typically structured as key-value pairs. For a large enterprise, there may be tens of thousands of events generated per second. In this phase, a key objective is to detect suspicious behaviours from the events and generate alerts, which are analysed and handled in the later phases of MAKE-K.\nThis is a topic where machine learning (ML) has been extensively studied by train- ing ML models on the vast amount of captured event data (see e.g. [5]). A challenge with such data is the lack of ground truth, in the sense that for the vast majority of events we do not know if they are benign or malicious. As most events will be benign (albeit we do not know which ones), one can exploit this assumption and use unsuper- vised methods to train anomaly detectors. This is a common approach. For at least research purposes, synthetic datasets from simulated attacks are also commonly used [53]. However, synthetic datasets suffer from several issues [50,9] and promising results in research papers using synthetic data tend not to be recreated in real-world settings whilst anomaly detectors often create a high number of false alerts [16,89]. Our first challenge, which follows from the European Union Agency for Cybersecurity (ENISA) [30], addresses this performance issue for ML models for real-world conditions:\nChallenge 1 Achieve optimal accuracy of ML models under real-world conditions.\nAs both normal software and malware are continuously updated, the notion of concept drift is prevalent, and ML models must thus be retrained regularly. Moreover, in addition to the large amount of data, requiring scalability, real-world conditions will have a large amount of noise (i.e. aleatoric uncertainty) in the data, which is not well reflected in synthetic data.\nFor previous incidents that have been handled, we know the ground truth of the associated alerts and events. Compared with the full set of events, this dataset will, however, be tiny. Still, it is important as it is labelled and contains data we know are relevant either in terms of actual attacks experienced or false alerts that should be filtered out. One important challenge, also identified by ENISA [30], is the ability to exploit such labelled \"incident datasets\" and train ML models based on them:\nChallenge 2 Learning with small (labelled) datasets (from cyber incidents).\nNew knowledge, e.g. about certain threats, attacks, malware or vulnerability ex- ploits, is frequently published (in e.g. threat reports). The traditional, and still most common, method of threat detection is so-called signature-based detection, where such knowledge is (often manually) encoded as specific patterns (called signatures). Detec- tion is achieved by matching events with these signatures, and generating alerts when they match. While signature-based methods have their limitations, such knowledge could improve the performance of ML-based detection models trained on event data, requiring the ability to extract relevant knowledge and including it in the ML models:\nChallenge 3 Extract knowledge (including about threats, malware and vulnerabilities) and enrich ML-based detection models with it.\nIn addition to reports, there are many knowledge bases and formal ontologies that can be used to enrich ML models with such knowledge. There are also attempts to ex- tend the coverage domain for such ontologies: one example is the unified cybersecurity"}, {"title": "The case for neurosymbolic AI", "content": "Kahneman's [48] distinction between (fast) instinctive and unconscious 'system 1' pro- cesses from (slow) more reasoned 'system 2' processes, has often been used to illustrate the NeSy integration of neural networks (system 1) and logical reasoning (system 2). Building on this analogy, system 1 can, in a SOC, be seen as the ML-based AI used to identify potentially malicious behaviour in the monitor phase. Here, a large amount of noise needs to be filtered out from the large amount of events (thus a need for speed and scalability). System 2 is the reasoning conducted in the analysis phase, which requires deeper insight with the need for scalability less significant.\nThis dichotomy of requirements entails that neither end-to-end pure statistical nor pure logical approaches will be sufficient, and a NeSy combination seems ideal. Three commonly used reasons for pursuing NeSy are to design systems that are human auditable and augmentable, can learn with less and provide out-of-distribution general- isation [37]. We have seen examples of each of these in the challenges described in \u00a72: the use of knowledge to both contextualise, analyse and explain alerts, and to gener- ate and explain and response actions; to learn from (relatively few) incidents; and to handle concept drifts and noise in order to achieve high accuracy of ML models under real-world conditions. From the challenges in \u00a72, we will here outline a set of NeSy use cases we believe are promising and identify some promising NeSy tools and techniques for each of them. This work is not complete and should be seen as a start (see \u00a75). Moreover, this section is speculative by nature, but we provide some evidence in terms of existing work and experiments conducted in \u00a74.\nMonitor The ability to integrate relevant knowledge into ML-based detection models (challenge 3) falls directly under the NeSy paradigm, and could both improve perfor- mance under real-world conditions (challenge 1) and help to reduce the number of false alerts (challenge 5):\nUse case 1 Use (symbolic) knowledge of threats and assets to guide or constrain ML- based detection engines.\nA similar case for such a NeSy use case is made in [79]. Logical Neural Networks (LNN) [85] are designed to simultaneously provide key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning), enabling both logic inference"}, {"title": "Proof-of-concept experiments", "content": "To showcase the feasibility of using NeSy for our use cases, we have conducted two initial experiments: 1) using LTN to show how knowledge in symbolic form can be used to improve an ML-based detection engine (use case 1); and 2) using LLMs and ASP to elicit and reason with adversary attack patterns and observed alerts for sit- uational awareness (use case 6). Both experiments are deliberately simplified and are not conducted in realistic conditions. They do, however, demonstrate the potential of NeSy in a SOC setting. Additional technical details can be found in appendix A and all source code can be found on GitHub: https://github.com/FFI-no/Paper-NeSy24.\nExperiment 1: LTN for knowledge-aware intrusion detection The first experiment addresses use case 1 and is in the monitor phase of MAPE-K. Here, the goal is to detect malicious traffic by training a classifier that can separate benign traffic from two different classes of malicious traffic: brute force attacks and cross-site scripting (XSS) attacks. The classifier will produce an alert if malicious traffic is seen. The input data are NetFlow entries [20], which provide aggregated information on traffic between two distinct ports on distinct IP addresses for a given protocol. The data is a subset of the CICIDS2017 dataset [86], which is a labelled dataset containing simulated attacks on a network, with additional details in appendix A.\nThe experiment consists of two parts: In the first part, a simple 3-layered fully connected neural network is trained and used as a baseline. In the second part, this neural network is extended using an LTN [12], where additional domain knowledge is encoded. In both cases, 70% of the data is used for training and 30% for testing. The experiment is inspired by [74], where LTN is used in a similar, but more limited, way.\nThe LTN consists of one predicate for class membership, P(x, class), and is config- ured as a neural network with the same structure as the baseline neural network. We define the following axioms (expressed in Real Logic [12]):\n\u2200x \u2208 B : P(x, Benign) \u2200x \u2208 BF : P(x, Brute-force) \u2200x \u2208 X : P(x, XSS)\nThese axioms describe how all flows in the training set labelled as a given class are a member of the class. This encodes the information of the baseline neural network with no additional knowledge. From the dataset network topology, we define NWS to be the set of all NetFlows that do not communicate with a web server. We know that if a NetFlow is in this NWS-set, then it cannot be a web attack (that we are interested in). We add this domain knowledge as a fourth axiom used by the LTN:\n\u2200x \u2208 NWS:\u00ac(P(x, Brute-force) V P(x, XSS))\nTraining consists of updating the neural network P to maximize the accumulated truth value of the axioms [12]. The following table shows the results on the test data:"}, {"title": "CTI transformed to LTL", "content": "This subsection explains how CTI reports of previous attacks are transformed into LTL temporal representations of the attack patterns. The CTI reports are in natural language and we utilize the NL2LTL [32] Python package for the translation. We define a custom pattern template ExistenceEventuallyOther to express the LTL property a\u028cb. Additionally, we create a custom prompt tailored for our domain. The prompt contains the allowed pattern (ExistenceEventuallyOther), allowed symbols (MITRE ATT&CK technique IDs), and multiple examples. The LTL representation is a chain of MITRE ATT&CK techniques. However, CTI descriptions of an attack might not reference any specific techniques. In those cases, we let the LLM deduce which MITRE ATT&CK technique is referenced from the general description in natural language. This is a minimal implementation, and extraction of MITRE ATT&CK tactics from CTI reports have been investigated before [75,98].\nTranslate natural language sentences into patterns:\nALLOWED PATTERNS: Existence Eventually Other\nALLOWED SYMBOLS: T1548 (Abuse Elevation Control Mechanism), T1530 (Data From Cloud Storage), [...]\nNL: The adversary logs into the Kubernetes console.\nThis leads to: The adversary can view plaintext AWS keys in the Kubernetes console.\nPATTERN: Existence Eventually Other\nSYMBOLS: T1133, T1552"}]}