{"title": "On the use of neurosymbolic AI for defending against cyber attacks", "authors": ["Gudmund Grov", "Jonas Halvorsen", "Magnus Wiik Eckhoff", "Bj\u00f8rn Jervell Hansen", "Martin Eian", "Vasileios Mavroeidis"], "abstract": "It is generally accepted that all cyber attacks cannot be prevented, creating a need for the ability to detect and respond to cyber attacks. Both connectionist and symbolic AI are currently being used to support such detection and response. In this paper, we make the case for combining them using neurosymbolic AI. We identify a set of challenges when using AI today and propose a set of neurosymbolic use cases we believe are both interesting research directions for the neurosymbolic AI community and can have an impact on the cyber security field. We demonstrate feasibility through two proof-of-concept experiments.", "sections": [{"title": "1 Introduction", "content": "Protecting assets in the cyber domain requires a combination of preventive measures, such as access control and firewalls, and the ability to defend against cyber operations when the preventive measures were not sufficient.\nOur focus in this paper is on defending against offensive cyber operations, and before going into details, some concepts and terminology need to be in place:\nTerminology & concepts The focus of this paper is to defend assets against threats in cyberspace. An asset can be anything from information or physical infrastructure to the internal processes of an enterprise. Threats manifest themselves in the form of cyber operations (or cyber attacks) conducted by an adversary (or threat actor). In our context of defending, the term incident is used for a potential attack that is deemed to have an impact on assets, and the process of defending comes under the area of incident management [19]. This is typically conducted in a security operations centre (SOC), which consists of people, processes and tools. One of the objectives of a SOC is to detect and respond to threats and attacks, where security analysts play a key role. Knowledge/intelligence of threats and threat actors in the cyber domain is called cyber threat intelligence (CTI)."}, {"title": "2 Challenges faced when using AI in a SOC", "content": "MAPE-K (Monitor-Analyse-Plan-Execute over a shared Knowledge) [51] is a common reference model to structure the different phases when managing an incident. For each phase of MAPE-K, we below discuss the common use of AI, including underlying representations, and identify key challenges security practitioners face when using A\u0399."}, {"title": "Challenge 1 Achieve optimal accuracy of ML models under real-world conditions.", "content": "As both normal software and malware are continuously updated, the notion of concept drift is prevalent, and ML models must thus be retrained regularly. Moreover, in addition to the large amount of data, requiring scalability, real-world conditions will have a large amount of noise (i.e. aleatoric uncertainty) in the data, which is not well reflected in synthetic data.\nFor previous incidents that have been handled, we know the ground truth of the associated alerts and events. Compared with the full set of events, this dataset will, however, be tiny. Still, it is important as it is labelled and contains data we know are relevant either in terms of actual attacks experienced or false alerts that should be filtered out. One important challenge, also identified by ENISA [30], is the ability to exploit such labelled \"incident datasets\" and train ML models based on them:"}, {"title": "Challenge 2 Learning with small (labelled) datasets (from cyber incidents).", "content": "New knowledge, e.g. about certain threats, attacks, malware or vulnerability exploits, is frequently published (in e.g. threat reports). The traditional, and still most common, method of threat detection is so-called signature-based detection, where such knowledge is (often manually) encoded as specific patterns (called signatures). Detection is achieved by matching events with these signatures, and generating alerts when they match. While signature-based methods have their limitations, such knowledge could improve the performance of ML-based detection models trained on event data, requiring the ability to extract relevant knowledge and including it in the ML models:"}, {"title": "Challenge 3 Extract knowledge (including about threats, malware and vulnerabilities) and enrich ML-based detection models with it.", "content": "In addition to reports, there are many knowledge bases and formal ontologies that can be used to enrich ML models with such knowledge. There are also attempts to extend the coverage domain for such ontologies: one example is the unified cybersecurity"}, {"title": "Challenge 4 Automated generation of hypotheses from CTI and validation of hypotheses using observations for threat hunting.", "content": "There are AI-based approaches to facilitate threat hunting [70]. Most have focused on supporting hypothesis generation by extracting relevant CTI, and using both ML/NLP [33] and symbolic AI [82]. Symbolic AI has been used to support validation [18]."}, {"title": "Analyse", "content": "The goal of the analysis phase is to understand the nature of the observed alerts, determine possible business impact and create sufficient situational awareness to support the subsequent pland and execute phases.\nBoth malware and benign software continuously evolve. This makes it difficult to separate malicious from benign behaviour [30], despite continuous detection engineering efforts improving the capabilities. For example, an update to benign software may cause a match with an existing malware signature, and may also show up as an anomaly in the network traffic. As a result, most of the alerts are either false or not sufficiently important for further investigation [16]. The analysis phase is, therefore, labour-intensive, where security analysts must plough through and analyse a large number of alerts - most of them false - to decide their nature and importance. This could lead to so-called alert fatigue among security analysts:"}, {"title": "Challenge 5 The volume of alerts leads to alert flooding and alert fatigue in SOCs.", "content": "Understanding the nature of alerts is important, and studies have shown that a lack of understanding of the underlying scores, or reasoning, behind the alerts have led to misuse and mistrust of ML systems [73]. Both studies [16] and guidance from ENISA [30] have highlighted the need for alerts to be reliable, explainable and possible to analyse. The use of explainable AI to support this has shown some promise [27], and both knowledge graphs [16] and LLMs have been identified as promising approaches.\nAn alert is just one observation and needs to be put into a larger context to identify an incident and provide necessary situational awareness as a result of the analysis [31]. Such contextualisation includes enriching alerts with relevant knowledge from previous incidents, common systems behaviour, infrastructure details, threats, assets, etc. The same attack or the same phase of a larger attack is likely to trigger many different alerts. Different ML techniques, particularly clustering, have been studied to fuse or aggregate related alerts [92,55]. In addition to understanding an incident and achieving situational awareness, contextualisation will also help a security analyst understand"}, {"title": "Challenge 6 Combine observation with knowledge to analyse, develop and communicate situational awareness.", "content": "Developing cyber situational awareness requires connecting a plethora of different sources, such as alerts and details about infrastructure and threats. There have been proposals to use knowledge graphs to combine these different sources to support analysis [88,60], including explanation [16]."}, {"title": "Challenge 7 Understanding the risk, impact, importance and priority of incidents.", "content": "When an incident is understood and sufficient situational awareness is achieved, a suitable amount of resources have to be allocated to handle the incident. There may be multiple incidents, requiring some prioritisation between them. This involves understanding the risks and potential impacts of the incident, including the risks and impacts of any mitigating actions that may be taken in subsequent MAPE-K phases:"}, {"title": "Plan & Execute", "content": "The last two phases of MAPE-K, plan and execute, focus on responding to detected incidents. This involves finding suitable responses in the plan phase, and prepare and execute the response(s) in the execute phase. From an AI perspective, research in these phases is not as mature as in the monitor and analyse phases. We will here only focus on the plan phase, which we currently consider to have the more interesting AI-related challenges. To plan a suitable response, two promising AI techniques are AI planning (e.g. [36]) and reinforcement learning (RL \u2013 e.g. [45,71]). Each of them have their pros and cons: AI planning requires considerable knowledge and formulation of the underlying environment, whilst reinforcement learning requires a considerable amount of interactions/simulations (often in the millions). In certain cases, a quick response time is necessary, which means this level of interaction would be too time-consuming. When generating response actions, their risk and impact must be taken into account (including the risk and impact of doing nothing), which is an unsolved problem when using AI. Moreover, when proposing a response action, an AI-generated solution must be able to explain both what the response action will do and why it is suitable for the given problem:"}, {"title": "Challenge 8 Generate and recommend suitable response actions in a timely manner that take into account both risk and impact and are understandable for a security analyst.", "content": "To support such generation, there are several frameworks and formal ontologies that can be used, such as MITRE D3FEND [49], RE&CT [2] and CACAO playbooks [65]."}, {"title": "Shared knowledge", "content": "The 'K' in MAPE-K stands for knowledge shared across the phases, and we have, for instance, seen knowledge about threats and the infrastructure being protected used across different phases. Moreover, this knowledge takes different forms and representations (structured and unstructured) and is analysed using different techniques (symbolic and sub-symbolic). In addition to consuming knowledge, it is also important to share knowledge with key stakeholders, both technical and non-technical [29]. This may be a report about an incident for internal use (e.g. to board members) or sharing of discovered threat intelligence with a wider community:"}, {"title": "Challenge 9 Generating suitable incident and CTI reports for the target audience.", "content": "To summarise, we have shown the need to learn and reason across MAPE-K and that both symbolic and connectionist AI are being used across the phases. We have identified several challenges, and next, we make the case for NeSy to address them."}, {"title": "3 The case for neurosymbolic AI", "content": "Kahneman's [48] distinction between (fast) instinctive and unconscious 'system 1' processes from (slow) more reasoned 'system 2' processes, has often been used to illustrate the NeSy integration of neural networks (system 1) and logical reasoning (system 2). Building on this analogy, system 1 can, in a SOC, be seen as the ML-based AI used to identify potentially malicious behaviour in the monitor phase. Here, a large amount of noise needs to be filtered out from the large amount of events (thus a need for speed and scalability). System 2 is the reasoning conducted in the analysis phase, which requires deeper insight with the need for scalability less significant.\nThis dichotomy of requirements entails that neither end-to-end pure statistical nor pure logical approaches will be sufficient, and a NeSy combination seems ideal. Three commonly used reasons for pursuing NeSy are to design systems that are human auditable and augmentable, can learn with less and provide out-of-distribution generalisation [37]. We have seen examples of each of these in the challenges described in \u00a72: the use of knowledge to both contextualise, analyse and explain alerts, and to generate and explain and response actions; to learn from (relatively few) incidents; and to handle concept drifts and noise in order to achieve high accuracy of ML models under real-world conditions. From the challenges in \u00a72, we will here outline a set of NeSy use cases we believe are promising and identify some promising NeSy tools and techniques for each of them. This work is not complete and should be seen as a start (see \u00a75). Moreover, this section is speculative by nature, but we provide some evidence in terms of existing work and experiments conducted in \u00a74."}, {"title": "Monitor", "content": "The ability to integrate relevant knowledge into ML-based detection models (challenge 3) falls directly under the NeSy paradigm, and could both improve performance under real-world conditions (challenge 1) and help to reduce the number of false alerts (challenge 5):"}, {"title": "Use case 1 Use (symbolic) knowledge of threats and assets to guide or constrain ML-based detection engines.", "content": "A similar case for such a NeSy use case is made in [79]. Logical Neural Networks (LNN) [85] are designed to simultaneously provide key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning), enabling both logic inference"}, {"title": "Use case 2 Learn detection models from a limited number of (labelled) incidents", "content": "Additional embedded knowledge in an LNN or LTN may help to reduce training time. NS-CL [62], which builds models to learn visual perception including semantic interpretation of the images, has shown it can be trained on a fraction of the data required by comparable methods albeit in a different domain with different data sources. NeSy-based inductive logic programming variants, such as OILP [28], would also be able to learn from small datasets. The learned logic program will also be inherently explainable (see challenge 6)."}, {"title": "Use case 3 LLM-driven threat hunting using symbolic knowledge and reasoning capabilities.", "content": "Threat hunting involves generating suitable hypotheses, applying and validating them, then update and iterate (challenge 4). Work has started investigating LLMs for this challenge [78]. It has been argued for symbolism in LLMs [40], and based on that we define an LLM-based NeSy threat hunting use case:\nLLMs have been used for hypothesis generation in other domains [83], which can be further investigated for threat hunting. Hypothesis generation is typically driven by CTI, which can be captured in a knowledge graph. The integration of LLMs and knowledge graphs is an active research field [77]. In addition, symbolic or computational methods could be used for other steps in the hunting process, including: planning how to answer the hypothesis; reasoning about available data sources to execute this plan; ensuring correct translation to required query language to validate the hypothesis using the observations; and finally, reason about the results from the execution and provide input for any refinement of the hypothesis for a new hunting iteration."}, {"title": "Analyse", "content": "A prominent characteristic of NeSy is its capacity to combine learning and reasoning. Such a combination is desirable in a SOC, and our next use case, which cuts across the monitor and analyse phases, addresses several of the challenges from \u00a72:"}, {"title": "Use case 4 Incorporate learning of detection models with the ability to reason about their outcomes to understand and explain their nature and impact.", "content": "In [79], the case for such integration of detection and analysis using NeSy is also made. One way to achieve this is to simultaneously train a neural network (for detection) with related symbolic rules that can be used for contextualization, analysis and explanation (challenge 6). Two NeSy techniques that can accomplish this are Deep Symbolic Learning (DSL) [23] and Neuro-Symbolic Inductive Learner (NSIL) [22]. dPASP [35] and NeurASP [96] are techniques based on Answer Set Programming (ASP) [15], which"}, {"title": "Use case 5 Extracting alerts in a symbolic form.", "content": "In [43], symbolic alerts are extracted from a graph neural network (GNN) based detection engine. A combination of GNNExplainer [97] and DL-Learner [57] is used to extract the symbolic alerts. The symbolic rules learnt by both DSL and NSIL may also provide such symbolic alert representation, and the use of e.g. JILP for detection will learn symbolic alerts by design. Embed2Sym [10] extracts latent concepts from a neural network architecture, and assigns symbolic meanings to these concepts. These symbolic meanings can then be used to encode symbolic alerts."}, {"title": "Use case 6 Use statistical AI to enrich or extract symbolic knowledge.", "content": "A SOC typically receives a large volume of threat intelligence, which is too large to thoroughly analyse manually. Such intelligence is used to contextualise alerts, and it is thus desirable to enrich the SOCs knowledge bases with relevant intelligence reports:\nThis use case addresses challenge 6. In \u00a72, we discussed several approaches to extract knowledge in a suitable symbolic form from reports [63,58,59]. STAR [84] is a possible NeSy technique that can be used. It combines LLMs with ASP, where ASP can be employed to reason over the extracted knowledge, in addition to just extracting it.\nThis ability to reason is crucial as the intelligence report may be incorrect or superseded for different reasons, including underlying (aleatoric) uncertainty, deterioration over time, or from sources one does not fully trust. It may also simply not be relevant for our purposes, or more importantly, intelligence reports may conflict with our existing knowledge or our observations. It would therefore be desirable to quantify and reason about knowledge, including the level of trust, from both own observations and existing knowledge:"}, {"title": "Use case 7 Reason about and quantify knowledge.", "content": "This use case is aimed at addressing challenge 7. It may play a role in implementing a technique known as risk-based alerting [90], which involves using data analysis to determine the potential severity and impact of alerts and incidents. Probabilistic attack graphs [38] has been used to add probabilities to CTI. One potential NeSy approach for this use case is Recurrent Reasoning Networks (RRNs) [44]. RRNs could be used to train a ML model from observations to reason about our existing knowledge graph, e.g."}, {"title": "Use case 8 Relate the different phases of cyber incidents.", "content": "One concrete NeSy use case would be to merge the statistics- and semantics-driven approaches outlined in [8]. Further, PyReason [4] enables temporal reasoning over graphical structures, such as knowledge graphs. This can be used to exploit the temporal aspect of relating the different phases. The second experiment in \u00a74 addresses this use case by exploring temporal reasoning using a combination of LLMs, temporal logic, ASP and plan recognition. The ontological reasoning supported by RRNs also seems promising for this type of problem."}, {"title": "Plan & execute", "content": "Neurosymbolic reinforcement learning (NeuroRL) [3] combines the respective advantages of reinforcement learning and AI planning. NeuroRL can learn with fewer interactions compared with traditional RL by using inherent knowledge, thus making it more applicable than both RL and AI planning when (near) real-time response is required and a complete model of the environment is infeasible. Moreover, it has the promise of more explainable response actions, whilst a reasoning engine could, in principle, help to take into account both risk and impact\u00b9\u00b9. Thus, this seems like promising approach for challenge 8:"}, {"title": "Use case 9 Generating impact and risk aware explainable response actions in a timely fashion using neurosymbolic reinforcement learning,", "content": "Neurosymbolic reinforcement learning has been used in offensive cyber security settings for penetration testing12 [25]. Whilst there are some commonalities with our challenges, defending has their own peculiarities. For example, speed, risk, impact and explainability are more prominent when defending against attacks."}, {"title": "Shared knowledge", "content": "Our final use case directly addresses challenge 9. LLMs are extensively studied for generating reports and this is also the case for cyber defence [69]. The generation process is likely to use symbolism (e.g. knowledge graphs [77]). The reports need to be correct, which is one area symbolic AI can help [40]. We, therefore, rephrase challenge 9 as a NeSy use case:"}, {"title": "Use case 10 Generation of incident reports and CTI reports tailored for a given audience and/or formal requirements, using (symbolic) knowledge and LLMs.", "content": ""}, {"title": "4 Proof-of-concept experiments", "content": "To showcase the feasibility of using NeSy for our use cases, we have conducted two initial experiments: 1) using LTN to show how knowledge in symbolic form can be used to improve an ML-based detection engine (use case 1); and 2) using LLMs and ASP to elicit and reason with adversary attack patterns and observed alerts for situational awareness (use case 6). Both experiments are deliberately simplified and are not conducted in realistic conditions. They do, however, demonstrate the potential of NeSy in a SOC setting. Additional technical details can be found in appendix A and all source code can be found on GitHub: https://github.com/FFI-no/Paper-NeSy24."}, {"title": "Experiment 1: LTN for knowledge-aware intrusion detection", "content": "The first experiment addresses use case 1 and is in the monitor phase of MAPE-K. Here, the goal is to detect malicious traffic by training a classifier that can separate benign traffic from two different classes of malicious traffic: brute force attacks and cross-site scripting (XSS) attacks. The classifier will produce an alert if malicious traffic is seen. The input data are NetFlow entries [20], which provide aggregated information on traffic between two distinct ports on distinct IP addresses for a given protocol. The data is a subset of the CICIDS2017 dataset [86], which is a labelled dataset containing simulated attacks on a network, with additional details in appendix A.\nThe experiment consists of two parts: In the first part, a simple 3-layered fully connected neural network is trained and used as a baseline. In the second part, this neural network is extended using an LTN [12], where additional domain knowledge is encoded. In both cases, 70% of the data is used for training and 30% for testing. The experiment is inspired by [74], where LTN is used in a similar, but more limited, way.\nThe LTN consists of one predicate for class membership, P(x, class), and is configured as a neural network with the same structure as the baseline neural network. We define the following axioms (expressed in Real Logic [12]):\n$\\forall x \\in B : P(x, Benign)$ $\\forall x \\in BF : P(x, Brute-force)$ $\\forall x \\in X : P(x, XSS)$\nThese axioms describe how all flows in the training set labelled as a given class are a member of the class. This encodes the information of the baseline neural network with no additional knowledge. From the dataset network topology, we define NWS to be the set of all NetFlows that do not communicate with a web server. We know that if a NetFlow is in this NWS-set, then it cannot be a web attack (that we are interested in). We add this domain knowledge as a fourth axiom used by the LTN:\n$\\forall x \\in NWS:\\neg(P(x, Brute-force) V P(x, XSS))$\nTraining consists of updating the neural network P to maximize the accumulated truth value of the axioms [12]. The following table shows the results on the test data:"}, {"title": "Experiment 2: LLMs and ASP for situational awareness", "content": "The second experiment illustrates the use of NeSy to relate different phases of an attack (use case 8). Here, alerts that are sequenced by time, are mapped to adversary attack patterns, gleaned from textual CTI reports into symbolic form using statistical methods (use case 6). The experiment is inspired by existing work such as: neurosymbolic plan recognition [6], attack plan recognition [7], and the use of LLMs to extract both LTL [32] and CTI (in the form of MITRE ATT&CK tactics or techniques) [41,75,98]. An LLM (GPT4) is first used to elicit formal representations of attack patterns described in CTI reports, affording us a rapid way of converting CTI to symbolic knowledge. Here, we use the NL2LTL Python library [32] to extract representations of attack patterns in LTLf [24], a temporal logic for finite traces. The following visualises a conceptual adversary attack pattern, sequencing MITRE ATT&CK techniques:\n{t1556}\n\u2022\n{t1059}\n{t1548}\n\u2022\n{t1059}\n...\nti\ntj\ntk\nti\nLTLf:(I\u2192\u25cb\u25c7(t1556\u2227\u25c7(t1059\u2227(t1548\u2227t1059))))\nEach 'txxx', where x is a number, is a unique technique from the MITRE ATT&CK framework [68], I is the initial state, and , and are the 'always', 'next' and 'eventually' operators in LTL. Next, telingo [17], an ASP solver for temporal programs, is used to postdict possible attacks. It is given: (i) LTLf representations of known attack patterns; (ii) sequences of observed alerts; and (iii) knowledge linking alerts to techniques. The attack patterns outlined in (i) are acquired by the elicitation step described above, and the sequences of observed alerts outlined in (ii) are assumed to come from a SIEM system. That is, alerts produced are in a structured form amenable to be represented as Prolog/ASP terms. We assume that this conversion of alerts to symbolic form (use case 5) exists (see e.g. [43]). Furthermore, they are temporally ordered inducing a sequence of alerts (where az is an alert in symbolic form):\n{@addGrpMem} {abenign} {aexeclam}\n{alatMvmSaml} {abenign} {aexecWinPsh}\n\u2022\nt1\nt2\n\u2022\nt3\n\u2022\ntn-2\n\u2022\ntn-1\ntn\nFinally, we assume that all the alerts produced can be associated with MITRE ATT&CK techniques, which is common for many signature-based alerts. Note, however, that it is a many-to-many relationship: an alert can be an indicator for several techniques, and"}, {"title": "CTI transformed to LTL", "content": "This subsection explains how CTI reports of previous attacks are transformed into LTL temporal representations of the attack patterns. The CTI reports are in natural language and we utilize the NL2LTL [32] Python package for the translation. We define a custom pattern template ExistenceEventuallyOther to express the LTL property a\u028cb. Additionally, we create a custom prompt tailored for our domain. The prompt contains the allowed pattern (ExistenceEventuallyOther), allowed symbols (MITRE ATT&CK technique IDs), and multiple examples. The LTL representation is a chain of MITRE ATT&CK techniques. However, CTI descriptions of an attack might not reference any specific techniques. In those cases, we let the LLM deduce which MITRE ATT&CK technique is referenced from the general description in natural language. This is a minimal implementation, and extraction of MITRE ATT&CK tactics from CTI reports have been investigated before [75,98].\nTranslate natural language sentences into patterns:\nALLOWED PATTERNS: Existence Eventually Other\nALLOWED SYMBOLS: T1548 (Abuse Elevation Control Mechanism),\nT1530 (Data From Cloud Storage), [...]"}, {"title": "A Further details of experiments", "content": ""}, {"title": "A.1 Experiment 1", "content": "The recorded NetFlows are an aggregation of metadata of the IP traffic in the network. All traffic where a specific subset of features are the same is believed to be regarding the same activity is recorded as the one flow [20].\nThe dataset is a subset of the CICIDS2017 dataset [86], specifically looking at the \"Tursday Morning\" part. For simplicity we have chosen a selection of flow features"}, {"title": "A.2 Experiment 2", "content": "telingo program The following code listing shows the telingo program from the first experiment:"}]}