{"title": "Understanding Gradient Boosting Classifier: Training, Prediction, and the Role of j", "authors": ["Hung-Hsuan Chen"], "abstract": "The Gradient Boosting Classifier (GBC) is a widely used machine learning algorithm for binary classification, which builds decision trees iteratively to minimize prediction errors. This document explains the GBC's training and prediction processes, focusing on the computation of terminal node values yj, which are crucial to optimizing the logistic loss function. We derive ; through a Taylor series approximation and provide a step-by-step pseudocode for the algorithm's implementation. The guide explains the theory of GBC and its practical application, demonstrating its effectiveness in binary classification tasks. We provide a step-by-step example in the appendix to help readers understand.", "sections": [{"title": "Introduction", "content": "The gradient boosting machine (GBM) [Friedman, 2001, Hastie et al., 2009] is a robust predictive model for tabular datasets. GBM constructs weak learners (trees) sequentially; each tree predicts the residual of the earlier predictions.\nWhen applying GBM to regression tasks, the training process of each tree is very similar to that of a standard decision tree. However, when applying GBM on binary classification tasks (we call the Gradient Boosting Classifier method, or GBC, below), the prediction of a terminal node j of a tree Tm is not the average residual of the training instances that fall at node j. Instead, the prediction of the terminal node j in the tree Tm is given by a mysterious equation below.\n\n\\gamma_j = \\frac{\\sum_{i \\in \\Omega_{m,j}} r_i}{\\sum_{i \\in \\Omega_{m,j}} p_{m-1}^{(i)}(1 - p_{m-1}^{(i)})} \n\nwhere $p_{m-1}^{(i)}$ is the estimated probability that the ith instance's target value is 1 in the previous iteration, $r_m^{(i)} = y^{(i)} - p_{m-1}^{(i)}$ is the residual of instance i that needs to be fit/predict in iteration m, and Om,j is the set of instance indices located in the terminal node j for tree Tm.\nIn the next section, we introduce the GBC training and prediction procedure, followed by a clear explanation of how yj is derived in Section 3. We give a step- by-step example in the appendix in Section A."}, {"title": "GBC training and prediction", "content": "The training of a Gradient Boosting Classifier begins with an initial model, which is typically a constant prediction for all instances. In binary classification, the model is initialized with a raw predicted logarithm of odds (called log odds below) F(i) set to 0 for each instance i, which corresponds to an initial predicted probability of 0.5 for all instances (since $p_0^{(i)} = \\sigma(F_0^{(i)})$, where o is the logistic function). This represents a neutral starting point since the model does not have prior information about the data.\nThe algorithm iteratively proceeds over M boosting iterations. At each iteration m, the model computes the residuals for each training instance. The residuals represent how far the current predicted probability is from the true label $y^{(i)}$. Specifically, the residual $r_m^{(i)}$ is computed as the difference between the true label $y^{(i)}$ and the predicted probability $p_{m-1}^{(i)}$ from the previous iteration. These residuals are used to train the decision tree Tm, where the tree is fitted to predict the residuals, thus focusing the model on correcting the errors made by the previous trees. The tree training strategy could be based on metrics such as information gain, Gini-index, or mean squared error, guiding the tree to select the optimal feature and threshold for splitting [Han et al., 2012].\nOnce the tree Tm is trained, the algorithm computes the optimal values for the terminal nodes. For each terminal node j, the output value ; is calculated by minimizing the logistic loss for the instances that fall into that node. This is done using Equation 1; the reasons will be explained in Section 3.\nNext, for each instance k that falls into terminal node j, the model prediction is updated by adding the scaled node value ; to the previous prediction $F_{m-1}^{(k)}$ with a scaling factor given by the learning rate n. This update is applied as follows:\n\n$F_m^{(k)} = F_{m-1}^{(k)} + \\eta \\gamma_j$\n\nThe predicted probability of instance k is then updated by applying the logistic function to the new raw score $F_m^{(k)}$:\n\n$p_m^{(k)} = \\sigma(F_m^{(k)})$.\n\nThe algorithm builds trees over M iterations, incrementally refining the model's predictions by reducing residual errors from previous iterations. At the end of the training process, we have an ensemble of M decision trees T1:M, each contributing to the final prediction."}, {"title": "Prediction process", "content": "Once the Gradient Boosting Classifier is trained, GBC predicts the labels of new test instances as follows: The prediction process starts by initializing the raw prediction F(i) for each test instance i to 0, just as in the training phase. The algorithm then iteratively applies each of the M trees to the test instance, adding the scaled output of each tree to the current raw score.\nSpecifically, for each tree Tm, the prediction for a test instance $x^{(i)}$ is ob- tained by evaluating the decision tree. The output value, $T_m(x^{(i)})$, is then scaled by the learning rate n and added to the current raw score $F^{(i)}$ as:\n\n$F^{(i)} = F^{(i)} + \\eta T_m(x^{(i)})$"}, {"title": "Where does j come from?", "content": "Assume that GBC has been trained for m 1 iterations, so $F_{m-1}(x^{(i)})$ is fixed. GBC attempts to train a new weak learner Tm that has J terminal node pre- dictions 1,...,YJ. The new predicted logarithm of odds $F_m(x^{(i)})$ is given by\n\n$F_m(x^{(i)}) = F_{m-1}(x^{(i)}) + \\gamma_j$,\n\nwhere j is the index of the terminal node that $x^{(i)}$ falls in.\nThe predicted probability is\n\n$p^{(i)} = \\sigma(F_m(x^{(i)})) = \\frac{1}{1+e^{-F_m(x^{(i)}}}$\n\nLet $y_m^{(i)}$ and $p_{m-1}^{(i)}$ denote the ground truth labels and predicted probabil- ities after iteration m for all the nodes that fall in the terminal nodej of the tree Tm, the cross entropy loss for these instances is defined below.\n\n$L(y_{m,j}, p_{m,j}) = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} log p_m^{(k)} + (1 - y^{(k)}) log(1-p_m^{(k)}))$\n$\\qquad \\qquad = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} log p_m^{(k)} - log(1 - p_m^{(k)}) + y^{(k)} log(1-p_m^{(k)}))$\n$\\qquad \\qquad = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} log \\frac{p_m^{(k)}}{1 - p_m^{(k)}} - log(1-p_m^{(k)}))$\n$\\qquad \\qquad = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} \\cdot log (\\frac{1}{1+e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}} ) - log (1-\\frac{1}{1+e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}))$\n$\\qquad \\qquad = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} \\cdot log (\\frac{1}{1+e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}} ) - log (\\frac{e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}{1+e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}))$\n$\\qquad \\qquad = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} \\gamma_j - log (\\frac{e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}{1+e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}))$\n$\\qquad \\qquad = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} \\gamma_j - log (\\frac{1}{1+e^{(F_{m-1}(x^{(k)})+\\gamma_j)}}))$\n$\\qquad \\qquad = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} \\gamma_j + log (1+e^{(F_{m-1}(x^{(k)})+\\gamma_j)}))$\n\nWe look for the ; value to minimize the cross-entropy loss."}, {"title": "Trial 1: set the derivative to zero and solve the equa- tion", "content": "The loss function can be considered a function of Yi since Y\u00a1 is the only variable in the function. We take the derivative of the loss with respect to y; and set it to zero:\n\n$\\frac{\\partial L(y_{m,j}, p_{m,j})}{\\partial \\gamma_j} = \\frac{\\partial f(\\gamma_j)}{\\partial \\gamma_j} = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} + \\frac{e^{F_{m-1}(x^{(k)})+\\gamma_j}}{1+e^{F_{m-1}(x^{(k)})+\\gamma_j}}} )$\n$\\qquad \\qquad = \\sum_{k \\in \\Omega_{m,j}} ( -y^{(k)} + \\frac{1}{1+e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}} ) = 0$\n$\\Rightarrow \\qquad \\sum_{k \\in \\Omega_{m,j}} ( \\frac{1}{1+e^{-(F_{m-1}(x^{(k)})+\\gamma_j)}}} ) = \\sum_{k \\in \\Omega_{m,j}} y^{(k)}$"}, {"title": "Trial 2: Approximate the loss function by Taylor's series", "content": "Taylor's expansion approximates a function f(x) based on a fixed to as fol- lows [Canuto and Tabacco, 2015].\n\n$f(x) = f(x_0) + (x - x_0) f'(x_0) + \\frac{1}{2}(x - x_0)^2 f''(x_0)$\n\nLeaving Equation 8 be f(j), we estimate f(yj) at xo = 0 based on the Taylor expansion:\n\n$f(\\gamma_j) \\approx f(0) + (\\gamma_j \u2013 0) f'(0) + \\frac{1}{2}(\\gamma_j \u2013 0)^2 f''(0)$\n$\\qquad = \\sum_{k \\in \\Omega_{m,j}} log (1+e^{F_{m-1}(x^{(k)})}) + \\gamma_j \\sum_{k \\in \\Omega_{m,j}} (-y^{(k)} + \\frac{e^{F_{m-1}(x^{(k)})}}{1+e^{F_{m-1}(x^{(k)})}})$\n$\\qquad \\qquad + \\frac{{\\gamma_j}^2}{2} \\sum_{k \\in \\Omega_{m,j}} \\sigma(F_{m-1}(x^{(k)})) (1 \u2013 \\sigma(F_{m-1}(x^{(k)})))$\n$\\qquad = \\sum_{k \\in \\Omega_{m,j}} log (1+e^{F_{m-1}(x^{(k)})}) + \\gamma_j \\sum_{k \\in \\Omega_{m,j}} (-y^{(k)} + p_{m-1}^{(k)})$\n$\\qquad \\qquad + \\frac{{\\gamma_j}^2}{2} \\sum_{k \\in \\Omega_{m,j}} p_{m-1}^{(k)}(1-p_{m-1}^{(k)})$\n\nTake the derivative of Equation 11 and set it to zero:\n\n$\\frac{\\partial f(\\gamma_j)}{\\partial \\gamma_j} = \\sum_{k \\in \\Omega_{m,j}} (-y^{(k)} + p_{m-1}^{(k)}) + \\gamma_j \\sum_{k \\in \\Omega_{m,j}} p_{m-1}^{(k)}(1-p_{m-1}^{(k)}) := 0$\n\n$\\Rightarrow \\qquad \\gamma_j = \\frac{\\sum_{k \\in \\Omega_{m,j}} (y^{(k)} \u2013 p_{m-1}^{(k)})}{\\sum_{k \\in \\Omega_{m,j}} p_{m-1}^{(k)} (1 \u2013 p_{m-1}^{(k)})}$"}, {"title": "Summary", "content": "This document explores the inner workings of the Gradient Boosting Classifier (GBC), a special case of the Gradient Boosting Machine (GBM) specifically de- signed for binary classification tasks. GBC builds a model by iteratively adding weak learners (decision trees) that predict the residual errors of previous iter- ations. Unlike GBM used for regression, GBC involves predicting probabilities for binary outcomes and, as such, each terminal node in a tree outputs values determined by the residual errors and prior probabilities from earlier iterations.\nWe introduce the central equation that determines j, the output value of a terminal node. The derivation of the terminal node value ; is explored in detail, demonstrating how it minimizes the logistic loss function. We tried to solve the optimal y; analytically by setting the derivative of the loss function to zero, though this appeared to be intractable. Consequently, we used a Taylor series approximation to derive a more tractable expression for yj, revealing how it is connected to the residuals and prior predictions of the model.\nIn conclusion, the document provides a comprehensive overview of the Gra- dient Boosting Classifier, covering both the theoretical aspects and practical implementation steps while highlighting the role of the terminal node values j in optimizing the model's performance in binary classification tasks."}]}