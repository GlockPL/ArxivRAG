{"title": "Networks of Networks: Complexity Class Principles Applied to Compound AI Systems Design", "authors": ["Jared Quincy Davis", "Boris Hanin", "Lingjiao Chen", "Peter Bailis", "Ion Stoica", "Matei Zaharia"], "abstract": "As practitioners seek to surpass the current reliability and quality frontier of monolithic models, Compound AI Systems consisting of many language model inference calls are increasingly used. In this work, we construct systems, which we call Networks of Networks (NoNs) organized around the distinction between generating a proposed answer and verifying its correctness, a fundamental concept in complexity theory that we show empirically extends to Language Models (LMs). We introduce a verifier-based judge NoN with K generators, an instantiation of \"best-of-K\" or \"judge-based\" compound AI systems. Through experiments on synthetic tasks such as prime factorization, and core benchmarks such as the MMLU, we demonstrate notable performance gains. For instance, in factoring products of two 3-digit primes, a simple NoN improves accuracy from 3.7% to 36.6%. On MMLU, a verifier-based judge construction with only 3 generators boosts accuracy over individual GPT-4-Turbo calls by 2.8%. Our analysis reveals that these gains are most pronounced in domains where verification is notably easier than generation-a characterization which we believe subsumes many reasoning and procedural knowledge tasks, but doesn't often hold for factual and declarative knowledge-based settings. For mathematical and formal logic reasoning-based subjects of MMLU, we observe a 5-8% or higher gain, whilst no gain on others such as geography and religion. We provide key takeaways for ML practitioners, including the importance of considering verification complexity, the impact of witness format on verifiability, and a simple test to determine the potential benefit of this NoN approach for a given problem distribution. This work aims to inform future research and practice in the design of compound AI systems.", "sections": [{"title": "1 Introduction", "content": "In scenarios where practitioners are willing to expend a higher budget to go beyond the current reliability and quality frontier achievable via today's state-of-the-art monolithic models, Compound AI Systems consisting of many language model inference calls are increasingly employed Zaharia et al. [2024], Alp, Stechly et al. [2023], Chen et al. [2024], Valmeekam et al. [2023]. Implicitly or explicitly, we are often constructing networks of these calls: networks of networks, of sorts. This begs the question of what principles can guide the structure of the composition of these networks. As a central idea in this direction, we propose constructions organized around an explicit delineation between generation complexity and verification complexity. Correctly answering a question typically involves both generating a proposed answer and verifying that this answer is correct. This distinction between generation and verification is an organizing principle in complexity theory since, in many cases, the fastest algorithms for generation are more computationally expensive than generic algorithms for verification.\nThe key message of this article is that verification vs. generation complexity asymmetry notions continue to hold empirically when replacing tailored algorithms with Language Models (LMs). That this should be the case is not obvious, because it is far from clear what sorts of algorithms LMs use when asked to either generate or verify. We demonstrate our findings using a simple compound LM inference system that explicitly separates generation and verification calls. Specifically, we consider a compound AI system we term a verifier-based judge system with K generators, which works as follows:\n\u2022 A query/question q is fed independently into K language model calls, which can be differ-ent models or the same model K times (we typically use GPT-40 or GPT4-turbo). K is a hyper-parameter.\n\u2022 For k = 1,..., K the kth LM generator outputs $G_k(q) = (\\alpha_k, \\pi_k)$, where ak is the pro-posed answer and \\(\\pi_k\\) is the proposed explanation for why this answer is correct.\n\u2022 Answer-explanation pairs $(\\alpha_k, \\pi_k)$ are provided one at a time to another LM, which acts as a verifier or judge. For each $k = 1,..., K$, the LM returns $J(q, \\alpha_k, \\pi_k) \\in \\{0,1\\}$. An output of 1 means ak correctly answers the question q, given the explanation \\(\\pi_k\\). If $J(q, \\alpha_k, \\pi_k) = 1$, the answer ak is accepted and the algorithm terminates and returns ak.\n\u2022 If all answers are rejected, then a random answer is returned.\nNote that these verifier-based judges explicitly separate generation and verification. We find that they lead to notable performance gains on both synthetic and benchmark algorithmic reasoning tasks.\nFor example, we consider factoring an integer into a product of two primes. This is a canonical prob-lem for which it is widely believed that generation is computationally expensive since it requires fac-toring, while verification is fast because it only involves multiplication. We find that individual calls to GPT-4o have a very low probability of correctly factoring even numbers p which are the product of 2 three digit integers (around 3.7%), while the same LM has approximately a 90% probability of correctly deciding whether a given factorization is valid. In particular, using a judge-based verifier system with K = 10 generators allows us to improve the performance from 3.7% to 36.6% (see Table 1). This matches almost exactly the probability of at least one generator proposing a correct factorization, as predicted by the our analysis in \u00a72.2.\nThis simple example suggests that when using compound systems made up of many calls to an LM it can be useful in problems with hard generation and relatively easy verification to invest more resources into generation than verification. This is precisely what is done in invoking chain of thought with Gemini Team et al. [2023] and in AlphaCode2 Li et al. [2022].\nTo further validate this beyond factorization tasks, we study the performance of verifier-based judge systems on MMLU Hendrycks et al. [2020]. We continue to find significant improvements in overall performance. Individual GPT-4-Turbo calls obtain an accuracy of 81.2%, but a verifier-based judge with K = 3 generators obtains 84% accuracy. The gains are magnified when broken down by topic, with subjects such as Abstract Algebra, Formal Logic, High School Math, and High School Physics-which arguably exhibit especially large gaps between the complexity of generation and verification-showing gains ranging from 6.35% to 8.76% (see Tables 3 and 4).\nBased upon our results, we encourage ML practitioners to Think about verification complexity. Some tasks will be more verifiable than others. Most traditional computing and software engineering tasks, in particular, possess this property. Our results suggest this asymmetry extends to LMs as well, when prompted to act as verifiers; however, you can also employ classical simulators, unit tests, or other components as verifiers in Compound AI Systems design. Also, note that given questions sampled from a distribution, there is a simple test for (i) whether the approach proposed in this paper will help for subsequent problems, as expressed in \u00a72.2, and (ii) to what extent.\nWe hope practitioners will find these discussions helpful in informing future research and practice in this emerging domain."}, {"title": "2 Verifier-Based Judges: Formalism and Analysis", "content": "In this section, we introduce verifier-based judges.\nDefinition 1 (Verifier-Based Judge). A verifier-based judge J for a query q and K answer-witness pairs (a, \u03c0) = ((a1, \u03c0\u2081),..., (\u03b1\u03ba,\u03c0\u03ba)) operates as follows:\nLet \u03c3 be a random permutation of {1, . . ., K}. Then:\n$J(q, (a, \\pi)) = \\begin{cases} a_{\\sigma(i)} & \\text{if } \\exists i \\leq K : V(q, a_{\\sigma(i)}, \\pi_{\\sigma(i)}) = 1 \\\\ & \\text{and } \\forall j < i, V(q, a_{\\sigma(j)}, \\pi_{\\sigma(j)}) = 0 \\\\ \\text{random } a_i & \\text{if } \\nexists i \\leq K, V(q, a_{\\sigma(i)}, \\pi_{\\sigma(i)}) = 0 \\end{cases}$ (1)\nNote that this is a version-zero construction without notions of verifier confidences. If no answer is accepted by the verifier, a random answer is returned. This is described in pseudo-code in Algorithm 1.\nIntuitively, a single verifier-based judge operating over many generators should be effective when: (i) each generator individually exhibits moderate variance; (ii) different generators are not too cor-related; (iii) for a typical query, the probability a typical generator gives the correct answer is low; (iv) it is notably easier for the verifier to discern the correct answer-witness pair than it is for the generators to produce it. In other words, verification is easier than generation. Formally, this final statement corresponds to requiring for a correct answer-witness pair (a*, \u03c0*) that\n$P(V(q, a^*, \\pi^*) = 1) > P(G_i(q) = (a^*, \\pi^*))$ for any generator Gi.\nWe make this precise in the following section."}, {"title": "2.2 Verifier-based Judge Performance Characterization", "content": "In this section, we determine analytically the performance of a verifier-based judge on a given query q with a unique correct answer a* in the simple case where we have access to K iid generators. Note that this iid assumption won't strictly hold in practice in many cases, but empirical results concord with this analysis well for the instance when generators are obtained by sampling with a fixed high temperature and independent random seeds from the outputs of a given LM. The performance of the verifier-based judge across a collection of queries is then simply obtained by averaging the results in this section over the query instance. We express our answers in terms of the three fundamental quantities:\n$c := P (J(q, (a, \\pi)) = 1 | a = a^*)$\n$s := P (J(q, (a, \\pi)) = 0 | a \\neq a^*)$\n$r(q) := P(G(q) = (a^*, \\pi) for some \\pi)$.\nThese quantities are the analogs of classical PCP notions of completeness, soundness, and generation complexity that are ubiquitous in complexity theory (see e.g. Arora and Safra [1998])."}, {"title": "Theorem 1 (Verifier-Based Judge Performance).", "content": "Consider a verifier-based judge system with K iid generators, and fix a query q with a unique correct answer a*. We have\n$\\begin{aligned} &P(J(q) = a^*) = rc\\frac{1 \u2013 (1 \u2013 \\beta)^K}{\\beta} + (1 \u2013 \\beta)^K\\frac{1}{|A|}, \\\\ &\\beta := 1 \u2013 ((1 \u2013 c)r + s(1 \u2013 r)).\\end{aligned}$ (2)\nProof. We have\n$\\begin{aligned} &IP (\\text{verifier correct}) = \\sum_{j=1}^K P (G_j \\text{ correct and accepted, } G_i \\text{ rejected } \\forall i < j) \\\\ &+ P(G_1, \u2026, G_K \\text{ rejected, random answer correct}) \\\\ &= \\sum_{j=1}^K P (G_j \\text{ correct}) P (G_j \\text{ accepted } | G_j \\text{ correct}) \\prod_{i=1}^{j} P (G_i \\text{ rejected}) \\\\ &+ \\prod_{i=1}^K P (G_i \\text{ rejected}) \\frac{1}{|A|}, \\end{aligned}$\nwhere for the second equality we have used the independence between both generators and the verifier. Note that, by definition,\n$P (G_j \\text{ correct}) P (G_j \\text{ accepted } | G_j \\text{ correct}) = rc, P (G_i \\text{ rejected}) = \\beta$.\nSumming on j = 1, ..., K completes the derivation."}, {"title": "Theorem 2 (Verifier-Based Judge Gain).", "content": "When the number of generators K goes to infinity, the gain of a verifier-based judge is positive if and only if:\nr\u22600,1 and $c = P (J(a, \\pi) = 1 | a = a^*) > P (J(a, \\pi) = 1 | a \\neq a^*) = 1 \u2212 s$. (3)\nThe second condition is precisely the statement that the judge is more likely to accept a correct answer than an incorrect answer.\nProof. Taking K\u2192\u221e in (2) gives\n$\\lim_{K \\rightarrow \\infty} [P(J(g) = a^*)-r] = r(\\frac{c}{\\beta} \u2013 1)$.\nNote that\nc = P (J(\u03b1, \u03c0) = 1 | a = a*)\n\u03b2 = P (J(\u03b1, \u03c0) = 1)\n= P (J(\u03b1, \u03c0) = 1 | a = a*) P (a = a*) + P (J(\u03b1, \u03c0) = 1 | a \u2260 a*) P (a \u2260 a*).\nHence,\nc > \u03b2 \u27fa P (J(\u03b1, \u03c0) = 1 | a = a*) > P (J(\u03b1, \u03c0) = 1 | a \u2260 a*),\nas desired.\nThis theorem shows that verifier-based judge system confers a gain, at least with a large number of generators, as soon as it's completeness exceeds a threshold determined by its soundness. Moreover, the extent of this gain is proportional to the query difficulty.\nOur analysis also reveals the diminishing returns of increasing the ensemble size K. While larger ensembles generally improve performance, the marginal benefit decreases, suggesting an optimal"}, {"title": "3 Experiments", "content": "In this section we present the two kinds of experiments discussed in the Introduction. First, in \u00a73.1, we detail experiments using LMs for prime factorization. Then, in \u00a73.3, we break down our experiments on MMLU."}, {"title": "3.1 Prime Factorization", "content": "As an example problem to demonstrate that sometimes verification is easier than generation for LMs, we invoke and probe the classic problem of prime factorization.\nThe prime factorization problem is core to cryptography and foundational algorithms like RSA [Rivest et al., 1978]. It turns out it is very laborious to factor a number that is a product of two large primes into the constituent primes; however, it is relatively easy to multiply two primes. Thus, if I have two candidate constituent primes, it can be easy and relatively quick to multiply them to test if they constitute a valid factorization.\nBuilding on this intuition, we conjectured that a simple verifier-based judge system could outper-form a baseline of single LM calls. To understand this, we first implemented two components:\n\u2022 Generators tasked with factorization.\n\u2022 Verifiers tasked with assessing a factorization and deeming it correct or incorrect.\nWe started off using GPT-4o and asking it to help us factor numbers that are a product of two 3-digit primes. This yielded a 3.70% success rate for the generators. For the verifier task of assessing a factorization as correct or incorrect, a GPT-40-based verifier was able to correctly classify a proposed factorization 90.11% of the time. Moreover, these results were well calibrated as excited by the completeness and soundness of the verifier, which intuitively correspond to the true positive and true negative rates.\nThese initial results convinced us to compose these components into a verifier-based judge system. Based on those results, the baseline GPT-40 in this case model achieved the 3.7% on up-to-3 digit factorization and a verifier-based judge system with K = 10 generators achieved 36.6%."}, {"title": "3.2 Lottery Ticket Problem", "content": "As another sub-experiment, we looked at a problem we viewed as classically non-verifiable, which we call the lottery ticket problem. In this problem, we have an oracle that picks a number within some range (0 and 100, for example). The task of the generator models is to guess the number. We then give the N guesses from the generators to the judge model and asks it to, optionally considering the guesses from the generators (\"advisors\") guess the oracle's number. As one might expect, a verifier-based judge confers no advantage in this case and success is roughly 1/N regardless."}, {"title": "3.3 MMLU", "content": "Beyond these specific examples, we also ran probes on the Massive Multitask Language Understand-ing (MMLU) benchmark [Hendrycks et al., 2020].\nOn the MMLU, we ran a baseline model of just gpt-4-1106-preview with temperature 1.0. We explored a number of temperatures in early probing and observed no clear performance gain pattern."}, {"title": "4 Related Work", "content": "The concept of composing multiple calls to models to improve performance has been implicitly invoked repeatedly in recent months. The Gemini Technical Report employed a novel scheme at inference time, termed CoT@32, as opposed to the default 5-shot prompting invocation scheme Team et al. [2023]. DeepMind's AlphaCode 2, in order to go \"beyond the capabilities of existing AI systems,\" employed a method that uses large-scale transformers to generate \"up to a million code samples per problem\" and then filters down the set to converge to a solution. Li et al. [2022]. N-shot prompting itself implicitly is a systems approach in that it often elicits longer (and thus higher latency / costlier) responses from models but can result in improved performance. Wei et al. [2022]. Chen et al. [2024], Li et al. [2024] probed how calling an LLM many times can improve or degrade performance on a task set, depending on the distribution of query difficulty in the set. Numerous other recent works have explored iteratively chaining self-improvement calls Madaan et al. [2024], Wang et al. [2024], Valmeekam et al. [2023], Stechly et al. [2023],\nThese self-improvement or ensemble systems can perhaps be unified and viewed through a single framework as wide or deep networks (or meshes) of calls to a given model or a cross a set of models and modules.\nSelf-Verification and Collections of Model Calls Stechly et al. [2023], Valmeekam et al. [2023] called into question claims that models can self-critique and iteratively self-refine [Madaan et al., 2024] their solutions when invoked in deep chains of calls. Stechly et al. [2023] evaluated iterative self-improvement claims over a suite of graph coloring problems and found that GPT4 achieves sub 20% accuracy; however, perhaps most surprisingly, they found the accuracy diminished in \"self-critiquing mode.\" Their work explained this by noting that even when GPT4 can guess a valid coloring by chance, its self-critiquing might lead to degradation in performance. This is congruent with results in Chen et al. [2024], which notes that making more calls to what it called voting-based LLM systems (ensembles with a voting-based aggregation function) can diminish performance for \"hard\" queries due to a sort of variance-reduction that hurts questions where the likelihood of a correct answer is <50% since the model might otherwise have \"gotten lucky\". Conversely, other recent works like Li et al. [2024], Madaan et al. [2024] also explored calling models in structured forms and demonstrated that this could lead to performance gain, even showing that systems consist-ing of weak LLMs can surpass the performance of the best individual LLM in social conversation settings Wang et al. [2024].\nThese seemingly disparate results can be reconciled, perhaps. Stechly et al. [2023], Valmeekam et al. [2023], suggest that when an \"externally sound verifier\" module is leveraged, \"try again\" systems methods can induce performance gains. This suggests that networks of calls can engender gains, but only under certain conditions.\nWe hope that our study and the discussion of the concept of verifiability complexity can shine a light on some of those conditions.\nClassical complexity notions of verification and generation. Comparison of verification and generation has been extensively studied via the lens of the classic computational complex-ity Papadimitriou [2003], Courcoubetis and Yannakakis [1995], Clarke [1997]. In fact, one of the most popular classes of problems, NP problems, are those whose solutions can be verified in poly-nomial time. Pioneering theoretical computer science (TCS) work Courcoubetis and Yannakakis [1995] places deep emphasis on understanding how verification can be leveraged to provably gener-ate (approximately) high-quality solutions to hard problems. In the context of LMs, however, it is unclear which \u201calgorithm\u201d an LLM uses and thus which \u201ccomplexity\" it incurs to solve a problem. This paper, though, demonstrates that the notion of verification and generation complexity can still apply to LMs, and can be leveraged to inform the design of compound LLM systems."}, {"title": "5 Conclusion and Future Work", "content": "Here we motivate the consideration of complexity class principles in Compound AI Systems design. We argue that LM practitioners should consider, measure, and build around the potential verification vs. generation difficulty asymmetry in their domain of interest.\nIn subsequent work, we hope to more extensively probe the verifier-based compound AI systems architecture landscape, investigating questions around how to measure and maximize the diversity among generator outputs, how the verifier's ability to discriminate varies as a function of tunable parameters, and more. We also hope to release code that the community can build upon to contribute to this further probe this theme."}]}