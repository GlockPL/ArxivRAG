{"title": "All You Need for Counterfactual Explainability\nIs Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty", "authors": ["Kacper Sokol", "Eyke H\u00fcllermeier"], "abstract": "This position paper argues that, to its detriment,\ntransparency research overlooks many founda-\ntional concepts of artificial intelligence. Here, we\nfocus on uncertainty quantification \u2013 in the con-\ntext of ante-hoc interpretability and counterfactual\nexplainability \u2013 showing how its adoption could\naddress key challenges in the field. First, we posit\nthat uncertainty and ante-hoc interpretability of-\nfer complementary views of the same underlying\nidea; second, we assert that uncertainty provides a\nprincipled unifying framework for counterfactual\nexplainability. Consequently, inherently transpar-\nent models can benefit from human-centred ex-\nplanatory insights \u2013 like counterfactuals \u2013 which\nare otherwise missing. At a higher level, integrat-\ning artificial intelligence fundamentals into trans-\nparency research promises to yield more reliable,\nrobust and understandable predictive models.", "sections": [{"title": "1. Uncertainty and Transparency", "content": "Artificial intelligence (AI) models can achieve impressive\nresults across many domains, but their deployment is often\nstymied by their opaqueness, unreliability and lack of ro-\nbustness [Rudin, 2019]. Consequently, two paradigms have\nemerged to alleviate such issues: ante-hoc interpretabil-\nity envisages building inherently transparent models whose\nfunctioning adheres to domain-specific constraints, and post-\nhoc explainability offers tools that elucidate the operation of\npredictive models through independent explanatory mech-\nanisms [Sokol & Flach, 2020a]. Additionally, uncertainty\nquantification has been proposed to improve the account-\nability of AI by looking beyond predictive performance; it\naims to provide truthful representation of models' aleatoric\ninherent to data generating processes - and epistemic\ndue to model non-uniqueness \u2013 uncertainty [H\u00fcllermeier &\nWaegeman, 2021].\nBut methods for reliably estimating uncertainty remain\nfairly underdeveloped, with such considerations often be-\ning neglected. Moreover, while post-hoc approaches are\nat the forefront of human-centred explainable AI (XAI) \u2013\nenabling diverse audiences, both with and without tech-\nnical expertise, to peer inside predictive models [Miller,\n2019] - these techniques are usually unable to faithfully\ncapture the operation of AI systems, offering misleading\ninsights [Rudin, 2019]. As a result, ante-hoc interpretable\nmodels are preferred in (high stakes) real-world applica-\ntions even though their functioning remains largely opaque\nto non-technical stakeholders, who nonetheless tend to be\ntheir primary users [Sokol & Vogt, 2023].\nIn this position paper we argue that AI transparency\ntechniques are largely oblivious to various notions of\nuncertainty despite the two being fundamentally inter-\nconnected. More broadly, we assert that XAI research\ntends to neglect the rich tapestry of foundational AI con-\ncepts, which can lead to reinventing what already exists;\nadmittedly, this sentiment is more true for post-hoc than\nante-hoc approaches since the latter draw upon decades of\nwork on classic AI models [Rudin et al., 2022]. We support\nthese arguments by demonstrating that connecting AI trans-\nparency and uncertainty quantification can address many\nopen challenges in XAI. Specifically, we use the example of\ncounterfactual explanations given that they are considered\nthe gold standard of human-centred XAI [Miller, 2019].\nTo this end, we first review relevant topics in Section 2:\n(\u00a72.1) counterfactual explainability, (\u00a72.2) uncertainty quan-\ntification and (\u00a72.3) their intersection. Next, in Section 3,\nwe present our arguments. We begin by (\u00a73.1) summarising\nopen challenges: misguided or overlooked modelling as-\nsumptions; ad-hoc fixes and unreliable proxies used in lieu\nof principled uncertainty handling; overemphasis on crisp\nclassification and inadequate XAI evaluation practice; domi-\nnance of neural models applied to unstructured data leading\nto neglect of inherently transparent AI; and largely missing\nhuman-centred perspective in ante-hoc interpretability.\nBuilding upon these observations, we posit that: (\u00a73.2) un-\ncertainty quantification and ante-hoc interpretability\nare fundamentally two facets of the same concept; and\n(\u00a73.3) uncertainty quantification provides a principled\nunifying framework for generating state-of-the-art coun-"}, {"title": "2. Background", "content": "Let us first overview latest research in counterfactual ex-\nplainability (\u00a72.1) and uncertainty quantification (\u00a72.2) as\nwell as work at the intersection of these two fields (\u00a72.3)."}, {"title": "2.1. Counterfactual Explainability", "content": "Counterfactuals capture hypothetical situations where the\noutcome of interest is different (usually, more desirable)\nthan what has been observed. They are considered the gold\nstandard of human-centred XAI given their strong social\nsciences foundations, natural familiarity to lay and expert\naudiences, and regulatory compliance [Wachter et al., 2017;\nMiller, 2019; Guidotti, 2022]. In their simplest form, they\nare generated by minimising the distance in the feature space\nbetween the current instance and a hypothetical data point\nthat is assigned the desired prediction, i.e., finding the most\nsimilar (distance-wise) instance of the selected class. This\nprocess optimises for the following properties:\nValidity guarantees that the counterfactual instance is clas-\nsified by the explained model with the desired class.\nSimilarity places the counterfactual instance as close as\npossible to the factual data point in the feature space\n(according to the chosen distance metric), making it\nhighly relevant and easy to reach.\nSparsity minimises the number of features changed be-\ntween the factual and counterfactual instances, enhanc-\ning human comprehensibility of the explanation.\nIn practice, however, this strategy often yields counter-\nfactuals that are perceptually closer to adversarial exam-\nples [Goodfellow et al., 2015] than meaningful explanations,\ni.e., the recommended change is either unrealistic or carries\nno intrinsic meaning [Freiesleben, 2022]. As a solution,\na collection of new social, technical and sociotechnical\ndesiderata was proposed [Pawelczyk et al., 2020; Delaney\net al., 2021; Schut et al., 2021; Guidotti, 2022]:\nPlausibility requires the counterfactual state to come from\nthe data manifold, thus be achievable in real life.\nConnectedness extends plausibility by ensuring that the\ncounterfactual instance is supported by (i.e., is close\nin the feature space to) another data point assigned to\nthe counterfactual class (with high probability) by the\nexplained model such that there exists a direct line (or a\nsequence of steps through known data points) between\nthe two that does not cross a decision boundary.\nDiscriminativeness makes the counterfactual instance un-\nambiguous, i.e., clearly distinguishable from similar\ninstances (refer back to similarity) that are not of the\ncounterfactual class, to avoid confusing humans with\nthe like of adversarial examples.\nRobustness prevents realistic data shifts and model\nchanges from invalidating counterfactual explanations.\nStability ensures that the neighbours of the explained data\npoint receive similar (or identical) counterfactuals.\nActionability guarantees that counterfactuals can be imple-\nmented in real life, as some features may be immutable,\ne.g., ethnicity, while others must obey monotonicity,\ne.g., age, simultaneously maintaining the congruity of\nthese changes given that they may be incompatible.\nNotably, all these properties \u2013 illustrated in Figure 1 per-\ntain to the counterfactual instance itself. Additional desider-\nata apply to collections of such explanations, used because\na single counterfactual is unlikely to satisfy the distinct\nneeds of different explainees [Sokol & Flach, 2020c]. In\nthis context, diversity ensures that a set of counterfactuals is\nrepresentative of the available explanations while remaining\nminimal. From the model selection perspective, counterfac-\ntual availability can also be considered [Kanamori et al.,\n2024]; it maximises the number of individuals for whom at\nleast one acceptable explanation can be generated, which is\nimportant when multiple models (from a single class) with\ncomparable performance exist [Sokol et al., 2024].\nA prominent extension of counterfactuals builds action se-\nquences guiding an explainee step-by-step to the chosen\noutcome [Poyiadzi et al., 2020]. Here, additional desider-\nata are introduced to formalise the properties of the path\nconnecting the factual and counterfactual instances. Feasi-\nbility entails building such a link by following pre-existing\ndata points, thus ensuring plausibility, connectedness and\nstability. Affinity, branching and divergence imbue these\npaths with spatial awareness by capturing the geometry\nof the feature space density [Sokol et al., 2023]. These\ndesiderata account for the order in which feature changes\nneed to be implemented, group counterfactuals based on\nthe (directional) similarity of their paths, and differentiate\nincompatible explanations, hence offer a richer perspective\non counterfactual diversity and actionability. These extra\nproperties are shown in Figure 2.\nAdhering to the desiderata listed above allows to retrieve\nstate-of-the-art counterfactuals without modelling the causal\nstructure of the data generating processes [Karimi et al.,\n2022], which in real life is often infeasible. Such expla-"}, {"title": "2.2. Uncertainty Quantification", "content": "In addition to transparency, other critical properties of data-\ndriven models \u2013 such as their fairness, robustness, reliability,\ntrustworthiness and accountability \u2013 have gained promi-\nnence with the proliferation of AI. The foundational aspect\nof these desiderata is predictive uncertainty given its crucial\nrole in enhancing AI systems' awareness of their limita-\ntions [H\u00fcllermeier & Waegeman, 2021]. Primarily, this in-\nvolves distinguishing between aleatoric uncertainty, which\narises due to the randomness inherent to the data generating\nprocess, and epistemic uncertainty, which stems from the\nlearning algorithm's ignorance of the true underlying model.\nAleatoric uncertainty is irreducible in the sense that more\nobservations of a given phenomenon will not improve our\nability to predict its outcome; a prototypical example is a\ncoin toss no matter how many data points we collect, we\ncannot reliably predict how the coin will land next given\nproperties such as actionability and plausibility can act as its proxy.\nthe intrinsic variability of this process. In certain cases,\nhowever, collecting more information (as opposed to more\nobservations), e.g., an extra feature, can help to alleviate\naleatoric uncertainty, e.g., by separating thus far overlap-\nping data points in the added dimension. Such an approach\nreduces aleatoric uncertainty often at the expense of its epis-\ntemic counterpart as fitting a model in higher dimensions\nis more difficult and requires a larger data set. Epistemic\nuncertainty, on the other hand, is reducible as additional\nobservations of a given phenomenon can help us to more\nreliably predict its outcome; revisiting the coin toss example,\nthe larger our set of observations is, the more precisely we\ncan estimate the coin's bias. Figure 3 demonstrates these\ntwo types of uncertainty for a two-dimensional toy example.\nMethods to reliably quantify aleatoric and epistemic un-\ncertainty, such as calibration, are still in early stages of\ndevelopment [Silva Filho et al., 2023]. By and large, we\nlack AI models that are truly uncertainty-aware, which cur-\ntails the use of this technology in high stakes domains. De-\nspite often being overlooked, uncertainty estimation is never\nassumption-free. It is influenced by the underlying, and\nsometimes implicit, data modelling assumptions, among\nwhich the model class choice is particularly consequential.\nNotably, the degree of predictive uncertainty is determined\nby the flexibility of the model class see Figure 4. More\nrestrictive families of functions result in lower uncertainty\nand vice versa, with those deemed universal approximators,\ne.g., neural networks, impacted the most [H\u00fcllermeier &\nWaegeman, 2021]. Other, model-specific, assumptions also\ninfluence uncertainty quantification. For example, linear\nmodels see Figure 4a \u2013 presuppose that the farther a data\npoint is placed from the decision boundary, the less uncer-\ntain its prediction is. This holds even if such an instance\ncomes from a sparse data region. While this property is\ninherent to linear models, its consequences may be unde-\nsirable and necessitate special handling, e.g., tweaking the"}, {"title": "2.3. Uncertainty and Counterfactual Explainability", "content": "Integrating uncertainty quantification with XAI has recently\ngained interest, albeit with much left to be explored [Bhatt\net al., 2021]. Notably, reliable uncertainty estimation re-\nmains a key challenge in building ante-hoc interpretable\nmodels [Rudin et al., 2022]. More broadly, explaining why\nan Al system is uncertain can help humans to understand\nits limitations and act to reduce its uncertainty; conversely,\npresenting the uncertainty of an explanation can improve\nhumans' ability to make responsible AI-aided decisions.\nIn this space, Antor\u00e1n et al. [2021] proposed a counterfac-\ntual explainer that shows how to reduce uncertainty of a\nprediction. Delaney et al. [2021] used uncertainty to eval-\nuate the quality of counterfactuals, expanding the validity\ndesideratum from crisp to probabilistic classification. To\nthis end, they proposed trust scores, which link (epistemic)\nuncertainty to counterfactual plausibility; others adapted\npre-existing outlier detection methods for this purpose [Ro-\nmashov et al., 2022]. Similarly, Thiagarajan et al. [2022]\ndesigned an explainer that generates high-confidence coun-\nterfactual data points by minimising their uncertainty.\nDuell et al. [2024] expanded the notion of counterfactual\nplausibility by computing uncertainty of the vector con-\nnecting the factual and counterfactual instances. Kanamori\net al. [2024] explored the implicit link between epistemic\nuncertainty and model multiplicity \u2013 a phenomenon where a\ngroup of models has comparable predictive performance de-\nspite intrinsic differences (see Figures 3b and 4), sometimes\ncalled the Rashomon effect of statistics [Breiman, 2001;\nRudin et al., 2024]; they did so in the context of counter-\nfactual robustness and availability. Schut et al. [2021] di-\nrectly coupled counterfactual explainability and uncertainty\nby connecting aleatoric and epistemic uncertainty respec-\ntively to counterfactual discriminativeness (which they call\nunambiguity) and plausibility (called realism) to generate\nhigh-quality explanations."}, {"title": "3. Building Bridges: Uncertainty-aware XAI", "content": "Despite latent connections between uncertainty and various\nforms of AI interpretability and explainability, (\u00a73.1) many\nsuch links remain unexplored with the underlying challenges\nunaddressed - XAI is thus largely uncertainty-unaware.\nThis section focuses on two critical gaps at the intersection\nof these fields. One, (\u00a73.2) we argue that (reliable) uncer-\ntainty quantification and ante-hoc interpretability are tightly\ncoupled and synergistic since the former reinforces AI trans-\nparency and accountability while the latter provides well-\ndefined model forms benefiting the former. Making ante-hoc\ninterpretable models uncertainty-aware additionally creates\na systematic pathway for adoption of human-centred devel-\nopments from across XAI, expanding the comprehensibil-\nity of such AI systems to non-technical stakeholders, e.g.,\nthrough counterfactual insights as we demonstrate next.\nTwo, (\u00a73.3) we argue that uncertainty quantification offers a\nprincipled unifying framework for generating state-of-the-\nart (non-causal) counterfactuals, superseding many ad-hoc\nproxies and criteria currently used to this end. Of particular\nrelevance are the distinction between aleatoric and epistemic\nuncertainty, which allows to overcome the limitations im-\nposed by employing its aggregate measure, and the proper-\nties of the entire path connecting factual and counterfactual\ninstances, rather than of the latter point alone. We finish this\nsection by (\u00a73.4) reviewing alternative perspectives."}, {"title": "3.1. Open Challenges", "content": "Section 2.3 showed that uncertainty has primarily been ap-\nplied, albeit often indirectly, to evaluate counterfactuals and,\ntoo a much lesser extent, guide their generation. Most ex-\nplainers overlook data modelling assumptions that are foun-\ndational to robust uncertainty quantification and use ad-hoc\nfixes when uncertainty estimates are unavailable. They em-\nploy proxies like operating within previously observed data"}, {"title": "3.2. Uncertainty Quantification and Ante-hoc\nInterpretability are Fundamentally Intertwined", "content": "Given their technological nature, AI models should not\nbe accepted based on the trust they engender, e.g., after\nprolonged interactions, but rather due to their operational\nreliability and robustness [Ryan, 2020]. Their sound techni-\ncal design and functioning are especially important in high\nstakes domains [Rudin, 2019]. These principles are embod-\nied by (ante-hoc) interpretability and uncertainty quantifica-\ntion, which allow humans to develop correct understanding\nof models' capabilities and limitations [Schut et al., 2021],\nleading to accountability and trust [Tomsett et al., 2020].\nWhile uncertainty is generally used to capture the quality of\nAI models and their predictions, it is important not to over-\nlook its decomposition into the aleatoric and epistemic parts\ngiven that they convey fundamentally distinct information.\nBased on our review of uncertainty and ante-hoc inter-\npretability, we posit that these concepts are not only comple-\nmentary [Schut et al., 2021] but rather constitute different\nviews of the same underlying idea \u2013 with the former being\nnecessary for a strong notion of the latter, and the latter en-\nhancing the former. While Rudin et al. [2022] have briefly\nnoted the link between these two concepts, there is much\nleft to be explored. Recall that ante-hoc interpretability\noffers inherently robust, accountable and transparent AI\nmodels by constraining their form [Rudin, 2019]. Speci-\nfying the modelling assumptions explicitly, in turn, allows\nfor reliable uncertainty quantification and decomposition\ngiven how sensitive these processes can be (see Figure 4).\nThis shared foundation is critical as post-hoc (and model-\nagnostic) uncertainty estimation can yield incorrect insights\ndue to its possible incompatibility with (implicit) data mod-\\elling assumptions \u2013 akin to how post-hoc explainers may\nnot truthfully capture the functioning of AI models.\nCurrent XAI research, nonetheless, often overlooks the qual-\nity (including predictive power) of a model as well as the\nimportance and implications of a model class choice; this is\nespecially true for post-hoc methods but also affects, albeit\nto a much lesser extent, ante-hoc interpretability. For ex-\nample, consider the fundamental differences in the decision\nboundary shape learnt by linear models and decision trees,\nand their impact on predictions and uncertainty estimates.\nFor the former, these properties rely largely on the distance\nfrom the decision boundary; whereas for the latter, the re-\nlation is more nuanced and depends on the hyper-rectangle\npartition of the feature space as well as the quantity and class\ndistribution of the training data used to learn it. Reducing\nepistemic uncertainty of models considered to be universal\napproximators is particularly challenging exactly because\nof their inherent expressiveness. Here, a popular solution is\nto introduce strong model form regularisation \u2013 an approach\nthat bears the hallmarks of ante-hoc interpretability.\nA slight modification to a model class \u2013 e.g., bounding the\nhyper-rectangles of a decision tree from all sides \u2013 could\nyield drastically different predictions and uncertainty esti-\nmates. Similarly, some model classes capture the training\ndata distribution, while other do not, leading to different\nhandling of instances located in sparse data regions. While\npost-processing techniques can enhance a particular model\nclass in this respect [Perello-Nieto et al., 2016], such an\napproach introduces asymmetry between the modelling as-\nsumptions and properties of the final predictions. If the latter\nare used in downstream tasks, this may be undesirable; more\nbroadly, it signals that another model class fulfilling the de-\nsired requirements ought to be considered instead [Rudin\net al., 2024]."}, {"title": "3.3. Uncertainty Quantification is a Unifying\nFramework for Counterfactual Explainability", "content": "Given the appeal of counterfactuals, improving their re-\ntrieval is crucial. We posit that uncertainty quantification is\nuniquely suited to this end and provides a principled unify-\ning framework for counterfactual explainability, subsuming\nthe desiderata listed in Section 2.1. Specifically, expressing\nthem through constraints on aleatoric and epistemic uncer-\ntainty, instead of its aggregate measure, and working with\nthe path-based conceptualisation of counterfactuals, rather\nthan the counterfactual instances alone, offer many benefits.\nChief among them is explanation consistency across models\n(whether from the same or different model classes) given\nthat modelling assumptions are directly accounted for in\nexplanation generation. This is more desirable than expla-\nnation consistency with respect to an explainer (applied to\ndistinct model classes) since its operational characteristics\nare unlikely to be compatible with a broad range of unique\nmodelling assumptions. Optimal, in terms of uncertainty,\nexplanations may thus necessarily be dissimilar in absolute\nterms across distinct modelling scenarios \u2013 e.g., different\ncounterfactual instances - but at the same time consistent\nwith regard to their high-level desiderata as shown in Fig-\nure 5. Consequently, building models that are uncertainty-\naware, reliable and robust because they are fundamentally\nsuitable for a particular application appears more important\nthan creating \"universal\u201d, thus overly complex, explainers.\nSimilarity & Sparsity These observations offer an alter-\nnative perspective on counterfactual similarity, allowing us\nto compare explanations to each other through their uncer-\ntainty in addition to their separation in the feature space as\nwell as distance from the explained instance. Otherwise,\nthe classic interpretation of counterfactual similarity and\nsparsity positions these properties as complementary to the\nnotion of uncertainty, hence they can be implemented inde-\npendently of it, e.g., by minimising the Manhattan and Eu-\nclidean distance metrics. For path-based explanations, these\ntwo properties should additionally account for the quantity\nand size of individual steps as well as the number of features\naffected by each segment; note that these characteristics can,\nto a degree, be reflected in uncertainty estimates, e.g., when\nthe data manifold shape captures feature (inter)dependence.\nValidity & Actionability More directly, aleatoric and\nepistemic uncertainty estimates offer a comprehensive per-\nspective on the validity of (instance-based) explanations.\nGoing beyond crisp classification allows to better differenti-\nate explanations, which can be especially insightful when\ndealing with more than two classes, where the interplay\nof their individual probabilities may be nuanced [Sokol\n& Flach, 2020b]. Furthermore, lack of validity due to\naleatoric uncertainty clearly communicates different infor-\nmation about an explanation than when the uncertainty is\nof epistemic nature. For example, qualifying the validity\nof a counterfactual though its actionability usually requires\nconfining it to features that are deemed mutable and ac-\ncounting for the directionality, monotonicity and rate of\ntheir change as well as between-feature compatibility and\nratio. However, uncertainty estimates \u2013 their epistemic com-\nponent in particular can offer useful insights with this\nregard. These validity and actionability perspectives can be\neasily extended to path-based counterfactuals by inspecting\nuncertainty of the points constituting their individual steps.\nA desirable uncertainty profile of a counterfactual path can\nthus be characterised by low epistemic uncertainty along its\nentirety, low aleatoric uncertainty towards its end, and de-\ncision boundary crossings in regions of increased aleatoric\nuncertainty or a balanced mixture of both uncertainty types.\nRobustness & Stability Quantifying the variability of\naleatoric and epistemic uncertainty within a hyper-sphere"}, {"title": "3.4. Alternative Views", "content": "Reliable uncertainty quantification is undoubtedly challeng-\ning, and in some cases even impossible. Using it as the\nfoundation of XAI may thus curtail the progress of this field,\nat least short-term. Embracing this perspective will also\nlikely require a fundamental shift in how explanations are\nformalised - e.g., they might become probabilistic [Chau\net al., 2023], which can be seen in Figure 4a for a coun-\nterfactual placed in the span of possible models (top-right)\nincreasing the technical complexity of XAI to the detri-\nment of lay explainees. With this in mind, for many (low\nstakes) domains, crisp (ante-hoc interpretable) modelling\nmay be sufficient, making uncertainty estimation redundant.\nHowever, simply considering it can catalyse important in-\nsights even if it is ultimately deemed unnecessary. When\nAl models are provided as inaccessible oracles, i.e., black\nboxes, our perspective is also inapplicable; here, uncertainty\nproxies and post-hoc explainability are largely unavoidable.\nOur strong focus on ante-hoc interpretability is rooted in\nits rich history of success and reported superiority for struc-\ntured data; while the latter claim can be challenged [Wang\n& Lin, 2021], dismissing inherently transparent AI alto-\ngether without further evidence seems premature. One can\nadditionally argue that using auxiliary explanatory mecha-\nnisms to inspect ante-hoc interpretable models inadvertently\nbreaks this property. But in our case this position can be\neasily refuted since the focus remains on reliable data mod-\\elling, with counterfactuals simply being a by-product of\naccess to uncertainty estimates. Crucially, counterfactual\nexplainability can be viewed through frameworks other than\nuncertainty quantification, most notably causality. While\nthis perspective promises more reliable explanations and is\ncompatible with ante-hoc interpretability, causal modelling\nis often unrealistic in practice as mentioned earlier."}, {"title": "4. Conclusion and Future Work", "content": "In this paper we argued that uncertainty quantification\nand ante-hoc interpretability are fundamentally intertwined;\nrecognising this synergistic relation promises to benefit com-\nprehensibility of predictive models as well as improve their\naleatoric and epistemic uncertainty. We also asserted that\nuncertainty quantification offers a sound unifying framework\nfor counterfactual explainability; this connection facilitates\nrigorous generation of reliable counterfactuals for different\nmodel classes. Consequently, we observed that counter-\nfactual explainers tend to be overcomplicated since they\nattempt to compensate for various shortcomings of AI mod-\nels. Grounding these tools in uncertainty could allow them\nto remain relatively simple, instead shifting the engineering\nefforts towards building more principled predictive mod-\nels that are ante-hoc interpretable and uncertainty-aware.\nTogether, our two tenets opened a pathway for bringing\nhuman-centred explanatory insights, like counterfactuals,\nto ante-hoc interpretable models, whose comprehensibility\nhas thus far been largely limited to technical experts.\nGiven their prevalence, this paper focused on counter-\nfactuals, nonetheless in future work we will expand our\nuncertainty-centred perspective to other explanation types.\nSince our approach is premised on access to reliable\naleatoric and epistemic uncertainty estimates, we will also\ninvestigate latest uncertainty quantification and probabil-\nity calibration methods, focusing on ante-hoc interpretable\nmodels given their strong connection to this topic. We addi-"}]}