{"title": "Prompt-Driven Continual Graph Learning", "authors": ["Qi Wang", "Tianfei Zhou", "Ye Yuan", "Rui Mao"], "abstract": "Continual Graph Learning (CGL), which aims to accommodate new tasks over evolving graph data without forgetting prior knowledge, is garnering significant research interest. Mainstream solutions adopt the memory replay-based idea, i.e., caching representative data from earlier tasks for retraining the graph model. However, this strategy struggles with scalability issues for constantly evolving graphs and raises concerns regarding data privacy. Inspired by recent advancements in the prompt-based learning paradigm, this paper introduces a novel prompt-driven continual graph learning (PROMPTCGL) framework, which learns a separate prompt for each incoming task and maintains the underlying graph neural network model fixed. In this way, PROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous tasks. More specifically, we propose hierarchical prompting to instruct the model from both feature- and topology-level to fully address the variability of task graphs in dynamic continual learning. Additionally, we develop a personalized prompt generator to generate tailored prompts for each graph node while minimizing the number of prompts needed, leading to constant memory consumption regardless of the graph scale. Extensive experiments on four benchmarks show that PROMPTCGL achieves superior performance against existing CGL approaches while significantly reducing memory consumption. Our code is available at https://github.com/QiWang98/PromptCGL.", "sections": [{"title": "I. INTRODUCTION", "content": "Graphs are prevalent in numerous real-world applications, including social networks, biochemistry, and recommendation systems [1]\u2013[4]. Consequently, Graph Neural Networks (GNNs) have emerged as powerful tools for processing graph-structured data [1], [5]\u2013[7]. However, traditional GNN methodologies typically assume static graph structures, which fail to capture the dynamic nature of the real world where graphs evolve continuously [8]\u2013[10]. For instance, citation networks continually expand with the publication of new research papers, and co-purchasing networks grow as new categories of products are introduced. This necessitates models that can efficiently incorporate the features and topological information of new graphs in a continuous manner.\nDue to the limitations in time overhead and computational resources, retraining the GNN models on entire datasets is impractical. Continual Graph Learning (CGL) thus emerges as a crucial paradigm to address the challenges posed by evolving graphs in the real world [11]\u2013[14]. Recent advancements in CGL can be broadly categorized into regularization [8], architectural design [15], [16], and memory replay-based methods [17]\u2013[19]. Among these, replay-based methods have shown state-of-the-art performance by storing sampled graphs in a memory buffer and replaying previous data while learning new tasks, as illustrated in Fig. 1 (a). Despite their effectiveness, replay-based CGL methods encounter two significant limitations. First, these methods demand substantial memory resources to store historical data, leading to performance degradation as buffer sizes decrease [20], [21], as demonstrated in Fig. 1 (b) and (c). Second, the storage of historical node information raises privacy concerns, especially in contexts involving sensitive data, such as purchase records in co-purchasing networks [22]. These limitations indicate that simply buffering past data and retraining the model is not the optimal approach for CGL.\nWith the success of foundation models, prompt learning has emerged as a key approach for transfer learning in large models [23], [24]. It shifts the focus from directly tuning model weights to designing prompts that effectively instruct the model to perform specific tasks while keeping the number of parameters fixed [25], [26]. Recently, significant advancements in prompt learning for natural language processing and"}, {"title": "II. RELATED WORK", "content": "Continual Graph Learning (CGL) aims to address the chal-lenge of learning from a stream of graph-structured data over time while mitigating catastrophic forgetting. Current CGL methods can be broadly categorized into three main strategies: regularization methods [8], architectural design methods [15], [16], and memory replay-based methods [17]\u2013[21]. Regular-ization methods focus on preventing catastrophic forgetting by adding constraints that help preserve knowledge across differ-ent learning tasks. Notable methods include TWP [8], which integrates topological information to retain learned features when adapting to new tasks. However, these methods often compromise the model's capacity to adapt efficiently to novel tasks, as the regularization can interfere with learning new knowledge. Architectural design approaches involve changes to the structure of the model to enhance its ability to learn and retain knowledge over time. For instance, HPNs [15] introduce atomic feature extractors and hierarchical systems that scale dynamically to accommodate new knowledge. This approach increases model parameters and memory requirements as new tasks are added. Other architectural approaches focus on spar-sity and modularization, which balance the trade-off between task retention and model expansion [16]. Memory replay-based methods are perhaps the most effective and widely studied, as they maintain a memory buffer to store data from previous tasks, which is replayed to mitigate forgetting while learning new tasks. A representative method is ER-GNN [17], where experience replay involves storing sampled nodes and replaying them while learning new tasks. Other methods, such as SSM [18] and CaT [20], improve the efficiency of replay by using sparsified subgraphs or condensed graph modules to reduce replay memory overhead. Despite these advances, the scalability of replay-based methods is limited by increasing memory requirements, especially as the graph scale increases. In contrast to memory replay-based approaches, which typically store graph nodes at a ratio of 0.01, our approach introduces a novel method that significantly reduces memory overhead. By preserving only a small number of task-specific prompts (just 2 or 3), we enable the model to effectively learn the sequential tasks without incurring the memory overhead typical of traditional replay-based methods. Our approach achieves SOTA performance while minimizing memory costs, and addressing the scalability limitations of current CGL techniques.\nRecently, prompt learning has emerged as a powerful tech-nique in machine learning, particularly in natural language processing (NLP) and computer vision (CV). This approach has gained prominence due to its ability to adapt pre-trained large models to new tasks with minimal retraining, making it highly efficient for transfer learning scenarios [26], [32], [34]\u2013[37]. In NLP, prompt learning can be divided into two major categories: hard prompts and soft prompts. Hard prompts are manually crafted text additions, such as those used in PET-SGLUE [32], where predefined templates are used to"}, {"title": "III. METHODOLOGY", "content": "A typical setup for CGL involves training the model on multiple tasks with non-overlapping classes that arrive se-quentially. Denote n tasks as T = {To, T\u2081,...,Tn} and corresponding sequence of datasets as D = {D1, D2, ..., Dn}. In this continual learning paradigm, the model only has access to the dataset Dt of the current task Tt, while datasets from"}, {"title": "A. Problem Definition"}, {"title": "B. Prompt Driven Continual Graph Learning", "content": "Fig. 2 shows the framework of our PROMPTCGL. Our model consists of three main parts: a backbone ge, consisting of a multilayer GNN for feature extraction, a prediction layer f4 for performing classification tasks, and the prompts P. During training, we first pre-train the backbone ge and the prediction layer fo on the initial task To without prompts and then freeze the backbone parameters in subsequent tasks to ensure model stability and consistency. The frozen backbone serves as a feature extractor for obtaining node representations with topological information in later tasks. The core of PROMPTCGL is to find the optimal prompts P for each task by solving the following objective:\n\n$\\max E_{(X_0,A,y)\\sim T} [P(y|f_{\\theta}(g_{\\theta}(X_0, A, P)))],$   (1)\n\nwhere Xo \u2208 RN\u00d7df indicates the node features, A \u2208 RNXN is the adjacency matrix, and P consists of node-level prompts Pn and subgraph-level prompts Ps, both in Rkxdf and composed of k independent prompt vectors, where df denotes the feature dimension. In the continual learning process, only the parameters of fo and prompts P are learnable. Upon completion of each task, we save the prompts into a prompt bank, which would be retrieved at inference time. Next, we present two core techniques of PROMPTCGL, including hierarchical prompting (HP) and a personalized prompt generator (PG).\nIn a continual learning setting, task graphs consist of nodes with non-overlapping classes, leading to significant differences in both features and topology structures between the target and initial task graphs. For exam-ple, in the case of social networks representing distinct interest groups, one graph may correspond to a community centered on entertainment, while another could represent a group focused on computer science. These communities differ not only in their feature distributions\u2014shaped by the distinct interests and behaviors of their members\u2014but also in their topologies. An entertainment-based community might form tightly-knit clus-ters, while a technology-focused group could exhibit a more dispersed and expansive network. Such disparities in both feature space and network structure complicate knowledge transfer across tasks. To address these challenges, we propose a hierarchical prompting strategy, which involves node-level prompts to address feature discrepancies and subgraph-level prompts to handle structural variations.\nFor node-level prompts, we generate personalized prompts for each node based on the initial node features Xo and the maintained node-level prompts Pn using the PG component."}, {"title": "2) Hierarchical Prompting:", "content": "These personalized prompts are then added to the node fea-tures to obtain the prompted node features X.\n\n$X' = X + PG(X_0, P_n),$  (2)\n\nwhere PG denotes the personalized prompt generator, whose output shares the same dimensions as node features X0. For subgraph-level prompts, we first encode the prompted node features and the relationships between neighboring nodes using the frozen first-layer GNN, resulting in node represen-tations that incorporate topological information X1:\n\n$X_1 = GNN_1(X', A),$  (3)\n\nwhere GNN1 is the first-layer GNN of the backbone, X1 \u2208 RNxdh, and dh is the hidden dimension. Then following a similar procedure to the node-level prompts, we generate the prompted node representations X \u2208 Rkxdh by utilizing subgraph-level prompts Ps and the node representations with topological information X1:\n\n$X' = X_1 + PG(X_1, P_s),$  (4)\n\nThe resulting prompted node representations, X1, are then passed on to the subsequent layers of the network for further processing."}, {"title": "3) Personalized Prompt Generator:", "content": "The inherent hetero-geneity of nodes implies that using a uniform prompt for all nodes leads to inefficiency and poor performance. Although customizing unique prompts for each node is ideal, this approach would significantly increase memory consumption, particularly as the graph scales. To mitigate this, we propose a personalized prompt generator, as illustrated in Fig. 3. This generator utilizes the unique representation of each node and a query matrix to derive personalized prompts. Specifically, for node i, the personalized prompt p is generated by dynamically aggregating the maintained prompts based on the query result of its representation xi:\n\n$P'_i = \\sum_{j=1}^{k} \\alpha_j P_j \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\alpha = Softmax(Qx_i),$  (5)\n\nwhere Q is a query matrix used to compute importance weights \u03b1 of each prompt to the node i. These weights, which differ across nodes, enable the creation of personalized prompts through tailored aggregation. To enhance model stability and improve computational and storage efficiency, we decompose the query matrix into two"}, {"title": "C. Algorithm", "content": "In order to present our method more systematically, the process of training PromptCGL is outlined in Algorithm 1.\nThe algorithm takes as input the dataset D, a pre-trained backbone ge, and a prediction layer f4. It initializes a memory bank P to store the learned prompts (Line 2). For each task D\u2081 in the dataset, node-level prompts Pin and subgraph-level prompts Pi,s are randomly initialized (Line 4). The node-level prompts are then personalized for each node in the graph using Eq. (5) and Eq. (6) (Line 5), and these prompts are added to the node features to generate the prompted node features X (Line 6). These features are passed through the backbone's first layer to compute topologically enriched node representations X1 (Line 7). Next, subgraph-level prompts Pi,s are personalized using the same mechanism as the node-level prompts (Line 8), and the updated node representations are computed (Line 9). The prompts and the prediction layer parameters are updated using the loss function defined in Eq. (7) (Line 10). Finally, the learned prompts Pin and Pis are stored in the memory bank for future retrieval (Line 11). This process enables the model to adapt to new tasks while preserving knowledge from previous tasks, ensuring efficient and scalable continual graph learning. At the end of the process, the memory bank P, containing all task-specific prompts, is returned (Line 12)."}, {"title": "Q = uv.", "content": "(6)"}, {"title": "4) Learning Objective:", "content": "For the current tasks, both the prompts and node features Xo are input into the backbone network g, followed by a predictive layer to generate the final outputs. The model is trained end-to-end using the following loss function:\n\n$\\min_{\\Phi, P_\\eta, P_\\xi} L(f_\\theta (g_\\theta (X_0, A, P_n, P_s))), X\\in D_t,$ (7)\n\nwhere L denotes the cross-entropy loss.\nTo further mitigate the forgetting ratio, a smaller learning rate is applied to the predictive layer compared to the learning rates used for the HP and PG components during training. This adjustment is necessary because, in CGL, the predictive layer is shared across all tasks. A lower learning rate ensures the stability and generalization of this layer over multiple tasks. Upon completion of the current task, the parameters associated with the learned prompts are stored in the prompt bank for future inference. The training process of PROMPTCGL is comprehensively outlined in Algorithm 1, providing a detailed overview of the method."}, {"title": "5) Inference:", "content": "For the i-th inference task, the HP and PG are initialized by retrieving the corresponding prompt parameters for the i-th task from the prompt bank based on the task identifier. Inference is then conducted to obtain the predicted result \u0177 as follows:\n\n$\\hat{y} = f_\\theta (g_\\theta (X_0, A, P^i)),$   (8)\n\nwhere Pi = (Ps,Pn) represents the prompt parameters associated with the i-th task."}, {"title": "D. Discussion", "content": "PROMPTCGL requires only a small number of prompts and low-order query vectors, and these quantities remain constant regardless of the graph scale. Specifically, the space complex-ity for maintaining two levels of prompts is O(k(df + dh)), where df and dh are the dimensions of the feature and hidden layers, respectively. The space complexity for low-order query vectors is O(df + dh), leading to a total space complexity of O(kd). Experiments in TABLE II confirm that the model achieves SOTA performance with k set to just 3. In contrast, replay-based methods require sampling based on a ratio krate, resulting in a space complexity of O(krate \u00b7 N. d), i.e., O(Nd). This linear relationship with graph scale means replay-based methods demand significantly more memory for large-scale graphs, whereas our method maintains minimal memory consumption.\nFurthermore, PROMPTCGL demonstrates lower training costs compared to regularization and replay-based methods. Unlike regularization techniques, which require the computa-tion of an additional loss function, and replay-based methods, which necessitate retraining all parameters and the inclusion of extra sampling modules, our approach focuses solely on fine-tuning the parameters associated with prompts and prediction layers. This streamlined process results in significantly reduced computational overhead.\nAdditionally, Our method offers superior privacy-preserving capabilities over replay-based methods by storing prompts instead of historical data, providing a more secure and reliable solution for integrating with GNNs in sensitive scenarios."}, {"title": "IV. EXPERIMENTS", "content": "Following [11], we conduct extensive experiments on four public datasets: CoraFull [45], OGB-Arxiv [46], Reddit [47], and OGB-Products [46]. TABLE I shows their detailed statis-tics. For all datasets, the data of each class is divided into 60% for training, 20% for validation, and the remaining 20% for testing, and each task includes data from two classes.\nThe detailed descriptions of these datasets are as follows:\n1) CoraFull [45] is an extension of the well-known Cora dataset. Nodes represent scientific publications and edges represent citation links between them.\n2) OGB-Arxiv [46] is part of the Open Graph Benchmark (OGB). Nodes represent Computer Science arXiv papers, and edges denote citation relationships. Each node is associated with a 128-dimensional feature vector derived from the paper's title and abstract.\n3) Reddit [47] is a graph dataset where nodes correspond to posts in the Reddit social network, and edges represent interactions between these posts.\n4) OGB-Products [46] is another dataset from the OGB. Nodes represent products sold on Amazon, and edges indicate that two products are frequently bought together."}, {"title": "A. Datasets"}, {"title": "B. Baselines", "content": "For a comprehensive assessment, PROMPTCGL is con-trasted with state-of-the-art baselines. These encompass four traditional continual learning methods: EWC [48], MAS [49], GEM [50], and LwF [51], along with five CGL methods: TWP [8], HPNs [15], ER-GNN [17], SSM [18], and CaT [20]. Additionally, two baselines are established: Bare, a lower-bound baseline that is directly fine-tuned on the task sequence without CGL techniques, and Joint, an ideal upper-bound baseline that utilizes data from all historical tasks during new task learning. The details of the baseline continual learning methods are as follows:\n1) Bare is fine-tuned directly on the task sequence without any CGL methods to avoid forgetting, therefore we regard it as a lower bound on continual graph learning performance.\n2) EWC [48] protects important weights for previously learned tasks by penalizing their updates during new task learning.\n3) MAS [49] introduces a memory module for slowing down the updating of important parameters by measuring their importance based on sensitivity to their prediction.\n4) GEM [50] stores representative data in episodic memory and modifies the gradients using the informative data stored in memory.\n5) LwF [51] preserves the knowledge of the model in the new model through knowledge distillation.\n6) TWP [8] preserves the topological information of previ-ous tasks by regularisation penalties.\n7) HPNs [15] use atomic feature extractors and a hierarchi-cal prototype system for continual learning.\n8) ER-GNN [17] integrates memory-replay to GNNs by sampling the informative nodes from the previous task graph into the memory buffer.\n9) SSM [18] stores the sparsified previous task graph in the memory buffer for replay.\n10) CaT [20] stores the condensed previous task graph in the memory buffer and utilizes a train in the memory scheme to update the model.\n11) Joint utilizes entire data of all historical tasks to update the model when learning new tasks, therefore we regard it as an upper bound on continual graph learning perfor-mance."}, {"title": "B. Baselines"}, {"title": "C. Evaluation Metrics", "content": "Following the methodology in [11] and [20], we eval-uate model performance using two metrics: average per-formance (AP) and average forgetting (AF). Given a se-quence of T tasks, the accuracy of the model on the q-th task after learning the p-th task is denoted as mp,q. The set mr,q|q = 0, 1, . . ., T \u2013 1 represents the accuracy of each learned task after completing the entire sequence of tasks. Formally, the definitions for AP and AF are as follows:\n\n$AP = \\frac{\\sum_{q=1}^{T} M_{T,q}}{T}$  \n\n$AF = \\frac{\\sum_{q=1}^{T-1} (M_{T,q} - m_{q,q})}{T-1}$(9)\n\nWe utilize AP to assess the overall performance across all tasks at the end of the task sequence. AP is considered a more critical metric compared to AF, which quantifies the extent of forgetting during the continual learning process."}, {"title": "D. Implementation", "content": "Following [18] and [20], we employ a two-layer GCN [33] as the backbone for all models, except for TWP, which uses GAT [52] due to its attention-based mechanism for assessing topological importance. The hidden layer dimension for all GNNs is set to 32. During the pre-training phase, we use the Adam optimizer with a learning rate of 0.001 and a weight decay of 5e-4. In the continual learning phase, we apply different learning rates: 5e-4 without weight decay for the prediction layer, and 0.01 with a weight decay of 5e-4 for the prompts.\nAll experimental results are reported as the mean and stan-dard deviation across three independent runs. The experiments were conducted on a machine equipped with two Tesla T4 GPUs, each with 16 GB of memory, and an Intel(R) Xeon(R) Silver 4210 CPU @ 2.20 GHz."}, {"title": "E. Analysis of Main Results", "content": "Performance Comparison: Table II presents a compara-tive analysis across four benchmark datasets. For the replay-based methods, we conducted experiments under two distinct scenarios, ensuring that memory consumption was aligned with PROMPTCGL for a fair comparison. In particular, the setting in TABLE II shows under what circumstances the memory consumption of storing prompts in our method equals that of storing nodes in the replay-based methods.\nThe results demonstrate that PROMPTCGL consistently achieves SOTA performance across all benchmarks with mini-mal memory consumption. Notably, PROMPTCGL reaches the ideal upper bound of joint training performance on all datasets with only three prompts. On the Arxiv dataset, PROMPTCGL even surpasses the joint training method, showing that the prompts in our approach can push model performance beyond traditional upper bounds. Although the AF for PROMPTCGL on the Arxiv and Products datasets are not optimal-0.1% and 0.2%, respectively\u2014these minor rates have negligible impact on the overall model performance. Furthermore, PROMPTCGL demonstrates robust performance with just two prompts, achieving the highest average accuracy across all datasets while keeping average forgetting ratios consistently below 1%. This highlights PROMPTCGL's ability to adapt to new tasks while effectively preserving historical knowledge, thus mitigating catastrophic forgetting.\nIn contrast, other baseline methods are unable to match the performance of PROMPTCGL. Regularization techniques such as EWC, MAS, GEM, and TWP, along with distillation-based approaches like LwF, impose additional constraints on the model while learning new tasks to mitigate forgetting. However, these constraints often limit the model's plasticity, resulting in suboptimal performance on new tasks despite some methods achieving low forgetting ratios. Replay-based methods, as demonstrated in previous studies [18], [20], can approach joint training performance with a storage ratio of krate = 0.01. Nonetheless, this level of memory consumption is prohibitively high for large-scale graphs. When replay-based methods are compelled to minimize memory usage by retaining only minimal historical information, they experience catastrophic forgetting, as shown in Table II. Under these constrained conditions, their performance deteriorates below that of regularization methods. Consequently, replay-based approaches are unsuitable for large-scale graphs, where mem-ory constraints are more stringent. In contrast, PROMPTCGL sustains robust performance with a low memory footprint, effectively mitigating catastrophic forgetting without the ne-cessity for extensive memory buffers, thereby demonstrating greater scalability for large-scale graph tasks."}, {"title": "1)", "content": "Furthermore, on the Arxiv dataset, our method's performance matrix appears slightly brighter than that of the Joint model. This observation suggests that our prompt-based approach not only matches but can also enhance the theoret-ical performance upper bound, demonstrating its efficacy in improving continual learning outcomes."}, {"title": "2) Visualization of Performance Matrices:"}, {"title": "F. Diagnostic Experiments", "content": "PROMPTCGL consists of two key components: Personalized Prompt Generator (PG) and"}, {"title": "1) Key Component Analysis:"}, {"title": "III. According", "content": "the resulting data. As shown in TABLE V, the data is organized under the key parameters with the corresponding results for each analysis."}, {"title": "3) to TABLE", "content": "III, and the subsequent sections, the specific data values are presented along with the corresponding conclusions for the performance. It also enables the model to more effectively navigate the diverse features and topological context. As the result shows in the table."}, {"title": "V. CONCLUSION", "content": "This paper presents PROMPTCGL, a novel framework designed to tackle memory consumption and data privacy challenges in CGL. For the first time, PROMPTCGL incorpo-rates graph prompt learning into CGL, employing hierarchical prompting to instruct the model through features and topolo-gies to address the variability of task graphs in CGL. Our personalized prompt generator generates tailored prompts for each node while reducing spatial complexity from O(N\u00b7d) to O(kd), demonstrating optimal performance with k = 3. Extensive experiments show that our method achieves SOTA performance while effectively minimizing memory usage and safeguarding data privacy."}]}