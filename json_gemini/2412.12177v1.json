{"title": "MODEL-DIFF: A TOOL FOR COMPARATIVE STUDY OF LANGUAGE MODELS IN THE INPUT SPACE", "authors": ["Weitang Liu", "Yuelei Li", "Ying Wai Li", "Zihan Wang", "Jingbo Shang"], "abstract": "Comparing two (large) language models (LMs) side-by-side and pinpointing their prediction similarities and differences on the same set of inputs are crucial in many real-world scenarios, e.g., one can test if a licensed model was potentially plagiarized by another. Traditional analysis compares the LMs' outputs on some benchmark datasets, which only cover a limited number of inputs of designed perspectives for the intended applications. The benchmark datasets cannot prepare data to cover the test cases from unforeseen perspectives which can help us understand differences between models unbiasedly. In this paper, we propose a new model comparative analysis setting that considers a large input space where brute-force enumeration would be infeasible. The input space can be simply defined as all token sequences that a LM would produce low perplexity on we follow this definition in the paper as it would produce the most human-understandable inputs. We propose a novel framework Model-diff that uses text generation by sampling and deweights the histogram of sampling statistics to estimate prediction differences between two LMs in this input space efficiently and unbiasedly. Model-diff achieves this by drawing and counting the inputs at each prediction difference value in negative log-likelihood. Experiments reveal for the first time the quantitative prediction differences between LMs in a large input space, potentially facilitating the model analysis for applications such as model plagiarism.", "sections": [{"title": "INTRODUCTION", "content": "It is crucial in many real-world scenarios to compare two (large) language models (LMs) side-by-side and pinpoint their prediction differences. For example, the prediction differences may help identify which model agrees more with human annotations (Liu et al., 2020; Hendrycks & Gim-pel, 2016; Hendrycks et al., 2019; Hsu et al., 2020; Lee et al., 2017; 2018; Liang et al., 2018; Mohseni et al., 2020; Ren et al., 2019; Szegedy et al., 2013; Rozsa et al., 2016; Miyato et al., 2018; Kurakin et al., 2016; Xie et al., 2019; Madry et al., 2017); it can also identify model plagia-rism between a licensed open-sourced model and its small variate version whose weights are added small noise (PrimerYang). Traditional analysis compares the LMs' outputs on the same benchmark datasets which only cover a limited number of inputs from their designed perspectives. However, (large) LMs have recently been deployed online and widely accessible to the public. As users in principle can input any types of data that could potentially cause the models to behave unexpectedly, it could be beneficial if we can compare models by a large amount of data without favoring the designed perspectives. The challenge of using benchmark datasets in this case is twofold: (1) the tested perspectives are limited by the types of test sets, and (2) the variety of inputs from the same perspective is limited by the dataset size.\nIn this paper, we propose to compare models with a large number of data, or more generally, in a discrete input space that is finite but computationally impossible to enumerate all inputs. While the combination of all token sequences is a straightforward space, it is not ideal as most of them are sequences with random tokens, hence not beneficial for analysis. One reasonable input space is the collection of human-understandable inputs. It can be defined as all token sequences that an LM produces negative log-likelihood (NLL) within a range of small values. Here we do not treat models"}, {"title": "THE MODEL-DIFF FRAMEWORK", "content": "Model-diff leverages the output distribution at each prediction difference D and the corresponding inputs mapped to D to analyze the types and count of the inputs at each D value. We first introduce the concept of output distribution and then how we use it for prediction difference analysis in an ideal case when enumeration of the input space is available. Next section we will discuss how to derive the quantities needed for this analysis when enumeration is replaced by sampling.\nBackground: Output distribution. Given the entire discrete input space $\\Omega = \\{0, ..., M\\}^N$ and a training set $\\Omega_\\tau \\subseteq \\Omega$, a model $f(x)$ learns to map inputs $x \\in \\Omega_\\tau$ to output $z \\in \\mathbb{R}$. As the current language models (LMs) are trained to predict the next token, we choose the loss function, negative log-likelihood (NLL), as the output. Later we also define output distribution for the parameter of prediction difference D. Each input x is a sequence of N tokens. M + 1 is the vocabulary size. Each of the N tokens takes one of the M + 1 words. The output distribution in an input space $\\Omega^*$ is the distribution of the count for each z. $\\Omega^*$ can be $\\Omega$ or some other space $\\Omega_M$ specified by a generative model M. As every input in $\\Omega^*$ holds equal importance for analysis, the inputs within $\\Omega^*$ should follow the principle of equal a priori probabilities \u2013 each input within $\\Omega^*$ follows a uniform distribution. Mathematically, the output distribution p(z) is defined as:\n$p(z) = \\sum_{x \\in \\Omega^*} \\delta(z - f(x))$,\nwhere $\\delta(\\cdot)$ is 1 if the input $z - f(x) = 0$, or $\\delta(\\cdot)$ is 0 otherwise. In practice, a histogram is used to collect the statistics (the y-axis is the count and the x-axis is the output values z). The sampled inputs with similar output values in a small range $[z - \\Delta z, z + \\Delta z)$ are called representative inputs at z and are mapped to the same z bin. $\\Delta z$ is a small positive constant."}, {"title": "MODEL-DIFF FRAMEWORK", "content": "Introductory example and goal. Assume we have infinite computing power to enumerate the inputs in a large input space to get the ground truth statistics. Because the inputs with very low NLL are repetitive sequences that are not understandable by humans (Holtzman et al., 2019) whereas inputs with (slightly) higher NLL are human understandable, we avoid the input space that favors the inputs with very low NLL or contains the inputs of with a specific NLL. Instead, we flexibly define a range of low (NLL) output values $Z = [z_-, z_+]$ and treat the inputs whose output values in Z as equally important. Thus, our comparative analysis is w.r.t. the chosen Z. In Fig 1 (b), there is an input space $\\Omega$ with a large number of inputs where model $M_A$ maps 5 inputs to outputs within Z and Model $M_B$ maps 6 inputs within Z. These 5 (and 6) inputs mapped by $M_A$ ($M_B$) form a set called $X_A$ ($X_B$). Some of the input(s) from $X_A$ may be predicted with higher (or lower) z by $M_B$ than $M_A$ predicts, such as the circled input (O). Model-diff's comparative analysis aims to find the types and counts of these inputs that are predicted with different output values by the two models.\nComparative analysis with $p_{A \\rightarrow B}(D)$. Define $A \\rightarrow B$ as the representative inputs $X_A$ from model $M_A$ are evaluated by model $M_B$, and $B \\rightarrow A$ is vice versa. It is more convenient to consider the prediction relation in Fig. 1(c) in terms of D. In Fig. 1(d), Model-diff uses the output distributions"}, {"title": "EFFICIENT AND UNBIASED SAMPLING IN MODEL-DIFF", "content": "In reality, enumeration is impossible because of computation inefficiency. We need to estimate the key distributions $p_{A \\rightarrow B}(D)$ and $p_{B \\rightarrow A}(D)$ by sampling. We first introduce the background of text generation by sampling, and the method(s) to sample the output distribution. We then discuss how to acquire comparable $p_{A \\rightarrow B}(D)$ and $p_{B \\rightarrow A}(D)$ through output distribution and normalization.\nBACKGROUND AND TERMINOLOGY\nText Generation by Sampling. Besides generating the next token in an autoregressive manner, sampling methods are common in text generation in language models (Kumar et al., 2022; Qin et al., 2022), by Markov Chain Monte-Carlo (MCMC). It starts with a sequence of random tokens and by tweaking the tokens randomly to lower the NLLs, a sequence of understandable text is generated. MCMC sampling is employed because enumeration of the input space in general is not possible. As pointed out (Du et al., 2023), text generation by sampling in principle should employ samplers of discrete input space (Goshvadi et al., 2024; Grathwohl et al., 2021; Zhang et al., 2022). These samplers sample the target distribution\n$p(x) \\propto \\exp(g(x)/T)$,\nwhere g(\u00b7) is called (negative) \u201cenergy\u201d and T is a predefined parameter (temperature). When T is 1, g(\u00b7) is the log-probability which is popular in many machine learning problems when they learn to model log-probability (LeCun et al., 2006). p(x) in Equ. 6 is a common target distribution for sampling in machine learning. Importantly, it biases the inputs with high g(x).\nModel-diff adopts the exact same sampling setting of discrete inputs and this is the major bottleneck of Model-diff. The time complexity of Model-diff is therefore similar to text generation by sampling. Post-processing of Model-diff after text generation by sampling only takes a few hours.\nSampling the output distribution. Parallel Tempering and Histogram Reweighting (PTHR) (Hukushima & Nemoto, 1996; Swendsen & Wang, 1986) is commonly used to sample output distribution. It starts with the results of text generation by sampling for target distribution of Equ. 6. Because the MCMC samplers sample x more often whose output g(x) is larger, it needs reweights the sampled distributions by exp(\u00b7) to acquire the output distribution. Therefore, sampling output distribution can generate the same statistics as if we were sampling uniformly the input space without biasing the inputs with large g(x). Moreover, PTHR is a downstream task of text generation by sampling and it is compatible with MCMC samplers. Therefore it can take advantage of the development of MCMC samplers that follow the same target distribution of Equ. 6."}, {"title": "SAMPLING WITH PROBABILITY WEIGHTS OF $p_A(z)$ OR $p_B(z)$", "content": "For very large input space, exact values of $p_{A \\rightarrow B}(D)$ and $p_{B \\rightarrow A}(D)$ cannot be estimated because $X_A$ and $X_B$ (Sec. 2.1 Introductory example and goal) are not available as enumeration of the input space is infeasible. Thus, we need to estimate them by sampling. We denote the sampled quan-tity used for practical analysis with \u201cTilde\u201d (e.g. $\\tilde{p}$) in contrast to the quantity from ground truth enumeration without \u201cTilde\u201d (e.g. p) for conceptual discussion purposes.\nAs mentioned in Sec. 2.1, we focus on the case where every input whose outputs within Z as equally important; therefore the outputs that contain more inputs should be sampled more often. Text gen-eration by sampling is not directly applicable because it favors low NLL. We instead leverage the output distribution $p_A(z)$ (or $p_B(z)$) that describes the various numbers of inputs mapped to each output value by a model. For example, as shown in Fig. 1 (b), one output value of model B has 4 inputs, and should be selected 2 times more frequently than the other output value with only 2 inputs. Output distribution $p_A(z)$ (or $p_B(z)$) ensures the sampled representative inputs follow the frequency of appearance for the different output values in Z for model A (or B) result in a sampling process as if we were uniformly extracting inputs from $X_A$ and $X_B$.\nIn practice, we apply the well-established algorithms of text generation by sampling and PTHR. After text generation by sampling, we compute $\\tilde{p}_A(z)$ and $\\tilde{p}_B(z)$ that approximate $p_A(z)$ and $p_B(z)$ through PTHR. We then sample an output value z with probability weights $\\tilde{p}_A(z)$ (or $\\tilde{p}_B(z)$) so that the output z with more inputs will be sampled more often. Afterward, we uniformly choose an"}, {"title": "NORMALIZATION", "content": "The sampled $\\tilde{p}_{A \\rightarrow B}(D)$ and $\\tilde{p}_{B \\rightarrow A}(D)$ are not comparable yet, because sampling needs normalization. Traditionally, we can normalize through the area under curve of the sampled histogram so the distribution is normalized to 1.0. However, we are only interested in comparing the inputs whose NLLs are low and do not need to sample the inputs to cover all the output values. Thus, we develop a normalization method by using the area where both models predict within Z (R3 in Fig 1(c)).\nTo see how this works, we had the ground truth result by enumeration is 3:2:3:3 (Equ. 3) for Fig. 1(d), when $x = 0$. On the other hand, we suppose enumeration is impossible. If we sample 100 in-puts from $M_A$, around 80 of which are expected to be predicted within Z by both models ($ \\blacksquare$ with \u201c2\u201d,\u201c5\u201d,\u201c3\u201d,\u201c6\u201d are in Z, but \u201c1\u201d is not.). Among these 100 inputs, 60 of them are $PD_{A \\rightarrow B}$ and 40 of them are $PA_{A \\rightarrow B}$. We can repeat this process when we sample 300 inputs by model $M_B$ and 200 of them are from Z by $M_A$. Among these 300 inputs, 150 of them are $PD_{B \\rightarrow A}$ and 150 of them are $PA_{B \\rightarrow A}$. We can use the two sets of the sampled inputs that are commonly predicted by the two models within Z as the denominators (80 for $M_A$ and 200 for $M_B$) to fix Equ. 3. This allows us to compare the sum of the following output distributions after being divided by denominators:\n$\\sum P_{A \\rightarrow B}(D) \\propto \\frac{\\sum \\tilde{P}_{A \\rightarrow B}(D)}{|X_{A \\rightarrow B}|}$\n$\\sum P_{B \\rightarrow A}(D) \\propto \\frac{\\sum \\tilde{P}_{B \\rightarrow A}(D)}{|X_{B \\rightarrow A}|}$\nwhere the proportions have the same weight (validation of this normalization is in Appendix D.2). $X_{A \\rightarrow B}$ ($X_{A \\rightarrow B}$ =80 in the above example) is the set of the inputs sampled by model $M_A$ within Z and model $M_B$ also predicts within Z, and $X_{B \\rightarrow A}$ ($X_{B \\rightarrow A}$ = 200) is vice versa. Therefore, by considering Equ. 7 and Equ. 8, Equ. 3 becomes the normalized ratio:\nModel-diff pipeline. In practice, Model-diff consists of four steps:\n(a) Use text generation by sampling (Sec. 3.1) to generate inputs, collect the statistics (frequency histogram), and use PTHR to compute output distribution $p_A(z)$ for model A's meaningful output values Z.\n(b) Sample the collected representative inputs from (a) with weights $p_A(z)$ (Sec. 3.2). Feed each sampled input from A to model B to compute prediction (output) difference D. The D of the sampled inputs from A forms a distribution $P_{A \\rightarrow B}(D)$ of prediction difference. It is normalized (Sec. 3.3) to get a comparable $P_{A \\rightarrow B}(D)$.\n(c) We repeat the same process to get $P_{B \\rightarrow A}(D)$ for model $M_B$.\n(d) $P_{A \\rightarrow B}(D)$, $P_{B \\rightarrow A}(D)$, and the correspondent sampled inputs are compared and analyzed to quan-tify prediction difference (Sec. 2.1 Analysis with output distribution), sometimes with input an-notations (Sec. 2.2)."}, {"title": "EXPERIMENTS", "content": "We first apply Model-diff to a Toy example where the enumeration of all inputs is affordable to con-firm Model-diff's correctness (Sec. 4.2). We then apply it to two pretrained GPT2 models (Radford et al., 2019) with various sequence lengths (25 and 100) and Llama models (Touvron et al., 2023a;b) with sequence length 25 (Sec. 4.3). This shows Model-diff is applicable to real-world models. As we have confirmed the applicability of Model-diff, we will show the types and count Model-diff focuses on can be useful in real-world applications (Sec. 4.4).\nEXPERIMENTAL SETTINGS\nOur sampling target is the negative-log-likelihood (NLL), the training loss used for next-token pre-dictions. Though a low NLL generally indicates the model (strongly) believes the input is close to the training distribution, the inputs with very low NLLs are repeating words that are incomprehensi-ble by humans (Appendix G and Holtzman et al. (2019)). Therefore, we generally set a reasonable range of Z and only consider inputs whose NLL \u2208 Z. We choose the range Z by ensuring the bins have human-understandable inputs, but our method works in any range (input space) selected for the specific task. Fig. 2 shows the sampling results of $D = NLL_A \u2013 NLL_B$ with one unit of standard deviation for three runs (two for Toy) after we have the PTHR results. Tab. 1 shows the detailed statistics about Fig. 2 for Model-diff analysis. More details of experimental settings are in Appendix F."}, {"title": "\u03a4\u039f\u03a5 EXAMPLE", "content": "Toy is a simple experiment with dataset of sequences ${x^{(i)}}$ with length 8. Each token $x_j$ for an input $x^{(i)}$ is an integer from 0 to 9 inclusive; vocabulary size is 10. The entire input space is $10^8$ which is enumerable. The training objective is $(\\sum x_j) \\mod 30 = 0$. The GPT2-small-Toy has 4 heads and 6 layers. The GPT2-large-Toy has 8 heads and 8 layers. Both models can generate sequences that satisfy the objective with 100.0%.\nAnalysis. Fig. 2(a) shows the output distribution $p(D)$, where we set $D = NLL_{small} - NLL_{large}$. Exp.1 in Tab. 1 shows the statistics of Fig. 2(a). $p(D)$ on GPT2-small-Toy's representative inputs ranges from -0.9 to 0.25, indicating that GPT2-large-Toy's predicted NLL on some GPT2-small-Toy's rep-resentative inputs can be up to 0.9 larger and 0.25 smaller on some other inputs than GPT2-small-Toy predicts. On the other hand, $p(D)$ on GPT2-large-Toy ranges from -0.35 to 0.55, indicating GPT2-small-Toy's predicted NLL on some GPT2-large-Toy's representative inputs can be up to 0.55 larger on some inputs but 0.35 smaller on some other inputs than GPT2-large-Toy predicts. Comparison of prediction disagreements between GPT2-small-Toy's min D (-0.9) and GPT2-large-Toy's max D (0.55) shows GPT2-large-Toy disagrees more strongly on GPT2-small-Toy's representative inputs than GPT2-small-Toy disagrees on GPT2-large-Toy's representative inputs.\nIn terms of the number of prediction disagreement/agreement, the normalized ratio of count is 1.0: 0.75: 0.55 : 0.75. Compared to $PD_{small\\rightarrow large}$ (1.0), $PA_{s\\rightarrow l}$ means 0.75 amount of GPT2-small-Toy's representative inputs would be assigned with lower NLL by GPT2-large-Toy, and $PA_{l\\rightarrow s}$ means 0.55 amount of GPT2-large-Toy's representative inputs would be predicted with lower NLL by GPT2-small-Toy. Lastly, compared to $PD_{s\\rightarrow l}$, $PD_{l\\rightarrow s}$ means 0.75 amount of GPT2-large-Toy's representative inputs would be predicted with higher NLL by GPT2-small-Toy. Model-diff shows the two models have a high overlap of predictions as the D concentrates around 0.\nCorrectness of Model-diff. We enumerate all the sequences as the ground truth in Fig 2(a). The ground-truth plot is closely aligned with our sampled plot. Lastly, our sampled ratio is very close to the ground truth enumeration ratio 1.0:0.72: 0.56 : 0.73. The toy example confirms the correctness of Model-diff where the sampling results can properly represent the enumeration; we can apply it to more complicated applications with confidence. Moreover, we use MCMC with a temperature equals to 1.0 to sample the GPT2-large-Toy. Fig. 6 shows that simple text generation by MCMC sampling does not lead to the same ground truth distribution for a range of output values (see Sec. 3.2), because it biases the inputs with low NLLs."}, {"title": "REAL-WORLD LANGUAGE MODELS", "content": "We apply Model-diff to two pretrained GPT2 models, GPT2-small and GPT2-medium with $D = NLL_{small} - NLL_{medium}$. GPT2-small-25 and GPT2-medium-25 sample 25 tokens with GPT2-medium and with GPT2-small respectively. For longer sequence length, GPT2-small-100 and GPT2-medium-100 sample 100 tokens with GPT2-small and with GPT2-medium respectively.\nFig. 2(b) shows the p(D) for both models with sequence length 25. In Exp.2 of Tab. 1, compari-son between GPT2-small-25's min D (-3.95) and GPT2-medium-25's max D (2.55) shows GPT2-medium-25 disagrees more strongly on some GPT2-small-25's representative inputs than GPT2-small-25 disagrees on some GPT2-medium-25's representative inputs. However, the count ratio (Equ. 9) on Tab. 1 (Exp 2) shows the number of inputs for prediction agreements (0.02 vs 0.01) and prediction disagreements (1.0 vs 1.03) are almost the same for both models.\nMoreover, the experiment on 100 sequence length in Fig. 2(c) and its statistics (Table. 1 Exp 3) show GPT2-small-100 and GPT2-medium-100 have distinctive characteristics. GPT2-small-100's min D (-2.25) is almost two times larger than GPT2-medium-100's max D (1.25) in absolute value, in-dicating the GPT2-medium-100 disagrees more strongly on some GPT2-small-100's representative inputs than vice versa. In terms count, $PD_{m(edium)\\rightarrow s(mall)}$ is 29.84 times larger than $PD_{s\\rightarrow m}$ (1.0). Lastly, the prediction agreement on each other's representative inputs is (extremely) low compared to prediction agreement."}, {"title": "APPLICATIONS", "content": "We demonstrate a few real-world examples of applications using the types and counts Model-diff focuses, as we have confirmed the applicability of Model-diff.\nDeciding which model is better. We define our task of which model is better in terms of which model's prediction agrees more with human annotation. We achieve this by annotating the in-puts. We choose to annotate the inputs from -1 to -0.6 and from 1 to 0.6 where the dominant number of inputs concentrates and |D| is not too small when the two models do not show signif-icant prediction differences. We sum from the -1 to -0.6 for $PD_{small\\rightarrow medium}$ and from 1 to 0.6 for $PD_{medium \\rightarrow small}$. Using Equ. 4 and Equ. 5, we compute 0.56 precision and recall is 0.32 for GPT2-small-25. For GPT2-medium-25, we compute 0.58 precision and recall is 0.57. This shows while the two models disagree with the prediction of the other model's representative inputs, GPT2-medium-25's disagreement aligns more closely to human annotation. Therefore, GPT2-medium is a better model as its prediction agrees more with human annotation. Without introducing extra bi-ases from datasets, we can use Model-diff to attain a better understanding of the models' prediction agreement and disagreement.\nModel-plagiarism. Nowadays, open-sourced LMs are easily accessed for commercial and research purposes. It remains an open question whether the new models are sufficiently distinct from their original counterparts or if they are merely altered by adding noise to the weights (PrimerYang). We offer a different angle to approach this problem than watermarking. We test Model-diff by compar-ing GPT2-small-25 and GPT2-small-0.001-noise-25 where we add Gaussian noise to each weight with zero-mean and standard deviation = 0.001 (Fig. 3(a)). It shows that GPT2-small-noise-25 almost always predicts a higher NLL on GPT2-small-25's representative inputs. This is reason-able since GPT2-small-noise-25 with noisy weights predicts inputs with higher NLL in general. However, it is noteworthy that GPT2-small-25 predicts a lower NLL on almost all GPT2-small-noise-25's representative inputs. This is in contrast with the output distributions in Fig. 2 where two different models disagree on each other's representative inputs. We further compare GPT2-small-25 and GPT2-small-0.00001-noise-25 a smaller noise with standard deviation 0.00001 to weights. Fig. 3(b) shows consistent results, though the area of overlap is larger because the two models are more similar. This shows the output distributions that Model-diff focuses on can produce potentially useful signal to detect if a model is sufficiently different from its original counterpart. This signal can be an indicator for further confirmation of plagiarism."}, {"title": "RELATED WORKS AND DISCUSSIONS", "content": "Model understanding and analysis. Recent works (Booth et al., 2021; Liu et al., 2023a) propose to understand models (Zeiler & Fergus, 2014; Ribeiro et al., 2016; Lundberg & Lee, 2017; Ghorbani et al., 2019) beyond the datasets by sampling the model itself, which can also avoid being biased even if the dataset is generated by (external) models (Luo et al., 2023; Prabhu et al., 2023; Shu et al., 2020; Leclerc et al., 2022). Model-diff follows the recent methods of estimating (Liu et al., 2023b) and leveraging the output distribution for analysis (Liu et al., 2023a). Its new normalization algorithms facilitate the analysis of model prediction differences without the need to sample accurately all the output values. Strobelt et al. (2021) proposes a microscopic view of how each token is predicted differently by the two models on the same input. It can serve as a microscopic analysis tool for Model-diff once the representative inputs are sampled. Model-diff, on the other hand, examines two important macroscopic properties: the types and count of inputs.\nOpen-world Model Evaluation is a unique challenge including in out-of-distribution detection (Liu et al., 2020; Hendrycks & Gimpel, 2016; Hendrycks et al., 2019; Hsu et al., 2020; Lee et al., 2017; 2018; Liang et al., 2018; Mohseni et al., 2020; Ren et al., 2019), adversarial sets (Szegedy et al., 2013; Rozsa et al., 2016; Miyato et al., 2018; Kurakin et al., 2016; Xie et al., 2019; Madry et al., 2017) etc. Instead of targeting specific types of inputs, Model-diff addresses the model comparison in an input space through output distribution. The two models first efficiently map the inputs in the input space to different output values. Human inspection of the mapping follows after computing prediction difference D.\nSamplers for output distribution were known in physics as sampling the density of states (Wang & Landau, 2001). The connection between the two has been discovered recently (Liu et al., 2023b). Parallel tempering and histogram reweighting algorithms can also sample output distri-bution (Hukushima & Nemoto, 1996; Swendsen & Wang, 1986), which are more compatible with the machine learning samplers (Grathwohl et al., 2021; Zhang et al., 2022) for energy function for discrete input space."}, {"title": "CONCLUSION AND FUTURE WORKS", "content": "We propose a novel framework, Model-diff, for comparative analysis between two models with-out introducing external models or datasets. Model-diff leverages the output distributions and the corresponding representative inputs of the two models to understand the types and quantity of the agreed/disagreed predictions in each model's meaningful input space. In future work, more efficient samplers could speed up the sampling procedure for Model-diff. Moreover, better normalization can be developed for comparing more than two models."}, {"title": "APPENDIX", "content": null}, {"title": "LIMITATION", "content": "Our framework is designed to be a general framework, but it is not preferable for all settings. First, our analysis depends on the sampler(s). As sampling the output distribution is a relatively new topic in the machine learning community, more advanced samplers with more computation resources can scale our experiments. Although our proof-of-concept method depends on the samplers' results, the analysis method itself is parallel to the development of the sampler, meaning that the method of how to use output distributions to analyze the models will be consistent, even though the sampled results may improve with better samplers.\nAnother is our analysis focuses on NLL. While it is the training loss for many next-token-predictions, it does not cover other interesting problems in NLP that do not use the loss. Our method in general targets a set of problems that uses log-probability as output. This problem is covered as energy-based models LeCun et al. (2006), where the \u201cenergy function\" (log-probability) is a mea-surement of the compatibility between the (input) variables. Therefore, our method can also choose these measurements as output to be sampled. Moreover, it is also important to scale our method to multi-dimensional output, such as feature embedding analysis. Concrete examples of applications for problems beyond NLL are left as future work."}, {"title": "POTENTIAL RISK", "content": "This paper presents a work to analyze two models side-by-side. Relying on the model itself to generate data for analysis, our method has a social sequence that the data may lead to privacy leakage and hallucination answers."}, {"title": "MATH DESCRIPTION OF THE MODEL-DIFF", "content": "Notations. We denote the sampled quantity used for practical analysis with \"Tilde\" (e.g. $\\tilde{p}$) and the quantity from ground truth enumeration without \"Tilde\" (e.g. p) for conceptual discussion purposes. We also efine a varying threshold $\\lambda$ for D values, and denote A $\\rightarrow$ B as the representative inputs"}]}