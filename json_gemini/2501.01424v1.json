{"title": "Object-level Visual Prompts for Compositional Image Generation", "authors": ["Gaurav Parmar", "Or Patashnik", "Kuan-Chieh Wang", "Daniil Ostashev", "Srinivasa Narasimhan", "Jun-Yan Zhu", "Daniel Cohen-Or", "Kfir Aberman"], "abstract": "We introduce a method for composing object-level visual prompts within a text-to-image diffusion model. Our approach addresses the task of generating semantically coherent compositions across diverse scenes and styles, similar to the versatility and expressiveness offered by text prompts. A key challenge in this task is to preserve the identity of the objects depicted in the input visual prompts, while also generating diverse compositions across different images. To address this challenge, we introduce a new KV-mixed cross-attention mechanism, in which keys and values are learned from distinct visual representations. The keys are derived from an encoder with a small bottleneck for layout control, whereas the values come from a larger bottleneck encoder that captures fine-grained appearance details. By mixing keys and values from these complementary sources, our model preserves the identity of the visual prompts while supporting flexible variations in object arrangement, pose, and composition. During inference, we further propose object-level compositional guidance to improve the method's identity preservation and layout correctness. Results show that our technique produces diverse scene compositions that preserve the unique characteristics of each visual prompt, expanding the creative potential of text-to-image generation.", "sections": [{"title": "1. Introduction", "content": "Text-to-image models [23, 27, 51, 54] have made remarkable progress, enabling photorealistic image synthesis with a wide variety of object compositions and arrangements. These models can create complex scenes with multiple interacting elements that generally align with user-provided textual prompts. However, integrating visual prompts,"}, {"title": "2. Related Works", "content": "Single-concept personalization. Recent large-scale image generative models typically rely on text prompts for conditioning [27, 47, 49, 51, 54]. While text provides an intuitive interface for image synthesis, its expressiveness is limited when describing specific visual elements. To address this limitation, numerous works have developed means to embed images into the model [4, 17, 32, 52], thereby enabling image-based conditioning for synthesis. Initial approaches required per-subject optimization, which restricted their applicability due to high computational costs. More recent works have focused on training encoders or adapters to condition the generation on input images in a feed-forward manner [3, 13, 18, 26, 53, 55, 64, 67, 69].\nMultiple-subject scene generation. Generating complex scenes with multiple interacting objects presents a substantial challenge [8, 11, 15, 19, 35, 43, 60]. As a result, most encoder-based personalization methods focus on a single object [3, 13, 26, 34, 55, 64, 69]. To address the difficulty of generating scenes with multiple subjects, researchers have developed several dedicated methods. For instance, certain methods [32, 45] merge separately learned concepts within a single image. Break-a-Scene [4] takes a differ-"}, {"title": "3. Method", "content": "Given a set of N input visual prompts {P_i}_{i=1}^N describing the N-1 individual objects and the background of an image, our goal is to generate diverse output images composed of these inputs. We first discuss text-to-image diffusion models and image encoder preliminaries in Section 3.1. Following this, Section 3.2 explores the trade-off between maintaining the identity of input elements and introducing variation in the generated images, which motivates our architecture design. Section 3.3 details our training method and datasets, and lastly, Section 3.4 describes our new compositional guidance for inference. We refer to our method as VisualComposer."}, {"title": "3.1. Preliminaries", "content": "Text-to-Image Diffusion. Diffusion models [23, 56, 58] are a family of generative models that use iterative denoising processes. Recent diffusion models are typically conditioned on text prompts [47, 51] through cross-attention layers [7]. Specifically, a text embedding vector c is derived from a text prompt $P_t$ using a frozen CLIP [48] text encoder $c = E_{text}(P_t)$. This text embedding interacts with the generated image deep spatial features $\\phi(x_t)$ as follows. The im-"}, {"title": "3.2. The VisualComposer Architecture", "content": "Exploring the identity preservation-diversity tradeoff.\nAs discussed in Section 3.1, prompting with images typically begins by extracting image features $E_{img}(P_{img})$, where $E_{img}$ is a pre-trained frozen image encoder. We find that the choice of feature extractor is crucial, as it impacts the trade-off between identity preservation and output diversity. Encoders with a narrow information bottleneck (i.e., heavy information compression) may not capture sufficient details about the object's identity, but they tend to generate more diverse results as they are less likely to overfit the original pose or spatial arrangement. In contrast, encoders with a wide information bottleneck (i.e., retaining highly detailed information) better capture the identity features but tend to overfit to the original pose and layout, thereby sacrificing the model's ability to generalize to new poses.\nKV-Mixed Cross-Attention. Our method overcomes the identity preservation-diversity tradeoff by leveraging the unique roles of keys and values in the cross-attention mechanism. We introduce KV-Mixed Cross-Attention Layers, where we employ a coarse (narrow bottleneck) encoder $E_{img}^C$ for the keys to promote diversity in poses and layouts, while a fine-grained (wide bottleneck) encoder $E_{img}^F$ is used for the values to preserve detailed identity features accurately. As shown in Figure 2c, by mixing the features obtained from the two encoders, we are able to achieve high identity preservation of the input image while also generating diverse layouts.\nArchitecture. Our method's architecture is illustrated in Figure 3. Given the N visual prompts {P_i}_{i=1}^N shown on the left, we generate images that preserves the identity of each prompt while allowing for flexible layouts and poses.\nOur method builds upon a pre-trained text-to-image diffusion model [47, 51], which remains frozen during training. Each input visual prompt, $P_i$, is processed through a two-stream architecture. The first stream (top of Figure 3) uses a fine-grained encoder $E_{img}^F$, followed by a transformer and an appearance adapter $A_{app}$ to extract appearance tokens ${A_{app}(E_{img}^F (P_i))}_{i=1}^N$. The second stream (bottom) utilizes a coarse encoder $E_{img}^C$ followed by a transformer and a layout adapter $A_{layout}$ to obtain layout tokens ${A_{layout} (E_{img}^C (P_i))}_{i=1}^N$.\nThe fine-grained encoder is implemented using a CLIP image encoder, extracting grid features from its penultimate layer. The coarse encoder uses the CLIP global image embedding. The appearance adapter is implemented as a Perceiver Transformer [25], and the layout adapter is implemented as a linear layer with layer normalization [6]. Extracted layout and appearance tokens from each visual prompt are concatenated and fed to our KV-Mixed cross-attention layers, serving as keys and values, respectively. These decoupled KV-Mixed cross-attention layers are added to each cross-attention layer."}, {"title": "3.3. Training", "content": "Dataset. Our training dataset combines real images [9] and synthetically generated multi-object images [33]. Each training sample consists of an input image x, a text prompt, and a set of N-1 binary object masks {m_n}_{n=1}^{N-1}. The sample also includes a background image $x_{bg}$, obtained by inpainting all masked objects in x. We define the object visual prompts {P_i}_{i=1}^{N-1} by applying each mask $m_n$ to the"}, {"title": "3.4. Inference", "content": "Previous works [8, 11, 15, 44] have shown that text-to-image models often struggle to faithfully follow input text prompts. These works use guidance-based inference-time techniques [16, 22] to improve text-image alignment. To enhance the model's adherence to input visual prompts, we introduce Compositional Guidance during inference. This technique relies on individual object segments in the generated image. To this end, our generation process is done in two stages. First, we generate an image without any intervention in the denoising process and find a segment for each visual prompt. These segments are used to generate the final output image as described below.\nAssigning Segments. We begin by applying an open-set segmentation [50, 68] on the image generated in the first stage, and denote by {$S_j$} the set of detected segments. Then, to match each visual prompt with a segment, we use an optimal assignment algorithm, where we compute the DINOv2 [38] similarity between each input visual prompt $P^n$ and the detected segment $S_j$:\n$Sim(n, j) = DINO(P^n, S_j)$.\nUsing DINOv2 similarity as the cost function (computed as 1 - Sim(n, j)), we use the Hungarian matching algorithm [31] to find the best one-to-one assignment $\\sigma(n)$ between the input visual prompts and the detected segments.\nCompositional Guidance. To reinforce the correspondence between input visual prompts and their generated counterparts, we adjust the attention maps during inference. For each input visual prompt $P_n$, we modify its associated attention map $M_{img}$ by zeroing out values outside the region of the matched segment $S_{\\sigma(n)}$. Specifically, we do so by setting these values as $-\\infty$ in the result of $QK_img^T$ before applying the Softmax that produces $M_{img}$. We further define a loss function to maximize the DINO similarity between each input visual prompt and its matched segment:\n$L_{id} = \\sum_n (1 - Sim(n, \\sigma(n)))$,\nwhere Sim is defined as in Equation 2. Note, that here the similarity is computed between the input visual prompt $P^n$ and the segmentation mask of $S_{\\sigma(n)}$ applied on the $x_0$ prediction of the current noisy image $x_t$. We backpropagate this loss through the model to update the appearance tokens of the fine-grained encoder $A_{app} (E_{img}^F (P_i))$. Updating only the appearance tokens ensures that only the identity features are refined without affecting the overall scene layout."}, {"title": "4. Experiments", "content": "In this section, we demonstrate the effectiveness of our method through a series of experiments. Section 4.1 begins by discussing the evaluation protocol used. Section 4.2 shows how our method compares with previous approaches, and Section 4.3 demonstrates the importance of each individual component of our method. We train our method using Stable Diffusion 1.5 [51] and Stable Diffusion XL [46] as the base text-to-image diffusion models. For a fair comparison to the baseline methods, Figures 5, 6, 7, and Table 1 use the Stable Diffusion 1.5 model. The results in Figures 1 and 4 use Stable Diffusion XL. A classifier free guidance value of 7.5 and the DDIM scheduler [57] with 25 denoising inference steps are used in all comparisons. Please see the Appendix for additional baseline comparisons, analyses, and discussion of our limitations."}, {"title": "4.1. Evaluation Protocol", "content": "We evaluate our method along two axes: adherence of the output image to each input visual prompt and the diversity of variations in the scene layout.\nAdherence to input. Compositional generation involves creating images with diverse scene layouts and poses, making it challenging to quantify how faithfully the output adheres to the input visual prompts. First, since composed images combine multiple prompts, measuring similarity between individual prompts and entire output image is inappropriate, as it does not accurately reflect each prompt's contribution. Second, variations in pose and spatial arrangement, which we desire in our output, can lower similarity scores even when object identities are preserved."}, {"title": "A. Additional Results", "content": "Additional qualitative results. We show addition qualitative results in Figure 8. We use a classifier-free guidance scale of 5 for these results.\nReshuffling. Reshuffling is a special case of compositional generation where all input visual prompts are extracted from the same starting image. Figure 9 shows a large grid of reshuffling results generated by our method.\nObject control. Our object-level cross-attention design enables precise control over individual objects in generated images. Figure 12 illustrates this with two examples. In the first example, the input visual prompts are a dog, a grassy background, and an orange ball. The leftmost column shows the initial output image. By manipulating the cross-attention maps corresponding to the ball, we can move it above the dog's head (middle column) or near its lower right foot (right column). As the ball is repositioned, the scene adapts accordingly: the dog adjusts its pose by ducking its head when the ball is above it or standing on the ball when it's near its feet.\nIn the second example shown at the bottom, we change the position of a man standing in a boat. By moving him to the right, the reflection in the water adjusts accordingly. When moved upward, the man stands taller, revealing more of his legs. These examples demonstrate how our method allows for fine-grained control over object placement, with the scene naturally adapting to the changes."}, {"title": "B. Analysis", "content": "KV-Mixed Cross-Attention. Figure 7 in the main paper quantitatively demonstrates the importance of KV-Mixed Cross-Attention layers, and Figure 13 visually illustrates their effects. The top row shows the results of using a fine-grained image encoder for both keys and values. This configuration causes the model to overfit to the poses of the input objects, producing outputs that closely mirror the input visual prompts. For example, the dog is always sitting in the same pose and looking to the right, identical to the input image. In contrast, the middle row uses a coarse image encoder. Here, the generated images exhibit diverse poses and layouts, but the identities of the objects are not well preserved. For instance, the red vase looks different from the input, and the dog's fur does not match the original. Finally, the bottom row illustrates the effects of our proposed KV-Mixed Cross-Attention. This approach enables us to generate diverse images while accurately retaining the identities of input visual prompts."}, {"title": "C. Additional Comparisons", "content": "Visual comparisons. Figure 5 in the main paper shows a visual comparison between our method and prior methods on two examples. In Figures 10 and 11, we show additional visual comparisons.\nUser preference study. We conduct a user preference study in addition to assessing adherence to the input visual prompts using automatic compositional identity metrics shown in the main paper Table 1 (DINOcomp and CLIPcomp). Specifically, we perform pairwise preference comparisons in which users are shown three images: the input visual prompt and output images generated by two different methods. Users are then asked to choose which output images more accurately portray the input visual prompt. Each comparison is performed by three different users, and a total of 13,500 comparisons are made for comparison with each of the baseline methods. The results in Table 2 show that our method is preferred over all prior encoder-based and multi-modal methods."}, {"title": "D. Implementation Details", "content": "Dataset creation. As described in Section 3.3 of the main paper, our training dataset consists of images, their corresponding text prompts, a background image, and binary masks for individual objects and the background. The text prompts are generated automatically by recaptioning the images using LLaVa [36]. To obtain precise binary segmentation masks, we first apply an open-set detection model [65] to identify bounding boxes within the images. We then use these bounding boxes to prompt SAM2 [50], which provides accurate segmentation masks for each object. The background images are generated using the SD2.1 inpainting pipeline. We filter the dataset by discarding images that have a CLIP-Aesthetic score below 5.0, a minimum dimension (height or width) less than 512 pixels, or contain fewer than three or more than six objects.\nTraining hyperparameters. We train all models using the Adam optimizer [30] with a learning rate of 0.0001 and a batch size of 32, for a total of 40,000 update steps on four NVIDIA A100 GPUs. To enable classifier-free guidance during inference, we randomly drop the text prompts and visual prompts during training: each is independently dropped 10% of the time, and both are simultaneously dropped 5% of the time."}, {"title": "E. Limitations and Societal Impacts.", "content": "We show the limitations of our model in Figure 14. Our method has difficulty when users input combinations of visual prompts that are not commonly associated. For instance, in the figure, the input visual prompts include a dog, a single shoe, and a forest background. This unusual combination is challenging for our model, leading to failure cases, such as hallucinating a leg wearing the shoe or generating an extra shoe.\nCompositional image generation has the potential to democratize creative expression, allowing users to effortlessly synthesize complex scenes by assembling various visual elements. However, they also pose societal challenges, such as the risk of creating realistic but deceptive images that could spread misinformation or infringe on intellectual property rights. To counter these issues, it is important to"}]}