{"title": "A Comprehensive Framework for Semantic Similarity Analysis of Human and AI-Generated Text Using Transformer Architectures and Ensemble Techniques", "authors": ["Lifu Gao", "Ziwei Liu", "Qi Zhang"], "abstract": "The rapid advancement of large language models (LLMs) has made detecting AI-generated text an increasingly critical challenge. Traditional methods often fail to capture the nuanced semantic differences between human and machine-generated content. We therefore propose a novel approach based on semantic similarity analysis, leveraging a multi-layered architecture that combines a pre-trained DeBERTa-v3-large model, Bi-directional LSTMs, and linear attention pooling to capture both local and global semantic patterns. To enhance performance, we employ advanced input and output augmentation techniques such as sector-level context integration and wide output configurations. These techniques enable the model to learn more discriminative features and generalize across diverse domains. Experimental results show that this approach works better than traditional methods, proving its usefulness for AI-generated text detection and other text comparison tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "The rise of AI-generated content, driven by language models like ChatGPT, has created problems for content moderation and text classification. Detecting machine-generated text is important for many use cases, including combating misinformation and verifying academic work. Traditional detection methods, which often rely on surface-level features such as syntax and word frequency, struggle to capture the nuanced differences in how humans and machines construct meaning. This study instead proposes a novel approach based on semantic similarity analysis, which focuses on the underlying patterns of semantic relationships rather than surface-level features. Our hypothesis is that while human and AI-generated text can convey similar meanings, they differ fundamentally in how semantic relationships are structured and maintained. These differences, though subtle, can be detected through careful analysis of semantic patterns.\nOur approach leverages a pre-trained DeBERTa-v3-large model as the foundation, which provides robust semantic understanding through its disentangled attention mechanism and enhanced mask decoder. This is particularly effective for capturing subtle semantic differences, as DeBERTa's ability to separately model content and position information allows for more precise analysis of semantic relationships. The model is further enhanced with two layers of Bi-directional LSTM to capture sequential dependencies and long-range semantic patterns, which are crucial for identifying the characteristic evolution of ideas in human writing. A linear attention pooling mechanism is then employed to focus on the most relevant semantic features, reducing noise and improving the model's ability to distinguish between human and AI-generated patterns. The final output is produced through a fully connected layer, ensuring robust classification.\nTo further improve performance, we employed several advanced input augmentation techniques such as Electra models pre-trained with Replaced Token Detection (RTD) objectives, sector-level context concatenation, adversarial weight perturbation, and dynamic target shuffling to improve the model's robustness. We also applied a wide output configuration to allow our model to capture both local and global semantic patterns effectively, making it particularly adept at identifying the subtle differences between human and AI-generated text.\nThese enhancements help the model generalize better, making it more accurate in distinguishing AI-generated text from those of human origin, and it is proven capable of achieving state-of-the-art performances in our metrics of choice."}, {"title": "II. RELATED WORK", "content": "Detecting AI-generated text has become an important research area due to the growth of large language models (LLMs) and generative adversarial networks (GANs). Yan et al. [1] discuss generative LLMs, focusing on the challenges they present to distinguish AI-generated text from human-written text. These challenges are also present with GANs, which Gui et al. [2] discuss in terms of their applications for content generation. GANs are particularly used for creating fake text, a problem addressed by Zellers et al. [3] in their study on defending against fake news generated by neural networks.\nTo improve the detection of AI-generated text, Chakraborty et al. [4] review different methods for identifying machine-generated content, looking at how various AI models perform. A major challenge is the domain specificity of text, as models trained on general datasets may not work well in specialized areas. For example, SciBERT, a model for scientific text, has been successful in detecting AI-generated academic papers by recognizing domain-specific language patterns [5]. Dehaerne et al. [6] also explore machine learning to detect machine-generated code, highlighting the difficulties in identifying such content.\nSome studies focus on user interactions with AI-generated content. Lu [7] suggests the use of decision trees and TF-IDF to improve the satisfaction of chatbot users, which can also help detect AI-generated dialogue. Li [8] examines how multimodal data can improve product recommendations, a method that could also be used to detect AI-generated content by combining different data sources.\nText summarization models, like those used by Liu and Lapata [9], have shown promise in detecting AI-generated text by analyzing the structure and coherence of the content. Schick and Sch\u00fctze [10] study few-shot learning, which could be used to detect subtle linguistic patterns of AI-generated text."}, {"title": "III. METHODOLOGY", "content": "This section presents a comprehensive framework using deep learning methods and ensemble techniques for semantic similarity detection. By integrating transformer-based architectures, bidirectional LSTM layers, and novel tricks such as Adversarial Weight Perturbation (AWP) and linear attention pooling, we achieve state-of-the-art performance. Additionally, we introduce dynamic target grouping and fine-tuned ensemble methods to boost diversity and robustness, ensuring superior generalization.", "A. Transformer Backbone": "We utilize DeBERTa-v3-large as the primary feature extractor:\nXbert = DeBERTa(Xinput),  (1)\nwhere Xinput represents the tokenized input sequence. The DeBERTa backbone provides contextualized embeddings, leveraging disentangled attention to capture fine-grained relationships. Additionally, freezing the embedding layers during fine-tuning stabilizes training and reduces overfitting, as the semantic similarity task involves short text sequences.", "B. Bidirectional LSTM Enhancement": "To capture sequential dependencies and enrich feature representation, a Bi-LSTM layer is appended to the transformer outputs:\nXlstm = Bi-LSTM(Xbert),  (2)\nwhere Xistm combines forward and backward dependencies. Adversarial Weight Perturbation (AWP) is introduced during the second epoch to enhance robustness by simulating adversarial scenarios, ensuring that the Bi-LSTM learns more generalizable features.", "C. Linear Attention Pooling": "For dimensionality reduction and improved focus on key features, linear attention pooling is applied:\nXpool = \\sum_{t=1}^{T} a_t X_{lstm,t},  (3)\nwhere $a_t$ are learned attention weights, and T is the sequence length. Dynamic target shuffling during each training step augments this module by exposing the pooling layer to diverse target sequences, enhancing generalization.", "D. Fully Connected Layer": "The final representation is passed through a fully connected layer to compute the similarity score:\nYpred = FC(Xpool).  (4)\nDifferentiated learning rates are applied, with a lower learning rate (2e-5) for the transformer and a higher rate (1e-3) for the LSTM and fully connected layers. This strategy ensures efficient optimization while preserving the pre-trained knowledge of the transformer.", "E. Alternative Model Architectures": "To enhance ensemble diversity, we incorporated additional architectures, each tailored to leverage specific strengths:", "1) Electra-Based Models:": "Electra models, pre-trained with a replaced token detection (RTD) objective, complement the transformer backbone by capturing finer-grained semantic nuances. The model is formulated as:\nXelectra = Electra(Xinput),  (5)\nwhere the RTD mechanism provides robust token-level understanding. Expanding dimensions for weaker models like SimCSE improves compatibility during ensemble integration:\nXwide = ExpandDims(Xelectra).  (6)", "2) Wide Output Configurations:": "For models with lower baseline performance, we expanded the output dimensions:\nXwide_{out} = Concat(X_{transformer}, X_{context}),  (7)\nwhere contextual information is explicitly integrated, enhancing representation diversity.", "3) Bi-LSTM and Sector Contexts:": "Bi-LSTM layers were adapted to integrate grouped sector-level contexts:\nXsector = Bi-LSTM(Xcontext[0]),  (8)\nwhere context[0] represents sector-level information (e.g., F21 for \"F\"). This hierarchical approach adds a structured representation for weakly supervised data.", "F. Loss Function": "The primary loss function is the Pearson correlation loss:\nLpearson = \\frac{Cov(Y_{pred}, Y_{true})}{\\sigma(Y_{pred})\\cdot \\sigma(Y_{true})}, (9)\nwhere Cov represents covariance, and \u03c3denotes standard deviation. Additionally, we employed a mean squared error (MSE) loss as a secondary measure:\nL_{mse} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_{pred,i} - Y_{true,i})^2. (10)\nThese loss functions, combined with AWP and dynamic target shuffling, ensure robust optimization.", "G. Data Preprocessing": "Effective data preprocessing is critical to model performance. The following steps were implemented:", "1) Target Grouping and Stratification:": "Data was grouped by anchor phrases and stratified based on semantic similarity scores:\nG = GroupBy(Anchor, Context) [Target], (11)\nensuring balanced data distribution across training folds. Targets sharing common words with anchors were allocated to the same folds to maintain contextual consistency.", "2) Dynamic Target Shuffling:": "During each training step, target sequences were shuffled dynamically:\nS^{(i)}_{targets} = Shuffle(T^{(i)}), (12)\nwhere $S^{(i)}_{targets}$ represents the shuffled target set at step i. This reduces overfitting and exposes the model to diverse input combinations.", "3) Contextual Augmentation:": "Sector-level contexts were extracted and added to the input:\nXaug_{input} = Concat(X_{anchor}, X_{target}, X_{sector}). (13)\nThis augmentation enriches the input representation, aligning it with hierarchical domain knowledge.", "4) Tokenization and Padding:": "Inputs were tokenized using a subword tokenizer and padded to a uniform sequence length:\nX_{token} = Pad(Tokenizer(X_{raw})). (14)\nPadding ensured compatibility with batch processing while preserving contextual integrity."}, {"title": "IV. EVALUATION METRICS", "content": "The performance of the models was evaluated using the following metrics:\n1) Pearson Correlation Coefficient: The primary metric is the Pearson correlation coefficient, which measures the linear correlation between predicted and true scores:\n\\rho = \\frac{Cov(Y_{pred}, Y_{true})}{\\sigma(Y_{pred}) \\cdot \\sigma(Y_{true})}, (15)\nwhere Cov represents covariance, and o is the standard deviation.", "2) Mean Squared Error (MSE):": "To evaluate prediction accuracy, the mean squared error was computed:\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_{pred,i} - Y_{true,i})^2. (16)\nThis metric captures the average squared difference between predictions and actual values.", "3) F1-Score:": "F1-score was used to evaluate the balance between precision and recall for binary classification tasks:\nF1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} (17)\nThis ensures a comprehensive evaluation of the model's performance on edge cases.", "4) Area Under Curve (AUC):": "The AUC metric evaluates the ability of the model to distinguish between classes by calculating the area under the Receiver Operating Characteristic (ROC) curve:\nAUC = \\int TPR(FPR) d(FPR), (18)\nwhere TPR is the true positive rate and FPR is the false positive rate."}, {"title": "V. EXPERIMENT RESULTS", "content": "Table I provides a detailed view of performance gains across the evaluation metrics in the ablation study. The losses and performance indicator metrics in each training epochs of the final model are shown in Figure 5."}, {"title": "VI. CONCLUSION", "content": "This study demonstrates the effectiveness of leveraging transformer-based architectures with Bi-LSTM enhancements, adversarial weight perturbation, and dynamic preprocessing strategies for comparing the semantic similarity between human and AI-generated text. The integration of diverse models, combined with linear attention pooling and target shuffling, significantly improves robustness and accuracy. The ensemble strategy achieves state-of-the-art performance across multiple evaluation metrics, setting a robust foundation for practical applications in patent search and examination processes. Future work will explore domain-specific pretraining and other augmentation techniques to further enhance model generalization."}]}