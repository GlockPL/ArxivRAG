{"title": "VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play", "authors": ["Zelai Xu", "Chao Yu", "Ruize Zhang", "Huining Yuan", "Xiangmin Yi", "Shilong Ji", "Chuqi Wang", "Wenhao Tang", "Yu Wang"], "abstract": "Multi-agent reinforcement learning (MARL) has made significant progress, largely fueled by the development of specialized testbeds that enable systematic evaluation of algorithms in controlled yet challenging scenarios. However, existing testbeds often focus on purely virtual simulations or limited robot morphologies such as robotic arms, quadrupeds, and humanoids, leaving high-mobility platforms with real-world physical constraints like drones underexplored. To bridge this gap, we present VolleyBots, a new MARL testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots features a turn-based interaction model under volleyball rules, a hierarchical decision-making process that combines motion control and strategic play, and a high-fidelity simulation for seamless sim-to-real transfer. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative MARL and game-theoretic algorithms. Results in simulation show that while existing algorithms handle simple tasks effectively, they encounter difficulty in complex tasks that require both low-level control and high-level strategy. We further demonstrate zero-shot deployment of a simulation-learned policy to real-world drones, highlighting VolleyBots' potential to propel MARL research involving agile robotic platforms.", "sections": [{"title": "1. Introduction", "content": "Multi-agent reinforcement learning (MARL) has demonstrated remarkable success across diverse domains, including competitive board games such as Go (Silver et al., 2016), cooperative card games like Hanabi (Bard et al., 2020), real-time strategy challenges such as the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019; Ellis et al., 2024) and Google Research Football (GRF) (Kurach et al., 2020), as well as human-AI cooperative games like Overcooked (Carroll et al., 2019). These testbeds have collectively propelled MARL research forward by showcasing how agents can effectively learn to cooperate, compete, and adapt within complex multi-agent interactions.\nDespite the progress in MARL, most established platforms are fully simulated games that do not account for physical-world interactions. To address this gap, several MARL testbeds based on robotic platforms have been developed, such as Multi-Agent MuJoCo(MAMuJoCo) (Peng et al., 2021), Bimanual Dexterous Hands (Bi-DexHands) (Chen et al., 2022), and Multi-agent Quadruped Environment (MQE) (Xiong et al., 2024). Among these, robot sports stand out as a typical task that integrates strategic decision-making under sport-specific rules\u2014such as teamwork and adversarial play-with complex, dynamic motion control constrained by real-world physical factors. Notable examples include robot table tennis (D'Ambrosio et al., 2024), quadrupedal robot football (Ji et al., 2022), and humanoid football (Liu et al., 2022; Haarnoja et al., 2024). However, robot sports often lack open-source simulation environments and real-world support, limiting their effectiveness as research testbeds. Additionally, these platforms mainly focus on robotic arms, quadrupedal, and humanoid, with limited exploration of high-mobility, agile platforms like drones.\nIn this work, we introduce a novel MARL testbed named VolleyBots, where multiple drones engage in the popular sport of volleyball. This testbed addresses gaps in existing robot sports platforms by incorporating: (i) a turn-based interaction model governed by volleyball rules, effectively capturing discrete offensive and defensive phases for MARL studies; (ii) a hierarchical decision-making process that combines low-level motion control with high-level strategic play"}, {"title": "2. Related Work", "content": "Executing precise and agile flight maneuvers is essential for drones, which has driven the development of diverse control strategies (Bouabdallah et al., 2004; Williams et al., 2017; Hwangbo et al., 2017). Among these, RL has shown significant promise, offering flexibility and efficiency in drone control. Drone racing is a notable single-drone control task where RL has achieved human-level performance (Kaufmann et al., 2023), showcasing near-time-optimal decision-making capabilities. Beyond racing, researchers also leveraged RL for executing aggressive flight maneuvers (Sun et al., 2022) and achieving hovering stabilization under highly challenging conditions (Hwangbo et al., 2017). As for multi-drone tasks, RL has been applied to cooperative"}, {"title": "3. VolleyBots Environment", "content": "In this section, we introduce the environment design of the VolleyBots testbed. The environment is built upon the high-throughput and GPU-parallelized OmniDrones (Xu et al., 2024) simulator, which relies on Isaac Sim (Mittal et al., 2023) to facilitate rapid data collection. We further configure OmniDrones to simulate realistic flight dynamics and interaction between the drones and the ball, then implement standard volleyball rules and gameplay mechanics to create a challenging domain for drone control tasks. We will describe the simulation entity, observation space, action space, and reward functions in the following subsections."}, {"title": "3.1. Simulation Entity", "content": "Our environment simulates real-world physics dynamics and interactions of three key components including the drones, the ball, and the court. We provide a flexible configuration of each entity's model and parameters to enable a wide range of task designs. For the default configuration, we adopt the Iris quadrotor model (Furrer et al., 2016) as the primary drone platform, augmented with a virtual \u201cracket\" of radius 0.2m and coefficient of restitution 0.8 for ball striking. The ball is modeled as a sphere with a radius of 0.1 m, a mass of 5 g, and a coefficient of restitution of 0.8, enabling realistic bounces and interactions with both drones and the environment. The court follows standard volleyball dimensions of 9 m \u00d7 18m with a net height of 2.43 m. All these models and parameters can be easily modified and randomized to facilitate sim-to-real transfer, and a zero-shot real-world deployment example will be presented in Sec. 6."}, {"title": "3.2. Observation Space", "content": "To align with the feature of partial observability in real-world volleyball games, we adopt a state-based observation space where each drone can fully observe its own physical state and partially observe the ball's state and other drones' state. More specifically, each drone has full observability of its position, rotation, velocity, angular velocity, and other physical states. For ball observation, each drone can only partially observe the ball's position and velocity. In multi-agent tasks, each drone can also partially observe other drones' position and velocity. Minor variations in the observation space may be required for different tasks, such as the ID of each drone in multi-agent tasks. Detailed observation configurations for each task are provided in the Appendix B."}, {"title": "3.3. Action Space", "content": "We provide two types of continuous action spaces that differ in their level of control, with Collective Thrust and Body Rates (CTBR) offering a higher-level abstraction and Per-Rotor Thrust (PRT) offering a more fine-grained manipulation of individual rotors."}, {"title": "Collective Thrust and Body Rates.", "content": "A typical mode of drone control is to specify a single collective thrust command along with body rates for roll, pitch, and yaw. This higher-level abstraction hides many hardware-specific parameters of the drone, often leading to more stable training. It also simplifies sim-to-real transfer by reducing the reliance on precise modeling of individual rotor dynamics."}, {"title": "Per-Rotor Thrust.", "content": "Alternatively, the drone can directly control each rotor's thrust. This fine-grained control allows the policy to fully exploit the drone's agility and maneuverability. However, it typically demands a more accurate model of the drone's hardware and may increase the difficulty of sim-to-real deployment."}, {"title": "3.4. Reward Functions", "content": "The reward function for each task consists of three parts, including the misbehave penalty for general motion control, the task reward for task completion, and the shaping reward to accelerate training."}, {"title": "Misbehave Penalty.", "content": "This term is consistent across all tasks and penalizes undesirable behaviors related to general drone motion control, such as crashes, collisions, and invalid hits. By imposing penalties for misbehavior, the drones are guided to maintain physically plausible trajectories and avoid actions that could lead to control failure."}, {"title": "Task Reward.", "content": "Each task features a primary objective-based reward that encourages the successful completion of the task. For example, in solo bump tasks, the drone will get a reward of 1 for each successful hit of the ball. Since the task rewards are typically sparse, agents must rely on effective exploration to learn policies that complete the task."}, {"title": "Shaping Reward.", "content": "Due to the sparse nature of many task rewards, relying solely on the misbehave penalty and the task reward can make it difficult for agents to successfully complete the tasks. To address this challenge, we introduce additional shaping rewards in multi-agent tasks to help steer the learning process. For example, the drone's movement toward the ball is rewarded when a hit is required. By providing additional guidance, the shaping rewards significantly accelerate learning in complex tasks.\nWe note that while our reward and observation design is effective for algorithm testing and yields sensible results, it is not optimized for real-world deployment scenarios. To encourage further exploration, we provide a flexible user API to enable experiments with improved designs."}, {"title": "4. VolleyBots Tasks", "content": "Inspired by the way humans progressively learn to play volleyball, we introduce a series of tasks that systematically assess both low-level motion control and high-level strategic play, as shown in Fig. 2. These tasks are organized into three categories: single-agent, multi-agent cooperative, and multi-agent competitive. Each category aligns with standard volleyball drills or match settings commonly adopted in human training, ranging from basic ball control, through cooperative play, to full competitive games. Evaluation metrics vary across tasks to assess performance in motion control, cooperative teamwork, and strategic competition. The detailed configuration and reward design of each task can be found in Appendix B."}, {"title": "4.1. Single-Agent Tasks", "content": "Single-agent tasks are designed to follow typical solo training drills used in human volleyball practice, including Back and Forth, Hit the Ball, and Solo Bump. These tasks evaluate the drone's basic capabilities such as flight stability, motion control, and ball-handling proficiency."}, {"title": "Back and Forth.", "content": "The drone sprints between two designated points to complete as many round trips as possible within the time limit. This task is analogous to the back-and-forth sprints in volleyball practice and requires basic motion control of the drone. The performance is evaluated by the number of completed round trips within the time limit."}, {"title": "Hit the Ball.", "content": "The ball is initialized directly above the drone, and the drone hits the ball once to make it land as far as possible. This task is analogous to the typical hitting drill in volleyball and requires both motion control and ball-handling proficiency. The performance is evaluated by the distance of the ball's landing position from the initial position."}, {"title": "Solo Bump.", "content": "The ball is initialized directly above the drone, and the drone bumps the ball in place to a specific height as many times as possible within the time limit. This task is analogous to the solo bump drill in human practice and requires motion control, ball-handling proficiency, and stability. The performance is evaluated by the number of successful bumps within the time limit."}, {"title": "4.2. Multi-Agent Cooperative Tasks", "content": "Multi-agent cooperative tasks are inspired by standard two-player training drills used in volleyball teamwork, including Bump and Pass, Set and Spike (Easy), and Set and Spike (Easy). Besides basic motion control and ball handling, these tasks also require teamwork and cooperation."}, {"title": "Bump and Pass.", "content": "Two drones work together to bump and pass the ball to each other back and forth as many times as possible within the time limit. This task is analogous to the two-player bumping practice in volleyball training and requires homogeneous multi-agent cooperation. The performance is evaluated by the number of successful bumps within the time limit."}, {"title": "Set and Spike (Easy).", "content": "Two drones take on the role of a setter and an attacker. The setter passes the ball to the attacker, and the attacker then spikes the ball downward to the target region on the opposing side. This task is analogous to the setter-attacker offensive drills in volleyball training and requires heterogeneous multi-agent cooperation. The performance is evaluated by the success rate of the downward spike to the target region."}, {"title": "Set and Spike (Hard).", "content": "Similar to Set and Spike (Easy) task, two drones act as a setter and an attacker to set and spike the ball to the opposing side. The difference is that there is a rule-based defense board on the opposing side to intercept the attacker's spike. The presence of the defense board further improves the difficulty of the task, requiring the drones to optimize their speed, precision, and cooperation to defeat the defense board. The performance is evaluated by the success rate of the downward spike that defeats the defense racket."}, {"title": "4.3. Multi-Agent Competitive Tasks", "content": "Multi-agent competitive tasks follow the standard volleyball match rules, including the competitive 1 vs 1 task and the mixed cooperative-competitive 3 vs 3 task. These tasks evaluate both the low-level motion control and the high-level strategic play of the drone policy."}, {"title": "1 vs 1.", "content": "One drone on each side competes against the other in a volleyball match and wins by hitting the ball in the opponent's court. When the ball is on its side, the drone is allowed only one hit to return the ball to the opponent's court. This two-player zero-sum setting creates a purely competitive environment that requires both precise flight control and strategic gameplay. To evaluate the performance of the learned policy, we consider three typical metrics including the exploitability, the average win rate against other learned policies, and the Elo rating (Elo & Sloan, 1978). More specifically, the exploitability is approximated by the gap between the learned best response's win rate against the evaluated policy and its expected win rate at Nash equilibrium, and the Elo rating is computed by running a round-robin tournament between the evaluated policy and a fixed population of policies."}, {"title": "3 vs 3.", "content": "Three drones on each side form a team to compete against the other team in a volleyball match. The drones in the same team cooperate to serve, pass, spike, and defend within the standard rule of three hits per side. This is a challenging mixed cooperative-competitive game that requires both cooperation within the same team and competition between the opposing teams. Moreover, the drones are required to excel at both low-level motion control and high-level game play. We evaluated the policy performance using approximate exploitability, the average win rate against other learned policies, and the Elo rating of the policy."}, {"title": "5. Benchmark Results", "content": "We present extensive experiments to benchmark representative (MA)RL and game-theoretic algorithms in our VolleyBots testbed. Specifically, for single-agent tasks, we benchmark two RL algorithms and compare their performance under different action space configurations. For multi-agent cooperative tasks, we evaluate four MARL algorithms and compare their performance with and without reward shaping. For multi-agent competitive tasks, we evaluate four game-theoretic algorithms and provide a comprehensive analysis across multiple evaluation metrics. We identify a key challenge in VolleyBots is the hierarchical decision-making process that requires both low-level motion control and high-level strategic play. We further show the potential of hierarchical policy in our VolleyBots testbed by implementing a simple yet effective baseline for the challenging 3 vs 3 task. Detailed discussion about the benchmark algorithms and more experiment results can be found in Appendix C and D."}, {"title": "5.1. Results of Single-Agent Tasks", "content": "We evaluate two RL algorithms including Deterministic Policy Gradient (DDPG) (Lillicrap, 2015) and Proximal Policy Optimization (PPO) (Schulman et al., 2017) in three single-agent tasks. We also compare their performance under different action spaces including CTBR and PRT. The averaged results over three seeds are shown in Table 2.\nUsing the same number of training frames, the performance of PPO and DDPG shows a clear distinction in all three tasks. PPO consistently achieves high task performance in all tasks, while DDPG struggles to learn effective policies that complete these tasks meaningfully. This disparity can be attributed to PPO's stable on-policy updates, which facilitate efficient exploration and robust learning, whereas DDPG's deterministic policy and reliance on off-policy updates result in different learning dynamics.\nComparing different action spaces, the final results indicate that PRT slightly outperforms CTBR in most tasks. This outcome is likely due to PRT providing more granular control over each motor's thrust, enabling the drone to maximize task-specific performance with precise adjustments. On the other hand, CTBR demonstrates a slightly faster learning speed in some tasks, as its higher-level abstraction simplifies the control process and reduces the learning complexity. For optimal task performance, we use PRT as the default action space in subsequent experiments. More experiment results and learning curves are provided in Appendix D.2."}, {"title": "5.2. Results of Multi-Agent Cooperative Tasks", "content": "We evaluate four MARL algorithms including Multi-Agent DDPG (MADDPG) (Lowe et al., 2017), Multi-Agent PPO (MAPPO) (Yu et al., 2022), Heterogeneous-Agent PPO (HAPPO) (Kuba et al., 2021), Multi-Agent Transformer (MAT) (Wen et al., 2022) in three multi-agent cooperative tasks. We also compare their performance with and without reward shaping. The averaged results over three seeds are shown in Table 3.\nComparing the MARL algorithms, on-policy methods like MAPPO, HAPPO, and MAT successfully complete all three cooperative tasks and exhibit comparable performance, while off-policy method like MADDPG fails to complete these tasks. These results are consistent with the observation in single-agent experiments, and we use MAPPO as the default algorithm in subsequent experiments for its consistently strong performance and efficiency.\nAs for different reward functions, it is clear that using reward shaping leads to better performance, especially in more complex tasks like Set and Spike (Hard). This is because the misbehave penalty and task reward alone are usually sparse and make exploration in continuous space particularly challenging. Such sparse setups can serve as benchmarks to evaluate the exploration ability of MARL algorithms. On the other hand, shaping rewards provide intermediate feedback that guides agents toward task-specific objectives more efficiently, and we use shaping rewards in subsequent experiments for efficient learning. More experimental results and learning curves are provided in Appendix D.3."}, {"title": "5.3. Results of Multi-Agent Competitive Tasks", "content": "We evaluate four game-theoretic algorithms including self-play (SP), Fictitious Self-Play (FSP) (Heinrich et al., 2015), Policy-Space Response Oracles (PSRO) (Lanctot et al., 2017) with a uniform meta-solver (PSROUniform), and a Nash meta-solver (PSRONash) in multi-agent competitive tasks. Their performance is evaluated by approximate exploitability, the average win rate against other learned policies, and Elo rating. The results are shown in Table 4. More results and implementation details are provided in Appendix D.4.\nIn the 1 vs 1 task, all algorithms manage to learn basic behaviors like returning the ball and positioning for subsequent volleys. However, the exploitability metrics indicate that the learned policies are still far from achieving a Nash equilibrium, suggesting that the strategies lack robustness in this two-player zero-sum game. This performance gap highlights the inherent challenge of hierarchical decision-making in this task, where drones must simultaneously execute precise low-level motion control and engage in high-level strategic gameplay under volleyball rules. This challenge presents new opportunities for designing algorithms that can better integrate hierarchical decision-making capabilities.\nIn the 3 vs 3 task, algorithms exhibit minimal progress, such as learning to serve the ball, but fail to produce other strategic behaviors. This outcome underscores the compounded challenges in this scenario, where each team of three drones must not only cooperate internally but also compete against the opposing team. The increased difficulty of achieving high-level strategic play in such a mixed cooperative-competitive environment further amplifies the hierarchical challenges observed in 1 vs 1. As a result, the 3 vs 3 task serves as a highly demanding benchmark that highlights the need for new approaches for learning in complex multi-agent settings with a hierarchical decision process."}, {"title": "5.4. Hierarchical Policy", "content": "We further investigate hierarchical policies as a promising approach. Using the 3 vs 3 task as an example, we first employ the PPO algorithm to develop a set of low-level skill policies, including Hover, Serve, Pass, Set, and Attack. The details of low-level skills can be found in Appendix D.5. Next, we design a rule-based high-level strategic policy to assign low-level skills to each drone. For the Serve and Attack skills, the high-level policy also determines whether to hit the ball to the left or right, with an equal probability of 0.5 for each direction. Fig. 4 illustrates two typical demonstrations of the hierarchical policy attaching the Serve skill to drone 1 in a serve scenario and the Attack skill to drone 3 in a rally scenario. In accordance with volleyball rules, the high-level policy uses an event-driven mechanism, triggering decisions whenever the ball is hit. We evaluate the average win rate of 1000 episodes where the hierarchical policy competes against the SP policy. The results show that the hierarchical policy achieves a significantly higher win rate of 86%. While the current design of the hierarchical policy is in its early stages, it demonstrates substantial potential and offers valuable inspiration for future developments."}, {"title": "6. Sim-to-Real", "content": "We use the Solo Bump task as a demonstration of the policy's ability to zero-shot transfer to the real world. We use a quadrotor with a rigidly mounted badminton racket, as shown in Fig. 1. The state of both the drone and the ball is captured using a motion capture system. The drone is modeled as a rigid body, with its position and orientation provided by the motion capture system. The drone's velocity is estimated using an Extended Kalman Filter (EKF) that fuses pose data from the motion capture system and IMU data from the PX4 Autopilot. The ball is modeled as a point mass, with its position sent by the motion capture system and its velocity indirectly computed through a Kalman Filter. The drone's dynamics parameters and the ball's properties are determined through system identification.\nTo simulate real-world noise and imperfect execution of actions, small randomizations are introduced in the ball's initial position, coefficient of restitution, and the ball's rebound velocity after each collision with the drone. Inspired by (Chen et al., 2024b), we also add a smoothness reward to encourage smooth actions. The policy uses CTBR as output and is deployed on the onboard Nvidia Orin processor. Experiment results show that the drone successfully performs bump tasks multiple times, providing initial evidence of sim-to-real transfer capability. To support further research, we make the drone configuration, model checkpoint, and real-world deployment videos publicly available on our website. We hope this will accelerate progress in this field."}, {"title": "7. Conclusion", "content": "We introduce VolleyBots, a new MARL testbed where drones cooperate and compete in volleyball under real-world physical dynamics. VolleyBots bridges the gap between existing MARL testbeds by introducing a turn-based interaction model, a hierarchical decision-making process, and a high-fidelity simulation for seamless sim-to-real transfer. Through extensive benchmarks from single-agent tasks to multi-agent cooperative and competitive tasks, we show that existing algorithms struggle to solve complex tasks that require both low-level motion control and high-level strategic play. We also showcase the feasibility of deploying policies trained in simulations directly onto physical drones, emphasizing VolleyBots' sim-to-real transfer and practical utility in real-world applications. We envision VolleyBots as a realistic and challenging testbed for MARL research in high-mobility, agile robotic platforms like drones."}, {"title": "A. Details of VolleyBots Environment", "content": ""}, {"title": "A.1. Court", "content": "The volleyball court in our environment is depicted in Fig. 5. The court is divided into two equal halves by the y-axis, which serves as the dividing line separating the two teams. The coordinate origin is located at the midpoint of the dividing line, and the x-axis extends along the length of the court. The total court length is 18m, with x = -9 and x = 9 marking the ends of the court. The y-axis extends across the width of the court, with a total width of 9 m, spanning from y = -4.5 to y = 4.5. The net is positioned at the center of the court along the y-axis, with a height of 2.43 m, and spans horizontally from (0, -4.5) to (0, 4.5)."}, {"title": "A.2. Drone", "content": "We use the Iris quadrotor model (Furrer et al., 2016) as the primary drone platform, augmented with a virtual \"racket\" of radius 0.2 m and coefficient of restitution 0.8 for ball striking. The drone's root state is a vector with dimension 23, including its position, rotation, linear velocity, angular velocity, forward orientation, upward orientation, and normalized rotor speeds.\nThe control dynamics of a multi-rotor drone are governed by its physical configuration and the interaction of various forces and torques. The system's dynamics can be described as follows:\n$\\dot{x}_{W} = v_{W},\\$\n$\\frac{1}{2} J \\dot{\\omega} = J^{-1} (\\eta - \\omega \\times J \\omega)$"}, {"title": null, "content": "$w = R_{WB}f + g + F$\\n$\\dot{q} = q \\otimes \\omega,$"}, {"title": null, "content": "where $x_{W}$ and $v_{W}$ represent the position and velocity of the drone in the world frame, $R_{WB}$ is the rotation matrix converting from the body frame to the world frame, $J$ is the diagonal inertia matrix, $g$ denotes gravity, $q$ is the orientation represented by quaternions, and $w$ is the angular velocity. The quaternion multiplication operator is denoted by $\\otimes$. External forces $F$, including aerodynamic drag and downwash effects, are also considered. The collective thrust $f$ and body rate $\\eta$ are computed based on per-rotor thrusts $f_{i}$ as:\n$f = \\sum R^{(i)}_{B}f_{i},\\$\n$\\eta = \\sum T^{(i)} \\times f_{i} + k_{i} f_{i}$"}, {"title": null, "content": "where $R^{(i)}_{B}$ and $T^{(i)}$ are the local orientation and translation of the i-th rotor in the body frame, and $k_{i}$ is the force-to-moment ratio."}, {"title": "A.3. Defense Racket", "content": "We assume a thin cylindrical racket to mimic a human-held racket for adversarial interactions with a drone. When the ball is hit toward the racket's half of the court, the racket is designed to intercept the ball at a predefined height $h_{pre}$. Since the ball's position and velocity data can be directly acquired, the descent time $t_{pre}$, landing point $P_{ball\\_land}$, and pre-collision velocity $v_{ball\\_pre}$ can be calculated using projectile motion equations. Additionally, to ensure the ball is returned to a designated position $P_{bdes}$ and crosses the net, the post-collision motion duration $t_{post}$ of the ball is set to a sufficiently large value. This allows the projectile motion equations to similarly determine the post-collision velocity $v_{ball\\_post}$. Based on these conditions, the required collision position $P_{collision}$, orientation $\\theta_{collision}$ and velocity $v_{collision}$ of the racket can be derived as follows:\n$P_{collision} = P_{ball\\_land}$"}, {"title": null, "content": "$\\vec{n}_{collision} = \\frac{\\vec{v}_{ball\\_post} - \\vec{v}_{ball\\_pre}}{||\\vec{v}_{ball\\_post} - \\vec{v}_{ball\\_pre}||} = [sin \\rho cos \\tau, \\cdot sin \\tau, cos \\rho cos \\tau]$"}, {"title": null, "content": "$\\theta_{collision} = [-arcsin \\vec{n}_{collision}(2), arctan(\\frac{\\vec{n}_{collision}(1)}{\\vec{n}_{collision}(3)})]$\n$\\vec{v}_{collision} = \\frac{1}{1 + \\beta} (\\beta\\vec{v}_{ball\\_pre} + \\vec{v}_{ball\\_post})$"}, {"title": null, "content": "where $\\vec{n}_{collision}$ represents the normal vector of the racket during impact, $\\tau$ denotes the roll angle of the racket, $\\rho$ denotes the pitch angle, while the yaw angle remains fixed at 0, and $\\beta$ represents the restitution coefficient. To simulate the adversarial interaction as realistically as possible, we impose direct constraints on the racket's linear velocity and angular velocity. Based on the simulation time step $t_{step}$ and the descent time $t_{post}$ of the ball, we can calculate the required displacement $d = \\frac{P_{bdes}-P_{ball\\_land}}{t_{step}}$ and rotation angle $\\theta = \\frac{\\theta_{collision}}{t_{post}}$ that the racket must achieve within each time step. If both $d$ and $\\theta$ do not exceed their respective limits ($d_{max}$ and $\\theta_{max}$), the racket moves with linear velocity $d$ and angular velocity $\\theta$. Otherwise, the values are set to their corresponding limits $d_{max}$ and $\\theta_{max}$."}, {"title": "B. Details of Task Design", "content": ""}, {"title": "B.1. Back and Forth", "content": "Task Definition. The drone is initialized at an anchor position (4.5, 0, 2), i.e., the center of the red court with a height of 2 m. The other anchor position is (9.0, 4.5, 2), with the target points switching between two designated anchor positions."}, {"title": "Sim-to-Real", "content": "$\\beta = \\frac{v_{ball\\_post\\_n}nc^T}{v_{ball\\_pre\\_n}n^T c}$"}]}