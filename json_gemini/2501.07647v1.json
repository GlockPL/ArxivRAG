{"title": "BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video Representations", "authors": ["Weixi Feng", "Chao Liu", "Sifei Liu", "William Yang Wang", "Arash Vahdat", "Weili Nie"], "abstract": "Existing video generation models struggle to follow complex text prompts and synthesize multiple objects, raising the need for additional grounding input for improved controllability. In this work, we propose to decompose videos into visual primitives \u2013 blob video representation, a general representation for controllable video generation. Based on blob conditions, we develop a blob-grounded video diffusion model named BlobGEN-Vid that allows users to control object motions and fine-grained object appearance. In particular, we introduce a masked 3D attention module that effectively improves regional consistency across frames. In addition, we introduce a learnable module to interpolate text embeddings so that users can control semantics in specific frames and obtain smooth object transitions. We show that our framework is model-agnostic and build BlobGEN-Vid based on both U-Net and DiT-based video diffusion models. Extensive experimental results show that BlobGEN-Vid achieves superior zero-shot video generation ability and state-of-the-art layout controllability on multiple benchmarks. When combined with an LLM for layout planning, our framework even outperforms proprietary text-to-video generators in terms of compositional accuracy.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in text-to-video generation have enabled us to generate more realistic videos with high visual quality and intricate motions. These advancements are driven by new model architectures [2, 13, 49], improved training techniques [1, 4, 18] and large-scale video datasets [6, 46]. Despite the progress, existing text-to-video models still struggle to follow complex prompts, where they often neglect key objects or confuse multiple objects as one concept. In addition, users cannot control semantic transitions or camera motion with merely text descriptions with these models. Therefore, it remains an open challenge to enhance the compositionality and controllability of video generators with layout guidance in the diffusion process.\nTo resolve these challenges, recent studies propose to condition video diffusion models on visual layouts. Since a text prompt can be ambiguous in object locations and visual appearances, video generators often fail to generate scenes with large motion or complex compositions. Additional grounding inputs can guide the generation process for enhanced controllability. These layouts are usually represented by bounding boxes moving across frames [19, 22, 23, 42]. Compared to other modalities such as depth [34] or semantic maps [52], bounding boxes are easier to create and manipulate by users while providing coarse-grained information of local objects. However, 2D bounding boxes lack perspective invariance: the 3D counterpart of a 2D bounding box on an image is not a 3D bounding box and vice versa. This makes it difficult to synthesize 3D scenes using models grounded by bounding-boxes.\nIn this work, we introduce a new type of visual layouts for video generation, named blob video representations, to serve as grounding conditions. Each blob sequence corresponds to an object instance and can be automatically extracted from videos (or 3D scenes), making it a more general and robust representation for different visual domains. Specifically, a blob video representation has two components: 1) the blob parameters, which formulate a tilted ellipse to specify the object's location, size, and orientation; and 2) the blob description, which is a free-form language description of the object's visual attributes. With this definition, our blob representation enables both motion and semantic control of visual compositions. It is also convenient for users to create and manipulate such representations as the blob parameters can be represented as structured text.\nWhile layout conditions have been widely studied in image generation [5, 20, 31, 52], directly applying these methods in video can lead to temporal inconsistency or compromised layout control [19]. Some recent studies have adapted these conditions for video generation with new techniques [19, 42]. However, they still suffer from the above issues and are limited to class conditions for each object box. To this end, we develop a blob-grounded text-to-video diffusion framework, termed BlobGEN-Vid, that is built upon existing video diffusion models using blob representations as grounding input. In our framework, we introduce a masked 3D attention module that facilitates object-centric spatial-temporal attention. We also utilize masked cross-attentions [31] to fuse free-form object descriptions into the blob regions. As some frames do not have blob captions, we integrate a context interpolation module to enhance semantic transition throughout time.\nBlobGEN-Vid is a model-agnostic framework that can be applied to both UNet [12] and DiT [32] based diffusion models. Our experiments in open-domain video generation indicate that BlobGEN-Vid outperforms existing layout-guided video generators by a large margin in multiple dimensions. We evaluate BlobGEN-Vid on a wide range of benchmarks [44] and show that it improves the layout controllability by at least 20% in mIOU and prompt alignment by 5% in CLIP similarity. When combined with a large language model (LLMs) for blob planning, our pipeline outperforms proprietary video generators in mutiple aspects. Last but not least, we demonstrate that BlobGEN-Vid also achieves improved consistency and camera control in multi-view image generation in indoor scenes.\nOur contributions: (i) We propose a new blob representation for text-to-video generation that enables fine-grained control of each object such as its motion and appearance. (ii) We propose BlobGEN-Vid, a blob-grounded framework that incorporates two types of masked attention modules and a context interpolation module to pre-trained video diffusion models for regional control and temporal consistency. BlobGEN-Vid can be applied to both UNet and DiT based diffusion backbones. (iii) We conduct extensive experiments in open-domain video generation and multi-view indoor scene generation, demonstrating BlobGEN-Vid's superior object-level controllability and temporal consistency"}, {"title": "2. Related work", "content": "Text-to-video generation. The field of text-to-video generation (T2V) has gained much attention thanks to the advancement in new model architecture [2, 16, 32, 54], large-scale video datasets [6, 46], and improved training techniques [4, 7, 18]. There are mainly two streams of video diffusion models in terms of model architecture. Primitive video diffusion models such as Video LDM [2], VideoCrafter [3, 4] and some others [41, 43, 51] are achieved by adding temporal self-attention modules into a U-Net diffusion backbone [12]. The U-Net usually operates in the latent space [40] and can be inherited from a pre-trained text-to-image model such as Stable Diffusion [2]. Recently, as the scale of model and dataset increases, a few models based on Diffusion Transformers (DiT) [32] have been proposed. For example, CogVideoX [49] proposes an expert DiT with stacked 3D attention blocks working on the concatenation of context embeddings and visual tokens. As synthetic videos are becoming more realistic, it is essential to endow controllability to the generation process.\nCompositional video generation. While compositional generation have been extensively studied in the image domain [20, 25, 31], the compositional tasks in video generation still demand more focus [38, 48]. Recently, a few works start to tackle compositional video generation. Vico [31] regularizes text tokens' attention maps to improve scene correctness with multiple objects. VideoTetris [38] proposes a spatial-temporal composing mechanism to deal with compositional change in long video generation. Several benchmarks are proposed to characterize compositionality [28, 29]. Beyond issues carried from image generation, the temporal dimension in videos introduces new challenging problems. For example, T2V-CompBench [35] and TC-Bench [8] features dynamic binding relations or object status change. These benchmarks show that existing T2V models lack of robust compositionality in generating videos with complex scenes and motions.\nLayout-guided video generation. There are several studies that attempt to add layout condition on top of a pre-trained T2V model. TrackDiffusion [19] inserts trainable gated cross attentions with an instance enhancer to improve object consistency across frames. Boximator [42] applies a self-attention to fuse object category and box coordinates into visual tokens. It also proposes self-tracking technique that fine-tunes the model to generate visible bounding boxes around objects first and then forget the behavior. LVD [22] and VideoDirectorGPT [23] adopt LLMs to plan bounding boxes for several keyframes, which are then passed to a video generator. As shown later, these methods may suffer from inconsistency issue and lack of controllabbility with complex layouts."}, {"title": "3. Preliminary: BlobGEN", "content": "BlobGEN [31] first introduced blob representations to guide the open-domain image generation. It has shown that blobs can provide more fine-grained controllability than other visual layouts (such as bounding boxes) in previous layout-conditioned approaches [9, 20], which motivates us to use blob representations for video generation. We will next introduce the blob representations and key method design in BlobGEN, which our method is built upon.\nBlob representations. The blob representations denote the object-level visual primitives in a scene, each of which consists of two components: blob parameters and blob description. The blob parameters depict an object's shape, size and location with a vector of five variables $\\tau = [C_x, C_y, a, b, \\theta]$ that defines a tilted ellipse, where $(C_x, C_y)$ is the center point of the ellipse, a, b are the radii of its semi-major and semi-minor axes, and $\\theta \\in (-\\pi, \\pi]$ is the orientation angle of the ellipse. The blob description denoted by s captures the visual appearance of an object using a region-level synthetic caption extracted by an image captioning model. Compared with other visual layouts (such as boxes and semantic maps), blob representations have both two advantages: 1) they retain the fine-grained spatial and appearance information about the objects in a complex scene; and 2) they can be easily constructed and manipulated by either human users or LLMs with in-context learning, since they are essentially in the form of text sentences [31].\nTo encode blob representations into blob embeddings in BlobGEN, we first obtain the blob parameter embedding $e_\\tau \\in \\mathbb{R}^d$ with Fourier feature encoding [37] and the blob description embedding $e_s := [e_{s_1},\u2026,e_{s_L}] \\in \\mathbb{R}^{L \\times d}$ with CLIP text encoder, separately, where $d$ denotes the embedding feature size and $L$ denotes the sentence length of blob description. We then concatenate $e_\\tau$ with each $e_{s_l}$ along the embedding feature dimension to get $\\bar{e}_l := [e_\\tau; e_{s_l}] \\in \\mathbb{R}^{d}$ and feed it into an MLP network to get the final blob embeddings $e_{blob} = MLP([\\bar{e}_1, ..., \\bar{e}_L]) \\in \\mathbb{R}^{L \\times d}$.\nMasked cross-attention. To incorporate the blob representations into the existing text-to-image models, BlobGEN adopts the similar network design of GLIGEN [20] and introduces new masked cross-attention layers in a gated way. Specifically, in the masked cross-attention layer, each blob embedding only attends to visual features in its local region as the visual feature maps are masked by the (rescaled) blob ellipses. Assume there are N blobs in an image, and the visual feature map is denoted by $g \\in \\mathbb{R}^{hw \\times d_g}$, where h and w"}, {"title": "4. Method", "content": "We first describe the extension of BlobGEN to video generation, including new blob representations for the video data and new masked spatial cross-attention layers that fuse blob video representations to video diffusion networks. Furthermore, we introduce new masked 3D attention layers to improve temporal consistency in the object level. Finally, we present blob video generation based on LLMs, which can serve as a stage before BlobGEN-Vid to save human efforts from manually designing layouts."}, {"title": "4.1. Blob representations for videos", "content": "Given a video of frame length T, we extract objects from the first frame and track each of the extracted objects in subsequent T - 1 frames. Accordingly, we obtain a blob video of the same frame length that contains N blob ellipses in each frame. Similar to BlobGEN, the nth object's spatial features (including shape, size and location) in the tth frame are depicted by blob parameters $\\tau_t^{(n)} := [C_x, C_y, a, b, \\theta]$, defined in the same way as Section 3. The blob video captures how the spatial features of each object and their spatial arrangements evolve temporally. On one hand, it can easily capture the object motion in a natural video (e.g., a cat running on the grass), by looking into the relative movement of a blob (e.g., cat) to other blobs (e.g., grass). On the other hand, it can also capture the camera motion by referring to the joint movements and/or deformations of all blobs.\nSimilar to BlobGEN, we also pair blobs with free-form text descriptions to provide fine-grained details of the local objects. Compared to previous works that use a single class label for each object across frames [19, 42], our blob captions complement the spatial layout with more information such as appearance attributes (color, texture, etc.) and camera focus. Besides, since many visual features of an object may change in a video, it becomes very challenging to use a single blob video caption to describe the object appearance and its dynamic variation across frames. Thus, we opt to apply multiple frame-wise object captions for each blob, which are independently extracted from an existing image captioning model. However, we do not apply blob captions to every object in every single frame because 1) it is neither efficient in the data annotation stage nor convenient for users to construct during inference, and 2) consecutive frames in most videos have little change in objects' visual features. Instead, we assign blob captions at a fixed interval across time, spacing them every k frames.\nIn summary, our blob video representations in a video are comprised of 1) blob parameters $\\{\\tau_t^{(n)}\\}$ for every single frame (t = 1,2,\u2026\u2026,T) and every single object (n = 1,2,\u2026, N), and 2) blob descriptions $\\{s_{t_k}^{(n)}\\}$ for every k frame ($t_k$ = 1,k + 1,...,T) and every single object (n = 1,2,...,N). Particularly, we denote the frames indexed by $t_k$ as anchor frames since they contain both blob parameters and blob descriptions. As we will show later, we can obtain complete context features for other frames through context interpolations based on the blob captions from anchor frames. This design offers consistent contextual information to avoid modality mismatch while applying our blob video representations."}, {"title": "4.2. Blob-grounded text-to-video generation", "content": "To incorporate blob video grounding into the pre-trained video diffusion models, we follow the design of Blob-"}, {"title": "Context interpolation.", "content": "To obtain blob embeddings, we follow BlobGEN to encode blob representations for each single frame independently. That is, for the nth object in the tth frame, we first get the blob parameter embedding $e_{\\tau_{t,n}}$ and blob description embedding $e_{t,n}^s$, and concatenate them along the embedding feature dimension as input to an MLP network for its blob embedding $e_{blob}^{t,n}$. However, not all frames are paired with blob captions, which means we do not have blob description embedding $e_{t,n}^s$ for those non-anchor frames whose frame index $t \\neq t_k$.\nA naive approach is to encode an empty text string with CLIP text encoder and use it as the blob description embedding for all non-anchor frames. But it can easily introduce inconsistency across frames due to the large contextual mismatch. To overcome this issue, we propose a simple method called context interpolation that linearly interpolates the blob description embeddings of two consecutive anchor frames for each non-anchor frame in the middle. Formally, given the indices of two anchor frames $t_k$ and $t_{k+1}$ where $t_{k+1} = t_k + k$, the interpolated blob description embedding of the non-anchor frame indexed by $t \\in (t_k, t_{k+1})$ is given by\n$$e_{t,n}^s = \\frac{t_{k+1}-t}{k}e_{t_{k+1},n}^s + \\frac{t-t_k}{k}e_{t_k,n}^s \\qquad (2)$$\nIntuitively, this linear interpolation ensures a smooth semantic transitioning of object captions across all frames in the CLIP embedding space, leading to better temporal consistency and blob-guided controllability. Besides linear interpolation, some learnable nonlinear interpolations can also be considered. For example, we can train a Perceiver IO network [14] that takes the blob description embeddings of anchor frames as input and learns the blob descriptions embeddings of other frames."}, {"title": "Masked spatial cross-attention.", "content": "The extension of masked cross-attention from BlobGEN to fuse blob video representations with video features is straightforward. Similar to spatial attention layers in many video diffusion backbones [4], both the visual features and blob embeddings are first reshaped in the form of $(B T (haw) c) \\rightarrow ((B T) (hw) c)$ and then they can be fused by applying the masked cross-attention in Eq. (1). That is, we fuse blob embeddings and visual features in the same frame independently for all the frames. This design makes the masked spatial cross-attention layers to solely focus on promoting the frame-wise alignment of generated content and the blob conditioning, without worrying about temporal consistency."}, {"title": "Masked 3D self-attention.", "content": "The masked spatial cross-attention can only apply per-frame consistency between frames and blobs and cannot guarantee temporal consistency across frames. To improve temporal consistency, we propose new masked 3D self-attention layers to enforce object-level temporal consistency. Note that even though many video diffusion models [1] based on U-Net are equipped with temporal self-attention, it only allows each \"pixel\" of the visual feature map in a frame to attend to \"pixels\" at the same spatial location in other frames. However, blobs provide a rough location of each object over time, and thus we can impose stronger coherence by biasing the attention towards the same object over time.\nSpecifically, in masked 3D self-attention, we flatten all three dimensions in a video feature (i.e., T, h, w) into one dimension and denote the resulting feature as $g \\in \\mathbb{R}^{Thw \\times d}$. Then we obtain query, key and value with three linear projections for self-attention as $q = gW_q, k = gW_k, v = gW_v$, all in the shape of $\\mathbb{R}^{Thw \\times d}$. Then, the masked 3D self-attention can be written as:\n$$MaskSA3D := Softmax\\left(\\frac{qk^T}{\\sqrt{d}} + M_{blob}\\right)v, \\qquad (3)$$\nwhere $M_{blob} \\in \\mathbb{R}^{Thw \\times Thw}$ is a 3D mask determined by blob ellipses across frames, which we describe in the next.\nSimilar to BlobGEN, we denote the binary blob mask for the nth object in the tth frame as $m_{t,n} \\in \\mathbb{R}^{hw}$, where its ith entry (denoted as $m_{t,n}^i$) equals 1 if the location i is within the blob ellipse, and 0 otherwise. Besides the N blob masks corresponding to N objects in each frame, we introduce another binary mask, called background mask, as $m_{t,bg} = 1 - \\cup_{n=1}^{N} m_{t,n}$, resulting in N + 1 blob masks that cover the whole (h \u00d7 w) spatial space. Given any two indices $i, j \\in \\{1,2,\u2026\u2026,Thw\\}$, we then define each entry of $M_{blob}$ indexed by (i, j) as\n$$M_{blob}^{i,j} =\\begin{cases}\n0 & \\text{if } m_{t,n}^i m_{t',n}^j = 1, \\exists t,t',n \\\\\n-\\infty & \\text{if } m_{t,bg}^i m_{t',bg}^j = 1, \\forall t,t' \\\\\n-\\infty & \\text{otherwise} \\\\\n\\end{cases} \\qquad (4)$$\nwhich allows the local object feature for the t frame (depicted by a blob ellipse) to only attend to local features of the same object for another frame (including the t frame itself). Note that each background feature only attends to other background features across frames. Thus, this 3D mask design implies an object-centric self-attention mechanism, leading to better object-level cross-frame consistency. Furthermore, the use of $m_{t,bg}$ is critical in practical implementation to avoid having all-zero rows in the input to the softmax function and improve training stability."}, {"title": "4.3. LLMs for blob generation", "content": "Inspired by previous work in using LLMs for layout planning [21-23], we also generate video layouts with in-context learning and structured text. Since video layouts need to expand over time dimension and may have multiple objects per frame, it is important to find a robust structure to represent them. Instead of using self-defined template [38] or stylesheet language [9], we form the layouts as nested dictionaries where frame index, object id, blob parameters and captions are settled in different layers of the structure. LLMs interpret and generate outputs in the same json format that can be directly parsed into blob layouts per frame. In addition, we only generate blobs for a sparse set of frames while interpolate the intermediate blob parameters to make the stage more efficient."}, {"title": "5. Experiment", "content": null}, {"title": "5.1. Experiment setup", "content": "Data preparation. Since there are no available video datasets that provides ground truth blob annotations align with our setting in Sec. 4.1, we build an annotation pipeline to extract blob parameters and captions. In general, we apply Grounding DINO [27] or ODISE [45] to the first frame of each video and obtain segmentation masks. Then we apply SAM2 [33] to every other frame to track the objects. After obtaining all segmentation masks throughout the video, we fit an ellipse to every mask by optimizing the Intersection Over Union (IOU) between the ellipse and the mask area. Using the segmentation masks, we also crop out objects in every eight frames and apply LLaVA-NeXT [26] to get blob captions in these frames.\nOur video-text pairs mainly come from OpenVid-1M [30], VidGEN-1M [36], and a small subset of HD-VILA [46]. We apply heavy filtering to each stage of the annotation pipeline to maintain high quality and good balance between human and non-human objects. We end up with ~1M videos having dense blob annotations for training. For indoor scene videos, we use the processed ScanNet++ [50] from BlobGEN-3D [25] where the frame blobs are projected from 3D blobs fitted on the point cloud segmentation extracted from scenes.\nBenchmarks and metrics. We have three different experiment settings/domains. First, we test layout-to-video generation on 717 validation and test videos of Youtube-VIS 2021 [47]. We report FVD [39] against the ground truth 717 videos for general visual quality, mean Intersection-over-Union (mIOU) for layout controllability, rCLIPt and rCLIP\u00bf for prompt-video alignment, and regional cross-frame CLIP similarity (rCFC) for object consistency. rCLIPt is the CLIP cosine similarity between each blob caption and blob-bounded region in the generated videos, and rCLIP\u00bf is the similarity between regions from generated videos and ground truth videos. Secondly, to evaluate compositionality in T2V setting, we report results on T2V-CompBench [35] and TC-Bench [8], which evaluate composition changes over time in different aspects. They both equip large multimodal models or detection/tracking models for different aspects. Lastly, we evaluate multi-view indoor videos on 1392 test videos from ScanNet++. We report FID and IS for frame quality, FVD"}, {"title": "5.2. Layout-grounded video generation.", "content": "As is shown in Table 1, our method outperforms all baselines in nearly all aspects of evaluation. In particular, our method based on VC2 achieves the highest mIOU score (0.6119), over 20% improvement in spatial controllability over TrackDiffusion [19]. BlobGEN-Vid based on CogVideoX-5B achieves a slightly lower mIOU score as it is trained on only half of the dataset but still outperforms all baselines. As for prompt-video alignment, our method outperforms all baselines by achieving 0.2888 rCLIPt and 0.8364 rCLIP\u00bf scores. As our blob captions are free-form language descriptions of the objects instead of a coarse-grained category name or id, they provide rich semantics to facilitate control of fine-grained details.\nWe show an qualitative comparison between TrackDiffusion (TD) and our method in Fig. 4. BlobGEN-Vid tightly follow the blobs to generate the car while the objects in TD's video often reach out of the box. In addition, our method also demonstrate better prompt-video alignment by showing \"orange sport car\" while the baseline cannot control such semantics because it uses the category label \"car\"."}, {"title": "5.3. Text-to-video generation", "content": "Table 2 shows the evaluation results of our complete text-to-video generation pipeline by combining GPT-40 and BlobGEN-Vid (CogVideoX-5B-based). We adopt in-context learning methods and input two fixed exemplars to GPT-40 to obtain blob parameters and blob captions for a sparse set of frames. Then we linearly interpolate blob parameters and feed the blob conditions into BlobGEN-Vid to generate videos. Our pipeline outperforms proprietary video generators in four challenging compositional issues, including dynamic attribute binding, spatial relation accuracy, motion binding and numerical accuracy."}, {"title": "5.4. Multi-view scene generation", "content": "For multi-view indoor scene generation, we directly compare to BlobGEN-3D [25] as shown in Table 3. BlobGEN-3D is fine-tuned from BlobGEN [31] with a depth-conditioned ControlNet [52] and a warped previous frame. It is an image diffusion model that generates free-view indoor images in an autoregressive frame-by-frame manner. \nWe can see BlobGEN-Vid outperforms BlobGEN-3D in all metrics, especially in video consistency. While BlobGEN-3D without depth condition (row 2 and 5) achieves the lowest FID score, it fails to maintain cross-frame consistency as indicated by the low PSNR (10.06) and CFC values (0.9168). When our proposed masked 3D attention is removed from BlobGEN-Vid (comparing row 7 and 8), PSNR and CFC both decrease, justifying the effectiveness of 3D masks in improving consistency."}, {"title": "5.5. Ablation study", "content": "In Table 4, we present the ablation study on three factors: Masked 3D self-attention, context interpolation method, and training data. We observe that adding the masked 3D self-attention is crucial to facilitate video diffusion models"}, {"title": "6. Conclusions", "content": "In this work, we propose a new layout design for text-to-video generation, called blob representations. The representation contains blob parameters for each object in every frame and paired blob captions in a sparse set of frame. Our free-form blob captions also provide more fine-grained semantics of each object. We then introduce a framework termed BlobGEN-Vid that endows video diffusion models with the ability to condition on blob inputs. BlobGEN-Vid consists of a context interpolation module, allowing more flexible semantic transition, and masked 3D attention blocks to enforce object consistency across frames. We demonstrate the effectiveness of our frame in multiple visual domains and settings. BlobGEN-Vid achieves strong performance in open-domain video generation and multi-view image generation."}, {"title": "A. Data annotation", "content": "Data annotation pipeline. Our data annotation pipeline consists of four to five key steps as shown in Fig. 6. The step 0 is to obtain a list of objects appeared in the video using a VLM. Though previous methods [42] directly use a language parser to extract object nouns or phrases from video captions, we found the parser often extracts words that do not represent concrete entities and introduces additional noise to step 1. Thus we feed videos into LLaVA-NeXT-Video-7B [53] and prompt it to generate a list of objects that appear in each video. Using the instant list, we could apply Grounding DINO [27] in step 1 to obtain segmentation masks for the first frame of the video. We also experiment with ODISE [45], which is a panoptic segmentation model that does not require the instance list from step 0 to work. For most videos, we first apply LLaVA-NeXT+Grounding DINO to get segmentation masks. If the mask coverage is below 20% of the frame size, we apply ODISE to get more dense panoptic annotation. This helps us keep most of the videos for further annotation and hence improve data utilization rate.\nAfter obtaining the segmentation masks in step 1, we apply SAM2 [33] to track each object mask throughout the video. To make the process efficient, we uniformly sample 1/4 of all the frames to do tracking. With the tracking masks for the frames, we can fit a set of blob parameters $(C_x, C_y, a, b, \\theta)$ for each mask. For frames without tracking masks, we linearly interpolate the blob parameters from the closest neighboring frames. As for step 4, we crop a tight rectangle region around each segmentation mask, and feed it to LLaVA-v1.6-mistral-7b [26] to get blob descriptions. For efficiency, we only annotate blob descriptions for the first of every eight frames.\nQualitative visualization. We visualize two example of our data annotation results in Fig. 10 (using Grounding DINO) and Fig. 11 (using ODISE). We observe that the instance list obtained from LLaVA-NeXT-Video-7B [53] usually contain instance names in different hierarchical levels. For example, in Fig. 11, the hat, scarf, and yellow jacket are listed as separate objects. Sometimes, the model would also list \"hands\" as separate objects from the whole human figure. However, it has the drawback of neglecting background objects even though we explicitly emphasize \u201cboth foreground and background\u201d objects in the prompt.\nIn contrast, ODISE [45] has more fine-grained segmentation of background since it applies a long list of category names merged from different datasets. As is shown in Fig. 11, ODISE segments the background into four different parts, including the sky, trees, grass and fence. However, ODISE's category set does not include some general objects or hierarchical parts of objects like \"cartoon character\" or \u201chands/arms\u201d compared to using instance list. In addition, the segmentation labels from ODISE can be less accurate. For example, it annotates the \"cartoon monkey\" as \"costume\" and the \"brown bag\" as \"suitcase\". The issue is mitigated as we use an VLM to obtain free-form blob descriptions instead of adopting ODISE labels."}, {"title": "B. BlobGEN-Vid framework", "content": "Context interpolation module. As shown in Fig. 7, we first encode the blob descriptions in all frames. For non-anchor frames, we use empty strings for the encoding process and later replace them with the learned features. If an object undergoes apparent semantic change (e.g. object changing color), the blob description in the anchor frames would have different meaning, reflected as the color differences in the penultimate feature sequence from CLIP text encoder. Apart from the linear interpolation introduced in Sec. 4.2, we also experiment with a learnable module using PerceiverIO [14]. To ensure object-wise interpolation, we reshape the context embeddings as (BTNL d) \u2192 ((BN) (TL) d) where T denotes the number of latent frames and L is the sequence length of the context features. In Fig. 7, we have omitted B, N and use L = 3 and T = 9 for demonstration purpose. In our implementation,"}, {"title": "C. Implementation details", "content": "Training data. For open-domain video generation, our training dataset is obtained by annotating 160K Open-Vid [30] videos, 460K VIDGEN [36] videos and 320K videos from HDVILA [46]. We try to maintain a good balance between human video and non-human videos where the latter outweighs the former as human figures are more challenging to synthesize. While all 940K videos are used for VideoCrafter2-based training, only half of the videos (~500K) satisfy the length requirement of CogVideoX. Therefore, the training dataset size for BlobGEN-Vid based on CogVideoX is effectively ~500K videos.\nFor the multi-view scene experiment, we use the ScanNet++ dataset, consisting of 1130 training video clips where each clip has 128 frames. As VideoCrafter2 generates videos of 16 frames, we sample 16 consecutive frames from the 128 frames with a stride of 8. Therefore, we obtain 15 sub-clips with overlaps from each 128-frame video. We prepare 517 16-frame clips for validation purpose and 1392 16-frame clips as the testing set for final evaluation.\nModel Architecture and Training. We fine-tuned both VideoCrafter2 (VC2) [4] (U-Net) and CogVideoX-5B [49] (DiT) with our annotated datasets. For VC2 fine-tuning, we add one masked spatial cross-attention in every spatial transformer block and one masked 3D attention after temporal transformer blocks where the latent feature has a spa-"}]}