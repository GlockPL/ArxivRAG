{"title": "PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation", "authors": ["Daniel C. Castro", "Aurelia Bustos", "Shruthi Bannur", "Stephanie L. Hyland", "Kenza Bouzid", "Maria Teodora Wetscherek", "Maria Dolores S\u00e1nchez Valverde", "Lara Jaques P\u00e9rez", "Lourdes P\u00e9rez Rodr\u00edguez", "Kenji Takeda", "Jos\u00e9 Mar\u00eda Salinas", "Javier Alvarez-Valle", "Joaqu\u00edn Galant Herrero", "Antonio Pertusa"], "abstract": "Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) extends RRG by including the localisation of individual findings on the image. Currently, there are no manually annotated chest X-ray (CXR) datasets to train GRRG models.", "sections": [{"title": "Introduction", "content": "In recent years, the use of artificial intelligence (AI) to improve medical image analysis has gained significant interest, with the potential to alleviate radiology workloads and enhance patient care. 1,2 The modelling task of GRRG can be defined as predicting a list of sentences or phrases describing all individual findings in an image, with associated spatial annotations (e.g. bounding boxes) for localisable findings.3\nBecause explainability is crucial in this domain,\u00b9 spatially grounding radiological findings is expected to help with verification of AI-generated draft reports. Such grounding can also underpin new interactive capabilities of medical AI models, facilitating their interpretation by clinicians and even patients. 2\nIn the literature, there exist many CXR image datasets labelled for diagnosis and finding classification tasks, 6-9 or accompanied by textual radiology reports for automated draft report generation.10-13 Some datasets also include spatial annotations to localise labels (for finding, anatomy, or device; e.g. 'pneumothorax\u2019)9,12,14\u201317 or single finding phrases, 18,19 such as 'Left retrocardiac opacity'.\nHowever, such datasets are not sufficient to construct full grounded reports, as they lack spatial annotations linked to complete sets of descriptive finding sentences."}, {"title": "Methods", "content": "PadChest is a large-scale CXR dataset which includes more than 160,000 images obtained from 67,000 patients that were interpreted and reported by radiologists at the University Hospital Sant Joan d'Alacant (Spain) from 2009 to 2017. The reports were labeled with 174 different radiological findings, 19 differential diagnoses, and 104 anatomical locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. 20 The text reports publicly released with PadChest had been heavily processed to preserve anonymization, normalise capitalisation, punctuation, accents, stop-words, and affixes.\nFor inclusion in PadChest-GR, each radiographic study in PadChest was first linked to the patient's most recent prior study based on the StudyDate_DICOM field. Studies from the same patient on the same date were excluded, as their order cannot be determined. We selected only frontal images (erect or supine, with projection 'PA', 'AP', or \u2018AP-horizontal') and excluded the few studies with more than one frontal image in either the current or prior study to avoid ambiguity. Paediatric patients (18 years old or younger) were also rejected, as well as studies acquired with a paediatric protocol. We also filtered out any studies that included the 'suboptimal', 'exclude', or 'unchanged' labels in PadChest, as they cannot be reliably annotated."}, {"title": "STRATIFIED DATA SELECTION", "content": "We selected 4,000 abnormal studies (i.e. with at least one positive finding), chosen randomly via stratified sampling according to the distribution of finding categories in the eligible cohort. Note that approximately 25% of PadChest samples were originally labelled by radiologists, whereas the remaining cases were annotated automatically by a recurrent neural network with a per-label attention mechanism. 21 For PadChest-GR, we sampled only from the more reliable manually labelled subset.\nFurther, we highlight that the original PadChest finding ontology is too granular for stratification and analysis, as many detailed findings are rare in the dataset and could not be faithfully represented in the much smaller PadChest-GR dataset. Therefore, we first enumerated a non-exhaustive subset of finding types that were clinically relevant, broad enough for significant prevalence but not too vague, and visible on a CXR. Finer-grained findings were subsumed into their parent super-categories (if selected) following the original PadChest hierarchy; see full mapping in Table A.1. The final list of findings was further validated by a panel of radiologists. All categories that remained unmapped or had lower than 1% prevalence after this grouping were mapped to 'Other' (see Table B.1) and were simply not tracked for stratification, but the corresponding samples could still be included at random.\nFinally, to preserve the composition of the original cohort, the dataset was complemented with normal studies-i.e. those with no positive findings according to the PadChest labels. After annotation and quality"}, {"title": "EXTRACTION OF FINDING SENTENCES", "content": "Sentences in routine free-text radiology reports frequently refer to multiple findings, and may also mention external information (e.g. patient history), communications between healthcare staff, clinical interpretation, follow-up recommendations, etc. which cannot be objectively inferred from the radiographic study alone. On the other hand, grounded radiology reporting involves separately localising each observed finding. Therefore, to enable annotation and model development for this task, existing radiology datasets must be processed to accurately extract sentences describing individual objective findings.\nreport in Spanish was processed to not only extract the single-finding sentences, but also translate the latter to English and link them to the existing PadChest labels.() See details and model instruction prompts in Appendix C. Positive and negative finding phrases were extracted in two separate stages.\nFor the positive findings, we leveraged PadChest's existing finding and location labels, which were originally collected at the sentence level. Labels for differential diagnoses that rely on additional clinical context, such as 'pneumonia', were ignored. The model was presented with the full original report and grouped sentence labels, and was asked to generate standalone sentences describing exclusively each of the given finding labels, along with English translations. Every extracted finding sentence was also matched to the relevant subset of provided location labels (see Appendix E). Further, the model classified the temporal progression of each finding, when applicable.\nNegative findings are statements referring to the absence of abnormal observations, such as 'No evidence of consolidation', 'Heart size is normal', or 'No significant findings'. In PadChest, negative findings were not labelled for the specific negated findings or their locations. We therefore relied on the LLM to identify negative finding sentences and split them if necessary. Following Bannur et al.3, we tasked the LLM with extracting sentences describing individual observations, with two additional requirements: (1) translation to English, and (2) classification of sentences as either positive or negative findings. This enables us to extract only the negative findings."}, {"title": "MANUAL ANNOTATION", "content": "The selected studies were annotated by nine senior and five junior radiologists from the University Hospital Sant Joan d'Alacant, Spain, using the HIPAA-compliant Centaur Labs labelling service (https://www.centaurlabs.com). The annotation was performed in two stages: study-level quality control followed by box annotations for each extracted positive finding. For both stages, every study or finding was analysed independently by two professionals.\nThe current frontal image was always displayed beside the linked prior image when available, so that findings reported with temporal progression could be accurately identified.\nFor each radiological study, annotators were presented with the original report in Spanish and the list of all extracted positive finding sentences in both Spanish and English. The study was assessed for any major issues with the image quality, report quality, selection of the prior image, or list"}, {"title": "\u0391\u039d\u039d\u039f\u03a4\u0391TION POST-PROCESSING AND ARBITRATION", "content": "We filtered out individual boxes with areas smaller than 0.1% of the respective image, likely corresponding to spurious clicks in the annotation interface. Then, for each finding, we arbitrated which of the two annotations (i.e. sets of boxes) should be treated as official. This was done by applying a sequence of binary criteria to both annotations until a unique choice could be made. In order of priority, we preferred the annotation that: (1) has any boxes; (2) has at least 80% of its area contained in the other, therefore assumed to be more precise; (3) was created by a senior radiologist; (4) has more boxes; (5) has smaller overall area. All remaining 436 cases (7.0%) were decided at random. The unselected extra annotations are also released as part of the dataset, for researchers interested in studying inter-observer variability."}, {"title": "ASSEMBLY OF GROUNDED REPORTS", "content": "Each finding sentence was extracted along with a source span from the original report. The positions of these excerpts in the text were then used to merge and sort the lists of positive and negative findings in the same order as originally reported.\nIn order to standardise the data for training machine learning models, we partitioned 70% of studies for training, 10% for validation, and the remaining 20% for testing. Splits were determined at random, stratified by the same finding categories as for data selection, and ensuring all studies of each patient were kept within a single partition. This process was also applied to the remaining eligible studies outside of PadChest-GR, resulting in a stratified split for the entire PadChest dataset."}, {"title": "Results", "content": "The resulting PadChest-GR dataset is composed of 4,555 studies, of which 1,456 (32%) are normal and 3,099 (68%) abnormal. The median age is 69 years, and samples have an equal representation of men and women. The studies included in PadChest-GR were all acquired in 2014-2017, as this was the time range selected for manual annotation in PadChest. Prior images are available for 31.7% of all studies. As expected, Fig. 4B shows this proportion is higher among abnormal studies (34.4%) than normal ones (26.2%), since patients with positive findings are more likely to require follow-up scans for monitoring.\nPadChest-GR includes 7,037 positive findings and 3,422 negative findings in total. On average, the grounded report for each abnormal study contains 2.27 positive and 0.52 negative findings, whereas normal studies have 1.25 negative findings. Detailed distributions are shown in Fig. 4D. We observe a long tail of positive finding counts, with 246 studies (5.4%) containing five or more. An overwhelming majority (84.4%) of all 2,786 reports that include any negative findings has a single one, typically equivalent to \"no (other) significant findings\".\nFigure 4E shows the distribution of the main 24 finding categories, with a balanced representation across all partitions.\nProgression status (see Fig. D.1) was available for 306 finding sentences (11.4% of all positive findings) in studies with linked prior studies and 323 (7.4%) in studies without. Combined, the evolution of these findings was as follows: 386 stable (61.3%), 108 improving (17.2%), 71 worsening (11.3%), 56 new (8.9%), and 8 resolved (1.3%). The five most common finding categories with progression status were pleural effusion, nodule, alveolar pattern, interstitial pattern, and atelectasis.\nLocation labels with anatomical regions were available for 64.3% of the finding sentences. In addition to the selected list of finding categories used for stratification, radiologists also labelled a total of 1,956 finding sentences that were not included in the primary stratification criteria. These additional findings were labelled with bounding boxes because they were deemed clinically relevant or radiologically visible.\nThe complete list of these additional entities, along with their respective counts of images and bounding"}, {"title": "BOX ANNOTATIONS", "content": "Box annotation statistics are visualised in Fig. 5. We observe that certain hardware categories including electrical device (e.g. pacemaker), nasogastric tube, and central venous catheter-tend to have relatively large boxes. This is because the rectangular annotations cover the full horizontal and vertical extent of the long tubes and pacemaker wires, as evidenced by the spatial heatmaps in Fig. 5B.\nIt is also worth noting the variability in how radiologists annotated some finding categories, e.g. nodules and interstitial/alveolar patterns. At times, the latter were annotated with multiple small boxes precisely localising each instance in the image, and, in other cases, with larger boxes encompassing the whole affected areas. Up to 81.9% of finding sentences have associated anatomical location labels.\nLastly, note that 11.7% of the total 7,037 positive findings were not localised with boxes. Consistently with the annotation instructions, this happened particularly often for diffuse, chronic, or global finding categories, such as osteopenia, bronchiectasis, hyperexpansion, and hypoinflation. Extra box annotations are available for 5,242 findings, with a median intersection-over-union of 0.530 compared to the official boxes."}, {"title": "Discussion", "content": "In this work, we present PadChest-GR, a first-in-class dataset designed to train and evaluate grounded re-"}, {"title": "LIMITATIONS", "content": "We acknowledge some PadChest-GR limitations, most of them derived from the use of the original PadChest data. First, although this dataset represents a large, retrospective cohort, it originates from a single hospital in Spain, likely containing biases related to regional healthcare practices, and does not fully represent diverse populations.\nRegarding image quality, all studies were acquired between 2014 and 2017, with many digitized from radiological films rather than captured digitally, resulting in lower image quality compared to contemporary standards. The images are in PNG format with a reduced grayscale range compared to DICOM, affecting the visibility of subtle findings, such as interstitial lung disease patterns or small pneumothoraces. Additionally, during annotation, radiologists were unable to adjust window width and level or apply image filters, affecting their assessment of certain areas, such as the pulmonary interstitium, retrocardiac regions, and the visualization of tubes and catheters.\nMoreover, the cohort includes many bedridden and ICU patients, whose radiographs are of lower quality due to immobility and suboptimal positioning. Nonetheless, non-optimal image quality reflects realistic conditions for training and testing robust models.\nRegarding projection limitations, the dataset predominantly contains postero-anterior (PA) images without corresponding lateral views. Certain findings, such as vertebral fractures, hyperkyphosis, and pulmonary hyperinflation, are better assessed on lateral projections. The absence of lateral views meant that some findings in the reports could not be corroborated or annotated on the available images and were discarded, limiting the representation of pathologies that are projection-dependent.\nAlthough LLMs have demonstrated impressive capabilities in de-identification, 22 translation, 23 and error detection in radiology reports, 24 they are not without flaws. In our data processing, GPT-4 occasionally produced truncated sentences during automated extraction from reports. However, labels from the original PadChest dataset ensured that associ-"}, {"title": "CONCLUSIONS AND FUTURE WORK", "content": "By providing comprehensive annotations of all clinically relevant findings along with their localisations, we hope that PadChest-GR will foster new avenues in medical imaging research and support the development of more robust and interpretable models in radiology.\nTo address the limitations mentioned, future efforts could focus on enhancing diversity by incorporating data from multiple institutions to improve generalisability. Additionally, including higher-resolution images in the standard DICOM format would facilitate more detailed visualization and analysis. Finally, expanding the dataset to include lateral views and other relevant imaging modalities would allow for a more comprehensive findings assessment."}]}