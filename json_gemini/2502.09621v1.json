{"title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency", "authors": ["Dongzhi Jiang", "Renrui Zhang", "Ziyu Guo", "Yanwei Li", "Yu Qi", "Xinyan Chen", "Liuhui Wang", "Jianhan Jin", "Claire Guo", "Shen Yan", "Bo Zhang", "Chaoyou Fu", "Peng Gao", "Hongsheng Li"], "abstract": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMS with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs.", "sections": [{"title": "1. Introduction", "content": "The emergence of Chain-of-Thought (CoT) (Wei et al., 2022) in Large Language Models (LLMs) has demonstrated promising advances in reasoning capabilities, exemplified by the recent OpenAI o1 (OpenAI, 2024a) and DeepSeek-R1 (Guo et al., 2025a). By engaging in a more deliberate, stepwise reasoning process before reaching a final answer, this methodology presents an effective solution in tackling complex scenarios."}, {"title": "2. Dataset Curation", "content": ""}, {"title": "2.1. Data Composition and Categorization.", "content": "MME-CoT composes 6 major domains with 17 subcategories, as visualized in Fig. 3. Different from textual reasoning questions, the extra visual input significantly enriches the scope of the visual reasoning questions. With the image input, the model needs to frequently visit the image for relevant information according to current reasoning progress. Describing the image area of interest becomes a crucial part of the CoT process. Thus, in addition to complex problems demanding rigorous logic, commonsense scenarios also pose a challenging reasoning problem, as shown in the general scenes in Fig. 2. To maintain focus on the reasoning process, we exclude questions that require complex domain-specific theorems or specialized knowledge.\nIn addition, to evaluate CoT robustness detailed in Section 3.2, we incorporate a variety of perception tasks along with the reasoning tasks in the benchmark. The reasoning tasks contain questions that demand multi-step logical inference, while the perception tasks consist of questions that primarily test visual recognition abilities or require very minimal reasoning. Existing benchmarks often conflate these two types of tasks, with perception and reasoning questions frequently appearing within the same categories."}, {"title": "2.2. Data Annotation and Review", "content": "To facilitate CoT evaluation, we provide key steps annotation and reference image captions for all the reasoning questions. Key steps are defined as those that must be done to reach the correct answer. For efficient annotation, we first employ GPT-4o to generate the answer rationale and image captions. For the rationale, we provide both questions and ground truth answers to the model, which yields more accurate rationales compared to question-only prompting. Annotators are then asked to provide key intermediate steps with the help of GPT-4o's responses. For cases where GPT-4o fails to generate reasonable rationales, annotators develop solutions independently. The intermediate steps fall into two categories: inference conclusion and image caption. Note that the final answer is also included as a concluding inference. All the steps are reduced to the simplest form, retaining only core conclusions and relevant visual element descriptions. Notably, for problems with multiple solutions, annotators are required to provide all possible methods. For reference captions, we also ask annotators to verify and correct the details."}, {"title": "3. CoT Evaluation Strategy", "content": "Existing benchmarks only focus on evaluating the final answer of the questions, leaving the whole chain of thoughts unvisited. We argue that the CoT process reflects reasoning capability from multiple aspects, serving as a crucial medium to understand LMM's thinking pattern and deficiency. Here, we present the first holistic CoT evaluation suite to facilitate a comprehensive understanding of the LMMs' reasoning abilities. We detail the evaluation of correctness in Section 3.1, stability and efficacy in Section 3.2, and reflection quality in Section 3.3."}, {"title": "3.1. CoT Quality Evaluation", "content": "Existing methods typically rely on state-of-the-art LLMs or LMMs to directly evaluate Chain-of-Thought reasoning based on self-defined criteria, using only the final answer as a reference (Hao et al., 2024; Zhang et al., 2024c). We identify two primary issues with the strategy. First, the scoring process only attends to the logical validity of each step, omitting the helpfulness evaluation. Second, there is a large number of complex visual reasoning questions that even the scoring model cannot solve. It is unreasonable for the scoring model to judge another model's reasoning process on these questions without knowing the ground truth solution process. Therefore, building upon our annotated key steps and reference image captions, we leverage two interpretable metrics to evaluate the CoT correctness: recall and precision (Figure 5). The two metrics respectively attend to the two aspects of the CoT correctness: informativeness and accuracy. We denote the key steps as $S = C \\cup I$, where $C = \\{c_1, ..., c_M\\}$ includes $M$ key inference conclusions and $I = \\{i_1, ..., i_N\\}$ includes $N$ key image captions."}, {"title": "Recall.", "content": "We prompt GPT-4o (OpenAI, 2024) to determine whether each key step occurs in the model's CoT response. Then we calculate the ratio of the matched key steps $S_{matched}$ against all the annotated key steps:\n$$ k_o = \\underset{k}{\\arg \\max} \\frac{|S_k^{matched}|}{|S_k|}, $$\n$$ Recall_C = \\frac{|C_{k_o}^{matched}|}{|C_{k_o}|}, $$\n$$ Recall_I = \\frac{|I_{k_o}^{matched}|}{|I_{k_o}|}, $$\n$$ Recall = \\frac{|S^{matched}|}{|S_{k_o}|} $$"}, {"title": "Precision.", "content": "We first instruct GPT-4o to partition the prediction into a sequence of steps $P$, as shown in Fig. 4. Each step is categorized into one of three classes: logical inference, image caption, and background information. The logical inference step draws an intermediate or final conclusion based on the previously obtained information. The image caption step depicts elements of interest in the image. The background information step states external knowledge or question information. Visual reasoning can be primarily characterized as an interleaved sequence of image captions and logical inferences, so we focus on measuring precision for these two key step types. We assess the correctness of logical inference steps ($C^P$) and image caption steps ($I^P$) using two criteria: 1. If the step exists in $S$, the step is correct. 2. If the step is logically correct or faithfully depicts the image based on the annotations, the step is also correct. Thus, we compute precision as:\n$$ Precision_C = \\frac{|C^P_{correct}|}{|C^P|}, $$\n$$ Precision_I = \\frac{|I^P_{correct}|}{|I^P|}, $$\n$$ Precision = \\frac{|C^P_{correct} \\cup I^P_{correct}|}{|C^P \\cup I^P|} $$"}, {"title": "3.2. CoT Robustness Evaluation", "content": "Here, we perform the first investigation on the robustness of CoT in visual reasoning. The effectiveness of CoT on reasoning tasks has been verified in many works (Wei et al., 2022; OpenAI, 2024a). However, how CoT impacts visual perception tasks or tasks requiring minimal reasoning still remains unknown. Despite the neglect, this question bears great importance. In real-world applications, what task is given is unknown in advance. Whether the model should perform CoT to solve the task is difficult to determine. In fact, there exists no golden standard to determine which question can benefit from CoT so far (Sprague et al., 2024). Instead of trying to define this criterion, we examine the performance of CoT across all kinds of tasks, both reasoning and perception. We argue that an ideal CoT process should assist in reasoning and not interfere with pure perception. Therefore, it can be applied for any tasks. Based on this, we propose to evaluate two metrics of CoT: stability and efficacy (Figure 6). We leverage two kinds of prompts: the direct prompt (DIR) and the CoT prompt (COT). The direct prompt asks the model to directly provide the final answer, while the CoT prompt instructs the model to perform step-by-step reasoning and finally give the answer. To directly compare the performance difference caused by these two prompts, we conduct the direct evaluation, which only judges the correctness of the final answer, i.e., accuracy. We instruct GPT-4o mini (OpenAI, 2024) to extract the final answer, and then compare it with the ground truth answer, following the two-step procedure introduced in (Zhang et al., 2024c).\nStability. We define the performance difference of the two prompts on the perception tasks $P$ as the stability score:\n$$ Stability = Acc_{COT}^P - Acc_{DIR}^P. $$\nIntuitively, applying the CoT prompt to perception tasks should not degrade performance compared with the direct prompt. Thus, a model with stable CoT should be not less than 0. Otherwise, the model's thinking process demonstrates inconsistency and harm. The overthinking process pushes over the original correct judgment."}, {"title": "Efficacy.", "content": "Similarly, the performance difference of the two prompts on the reasoning tasks $R$ is defined as the score:\n$$ Efficacy = Acc_{COT}^R - Acc_{DIR}^R $$\nIntuitively, CoT facilitates stepwise thinking and therefore benefits answering reasoning tasks. The difference reflects how much CoT can enhance reasoning."}, {"title": "3.3. CoT Efficiency Evaluation", "content": "Models like o1 generate extremely long thinking processes with reflection and verification of current steps and outcomes. We perform the first exhaustive analysis of the CoT efficiency of visual reasoning with two carefully designed metrics (Figure 7):\nRelevance Rate. Although the long reasoning content allows for deeper thinking, it may also introduce a large amount of irrelevant information. As shown in the bottom left of Fig. 7, the model has identified the critical element in the image for answering the question, but it still generates a detailed description of other objects. This irrelevant information provides no helpful information to work out the answer. In the meantime, this extra content slows down the generation speed. Similar to the calculation of precision, we employ the same method to partition the prediction into steps. Then, we instruct GPT-4o to determine all the relevant steps $P_{relevant}$. The step is considered relevant only when the majority of its content works towards solving the question. We first compute the raw relevance rate and then apply a scaling factor to amplify the differences between models. Let $r_x$ denote the raw relevance rate:\n$$ r_c = \\frac{|C^P_{relevant}|}{|C^P|}, $$\n$$ r_I = \\frac{|I^P_{relevant}|}{|I^P|}, $$\nThen, the final relevance rate Relevance Rate is defined as:\n$$ Relevance Rate = \\frac{x - \\alpha}{1 - \\alpha}, x \\in C, I, \\emptyset $$"}, {"title": "Reflection Quality.", "content": "The superior reasoning ability could be largely attributed to the reflection and verification process. However, our analysis reveals that not all reflective steps contribute meaningfully to finding correct answers. We identify distinct failure patterns in the reflection process. Some reflective steps mislead the reasoning by introducing new errors or incorrect assumptions, while others are redundant, simply echoing previous conclusions without contributing new insights. To account for failure reflection scenarios, we propose to measure the validity of the reflection. We define a valid reflection as either correctly pointing out the previous mistakes or verifying the previous conclusion with a new insight. Otherwise, the reflection only slows down the reasoning. To instruct GPT-4o to determine all the valid reflection steps $R_{valid}$, we list a set of common indicators of the start of the reflection, such as \"Wait\" and \"Alternatively\", and illustrate the definition of valid reflection. For all the valid reflection steps $R_{valid}$, the reflection quality is computed as:\n$$ Reflection Quality = \\frac{|R_{valid}|}{|R|} $$"}, {"title": "4. Experiments", "content": "In this section, we conduct a systematic evaluation of state-of-the-art models on MME-CoT. We first detail the experiment setup in Section 4.1. Then in Section 4.2, we report the quantitative results and provide valuable insights derived from our analysis."}, {"title": "4.1. Experiment Setup", "content": "Evaluation Models. We select top-performing LMMs for comprehensive CoT evaluation. We test earlier models such as LLaVA-OneVision (7B, 72B) (Li et al., 2024a), Qwen2-VL (7B, 72B) (Qwen Team, 2024), MiniCPM-V-2.6 (Yao et al., 2024b), and InternVL2.5 (8B) (Chen et al., 2024b), which are not trained for the reasoning capability. We also include GPT-4o (OpenAI, 2024b) as a strong baseline model. Besides, we test recent models targeting reasoning, including LLaVA-CoT (11B) (Xu et al., 2024), Mulberry (8B) (Yao et al., 2024a), InternVL2.5-MPO (8B, 78B) (Wang et al., 2024c). Finally, we evaluate LMMs with reflection capabilities, including both closed-source models like Kimi k1.5 (Team et al., 2025) and open-source implementations such as QVQ-72B (Team, 2024) and Virgo-72B (Du et al., 2025).\nNote that we sample 150 questions from MME-COT to evaluate Kimi k1.5, due to the access limitations. The sample comprises 115 reasoning and 35 perception questions.\nImplementation Details. We define the CoT prompt as: Please generate a step-by-step answer, include all your intermediate reasoning process, and provide the final answer at the end. and the direct prompt as: Please directly provide the final answer without any other output. We only calculate recall of image observation and logical inference on questions where key inference conclusion or image observation exists. We employ GPT-4o mini for the direct evaluation and GPT-4o for all other criteria. For hyperparameters, we follow the settings in VLMEvalKit (Duan et al., 2024)."}, {"title": "4.2. Quantitative Results", "content": "We conduct extensive experiments on various LMMs with our proposed CoT evaluation suite. The main results are presented in Table 2 and Table 3. We begin by analyzing the overall performance and then highlight key findings.\nOverall Results. In Table 2, we present the overall performance of three CoT evaluation perspectives with specific metrics. To provide a comprehensive understanding, we report precision, recall, and relevance for both logical inference and image caption steps. For robustness, we provide the direct evaluation result on the perception and reasoning tasks, with either CoT or direct prompt. We employ the average value of the stability and efficacy as the final robustness metric. Notably, we define the reflection quality as 100 on models incapable of reflection.\nFor CoT quality, Kimi k1.5 achieves the highest F1 score. Open-source models with larger sizes consistently demonstrate better performance, highlighting the scalability of LMMs. Notably, Qwen2-VL-72B outperforms all other open-source models without reflection, even surpassing InternVL2.5-78B-MPO, which is specifically enhanced for reasoning. Analysis reveals that GPT-4o achieves superior performance across all recall metrics, while Kimi k1.5 demonstrates the highest scores in precision evaluations. For CoT robustness, Mulberry obtains the highest average score. However, when we look into its output, we find it still generates lengthy rationales despite receiving a direct prompt. Even worse, the direct prompt seems to be an out-of-distribution input for Mulberry, frequently leading to nonsensical outputs. Further analysis of other models' predictions reveals that LLaVA-CoT, Virgo, QVQ, and Kimi k1.5 similarly neglect the direct prompt, instead generating extended rationales before answering. Consequently, their robustness scores may be misleading. Once again, GPT-4o achieves the highest robustness score. Among open-source models, only InternVL2.5-MPO, in both its 8B and 78B variants, attains a positive robustness score. Finally, for CoT efficiency, InternVL2.5-8B obtains the maximum relevance of 98.4%, suggesting its consistent focus on questions.\nNow, we summarize our key observations as follows:\nModels with reflection largely benefit CoT quality. As shown in Table 2, the F1 scores of the two models with reflection capability most closely approach GPT-4o. After specifically fine-tuning for the reasoning capabilities from Qwen2-VL-72B, QVQ surpasses its base model by 5.8%. Notably, although QVQ generates longer CoT sequences than Qwen2-VL-72B, QVQ's precision still exceeds Qwen2-VL-72B by 2.9%, indicating superior accuracy in each reasoning step. Kimi k1.5 also surpasses the previous state-of-the-art model GPT-4o, obtaining the highest CoT quality.\nLong CoT does not necessarily cover key steps. Despite high precision in long CoT models, the informativeness of each step is not guaranteed. We observe that the recall trend among GPT-4o, QVQ, and Virgo does not align with their CoT Rea. performance (i.e., their final answer accuracy on the reasoning tasks under the CoT prompt). Specifically, while both Virgo and QVQ outperform GPT-4o in direct evaluation, they lag behind in recall. This suggests that long CoT models sometimes reach correct answers while skipping intermediate steps, which contradicts the principle of stepwise reasoning and warrants further investigation.\nCoT impairs perception task performance in most models. Surprisingly, most models exhibit negative stability scores, indicating that CoT interferes with perception tasks. The most significant degradation occurs in InternVL2.5-8B, where performance drops by 6.8%. This reveals inconsistency and potential overthinking in current models, presenting a significant barrier to adopting CoT as the default answering strategy. Among models that provide direct answers, only LLaVA-OV-72B and InternVL2.5-8B-MPO achieve a modest positive score of 0.3%.\nMore parameters enable models to grasp reasoning better. We find that models with larger parameter counts tend to achieve higher efficacy scores. This pattern is evident across LLaVA-OV, InternVL2.5-MPO, and Qwen2-VL. For instance, while Qwen2-VL-7B shows a 4.8% decrease in performance when applying CoT to reasoning tasks, its larger counterpart, Qwen2-VL-72B, demonstrates a 2.4% improvement. This discrepancy suggests that models with more parameters could better grasp the reasoning ability under the same training paradigm.\nLong CoT models may be more susceptible to distraction. Long CoT models may demonstrate lower relevance scores compared to other models. They frequently generate content unrelated to solving the given question, corresponding to their relatively low recall scores compared to direct evaluation, like QVQ. Although a few models with short CoT, like Mulberry and LLaVA-OV-7B, also obtain a low relevance rate, we find that it is because these models may keep repeating words when dealing with specific type of questions, resulting in irrelevant judgment. The fine-grained metric reveals that models tend to lose focus when describing images, often producing exhaustive captions regardless of their relevance to the question. From Table 3, we find that this phenomenon prevails in general scenes, space-time, and OCR tasks. This behavior can significantly slow inference by generating substantial irrelevant content. Teaching long CoT models to focus on question-critical elements represents a promising direction for future research.\nReflection often fails to help. While reflection is a key feature of long CoT models for answer verification, both QVQ and Virgo achieve reflection quality scores of only about 60%, indicating that approximately 40% of reflection attempts fail to contribute meaningfully to answer accuracy. Even for the closed-source model Kimi k1.5, over 25% reflection steps are also invalid. This substantial failure rate compromises efficiency by potentially introducing unnecessary or distracting steps before reaching correct solutions. Future research should explore methods to reduce these ineffective reflections to improve both efficiency and quality."}, {"title": "4.3. Error Analysis", "content": "In this section, we analyze error patterns in the LMM reflection process. An effective reflection should either correct previous mistakes or validate correct conclusions through new insights. We examined 200 model predictions from QVQ and identified four distinct error types that hinder productive reflection."}, {"title": "5. Conclusion", "content": "In this paper, we have introduced MME-CoT, a comprehensive benchmark designed to evaluate Chain-of-Thought reasoning in Large Multimodal Models. Our dataset comprises six categories to cover most scenarios of visual reasoning tasks. To gain a thorough understanding of the reasoning process, we design a novel CoT evaluation suite with three metrics. Our systematic evaluation obtains useful insights into the issues within the current state-of-the-art Large Multimodal Models. We identify critical flaws in all the tested open-source models. As the field continues to evolve, MME-CoT stands as a valuable tool for measuring progress and identifying areas for improvement in the development of more sophisticated multimodal AI systems."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Computer Vision and Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "Appendix Overview", "content": "\u2022 Section A: Related Work.\n\u2022 Section B: More Dataset Details.\n\u2022 Section C: Error Analysis.\n\u2022 Section D: More Qualitative Examples.\n\u2022 Section E: Evaluation Prompts."}, {"title": "A. Related Work", "content": ""}, {"title": "A.1. Large Multimodal Models", "content": "The field of multimodal (Radford et al., 2021; Li et al., 2022; OpenAI, 2023; 2024b) AI has experienced extraordinary growth, particularly through the development of Large Multimodal Models (LMMs) (Liu et al., 2023; Zhu et al., 2023; Lin et al., 2023; Qwen Team, 2024). These models build upon the achievements of Large Language Models (LLMs) (Touvron et al., 2023; Yang et al., 2024) and advanced vision models (Radford et al., 2021), expanding their capabilities to process multiple kinds of visual input (Li et al., 2024b; Guo et al., 2023; Li et al., 2023).\nClosed-source models, such as OpenAI's GPT-4o (OpenAI, 2024b), have demonstrated exceptional capabilities in visual understanding and reasoning. However, their closed-source nature creates barriers to widespread adoption and further development by the broader research community. In response, significant progress has been made in developing open-source alternatives. Early approaches like LLaVA (Liu et al., 2023), LLaMA-Adapter (Zhang et al., 2024b), and MiniGPT-4 (Zhu et al., 2023) established a foundation by combining frozen CLIP models for image encoding with LLMs, enabling multimodal instruction tuning. Subsequent developments through projects such as InternVL2 (Chen et al., 2024c), Qwen2-VL (Qwen Team, 2024), SPHINX (Gao et al., 2024; Lin et al., 2023), and MiniCPM-V (Yao et al., 2024b) have expanded these capabilities by incorporating more diverse visual instruction datasets and broadening application scenarios.\nRecently, with the introduction of o1 (OpenAI, 2024a), the field of LMMs has also focused on enhancing the reasoning capability. (Wang et al., 2024d) introduces mixed preference optimization with automatically constructed data. (Yao et al., 2024a) proposes to leverage collective knowledge from multiple models to identify effective reasoning paths. Besides, several works (Team, 2024; Du et al., 2025) have demonstrated the ability to replicate behaviors similar to o1 models, particularly regarding multi-step CoT reasoning with iterative self-reflection and verification processes."}, {"title": "A.2. Reasoning Evaluation", "content": "Several methods have been developed to evaluate reasoning in natural language processing, including ROSCOE (Golovneva et al., 2022) and ReCEval (Prasad et al., 2023), which assess reasoning chains across multiple dimensions such as correctness and informativeness. However, these approaches are limited to text-only scenarios and do not address the unique challenges present in visual reasoning tasks. Furthermore, the emergence of long chain-of-thought (CoT) reasoning has introduced additional considerations, such as output efficiency and reflection quality, which existing evaluation methods do not adequately address.\nOn the other hand, various multimodal benchmarks have been developed to assess reasoning abilities across specific domains. Current exploration of visual reasoning predominantly focuses on the mathematics (Zhang et al., 2024d; Peng et al., 2024) domains. MathVista (Lu et al., 2023) provides a comprehensive collection of mathematical problems that assess mathematical and logical reasoning abilities. Building on this, MathVerse (Zhang et al., 2024c) introduces a new benchmark by eliminating redundant textual information to evaluate whether LMMs can accurately interpret graphical representations. OlympiadBench (He et al., 2024) further raises the complexity bar by incorporating challenging Olympiad-level mathematics and physics problems. Despite these advances in specialized domains, broader applications such as general-scene reasoning remain relatively unexplored. Recent developments have begun to expand beyond purely scientific reasoning. For instance, M3COT (Chen et al., 2024a) and SciVerse (Guo et al., 2024a) incorporate commonsense tasks alongside scientific reasoning and knowledge-based assessment in the multimodal benchmark. However, most existing benchmarks focus solely on evaluating final answers while overlooking the intermediate steps, thus providing limited insights into the process through which models arrive at their conclusions."}, {"title": "B. More Dataset Details", "content": ""}, {"title": "B.1. Data Source Distribution", "content": "We visualize the data source distributions in our benchmark, which consists of 15 sets, including MathVerse (Zhang et al., 2024c), MMMUPro (Yue et al., 2024), OlympiadBench (He et al., 2024), MMT-Bench (Ying et al., 2024), MuirBench (Wang et al., 2024a), ml-rpm-bench (Zhang et al., 2024e), MMSearch (Jiang et al., 2024), CharXiv (Wang et al., 2024e), and SciVerse (Guo et al., 2024a)."}, {"title": "B.2. Preliminary Categorization Result", "content": ""}, {"title": "C. Error Analysis", "content": "We showcase the examples of the identified error types of reflection in Fig. 10."}, {"title": "D. More Qualitative Examples", "content": ""}, {"title": "E. Detailed Evaluation Setup", "content": ""}, {"title": "E.1. CoT Quality Evaluation Prompts", "content": ""}, {"title": "E.2. CoT Efficiency Prompt", "content": ""}, {"title": "E.3. Direct Evaluation Prompt", "content": ""}]}