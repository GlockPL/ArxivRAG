{"title": "Uncovering Biases with Reflective Large Language Models", "authors": ["Edward Y. Chang"], "abstract": "Biases inherent in human endeavors pose significant challenges for machine learning, particularly in supervised learning that relies on potentially biased \"ground truth\" data. This reliance, coupled with models' tendency to generalize based on statistical maximal likelihood, can propagate and amplify biases, exacerbating societal issues. To address this, our study proposes a reflective methodology utilizing multiple Large Language Models (LLMs) engaged in a dynamic dialogue to uncover diverse perspectives. By leveraging conditional statistics, information theory, and divergence metrics, this novel approach fosters context-dependent linguistic behaviors, promoting unbiased outputs. Furthermore, it enables measurable progress tracking and explainable remediation actions to address identified biases.", "sections": [{"title": "Introduction", "content": "AI systems are increasingly being integrated into critical sectors such as education, healthcare, and public policy, where their decisions can have profound impacts. Despite their potential, these systems are prone to exhibit discriminatory behaviors, propagate existing biases, or make errors. Researchers in machine learning are diligently addressing these challenges by investigating the sources, patterns, and types of biases inherent in AI systems (Mehrabi et al., 2021). While achieving absolute fairness is complicated by diverse cultural, religious, and ideological perspectives, the primary objective remains to minimize the propagation of biases within machine learning models (Kleinberg et al., 2017; Mehrabi et al., 2021; Selbst et al., 2019; Gautam and Srinath, 2024; Baeza-Yates, 2018).\nIn this data-centric era, the accuracy of training data, especially the ground-truth labels, is crucial. Machine learning algorithms are designed not to alter the data but to accurately reflect any biases present in the labels. Incorrect labels, such as a biased news article marked as neutral or a wrong medical diagnosis, can mislead models, resulting in harmful outcomes. Consequently, our research focuses on identifying and correcting mislabeled data to effectively mitigate bias and ensure the responsible application of AI.\nEvidence of annotation biases is illustrated in Section 4. Tables 1 and 2 present real data (Budak et al., 2016) showing how annotators' political affiliations can influence their labeling of news articles. For instance, annotators aligned with the Democratic party are more inclined to perceive scandals involving Democrats negatively, whereas Republican annotators might view the same incidents neutrally. Conversely, Republican annotators might downplay criticisms directed at their party, whereas Democrats might view them as justified, underscoring the influence of personal ideologies on annotation practices.\nTo counter the perpetuation of these biases through classifiers, we propose a check-and-balance framework wherein two Large Language Models (LLMs) engage in dialogue to scrutinize and challenge human annotations. One LLM supports the original annotation while the other introduces alternative perspectives, thereby enriching the understanding of the content. This dialogue is structured to foster a productive exchange that cultivates balanced insights into the discussed topics and makes recommendations to the editorial board if concerns arise.\nTo evaluate the effectiveness of this dialogue, we employ several metrics grounded in statistical and information theory principles. Measures such as Shannon entropy (Shannon, 1948) and mutual information (Cover and Thomas, 2006) assess enhancements in shared understanding, while metrics like Jensen-Shannon divergence (JSD) (Lin, 1991), Wasserstein distance (WD) (Kantorovich, 1942), and cross-entropy (CE) (Shore and Johnson, 1980)"}, {"title": "Related Work", "content": "This study focuses on mitigating training data label (ground truth) bias, a primary concern in machine learning (Mehrabi et al., 2021). Accurate labeling is crucial, as a label that aligns with biased content reinforces that bias, while a label that correctly identifies it allows for education and correction (Baeza-Yates, 2018; Danks and London, 2017). This underscores the importance of label accuracy in minimizing bias propagation.\nThis work specifically addresses mislabeled ground truth and explores remediation actions. Efforts to improve annotation accuracy can be broadly categorized into three approaches:\nCross-Validation with Multiple Annotators:\nEnsemble methods, utilizing multiple annotators and statistical techniques, can mitigate individual biases and enhance data reliability (Snow et al., 2008). This approach has been successful in tasks with broad consensus, such as image annotation with ImageNet (Deng et al., 2009; Krizhevsky et al., 2012). However, it may be less effective for complex or biased content, where subtle interpretations are required. For instance, the term \u201cdiscovered\" in reference to Columbus's arrival in the Americas reflects a Western bias, while \u201cencounter\" offers a more accurate representation from Indigenous perspectives (Wikipedia, 2023; Mostafazadeh Davani et al., 2022). Relying on majority votes in such cases can be counterproductive, and while diverse annotator pools are important, the inherent limitations of human annotation, including the assumption of a single truth, must be acknowledged (Aroyo and Welty, 2015).\nCross-Validation between Machine and Human Annotators:\nIntegrating machine learning algorithms into the annotation process can improve quality by leveraging both human expertise and machine efficiency (Wang et al., 2021). Semi-supervised learning, which utilizes both labeled and unlabeled data, is one such technique (Ratner et al., 2017). However, machine learning algorithms can be inherently biased due to their training data, and ensemble methods may not fully resolve these underlying biases (Wang et al., 2021; Baeza-Yates, 2018).\nGAI Opportunity:\nThe emergence of GAI and LLMs, with their vast knowledge base and powerful Transformer architecture (Vaswani et al., 2017), presents a new avenue for addressing annotation bias. LLMs can potentially uncover diverse perspectives on a given topic, including historical shifts and evolving narratives. Recent studies have explored combining LLM output with human feedback for annotation tasks (Tan et al., 2024).\nHowever, due to their \u201cmaximal likelihood\" next-token prediction training objective, LLMs may prioritize popular viewpoints over minority ones. This work addresses this limitation by proposing a novel approach, grounded in statistical and information theories, that aims to uncover and balance diverse viewpoints, ensuring that both majority and minority perspectives are adequately represented in the annotation process."}, {"title": "EVINCE Algorithm", "content": "Expanding on our prior work (Chang, 2023a, 2024) with theoretical foundations and quantitative metrics, EVINCE (Entropy and Variation IN Conditional Exchanges) leverages LLMs to promote content neutrality through the incorporation of diverse perspectives.\nEVINCE facilitates structured dialogues between LLMs to address the \"maximal likelihood\" bias inherent in conventional information retrieval systems. This bias manifests in search engines like Google, where popular viewpoints are often prioritized based on metrics like click-through rates (Adams-Hands, 2023), potentially sidelining less common perspectives. Similarly, LLM text generation, which relies on predicting the next most likely token, can inadvertently amplify existing biases present in the training data (Li et al., 2023).\nTo foster divergent perspectives, EVINCE addresses two sub-goals:\n\u2022 Exploration: Encouraging the generation of a wide array of viewpoints.\n\u2022 Meaningful Diversity: Ensuring collected perspectives are substantive and not merely contrarian for the sake of disagreement.\nEVINCE achieves these goals by analyzing probability distributions of top-k labels elicited from each LLM in the committee. Through this analysis, individual entropies, cross-entropy between distributions, and mutual information are computed. Based on this quantitative assessment, EVINCE dynamically adjusts its linguistic behaviors (e.g, more contentious vs. more conciliatory) to optimize the annotation recommendations.\nThe initial phase of the EVINCE algorithm aims to induce dual entropy and high cross-entropy between the LLM-generated distributions. I will prove shortly that dual entropy is the ideal condition to foster information exchange between LLMs. This signifies disagreement and creates a fertile ground for novel perspective discovery and exchange. Through iterative dialogue, mutual information increases while divergence decreases, ultimately converging towards a consensus."}, {"title": "Maxims and Optimal Theorem", "content": "Maxim 1: Orchestrate Two Equally Competent LLMs in Structured Debate: Integrating two equally competent LLMs ensures a balanced exchange of insights and avoids bias from knowledge asymmetry. This adversarial setup fosters diversity in predictions, each supported by justifications, promoting critical evaluation and uncovering potential blind spots. The concern is not about the potential non-overlapping training data, as information exchange can remedy this. Instead, the focus is on ensuring that both models have similar quality, primarily determined by their size, to prevent one model from dominating the other due to a disparity in reasoning quality.\nMaxim 2: Encourage the Accurate Rather Than the \"Popular\" Prediction: Typically, LLMs, with their maximum likelihood next-token prediction objective, tend to favor the most popular predictions. By conditioning LLMs within specific contexts, we can prioritize accuracy over popularity, thus mitigating confirmation biases.\nMaxim 3: Combine Predictions Weighted by Diversity and Quality: Weighting the probability distributions from two LLMs based on diverse probabilistic insights and the quality of supporting arguments.\nHow? Following these three sub-maxims:\n\u2022 Maxim 3.1: Prediction Reliability: Estimate the reliability of predictions using entropy-based measures to quantify uncertainty and information content. Typically, lower entropy indicates higher confidence in a prediction, suggesting higher reliability.\n\u2022 Maxim 3.2: Argument Quality: Evaluate the quality of supporting arguments using techniques inspired by the Socratic method. This includes identifying logical fallacies and assessing the relevance and credibility of evidence.\n\u2022 Maxim 3.3: Aggregation: Employ a weighted aggregation method, such as a Bayesian model, to combine weighted predictions accounting for both probabilistic insights and the quality of supporting arguments.\nMaxim 4: Evaluating the Convergence Rate of the Predictions Across the Rounds: This maxim focuses on measuring how quickly and effectively the predictions from the LLMs converge over successive rounds, assessing the efficiency of the debate and aggregation mechanisms. Convergence is assessed by measuring mutual information and using proxy metrics such as Wasserstein distance and cross entropy. When mutual information is low or the similarity between predictions is high, the dialogue is considered to be converging."}, {"title": "EVINCE Algorithm", "content": "Algorithm 1 Specifications of Algorithm EVINCE\n1: Input: Information set S, Class labels C; Two equally competent LLMs: LLMA and LLMB (Maxim #1);\n2: Output: Pf, final probability distribution over C;\n3: Variables: t: debate round; R = (\u00d8 aggregated arguments;\n(): supporting reason sets;\nP(t), Pft): prediction distributions of LLM\u0100 and LLMB on C of round t; R), R): p(t).\nB\nA = 90%: debate contentiousness, initialize to high to foster adversary between LLMs (Maxim #2);\np: prompt = \"Predict top-k probability distribution on C with S and R at contentiousness A\";\n4: Functions: CRIT(d) (Chang, 2023c), Critical Reading Inquisitive Template for evaluating argument quality;\nARA (Guo et al., 2024), Algorithmic Robust Aggregation for optimal prediction aggregation (Maxims #3);\n5: Initial Predictions t = 0:\nLLMs generate their predictions in probability distributions with supporting reasons:\n(P(t=0), R(t)) = LLMA(S,p), (Pt=0), R)) = LLMB (S, \u0440).\n6: Debate Iterations:\n6.1. Update Predictions:\nCalculate the confidence-based weights using the inverse of entropy (Maxim #3.1):\na = 1/(H(P(t)) + 1), \u03b2 = 1/(H(Pt)) + 1).\nUse the blending mechanism to update predictions (Maxim #3.3):\nP'(t) = P(t) + (1 \u2212 a)Pt), Pt) = BP) + (1 \u2212 \u03b2)Pt).\n6.2. LLMs Generate New Predictions: Both LLMs use accumulated R = RURUR.\n(P(t+1), R(t+1)) = LLMA((P'(t)), R, p), (Pt+1), R(t+1)) = LLMB((P'(t)), R, p).\n6.3. Exit Condition Check with Wasserstein distance (Maxim #4):\nIf WD(P(t+1), P(t+1)) <\u20ac EXIT; t = t + 1, \u2206 = \u25b3 \u00d7 80%.\n7: Final Decision: Weighted prediction by quality scores of the evaluator e.g., CRIT (Chang, 2023c) (Maxim #3.2):\nPf = \u03a9\u0391\u03a1(+1) + \u0392\u03a1+1)/\u03a9\u0391 + \u03a9\u0432.\nProblem Statement: Organize a structured dialogue between two equally competent large language models (LLMs), LLMA and LLMB, to conduct t rounds. At each round t, each model produces a probability distribution, denoted as $P_A^{(t)}$ and $P_B^{(t)}$, over C possible outcomes, accompanied by supporting arguments $R_A^{(t)}$ and $R_B^{(t)}$. The goal is to design an iterative debate process that leverages the structured exchange of arguments to enable the models to converge on an optimal prediction distribution $P^*$ across the C classes."}, {"title": "Algorithm Specifications", "content": "With all proxy metrics and their pros, cons, and combined strengths comprehensively surveyed (Appendix A), Algorithm 1 formally specifies the algorithm of EVINCE with the maxims."}, {"title": "Entropy Duality Theorem (EDT)", "content": "Theorem EDT: Optimal Pairing of LLMs for Probabilistic Prediction Accuracy. The optimal pairing of LLMs for diagnosis accuracy, in terms of stability, accuracy, and robustness, occurs when the LLMs are 1) equivalent in the quality of the information they process, and 2) exhibit contrasting entropy values in their prediction distributions-one high and one low.\n[Proof]: Given two LLMs, LLMA and LLMB, following Maxim #1 with prediction distributions PA and PB, respectively. The information entropy of LLMA, H(PA), is high, and of LLMB, H(PB), is low. The proof proceeds:\nStep 1: Define combined prediction distribution. Let the combined prediction distribution of LLMA and LLMB be denoted as Pc. We can express PC as a weighted average of PA and PB:\n$P_c = \\alpha P_A + (1 - \\alpha) P_B$, where 0 \u2264 \u03b1 \u2264 1.\nStep 2: Express the information entropy of the combined prediction distribution. Using the definition of information entropy, we calculate:\n$H(P_c) = - \\sum_i P_c(x_i) \\log_2 P_c(x_i) = - \\sum_i [\\alpha P_A(x_i) + (1 - \\alpha) P_B(x_i)] \\log_2 [\\alpha P_A(x_i) + (1 - \\alpha) P_B(x_i)]$.\nStep 3: Apply Jensen's Inequality to the information entropy of the combined prediction distribution. Jensen's inequality is applied to the convex function $f(x) = -x \\log_2 x$. For a convex function and a set of probabilities $p_i$, Jensen's inequality states that:\n$f(\\sum p_i x_i) \\le \\sum p_i f(x_i)$\nThus, the entropy of the combined distribution is:\n$H(P_c) \\ge \\alpha H(P_A) + (1 - \\alpha) H(P_B)$\nwhere equality holds when PA = PB.\nStep 4: Analyze the lower bound of the combined information entropy. As H(PA) is high and H(PB) is low, their relationship is:\n$H(P_A) = H(P_B) + \\Delta$, where \u0394 > 0."}, {"title": "Experiments", "content": "Our experimental framework aims to assess the feasibility of both detecting biases in textual content and implementing effective mitigation strategies. The first experiment focuses on bias detection, while the second explores the generation of balanced textual outputs as a corrective measure, moving beyond the limitations of prior studies that primarily focused on identification (Section 2).\nWe utilized GPT-4 via OpenAI API on Microsoft Azure, setting the temperature to 0.1 with maximum token size. The cost is around US$1,000."}, {"title": "Experiment #1: Bias Detection", "content": "The aim of this experiment is to evaluate if personal ideology may affect annotations, and can EVINCE help flag and rectify the biases."}, {"title": "Dataset", "content": "The dataset for this experiment consists of 619 news articles (54.3% about Democrat scandals, 45.7% about Republican scandals) selected from a larger 2013 repository of 14,033 articles compiled by fifteen reputable news organizations (Budak et al., 2016). These articles cover diverse topics like civil rights, healthcare, elections, and national security. This dataset is provided as supplementary material.\nThe articles were originally labeled through Amazon Mechanical Turk by 749 qualified U.S. workers, each annotating up to 1,000 randomly selected articles (Budak et al., 2016). For each\"scandal\" article in our subset, one Democrat and one Republican annotator independently classified its bias as \"negatively biased,\u201d \u201cweak negative,\u201d \"neutral,\u201d \u201cweak positive,\u201d or \u201cpositively biased.\"\nThis subset is valuable due to its ground-truth labels provided by annotators from opposing political affiliations, revealing inherent biases in evaluating negative coverage of one's own party. The original study (Budak et al., 2016) found that Republican annotators often perceive news about Republican scandals as negatively biased, while Democrat annotators tend to view such news as neutral or \"just right,\" potentially indicating satisfaction with the coverage's perceived fairness."}, {"title": "Results on Democrat Scandals", "content": "We apply EVINCE to analyze these 619 news articles, comparing its labels with the dataset's provided \"ground truth.\"\nTable 1 compares the judgments of EVINCE (S), Republicans (R), and Democrats (D) on 16 representative articles concerning \u201cDemocrat Scandals.\" As expected, Democrats' judgments are generally more negative than Republicans', with EVINCE's assessments typically falling in between, except for two cases. Notably, there's a 5-to-1 Democrat-to-Republican ratio in the \"Negative\" column and a 12-to-4 Republican-to-Democrat majority in the \"Neutral\" column.\nTables 5 and 6 in Appendix B provide detailed justifications for EVINCE's ratings. To further investigate bias, we examine two specific articles: one from HuffPost (rated far left by AllSides Bias Chart (Allsides)) and another from Breitbart (rated far right).\n* D8 HuffPost (Left): EVINCE rates D8 (on the third row) as neutral, citing the article's direct presentation of facts and inclusion of diverse perspectives on NSA surveillance practices and global reactions. This contrasts with Democrat-leaning annotators, who view the article as negatively biased towards Democrats, while Republican-leaning annotators favor it for exposing a Democratic scandal.\n* D69 Breitbart (Right): EVINCE assesses D69 as weakly negatively biased towards"}, {"title": "Results on Republican Scandals", "content": "Table 2 presents the bias assessments from EVINCE (S), Republicans (R), and Democrats (D) on articles related to \u201cRepublican Scandals.\" In contrast to the \u201cDemocrat Scandals\" dataset, where Republican-leaning evaluations were more favorable, this dataset reveals a shift, with Republican-leaning assessments being notably more critical and Democrat-leaning assessments relatively neutral. The distance triangle for \"Republican Scandals\" mirrors the pattern seen in Figure 1, with the divergence between Republican and Democrat annotators being the largest (15). The distances between EVINCE and Democrat-leaning annotators (9) and between EVINCE and Republican-leaning annotators (11) are smaller, further highlighting EVINCE's relative neutrality."}, {"title": "Experiment #2: Bias Mitigation", "content": "This experiment illustrates EVINCE's ability to identify bias in text, provide reasoned justifications, and propose remediation through the integra-"}, {"title": "Concluding Remarks", "content": "This study demonstrates a significant advancement in mitigating bias in public articles, such as those found in Wikipedia and news sources, by leveraging multiple LLMs through an adversarial dialogue framework. EVINCE effectively identifies biases, provides justifications, and recommends remedial actions to authors and editorial boards, facilitating a balanced perspective that surpasses traditional human annotation methods. The debate-driven methodology, incorporating diverse viewpoints and guided by information-theoretic metrics, significantly enhances content neutrality and quality.\nFurthermore, our work has led to the development of the dual entropy theory and several maxims with metrics to evaluate content's logical coherence and credibility, ensuring a comprehensive inclusion of perspectives while maintaining accuracy. This study also highlights the limitations of human-labeled data, revealing significant rates of mislabeling and misdiagnosis, emphasizing the challenges of subjective labeling and heuristic approaches.\nFuture work will focus on deploying EVINCE with platforms like Wikipedia to provide real-time perspective suggestions, empowering users with diverse viewpoints and promoting informed discourse. To further safeguard AI safety and ethics, we will investigate the potential of integrating EVINCE with other bias mitigation techniques, aiming to create a comprehensive and robust framework for ensuring fairness and impartiality in both AI-generated and human-curated content."}, {"title": "Limitations", "content": "Several key research questions remain open in the field of multi-LLM research. First, it is crucial to investigate whether the adversarial behavior exhibited by LLMs is genuine or artificially constructed based on their training data. Further research is needed to determine if these models can trace back to minority perspectives within their training data, validating the authenticity of adversarial stances.\nSecond, the metrics proposed for measuring debate quality and consensus convergence need more comprehensive evaluation. Assessing these metrics thoroughly will help identify and address their limitations, ensuring they provide reliable and meaningful insights into the dynamics of multi-LLM debates.\nThird, the cost of supporting multi-LLM, multi-round dialogue increases about tenfold. Ideally, the framework should be integrated into an LLM to conduct internal cross-validation before generating output, reducing the need for external multi-round communication. We have been investigating the use of a lightweight, independent guardrail-LLM to provide ethical advice and mitigate this cost issue. However, this approach must be carefully implemented to avoid introducing catastrophic forgetting (Kirkpatrick et al., 2017; Rusu et al., 2016), where the integration of new knowledge or parameters can lead to the loss of previously learned information in the knowledge-LLMs.\nWhile we believe our work is among the earliest proposals for multi-LLM debate frameworks (July 2023), providing definitive evidence is challenging due to anonymity policies on platforms like arXiv and the difficulty of identifying all subsequent work that may have been influenced by our ideas. Nevertheless, we are confident in the originality of our approach, which conditions the linguistic behaviors of LLMs with carefully guided theories and metrics. We hope that this work will inspire further research and development in this promising area."}, {"title": "Ethical Statements", "content": "This research, conducted in alignment with the Association for Computational Linguistics' ethical guidelines, primarily aims to mitigate fairness and biases in computational linguistics and to address the challenges of inaccurate and biased information. We have not involved any direct human or animal subjects in our study. All data utilized for computational analysis is sourced from publicly available datasets or collected with explicit consent, respecting privacy and data protection standards.\nWe have implemented rigorous measures to anonymize any sensitive data to safeguard individual privacy. Our algorithms are specifically designed to promote fairness, actively working to identify and rectify biases with LLMs. Additionally, this study contributes to the detection and correction of inaccurate and misleading information, a crucial step towards ensuring the integrity and reliability of data in natural language processing.\nWe acknowledge the potential impact of our research, especially in the context of misinformation and bias in AI technologies. Our commitment is to foster advancements in the field that are both ethically responsible and socially conscious, acknowledging the significant role these technologies play in shaping public discourse and information dissemination.\nIn the execution of this project, we utilized GPT-4, specifically for the purposes of conducting multi-LLM debates and providing editorial assistance. Apart from these specified uses, no artificial intelligence tools were employed in any other aspects of this work's completion."}]}