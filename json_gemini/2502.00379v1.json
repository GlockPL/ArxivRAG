{"title": "Latent Action Learning Requires Supervision in the Presence of Distractors", "authors": ["Alexander Nikulin", "Ilya Zisman", "Denis Tarasov", "Nikita Lyubaykin", "Andrey Polubarov", "Igor Kiselev", "Vladislav Kurenkov"], "abstract": "Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by 8x, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by 4.2x on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.", "sections": [{"title": "1. Introduction", "content": "Recently, a new wave of approaches based on latent action learning has emerged (Edwards et al., 2019), demonstrating superior pre-training efficiency on datasets without action labels in large-scale robotics (Ye et al., 2024; Chen et al., 2024; Cui et al., 2024; Bruce et al., 2024) and reinforcement learning (Schmidt & Jiang, 2023). Latent Action Models (LAM) infer latent actions between successive observations, effectively compressing observed changes. Under certain conditions, latent actions can even rediscover the ground truth action space (Schmidt & Jiang, 2023; Bruce et al., 2024). After training, LAM can be utilized for imitation learning on latent actions to obtain useful behavioral priors. For example, LAPA (Ye et al., 2024) showed that latent action learning can be used to pre-train large model on only human manipulation videos, and despite the huge cross-embodiment gap, still outperform OpenVLA (Kim et al., 2024b), which was pre-trained on expert in-domain data with available action labels.\nDespite the initial success and the promise of unlocking vast amounts of video available on the web (Schmidt &\nJiang, 2023; Ye et al., 2024), there is a critical shortcoming of previous work \u2013 it uses distractor-free data, where all changes between observations are mainly and most efficiently explained by ground truth actions only, such as robot manipulation on a static background (Khazatsky et al.,"}, {"title": "2. Preliminaries", "content": "Learning from observations. Most methods in reinforcement learning require access to the dataset \\(D := {T_n}_{n=1}^N\\) of N trajectories, where each \\(t_n := {(o^\u03c4 , a^\u03c4 , r^\u03c4 )}_{\u03c4=1}^{|t_n|}\\) contains observations, actions and rewards. Similarly, imitation learning requires access to trajectories \\(t_n := {(o^\u03c4 , a^\u03c4 )}_{\u03c4=1}^{|t_n|}\\) that contain actions. Unfortunately, most expert demonstrations in the real world, such as YouTube videos of some human activity (Aytar et al., 2018; Baker et al., 2022; Zhang et al., 2022a; Ghosh et al., 2023), do not include rewards or action labels. Thus, researchers are actively exploring how to most effectively use the data \\(T_n := {(o^\u03c4 )}_{\u03c4=1}^{|t_n|}\\) without action labels to accelerate the learning of embodied agents at scale (Torabi et al., 2019). Still, we can often assume that a very small number of action labels are available. For example, previous work has explored ratios of up to 10% (Zheng et al., 2023), whereas in our work we allow a maximum of ~2.5% of labeled transitions.\nLatent action learning. Latent action learning approaches (Edwards et al., 2019; Schmidt & Jiang, 2023; Chen et al., 2024; Cui et al., 2024; Ye et al., 2024) aim to infer latent actions zt such that they are maximally informative about each observed transition (Ot, Ot+1) while being minimal. After the latent action model (LAM) is pre-trained, we can train policies to imitate latent actions on full data to obtain useful behavioral priors. Finally, small decoder heads can be learned from latent to real actions of domain of interest."}, {"title": "3. Experimental Setup", "content": "Environments and datasets. To decouple the effects of latent action quality and exploration on performance, we work in an offline setting. For our purposes, it is essential that the Behavior Cloning (BC) agent should recover most of the expert performance when trained on the full dataset with ground-truth actions revealed, otherwise it would be difficult to understand the effect of latent action quality on pre-training.\nAs currently existing benchmarks with distractors (Stone et al., 2021; Ortiz et al., 2024) are not yet solved, we collect new datasets with custom difficulty, based on Distracting Control Suite (DCS) (Stone et al., 2021). DCS uses dynamic background videos, camera shaking and agent color change as distractors (see Figure 3 for visualization). The complexity is determined by the number of videos as well as the scale for the magnitude of the camera and the color change. We empirically found that using 60 videos and a scale of 0.1 is the hardest setting when BC can still recover expert performance. We collect datasets with five thousand trajectories for four tasks: cheetah-run, walker-run, hopper-hop and humanoid-walk, listed in the order of increasing difficulty. See Appendix C for additional details.\nEvaluation. To access the quality of the latent actions, we use two methods. First, we follow the approach of Zhang et al. (2022b) and use linear probing (Alain, 2016), which is a common technique used to evaluate the quality of learned representations by training a simple linear classifier or regressor on top the representations. Since we include ground truth actions in our datasets for debugging purposes, we train linear probes to predict them from latent actions simultaneously with the main method, e.g. LAPO (Schmidt & Jiang, 2023). We do not pass the gradient through the latent actions, so this does not affect the training. Second, following the most commonly used three-stage pipeline (Schmidt & Jiang, 2023; Chen et al., 2024; Ye et al., 2024), we first pre-train LAM, then train BC model to predict latent actions on the full dataset, and finally, we reveal a small number of labeled trajectories to train a small two-layer MLP decoder from latent to real actions. Using this decoder, we then evaluate the resulting agent in the environment for 25 episodes. To access scaling properties with different budgets of real actions, similar to Schmidt & Jiang (2023), we repeat this process for a variable number of labeled trajectories, from 2 to 128. All experiments are averaged over three random seeds."}, {"title": "4. Latent Action Learning Struggle in the Presence of Distractors", "content": "To access the effect of distractors on latent action learning we start by carefully reproducing and adapting LAPO (Schmidt & Jiang, 2023) for our domain. We use similar architecture (see Figure 2) with ResNet (He et al., 2016) as observation encoders, borrowed from the open-source official LAPO implementation. Similar to Schmidt & Jiang (2023) we resize observations to 64 height and width, stacking 3 consecutive frames.\nQuantization hinders latent action learning. To validate our implementation, we first measured performance on distractor-free datasets, which should not cause any difficulty. Contrary to previous research (Schmidt & Jiang, 2023; Chen et al., 2024; Ye et al., 2024; Bruce et al., 2024), we found that commonly used latent action quantization during training significantly hindered the resulting latent action quality. We initially hypothesized that the problem might be with the VQ-VAE used for quantizing. In con-"}, {"title": "5. Latent Action Learning Requires Supervision in the Presence of Distractors", "content": "In previous sections, we proposed LAOM, an improved version of LAPO which almost doubled the downstream performance in the presence of distractors for all budgets of true action labels considered. However, overall performance remained quite low. Similar to unlikelihood of recovering the control-endogenous minimal state in the presence of distractors (Misra et al., 2024), our results suggest that without any supervision latent action learning may not be able to learn actions useful for efficient pre-training. What if we can provide supervision? Even the smallest number of true actions may ground latent action learning to focus on control-related features. We explore this in the following experiments.\nSupervision significantly increases downstream perfor-"}, {"title": "6. Related Work", "content": "Action relabeling with inverse dynamics models. Simplest approach to utilize unlabeled data it to pretrain IDM on small number of action labels to further re-label a much large dataset (Torabi et al., 2018). Baker et al. (2022) showed that this approach can work on a scale, achieving great success in Minecraft (Kanervisto et al., 2022). Zhang et al. (2022a) used similar pipeline, unlocking hours of in-the-wild driving videos for pretraining. Schmeckpeper et al. (2020) used unlabeled human manipulation videos within online RL loop, which supplied labels to IDM for re-labeling. Zheng et al. (2023) conducted large scale analysis of IDM re-labeling in offline RL setup, showing that only 10% of suboptimal trajectories with labels is enough to match performance on fully labeled dataset.\nIn contrast to previous work (Schmeckpeper et al., 2020; Baker et al., 2022; Zheng et al., 2023), we show that while IDM is a strong baseline in setups without distractors (see Figure 12 in Appendix F), it generalizes poorly when distractors are present. Our results show that when a small number of action labels are available, it is much better to combine IDM and latent action learning to achieve much stronger performance and generalization (see Figure 7), suggesting that for web-scale data (Baker et al., 2022; Zhang et al., 2022a) our approach may be better than simple IDM re-labeling.\nLatent action learning. To our knowledge, Edwards et al. (2019) was the first to propose the task of recovering latent actions and imitating latent policies from observation, with limited success on simple problems. However, the original objective had scalability issues (Struckmeier & Kyrki, 2023). LAPO (Schmidt & Jiang, 2023) greatly simplified the approach, removed scalability barriers, and for the first time achieved high success on the hard, procedurally generated ProcGen benchmark (Cobbe et al., 2020). Latent action"}, {"title": "7. Conclusion", "content": "In this work, we empirically investigated the effect of action-correlated distractors on latent action learning. We showed that LAPO struggles to learn latent actions useful for pre-training. Although we proposed LAOM, a simple modification of LAPO, which doubled performance, it did not fully close the gap with the distractor-free setting. Crucially, we found that even minimal supervision - reusing as little as 2.5% of the dataset's ground-truth action labels during latent action learning significantly improved downstream performance, challenging the conventional pipeline of first pre-training LAM and only then decoding from latent to real actions. Our findings suggest that integrating supervision is essential for robust latent action learning in real-world scenarios, paving the way for unlocking the vast amounts of video data available on the web for embodied AI. We discuss the limitations of our work in the Appendix A."}, {"title": "A. Limitations", "content": "There are several notable limitations to our work. First, although we used the Distracting Control Suite (Stone et al., 2021), which allows us to precisely control the difficulty of distractors in a convenient way and clearly access generalization to new distractors, the overall distribution and noise patterns may be quite different compared to real-world videos on the web. Thus, our conclusions may not be fully applicable, e.g. it is possible that supervision is not as important for relevant to embodied AI data, or vice versa, it may turn out to be much more necessary for good results than we have used. Nevertheless, we believe that the overall conclusion about the need for some form of supervision is quite general.\nSecond, the need for supervision for latent action learning is a serious limitation, as compared to our setup, which is more reminiscent of Minecraft (Kanervisto et al., 2022) or Nethack (Hambro et al., 2022), where both labeled and unlabeled data are available, we have no chance to get real labels for already existing videos on the web or to fully cover their diversity with hand-crafted labels. Therefore, further research is needed to find out whether pre-training LAM on web data combined with supervision on robot data will achieve a similar effect, although our preliminary experiment on cross-embodied pre-training is pessimistic. It is quite possible that supervision can come in other forms than ground-truth actions, as we simply need a way to ground latent actions on control-related features of the observations. For example, for egocentric videos (Grauman et al., 2022) we can use hand tracking as a proxy action to supervise latent action learning.\nFinally, similar to offline RL (Levine et al., 2020), the problem of hyperparameter tuning remains, since without action labels there is currently no way to access the quality of latent actions."}, {"title": "B. Additional Related Work", "content": "Learning with distractors. Distractors in various forms are commonly used in many sub-fields of reinforcement learning, such as: visual model-based learning, model-free learning, and representation learning.\nIn model-based learning, researchers explore ways to efficiently train world models that do not waste their capacity to model task-irrelevant details, either via decomposing world models to predict relevant and irrelevant parts separately (Fu et al., 2021; Wang et al., 2022; Wan et al., 2023; Wang et al., 2024) or by avoiding reconstructing observations (Okada &\nTaniguchi, 2021; Deng et al., 2022; Zhu et al., 2023; Liu et al., 2023b; Burchi & Timofte, 2024). In our work, we have a similar need to not model action irrelevant details, as this will result in latent actions that describe changes in exogenous noise, not changes cased by ground truth actions. Thus, we use the commonly occurring latent temporal consistency loss (Schwarzer et al., 2020; Hansen et al., 2022; Zhao et al., 2023).\nIn model-free learning, researchers explore various techniques to improve generalization to new distractors and domain shifts (Hansen & Wang, 2021; Hansen et al., 2021; Bertoin et al., 2022; Huang et al., 2022b; Batra & Sukhatme, 2024;\nAlmuzairee et al., 2024), which often revolves around the use of augmentations (Ma et al., 2022). In our work we also use augmentations, specifically a subset of ones proposed by Almuzairee et al. (2024), to stabilize LAM training with latent temporal consistency loss (Schwarzer et al., 2020).\nIn representation learning, researchers search for ways to obtain minimal representations that contain only task- (Yamada et al., 2022), reward- (Zhou et al., 2023) or control-related information (Zhang et al., 2020; Lamb et al., 2022; Liu et al., 2023a; Ni et al., 2024; Levine et al., 2024), as this can greatly increase sample efficiency and generalization (Kim et al., 2024a). In our work, inspired by Lamb et al. (2022), we incorporate the multi-step IDM into LAM and show that it can help learn better latent actions in the presence of exogenous noise. Moreover, when small number of ground truth actions is available for pre-training (see Figure 1), our model on them conceptually reduces to one proposed by Levine et al. (2024), for which it has been theoretically shown that it can recover control-endogenous minimal state. This may explain why incorporating labels during LAM pre-training, rather than during final fine-tuning, brings so much benefit, since discovering true actions is trivial given a minimal state. We however, found a contradicting evidence, as Figure 10 shows that our proposed methods do not learn minimal state in practice.\nOverall, although we were inspired by existing approaches, they have not previously been used to improve latent action learning, especially in combination, which, as we show (see Figure 5) is essential for good performance in the presence of distractors."}, {"title": "C. Data Collection", "content": "We used environments from the Distracting Control Suite (DCS), wrapped with Shimmy wrappers for compatibility with the Gymnasium API. For cheetah-run, walker-run and hopper-hop we used PPO (Schulman et al., 2017), adapted from the CleanRL (Huang et al., 2022a) library. For humanoid-walk, we used SAC (Haarnoja et al., 2018) from the stable-baselines3 (Raffin et al., 2021) library, as PPO from CleanRL was not able to solve it at the expert level. We used default hyperparameters and trained on 1M transitions in each environment, except for humanoid-walk, where we trained on 100k transitions. Importantly, for speed, all experts were trained with proprioceptive states and no distractors, we later rendered proprioceptive states to 64px images with or without distractors during data collection. For each environment, we collected 5k trajectories, with an additional 50 trajectories for evaluation with novel distractor videos (from the evaluation set in the DCS). As each trajectory consists of 1000 steps, the datasets contain 5M transitions. We include ground truth actions and states for debugging purposes. The datasets will be released together with the main code repository."}, {"title": "D. Implementation Details", "content": "All experiments were run on H100 GPUs, in single-gpu mode and PyTorch bf16 precision with AMP. For the visual encoder, we used ResNets from the open-source LAPO (Schmidt & Jiang, 2023) codebase, which also borrowed from baselines originally provided as part of the ProcGen 2020 competition. For the action decoder, we used a two-layer MLP with 256 hidden dimensions and ReLU activations.\nIn contrast to the commonly used cosine similarity, we used MSE for temporal consistency loss. We also found that projection heads degraded performance, so we did not use them. We use slightly non-standard MLP for latent IDM and FDM: we compose it from multiple MLP blocks inspired by Transformer architecture (Vaswani, 2017) and condition on latent action and observation representation on all layers instead of just the first. We have found that this greatly improves prediction, especially for latent actions. We also use ReLU6 activations instead of GELU, as it naturally bounds the activations, which helps with stability during training, similar to target networks in RL (Bhatt et al., 2019). Without supervision, we use the EMA target encoder. With supervision, we find that a simple stop-grad is sufficient to prevent any signs of collapse, a finding also reported by Schwarzer et al. (2020)."}, {"title": "E. Evaluation Details", "content": "We outline the evaluation procedures used in our experiments for each method. First, we review the general setup. For each environment, we have a large dataset without action labels, with and without distractors. To decode the learned latent actions"}]}