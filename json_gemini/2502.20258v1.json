{"title": "LLM as a Broken Telephone: Iterative Generation Distorts Information", "authors": ["Amr Mohamed", "Mingmeng Geng", "Michalis Vazirgiannis", "Guokan Shang"], "abstract": "As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the \"broken telephone\" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) are becoming an integral part of our daily lives, helping us process, comprehend, and convey information via text, while also expanding their support to additional areas (Yin et al., 2023). Consequently, an increasing amount of online content is now model-generated or assisted (Geng and Trotta, 2024), and such content is almost indistinguishable from human-produced data (Uchendu et al., 2023).\nThis prompts us to consider the question: what effects arise when the same piece of information is repeatedly processed by LLMs through multiple iterations? This procedure is analogous to the telephone game in human communication, a widely known children's game in which a message is passed sequentially from one player to the next, with the final version often differing significantly from the original, usually with amusing or humorous effect. This happens because players often act as broken telephones, where information is gradually distorted as it is passed along the chain of individuals, highlighting how repeated transmission can lead to the accumulation of errors, omissions, or unintended alterations (Hitchcock et al., 2011).\nInvestigating these effects for LLMs is becoming increasingly crucial in the present era, because LLMs are not only consuming human-supplied information at one time, but also processing their own outputs in an iterative way. Therefore, our study focuses on exploring whether LLM also acts as a broken telephone, when the same content is continuously refined, paraphrased, or reprocessed, and particularly when the generated output becomes the input for subsequent model iterations. We expect to observe an effect similar to that of human information distortion through iterative generation.\nIn our study, we simulate the LLMs' telephone game in conjunction with the translation task mainly, under three experimental setups. As illustrated in Figure 1, within each iteration, a document in English is subsequently translated into one or more different languages, then back to English, by leveraging LLMs. We compare the back-translated"}, {"title": "Related Work", "content": "Model Collapse. Iterative training on synthetically generated data induces model collapse, a phenomenon characterized by systematic erosion of the long-tail components of the original data distribution (Shumailov et al., 2023). Theoretical analyses further elucidated how self-consuming training loops alter intrinsic scaling laws, thereby intensifying this collapse (Fu et al., 2024; Dohmatob et al., 2024), complementing earlier findings on distributional distortions (LeBrun et al., 2022). Furthermore, Guo et al. (2024b) demonstrated that iterative training on synthetic text does not preserve the nuanced richness of human language, particularly in creative tasks, underscoring the broader challenges of maintaining linguistic diversity in iteratively generated content.\nIterative Generation and Information Evolution. Iterative generation can trigger model collapse, whereby the diversity of real-world information degrades over time\u2014a process that Peterson (2024) defines as knowledge collapse. Research on language evolution offers a framework for analyzing these degradations (Markov et al., 2023), aligning with broader perspectives on cultural evolution (Mesoudi and Whiten, 2008; Caldwell and Millen, 2008). In the context of LLMs, Perez et al. (2024) analyzed text properties evolution in rephrasing, continuation, and inspiration-taking tasks. Their work, however, overlooked translation\u2014a key LLM application\u2014and focused solely on chains involving a single model. Our work overcomes these shortcomings by investigating how iterative information translation accelerates distortion."}, {"title": "Methodology", "content": "In this section, we formalize the telephone game procedure with machine translation, noting that the broken telephone effect may occur with any generative task when carried out iteratively."}, {"title": "Notations and Definitions", "content": "Let $D = \\{d_i\\}_{i=1}^I$ denote a set of $I$ documents, $L = \\{l_j\\}_{j=1}^J$ as a set of $J$ natural languages, and $M = \\{m_k\\}_{k=1}^K$ for a set of $K$ models.\nWe define a translation chain as a sequence of $N$ translation iterations that progressively transform a document. For iteration $t \\geq 1$, let $d_{i,l_{source}}^{(t-1)}$ be the $i$-th document in the source language at iteration $t-1$. At iteration $t$, an ordered language chain $L^{(t)}$ is constructed by selecting a permutation $\\pi^{(t)}$ of $J-1$ languages from $L$ and forming the sequence\n$L^{(t)} = (l_1^{(t)}, l_2^{(t)}, \\dots, l_{J-1}^{(t)})$\n                                                                                                                                                                                                                                                                                                                                                                                                                          (1)\nwith the requirement that $l_1^{(t)} = l_{source}$ (ensuring that the final translation returns to the source language). Simultaneously, a model sequence\n$M^{(t)} = (m_1^{(t)}, m_2^{(t)}, \\dots, m_{J-1}^{(t)})$\n                                                                                                                                                                                                                                                                                                                                                                                                                                          (2)\nis defined, where each $m_j^{(t)}$ is sampled uniformly from $M$ (allowing repeats; if $|M| = K = 1$, the same model is used throughout).\nLet $T_{a\\leftarrow b}^{m}$ denote the translation operator that converts an input from language $b$ to language $a$ using model $m$. The composed operator for iteration $t$ is then\n$T^{(t)} = T_{l_1^{(t)} \\leftarrow l_2^{(t)}}^{m_1^{(t)}} \\circ T_{l_2^{(t)} \\leftarrow l_3^{(t)}}^{m_2^{(t)}} \\circ \\dots \\circ T_{l_{J-2}^{(t)} \\leftarrow l_{J-1}^{(t)}}^{m_{J-2}^{(t)}} \\circ T_{l_{J-1}^{(t)} \\leftarrow l_{source}}^{m_{J-1}^{(t)}}$\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (3)\nso that the updated document is given by\n$d_{i,l_{source}}^{(t)} = T^{(t)} (d_{i,l_{source}}^{(t-1)})$\n                                                                                                                                                                                                                                                                                                                                                                                                        (4)\nStarting with $d_{i,l_{source}}^{(0)} = d_i$, the process yields the sequence $(d_{i,l_{source}}^{(0)}, d_{i,l_{source}}^{(1)}, \\dots, d_{i,l_{source}}^{(N)})$, where $N$ is the total number of iterations."}, {"title": "Experimental Settings", "content": "Languages. We selected English (EN) as $l_{source}$ for all experiments and French (FR), German (DE), Dutch (NL), Vietnamese (VN), Chinese (ZH), and Thai (TH) as the bridge (intermediate) languages in the translation chains. Within each iteration, a document in English is subsequently translated into one or more bridge languages, then back to English. This set creates varying degrees of semantic, lexical, and syntactic similarities between the source language and the bridge languages, which may differentially influence the extent of distortion introduced within the translation chains (Marchisio et al., 2020; Guerin et al., 2024)."}, {"title": "Datasets.", "content": "We utilized three datasets that span distinct domains: BookSum (Kry\u015bci\u0144ski et al., 2021), ScriptBase-alpha (Gorinski and Lapata, 2015), and (BBC)News2024 (Li et al., 2024a), from which we select articles published in 2024 to minimize the chances of data exposure that may result in biases amplification over the iterations (Luo et al., 2024; Li et al., 2024a). For our experiments, we randomly select 150 documents from each dataset, with each document containing between 100 and 200 words long."}, {"title": "Models.", "content": "We primarily used two models, LLAMA-3.1-8B-INSTRUCT (Llama) (Dubey et al., 2024) and MISTRAL-7B-INSTRUCT-V0.2 (Mistral) (Jiang et al., 2023), for our main experiments. Additionaly, GEMMA-2-9B-IT (Gemma) (Team et al., 2024) is incorporated into Experiment 3 (Section 4.3) to evaluate higher complexity chains."}, {"title": "Decoding Parameters and Translation Prompt.", "content": "Each model was used for inference with its default decoding parameters. We capped the maximum number of newly generated tokens at 8000 to encourage open-ended generation. This high limit allows translations, which can vary in length across different languages, to conclude naturally rather than being prematurely truncated. Models within the main experiments were prompted to translate documents from a source to a target language with a moderately constrained prompt. The full translation prompt can be found in Appendix C."}, {"title": "Evaluation Metrics", "content": "To comprehensively assess the impact of iterative generation on text quality, we employ two complementary sets of evaluation metrics: textual relevance and factuality preservation. The former quantifies the lexical, syntactic, and semantic deviations introduced at each generation step, while the latter evaluates the degree to which the generated text remains faithful to the original information.\nTextual Relevance. We used BLEU (Papineni et al., 2002) to detect incremental errors, ROUGE-1 (Lin, 2004) to quantify word-level omissions and subtle deviations, CHR-F (Popovi\u0107, 2015) for capturing character-level deviations and errors accumulation, METEOR (Banerjee and Lavie, 2005) for being adept at capturing paraphrastic variations and subtle semantic shifts, and finally BERTScore (Zhang et al., 2019) for its focus on nuanced contextual and semantic relationships beyond traditional n-gram overlap-based methods.\nFactuality Preservation. FActScore (Min et al.,"}, {"title": "Experiment 1: Bilingual Self-loop", "content": "Setup. We fix the language set to\n$L = \\{EN, l_{bridge}\\}$                                                                                                                                                                                                                                                                                                                                                                                                    (5)\nwhere $l_{bridge} \\in \\{FR, DE, NL, VN, ZH, TH\\}$. We consider the case when $|M_1| = |M_2| = 1$, with $M_1$ and $M_2$ containing Llama and Mistral respectively. We also consider the three datasets: BookSum, ScriptBase-alpha, and News2024. For each dataset $D$, every document $d_i^{(0)} \\in D$ undergoes $N = 100$ translation iterations with an iteration of the form:\nEN $\\rightarrow l_{bridge} \\rightarrow$ EN.\n                                                                                                                                                                                                                                                                                                                                                        (6)\nAll translations within a single chain are performed by a single model. Concretely, at iteration $t$, the translation operator\n$T^{(t)} = T_{EN\\leftarrow l_{bridge}}^{m_1} \\circ T_{l_{bridge}\\leftarrow EN}^{m_1}$\n                                                                                                                                                                                                                                                                                                                                                                    (7)\nis applied to produce\n$d_i^{(t)} = T^{(t)} (d_i^{(t-1)})$.\n                                                                                                                                                                                                                                                                                                                                                                                                                 (8)\nThis yields the sequence $(d_i^{(0)}, d_i^{(1)}, \\dots, d_i^{(100)})$ for each document $d_i^{(0)} \\in D$.\nHypothesis 1 (H1) We hypothesize that iterative translation chains better preserve relevance and factuality when the bridge language shares lexical overlap, script, and syntax with the source language. In contrast, languages markedly dissimilar from the source language are expected to introduce greater distortion over iterations.\nResults. Figure 2 presents Llama's iterative translation outcomes on the News2024 dataset. Across all language pairs, there is a gradual decline in both factuality and relevance. Notably, language pairs exclusively using Latin script\u2014with bridge languages such as French, German, and Dutch-demonstrated superior preservation of"}, {"title": "Experiment 2: Bilingual Two-player", "content": "Setup. We fix the language set to\n$L = \\{EN, l_{bridge}\\}$\n                                                                                                                                                                                                                                                                                                                                                                                                                      (9)\nwhere $l_{bridge} \\in \\{FR, TH\\}$. Following the results presented in Section 4.1, we selected EN $\\leftrightarrow$ FR and EN $\\rightarrow$ TH for Experiment 2, as they demonstrated the lowest and highest levels of information distortion, respectively. We consider a model set $M$ that includes both Llama and Mistral.\nFor this experiment, we used the News2024 dataset because, as shown in Section 4.1, the choice of dataset did not significantly influence the observed trends, and to further mitigate data exposure (Luo et al., 2024; Li et al., 2024a).\nUnlike Experiment 1, where a single model was used for both translation directions, we allow each translation step to potentially use a different model. At iteration $t$, we define a two-component model sequence:\n$M^{(t)} = (m_1^{(t)}, m_2^{(t)})$\n                                                                                                                                                                                                                                                                                                                                                                                                                                  (10)\nwhere $m_1^{(t)}$ is the model used for the translation from English to $l_{bridge}$, and $m_2^{(t)}$ is the model used for the translation from $l_{bridge}$ to English. Each component is sampled uniformly from $M$."}, {"title": "Experiment 3: Multilingual Multiplayer", "content": "Setup. In this experiment, we design three settings of increasing complexity, each incorporating at least two bridge languages and at least two models within the same translation chain. The objective is to examine whether introducing a greater number of languages or models accelerates distortion.\nSetting 1. We fix\n$L = \\{EN, FR, TH\\}$\n                                                                                                                                                                                                                                                                                                                                                                                                                        (12)\nand define $M$ to contain both Llama and Mistral. At each iteration $t$, we sample a permutation $L^{(t)} = \\pi^{(t)} (L)$ that enforces a cyclic translation path:\nEN $\\rightarrow l_1^{(t)} \\rightarrow l_2^{(t)} \\rightarrow$ EN,\nwith $l_1^{(t)}$ and $l_2^{(t)}$ drawn from \\{FR, TH\\} and satisfying $l_1^{(t)} \\neq l_2^{(t)}$ . The corresponding model sequence is\n$M^{(t)} = (m_1^{(t)}, m_2^{(t)}, m_3^{(t)})$\n                                                                                                                                                                                                                                                                                                                                                                                                                                      (13)\nwith each $m_i^{(t)}$ sampled uniformly from $M$. The translation operator at iteration $t$ is composed as:\n$T^{(t)} = T_{EN\\leftarrow l_1^{(t)}}^{m_1^{(t)}} \\circ T_{l_1^{(t)}\\leftarrow l_2^{(t)}}^{m_2^{(t)}} \\circ T_{l_2^{(t)}\\leftarrow EN}^{m_3^{(t)}}$\n                                                                                                                                                                                                                                                                                                                                                                (14)\nwhich is applied iteratively to generate:\n$d_i^{(t)} = T^{(t)} (d_i^{(t-1)})$.\n                                                                                                                                                                                                                                                                                                                                                                                                                 (15)\nThis produces $(d_i^{(0)}, d_i^{(1)}, \\dots, d_i^{(N)})$, where $d_i^{(0)}$ is the original document and $N = 100$.\nSetting 2. We here retain $L$ and the translation chain structure from Setting 1, utilizing the same translation operator as defined in Equation 14, while expanding $M$ with an additional model, Gemma, to assess the impact of adding more models of similar size into the chain."}, {"title": "Ablation Studies", "content": "Other Tasks: Rephrasing\nBuilding on our findings in section 4, we extend our experiments to explore whether information distortion manifests in other types of iterative generation chains. Inspired by the work of Perez et al. (2024), who examined the evolution of toxicity, positivity, difficulty, and length in rephrasing as well as in"}, {"title": "Temperature Variation Affects Outputs", "content": "To further investigate the impact of decoding parameters on the models' outputs, we conducted several experiments using Llama across a spectrum of temperature parameter values, including 1 \u00d7 10-6, 0.25, 0.5, 0.75, and 1.0 on 30 randomly sampled documents from News2024.\nFrom Figure 5, higher temperature settings lead to greater factual and semantic degradation. At extremely low temperatures (1 \u00d7 10\u22126), factuality drops slightly in the first two iterations but stabilizes thereafter. As temperature increases, stability diminishes, and factuality gradually diverges."}, {"title": "Sensitivity of Iterative Translation Outputs to the Chosen Prompt", "content": "We subsequently investigated the influence of the translation prompt on the outputs produced by the iterative process. To this end, 30 documents were randomly sampled from the News2024 dataset, and Llama was tasked with translating them using three distinct prompts characterized by varying levels of constraint: simple, base (used in all our experiments), and constrained."}, {"title": "Discussion and Conclusion", "content": "As LLMs increasingly shape online content, the likelihood that they re-process their own outputs continues to rise. This study confirms that such iterative generation leads to progressive information distortion, akin to the \u201cbroken telephone\u201d effect in human communication. Our findings from translation-based experiments are multifaceted.\nEffect of intermediate language(s) on information distortion. As found in Experiment 1, different language chains have varying levels of sensitivity to information distortion. As presented in Figure 2, we found that transmitting information between English and a highly similar language significantly reduces the distortion effect, while transmitting through a dissimilar language results in a more pronounced distortion. We suggest that this variation in information retention and distortion stems from the proportion of each language encountered during the models' training, with underrepresented languages experiencing greater distortion.\nChains of higher complexity may result in higher levels of distortion. Experiments 2 and 3 showed that increasing the levels of complexity of chains can result in higher levels of distortion. Figure 3 illustrates how the combination of Llama and Mistral amplified the distortion in the chain when French served as the bridge language. However, when Thai was used as the bridge language, their collaboration helped reduce distortion-likely due to the stronger model (Llama) and the weaker model (Mistral) interacting with an intermediate language that may have been underrepresented in Mistral's training compared to Llama. Moreover, we observed that increasing the number of lan-guages in the translation chain amplifies information distortion, likely due to the cumulative effects of longer generation sequences. In contrast, incorporating Gemma into the chain improved information retention, which we hypothesize stems from its larger parameter count\u2014one to two billion more than Llama and Mistral. We leave the broader impact of model scaling for future work.\nInformation distortion can be reduced through temperature control and constrained prompting. Our findings suggest that while information distortion is unavoidable, it can be significantly mitigated through careful control of the model's generation temperature. Figure 5 shows that higher temperature values lead to greater distortion in the outputs, which we attribute to increased model creativity. A higher temperature encourages the generation of atypical tokens that may not fully preserve the meaning of the source document. Additionally, our analysis of prompt effects revealed that less constrained prompts contribute to greater noise accumulation over multiple iterations, resulting in higher divergence from the original meaning.\nThese findings underscore the need for strategies to mitigate such degradation and ensure the reliability of AI-generated content."}]}