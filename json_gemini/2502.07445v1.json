{"title": "Forget What You Know about LLMs Evaluations - LLMs are Like a\nChameleon", "authors": ["Nurit Cohen-Inger", "Yehonatan Elisha", "Bracha Shapira", "Lior Rokach", "Seffi Cohen"], "abstract": "Large language models (LLMs) often appear\nto excel on public benchmarks, but these high\nscores may mask an overreliance on dataset-\nspecific surface cues rather than true language\nunderstanding. We introduce the Chameleon\nBenchmark Overfit Detector (C-BOD), a\nmeta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric\ntransformation and detects overfitting of LLMs.\nBy rephrasing inputs while preserving their\nsemantic content and labels, C-BOD exposes\nwhether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU\nbenchmark using 26 leading LLMs, our method\nreveals an average performance degradation of\n2.15% under modest perturbations, with 20 out\nof 26 models exhibiting statistically significant\ndifferences. Notably, models with higher base-\nline accuracy exhibit larger performance dif-\nferences under perturbation, and larger LLMs\ntend to be more sensitive to rephrasings indi-\ncating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family\nand models with lower baseline accuracy show\ninsignificant degradation, suggesting reduced\ndependency on superficial cues. Moreover, C-\nBOD's dataset- and model-agnostic design al-\nlows easy integration into training pipelines to\npromote more robust language understanding.\nOur findings challenge the community to look\nbeyond leaderboard scores and prioritize re-\nsilience and generalization in LLM evaluation.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved\nimpressive results on a wide range of NLP tasks\n(Chang et al., 2024). Consequently, hundreds of\nbenchmarks have been established to track progress\nand evaluate model capabilities (Lu et al., 2024;\nLiang et al., 2022). However, the rapid prolifera-\ntion of LLMs and the frequent use of public leader-\nboards raise concerns about the robustness of these\nevaluation practices (Castillo-Bolado et al., 2024).\nSpecifically, as benchmark data becomes more\nwidely recognized, models may learn to exploit sur-\nface patterns or spurious correlations, rather than\nexhibit genuine language understanding. This is-\nsue can lead to deceptively high scores that do not\nreflect true progress.\nIn this paper, we examine whether LLMs rely\nexcessively on benchmark-specific cues potentially\noverfitting to the patterns inherent in widely pub-\nlished evaluation benchmarks and explore system-\ntic methods to detect and mitigate this behavior.\nIn other words, are LLMs prone to overfitting on\npopular benchmarks, and what underlying factors\ncontribute to this phenomenon?\nTo answer this question, we introduce\nChameleon Benchmark Overfit Detector (C-BOD),\na framework that reveals how heavily a model\ndepends on the exact wording or structure of a test\nset. By introducing controlled textual distortions to\nbenchmark prompts at varying intensities (defined\nby a distortion parameter \u03bc), as demonstrated in\nFigure 1, our method reveals whether high per-\nformance is built on superficial patterns. Notably,\nour framework requires only the evaluation set,\nwithout accessing the model's training data or\narchitecture. Unlike conventional leaderboards\nthat solely track performance, our meta-evaluation\nframework acts as a safeguard ensuring that high\nscores do not stem from superficial memorization\nof benchmark cues.\nOur Contributions:\n1. Robust Overfitting Detection with Statistical\nSignificance. Our framework computes the per-\nformance difference \u0394\u00b5 between original and\nperturbed prompts and confirms its statistical\nsignificance, ensuring that observed differences\nindeed indicate overfitting rather than chance\nvariations.\n2. New Findings For LLM Community Our ex-\ntensive analysis reveals new trends regarding\nhow LLMs function with respect to their num-\nber of parameters and baseline performance.\n3. Extensive Empirical Validation. We apply\nour method to multiple LLM families of var-\nious architectures and parameter sizes. Even"}, {"title": "2 Related Work", "content": "LLMs have achieved impressive results on many\nbenchmarks. This success has driven the develop-\nment of comprehensive evaluation suites such as\nBIG-Bench (Srivastava et al., 2022) and HELM\n(Liang et al., 2022). MMLU benchmark set\n(Hendrycks et al., 2020) evaluates question answer-\ning across 57 subjects including STEM, humani-\nties, and social sciences, while (Zhang et al., 2024a)\nintroduced 25 enterprise-focused datasets covering\ndomains like finance, legal, cybersecurity, and cli-\nmate sustainability for tasks such as classification,\nNER, and summarization. Another recent resource,\nJUDGE-BENCH (Bavaresco et al., 2024), com-\nprises 20 NLP datasets that assess models against\nhuman judgments. We focus on MMLU because\nof its widespread adoption and comprehensive do-\nmain coverage (Wang et al., 2024), which makes\nit particularly effective for exposing overfitting to\ncanonical prompt structures.\nWhile these benchmarks have been critical for\ncomparing new models' versions, recent studies\nwarn that publicly released evaluation sets can be-\ncome less reliable over time due to overexposure\nand memorization (Yu et al., 2024; Chang et al.,\n2024). In some cases, LLMs learn superficial pat-\nterns specific to well-known datasets, boosting per-\nformance without reflecting genuine semantic or\nconceptual understanding. (Kiela et al., 2021) fur-\nther emphasize the need for continuously refresh-\ning benchmarks to ensure real progress in language\nunderstanding. For example, OpenAI's GPT mod-\nels have shown steady improvement on MMLU:\nGPT-3 achieved approximately 43% accuracy in\n2020 (Brown et al., 2020), rising to nearly 70%\nwith GPT-3.5 in 2022\u00b9, and reaching 86% with\nGPT-4 in 2023 2.\nMemorization in LLMs has been widely stud-\nied (Kiyomaru et al., 2024; Biderman et al., 2024),\nwith larger models especially prone to retaining\ntraining data verbatim (Carlini et al., 2022). This\nphenomenon can inflate performance metrics while\nobscuring genuine model capabilities. Moreover,\nseveral works highlight training-set contamina-\ntion, where test samples appear exactly or as near-\nduplicates in the training data, as another crucial\nform of overfitting (Deng et al., 2023; Yao et al.,\n2024), leading to overly optimistic performance\nestimates (Yang et al., 2023)."}, {"title": "2.2 Gap in Current Work", "content": "Researchers have introduced various methods to\ndetect or mitigate training contamination: Finding\nthe N-gram overlap (e.g., 13-grams or 50-character\nmatches) between training and test data (Brown\net al., 2020; OpenAI, 2023), though it can miss se-\nmantically equivalent rephrasing. Embedding simi-\nlarity search (Reimers, 2019) that uses transformer-\nbased embeddings to identify semantically close\ntraining-test pairs (Lee et al., 2023). Decoding\nMatching probes the model by providing partial\ntest prompts and measuring how likely it is to com-\nplete them exactly (Li, 2023) or completing miss-\ning words (Deng et al., 2023). A recent study pre-"}, {"title": "3 Method", "content": "Let D denote a benchmark dataset with N sam-\nples, and $E$ a LLM to be evaluated with respect\nto a given performance function M. Our goal is\nto detect whether $E$ exhibits overfitting to D. Fig-\nure 2 provides an overview of our proposed method,\nChameleon Benchmark Overfit Detector (C-BOD).\nC-BOD employs a rephrasing transformation to\ngenerate a perturbed dataset from D, evaluates on\nboth the original and perturbed datasets, and ap-\nplies a statistical test to assess whether performance\ndiscrepancies indicate overfitting. The following\nsubsections detail each component of C-BOD."}, {"title": "3.1 C-BOD rephrased dataset generation", "content": "To systematically introduce textual variations, C-\nBOD utilizes a rephrasing tool, denoted as T,\nwhich uses as a distortion operator to generate a\nperturbed dataset Du from D. This operator is\nparameterized by \u00b5 (temperature), which controls\nthe extent of textual modification, ranging from\nlow (e.g., 0.1 for minimal changes like synonym\nsubstitution) to moderate (e.g., 1.0 for rewording\nand sentence fragment reordering) and high (e.g.,\n1.5 for aggressive modifications such as question\nreformulation). We define:\n$\\\u03a4\u03bc : X \u2192 \u03a7'$\nGiven a prompt xi, the distortion operator pro-\nduces a perturbed prompt x = $\u03a4\u03bc(xi)$. The per-\nturbed dataset is then constructed as:\n$D\u00b5 = {(x'i, Yi) | (Xi, Yi) \u2208 D}$\nAlthough each pair $(x1,yi)$ in the perutbed\ndataset remains semantically equivalent to $(xi, Yi)$\nin the original dataset, the textual variations intro-\nduced by Tu can disrupt purely memorized map-\npings from surface patterns to correct labels. This\nstep presented in Lines 5-6 of Algorithm 1."}, {"title": "3.2 Evaluating the Impact of Distortion", "content": "To assess the impact of distortion, we evaluate E\nusing a performance function, M. This function\nevaluates $E$ based on a given ground truth or prompt\nYi, considering two versions of an input: the orig-\ninal xi \u2208 D and the perturbed version x \u2208 Du,\nwhere i denotes the index of a sample in the dataset.\nSpecifically, M is a boolean function that takes as"}, {"title": "3.3 Statistical Validation", "content": "To assess the statistical significance of performance\ndifferences, we employ McNemar's test (McNe-\nmar, 1947), which is specifically designed for\npaired data. This test evaluates whether the dis-\ncrepancies between two related sets of classifica-\ntion outcomes, correct and incorrect predictions,\nare significant. In our context, McNemar's test\nis well-suited for comparing each pair of samples\n(xi, yi) \u2208 D and (x'i, yi) \u2208 D\u00b5, we record whether\n$E$ classifies them correctly and aggregate into b\n(original is better) and c (perturbed is better) as pre-\nsented in Equation 1, Equation 2. The McNemar\nstatistic is then calculated as:\n$x\u00b2 = (b - c) 2\nb+c$\nWe derive a p-value from the chi-squared dis-\ntribution (with df=1, i.e., one degree of freedom),\nrejecting the null hypothesis if p < a. A significant"}, {"title": "4 Experimental Setting", "content": "In this section, we describe the experimental setup\nused to evaluate our overfitting detection frame-\nwork. We detail the benchmark dataset, the pro-\ncedure for generating perturbed inputs, the LLMs\nunder evaluation, implementation specifics, and the\nevaluation metrics."}, {"title": "4.1 Dataset and Rephrasing Process", "content": "Our experiments use the MMLU bench-\nmark (Hendrycks et al., 2020), which comprises\nmultiple-choice questions spanning 57 subjects.\nThe broad coverage and public availability of\nMMLU make it an ideal candidate for assessing"}, {"title": "4.2 Models Under Evaluation", "content": "provides an overview of the LLMs evalu-\nated in our experiments. Our study covers a diverse\nset of architectures and parameter scales ranging\nfrom 1B to 236B parameters. This broad selection\nenables an in-depth analysis of how both architec-\ntural choices and model scale affect robustness to\nprompt perturbations."}, {"title": "4.3 Implementation Details", "content": "All experiments were executed under standardized\nconditions to ensure reproducibility and fair com-\nparisons:\n(1) Inference Environment: Most models were\naccessed via the HuggingFace transformers li-\nbrary using RTX 6000 GPU. DeepSeek 236B\nmodel was evaluated using the official API.\n(2) Dataset Rephrasing Prompt: We instruct the\nrephrasing tool using the following prompt to\ngenerate an alternative version of each ques-\ntion while preserving its original meaning and\ncorrect answer: \u201cRephrase the following ques-\ntion without changing its context or the correct\nanswer: {question}\u201d\n(3) Query Prompt: For every query, we con-\nstruct a standardized input by prepending a\nfixed instruction to the original MMLU ques-\ntion. Importantly, the multiple-choice options\nremain identical between the original and the\nrephrased forms. The fixed instruction is:\n\"Select the best answer from the given options.\nRespond with only the letter corresponding to\nthe correct choice.\nQuestion: {question}\""}, {"title": "4.4 Evaluation Metrics", "content": "We assess model performance by comparing the\noriginal dataset, D, with its perturbed counterpart,\nD1.0, using the following metrics:\nCorrect Predictions and Accuracy: For each\ndataset, we report the number of correct answers\nand the corresponding accuracy, defined as\nAccuracy = #Correct Predictions\n#Total Samples\nAbsolute and Percentage Performance Differ-\nence: The absolute difference in the number of\ncorrect answers between D and D1.0 is denoted by\n\u25b31.0; we also report the relative difference. Statis-\ntical Significance: McNemar's test is applied on\nthe paired predictions to determine whether the per-\nformance gap is statistically significant (p < 0.05)"}, {"title": "4.5 Reproducibility", "content": "C-BOD source code and datasets, including scripts\nfor data pre-processing, perturbation generation,\nperturbed datasets, model evaluation, and statistical\nanalysis, are publicly available. This ensures that\nour experiments can be independently replicated\nand verified."}, {"title": "5 Results", "content": "As shown in Table 2, most models (20 out of 26)\nexhibit a noticeable drop in performance on the\nrephrased test set compared to the original, reinforc-\ning our motivation that these LLMs overfit to the\nstandard MMLU format. Notably, the Llama 1B,\nLlama 3B models maintain relatively stable accu-\nracy, suggesting they are less susceptible to overfit-\nting. We also observed that Falcon 7B, DeepSeek\n7B, Qwen 2.5 3B and Jetmoe 8B show statistically\ninsignificant differences, likely due to their lower\nbaseline accuracy. McNemar's test confirms that\nthe performance declines observed in most mod-\nels are statistically significant. Notably, no model\nshows a significant improvement when inputs are\nrephrased. This indicates that the C-BOD method\nreliably uncovers model vulnerabilities rather than\noccasionally yielding unintentional performance\ngains.\nAcross all evaluated models, the average drop in\naccuracy was 2.15%, and when considering only\nthe models with statistically significant differences,\nthis drop increased to 2.72%."}, {"title": "5.2 Relationship Between Model Size and\nOverfit Detection", "content": "Figure 3 illustrates the scatter plot of the percent-\nage performance difference versus the number of\nparameters, with a red dashed line representing the\nlogarithmic fit (\u03941.0 = 0.6318. ln (#Params) +\n0.7920). The significant log-linear relationship in-\ndicates that the performance difference increases\nwith model size in a logarithmic fashion, suggest-\ning diminishing returns as the number of parame-\nters grows."}, {"title": "5.3 Relationship Between Model Accuracy\nand Overfit Detection", "content": "Figure 5 examines the relationship between base-\nline accuracy on the original prompts and the cor-\nresponding percentage difference in performance\nwhen evaluated on rephrased inputs. The plot\nclearly indicates that models with higher original\naccuracy tend to experience larger declines when\nexposed to prompt perturbations. For example, a\nmodel achieving over 80% accuracy on the origi-\nnal set shows one of the largest \u03941.0 values, while\nmodels with lower baseline accuracy exhibit only\nminor, often statistically insignificant, differences.\nThis observation highlights a paradox in current\nLLM evaluation: models that perform exception-\nally well on standard benchmarks may be capital-\nizing on dataset-specific cues rather than demon-\nstrating robust language understanding. The posi-\ntive correlation between original accuracy and \u03941.0\nunderscores the need to carefully interpret high\nbenchmark scores, as they might mask underlying\nvulnerabilities to prompt variations.\nThese findings underscore the importance of\nevaluating LLMs under varied prompt formula-\ntions to ensure that improvements in benchmark\nperformance reflect genuine advances in language\nunderstanding rather than overfitting."}, {"title": "6 Discussion", "content": "Table 3 highlights cases where LLMs answer\nthe original questions correctly but fail on the\nrephrased versions. The failures suggest potential\noverfitting, where models overly rely on surface-\nlevel cues, memorized patterns, or specific termi-\nnologies. Overfitting in this context occurs because\nthe model tends to associate certain question for-\nmats or keywords directly with answers instead of\ngeneralizing underlying concepts. Common root\ncauses include shifts in terminology, subtle changes\nin phrasing that alter the semantic scope, and depen-\ndence on memorized patterns from training data."}, {"title": "6.2 Forget What You Know About LLM\nEvaluation", "content": "Ideally, LLMs should exhibit resilience when faced\nwith variations in prompt wording and structure. In\nother words, robust LLMs are expected to main-\ntain their performance regardless of how a question\nis phrased, thereby reflecting true language under-\nstanding rather than mere memorization. However,\nour experiments reveal a contrary trend: models"}, {"title": "7 Conclusion", "content": "In this paper, we introduced a novel approach for\ndetecting overfit to benchmarks datasets in LLMs\nby applying parametric transformations to these\ndatasets. Our method revealed that many mod-\nels rely heavily on surface features of public test\nsets, leading to significant performance drops when\nthese features are altered. This finding underscores\na critical insight: what appears to be robust perfor-\nmance may, in fact, be largely driven by memoriza-\ntion rather than true generalization.\nWe demonstrated the effectiveness of our ap-\nproach across multiple LLM families. Notably,\nlarger models tend to exhibit more pronounced per-\nformance declines under perturbation, while cer-\ntain models (such as Llama) show greater stability.\nThese observations suggest that training strategies\nand architectural choices play a significant role\nin mitigating overfitting, prompting a necessary re-\nthinking of how we evaluate and benchmark LLMs.\nBy providing a practical, dataset-agnostic frame-\nwork, our work equips the community with a pow-\nerful tool to uncover overfitting and to drive the\ndevelopment of benchmarks that better capture gen-\nuine generalization. Incorporating these parametric\ntransformations into the evaluation process not only\nexposes hidden vulnerabilities in current LLMs but\nalso suggests a way for the creation of more re-\nsilient models that can adapt to the evolving chal-\nlenges of language tasks."}, {"title": "8 Limitations", "content": "While C-BOD serves as a promising framework\nfor detecting overfitting in LLMs and has success-\nfully identified overfitting in most evaluated mod-\nels, it remains subject to several limitations. First,\nour approach primarily targets textual rephrasings\nthat preserve semantic content. Consequently, it\nmay overlook deeper forms of overfitting, such\nas factual inaccuracies or logical inconsistencies,\nwhich may require more specialized probing tech-\nniques. Moreover, incorporating \u00b5-based transfor-\nmations into the training or fine-tuning loop can\nsignificantly increase computational cost. Itera-\ntively rephrasing large datasets and retraining with\nmultiple \u00b5 values imposes a heavy resource burden,\nwhich may not be feasible for LLMs or under re-\nstricted computational budgets. Future work should\ninvestigate more lightweight or partial-integration\nstrategies. In summary, while C-BOD provides\nan effective means of detecting surface-level over-\nfitting, further advancements are necessary to en-\nhance its efficiency, scalability, and ability to cap-\nture more nuanced forms of model overfitting."}]}