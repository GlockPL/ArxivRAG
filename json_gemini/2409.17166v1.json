{"title": "ScriptSmith: A Unified LLM Framework for Enhancing IT Operations via Automated Bash Script Generation, Assessment, and Refinement", "authors": ["Oishik Chatterjee", "Pooja Aggarwal", "Suranjana Samanta", "Ting Dai", "Prateeti Mohapatra", "Debanjana Kar", "Ruchi Mahindru", "Steve Barbieri", "Eugen Postea", "Brad Blancett", "Arthur De Magalhaes"], "abstract": "In the rapidly evolving landscape of site reliability engineering (SRE), the demand for efficient and effective solutions to manage and resolve issues in site and cloud applications is paramount. This paper presents an innovative approach to action automation using large language models (LLMs) for script generation, assessment, and refinement. By leveraging the capabilities of LLMs, we aim to significantly reduce the human effort involved in writing and debugging scripts, thereby enhancing the productivity of SRE teams. Our experiments focus on Bash scripts, a commonly used tool in SRE, and involve the CodeSift (Aggarwal et al. 2024) dataset of 100 tasks and the InterCode (Yang et al. 2023) dataset of 153 tasks. The results show that LLMs can automatically assess and refine scripts efficiently, reducing the need for script validation in an execution environment. Results demonstrate that the framework shows an overall improvement of 7 - 10% in script generation.", "sections": [{"title": "Introduction", "content": "Modern IT's growing complexity in multi-cloud environments creates challenges for SREs, as they strive to ensure systems operate efficiently. Organizations face the challenge of managing a growing number of incidents and outages across a diverse range of technologies and complex environments. Automation is essential to improve IT operations efficiency and reduce incident resolution time. A typical Incident Remediation pipeline (Figure 2) consists of (1) Root cause diagnosis which creates an incident report with probable root cause, (2) Action Recommendation that provides actionable recommendations, and (3) Action Automation where action recommendation outputs are transformed into scripts that can be executed to resolve the incidents.\nFrom our experience, we have seen that domain-specific scripting languages like Bash and PowerShell are commonly used in IT operations (ITOPs) for action tasks. Recent advances in Large Language Models (LLMs) have made it easier to turn natural language recommendations into script. This reduces the manual work of writing and debugging, boosting productivity for SREs."}, {"title": "Related Work", "content": "Benchmarking: Traditional coding benchmarks like NL2Bash (Lin et al. 2018), HumanEval (Chen et al. 2021), and MBPP (Austin et al. 2021) treat coding as a sequence transduction task, converting instructions directly into code without interactive execution. Recent efforts have expanded into interactive coding for Python, Bash, and other languages. Previous works (Huang et al. 2022; Lai et al. 2022; Yang et al. 2023) use Jupyter Notebooks and docker containers as an execution environment to support automatic execution-based evaluation.\nGeneration and Refinement: Recent work on code generation and refinement can be classified into three main approaches: (1) In-Context-Learning (Aky\u00fcrek et al. 2023; Min et al. 2022; Xie et al. 2022) enable models to adapt to new context data at deployment time without requiring traditional fine-tuning or parameter updates; (2) Chain-of-Thought (CoT) prompting (Wei et al. 2023; Kojima et al. 2023) enable models to perform multi-step reasoning using internal representations to perform tasks; (3) ReAct (Yao et al. 2023) prompts LLMs to generate reasoning traces and actions in an interleaved manner, enabling dynamic reasoning and plan adjustments (reason to act), while also interacting with external environments to refine reasoning (act to reason).\nSome of the works like (Chen et al. 2024; Madaan et al. 2023) have a feedback based framework for the task of code generation and refinement. However, they use unit test cases and execution accuracies for evaluation which makes it hard for adoption concerning Bash use cases.\nAssessment and Evaluation: Recent work on code evaluation can be classified into four main approaches: (1) Match-based: Metrics like BLEU and CrystalBLEU (Papineni et al. 2002; Eghbali and Pradel 2023) rely on n-gram matching to assess code similarity. (2) Embedding-based: Methods such as CodeBertScore (Zhou et al. 2023) measure similarity between generated and reference code by using token embeddings and contextual information. (3) Execution-based: These approaches evaluate code quality based on runtime correctness, using metrics like pass@k (Kulal et al. 2019; Chen et al. 2021). (4) Prompt-based: Methods utilize LLMs for pairwise comparison (selecting the better of two codes), single answer grading (scoring a single code), and reference-guided grading (using a reference code if available) (Zheng et al. 2023; Liu et al. 2023; Zhuo 2024). CodeSift (Aggarwal et al. 2024) uses a text-to-text approach, comparing the code's textual representation with the task description to evaluate correctness.\nTo summarize, the state-of-the-art methods discussed above have primarily been tested on datasets for languages like Python, Java, C, C++, and C#. However, these approaches cannot be directly applied to Bash scripts for the ITOps domain, as they rely heavily on execution-based accuracy or unit tests, which are challenging to obtain for Bash data with reliable accuracy. To address this gap, we propose the first end-to-end framework that automates both the generation and assessment of Bash scripts."}, {"title": "ScriptSmith", "content": "We describe the details of ScriptSmith for the automated generation, assessment and refinement of bash scripts. Our framework (Figure 1) aims to get the correct bash script for each of the recommended action steps. First, it tries to find a matching bash script from the catalogue of past actions. If none is found, it generates a new script dynamically. As the user validates the generated scripts for various kinds of recommended actions, they are added to the catalogue, for future reference."}, {"title": "Script Generation using LLMS", "content": "Scripts are generated using LLMs if a similar action statement is not been found in the catalogue. The steps of script generation are as follows:\n1. Initial Script Generation - We generate the script using a code-based LLM. A post-processing step is performed as the raw output of LLMs may have scripts enclosed within other texts. We extract scripts following predefined rules, such as capturing text enclosed in three backticks.\n2. Script Evaluation without Execution Bed - We use the evaluation framework proposed in CodeSift (Aggarwal et al. 2024) to ensure that the generated script aligns with the desired behavior specified by a given task. It involves three main steps: similarity analysis, difference analysis, and ensemble synthesis. The process starts by using syntax checkers to identify any syntactically incorrect script. Next, the framework generates the script functionality and begins the similarity and difference analysis between the generated functionality and the given task, by prompting on pre-trained LLMs. The final ensemble synthesis integrates the similarity and difference analysis results to determine the script's functional correctness comprehensively. If either analysis indicates a deviation from the task, the script is labeled as functionally incorrect.\nCodeSift is particularly helpful where it is difficult to write the unit test cases, certain prerequisites are required (eg. move filel from dirl to dir2 - filel, dirl and dir2 should be present) or there are no absolute answer of a script to match to (eg. free memory in the system).\n3. Script Refinement - If the evaluation step identifies the generated script to be incorrect, we refine the script based on model generated feedback. We first prompt LLMs to briefly explain why the script fails to perform the specified action. We then use this explanation as feedback to prompt LLMs to refine the generated script.\nHence, ScriptSmith automatically generates Bash scripts for a given action without human intervention or reliance on an execution environment, thereby enhancing the SRE experience by significantly improving the overall accuracy of script generation."}, {"title": "Results", "content": "In this section, we study the efficacy of ScriptSmith for the automated generation and refinement of Bash scripts using LLMs. Our experimentation primarily centres on the script generation and refinement processes. For script retrieval, we employ state-of-the-art methods, while acknowledging that the current catalog is limited and will expand over time as the deployed system continues to be utilized. \nPerformance of ScriptSmith\nWe evaluate the performance of ScriptSmith on two Bash datasets from CodeSift (Aggarwal et al. 2024) and InterCode (Yang et al. 2023). For the Bash dataset from CodeSift, which has 100 samples, we utilize Execution Accuracy (EA) using the testbed provided by CodeSift to determine the correctness of generated and refined script. For the Bash dataset from Intercode consisting of 153 samples, we ask the domain experts to evaluate the correctness of the generated and refined script. This is due to the unreliability of the execution environment provided by InterCode as discussed in the User Study section.\nWe compare the performance of script generation, assessment, and refinement across four models: Llama3_8B, Llama3_70B, Gemini1.5_Flash, and Gemini1.5_Pro. We primarily explore two different configurations of script generation/refinement and script assessment models: 1) Self-Reflection: Both script generation and script assessment models are the same. 2) Peer-Review: A smaller model is used to evaluate the script quality generated by a larger model. The motivation for peer review is that models are often biased when evaluating their own generated output. \nThe performance of script assessment also affects script refinement performance. For Llama3_70B model, we see that assessment with Llama3_8B model (peer-review) results in 3% and 10% improvement in script accuracy for CodeSift and Intercode dataset respectively compared to 0% and 7% when assessment is done with Llama3_70B model (self-refine).\nWe also compare the performance of open-source and closed-source models. As can be seen from Table 1, the closed-source Gemini1.5 model outperforms the open-source Llama3 model by 6% on the CodeSift-Bash dataset.\nHowever, cost of calling gemini1.5 models is much higher than Llama3 models (which can be run locally).\nFinally, we explore another configuration where we keep script generation and script assessment models as Llama_8B (smaller sized model) but change the script refinement model to Llama-70B. The motivation behind this configuration is that the number of calls to the LLM is much less in the script refinement phase as compared to the script generation and assessment phase as it is only applied to instances flagged as incorrect during assessment. \nTo summarize, we have the following takeaways from our experiments:\n\u2022 Accuracy of generated scripts increases using ScriptSmith framework for bash scripts in ITOPs domain. The increase is significantly bigger when initial script generation accuracy is less. However, if the initial accuracy is high, then refinement does not add significant value due to the saturation of model performance.\n\u2022 Peer-Review performs significantly better than Self-Refine since it does not suffer from biases.\n\u2022 Performance of open-source models with ScriptSmith (through automatic assessment and refinement) can match the performance of raw closed-source models for script generation."}, {"title": "Deployment", "content": "Figure 2 shows the intelligent remediation pipeline's complete software architecture, including the automation generation block (ScriptSmith) and its modules. The monitoring agents deployed in the user environment collect observability data through policies. These event metadata are pushed to IBM Instana from Apache Kafka. The Incident Processing Service combines the event metadata from different monitoring agents and creates an incident report for them. After the incident is stored in the ElasticSearch database, an alert is created by the Alert Service to notify SREs via Slack or Pagerduty. An SRE logs into the Instana UI to diagnose the issue with the help of the root cause diagnosis service and works to mitigate and remediate the incident. SRE first uses the Action Recommendation Service to create human-assisted steps. SRE then triggers the action automation framework to recommend bash scripts for a specified action that is divided into two primary steps:\n\u2022 The framework first attempts to retrieve a relevant script from its pre-existing knowledge catalog. This catalog is a repository of verified scripts that the model has previously encountered and the solution of which has been stored. We build an embedding database by converting all script descriptions in our catalogue into high-dimensional vectors using transformer models. These vectors are indexed for efficient similarity searches using approximate nearest-neighbor algorithms. When our tool receives a prescription text, it transforms it into a vector, retrieves the most similar vectors from the indexed database, maps these vectors back to their original script descriptions, and then returns the relevant scripts. If the model can retrieve a script with high confidence, this script is directly shown to the user. The confidence level is determined by the model's similarity measures and relevance scoring against the user's request.\n\u2022 For cases where the framework cannot retrieve a script from the knowledge catalogue, it generates a new script using LLMs. The framework incorporates an assessment step before presenting the code to the user. We use the approach presented in (Aggarwal et al. 2024) for evaluating the generated script without any execution environment. If the validation identifies the script as incorrect, the model is prompted to explain why the script is wrong given the incident. This explanation is then used to regenerate the script, to provide the user with the correct script for incident remediation.\nFinally, each recommended script (either retrieved or regenerated) is reviewed by a SRE for its correctness. Based on their domain knowledge, the SRE reviews the script, approves it, makes changes, or rejects the recommendation entirely. The final recommendation is then published in a Postgres database serving as our curated knowledge catalog, enriching the catalog with verified and improved scripts. This continuous feedback loop ensures that the knowledge catalogue evolves and improves over time, reducing the need for frequent script generation and enhancing the accuracy and relevance of script recommendations. The framework is designed to prioritize script quality and minimize noise in recommendations. By leveraging the dual approach of retrieval and generation, along with built-in validation and feedback mechanisms, the system ensures that users are presented with scripts that are both functional and relevant to SRE's needs. This method streamlines script generation and refines script quality through continuous learning and validation."}, {"title": "User Study", "content": "In our study, we involved four domain experts to evaluate the performance of the Bash script generation model. We asked the experts to label the initial script, the model-generated feedback (explanation for the error), and the refined script. Experts labeled instances on a scale of 0 (incorrect), 1 (partially correct), and 2 (correct). We used two criteria: strict (only 2 is correct) and partial (1 or 2 is correct). The goal was to compare the accuracy of the model's initial and refined output and assess the usefulness of the feedback provided to fixing the bugs. The experts provided feedback for 153 cases from the interCode Bash dataset (Yang et al. 2023). The script generation accuracy for the first pass was 42% using strict labeling criteria. The cases that were identified as incorrect (labeled as either 0 or 1) were refined using the model's feedback. This resulted in an overall accuracy of 76%.\nFrom the user study, we analyze the following four key aspects:\n\u2022 Expert Judgment vs. Execution Accuracy: Expert judgment shows an initial script generation accuracy of 42%, while EA reports 27%. We perform a detailed analysis to understand this discrepancy and identify three primary reasons:\n1. Different Interpretations: Expert evaluators and the execution-based system can interpret the input task differently, leading to varying assessments of script correctness. Row 1 in Table 2 illustrates how divergent interpretations resulted in different evaluations. The execution environment expects disk usage of files and folders in subdirectories where as human is satisfied with disk usage of file only in the given directory.\n2. Restricted Execution-Based Evaluation: EA's critiques are too stringent to be considered fair. In row 2 in Table 2, the script's additional text output alongside the IP address led to a misleading assessment, as the execution environment required only the IP address. Similar issues arise when the EA expects precise final answers, and accompanying text causes the script to be incorrectly labeled.\n3. Incorrect Expected Output: There were also cases where the expected output for the given task was incorrect. Row 3 in Table 2 has is an example of such a case where the exepected output is 1 (checking the number of processes) instead of boolean answer whether current shell is within a screen process.\nGiven these discrepancies, we decided to rely on expert judgment to analyze the other aspects of the study. This approach ensured a more accurate and consistent evaluation of the model's performance and the effectiveness of the ScriptSmith framework.\n\u2022 Expert Judgement vs. CodeSift Assessment: Next, we examine the alignment between automatic script assessment (CodeSift) and expert preferences, using two models for evaluation. CodeSift's assessment using the Llama3_8b model matched expert annotations in 61% of the 153 cases under strict labeling criteria and in 66% of the cases under partial labeling criteria. In comparison, with the Llama3_70b model, CodeSift showed a lower alignment with expert annotations, with 54% for strict labeling and 63% for partial labeling. These results suggest that the larger model, Llama3_70b, may exhibit self-bias, particularly in cases where it incorrectly labels script as correct as shown in row 3 in Table 2.\n\u2022 Usefulness of Model-Generated Feedback: We assess the effectiveness of model-generated feedback in two ways: (1) Human Support, i.e., computing the frequency of cases where experts found the feedback to be useful, and (2) Model Correction, i.e., computing the frequency of cases where the model used the feedback to correct the script. For this analysis, we applied strict labeling criteria. Among the 88 cases that experts labeled as incorrect during the first pass, they reviewed the reasons provided by the model for the script's incorrectness. In 69% of these cases (61 out of 88), experts found the feedback to be correct. Additionally, in 77% of the cases where the feedback was labeled as 2 (correct), the model was able to use this feedback to successfully correct the script.\n\u2022 Effectiveness of the Proposed Framework: We assess the usefulness of the proposed framework, specifically by considering the automatic script assessment using CodeSift with the LLama3_8b model. The scripts labeled as incorrect by the CodeSift model were then considered for refinement. Out of 153 cases, CodeSift correctly identified 38 incorrect cases, and out of these, 47% were successfully corrected using automatic feedback (considering strict correctness criteria) and 62% in case of partial correctness. However, we also encountered scenarios where initially correct scripts were wrongly assessed as incorrect by CodeSift. Observation: During refinement, asking the model to explain errors led to hallucinations, turning a correct script into an incorrect one. Row 5 in Table 2 illustrates such a scenario, where the model's unnecessary attempts to identify errors in a correct script led to incorrect final output. Recommendation: Add guardrails during prompting to prevent the model from self-doubt. Overall, we observed a 10% improvement in the accuracy of the script generation pipeline. The proposed framework uses automation to improve script accuracy, enhance the expert experience, and streamline workflow by reducing manual debugging and refinement time."}, {"title": "Conclusion and Future Work", "content": "In this paper, we introduce ScriptSmith, a reference and execution-free framework for generating Bash scripts. The framework effectively identifies faulty scripts and provides detailed reasoning for these inaccuracies, which in turn helps refine the scripts. Our findings demonstrate that automatically generated feedback improves debugging and helps experts quickly locate and fix issues in the script. The alignment between generated feedback and expert judgment further underscores the potential of this approach to improving script quality in automated settings. A key challenge is scaling the testing of generated scripts. This requires developing methods to automatically generate comprehensive test cases that cover a wide range of scenarios, ensuring more robust script validation. Additionally, executing these scripts within a controlled environment would offer more reliable assessments, minimizing discrepancies between execution-based evaluations and expert judgment. In the future, we aim to enhance the effectiveness and reliability of the proposed framework, making it a more valuable tool for automated script generation and refinement. We also plan to explore ScriptSmith to other scripting languages like Powershell."}]}