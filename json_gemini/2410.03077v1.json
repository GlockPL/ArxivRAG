{"title": "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "authors": ["Jun Rao", "Xuebo Liu", "Lian Lian", "Shengjun Cheng", "Yunjie Liao", "Min Zhang"], "abstract": "With instruction tuning, Large Language Models (LLMs) can enhance their ability to adhere to commands. Diverging from most works focusing on data mixing, our study concentrates on enhancing the model's capabilities from the perspective of data sampling during training. Drawing inspiration from the human learning process, where it is generally easier to master solutions to similar topics through focused practice on a single type of topic, we introduce a novel instruction tuning strategy termed CommonIT: Commonality-aware Instruction Tuning. Specifically, we cluster instruction datasets into distinct groups with three proposed metrics (TASK, EMBEDDING and LENGTH). We ensure each training mini-batch, or \"partition\", consists solely of data from a single group, which brings about both data randomness across mini-batches and intra-batch data similarity. Rigorous testing on LLaMa models demonstrates CommonIT's effectiveness in enhancing the instruction-following capabilities of LLMs through IT datasets (FLAN, CoT, and Alpaca) and models (LLaMa2-7B, Qwen2-7B, LLaMa 13B, and BLOOM 7B). CommonIT consistently boosts an average improvement of 2.1% on the general domain (i.e., the average score of Knowledge, Reasoning, Multilinguality and Coding) with the LENGTH metric, and 5.2% on the special domain (i.e., GSM, Open-functions and Code) with the TASK metric, and 3.8% on the specific tasks (i.e., MMLU) with the EMBEDDING metric.", "sections": [{"title": "1 Introduction", "content": "The emergence of ChatGPT (OpenAI, 2023) and a range of large language models (LLMs) (Touvron et al., 2023; Muennighoff et al., 2023) brings light to artificial general intelligence (AGI). As the size of the model increases, instruction fine-tuning becomes necessary to align human intentions with machine understanding of language. Compared to traditional fine-tuning, instruction tuning (IT) represents different tasks through instructions followed by some task-specific inputs (Iyer et al., 2022). Current researchers have obtained good results by passing only the IT stage (Zhang et al., 2024) on datasets generated by multiple construction methods (Taori et al., 2023; Zheng et al., 2023; Xu et al., 2023; Fan et al., 2024). Typically after this stage of training, LLMs show a strong multi-task capability and the models can perform multifaceted tasks such as summarization, conversation, writing and other basic tasks.\nExisting research has already underscored the significance of data mixing in fine-tuning large models for achieving a generalized model. For example, Chung et al. (2022) pointed out that the key for LLMs in the IT phase is to enhance the understanding of instructions and they increase the diversity of instructions, e.g., by using more instructions to describe the same task (Longpre et al., 2023). Despite this, these LLMs are susceptible to bias in the model's understanding of a particular task (Kim et al., 2023) due to the mixing of multiple tasks or diverse instructions, which can lead to a decline in the average ability across multiple tasks (Scialom et al., 2022; Iyer et al., 2022; Wang et al., 2023). As illustrated in Figure 1, this situation results in a diminished ability of the model to follow instructions accurately. Specifically, it struggles to comprehend the requirements laid out in the instructions, leading to incorrect responses.\nTo enhance the model's ability to follow instructions, we introduce CommonIT, a methodology that enhances model comprehension of data features by engaging distinct data classes in individual gradient updates and interchanging data classes across batches. Central to our approach is the principle of data commonality (Cui et al., 2022), inspired by humans' learning process when preparing for exams. This means focusing preparation on one exam subject at a time rather than attempting to study for multiple subjects simultaneously. Specifically, we advocate a two-phase fine-tuning process for LLMs. Initially, raw data are segmented into several groups and partitioned into designated sizes for training. This strategy enhances the model's ability to follow instructions, as evidenced by comprehensive evaluations across four dimensions of capacity testing.\nOur experiments demonstrate its applicability across diverse IT datasets and potential for future applications in different models and domain-specific tasks. Our findings indicate that length serves as the most effective criterion for grouping within the generic domain. Furthermore, embedding divisions should be tailored specifically for each task, and task-related information is crucial for optimizing fine-tuning quality in specific domains. Our analysis of commonalities, from the perspectives of data quality, input question sentence representation and multi-task generalizability, delves into the reasons for improvements, demonstrating that the common learning strategy indeed enhances the effectiveness of representations and the capabilities across various sub-tasks.\nOur contributions are as follows:\n\u2022 We propose the CommonIT framework, which leverages commonalities to enhance models' capabilities in following instructions. This framework includes three strategies for categorizing groups within IT datasets and incorporates a batch-based constraint policy for optimization. (\u00a73).\n\u2022 CommonIT demonstrates broad applicability across multiple dimensions, including various datasets, general and specialized domains, and diverse models. Additionally, we have explored scenarios to determine the most suitable group strategy for each context (\u00a75.1).\n\u2022 Our exploration of commonalities provides a possible explanation for the sources of improvements (\u00a75.2 and \u00a75.3)."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Data-centric AI", "content": "There are current discussions on the curriculum (Bengio et al., 2009; Platanios et al., 2019; Feng et al., 2023; Lee et al., 2023), data mixing (Wang et al., 2023; Xu et al., 2023), and data filtering (Zhou et al., 2023; Chen et al., 2024; Xie et al., 2023) during the training of large language models. LIMA (Zhou et al., 2023) finds better performance could be achieved with just a few percent of samples from these datasets if selected properly with the very large models (65B). When smaller scale models are used (7B), the amount of data in SFT is usually related to the model size. Alpaca (Taori et al., 2023) demonstrates that models at the 7B level, fine-tuned with a small amount of data (less than 100K), can exhibit strong alignment capabilities. Further, AlpaGasus (Chen et al., 2024) found that models at the 7B level can also achieve strong alignment capabilities with fewer data. We have initiated early explorations in the data sampling strategy and find that the training scheme possibly has a major impact on the model's final performance under the small training epochs."}, {"title": "2.2 Instruction Tuning", "content": "Many works (Taori et al., 2023; Peng et al., 2023; Xu et al., 2023; Chen et al., 2023; Wang et al., 2023) employ distilled datasets and curate various data for fine-tuning language models, achieving enhanced performance. Early works like FLAN 2021 (Wei et al., 2022a) and Super-Natural Instructions (Wang et al., 2022) are to convert traditional NLP tasks into instruction format through manually defined instruction templates. FLAN-CoT (Wei et al., 2022b) and FLAN 2022 (Chung et al., 2022) employ Chain-of-Thought training prompts to strengthen the reasoning of the model. Shen et al. (2023) state that instruction finetuning can be considered a \"continual finetuning stage\". Su et al. (2023) demonstrate how various text encodings represent different tasks, enabling a single model to accomplish multiple downstream tasks and thereby achieving a more generalized model."}, {"title": "3 Our Proposed CommonIT Method", "content": "Motivation Data imbalance can lead to inconsistent performance across tasks, with some exhibiting exceptional results and others underperforming significantly (Shi et al., 2023). This phenomenon is typically evidenced by instruction misinterpretation, as illustrated in Figure 1, where the model fails to comprehend the user's intention accurately.\nOur approach, CommonIT, aims to balance task-specific sampling for gradient updates with diverse data grouping. This strategy draws upon the human learning process of understanding through analogy, leveraging similarities across different contexts to facilitate comprehension. Through this approach, the model gains an enhanced ability to differentiate among the instructions of various tasks. Consequently, it can more accurately respond to distinct task directives, thereby minimizing the misinterpretation of instructions and reducing the generation of irrelevant content (\u00a75).\nOverview As shown in Figure 2, CommonIT is divided into two steps. First, we need to perform a clustering operation to categorize the dataset into multiple categories. It is worth noting that the clustering here does not require precise categorization but only focuses on a certain aspect of the dataset division. In the figure, the square, circle, and triangle indicate three kinds of data with different attributes. At the same time, the different colors indicate that the data can also be divided by the attribute of color, which is not divided here just due to space constraints. The second step is to construct partitions of the divided data by batch size and randomly take the divided partitions as one batch when the data are fed into the model."}, {"title": "3.1 Background", "content": "After large-scale pre-training, instruction tuning is the next phase of LLMs to enable the model to understand and interpret instructions for human language preferences (Zhang et al., 2022; Muennighoff et al., 2023; Touvron et al., 2023).\nFor a training data source $\\mathcal{D} = \\{x^n, y^n\\}$, the standard instruction tuning of the language model"}, {"title": "3.2 Group the Dataset (GD)", "content": "The primary objective in organizing a dataset is to segregate distinct data categories, typically achieved by partitioning the data into tasks. Formally, we aim to decompose a dataset $\\mathcal{D}$ into distinct sub-datasets $\\mathcal{D}_0, \\mathcal{D}_1, ..., \\mathcal{D}_n$. If task-based partitioning proves infeasible, data clustering methods can be employed for learning. Drawing an analogy with human learning, individuals often find it easier to grasp concepts when exposed to related topics concurrently, leveraging inherent similarities. Conversely, disparate topics can pose learning challenges. We subsequently detail three potential group strategies:\n\u2022 Group by Task The task-specific information facilitates the model's ability to differentiate between various task instructions, enhancing its generalization capacity across different instructional contexts. For the dataset that transforms traditional NLP tasks by designing instructions, we can obtain the task type of the original data. With this type, we can divide the dataset to get different categories.\n\u2022 Group by Embedding This approach is an alternative to embedding clustering, typically used when specific task categories cannot be readily identified. Automatic division can significantly enhance the effectiveness within specific domains for particular tasks. We use the category with the highest number of corresponding categories among the k retrieved pieces of data as the category of the training set data. For simplicity, we use the category of MMLU (development set corresponds to a category of 57 exams) to categorize the training data. We first use a certain sentence encoder to convert sources in both the training set and development set to vector representations.\u00b9 Then, for each training source s, we retrieve its nearest k neighbors $s_1, s_2, ..., s_k$ from the MMLU development set (according to the distances in the sentence encoder's embedding space).\u00b2 Given some predefined similarity measure d, such as the cosine similarity, the neighbors are ordered in such a way that $d(s_i, s) \\leq d(s_j, s)$ when i < j.\n\u2022 Group by Statistics Due to the nature of IT data, responses of the same length typically belong to the same data category, such as multiple-choice, question-answering, or translation tasks. This inherently includes task similarity within the data. The length can be the simplest metric for classifying clusters when the task information is missing. We count the length distribution of each IT dataset such that the number of samples in the sub-datasets after each IT dataset is divided remains the same for each length interval."}, {"title": "3.3 Fine-tune with Shuffled Partitions (FP)", "content": "In conventional model training, the batch data is usually sampled randomly from the entire data source $\\mathcal{D}$. The training of language model employs mini-batch gradient descent rather than batch gradient descent or stochastic gradient descent.\nCommonIT supposes that the mini-batches are bucketed in a particular way and upgrade these samples. In our proposed CommonIT, we can perform the batch construction as follows since we obtain the \"class division\" of the data. We ensure instances within each batch come from the same group while the order of sampled groups is random. Take Figure 2 for example, the first batch $\\mathcal{B}_1$ comes from a group (a), and the tth batch $\\mathcal{B}_t$ may be composed of samples of the only group (a) or (b), or (c). So the batch loss can be calculated as:\n$\\mathcal{L}_t(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} log\\,P(\\mathcal{Y}^{s_t(i)} | \\mathcal{X}^{s_t(i)}; \\mathcal{s}_t^{s_t(i)}; \\theta)$ (2)\nwhere s, x and y represent the instruction, the input, and the target, respectively. The $\\mathcal{L}_t(\\theta)$ is the training loss of a mini-batch that has N examples at the t-th training step."}, {"title": "4 Evaluation Setup", "content": ""}, {"title": "4.1 Evaluations", "content": "Factual knowledge, reasoning, multilinguality, and coding are foundational to LLMs, as they encapsulate the core capabilities required to comprehend, analyze, and generate human-like text. In the evaluations, we examine the model's performance in these areas, employing four out-domain benchmarks to assess its ability to assimilate knowledge (MMLU (Hendrycks et al., 2021)) and execute complex reasoning (BBH (Suzgun et al., 2023)), multilinguality (TydiQA (Clark et al., 2020)), and coding (Codex-Eval (Chen et al., 2021b)) tasks.\n\u2022 Factual Knowledge represents a critical dimension of capability in LLMs, essentially reflecting their memory capacity. We employ the Massive Multitask Language Understanding dataset (MMLU (Hendrycks et al., 2021)) as a benchmark to measure the model's factual knowledge.\n\u2022 Reasoning is another crucial capability for large models, particularly in solving complex problems. We utilize the Big-Bench-Hard dataset (BBH (Suzgun et al., 2023)), comprising 23 intricate tasks, to assess the model's general reasoning capabilities.\n\u2022 Multilinguality is essential for enabling large models to serve speakers of various languages. We employ the TyDiQA (Clark et al., 2020) dataset, a multilingual testing dataset encompassing 11 different language types.\n\u2022 Coding is another important ability that people need large language models. We use the HumanEval dataset (Chen et al., 2021b) (we refer to it as Codex-Eval) to evaluate the models' capability to generate functionally correct programs from docstrings."}, {"title": "4.2 Training Datasets", "content": "We have selected several representative IT datasets from distinct sources, encompassing task-constructed data and model-generated high-quality data. Our diverse selection facilitates a thorough evaluation, enabling us to gauge performance across varied data types and characteristics. The selections include: FLAN V2 (Chung et al., 2022) A collection of NLP tasks that combines several existing datasets with various data augmentations. FLAN CoT (Wei et al., 2022b) A collection of datasets annotated with chain-of-thoughts. We use the CoT mixture from the FLAN v2 collection (Chung et al., 2022), splitting it out as a separate dataset. Alpaca GPT4 (Peng et al., 2023) A dataset was created using the Alpaca dataset as inputs, replacing the example generations with generations from GPT-4. Detailed information regarding the datasets and training settings can be found in Appendix A."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Main Results", "content": "CommonIT Improves Instruction Following In Several Dimensions Table 1 illustrates the comparative results of the IT on LLaMa 7B of general IT datasets. For LLaMa, the conventional IT approach (IT) has enhanced the model's performance across three capability dimensions, with an average improvement of 10 points in MMLU, 4 points in BBH, and 5 points in TydiQA. Employing our training strategy (CommonIT), we improved further on an already strong and influential baseline (Wang et al., 2023). This indicates CommonIT's effectiveness with varied data distributions.\nLENGTH Metric Works Best in General Domain Table 1 also presents the results of different grouping methods by combining different grouping strategies with three datasets. Since only the FLAN dataset contains partial task category information, we conducted ablation experiments on this dataset using three grouping methods. Due to the lack of original task divisions, we could only perform embedding and length ablation experiments in these two datasets. Various grouping strategies improve the baseline results across multiple capability dimensions. These strategies yield varying degrees of enhancement in different capability areas. Employing embedding similarity as a group method also resulted in performance gains across these datasets. As a simple grouping criterion, length has already achieved notable improvements. This significant gain demonstrates the importance of leveraging specific training approaches tailored to the unique properties of the data, ultimately leading to enhanced model performance on the IT datasets.\nEMBEDDING Metric Works Best Based on the Specific Tasks Given that our experiments on embedding similarity are based on the division of MMLU's 57 categories, we further present detailed results for each task category in MMLU, as illustrated in Table 2. The validation of exam questions across diverse subjects mirrors how humans prepare for exams in various disciplines. Integrating our CommonIT method shows the model substantially enhances various disciplinary tasks relative to the baseline approach (IT). This indicates that training on similar questions grouped within a single batch using embedding can enhance the effectiveness of individual sub-tasks, thereby augmenting the overall capabilities. This observation further suggests that strategically designing the division of embeddings for specific tasks can enhance performance."}, {"title": "5.2 Ablation Study", "content": "The success of CommonIT hinges on the initial grouping in Stage One (GD) and the subsequent assurance in Stage Two (FP) that data within a single batch exclusively originates from a single group. We conducted ablation studies on these two components and have presented the corresponding results (w/o GD and w/o FP) in Table 5. Previous main results have demonstrated that the length metric is most effective in general domain IT, and we have adopted this metric as the foundational basis for CommonIT. It is observable that in the absence of GD (baseline IT), all evaluation results experience a significant decline, underscoring the critical importance of grouping for the training process. Furthermore, the omission of FP led to a significant drop in most results."}, {"title": "5.3 Analysis", "content": "Learning from High&Low-Quality Instructions\nWe compare one existing state-of-the-art method: Alpagasus: GPT 3.5 selected the highest scoring 9,000 samples. We show the average results of multitasking with a uniform setup in Table 6, showing that CommonIT gets the best score (37.1). To further ensure a fair comparison, we supplemented our evaluation with GPT assessments and results from Alpaca-Eval (Dubois et al., 2023). CommonIT achieves better results in dialogue scenarios gained close to a 10-point boost, indicating that our method obtains more reliable and higher-quality replies. Alpagasus (Chen et al., 2024) belongs to data filtering approaches, whereas ours is a data learning approach. They focus on learning from a subset of high-quality data, while CommonIT enables the model to learn from good and bad data. The results show that CommonIT demonstrates a superior understanding and learning of High&Low-quality instructions.\nQuestion Representation Capacity In Figure 3, comparative results for T-SNE (Van der Maaten and Hinton, 2008) from the question embedding for 10 randomized categories in MMLU are visualized. We compute their average pairwise distance within the embedding space. A reduced distance indicates a superior model's ability to aggregate similar task-related questions. As the reduced average distance indicates, our CommonIT aggregates similar questions more effectively than the LLaMa IT and LLaMa. It can also be seen that after instruction tuning, the model can discriminate between different task instructions.\nCorrelation Between Two Strategies in Clustering Results Table 7 illustrates the relationship between the dataset divided by length and the embedding metrics. For datasets without clustering (original FLAN, FLAN CoT, and Alpaca), we sampled a total of 500 samples, referred to as \u201cVanilla.\u201d For the multiple sub-datasets clustered by length, we sampled 500 samples from each sub-dataset and calculated the average results, denoted as \u201cLength.\u201d We categorized these sampled instances according to the embedding metric and reported the average results of the total number of embedding categories after conducting ten runs. The findings indicate that the number of embedding categories decreases with length clustering, suggesting that data of the same length exhibits more similar embedding representations.\nModel Generalization Figure 4 shows the loss curves for the training of the model (left) and the results of the MMLU 0-shot evaluation at different training epochs (right). As can be seen from the loss curves, compared to the baseline method, CommonIT exhibits a smaller loss, indicating better generalizability of our approach. We compared the out-of-domain test of MMLU results across different training epochs, demonstrating that our model improved further with longer training. This observation indicates that the CommonIT-trained model remains underfitting compared to the baseline at the same number of steps. Unlike our method, the baseline model showed a little decrease with more training epochs, hinting that IT might limit the model's generalization."}, {"title": "6 Conclusion", "content": "We present a simple and effective fine-tuning method CommonIT. Leveraging data commonality with three metrics significantly enhanced the effectiveness of LLMs across multiple competency dimensions during the IT. The evaluation across diverse models, IT datasets, and specific tasks has showcased the methodology's application scalability. Ablation experiments across various stages and data clustering techniques have illustrated the effectiveness of our method. Explorations of commonalities confirm that CommonIT mirrors the human learning process, improving overall performance."}, {"title": "Limitations", "content": "There are several limitations of our work. Our limitations are primarily constrained by experimental resources, such as available GPU memory capacity (4*80G). Due to these constraints, we cannot test more models at different scales (30B-65B). The group approach we used was not further selected, such as embedding, which was only analyzed for MMLU and may lead to further room for improvement in the final results. We tried some intuitive methods of data categorization, but more complex ones were not considered. The theory behind the improvements remains to be revealed. Apart from empirical explanations, we believe further investigations (e.g., mathematically provable bound) will be useful. Given the balance between the number of groups and batch size, when the overall training data is fixed, having more groups means less data within each group. If the batch size is too large, some groups may have fewer data points than the batch size, which may lead to some performance degradation."}, {"title": "Ethics Statement", "content": "Our work follows the ACL Ethics Policy. Our findings are based on publicly available datasets for reproducibility purposes. LLMs can contain potential racial and gender bias. Therefore, if someone finds our work interesting and would like to use it in a specific environment, we strongly suggest the user check the potential bias before usage. In addition, it is hard to control the generation of LLMs. We should be aware of the potential problems caused by hallucinations."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Benchmarks", "content": "\u2022 MMLU MMLU consists of a series of questions, ranging from basic to professional levels, across 57 academic subjects. Its multiple-choice format facilitates a relatively straightforward testing process. We use the official MMLU evaluation script and prompts\u00b3, with modifications to allow for batch processing. We evaluate using 0 and 5 few-shot examples, following the original setup of MMLU.\n\u2022 BBH We follow the setup described in the original paper (Suzgun et al., 2023), and evaluate with and without chain-of-thought (CoT vs Direct). The officially provided prompts, with three few-shot in-context examples, are used for both CoT and Direct setups. For the CoT setup, we extract the first word after the phrase 'So the answer is,' or the entire response if no such substring is present.\n\u2022 TydiQA We adhere to the protocol delineated in the PaLM 2 technical report (Anil et al., 2023). This approach assesses the models' proficiency in responding to multilingual queries, particularly when the definitive gold passage containing the answer is provided (referred to as GoldP/GP). One in-context example is incorporated to acclimate the model to the expected answering format. Here we follow previous work (Sun et al., 2024; \u00dcst\u00fcn et al., 2024) and report the GP score.\n\u2022 Codex-Eval For evaluating the coding capabilities of the models, we employ the HumanEval dataset presented in the Codex paper (Chen et al., 2021a). This dataset encompasses 164 programming challenges, wherein models are prompted to finalize a Python function based on its provided docstring. Following the original paper, we calculate the pass@k to gauge the functional accuracy of the models' outputs. Our findings are presented as pass@10 results, employing a temperature setting of 0.8.\n\u2022 Alpaca Eval We use the code provided by Dubois et al. (2023). We adopt Davinci-003 reference text generated by Wang et al. (2023). We greedily decode up to 1024 tokens and then pairwise compare these responses with those from Davinci-003. The reported win-rate is the percentage of model generations that ChatGPT reports as being preferred over the generations from Davinci-003.\n\u2022 GSM The mathematical reasoning capabilities are enhanced through the use of the GSM8K dataset, which consists of 8.5K high-quality arithmetic word problems designed for the grade school level. The samples are divided into 7.5K training and 1K test problems.\n\u2022 Openfunctions Proficiency in using the tool is assessed by leveraging function-calling datasets, including the Gorilla Openfunctions dataset. The training set contains 2211 samples, while the test set contains 112.\n\u2022 Code The code generation skills are boosted using MagiCoder (Wei et al., 2023) of 2,000 samples. The test set includes 164 samples, and the evaluation is conducted using the HumanEval dataset (Chen et al., 2021b)."}, {"title": "A.2 Overall Learning Strategy", "content": "Algorithm 1 illustrates the overall training flow of CommonIT. Besides the component and training flow of LLMs, only some low-cost operations, such as grouping, have been included in the pre-process to get many sub-datasets (Do...Dn), allowing an easy implementation as a practical language model."}, {"title": "A.3 Training Details", "content": "We set learning rate to 5 \u00d7 10\u20136 and set batch size to 32, with no weight decay and a learning rate with linear decay and linear warmup for 3% of the total training steps. We use a maximum sequence length of 2048, truncating samples where necessary. During training, we make use of the DeepSpeed library (Rasley et al., 2020) and ZeRO (Rajbhandari et al., 2020) optimizer to allow for large-scale model finetuning. For FLAN and CoT, we train models for two epochs. For Alpaca, we train models for three epochs. For retrieval settings, we take the categories (57 task categories) classified by MMLU as the categories to be classified, use the data in the development set as the database, and classify the data categories in the dataset to the ones that are the most similar to the data in the database by doing a similarity ordering with the data in the"}, {"title": "A.4 Details of Group Methods", "content": "Regarding task division, we use the task types provided by the original FLAN (Chung et al., 2022), such as translation tasks, QA tasks, etc., for practical data categorization. The statistical distribution of the divided dataset is shown in Figure 5\nFor embedding division, we employ the top-k algorithm from (Liu et al., 2021) to select the top 8 results based on similarity. We then classify the entry with the highest number of votes in the top 8 as the final retrieved item, with other parameter settings as described in (Liu et al., 2021). In the simplest scenario, where k=1, we categorize each query data point by assigning it to the category of its most closely related data point. For example, if the label of the nearest data point is \u201canatomy\u201d, then the query data is classified under the \u201canatomy\u201d category. This process is applied uniformly across all the data points (queries) that need categorization, effectively segregating the dataset into 57 distinct sub-datasets."}, {"title": "A.5 More Results", "content": "Due to space constraints and the consistency of the results, we did not put the results of 5-shot and CoT to save space. We add these results to the appendix.\nThe results are as Table 11 and Table 12."}, {"title": "A.6 Further Analysis", "content": "Effect of Batch Size we added experiments related to larger batch sizes in Table 8 and 9. Because hyperparameter search is time-consuming, we adopted the same learning rate and conducted experiments with batch sizes of 64 and 128. These experiments were conducted using the Alpaca dataset. The results show that CommonIT is still better than IT in this setting.\nEffect of Data Mixing In the Table 10, we have supplemented the experiments with a mix of FLAN and Alpaca. The results indicate that our method remains effective (+2.2).\nEffect of Inference Seeds In Generation The test results with different random seeds in inference are shown in Table 13 and 14. Our findings indicate that the results exhibited minimal variability, ultimately showing that the fluctuations were insignificant when averaged across multiple tasks. The fluctuations in TydiQA's results are due to the occasional occurrence of models not answering in the corresponding language after training (both IT baselines and our CommonIT). This fluctuation is minor in our method and more prominent in"}, {"title": "A.7 Case Study", "content": "Figure 12 shows some typical output examples. We offer three tasks: factual Q&A, summarization, and grammar correction. Overall, our approach obtains higher quality response results and a correct understanding of the instructions. However,"}]}