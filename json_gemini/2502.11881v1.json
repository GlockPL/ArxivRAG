{"title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models", "authors": ["Hyunwoo Kim", "Melanie Sclar", "Tan Zhi-Xuan", "Lance Ying", "Sydney Levine", "Yang Liu", "Joshua B. Tenenbaum", "Yejin Choi"], "abstract": "Existing LLM reasoning methods have shown impressive capabilities across\nvarious tasks, such as solving math and coding problems. However,\napplying these methods to scenarios without ground-truth answers or\nrule-based verification methods\u2014such as tracking the mental states of an\nagent-remains challenging. Inspired by the sequential Monte Carlo algo-\nrithm, we introduce thought-tracing, an inference-time reasoning algorithm\ndesigned to trace the mental states of specific agents by generating hy-\npotheses and weighting them based on observations without relying on\nground-truth solutions to questions in datasets. Our algorithm is modeled\nafter the Bayesian theory-of-mind framework, using LLMs to approximate\nprobabilistic inference over agents' evolving mental states based on their\nperceptions and actions. We evaluate thought-tracing on diverse theory-of-\nmind benchmarks, demonstrating significant performance improvements\ncompared to baseline LLMs. Our experiments also reveal interesting behav-\niors of the recent reasoning models \u2013 e.g., o1 and R1 \u2013 on theory-of-mind,\nhighlighting the difference of social reasoning compared to other domains.", "sections": [{"title": "Introduction", "content": "New reasoning algorithms and training paradigms for large language models (LLMs) have\nrecently achieved remarkable breakthroughs in domains where well-defined ground-truth\nanswers are readily available, enabling partial-solution evaluation and objective verification \u2013\ne.g., mathematics, coding, and puzzles (Yao et al., 2023; Besta et al., 2024; OpenAI et al., 2024b;\nDeepSeek-AI et al., 2025b). This stands in stark contrast to social reasoning, where objective\nanswers are not easily obtainable, making partial-solution evaluation less straightforward.\nFurthermore, the information-asymmetric nature of theory-of-mind (ToM) \u2013 which requires\ninferring hidden mental states \u2013 makes social reasoning more challenging due to its increased\nuncertainty (Zhi-Xuan et al., 2024b; Nafar et al., 2024).\nExisting AI research on theory-of-mind has focused on either natural language benchmarks\nand tailored approaches to solve them (Sclar et al., 2023; Kim et al., 2023; Wilf et al., 2024;\nJung et al., 2024; Jin et al., 2024), or on mental state inference in non-linguistic settings\n(Baker et al., 2009; Doshi & Gmytrasiewicz, 2009; Zhi-Xuan et al., 2020; Alon et al., 2023). As\nLLM-powered AI agents increasingly interact with humans (Collins et al., 2024), the ability\nto track and infer others' mental states from open-ended textual input will become crucial\nfor broader applications and data synthesis in the social interaction domain.\nTo this end, we propose thought-tracing, an inference-time reasoning algorithm for LLMs\nthat can infer and track the mental states of target agents. Our method is modeled after the\nBayesian theory-of-mind framework (BToM; Baker et al., 2017), viewing the input context as\na sequence of states and agent actions, and treating ToM reasoning as probabilistic inference\nabout the agent's mental states based on their perceptions and actions. To account for the\ninherent uncertainty in this task, we follow the high-level structure of sequential Monte\nCarlo inference (SMC; Del Moral et al., 2006; Lew et al., 2023b), tracking multiple weighted"}, {"title": "Thought-tracing", "content": "We introduce thought-tracing, an inference-time algorithm that performs ToM reasoning by\ninferring and propagating weighted hypotheses about agents' thoughts. These hypotheses\nare represented in natural language, and are generated and weighted using LLMs. Thought-"}, {"title": "Background", "content": "Bayesian Theory of Mind (Baker et al., 2017) frames mental state attribution as probabilistic\ninference over a generative model of a rational agent. It focuses on several important roles\nthat beliefs and goals play in a theory-of-mind: the agent's perceptions and their prior\nbeliefs jointly effect the current belief, and beliefs and goals are the causes of the agent's\nactions. As such, beliefs and goals can be inferred in various ways: forward simulation\nof beliefs from the agent's perceptions and prior beliefs, backward inference of from the\nagent's observed actions, or through a joint integration of all available information.\nThought-tracing follows the structure of this framework without using explicit models of\nrational belief updating and action selection. Instead, we use LLMs to simulate how an agent\nis likely to update their mental states in response to what they perceive, and to evaluate the\nlikelihood of an agent's actions given their mental states. This enables greater generality,\nand decomposes social inference into the simpler tasks of forward simulation and likelihood\nevaluation by LLMs.\nSequential Monte Carlo (SMC) refers to a family of algorithms designed for incremental\ninference over sequences of posterior distributions (Del Moral et al., 2006), such as posterior\ninference over latent dynamics from time series data. SMC uses a collection of weighted\nhypotheses (called particles) to approximate each distribution in the sequence. Given\nparticles for step $t \u2013 1$ in the sequence, SMC generates particles for step $t$ via propagation\n(extending each previous particle with new latent states that exist at step $t$) and reweighting\n(weighting each particle by the likelihood of the observed data under that particle's latent\nstates). The particles are then resampled according to their weights (focusing samples on\nregions of high posterior probability) and optionally rejuvenated with Markov chain Monte\nCarlo (MCMC) to increase particle diversity (Chopin, 2002). To improve inference quality,\nSMC can make use of custom proposals at for propagation or rejuvenation (Lew et al., 2023a),\nusing data-driven cues to generate more plausible hypotheses (Perov et al., 2015)."}, {"title": "Algorithm", "content": "Algorithm 1 shows the pseudo-code for\nthought-tracing. Specifically, it leverages\nsequential Monte Carlo principles to main-\ntain a dynamically updated set of natu-\nral language hypotheses about the target\nagent's mental state, weighting them in\nresponse to new observations. These hy-\npotheses are updated based on the agent's\nperception, prior beliefs, and actions.\nPreprocessing Given a text input C, we\nparse it into a trajectory (i.e., sequence of\nstate-action pairs) of the target agent A.\nSpecifically, we use the LLM for action la-\nbeling of each sentence of the input. We\nclassify each sentence whether it includes\nactions (e.g., physical movements or utter-\nances) from A. If the sentence does not"}, {"title": "Experiments", "content": "Our thought-tracing algorithm offers a general method for tracing the mental states of a target\nagent, given context. While the algorithm is not explicitly designed to solve benchmark\nquestions, we evaluate its validity by testing whether it can improve existing models'\nperformance on theory-of-mind benchmarks. We apply thought-tracing to off-the-shelf LLMs\nand feed the generated thought traces along with the original context and compare their\nperformance against other baselines, such as reasoning models.\nBenchmarks & Metrics We test our approach on four theory-of-mind benchmarks that\nspan diverse format and question types:\n(1) ParaphrasedToMi (Sclar et al., 2023) is a revised version of the classic theory-of-mind\nbenchmark ToMi (Le et al., 2019) that addresses the limited linguistic diversity of the\noriginal ToMi by rewording all ToMi templates using GPT-3 with additional manual filtering.\nThe resulting dataset is considerably more complex, as actions are expressed in a less\nstraightforward way. We use a total of 600 questions and report three metrics: accuracy of\n(i) all questions, (ii) false belief questions, and (iii) true belief questions.\n(2) BigToM (Gandhi et al., 2023) is a benchmark featuring synthetic narratives centered on\na person's desires, actions, and beliefs, along with an event that changes the state of the\nenvironment. It includes questions about the person's beliefs and actions. We find some of\nthe narratives and ground-truth answers in BigToM include incoherent stories and errors.\nTo address this issue, we randomly sampled 600 scenarios from the dataset and re-annotated\nby crowdsourcing 510 online participants (See details in Appendix A). We only retained\nsamples with an agreement rate higher than 90%, which resulted in 249 samples. We report\nthe average accuracy of the questions.\n(3) FANTOM (Kim et al., 2023) is a multi-party conversation question-answering dataset\ndesigned to test coherent theory-of-mind capabilities. In FANTOM, the speakers join and\nleave the conversation while it continues, making the participants hold both false and true\nbeliefs. The benchmark includes first-order and second-order theory-of-mind questions\nabout the beliefs of conversation participants in various formats, such as multiple-choice\nand list type questions. We use 64 sampled conversations from the short version of FANTOM,\ncontaining a total of 1,086 questions. We report three metrics (i) All Qs, (ii) Answerability\nAll Qs, and (iii) Info-Accessibility All Qs, each requiring the model to correctly answer all\nquestions for the corresponding question types for a given conversation snippet.\n(4) MMTOM-QA (Jin et al., 2024) is a multi-modal question-answering benchmark that\nincludes questions covering the beliefs and goals of a person searching for an object (e.g., a\nremote controller). We use 200 questions in its text-only subset, which describes a person's"}, {"title": "Overall Results", "content": "Thought-tracing (+TT) improves all baseline models across all benchmarks. Table 2\nshows the results on all four benchmarks. Although thought-tracing does not take benchmark\nquestions as input, its output significantly improves the model performance on the main\nmetrics \u2013 the average accuracy (Avg.) on Paraphrased ToMi, BigToM, MMTOM-QA, and the\nAllQs score on FANTOM. For example, Llama 3.3 70B + TT, Gemini 1.5 Pro + TT, and Qwen\n2.5 72B + TT significantly outperforms GPT-40 + CoT on Paraphrased ToMi. These results\nindicate that the traced mental states from thought-tracing provide accurate intermediate\nreasoning steps that helps models correctly answer mental state related questions.\nThought-tracing guides better reasoning trajectories. The fact that thought-tracing outper-\nforms CoT indicates the traced thoughts exhibit better reasoning traces. Moreover, when\nCoT is applied on top of the traced thoughts, models' performance improves further in many\ncases. For example, Llama 3.3 70B and Qwen 2.5 72B achieve the first and second best scores"}, {"title": "Comparison with Reasoning Models", "content": "Thought-tracing applied models show better performance than reasoning models. Ex-\ncept for o1 on MMToM-QA, models with thought-tracing significantly outperform 01, 01-\nmini, o3-mini, DeepSeek R1, and QwQ 32B preview on all benchmarks.\u00b9 Moreover, even\nLLMs with CoT performs better than reasoning models, such as o3-mini, DeepSeek R1\nand QwQ 32B. The o1 model also underperforms than CoT on FANTOM. This suggest that\nreinforcing mathematical or programming reasoning does not generalize to social reasoning,\nnecessitating separate training scheme in this domain.\nReasoning models show performance trade-off. A performance trade-off can be observed\nbetween false belief scenarios and true belief scenarios in Paraphrased ToMi. While 01 and\no3-mini achieve near-perfect scores (i.e., 100) on the false belief scenarios, their performance\non the easier true belief scenarios drops sharply to below 50 and even below 30 in some\ncases. A similar pattern is observed with DeepSeek R1, whose scores on the easier true\nbelief scenarios are lower than those on the false belief scenarios. In contrast, the QwQ 32B\npreview follows the pattern seen in other vanilla LLMs, displaying higher scores for true\nbelief scenarios but lower scores for false belief scenarios. Although these reasoning models\ndo not release their training data, an interesting future direction would be to investigate how\nreinforcement learning influences this trade-off. On the other hand, thought-tracing applied\nmodels show a more balanced performance across false belief and true belief scenarios, with\nimprovements observed in both cases, except for GPT-40.\nOutput token counts and performance Figure 2 summarizes the average output token\ncounts for 03-mini, 01, DeepSeek R1, and GPT-40 with thought-tracing on Paraphrased ToMi.\n(1) Output token counts in o1 and o3-mini do not correlate with performance. OpenAI's ol\nand o3-mini models include an additional parameter that controls 'reasoning effort' using\nthree settings: low, medium, and high. This parameter guides the model in determining\nhow many reasoning tokens to generate before producing a final response. Although the 01\nmodel with high reasoning effort outperforms the medium and low settings on ToMi, this\npattern does not consistently appear across other benchmarks. For example, on BigToM,\nFANTOM, and MMToM-QA, the o1 model with low reasoning effort achieves the highest\nperformance. Similarly, while increased reasoning effort in o3-mini improves performance\non MMTOM-QA, this is not the case for other benchmarks. Moreover, performance on true\nbelief scenarios in Paraphrased ToMi gets worse as reasoning effort increases.\n(2) Theory-of-mind (ToM) questions elicit more output tokens than factual questions. In\nParaphrased ToMi, all reasoning models use significantly more output tokens when address-"}, {"title": "Analysis on Thought-tracing", "content": "Qualitative Error Analysis We manually evaluated thought-tracing's errors on MMTOM-\nQA and find an inherent bias across models. In many cases, when an agent searches for\nsomething and repeatedly encounters an object-despite not interacting with it\u2014the models\ngenerate hypotheses suggesting that the object is what the agent is looking for. However,\naccording to both social common-sense and explicit models of ToM (Baker et al., 2017; Ying\net al., 2024), if an agent repeatedly sees an object but continues searching elsewhere, it is\nlikely that they are actually searching for something else. Since this error may be a side\neffect of the perception inference in thought-tracing, we conduct ablation experiments.\nAblation We conducted ablation exper-\niments on three models-GPT-40, Llama\n3.3 70B, and Gemini 1.5 Pro-by remov-\ning either the perception inference or the\nweight update (\u00a72.2). For the perception\ninference removal, we directly sample hy-\npotheses only from the state-action pairs\nin the trajectory at each time step. For\nthe weight update removal, we apply uni-\nform weights for all hypotheses across the\ntrajectory at every time step."}, {"title": "Related Work", "content": "Model-based Theory-of-Mind Reasoning Cognitive science research has shown that\nhumans reason about other agents' minds by assuming their actions are rational and goal-\ndirected (Dennett, 1981; Csibra et al., 1999; Baillargeon et al., 2016). These processes have\nbeen formalized as Bayesian Theory of Mind (Baker et al., 2017; Jara-Ettinger et al., 2019),\nwhich models agents as rational actors that plan and act upon their beliefs and desires\nto achieve their goals. This probabilistic model can then be inverted via inverse planning,\ninferring agents' mental states from their behavior (Baker et al., 2009; Ying et al., 2023; 2024),\nincluding via SMC methods (Zhi-Xuan et al., 2020; 2024a). While thought-tracing is patterned\non this model-based approach, we do not explicitly model rational agents, but instead use\nLLMs as implicit models of agents' behavior, and as hypothesis generators for agents' thoughts,\nenabling our method to be applied to open-ended inputs.\nTheory-of-Mind Reasoning in LLMs Debates on whether LLMs are capable of ToM\nreasoning have sparked extensive controversy (Ullman, 2023; Whang, 2023; Ma et al., 2023;\nShapira et al., 2024). As a result, many benchmarks for evaluating ToM reasoning have been\nreleased (Le et al., 2019; Gandhi et al., 2023; Wu et al., 2023; Shapira et al., 2023; Jin et al., 2024;\nChen et al., 2024; Xu et al., 2024, inter alia) along with task complexity assessments (Huang\net al., 2024). While LLMs succeed on some tasks, analyses on these benchmarks show that\nthey are not yet at the level of human ToM, with signs of overfitting (Sclar et al., 2023) and\noverall poor capabilities (Sap et al., 2022; Kim et al., 2023). To mitigate this, several inference-\ntime methods have been introduced (Sclar et al., 2023; Wilf et al., 2024; Jung et al., 2024).\nHowever, they rely on specific assumptions or few-shot examples, making them less scalable.\nRecently, Sclar et al. (2025) showed search for generating ToM training data and Cross et al.\n(2025) introduced a modular design for ToM hypothesis generation in multi-agent game\nscenarios. In this work, we aim to minimize assumptions and introduce a flexible algorithm\ncapable of generating traces of a target agent's mental states in open-ended text.\nInference-time Reasoning for LLMs has emerged as a pivotal area of research, particularly\nin mathematics, coding, and puzzle solving. Early work, such as Chain-of-Thought (Wei\net al., 2022), demonstrated the benefits of generating intermediate reasoning steps, and was\nlater augmented by approaches capable of exploring multiple reasoning paths (Yao et al.,\n2023; Besta et al., 2024), aggregation across reasoning chains (Wang et al., 2023), and problem\ndecomposition (Zhou et al., 2023). Recently, reasoning models trained via reinforcement\nlearning-01 (OpenAI et al., 2024b), R1 (DeepSeek-AI et al., 2025a), and QwQ (Qwen-Team,\n2024)-have shown remarkable performance. Most of these approaches leverage objectively\nverifiable answers to enable reinforcement and accurate selection among multiple reasoning\npaths. However, such verification is challenging in social reasoning due to subjectivity\nand uncertainty. To handle uncertainty, recent methods perform probabilistic inference in\nLLMs via sequential Monte Carlo (Lew et al., 2023b; Zhao et al., 2024; Loula et al., 2025), but\nhave not been used to infer and track mental states. In this work, we introduce a general\nSMC-like algorithm capable of operating effectively in the social domain while bypassing\nthe need for objective verification."}, {"title": "Conclusion", "content": "In this paper, we introduced thought-tracing, a new inference-time algorithm that performs\napproximate inference over agents' mental states. Our approach draws inspiration from the\nsequential Monte Carlo (SMC) method and follows the conceptual structure of Bayesian\nTheory of Mind (BTOM; Baker et al., 2017). Thought-tracing leverages an SMC-like structure\nby using LLMs as proposals for natural language hypotheses, which are then weighted with\nactions from the given text\u2014all without requiring ground-truth answers to questions in\ndatasets. Our evaluation on four theory-of-mind benchmarks demonstrates its effectiveness,\nsignificantly improving upon baseline LLMs and outperforming recent reasoning models\nsuch as 03-mini and R1. Additionally, our findings reveal that reasoning models show\ndifferent behaviors in the social domain compared to their remarkable performance in\nareas like math and coding. Nonetheless, there remains room for improvement due to\nerrors in areas such as how hypotheses about agents' goals are generated (Section 3.3).\nThis suggests that more work needs to be done to achieve the reliability that explicit BTOM\nmodels demonstrate in closed-world settings (Zhi-Xuan et al., 2022; 2024b; Ying et al., 2024).\nWe hope these insights pave the way for future research on inference-time reasoning in\nsocial settings, where AI agents will play an increasingly important role."}, {"title": "BigToM Re-annotation", "content": "The original BigToM dataset (Gandhi et al., 2023) includes 6 tasks, each with 200 stimuli. In\nour re-annotation, we took the first 100 stimuli from each task category.\nWe recruited 510 participants (Average age = 41.33, 238 males, 265 females, 7 others) via\nProlific. To minimize potential bias, we did not conduct tutorials for this task. The experi-\nment took place over a customized interface. Each participant was randomly assigned to\ncomplete 30 stimuli randomly sampled from the set of stimuli. Each stimulus received on\naverage 13 ratings."}, {"title": "Thought-tracing Setup", "content": "We use greedy decoding through out our thought-tracing algorithm. We set the number of\nhypotheses to 4 across all experiments. The threshold for the effective sample size is set to\n2. We measure the text similarity across hypotheses using the Jaccard similarity, and the\nthreshold for rejuvenation is set to 0.25. For action labeling, we use GPT-40-2024-08-06 for\nParaphrasedToMi and BigToM, and a rule-based algorithm for FANTOM and MMTOM-QA."}]}