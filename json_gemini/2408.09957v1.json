{"title": "Contextual Importance and Utility in Python: New Functionality and Insights with the py-ciu Package", "authors": ["Kary Fr\u00e4mling"], "abstract": "The availability of easy-to-use and reliable software implementations is important for allowing researchers in academia and industry to test, assess and take into use eXplainable AI (XAI) methods. This paper describes the py-ciu Python implementation of the Contextual Importance and Utility (CIU) model-agnostic, post-hoc explanation method and illustrates capabilities of CIU that go beyond the current state-of-the-art that could be useful for XAI practitioners in general.", "sections": [{"title": "Introduction", "content": "Explainable AI (XAI) has surged in significance in recent years as AI systems permeate various facets of our lives, notably through novel achievements of Machine Learning (ML) methods. Understanding why AI makes certain decisions is crucial, particularly in fields like healthcare, finance, and criminal justice, where transparency and accountability are paramount for maintaining the trust in such AI systems.\nFeature attribution methods such as the Local Interpretable Model-agnostic Explanations (LIME) [Ribeiro et al., 2016] and SHapley Additive exPlanations (SHAP) [Lundberg and Lee, 2017] seem to be the most popular XAI methods for the moment, at least in the category of so called model-agnostic post-hoc (/outcome) explanation methods. The popularity of those methods might partially be due to the availability of open source implementations, which makes it possible to use them relatively easily in research and in real-life applications.\nIn this paper we present a Python implementation for tabular data of the Contextual Importance and Utility (CIU) method. Like LIME and SHAP, CIU is a model-agnostic post-hoc explanation method. However, CIU's theoretical foundation is different from both LIME, SHAP and most current XAI methods by making a difference between feature importance and feature influence, where feature influence conceptually corresponds to the values produced by LIME and SHAP. This theoretical difference makes it possible to produce different explanations than other XAI methods, such as the potential influence plot presented in section 4.1.\nThe objectives of the paper are twofold:\n1. Introduce a Python implementation of CIU for tabular data that is comparable with those that exist for LIME and SHAP.\n2. Using the presented implementation, demonstrate in what way CIU differs from other methods and what additional capabilities it offers in terms of explainability."}, {"title": "Contextual Importance and Utility", "content": "Contextual Importance and Utility (CIU) was initially presented by Kary Fr\u00e4mling in 1992 [Fr\u00e4mling, 1992] for explaining recommendations of decision support systems in a model-agnostic way, where \u201cmodel-agnostic\" includes non-ML based models. CIU was presented formally in [Fr\u00e4mling, 1996a; Fr\u00e4mling, 1996b]. More recent research on CIU has been presented e.g. in [Anjomshoae et al., 2019; Fr\u00e4mling, 2022; Fr\u00e4mling, 2023].\nCIU uses the notion of utility known from domains such as utility theory, multiple criteria decision making etc. [Keeney and Raiffa, 1976; Roy, 1985; Vincke, 1992; K\u00f6ksalan et al., 2011]. Utility theory is postulated in economics to explain behavior of individuals based on the premise that people can consistently rank order their choices depending upon their preferences. Each individual will show different preferences. Under some assumptions, those preferences can be represented analytically using a utility function. For the purposes of this paper, it is adequate to say that a utility function maps values x{i} or yj into a utility value u \u2208 [0, 1]. For instance, the utility value of output yj is uj = hj(yj), where hj is the utility function for output yj. In classification tasks, we have uj = yj because yj is typically a probability that can be used directly as the utility value uj.\nWe begin with the formal definitions of CI and CU and then go on with a definition of Contextual influence, denoted with the symbol \u00a2.\nDefinition 1 (Contextual Importance (CI)). CI expresses to what extent modifying the value of one or more feature(s) X{i} can modify the output utility value uj. CI is expressed formally as:\n$\\displaystyle CI_j(x, {i}, {I}) = \\frac{umax_j(x, {i}) \u2013 umin_j(x, {i})}{umax_j(x, {I}) \u2013 umin_j(x, {I})},$ (1)\nwhere x is the studied instance, {i} \u2286 {I} and {I} \u2286 {1,...,n}, and n is the number of features. uminj and umaxj are the minimal and maximal output utility values that can be achieved by varying the value(s) of feature(s) x{i} while keeping all other feature values at those of x.\nFor classification tasks where uj(yj) = yj \u2208 [0,1] and for regression tasks where the utility value can be obtained by an affine transformation of form uj(yj) = Ayj + b (which applies to most regression tasks), we can write:\n$\\displaystyle CI_j(x, {i}, {I}) = \\frac{ymax_j(x, {i}) \u2013 ymin_j(x, {i})}{ymax_j(x, {I}) \u2013 ymin_j(x, {I})},$ (2)\nwhere yminj and ymaxj are the minimal and maximal output values observed when varying the value(s) of feature(s) x{i} and {1}. When {I} = {1, ..., n}, i.e. all features, then ymaxj(x, {I}) and yminj(x, {I} rarely need to be estimated because they are one and zero for classification tasks. For regression tasks, they can be set to the minimal and maximal output values in the training set.\nDefinition 2 (Contextual Utility (CU)). CU expresses how the current value(s) of feature(s) x{i} contribute to obtaining a high output utility uj:\n$\\displaystyle CU_j(x, {i}) = \\frac{uj(c) \u2013 umin_j(x, {i})}{umax_j(x, {i}) \u2013 umin_j(x, {i})}.$ (3)\nWhen uj (yj) = Ayj + b, this can again be written as:\n$\\displaystyle CU_j(x, {i}) = \\frac{yj(c) \u2013 yumin_j(x, {i})}{|ymax_j(x, {i}) \u2013 ymin_j(x, {i})|},$ (4)\nwhere yumin = ymin if A is positive and yumin = ymax if A is negative.\nDefinition 3 (Contextual influence). Contextual influence expresses how much one or more feature(s) influence the output value (utility) relative to a reference value or baseline, here denoted CUref \u2208 [0,1]. Contextual influence is defined as:\n\u00a2 = CI \u00d7 (CU \u2013 CUref), (5)\nwhere indices \u201cj(x, {i}, {I})\u201d have been omitted for easier readability.\nContextual influence is conceptually similar to Shapley value and similar additive feature attribution methods, which is a reason to use the symbol \u03c6.\nCIU equations apply to individual features as well as to coalitions of features {i} versus other coalitions of features {I}, where {i} \u2286 {I} and {I} \u2286 {1,...,n}. When such coalitions are given labels, they can be used as Intermediate Concepts (IC) in the explanations. Such ICs make it possible to define arbitrary explanation vocabularies with abstraction levels that can be adapted to the target user.\nIt is worth noting that CI and CU are values in the range [0, 1] by definition. Such a known value range makes it possible to assess whether a value is high or low. Contextual influence also has a maximal amplitude of one, where the range is [-CUref,1 \u2013 CUref]."}, {"title": "Estimation of ymin and ymax", "content": "CIU only depends on identifying sufficiently correct ymin and ymax values for the studied model, instance and set of features {i}, and {I} if needed. In py-ciu the class PerturbationMinMaxEstimator takes care of searching for ymin and ymax values in a model-agnostic way. PerturbationMinMaxEstimator generates a set of samples for which y values are calculated. The principle of building the set of samples is as follows:\n1. For categorical features in {i}, include all possible value combinations.\n2. For numerical features in {i}, include all possible combinations of minimal and maximal input values.\n3. Combine all the possible value combinations for categorical and numerical features from the two previous steps.\n4. If the number of samples so far is smaller than the number requested, then fill up with random values for numerical features in {i}, and for random combinations of categorical features values in {i}.\n5. Concatenate current instance with the other samples.\nPerturbationMinMaxEstimator is the default class for estimating ymin and ymax. The default number of samples is 100, which in most cases is a good compromise when dealing with only one feature xi. However, identifying minimal and maximal function values could be done in many different ways. It would, for instance, be possible to use an estimator that only produces in-distribution samples. Information about the model f could also be used if available. If f is linear, then only two samples are needed per feature. For rule- or tree-based systems it would presumably be possible to identify ymin and ymax efficiently by known information about their properties, as for TreeSHAP [Lundberg et al., 2018]. In [Fr\u00e4mling, 1996a], a Radial Basis Function (RBF) neural network architecture was used, where ymin and ymax values were by definition found at, or close to, the RBF centroids. The estimation of ymin and ymax is a mathematical task that is not specific to CIU and where significant improvements can be expected in future research."}, {"title": "Out Of Distribution?", "content": "The sampling approach described in the previous section can generate so called out of distribution (OOD) samples, i.e. feature value combinations that are not possible in reality or that are significantly different from the data in the training set used to build the ML model being explained. For such samples, the model f may be incapable to provide correct output values. OOD challenges related to the used sampling method and potential solutions to those challenges can be grouped into the three following cases:\n1. Predictable OOD behaviour. If OOD samples do not lead to undershooting of the ymin value, nor to overshooting of the ymax value, then OOD is not an issue. Many ML models do not under- or overshoot even when extrapolating outside the training set, e.g. Linear Discriminant Analysis (LDA), and most rule- or tree-based methods such as random forest and gradient boosting. Neural network models such as the Interpolating, Normalising and Kernel Allocating (INKA) used in [Fr\u00e4mling, 1996a] also guarantee that under- or overshooting does not occur. Input-output value graphs such as those in Figure 2 can be used for studying the model behaviour within the value ranges used by CIU.\n2. Non-predictable OOD behaviour. This happens if under- or overshooting occur with OOD samples. In that case the sampling approach used here will not be appropriate. Various approaches could be imagined for addressing this problem, such as only using samples that are \"sufficiently\u201d close to instances in the training set.\n3. Detecting model instability. Since CI and CU values are in the range [0, 1] by definition, obtaining CI or CU values that are outside this range indicate that the model undershoots or overshoots the permissible range for one or more samples. This could be an indication that those samples should be removed or that the model should be corrected in order to increase its trustworthiness. A correction approach using so called pseudo-examples has been proposed e.g. in [Fr\u00e4mling, 1996b].\nThe second and third cases are left out of scope for the current paper and remain topics of further research. Similar OOD challenges exist for all permutation-based XAI methods, including kernel-SHAP and LIME."}, {"title": "State of the Art", "content": "CIU was originally implemented in Matlab as described in [Fr\u00e4mling, 1996b] and [Anjomshoae et al., 2019]. Since then, CIU has been implemented in R for tabular data [Fr\u00e4mling, 2021] and image classification [Fr\u00e4mling et al., 2021]\u00b9. A first Python implementation of CIU for tabular data was published and described in [Anjomshoae et al., 2020]. However, that implementation was very \"bare-bone\" and instantly lagged behind the functionality available in the R version.\nThis paper describes the functionality of py-ciu after a complete rewrite performed in the end of 2023. The goal of the rewrite was to take the Python implementation of CIU for tabular data to at least the same level as the corresponding R implementation, as it is described in [Fr\u00e4mling, 2023]. The intention has also been to make the use of the R and Python implementations as similar as possible. In the remainder of this section, we show how to produce well-known visualisations and explanations with py-ciu. py-ciu is available at https://github.com/KaryFramling/py-ciu and the code that has been used for producing the results of this paper are in the notebook XAI_IJCAI_2024. ipynb of the repository.\nTo begin with, we will use the well-known Titanic data set and a Random Forest classifier model for predicting the probability of survival. The explained instance is \"Johnny D\", who is also used in https://ema.drwhy.ai. \u201cJohnny D\" is an 8-year old boy who travels alone. The model predicts a 61.0% probability of survival, which is significantly higher than the average probability of survival 40.4% for the data set used. We want to explain why the estimated probability of survival is rather high."}, {"title": "Input-Output Plots", "content": "Plotting the output value yj as a function of one (or two) inputs x{i} while keeping the values of all other inputs static is a rather obvious way of studying the behaviour of any model. Such input-output (IO) plots are used extensively in CIU papers, such as [Fr\u00e4mling, 1996b; Fr\u00e4mling, 1996a]. Later, [Friedman, 2001] suggested to use the name Partial Dependence Plot (PDP) for such visualisations. Names such as Individual Conditional Expectation (ICE) plots and Ceteris Paribus (PB) plots have also been suggested for essentially the same thing. In this paper, we will simply call them IO plots because that name seems to be oldest in a XAI context and is also the name associated with CIU. Figure 2 shows IO plots generated by the commands shown in Listing 2.\nAs illustrated in Figure 1 and Figure 2, CI, CU and contextual influence values can be \"read\" directly from IO plots: CI is the ratio (ymax \u2013 ymin)/(whole-y-range), while CU corresponds to the position of the red dot (current instance values) within the interval [ymin, ymax]. Contextual influence corresponds to the y-axis position of the red dot relative to the orange \"neutral\u201d line that corresponds to the value of CUref, while being constrained by the red and green ymin and ymax lines. In the case of several outputs, which then typically represent different classes, py-ciu allows for including more than one or even all outputs yj in IO plots, which is useful for detecting where the transition happens between different classes.\nThe 3D plot does not currently support visualising ymin, ymax and CUref, mainly because introducing multiple planes in the 3D plot might make it more challenging to read the plot. 3D plots are useful because they make it possible to show dependencies between two features at a time. For instance, in Figure 2 we can see that it would be better for Johnny D to have one sibling rather than zero, whereas having no siblings seems to be favorable for survival for most other ages according to the model."}, {"title": "Feature Influence Explanations", "content": "Figure 3 shows the potentially most used kind of XAI plot for tabular data. The leftmost plot uses CIU's contextual influence and has been produced with the command CIU_titanic.plot_influence(). It is worth noting that in Listing 1, we gave the parameter neutralCU=mean_surv_prob for setting the value of CUref to the average output probability rather than using the default value CUref = 0.5. This was done in order to use the same reference value for contextual influence as SHAP does.\nComparing these influence-based explanations given by different methods is out of the scope of this paper. However, it is worth noting that contextual influence has a known range of one between the lowest and highest possible & values. That signifies that it is possible for the explainee to assess \"how negative\" or \"how positive\" a contextual influence value is even without seeing the & values of other features. On the other hand, SHAP and LIME values do not have a pre-defined interpretation of $'s magnitude and mainly give an indication of relative influence between features."}, {"title": "CIU-specific Capabilities", "content": "By \"CIU-specific\" we mean XAI capabilites that are not possible or that have not been proposed with LIME, SHAP or other methods, to our best knowledge."}, {"title": "Potential Influence Plot", "content": "Explanations based on influence values as in Figure 3 can be misleading especially for instances that are similar or close to the reference instance or reference value. As an example, for an average instance with average feature values, all SHAP values might be zero, which could be called a null explanation. The same is true also for Contextual influence and LIME values. This signifies that even the most important feature according to CI might have a zero influence value, which might lead to misunderstandings with the explainee.\nThe Potential Influence (PI) plot avoids null explanations, as shown in Figure 4. A PI plot illustrates the CI value with a transparent bar and overlays is with a solid bar that covers the transparent bar with as many percent as indicated by the CU value. Since CI and CU have know value ranges [0,1], we can interpret them directly: CI = 0 signifies no importance and no transparent bar, whereas CI = 1 gives a transparent bar that fills the range entirely. Similarly, CU = 0, signifies the worst possible value and no solid bar, whereas CU = 1 gives a solid bar that covers the transparent bar entirely. Figure 4 was produced with the command CIU_titanic.plot_ciu(plot_mode=\u2019overlap\u2032).\nThe reason for calling this a \"potential influence\" plot is that the area of the transparent part of each bar indicates the potential for improvement that can be achieved by modifying the value of that feature (or Intermediate Concept). The area of the solid part of each bar again indicates the potential for getting a worse result by modifying the value of the feature.\nFigure 5 shows an alternative PI visualisation using colours for indicating the CU value. Such a visualisation might be preferred in some contexts and by some users but we find that it doesn't indicate the potential influence of changing the feature value as clearly as Figure 4.\nIn [Fr\u00e4mling, 2023] the PI plot is described with the term \"counterfactual\" because it provides a partial answer to the question \"what if?\u201d. The PI plot indeed gives an indication of which changes in feature values would have the greatest impact on the result. For instance, in Figure 4 we see that having one sibling, rather than zero, could increase the probability of survival. However, knowing what exact changes to make, which changes are possible, or the cost of making them requires additional information. Furthermore, changing feature values will presumably also change CI and CU values due to the changing \u201ccontext\u201d as defined by x. Still, PI plots do provide guidance about what changes are worth trying first among the potentially numerous counterfactual options."}, {"title": "Textual explanations", "content": "Figure 6 shows the output obtained from the commands in Listing 3. The command CIU_titanic.textual_explanation() by itself produces raw text. Since CI and CU are constrained to the interval [0, 1], their values have a meaning that can be given a textual quantification. The thresholds and texts to use can be passed as parameters to the method."}, {"title": "Beeswarm Plot for Global Assessment", "content": "Beeswarm plots give an overview of an entire data set by showing CI/CU/influence values of every feature and every instance.\nThe dot colors represent the feature value. The CI beeswarm in Figure 8 reveals, for example, that the higher the value of lstat (% lower status of the population), the more importance is given to the lstat feature as shown by higher CI values for high lstat values. The influence plot reveals that a high lstat value lowers the predicted home price, which is also shown in the CU plot where a high lstat value gives a low CU value. We use CUref = 0.390 for Contextual influence, which corresponds to the average price so the reference value is the same as for the Shapley value.\nUnfortunately, we have not yet found a Python package that would produce beeswarm plots of similar quality as the SHAP package, as seen e.g. at https://github.com/shap/shap.\nTherefore, we reproduce the beeswarm produced by the CIU R package in Figure 7, which is nearly identical to the one produced by SHAP. Similar beeswarm plots can also be produced for LIME and other XAI methods, which gives them some \"global\" explanation capabilities that are sometimes attributed only to SHAP."}, {"title": "Intermediate Concepts", "content": "CIU is not constrained to one input feature versus an output value. In reality, CIU can be used for any coalition of features {i} relative to a coalition of features {I}, where {i} \u2286 {I} and {I} \u2286 {1,...,n}.\nFurthermore, dependencies between features in {i} and {I} are automatically taken into account by how CIU is defined, as shown in [Fr\u00e4mling, 2022]. When we give names to such coalitions of features, then they can be used as Intermediate Concepts (IC) in explanations. By using ICs, we can build explanation vocabularies that correspond to the explainee's background and preferred levels of abstraction. The explanations and the explanatory interaction can also be adjusted accordingly. ICs could also be formed by studying feature dependencies; However, we believe it is more useful to focus on the explainees' needs."}, {"title": "Contrastive Explanations", "content": "Contrastive explanations compare one instance A with another instance B, where A and/or B can be real or counterfactual. Humans often use and expect contrastive explanations that answer questions such as \u201cWhy alternative A rather than B?\u201d or \u201cWhy not alternative B rather than A?\u201d. Just showing the explanation for A and the explanation for B is not a contrastive explanation. A contrastive explanation highlights the particular differences between A and B. In practice, this signifies that A is the reference against which we compare B.\nSince any value in the range [0, 1] can be used for CUref in Equation 5, CU values of instance A can also be used when producing a contextual influence explanation for B. Figure 11 shows a contrastive explanation for why the previously used Ames instance with the predicted price of $740 222 is more expensive than another house that has a predicted price of $568 000. Contrastive values are in the range [-1,1] by definition, so here again the actual \u03c6 values can be interpreted. For instance, the Basement of A is about 14% better than the Basement of B. One might ask oneself what \"14%\" signifies for a Basement but it is clearly better. Since Basement is an IC, it is also possible to obtain a detailed explanation for it.\nCIU results produced by explain_voc() and the basic explain() methods are identical in format, so they can be passed to all plotting functions as such, including the plot_contrastive() function."}, {"title": "Conclusion", "content": "The paper presents a Python implementation of the CIU method with the intention to provide researchers with an easy way to compare CIU with other XAI methods, and potentially even be used in commercial products. Furthermore, the paper shows capabilities that are specific to CIU and that presumably are not feasible or have not been exploited with currently popular methods such as LIME and SHAP. It is not possible to compare such CIU-specific capabilities against other methods, which is the reason for the lack of theoretical or empirical comparisons between CIU and other XA\u0399 methods. Regarding comparable capabilities such as those presented in Section 3, comparisons have been made using the R version of CIU and presented e.g. in [Fr\u00e4mling, 2022; Fr\u00e4mling, 2023].\nFuture work includes implementing CIU for use with natural language, time series and other kinds of input data. User studies regarding the understanding of different kinds of explanations by CIU and other XAI methods are also foreseen. Finally, our intention is to develop truly \u201csocial XAI\" where the XAI system and the explainee could even engage in a co-constructive dialog [Rohlfing et al., 2021] enabled by CIU's capability of generating explanations that answer different questions and by using ICs.\"\n    }"}]}