{"title": "Policy Guided Tree Search for Enhanced LLM Reasoning", "authors": ["Yang Li"], "abstract": "Despite their remarkable capabilities, large language models often struggle with tasks requiring complex reasoning and planning. While existing approaches like Chain-of-Thought prompting and tree search techniques show promise, they are limited by their reliance on predefined heuristics and computationally expensive exploration strategies. We propose Policy-Guided Tree Search (PGTS), a framework that combines reinforcement learning with structured tree exploration to efficiently navigate reasoning paths. Our key innovation is a learned policy that dynamically decides between expanding, branching, backtracking, or terminating exploration, eliminating the need for manual heuristics or exhaustive search. Experiments across mathematical reasoning, logical deduction, and planning benchmarks demonstrate that PGTS achieves superior reasoning performance while significantly reducing computational costs compared to existing methods. These results establish PGTS as a scalable and effective solution for tackling complex reasoning tasks with LLMs.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, driven by advances in model architecture, parameter scaling, and pretraining data (Achiam et al., 2023; Team et al., 2023; Jiang et al., 2023; Dubey et al., 2024). However, these models consistently struggle with tasks requiring complex reasoning and planning (Valmeekam et al., 2022; Kambhampati, 2024; Kambhampati et al., 2024). In mathematical problem-solving, for instance, while LLMs excel at direct arithmetic calculations, they frequently falter when faced with multi-step word problems that demand strategic decomposition and careful planning (Kao et al., 2024; Huang & Chang, 2022). These limitations extend to logical reasoning and real-world planning scenarios, where success depends on systematically breaking down complex problems into interconnected, actionable steps (Kambhampati, 2024).\nRecent approaches to enhance LLM reasoning fall into three main categories. First, advanced prompting techniques like Chain-of-Thought (CoT) (Wei et al., 2022) and Least-to-Most prompting (Zhou et al., 2022) encourage step-by-step reasoning by generating intermediate steps. Second, verification-based methods aim to improve reasoning quality through step validation (Cobbe et al., 2021; Lightman et al., 2023; Li et al., 2022; Wang et al., 2024b; Zhang et al., 2024) or iterative refinement (Qu et al., 2024), using either self-evaluation (Qu et al., 2024) or external correctors (Havrilla et al., 2024). Third, tree-based search methods reframe reasoning as a planning problem, using reward signals to guide the exploration of reasoning paths (Feng et al., 2023; Yao et al., 2024; Besta et al., 2024; Hao et al., 2023; Xie et al., 2024b; Khalifa et al., 2023; Wang et al., 2024a).\nWhile these approaches show promise, they face several key limitations. Heuristic search methods rely heavily on predefined rules and reward definitions, requiring significant expert knowledge. Additionally, their trial-and-error exploration process can be computationally expensive due to the vast space of possible reasoning steps. The challenge becomes even more pronounced when using self-evaluation as a reward signal, making the process even more resource-intensive (Feng et al., 2023; Hao et al., 2023; Xie et al., 2024b). Moreover, the ability of LLMs to effectively critique their own outputs and refine their responses remains an area of active research. This limitation is particularly pronounced in tasks requiring intricate planning and reasoning (Stechly et al., 2024; Huang et al., 2023; Hong et al., 2023). Furthermore, even when the exploration process identifies improved reasoning chains, distinguishing successful reasoning paths from failed ones without external guidance continues to pose a significant challenge (Qi et al., 2024).\nTo address these challenges, we propose Policy-Guided Tree Search (PGTS), a framework that integrates reinforcement learning with structured tree exploration. The core of PGTS is a learned policy that dynamically guides the reasoning process through four key actions: expanding current nodes, branching to alternative paths, backtracking to previous states, or terminating exploration. This structured approach enables efficient navigation of reasoning paths, focusing computational resources on the most promising paths while"}, {"title": "2. Method", "content": "Language model-based reasoning can be formalized as a sequence generation problem with intermediate steps. Large language models, parameterized by \\(\\theta\\) and denoted as \\(p_{\\theta}\\), generate text autoregressively. Given an input prompt \\(x = [x_1, ..., x_n]\\) consisting of tokens from a predefined vocabulary, the model produces a response \\(y = [y_1, ..., y_m]\\). Each token \\(y_i\\) is produced sequentially, conditioned on the prompt \\(x\\) and all previously generated tokens \\(y_{<i}\\). The probability of generating sequence \\(y\\) can be expressed as:\n\\[P_{\\theta}(y | x) = \\prod_{i=1}^{m} P_{\\theta}(y_i | x, y_{<i}).\\]\nFor reasoning tasks, \\(y\\) typically comprises both intermediate reasoning steps and the final answer.\nThis autoregressive procedure naturally maps to a Markov Decision Process (MDP), defined as \\((S, A, T, R, \\gamma)\\), where the state \\(s \\in S\\) represents the current context, including the prompt \\(x\\) and generated tokens so far; the action \\(a \\in A\\) corresponds the next token to be generated; the transition"}, {"title": "2.1. Problem Formulation", "content": "\\(s' = T(s, a)\\) is deterministically defined by appending action \\(a\\) to state \\(s\\); the reward \\(R(s, a)\\) evaluates the quality of each state-action pair; \\(\\gamma\\) denotes the discount factor, weighting immediate rewards over future ones. The objective of reasoning is to find an optimal sequence of actions \\(a^* = [a_1, ..., a_H]\\) that maximizes the cumulative discounted rewards, \\(r = \\sum_{h=1}^{H} \\gamma^h R(s_h, a_h)\\), where \\(H\\) is the trajectory horizon. In practice, actions can represent tokens, phrases, or complete sentences to improve optimization efficiency (Feng et al., 2023; Zhao et al., 2024; Xie et al., 2024a; Yao et al., 2024; Wang et al., 2024a). We refer to this MDP formulation as LM-MDP."}, {"title": "2.2. Background: Tree Search for LLM Reasoning", "content": "Tree search methods offer a structured approach to explore and optimize reasoning paths in LLMs by systematically evaluating different sequences of reasoning steps. The reasoning process can be formalized as a tree \\((V, E)\\), where each node \\(v \\in V\\) corresponds to a state \\(s \\in S\\), and each edge \\((v_i, v_j) \\in E\\) represents an action \\(a \\in A\\). The root node represents the initial prompt \\(x\\), while a path from the root to a leaf node denotes a complete reasoning chain \\(y\\). To evaluate the quality of each reasoning step, a process reward model (PRM) \\(R(s, a)\\) is employed, implemented either as a pretrained reward model (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2024b) or an LLM-based evaluator (Yao et al., 2024; Gao et al., 2024; Hao et al., 2023).\nVarious tree search algorithms have been developed to navigate these reasoning trees effectively. Depth-First Search (DFS) (Yao et al., 2024) and Breadth-First Search (BFS) (Xie et al., 2024b) offer systematic exploration strategies, while A* Search (Wang et al., 2024a) and Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Zhao et al., 2024; Xie et al., 2024a) incorporate heuristics and sampling to balance exploration with exploitation. While these methods have proven effective in black-box optimization (Hart et al., 1968; Zhai & Gao, 2022; Malherbe et al., 2022; Wang et al., 2024c; 2020) and reinforcement learning (Silver et al., 2017; Kartal et al., 2019; Grill et al., 2020) contexts, their application to LLM reasoning introduces distinct challenges: First, the potential sequences of tokens, phrases, or sentences at each step create an enormous action space, making exhaustive exploration computationally infeasible. Second, feedback signals \\(R(s, a)\\) are often sparse, noisy, or difficult to compute, particularly in the absence of ground-truth annotations for reasoning chains, complicating the evaluation of reasoning paths. Third, querying the LLM at each reasoning step incurs significant resource demands, amplifying the computational overhead of traditional tree search methods.\nThese challenges highlight the need for more efficient and adaptive search strategies in LLM reasoning. To address these limitations, we propose the PGTS framework, which integrates a learned policy to dynamically guide the tree"}, {"title": "2.3. Policy Guided Tree Search", "content": "The PGTS policy performs an exploratory walk on the underlying reasoning tree, which is initially unobserved and gradually revealed through exploration. This process can be modeled as a MDP operating within an environment represented by the complete reasoning tree. The state \\(S\\) corresponds to the portion of the tree revealed up to the current step, transitioning deterministically based on the chosen action \\(a \\in A\\). Note that this formulation differs from the previously introduced LM-MDP, as it operates at the level of tree exploration rather than token generation. Please see Sec. 2.3.2 for the formal definition of this Tree Search MDP.\nSince the state \\(s \\in S\\) is naturally tree-structured, we implement the PGTS policy \\(\\pi_{\\phi}(\\cdot | s)\\) as a graph neural network. Specifically, we utilize the Graph Transformer architecture, GPS, proposed in (Ramp\u00e1\u0161ek et al., 2022). The GPS architecture utilizes both local message passing and global attention to capture both the local structural relationships and global context within the tree. Please see Sec. 2.3.4 for details about the PGTS policy.\nTo guide the reasoning process effectively, the reward design for PGTS balances two objectives: promoting high-quality reasoning chains while encouraging efficient exploration. We define immediate rewards based on task-specific metrics that evaluate the relevance and correctness of generated reasoning steps, \\(R(s, a)\\), while introducing costs for exploration actions. This reward structure ensures the policy learns to navigate the reasoning tree efficiently, focusing computational resources on the most promising paths. Please see Sec. 2.3.2 for details about the reward design."}, {"title": "2.3.1. OVERVIEW", "content": "Building on the tree search formulation for LLM reasoning, PGTS introduces a structured action space designed to navigate the reasoning tree effectively. Rather than relying on predefined search strategies, PGTS learns to dynamically select between four fundamental operations:\n\u2022 Expand: Generate the next intermediate step in the reasoning chain by expanding the current node. This allows the policy to progress along promising reasoning paths.\n\u2022 Branch: Explore alternative reasoning paths by branching to a sibling node from the current node. This enables the policy to consider alternative solutions when the current one is suboptimal.\n\u2022 Backtrack: Revisit a previous node to explore alternative paths, enabling recovery from suboptimal reasoning trajectories. The backtrack operation involves multiple actions, as the policy specifies how many steps to backtrack.\n\u2022 Terminate: Conclude the search process once sufficient evidence or a satisfactory reasoning chain is obtained, preventing unnecessary exploration.\nThese actions (denoted as \\(A\\)) empower PGTS to adaptively explore and refine reasoning chains, striking a balance between exploring promising steps and exploiting valuable chains. This dynamic approach allows for efficient and targeted navigation of the reasoning tree, even in the presence of a vast reasoning steps.\nIn practice, we impose practical constraints to ensure computational feasibility. We limit the maximum depth of the reasoning tree, which naturally bounds the number of possible backtracking steps. To prevent unbounded exploration from branching actions, we also restrict the tree's breadth by capping the number of child nodes per parent. These constraints maintain a finite action space while preserving the policy's ability to explore diverse reasoning paths. Please see Sec. 2.3.3 for detailed discussion about the implication of these depth and breadth limits."}, {"title": "2.3.2. TREE SEARCH MDP", "content": "The tree search problem in PGTS can be formulated as a MDP, which we refer to as Tree Search MDP (TS-MDP), defined by the tuple \\((S, A, T, R, \\gamma)\\). The MDP interacts with the underlying reasoning tree as its environment, with each reasoning problem corresponding to a new environment. Similar to the typical MDP settings, the environment is not directly observable to the policy, while it gradually reveals a part of the tree as the policy explores the environment.\nThe state space \\(S\\) represents the set of possible states in the tree exploration process, where a state \\(s \\in S\\) corresponds to the portion of the reasoning tree that has been revealed through previous interactions. The state includes the structure and content of all visited nodes, as well as the current position of the policy in the tree. Each node in the tree represents a partial reasoning path, which is equivalent to a state \\(s \\in S\\) in the LM-MDP. Node features are derived from the hidden states extracted from the target LLM, specifically the hidden state from the final layer corresponding to the last generated token. Each edge in the tree captures the parent-child relationship established by the reasoning step \\(a \\in A\\). The edge features include the immediate reward"}, {"title": "2.3.3. CONSTRAINED ACTION SAMPLING", "content": "\\(R(s, a)\\), reflecting the quality of the reasoning step \\(a\\). Together, the structure of nodes and edges forms a dynamic representation of the reasoning tree, allowing the PGTS policy to effectively navigate and optimize the exploration.\nThe action space \\(A\\) comprises the structured actions available to the PGTS policy: expand, branch, backtrack, and terminate. Each action transitions the policy to a new state by modifying the observed reasoning tree. For a reasoning tree with a maximum depth of \\(D\\), the backtrack operation includes \\(D - 1\\) distinct actions, corresponding to backtracking between 1 and \\(D - 1\\) steps. Notably, backtracking to the root node (corresponding to the prompt \\(x\\)) is invalid, as the root node lacks sibling nodes for exploration.\nThe transition function \\(T\\) determines how the state transitions from \\(s\\) to \\(s'\\) given an action \\(a \\in A\\). The transitions in the TS-MDP are deterministic, as applying an action updates the tree structure in a predefined manner. The reasoning process starts from a single root node, representing the initial state, and terminates when the policy selects the terminate action or when the exploration budget, i.e., the maximum number of tree search steps, is exhausted. As the policy updates the state, it records the node features and edge features, and update the current node accordingly to reflect the latest position of the policy in the reasoning tree.\nThe reward function \\(R(s, a)\\) assigns a scalar reward for executing action \\(a\\) in state \\(s\\). For each action type, the reward is defined as follows:\n\u2022 Expand: Let \\(s_a \\in S\\) be the partial reasoning path at the current node and \\(a_d \\in A\\) be the new reasoning step generated by the expand action, where \\(d\\) denotes the depth of the current node. The reward is defined as\n\\[R(s, a) = R(s_d, a_d) - C(a),\\]\nwhere \\(R(s_d, a_d)\\) evaluates the quality of the new reasoning step, and \\(C(a)\\) denotes the cost of the expand action.\n\u2022 Branch: When the policy selects the branch action to explore an alternative sibling node, let \\(s_a\\) and \\(s'_a\\) denote the partial reasoning paths corresponding to the current node and its sibling node respectively, with shared parent \\(s_{d-1}\\), the reward is defined as\n\\[R(s, a) = R(s'_{d-1}, a'_{d-1}) - R(s_{d-1}, a_{d-1}) - C(a).\\]\nHere, \\(R(s'_{d-1}, a'_{d-1}) - R(s_{d-1}, a_{d-1})\\) quantifies the potential improvement in reasoning by exploring the alternative path, while \\(C(a)\\) reflects the cost of branching.\n\u2022 Backtrack: For the backtrack action, which reverts to a previously visited node, all intermediate rewards along the backtracked path are revoked. Suppose the backtrack action reverts \\(K\\) steps. Denote the partial reasoning paths for the current node and previous node as \\(s_d\\) and \\(s_{d-K}\\),"}, {"title": null, "content": "The reasoning tree enforces explicit depth and breadth limits to maintain computational feasibility, which naturally constrains the validity of actions in different states. For expand actions, validity is determined by the depth limit and answer state: the expand action becomes invalid when the current node has reached the maximum depth limit or when a final answer has already been generated at the current"}, {"title": null, "content": "respectively. The reward \\(R(s, a)\\) is defined as\n\\[R(s, a) = \\sum_{k=1}^{K} R(s_{d-K-1}, a'_{d-K-1}) - \\sum_{k=1}^{K} R(s_{d-k}, a_{d-k}) - C(a),\\]\nwhere \\(\\sum_{k=1}^{K} R(s_{d-k}, a_{d-k})\\) represents the accumulated reward along the backtracked path, \\(R(s_{d-K-1}, a'_{d-K-1})\\) captures the reward from the new path, and \\(C(a)\\) indicates the cost of the backtrack action.\n\u2022 Terminate: When the policy selects the terminate action to end the reasoning process, let \\(s_d\\) denote the final reasoning path at the terminal node, the reward is\n\\[R(s, a) = R(s_d) - C(a),\\]\nwhere \\(R(s_d)\\) measures the overall quality or correctness of the reasoning chain at the terminal node, and \\(C(a)\\) represents the cost of concluding the search. The reward \\(R(s_d)\\) can be derived from the commonly used outcome reward model (ORM), and we can even compare the final answer to the ground truth, as the reward is only used during the training of the PGTS policy.\nThe cost function \\(C(a)\\) plays a pivotal role in guiding the PGTS policy by assigning penalties to actions based on their computational or logical expense. These costs act as a mechanism to balance efficiency and reasoning quality. Actions such as expand and branch typically incur lower costs, as they change the reasoning tree locally. In contrast, the backtrack actions involve higher cost, discouraging excessive reversions unless the potential improvements justify the expense. The terminate action, which concludes the reasoning process and avoids further resource usage, generally carries negligible or zero cost. In our implementation, \\(C(a)\\) is treated as a hyperparameter that can be tuned to align the system's behavior with task-specific requirements or computational constraints. By appropriately setting these costs, the system fosters a balance between computational efficiency and reasoning complexity. This cost structure works in tandem with the reward function, which prioritizes accurate, coherent, and relevant reasoning paths while penalizing unproductive exploration. Together, the rewards and costs enable the PGTS policy to navigate reasoning trees effectively, balancing exploration and exploitation, and adapting flexibly to diverse tasks and constraints."}, {"title": "2.3.4. POLICY DESIGN FOR PGTS", "content": "The effectiveness of PGTS heavily depends on its ability to process and navigate reasoning tree. In our TS-MDP formulation, the state is naturally represented as a tree where each node corresponds to a partial reasoning path. To effectively learn from this structured data, we need a policy architecture that can capture both local relationships between reasoning steps and global patterns across the entire tree.\nWe design our policy using a graph-based architecture that processes two types of features. Node features are derived from the hidden states of the target LLM, capturing the semantic content of each reasoning step. Edge features encode the intermediate rewards of reasoning transitions, quantifying the quality and relationships between consecutive steps. This rich feature representation enables the policy to assess both the local quality of individual steps and their contribution to the overall reasoning path.\nTo process these features effectively, we employ a Graph Transformer-based model using the GPS framework (Ramp\u00e1\u0161ek et al., 2022). This architecture combines local message passing operations from graph neural networks with global attention mechanisms from transformers. The local operations capture step-by-step reasoning relationships, while global attention helps maintain coherence across the entire reasoning process. We further enhance the struc-"}, {"title": "2.3.5. TRAINING", "content": "tural understanding by incorporating random-walking structure embeddings (Dwivedi et al., 2021), which encode each node's position and connectivity within the tree.\nThe policy network processes these inputs through a sequence of GPS layers, each layer aggregating both local and global information. The final representation of the current node is concatenated with the action constraints vector to ensure awareness of valid actions. This combined representation then passes through linear layers to produce logits for a categorical distribution over the \\(D+2\\) possible actions. Invalid actions are masked out during sampling, ensuring the policy's decisions respect the tree's structural constraints.\nWe implement the value network using the same architectural backbone, sharing the GPS layers with the policy network to maintain consistent representation learning. The value network differs only in its final layers, which produce a scalar estimate of the expected cumulative reward. This shared structure allows both networks to leverage the same learned representations of the reasoning tree while serving their distinct purposes in the decision-making process.\nThe training process for the PGTS policy aims to enhance reasoning effectiveness while minimizing unnecessary exploration. This is accomplished by optimizing the policy through reinforcement learning, with rewards designed to promote both accuracy and efficiency. The training iteratively refines the policy's ability to navigate reasoning trees and select actions that yield high-quality solutions.\nWe adopt Proximal Policy Optimization (PPO) (Schulman et al., 2017) as our training algorithm due to its stability and sample efficiency. Starting from randomly initialized weights, the policy interacts with reasoning tasks in episodes, learning to select actions that maximize cumulative rewards. When sampling actions during training, we apply the constraint mask to zero out logits of invalid actions, ensuring the decisions respect the tree's structural limits. The remaining logits are normalized to form a valid categorical distribution over permitted actions. This mechanism allows the policy to learn feasible exploration strategies while maintaining the integrity of the reasoning tree.\nTo encourage efficient exploration, we incorporate entropy regularization into the policy loss. This ensures that the policy maintains a balance between exploiting known high-reward paths and exploring less certain but potentially rewarding alternatives. Additionally, the cost components \\(C(a)\\) in the reward function are tuned to discourage excessive backtracking or branching, guiding the policy toward concise, meaningful reasoning paths. Please refer to Algorithm 1 for the complete training procedure."}, {"title": "3. Related Works", "content": "LLM Reasoning LLM reasoning has advanced through techniques such as CoT (Wei et al., 2022), ToT (Yao et al., 2024), and programmatic reasoning paradigms (Chen et al., 2022; Sel et al., 2023), fostering structured and iterative problem-solving. Recent innovations include heuristic search methods like MCTS (Feng et al., 2023; Hao et al., 2023) and A* search (Wang et al., 2024a). Building on these developments, our PGTS framework integrates learned policies to improve search efficiency and reasoning performance. For a detailed review of related approaches and their connection to inference-time scaling, please refer to Sec. B.\nGraph Transformers Graph Transformers (GTs) have emerged as powerful architectures for processing graph-structured data, building upon the success of Transformers in other domains. These models are particularly attractive due to their ability to address fundamental limitations of traditional Message Passing Neural Networks (MPNNs), such as over-smoothing and over-squashing issues (Alon & Yahav, 2020; Topping et al., 2021). Various GT architectures have been proposed, from the initial Fully-connected Graph Transformer with basic positional encodings (Dwivedi & Bresson, 2020), to more sophisticated designs like SAN with invariant eigenvector aggregation (Kreuzer et al., 2021), and Graphormer with distance-based encodings (Ying et al., 2021). GraphTrans (Wu et al., 2021) introduces the first hybrid architecture, which combines local message passing with global attention mechanisms. GPS (Ramp\u00e1\u0161ek et al., 2022) systematically investigates and integrates different components of GTs, offering a modular and scalable framework. In this work, we implement the PGTS policy using GPS layers given its ability to effectively combine local and global information while maintaining linear complexity."}, {"title": "4. Experiments", "content": "In this section, we showcase the flexibility and effectiveness of our PGTS framework across diverse problem domains, including mathematical reasoning, commonsense reasoning, logical reasoning, and real-world planning. For mathematical reasoning, we evaluate our framework on the GSM8K (Cobbe et al., 2021), MATH500 (Hendrycks et al., 2021; Lightman et al., 2023), and AQUA (Ling et al., 2017) datasets, using 4-shot settings for GSM8K and MATH500, and a 10-shot setting for AQUA. The in-context learning (ICL) examples are adapted from OpenCompass (Contributors, 2023) for GSM8K and MATH500, and from LLM-Reasoner (Hao et al., 2024) for AQUA. For commonsense reasoning, we evaluate StrategyQA (Geva et al., 2021) in a 5-shot setting, with ICL examples also adapted from Open-Compass. For logical reasoning, we evaluate the PrOntoQA (Saparov & He, 2022) dataset for logical deduction in a 5-shot setting and the GPQA (Rein et al., 2023) dataset for"}, {"title": null, "content": "graduate-level multiple-choice questions in a 0-shot setting. Finally, for the planning task, we evaluate our framework on the Blocksworld benchmark (Valmeekam et al., 2022).\nAcross all datasets, we define a single reasoning step as one sentence, maintaining consistency and simplicity. Unlike RAP (Hao et al., 2023), which incorporates a world model to simulate environment states after each action, our approach directly focuses on reasoning in the generated text without state modeling. For detailed dataset descriptions and reasoning setups, refer to Sec. C.\nWe compare PGTS against CoT and MCTS baselines. MCTS inherently explores multiple reasoning chains by traversing different paths during search, while we enhance CoT and our PGTS with self-consistency (SC) (Wang et al., 2022). Specifically, CoT aggregate outcomes from multiple chains using majority voting, whereas MCTS and PGTS utilize weighted voting based on the reward of each reasoning chain. Additionally, we report MCTS results for the highest-reward trajectory. Since one of our primary goals is to reduce reasoning costs, we exclude MCTS approaches that rely on self-evaluation as intermediate rewards (Xie et al., 2024a; Gao et al., 2024), as they are computationally expensive. Instead, we simply use the likelihood of each reasoning step as the intermediate reward, \\(R(s, a)\\). An oracle setting is included for MCTS, allowing it to access task rewards by comparing generated answers with ground truths during search. For both MCTS and PGTS approaches, the tree breadth is limited to 4 child node per parent. For detailed descriptions of the baselines, refer to Sec.D, and for ablations on the reasoning tree constraints, see Sec.4.1.\nFor PGTS, we train the policy using up to 1,000 examples from the training split of each dataset. This highlights the sample efficiency of our approach, as 1,000 examples suffice to learn an effective policy. For ablations on training settings, refer to Sec. 4.1. The policy architecture consists of two GPS layers followed by a single linear layer for action and value prediction. To simplify experiments, the action cost \\(C(a)\\) is fixed across datasets, with values of 0.1, 0.2, 0.5, and 0.0 for expand, branch, backtrack, and terminate actions, respectively. While dataset-specific tuning could further enhance performance, we leave this for future exploration. See Sec. E for training details of our PGTS policy."}, {"title": "4.1. Ablations", "content": "Training Examples In the main results, we train the PGTS policy using up to 1,000 examples. Here, we demonstrate the convergence behavior of the training process. Figure 3 presents the training curve with evaluation results at intermediate checkpoints, showing that the policy converges quickly and plateaus at approximately 1,000 examples.\nTree Constraints In our main results, we limit the tree breadth to 4 child nodes per parent. Table 2 shows evaluation results on AQUA with varying tree breadths. Both MCTS and PGTS achieve better performance with broader trees, as expected, since a broader tree facilitates more diverse reasoning chains. However, broader trees also generate more tokens, so we use a breadth of 4 in our main results to balance accuracy and reasoning cost. For tree depth, we set an upper bound that ensures all examples can reach the terminal state (i.e., the final answer), with the terminate action allowing early stopping as needed.\nPolicy Network We implement our PGTS policy using GPS layers, which combine local message passing and"}, {"title": "5. Conclusion", "content": "global attention to extract node features effectively. The reasoning tree also incorporates the immediate reward \\(R(s, a)\\) as edge features to inform decision-making. Table 3 presents ablation studies evaluating the importance of each component in the GPS-based policy and comparing it to alternative policy implementations. SAN replaces GPS layers with a different graph transformer architecture proposed in (Kreuzer et al., 2021). SLM replaces the graph-based policy with a small language model (distilbert-base-uncased), which processes the previous reasoning trajectory to predict the next action. LLM Agent prompts the same target LLM to predict the exploration action, aligning with an agentic approach where the LLM assumes the role of an autonomous agent collaborating with others to solve the problem. Details of these policy implementations are provided in Sec. F. The ablation results demonstrate that the full GPS policy achieves the best performance, highlighting the effectiveness of integrating local message passing, global attention, and edge features. Removing edge features or global attention results in significant performance drops, especially on AQUA, emphasizing their importance for reasoning. While SAN performs competitively, it falls short of GPS, suggesting that\nIn this work, we introduced Policy-Guided Tree Search (PGTS), a novel framework for reasoning with large language models that combines the efficiency of policy-guided exploration with the structured advantages of tree search. PGTS dynamically allocates inference resources, prioritizing promising reasoning paths for targeted exploration, leading to significant improvements in inference efficiency and the ability to tackle complex reasoning tasks. Moreover, PGTS addresses the \"overthinking\" problem commonly observed in many o1-like models (Chen et al., 2024c), where excessive reasoning steps are generated for simple problems.\nA key paradigm shift in PGTS is treating the LLM as an environment rather than a policy, enabling external decision-making components to guide reasoning processes more effectively. While the current implementation uses simple log-likelihood-based rewards as intermediate feedback, future work could incorporate more sophisticated reward mechanisms, such as LLM self-evaluation or task-specific metrics, to provide richer guidance during the search and further enhance reasoning performance. PGTS represents a significant step toward more efficient and structured inference-time reasoning with LLMs."}, {"title": "A. Training Algorithm", "content": "{\\it Require:} Reasoning tasks T, depth limit D, breadth limit B, reward function R(s, a), cost function C(a)\n{\\it Ensure:} Trained policy \\(\\pi(as)\\) and value function V(s)\n{}\\it Initialize: Parameters of policy \\(\\pi_{\\phi}\\) and value function \\(V_{\\psi}\\)\n{}\\it while\\) not converged do\n{}\\it Sample\\) a task from T, and initialize the root node \\(s_0\\)\n{}\\it for\\) t = 0, 1, ..., Tmax do\n{}\\it Extract\\) valid actions and construct the constraints vector c\n{}\\it Compute\\) policy logits \\(\\pi \\leftarrow \\pi_{\\phi}(s_t, c)\\)\n{}\\it Sample\\) action \\(a_t \\sim Categorical(\\pi)\\)\n{}\\it Execute\\) action \\(a_t\\) and observe next state \\(s_{t+1}\\), reward \\(r_t\\), and constraints \\(c_{t+1}\\)\n{}\\it Store\\) transition \\((s_t, a_t, rt, s_{t+1}, c_{t+1})\\) in replay buffer B\n{}\\it if \\(a_t\\) is Terminate then\n{}\\it Break\\)\n{}\\it end if\\)\n{}\\it end for\\)\n{}\\it Compute\\) Returns and Advantages:\n{}\\it for\\) each sampled trajectory in B do\n{}\\it Compute\\) discounted returns: \\(G_t = \\sum_{j=0}^{T-t-1} \\gamma^j r_{t+j}\\)\n{}\\it Compute\\) advantages: \\(A_t = G_t - V_{\\psi}(s_t)\\)\n{}\\it end for\\)\n{}\\it Policy Update: \\(\\ \\Compute\\) policy loss \\(L_{\\pi} = -E[log \\pi_{\\phi}(a_t | s_t, c_t) \\cdot A_t] - \\omega \\cdot H (\\pi_{\\phi}(a_t | s_t, c_t))\\)\n{}\\it Update\\) policy parameters: \\(\\phi \\leftarrow \\phi - \\eta_{\\pi} \\nabla_{\\phi} L_{\\pi}\\)\n{}\\it Value Function Update: \\(\\ \\Compute\\) value loss \\(L_V = E[(V_{\\psi}(s_t) - G_t)^2]\\)\n{}\\it Update\\) value parameters: \\(\\psi \\leftarrow \\psi - \\eta_{V} \\nabla_{\\psi} L_{V}\\)\n{}\\it end while\\)"}, {"title": "B. Related Works", "content": "LLM Reasoning In the context of LLM reasoning, Chain-of-Thought (CoT) resoning (Wei et al., 2022) serves as"}]}