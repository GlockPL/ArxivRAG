{"title": "Towards a Problem-Oriented Domain Adaptation Framework for Machine Learning", "authors": ["Philipp Spitzer", "Dominik Martin", "Laurin Eichberger", "Niklas K\u00fchl"], "abstract": "Domain adaptation is a sub-field of machine learning that involves transferring knowledge from a source domain to perform the same task in the target domain. It is a typical challenge in machine learning that arises, e.g., when data is obtained from various sources or when using a data basis that changes over time. Recent advances in the field offer promising methods, but it is still challenging for researchers and practitioners to determine if domain adaptation is suitable for a given problem-and, subsequently, to select the appropriate approach. This article employs design science research to develop a problem-oriented framework for domain adaptation, which is matured in three evaluation episodes. We describe a framework that distinguishes between five domain adaptation scenarios, provides recommendations for addressing each scenario, and offers guidelines for determining if a problem falls into one of these scenarios. During the multiple evaluation episodes, the framework is tested on artificial and real-world datasets and an experimental study involving 100 participants. The evaluation demonstrates that the framework has the explanatory power to capture any domain adaptation problem effectively. In summary, we provide clear guidance for researchers and practitioners who want to employ domain adaptation but lack in-depth knowledge of the possibilities.", "sections": [{"title": "1. Introduction", "content": "In machine learning (ML), domain adaptation refers to approaches that bridge the gap between two problems that share the same task but differ in the feature distribution (Zhuang et al., 2020). Domain adaptation aims to leverage knowledge about one problem (i.e., the source domain) to find a solution for the other problem (i.e., the target domain). Transferring knowledge from one domain to another is often desired in practical applications of supervised learning, e.g., if labeled data in the target domain can hardly be collected or is more expensive than in the source domain (Kouw and Loog, 2021).\nIn autonomous driving, for instance, reliable object recognition models to navigate the self-driving car and detect pedestrians, other cars, traffic signs, etc. are required. Typically, however, there is only a limited number of annotated pictures of road scenes available to train a model in a supervised manner, as manually labeling these pictures is expensive. Instead, synthetic data, i.e., rendered traffic scenes, are easy to generate and, thus, cheap (Cordts et al., 2016). However, usually, models tend to perform well on these synthetic data, but the performance drops significantly when applied to real-world images caused by a domain shift, i.e., the synthetic pictures do not exactly behave or look like the ones from the real-world (Hoffman et al., 2018).\nEven if researchers or ML practitioners suspect to profit from domain adaptation for a given problem, there is little actionable guidance on selecting a promising approach from the ever-growing pool of domain adaptation techniques and implementations. Thus, we propose a problem-oriented domain adaptation framework that enables users to identify and classify domain adaptation problems, delivers guidance on suitable solution approaches, and helps to pinpoint reasons for ill-performing domain adaptation attempts. The framework is developed and presented according to the design science research (DSR) paradigm as described by (Hevner et al., 2004) to ensure a user-centered design of the artifact. Furthermore, by drawing on the DSR paradigm, we acknowledge the importance of demonstrating practical implications in real-world contexts while ensuring theoretical rigor. This alignment with DSR allows us to effectively bridge the gap between theory and practice, making our framework valuable for both researchers and practitioners dealing with domain adaptation scenarios (Hevner, 2007)."}, {"title": "1.1. Theoretical and Practical Relevance", "content": "Expanding from the motivational examples at the beginning, in this section, we highlight the importance of domain adaptation in computer science (CS) and information systems (IS) as well as the relevance of actionable decision support in the application domains.\nWe highlight the theoretical relevance by connecting current state-of-the-art theories and algorithms in CS to the application of domain adaptation, which is a special case of transfer learning. Transfer learning is an important aspect"}, {"title": "1.2. Research Objectives and Scope", "content": "Domain adaptation serves as a powerful ML technique for utilizing data from a similar yet distinct domain, where data is more readily accessible than in the original domain, to solve a task in the original domain. The efficacy of domain adaptation is highly dependent on domain characteristics, and the successful application of domain adaptation methods often requires a deep understanding of both the domain and the underlying theory (Wilson and Cook, 2020).\nAs reasoned in Section 1.1, it is reasonable to assume that researchers and practitioners may be hesitant to employ domain adaptation due to the resources required for evaluating its potential benefits and subsequently applying it to"}, {"title": "1.3. Contributions", "content": "To summarize the contributions of this work, the proposed framework significantly advances the field of domain adaptation in machine learning by delivering a novel, problem-oriented domain adaptation perspective. They can be summarized as follows:\n1. Development of a Comprehensive Framework: We introduce a structured framework that categorizes domain adaptation into five distinct scenarios, providing clear and actionable guidelines to assist in determining the suitability of domain adaptation for specific problems, a feature notably lacking in the existing literature. Thus, we support practitioners and researchers in the field alike.\n2. Empirical Validation: The proposed framework has been rigorously evaluated across multiple datasets and an experimental study involving 100 participants, demonstrating its robustness and versatility in various domain adaptation contexts.\n3. Bridging Theory and Practice: Our approach is grounded in rigorous theoretical research while also being keenly attuned to practical applications, effectively bridging the gap between theoretical concepts and real-world applicability. By providing a clear categorization and decision-making process, our framework makes the complex field of domain adaptation more accessible to a broader audience, including those with limited background in the area.\nThese contributions collectively represent a significant step forward in the domain adaptation domain, offering both theoretical insights and practical tools for researchers and practitioners."}, {"title": "2. Related Work", "content": "Domain adaptation is a sub-field of transfer learning. Zhuang et al. (2020) published a survey on transfer learning, which discusses numerous approaches and serves as a starting point. This survey builds on previous surveys by Weiss et al. (2016); Pan and Yang (2010) on the same topic. Zhang and Gao (2020) provide another survey on transfer learning, focusing on the historical development of solution approaches. More specific surveys on domain adaptation include single-source unsupervised cases (Kouw and Loog, 2021) and deep domain adaptation techniques (Wang and Deng, 2018; Wilson and Cook, 2020). Additionally, Wilson and Cook (2020) offer the most comprehensive overview of empirical results for various deep domain adaptation approaches and problems.\nMoreover, there are plenty of articles presenting different approaches to domain adaptation, of which deep domain adaptation (i.e., domain adaptation using deep neural networks) is the most prominent (Wu et al., 2022; Liu et al., 2024; Jim\u00e9nez-Guarneros and G\u00f3mez-Gil, 2021), for instance, to cope with distribution shifts of electrocardiograms He et al. (2023). However, this short summary does not aim to be a comprehensive overview of methods present in recent literature. The articles listed here should be understood as representatives for a multitude of implementations. The idea of using adversarial domain adaptation gained traction after the proposal of \"Domain-Adversarial Training of Neural Networks\" by Ganin et al. (2016). Because adversarial training yielded promising results for many domain adaptation tasks, various modern approaches picked up the idea. Examples of this are Co-DA (Kumar et al., 2018), CyCADA (Hoffman et al., 2018) (cf. Figure 2) or DIRT-T (Shu et al., 2018). The authors in Hong et al. (2024) improve the performance of entropy-based adversarial networks by supplementing the discriminator with additional probability values. Their results show superior performance to state-of-the-art methods. In a similar work of Lee et al. (2023), the authors present an approach to tackle the simultaneous occurrence of domain shift and class imbalance based on adversarial networks. Through various evaluations, they show that their method, based on learning domain-invariant features and applying a label-aligned sampling strategy, improves the performance compared to conventional methods. Nevertheless, there are also deep domain adaptation approaches that are not based\non adversarial learning, e.g., French et al. (2018) use self-ensembling which has delivered at least state-of-the-art performance for some popular scenarios. The study Taghiyarrenani et al. (2023) presents a method to cope with distribution shifts in regression tasks. Their method builds on Siamese neural networks and shows its effectiveness in multi-domain settings. Similarly, Liu et al. (2022) outline how to deal with domain shifts in remote sensing domains. Their approach improves the performance of state-of-the-art models in cross-domain change detection. Next to deep domain adaptation frameworks, there are approaches that do not rely on deep neural networks. Instances of these are class-based reweighting (Saerens et al., 2002) or sample-based reweighting (Huang et al., 2006) for correcting sample selection bias for unlabeled data.\nFurther articles explore domain adaptation by examining the statistical properties of domain shifts more closely. Kouw and Loog (2019) employ the same definitions for classifying dataset shifts based on Moreno-Torres (2012) as we do in this work. However, their application of these principles is less rigorous, as they do not place the same emphasis on the causality of the underlying problem. In addition, Zhang et al. (2013) provides another example of applying dataset shift definitions in domain adaptation.\nRecent works provide a comprehensive set of methods to deal with domain shifts in various settings. However, the question of when to apply domain adaptation is highly relevant for the successful use and implementation of these methods in real-life settings. Few works provide guidance for the successful application of domain adaptation methods Kouw and Loog (2021, 2019); Ben-David et al. (2007); Zhao et al. (2019); Bashath et al. (2022); Chang et al. (2020). The work of Chang et al. (2020) informs about the use of unsupervised domain adaptation methods for wearable sensors. In their work, the authors provide guidelines for practitioners on how to apply domain adaptation approaches in this domain. Furthermore, both works by Kouw et al. explore when to apply domain adaptation from a theoretical perspective (Kouw and Loog, 2019, 2021). Error bounds are another way to define the suitability of certain domain adaptation approaches. Bounds for domain adaptation are presented by Ben-David et al. (2007) or Zhao et al. (2019), of which the latter is significantly more tailored towards modern domain adaptation. In the work of Bashath et al. (2022), the authors present a data-centric perspective of deep learning models in text data domains. They provide a new nomenclature and taxonomy of deep learning approaches for the application of domain adaptations with text data.\nLooking at domain adaptation from an implementation perspective, there is a framework called \"Semi-supervised Adaptive Learning Across Domains\", in short SALAD, that provides tools to perform experiments for some domain adaptation approaches (Schneider et al., 2018). It was last updated in 2018. Besides a pre-release of the same authors for domain adaptation toolbox that is especially tailored towards ImageNet (Bethge Lab, 2021), we could not find any further studies that are targeted towards ML practitioners and are sufficiently broadly applicable."}, {"title": "3. Methodology", "content": "The underlying research paradigm which is used for this work is DSR. DSR is an outcome-based methodology that focuses on generating knowledge via the design or improvement of usable artifacts. It was first developed by Simon (1996) and later refined by among others Hevner and Chatterjee (2010) and van Aken (2005). Hevner and Chatterjee (2010) describe DSR as \u201ca research paradigm in which a designer answers questions relevant to human problems via the creation of innovative artifacts, thereby contributing new knowledge to the body of scientific evidence\" (Hevner and Chatterjee, 2010, p. 5).\nAt its core, this classifies DSR as constructive research that can be differentiated from explanatory research. In explanatory research, practical implications are often derived as a last step based on theoretical foundations. The decoupling of practice and theory meant that research can fail to provide any significant results for practical applications. A problem that was often encountered during the early days of CS (Iivari, 2005). As a result, researchers and practitioners developed a new approach that focuses on putting the product or the solution to an actual problem first and then derives knowledge and novel theories from the construction of the product, which in DSR terms is referred to as an artifact. Putting the product first is the essence of constructive research and has shown to be a valid strategy for tackling complicated problem settings.\nThe general design cycle was first analyzed by Takeda et al. (1990) and later adapted by Vaishnavi (2007) to specifically target DSR. The design phase of the artifact follows the notion of the design cycle and the DSR methodology as proposed by Peffers et al. (2007). Both approaches require (intermediate and final) evaluation to guide the development and measure the suitability of the final artifact. A framework for constructing such evaluation episodes (EEs) in DSR is proposed by Venable et al. (2016). Its implementation for this article is explained in Section 5. The general intuition is that formative artificial evaluation is used throughout the design to guide the development and more naturalistic and summative evaluation scenarios are used to test the result of the development.\nGiven the exploratory nature of our research, we have adopted an iterative design approach, linking intermediate findings and including technical evaluations to validate the design. Figure 4 delineates our research methodology, which encompasses five iterative steps: 1) identifying potential domain adaptation scenarios from empirical evidence and theory, 2) selecting effective algorithms or types for each identified scenario, 3) creating testable descriptions\nfor these scenarios, 4) developing a framework that coherently integrates scenarios with solution strategies based on solid theoretical grounds, and 5) evaluating this framework through both descriptive and observational methods."}, {"title": "4. A Problem-Oriented Domain Adaptation Framework", "content": "The core artifact is the identification of four (respectively five) domain adaptation scenarios that are the result of applying Moreno-Torres' unified view on data shifts (Moreno-Torres, 2012) in the context of a ML problem, more precisely an unsupervised homogeneous domain adaptation problem as previously defined. This is not a completely new approach. For instance, in their survey of domain adaptation methodologies, Kouw and Loog (2019) acknowledge the perspective of Moreno-Torres (2012) regarding data shifts, although it is not strictly incorporated. Many sources do, if at all, only superficially mention the connection and focus much more on specific details in the respective domain adaptation approach. An example of this is \"Domain Adaptation Under Target And Conditional Shift\" by Zhang et al. (2013). During the research, it became more evident that in order to provide qualified support for practitioners, one cannot rely on a set of statistical markers, e.g., domain divergence via Jenson-Shannon divergence, alone, but must understand the characteristics of domain adaptation problems on a more basic level. This is because actual measures are confronted with a wide range of practical problems, e.g., the curse of dimensionality (Bellman, 1957), the difficulties in estimating probability distributions in regions with sparse sample coverage (Silverman, 1986), and extreme variations in the empirical divergence measures based on entropy for regions in which source\nand target differ significantly (Lee and Seung, 2000). As a result, a central question of the framework is: what is the causality of the system? This question is almost always overlooked in domain adaptation literature (Pearl, 2009). It is not only a core component of Moreno-Torres' view on data shift, but also highly important in interpreting what we know about a given domain adaptation problem correctly in order to choose a suitable solution approach as will be shown later in this section.\nThe concept of causality can be briefly explained as follows: Anyone familiar with ML expects a problem where a label or value y is predicted based on observable features x with the help of experience that is learned from samples. Often the samples are said to be created by an unknown system with joint probability P(x, y). In the real world, there is a connection present between x and y that is defined by the causal relationship. If X is the reason for Y, the notation is X \u2192 Y. An example of this is predicting the immediate failure probability y of an engine based on various sensor readings x. The alternative is a system where Y is the reason for X, i.e., Y \u2192 X. This is the case, if we try, e.g., to recognize animals y on image data x. Depending on the causality of the system, the joint probability P(x \u2229y) can be decomposed into\n\u2022 P(x \u2229 y) = P(y|x) P(x) for X \u2192 Y systems, or\n\u2022 P(x \u2229 y) = P(x|y) \u00b7 P(y) for Y \u2192 X systems.\nThe important fact is, that although we always predict values of Y based on observations of X, the causal reality can be the other way around.\nTable 1 on page 13 shows the basic domain adaptation scenarios (prior shift, covariate shift, concept shift, class-conditional shift, and data set shift) in accordance with the definitions by Moreno-Torres (2012) and how the probabilities change for each scenario. To help the reader differentiate between inferred and given properties, the properties given by the definition are written in bold letters. In the following sub-chapters, each scenario and the corresponding domain adaptation procedures will be introduced in detail. The suggested solution procedures for each scenario are summarized in Table 2 and support for determining the correct scenario is given in Table 3."}, {"title": "4.1. Prior Shift", "content": "A prior shift, or prior probability shift, appears only in Y \u2192 X problems and is defined as the case where Ps(x|y) = Pt(x|y) and Ps(y) \u2260 Pt(y) (Moreno-Torres, 2012). An example from ML for a prior shift is a binary computer vision task, in which images of dogs and cats must be correctly classified and the source and target domain differ in the share of dog pictures, e.g., in the source domain we have Ps(y = dog) = P(y = cat) = 0.5 and in the target domain Pt(y = dog) = 0.7 \u2260 Pt(y = cat) = 0.3. On the other hand, the class conditionals remain the same, i.e., in both domains, a picture of a dog looks the same."}, {"title": "4.2. Covariate Shift", "content": "A covariate shift appears only in X \u2192 Y problems and is defined as the case where Ps(y|x) = Pt(y|x) and Ps(x) \u2260 Pt(x) (Moreno-Torres, 2012). An example of a covariate shift in ML is learning from medical data about patients in a rural area to predict the likelihood of disease versus learning from medical data in a city hospital. In literature, this is also referred to as sample selection bias (Huang et al., 2006). If the disease probability only depends on certain risk factors of the patients, this concept P(y|x) will be constant in all domains. However, if we assume that the population in the rural area is older on average, this can mean that the proportion of patients with elevated risk factors is higher simply because of that. The difference compared to a prior shift is that the concept remains the same, which means that in theory the correct hypothesis can be learned from the source domain alone. However, in cases where the model used is not able to correctly represent the real concept, e.g., if we train a linear classifier to describe a non-linear problem, domain adaptation approaches can still be beneficial. This will be elaborated based on the error bound in Equation 4. From the perspective of a ML practitioner, prior shift and covariate shift can look the same for a new problem, because both result in shifts in the feature and the label space, but they require different solution procedures, and, therefore must be correctly identified.\nSolution procedures of covariate shifts share the general idea with solutions to the prior shift as both scenarios can be seen as parallel cases for different causal directions. Keeping in mind that only the feature distribution changes\nwhile the concept stays constant, the target risk can be described as a decomposition similar to the prior shift scenario.\nR\u2081(h) = \\displaystyle \\sum_{y \u2208 Y} \\int_X \u2113(h(x), y) \\frac{P_t(x, y)}{P_s(x, y)} P_s(x, y)dx\n(3)\nEven if these look almost the same, from a domain adaptation point of view there is a difference, because the weight function can be directly determined as we know the feature distribution of both source and target domain, and also the actual concept does not change between domains. What does this mean for a ML practitioner? If the classifier of the source domain is able to correctly approximate the real concept, there is no need for domain adaptation. This is the case if the classifier has the capacity to correctly learn the real concept, e.g., a linear classifier can learn a linear concept, a quadratic classifier can learn a quadratic concept, etc. For more complex problems, it is not always clear if a classifier actually learns correctly, i.e., is well-specified. Indicators of a well-specified model are high accuracy and good generalization to new problems of the same type. In this case, the error of the source model on the target domain will asymptotically converge towards the performance on the source domain with increasing sample size.\nIn the case of a misspecified model, e.g., a linear classifier on a non-linear concept, the real concept is not learned and therefore there is potential for increasing the performance by reweighting the samples. Intuitively, this allows the classifier that must make mistakes, to make the mistakes where they hurt the overall performance least. Theoretically, for a probability of 1 \u2212 \u03b4 for \u03b4 > 0 the difference between the error of the reweighted classifier \u0125w and the actual error on the target domain er for a classifier h is defined by the following upper bound\ner(h) - \u00eaw(h) \u2264 \\frac{2^{5/4} c^2 D_{2R}(P_t||P_s)^{1/2}}{n} + \\sqrt{\\frac{3}{8n}log(\\frac{4}{\u03b4})} + \\frac{4}{n}log(\\frac{4}{\u03b4})\n(4)\nwhere D2R(Pt||Ps) is the second-order R\u00e9nyi divergence from ??, c is the pseudo-dimension of the hypothesis space, and n is the sample size (Cortes et al., 2010). This bound shows us that for a fixed hypothesis space, more samples are needed to compensate for a higher difference between the domains. Considering the definition of the R\u00e9nyi divergence, the importance-weighted classifier will only converge, if the expected squared weight is finite, i.e., for w(x) = Pt(x)/Ps(x), it must hold that Es[w(x)\u00b2] < \u221e (Kouw and Loog, 2021). For a correctly specified model, as the sample size goes to infinity, a weighted estimator will converge to the optimal solution for any fixed set of non-negative weights that sums to 1 (White, 1981), which includes a non-weighted estimator.\nThere exist various approaches for reweighting, whereas an important differentiation is if the approach is parametrized, i.e., a certain feature distribution (e.g., a Gaussian distribution (Shimodaira, 2000)) is assumed, or"}, {"title": "4.3. Concept Shift", "content": "Technically, concept shift can occur in both causal directions and is therefore defined as either\n\u2022 Ps(y|x) \u2260 Pt(y|x) and Ps(x) = Pt(x) in X \u2192 Y problems, and\n\u2022 Ps(x|y) \u2260 Pt(x|y) and Ps(y) = Pt(y) in Y \u2192 X problems (Moreno-Torres, 2012).\nDespite the similarities, these two scenarios require completely different approaches to domain adaptation. Also, it is worth noting, that in a narrow sense concept shift in X \u2192 Y problems is not a case of domain adaptation per se, because the task is changing. This violates ??. In the literature, the term concept drift is often used for this scenario (Widmer and Kubat, 1996). To avoid possible confusion, the framework slightly deviates from the definitions of Moreno-Torres (2012) and reserves the term concept shift for X \u2192 Y scenarios, whereas a concept shift for Y \u2192 X will be called class-conditional shift (without the loss of generality concerning regression problems) and dealt with in Section 4.4.\nTo illustrate concept shift, we revisit the example in the introduction of this section. In the example, the failure probability of a machine is predicted based on various sensor readings. We can assume that even if the model is trained and performs well at the moment, at some point in the future the real failure probability will change, because of wear-down in the machine. So the model that was trained at the beginning of the life cycle might underestimate the failure probability for an older machine. In a broad sense, the task predict failure probability remains the same, however from a statistical point of view, because the parameters change explicitly, the task is also considered different. This is unlike other domain adaptation scenarios, where the task Pt(y|x) changes implicitly, because of another change that also affects the feature space probability P(x)."}, {"title": "4.4. Class-Conditional Shift", "content": "In contrast to concept shift in X \u2192 Y problems, a class-conditional shift is a highly relevant scenario for domain adaptation. This type of shift is often found if observational data of the same event, but from different sources must be integrated. Remember the example of image recognition of dogs and cats from previously, but now assume that the share of dogs and cats is equal in both domains. Instead, in the target domain all pictures are taken with a different kind of camera, or maybe at night. It is immediately evident, that this will change the class-conditional Pt(x|y) and\nit can be shown that this will also change the concept Pt(y|x) and the feature distribution Pt(x) as a result. Thus, we can express the relationship between the image created by a given y in the source domain and the image created in the target domain via a transformation t (cf. Proof in ??).\nThis means that there exists a transformation function that can be learned only in the feature space, i.e., without labels which also connects concepts of the source and the target domain. Finding this transformation function is the underlying task of all domain adaptation approaches for class-conditional shifts. Figure 9 shows that the transformation function can either map the source domain directly to the target domain or, alternatively, map both domains to a common domain-invariant space. A lower bound for the error of the latter procedure is given by Gong et al. (2016).\nThe error requires the definition of Conditional Invariant Components: Xci are d-dimensional components of X, obtained by transforming the data Xci = t(x), such that Ps(xci|y) = Pt(xci|y) (Gong et al., 2016). If the transformation t() is non-trivial and the target class-conditional distributions are assumed to be linearly independent of each other for any two classes, the upper bound can be expressed as (Kouw and Loog, 2021):\ne_T \u2013 e_\u015d \u2264 J_{1(0,\u03c0/2]}(\u03b8) + \\frac{2}{sin^2 \u03b8} J_{CI}\n(5)\nwhere es is the error on the transformed source data, 1 is the indicator function, and Jci = ||Es [\u03c6(t(x))]-ET[\u03c6(t(x))]||\u00b2 is the maximum mean discrepancy (MMD) divergence between the transformed data in the source and the target domain (Gong et al., 2016; Kouw and Loog, 2021). This bound tells us that for a perfect transformation function, i.e., if Jci = 0, the difference between the two errors will also be 0. Otherwise, the difference in error depends on how well the transformation is performed for each class c \u2208 y, which is expressed via the angle @ between two Kronecker delta kernels (Gong et al., 2016). For too dissimilar transformed distributions, i.e., if the transformation function t() is not well selected, \u03b8 = \u03c0 is true for the angle and the MMD divergence cannot be used as an upper bound for the error anymore.\nThe theoretical validity of a solution for a class-conditional shift is given above, but we have not yet established actual solution procedures. To strike a balance between practice and theory without getting lost in the details of state-of-the-art approaches, we will briefly elaborate on established views. For deeper insights, we refer to surveys of Kouw and Loog (2021) for general unsupervised domain adaptation and Wilson and Cook (2020) for unsupervised deep domain adaptation, which describes the state of the art for domain adaptation in computer vision. To reiterate: the goal of all solution procedures in this scenario is to approximate a transformation function between the source and the target feature space. If this is done directly, it is called subspace mapping. Usually, the details of the actual transformation are less interesting in practice because the procedure should also work for other but similar target domains. In that case, there is a common domain defined that is reachable via source and target domain with respective transformations. These procedures are commonly referred to as finding domain invariant spaces (Kouw"}, {"title": "4.5. General Data Set Shift", "content": "All combinations of shifts that are not captured in the first four rows of Table 1 are defined as general data set shifts or simply other types of data set shifts, this includes mainly (Moreno-Torres, 2012):\n\u2022 Ps(y|x) \u2260 Pt(y|x) and Ps(x) \u2260 Pt(x) in X \u2192 Y problems, and\n\u2022 Ps(x|y) \u2260 Pt(x|y) and Ps(y) \u2260 Pt(y) in Y \u2192 X problems.\nAnother way to look at general data set shifts is to see these scenarios as combinations of the two basic scenarios depending on the causality of the system. Moreno-Torres (2012) states that possible reasons for the rarity of these types of shifts in current literature are that they appear not as often as the basic variants and that they simply are too hard to solve. At least the first point does not hold for domain adaptation, because general data set shifts can easily occur in practice. For example, let us revisit the problem of discriminating between pictures of cats and dogs with a binary classifier for the last time. One can easily imagine a scenario in which the target differs from the source in both the prior probabilities, i.e., the different share of cats versus dog images, and in the class conditionals, i.e., different cameras or picture qualities. The second point, namely the difficulty of these problems, holds still true.\nIn the case of a general data set shift, none of the solution procedures above can be expected to work as before, because for each of them at least one assumption concerning the equality of certain distributions, is violated. For purely statistical methods, e.g., reweighting in the case of a covariate or prior shift, this is completely true. For a class-conditional shift, it is possible that depending on the extent of the prior shift, certain approaches will still deliver adequate results. As a suggestion, for general data set shifts in Y \u2192 X scenarios, the same solution procedures as for a class-conditional shift can be applied, but following the line of argumentation from the error bound in Equation 7, a performance drop can occur, because the real concept cannot be learned from aligning the feature space in source and target domain. Intuitively, a reasonable assumption is that the more accurate the transformation function and the smaller the prior shift, the better the performance of the approach.\nZhao et al. (2019) formalized this reasoning and constructed an upper and lower bound for learning domain invariant features, i.e., domain adaptation for class-conditional shifts, under different label distributions. Consider\nthe following upper bound.\ne_T(h) \u2264 e_\u015d(h) + d_H(D_S, D_T) + 2Rad_S(H) + 4Rad(H) +\nmin{E_{D_S}[[f_S \u2013 f_T]], E_{D_T}[[f_S \u2013 f_T]]} + O(\\sqrt{log(1/\u03b7)/n})\n(6)\nwhere H := {sgn(|h(x) \u2013 h'(x)| \u2013 t|}h,h' \u2208 H, t \u2208 [0,1]} and Rads(H) denotes the empirical Rademacher complexity. The bound is not discussed in detail here. More important is that three major components of the upper bound can be identified: the part of the error that belongs to the domain adaptation setting, e.g., the empirical source error and the shift between the label distributions, the part of the error that stems from the complexity measures in the hypothesis spaces and the error that is caused by only having finite samples (Zhao et al., 2019).\nThe lower bound for the difference in the errors is also interesting because it describes which performance can be expected in the best case for any domain adaptation based on domain invariant representations:\nes(ho g) + e_T(ho g) \u2265 1/2(ds (D_S^Y, D_T^Y) - d_JS (D_S^Z, D_T^Z))\u00b2\n(7)\nIn this equation, following the notation of Zhao et al. (2019), dJS is the Jensen-Shannon distance and the functions g and h represent the domain-invariant learning as a Markov chain:\nX \u2192 Z \u2192 Y\n(8)\nwhere Y = h(g(X)) is the predicted random variable of interest, Z = g(X) is the domain invariant feature space, and X is the original domain. This lower bound shows that for sufficiently different label distributions, minimizing the source error es(ho g) and aligning the distributions in the domain invariant feature space by minimizing djs (D3, D4) will only increase the target error (Zhao et al., 2019). The error bounds show that for a general data set shift, there is no guarantee for a domain adaptation approach to work if there is a sufficiently large difference in the marginal label distributions, or if the domains are too different, if the real concept is too complex or if the finite samples are too limited.\nMany articles that present new algorithms do not state the performance in the case of unbalanced classes so it is difficult to make exact statements. A possible direction of research is to evaluate the performance of the state-of-the-art approaches for problems that are categorized by the here presented framework. Lastly, it should be noted that not all solution procedures necessarily rely on risk minimization and some approaches are naturally robust against certain types of shifts. Support vector machines (Cortes and Vapnik, 1995) are a popular example because they maximize"}, {"title": "4.6. Scenario Determination", "content": "From a practitioner's point of view, one of the first questions is likely to be whether or not a given problem is suitable for domain adaptation at all. While there is never a guarantee that a specific domain adaptation approach yields the desired results, at the start of a project it can be helpful to have an indicator if the problem fits at least in the correct class of problems. This is why this section elaborates on how to determine if a ML problem belongs to one of the scenarios described in Table 1. For the general case, domain adaptation is probably too complex to ever find a closed set of statistical tests that can precisely pinpoint which approach is suitable. Although domain knowledge remains the most important factor in determining which scenario a problem belongs to, some of these scenarios can also be determined by a statistical test. This, however, usually assumes that the causality of the"}]}