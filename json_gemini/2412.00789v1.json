{"title": "A Cognac SHOT TO FORGET BAD MEMORIES: CORRECTIVE UNLEARNING IN GNNS", "authors": ["Varshita Kolipaka", "Akshit Sinha", "Debangan Mishra", "Sumit Kumar", "Arvindh Arun", "Shashwat Goel", "Ponnurangam Kumaraguru"], "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML applications on graph data. As graph data does not follow the independently and identically distributed (i.i.d) assumption, adversarial manipulations or incorrect data can propagate to other datapoints through message passing, deteriorating the model's performance. To allow model developers to remove the adverse effects of manipulated entities from a trained GNN, we study the recently formulated problem of Corrective Unlearning. We find that current graph unlearning methods fail to unlearn the effect of manipulations even when the whole manipulated set is known. We introduce a new graph unlearning method, Cognac, which can unlearn the effect of the manipulation set even when only 5% of it is identified. It recovers most of the performance of a strong oracle with fully corrected training data, even beating retraining from scratch without the deletion set while being 8x more efficient. We hope our work guides GNN developers in fixing harmful effects due to issues in real-world data post-training.", "sections": [{"title": "INTRODUCTION", "content": "Graph Neural Networks (GNNs) are seeing widespread adoption across diverse domains, from rec- ommender systems to drug discovery (Wu et al., 2022; Zhang et al., 2022). Recently, GNNs are being scaled to large training sets for graph foundation models (Mao et al., 2024). However, in these large-scale settings, it is prohibitively expensive to verify the integrity of every sample in the training data, which can potentially affect desiderata like fairness (Konstantinov & Lampert, 2022), robustness (Paleka & Sanyal, 2023; G\u00fcnnemann, 2022) and accuracy (Sanyal et al., 2021).\nMaking the training process itself robust to minority populations (G\u00fcnnemann, 2022; Jin et al., 2020) is challenging and can have adverse effects on fairness and accuracy (Sanyal et al., 2022). Conse- quently, model developers may want post-hoc ways to remove the adverse impact of manipulated training data if they observe problematic model behavior on specific distributions of test-time inputs. Such an approach follows the recent trend of using post-training interventions to ensure models be- have in intended ways (Ouyang et al., 2022). Recently, Goel et al. (2024) formulated Corrective Unlearning as the challenge of removing adverse effects of manipulated data with access to only a representative subset for unlearning while being agnostic to the type of manipulations. We study this problem in the context of GNNs, which face unique challenges due to the graph structure. The traditional assumption of independent and identically distributed (i.i.d.) samples does not hold for GNNs, as they use a message-passing mechanism that aggregates information from neighbors. This process makes GNNs vulnerable to adversarial perturbations, where modifying even a few nodes can propagate changes across large portions of the graph and result in widespread changes in model"}, {"title": "RELATED WORK", "content": "Graph-based attacks, such as Sybil (Douceur, 2002) and link spam farms (Wu & Davison, 2005), have long affected the integrity of social networks and search engines by exploiting the trust in- herent in node identities and edge formations. Recent works reveal that even state-of-the-art GNN architectures are vulnerable to simple attacks on the trained models which either manipulate exist- ing edges and nodes or inject new adversarial nodes (Sun et al., 2019; Dai et al., 2018; Z\u00fcgner & G\u00fcnnemann, 2019; Geisler et al., 2024). Parallelly, works have characterized the influence of spe- cific nodes and edges that can guide such attacks (Chen et al., 2023). One strategy to mitigate the influence of such attacks is robust pretraining, such as using adversarial training (Yuan et al., 2024; Zhang et al., 2023). Post-hoc interventions like unlearning act as a complementary layer of defense, helping model developers when attacks slip through and still adversely affect a trained model.\nRemoving the impact of manipulated entities begins with their identification (Brodley & Friedl, 1999), for which multiple strategies exist, like data attribution (Ilyas et al., 2022), adversarial detec- tion, and automated or human-in-the-loop anomaly detection (Northcutt et al., 2021). Once iden- tified, various approaches have been proposed to edit models to remove the effects of the manipu- lated data, including model debiasing (Fan et al., 2024) and concept erasure (Belrose et al., 2023). While these approaches have similar goals as unlearning of post-hoc removing undesirable effects of corrupted training data, unlearning attempts to do this without precise knowledge of the nature of corruptions and its effects. This is useful in adversarial settings where effects can be obfuscated, and harm multiple desiderata simultaneously (Paleka & Sanyal, 2023).\nMachine unlearning gained initial interest for privacy applications to serve user data deletion re- quests (Nguyen et al., 2022). Exact Unlearning procedures remove or retrain parts of the ML sys- tem that saw the data to be deleted, guaranteeing perfect unlearning by design (Chen et al., 2022b;a; Bourtoule et al., 2021). However, they can incur exponential costs with sequential deletion requests (Warnecke et al., 2023). Therefore, we focus on Inexact Unlearning methods, which either provide approximate guarantees for simple models (Chien et al., 2022; Wu et al., 2023b) or like us, empiri- cally show unlearning through evaluations for deep neural networks (Wu et al., 2023a; Cheng et al., 2023; Li et al., 2024b). Due to the non-i.i.d nature of graphs, GNN unlearning methods need to remove effects of deletion set entities on the remaining entities, which distinguishes the subdomain of Graph Unlearning (Said et al., 2023).\nRecently, machine unlearning has received newfound attention beyond privacy applications (Pawel- czyk et al., 2024; Schoepf et al., 2024; Li et al., 2024a). Goel et al. (2024) demonstrated the distinc- tion between the Corrective and Privacy-oriented unlearning settings for i.i.d classification tasks, emphasising challenges when not all manipulated data is identified for unlearning. In this work, we focus on the intersection of corrective unlearning for graphs, evaluating existing methods and making significant progress through our proposed method Cognac."}, {"title": "CORRECTIVE UNLEARNING FOR GRAPH NEURAL NETWORKS", "content": "We now formulate the corrective unlearning problem for graph-structured, non-i.i.d data. We con- sider a graph G = (V,E), where V and E represent the constituent set of nodes and edges re- spectively. For each node Vi \u2208 V, there is a corresponding feature vector Xi and label Vi, with V = (X,Y). Consistent with prior work in unlearning on graphs (Wu et al., 2023a; Li et al., 2024b), we focus on semi-supervised node classification using GNNs. GNNs use the message- passing mechanism, where each node aggregates features from its immediate neighbors. The effect of this aggregation process propagates through multiple successive layers, effectively expanding the receptive field of each node with network depth. This architecture inherently exploits the principle of homophily, a common property in many real-world graphs where nodes with similar features or labels are more likely to be connected than not."}, {"title": "OUR METHOD: Cognac", "content": "Our proposed unlearning method, Cognac, requires access to the underlying graph G', the known set of entities to be deleted Sf, and the original model M. We define Vf as the set of nodes whose influence is to be removed. For node unlearning, this is the same as the deletion entities Sf; for edge unlearning, this is the set of vertices connected to the edge set to be deleted. Manipulated data has two main adverse effects on the trained GNN: 1) Message passing can propagate the influence of the manipulated entities Sm on their neighborhood, and 2) The layers learn transformations to fit potentially wrong labels in Sm. We tackle these two problems using separate components."}, {"title": "REMOVING ADVERSE EFFECTS ON NEIGHBORING NODES WITH COGN", "content": "The first question we address is: How can we remove the influence of manipulated entities on their neighboring nodes? This requires us first to identify the nodes affected by the manipulations and then mitigate the influence on their representations. Identifying affected nodes is challenging, as the impact of message passing from manipulated entities Sm depends on the interference from messages of other neighboring nodes. Therefore, we use an empirical estimation to identify the affected nodes from each entity in the deletion set. On these nodes, we then perform contrastive unlearning, simul- taneously pushing the representations of the affected nodes away from nodes in Vf while keeping them close to other nodes in their neighborhood. We call this component Contrastive unlearning on Graph Neighborhoods (CoGN), formalized below.\nAffected Node Identification. To speed up our method, we make use of the fact that not all nodes in the n-hop neighbourhood of the manipulated nodes may be affected enough by the attack. To find the most affected nodes, we invert the features of v \u2208 Vf and select neighbouring nodes where final output logits are changed the most. Formally, the inversion is performed by the transformation 1 \u2212 X\u2200 v \u2208 Vf, leading to a new feature matrix x', where X represents a one-hot-encoding vector. We then compute the difference in the original output logits M(x), and those obtained by on the new feature matrix, M(x') given by: \u2206x = |M(x') \u2013 M(x)|. The top k% nodes with the most change Ax are selected as the affected set of entities, from which we remove the influence of the manipulation using CoGN. More details and ablations in Appendix:D.2.\nContrastive Unlearning. To remove the influence of the deletion set Sf on the affected nodes identified in the previous step, we can optimize a loss function that updates the weights such that the final layer logits of Sf and the affected nodes are pushed away. However, this alone will lead to unrestricted separation and damage the quality of learned representations. To prevent this, we also counterbalance the loss with another term that penalizes moving away from logits of neighboring nodes not in the deletion set Sf. For each node v \u2208 S, let zv represent its internal embedding, with p\u2208 N(v) and n \u2208 Vf serving as the positive and negative samples, respectively. We use the following unsupervised contrastive loss:\n$L_c = -log(\\sigma(z_v^T z_p)) - log(\\sigma(-z_v^T z_n))$"}, {"title": "UNLEARNING OLD LABELS WITH ACDC", "content": "Next, we ask: Can we undo the effect of the task loss Ltask explicitly learning to fit the node rep- resentations of Sm to potentially wrong labels? We do this by performing gradient ascent on Sf, which non-directionally maximizes the training loss concerning the old labels. Ascent alone aggres- sively leads to arbitrary forgetting of useful information, so we counterbalance it by alternating with steps that minimize the task loss on the remaining data. More precisely, we perform gradient ascent on Vf and gradient descent on Vr, iteratively on the original GNN M.\n$L_a = -L_{task}(V_f), L_d = L_{task}(V_r)$\nWhile variants of ascent on Sf and descent on remaining data have been studied for image classifi- cation (Kurmanji et al., 2023) and language models (Yao et al., 2023), we find the need for a spe- cific optimization strategy to achieve corrective unlearning on graphs. The challenge arises when Sf C Sm, as the remaining data could still contain manipulated entities, which we aim to avoid reinforcing. However, in realistic scenarios, the manipulated entities Sm typically constitute a small fraction of the training data, allowing us to mitigate their impact through ascent on the representative subset Sf.\nThis requires a careful balance between ascent and descent, which we can achieve by using two different optimizers and starting learning rates for these steps. This insight is similar to prior work"}, {"title": "FORMAL DESCRIPTION OF Cognac", "content": "In this section, we outline the formal procedure of our proposed unlearning method, Cognac de- signed to effectively remove the influence of manipulated data from GNNs. First, the algorithm identifies the nodes affected by the manipulation and then applies a contrastive learning-based ap- proach to unlearn their influence. The key steps include identifying the affected nodes, performing contrastive learning to re-optimize the embeddings, and minimizing classification loss on the un- affected nodes while maximizing it on the manipulated set. The complete algorithm is detailed in Algorithm 1."}, {"title": "EXPERIMENTAL SETUP", "content": "Given a fixed budget of samples to manipulate, ideal corrective unlearning evaluations should maxi- mally deteriorate model performance on the affected distribution so that there is a wide gap between clean and poisoned model performance to measure unlearning method progress. We thus evaluate"}, {"title": "BASELINES", "content": "We evaluate four popular graph unlearning methods and adapt one popular i.i.d unlearning method for graphs. For reference, we also report results for the Original model, Retrain which trains a new model without Sf, and an Oracle trained on the whole training set without manipulations, indicating an upper bound on what can be achieved. The Oracle has correct labels for the unlearning entities, information that the unlearning methods cannot access.\nExisting Unlearning Methods. We choose five methods as baselines where unlearning incorrect data explicitly motivates the method. (1) GNNDelete (Cheng et al., 2023) adds a deletion operator after each GNN layer and trains them using a loss function to randomize the prediction probabilities of deleted edges while preserving their local neighborhood representation, keeping the original GNN weights unchanged. (2) GIF (Wu et al., 2023a) draw from a closed-form solution for linear-GNN to measure the structural influence of deleted entities on their neighbors.Then, they provide estimated GNN parameter changes for unlearning using the inverse Hessian of the loss function. (3) MEGU (Li et al., 2024b) finds the highly influenced neighborhood (HIN) of the unlearning entities and removes their influence over the HIN while maintaining predictive performance and forgetting the deletion set using a combination of losses. (4) UtU (Tan et al., 2024) proposes zero-cost edge- unlearning by removing the edges to be deleted during inference for blocking message propagation from nodes linked to these edges. Finally, we include a popular unlearning method studied in i.i.d classification settings. (5) SCRUB (Kurmanji et al., 2023) employs a teacher-student framework with alternate steps of distillation away from the forget set and towards the retain set. For edge unlearning, we use SCRUB by taking nodes across spuriously added edges as the forget set and the rest as the retain set."}, {"title": "BENCHMARKING DETAILS", "content": "We now describe design choices made for benchmarking, first specifying the datasets and architec- tures, then how to ensure a fair comparison between methods."}, {"title": "RESULTS & DISCUSSION", "content": "We now report our results, first showing comparisons of our method to existing methods across the manipulation types and datasets, followed by ablations of our method and analysis of what can be achieved in this setting."}, {"title": "MAIN RESULTS", "content": "Figure 3 contain unlearning measurements upon varying the fraction of the manipulation set known for unlearning (Sf/Sm). Table 2 accompanies this with utility measurements averaged across the deletion fractions for each method. We make three main observations:\n1. Existing unlearning methods perform poorly even when |Sf| = |Sm|. First, observing the rightmost points in Figure 3, we can see that existing methods fail to unlearn across manipulation types even when the whole manipulation set is known. The only exception is GNNDelete on the"}, {"title": "WHY Accaff SOMETIMES REDUCES AS IDENTIFIED MANIPULATED ENTITIES INCREASE?", "content": "We find an interesting trend that sometimes, as more of the manipulation set (Sm) is known and used as the deletion set (Sf) (going left to right in Figure 3), Accaff reduces. This can seem counter- intuitive, as one would expect the accuracy of affected classes to improve as more samples are used for unlearning. We hypothesize unlearning a larger fraction of the manipulation set reduces Accaff due to two factors that adversely affect the neighborhoods of the nodes removed, which typically have other nodes of the affected classes due to homophily. First, in the case of label manipulation, when we model it as node unlearning for consistency with prior work, we lose correct information about the graph structure. Second, when modifying the graph structure, i.e., removing some edges or nodes changes the feature distribution of their neighboring nodes after the message passes, making it out of distribution for the learned GNN layers. The same rationale is why the test nodes are kept in the graph structure (without optimizing the task loss for them) during training (Kipf & Welling, 2017). We investigate this by adding an ablation where in the unlearning of the label manipulation, instead of unlearning the whole node, we keep the structure, i.e., the node and connected edges, but unlearn the features and labels.\nAs observed in Table 3, retaining the node structure leads to large improvements in Accaff when the deletion set is larger (the full set of manipulated entities), while not benefiting much when the deletion set is smaller. In the full manipulation set deletion setting, Cognac even slightly outperforms Oracle. This highlights how unlike traditional node unlearning in graphs, removing the nodes is not always the best way to unlearn manipulations. They can simply be moved from the train set to the test set to still partake in message passing, so the task loss is not optimized over wrong labels."}, {"title": "IMPORTANCE OF DECOUPLED OPTIMIZERS FOR ASCENT AND DESCENT", "content": "While ascent and descent has been studied in prior unlearning work, it often performs poorly due to difficulties in optimizing for competing objectives. We find this can be fixed with a simple trick: using different learning rates for the ascent and descent steps. The final version of Cognac uses"}, {"title": "RESULTS SHOWING THE BREADTH OF APPLICABILITY OF Cognac", "content": "To provide a comprehensive comparison between Cognac and other methods, we provide results on commonly used GNN backbone architectures - GCN and GAT."}, {"title": "Graph Attention Network (GAT)", "content": "Graph Attention Network (GAT) employs computationally efficient masked self-attention layers that assign varying importance to neighborhood nodes without needing the complete graph structure upfront, thereby overcoming many theoretical limitations of earlier spectral-based methods."}, {"title": "CONVERGENCE", "content": "We now discuss the convergence properties of Cognac. Plots in Figure 9 describe the losses of each of the components of our method (contrastive, ascent, descent) after the last epoch of every step, the meaning of which should be clear from Algorithm 1: Line 4 (where it's called 'num_steps'). The loss plots are constructed over the best hyperparameters, and we would likely not see such convergence trends with sub-optimal hyperparameters, which may provide insights to improve performance when it's used in other settings as well."}, {"title": "ANALYSIS OF METHOD USED TO FIND AFFECTED NEIGHBOURS", "content": "Our strategy to find affected neighbours is likely not the perfect for finding the most affected nodes and more sophisticated influence functions such as the one presented in Chen et al. (2023) could be used to potentially improve performance. Still, we note that it achieves a 5% higher Accaff than while choosing random k% nodes in the n-hop neighbourhood (where n is the number of layers of message passing) while being cheap to compute: we only require a single forward pass over the model with the inverted features. Interestingly, Figure 10 also shows that even if the GNN is not well-trained, if we choose the top k% affected nodes, the unlearning performance does not change much, while still being noticeably better than when we use a random k% of the neighbours.\nFigure 11 (left) shows that there are no noticeable changes in taking a smaller or larger k%. How- ever, removing this step entirely (k = 0%) results in worse performance, suggesting performing contrastive unlearning on even a small k% is significant. Additionally, by keeping this percent- age small, we ensure computational efficiency without diminishing performance (Figure 11 (right)), which is essential for unlearning methods."}]}