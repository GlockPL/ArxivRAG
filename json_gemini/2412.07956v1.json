{"title": "Reciprocal Learning of Intent Inferral with Augmented Visual Feedback for Stroke", "authors": ["Jingxi Xu", "Ava Chen", "Lauren Winterbottom", "Joaquin Palacios", "Preethika Chivukula", "Dawn M. Nilsen", "Joel Stein", "Matei Ciocarlie"], "abstract": "Intent inferral, the process by which a robotic device predicts a user's intent from biosignals, offers an effective and intuitive way to control wearable robots. Classical intent inferral methods treat biosignal inputs as unidirectional ground truths for training machine learning models, where the internal state of the model is not directly observable by the user. In this work, we propose reciprocal learning, a bidirectional paradigm that facilitates human adaptation to an intent inferral classifier. Our paradigm consists of iterative, interwoven stages that alternate between updating machine learning models and guiding human adaptation with the use of augmented visual feedback. We demonstrate this paradigm in the context of controlling a robotic hand orthosis for stroke, where the device predicts open, close, and relax intents from electromyographic (EMG) signals and provides appropriate assistance. We use LED progress-bar displays to communicate to the user the predicted probabilities for open and close intents by the classifier. Our experiments with stroke subjects show reciprocal learning improving performance in a subset of subjects (two out of five) without negatively impacting performance on the others. We hypothesize that, during reciprocal learning, subjects can learn to reproduce more distinguishable muscle activation patterns and generate more separable biosignals.", "sections": [{"title": "I. INTRODUCTION", "content": "Developing intuitive, user-driven methods for interfacing robotic devices with humans offers the potential to restore or enhance movement capabilities that are lost after neuromotor injuries such as stroke. Approaches that infer intent with the use of electromyography (EMG) and other biosignal data can encourage users to practice integrating the impaired limb in daily activities [1], which in turn can facilitate neuroplasticity to improve motor function [2].\nRecent advances in machine learning (ML) can infer intent from EMG even when the strength of muscle activation is insufficient to move the limb [3], [4], [5]. However, neu-romotor impairments after stroke include abnormal muscle coactivation and diminished capacity to generate consistent EMG patterns. Such abnormalities in biosignal generation increase with volitional effort when attempting to use the arm [6], [7], which in turn increases the challenge of intent inferral as a control method for robot-assisted movement.\nTypical intent inferral methods take in user intents as uni-directional ground truths, but wearable interfaces are inher-ently bidirectional systems. Humans modify their control in-puts in response to sensory feedback of device behavior, such as whether the device moves according to expectations [8]. Device developers have sought to provide augmented feed-back, or feedback in addition to intrinsic human sensing, to communicate information about biosignals that are not directly observable in order to aid human adaptation. Augmented feedback can compensate for reduced proprioception associated with sensory impairments after stroke [9], and when applied to EMG signals can offer insights into muscle coactivations [10]. Multiple groups [11], [12] have suggested augmented feedback of biosignals as a potential modality to mediate co-adaptive learning and enable personalized control, such as human and device simultaneously adjusting control parameters in response to the other agent's behavior. However, co-adaptation in practical deployments such as physically assistive devices or rehabilitative interventions has yet to be realized for non-able-bodied users.\nOur approach uses bidirectional intent inferral on a robotic device to dynamically encourage greater separability of con-trol input patterns by stroke-impaired users, instead of solely relying on static thresholding of physiological or kinematic signals [13]. Furthermore, we can immediately deploy this approach for robot-assisted movement. We treat the human as a dynamic co-learner alongside the classifier algorithm, prompting each to update their understanding of the other's behavior. The combination of an adaptive classification al-gorithm and augmented feedback can increase the coherence of sensory input and action output, which in turn helps users develop greater automaticity when controlling the hand. In reciprocal learning, the user learns to generate better biosignals for ML models while practicing actively using the hand. In summary, our main contributions are as follows:\n\u2022\nWe introduce a novel paradigm for training EMG-based intent inferral, consisting of interwoven sessions that alternate between updating ML models based on human biosignal generation and encouraging human exploration and adaptation to models with the use of augmented feedback.\n\u2022\nAs far as we know, this bidirectional training system is a completely unexplored area in the context of upper-limb rehabilitation after stroke. We are the first to use augmented feedback to communicate the inner state of the learning model running on a wearable robotic orthosis as it assists hand movement.\n\u2022\nOur results demonstrate that reciprocal learning leads to better predictions for intent inferral on a subset of stroke subjects by aiding in the generation of more separable muscle activation patterns and biosignals."}, {"title": "II. RELATED WORK", "content": "a) EMG Intent Inferral for Stroke: ML offers a data-driven approach to learn personalized human motor pattern classification. Many works have demonstrated robust EMG control of exoskeletons by healthy users and amputees [1], [11], [12]; however, the interpretation of biosignals from stroke-impaired users is a challenge. The majority of works on intent inferral for stroke use supervised [3], [14] or semi-supervised [4] learning approaches to train models on an initial dataset for later use during that same session, but these paradigms are vulnerable to severe shifts in distribution due to scarce data and time available for training. Recent works have sought to generalize intent inferral to enable few-shot transfer from data trained on a cohort of stroke survivors [15], [16], or have attempted to expand the data available from a single training session with a given stroke subject by use of synthetic data generation [17]. However, these methods expect muscle activation inputs to not sub-stantially change as the user attempts to repeat movements. Impaired users who are unable to execute, and therefore are unable to observe, movements due to muscle weakness after stroke generate highly variable and inconsistent EMG patterns [14] that are prone to drift.\nb) Interactive Learning with Augmented Feedback: Augmented feedback is a method of providing the user with information on biosignals by sources outside the body. Visual augmented feedback, including computer monitor displays or indication lights embedded on a device, has extensive support for improving rates of human motor adaptation beyond performing the task alone [18], and has long been deployed in EMG-based rehabilitation regimens for stroke [19]. Pro-viding continuous, real-time concurrent feedback with human actions has widely shown benefit for stroke-impaired users to interactively learn mappings between different movement behaviors and resulting muscle activation patterns [20], [21]. In these works, augmented feedback is used to benchmark human efforts against a fixed healthy ideal; instead, we use augmented feedback to update the user about personalized device adaptation. Also, previous works focused on rein-forcing or modulating already-learned activation-movement mappings, whereas we focus on the task of training a stroke-impaired user to operate a robotic orthosis that provides movement capabilities that the user does not already possess.\nRecent works [11] have combined augmented feedback with task-specific training to develop new control inputs. Seo et al. [22] altered flexor synergies in healthy subjects using EMG signal-guided conditioning on an isometric task. Silversmith et al. [23] used co-adaptation to enable a par-alyzed subject to add additional control dimensions to a brain-computer interface. However, our reciprocal learning paradigm uniquely co-trains adaptation in the context of physical devices and movement. To our knowledge, no pre-vious works have tackled the challenge of mediating intents between adaptive devices and adaptive users."}, {"title": "III. RECIPROCAL LEARNING PARADIGM", "content": "The ultimate goal of this project is to develop algorithms that can infer intent (one of {open, relax, close}) from EMG signals generated by stroke-impaired users attempting to move their hands, such that our robotic device can provide appropriate assistance. If the predicted intent is open, the robot extends the user's fingers to open the hand; if the predicted intent is close, the robot releases to allow the user to use their own strength to close the hand; if the predicted intent is relax, the robot simply maintains the previous state.\nThe classical way to train such intent inferral algorithms is completely unidirectional. A researcher would issue cues to open, close, or relax the hand to a user wearing the device, then would record the real-time EMG signals and label the data using the provided cues as ground truth intent. In this unidirectional process, ML models learn personalized patterns through the labeled dataset and generate predictions. Although the user can observe the robot opening or closing,"}, {"title": "IV. METHODS AND IMPLEMENTATION", "content": "Having described our general concept and approach, we now expand on the method and implementation of each stage.\nIn the data collection stage of each iteration, we collect a labeled dataset by providing verbal cues for subjects to open, close, and relax the hand. We simultaneously record the EMG signals and the given verbal cues as ground truth intent labels. The EMG armband (Myo, Thalmic Labs) has eight electrodes encircling the forearm and samples 8-channel EMG signals from the forearm surface at 50Hz.\nNote that the arm conditions under which the data is collected differ between the first iteration and the following iterations. A condition is a particular combination of hand position and whether the orthosis is actively providing as-sistance. In the initial iteration, the orthosis has no existing classifiers to run and thus no augmented visual feedback (see Sec. IV-C for details on the feedback mechanism) to provide. The subject has not done any reciprocal learning practice and thus has not found any muscle activations that consistently generate distinguishable EMG signals. As a result, we collect data from a diverse set of predefined arm conditions in the hope that this set provides enough diversity of muscle activation patterns such that one of them or interpolation between them is easily reproducible and separable.\nThe four different arm conditions of the initial iteration are as follows. (1) Arm on table, motor off: the assisted arm of the subject rests on the table, and the motor of the orthosis is disengaged, providing no physical assistance to the subject. (2) Arm on table, motor on: the assisted arm of the subject rests on the table, and the motor of the orthosis is engaged, allowing the device to provide active grasp assistance to the user. (3) Arm off table, motor off: the subject keeps their arm raised to shoulder level for the duration of data collection, and the motor is disengaged, providing no physical assistance to the subject. (4) Arm off table, motor on: the subject keeps their arm raised to shoulder level for the duration of data collection, and the motor of the orthosis is engaged, allowing the device to provide active hand-opening assistance.\nFor each condition, we collect two recordings, one for training and the other one held out for testing. A recording is defined to be a continuous and uninterrupted recorded sequence of EMG signals. For each recording, we instruct the subject to open and close their hand 3 times. Each verbal cue lasts for 5 seconds, with a relax cue between each open and close cue. For conditions where the motor is active, the orthosis moves approximately 1 second after the verbal cue.\nIn the data collection stage of the following iterations, there are no more predefined arm conditions. The orthosis runs the trained classifier, providing active assistance and augmented visual feedback. The subject's goal is to repro-duce the muscle activation patterns learned in the reciprocal learning practice of the previous iteration. We collect four recordings: two for training and the other two for testing.\nIn each classifier training stage, we train a Linear Discriminant Analysis (LDA) classifier. We choose LDA because it is fast, trackable, and widely used in biomedical research [3], [4], [15], [17]. In the optional performance evaluation stage, we evaluate the performance of the classifiers both qualita-tively and quantitatively (see Sec. VI for details).\nThe classifiers take in a single time step of an 8-channel, 50Hz EMG signal. We clip the signal to [0, 1000] and then normalize it to [-1, 1]. Prediction on a single step tends to be noisy, so we apply a median filter of window size 20 to the predicted probabilities of relax, open, and close over time. This median filter smooths out the predicted probabilities and removes unnecessary frequent changes in the intent.\nOur augmented visual feedback consists of two progress-bar displays mounted on the dorsal side of the hand orthosis so that the subject can observe them comfortably during reciprocal learning, as shown in Fig. 1. The relative heights of the green and red LED bars correspond to the model's pre-dicted probabilities for open and close intents, respectively. In the reciprocal learning practice stage, we ask the subject to practice opening and closing their hands guided by the augmented visual feedback. We run the trained classifier on the device to provide appropriate assistance. In this stage, we instruct the subject to trigger different device states (open and close) by maximizing the respective green or red LED bars.\nThis is the exploration stage, where the subject adapts to the behavior of the device, and tries to discover and reinforce the muscle activation pattern that is easily reproducible and distinguishable. In this stage, the subject uses augmented visual feedback to learn to generate more separable EMG signals for each intent. This idea is conceptually illustrated in Fig. 2. Reciprocal learning practice essentially helps the subject locate the EMG subspace that is both reproducible and separable (i.e., far from the decision boundary learned by the classifier of the previous iteration). Then, in the next iteration, the subject can reproduce data in these more separable regions, leading to new retrained classifiers that can more accurately predict user intent."}, {"title": "V. EXPERIMENTAL SETUP", "content": "We tested reciprocal learning in single-session experiments with five stroke subjects, and compared intent inferral ac-curacies from two iterations. Reciprocal learning supports multiple iterations; however, here we only complete two iterations due to session time constraints. We leave the study of reciprocal learning with more iterations as future work."}, {"title": "A. Subjects", "content": "We performed experiments with five chronic stroke sur-vivors having hemiparesis and moderate muscle tone: Mod-ified Ashworth Scale (MAS) scores < 2 in the upper ex-tremity. Our MAS criteria exclude subjects whose fingers are difficult to move passively-fingers with more severe spasticity cannot be quickly extended with external forces without increasing muscle tone and risking damage to the joints. Our subjects can fully close their hands but are unable to completely extend their fingers without assistance. The passive range of motion in the fingers is within functional limits. Testing was approved by the Columbia University Institutional Review Board (IRB-AAAS8104) and was per-formed in a clinical setting under the supervision of an occupational therapist. See Table I for details on subjects.\nOur subjects have different degrees of hand impairment severity, measured by their varied Fugl-Meyer scores for upper extremity (FM-UE). Subjects S1, S2, and S3 have no active finger extension (minimal observable movement) and lower corresponding FM-UE scores (27, 26, 26, respec-tively), whereas S4 and S5 have some residual active finger extension capacity (limited observable movement) and higher corresponding FM-UE scores (50, 47, respectively)."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "In this section, we describe the evaluation of intent in-ferral performance across iterations and show the potential effectiveness of reciprocal learning."}, {"title": "A. Intent Inferral Accuracy", "content": "During the performance evaluation stage of each iteration, we compute the intent inferral accuracy of the trained clas-sifier on the test set, shown in Table II. While the intent inferral accuracies for S2, S3, and S5 remain similar across the first two iterations, we observe an accuracy improvement for S1 and S4, from 0.61 to 0.88 and from 0.86 to 0.94, respectively. Augmented visual feedback during reciprocal learning practice may have helped S1 and S4 generate more consistent and distinguishable muscle activation patterns, creating more separable intent signals. As a result, the data collected after the first reciprocal learning practice become easier to classify. We note that S1 has more limited hand movement capacity compared to S4 (also reflected by their intent inferral accuracy). This suggests that whether recip-rocal learning can improve intent inferral accuracy may not depend on stroke subjects' residual hand function.\nOne possibility for why reciprocal learning does not improve performance across all subjects is the randomness of exploration in the practice stage. Although the prescribed arm conditions during the data collection stage of the first iteration provide data variance, due to the variation of impairment across stroke subjects, it is not guaranteed that the initial dataset is diverse enough to contain reproducible and distinguishable muscle activation patterns for all intents. In addition, even though the initial dataset collected with predefined arm conditions contains many activation patterns, due to randomness of exploration, it is not guaranteed for a subject to successfully locate or reproduce a specific pattern. While advancing the proposed paradigm to have a positive effect on most participants is obviously desirable and is the focus of our future work, we believe that an approach that improves performance on a subset of participants without hindering performance on the others can also have practical usefulness. Furthermore, a detailed study of the differences between cases showing successes versus more limited effects can suggest future areas of exploration."}, {"title": "B. Data Separability", "content": "We further investigate the data of subjects S1 and S4 to show how reciprocal learning may lead to biosignals with more discriminative information for classifying intent. We visualize 1000 randomly sampled 8D EMG signals for each intent before and after the reciprocal learning practice in 3D space using t-distributed stochastic neighbor embedding (t-SNE) [26], as shown in Fig. 4. Each data point is labeled using its ground truth label, which is only available during training. After reciprocal learning, the data clusters associ-ated with each intent improve in separability, which could explain the increased classification accuracy and lend support to the hypothesis that reciprocal learning may help some subjects generate signals that are more easily interpretable by intent inferral algorithms.\nWe also visualize the LDA weights associated with each EMG electrode in Fig 4. Each of the eight electrodes uniformly distributed on the forearm circumference covers a particular area of forearm muscles. Positive values (red) mean that muscle activity under that particular electrode is positively associated with a specific intent, while negative values (blue) mean that muscle activity contributes nega-tively to that intent. We notice that after reciprocal learning practice, the weights (especially for the open intent) become more diverse, meaning that the positive weights are more positive, and the negative weights are more negative. In fact, the variance of the 8D weights for the open intent increases consistently for all subjects, as shown in Table III. These results suggest that subjects have stronger muscle activation patterns after reciprocal learning practice and explain im-provements in intent inferral accuracy from another angle."}, {"title": "VII. CONCLUSION", "content": "We propose a novel bidirectional reciprocal learning paradigm that co-trains stroke-impaired users with intent inferral algorithms. We achieve reciprocal learning through augmented visual feedback in the form of LED progress-bar displays, which communicate the probabilities of the open and close intents predicted by the classifier. Our paradigm consists of iterative and interwoven stages of training the ma-chine learning models and human exploration and adaptation to the classifiers. By using the augmented visual feedback of the classifier internal state as guidance, a subset of our stroke subjects learn to modulate muscle activation and generate more separable EMG signals, leading to better intent inferral performance, while the performance of the other subjects is not adversely affected. Future work includes conducting detailed studies of reciprocal learning involving multiple iterations, and exploring the factors that influence a subject's ability to find consistent and reproducible muscle activation patterns during reciprocal learning practice."}]}