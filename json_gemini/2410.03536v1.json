{"title": "Computer Vision Intelligence Test Modeling and Generation: A Case Study on Smart OCR", "authors": ["Jing Shu", "Bing-Jiun Miu", "Eugene Chang", "Jerry Gao", "Jun Liu"], "abstract": "AI-based systems possess distinctive characteristics and introduce challenges in quality evaluation at the same time. Consequently, ensuring and validating AI software quality is of critical importance. In this paper, we present an effective AI software functional testing model to address this challenge. Specifically, we first present a comprehensive literature review of previous work, covering key facets of AI software testing processes. We then introduce a 3D classification model to systematically evaluate the image-based text extraction AI function, as well as test coverage criteria and complexity. To evaluate the performance of our proposed AI software quality test, we propose four evaluation metrics to cover different aspects. Finally, based on the proposed framework and defined metrics, a mobile Optical Character Recognition (OCR) case study is presented to demonstrate the framework's effectiveness and capability in assessing AI function quality.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) has experienced significant growth and evolution in the past few years [1]. Integrating AI functions, such as identification, recognition, prediction, and recommendation, is increasingly prevalent in software systems. As a result, testing AI software, which refers to diverse quality testing activities for AI-based software systems [2], is essential to guarantee that Al features operate correctly, reliably, and consistently. However, AI software testing differs from conventional software testing since AI features have unique characteristics like large-scale unstructured input data, unpredicted scenarios, uncertainty in system outputs, responses or actions, and data-driven learning features [3]. This introduces challenges in AI software testing, such as the interpretability of ML and DL models, lack of clear specifications and defined requirements, test input generation, defining test oracles, and managing dynamic environments [4]. To address these issues, extensive research has been done on a variety of AI testing top- ics, including test modeling [5] [6] [7] [8], test case generation [9] [10] [11], test framework, and automation [8] [9] [12] [13]. AI software systems, based on their functionalities, can be classified into categories like expert systems, computer vision, speech recognition, NLP, and business intelligence systems.\nGiven that different AI systems possess distinct characteristics, they need varied testing approaches for validation.\nOptical Character Recognition (OCR) is a fundamental computer vision technique for identifying text in images or scanned documents [14]. In modern OCR systems, text recognition usually consists of 6 steps: image acquisition, pre- processing, segmentation, feature extraction, classification, and post-processing [15]. Techniques are utilized in every step to improve the overall accuracy of the OCR system, such as the Conventional Neural Network (CNN), one of the most popular techniques employed for OCR [16] [17]. Stephen V. Rice introduced a series of performance measures, including character accuracy, word accuracy, non-stopword accuracy, and phrase accuracy for the page-reading system in his 1996 doctoral dissertation [18]. Character Error Rate (CER) and Word Error Rate (WER), which are the inverted accuracy, have also been commonly used in OCR evaluation. When documents have multiple columns or distinct text blocks, layout analysis determines the text segmentations, reading order, and geometric position, which becomes a crucial aspect of the evaluation process. Modified CER and WER with configurable penalties in reading order, over-segmentation or under-segmentation of text lines are proposed for end-to-end evaluation [19]. Based on text bounding coordinates, Hwang et al. [20] defined DISGO WER to evaluate four types of errors: Deletion, Insertion, Substitution, and Grouping/Ordering errors. In some situations, the text reading order is not critical, which means different reading orders are allowable. In this case, a flexible character accuracy independent of the reading order is proposed by Clausner et al. [21]. These metrics are used to evaluate OCR systems' performance based on various datasets, such as IMPACT [22], which has over 600,000 representative document images collected from major European libraries, and COCO-Text [23], which contains text in natural images. Although researchers and practitioners could assess the overall performance of OCR algorithms and systems or a specific method like parameter tuning from these metrics, a systematic way to identify the bottlenecks is still needed.\nSoftware testing generally consists of unit testing, integration testing, function testing, and system testing. As pointed out by Gao et al. (2021) [5], the process of testing AI functions primarily comprises test modeling, test case generation, test"}, {"title": "II. AI FUNCTION TEST MODELING", "content": "execution, and test quality evaluation.\n1) Test Modeling: Several typical validation approaches for traditional software have been applied for AI software testing, like Metamorphic Testing (MT). MT, a methodology initially introduced by Chen et al. (1998) [6], employs the properties of functions such that it is possible to enable the prediction of specific output changes in response to particular modifications in the input. In this way, it could address \u201cTest Oracle Problems\u201d. C. Tao et al. (2019) [7] propose an MT-based testing model, especially for facial age recognition systems. Y. He et al. (2023) [8] propose a semantic tree model for generating driving scenarios in Autonomous Vehicle (AV) testing, along with a 3D test model for performance evaluation through automation tests. This 3D test model includes a context semantic tree detailing constant aspects through a test case and separating from the dynamic agents, an input semantic tree modeling dynamic agents like pedestrians, vehicles, and major landmarks, and an output semantic tree capturing behaviors with the given context and input. Based on this model, complete scenarios are generated.\n2) Test Case Generation: The distinctive features of AI software, such as large-scale input data and data-driven learning features, make it extremely difficult and expensive to generate test cases and validate results. To tackle these challenges, H. Zhu et al. (2019) [9] present a novel approach called datamorphic testing for AI applications. In the testing, datamorphism involves transforming existing test data, termed seed, to create new test data, called mutants. These alterations result in a variety of mutants derived from the original seed data. D. Berend (2021) [10] points out that in recent studies, the distribution of generated test data has not been considered when designing testing techniques for AI software. Therefore, the author proposes an innovative distribution-aware testing methodology that seeks to generate new unseen test cases related to the fundamental system task. J. Huang et al. (2022) [11] have found that 44% of the test cases generated by the state-of-the-art test approaches are false alarms because of inconsistent and unnatural issues. To address this problem, they propose AEON for Automatic Evaluation of NLP test.\n3) Test Framework and Automation: To speed up the AI- based system validation process and reduce its cost and time, numerous studies have focused on innovative testing frameworks and test automation. L. Li et al. (2023) [12] propose an innovative AI model evaluation framework based on a quality evaluation model. This model is distinguished by three attributes: mathematical, comprehensive, and software, each encompassing various quality dimensions. H. Zhu et al. (2020) [13] extend the datamorphic testing framework by categorizing software artifacts into two types: entities and morphisms. They also present an automated testing tool named Morphy.\nThe rest of this paper is organized as follows. Section II describes the proposed AI test modeling and presents the test coverage criteria and complexity. The proposed evaluation metrics are introduced in Section III. The experimental results and analysis are discussed in Section IV. The conclusion and future work are given in Section V."}, {"title": "A. Preliminaries: OCR Evaluation", "content": "The most common way to evaluate OCR performance is to compare OCR result text with Ground Truth (GT). The edit distance between OCR text and GT is typically used to define OCR accuracy. Levenshtein Distance [24] is one type of edit distance, in which each insertion, deletion, and substitution corresponds to one edit operation. Let R = r1r2r3\u2026\u2026rm be an OCR result text string of m symbols and G = 919293\u2026qn be the correct text of n symbols. The Character Accuracy (CA) [18] is defined as CA = E, where the number of errors E is the edit distance that denotes the minimum number of edit operations of transferring string R into string G. Similarly, Word Accuracy is the percentage of words that are correctly recognized. Character Accuracy indicates how well the OCR system recognizes characters. But in some cases, not all the characters are interested. Therefore, Non-Stop word Accuracy, Phrase Accuracy (accuracy over a sequence of k characters), and Accuracy by Character Class (accuracy over a subset of characters) are also introduced.\nTo calculate the edit distance, both the serialized OCR output and the serialized GT text are required. The complexity of a text's layout can impact OCR accuracy, especially due to the reading order of various text blocks or columns. However, in some cases, the reading order is less critical. Clausner et al. [21] proposed flexible character accuracy to solve this problem. The texts to be compared are broken down into smaller segments. Then, individual edit distance calculations will be conducted on these segments. Finally, these distances are aggregated to obtain an overall measure of character accuracy. Layout analysis [25] is another important step if the OCR system aims to extract the geometric and logical structure of the text in an image or scanned document.\nAccording to the related work, there are different AI software testing approaches, such as metamorphic testing and model- based testing. Unlike NLP, the OCR text extraction AI feature has a predictable output for a specific image or scanned document. This predictability stems from the fact that for a specific picture or scanned document, the text content on it is objectively present. As a result, the OCR process inherently has a deterministic output. Recognizing this characteristic, it becomes crucial to identify a suitable and cost-effective approach for validating the OCR function.\nThis paper is focused on an AI function that takes image- based receipts as input data and converts them to editable text via OCR technology. Recent receipt OCR applications mainly focus on Key Information Extraction (KIE) such as merchant name, address, and total expense, which does not fulfill our requirements. So, we resort to \u201cOCR text scanner\" mobile applications like CamScanner and Scan Pro for our case study.\""}, {"title": "B. Context Classification Modeling", "content": "In this paper, a 3D classification testing model [5] is designed for the image-based receipt text extraction AI function. The term \"3D\" refers to 3 dimensions: context, input, and output."}, {"title": "C. Input Classification Modeling", "content": "The input aspect of the model relates to the textual contents within the image. The author divides the receipt content into four main sections: store details, item list, transaction specifics, and miscellaneous information. Consider a typical shopping receipt illustrated in Fig. 2. The first section of receipts typically contains the store's name, address, and contact details, along with the store's logo, if it exists. Logos vary widely: some are purely graphical or textual with diverse font styles, while others combine text and images. In some logos, the text is integrated into the graphic element with a colored background. The right part of Fig. 2 shows different types of stores' logos. The logo in (6) is harder to identify, in which text is 45 degrees rotated and embedded in a dark background.\nIn the purchased items section, the content is classified into three types: short, medium, and long list. This section may include discount details and special characters such as @, #, and %. If a credit or debit card is used for payment, the card details appear in the receipt's transaction section. Conversely, transaction details tend to be simpler for cash payments. The miscellaneous section encompasses various other details such"}, {"title": "D. Output Classification Modeling", "content": "According to the related work, accuracy or error rate is commonly used for OCR function evaluation. To recognize where the error comes from, total accuracy, store info sec- tion accuracy, item list section accuracy, transaction section accuracy, and miscellaneous section accuracy are defined. The method for calculating accuracy depends on the requirements. The details of evaluation metrics are discussed in Section III.\nBased on the analysis, plain text accuracy, which can be classified into four categories, is used to evaluate a test case. For the OCR output, the benchmark for a successful test (pass) could be set at a character accuracy rate of 95% and above. This criterion ensures that the AI function not only detects text but also recognizes it with high precision. The output classification tree is shown in Fig. 4."}, {"title": "E. 3D Decision Table", "content": "Based on the three classification tree models, the authors construct a 3D decision table that comprises all the considered parameters. This table presents a comprehensive view of potential test cases. Nevertheless, considering every possible combination of these parameters will lead to an extremely large number of test cases. Combinatorial testing [26] is a systematic method to select combinations of inputs, parameters, or features for testing, which has been extensively studied in the past few decades. Combinatorial T-way testing is effective in balancing the test costs and coverage. This approach can be used to generate test cases. However, the primary goal of this project is to identify the factors affecting OCR accuracy during evaluation. To isolate the influence of different factors, we change only one parameter at a time in each test to determine the most significant influencing factor in this paper. For example, if dark lighting and long-distance image capture conditions are considered simultaneously in one test and it fails, we need to identify the main contributor to the failure. Based on the test results, combinatorial testing will be carefully designed to increase test coverage later."}, {"title": "F. Test Coverage Criteria", "content": "For the given Al function, its 3D decision table has a set of context conditions, input, and output parameters. The test coverage criteria are fulfilled when any context condition, input, and output parameter are covered by at least one test case. Based on the test models, the test complexity of each classification tree can be easily computed below:\n\u2022 Context classification test Complexity (CC) = Number of context classification stubs in Table I = 14\n\u2022 Input classification test Complexity (IC) = Number of input classification stubs in Table I = 25\n\u2022 Output classification test Complexity (OC) = Number of output classification stubs in Table I = 10\nThe total test complexity is CC\u00b7IC-OC = 14.25.10 = 3500. However, the author reduces the test complexity to 24 by selecting test cases based on the test coverage criteria in the decision table."}, {"title": "III. MODEL EVALUATION METRICS", "content": "If the sequence in which text is read doesn't matter, which means Ground Truth is independent of reading order, a measure such as the flexible character accuracy [21] could be applied. Furthermore, if the accuracy assessment requires considering the text's geometric position, the text position, like bounding box coordinates, should be incorporated into the error calculation process.\nIn this paper, text positioning is not considered in the accuracy calculation, as text scanner applications typically produce unformatted text outputs. Accuracy is focused on characters and string segments, assessed at four levels: Flex Character Accuracy (FCA), String Segment Accuracy (SSA), Ordered String Segment Accuracy (OSSA), and Text-Line Accuracy (TLA). Characters are categorized into alphabet characters, special characters, and digits to further evaluate the OCR system's proficiency in recognizing different types of characters. Before determining the accuracy for a specific character type, both the ground truth and OCR-generated text are preprocessed to remove other types of characters. Subsequently, the FCA algorithm and its associated formulas"}, {"title": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "This section presents a case study based on the proposed test model and framework for two popular \u201ctext scanner\" mobile applications (CamScanner and Scanner Pro) to evaluate OCR AI function. The authors generate 24 test cases based on the 3D decision table. Among these test cases, pass rates based\naccuracy falls to 88.5%, and Scanner Pro's accuracy decreases to 82.5%. CamScanner outperforms Scanner Pro in handling complex contexts.\nExclusive of the blurry image test case with super low accuracy, accuracies are recalculated. Fig. 9 and Fig. 10. provide insights into different accuracy results for Scanner Pro and CamScanner. FCA emerges as the highest, with SSA ranking next. The Scanner Pro app demonstrates a notable drop in OSSA compared to SSA, indicating a tendency for OCR outputs to rearrange words out of the original order, particularly within the item and miscellaneous sections. In contrast, the CamScanner app maintains an SSA that is very close to OSSA, suggesting minimal alteration to word order by its OCR feature."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This paper discusses the challenges associated with AI software testing and provides a comprehensive literature review on related topics, including test modeling, test case generation, and test frameworks and automation. We have introduced a 3D classification testing model to systematically evaluate the image-based text extraction AI function. Character accuracy and"}]}