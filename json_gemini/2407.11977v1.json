{"title": "Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents", "authors": ["Guangzhi Sun", "Xiao Zhan", "Jose Such"], "abstract": "The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era.\nWe delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.", "sections": [{"title": "WHAT DOES 'PERSONA' MEAN IN THE CONTEXT OF CONVERSATIONAL AGENTS?", "content": "In the context of conversational agents (CAs), the concept of persona represents the essence or 'soul' of these agents. Persona encapsulates the distinct tone, voice, and personality that characterizes a CA, transforming mechanical interactions into engaging, human-like conversations [18, 46]. Commonly, these attributes of persona can consist any type of information that intend to capture personal characteristics about an individual [28], and are relatively static (race), and slowly change over time (age), or temporary (emotional status) [26, 54].\nBefore delving deeper into the discussion of personas in CAs, it's important to distinguish this concept from the idea of 'personality' that has been explored in prior research [24, 27, 36, 38]. While personality traits, such as being \"friendly\" or \"smart,\" or frameworks like the Myers-Briggs Type Indicator (MBTI) [4], might define certain characteristics shared by groups of individuals, a persona in CAs represents a more complex and consistent identity [36, 55]. This persona transcends mere personality traits, serving as an external manifestation of a character's unique identity. For instance, when a CA is designed with the persona of a specific character, say, Sherlock Holmes, it consistently embodies the unique attributes and behaviors of that character throughout interactions. This specificity differs significantly from assigning generic traits like 'bravery' and 'smartness' to a CA. In the latter case, the CA might alternate between different characters who share these traits, such as both Sherlock Holmes and Hermione Granger, depending on the context of the interaction. Thus, the persona of a CA is a more nuanced and stable layer that defines its interaction style and character representation."}, {"title": "1.1 Persona in CAs in pre-LLM era", "content": "Recent research in the field of CAs has focused extensively on enhancing the capabilities of chatbots, aiming to imbue them with more human-like characteristics. This initiative is driven by the goal to significantly boost user engagement, among other benefits. The development of a persona for CAs such as chatbots has emerged as a key strategy in this domain. The introduction of these personas is a testament to the evolving sophistication of chatbot technology, reflecting a deeper understanding of human-chatbot interaction dynamics [16].\nTwo main avenues of exploration have emerged: the technical research stream pushes the boundaries of what is technically"}, {"title": "1.1.1 Technical research.", "content": "Previous studies have proposed various methods for embedding personas into traditional chatbots\u00b9. The categories used are broad - for a comprehensive summary of the model and a survey see [46]. The more widely known examples are that neural models of conversation generation provided a simple mechanism for incorporating personas as embeddings [26, 44, 49]. More recently, Liao and He created personas for conversational agents that had distinct gender and race to understand user preferences [27]. As one example of a project that is guided by user data, persona Xiaolce was designed based on a large scale analysis of human conversations [57]. In doing so, the designers found that the majority of \"desired\" users are young and female. Hence, they designed XiaoIce's persona around an \"18-year-old girl\" [57]. As another example, Danielescu and Christian [8] designed personas for a conversational coaching system where they involved customers by interviewing them and brainstorming with them, finding that their preferences may vary based on their culture and region."}, {"title": "1.1.2 Social research.", "content": "The academic community has consistently maintained a positive attitude towards endowing chatbots with personas. Incorporating a distinct persona in CAs significantly influences the development of a robust relationship in human-agent interactions. It has been demonstrated that a well-crafted persona can significantly enhance the capacity of CAs to engage in empathetic conversations [37, 56]. This is mirrored from empirical research, such as that by Zhong et al. [56], has established the role of persona in fostering empathy within human conversations from the psychological perspective. Moreover, the positive contribution of persona is recognised in specific areas such as healthcare where CAs assume varied roles. For instance, Bickmore et al. [3] found that an empathetic persona in an agent is effective for managing mental health, whereas an agent with a subtle persona guiding exercise can enhance commitment to behavior change. Similarly, preliminary research conducted in [16] indicates that chatbots embodying roles like doctors, in comparison to generic bots, achieve higher user acceptance, intimacy, and trust in healthcare-related interactions."}, {"title": "2 REALITY OR ASPIRATION?", "content": "Large Language Model (LLM)-based CAs, exemplified by systems like ChatGPT2, are rapidly being integrated into various critical sectors, underscoring their growing significance in practical applications. These include, but are not limited to, healthcare [6, 21, 47], education [19, 31, 52], and finance [22, 23, 51], among others.\nThese LLM-based CAs, which are originally developed for general-purpose applications, do not prioritize the establishment of a distinct persona during their design phase. For example, as illustrated in Figure 1, ChatGPT, a typical instance of such systems, is structured to function without a predefined persona, focusing instead on\ndelivering information and interaction capabilities that are broadly applicable across various contexts and user requirements\u00b3.\nNevertheless, the integration of personas in LLM-based CAs should not be viewed as an unattainable goal. Online resources (including blogs [5, 32], and technical reports [50]) already provide guidance on designing specific personas to optimize ChatGPT's effectiveness across various roles, typically achieved by customizing initial conversation prompts to assign a desired persona. Concurrently, numerous empirical studies [2, 7, 12, 17, 20, 34, 35, 58] have examined and demonstrated the practicality of assigning personas to LLM-based CAs. Among them, some promising results indicated that endowing LLM-based CAs with personas leads to satisfactory outcomes. These include the ability to express opinions similar to people from some controes [12], offering useful answers [20], team working [7], and enhancing the overall truthfulness in their responses [58].\nHowever, upon deeper analysis of persona-based CAs, it becomes evident that LLM-based CAs are still far from embodying specific personas at this stage, highlighting a substantial developmental path that lies ahead. For instance, significant performance disparities exist between different GPT versions. Stories from GPT-4 personas are generally more readable, coherent, and believable, while ChatGPT tends to deviate from the provided prompts, failing to adhere strictly to the prescribed personas [17]. In [41], a study was conducted to assess if the prevailing prompt-based approach facilitates LLM-based CAs in delivering consistent and robust responses. Their investigation, which included testing 15 open-source LLMs, ultimately revealed that most models lacked a consistent persona. Furthermore, it's noteworthy that malicious actors sometimes exploit these characteristic, manipulating them to generate toxic responses [10, 59]."}, {"title": "3 PERSONA NEEDS IN LLM-BASED CAS", "content": "In the current landscape dominated by LLMs, the importance of persona has not diminished, rather, it often takes on an even more critical role. In this section, we will explore various situations and use cases where the persona of a LLM-based CA is particularly crucial."}, {"title": "3.1 Participant Simulation", "content": "Hagendorff et al. [15] conducted an evaluation of GPT-3.5 through cognitive response tests and discovered that the error patterns of the language model qualitatively reflect intuitive behaviors akin to those found in humans. Furthermore, it often fails in similar reasoning tasks as humans do [9]. These findings underscore the significant potential of LLMs in capturing aspects of human behavior. Based on these findings, LLMs are increasingly being considered and used to simulator human beings with different personas. Recent studies [1, 2, 35] have provided substantial evidence that LLMs simulating user responses can replicate social science experiments and online forums with a high degree of consistency comparable to those obtained using actual human participants.\nThe future of simulating various user types appears brighter as the accuracy of such simulations continues to improve. Experiments and studies in fields constrained by traditional methodologies stand to benefit significantly from advanced technologies like LLMs. For example, research exploring interactions with individuals who have mental health issues often faces ethical dilemmas and heightened risk assessments. Utilizing LLMs equipped with well-defined personas to simulate such participants can expedite research processes while minimizing potential risks to the interaction between researchers and subjects. Additionally, in studies seeking diverse and balanced samples, recruitment challenges often arise, especially when targeting specific demographic backgrounds. LLMs can be programmed to represent a range of demographics and personas, thus addressing recruitment limitations efficiently. Moreover, the financial implications of user studies involving large participant groups are considerable. By incorporating personas into LLMs, researchers can conduct extensive studies more cost-effectively, without compromising the breadth and diversity of participant profiles."}, {"title": "3.2 Role Playing in Specific Domains", "content": "LLM-based CAs, when programmed with specific personas, offer substantial support to educators, especially teachers, in improving their development of educational content, enriching their teaching methodologies, and bolstering their self-assurance. For instance, such agents can simulate a variety of student personas, enabling teaching assistants (TAs) to engage in realistic interaction scenarios [30]. This approach allows TAs to refine their skills in providing feedback and effectively addressing the needs of students with diverse characteristics, learning goals, and educational backgrounds. This comprehensive and authentic practice environment is instrumental in equipping TAs with the necessary competencies to minimize instructional mishaps in real-world teaching situations.\nSimilarly, they have the potential to significantly enhance the professional skills of lawyers, physicians, and other specialists. These LLM-based CAs can simulate interactions with diverse patient types, including the elderly and those with unique symptoms or needs. Traditionally, such simulations form a crucial part of training before professionals are fully qualified. Now, with the integration of LLM agents equipped with specialized personas, this training phase can be streamlined and made more intelligence-oriented, offering a sophisticated approach to professional skill development."}, {"title": "3.3 Brand Representation", "content": "The persona of an LLM-based CA plays a crucial role in brand representation by aligning with the brand's values, enhancing user engagement, and serving as a differentiator in a crowded market. For instance,\n\"Domino's pizza created 'Dom', a virtual ordering assistant. Dom's persona is friendly and efficient, reflecting the brand's focus on convenient and fast service. Dom allows customers to order pizza using conversational language, making the process more engaging and aligning with Domino's commitment to innovation in delivery and customer service.\" [11]\nA well-defined persona ensures that the agent's communication style and tone are consistent with the brand's identity, fostering a stronger and more coherent brand image. This alignment is essential not only for maintaining brand consistency but also for creating a more engaging and relatable experience for users. In an environment where many companies employ similar technologies, a distinctive persona can significantly set a brand apart, making it more memorable and appealing to customers. This unique identity helps in building customer loyalty and establishing a competitive edge."}, {"title": "4 CHALLENGES AND CAVEATS", "content": ""}, {"title": "4.1 Consistency Is the Top Priority", "content": "The primary objective in the design of CAs is to establish and nurture a robust connection with users, facilitating ongoing engagement over extended periods [42]. Achieving this necessitates the ability of the CAs to engage in sustained, meaningful conversations [43, 53]. Recent findings [13, 40] indicated that LLM-based CAs exhibit a heightened sensitivity to subtle and sensitive words within the context, leading to inconsistent outputs. This characteristic has raised concerns about the ability of LLM-based CAs to maintain a consistent persona\u2074 throughout multiple dialogue exchanges [17, 25]. This observation underscores the challenge of ensuring that these AI systems not only understand and process language effectively but also retain a consistent and contextually appropriate persona over successive interactions. Moreover, as discovered in [49], having inconsistency of persona is one of the major obstacles in achieving the long-term objective of developing human-like CAs to pass the Turing test [48] Addressing this issue is critical"}, {"title": "4.2 Are There Effective Ways to Evaluate Persona and Its Consistency?", "content": "So far, a systematic approach to evaluate and verify persona application in LLM-based CAs has not been established. However, there exists some noteworthy attempts, such as employing empirical frameworks for indirectly assessing the persona of CAs [14, 39, 41]. This can be achieved through psychometric testing or by analyzing survey results.\nIt appears that one cannot ascertain the specific persona a LLM-based CA is exhibiting simply by prompting queries such as \"what is your persona\" or \"describe your persona.\" Consider a scenario where an LLM-based CA is programmed or processed to embody a certain persona. The reality is, people cannot exhaustively enumerate all the traits of this persona, leaving room for the CA to exhibit some degree of self-expression in its responses. Moreover, the inherent unpredictability of the LLM adds a layer of complexity. For instance, the CA might be defined as \"a 21-year-old physics student from Canada with a particular temperament...\" but these specifications are insufficient to confine it to a specific character or individual. For example, in one interaction round, the LLM-based CA may fit this description but have a preference for bowling, while in the next round, it might have the same foundational characteristics but prefer skiing. In this situation, its persona has changed, yet such changes are subtle and challenging to detect and define. We can only ascertain their adherence to our initial constraints through certain predetermined questions. The CA might perfectly execute the task, but when asked about other aspects, like hobbies, it might reveal inconsistencies. Such situations are unpredictable and difficult to capture.\nMoreover, we acknowledge that individual perceptions of a system's persona can vary. For instance, a chatbot with a female-like voice might be considered by someone as sufficiently demonstrating a persona. This variability poses challenges in establishing a universally accepted standard for assessing a system's capability to exhibit a persona."}, {"title": "4.3 More than Persona", "content": "This point becomes particularly prominent in our discussion about endowing \"characters\" with the ability to play different domain experts in LLM-based CAs. We believe that for CAs to successfully assume the required roles, it is essential to impart not only fundamental character traits such as demographics, age, and gender but also corresponding knowledge. Characters must also be equipped with professional knowledge that aligns with their identities. For example, a character role as an ophthalmologist should be familiar"}, {"title": "4.4 Ethical Considerations", "content": "As with any AI-based technology, integrating personas into LLM-based CAs presents a dual-edged sword. It offers significant benefits but can also harbor potential risks. The use of such technology inevitably necessitates careful consideration of ethical issues. Potential harms include, but are not limited to, the ethics of deception and the reinforcement of societal stereotypes. We encourage our audience to refer to the provocation paper [36] for a more comprehensive discussion on the ethical considerations surrounding the use of personas in these systems. Here we refrain from redundant elaboration of previously stated points."}, {"title": "5 CONCLUSION", "content": "In integrating persona into LLM-based CAs, this provocation highlights the significance of persona to enhance human-like interactions. It covers the criticality of various applications, and meanwhile puts forward challenges in achieving persona consistency and domain-specific adaptability. In conclusion, although the prospect of creating CAs with high effectiveness and human resemblance is promising, prioritizing ethical standards and tackling technical challenges is essential. Future efforts must aim for responsible development that maximizes the benefits of persona integration while addressing its complexities."}]}