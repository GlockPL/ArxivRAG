{"title": "From Hallucinations to Facts: Enhancing Language Models with Curated Knowledge Graphs", "authors": ["Ratnesh Kumar Joshi", "Sagnik Sengupta", "Asif Ekbal"], "abstract": "Hallucination, a persistent challenge plaguing language models, undermines their efficacy and trustworthiness in various natural language processing endeavors by generating responses that deviate from factual accuracy or coherence. This paper addresses language model hallucination by integrating curated knowledge graph (KG) triples to anchor responses in empirical data. We meticulously select and integrate relevant KG triples tailored to specific contexts, enhancing factual grounding and alignment with input. Our contribution involves constructing a comprehensive KG repository from Wikipedia and refining data to spotlight essential information for model training. By imbuing language models with access to this curated knowledge, we aim to generate both linguistically fluent responses and deeply rooted in factual accuracy and context relevance. This integration mitigates hallucinations by providing a robust foundation of information, enabling models to draw upon a rich reservoir of factual data during response generation. Experimental evaluations demonstrate the effectiveness of multiple approaches in reducing hallucinatory responses, underscoring the role of curated knowledge graphs in improving the reliability and trustworthiness of language model outputs.", "sections": [{"title": "Introduction", "content": "Hallucination in language models refers to the generation of factually incorrect or nonsensical responses, often stemming from the model's over-reliance on patterns in the training data (Yao et al., 2023). This phenomenon poses a significant challenge in natural language processing, particularly in applications where generating accurate and contextually relevant responses is crucial, such as dialogue systems, question-answering, and content generation. Hallucinations can undermine the credibility and usefulness of language models, leading to misleading or erroneous outputs that may propagate misinformation. Addressing hallucinations requires a nuanced understanding of their underlying causes and mechanisms (Xu et al., 2024). Furthermore, hallucinations can arise due to the lack of factual knowledge, leading the model to generate responses that deviate from reality.\nWe propose leveraging curated knowledge graph (KG) triples to ground language models in factual information to mitigate hallucinations. By incorporating relevant KG triples into language models, we aim to enhance their ability to generate responses that are contextually appropriate and factually accurate. Our approach involves selecting KG triples based on their informativeness and relevance to the input context. Specifically, we prioritize triples where the tail entities are associated with factual or statistical information, ensuring that the generated responses are grounded in empirical data.\nEnvironmental sustainability is a paramount topic deserving attention in leveraging curated knowledge graphs (KG) to enhance language models (Change, 2018; Neale et al., 2021). By incorporating environmental sustainability-related KG triples, language models can raise awareness, facilitate informed decision-making, and promote proactive measures for mitigating environmental degradation.(Rockstr\u00f6m et al., 2009).\nAlthough works exist on the use of KG for hallucination mitigation, they may not utilize resources built explicitly for factual grounding. To this end, we build our KG to comply with the specific problem of hallucinations.  We observe a significant increase in factual accuracy by curating a list of relations specifically curated for fact grounding ( results shown Table 3). This paper presents our contribution to mitigating hallucinations in language models by integrating curated KG triples.\nOur work includes working on hallucination in environmental sustainability. Hence, our primary focus and contribution is the creation of a specialized KG tailored to counter hallucination-specific challenges and a detailed comparison of methods incorporating KG triples into language models to generate factual responses. By enhancing the factual grounding of language models, we aim to improve their reliability and trustworthiness in various natural language processing tasks."}, {"title": "Related Work", "content": "Recent research in natural language processing has highlighted hallucinations' prevalence and detrimental effects in language models (Yao et al., 2023). Studies have shown that language models, particularly large-scale neural architectures such as GPT (Generative Pre-trained Transformer) variants (Yenduri et al., 2023), often generate responses lacking factual accuracy or coherence with the input context. Hallucinations can manifest in various forms, including generating improbable scenarios, incorporating misinformation, and repeating nonsensical phrases(Lu et al., 2023; Yao et al., 2023). These hallucinatory outputs pose significant challenges in real-world applications, where generating contextually relevant and factually accurate responses is paramount. Major concerns surrounding hallucinations in language models revolve around their potential to propagate misinformation and erode user trust (Stringhi, 2023). Hallucinatory outputs can lead to the dissemination of false information, especially in domains where users rely on language models for factual knowledge or decision-making. Moreover, the lack of grounding, in reality, undermines the credibility of language models, hindering their adoption in critical applications such as medical diagnosis, legal advice, and education (Weidinger et al., 2021).\nTo address the issue of hallucinations in language models, researchers have explored various remediation strategies aimed at improving the models' factual grounding and coherence with the input context (Martino et al., 2023; Ji et al., 2023). One common approach involves fine-tuning language models on task-specific datasets or incorporating domain-specific knowledge during training (Zhang et al., 2023). By exposing the model to relevant data and constraints, fine-tuning methods seek to mitigate hallucinations by encouraging the generation of contextually appropriate responses. Another line of research focuses on post-generation filtering techniques, where hallucinatory outputs are identified and corrected through heuristics or external validation mechanisms. These approaches often rely on syntactic or semantic analysis to detect inconsistencies or factual inaccuracies in generated responses, enabling the model to refine its outputs iteratively (Tang et al., 2024).\nFactual guidance is crucial in substantiating claims made through digital channels (Van Der Pligt and Vliek, 2016; Baker and Martinson, 2001). Prior research consistently underscores the importance of fact-checking procedures in digital communication, emphasizing the pivotal role of trustworthiness, credibility, and message alignment in bolstering the effectiveness of guiding messages (Raven, 2008; Pornpitakpan, 2004). Moreover, there is a strong emphasis on cultivating authentic connections with audiences while eschewing deceptive tactics or self-serving agendas. These ethical principles serve as foundational tenets in the literature, underscoring the imperative of responsible communication practices, particularly within online platforms (Richardson, 2009; Brittain et al., 2020). Digital agents often leverage such strategies to enhance user engagement and drive desired actions (Samad et al., 2022; Wang et al., 2019). To address these concerns, ethical design principles prioritize transparency, user autonomy, and safeguarding privacy.\nKnowledge graphs offer a promising avenue for improving the factual grounding and contextual coherence of language models (Qian et al., 2024). By representing knowledge in a structured format consisting of entities, relationships, and attributes, knowledge graphs provide a rich source of factual information that can be leveraged to augment language understanding and generation capabilities. Previous research has demonstrated the utility of knowledge graphs in various natural language processing tasks, including question answering, entity linking, and semantic search. However, while these strategies have shown promise in mitigating hallucinations to some extent, they often fail to address the problem's root cause: the lack of comprehensive factual grounding in language models (Pavlick, 2023). To this end, we propose our knowledge-grounded fact correction method.\nRecent studies have highlighted several effective techniques for enhancing the relevance and accuracy of information in language model prompts. In-context learning involves providing examples within the prompt to guide the model's responses, effectively enabling the model to understand and mimic the desired output style and structure (Dong et al., 2023). Retrieval-augmented generation (RAG) combines the strengths of retrieval-based methods and generative models, where the model retrieves relevant documents or snippets from a large corpus and uses this information to generate more accurate and contextually relevant responses (Lewis et al., 2021a). Chain-of-thought prompting encourages the model to produce intermediate reasoning steps before arriving at a final answer, enhancing the model's ability to handle complex tasks and improving the overall coherence and depth of its responses. They collectively help in leveraging the model's capabilities to provide more precise and context-aware information (Wei et al., 2023)."}, {"title": "KG Creation", "content": "The KG primarily focuses on environmental sustainability. We chose this domain due to its vast range of topics and problems and its critical importance in addressing global challenges and promoting a sustainable future. To collect data for our KG, we leveraged Wikipedia. We chose Wikipedia for its open, collaborative nature, engenders a degree of accuracy and reliability in its content.\nAs an overview of the KG creation process, we first select a list of topics, extract additional relevant subtopics using cosine similarity metrics, crawl data, and then create relevant triples using our established relations."}, {"title": "Data Extraction", "content": "The data extraction process involved four main steps. First, we selected a list of pertinent topics related to environmental sustainability. Then, we extracted subtopics for each selected topic. Next, we retained only the most relevant topics from this extensive list. Finally, we conducted data crawling to gather a comprehensive corpus of text."}, {"title": "Topic Selection", "content": "We curated a list of relevant topics for environmental sustainability. These topics cover crucial aspects of environmental sustainability, including carbon neutrality, biodiversity conservation, and sustainable transportation. Our selection process prioritizes these interconnected issues, aiming to contribute meaningfully to global efforts in addressing climate change and promoting a more sustainable future.\n\u2022 Sample Topics: Renewable Energy, Climate Change, Greenhouse Gas Emissions, Carbon Footprint, Sustainable Development, Biodiversity Conservation, Ecological Footprint, Circular Economy, Clean Energy, Energy Efficiency, Carbon Neutrality,"}, {"title": "Sub Topic Selection and Filtering", "content": "We retrieve related links from each main heading's corresponding articles to expand our list of topics. This process involves accessing articles corresponding to our main topics and extracting embedded links. These links lead to related topics or subtopics, offering additional avenues for exploration.\nWith our extensive collection of environment-related topics, we aim to extract the most relevant titles using a BERT Embedding-based similarity score. We then calculate the relevance of each subtopic by computing the dot product of its embedding vector with that of our main topic heading. By leveraging the similarity scoring, we can prioritize the most pertinent subtopics for further investigation, facilitating a more targeted and informed exploration of environmental sustainability themes.\n\u2022 (Selected) Sub Topics from Renewable Energy: Agriculture, Biodiesel, Clean technology, Electricity, Fisheries management, Green building, Hydropower.\n\u2022 (Rejected) Sub Topics from Renewable Energy: Altitude, Capacitor, Foodscaping, Kick scooter, Manual labor."}, {"title": "Raw Text Extraction", "content": "Using the finalized set of topics, we again employ the Wikipedia API to extract a corpus of text for each corresponding web page. This step involves retrieving comprehensive-textual content from Wikipedia articles associated with each topic in our list. By utilizing the Wikipedia API, we can efficiently access structured data containing detailed information and descriptions of each topic. This rich text data collection can construct a knowledge graph encompassing interconnected concepts and relationships within environmental sustainability."}, {"title": "Relation Selection", "content": "To augment our model, we have established relevant relations to ensure the authenticity and credibility of the information presented. These relations have been developed by reverse-engineering curated support statements generated manually. Examples of the support statements and the relations are provided in the Appendix E.\nThe chosen 46 relations are: HasStatistic, HasNumericValue, HasUnitOfMeasurement, HasContext, HasSource, HasSubject, HasAction, HasAverageValue, HasMinValue, HasMaxValue, HasImpact, HasPercentileValue, HasTrend, HasComparison, HasImpact, HasCorrelation, Reduces, Saves, Decreases, Increases, EfficiencyOf, PercentageOf, RatioOfFrequencyOf, RateOf, VolumeOf, EmissionOf, ConsumptionOf, ImpactOf, BenefitOf, AdvantageOf, DisadvantageOf, RiskOf, PreventionOf, ProtectionOf, PreservationOf, ConservationOf, RecoveryOf, ManagementOf, RegulationOf, PolicyOf, InitiativeOf, StrategyOf, AdaptationOf, MitigationOf, HasPolicyTarget, HasCapacity"}, {"title": "Tail entity selection", "content": "After conducting tail entity selection on our KG, with a specific focus on entities containing either statistical information terms, such as up to, approximately, on average, can save, can reduce, etc, wherever possible. Some generated samples are as follows:\n\u2022 Tail entity: Reduced landfill waste by 30%, Relation: HasImpact\n\u2022 Tail entity: Over 15% Total Electricity Consumption, Relation: PercentageOf\n\u2022 Tail entity: 80-90% Reduction in Shower Water Consumption, Relation: PercentageOf"}, {"title": "Generating Triples", "content": "Using the identified relations and statistical tail entities, we construct KG triples, each consisting of a subject, a relation, and an object. A few examples are:\n\u2022 (\"Renewable energy\", \"HasNumericValue\", \"20% to 28%\")\n\u2022 (\"Fossil energy\", \"HasSubject\", \"Global electricity supply\")\n\u2022 (\"Renewables\", \"Reduces\", \"Dependence on fossil fuels\")\n\u2022 (\"Global electricity generation\", \"HasNumericValue\", \"90%\")\nThese triples provide structured information about various aspects of renewable energy, fossil energy, national renewable energy markets, and global electricity generation."}, {"title": "Overall flow and Statistics", "content": "Figure 2 provides a sample for the KG creation process. The relation extraction is done using similarity matching using BERTScore (Zhang et al., 2019). We experimented with the triples creation process using 2 LLMs, Llama3 (AI@Meta, 2024), and ChatGPT (Floridi and Chiriatti, 2020). We were able to get similar results using both; however, the prompt for Llama3 required a lot more tuning and information to generate comparable results. The prompts have been added to Appendix"}, {"title": "Methodology", "content": "Our methodology integrates language models with curated knowledge graphs to generate factually accurate and coherent text responses. We evaluate multiple hallucination mitigation approaches. This section details our process, including topic extraction, information retrieval, triple generation, and response formulation."}, {"title": "Embedding-Based Matching", "content": "To align the context C provided as input with relevant triples from the knowledge graph KG, we employ an embedding-based matching technique. The matching function M computes the semantic similarity between the context C and the triples in KG. Let $\\text{Vec}$ and $\\text{V}_T$ represent the embeddings of the context C and the $i$th triple $T_i$, respectively. The similarity score $sim(C, T_i)$ is calculated as the cosine similarity between $\\text{V}_c$ and $\\text{V}_{T_i}$:\n$\\displaystyle sim(C, T_i) = \\frac{\\text{V}_c \\cdot \\text{V}_{T_i}}{\\|\\text{V}_c\\| \\|\\text{V}_{T_i}\\|} $\nThe matching function M selects the top k triples with the highest similarity scores:\n$M(C, KG) = \\{T_1, T_2, ..., T_k\\}$"}, {"title": "Triple Conversion", "content": "Once the relevant triples are identified, we convert them into coherent sentences that can seamlessly integrate into the generated response. The conversion function F transforms each triple $T_i$ into a sentence $S_i$ ensuring grammatical correctness and contextual relevance:\n$F(T_i) = S_i$\nwhere $S_i$ is a natural language sentence derived from the $i$th triple $T_i$. For each of the LLM, the this corresponds to converting the triple(T) into a textual sentence(S) using the respective LLM."}, {"title": "Response Generation", "content": "Incorporating these sentences into the language model's output enriches the generated response with factual information grounded in empirical data. Let LM represent the language model, and let $R_{LM}(C)$ denote the initial response generated by LM given the context C. The final response R integrates the sentences derived from the relevant triples:\n$R(C, KG) = R_{LM}(C) + \\{S_1, S_2, ..., S_k\\}$\nBy combining the context C, the initial response $R_{LM}(C)$, and the sentences $\\{S_1, S_2, ..., S_k\\}$, we ensure that the final response R is both contextually relevant and factually accurate."}, {"title": "Experimental Setup", "content": "We conduct experiments employing various advanced language models to explore their effectiveness in our study. The models utilized in our experiments are detailed below:\n\u2022 Llama3 (Large Language Model Meta AI): Llama3 (AI@Meta, 2024), developed by Meta AI, is a foundational Large Language Model(LLM) pre-trained on an extensive dataset of 2 trillion tokens.\n\u2022 GPT-2: GPT-2 (Radford et al., 2019), a Transformer-based model by OpenAI, has undergone pre-training on a vast corpus of English text using self-supervised learning.\n\u2022 Blenderbot: Blenderbot (Xu et al., 2020), is a chatbot developed by Meta, designed to engage in open-domain conversations. It enables finetuning to perform dialogue generation, knowledge grounding, and persona-based conversations.\n\u2022 GODEL: GODEL (Peng et al., 2022) is a large-scale pre-trained model tailored for goal-directed dialogues. Parameterized with a Transformer-based encoder-decoder architecture generates responses grounded in external textual knowledge.\n\u2022 Chat-GPT: Chat-GPT (Floridi and Chiriatti, 2020), developed by OpenAI, boasts remarkable fluency and context retention. Chat-GPT excels in capturing dependencies and nuances by employing a deep Transformer architecture, allowing for nuanced and contextually appropriate replies."}, {"title": "Hallucination mitigation Techniques", "content": "We explore three prevalent mitigation techniques. The triple information is added in three ways. The In Context learning (IC) method adds the information as concatenation. The Chain of Verification (Cov) method adds this information as a secondary prompt with examples. Retrieval Augmented Generation (RAG) adds this information in the form of concatenation of embeddings of context and triples.\n\u2022 Retrieval Augmented Generation (RAG): By grounding responses in external, verifiable data sources, Retrieval Augmented Generation (Lewis et al., 2021b) reduces the chances of hallucination by enhancing factual accuracy.\n\u2022 In-Context learning: Providing relevant examples within the prompt enables In-Context learning (Dong et al., 2024) models to reduce hallucinations by following patterns of correct behavior demonstrated in the input.\n\u2022 Chain-of-Verification: CoV (Dhuliawala et al., 2023) mitigates hallucinations by validating intermediate steps of reasoning or generation, ensuring each output is logically consistent and factually correct before proceeding."}, {"title": "Evaluation Metrics", "content": "We evaluate our system using 2 broad parameters. The first focuses on evaluation of generation quality and the other focuses on evaluation of the degree of hallucination.\nFor evaluation of generation quality we utilize syntactic metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016); embedding based metrics like Skip-Thought Cosine Similarity (STCS) (Kiros et al., 2015), Greedy Matching Score (GMS) (Rus and Lintean, 2012), Vector Extrema Cosine Similarity (VECS) (Forgues et al., 2014), and Embedding Average Cosine Similarity (EACS) (Landauer and Dumais, 1997) to evaluate semantic coherence and relevance between generated responses and ground truth. We also utilize human evaluation metrics like Dialogue Engagingness (DE): which evaluates the extent to which a conversation or dialogue is engaging, captivating, and can hold the participants' attention, Fluency (F): (Hoffman, 2019) which measures the smoothness and naturalness of language expression in communication or text generation, and Context relevance (CR): (Sai et al., 2022) measures the degree to which information or responses align with the situational or thematic context, ensuring proper coherence and appropriateness.\nFor evaluation the degree of hallucination mitigation, we utilize the evaluation metric FacTool (Chern et al., 2023) which is a versatile framework designed to address the challenge of detecting factual errors in generative AI outputs. This framework expands the conventional understanding of facts, focusing on evaluating generative AI models. This approach helps with thorough fact-checking of complex AI-generated texts. We also manually check for the hallucination mitigation using the task specific metric Factual Consistency:, which measures the consistency of factual information between the original and generated response text. The three methods of information infusion measure are, Retrieval Augmented Generation (FC-RAG), In Context Learning (FC-IC), Chain-of-Verification (FC-CoV).\nWe employ three human evaluators for all human evaluations. The human evaluation are graded on a scale of 1-5, with 1 being the lowest score and 5 as the highest."}, {"title": "Results and Analysis", "content": "Generated Response Quality: The results in Table 2 demonstrate the performance of various models across multiple automated evaluation metrics. ChatGPT consistently performs above the rest, achieving the high scores in BLEU_1 (0.391), \u039c\u0395\u03a4\u0395OR (0.247), ROUGE_L (0.342), CIDEr (0.895), SPICE (0.386), and embedding-based metrics such as STCS (0.906), EACS (0.982), VECS (0.645), and GMS (0.847), indicating its strong syntactic and semantic coherence. Llama3 follows closely, particularly in CIDEr (0.875) and Fluency (4.8), while GPT2 shows a more modest performance, particularly in semantic metrics like EACS (0.924) and GMS (0.805). Blenderbot, despite achieving relatively lower scores across metrics, maintains competitive Dialogue Engagingness (DE) (4.2), demonstrating some ability to engage in conversation despite its lower generation quality scores. Overall, ChatGPT outperforms others in both automated syntactic and semantic metrics, as well as in fluency and dialogue engagement, making it the most robust model in this evaluation. These results are in line with the mostly reliable assumption of relation between model size and response quality.\nHallucination mitigation : The results in Table 3 show the factual accuracy improvements achieved by different models using various knowledge grounding methods: Retrieval-Augmented Generation (FC-RAG), In-Context Learning (FC-IC), and Chain-of-Verification (FC-CoV). ChatGPT outperforms other models in FC1 (3.2), indicating higher baseline factual accuracy before knowledge infusion, and it maintains strong performance across all grounding methods, particularly in FC-RAG (4.8) and FC-IC (4.7). Llama3 follows closely, especially in FC-RAG (4.8) and FC-IC (4.7), demonstrating comparable capabilities. GODEL, while showing modest performance in FC1 (2.1), improves significantly when utilizing FC-RAG (4.3) and FC-IC (4.1). GPT2 and Blenderbot exhibit lower baseline factual accuracy (1.9 and 2.1, respectively) and achieve slightly lower scores across grounding methods, especially in Chain-of-Verification (3.2 and 3.3). Overall, Retrieval-Augmented Generation (FC-RAG) consistently enhances factual accuracy the most, across all models, with ChatGPT and Llama3 leading the evaluation.\nFacTool Benchmark: The FacTool benchmark reports the factual accuracy of a claim using the average_claim_level_factuality score parameter, which is generated by dividing our response into claim(s) using GPT-4 and fact checking each claim using data retrieval via google search API. We report a score of 0.87 for our method of knowledge infusion using RAG on Llama3. The ChatGPT response to the same context resulted in a score of 0.81. This improvement underscores the effectiveness of integrating external knowledge sources and sophisticated prompting strategies to enhance the reliability of information provided by language models.\nThe findings show that Retrieval-Augmented Generation (FC-RAG) consistently yields the highest improvements in factual accuracy across all models, outperforming both In-Context Learning and Chain-of-Verification methods.\nGiven our original text, we generate utterances using the relevant triples. Utterances produced by Llama3 were mainly similar to the original text, but ChatGPT provided additional words that enhanced the response. We have compared the original utterance and the generated utterance provided by ChatGPT, along with the relevant triples in Table 1. The examples provided in Table 1 illustrate the integration of curated knowledge graph triples into language model responses aimed at mitigating hallucinations. A general trend emerges where the model is augmented with factual information derived from the corresponding knowledge graph triple. For instance, in the first example, the original statement about metering's potential to reduce consumption is enhanced by specifying the percentage reduction achievable through Universal Water Metering. Similarly, subsequent examples showcase how factual details enrich the generated responses, such as the percentage of households with water metering in the UK or the volume of water used annually for showering in the United States. This consistent pattern underscores the effectiveness of incorporating curated knowledge graph triples into language model outputs, ensuring that the generated responses are grounded in factual and contextually relevant information, thereby reducing the likelihood of hallucinations."}, {"title": "Conclusion", "content": "This paper proposes a novel approach to mitigate hallucinations in language models by incorporating curated knowledge graph (KG) triples. By grounding language models in factual information extracted from knowledge graphs, we aim to improve the reliability and trustworthiness of their generated responses. Our method involves selecting informative KG triples based on context and integrating them into language models to enhance their factual grounding and coherence with the input context.\nThrough experimental evaluations, we have demonstrated the effectiveness of our approach in reducing hallucinatory responses across various natural language processing tasks. By providing language models access to empirical data through curated KG triples, we have observed improvements in their outputs' factual accuracy and context relevance. These findings reiterate the potential of knowledge graphs as a valuable resource for enhancing language model performance and addressing the challenges of hallucination. Focusing on curated knowledge graphs for fact verification can significantly enhance language model performance.\nWhile our approach shows promising results, there are several avenues for future research. Further investigation is needed to explore the scalability and generalizability of our method across different downstream tasks and languages. Additionally, refining the techniques for selecting and incorporating KG triples into language models can lead to even more significant improvements in mitigating hallucinations."}, {"title": "Limitations", "content": "Our proposed work has certain limitations. One limitation of this study is the focus on curated knowledge graphs to enhance language model performance. While these graphs offer structured factual information, they may not cover all human knowledge or capture emerging trends. This reliance may introduce biases and overlook contextual nuances, potentially leading to lower performance in tasks that are not fact-related. We plan to explore this in future works."}, {"title": "Ethics Statement", "content": "We split the ethical considerations/guidelines into two parts during the implementation of the work: 1) Regarding the generated Resources and 2) Regarding the usage of methods implemented.\n1. The ethical statement regarding the generated Resources: Given their prowess to be utilized in various tasks, we recognize that there are substantial states when dealing with language models. Although the models are utilized locally, to keep the setup fair, we remove all personal information regarding the entities in conversation and replace them with specialized tags to ensure no data leakage. This included personal details like name, gender, ethnicity, etc. Organizational details were kept, as they are relevant to the credibility of initiatives. The paper provides a new KG resource and a guideline for their usage. We do not claim that the best model for us will perform best in all cases.\n2. Regarding the usage of methods implemented in the paper: Manipulation in the age of generative AI is a concern and a genuine research topic. Our work was done inside an Institute of repute and in association with a global multinational corporate organization. Both of these institutions follow their internal ethics and responsible AI protocols. Manipulation detection/prevention is a separate research topic. Prevention of unethical practices in the current research was done by following the organizational checks and protocols. The resources (dataset and code) generated will be provided on-demand to ensure usage for research purposes only. A declaration statement and research proposal will be mandatory."}, {"title": "Experimental Setup", "content": "The base LM model is GPT2-medium(Radford et al., 2019), which forms the foundation for generating text. We utilize the GODEL base version 2 and Llama3-3B version 3 4for our experiments.\nThe training was done on four RTX-2080TI-11GB GPUs, with CUDA 11.2, using the Autotrain package. The experiment took eight days' worth of cumulative runtime.\nGPT2, GODEL, Blenderbot and Llama3 were all fine-tuned for 30 epochs with a batch size of 4. The GPT2-rl model was optimized for 438 total steps over a batch size of 128. The intent and other classifier modules were fine-tuned for 15 epochs each for a batch size of 64.\nWe compared Open Source models, including Llama3-8B, Blenderbot V1, Falcon-7B, MPT-7B, OPT -6.7B, and Alpaca 7-B. Out of these, Llama3 was selected as the best one after internal discussion and our experiences with previous research works."}, {"title": "Human Evaluation Details:", "content": "Eight human participants conducted manual checks and guideline-based scoring to ensure the conversations' internal coherence, content consistency, and naturalness. The team rated the dialogues on a Likert scale from 1 to 5, adhering to predefined guidelines covering grammatical correctness, thematic consistency, language appropriateness, user profile consistency, and clinical sensitivity. The guidelines were as follows:\n\u2022 Participants were tasked with identifying grammatical errors, subject-verb agreement issues, and improper word usage within the conversations.\n\u2022 They checked for thematic coherence and logical flow, aiming to maintain consistency and avoid abrupt topic shifts or dialogue discontinuity.\n\u2022 Evaluating the appropriateness of language used in the conversations, particularly in terms of natural tone and flow, formality, and cultural sensitivity, was emphasized.\n\u2022 Ensure that user attributes and characteristics remain consistent throughout the conversation to maintain coherence and believability.\n\u2022 Participants were requested to pay special attention to the portrayal of clinical interactions, aiming to enhance politeness and empathy.\nAn inter-evaluator Kappa (McHugh, 2012) agreement ratio of 80.3%, 82%, 84% is observed among the experts for internal coherence, content consistency, and naturalness, respectively. After conducting manual checks and receiving feedback from users, the data significantly improved, reflecting a refined level of quality control."}, {"title": "Human Evaluation", "content": "We utilized three human annotators (one employed annotators, one non-author Ph.D. scholars, and one masters student) for human evaluation. The inter-annotator agreement between the three human annotators was 83.5%. We tried with FactScore evaluation method as well but since it was similar to FactTool we eventually decided to choose only one to reduce redundancy in evaluation methods."}, {"title": "Prompts for LLM-based KG creation", "content": "We use the following prompts to generate triples from raw text. The list of relations is passed to the prompts from Appendix E. The relation examples were unnecessary for ChatGPT, but Llama3 required the examples to generate proper triples. Also, the list of sample entities was not necessary for ChatGPT."}, {"title": "Llama3", "content": "\u201c\u201d\u201dGiven the provided relations and sample entities related to environmental sustainability, create a comprehensive set of KG triples by establishing relationships between the entities based on the given relations.\nRelations: {Relations}\nSample entities:\nRainwater Harvesting Permaculture Urban Green Spaces Regenerative Agriculture Ocean Conservation Sustainable Transportation Wildlife Corridors Energy Audits Low-impact Development Community Gardens Green Building Standards Forest Stewardship Public Transit Infrastructure Carbon Sequestration Sustainable Forestry Energy Star Rating Pollution Prevention Green Technology Food Security Land Use Planning\nInstructions:\n1) Identify and establish relationships with other entities based on the provided relations for each sample entity. 2) Provide KG triples for each relation and entity combination, ensuring logical connections within the environmental sustainability domain. 3) Use the provided sample entities as subjects or objects to construct KG triples. 4) Utilize the relations to create meaningful connections between entities, reflecting various aspects of environmental sustainability such as conservation practices, sustainable technologies, and ecosystem management. 5) Ensure that KG triples are coherent and relevant within environmental sustainability, reflecting real-world relationships and concepts. 6) If there are no more triples, stop. 7) Make sure there are no repeated triples.\u201d"}, {"title": "ChatGPT", "content": "\u201d\u201dGiven the provided relations and sample entities related to environmental sustainability, create a comprehensive set of KG triples by establishing relationships between the entities based on the given relations.\nRelations: {Relations}\nInstructions:\n1) Identify and establish relationships with other entities based on the provided relations for each possible entity. 2) Provide KG triples for each relation and entity combination, ensuring logical connections within the environmental sustainability domain that contain a numerical value, as per the sample triples provided. 3) Utilize the relations to create meaningful connections between entities, reflecting various aspects of environmental sustainability such as conservation practices, sustainable technologies, and ecosystem management. 4) Ensure that KG triples are coherent and relevant within environmental sustainability and contain a numerical value, reflecting real-world relationships and concepts. 5) Make sure there are no repeated triples. 6) If there are no more triples, stop.\u201d\u201d"}, {"title": "Human Curated Fact Statements:", "content": "To properly curate the list of relations, we asked human annotators to generate factual statements to provide environmental sustainability suggestions."}, {"title": "Data Quality Control", "content": "Four human participants conducted manual checks and guideline-based scoring to ensure the claims' internal coherence, content consistency, and naturalness. The team rated the dialogues on a Likert scale from 1 to 5, adhering to predefined guidelines covering grammatical correctness, thematic consistency, language appropriateness, user profile consistency, and clinical sensitivity. The guidelines were as follows:\n\u2022 Participants were tasked with identifying grammatical errors, subject-verb agreement issues, and improper word usage within the conversations.\n\u2022 They checked for thematic coherence and logical flow, aiming to maintain consistency and avoid abrupt topic shifts or dialogue discontinuity.\n\u2022 Evaluating the appropriateness of language used in the conversations, particularly regarding natural tone and flow, formality, and cultural sensitivity, was emphasized.\n\u2022 Ensure that user attributes and characteristics remain consistent throughout the conversation to maintain coherence and believability.\n\u2022 Participants were requested to pay special attention to the portrayal of clinical interactions, aiming to enhance politeness and empathy.\nAn inter-evaluator Kappa (McHugh, 2012) agreement ratio of 80.3%, 82%, 84% is observed among the experts for internal coherence, content consistency, and naturalness, respectively. After conducting manual checks and receiving feedback from users, the data significantly improved, reflecting a satisfactory level of quality control."}, {"title": "Support Statements and Relation", "content": "Given below are examples of support statements and relations that were utilized to create our KG."}, {"title": "Support Statements", "content": "Here are a few examples of manually created support statements from which we reverse-engineered a list of relevant relations.\n\u2022 Approximately 8 million metric tons of plastic waste enter the oceans every year, endangering marine life and ecosystems.\n\u2022 Residential energy use accounts for about 20% of global CO2 emissions.\n\u2022 Deforestation accounts for approximately 11% of global greenhouse gas emissions.\n\u2022 The transportation sector is responsible for around 14% of global greenhouse gas emissions.\n\u2022 The average food item in the United States travels about 1,500 miles to reach consumers.\n\u2022 Approximately one-third of all food produced globally goes to waste.\n\u2022 Energy-efficient appliances can reduce energy consumption by up to 50% compared to standard models.\n\u2022 In 2020, renewable energy sources accounted for 29% of global"}]}