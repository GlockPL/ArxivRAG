{"title": "Towards Cross-Lingual LLM Evaluation for European Languages", "authors": ["Klaudia Thellmann", "Bernhard Stadler", "Michael Fromm", "Jasper Schulze Buschhoff", "Alex Jude", "Fabio Barth", "Johannes Leveling", "Nicolas Flores-Herr", "Joachim K\u00f6hler", "Ren\u00e9 J\u00e4kel", "Mehdi Ali"], "abstract": "The rise of Large Language Models (LLMs) has revolutionized natural language processing across numerous languages and tasks. How- ever, evaluating LLM performance in a consis- tent and meaningful way across multiple Eu- ropean languages remains challenging, espe- cially due to the scarcity of multilingual bench- marks. We introduce a cross-lingual evalua- tion approach tailored for European languages. We employ translated versions of five widely- used benchmarks to assess the capabilities of 40 LLMs across 21 European languages. Our con- tributions include examining the effectiveness of translated benchmarks, assessing the impact of different translation services, and offering a multilingual evaluation framework for LLMs that includes newly created datasets: EU20- MMLU, EU20-HellaSwag, EU20-ARC, EU20- TruthfulQA, and EU20-GSM8K. The bench- marks and results are made publicly available to encourage further research in multilingual LLM evaluation.", "sections": [{"title": "1 Introduction", "content": "The development of Large Language Models (LLMs) has brought transformative advancements to natural language processing, enabling sophis- ticated tasks such as question-answering, summa- rization, and machine translation. However, despite these advances, the evaluation of LLMs across lan- guages, especially those beyond English and Chi- nese, remains a significant challenge. Initiatives like CLEF\u00b9, FIRE2, and WMT3 have focused on multilingual evaluations, yet there is a lack of ded- icated benchmarks for systematically comparing LLM performance across the languages of Europa. Creating bespoke benchmarks for each language is a costly and time-consuming endeavor. This not only limits the scalability of evaluations but also results in a fragmented understanding of model performance across different languages. In many cases, evaluations are conducted in isolation, with- out a basis for meaningful comparison between languages. Moreover, human evaluation is widely recognized as the gold standard for assessing lan- guage model performance, as it captures nuances that automated benchmarks may miss. However, human evaluations are resource-intensive and diffi- cult to scale across multiple languages.\nTo address these challenges, we explore whether automatically translated benchmarks can provide a viable method to estimate human preferences in chat scenarios. While human evaluation would likely be the most accurate way to assess LLM per- formance, the resource demands of manual dataset creation and evaluation make it impractical for large-scale multilingual comparisons. Instead, this approach aims to reduce these resource demands while ensuring comprehensive evaluation coverage across all languages.\nIn this paper, we focus on evaluating multilin- gual LLMs by utilizing translated versions of exist- ing benchmarks. Our goal is to determine the effi- cacy of translated benchmarks and assess whether they can substitute manually generated ones, al- lowing for a more scalable and uniform evaluation across multiple languages.\nWe make the following contributions:\n\u2022 We evaluate the performance of 40 state-of- the-art LLMs for five multilingual datasets covering English and 20 additional languages in Section 4.\n\u2022 We assess whether translated benchmarks re- flect human preferences to evaluate their via- bility in Section 5.\n\u2022 We examine how the choice of translation ser- vice (i.e., DeeplPro and ChatGPT) affects the"}, {"title": "2 Related Work", "content": "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023; Goyal et al., 2022; Conneau et al., 2018) or by using LLMs to translate an exist- ing benchmark into multiple languages (Lai et al., 2023; Tiedemann, 2012).\nCreating a multilingual dataset with human anno- tators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each lan- guage instead of heterogeneous multilingual bench- marks for a cross-lingual analysis (Lewkowycz et al., 2022; Tiedemann, 2012).\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their rel- evance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and time- efficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022).\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these lan- guages, in particular, can lead to imbalanced eval- uations across the benchmarks (Team et al., 2022; Goyal et al., 2022; Conneau et al., 2018). There- fore, the selection of languages is a central decision to make when creating a multilingual benchmark.\nIn addition to the choice of languages, it is cru- cial to consider the range of tasks that need to be covered for a benchmark. For example, creating a multilingual benchmark for a specific task pro- vides valuable insights into model performance for that exact task but says little about a model's gen- eral applicability (Conneau et al., 2018). This also includes benchmarks that, for example, cover the question-answering (QA) task for a wide range of domains, as only the QA task is tested here (Lewis et al., 2020).\nAnother approach is to cover multiple tasks to evaluate language models broadly. An example is the OKAPI dataset published by Lai et al. (Lai et al., 2023). For this benchmark Three datasets (AI2 Reasoning Challenge (ARC), HellaSwag, and MMLU) were translated into 26 languages: eight high-resource, 11 medium-resource, and seven low-resource languages (Lai et al., 2023).\nBuilding on this approach, we have expanded the linguistic reach through additional translations with special consideration on European languages and increased the variety of tasks for a wider capa- bility test of LLMs. Furthermore, we show that our data pre-processing has improved quality in certain language areas (See Section 3)."}, {"title": "3 Experimental Setup", "content": "Firstly, we describe in Section 3.1 the translation process of the benchmarks, secondly Section 3.2 details our evaluation framework and process."}, {"title": "3.1 Translation Process", "content": "We chose DeepL as translation service for its balance between translation accuracy and scal- ability. We translated five well-known data- sets, ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2022), GSM8K (Cobbe et al., 2021), and MMLU (Hendrycks et al., 2020) from English into 20 European languages. These datasets en- compass a mix of multiple-choice and open-ended generation tasks, each of which presented unique translation challenges. The translations maintained the original structure of each task to ensure the benchmarks were as consistent as possible across languages. Using DeepL's XML-tag handling, we avoided issues with prompt formatting and ensured that key contextual elements, such as 'Question'"}, {"title": "4.1 Global European LLM Evaluation", "content": "This section provides an overview of the perfor- mance of several LLMs on various tasks and lan- guages, grouped by their number of parameters in small-sized (\u22649B parameters), medium-sized (>9B & <27B parameters) and large (>27B pa- rameters) models. The models are evaluated on our newly introduced tasks EU21-ARC, EU21- GSM8K, EU21-HellaSwag, EU21-MMLU, and EU21-TruthfulQA. An extensive overview of all models are provided in the Appendix as Tables (cf. Table 8, 9) and task specific heatmaps (cf. Fig- ure 5, 6 7, 8, 9)."}, {"title": "Small-Sized Models (\u22649B Parameters)", "content": "Among the small-sized models, the focus is on Gemma-2- 9b-Instruct (Team et al., 2024b), Meta-Llama-3.1- 8B-Instruct (Dubey et al., 2024), Mistral-NeMo- Minitron-8B-Base (Sreenivas et al., 2024), and Qwen2-7B-Instruct (Yang et al., 2024), further mod- els with a lower average performance like Gemma- 7b (Team et al., 2024a) are in the appendix Table 8 and Table 9\nGemma-2-9b-Instruct consistently performs well (cf. Figure 1), by having a narrow interquar- tile ranges, signaling low variability and therefore stable performance in all languages. On the multi- lingual grade school math word benchmark EU20- GSM8K the median accuracy is worse than Meta- Llama-3.1-8B-Instruct, indicating lower math ca- pabilities. In EU20-MMLU the model shows a robust behaviour, but the median is slightly lower than Qwen2-7B-Instruct. Both latter benchmarks indicate that the capacity of small models might not allow for both a reliable performance on all"}, {"title": "Medium-Sized Models (>9B & <27B Parameters)", "content": "This analysis (cf. Figure 2) focuses on medium-sized LLMs such as Mistral-Nemo-Instruct-12.2B, Mixtral-8x7B- Instruct-v0.1 (Jiang et al., 2024), Phi-3-medium- 14B-4k-Instruct (Abdin et al., 2024) and Vicuna-13b-v1.5 (Peng et al., 2023).\nMixtral-8x7B-Instruct-v0.1 shows the highest median performance on four out of the five tasks. Only on EU21-GSM8K its performance is lack- ing behind Mistral-Nemo-Instruct-12.2B, indicat- ing worse performance on math related tasks. However, its performance tends to be more vari- able than Mistral-Nemo-Instruct-12.2B_2407, with some languages like Latvian displaying signifi- cantly lower scores, reflecting higher inconsistency across the board.\nMistral-Nemo-Instruct-12.2B_2407 delivers solid results across all tasks and languages, with a particularly strong median performance in the EU21-GSM8K task. As evidenced by its narrow interquartile ranges the model signals low variability across all 21 languages compared to the other models.\nPhi-3-medium-14B-4k-Instruct presents compet- itive results mostly in English by having the highest GSM8K, MMLU values but lags vastly behind the previous two models in median peformance across all languages and tasks, indicating only a small amount of multilingual training corpora. Lastly, Vicuna-13b-v1.5 based on the older LLama-2-13B is not competitive on any task, due to its missing multilingual capabilities.\nCompared to the small-sized category, Mixtral- 8x7B-Instruct-v0.1 provides a substantial gain in EU21-Hellswag by achieving an median perfor- mance of 0.64, an absolute increase of 4% com- pared to Gemma-2-9b-Instruct. On EU21-GSM8K Mistral-Nemo-Instruct-12.2B achieves 57% accu- racy a small increase of 1% compared to Meta- Llama-3.1-8B-Instruct. On EU21-MMLU Qwen2- 7B achieved a accuracy of 59.4%, slightly behind medium sized models such as Mistral-Nemo-Base- 12.2B with 60.2% and Mixtral-8x7B-Instruct-v0.1 with an performance of 60.8%.\nOverall the medium category provides only small task dependend gains with specific mod- els compared to the smaller Gemma-2-9b-Instruct model, indicating its strength in multilinguality."}, {"title": "Large-Sized Models (\u226527B Parameters)", "content": "This analysis (cf. Figure 3) focuses on large-sized LLMs such as Gemma-2-27b-Instruct (Team et al., 2024b), Vicuna-33b-v1.3 (Peng et al., 2023), Meta-Llama-3.1-70B-Instruct (Dubey et al., 2024) and c4ai-command-r-35B-v0113. Gemma-2-27b- Instruct and Meta-Llama-3.1-70B-Instruct reliable outperforms the other models across most tasks, with median scores often exceeding an accuracy of 0.7. Their performance is not only high but also sta- ble across different languages, as evidenced by its narrow interquartile ranges, signaling low variabil- ity. Probably due to the higher capacity (compared to Gemma-2-9b), Gemma-2-27b-Instruct can now also score higher on specialized knowledge tasks such as MMLU and GSM8K.\nVicuna-33b-v1.3 and c4ai-command-r-35B-v01 perform worse compared to the previous two mod- els. This indicates a monolingual training corpora paired with a monolingual tokenizer.\nEspecially Gemma-2-27b-Instruct convinces with a remarkable performance. It provides a vast improvements over the small and medium sized LLMs and is even competitive with the more than twice as large Meta-LLama-3.1 70B model."}, {"title": "4.2 Impact of available training corpora", "content": "To better analyze the performance of LLMs across different languages, we categorized our target lan- guages into high-resource (HRL) and medium- resource (MRL) groups, based on their proportion in the CommonCrawl dataset, which serves as the primary pre-training source for many LLMs. High- resource languages (HRL) had a CommonCrawl representation of 1% or more, whereas medium- resource languages (MRL) had between 0.01% and 1%. The language statistics accompanied with evaluation results of the LLama-3.1-70B-Instruct (Dubey et al., 2024) model is depicted in Table 1.\nAs can be seen there is a downwards trend with outliers in Swedish and Danish, both Germanic languages that might profit from English or other Germanic training corpora (cf. Table 1)."}, {"title": "4.3 Influence of Language Family", "content": "In this subsection we investigate how the LLM performance depend on the language family. We categorize the larger language groups based on their corresponding language family like the following:\n\u2022 Germanic: Danish, German, English, Dutch, Swedish (in total 53.38% of CC)\n\u2022 Romance: Spanish, French, Italian, Por- tuguese, Romanian (in total 13.9% of CC)\n\u2022 Slavic: Bulgarian, Czech, Polish, Slovak, Slovenian (in total 3.58% of CC)\n\u2022 Not categorized: Greek (hellenic), Hungar- ian (ugric), Estonian (finnic), Finnish (finnic), Lithuanian (baltic), Latvian (baltic)\nExamining the average accuracies (cf. Ap- pendix Table 10, 11, 12, 13 14, 15) across the three language families, we observe that models gener- ally achieve higher performance on Romance and Germanic languages compared to Slavic languages. For instance, the Gemma-2-27b-Instruct(Team et al., 2024b) model attains average accuracies of 0.727 on Germanic languages (cf. Table 10), 0.716 on Romance languages (cf. Table 12), and 0.694 on Slavic languages (cf. Table 14).\nSimilarly, the Meta-Llama-3.1-70B-Instruct (Dubey et al., 2024) model achieves average ac- curacies of 0.745 (Germanic), 0.726 (Romance), and 0.680 (Slavic), reinforcing the trend of lower performance on Slavic languages. In the follow- ing we compare the model performance on similar sized (in CC %) languages such as Romanian (Ro- mance, 0.52%), Swedish (Germanic, 0.63%) and Polish (Slavic, 1.09%). Table 1 indicates that even though, Swedish is less represented than Polish, it achieves higher performance across all tasks. Also Romanian as the smallest represented language achieves higher performance across all five tasks, indicating that the language family is of influence in the performance analysis."}, {"title": "5 Correlation with Human Preference Scores", "content": "We examine the correlation between our introduced EU21 benchmarks and the Elo scores from the LMSYS Chatbot Arena 16 to better understand how human preferences are reflected."}, {"title": "6 Influence of the translation service", "content": "While OKAPI (Lai et al., 2023) and our evalua- tion datasets share a similar structure, they differ in key aspects such as translation methodology and sample alignment. The most notable differ- ence between the two datasets is the translation approach: The OGX dataset was translated using DeepL, whereas OKAPI employed ChatGPT for this task. The translations for the OKAPI tasks cover 31 languages, including both EU and non-EU languages. For the OpenGPT-X evaluation datasets, we focused on translations into 20 European lan- guages. The common and new set of EU languages between the OKAPI and OpenGPT-X are shown in Table 3.\nIn this section, we present a correlation analysis with a focus on the ARC, HellaSwag, and MMLU tasks, alongside a COMET score (Rei et al., 2023) evaluation to assess the translation quality across both datasets.\nThe idea behind the correlation analysis is to record18whether the model responded correctly or incorrectly for each OKAPI and OGX sample and to examine if the predictions are correlated, allow- ing direct performance comparison on sample-level on the same data. For a sample-level compari-"}, {"title": "7 Conclusion", "content": "This work provides a comprehensive framework for evaluating Large Language Models (LLMs) across multiple European languages by utilizing translated versions of widely-used benchmarks. Our experi- ments demonstrate that translated benchmarks can"}, {"title": "8 Limitations", "content": "A hindrance in achieving benchmarks compara- ble across languages is localization of cultural as- pects. Toxicity benchmarks in particular can suffer from this, as stereotypes and terms related to them likely do not translate directly been cultures and hence languages. To a lesser extent this applies to all benchmarks requiring some degree of common sense and world knowledge. A team of reseachers has launched an initiative19to correct some of these cultural biases in multilingual datasets (predom- inantly MMLU), through crowd-sourced manual work. While we are aware of this initiative, we opted not to conduct a similar enterprise in order to avoid doubled efforts. Aligning the results from the initative with our own translations will be an avenue for further work.\nFurthermore the degree to which a translation for an given task is not always feasible. The pronoun-resolution benchmark WinoGrande (Sak- aguchi et al., 2019) for instance is posed as a mul- tiple choice task, where a on object or subject in a sentence is masked and each choice constitutes a replacement of the mask. The model is then tasked with selecting the sentence that is semanti- cally correct. Tables ?? and ?? demonstrate how in languages with gendered nouns or gendered ad- jectives in predicative use, the benchmark becomes easier due to strong grammatical signals indicat- ing the correct choice of subject or object. It is for that reason we decided against translation of WinoGrande.\nIn order to compare the translations of OGX with the original benchmark or other translations such as OKAPI, the samples must be mapped. This requires a one-to-one mapping. However, this map- ping was impossible for all benchmarks; therefore,"}]}