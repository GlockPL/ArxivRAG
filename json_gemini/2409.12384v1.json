{"title": "Privacy-Preserving Student Learning with Differentially Private Data-Free Distillation", "authors": ["Bochao Liu", "Jianghu Lu", "Pengju Wang", "Junjie Zhang", "Dan Zeng", "Zhenxing Qian", "Shiming Ge"], "abstract": "Deep learning models can achieve high inference accuracy by extracting rich knowledge from massive well-annotated data, but may pose the risk of data privacy leakage in practical deployment. In this paper, we present an effective teacher-student learning approach to train privacy-preserving deep learning models via differentially private data-free distillation. The main idea is generating synthetic data to learn a student that can mimic the ability of a teacher well-trained on private data. In the approach, a generator is first pretrained in a data-free manner by incorporating the teacher as a fixed discriminator. With the generator, massive synthetic data can be generated for model training without exposing data privacy. Then, the synthetic data is fed into the teacher to generate private labels. Towards this end, we propose a label differential privacy algorithm termed selective randomized response to protect the label information. Finally, a student is trained on the synthetic data with the supervision of private labels. In this way, both data privacy and label privacy are well protected in a unified framework, leading to privacy-preserving models. Extensive experiments and analysis clearly demonstrate the effectiveness of our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning models have proven success in many inference tasks [1]\u2013[5] by extracting rich knowledge from massive well-annotated data. However, the deployment of these well-performing models may has risk of data privacy leakage since the training data often contain private information [6] and the models may be attacked. For example, the existing works [7], [8] have shown that the private information in the training data can be obtained from models even if the parameters are not known. Thus, it is very meaningful to design effective solutions that can learn privacy-preserving models with small accuracy loss.\nDifferential privacy [9] is one of the most widely used algorithm of privacy protection and provides privacy measurement standard for data and label. Abadi et al. [10] first introduced differential privacy into stochastic gradient descent to train deep learning models, and their proposed DPSGD algorithm achieves good differential privacy but leads to large accuracy degradation. Papernot et al. [6] proposed private aggregation of teacher ensembles (PATE) that achieves differential privacy by limiting privacy loss with the number of labels. For only label-sensitive setting, Chaudhuri et al. [17] proposed the concept of label differential privacy (LabelDP). Badih et al. [11] then introduced prior probabilities into the randomized response and trained privacy-preserving models in a multi-step manner. Malek et al. [12] applied the [6] and bayesian inference to the label differential privacy setting. Correspondingly, Yuan et al. [13] applied [10] to the label differential privacy setting and proposed a method named Protocol. Esfandiari et al. [14] improved the model performance by adding a clustering operation before the randomized response. We found that it is much easier to perform the differential privacy algorithm on only the labels than on the data at the same time, and the model accuracy will be much higher through the above works. However, a major issue is how to convert differential privacy setting into label differential privacy setting.\nTo convert differential privacy setting into label differential privacy setting, a key is learning models with generative data and private labels. Recent data-free knowledge distillation can provide this function. Data-free knowledge distillation is a class of approaches which aims to train a student model with a pre-trained teacher model without access to original training data. It uses the information extracted from the teacher model to synthesize data used in the distillation process. Chen et al. [15] proposed data-free learning for training the student model by exploiting GAN. It uses the teacher model as a fixed discriminator to train a generator to generate the training data used for distillation. Fang et al. [16] proposed a FastDFKD that applied the idea of meta-learning to the training process to accelerate the efficiency of data synthesis. We found that a slight modification of the generator training method for such methods can learn only the data distribution information and ignore the data representation information.\nInspired by the above works, we propose a privacy-preserving data-free distillation method. As shown in Fig. 1, publishing a model (e.g., teacher model) trained directly from private data would compromise privacy, so we treat it as a fixed discriminator to train a generator in a data-free manner. This generator learns only the data distribution to protect the private data. Using this generator implicitly generates data for the distillation process from teacher model to student model. Because querying the teacher model using the generated synthetic data can compromise private information, we propose a LabelDP algorithm selective randomized response to protect the output of the teacher model. The selective randomized response algorithm treats the output of the student model as"}, {"title": "II. APPROACH", "content": "Given a private dataset $\\mathcal{D}$, the goal is to train a student model $s$ with privacy-preserving capabilities and its accuracy close to the teacher model $t$ trained directly on $\\mathcal{D}$. To achieve this goal, we propose a privacy-preserving differentially private data-free distillation method. First, we train a teacher model $t$ directly on $\\mathcal{D}$. Then, we use t as a fixed discriminator to train a generator $g$ that is used to generate massive synthetic data $\\mathcal{D}$. We obtain predictions on $\\mathcal{D}$ by querying the teacher model and apply the selective randomized response function which follows $\\epsilon$-LabelDP on them to get labels $\\mathcal{L}$. Finally, the student learning can be formulated by minimizing an energy function $E$:\n$E(\\theta_s; \\mathcal{D}) = E(\\phi_s(\\theta_s; \\mathcal{D}), \\mathcal{L}) = E(\\phi_s(\\theta_s; \\mathcal{D}), \\mathcal{R}(\\phi_t(\\theta_t; \\mathcal{D})))$, (1)\nwhere $\\theta_s$ and $\\theta_t$ are the parameters of the student and teacher, respectively. $\\mathcal{R}$ is selective randomized response function.\nFrom Eq. 1, we can see that our approach can learn privacy-preserving models by two main processes. First, the training of student does not directly access the private data. Second, the labels from the teacher are protected by the selective randomized response module which implements $\\epsilon$-LabelDP. Therefore, privacy leakage can be suppressed very effectively. During training, the teacher knowledge is transferred to the student through the label $\\mathcal{L}$. We solve Eq. 1 via two steps, including: 1) data-free generator learning that trains a generator $g$ with the pre-trained teacher $t$ as a fixed discriminator to generate synthetic data $\\mathcal{D}$, and 2) student learning that applies knowledge distillation is to label the synthetic data with $t$ and selective randomized response function. And then use these data-label pairs to train the student model s and fine-tune the generator $g$. The detailed process is introduced in Alg. 1."}, {"title": "B. Data-Free Generator Learning", "content": "Directly using private data to train the generator will lead to privacy leakage, while using public data will lead to a serious decrease in the accuracy of the student model obtained by distillation, so we want to find a generator training method that does not leak privacy and could match the distribution of the private data. Inspired by [15], multi-class classifiers instead of two-class classifiers as discriminators can better learn data distribution, so we adopt a new training approach. We first train a teacher model directly using the private data, and then train a generator using that teacher model as a discriminator with fixed parameters. At the heart of this idea is to take the teacher as a bridge to indirectly learn the distribution of private data. We optimize them by the following loss:\n$\\mathcal{L}_g(\\phi) = \\ell_{CE}(\\phi_t(\\theta_t; \\tilde{x}), \\text{argmax}(\\phi_t(\\theta_t; x));) + \\alpha \\phi_{\\iota} (\\theta_t; x) \\log \\phi_t(\\theta_t; x) + \\beta \\mathcal{N}(\\phi_t, x),$ (2)\nwhere $x = \\phi_g(\\theta_g; z)$ generated by $g$ with parameters $\\theta_g$, $z$ is a random vector, $\\alpha$ and $\\beta$ are the tuning parameters to balance the effect of three terms. The cross entropy $\\ell_{CE}(\\cdot)$ is used to enforce the outputs of the teacher model closer to the one-hot labels. The smaller it is, the closer the synthetic data distribution is to the private data. The second term is the information entropy loss to measure the class balance of synthetic data. The $\\mathcal{N}(\\cdot)$ is $l_2$-norm $||*||_2$ to measure the mean and variance of the total synthetic data and the running data fed into the model. In this way, the synthetic data $\\mathcal{D}$ generated by the trained generator has a similar distribution to private data without compromising privacy."}, {"title": "C. Student Learning with Synthetic Data and Private Labels", "content": "During the training of the student model, we use randomized response [18] for the sensitive labels to achieve $\\epsilon$-DP. RRE mechanism will return correct class label with the probability $\\frac{e^{\\epsilon}}{e^{\\epsilon}+K-1}$, and return other labels with probability $\\frac{1}{e^{\\epsilon}+K-1}$, where $K$ is the number of classes. To improve the probability of returning the true label without compromising privacy, we introduce the student prediction $y_s$ and propose selective randomized response algorithm. As shown in function selective randomized response in Alg. 1, we first set a threshold $t$ and select the set of indexes $I$ with condition $y_s > t$. To ensure the randomness of the output labels, we require that the number of elements in $I$ to be at least 2. We will set $I$ to the set of indexes of top two largest elements in $y_s$ if the number of elements in $I$ is less than 2. Let $k$ be the number of $I$. If the teacher model's output in $I$, return the $y_t$ with the probability $\\frac{e^{\\epsilon}}{e^{\\epsilon}+k-1}$ and return the one-hot type of other elements with probability $\\frac{1}{e^{\\epsilon}+k-1}$ (RRE(I,yt) in Fig. 1). If the teacher model's output not in $I$, return the one-hot type of the elements in $I$ with probability $\\frac{1}{k}$ (Uniform(I) in Fig. 1).\nFor learning with LabelDP guarantee, we use selective randomized response to randomized outputs from the teacher model for each example of the synthetic data and then apply a general learning algorithm that is robust to random label noise to these data-label pairs. Unlike DPSGD and PATE, which require the composition theorems to calculate the final privacy budget $\\epsilon$, we query the random labels once and reuse them in training process. At each stage $i \\in [T]$, the synthetic dataset $\\mathcal{D}^{(i)}$ is first generated using the generator $g$ and then enter it into the most recent student model $\\phi_s^{(i-1)}$ to obtain $y_s$ as the prior knowledge. We run selective randomized response algorithm with $y_s$ to obtain the label $L_i$. We use $\\mathcal{D}^{(i)} = {\\mathcal{D}^{(i)}, L_i}$ to train the student model $\\phi_s^{(i)}$ and fine-tune the generator $g$. The loss function for the $i$th epoch is\n$\\mathcal{L}_{kd} = \\sum_{j=1}^{|\\mathcal{D}^{(i)}|} KL(\\phi_s(\\theta_s; \\tilde{x}_j), \\bar{y}_j), s.t. (x_j, \\bar{y}_j) \\in \\tilde{\\mathcal{D}}^{(i)}$, (3)\nwhere $\\ell_{KL}(\\cdot)$ represents the Kullback-Leibler divergence. For the synthetic dataset $\\tilde{\\mathcal{D}} = \\cup_{i=1}^{T} \\tilde{\\mathcal{D}}^{(i)}$, iff the size of dataset in each stage is the same, their order will have no effect on the accuracy of student, but $T$ will affect the student accuracy.\nIn our approach, the private data first transfers the knowledge to the teacher model. Directly publishing the teacher model would lead to privacy leakage, so we use the teacher model as a fixed discriminator to train a generator to generate a non-sensitive synthetic dataset with the similar distribution to the private data. We use this dataset to transfer the knowledge from the teacher model to the student model. Because only the predictions from the teacher model are sensitive, we protect the predictions of the teacher model by implementing $\\epsilon$-LabelDP through our propose selective randomized response module. In this way, the knowledge of the private data is transferred to the privacy-preserving student model without access through a data-free distillation approach."}, {"title": "III. EXPERIMENTS", "content": "To verify the effectiveness of our differentially private data-free distillation approach (DP-DFD), we conduct experiments on five datasets and perform comprehensible comparisons with 11 state-of-the-arts. To make the comparisons fair, our experiments use the same settings as these approaches and take the results from their original papers."}, {"title": "A. Experimental Setting", "content": "Datasets. The experiments are conducted on five datasets. MNIST [19] and FashionMNIST (FMNIST) [20] are both 10-class datasets for 28 \u00d7 28 gray handwritten number images and fashion images, respectively. They includes 60K train examples and 10K test examples. CIFAR10 and CIFAR100 [21]) consists of 60K 32 \u00d7 32 color object images in 10 and 100 subjects, where 50K for training and 10K for testing. CelebA [5] contains 202,599 color facial images that are preprocessed by aligning and resizing into 64 \u00d7 64. According to hair color and gender attributes, we create two CelebA datasets, CelebA-H and CelebA-G and they uses black/blonde/brown and male/female as labels, respectively. We partition them into training set and test set according to the official criteria [5].\nImplementation. In all experiments, the structure of teacher is Resnet34 and we set $\\alpha$ and $\\beta$ in Eq. 2 as 5 and 10 respectively. The structure of student is the same as [27] in data-sensitive experiments and Resnet18 in label-sensitive experiments, respectively. For each dataset, we set the threshold value to 1/(2*nc) where nc is the number of classes. We evaluate the test accuracy of student under privacy protection."}, {"title": "B. State-of-the-art Comparisons", "content": "Comparisons with data-sensitive approaches. First, we compare with 7 data-sensitive approaches on MNIST, FMNIST, CelebA-H and CelebA-G under $\\epsilon$=1 and $\\epsilon$=10, including DP-GAN [22], PATE-GAN [23], GS-WGAN [24] and G-PATE [25], DP-MERF [26], DataLens [27] and DP-Sinkhorn [28]. The results are shown in Tab. I. All other approaches are under a failure probability $\\delta = 10^{-5}$. The accuracy of baseline model which trained directly using private data is 99.21% on MNIST, 91.02% on FMNIST, 93.53% on CelebA-G and 88.68% on CelebA-H. From Tab. I, we can see that our DP-DFD shows substantially higher performance than other approaches especially when $\\epsilon$ = 1. In particular, the accuracy of our DP-DFD outperforms the other best-performing approaches by 21 percentage points. When $\\epsilon$ = 10, our DP-DFD also has an absolute advantage and is at least 13 percentage points higher than the other approaches. Even for high-dimensional datasets like CelebA-G and CelebA-H, our DP-DFD still shows the state-of-the-art performance, which also demonstrates the advantage of DP-DFD over other privacy-preserving approaches on high-dimensional datasets.\nComparisons with label-sensitive approaches. Then, we compare with 4 LabelDP approaches (LP-MST [11], AL-IBI [12], ClusterRR [14] and Protocol [13]) on MNIST, FMNIST, CIFAR10 and CIFAR100 under the same $\\epsilon$. The results are shown in Tab. II. We can find that our method performs optimally for all four datasets and for different $\\epsilon$. In particular, it achieves a correct rate of 74.67 when $\\epsilon$ equals 8 on the CIFAR100 dataset, surpassing many methods that use the raw data for direct distillation. The effectiveness of our method is further demonstrated by the fact that our method has higher performance than the other four methods trained directly using raw data when there is no restriction on the amount of generated synthetic data."}, {"title": "C. Ablation Studies", "content": "After the promising performance is achieved, we further analyze each influencing factor in our approach, including the impact of loss terms in the data-free generator learning, the amount of synthetic data and the number of stages.\nLoss function. To further understand the improvement of each component of the loss function during data-free training of the generator, we designed experiments on MNIST and FMNIST under $\\epsilon$=10 to explore the contribution of each component. The results are shown in Tab. III. where CE means the cross entropy loss term, IE is the information entropy loss term and Norm is the normalized term for the mean and variance of the data. We can see that the normalization term of the data has the greatest impact, followed by the information entry loss term and finally the cross entropy loss term. We speculate that this may be related to the randomness of the data generated by the generator, which limits the distribution of the data to make the generated synthetic data more usable, so it has a greater impact on the accuracy of the student model.\nData amount. We further conducted experiments on MNIST, FMNIST, CIAFR10 and CIFAR100 datasets under $\\epsilon$ = 1. The results are shown in Fig. 2. We found that MNIST dataset converges at about 50,000 data volume, FMNIST converges at about 120,000, CIFAR10 and CIFAR100 converge at about 220,000 and 500,000, respectively. As the difficulty of datasets increases, the amount of data required to achieve convergence increases. We suspect that this is because the more difficult the dataset is, the more difficult its distribution knowledge is to learn, so the larger the amount of data required. We note that the CIFAR10 dataset is more difficult than FMNIST, but the reason why CIFAR10's final accuracy is similar to FMNIST's is that the network structure is different.\nNumber of stages. To explore the effect of the number of stages, we conducted experiments on MNIST, FMNIST and CIFAR10 datasets under $\\epsilon$=10. The results are shown in Fig. 3. Experimental results show that between 20 and 320, the accuracy of the student model increases with the increase of stages. As the classification difficulty of MNIST, FMNIST and CIFAR10 datasets increases, the effect of stages becomes greater. The experimental results are as we expected because we used the prediction of the student model as the prior knowledge. As the training process proceeds, the more accurate the prediction of the student model becomes, which means the higher the probability of outputting the correct label."}, {"title": "D. Privacy-Preserving Analysis", "content": "Data generation. To demonstrate that the direct use of synthetic data in our approach doesn't leak information of private data, we visualize some examples for MNIST, FMNIST, CIFAR10 and CelebA, as shown in Fig. 4. The first row is MNIST, followed by FMNIST, CIFAR10, CelebA-G and CelebA-H in that order. We found that even for the simplest MNIST synthetic data, we could not semantically identify it as a handwritten font. Despite its inability to be recognized by humans, it has high utility in terms of training high performance models. We also found something interesting: such synthetic data can train a model that performs well, which raises an interesting question about what machine learning models actually learn from data?\nModel-inversion attack. We perform a model-inversion attack [8] on a typical data-sensitive approach and a label-sensitive approach to further demonstrate that our approach can protect data privacy. The results are shown in Fig. 5. The first row is the results of the attack on a typical data-"}, {"title": "IV. CONCLUSION", "content": "Typically, publishing deep learning models may pose the risk of privacy leakage. To facilitate model deployment, we propose a differentially private data-free distillation approach (DP-DFD) that does not use private data in the training process of publish model. This approach uses the teacher model trained directly with private data as a bridge to transfer knowledge from private data to publish model. The generator trained in a data-free manner can learn the distribution of the private data and enhance the knowledge of the publish model to compensate for the loss of the accuracy without compromising privacy. In addition, we also provide differential privacy analysis for our selective randomized response and DP-DFD to demonstrate that it provides strong privacy guarantees in theory. We have conducted extensive experiments and analyses to show the effectiveness of our approach. In the future, we will explore the approach in more practical applications, such as federated learning on medical images and financial data."}]}