{"title": "Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh through Large Language Modeling", "authors": ["Azmine Toushik Wasi", "Wahid Faisal", "Mst Rafia Islam", "Mahathir Mohammad Bappy"], "abstract": "Purpose: Bangladesh's legal system struggles with major challenges like delays, complexity, high costs, and millions of unresolved cases, which deter many from pursuing legal action due to lack of knowledge or financial constraints. This research seeks to develop a specialized Large Language Model (LLM) to assist in the Bangladeshi legal system.\nMethods: We created UKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and scraping data on various legal acts. We fine-tuned the GPT-2 model on this dataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance in English.\nResults: The model was rigorously evaluated using semantic assessments, including case studies supported by expert opinions. The evaluation provided promising results, demonstrating the potential for the model to assist in legal matters within Bangladesh.\nConclusion: Our work represents the first structured effort toward building an AI-based legal assistant for Bangladesh. While the results are encouraging, further refinements are necessary to improve the model's accuracy, credibility,", "sections": [{"title": "1 Introduction", "content": "Legal Natural Language Processing (NLP) is transforming the legal industry by automating the extraction and analysis of information from complex documents such as statutes, contracts, case law, and legal opinions. These advancements are revolutionizing tasks like document review, contract analysis, and legal research, significantly reducing the time and cost associated with legal procedures [1, 2]. Tools that leverage NLP can swiftly parse through vast amounts of legal text, identifying key clauses, obligations, and relevant case precedents, providing a much-needed solution in areas like contract drafting, litigation prediction, and regulatory compliance [3, 4]. Recent advancements in machine learning, particularly transformer-based models like BERT and GPT, have further enhanced the ability of NLP systems to understand legal texts in context, producing more accurate and relevant insights for legal practitioners [5, 6]. Despite these advancements, challenges in legal reasoning and the application of AI to complex legal scenarios persist [2, 7]. Legal reasoning often requires a deep understanding of context, precedent, and the intent behind legal provisions, which current NLP models struggle to grasp fully [8]. Moreover, the field is growing rapidly, with an increasing number of publications exploring sophisticated methods to tackle these challenges, such as neural-symbolic approaches and domain-specific knowledge graphs. Ongoing developments in Legal NLP aim not only to improve these systems' understanding of legal texts but also to broaden their applications to more complex legal reasoning and cross-jurisdictional cases, overcoming some of the current limitations [2, 8, 9]. A major challenge for Legal NLP, however, is the variability in legal systems across countries and states. Penal codes, legal doctrines, and norms vary widely, reflecting regional, cultural, and communal values. Legal language itself is often hard to understand and subject to interpretation, requiring models to adapt to different jurisdictions to ensure both accuracy and relevance [9]. Overlooking these regional distinctions can lead to the misapplication of law, erroneous advice, or incorrect case predictions, posing a significant risk to legal outcomes. Thus, the development of NLP systems for legal applications must account for the intricate and context-dependent nature of legal systems worldwide, ensuring that models are both adaptable and interpretable across varied legal environments [7, 8, 10].\nLegal system in Bangladesh faces severe and longstanding challenges, including significant delays, complex procedural requirements, police harassment, inadequacies in legal provisions, and prohibitive legal costs [11-14], which collectively deter many individuals from seeking justice. With a backlog exceeding 3.7 million cases as of 2021 [15], the judicial system is overburdened, leading to frustration and widespread distrust in"}, {"title": "3 Developing UKIL-DB-EN Corpus", "content": "In this section, we outline the data collection and curation methodology for UKIL-DB-EN, along with its corpus structure and descriptive statistics."}, {"title": "3.1 Data Collection and Corpus Development", "content": "In this section, we outline the comprehensive data collection and curation process undertaken for the development of the UKIL-DB-EN dataset. This process involved systematic scraping of legal texts from an open-access government portal and meticulous preprocessing to ensure the quality and relevance of the data for our legal language model."}, {"title": "3.1.1 Data Collection", "content": "To create the UKIL-DB-EN dataset, we initiate a systematic data collection process by scraping an open-access government portal dedicated to providing legal information in Bangladesh. This portal serves as a comprehensive repository of legal texts, including statutes, regulations, and official legal documents that are essential for understanding the legal landscape of the country."}, {"title": "3.1.2 Data Curation", "content": "Following the extraction of raw data, we engaged in a comprehensive preprocessing phase aimed at enhancing the quality and relevance of the dataset for model training. This included several key steps: Data Cleaning. We meticulously removed repealed acts and sections to ensure that only current and applicable legal texts were included in the dataset. This step was critical to maintain the integrity and reliability of the information, as outdated laws could mislead users and negatively impact the model's performance.\nNoise Reduction. During the extraction process, we encountered various unwanted characters and formatting inconsistencies. We implemented string manipulation techniques to eliminate noise, including special characters, irrelevant whitespace, and other extraneous elements that could interfere with natural language processing tasks.\nStandardization. We standardized the formatting of the legal texts to ensure uniformity across the dataset. This included converting all text to a consistent case, structuring citation formats, and ensuring that the presentation of legal clauses and provisions adhered to a clear and recognizable structure.\nVerification and Validation. To ensure the accuracy and reliability of the collected information, all data was rigorously checked and verified by the authors. We cross-referenced the scraped texts with official sources and documentation to confirm their validity. This validation process was crucial in establishing the credibility of the dataset, as it forms the foundation for subsequent model training and application.\nThrough these meticulous data collection and curation efforts, we have established the UKIL-DB-EN dataset as a valuable resource for developing a specialized legal language model tailored to the unique context of Bangladeshi law. This dataset is not only a reflection of the current legal framework but also a vital step towards improving access to legal information and assistance for the people of Bangladesh."}, {"title": "3.2 Data Structure", "content": "We have organized the related acts and sections hierarchically within the JSON structure for efficient information retrieval. At the top level, the key act contains essential information about the act itself, including a unique id, name of the act, a boolean repelled indicating whether the act has been repealed, the text containing details of the act, the published_date, a list of related_act IDs that connect the act to other acts, lower_text array for additional notes, the num_of_sections indicating the total sections present, and an array of sections. Each section within the sections array is represented as an object with keys such as section_id, name, and details that provide specific information about that section. Additionally, each section may include a related_acts array for further connections to other acts and an act_id linking it back to the main act. Data sample is available in Appendix, in Table 1."}, {"title": "3.3 Descriptive Statistics", "content": "We have collected information on a total of 595 Acts, which include approximately 18,023 sections. On average, each act contains about 24 sections. The mean character length for act names is 50.30, while act details have a mean length of 438.37 characters. Section names are shorter, averaging 38.07 characters, reflecting their concise titles. In contrast, section details are more comprehensive, with an average length of 736.69 characters, indicating detailed descriptions and provisions."}, {"title": "4 LLM Model : GPT2-UKIL-EN", "content": "In this section, we discuss the model fine-tuning process, including prompt design and implementation details such as hyperparameters and other technical specifications."}, {"title": "4.1 Model Development", "content": "Our work uses pre-trained GPT-2 model from HuggingFace, originally developed by Radford et al. [17]. We also use the original tokenizer (GPT2Tokenizer class) to tokenize our data, from the transformers library [33]. To train the model with legal information, we have developed prompts using a simple instruction-tuning approach. For acts, we have used the prompt: What do you know about \"#act_name\", \"year\", Bangladesh? The model has responded with a concatenation of all section details in text format. For sections, we have used the prompt: What do you know about \"#section_name\" from \"#act_name\", \"year\", Bangladesh? The model has provided the specific section detail texts. Upon acceptance, we will publicly release both the text corpus and the trained model."}, {"title": "4.2 Implementation Details", "content": "All the random seeds used are 42. We configured the model with a batch size of 64 and further divided it into micro-batches of size 8 for computation, leading to a gradient accumulation step of 8. We have trained the model for 13 epochs with a learning rate of 3 \u00d7 10-4 with warm-up steps of 2 and set fp16 to for computational efficiency. The validation set comprised 2000 samples from 18,488 total prompts. We have implemented Low-Rank Adaptation (LoRA) for efficient training, with rank 3, alpha set 16, and task type CAUSAL_LM. Targeting modules used are attn.c_attn, mlp.c_fc, with a dropout rate of 0.1 and bias set to none. For quantization, we have employed nf4 double quantization, with compute type float16. For the tokenizer parameters, we have used the maximum length of 768, set padding strategy set to max_length and truncation to true."}, {"title": "5 Model Performance Analysis", "content": "In this section, we assess the performance of GPT2-UKIL-EN through a combination of semantic analysis and error analysis. This evaluation aims to identify the model's strengths and weaknesses, providing insights into its effectiveness in delivering accurate legal assistance."}, {"title": "5.1 Semantic Similarity Analysis", "content": "To assess the model semantic similarity quantitatively, we use Cosine similarity and Jaccard index to measure the text similarity between the original text and the output of different models, as presented in Table 1. These metrics help evaluate how well the models' generated outputs match the original texts, which is important in legal work. GPT2-UKIL-EN, our fine-tuned model, significantly outperforms the others, achieving the highest scores in both Cosine similarity (0.515) and Jaccard index (0.133). This indicates that fine-tuning on the UKIL-DB-EN dataset improves model performance in capturing legal text semantics and overlap. While Mistral-7b and Gemma-2b show competitive results without fine-tuning, their scores fall short, suggesting the value of domain-specific fine-tuning for legal tasks. GPT-2 Medium performs the lowest, emphasizing the impact of both model size and fine-tuning.\nAdditionally, we observe a general correlation between model size and text similarity scores: as model size increases, the similarity scores tend to improve. Despite being 6-10x smaller than the larger models, our fine-tuned model performs remarkably well, showing a significant improvement over the original GPT-2 model. This demonstrates that our fine-tuning process substantially enhanced the model's capabilities. Further insights and concerns on this are discussed in Limitations."}, {"title": "5.2 Error Analysis", "content": "The error analysis presented in Table 2 reveals significant discrepancies between the expected answer and our model's response regarding the \"Power to make rules\" section from The Pensions Act, 1871 of Bangladesh. The expected answer outlines specific powers granted to the Parishad for rule-making related to the Academy's membership, functions, and terms of service, detailing practical aspects and administrative functions. In contrast, our model's response inaccurately describes rule-making authority related to a different chapter, emphasizing rules that were supposed to be in force from March 1979 and including information about the Board and Education Act amendments. This response introduces unrelated content such as rule validity and Board establishment, diverging from the actual provisions of the 1871 Act. The model's failure to accurately identify and reproduce the specific administrative details reflects a misunderstanding of the context and subject matter, indicating a need for improved contextual comprehension and accurate content retrieval. This highlights the model's struggle with aligning its responses to the specific legal and historical context required, suggesting areas for refinement in handling detailed legislative texts."}, {"title": "6 Case Study and Comparisons", "content": "To better understand and assess the model, we developed three cases and collected expert opinions on these cases, on different statements and comments from the experts."}, {"title": "6.1 Case Design", "content": "We developed three distinct legal cases for expert evaluation: a property dispute (Case 1, Hard), an illegal possession case under special powers act (Case 2, Easy), and a murder charge (Case 3, Medium), which from different areas and difficulty. In the property case, the model was expected to verify the authenticity of ownership documents for a 500-square-foot plot in Dhaka. In the illegal possession case, involving Imran Hossain under the Special Powers Act, 1974, the model was tasked with determining whether his actions posed a significant public safety risk, justifying prosecution. Lastly, in the murder case, the model was expected to analyze the evidence against Amirul Islam, charged under Section 302 of the Penal Code, and decide if the crime fell under any exceptions to the murder charge. Table 3 shows three different cases developed by us, and their responses solved by the model. The expert opinions are shortly presented at Table 4, detailed collected results are available in Table 2. In Table 2, each asterisk (*) denotes one expert's opinion on that particular case and statement."}, {"title": "6.2 Study Design", "content": "We conducted a case study with five legal experts, consisting of four male and one female law faculty members\u00b3, to evaluate our model's performance. We presented our motivation, explained the model's components and functionality, and demonstrated three cases (Table 3) solved by the model. The experts were asked to assess the model based on seven affirmative statements (questions are available in Table 4 and 2) designed to evaluate aspects such as accuracy, quality, reasoning ability, approach, and writing style. Each statement was rated on a scale from 1 to 7: Strongly Disagree (1), Disagree (2), Somewhat Disagree (3), Neutral (4), Somewhat Agree (5), Agree (6), and Strongly Agree (7). All the statements are affirmative, so more than 4 means positive and less than 4 means negative. Along with the study, the experts provided valuable insights verbally and in written."}, {"title": "6.3 Performance Analysis", "content": "Overall Performance Analysis. From the expert opinions presented in Table 4, our model GPT2-UKIL-EN excels in reasoning and approach, with high scores indicating strong logical thinking and effective methods. However, it struggles with accuracy and clarity, particularly in complex scenarios, as reflected in lower scores for these aspects. While the model performs well in simpler cases, its accuracy and clarity suffer in more intricate contexts, revealing a need for improvement. The highest score for approach and style shows that the model generally presents information effectively, but its inconsistent performance in accuracy highlights areas for improvement in diverse tasks."}, {"title": "7 Discussion", "content": "We believe our GPT2-UKIL-EN model represents a valuable step forward in Legal NLP for Bangladesh, offering a tailored approach that could help reduce costs and accelerate legal procedures. This model not only aims to streamline the legal process but also strives to democratize access to legal information, thereby empowering individuals who have historically faced barriers in navigating the legal system. By providing clear and understandable explanations of legal terms and processes, our model can significantly enhance public understanding, making the legal system more approachable for those who may feel intimidated or overwhelmed by its complexity.\nWhile our approach has the potential to decrease the case backlog and improve access to legal assistance, particularly for marginalized communities, we acknowledge that the model still has several limitations. Issues such as incomplete legal coverage, potential inaccuracies in generated content, and a reliance on existing datasets could impede the model's effectiveness in certain contexts. However, we believe that with further exploration and increased computational resources, these challenges can be addressed, leading to continuous improvements in the model's accuracy and reliability. The societal impact of GPT2-UKIL-EN could be transformative. By facilitating easier access to legal information, it empowers individuals to make informed decisions regarding their legal rights and responsibilities. This is particularly crucial in Bangladesh, where many individuals may avoid seeking legal recourse due to a lack of understanding or financial constraints. By simplifying legal jargon and enhancing public understanding of legal processes, our model contributes to a more equitable legal system that can support the rights of all citizens, regardless of their socio-economic status.\nGlobally, the implications of our work extend beyond the boundaries of Bangladesh. The methodologies and frameworks established through this project can serve as a model for other countries facing similar challenges in their legal systems. As legal systems around the world increasingly adopt AI and NLP technologies, our approach may provide valuable insights into developing multilingual legal assistants tailored to diverse legal contexts. This adaptability underscores the potential for a global impact, as nations can leverage our findings to enhance their own legal frameworks, thereby contributing to the ongoing evolution of legal technology. Moreover, the use of AI in legal settings can facilitate more efficient court processes, reducing the time and resources spent on legal proceedings. This efficiency could lead to a significant decrease in the overall costs associated with legal disputes, benefiting not only individuals but also the judicial system as a whole. By addressing delays and streamlining case management, our model could help restore public trust in legal institutions, encouraging more individuals to seek justice and ensuring that their rights are upheld. Overall, we belive, GPT2-UKIL-EN holds promise not just as a tool for legal assistance but as a catalyst for broader societal change. With continued refinement and exploration, this approach could pave the way for similar advancements in other legal contexts facing unique challenges, ultimately fostering a more just and accessible legal landscape worldwide.\nLimitations of the Study and Future Work Possibilities. This study was conducted under several constraints, primarily due to limited computational resources. As a result, we were unable to experiment with very large models or state-of-the-art multilingual language models, which are typically required for effective performance in both Bangla and English. Given that the legal system in Bangladesh uses both languages English predominantly in higher courts and a mixture of Bangla and English in lower courts this limitation restricts the broader applicability of our model across all legal contexts in Bangladesh. Furthermore, our exploration was limited to training on English data using an English-based model, which means our model may not fully capture the complexities of the legal processes in lower courts where Bangla is also widely used. Developing and fine-tuning multilingual models that can handle both languages effectively was beyond the scope of this work, but we believe this is a critical area for future research. Additionally, due to resource limitations, we could not explore larger-scale models, which may yield better performance in complex legal reasoning tasks. Despite these limitations, our findings show significant promise and lay the groundwork for future studies to address these challenges."}, {"title": "8 Conclusion", "content": "The current legal system of Bangladesh faces challenges such as delays, complexity, and high costs, with over 3.7 million cases pending in 2021. LLMs have the potential to ease these issues. Motivated by this, we have developed UKIL-DB-EN, an English legal corpus dataset, and GPT2-UKIL-EN, a large language model for legal assistance. We created UKIL-DB-EN by scraping data on legal acts and fine-tuned the GPT-2 model on this dataset. Our work, the first ever effort to create a Bangladesh-focused AI-based legal assistant model, evaluated through various assessments and expert opinions, shows promising results and marks an important first step toward developing a reliable legal AI for a country of 180 million people."}, {"title": "Declarations", "content": "\u2022 Funding: No funding was received to assist with the preparation of this manuscript.\n\u2022 Conflict of interest: All authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest or non-financial interest in the subject matter or materials discussed in this manuscript.\n\u2022 Ethical Concerns: As we are developing legal AI, the correctness of the model is crucial. Given that our model is in its early stages, there may be issues related to its accuracy and reliability. This raises important ethical concerns, particularly regarding the potential impact of inaccuracies in legal contexts.\n\u2022 Ethics approval and consent to participate: We adhered to all ethical guidelines outlined in the Springer Nature guidelines during data collection, curation, and modeling experiments. We also reported all hyperparameters used in our experiments to ensure reproducibility and transparency, aiming to uphold the highest standards of ethical practice in our research. Our study also involved working with experts who provided opinions on various legal cases. In accordance with Springer Nature and ethical research guidelines, we ensure that the experts are neutral, with no vested interest in the work, thus maintaining objectivity and impartiality.\n\u2022 Consent for publication: All identifying details of participants have been published with informed consent, ensuring compliance with ethical guidelines. Anonymization has been applied where applicable, and no identifying information has been included without proper consent.\n\u2022 Potential Risks and Response: Developing legal AI requires a high level of accuracy due to the critical nature of its applications. As our model is still in its early development phase, there is a risk of inaccuracies and reliability issues that could impact its performance in legal contexts. These potential risks underscore the importance of addressing ethical concerns, as any errors could have significant consequences. To mitigate these risks, we have followed ethical guidelines for data collection, curation, and modeling as specified by ACL.\n\u2022 Data availability: The UKIL-DB-EN dataset is publicly available in Hugging Face with DOI: 10.57967/hf/3233 [34] with Apache-2.0 license.\n\u2022 Materials availability: The GPT2-UKIL-EN model is publicly available in Hugging Face with DOI: 10.57967/hf/3235 [35] with Apache-2.0 license.\n\u2022 Code availability: All codes are available in GitHub on ciol-researchlab/UKIL with Apache-2.0 license.\n\u2022 Author contribution: A.T.W., W.F., and M.R.I. contributed equally to the project. A.T.W. conceptualized the study, managed data curation, formal analysis, project administration, validation, visualization, made drafts and finalized the manuscript. W.F. led data collection, formal analysis, investigation, methodology, software, resource management, and contributed to writing the manuscript. M.R.I. contributed to data collection, investigation, methodology, case development, interviews, resource management, study validation, and assisted with drafting and reviewing the manuscript. M.M.B. supervised the work, reviewed the manuscript, and provided feedback."}]}