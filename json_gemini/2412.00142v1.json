{"title": "Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers", "authors": ["Chancharik Mitra", "Brandon Huang", "Tianning Chai", "Zhiqiu Lin", "Assaf Arbelle", "Rogerio Feris", "Leonid Karlinsky", "Trevor Darrell", "Deva Ramanan", "Roei Herzig"], "abstract": "Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for foundational discriminative vision-language tasks (i.e., tasks requiring discrete label predictions) such as image classification and multiple-choice VQA. One key challenge in utilizing LMMs for discriminative tasks is the extraction of useful features from generative models. To overcome this issue, we propose an approach for finding features in the model's latent space to more effectively leverage LMMs for discriminative tasks. Toward this end, we present Sparse Attention Vectors (SAVs)\u2014a finetuning-free method that leverages sparse attention head activations (fewer than 1% of the heads) in LMMs as strong features for VL tasks. With only few-shot examples, SAVS demonstrate state-of-the-art performance compared to a variety of few-shot and finetuned baselines on a collection of discriminative tasks. Our experiments also imply that SAVs can scale in performance with additional examples and generalize to similar tasks, establishing SAVs as both effective and robust multimodal feature representations.", "sections": [{"title": "1. Introduction", "content": "Large Multimodal Models (LMMs) such as GPT-4V [67], LLaVA [54, 55], and QwenVL [3] demonstrate state-of-the-art performance on open-ended vision-language (VL) tasks like image captioning [51, 99], visual question answering [2, 28, 39], and language grounding [34, 60]. However, despite their remarkable performance on generative tasks, these models struggle on discriminative tasks, where responses are restricted to a discrete set of labels [7, 103]. Indeed, LMMs with billions of parameters and trained on trillions more tokens of data underperform smaller discriminative VLMs [7, 103] and even classical machine learning methods [5] on image classification tasks. Nevertheless, there are many discriminative tasks that generative LMMs may be better suited for than CLIP-like models such as hallucination detection and VQA. Thus, it is enticing to have one type of model that can effectively accomplish both generative and discriminative vision-language tasks.\nAnother drawback of generative LMMs is that it is still unclear how best to extract features directly as in discriminative VLMs like CLIP or SigLIP. Feature extraction is a well-explored field in both vision-only [12, 77, 87] and language-only discriminative models [11, 75], but such is not the case generative models. Most current methods for extracting features from generative models require carefully constructed prompts [37], specialized architectures [48], and finetuning [59]. However, generative models still offer the promise of more flexible, truly multimodal features as compared to modality-specific features extracted from CLIP-like models. As such, we are motivated to extract multimodal features from a generative LMM without fine-tuning to be used for any discriminative VL task.\nA natural question arises is how best to empower generative LMMs with discriminative capabilities. The simplest strategy is to use prompt engineering [62, 96] and few-shot prompting [6, 104] that guide the model to output class labels [9]. However, recent work [103] shows that prompting an LMM for discriminative tasks does not close the gap with discriminative VLMs. This result suggests a more direct approach of finetuning the LMM on discriminative tasks. While the finetuning approach appears to work [7, 103], it veils the key problem of requiring training-scale data for every new discriminative task. As LMMs are a combination of a strong encoder VLM and a large language model trained on internet-scale data, it should suggest the existence of a more efficient way to enable performant, generalizable discriminative capabilities in LMMs without finetuning.\nOne source of inspiration for our method is long standing work in the field of neuroscience that suggests certain areas of the brain are reserved for specific tasks [14, 33] (i.e. functional specificity). Motivated by this idea, we refer to recent interpretability research that has focused on identifying specific heads in transformer-based models that correspond to particular tasks [66]. The most prominent of these methods is a line of work that looks to enhance vision-language capabilities using task vectors [20, 23, 26, 86], which are compact implicit representations of tasks encoded in the activations of a transformer model. While promising, these methods ultimately use these representations to augment a model's generative capabilities. On the other hand, we seek to use feature representations directly as classifiers. Nevertheless, this intuition from interpretability informs our work on Sparse Attention Vectors (SAVs), which are sparse features in an LMMs activation space that can be directly exploited for few-shot discriminative reasoning.\nOur method has three steps: First, we extract features (called attention vectors) from the output of each head of the LMM for some few-shot labeled examples (\u2248 20 per label). Second, we average these attention vectors over the examples in each class and evaluate their accuracy as centroids in a class centroid classifier. We then select the top 20 heads by classfication accuracy as our SAVs. In this way, we identify a very sparse set of attention vectors (less than 1% of the total number of heads) that can be used for discriminative tasks. Finally, we perform inference on the given task by doing a majority vote across this sparse set of attention vectors for each new query. This approach requires only few-shot examples at test-time to extract effective multimodal embeddings. An overview is shown in Figure 1.\nWe summarize our main contributions as follows: (i) We introduce a novel method that yields a sparse set of attention vectors (less than 1%) for each individual task can serve as highly effective features for discriminative tasks; (ii) We demonstrate that our method can help close the gap with discriminative VLMs on classification tasks using only few-shot examples at test time; (iii) Our method surpasses zero-shot, few-shot, and LoRA fine-tuned baselines across multiple tasks (+7% improvement on average over LoRA on challenging benchmarks like BLINK [15], VLGuard [106], and NaturalBench [40]); (iv) We establish several advantageous properties of our approach, including strong generalization capabilities and favorable scaling characteristics."}, {"title": "2. Related Works", "content": "Controllable Generation for Classification. Controllable text generation in LMMs has become an important area of research, aiming to guide model outputs to adhere to specific attributes or constraints. In particular, an important application of controllable generation is utilizing a generative LMM for discriminative tasks. One controllable generation method often used is test-time hard prompting [6, 96], where prompt engineering or few-shot examples guide the model to output the desired class labels [9, 81, 94, 97, 103]. Another similar technique is directly using the probability of a specific class label being generated by the LMM [52, 53], which is commonly used for many image-text retrieval tasks. Furthermore, soft prompting methods that finetune specific learnable tokens [38, 46] can also be a viable option for discriminative classification with LMMs. Apart from these approaches, one can directly instruction finetune the model [95] on labeled classification data [5, 103]. Another option is preference modeling, which feature methods like DPO and RLHF to align the model to accurately output class labels [13, 68, 73]. In contrast, our method is a finetuning-free approach that directly chooses class labels without the need for preference data.\nMost related are works which show that internal representations of transformer models called task vectors [19, 23, 26, 85] (or function vectors) can encapsulate tasks outlined by ICL examples. In these works, such vectors are patched directly back in the model for generation. Going beyond previous work, however, we utilize a very sparse set of attention vectors directly as features for a discriminative task.\nVision-Language Features. The study of feature extraction in deep learning is concerned with finding useful representations that can be applied to a diverse array of downstream tasks. Early development of embedding techniques include autoencoder methods [4, 35, 57, 58, 76], Word2Vec [61] and GloVe [71] which transformed inputs into computable vector representations. These methods were quickly followed up by similar works in NLP [11, 16, 63, 75] and computer vision [12, 77, 87]. More recently, methods like CLIP or SigCLIP [43, 44, 47, 72, 100, 101] explore the correlation between multiples modalities (primarily images and text) through contrastive learning or a sigmoid loss on image-text pair data. The value of such representations is their flexibility in being applied to a variety of downstream tasks [10, 21, 29, 36, 74, 88] and domains [50, 93, 102].\nExtracting features from generative models is a notably more challenging problem as it is not immediately obvious where in the model architecture to extract a distilled representation from (unlike discriminative models). Nevertheless, this direction is attractive due to the potential flexibility of the embeddings. Some methods finetune encoder VLM models on synthetically generated data from generative LLMs [45, 89]. Another more direct approach is to finetune an LLM or LMM directly on classification and similarity tasks [27, 103]. A more efficient line of work finetunes encoder-decoder and decoder-only representations directly to better align modalities or tasks [32, 59, 64, 65]. Another area of finetuning-free approaches proposes prompting the model with a customized distillation prompt (e.g. \"[TEXT] The meaning of the previous sentence in one word is:\") and then extracting a representation from the weights or activations of the model [30, 31, 37, 56]. Other methodologies require the use of more complex methods like mixture-of-experts models [49], a finetuned expert model [92], or LLM-based embedding reranking [17].\nTo summarize, the current SOTA still faces the following challenges when extracting features from generative models: (1) being limited to modality-specific rather than truly multimodal features, (2) requiring finetuning of the model or embedding, (3) limited flexibility due to specialized prompts, and (4) relying on expert or multiple models. Our approach remedies each of these problems. SAVs yield effective multimodal embeddings (as opposed to modality-specific embeddings) without the need for any gradient-based finetuning-whether the LMM or the embedding itself. Furthermore, SAVs can be flexibly applied to a variety of discriminative VL tasks without any additional models."}, {"title": "3. Methods", "content": "In this section, we outline our approach for using sparse attention vectors from the activation space of a transformer-based large multimodal model (LMM) as features for any discriminative VL task. The method consists of three main steps: (i) extracting the attention vectors from all attention heads in the model, (ii) identifying a sparse set of vectors based on their ability to consistently return the correct label for some support set of examples, and (iii) using these sparse features to classify new queries. We begin with a formal description of the transformer decoder LLM and its attention mechanism, followed by the detailed methodology for sparse attention vector selection and classification. A detailed view of our method is shown in Figure 2.\n3.1. Preliminaries\nA transformer-based large language model (LLM) with L layers and H attention heads per layer processes input sequences through multi-head self-attention mechanisms. Each layer combines multiple attention heads to capture different aspects of the input sequence, followed by feed-forward networks for further processing.\nMulti-Head Attention. Let $x = \\{x_1, x_2,..., x_N\\}$ represent a sequence of input tokens, where $x_i$ is the i-th token. For each layer $l \\in \\{1, ..., L\\}$, the input sequence is projected into queries, keys, and values for each attention head $m \\in \\{1, ..., H\\}$. Each head performs the following scaled dot-product attention:\n$h^m_l(x_i) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_m}})V$\nwhere Q, K, and V are the query, key, and value matrices respectively, and the dimensionality of each head $d_m$ which is given by $\\frac{d}{H}$ (the embedding dimension divided by the number of heads). We denote $h^m_l(x_i)$ as an attention vector for head m in layer l.\nThe outputs of all heads are concatenated and projected to form the layer output:\n$\\text{MultiHead}(x_i) = \\text{Concat}(h^1_l(x_i), ..., h^H_l(x_i))W^O$\nwhere $W^O$ is the output projection matrix.\nIn our work, we look to leverage attention vectors for the purpose of vision-language classification tasks. Specifically, the attention vectors are used as latent representations of the inputs to both find attention heads in an LMM suited for a classification task and then perform downstream inference using those selected attention heads. We describe our method in detail in the sections that follow.\n3.2. Sparse Attention Vectors\nOur key insight is that within the many attention heads and transformer layers of an LMM, there exists a sparse subset that can serve as effective features for vision-language classification tasks. We present a three-step method to identify and utilize these features to build lightweight classifiers.\nStep 1: Extracting Attention Vectors. Given a frozen LMM and few-shot examples of sequence-label pairs ${(X_1, Y_1), (X_2, Y_2), ..., (x_N, Y_N)}$ we first extract the attention vectors for each sequence $x_i$. Specifically, we compute the attention vector $h^m_l(x)$ for head m from layer l for the final token $x_f$. This yields a set of attention vectors $\\{h^m_l(x) | i = 1, . . ., N \\}$ for each head m and layer l.\nStep 2: Identifying Relevant Vectors. The central question is how to identify which attention vectors are naturally suited for the discriminative task at hand. We evaluate each vector's discriminative ability by computing its performance under a nearest class centroid classifier.\nSpecifically, for each class $c \\in C$, compute its centroid (or mean) attention vector across the few shot examples:\n$\\mu_c^{l,m} = \\frac{1}{N}\\sum_{j:y_j=c} h^{l,m}(x_j)$\nwhere N = ${j : Y_j = c}$ is the set of indices of examples with label c. For each input $x_i$, we compute its cosine similarity to each class centroid head:\ns_{l,m}(x_i, C) = \\frac{h^{l,m}(x_i) \\cdot \\mu_c^{l,m}}{||h^{l,m}(x_i)|| ||\\mu_c^{l,m}||},  \\forall c \\in C\nNext, we measure the discriminative ability of each head by its performance as follows:\n$\\text{score}(l, m) = \\sum_{i=1}^N 1[y = Y_i]$\nwhere the nearest class centroid label is given as $\\hat{y} = \\text{arg max}_{c \\in C} s_{l,m}(x_i, c)$, and $1[]$ is the indicator function that evaluates to 1 when the condition is true (and 0 otherwise). We denote the set of k top-scoring heads as $H_{sav}$:\n$H_{sav} = \\{(l,m) | \\text{score}(l, m) \\text{ is among k highest scores}\\}$\nStep 3: Classification with Sparse Attention Vectors.\nGiven a query sequence Q to classify, we leverage our sparse set of heads $H_{say}$ for prediction. For each head $(l,m) \\in H_{sav}$, we compute the class centroid $\\mu_c^{l,m}$ closest to the query as follows:\n$\\hat{y}_{l,m} = \\text{arg max}_{c \\in C} s_{l,m}(Q^T, c)$\nwhere $s_{l,m}(\\cdot, \\cdot)$ is defined as in Step 2. Our final class prediction counts the majority vote across all heads in $H_{SAV}$:\n$\\text{arg max}_{Y \\in C} \\sum_{(l,m) \\in H_{SAV}} 1[\\hat{y}_{l,m} = y]$\nThis approach reveals a surprising capability of LMMs: with just a few carefully selected attention heads ($H_{SAV} <<< LH$), we can transform a generative language model into a lightweight vision-language classifier. This finding suggests that classification-relevant features naturally emerge within specific attention heads during model pretraining."}, {"title": "4. Evaluation", "content": "We apply SAVs to two state-of-the-art LMMs-LLaVA-OneVision [41] and Qwen2-VL [91]. We also do a rigorous comparison of our method to strong few-shot and finetuning baselines on a variety of different discriminative vision-language tasks covering safety, VQA, and classification.\n4.1. Implementation Details\nWe implemented our approach in PyTorch [70]. We use the official implementations of each model, and all of our experiments can be run on a single NVIDIA A100 GPU. More details of the implementation is included in the supplementary material in Section B.\n4.2. Models\nIn our work we demonstrated the effectiveness of Sparse Attention Vectors utilizing the following models: (1) Qwen2-VL [90] improves on its predecessor with dynamic processing of images of varying resolutions into different numbers of visual tokens in order to align with human perception of those images. This dynamic processing paired with novel positional embeddings that effectively fuse positional information across modalities affords Qwen2-VL its SOTA performance on a variety of image-text and video-text tasks. (2) LLaVA-OneVision [42] is an open source LMM that performs well in single-image, multi-image and video tasks thanks to its pretraining and AnyRes visual processing.\nBoth Qwen2-VL and LLaVA-OneVision finetune on a variety of tasks, from visual question-answering to document understanding and video tasks. These much larger and diverse finetuning regimes contribute significantly to these models' remarkable performance on an eclectic variety of generative vision-language tasks.\n4.3. Datasets\nVQA Datasets. In our work, we evaluate on VQA benchmarks, many of which can be formulated as a discriminative task. (1) BLINK [15] contains many tasks that are intuitive for humans but complicated for multimodal models such as multi-view reasoning, and visual similarity comparison. Since potential answers in BLINK are multiple choice, the class labels would be given as C = {\u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201cD\u201d} (note: the number of labels depends on the possible number of options allowed for a task). (2) NaturalBench [40] is a compositional dataset collected from natural image-text corpora but validated with human filtering. Each sample of the dataset contains two questions on challenging compositional differences between two similar images, making NaturalBench especially challenging for any existing VL models. The class labels are C = {\"A\", \"B\"}. As suggested in the paper, we evaluate \u201cquestion accuracy\u201d which awards one point if a model correctly answers a question for both images, \"image accuracy\" which awards a point when a model answers both questions for an image, and finally \"group accuracy\" awards one point when a model correctly answers all four pairs.\nSafety. (1) LMM-Halucination [8] is a dataset which evaluates the hallucinations of the models when answering multimodal tasks. We report the raw classifcation accuracy of our method. Thus, the set of class labels for this task is given by C = {\"hallucinating\", \u201cnot hallucinating\"}. (2) VLGuard [106] is a dataset focusing on vision-language safety which identifies 4 main categories of harmful content: Privacy, Risky Behavior, Deception and Hateful Speech. VLGuard proposes Attack Success Rate (ASR) for evaluating unsafe inputs and Helpfulness for evaluating safe inputs. We reformat it to be a classification task, where the set of class labels is given by C = {\"safe\", \"unsafe\"}. Note that the ASR = 1 unsafe subset accuracy.\nImage Classification. (1) EuroSAT [18] is a dataset that tackles the challenge of land use and land cover classification from satellite images. (2) Oxford-IIIT-Pets [69] is a dataset containing 37 categories of pet and roughly 200 images for each category. It is a very challenging dataset as it contains some breeds of cats that is very hard to discern from one another.\n4.4. Baselines\nFor our primary results, we utilized SAVs with 20 examples per label. We compared our method with multiple state-of-the-art baselines. Zero-shot (ZS) baselines are implemented through querying the model directly and generating a response. In addition to ZS, we also compare to several test-time and finetuning few-shot methods (all with the exact same sample complexity as SAVs). For instance, we compare to the current state-of-the-art multimodal few-shot method, MTV [26] as well as LoRA [25] finetuning the model for each task. Since our method involves sparse, informed feature selection, we also compare to common naive feature extraction methods: using last-layer and all-layer attention vectors instead of selecting task's specific heads.\n4.5. Results\nResults are shown in Table 1. An advantage of our method is its adaptability to any discriminative vision-language task that has image, text, or interleaved image-text inputs. As such, we demonstrate that applying SAVs to a wide range of tasks in safety, VQA, and image classification outperforms all zero-shot baselines, and even drastically closing the gap with discriminative VLMs on image classification compared to SigLIP and CLIP. It is worth noting that these models cannot be directly compared to on tasks requiring interleaved image-text data. Furthermore, our approach even significantly improves over both state-of-the-art few-shot and finetuning methods. Finally, not only is our method broadly successful across a wide-range of tasks, but it also improves on challenging perception tasks that require visual and compositional reasoning abilities (e.g. BLINK and NaturalBench) that all VL models struggle with. For more results, please refer to our Supp. section in Section A.\nIn the following subsections, we demonstrate important properties and capabilities of our method through various ablation studies and additional experiments.\n4.6. Ablations\nWe perform a comprehensive ablation study of our method on MHaluBench, NaturalBench, and EuroSAT (see Table 2). For more ablations, please refer to Section A.1 in the Supp. For all ablations, we use LLaVA-OneVision-7B. Varying number of examples. In Figure 4 (left), we examine the impact of varying the number of few-shot examples used in our method. Our primary results in Table 1 indicate that just 20 examples per label are necessary to yield state-of-the-art performance on a variety of discriminative VL tasks. This ablation shows that accuracy on these tasks can scale with increasing numbers of examples per label.\nVarying number of attention vectors. Our method involves selecting a very sparse set of heads out of hundreds from which to extract attention vectors. We vary the number of attention vectors selected in Step 2 of our method and demonstrate that just 20 vectors are enough to realize nearly all of the classification accuracy of our method. Results are shown in Figure 4 (right).\nSAVs are flexible to different evaluation strategies. In our method, we leverage class centroid classification as the evaluation method for selecting sparse features. We view this flexibility of the sparsification method to be a key feature of our work. As such, we compare our class centroid classification approach to k-nearest neighbors (KNN) and linear probing. For linear probing, we train a lightweight MLP module for 20 epochs using the top heads' features. All methods make use of the same 20 examples per label for consistency. Our results in Table 2a show that class centroid classification outperforms both KNN classification and is comparable with linear probing.\nComparing heads vs. layers. Based on prior work and transformer-architecture intuition, we treat the attention vectors outputted by the heads as a viable set of features for discriminative VL tasks. We verify this intuition by comparing the performance of selecting 2 sparse layers to selecting sparse heads as feature maps for our tasks. As shown in Table 2b, head-based attention vectors outperform concatenated layer features on all three benchmarks.\nToken position selection. Because the last-token of a sequence in a decoder-only LMM attends to all of the prior tokens in an input sequence, it is natural to extract SAVs from the heads of the last token. However, to validate this intuition, we compare the performance SAVs to extract sparse vectors from other tokens (first, middle, and last). Overall, our results in Table 2c show that the last token is the best option for selecting heads for SAVs.\n4.7. Additional Experiments\nIn this subsection, we present experiments that demonstrate additional properties and capabilities of SAVs, beyond its use as features for discriminative VL tasks. Additional experiments can be found in Section A.2 of the Supplementary. For all experiments, we use LLaVA-OneVision-7B.\nVisualizing SAVs. SAVs is both an efficient and interpretable method for leveraging generative LMMs for discriminative VL tasks. To emphasize this point, we demonstrate the selected heads for hallucination detection, relative depth, and image classification in the first row of Figure 3. The visualizations demonstrate both the sparsity and specificity of the SAVs that are used for each individual task. Unlike other black-box methods, our approach clearly out-"}, {"title": "5. Conclusion", "content": "Our research demonstrates the effectiveness of extracting Sparse Attention Vectors (SAVs) from the heads of an LMM and utilizing them directly for discriminative classification. Our method stands out by using only few-shot examples per label and only less than 1% of the heads to outperform zero-shot, few-shot, and fine-tuned baselines on a variety of VL tasks. In addition, SAVs allows generative LMMs to close the gap with discriminative VLMs on image classification tasks while also being an interpretable method that can generalize to similar tasks. Our ablations reveal the flexibility of using any classification method as a sparsification method for attention vectors and also shows that features are found as outputs of heads rather than layers. Overall, these results demonstrate that SAVs is a lightweight, performant, and generalizable method for extending generative LMMs' capabilities to discriminative tasks. We are encouraged by the outcomes, and anticipate many directions for future work. In addition to methodological improvements, we look forward to the application of SAVs as features for multimodal retrieval, data compression, or more generally as a distilled representation for downstream models."}, {"title": "6. Limitations", "content": "Sparse Attention Vectors are a significant step in generalizing the capabilities of generative LMMs to discriminative tasks. Nevertheless, it is valuable to consider certain limitations of our approach. SAVss are a method that requires access to the model's internal architecture and so may not be directly applicable to closed-source models like GPT-4 [67] and Gemini [82, 83]. Additionally, some tasks like image-text retrieval [24, 84] can benefit from more fine-grained confidence metrics attached to each label than proportion of voting heads per label. These challenges prompt future work in these directions as well as exciting questions about how to use SAVs as feature embeddings for other tasks."}]}