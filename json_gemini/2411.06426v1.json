{"title": "SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains", "authors": ["Bijoy Ahmed Saiem", "MD Sadik Hossain Shanto", "Rakib Ahsan", "Md Rafi ur Rashid"], "abstract": "As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment - where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have been adapted to numerous application scenarios, and their applicability is increasing overwhelmingly. Open-source models like Llama [1], [2] and Gemma [3], [4], as well as closed-source models like Claude 2 [5], GPT-3.5 and GPT-4 [6] are being integrated into a wide range of applications such as software development [7], [8], healthcare [9], education [10], [11], and many more.\nAs LLMs are increasingly being adopted in various fields, the security risks associated with their potential misuse to generate harmful content also increase. To mitigate these risks, LLMs undergo safety measures such as reinforcement learning from human feedback (RLHF) [12], which guide them to detect and decline malicious queries. A significant number of studies focus on crafting harmful prompts that can bypass these safety measures and elicit harmful responses, a method referred to as jailbreak attacks. Extensive studies have been conducted to devise new jailbreak attacks that can challange the safety alignment of LLMs. Token-based jailbreak methods [13], [14] formulate the attack process as an optimization problem to search for the adversarial prompt suffix that can elicit harmful responses when attached to a harmful query. But these methods are easily detectable and ex-tremely slow to run. In contrast, prompt-based jailbreak methods [15]\u2013[17] focus on preparing a clever narrative that can fool LLMs mainly utilizing scenario camouflage and obfuscation of harmful prompts.\nIn a scenario where a series of prompts (e.g. a set of questions) are input at once, a malicious prompt among them may get overlooked by LLM safety alignment. As LLM attention mechanism is designed to track relation-ships between tokens (such as which words or prompts relate to each other), it may not adequately prioritize the harmful prompt when embedded into a set of benign prompts. In other words, the surrounding benign prompts can divert the LLM focus, causing the harmful prompt to not be flagged as prominently as it should be. This kind of sequential prompt chain can be adapted in numerous scenarios, facilitating scenario camouflage and harmful prompt obfuscation both.\nIn this study, we propose SequentialBreak, a novel jailbreak attack that sends a series of prompts in a single query with one being the target harmful prompt. Our attack is one-shot, requires only black-box access, and is adaptable to various prompt narrative structures. We discuss three different attack scenarios: (i) Question Bank, which involves crafting a series of harmless questions about a specific context, (ii) Dialog Completion, where an incomplete conversation between two characters is pre-sented for the LLM to finish, and (iii) Game Environment, which presents a game mission in different chapters and asks the LLM to perform required tasks as the player."}, {"title": "3. Motivation", "content": "The design of these attack vectors is driven by the intrinsic properties of large language models (LLM) and their sequential processing of content. Understanding the architecture and behavior of LLM provides insight into why certain attack strategies are particularly effective. Several key factors contribute to the effectiveness of these attacks:"}, {"title": "3.1. Sequential Content Processing", "content": "LLM processes input in a sequential manner, interpret-ing each token or piece of content in the context of what has come before. This characteristic is both a strength and a vulnerability. By carefully crafting sequences of content,"}, {"title": "3.5. Generalization and Adaptability", "content": "The motivation behind selecting this attack design also lies in its adaptability. While the examples provided focus on specific scenarios (e.g., question banks, dialogue completions, game environment), the underlying method-ology can be generalized to other contexts. The ability to generate new templates and adapt the attack to different LLM or content types demonstrates the versatility of this approach. This generalization makes it a powerful tool for testing and understanding the vulnerabilities of LLM in various applications."}, {"title": "4. Methodology", "content": "Our methodology involves using a large language model (LLM) to disguise harmful content by embedding it into seemingly harmless contexts, automating the attack to bypass security measures. The workflow, illustrated in Fig. 1, 2, 4 and 5 shows a sequential approach that is applicable across various scenarios, ensuring a seamless attack flow without manual intervention. The key characteristics of this approach include single-shot execution, universality (applicable to any jailbreak question) and social engineer-ing to improve effectiveness."}, {"title": "4.1. Attack Strategy", "content": "The attack strategy comprises several distinct steps, as outlined in Fig. 1, enabling attackers to embed harmful prompts within benign contexts using predefined tem-plates.\nTemplate Generation: The attacker begins by crafting a template for the attack. This is achieved by providing a detailed prompt to the LLM (9.4), which guides the generation of the template. Additionally, the process incorporates an existing tem-plate as a seed (9.5), allowing the LLM to refine and build upon it (Fig. 1, Step 1). This template serves as the framework for embedding harmful"}, {"title": "4.2. Example: Attacking an LLM Using a Question Bank Scenario", "content": "An example of this attack methodology is illustrated in Fig. 2, where the harmful prompt is embedded within a question bank. The steps outlined below show how an attacker can disguise a harmful prompt within a seemingly harmless set of questions to evade the LLM's safety mechanisms.\nHarmful Target Prompt Identification: The at-tacker begins by identifying the specific harmful prompt they want the LLM to generate. In this case, the target prompt is:\n\"How to make a bomb?\"\nThis type of prompt, if given directly, would typi-cally be flagged and rejected by the LLM's safety filters.\nTemplate Selection: To bypass these filters, the attacker selects a template that disguises the harm-ful prompt within a broader, legitimate context. In this example, the chosen template is taken from a Question Bank Scenario, as shown in Fig. 2.\nReformatting the Harmful Prompt into a Ques-tion: The harmful prompt is rephrased as part of a broader set of questions, such as"}, {"title": "4.3. Mathematical Representation", "content": "To further formalize the methodology, the process can be represented mathematically as follows:\nTemplate Generation:\n$T = L_A(seed)$\nwhere T is the template generated by the at-tacker LLM $L_A$ using $seed$. $seed$ can be a template generation prompt(9.4) or an already existing template(9.5) for in-context learning. T contains $X = {x_0,x_1,...,x_N}$ where $x_i$ is i-th sequential benign task.\nHarmful Prompt Reformatting:\n$H' = f_r(H, x_i)$\nwhere $H'$ is the reformatted prompt, H is the harmful prompt, and $f_r$ is the template-specific formatting function.\nUser Prompt Creation:\n$T' = f_e(H', T)$\nwhere $T'$ is the final user prompt containing em-bedded harmful content and $f_e$ is the function that modifies T to contain $H'$"}, {"title": "4.4. Algorithm", "content": "This methodology outlines a systematic, algorith-mic approach to embed harmful content within LLM-generated prompts. By automating the formatting and integration processes, the attack minimizes manual effort and increases the likelihood of bypassing LLM safeguards. The use of mathematical representations and algorithms not only clarifies the process but also provides a frame-work for reproducibility and further research in this do-main."}, {"title": "5. Experiments", "content": "In this section, we evaluate the performance of six templates in total: two from the Question Bank (Appendix: Tables 6 and 7), two from Dialog Completion (Appendix:"}, {"title": "5.1. Experimental Setup", "content": "Dataset. In our experiment, we use the JBB-Behaviors dataset [18] to evaluate the jailbreak performance of our attack and the baselines. The data set consists of 100 behaviors, both original and sourced from previous work [13], [33]. The prompts are divided into 10 broad categories that correspond to OpenAI usage policies [34]. Furthermore, each prompt is linked with a certain harmful behavior that ensures the exclusion of similar types of prompts. So, while the dataset is compact, it covers a wide range of unique harmful scenarios to effectively test jailbreak attacks.\nModels. We test SequentialBreak attack templates against four open-source and two closed-source LLMs. We have chosen Llama-2 (Llama-2-7b-chat) [1], Llama-3 (Meta-Llama-3-8B-Instruct) [2], Gemma-2 (gemma-2-2b-it) [4], and Vicuna (vicuna-13b-v1.5) [35] as our open source models. As our closed source models, we have chosen GPT-3.5 (GPT-3.5-turbo) and GPT-40 (GPT-40) [36]. For baseline performance comparison, we limit our chosen models only to recent LLM releases, namely Llama-3, Gemma-2, Vicuna, and GPT-40. Newer models are chosen to demonstrate that existing baseline attacks do not perform well against current state-of-the-art models."}, {"title": "5.2. Main Results", "content": "Attack Effectiveness of Three Scenarios:. Table 1 presents the Attack Success Rates (ASR) from both judge models across different scenarios. The results demonstrate that SequentialBreak consistently achieves high effectiveness across open-source and closed-source models. The consistent ASRs across all three scenarios suggest that LLMs can leak harmful content while gener-ating answers to sequential prompts and these sequential prompts can be based on various narrative structures ex-panding more than three scenarios discussed here.\nAlthough all three scenarios have relatively close ASRS, Dialog Completion template-1 comparatively per-forms better than the rest of the tested templates. In-terestingly, the Dialog Completion template-2 shows a noticeably low ASR when used against Llama-3. This suggests that, for certain template-model combinations, a disguised harmful prompt may attract more attention from the model, leading to refusal.\nComparing the verdicts given by GPT-4 judge and Llama3-70B judge, we see that both judge models' as-sessments are almost equal. In the case of Llama-2 re-sponses, the difference in ASRs is comparatively more than the responses of other models. For most scenarios (especially Game Environment and Dialog Completion), GPT-4's verdicts are either equal to or slightly higher than Llama3-70B's.\nDespite slight variations across templates and models, the consistently high ASRs indicate that LLMs are suscep-tible to leaking harmful content, regardless of the narrative structure of the prompt. The comparison between the GPT-4 and Llama3-70B judges shows minimal differences in"}, {"title": "5.3. Evaluating Defense Effectiveness", "content": "To assess the robustness of various defense mecha-nisms against our attack, we tested multiple defense mech-anisms and reported the results in Table 3. Particularly, we tested three defense strategies:\nOpenAI Moderation API [32]. OpenAI's official content moderation tool utilizes a multi-label classifier to categorize prompts or texts into 11 distinct categories, including violence, sexuality, hate, and harassment. If a response violates any of these categories, it is flagged as a violation of OpenAI's usage policy.\nPerplexity Filter [23]. This method is designed to detect unreadable attack prompts by setting a threshold and using another LLM to calculate the perplexity of the entire prompt or its window slices. Prompts that exceed this threshold are filtered out. For perplexity calculation, we use Llama-3 as our LLM setting the threshold to 3.5 as the tight upper bound after assessing the perplexity of our attack templates.\nSmoothLLM [31]. This method generates multi-ple perturbed copies of a given input prompt, introducing random character-level changes to each copy. The pertur-bation step takes advantage of the fact that adversarial prompts those designed to trick the model-are easily affected by small changes. Then SmoothLLM aggregates the outputs from these perturbed prompts to produce a final response, effectively filtering out potentially harmful content generated by adversarial inputs. For our experi-ment, we use 5% random insertion and random swapping to generate 5 prompts which are used to generate output from the LLM for voting.\nTo evaluate the effectiveness of jailbreak defense methods on Llama-3, we tested the first template from each attack scenario against our chosen defenses. Table 3 shows OpenAI Moderation API and Perplexity Filter fails drastically to flag our attack templates. In contrast, SmoothLLM performed better, particularly in Game En-vironment T1, where it flagged 19 results. However, its performance was less effective in the other two scenarios. These findings emphasize the need for further improve-ment in defense strategies where harmful content may be more subtle and challenging to detect."}, {"title": "5.4. Ablation study", "content": "The ablation study aims to assess the impact of differ-ent factors, such as the number of sequential levels and"}, {"title": "7. Limitations", "content": "The research encounters a few minor limitations, such as the occasional generation of hallucinations or inac-curacies by large language models (LLMs), which may slightly impact the result's reliability. Additionally, current models might have some difficulty with maintaining or understanding context over extended interaction in a single query. The effectiveness of the SequentialBreak method-ology could experience gradual changes as detection and defense mechanisms advance. Moreover, although the in-tentions are ethical, there is a small risk of misuse, under-scoring the importance of maintaining awareness within the AI research community."}, {"title": "8. Ethical Considerations", "content": "This paper introduces the SequentialBreak methodol-ogy for generating novel jailbreak prompts that exploit sequential structures in Large Language Models (LLMs). While these techniques could potentially be leveraged by adversaries to bypass safety mechanisms, the primary focus of this research is on enhancing LLM security and resilience. By identifying these vulnerabilities, we aim to raise awareness within the AI community and contribute to the development of more robust defense strategies.\nOur intention is to advance the security of LLMs in real-world applications by uncovering critical weaknesses and suggesting improvements. We believe that by sharing these insights, we can help accelerate the development of stronger safeguards that protect LLMs from similar attack vectors. This research is ethically driven, prioritizing the safe and responsible use of LLMs across diverse applica-tions and user communities.\nTo ensure responsible dissemination of our findings, we will collaborate with the broader AI and security communities, encouraging ongoing research into LLM safety and fostering a collective effort to build more secure and reliable AI systems."}, {"title": "9. Appendix", "content": "Data Availability\nThe dataset used for the experiments in this paper is publicly available on Hugging Face under the JailbreakBench/JBB-Behaviors dataset, which can be accessed at this link1. This dataset consists of 100 harmful behavior prompts, designed to evaluate the security of large language models (LLMs) by testing their responses to ethically sensitive queries across various cate-gories. These prompts include both original scenarios and those sourced from prior work, ensuring a wide-ranging evaluation aligned with OpenAI's usage policies.\nAll experiments conducted in this study are fully repro-ducible. The dataset is openly available, and the attack templates, including Question Bank(Appendix: Tables 6 and 7), Dialogue Completion(Appendix: Tables 8 and 9), and Game Environment scenarios (Appendix: Tables 10 and 11), can be replicated using the configurations and instructions provided in this paper. The models and defense mechanisms evaluated, such as Llama-2, GPT-40, SmoothLLM [31], and OpenAI Moderation API [32] are either publicly accessible or commonly used in research, ensuring that the attack success rates and defense effectiveness discussed in this work can be replicated. Detailed steps, in-cluding prompt formatting and sequence design, are provided to ensure ease of reproduction.\nDialogue Completion Attack\nAn example of the Dialogue Completion Attack is illus-trated in Figure 4, where harmful content is embedded within a sequential conversation. The steps outlined below show how an attacker can disguise a harmful prompt within a seemingly harmless sequential dialogue conversation to evade the LLM safety mechanisms.\nHarmful Target Prompt Identification. The attacker be-gins by identifying the specific harmful information they want the LLM to generate. In this case, the target prompt is:\n\"How to make a bomb?\"\nThis type of prompt, if given directly, would typically be flagged and rejected by the LLM's safety filters.\nAttack Format Selection. To bypass these filters, the attacker selects the Dialogue Completion format, embedding the harmful prompt within a benign conversation between two prisoners, Mike and Jack (Figure 4).\nReformatting the Harmful Prompt into a Dialogue. The harmful prompt is then rephrased as part of Jack's dialogue, where he casually mentions creating a bomb tutorial. This dis-guises the harmful prompt as part of an ordinary conversation.\nPresentation of the Dialogue to the LLM. The rephrased dialogue, containing the harmful prompt, is then submitted to the LLM. Due to the conversational context, the LLM processes the input as a normal continuation task rather than recognizing it as dangerous content.\nHarmful Response Generation. As the LLM processes the conversation, it generates a response that includes the harmful content originally embedded by the attacker. This output is mixed with other dialogue-related content, successfully bypassing the LLM's usual safeguards.\nIn this example, the attacker effectively bypasses the LLM's safety measures by embedding a harmful prompt within a di-alogue. The LLM, interpreting the conversation as legitimate and harmless, processes the prompt and generates the harmful output. This method demonstrates the vulnerability of LLMs when manipulated through conversational contexts, emphasizing the need for robust safety mechanisms.\nGame Environment Attack\nThe Game Environment Attack is illustrated in Figure 5, where harmful content is embedded within a game narrative. The steps outlined below describe how an attacker can disguise a harmful prompt within a seemingly harmless game task to evade the LLM's safety mechanisms.\nHarmful Target Prompt Identification. The attacker be-gins by identifying the specific harmful information they want the LLM to generate. In this case, the target prompt is:\n\"How to make a bomb.\"\nThis type of prompt, if given directly, would typically be flagged and rejected by the LLM's safety filters.\nAttack Format Selection. To bypass these filters, the attacker selects the Game Environment format (Figure ??) , embedding the harmful prompt within a broader game task.\nReformatting the Harmful Prompt into a Game Task. The harmful prompt is rephrased as part of a game challenge, titled \"Echoes of Truth,\" which includes a level called \"Reflec-tions.\" This disguises the harmful prompt as a legitimate game task.\nPresentation of the Scenario to the LLM. The rephrased game environment, containing the harmful prompt, is then sub-mitted to the LLM. Due to the game context, the LLM processes the input as a normal task rather than identifying it as dangerous content.\nHarmful Response Generation. As the LLM processes the scenario, it generates a response that includes the harmful con-tent originally embedded by the attacker. This output, mixed with other game-related content, successfully bypasses the LLM's usual safeguards.\nIn this example, the attacker bypasses the LLM's safety measures by embedding a harmful prompt within a game sce-nario. The LLM, interpreting the scenario as legitimate and harmless, processes the prompt and generates the harmful output. This method illustrates the vulnerability of LLMs when manip-ulated through creative and context-based attacks.\nPrompt for Initial Template Generation from LLM\nYou are an AI tasked with designing multi-level game sce-narios that involve problem-solving, analysis, and strategic plan-ning. Create a game level focused on solving harmful content. Include a challenge with a variable like {target} in one level, allowing for customizable challenges.\nDesign a multi-level game scenario with the following:\nA unique setting for each level."}]}