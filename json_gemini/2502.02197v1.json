{"title": "AN EFFICIENT LOCAL SEARCH APPROACH FOR POLARIZED\nCOMMUNITY DISCOVERY IN SIGNED NETWORKS", "authors": ["Linus Aronsson", "Morteza Haghir Chehreghani"], "abstract": "Signed networks, where edges are labeled as positive or negative to indicate friendly or antagonistic\ninteractions, offer a natural framework for studying polarization, trust, and conflict in social systems.\nDetecting meaningful group structures in these networks is crucial for understanding online discourse,\npolitical division, and trust dynamics. A key challenge is to identify groups that are cohesive internally\nyet antagonistic externally, while allowing for neutral or unaligned vertices. In this paper, we address\nthis problem by identifying k polarized communities that are large, dense, and balanced in size.\nWe develop an approach based on Frank-Wolfe optimization, leading to a local search procedure\nwith provable convergence guarantees. Our method is both scalable and efficient, outperforming\nstate-of-the-art baselines in solution quality while remaining competitive in terms of computational\nefficiency.", "sections": [{"title": "Introduction", "content": "Signed networks extend traditional graph representations by associating each edge with a positive or negative number,\nindicating friendly or antagonistic relationships. Originating from studies on social dynamics in the 1950s (Harary,\n1953), signed networks introduce fundamental differences in graph structure that make many algorithms designed for\nunsigned networks inapplicable (Tang et al., 2016; Bonchi et al., 2019; Tzeng et al., 2020). These challenges have\nfueled extensive research in recent years, leading to advances in signed network embeddings, signed clustering, and\nsigned link prediction. We refer to the survey by (Tang et al., 2016) for a comprehensive review of these methods.\nMost relevant to this paper is the problem of signed clustering, which we split into two categories: (i) signed network\npartitioning (SNP), and (ii) polarized community discovery (PCD). The latter is the problem studied in this paper.\nThe goal of signed clustering is to identify k conflicting groups (clusters) where intra-cluster similarity is maximized\n(predominantly positive) and inter-cluster similarity is minimized (predominantly negative). This problem has numerous\nreal-world applications (Tang et al., 2016), particularly in social networks, where vertices represent individuals and\nedges capture friendly or antagonistic relationships (e.g., shared or opposing political views). Detecting conflicting\ngroups in such networks is crucial for analyzing polarization (Adamic and Glance, 2005; Yardi and Boyd, 2010; Xiao\net al., 2020), echo chambers (Garrett, 2009; Flaxman et al., 2016), and the spread of misinformation (Shu et al., 2017;\nCooke, 2018; Yang et al., 2019).\nIn the SNP problem, the k groups must form a partition of the vertices, meaning every vertex must be included. Spectral\nmethods based on the signed Laplacian have been widely used to tackle this problem (Kunegis et al., 2010; Chiang\net al., 2012; Mercado et al., 2019; Cucuringu et al., 2019). Alternatively, formulating SNP explicitly as an optimization\nproblem leads to the well-studied correlation clustering (CC) problem (Bansal et al., 2004), which is known to be\nAPX-hard. Consequently, numerous approximation algorithms have been developed (Bansal et al., 2004; Charikar et al.,\n2005; Demaine et al., 2006; Ailon et al., 2008), with local search methods standing out for their strong performance in\nboth clustering quality and computational efficiency (Thiel et al., 2019; Chehreghani, 2023; Aronsson and Chehreghani,\n2024a;b).\nThe problem formulation of PCD is identical to that of SNP, except that the k clusters are not required to form a\npartition of the vertices, allowing some vertices to remain unassigned. The goal is therefore to only find the dense\nsubgraphs of polarized communities. This accounts for cases where certain vertices are neutral w.r.t. the underlying"}, {"title": "Problem Formulation", "content": "We start by introducing the relevant notation, followed by an introduction to correlation clustering (CC), which is\nconnected to our problem. Finally, we present the problem of polarized community discovery (PCD)."}, {"title": "Notation", "content": "Consider a signed network $G = (V, E)$, where $V$ is the set of objects and $E$ the set of edges. The weight of an edge\n$(i, j) \\in E$ is represented by the element $A_{i,j} \\in \\{-1,0,+1\\}$ of an adjacency matrix $A$. The matrix $A$ is symmetric\nwith zeros on the diagonal, which means $A_{i,j} = A_{j,i}$ and $A_{i,i} = 0$. We use $A_{i,:}$ and $A_{:,j}$ to denote row $i$ and column\n$j$ of $A$, respectively. While we restrict all similarities to be in $\\{-1,0, +1\\}$ (for clarity), all methods presented in\nthe paper extend to arbitrary similarities in $\\mathbb{R}$. We can decompose the adjacency matrix as $A = A^+ - A^-$ where\n$A^+ = \\max(A, 0)$ and $A^- = \\max(-A, 0)$. A clustering with $k$ clusters is denoted $S[k] = \\{S_1, ..., S_k\\}$, where each\n$S_m \\subseteq V$ is the set of objects assigned to cluster $m \\in [k] = \\{1, ..., k\\}$. Let $N_{intra}^+ = \\sum_{m \\in [k]} \\sum_{i,j \\in S_m} A_{i,j}^+$ and $N_{intra}^- =\n\\sum_{m \\in [k]} \\sum_{i,j \\in S_m} A_{i,j}^-$ be the sum of positive and absolute negative intra-cluster similarities, respectively. Furthermore,\nlet $N_{inter}^+ = \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} A_{i,j}^+$ and $N_{inter}^- = \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} A_{i,j}^-$ be the\nsum of positive and absolute negative inter-cluster similarities, respectively."}, {"title": "Correlation Clustering", "content": "We begin by noting that for CC, unlike PCD to be discussed in the next subsection, a clustering $S[k]$ is a partition of $V$,\nmeaning $V = \\bigcup_{m \\in [k]} S_m$ and each $S_m$ is disjoint. A notable feature of CC is its ability to automatically determine the\nnumber of clusters based on the similarities encoded in $A$. However, a variant of this problem has also been studied\nwhere the number of clusters, $k$, is specified as an input (Giotis and Guruswami, 2006). Automatic detection of the\nnumber of clusters could be a desirable property of a clustering algorithm. However, constraining the number of clusters\nto $k$ can act as a form of regularization, and has been shown to produce higher-quality clusters in many scenarios\n(Chehreghani et al., 2012). Given this, the $k$-CC problem can be defined as shown below.\nProblem 1 (k-CC). Find a clustering S[k] that maximizes\n$N_{intra}^+ - N_{intra}^- + N_{inter}^+ - N_{inter}^-$\n(1)\nIn other words, we want to find a clustering that maximizes the sum of intra-cluster similarities and minimizes the\nsum of inter-cluster similarities. The following proposition presents alternative objectives equivalent to maximizing\nEq. 1. While this is known in the CC literature (Chehreghani, 2013), we include a complete summary here to better\ncontextualize our problem.\nProposition 1. Problem 1 is equivalent to finding a clustering S[k] that maximizes any one of the four objectives below\u00b9.\n$N_{intra}^+ + N_{inter}^-$\n(2)\nBy equivalent, we mean they share all local maxima, including the global maximum."}, {"title": "Polarized Community Discovery", "content": "The problem of PCD is similar to CC in that the goal is to identify k clusters $S_1,..., S_k$, where the similarity within\neach cluster is large (and positive) and the similarity between different clusters is small (and negative). However, unlike\nCC, we also allow objects to remain neutral, introducing an additional neutral set $S_0$. This means an object can either\nbe assigned to one of the non-neutral clusters $S_1, ..., S_k$ or designated as neutral by assigning it to $S_0$. Assigning an\nobject to $S_0$ effectively excludes it from influencing the objective function. Specifically, assigning an object $i \\in V$ to\n$S_0$ is equivalent to setting both row $A_{i,:}$ and column $A_{:,i}$ of the matrix $A$ to zero. Consequently, a clustering $S[k]$ is no\nlonger a partition of $V$ and we have $S_0 = V \\setminus \\bigcup_{m \\in [k]} S_m$ (although all clusters are still disjoint). In our view, the goal\nof PCD is to extract k non-neutral clusters that are (i) large, (ii) balanced, and (iii) dense. Dense clusters imply that\neach object should strongly align with its assigned cluster\u2014being highly similar to most objects within its cluster and\nmarkedly dissimilar to those in other clusters. Objects lacking a clear association, such as those similar to objects in\nmultiple clusters or those that are inherently neutral (e.g., low-degree objects), should be labeled as neutral by assigning\nthem to $S_0$. We notice that a natural trade-off exists between the size of the non-neutral clusters and their density, as\nvery small clusters can achieve high density trivially.\nWe begin this section by explaining why Eq. 1, which incorporates all relevant terms, must be considered when neutral\nobjects are allowed. Much prior work on PCD also optimize all terms, but often without providing a detailed justification\nfor this choice. The next proposition provides such an intuition.\nProposition 2. A clustering $S[k]$ with neutral objects $S_0 = V \\setminus \\bigcup_{m \\in [k]} S_m$ that maximizes one of the objectives in\nEqs. 1-5 is not guaranteed to maximize any of the other objectives\u00b2.\nFrom Proposition 2, we conclude that each term in Eq. 1 provides unique information when neutral objects are allowed,\nunlike the standard CC problem, where the different objectives are equivalent, as outlined by Proposition 1. This makes\nEq. 1 the most reasonable objective for optimization in this context, as it effectively balances all contributing terms.\nMoreover, since each term captures unique aspects of the PCD problem, it may be beneficial to weight them differently\nto achieve an optimal trade-off. Furthermore, (Bonchi et al., 2019) showed that for k = 2, an optimal solution to Eq.\n1 consists of no neutral objects, i.e., $S_0 = \\emptyset$. While we cannot directly extend this conclusion to k > 2 (see proof\nof Proposition 2), it is evident that even for k > 2, an object may be assigned to a non-neutral cluster as long as it\nmarginally improves the objective in Eq. 1, even if it does not maintain the density of the graph induced by non-neutral\nclusters $S[k]$. In other words, low-degree objects that should ideally remain neutral may be included in non-neutral\nclusters.\nBased on the above discussion, we conclude that (i) it may be beneficial to weight the terms in Eq. 1 differently and (ii)\nwe must encourage the presence of neutral objects by penalizing large/sparse non-neutral clusters. In prior work, these\nconcerns are typically addressed by weighting inter-cluster terms with a parameter $\u03b1 \\in \\mathbb{R}$ and normalizing Eq. 1 by the\nnumber of non-neutral objects, i.e.,\n$\\frac{(N_{intra}^+ - N_{intra}^-) + \u03b1 (N_{inter}^+ - N_{inter}^-)}{\\sum_{m \\in [k]} |S_m|}$\n(6)\nIf $\u03b1 = 1/(k-1)$, Eq. 6 is commonly referred to as polarity in prior work and is a well-established objective for PCD\n(Bonchi et al., 2019; Tzeng et al., 2020). However, as highlighted in (Gullo et al., 2024), maximizing polarity often\nresults in highly imbalanced clustering solutions. In particular, clustering solutions with the same polarity can differ\nsignificantly in terms of cluster balance. A concrete example illustrating this issue is provided in Appendix C.\n\u00b2Unless k = 2, in which case Eq. 2 and Eq. 1 are equivalent as established in (Bonchi et al., 2019)."}, {"title": "Algorithms", "content": "In this section, we demonstrate how our problem can be solved using Frank-Wolfe (FW) optimization (Frank and\nWolfe, 1956). Specifically, we consider a variant called block-coordinate FW, which we begin by describing in the next\nsubsection. After this, we establish its equivalence to a straightforward and provably efficient local search procedure.\nNext, we analyze the convergence rate of this approach. Following that, we propose practical enhancements to improve\nscalability, enabling the method to handle large problems. Finally, we provide a detailed analysis of the impact of a and\n\u03b2."}, {"title": "Block-Coordinate Frank-Wolfe Optimization", "content": "The Frank-Wolfe (FW) algorithm is one of the earliest methods for nonlinear constrained optimization (Frank and Wolfe,\n1956). In recent years, it has regained popularity, particularly in machine learning, due to its scalability (Jaggi, 2013).\nIn this paper, we use a variant of this method called block-coordinate FW (Lacoste-Julien et al., 2013). This method\nyields a significantly faster optimization procedure while enjoying similar theoretical guarantees. Block-coordinate\nFW is applied to problems where the feasible domain can be split into blocks $D = D^{(1)} \\times ... \\times D^{(n)} \\subseteq \\mathbb{R}^d$, where\neach $D^{(i)} \\subset \\mathbb{R}^{d_i}$ is convex and compact and we have $d = \\sum_{i=1}^{n} d_i$. Let $x_{[n]}$ denote the concatenation of the variables"}, {"title": "Equivalence to a Local Search Approach", "content": "We now show that optimizing Eq. 11 using Alg. 1 is equivalent to the local search procedure in Alg. 2. Let matrix\n$G \\in \\mathbb{R}^{n \\times (k+1)}$, where element $G_{i,m} \\triangleq [\\nabla_i f (x_{[n]})]_m$ is the gradient of $f(x_{[n]})$ w.r.t. variable m of block i evaluated\n at $x_{[n]}^{(t)}$ (solution at step t of Alg. 1). Given this, we present the following theorem.\nTheorem 2. If $x_{[n]}^{(0)}$ in Alg 1 is discrete, the following holds. (a) For our problem (Eq. 11), the solution $x_i^*$\n(line 4 of\nAlg. 1) is the basis vector $e_p$, where $p = \\arg \\max_{m \\in \\{0,...,k\\}} G_{i,m}$ and the optimal value of the step size on line 6 is\n$\u03b3 = 1$. (b) Our objective function in Eq. 10 satisfies $(x^* - x_i^{(t)})^T \\nabla_i f (x_{[n]}^{(t)}) = f(x_{[n]}^*) - f(x_{[n]}^{(t)})$, where $x_{[n]}^*$ is $x_{[n]}^{(t)}$ with\nblock i modified to $x_i^*$.\nFrom part (a) of Theorem 2, the current solution, $x_{[n]}^{(t)}$, remains discrete (i.e., hard cluster assignments) at every step of\nAlg. 1 for all $i \\in [n]$. Moreover, each step of Alg. 1 consists of placing object i in the cluster $m \\in \\{0,..., k\\}$ with\nmaximal gradient $G_{i,m}$. By part (b) of Theorem 2, this is equivalent to placing object i in the cluster that maximally\nimproves our objective in Eq. 7. Based on this, we conclude the following corollary."}, {"title": "Convergence Analysis", "content": "Following the prior work on the analysis of general FW algorithms (Jaggi, 2013; Lacoste-Julien et al., 2013), we begin\nby providing the following definitions.\nDefinition 1 (FW duality gap). The FW duality gap is defined as (Jaggi, 2013)\n$g(x_{[n]}) = \\max_{s_{[n]} \\in D} (s_{[n]} - x_{[n]}) \\cdot \\nabla f(x_{[n]})$, \n(12)\nwhich is zero if and only if $x_{[n]}$ is a stationary point. Furthermore, let $\u011f_t = \\min_{0 \\leq \\tau \\leq t-1} g(x_{[n]}^{(\\tau)})$ be the smallest duality\ngap observed in Alg. 1 up until step t.\nDefinition 2 (Convergence rate). We say the convergence rate of Alg. 1 is at least $O(1/r_t)$ if $E[\u011f_t] \\leq O(1/r_t)$, where\n$r_t$ is some expression involving only t and the expectation is w.r.t. the random selection of blocks on line 3. If n = 1\nthe bound is deterministic.\nThe FW algorithm has been shown to converge to a stationary point of f under various settings, with well-established\nconvergence rates. We summarize a few known results below. The standard FW algorithm (n = 1) achieves a\ndeterministic convergence rate of O(1/t) for concave f (Frank and Wolfe, 1956) and $O(1/\\sqrt{t})$ for non-concave f\n(Lacoste-Julien, 2016; Reddi et al., 2016). For the block variant, (Lacoste-Julien et al., 2013) prove a convergence rate\nof O(1/t) for concave f in expectation. For non-concave f, (Thiel et al., 2019) prove a convergence rate of O(1/t) in\nexpectation, under the assumption that $f(x_{[n]})$ is multilinear in each block $x_i$ including correlation clustering. We here\nextend the analysis of (Thiel et al., 2019) to Problem 2 (k-PCD) using Alg. 2, described in Theorem 3. Note that their\nanalysis cannot be applied directly to our objective function in Eq. 10 as this objective does not satisfy the multilinearity\nproperty.\nTheorem 3. The convergence rate of Alg. 2 is at least $n \\hbar_0/t = O(1/t)$, where $\\hbar_0 = \\sum_{(i,j) \\in E} |A_{i,j}|$.\nThe $O(1/t)$ convergence rate presented in Theorem 3 should be compared with the deterministic convergence rate of\n$O(1/\\sqrt{t})$ for general non-concave functions f under the standard FW method (n = 1) (Lacoste-Julien, 2016; Reddi\net al., 2016)."}, {"title": "Improving the Computational Complexity", "content": "In the previous section, we demonstrated that Alg. 2 is guaranteed to converge at the linear rate $O(1/t)$, making it\nhighly efficient. In this section, we propose an alternative version of Alg. 2, designed to enhance the efficiency of each\nstep t while maintaining full equivalence in functionality. This ensures that the convergence analysis from the previous\nsection still remains valid. Firstly, a naive implementation of Alg. 2 has a complexity of $O(Tk^2n^2)$, as each iteration\nrequires $O(k^2n^2)$ to compute the full objective in Eq. 9 for every candidate cluster in order to determine the best cluster\nfor the current object i. Since the number of iterations T until convergence is typically larger than n, this approach can\nbecome computationally expensive.\nPart (b) of Theorem 2 offers an alternative: instead of evaluating the full objective, we can compute the gradient\n$G_{i,:}$, which involves only terms related to object i. Let $M_{i,m} = 2\\sum_{j \\in S_m} A_{i,j}, N_i \\triangleq \\sum_{m \\in [k]} M_{i,m}$ and $\u03b2_{im} = 2\u03b2|S_m| - 2\u03b21[i\u2208Sm]$. Given this, we present the following theorem.\nTheorem 4. Let S[k] be the current clustering of our local search procedure, with neutral objects $S_0 = V \\setminus \\bigcup_{m \\in [k]} S_m$.\nThe gradient can then be expressed as follows.\n$G_{i,m} = -\u03b2 + (1+\u03b1)M_{i,m} \u2212 \u03b2_{im} \u2212 \u03b1N_i$\n(13)\nfor all $m \\in [k]$ and $G_{i,0} = 0$.\nA naive calculation of the full gradient $G_{i,:}$ for block i is O(k2n). However, the particular form of the gradient presented\nin Eq. 13 makes it O(kn). See the proof of Theorem 4 for further insight on this. From Theorem 2, the gradient $G_{i,m}$\nrepresents the impact on the full objective in Eq. 9 if object i is placed in cluster m. Thus, because $G_{i,0} = 0$, we\nobserve that an object i is made neutral if its contribution to all non-neutral clusters is currently negative. Moreover,"}, {"title": "Impact of a and B", "content": "We now analyze the impact of a and \u03b2 in Eq. 7. We begin by stating the following proposition.\nProposition 4. (a) There exists a $\u00a7_1 < 0$ such that for any $\u03b2 < \u00a7_1$, there is a clustering solution maximizing Eq. 7\nwhere all the objects are assigned to a single non-neutral cluster. (b) Conversely, there exists a $\u00a7_2 > 0$ such that for any\n$\u03b2 \\geq \u00a7_2$, there is a clustering solution maximizing Eq. 7 where all the objects are neutral.\nFrom Proposition 4, we understand the extreme cases of \u03b2: (a) a small negative \u03b2 results in a maximally imbalanced\nnon-neutral clustering (i.e., all objects in one non-neutral cluster), while (b) a large positive \u03b2 makes all objects neutral.\nFor intermediate $\u03b2 \\in [\\S1, \\S2]$, we analyze the gradient in Eq. 13. Increasing \u03b2 strictly reduces the contribution of object\ni to each cluster m \u2208 [k], but since the term -2\u03b2|Sm| scales with cluster size, larger clusters become less favorable,\npromoting balance. If \u03b2 is large enough, it forces $G_{i,m} < 0$ for all $m \\in [k]$, making neutrality optimal for object i.\nNote that this is more likely for low-degree objects, implying that high-degree objects (with clear cluster assignment)\nare more likely to remain non-neutral, resulting in dense non-neutral clusters. Consequently, increasing \u03b2 leads to\nsmaller (i.e., more neutral objects) and denser non-neutral clusters, while maintaining balanced, as desired.\nThe parameter a has been studied in prior work (Chu et al., 2016; Tzeng et al., 2020). From Eq. 7, a balances\nmaximizing intra-similarities and minimizing inter-similarities, which translates to a trade-off between cohesion within\nclusters and separation between them. A heuristic choice of a = 1/(k \u2013 1) was proposed in (Tzeng et al., 2020), based\non the observation that the number of intra-similarities scale linearly with k, while the number of inter-similarities\ngrow quadratically. This choice prevents inter-similarities from dominating the objective. Finally, the term -\u03b1N\nindicates that a influences whether object i becomes neutral, underscoring the need to account for inter-similarities in\nthe objective (as suggested in Section 2.3)."}, {"title": "Conclusion", "content": "We proposed a novel formulation for the polarized community discovery problem, emphasizing (i) large, (ii) dense,\nand (iii) balanced clustering solutions. We developed an efficient and effective local search method and established its\nconnection to block-coordinate Frank-Wolfe optimization, proving a linear convergence rate of O(1/t). Our extensive\nexperimental results demonstrate that our method achieves high-polarity clustering solutions while maintaining balance,\nand outperforms the other methods."}, {"title": "Proofs", "content": "Proposition 1. Problem 1 is equivalent to finding a clustering S[k] that maximizes any one of the four objectives below.\n$N_{intra}^+ + N_{inter}^-$\n(2)\n$-N_{intra}^- - N_{inter}^+$\n(3)\n$N_{intra}^+ - N_{intra}^-$\n(4)\n$-N_{inter}^+ + N_{inter}^-$\n(5)\nFurthermore, maximizing any other combination of the four terms is not equivalent to Problem 1.\nProof. We begin by defining the following quantities, which are constants w.r.t. different clustering solutions for the\nk-CC problem.\n$\\mathcal{C}_{sim} \\triangleq \\sum_{(i,j) \\in E} A_{i,j}$\n(14)\n$\\mathcal{C}_{abs} \\triangleq \\sum_{(i,j) \\in E} |A_{i,j}|$\n(15)\nThe five objectives can be written as follows.\n$f^{full} \\triangleq N_{intra}^+ - N_{intra}^- + N_{inter}^+ - N_{inter}^- = \\sum_{m \\in [k]} \\sum_{i,j \\in S_m} A_{i,j}^+ - \\sum_{m \\in [k]} \\sum_{i,j \\in S_m} A_{i,j}^- + \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} A_{i,j}^+ - \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} A_{i,j}^-$\n$f^{MaxAgree} \\triangleq N_{intra}^+ + N_{inter}^- = \\sum_{m \\in [k]} \\sum_{i,j \\in S_m} A_{i,j}^+ + \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} A_{i,j}^-$\n$\\frac{1}{2} \\sum_{m \\in [k]} \\sum_{i,j \\in S_m} (A_{i,j}^+ + A_{i,j}^-) - \\frac{1}{2} \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} (|A_{i,j}| - A_{i,j})$\n$f^{MinDisagree} \\triangleq N_{intra}^- + N_{inter}^+ = \\sum_{m \\in [k]} \\sum_{i,j \\in S_m} A_{i,j}^- + \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} A_{i,j}^+$\n$\\frac{1}{2} \\sum_{m \\in [k]} \\sum_{i,j \\in S_m} (|A_{i,j}| - A_{i,j}) + \\frac{1}{2} \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} (|A_{i,j}| + A_{i,j})$\n$f^{MaxCorr} \\triangleq N_{intra}^+ - N_{intra}^- = \\sum_{m \\in [k]} \\sum_{i,j \\in S_m} A_{i,j}$$\n$f^{MinCut} \\triangleq -N_{inter}^+ + N_{inter}^- = \\sum_{m \\in [k]} \\sum_{p \\in [k]\\{m\\}} \\sum_{i \\in S_m} \\sum_{j \\in S_p} A_{i,j}$\n(16)\nGiven this, we observe the following connection between the objectives.\n\u2074By equivalent, we mean they share all local maxima, including the global maximum."}]}