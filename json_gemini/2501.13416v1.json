{"title": "M3PT: A TRANSFORMER FOR MULTIMODAL, MULTI-PARTY SOCIAL SIGNAL PREDICTION WITH PERSON-AWARE BLOCKWISE ATTENTION", "authors": ["Yiming Tang", "Abrar Anwar", "Jesse Thomason"], "abstract": "Understanding social signals in multi-party conversations is important for human-robot interaction and artificial social intelligence. Multi-party interactions include social signals like body pose, head pose, speech, and context-specific activities like acquiring and taking bites of food when dining. Incorporating all the multimodal signals in a multi-party interaction is difficult, and past work tends to build task-specific models for predicting social signals. In this work, we address the challenge of predicting multimodal social signals in multi-party settings in a single model. We introduce M3PT, a causal transformer architecture with modality and temporal blockwise attention masking which allows for the simultaneous processing of multiple social cues across multiple participants and their temporal interactions. This approach better captures social dynamics over time by considering longer horizons of social signals between individuals. We train and evaluate our unified model on the Human-Human Commensality Dataset (HHCD), and demonstrate that using multiple modalities improves bite timing and speaking status prediction.", "sections": [{"title": "INTRODUCTION", "content": "Human behavior in social interactions is shaped by a continuous dance of multimodal signals. It requires the simultaneous processing of multiple cues such as body pose, head orientation, gaze, and speech. In shared social settings like dining, where individuals engage in simultaneous verbal and non-verbal communication, these signals do not exist in isolation. Instead, each person's behavior is intertwined with the actions of others. A person's gaze may shift in response to a conversational partner's body language, or they may pause mid-sentence as another prepares to take a bite of food. This capability of understanding social settings is important for social robotics or robot-assisted feeding, where robots can engage in social behaviors similar to that of humans. Models of social behavior must be able to learn from diverse social signals while accounting for inherent temporal dependencies.\nIn past work, typically a specific model is trained for each social signal that one wants to predict. In this work, we propose a causal transformer model M3PT (Multi-Modal, Multi-Party Transformer), which leverages modality-specific and time-based blockwise attention masking so that a single model can predict and leverage multiple features. This architecture is capable of leveraging information from multiple modalities such as body pose, head orientation, gaze direction, speech, and others while also attending to some length of past history. By processing multimodal features and incorporating interaction history, the model can learn the choreography of social signals and predict an individual's behaviors.\nWe leverage M3PT on the Human-Human Commensality (HHCD) dataset, which contains rich multimodal interactions in triadic dining settings. Though the dataset was built for predicting understanding social signals for predicting when someone takes a bite of food, which is called bite timing, the dataset contains various multimodal signals, such as speech, body pose,\ngaze, food interactions, and more. We restructure this dataset for the task of multimodal social signal prediction across these social signals. As part of this work, we will release the restructured dataset to encourage further research in this area.\nOur key contributions include:\n\u2022 the introduction of a causal transformer architecture with modality-specific and temporal attention masking to predict social signals in group dining settings;\n\u2022 demonstrating M3PT's ability to capture the connected nature of social interactions by predicting an individual's behavior based on the multimodal cues of others in their group;\n\u2022 results indicating the importance of including multiple modalities to predict social signals;\n\u2022 and an ablation study on the role of larger temporal contexts and temporal chunking in predicting social signals"}, {"title": "RELATED WORK", "content": "Past work has typically built single-task social behavior prediction tasks. In this work, we focus on using a single model to learn a social signal prediction task.\nSocial Behavior Prediction. Social behaviors between participants in a multi-party setting are often synchronized. Predicting social behaviors often involves understanding not only the movements of individuals but also their social cues, such as gaze, gestures, and speech. Past work on nonverbal signal generation for a single person has focused on predicting signals such as gesture purely from text, laughter, affect. Research on learning from groups typically focuses on learning individual features such as facial features, head pose, body pose, hands, next speaker prediction, end-of-turn prediction, or bite timing from other interactants. Each of these social features are predicted using tailored models to process and predict these social signals.\nApplications. These kinds of social behavior prediction models can then have various applications. They have been used to build social robots for teaching language, robots that backchannel and nod during conversations, autism therapy, multiparty group conversations with a robot, and robot-assisted feeding in social settings. Many of these systems focus on leveraging only a single social signal prediction; however, it would be inefficient to predict a large number of social signals if a new model was required for each one. In this work, we focus on using a single transformer-based model to predict multi-party, multimodal social signals."}, {"title": "MULTIMODAL SOCIAL SIGNAL PREDICTION TASK", "content": "The goal of a social prediction task is to predict the social behaviors of a target person by using social cues from their interlocutors. Formally, at timestep t, an individual i gives off signals Xi(t). Xi(t) contains all the social signals a person gives off such as gaze, body gesture, and speech. In a scenario with n interactants, the social signals of person i are dependent on those from other interactants j, j \u2260 i from the current and past timesteps in addition to social signals of person i from previous timesteps. Thus the goal of social signal prediction is to learn a function F that is able to predict the social signals of an individual i at timestep t:\nXi(t) = F(Xi(0 : t \u2212 1), {Xj(0 : t), \u2200j \u2260 i}).\nThen, the function F is able to predict social signals of arbitrary participants. We consider each timestep t to be an interval of length c as opposed to an absolute point of time. For example, a social signal such as gaze at timestep t is represented as a c-second gaze signal segment. Then, each of these segments can be downsampled to any framerate. We note that there is a distinction between social signal prediction and social signal forecasting, as the latter involves predicting signals in some time frame in the future, while prediction involves predicting concurrent signals. In this work, we limit our scope to signal prediction, which includes conditioning on concurrent features from other interactants."}, {"title": "M3PT (MULTI-MODAL, MULTI-PARTY TRANSFORMER)", "content": "We introduce M3PT (Multi-Modal, Multi-Party Transformer), a model designed for multi-party social signal prediction. Unlike previous work that focuses on predicting a single social signal with a small time horizon or a single social signal, M3PT simultaneously considers and predicts multiple social signals.\nCurrently, our model is designed to handle discrete social signal prediction tasks, but can handle a wide array of multimodal social signals, such as gaze, head pose, pose, and speech transcriptions. We handle the diverse number of inputs by tokenizing each modality input from a person using a vector-quantized autoencoder. Then, to learn the temporal relationship of these social signals across multiple interlocutors, we introduce a person-aware and modality-aware blockwise attention masking approach."}, {"title": "LEARNING MODALITY-SPECIFIC QUANTIZED CODEBOOKS", "content": "As we are operating on c-second time segments, it is difficult to learn features for continuous features. Thus, we tokenize continuous inputs with a Vector Quantized Variational Autoencoder (VQ-VAE). As signals like gaze, headpose, or pose can be represented as keypoints, we use a 1D CNN-based encoder and decoder to learn quantized keypoint representations. We pretrain these modality-specific VQ-VAEs on the training data and freeze them during the training of the transformer.\nTo train the VQ-VAE, we take a c-second segment and encode each individual keyframe with a 1D-CNN. If the segment contains m frames, we then have m embeddings to represent the segment. Each keyframe's embedding is then quantized by selecting from a codebook, which provides a discrete representation of the social signal. Given these m quantized embeddings, we aggregate them into a single embedding z with a linear projection step. During the decoding step, we up-project z back into m temporal embeddings, from which we use the codebook to re-select m quantized embeddings.\nWe train the VQ-VAE with a selection loss that encourages the selection of the right embedding and a commitment loss:\nLselect = ||Sg[ze(x)] \u2013 e||2 + \u03b2 ||ze(x) \u2013 sg[e]||2,"}, {"title": "TRANSFORMER ARCHITECTURE", "content": "In a multi-party setting, each participant produces various social signals at each timestep. We use a VQ-VAE to compute discretized tokens for each c-second segment in time. The encoded representations of all social signals from all individuals are proccessed by a causal transformer designed to capture relationships between individuals and modalities over time. For a modality k for person i over time zik,  ,we add a cyclic positional encoding over time, an embedding representing the person, and a modality-specific embedding.\nWe sequence the modalities over time per-person as shown in Figure 1, where each timestep block contains blocks for the modalities of each individual. However, this blockwise structure is not useful for the traditional lower triangular attention mask typically used in causal transformers. Typically, a lower triangular causal attention allows for a model to leverage past information to predict a given token; however, we want to predict the features of a person i based on features from previous timesteps and features from other participants in the current timestep. Due to the structure of our transformer, this structure would not work for our task. Thus, we introduce multi-party, multimodal blockwise causal masking along with relevant changes to the transformer architecture.\nBlockwise Attention Masking. To enforce temporal structure and modality-specific interactions, we design a blockwise causal masking strategy. This mask consists of a lower triangular matrix that restricts attention to only past and present time steps, preserving the autoregressive nature of the model. Additionally, we modify the mask by creating small blocks along the diagonal, where signals from each individual can attend to both their own previous signals and those of others in the group as shown in Fig.2. This design ensures that the model captures not only each person's behavior but also the reciprocal influence of others' social signals, effectively modeling the intertwined dynamics of group interactions.\nRight-shifted Residual Connection. In bidirectional encoder architectures like BERT, mask tokens prevents residual connections from leaking information to masked positions. Similarly, in unidirectional decoder architectures like GPT, predicting the next word inherently avoids data leakage through residual connections. However, in our case, the blockwise attention mechanism prevents the direct application of conventional residual connection strategies. Simply removing residual connections makes the model harder to train due to the lack of signal propagation.\nTo address this difficulty, we use a right-shifted residual connection. Instead of directly adding residual features to the hidden states, we right-shift all features by one segment before adding them. This shift ensures that each position receives residual information only from the preceding segment, preserving the integrity of the blockwise attention masking while facilitating effective learning."}, {"title": "EVALUATION ON HHCD DATASET", "content": "To evaluate M3PT, we use the Human-Human Commensality Dataset (HHCD), which contains triadic interactions between three participants without mobility limitations during shared meals. For the task of robot-assisted feeding, a robot must feed a person with mobility limitations, however, a key challenge is when to feed a person if they are talking to others. The purpose of the dataset is to predict bite timing events in social settings, which can be used to build socially-aware robot-assisted feeding systems. The dataset contains many kinds of annotations such as multimodal social data like gaze, body pose, speech, as well as bite events, drink interactions, and utensil usage. In this work, we repurpose this dataset towards our task of multi-party, multimodal social signal prediction. In this dataset, we process gaze, headpose, body pose, transcripted speech,\nspeaking status, and bite timing. Each of these features is used as input to M3PT, and we focus on improving the prediction of the two binary social signals: speaking status and bite timing."}, {"title": "FEATURE PREPROCESSING", "content": "HHCD contains 30 unique sessions of triadic social dining. For each session, we generate 36-second sequences by sampling every 18 seconds. We then split this 36-second sequence into 12 3-second segments. Thus each token in M3PT represents a 3-second chunk of time.\nTo extract relevant features from the HHCD videos, we used a series of preprocessing steps per-modality. We isolated speech segments using voice activity detection and extracting direction from a microphone array. To compute transcribed speech, we utilize OpenAI Whisper to decode speech for each person and compute a word embedding using BERT. To extract body pose, we use OpenPose to track body keypoints of each participant. Pose feature is smoothed to be more contiguous through interpolation. We use RT-GENE on each person's video to extract head pose and gaze direction estimates.\nM3PT performs binary classification tasks for both speaking status and biting time over each segment. Since each segment is 3-seconds long, we classify a segment as \u201cspeaking\" if more than 30% of the frames indicate speaking, and as \u201cbiting\" if at least one frame indicates biting."}, {"title": "IMPLEMENTATION", "content": "VQ-VAE We use a 1D CNN with a window size of 3 and a stride of 1. The codebook contains 512 embeddings, each with a dimension of 32. The commitment loss uses a \u03b2 value of 0.25, and the hidden size of the linear projector is set to 1024. The model is trained with a learning rate of 5e-4, weight decay of le-6, and optimized using a cosine annealing warm restart strategy.\nTransformer Our model has 12 attention layers with 16 attention heads and an attention dropout rate of 0.1. The embedding dimension is 1024. We use a learning rate of 3e-5, weight decay of le-6, and a cosine annealing warm restart optimizer."}, {"title": "RESULTS", "content": "In this section, we present the test set performance of our model and compare it with the previous state-of-the-art models. Additionally, we report the average performance and standard deviation of our model and various ablations on the validation set, calculated over 3 folds."}, {"title": "MULTIMODALITY IMPROVES SOCIAL SIGNAL PREDICTION PERFORMANCE", "content": "As shown in Tables 1 and 2, using all the modalities improves performance, both for speaking status and bite timing. For bite timing prediction, we find large distinctions across features. We find that all metrics, especially F1 and nMCC are reduced when gaze, words, or speaking status are removed. This indicates that these are the most important features for predicting bite timing.\nWe find similar results indicating that multiple modalities improves speaking status prediction. We find that using no other signals except for speaking status, the M3PT produces nearly random predictions, but using all modalities improves the performance the most. The largest performance degradation occurs when bite timing is removed, indicating that it is an important feature for speaking status prediction. Other features have slight performance losses, however, they are not as strong of a drop-off compared to bite timing."}, {"title": "LARGER TEMPORAL CONTEXT IMPROVES BITE TIMING.", "content": "Table 3 presents an ablation that explores how varying the total time length, while maintaining a constant segment size, affects model performance for speaking status (S) and bite timing (B) tasks. The results show that increasing the sliding window size improves performance for bite timing, with accuracy, precision, and nMCC increasing as the window size grows. For instance, the F1 score for bite timing rises from 0.89 to 0.95 as the window size increases from 6 to 36 seconds, indicating that the temporal contexts aid in more accurate predictions. However, we find this is not true for all modalities, as for speaking status prediction, the performance remains stable or slightly decreases in precision as the window size increases. This may be due to speaking status being a noisier task as it has a lot of high-frequency changes as opposed to bite timing."}, {"title": "LARGE SEGMENT LENGTHS LEADS TO MODE COLLAPSE.", "content": "Table 4 shows that increasing the segment length results in significant mode collapse. As the segment length increases, both speaking status prediction and bite timing prediction drastically fall in performance across all metrics. This is consistent with mode collapse in the tokenization, as the VQ-VAE step must represent more of the temporal information as opposed to the transformer. Our use of 12 segments with a length of 3 seconds is a good balance of having the transformer represent the temporal and multi-party information, while the vector quantized encoders represent only the modality information."}, {"title": "CONCLUSION AND LIMITATIONS", "content": "In this work, we presented M3PT, which demonstrates the importance of modeling multiple modalities in multi-party social signal prediction. We find that it is important to model multiple signals in one model. We investigated our design decision for 3-second temporal segments, and found that the larger segments cause the transformer to learn less of the temporal structure. We also found that an increased temporal context into the transformer can help improve performance in bite timing. M3PT is a first step in building models for multi-party multimodal social signals.\nWe focused only on predicting discrete social signals; however, M3PT is not restricted towards only discrete outputs. In preliminary experiments, we found that predicting continuous signals, namely body pose, was difficult. The reconstructions were often poor as transformer models are often better at predicting discretized inputs; however, when we applied vector quantization from the VQ-VAE struggled to reconstruct these social signals. We hope to explore this direction in future work."}]}