{"title": "VaeDiff-DocRE: End-to-end Data Augmentation Framework\nfor Document-level Relation Extraction", "authors": ["Khai Phan Tran", "Wen Hua", "Xue Li"], "abstract": "Document-level Relation Extraction (DocRE)\naims to identify relationships between entity\npairs within a document. However, most\nexisting methods assume a uniform label\ndistribution, resulting in suboptimal perfor-\nmance on real-world, imbalanced datasets.\nTo tackle this challenge, we propose a novel\ndata augmentation approach using generative\nmodels to enhance data from the embedding\nspace. Our method leverages the Variational\nAutoencoder (VAE) architecture to capture\nall relation-wise distributions formed by\nentity pair representations and augment data\nfor underrepresented relations. To better\ncapture the multi-label nature of DocRE, we\nparameterize the VAE's latent space with a\nDiffusion Model. Additionally, we introduce\na hierarchical training framework to integrate\nthe proposed VAE-based augmentation module\ninto DocRE systems. Experiments on two\nbenchmark datasets demonstrate that our\nmethod outperforms state-of-the-art models,\neffectively addressing the long-tail distribu-\ntion problem in DocRE. Our code is released at:\nhttps://github.com/khaitran22/VaeDiff\n-DocRE", "sections": [{"title": "1 Introduction", "content": "The task of relation extraction (RE) involves iden-\ntifying possible relationships between two entities\nwithin a given context. This is a fundamental task\nin the field of Information Extraction, as it serves\nas a foundation for other tasks such as Question\nAnswering (Xu et al., 2016). Early approaches to\nRE focused on sentence-level settings, where the\ncontext is limited to a single sentence. However,\nthis approach is often impractical, as many rela-\ntionships can only be inferred from information\nspread across multiple sentences. For instance, at\nleast 40.7% of relational facts on Wikipedia can\nonly be identified from multiple sentences (Yao\net al., 2019). As a result, there has been growing"}, {"title": "2 Methodology", "content": "In this section, we introduce our VaeDiff-DocRE\nframework, a novel data augmentation approach at\nthe embedding space to assist the DocRE task. Sec-\ntion 2.2 introduces the data augmentation module\nVaeDiff, which comprises a VAE architecture to\nmodel relation-wise distributions in the embedding\nspace and a DPM to parameterize the prior distri-\nbution of the VAE, specifically designed for multi-\nlabel settings. Section 2.3 presents VaeDiff-DocRE,\nan end-to-end training framework that learns the\nrelation-wise distributions, trains VaeDiff, and inte-\ngrates it to enhance the performance of the DocRE\nmodel."}, {"title": "2.1 Problem Formulation", "content": "Let D = {w}}=\u2081 be a document containing L\nwords and a set of entities Ep = {e}ED. Each\nentity ei is associated with a set of mentions\nMei = {m} Me (i.e., a set of phrases refer-\njj=1\nring to the same entity ei). In DocRE task, we\npredict the subset of relations in a predefined set\nR={rk}R that hold between each pair of entities\n(eh, et)h,t=1,...,|ED|,h\u2260t. We sometimes abbreviate\nan entity pair (eh, et) as (h, t) to simplify notation.\nA relation is deemed to exist between the head en-\ntity en and tail entity et if it is expressed between\nany of their corresponding mentions. If no relation\nexists between any pair of their mentions, the entity\npair is labeled NA. For each entity pair, we term a\nrelation that holds between its constituent entities\nas positive, and the remaining relations in Ras\nnegative. An entity pair that is NA does not have\nany positive relation, and has the entire set R as\nnegative relations (we could consider such a pair\nas having a special NA relation between them)."}, {"title": "2.2 Data Augmentation Module - VaeDiff", "content": "The proposed VaeDiff module consists of a VAE\narchitecture, called the Entity Pair VAE (EP-VAE),\ndesigned to learn relation-wise distributions with\na compact latent space. We then parameterize the\nlatent space in EP-VAE using a diffusion model,\nreferred to as the Diffusion Prior, specifically tai-\nlored for multi-label settings in DocRE. During\ninference, the Diffusion Prior generates relation-\nspecific latent variables, which are subsequently\ndecoded by the EP-VAE to generate new entity pair\nrepresentations corresponding to the target relation."}, {"title": "2.2.1 Entity Pair VAE", "content": "The Entity Pair VAE, referred to as EP-VAE, con-\nsists of an encoder and a decoder fo(), parameter-\nized by 0. The encoder includes two components:\n(1) a projector f\u00f8(\u00b7) that maps the input to the EP-\nVAE latent space, and (2) separate projectors for\nthe mean (\u00b5\u03c6) and log variance (04), which esti-\nmate the parameters of the posterior distribution in\nthe latent space. Both the encoder projector and the\ndecoder share an identical architecture comprising\nalternating feedforward layers and LeakyReLU ac-\ntivation functions, with a single BatchNorm layer\n(Ioffe and Szegedy, 2015) incorporated mid-stack.\nThe mean (\u03bc\u03c6) and log variance (64) projectors\nare implemented as feed-forward networks. The\ndetailed structure of EP-VAE is depicted in Figure\n3.\nLet P = (P1, P2, ..., pn) represent the set of\ntraining entity pair representations, and let zp de-\nnote the latent vectors. The projector f\u00f8(\u00b7) first\nmaps the input p to a latent space representation\np' of the EP-VAE framework. Subsequently, the\nmean (\u00b5\u03c6) and log variance (\u03c3\u03c6) projectors pro-\nduce the parameters \u03bcp and log(\u03c3p), which define\nthe posterior distribution q$(zp|p):\np' = f(p)\n\u03bc\u03c1 = \u03bc\u03c6(p')\nlog(\u03c3p) = \u03c3\u03c6(p') (1)\nwhere up and op are parameters of multivariate\nGaussian distribution. To enable back-propagation,\nwe use reparameterization trick proposed by"}, {"title": "2.2.2 Diffusion Prior", "content": "In the original VAE formulation, the prior distribu-\ntion p(zp) is assumed to follow a standard normal\ndistribution, which limits the latent space's ability\nto model the context in a multi-label scenario. This\ncan reduce the effectiveness of the generated entity\npair representations for target relations. To address\nthis, we introduce a Diffusion Prior by parameteriz-\ning p(zp) with a DPM, enhancing the complexity\nof the latent space in the EP-VAE. This new prior is\ndenoted as py (zp), where & represents the model's\nparameters. Additional background on DPM can\nbe found in Appendix A.2.\nArchitecture. Our denoising network \u017cy(\u00b7) is a\npre-LayerNorm transformer (Xiong et al., 2020).\nEach layer is conditioned on the timestep t by in-\ncorporating the Transformer sinusoidal position\nembedding (Vaswani et al., 2017), followed by an\nMLP with a single hidden layer.\nObjective Function. Unlike the approach pro-\nposed by Ho et al. (2020), we discovered that train-\ning our denoising network \u017cy(zt, t) to directly pre-\ndict the original latent vectors \u017eo from its arbitrarily\nnoisified version z\u0142 yields better results. Thus, we\ntrain our Diffusion Prior with the objective function\nas below:\nLDiff = ||\u017cy (Zt, t) - zo||2 (5)"}, {"title": "2.2.3 Training Objective", "content": "To co-train Diffusion Prior and EP-VAE, we decom-\npose the Kullback-Leibler divergence term DKL\nin Eq. (4) into its negative encoder entropy and\ncross-entropy. Specifically, the decomposition is as\nfollows:\nDKL [94(Zp|P)||Py(Zp)] = Eqq(zp|p) [log(94(Zp|P))]\nnegative encoder entropy\n+ Eq\u03c6(zp|p) [-log(py(Zp))]\ncross-entropy (8)\nFollowing Wehenkel and Louppe (2021), the cross-\nentropy term can further be replaced by the regres-\nsion objective LDiff of the diffusion model in Eq."}, {"title": "2.3 Hierarchical Training Data Augmentation\nFramework - VaeDiff-DocRE", "content": "In this section, we introduce VaeDiff-DocRE, an\nend-to-end hierarchical three-stage training data\naugmentation framework designed to improve the\nperformance of the DocRE system under long-tail\ndistribution settings. The overall framework is de-\npicted in Figure 4."}, {"title": "2.3.1 Stage 1: Learn Relation-wise\nDistribution", "content": "To obtain the structured embedding space depicted\nin Figure 2, we train a simple baseline DocRE\nmodel, which initially performs lower than the com-\npared baseline methods. The primary goal of this\nbaseline is to mitigate the negative impact of the\nPN imbalance, which is not the main focus of this\nwork, and to demonstrate the effectiveness of our\nproposed data augmentation approach. This base-\nline DocRE model can be replaced by any more\nadvanced DocRE model to achieve better results.\nFor entity pair representation learning, we adopt the\nstructure of KD-DocRE (Tan et al., 2022a) as the\nbackbone for our baseline model. Regarding the\nobjective function, we replace the Adaptive Focal\nLoss used in KD-DocRE with PMTEM loss (Guo\net al., 2023) and Supervised Contrastive Learning\nloss (Khosla et al., 2020). The detailed structure of\nthis baseline model is provided in Appendix A.3."}, {"title": "2.3.2 Stage 2: Train VaeDiff Module", "content": "Following the training of the baseline DocRE\nmodel in stage 1, we proceed to train the VaeDiff\naugmentation module as described in Section 2.2.\nThe training inputs for the VaeDiff module are gen-\nerated by utilizing the document encoder from the\ntrained DocRE baseline to extract entity pair rep-\nresentations within a document. Initially, the EP-"}, {"title": "2.3.3 Stage 3: Retrain baseline DocRE with\nVaeDiff", "content": "In the final stage, we retrain the baseline DocRE\nmodel using the trained VaeDiff augmentation\nmodel to address the PP imbalance in the DocRE\ntask. The detailed algorithm of stage 3 is provided\nin Algorithm 1.\nFollowing the warmup approach in stage 2, we\ninitially train the baseline DocRE model for the\nfirst n epochs without augmenting any new entity\nrepresentations. After training for initial n epochs,\nwe then train the DocRE model using both the fea-\ntures x from its document encoder and the pseudo-\nfeatures x generated by the VaeDiff model.\nTo generate x, assume there are |Ep| positive en-\ntity pairs in the current batch B. We select the label\nvectors le, of these positive entity pairs. The VaeD-\niff model then generates m \u00d7 |Ep| additional entity\npair representations using l\u025b\u201e as class conditional\ninformation. The DocRE model makes predictions\nbased on the combined set X = {x, x}, and these\npredictions are concatenated to compute the loss\nfor training the DocRE model."}, {"title": "3 Experiments", "content": "3.1 Experimental Setup\nDataset. We evaluated our proposed method us-\ning two benchmark datasets: Re-DocRED (Tan\net al., 2022b) and DWIE (Zaporojets et al.,\n2021). Re-DocRED is a high-quality, revised\nversion of DocRED (Yao et al., 2019) a large-\nscale document-level RE dataset constructed from\nWikipedia and Wikidata, which supplements pre-\nviously missed false negative triplets. DWIE is a\ndocument-level information extraction dataset that\nincludes relation extraction. The statistics for these\ntwo datasets are provided in Table 3.\nMetrics. Following Yao et al. (2019), we use mi-\ncro F1 (F1) and micro Ignore F1 (Ign F1) as evalu-\nation metrics to assess overall DocRE performance.\nIgn F1 measures the F1 score while excluding rela-\ntional triplets shared between the training and test\nsets. Additionally, we use two F1 scores specific\nto long-tailed distribution settings proposed by Tan\net al. (2022b): Freq. F1 for common relations"}, {"title": "3.2 Baseline", "content": "Re-DocRED For the Re-DocRED dataset, we\ncompared our model with the following baselines\nin addressing PP imbalance: (1) KD-DocRE (Tan\net al., 2022a) introduces a loss function based on\nFocal Loss (Lin et al., 2017), specifically tailored\nfor the DocRE task under long-tail distribution set-\ntings. (2) PEMSCL (Guo et al., 2023) proposes a\nmodified supervised contrastive loss function to ad-\ndress similar challenges in the task. (3) CAST (Tan\net al., 2023) combines self-training with a pseudo-\nlabel re-sampling method to improve performance\non infrequent relations.\nDWIE For the DWIE dataset, we compared our\nmethod with the state-of-the-art (SOTA) method\nin address the PP imbalance \u2013 Correl (Han et al.,\n2024). It introduces a module designed to learn the\nrelational correlation by leveraging the semantic\ndistance between common and uncommon rela-\ntions to enhance performance on the latter."}, {"title": "3.3 Main Results", "content": "We present the performance of our proposed\nVaeDiff-DocRE method for the DocRE task, bench-\nmarking it against baseline methods in Table 1 and\nTable 2. To validate the significance of our improve-\nments, we performed significance tests following\nthe methodology outlined by Dror et al. (2018),\nwith all p-values being less than 0.05. These ta-\nbles illustrate that our VaeDiff-DocRE framework\nsurpasses state-of-the-art methods, particularly in\nboth overall and long-tail performance measured\nby the F1 score.\nIn the Re-DocRED dataset, as presented in Table\n1, the VaeDiff-DocRE approach demonstrated su-\nperior performance compared to baseline methods,\nachieving an average F1 score improvement of 0.58\nand 0.68 on the development set and 0.19 and 0.75\non the test set, when utilizing BERT and RoBERTa\nas document encoders, respectively. Furthermore,\nall PLM-based approaches, including ours, out-\nperformed LLM-based methods by a substantial\nmargin. Regarding the long-tailed distribution, par-\nticularly for minority relations, our method outper-\nformed the SOTA method by 0.54 and 0.42 points\nin F1 score on the test sets. While VaeDiff-DocRE\nachieved the second-highest performance for ma-\njority relations on the test set when using BERT,\nit achieved the best performance for majority re-\nlations with RoBERTa. Consequently, VaeDiff-\nDocRE secured the highest overall F1 score for\nboth PLMs. Similarly, in the DWIE dataset, as\nshown in Table 2, VaeDiff-DocRE surpassed the\nSOTA method by 1.6 and 2.27 points in F1 scores\non the development and test sets, respectively.\nThese results underscore the effectiveness of our\napproach in addressing the challenges posed by\nlong-tail distributions while maintaining strong per-\nformance for majority classes, thereby enhancing\noverall performance in the DocRE task."}, {"title": "3.4 Ablation Study", "content": "We conducted an ablation study on the Re-DocRED\ndataset to demonstrate the effectiveness of each\nproposed component, and the results are presented\nin Table 4.\nFirst, we removed the VaeDiff module, and in-\nstead added Gaussian noise to the representations\nof positive entity pairs in the current batch B, re-\nferred to as Gaus Noise. The results show that\nsimply adding Gaussian noise to the entity pair rep-\nresentations yields better performance compared\nto the KD-DocRE method. However, this basic\naugmentation technique performs worse than our\nmethod, further highlighting the effectiveness of\nthe proposed approach.\nSecond, we did not augment new entity pair rep-\nresentations during training, referred to as No Aug.\nThis model corresponds to the baseline DocRE\nmodel used in stage 1 (\u00a72.3.1). As shown in the\nresults, this led to a significant drop in performance,\nwith an overall F1 score decrease of 0.69. How-\never, when augmentation was introduced, perfor-\nmance improved for both majority and minority\nrelations. Notably, augmenting new data increased\nthe F1 score for minority relations by nearly 1%\nin the LTail F1 score, demonstrating the positive\nimpact of data augmentation in addressing the class-\nimbalance issue of the DocRE task."}, {"title": "3.5 Analysis of Generated Features", "content": "In Figure 5, we visualize the generated representa-\ntions of entity pairs expressing minority relations\nusing our VaeDiff approach. By comparing these\nwith the encoded features from the document en-\ncoder of the trained DocRE model on the develop-\nment set of the Re-DocRED dataset, we observe a\nstrong alignment between the generated pair rep-\nresentations and the encoded features. This align-\nment addresses the lack of high-quality data in\nminority classes by enriching the true entity pair\nrepresentations for these relations, effectively en-\nhancing the performance of DocRE models in long-"}, {"title": "4 Related Work", "content": "Document-level Relation Extracion (DocRE)\nEarly works in Relation Extraction (RE) focused\non sentence-level tasks, where context was con-\nfined to a single sentence. However, many rela-\ntional facts require synthesizing information across\nmultiple sentences. Since Yao et al. (2019) intro-\nduced DocRED, a large-scale document-level RE\ndataset, various methods have been developed for\nthe DocRE task. One approach leverages Graph\nNeural Networks (GNNs) to transform documents\ninto graph structures, as seen in the work of Nan\net al. (2020) and Zeng et al. (2020). Another ap-\nproach uses Transformer architectures, with Zhou\net al. (2021) and Tan et al. (2022a) improving entity\npair representation and reasoning through attention\nmechanisms.\nLong-tail Issue in DocRE Real-world DocRE\ndatasets often have a skewed distribution, where a\nfew relations dominate most instances. This creates\ntwo challenges: an imbalance between negative\nand positive instances, and an imbalance among\nthe positive instances themselves. The first issue"}, {"title": "5 Conclusion", "content": "In this paper, we present VaeDiff, a novel data aug-\nmentation framework aimed at addressing long-tail\ndistribution challenges in the DocRE task. Our ap-\nproach utilizes a Variational Autoencoder (VAE)\nto learn relation-wise distributions and a Diffusion\nProbabilistic Model to enhance the EP-VAE's la-\ntent space for multi-label scenarios. We also pro-\npose a hierarchical framework VaeDiff-DocRE to\nboost DocRE system performance under long-tail\nconditions by integrating the VaeDiff module. Ex-\ntensive experiments and ablation studies show that\nour framework outperforms SOTA methods."}, {"title": "Limitation", "content": "The proposed VaeDiff-DocRE framework requires\nmultiple training rounds and inherits the same\nlimitations as the original Diffusion Probabilistic\nModel framework. As a result, the GPU training\ntime for this framework is longer compared to other\nmodels. Additionally, while the framework has\nbeen tested in the general domain, its performance\nin more specialized domains, such as the medical\nfield, has yet to be evaluated."}, {"title": "A.1 Variational Autoencoder (VAE)", "content": "In variational autoencoder (VAE), a set of in-\nput data is denoted as X = (x1,...,xn) where n\ndenotes the number of total input samples. The\nlatent variables are denoted by vector z. The\nVAE includes a encoder network including network\nand variational parameters & that produces varia-\ntional probability model q(z|x) and a decoder\nnetwork parameterized by a to reconstruct sample\nx = fe(z). The objective of VAE is to maximize\nthe Evidence Lower Bound (ELBO) (Kingma and\nWelling, 2013) as follows:\nmax ELBO(\u03b8, \u03c6; x\u2081) (10)\n\u03b8,\u03c6\ni=1\nwhere:\nELBO(0, \u03c6; x) := Eq\u00a2(z|x) [log(pe(x|z))]\n\u2013 KL(qq(z|x)||p(z)) (11)\nThe first term in Eq. (11) corresponds to the\nnegative reconstruction loss between input x and"}, {"title": "A.2 Diffusion Probabilistic Model", "content": "Given an observation of data x0, diffusion mod-\nels (Ho et al., 2020) learn the data distribution\np(x0) by reversing a diffusion process. The dif-\nfusion (forward) process is a Markov chain that\ncorrupts the sampled data x0 by gradually adding\nrandom noise to it:\nq(xt|xt-1) = N(\u221a\u221a1 \u2013 \u1e9etxt\u22121, \u1e9et\u0399) (12)\nwhere \u1e9e1:T are the pre-defined noise variances,\n\u1e9et \u2208 (0,1) at time step t. When \u1e9et \u2192 T, the\ndata distribution will be corrupted to N (0, I). By\ndefining at = \u03a0=1(1 \u2013 \u03b2\u2081), we can directly get\nxt by adding noise to the input as follows:\nq(xt|x0) = N(\u221a\u03b1\u0390\u03c7\u03bf, (1 \u2013 at)I) (13)\nwhere at \u2208 (0, 1).\nGiven access to the original data xo, the back-\nward process can be inverted analytically:\np(xt-1|Xt, X0) = N(ft(xt, xo), \u03c3\u03b5\u0399) (14)\nwhere ot can be derived from \u1e9et, ft(xt, x0) has a\nclosed form parameterized by t (Ho et al., 2020).\nHowever, since the original data x0 is not available\nin the actual generation process, (i.e., the response\nis supposed to be generated), we can not directly\nuse Eq. (14) to sample data and thus approximate\nft() using a neural network with the parameter \u03c8,\nnamely denoising network.\nThe training of a denoising diffusion network is\ndefined by minimizing the Kullback-Leibler diver-\ngence DKL [q(x0:T) || Py(X0:T)] between the for-\nward trajectory q(x0:T) and the backward trajec-\ntory py(x0:T). This divergence term can be decom-\nposed as follows:\nDKL [9(X0:T)||P\u2084(X0:T)] = Exo~g [log q(x0)] -\nExo:T~9(15)\nThe second term in Eq. (15) represents the ex-\npectation of an evidence lower bound (ELBO) with\nrespect to x0 ~ q:\nELBO :=Exo:Tg log P(Xo:T) (16)"}, {"title": "A.3 Baseline DocRE", "content": "This section introduces the baseline DocRE model\nused in our framework introduced in Section 2.3.1."}, {"title": "A.3.1 Representation Learning", "content": "Encoding We insert a special token \u201c*\u201d before\nand behind each mention m\u00b2 of er in document\nD as an entity marker (Zhang et al., 2017). We\nuse a pre-trained language model (PLM) to ob\ntain token embeddings H \u2208 RL\u00d7d and token-level\ncross-attention matrix A \u2208 RLXL of D:\nH, A = PLM([W1, W2, ..., WL]) (18)\nThe representations of an entity e\u00a1 and an entity\npair (en, et) are computed as Eq. (19) and Eq. (20)\nrespectively.\nei (19)\nMei\nlog \u2211 exp (m)\ni=1\n(20)\nph,t = tanh (Wp [eh; et; ch,t] + bp)\nwhere m is embedding of starting token \u201c*\u201d of\neach mention m\u00b3 of ei; Wp, bp are trainable pa-\nrameters; [ ; ] is the concatenation; ch,t is the lo-\ncalized context embedding of (en, et) (Zhou et al.,\n2021).\nAxial Attention After obtaining the representa-\ntion of entity pairs, we incorporate Axial Attention\n(Ho et al., 2019) proposed in (Tan et al., 2022a),\nwhich allows us to integrate neighboring axial in-\nformation, thereby enhancing the performance of\nDocRE systems.\nGiven a set of entities ED = {ei}_1 in a docu-\nment D, we construct an n\u00d7n entity table, where\neach row and column corresponds to an entity in\nD. For any entity pair (en, et), the axial atten-\ntion mechanism attends to the pair's axial elements,\nwhich can be either (en, ei) or (ei, et). Since multi-\nhop reasoning is crucial in the DocRE task, if"}, {"title": "A.3.2 Objective Function", "content": "To enhance the robustness of the baseline DoCRE\nsystem against the significant PN imbalance issue,\nwe utilize Pairwise Moving-Threshold Loss com-\nbined with Entropy Minimization (Guo et al., 2023)\nas the primary objective, denoted as Lpmt.\nPairwise Moving-Threshold Loss with En-\ntropy Minimization (Chint) For each entity pair\n(en, et), the relation set R is divided into a pos-\nitive relation set Ph,t and a negative relation set\nNh,t. Given the prediction score fr for a relation\nr\u2208 R and the prediction score fn for the NA class\nof (en, et), the probability that the relation C of\n(eh, et), denoted as Prt(r), is r given that C is\neither r or the NA class is computed as follows:\nPrh,t(r) = exp(fr)/exp(fr) + exp(fn) (22)\nInversely, the probability of C is NA class is as:\nP(r)=1-Pht(r) = exp(fn)/exp(fr)+exp(fn) (23)"}, {"content": "Then, the pairwise moving-threshold loss \npairs (et, eh) = -log (\u03a0 Pre,Phot(h, et) Pre,Nhat(h, et)) (24)"}, {"content": "to improve the model , we compute the information entropy H(r) as: H(r) = -Pr(r) log Pr(r) - Pat(r) log Pt(r) (25)"}, {"content": "Finally, we get H as: H =  \u03a3 (h, et) (26), and combining this with other losses, we get L = \\xce\\xb31 L(phot) H (27)"}, {"content": "Supervised Contrastive Learning:We fur-ther incorporate the Supervised Contrastive Learning loss () as :Ch,et(xh,t) = -  log (28)", "tags": []}, {"title": "A.4 Derivation of Equation (9)", "content": "First, Eq. (11) is reformulated using Eq. (8):\nELBO(\u03b8, \u03c6, \u03c8; p) = Eqp(zp|p) [log(fe(p|Zp))]\n+ Eqo (zp|p) [-log(q4(zp|p))]\n+ Eq\u2084(zp/p) [log(py(Zp))] (30)\nwhere pry denotes the diffusion model, pe is the\nVAE decoder, and qf represents the VAE encoder.\nNext, the final term in Eq. (30) is expanded using\nEq. (16), yielding the following:\nEqo [log(fe (P/Zpo)) \u2013 log(q4(Zpo|P))+\nEq\u2084(zp/p) (31)\nwhere Zpo corresponds to the latent space represen-\ntation generated by the EP-VAE module.\nEqs. (31) and (32) establish a valid ELBO for-\nmulation. Maximizing Eq. (31) is equivalent to\nminimizing the VaeDiff module's objective func-\ntion. Additionally, maximizing the final term in Eq.\n(31) aligns with minimizing the Diffusion Prior's\nobjective function. By substituting the last term of\nEq. (31) with the Diffusion Prior loss, we derive\nEq. (9)."}]}