{"title": "EXPLORING EFFICIENT FOUNDATIONAL MULTI-MODAL MODELS FOR VIDEO SUMMARIZATION", "authors": ["Karan Samel", "Apoorva Beedu", "Nitish Sontakke", "Irfan Essa"], "abstract": "Foundational models are able to generate text outputs given prompt instructions and text, audio, or image inputs. Recently these models have been combined to perform tasks on video, such as video summarization. Such video foundation models perform pre-training by aligning outputs from each modality-specific model into the same embedding space. Then the embeddings from each model are used within a language model, which is fine-tuned on a desired instruction set. Aligning each modality during pre-training is computationally expensive and prevents rapid testing of different base modality models. During fine-tuning, evaluation is carried out within in-domain videos where it is hard to understand the generalizability and data efficiency of these methods. To alleviate these issues we propose a plug-and-play video language model. It directly uses the texts generated from each input modality into the language model, avoiding pre-training alignment overhead. Instead of fine-tuning we leverage few-shot instruction adaptation strategies. We compare the performance versus the computational costs for our plug-and-play style method and baseline tuning methods. Finally, we explore the generalizability of each method during domain shift and present insights on what data is useful when training data is limited. Through this analysis, we present practical insights on how to leverage multi-modal foundational models for effective results given realistic compute and data limitations.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning to model complex modality data such as documents, images, or videos has been historically done with domain-specific architectures suited to a particular modality and dataset (Vaswani et al., 2017; Liu et al., 2021; Sun et al., 2019). As new problems evolved that require bridging modalities, such as image or video question answering, multi-modal architectures have been used to learn joint representations between modalities for a specific end task (Tan & Bansal, 2019; Alayrac et al., 2022). Most recently large foundational models have emerged to improve domain-specific tasks with large-scale pre-training. Foundation models from multiple modalities have been combined and fine-tuned toward specific multi-modal tasks. Due to the large scale of these models and datasets, there remain fundamental questions about their computational requirements and generalizability once they are fine-tuned.\nComputation requirements Compute for multi-modal video language models are typically conducted in multiple stages (Lyu et al., 2023; Zhao et al., 2023). In the first stage, the embeddings produced from models of different modalities are aligned to generate a unified representation for the downstream large language models (LLMs). In videos, this involves aligning, image, audio, or text modalities to one another. However, with the frequency of new foundational model releases, it becomes impractical to realign all modalities when an improved foundational model is introduced.\nIn addition to modality alignment, there is the second stage fine-tuning of the LLM or individual modality models on the domain-specific task carried out. Such tuning is the de facto method to achieve state-of-the-art (SOTA) performance, while zero-shot methods are tested for generalizability. However these are binary choices, and understanding how training data and compute availability scale with performance is yet to be deeply explored in the multi-modal domain."}, {"title": "2 RELATED WORKS", "content": "2.1 VIDEO REPRESENTATION LEARNING\nFor video-level tasks, learning the shared representation of visual, text, and audio features has been key to improving downstream tasks. Recent BERT (Devlin et al., 2018) based masking methods like VideoBERT (Sun et al., 2019) jointly infer masked caption tokens as well as frame image tokens, shown in Figure 1 a). Similarly, BEVT (Wang et al., 2022) jointly predicts image tokens as well as changes within video frames. UniVL (Luo et al., 2020) aligns the video and caption embeddings and attempts to generate the caption, in addition to image and text masking objectives. Video language model (Xu et al., 2021) leverages the base BERT architecture to alternate between random and full masking of captions and frames while providing different self-attention masking strategies for downstream tasks. Merlot (Zellers et al., 2021) aligns captions and frames, learns temporal orderings between frames, and recovers masked captions. This is extended in Merlot Reserve (Zellers et al., 2022) where audio is jointly input with images and captions into a Transformer (Vaswani et al., 2017) where the text and audio signature are inferred. Vid2seq (Yang et al., 2023) operates over longer videos to densely generate a sequence of captions. While these models effectively learn dense video representations, it is hard to efficiently adapt them to instruction following tasks handled by recent generative foundational models.\n2.2 FOUNDATIONAL MODEL GENERATION\nLarge generative text models are being used as the backbone to answer questions about unstruc-tured input texts. These models have been infusing data from other modalities in order to generate responses on multi-modal video data. Macaw-LLM (Lyu et al., 2023) encodes video, audio, and images with corresponding foundational models. The individual modality embeddings are contex-tualized through cross-attention and are appended with the input prompt text into the final LLM for instruction fine-tuning, illustrated in Figure 1 b). Similarly, ChatBridge (Zhao et al., 2023) also aligns these video modalities but performs an initial pre-training stage to align each modality before performing instruction fine-tuning. These methods require fine-tuning on instruction datasets since they incorporate dense embeddings from each modality. Thus the performance is unclear when the domain shifts to a new distribution of videos and would require expensive re-training if data is avail-able. Instead, we investigate the performance when no alignment is performed and the text output from video modalities is used directly within the LLM for video tasks as shown in Figure 1 c)."}, {"title": "3 METHOD", "content": "To operate over video data we leverage both the visual and the audio modalities that it consists of. These include video frames, object segmentations obtained from frames, audio extracted from the video, and any metadata provided by the video. In contrast to previous video language models, we input each of these intermediate modalities as text representations instead of dense embeddings. The generated texts from the corresponding foundational model are fed into a language model to perform the end task as shown in Figure 2. In such a setting we avoid joint training of the separate modality embeddings along with the language model, allowing us to test different modality models without any re-training. Instead, we leverage the variance of data captured by each foundational model independently on large-scale datasets and apply it directly within our task of interest.\n3.1 VIDEO MODALITIES TO TEXT\nTo represent the video as text, we first select the middle frame of the video clip as a visual repre-sentation of the clip. We use a BLIP-2 Li et al. (2023b) vision language Transformer to obtain the caption for the frame. While the frame caption obtains a high-level description of the scene, the individual objects in the frame are useful to describe the scene. Using fixed-vocabulary detectors covers common objects, but does not cover the variety of objects that can be observed in different video domains. Instead, we use the Segment Anything Model (SAM) (Kirillov et al., 2023) to ob-tain segmentation regions within our video frame. Then we crop each segmentation and pass it back to BLIP-2 to caption the patch. Since we are looking for object descriptions, we only keep noun phrases found within each caption as an object pseudo-label. For the clip audio, we use Wav2Vec2 (Baevski et al., 2020), where we directly use the recognized speech as the text output. Finally, any metadata for the video, including its title or categorization classes, is also added to the text prompt."}, {"title": "3.2 FEW-SHOT ADAPTATION", "content": "We feed the text modalities extracted from the video into LLaMa (Touvron et al., 2023) for in-struction following. We are motivated to adapt the video inputs for new tasks without end-to-end fine-tuning over a large number of training samples. Our processed training set $\\mathcal{T}$ = {$\\mathcal{X}$, $\\mathcal{Y}$} is composed of the text representation outputs from each frozen modality model $\\mathcal{X}$ = {image, metadata, speech, objects}$_i^N$ along with the desired output text generations $\\mathcal{Y}$ = {outputs}$_i^N$. In the simplest setting, we select few-shot samples at random $\\mathcal{R} \\subset \\mathcal{T}$ and con-struct a prompt that indicates what we want to generate, a description of the modalities, followed by our few-shot samples $\\mathcal{R}$. Then for each inference example, we append the input text modalities of the video and ask it to generate the corresponding output. Given additional training data, Lu et al. (2021) show that the selection and ordering of few-shot examples makes a large difference in final evaluation performance. Therefore we test alternative strategies to select better prompt examples.\nGreedy Few-shot Search To select better prompts, we adapt the in-context influences proposed by Nguyen & Wong (2023) where we build our few-shot samples in a greedy fashion from our training set $\\mathcal{T}$. This is done by selecting a holdout set from the training samples $\\mathcal{H} \\subset \\mathcal{T}$ for evaluation. Then we select a search subset of training samples that are not within our holdout set $\\mathcal{S} \\subset \\mathcal{T} \\setminus \\mathcal{H}$. We then find a sample with the search set to add to our running set of previous few-shot examples selected $\\mathcal{F} = {\\mathcal{X}_F, \\mathcal{Y}_F}$. In particular, we find the sample that maximizes our generation metric score on the holdout set: arg max$_{x_s, y_s \\in S}$ Exh, yh\u2208H Score($\\hat{y}_h$, $\\mathcal{Y}_h$). Here $\\hat{y}_h$ is generated from our model given the input few-shot prompt built from {$\\mathcal{X}_F \\cup {\\mathcal{X}_s, \\mathcal{X}_h}$, $\\mathcal{Y}_F\\cup y_s$}. The optimal sample $\\mathcal{x^*, y^* \\in S}$ is added to the set of few-shot examples used for the next iteration $\\mathcal{F}$ = {$\\mathcal{X}_F\\cup \\mathcal{X^*}$,$\\mathcal{Y}_F \\cup y^* $}. Note that we fix the holdout set $\\mathcal{H}$ but re-sample the search set $\\mathcal{S}$ at each iteration to capture a larger variance of candidate samples. We iterate through this process and greedily build the set of n-shot examples that are used for inference, where n = |$\\mathcal{F}$|."}, {"title": "4 EXPERIMENTAL SETUP", "content": "4.1 DATASETS\nWe test our method on video summarization tasks, where given a video clip, the model should provide a concise summary of the events described in the video. We test on two datasets that have human-annotated captions alongside each corresponding video clip. The first dataset is YouCook2 (Zhou et al., 2018) which contains long-form cooking videos from different cuisines, where each clip segment within the video is annotated with a caption. We also test on COIN (Tang et al., 2019) which is a larger instructional dataset of household tasks and contains sentence-level text labels for each clip in the video. Given the individual video clips, we compute generation similarity metrics from the model output and the ground truth caption.\n4.2 MODEL SETUPS\nFor our modeling setup, we use the modality architectures specified in the methods section 3 with the largest model variant offered and use LLaMa as our final generator. We set the few-shot examples to n = 5 due to the size of texts generated from the combined input modalities. In the greedy search setting, we fix the holdout and search set sizes to |$\\mathcal{H}$| = |$\\mathcal{S}$| = 30.\nIn addition to testing our few-shot approaches, we directly optimized the LLaMa weights to gauge the performance improvements given the fine-tuning computational costs. Instead of full model parameter updates, we use low-rank updates for training in this setting, as described by Hu et al. (2021). Here we are only updating the LLaMa model where the individual modality models are frozen and only provide the input modality texts used for training.\nFinally, we test our approach against modality alignment strategies as done in ChatBridge (Zhao et al., 2023). ChatBridge similarly uses foundational models to encode frame and audio data. It uses a Perceiver model, which outputs the embeddings from the audio or frames into a learned fixed-sized dictionary. In this way, these embeddings can be used within LLaMa to provide additional context for the dense video embedding in conjunction with the instruction text. In the first-stage training, ChatBridge optimizes the Perceiver models by aligning each modality with ground truth text caption"}, {"title": "5 RESULTS", "content": "We analyze how PP-VLM performs across YouCook and COIN under different modalities, model sizes, and the corresponding computation costs of our few-shot search strategies. Then we ablate the best model setups for ChatBridge with respect to different input prompts and training stages. We finally compare the results of our proposed few-shot method to fine-tuning methods.\n5.1 PP-VLM GENERATION PERFORMANCE\nEffect of Modalities We show the caption generation results on YouCook and COIN using our method in Table 1 using LLaMa-30B. From the results in both settings, we see that adding corre-sponding modalities generally helps the performance. This increase in performance occurs when we cumulatively add the ASR audio captions, the video topic label, and the image frame caption from the middle of the clip. Adding the objects obtained from the image with the other modalities doesn't improve performance. Further inspection shows that using a SAM-based method for the extraction of objects in amateur videos leads to many caption artifacts which detract from generating a rele-vant video caption. We also tested using object labels with SemanticSAM (Li et al., 2023a), which uses an objective for more granular segmentation descriptions but saw similar results. This indicates that to benefit from object detections, a domain-specific trained detector would be helpful, or a sec-ondary filtering process to determine if the objects detected are relevant in context. Therefore, for the remainder of our experiments, we omit the object input modality unless indicated otherwise.\nEffect of Model Size We further investigate how different datasets perform under varying model sizes. We evaluate under the random search setting over 5 trial runs which best repre-sents performance when manually curating in-put prompts for a domain task. From the results in Figure 3 there are different patterns for each dataset we tested.\nIn YouCook, adding modalities improves per-formance as the base LLM gets larger. This is counterintuitive since leveraging additional information would provide more context when working with smaller models. Since the ASR has the highest correlation with the expected caption, the smaller models use cues from this modality best. For specific domains like cook-"}, {"title": "5.2 FEW-SHOT GENERALIZABILITY", "content": "Large foundational models are built and deployed in mind to be used in a large variety of domains. However, for applications in specific domains of interest, we analyze how transferable few-shot prompts are between domains. Furthermore, we analyze which data subsets to select for search when performance is evaluated on specific domain data.\n5.2.1 INTER-DATASET TRANSFER\nTo analyze the transfer capability of these models between domains, we test how PP-VLM optimized for YouCook performs on COIN and vice versa. The characteristics of both datasets are important in this task. YouCook has videos only regarding cooking but typically has dense audio captions and a larger variance in human captions. COIN is a dataset composed of diverse tasks, where audio captions are more sparse and have a smaller variance of output captions.\nThe transfer capability of these models between YouCook and COIN is shown in the first column of Table 2. We find that YouCook prompts transferred to COIN more gracefully than from COIN to YouCook. It is evident that smaller, but higher quality sample annotations provide more generaliz-able results between domains when searching few shot prompts.\n5.2.2 INTRA-COIN RESULTS ANALYSIS\nBeyond the transfer capability of entire datasets between domains, we further analyze the perfor-mance within subsets of COIN categories in columns 2 through 4 of Table 2. We evaluate the strategy of leveraging the higher variance in modalities present within large-scale datasets for gen-eralizable few-shot performance. In particular, we test three subsets of categories within COIN: furniture, electrical, and vehicle videos. Within each category, we test the evaluation performance of that category under three different training settings. The first is if we only use training samples from that category. The second is if we use all training samples besides the one in that category, which replicates transfer learning performance. Third is our original setting where data from all categories are used.\nWe observe that the few-shot case benefits most from just using domain data when evaluating that domain. Providing searches over all categories performs closely in select instances, but since we do not fine-tune, a search limited to domains of interest provides the best results over searching through a broader dataset. Searching over a large set where the category is not present leads to the latest drop in performance for that category, and reflects the domain transfer performance between YouCook and COIN.\n5.3 CHATBRIDGE ALIGNMENT\nAfter investigating the properties of PP-VLM, we compare how training-based methods like Chat-Bridge perform on these summarization tasks. ChatBridge has two stages of training: stage 1 modal-ity alignment and stage 2 fine-tuning. We perform stage 1 fine-tuning where we align the frozen image and audio models to generate captions for the corresponding clip by training the Perceiver models. We sample 16 frames from each clip to pool for the video token representation vector. For stage 2 fine-tuning, we tune over the same output captions but optimize the base image and audio models in this stage while the Perceiver is frozen."}, {"title": "5.4 PLUG-AND-PLAY FEW-SHOT VERSUS MODEL TUNING", "content": "Given the exploration of ChatBridge setups, we compare the performance against our PP-VLM few-shot search schemes. For PP-VLM we compare our random search strategy, which re-flects the setting where new domain samples are provided to the model, rather than hav-ing a large dataset to search for samples over. We also perform LoRA fine-tuning on LLaMa where we use video text input modalities (IM), and the caption as the expected output. This allows us to directly compare fine-tuning ver-sus few-shot performance for our summariza-tion task given only text inputs.\nFrom the results in Table 4, we can see that adding additional shot samples helps PP-VLM performance. LoRA fine-tuning is comparable to our 2-3 shot methods. This indicates the strength of in-context learning within founda-tional models for summarization tasks, where only a few demonstrations are necessary. For ChatBridge we compare the best input modal-ity (IM) setting we determined from our pre-vious evaluation at different stages of training, which performs comparable to our random 4-5 shot methods. When using greedy search (S:5), which uses less training data and compute, our PP-VLM is able to provide better quality summaries. Notably for pre-trained ChatBridge, it out-performs 0-shot PP-VLM, which indicates that the combination of latent modality embeddings and semantic text embeddings can be useful in hybrid methods. These can be tasks beyond summariza-tion where the intermediate modality outputs cannot be explicitly expressed as text from a single frame, such as learning cause and effect, or temporal dynamics of a subject."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "Foundational models from multiple modalities leverage large-scale data to achieve SOTA perfor-mance within their domains. Due to this, leveraging foundational models from multiple modalities has been an active area of research for inputs that leverage this multi-modality, such as videos. These video models use the corresponding video and audio inputs to carry out instructions prompted by the user. To leverage the embeddings of these modalities, each input modality is aligned to share the same embedding space. Then the individual modality models and the language model can be further fine-tuned for a specific task. Instead of leveraging latent modality embeddings, we test using the text descriptions of these modalities directly. This plug-and-play approach allows using off-the-shelf generative models and composes their outputs within an LLM to carry out an instruction. Testing few-shot approaches to adapt this model to video summarization tasks reduces computation cost and data overhead needed by fine-tuning models. Our results provide key insights in:\n\u2022 Understanding the domain of the videos when selecting which modalities and model sizes to use for few-shot modeling.\n\u2022 Evaluation of the value added by obtaining more data and compute requirements when testing different few-shot evaluation strategies.\n\u2022 Transferring few-shot prompts between domains works best with high-variance captions, and performance improvement within a specific domain only requires few-shot samples from that domain.\n\u2022 Showing what stages of modality fine-tuning are best given the size of the training corpus and which text modality prompts to use.\nIn future work, we want to leverage PP-VLM to incorporate more information beyond just the input video text modalities. External documents or knowledge graph information can provide greater con-text when performing video tasks. For example, recipe data or instructional steps can help improve the performance of YouCook and COIN captioning respectively. In longer-form videos composed of multiple sub-clips, we want to investigate how to pass information between clips to provide a better context of the video as a whole. This involves deciding which information is most useful to share between video clips. Proving such external knowledge helps the video model bridge more compli-cated tasks beyond input summarization, such as predicting future video events or conversational tasks."}]}