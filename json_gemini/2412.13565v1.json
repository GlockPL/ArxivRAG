{"title": "CA-Edit: Causality-Aware Condition Adapter for High-Fidelity Local Facial Attribute Editing", "authors": ["Xiaole Xian", "Xilin He", "Zenghao Niu", "Junliang Zhang", "Weicheng Xie", "Siyang Song", "Zitong Yu", "Linlin Shen"], "abstract": "For efficient and high-fidelity local facial attribute editing, most existing editing methods either require additional fine-tuning for different editing effects or tend to affect beyond the editing regions. Alternatively, inpainting methods can edit the target image region while preserving external areas. However, current inpainting methods still suffer from the generation misalignment with facial attributes description and the loss of facial skin details. To address these challenges, (i) a novel data utilization strategy is introduced to construct datasets consisting of attribute-text-image triples from a data-driven perspective, (ii) a Causality-Aware Condition Adapter is proposed to enhance the contextual causality modeling of specific details, which encodes the skin details from the original image while preventing conflicts between these cues and textual conditions. In addition, a Skin Transition Frequency Guidance technique is introduced for the local modeling of contextual causality via sampling guidance driven by low-frequency alignment. Extensive quantitative and qualitative experiments demonstrate the effectiveness of our method in boosting both fidelity and editability for localized attribute editing.", "sections": [{"title": "Introduction", "content": "Efficient and high-fidelity local facial attribute editing with textual description represents a challenging task in computer vision. GANs-based methods (Wang et al. 2022; Pernu\u0161, \u0160truc, and Dobri\u0161ek 2023) have explored this task, which primarily optimize the original image within the latent space with a pre-trained StyleGAN model (Karras et al. 2020). However, these GANs-based methods require additional fine-tuning for different attributes. Subsequently, the prior diffusion-based image editing methods based on the text-to-image (T2I) diffusion models achieve image editing in various ways. These methods are either based on P2P (Hertz et al. 2022), utilizing the original image attention injection mechanism to preserve the layout, or based on DDIM Inversion (Song, Meng, and Ermon 2020), modifying the latent at the noise level. However, such methods may lead to inconsistencies beyond the editing target area. Regarding local facial attribute editing, image inpainting is a technique focused on local masked region painting, which also benefits from the recent advances in diffusion models (Avrahami, Lischinski, and Fried 2022; Yang et al. 2023; Yang, Chen, and Liao 2023). Besides, image inpainting has been also developed for local facial attribute editing, which focuses on the inpainting of local masked regions, based on advanced diffusion models (Avrahami, Lischinski, and Fried 2022; Yang et al. 2023; Yang, Chen, and Liao 2023). Text-guided image inpainting(Avrahami, Lischinski, and Fried 2022) allows prompt-driven content generation in specific areas without finetuning during inference, while maintaining consistency between the editing and unmasked regions, which is thus used in our method.\nHowever, existing methods for image inpainting may suffer from concerns in terms of editability and fidelity. The first problem: they (Zhang, Rao, and Agrawala 2023; Ju et al. 2024) struggle to understand the contextual relationship between unmasked facial regions and the textual description, resulting in the neglect of the text prompt while creating a plain completion (Fig.1 (a) ). For addressing this problem, Hd-Painter (Manukyan et al. 2023) can better align the inpainting generation with the text by modifying the latent, while it still fails for local facial text prompts. The root cause is that previous diffusion models are primarily trained on natural image-text pairs, lacking the fine-grained knowledge of human faces.\nThe second problem: For facial inpainting, previous works (Rombach et al. 2022; Yang, Chen, and Liao 2023) do not take adequate consideration of the contextual causality between the masked region and the specific details (skin texture, skin tone, and other details) of the original image. The causality consideration is further constrained by the conflict between textual editing conditions and the preservation of these details in original image. In facial images, even slight differences in these details become visibly obvious, largely impairing the overall naturalness. (Fig.1 (b)) Therefore, the key to maintaining the skin details and mitigating the difference lies in the reasonably causality-aware modeling of these specific details from the original image.\nFor addressing this problem, existing approaches adapt the parallel attention with textual conditions(i.e. IP-Adapter (Ye et al. 2023)) to inject original image information and enhance contextual causality modeling. However, as shown in Fig. 1, this causality conflicts modeling with the text condition may lead to severe content leakage. (Fig.1 (c)) Meanwhile, from a localized contextual perspective, existing methods (Ju et al. 2024; Xu et al. 2024) lack explicit approaches for this fine-grained local context, causing disharmony in boundary regions of the primary editing regions, while the skin transitions are generally smooth.\nTo address these challenges, we proposed our CA-Edit from the local attribute data construction and causality-aware condition adapter. For addressing the first problem, training on detailed textual captions of local facial attributes would be crucial for editability. To this end, we introduce a data construction pipeline, leveraging Multimodal Large Language Models (MLLMs) (Chen et al. 2023; Li et al. 2023) for automatic local facial attribute captioning and the face parsing model for segmentation acquisition. For addressing the second problem, we introduce an additional adapter for original image condition, as well as a sampling guidance during inference, to fully explore original image cues. Specifically, (i) the Causality-Aware Condition Adapter (CA\u00b2) is proposed to enhance the causality modeling while preventing the conflict with textual condition. (ii) a sampling guidance technique called Skin Transition Frequency Guidance (STFG) is proposed to mitigate the artifacts on the 'boundary regions' via enhancing the similarity between the generated image and the low-frequency components of the original image.\nThe main contributions of this work are summarized as:\n\u2022 To address the limitations of existing datasets lacking local facial attribute captions, we propose LAMask-Caption, the first dataset with detailed local facial captions which contains 200,000 high-quality facial images and employs Large Multimodal Models (MLMMs) for automatic captioning of local facial regions.\n\u2022 To jointly address the issues of fine-grained context modeling and content leakage, we propose the novel CA\u00b2) that enhances contextual causality modeling in primary editing regions while regularizing the visual condition according to the textual condition and latent. Furthermore, we propose the novel STFG to preserve the skin details on the boundary regions by enhancing the low-frequency similarity with the original image during inference.\n\u2022 Quantitative and qualitative experiments demonstrate that CA-Edit produces more harmonious and natural outcomes, showcasing the superiority of our method in local attribute editing."}, {"title": "Related Work", "content": "The advancement of facial editing and manipulation has been promoted by the emergence of recent generative approaches. Early efforts in this area have explored the application of GANs-based models (Karras, Laine, and Aila 2019; Shen et al. 2020; Yang et al. 2021; Xia et al. 2021). MaskGAN (Lee et al. 2020) demonstrated the benefit of using spatially local face editing. InterFaceGAN (Shen et al. 2020) regularizes the latent code of an input image along a linear subspace. Recently, increasing researchers have resorted to diffusion models to enhance the generative capability for face editing. Methods like (Ding et al. 2023; Jia et al. 2023) both explored the use of 3D modalities as reference cues to make facial image editing more robust and controllable. Xu et al. (Xu et al. 2024) finetune a diffusion model for editing tasks tailored to the individual's facial characteristics. However, these approaches require extra conditions beyond text, limiting their suitability for our task due to user accessibility issues.\nEarly works (Nitzan et al. 2022; Andonian et al. 2021; Xia et al. 2021) leveraging pretrained GAN generators (Karras, Laine, and Aila 2019) have explored the text-driven image synthesis. Among approaches for semantic image editing, text-guided image editing based on diffusion models has garnered growing attention. (Gal et al. 2022a; Ruiz et al. 2023; Rombach et al. 2022; Morelli et al. 2023; Mao, Wang, and Aizawa 2023; Zhong et al. 2023; Brooks, Holynski, and Efros 2023) have exploited diffusion models for text-driven image editing. Textual Inversion (Gal et al. 2022a) generates an image by learning a concept embedding vector combined with other text features. For better control of the original semantic cues, InstructPix2Pix (Brooks, Holynski, and Efros 2023) enables image editing based on textual instructions by leveraging a conditioned diffusion model trained on a dataset generated from the combined knowledge of a language model and a text-to-image model. DiffusionCLIP (Kim, Kwon, and Ye 2022) and Asyrp (Kwon, Jeong, and Uh 2022) draw inspiration from GAN-based methods (Gal et al. 2022b) that use CLIP, and use a local directional CLIP loss between image and text to manipulate images. However, these methods either require additional finetuning or lead to changes outside target editing regions, which fail to meet the requirement of local editing."}, {"title": "Preliminaries", "content": "Diffusion Model. Diffusion models are a family of generative models that consist of the processes of diffusion and denoising. The diffusion process follows the Markov chain and gradually adds Gaussian noise to the data, transforming a data sample xo ~ q(x0) into the noisy sample x1:T = X1, X2,\u2026, XT in T steps. The denoising process utilizes a learnable model to generate samples from this Gaussian noise distribution denoted as po(x0:T) at time step t based on the condition c, where @ denotes the learnable parameter. Eventually, the training of the model is formulated as:\n$\\mathcal{L} = \\mathbb{E}_{x_0,\\epsilon \\sim \\mathcal{N}(0,1),c,t}||\\epsilon - \\epsilon_{\\theta}(x_t, c, t)||^2,$"}, {"title": "Method", "content": "To enable local facial attributes inpainting, we first construct the dataset LAMask-Caption including the face images, textual descriptions of local facial attributes and the specific segmentation mask of the attributes (Fig. 2). To adapt the T2I model to our task, we trained a reference network copied from the U-Net. Based on this network, we introduced Causality-Aware Condition Adapter (CA\u00b2) to enhance skin detail causality while balancing textual and visual cues for precise and seamless attribute editing. Additionally, to reduce the artifacts between generated content and the unmasked regions, our Skin Transition Frequency Guidance (STFG) technique further leverages the skin detail in the original image during inference, to avoid the effect of imprecise input masks.\nA key reason that current diffusion models encounter difficulties with local facial editing is the lack of precise textual captions describing local facial attributes in the training data, as mainstream diffusion models are primarily trained on large-scale natural image datasets such as Laion-2B (Schuhmann et al. 2022) or MS-COCO (Lin et al. 2014). Hence, a face dataset with local attributes-text pairs is essential for finetuning the pretrained diffusion model to adapt to facial local attribute editing. While the existing CelebA-dialog dataset (Jiang et al. 2021) and FaceCaption-15M (Dai et al. 2024) contain manually annotated textual captions for each image, it mainly focuses on overall attributes (i.e. age, skin) rather than local facial attributes. Therefore, their global captions would fail to meet the demand as training data of local facial attribute editing, which motivates us to develop a new dataset with complete local facial attribute captions.\nSpecifically, we introduce our LAMask-Caption, a dataset consisting the triples of detailed textual captions of local facial attributes, high-resolution images and attribute masks. The overview of our LAMask-Caption construction pipeline is shown in Fig. 2. Via this framework, we collect a high-quality facial image dataset comprising 200,000 high-quality images by combining filtered images from FaceCaption-15M with selections from FFHQ and CelebMask-HQ datasets.\nWe employ Multimodal Large Language Models"}, {"title": "Causality-Aware Condition Adapter (CA\u00b2)", "content": "One naive approach for injecting skin detail as a visual condition into a diffusion model is usually achieved through cross-attention, which requires parallel addition of cross-attention modules for the original image embedding, akin to IP-Adapter (Ye et al. 2023; Wang et al. 2024). However, we argue that the direct injection of visual cross-attention would lead to over-reliance on the visual condition during training while ignoring textual editing conditions (Jeong et al. 2024; Qi et al. 2024). To this end, we propose the novel Causality-Aware Condition Adapter (CA\u00b2), as shown in Fig. 3, which injects specific skin details from the original image as image embedding through an additional attention mechanism, and adaptively adjusts the intensity of visual condition injection. The adjustment is conducted based on the influence of the textual prompt on the existing features, aiming to balance the impact of textual and visual conditions. The adapter encodes the contextual causality between the main editing region and specific skin details, while preventing visual-textual condition conflicts.\nIn our proposed CA\u00b2, both the vision and text encoders of a pretrained CLIP are utilized for the feature extraction, formulated as:\n$\\begin{cases}f_{txt} = CLIP_{txt}(txt) \\in \\mathbb{R}^{n_t \\times C_t}\\\\f_{vis} = CLIP_{vis}(x) \\in \\mathbb{R}^{n_v \\times C_v}\\end{cases}$"}, {"title": "Skin Transition Frequency Guidance (STFG)", "content": "While CA\u00b2 preserves skin details in the main editing areas, real-world facial editing often uses imprecise masks, leading to unnatural transitions in 'boundary regions'. These smooth skin areas are sensitive to low-frequency changes. To address this, we introduce a sampling guidance technique for low-frequency components during denoising, to produce natural transitions in these regions.\nSpecifically, given the localization and semantic representation capabilities of textual cross-attention maps in diffusion models to identify 'boundary regions'. The mean of attention maps, i.e., Atxt is computed across all text tokens and attention layers. We identify the 'boundary regions' as regions within the mask M where the attention values on Atxt are below a threshold (Atxt, M). The indexes Idx of the pixels belonging to 'boundary region' is represented as:\n$Idx = \\{(i, j)|A_{txt}(i, j) \\le \\gamma(A_{txt}, M)\\}\\\\ \\gamma(A_{txt}, M) = \\mu(A_{txt} \\circ M) - \\sigma(A_{txt} \\circ M)$"}, {"title": "Evaluation Metric", "content": "Objective Metrics. To comprehensively evaluate the performance of different methods on the task of local facial attributes editing, we utilize FID / Local-FID (Heusel et al. 2017), LPIPS (Zhang et al. 2018), identity similarity (ID), MPS (Zhang et al. 2024) and HPSv2 (Wu et al. 2023) as evaluation metrics. FID and LPIPS are used to provide an estimate of image fidelity. It's important to note that in this specific task, unlike general image generation, lower LPIPS values indicate higher fidelity. MPS and HPSv2 are more effective and comprehensive zero-shot objective evaluation metrics on text-image alignment and human aesthetics preferences. ID evaluates the face identity between the results and the original images.\nUser Study. Besides comparisons on objective metrics, we also conduct a user study via pairwise comparisons to determine whether our method is preferred by humans. The generation results are evaluated on three dimensions: face fidelity (FF), text-attribute consistency (TAC), and human preference (HP)."}, {"title": "Experimental Setup", "content": "Benchmark. As this work serves as one of the text-guided local facial attribute editing, we introduce FFLEBench, i.e., one pioneering benchmark evaluation dataset for this task, motivated by the lack of corresponding benchmark and evaluation dataset. FFLEBench comprises a total of 15,000 samples drawn from FFHQ, accompanied by the local masks and the corresponding textual captions. Note that the samples drawn from FFHQ to construct the FFLEBench are independent with those used for training. The masks are the convex hull or the dilation of the segmentation masks, aiming to imitate the rough mask input.\nImplementation Details. All the cross-attention maps and the score map are upsampled to the resolution of 64 \u00d7 64. To preserve the original information in the regions outside the mask, we blend the latent variable following Blended Diffusion (Avrahami, Lischinski, and Fried 2022)."}, {"title": "Extended Qualitative Results", "content": "Results with Diverse Description. To showcase the capability of our proposed approach in following intricate instructions, we present the generated images under the same input mask for various text descriptions, including both direct and indirect ones. As shown in Fig. 11, the output images highlight the adaptability of our method in accommodating diverse textual inputs while maintaining the reasonability of the edited content as well as the specific skin details.\nComparison with Existing Methods. In Fig. 15 and Fig. 16, we show additional visual comparison with image editing methods on more facial attributes. In addition to the Qualitative Experiment Results in the main body, we include more inpainting methods ((Avrahami, Lischinski, and Fried 2022; Manukyan et al. 2023)) and the Inversion-based method (Renoise Inversion (Garibi et al. 2024)) for the comparison. In these figures, we highlight the mask-free methods with blue color.\nFigs. 15 and 16 show that these compared approaches exhibit inferior performance when confronted with the task of editing local regions, due to the lack of mask integration. Meanwhile, such methods often result in substantial leakage into incorrect regions during the process of localized editing with complex semantic textual description, or even changes of the individual identity (fourth column in Fig. 16).\nComparison with Inversion-based Diffusion Methods. We also extend the comparison with existing approaches depending on inversion-based diffusion, including both the finetuning-required and finetuning-free paradigms. Among them, DiffusionClip (Kim, Kwon, and Ye 2022) and Asyrp (Kwon, Jeong, and Uh 2023) both require additional finetuning for each previously unseen editing target with text prompt pairs. These inversion-based methods introduce a CLIP direction loss that aims to align the vector between the original and edited images with the one between the corresponding textual prompts in CLIP space. Null-text Inversion (Mokady et al. 2023) and Renoise Inversion optimize the noise map during DDIM to further mitigate the error between the original image and the edited one in the resampling path during inference. However, as illustrated in Fig. 12, such methods fail to deal with localized editing, struggling to strike a trade-off between editability and fidelity. Specifically, they either perform minor editing on the target attributes or produce undesirable effects outside the target attributes."}]}