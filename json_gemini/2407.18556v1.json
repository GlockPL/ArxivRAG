{"title": "Look Globally and Reason: Two-stage Path Reasoning over Sparse Knowledge Graphs", "authors": ["Saiping Guan", "Jiyao Wei", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "abstract": "Sparse Knowledge Graphs (KGs), frequently encountered in real-world applications, contain fewer facts in the form of (head entity, relation, tail entity) compared to more populated KGs. The sparse KG completion task, which reasons answers for given queries in the form of (head entity, relation, ?) for sparse KGs, is particularly challenging due to the necessity of reasoning missing facts based on limited facts. Path-based models, known for excellent explainability, are often employed for this task. However, existing path-based models typically rely on external models to fill in missing facts and subsequently perform path reasoning. This approach introduces unexplainable factors or necessitates meticulous rule design. In light of this, this paper proposes an alternative approach by looking inward instead of seeking external assistance. We introduce a two-stage path reasoning model called LoGRe (Look Globally and Reason) over sparse KGs. LoGRe constructs a relation-path reasoning schema by globally analyzing the training data to alleviate the sparseness problem. Based on this schema, LoGRe then aggregates paths to reason out answers. Experimental results on five benchmark sparse KG datasets demonstrate the effectiveness of the proposed LoGRe model.", "sections": [{"title": "1 Introduction", "content": "Sparse Knowledge Graphs (KGs) are a special type of KGs, containing fewer facts in the form of (head entity, relation, tail entity) compared to more populated KGs. They are frequently encountered in real-world applications, such as location recommendation, fund recommendation, and fraud detection [8, 21, 27, 31]. The sparseness of sparse KGs impedes the performance improvement of such applications. Hence, sparse KG completion, formalized as reasoning answers for given queries in the form of (head entity, relation, ?) for sparse KGs, is a crucial and urgent task.\nThe sparse KG completion task is challenging, as it requires reasoning missing facts based on limited available facts. Thus, the performance of existing models for KG completion drops significantly when applying them to sparse KGs [14, 20, 23, 25]. To address"}, {"title": "2 Problem Statement", "content": "DEFINITION 1. KG is a graph of entities E and their relations R with facts represented as (h, r, t), where {h,t} \u2208 E and r \u2208 R, indicating the head entity h has the relation r with the tail entity t.\nDEFINITION 2. Sparse KG is a special type of KG, where entities are less connected, containing fewer facts compared to a regular KG.\nSparse KG is a relative concept compared to KG. As introduced in DacKGR [14], a KG is considered dense or normal if the average degree of its entities exceeds a certain threshold; otherwise, it is considered sparse. There is no common consensus on the precise value of this threshold.\nDEFINITION 3. Sparse KG completion is the task of reasoning missing facts based on limited facts available in a sparse KG. For example, given a query (h,r,?), it is to determine answers\u00b9, also called correct tail entities in this paper."}, {"title": "3 Related Works", "content": "This paper handles sparse KG completion. Its literature can be divided into embedding-based, rule-based, and path-based models."}, {"title": "3.1 Embedding-based Models", "content": "Embedding-based KG completion models learn embeddings of entities and relations, and devise a score function to determine the likelihood of a candidate fact or candidate tail entity through vector operation. For example, TuckER [2] utilize multiplication operations. More complexly, ConvE [6] reshapes and concatenates the embeddings of the head entity and relation to form a matrix, where a 2D convolution operation is applied to predict the tail entity. InteractE [28] further enhances the interaction between the head entity and relation through feature permutation, feature reshaping, and circular convolution. NBFNet [39] adopts a graph neural network to predict the conditional likelihood of the tail entity.\nTo address the sparseness problem of sparse KGs, researchers incorporate additional information into their models. RelaGraph [23] extends neighborhood information during entity encoding and incorporates neighborhood relations to mine deeper graph structure information. KRACL [25] introduces a knowledge relational attention network to integrate neighboring facts and proposes a knowledge contrastive loss that considers more negative samples. Liu et al. [13] fine-tuned pretrained embeddings of textual information based on the sparse KG to compensate for the lack of structure information. He et al. [9] trained existing structure-based and text-based models, and fused knowledge acquired by them. Jia et al. [11] even linked to external KGs to introduce additional entity information and employed graph convolutional neural networks to aggregate the information."}, {"title": "3.2 Rule-based Models", "content": "Rule-based KG completion models deduce logical rules based on the graph structure of KG and use these rules to reason out answers for given queries. For example, NTP [22] introduces neural networks for end-to-end differentiable proving of queries to KG, where rules are induced using gradient descent. RIvlR [17] proposes a target relation-oriented sampling method and a matrix operations-based rule evaluation mechanism to mine massive rules efficiently. RuLES [10] iteratively extends rules induced from a KG by relying on the feedback from a precomputed embedding model over the KG"}, {"title": "3.3 Path-based Models", "content": "Path-based KG completion models perform path exploration on KG, starting from the given head entity, to reach answers with the guidance of the given relation. For example, Das et al. [4, 5] applied paths from similar entities of the given head entity to reason out tail entities. RuleGuider [12] leverages high-quality rules to provide reward supervision for RL agents. In contrast, CPL [7] trains two collaborative RL agents jointly: a fact extractor and a multi-hop reasoner. The extractor generates facts from corpora to enrich KG, while the reasoner offers feedback to the extractor and guides it towards promoting facts helpful for the reasoning.\nTo address the challenges in selecting correct paths due to the lack of facts in sparse KGs, researchers usually use external models to fill in missing facts before path reasoning. DacKGR [14] dynamically adds edges based on embedding-based models as additional"}, {"title": "3.4 Summarization and Comparison", "content": "In general, there have been numerous studies for KG completion, with fewer and more recent studies specializing in sparse KGs. Among them, path-based models are anticipated to offer superior explainability compared to embedding-based models, while obviating the need for meticulously designed rules compared to rule-based models. However, existing path-based sparse KG completion models necessitate external assistance from other models to tackle the sparseness problem. They introduce unexplainable embeddings or rely on high-quality rules, deviating from the original intention of path-based models. Thus, this paper proposes an alternative approach by internally addressing the sparseness problem instead of seeking external assistance."}, {"title": "4 The Proposed LoGRe Model", "content": "We propose a two-stage model called LoGRe to look inward without reliance on external models. As depicted in Figure 1, it looks globally to construct a relation-path reasoning schema from the training data and then aggregates paths to reason."}, {"title": "4.1 Stage 1: Construction of Relation-path Reasoning Schema", "content": "This stage globally analyzes the training data to obtain type-specific relation-path reasoning schema (referred to as type-specific reasoning schema), and finally relation-path reasoning schema, which encompasses type-specific and cross-type ones. For clarity, we present a small example of this stage in Figure 2. Now, Let us elaborate on this stage."}, {"title": "4.1.1 Type-specific Reasoning Schema. The Construction Process", "content": "Type-specific relation-path reasoning schema is constructed as follows:\n\u2022 For each entity $e \\in E$, we randomly collect a maximum number of $N_{path}$ paths\u00b2 from the training data to construct the entity-path dictionary D. To avoid the time-consuming collection of excessively long paths that may not contribute to the reasoning, the hop of each path is restricted to a maximum of $N_{hop}$.\n\u2022 Entities are divided into groups based on their types, such as person, organization, and location.\n\u2022 For each group of index i, we gather the relation set $R_i$ possessed by its entities $E_i$.\n\u2022 For each relation $r_j \\in R_i$, we retrieve the specific paths from the entity-path dictionary D by the entities in $E_i$ that have $r_j$. These paths should lead to the correct tail entities in the training data.\nThus, the relations and their corresponding paths constitute an initial type-specific relation-path reasoning schema. As illustrated in the left part of Figure 1 and Figure 2, its relation-path reasoning elements grouped by entity types, are in the form of:\n$r_j: [p_{j1}, p_{j2},\u2026 ],$                                                             (1)\nwhere $p_{jk}$ (k = 1, 2, \u2026\u2026\u2026) is the k-th path of relation $r_j$ in the group, which satisfies the two conditions mentioned above: At least one"}, {"title": "Path Score Calculation", "content": "To facilitate path reasoning, we calculate a score for each path. For sparse KGs, where facts and certain paths are missing, we think path precision is crucial. Thus, the score $s(p_{jk})$ of path $p_{jk}$ is defined as its precision:\n$s(p_{jk}) = \\frac{N_{jk}}{M_{jk}}$                                                             (3)\nwhere $M_{jk}$ is the number of occurrences of $p_{jk}$ for the entities in the group and $N_{jk}$ is the number of times that $p_{jk}$ reaches the correct tail entities of $r_j$."}, {"title": "4.1.2 Cross-type Reasoning Schema. The Motivation and Construction Process", "content": "Some relations may appear in multiple entity type groups. For instance, the entity types of person and organization have common attributes (also treated as relations, as they are handled in the same way), such as telephone number and address. Similarly, relations like country are possessed by the entity types of person, organization, and geo-political entity, as exemplified in Figure 2. To handle this, we adopt an approach inspired by the bottom-up construction process of KG schema, where common relations of entity types are moved to their parent type. Analogically, we move the common relations of certain entity types to the cross-type group, which is considered as their parent group. As depicted in the left part of Figure 1, relation $r_2$, appearing in all the four groups, is moved to the top cross-type group. Likewise, relation $r_3$, shared by person and organization, is also relocated. Illustratively, as demonstrated in Figure 2, country possessed by person, organization, and geo-political entity is shifted to the top cross-type group. The corresponding paths are combined:\n$P(r) = \\cup_{i=1}^{n}P(r)_i,$                                                          (4)"}, {"title": "Path Score Calculation for the Cross-type Relations", "content": "The path scores of the cross-type relations are aggregated from the entity types where they appear. For a path p of the cross-type relation r, its score is:\n$s(p) = \\frac{\\sum_{i=1}^{n'} N_i}{\\sum_{i=1}^{n'} M_i}$                                                            (5)\nwhere n' is the number of the entity type groups in which r and p co-appear in their relation-path reasoning elements, Mi is the number of occurrences of p for the i-th group, and Ni is the number of times that p reaches the correct tail entities of r."}, {"title": "4.1.3 Relation-path Reasoning Schema", "content": "After shifting common relations to the cross-type group and aggregating their paths and scores, we obtain the final relation-path reasoning schema consisting of type-specific and cross-type ones. The paths of each relation are sorted in reverse order by their scores. Before sorting, these scores are adjusted based on the following consideration: Although for sparse KG, missing facts often make the model need to try more hops when short paths are unavailable, short paths are preferred. Actually, rules are typically short in length. Short paths are more likely to be rules. Therefore, we introduce a hop decay to the score of each path p:\n$s(p) = f(len(p))s(p),$                                                              (6)\nwhere len(p) is the length of p, i.e., the number of hops of p, and f(len(p)) is defined as:\n$f(len(p)) = d^{len(p)},$                                                        (7)\nwhere d is the hop decay factor. The score is multiplied by d for each increase in the number of hops.\nGenerally, the construction of the relation-path reasoning schema is a bottom-up approach. It thoroughly explores the training data, thereby alleviating the sparseness problem of sparse KGs. Besides, it enables us to obtain more precise path scores by considering paths from a global perspective."}, {"title": "4.2 Stage2: Path Reasoning", "content": "Based on the relation-path reasoning schema, we perform path reasoning and get the scores of candidate tail entities."}, {"title": "4.2.1 The Reasoning Process", "content": "In particular, we perform path reasoning as follows:\n\u2022 For each given query (h, r, ?), we retrieve the corresponding reasoning paths [p1, p2,] by the entity type of h and r.\n\u2022 For the top Ntop paths, we start from h and follow the paths to arrive at candidate tail entities. If a relation in the path does not appear, the process is halted midway. Here, Ntop is the number of explored top paths. We try top paths instead of all paths to relieve the impact from spurious paths.\n\u2022 Sort all the candidate tail entities reversely by scores, which are introduced in the following."}, {"title": "4.2.2 The Scores of Candidate Tail Entities", "content": "We obtain the score of each candidate tail entity by summing the scores of the paths reaching it. The rationale is that individual indeterministic paths alone cannot be reliable. However, when multiple paths converge towards the same answer, it is possible to infer a meaningful result. Specifically, for a candidate tail entity $t_i$, its score $s(t_i)$ is:\n$s(t_i) = \\sum_{p \\in Q} s(p),$                                                          (8)\nwhere Q is the set of paths of relation r that connect h to $t_i$."}, {"title": "4.2.3 One Update to the Scores", "content": "The scores of candidate tail entities are further adjusted with the following answer similarity: The answer should be similar to other tail entities with the same r and path. This is inspired by the phenomenon of analogy. It suggests that entities sharing the same r and path may also exhibit similarities in their tail entities. For example, these tail entities may also belong to the same group.\nThus, the score $s(t_i)$ is multiplied by the similarity of the candidate and other similar tail entities:\n$s(t_i) = max_{t' \\in T} cos\\_sim(t_i, t')s(t_i),$                                                (9)\nwhere T is the set of other similar tail entities sharing the same r and path, and $cos\\_sim(t_i, t')$ is the cosine similarity between the vectors of $t_i$ and $t'$. The maximum similarity is used to mitigate the impact of spurious paths, which arrive at the answer coincidentally. To keep the explainability of LoGRe, we represent each entity as a |R|-hot vector, where |R| is the size of the relation set R. An entry in the vector is set to 1 if the entity has that relation with some entities; otherwise, it is set to 0.\nThen, the candidates are sorted by their scores and candidates with higher scores are more likely to be the correct tail entities."}, {"title": "4.3 The Explainability of LoGRe", "content": "The proposed LoGRe model is inherently explainable in its design. Moreover, for each given query, it outputs not only the candidate tail entities but also the reasoning paths with scores for each candidate tail entity. As illustrated in the right part of Figure 1, for candidate tail entity $t_1$, the reasoning paths $p_2$ and $p_4$ with scores reveal the step-by-step processes from the head entity h to $t_1$ via the relations."}, {"title": "5 Experiments", "content": "We evaluate LoGRe on five benchmark sparse KG datasets and conduct comprehensive analyses to examine its performance."}, {"title": "5.1 Datasets", "content": "The adopted sparse KG datasets include FB15K-237-10%, FB15K-237-20%, FB15K-237-50%, NELL23K, and WD-singer [14]. The first three datasets randomly retain 10%, 20%, and 50% facts from FB15K-237 [26], respectively. WD-singer is a dataset of singer domain extracted from Wikidata [30]. NELL23K is a randomly sampled dataset from NELL [3]. The detailed statistics of these datasets are listed in Table 1, where |E|, |Train|, |Valid, and Test are the sizes of the entity set, training set, validation set, and test set, respectively; Degree is the average degree of the entities in the corresponding training set."}, {"title": "5.2 Experimental Settings", "content": "Baselines and Evaluation Metrics. Baselines comprise three types of representative or state-of-the-art models. The first type is path-based models, of the same type with LoGRe, including DacKGR [14], SparKGR [32], DT4KGR [33], and Hi-KnowE [35]. The second type is rule-based models\u00b3, including NTP [22], RlvlR [17], and AnyBURL [16]. We also refer to embedding-based models, including TuckER [2], ConvE [6], InteractE [28], NBFNet [39], KR-ACL [25], and Ref. [11]. Lastly, we compare to ChatGPT\u2074, the prominent large language model.\nFor every fact (h, r, t) in the test set, we convert it to a query (h, r, ?) and apply models to get the ranking list of candidate tail entities, following Refs. [14, 32]. The standard Mean Reciprocal Rank (MRR) and Hits@K are adopted as evaluation metrics for model comparison. MRR is the mean reciprocal rank of all the correct tail entities among the candidates and Hits@K is the proportion of correct tail entities ranking within the top K positions. Higher values of MRR and Hits@K indicate better performance."}, {"title": "5.2.2 Implementation Details. Entity Types", "content": "LoGRe requires entity types to construct the relation-path reasoning schema and perform path reasoning. For the FB15K-237 series, we retrieve entity types from FB15K-237 [34]. For NELL23K, entity types are obtained from entity names, as they are in the form of \"concept_type_entity\". For WD-singer, we obtain entity types from Wikidata [30] via the tail entities of relation \"instance of\". When there are multiple types, we select the most frequent one as the type to mitigate the impact of noisy data. If no type is available for an entity, it is assigned to the type \"others\".\nHyper-parameters. The hyper-parameters of LoGRe are selected from the following ranges in terms of MRR: The maximum number of collected paths $N_{path} \\in$ {1000, 5000, 10000, 15000, 20000}, the maximum number of path hops $N_{hop} \\in$ {3, 4, 5, 6}, the number of explored top paths $N_{top} \\in$ {100, 500, 1000}, and the hop decay factor $d \\in$ {0.95, 0.9, 0.8, 0.7,, 0.1}. The applied settings are: $N_{path}$ = 20000, $N_{hop}$ = 6, $N_{top}$ = 1000, d = 0.95 for FB15K-237-10%, $N_{path}$ = 5000, $N_{hop}$ = 5, $N_{top}$ = 500, d = 0.6 for FB15K-237-20%, $N_{path}$ = 1000, $N_{hop}$ = 4, $N_{top}$ = 100, d = 0.8 for FB15K-237-50%, $N_{path}$ = 10000, $N_{hop}$ = 6, $N_{top}$ = 100, d = 0.5 for NELL23K, and $N_{path}$ = 20000, $N_{hop}$ = 6, $N_{top}$ = 100, d = 0.2 for WD-singer."}, {"title": "5.3 Results of Sparse KG Completion", "content": "The experimental results are presented in Tables 2 and 3. For the experiments on ChatGPT, we focus on FB15K-237-50%, which has the largest test set among the FB15K-237 series, NELL23K, and WD-singer. As we can only get Hits@1 by evaluating the accuracy of the responses from ChatGPT, we adopt only Hits@1 following LambdaKG [36]. For the other baselines, we utilize all the five datasets and follow Refs. [11, 14, 32] to apply MRR and Hits@{3, 10} as metrics. From the results, we have the following observations:\nCompared with rule-based and path-based models, LoGRe outperforms them (except for Hi-KnowE [35]), particularly on the challenging dataset NELL23K, where the state-of-the-art baselines gain the worst performance among the five datasets. LoGRe significantly improves all the metrics on NELL23K, 0.056 on MRR (27.6% relative improvement), 5.70% on Hits@3 (25.7% relative improvement), and 7.80% on Hits@10 (23.0% relative improvement). LoGRe is better or comparable than much more intricate Hi-KnowE [35], which relies on AnyBURL [16], ConvE [6], and Transformer [29]. This validates the effectiveness of LoGRe looking inward globally. Additionally, state-of-the-art path-based models perform better than state-of-the-art rule-based ones. The poor performance of rule-based models may be due to the large number of truncated paths in sparse KG, which prevents these models from effective rule mining. Whereas, path-based models have more flexibility in exploring KG rather than relying on strict rules. This enables them to consider various possibilities for reaching the correct tail entities. LoGRe further aggregates and evaluates the paths and candidate tail entities from a global perspective. Thus, LoGRe is more capable of helping the head entities select more high-quality paths to arrive at the correct tail entities than path-based baselines."}, {"title": "5.4 Ablation Study", "content": "LoGRe has two stages, with the construction of the type-specific reasoning schema in stage 1 and stage 2 (path reasoning) being essential. Thus, we remove the construction of the cross-type reasoning schema in stage 1 to evaluate its effectiveness. This ablation is denoted as LoGRe(-cross_type). Besides, LoGRe prefers short ones via hop decay and considers the answer similarity. To study their contributions to the performance, we remove one of them each time, denoted as LoGRe(-prefer_shortpath) and LoGRe(-answer_similarity), respectively. The experimental results are reported in Table 5.\nBased on the results, we observe that removing any of them reduces the effectiveness of LoGRe, which confirms the necessity of all the three designs. Particularly, removing the construction of the cross-type reasoning schema significantly decreases the performance. It underscores the importance of shifting common relations to the cross-type group and aggregating their paths and scores globally. Additionally, not giving preference to short paths on WD-singer leads to a more obvious drop in performance compared to the other datasets. It is reasonable, as WD-singer is specific to the singer domain, while the other datasets contain general facts. Specific WD-singer may have simpler relation patterns among entities, and when there are high-quality short paths, long paths are less important. This observation aligns with the small optimal hop decay factor, i,e, 0.2, as introduced in Section 5.2.2. In contrast, on general-domain datasets like the FB15K-237 series, long paths are crucial for reasoning out the answer across entities of diverse types."}, {"title": "5.5 Case Study", "content": "To illustrate the explainability of LoGRe, we conduct case study on WD-singer, following DacKGR [14]. Two representative examples, (Louise Kirkby Lunn, citizenship, ?) and (Carlo Scattola, participant in, ?) are selected from the test set. The first one is about person's basic information, also selected in DacKGR [14] and the second one relates to person's activities. We find that the corresponding correct tail entities United Kingdom and La Cenerentola o sia La"}, {"title": "5.6 Hyper-parameter Analysis", "content": "The hyper-parameters of LoGRe are the maximum number of collected paths, maximum number of path hops, number of explored top paths, and hop decay factor. We thoroughly analyze these hyperparameters to evaluate their impacts on the performance. Without loss of generality, we also conduct the analysis on WD-singer. The results are exhibited in Figure 3.\nIt can be observed from the first and third subfigures of Figure 3 that the maximum number of collected paths and number of explored top paths impact the performance of WD-singer slightly. Notably, 20000 and 100, respectively, yield marginally superior results on MRR. Concerning the second subfigure, the performance improves on all the metrics as the maximum number of path hops increases. When the maximum number of path hops is 6, LoGRe achieves the best performance. As illustrated in the fourth subfigure, a decrease in the hop decay factor leads to performance improvement. The optimal value is 0.2, indicating a substantial penalization of long paths. Overall, the performance variations of different parameter values are limited, demonstrating the robustness of LoGRe."}, {"title": "5.7 Complexity Analyses", "content": "Model complexity comprises space and time aspects. We compare LoGRe with the best baselines for these two dimensions. In terms of space complexity, LoGRe only necessitates storage for the relation-path reasoning schema of space complexity O(|E|$N_{path}N_{hop}$). As presented in Table 7, it generally occupies more space compared to excellent embedding-based KG completion models TuckER [2], ConvE [6], InteractE [28], and NBFNet [39]. However, it stands out as the most space-efficient option when compared to the state-of-the-art sparse KG completion models SparKGR [32], DT4KGR [33], and Hi-KnowE [35], which rely on external assistance. These models typically require embeddings for all entities and relations, parameters for neural networks, and storage space for rules."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we looked inward rather than seeking external help for the first time in the line of path-based sparse KG completion models. We proposed a two-stage model LoGRe, which constructs a relation-path reasoning schema by globally analyzing the training data to alleviate the sparseness problem and then aggregates paths to reason out answers. Experiments on five benchmark sparse KG datasets demonstrate that LoGRe outperforms rule-based and path-based baselines. It is also comparable to the referred embedding-based models and significantly superior to ChatGPT. Further comprehensive analyses substantiate the effectiveness and efficiency of LoGRe.\nThis paper looks back to the path-based research line and proposes an explainable model, in contrast to the prevailing trend of embedding-based models and large language models. We hope to inspire research endeavors that prioritize explainability, lacking in the current research landscape. In the future, we intend to enhance LoGRe by introducing external models and knowledge, such as harnessing the natural language understanding capabilities of large language models as mentioned in Section 5.3, while striving to preserve a high degree of explainability."}]}