{"title": "The Role of Generative Systems in Historical Photography Management: A Case Study on Catalan Archives", "authors": ["Eric Sanchez", "Adri\u00e0 Molina", "Oriol Ramos Terrades"], "abstract": "The use of image analysis in automated photography management is an increasing trend in heritage institutions. Such tools alleviate the human cost associated with the manual and expensive annotation of new data sources while facilitating fast access to the citizenship through online indexes and search engines. However, available tagging and description tools are usually designed around modern photographs in English, neglecting historical corpora in minoritized languages, each of which exhibits intrinsic particularities. The primary objective of this research is to study the quantitative contribution of generative systems in the description of historical sources. This is done by contextualizing the task of captioning historical photographs from the Catalan archives as a case study. Our findings provide practitioners with tools and directions on transfer learning for captioning models based on visual adaptation and linguistic proximity.", "sections": [{"title": "1 Introduction", "content": "The task of image captioning aims to produce a coherent sentence that describes the relationships and context of the visual attributes of an image. Incorporating image captioning models in historical management could offer advantages in human-hours of manual annotations in recognizing the elements of the scene and, consequently, causing delays in public access and indexing of the content.\nIn this work, we explore the use case of the management of multimedia heritage by the Xarxa d'Arxius Comarcals (XAC, Catalan for Network of County Archives). In order to grant access to the citizenship with all the correctly tagged archives on their public access portal\u00b3, extensive research and writing by the\nThus, this work focuses on adapting image description generation models to specific historical and linguistic contexts. By using temporally adapted synthetic datasets and pretrained models in languages with more resources, we aim to improve the accuracy and relevance of descriptions of captioning systems. This research also utilizes a large amount of data from multiple languages to train the models, ensuring broader coverage. Ultimately, it ablates on the role of synthetic visual and text features in bridging the gap of historical and linguistically biased distributions in the context of the Catalan county archives."}, {"title": "2 Related Work", "content": "Image captioning is a descriptive task that integrates computer vision and natural language processing to generate textual descriptions of images. The first deep learning models designed for the task of image captioning [1,15,34] consisted of a CNN [17] to extract image features and the use of an RNN [26] or LSTM [14] for text generation. Later, attention mechanisms and Transformers [32] emerged, providing additional efficiency in feature extraction from images and especially in text generation. A different perspective on the task of associating images with text is provided by [12] and, later on, CLIP [23], which projects images and texts into a common vector space where similar pairs are closer than dissimilar ones, thereby allowing the calculation of a proximity value between them based on distance. This is useful for assessing the quality of model predictions and training data.\nOne of the main challenges in working with historical images is temporal and stylistic variability. The Vision Transformer (ViT) architecture [8] has proven effective in computer vision tasks, but adaptation to historical images requires additional techniques such as weight adaptation with specific data and data augmentation. Additionally, some authors suggest that the generation of synthetic datasets of historical images provides a robust basis for training models when real data is scarce [2]. Capturing the intrinsic properties of historical photographs is a necessary condition for synthetic data generation. In this study, we will address the convenience of such approaches with respect to text-based generation.\nAdditionally, recent trends on multi-modal large language modeling [37] showed an excelling performance in downstream tasks, including multi-lingual vision and text reasoning [10]. However, this case of study will neglect the existence of LLMs, as they support an unreasonable economical and environmental burden that heritage and governmental institutions, such as XAC, cannot afford as a means for automation (see Section 4.2, where the additional cost of text processing in non-English languages due to sub-efficient language processing is empirically assessed).\nIn the multilingual context, Transformers like mBERT (Multilingual BERT) [7] and XLM-R [4] have been widely used to generate descriptions in multiple languages. These models demonstrate that training in multiple languages helps improve performance in languages with fewer training samples. Moreover, adapting the weights for specific languages allows maintaining a smaller model size, as it does not need to capture information for all languages simultaneously, but can be pretrained on resource-rich languages and transfer the information to"}, {"title": "2.2 The Xarxa d'Arxius Comarcals", "content": "As it has already been introduced, this work contextualizes image captioning as a case study for the integration of descriptive systems in automated historical photography management. The Xarxa d'Arxius Comarcals is a cultural heritage institution that dates back to 1932 [35]. The function of regional archives is the conservation, management, and dissemination of documentary heritage from its original environment, thus avoiding provincial concentrations.\nTheir first steps towards the integration of digital humanities in their strategy and development, started in 2005 [5] by extracting information from francoist border files in an effort for automated recovery of victims during such historical periods. After that pioneering collaboration, the XAC reprised their collaboration, now in a regular basis, of extracting information from historical census records [9], aiming to reconstruct and model the relationships and demographic behaviors of our ancestors. Other projects involved extractive tasks, such as named entity recognition, on marital census [24,25], serving the same purpose. In previous applications, the specificity of the data and the use of fixed-structure sentences served as a proxy to facilitate information extraction (see [25]). In this work, we explore the feasibility of XAC transitioning from extractive to descriptive tasks in digital humanities applications. This shift involves addressing challenges associated with an open-domain Catalan vocabulary, for which descriptive systems are not widely available, and working with photographs ranging from the 19th to the early 21st century."}, {"title": "3 Architecture", "content": "In this section, we introduce the technical details of the implementation on which we will conduct our study. Since LLMs are not well suited for heritage and government institutions, both from an economical and environmental perspective, we seek compact models that can fit on a reasonable scale.\nThe Image Captioning model used as the basis for the project is called CATR (CAption Transformer) \u2074. It consists of a Convolutional Neural Netowork (CNN) followed by a vision transformer block in the encoder and a textual Transformer in the decoder part. The main difference between the encoder part of this model\nand the Visual Transformer [8] is the way it obtains the embedding of the input image.\nFirst, instead of splitting the image into patches, the image is processed by a ResNet [13] and the output features are used along with their positional embedding. The visual features are contextualized through an encoder (visual transformer) and, secondly, this feature conditions an autoregressive transformer decoder. This decoder produces tokens given the previous state and conditioned to a set of visual features. The generated tokens are projected into the vocabulary size through a multi-layered perceptron (MLP)."}, {"title": "4 Datasets", "content": "In this section, we introduce the experimental setup that this study will follow to quantitatively assess the contribution of both text and image generative systems in adapting captioning models to historical and Catalan data from XAC. In Section 5.1, we will introduce and analyze the different datasets used to train and fine-tune our models, including the set of XAC images for this case study and the generation of synthetic datasets that will support this research endeavor. Lastly, in Section 4.3, we provide a brief qualitative image assessment on the synthetically generated images."}, {"title": "4.1 Benchmark Datasets", "content": "In this study, we leverage several datasets serving different purposes. First, we consider COCO [18] as one of the best established research datasets for modern image captioning. COCO will serve as the main comparative mechanism and baseline, as its size and data quality are on a sweet spot for research and reproducibility in every laboratory and institution. Secondly, we leverage the Ducth language from CrossModal3600 [31].\nIn our case study, ~30K XAC images have been yielded to our research as a preliminary extraction for studying the feasibility of incorporating automated description systems into their production scheme.\nAs it can be seen in Figure 3 the XAC collection contains captions with a high presence of named entities (names, locations, etc.). This is due to the historical relevance of the institution in categorizing and preserving heritage at the finest granularity. Because named entities are usually arbitrary pieces of information rarely associated with recognizable patterns in images, this is avoided in curated research datasets such as COCO.\nThe incorporation of named entities in captioning systems is a widely explored field [3, 21, 38] that usually requires external knowledge to be solved. Therefore, in the XAC dataset we apply the pre-processing step of named entity masking using the Catalan transformer model from Spacy \u2075. The detected named entities are then masked with [PER], [LOC], [ORG] and [MISC] regarding their detected labels (person, location, organization, and miscellany).\nAdditionally, the XAC dataset contains historical and Catalan data, which promotes important performance gaps when fine-tuning from the previously introduced datasets containing modern images with, non-Catalan captions. Therefore, in this work we aim to explore the quantitative contribution of both image and text generation models in bridging this gap."}, {"title": "4.2 Synthetically Generated Dataset", "content": "Synthetic data is created with the aim of having a set of images halfway between the COCO and XAC datasets in terms of historical domain, that is, images artificially adapted to an older temporal domain. To generate these images, the Stable Diffusion XL model [22], a text-to-image generation model, was used, taking the COCO descriptions as a reference. The strategy followed to obtain these images in a different historical domain was to use each of the COCO descriptions (one description per image, not all descriptions) as input to the model with an addition of the year we want the image to represent, specifically: <caption> in <year>.\nThe selected years are, as a controlled set, the same as those present in the Date Estimation in the Wild dataset [20], from 1930 to 1999. The years are randomly chosen following a uniform distribution $U \\sim (1930, 1999)$. This will help us explore how fine-tuned models behave on unseen years during training with respect to this set."}, {"title": "4.3 Image Generation Qualitative Assessment", "content": "The Stable Diffusion XL model [22] is trained on the LAION 5B dataset [28] and has certain biases from both the data it was trained on and the model itself. The model has difficulties in representing faces and limbs of people, and sometimes some objects interact strangely with each other (see Figures 5 and 6). The images generated with this noise complicate the task of recognition and subsequent description by the image description generation model.\nTo identify these biases, a test was carried out. The goal is to identify the objects or attributes that are most often incorrectly labeled and to conduct a qualitative study on whether this is due to biased image generation or a deficiency in the image description generation model.\nA model was trained using the generated images and the associated original COCO descriptions, which was then used to generate predictions for the descriptions of the validation partition. For each generated description, the loss function relative to the actual description and all the tokens appearing in it were recorded. Finally, for each token, the average loss function values of all samples where it appears were calculated (see Figure 7). Below are the 28 words with the highest average loss function values.\n{suggests, immediate, personi, closest, similarly, benefit, Barclay, institution,\nthirty, operates, Lower, centered, Outside, Cuba, whom, coins,\nhigher, protein, Away,\nvictim, trouble, headquartered, danger, assault, bordered,\nwritings, services, Silk}\nFrom the above list, several semantic relationships can be observed in certain words:\nCompositional relationships: the words immediate, closest, Lower, centered, Outside, higher express compositional qualities of the entities represented in the original image. These relationships are lost in the image generation process, and consequently, the description generation model does not observe these relationships.\nAbstract concepts: the terms suggests, similarly, benefit, Cuba, victim, services deal with concepts that are not explicitly representable but must be inferred with additional context or by interpreting more complex relationships between the entities that appear. This interpretation is difficult for the generator model to perceive and subsequently represent, so this information is lost.\nBrands and materials: the words Barclay, Silk may not be well represented in the generated image for various reasons; in the case of the brand, due to the diffusion model's lack of knowledge of its existence or the difficulty in generating a logo seen in few training samples. On the other hand, materials like silk require high image quality to be clearly distinguishable, which is not present in either the generated image or the processed image received by the generator model."}, {"title": "5 Experiments and Results", "content": "In this section we perform a deep discussion on the results of our case study. Here, we pose asses the initial research questions regarding image (RQ1) and text (RQ2) generative systems contribution to domain adaptation and sensitivity of language models to cognitive lexical rules (RQ3).\nIn Section 5.2 we discuss the interplay between RQ1 and RQ2, on which we draw the line through vision and language through an ablation study on generated pretraining data for both modalities. In Section 5.3, we propose a comparison of transferring knowledge from source and target languages via language clusters based on lexical rule matching as a means to address RQ3."}, {"title": "5.1 Experimental setup", "content": "As it was previously introduced during Section 3, this work aims to explore its hypothesis within the context of reasonable-scale descriptive visual systems. Therefore, all experiments, training, and inference are conducted on single NVIDIA Titan Xp GPUs, CUDA Version: 11.2, with a maximum training time of 4 days.\nThe defined hyperparameters of the model can be found at 2, where a brief description of each variable and its selected value is defined."}, {"title": "5.2 Historical Domain Adaptation", "content": "Upon inspection of Table 3, several notable trends emerge. Firstly, the use of synthetic captions proves to be a dominant strategy in knowledge transfer during the fine-tuning step, unlike the case with synthetic images. Secondly, while the use of synthetic and historically adapted images shows advantages over modern photographs even without automatic caption translation, the accumulation of noise from both synthetic approaches leads to a decrease in CIDEr and BLEU4 scores compared to the top-performing strategy: using modern images with synthetic captions (RQ2). This behavior suggests that much of the error in historical captioning systems lies within the language modeling rather than the visual encoder, which effectively generalizes its features from modern photographs. Nevertheless,"}, {"title": "5.3 The Role of Language", "content": "Following [33], we consider in Figure 10 two clusters of languages. First, we synthetically generate captions in Spanish (es) and Italian (it) as higher resource languages that should alleviate the scarcity of historical Catalan (cat) sources. In order to contrast this hypothesis of language modeling operating proportionally to ling\u00fcistic proximity, we pose a similar challenge with the Ducth (nl) language, which is clustered to English and German (de)."}, {"title": "6 Conclusions and Further Work", "content": "This work deepens into the role of image and text generation in adapting descriptive systems to unseen historical and linguistic domains. Framed within the context of Catalan historical archives, this research highlights several key findings:\nBias in Pretraining: Although some positive bias is introduced during the pretraining stage using image generation systems, it proves more advantageous to use natural images with translated captions. This is likely due to the accumulated noise from both generative approaches during the training stage.\nHistorical Context Sensitivity: Current image generation models are not capable of introducing fine-grained historical cues. The dependence of historical photographs on contextual attributes exacerbates the gap between text and image generation, leading to less effective captioning.\nLanguage Proximity: Image captioning models are quantitatively sensitive to language proximity. These models show higher performance when pretrained on similar languages, even when synthetic captions are used, indicating the importance of linguistic similarity in pretraining datasets.\nIn summary, the combination of natural images and translated captions, attention to historical context, and leveraging language proximity are critical for improving the performance of descriptive systems in historical and linguistically diverse archives.\nHowever, this work is limited by the reliance of generative systems in order to maximize the performance of the image captioning task. This leads to bias, which ties the final performance to the ability of the generative system to capture reality. We highlight the necessity of exploring further domain adaptation techniques (such as multi-task learning) as model-independent alternatives.\nAdditionally, measuring the impact of such models in the context of Catalan archives is crucial to proving that all the efforts pursued by the digital humanities field are worthwhile. This could be extended to other minoritized languages in a wider study, which would require the coordinated effort of a significant number of heritage institutions and technological centers."}]}