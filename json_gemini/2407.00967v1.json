{"title": "DEEP LEARNING FOR AUTOMATED DETECTION OF BREAST CANCER IN DEEP\nULTRAVIOLET FLUORESCENCE IMAGES WITH DIFFUSION PROBABILISTIC MODEL", "authors": ["Sepehr Salem Ghahfarokhi", "Tyrell To", "Julie Jorns", "Tina Yen", "Bing Yu", "Dong Hye Ye"], "abstract": "Data limitation is a significant challenge in applying deep\nlearning to medical images. Recently, the diffusion prob-\nabilistic model (DPM) has shown the potential to generate\nhigh-quality images by converting Gaussian random noise\ninto realistic images. In this paper, we apply the DPM to aug-\nment the deep ultraviolet fluorescence (DUV) image dataset\nwith an aim to improve breast cancer classification for intra-\noperative margin assessment. For classification, we divide\nthe whole surface DUV image into small patches and ex-\ntract convolutional features for each patch by utilizing the\npre-trained ResNet. Then, we feed them into an XGBoost\nclassifier for patch-level decisions and then fuse them with\na regional importance map computed by Grad-CAM++ for\nwhole surface-level prediction. Our experimental results\nshow that augmenting the training dataset with the DPM sig-\nnificantly improves breast cancer detection performance in\nDUV images, increasing accuracy from 93% to 97%, com-\npared to using Affine transformations and ProGAN.", "sections": [{"title": "1. INTRODUCTION", "content": "Deep ultraviolet fluorescence scanning microscopy (DUV-\nFSM) provides rapid whole-surface imaging of dissected\ntissue during breast-conserving surgery without the need for\ninvasive techniques or excessive sectioning. DUV images are\nparticularly helpful in identifying cancer cells at the surgical\nspecimen's edge (margin), thanks to their clear color and\ntexture differences from healthy tissue. Then, an automated\nbreast cancer detection in DUV images is required for intra-\noperative margin assessment. Deep learning-based methods\nhave shown potential in medical image classification, but\nthey often face due to their reliance on extensive training\ndata [1],[2]. This challenge is especially notable in the clas-\nsification of DUV images with a limited number of subjects,\ngiven its novelty [3].\nData augmentation techniques are used to boost the med-\nical image dataset. A widely used augmentation technique\nis Generative Adversarial Networks (GAN) introduced by\nGoodfellow et. al. [4], with its later variants being the most\ncommon models for creating synthetic images. For example,\nSH Gheshlaghi et. al.[5] employed an Auxiliary Classifier\nGenerative Adversarial Network (ACGAN) to augment a\nsmall dataset with realistic images and class labels, specif-\nically focusing on breast cancer histopathological image\nclassification. Nevertheless, GANs tend to capture a lower\ndegree of diversity in generated content when compared to\ncontemporary likelihood-based models [6],[7],[8]. Addition-\nally, GANs can be challenging to train, often susceptible to\nissues such as mode collapse, which can be mitigated through\nmeticulous hyperparameter selection and the application of\nsuitable regularizers [9],[10].\nTo tackle these challenges, we apply the diffusion proba-\nbilistic model (DPM) in data augmentation to generate realis-\ntic and diverse DUV images. DPM has recently surfaced as a\npotent generative model, positioning itself as a potential sub-\nstitute for GANs [11]. The DPM harnesses cross-attention\nand adaptable conditioning to facilitate the creation of de-\nsired images. Commencing with Ho et. al.[12], a series of\nstudies have demonstrated that DPMs have the ability to pro-\nduce high-fidelity images akin to those produced by GANs\n[7],[13]. These models offer a range of advantageous qual-\nities for image synthesis, including stable training. [14] uti-\nlized a DPM for the synthesis of histopathology images and\ncompared it with ProGAN. Generative metrics demonstrated\nthe superiority of the diffusion model with respect to data aug-\nmentation [15],[16]. In this paper, we adopt the DPM to en-\nhance deep learning-based breast cancer detection in DUV\nimages. The key contributions of this paper can be summa-\nrized as follows:\n\u2022 DPMs are employed to generate authentic DUV images,\nrepresenting the initial use of this application.\n\u2022 DUV breast cancer detection is enhanced through DPM's\naugmented training, surpassing GAN performance."}, {"title": "2. METHOD", "content": "This section outlines our utilization of the DPM in the con-\ntext of breast cancer classification in DUV images, as de-\nscribed in Figure 1. The pivotal role of the DPM lies in its\ncapacity to generate synthetic DUV patch images, addressing\nthe challenge of limited training data. Subsequently, the aug-\nmented dataset is employed to extract convolutional features\nusing a pre-trained ResNet50 network. Ultimately, the patch-\nlevel classification is executed with the aid of the XGBoost\nclassifier and fused into a whole-surface-level decision with\na regional importance map. The ensuing sections will delve\ninto each of these steps in greater detail."}, {"title": "2.1. Diffusion Probabilistic Model", "content": "Diffusion probabilistic models (DPM) fall under the category\nof generative models, aiming to produce data resembling their\noriginal training data. These models function by iteratively\nintroducing random noise to the training data (forward dif-\nfusion) and subsequently learning to eliminate this noise (re-\nverse diffusion). Once trained, the DPM can generate new\ndata by applying random noise through the learned process\nthat eliminates the noise. We apply DPM to augment the\ntraining dataset by considering DUV patch images as the in-\nput, denoted as x. Specifically, a DUV WSI for a sample i is\ndivided into multiple DUV patches where each sample's field\nof view N\u2081 is the union of non-overlapping patches such\nthat \u03a9\u2081 = 1, and \u2229\u2229\u2229 = \u00d8 for \u2200k, l.\nN\nThe DPM includes two steps: forward and reverse diffu-\nsion. It's important to note that our patch images are catego-\nrized into two distinct class labels (type-embed) represented\nby c, which c \u2208 {Benign, Malignant}. In the forward dif-\nfusion step, we add random Gaussian noise to our data repeat-\nedly, for a certain number of times called T (t-embed) until it\nreaches the desired complex data points distribution. If we la-\nbel the data distribution for our input as q(x0.), the forward\nprocess defined as following steps:\n$q(x_{t,c}|x_{t-1,c}) = N(x_{t-1,c}\\sqrt{1 - \\beta_t}, I\\beta_t)$, (1)\nwhere \u03b2t \u2208 (0, 1) is noise scales. By using reparametriza-\ntion xt can be expressed as a linear combination of 20 and a\nGaussian noise variable \u025b = N(0, I):\n$x_{t.c} = \\sqrt{\\alpha_t}x_{0.} + \\sqrt{1 - \\alpha_t}\\epsilon$, (2)\n$\\alpha_ = \\prod_{t=1}^T 1 - \\beta_t$. (3)\nFor the reverse diffusion step, we want to generate a sam-\nple from q(xt-1,c|xtc). Since q(xt\u22121,c|xt,c) is an unknown\ndistribution, we train a neural network po(Xt\u22121,c|Xt.c, at) to\napproximate it. To generate a random sample in the reverse\ndiffusion, the latent variable xt,c should approximately fol-\nlow an isotropic Gaussian distribution. This implies that key\nvariables, including at, need to be very close to zero, and \u1e9et\nshould also have a small value, ensuring that xt. ~ N(0, I).\nThe network for pe has a similar role to the decoder network"}, {"title": "2.2. Deep learning classification of DUV images", "content": "Given generated patch DUV images in two labels, represented\nby xo, we add them to our training data to improve our breast\ncancer detection. Employing the deep learning-based breast\ncancer classification method for DUV images [19], N DUV\npatches, consisting of both generated and original patch DUV\nimages (denoted as p, j = {1, ..., N}), are categorized be-\nnign (-1) or malignant (+1). Features are extracted using\nthe final layer of a pre-trained ResNet50 [17], and an XG-\nBoost classifier [20] assigns a binary output y \u2208 {\u22121,+1}\nto each patch p. Additionally, Grad-CAM++[21] on the pre-\ntrained DenseNet169 model calculates the regional impor-\ntance map r for each DUV patch by taking the average rele-\nvance value over a patch's region ? [19]. Finally, a decision\nfusion method is employed to determine the WSI-level classi-\nfication label Li \u2208 {\u22121, +1} based on the patch-level classi-\nfication labels y for all patches j = {1, ..., m}. Toward this,\nwe define the weight w for each patch p as the thresholded\nregional importance value r.\n$w = \\begin{cases}\n0 & \\text{if } r < 0.25 \\\\\n1 & \\text{otherwise}\n\\end{cases}$ (6)\nThis weighting scheme neglects patches with low importance\nfor either malignant or benign conditions in the fused deci-\nsion for the DUV WSI. Then, the weighted majority voting\nis employed to determine the WSI-level classification label\nLi \u2208 {\u22121, +1}.\n$L_i = sign(\\sum_{j=1}^m w_j^*)$ (7)\nwhere sign() is the sign function to map positive (malignant)\nand negative (benign) values to -1 and +1, respectively. It is\nworth noting that the DPM-augmented dataset mitigates the\nrisk of overfitting."}, {"title": "3. EXPERIMENT", "content": "In this study, we employed the Diffusion Probabilistic Model\n(DPM) to enhance our breast cancer DUV image classifica-\ntion. The training dataset is augmented with 1000 synthesized\npatches, evenly distributed between benign and malignant la-\nbels. These images, along with disease labels, seamlessly\naugment the training dataset without additional input from\npathologists, addressing the limitations of small datasets. For\ncomparison, we used the traditional affine transform (rotation\nand flip) and ProGAN[22] to boost the DUV patch training\ndataset. It is important to mention that we trained separate\nProGAN networks for the benign-only and malignant-only\ndatasets to ensure the automatic assignment of labels to gener-\nated images, as ProGAN does not incorporate label informa-\ntion. We evaluated the results in two parts: Visual Inspection\nand Classification Performance."}, {"title": "3.1. Dataset", "content": "The breast cancer dataset consists of DUV images from 60\nsamples (24 normal/benign and 36 malignant). This DUV\ndataset was collected from the Medical College of Wisconsin\n(MCW) tissue bank (4) with a custom DUV-FSM system. The\nDUV-FSM used a deep ultraviolet (DUV) excitation at 285\nnm and a low magnification objective (4X), which achieved\na small spatial resolution from 2 to 3 mm. To enhance flu-\norescence contrast, breast tissues are stained with propidium\niodide and eosin Y. This technique produces images of the mi-\ncroscopic resolution, sharpness, and contrast from fresh tissue\nstained with multiple fluorescence dyes. Following the ex-\ntraction of patches from these images, the dataset comprises\n25,024 patches from normal/benign cases and 9,444 patches\nfrom malignant cases for training our diffusion model."}, {"title": "3.2. Visual Inspection", "content": "Our examination involves visual comparison of the DUV\npatches generated through DPM with those produced by the\nproposed ProGAN, as depicted in Figure 2. DPM excels in"}, {"title": "3.3. Classification Performance", "content": "For a quantitative assessment, we gauged the classification\nperformance of our method by juxtaposing it with the Affine\nTransform and ProGAN approaches when integrating synthe-\nsized images from each into our original dataset. Table 1\npresents the outcomes of the 5-fold cross-validation for clas-\nsification performance, underscoring the efficacy of our pro-\nposed method (DPM). It substantially enhanced accuracy to\n(97%), achieving noteworthy sensitivity (97%) and specificity\n(93%). In contrast, ProGAN failed to enhance classification\nperformance compared to the value of Affine Transform about\n(93%) of accuracy. Importantly, it should be noted that while\nWSI-level accuracy remains consistent, there are variations\nin the patch-level classification results for Affine Transform\nand ProGAN. These findings underscore the resilience of our\nproposed method, in stark contrast to the over-fitting issues\nfaced by the Affine Transform and ProGAN. Noteworthy is\nthe observation of remarkably high sensitivity and specificity,\nhighlighting the benefits of our approach in intra-operative\nmargin assessment. This suggests its potential to significantly\nmitigate the risk of cancer recurrence by minimizing the prob-\nability of surgeons misidentifying breast cancer margins in\ndissected tissue. Considering that separate ProGAN networks\nwere trained for benign-only and malignant-only datasets due\nto its inability to directly incorporate label information, this"}, {"title": "4. CONCLUSION", "content": "This study presents a compelling solution to the challenge\nof limited data in deep learning applications for medical im-\nage analysis, particularly in breast cancer classification of\nDUV images. Utilizing a Diffusion Probabilistic Model, the\nresearch effectively generates synthetic DUV patch images,\nthereby augmenting the dataset and improving the perfor-\nmance of deep learning models. The approach emphasizes\ndiverse morphology levels during image synthesis, resulting\nin a dataset of 1000 patch images that encompasses both be-\nnign and malignant cases. Quantitative results demonstrate\na significant enhancement in performance, with accuracy,\nsensitivity, and specificity reaching 97%, 97%, and 93%, re-\nspectively. This highlights the approach's potential to greatly\nimprove breast cancer detection. The study underscores the\nefficacy of data augmentation techniques in addressing data\nlimitations in medical image analysis, ultimately contributing\nto more accurate and robust diagnostic systems. However,\ndue to the unique and original nature of our dataset, which re-\nquires extensive review by pathologists for accurate labeling,\nfuture work will focus on expanding our dataset with more\nlabeled images to enhance our model's performance."}]}