{"title": "Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?", "authors": ["Zhongjian Zhang", "Xiao Wang", "Huichi Zhou", "Yue Yu", "Mengmei Zhang", "Cheng Yang", "Chuan Shi"], "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations, especially for topology attacks, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attack. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph neural networks (GNNs), as representative graph machine learning methods, effectively utilize their message-passing mechanism to extract useful information and learn high-quality representations from graph data [20, 35, 42]. Despite great success, a host of studies have shown that GNNs are vulnerable to adversarial attacks [18, 23, 29, 33, 40], especially for topology attacks [43, 54, 55], where slightly perturbing the graph structure can lead to a dramatic decrease in the performance. Such vulnerability poses significant challenges for applying GNNs to real-world applications, especially in security-critical scenarios such as finance networks [38] or medical networks [26].\nThreatened by adversarial attacks, several attempts have been made to build robust GNNs, which can be mainly divided into model-centric and data-centric defenses [52]. From the model-centric perspective, defenders can improve robustness through model enhancement, either by robust training schemes [7, 21] or new model architectures [17, 50, 53]. In contrast, data-centric defenses typically focus on flexible data processing to improve the robustness of GNNs. Treating the attacked topology as noisy, defenders primarily purify graph structures by calculating various similarities between node"}, {"title": "2 PRELIMINARIES", "content": "Here, a Text-attributed graph (TAG), defined as G = (V, &, S), is a graph with node-level textual information, where V = {01, ..., 0|v| },\n& = {1, ..., e|&| } and S = {1, ..., s|1v|} are the node set, edge set,\nand text set, respectively. The adjacency matrix of the graph G is\ndenoted as A \u2208 R|V|\u00d7|V|, where Aij = 1 if nodes vi and vj are\nconnected, otherwise Aij = 0. In this work, we focus on the node\nclassification task on TAGs. Specifically, each node vi corresponds\nto a label yi that indicates which category the node vi belongs\nto. Usually, we encode the text set S as the node feature matrix\nX = {x1,...,x|v|} via some embedding techniques [3, 11, 28] to\ntrain GNNs, where xi \u2208 Rd. Given some labeled nodes VL CV, the\ngoal is training a GNN f (A, X) to predict the labels of the remaining\nunlabeled nodes Vu = V \\ VL."}, {"title": "2.2 Graph Adversarial Robustness", "content": "This paper primarily focuses on stronger poisoning attacks, which can lead to an extremely low model performance by directly modifying the training data [53, 54]. The formal definition of adversarial robustness against poisoning attacks is as follows:\n$\\max_{\\delta \\in \\Delta} \\min_{\\theta} L(f_{\\theta} (G + \\delta), y_T),$                                                                                                                                                     (1)\nwhere 8 represents a perturbation to the graph G, which may include perturbations to node features, inserting or deleting of edges, etc., A represents all permitted and effective perturbations. yT is the node labels of the target set T. L denotes the training loss of GNNs, and @ is the model parameters of f. Equation 1 indicates that under the worst-case perturbation 8, the adversarial robustness of model f is represented by its performance on the target set T. A smaller loss value suggests stronger adversarial robustness, i.e., better model performance. In this paper, we primarily focus on the robustness under two topology attacks: 1) Targeted attacks [54], where attackers aim to mislead the model's prediction on specific"}, {"title": "3 THE ADVERSARIAL ROBUSTNESS OF GNNS COMBINING LLMS/LMS", "content": "In this paper, we empirically investigate whether LLMs enhance or weaken the adversarial robustness of GNNs to a certain extent. Specifically, for the Cora [27] and PubMed [32] datasets, based on non-contextualized embeddings encoded by BoW [11] or TF-IDF [31], we employ Mettack [55] with a 20% perturbation rate to generate attack topology. We compare seven representative baselines: TAPE [12] utilizes LLMs to generate extra semantic knowledge relevant to the nodes. OFA [24] employs LLMs to unify different graph data and tasks, where OFA-SBert utilizes Sentence Bert [30] to encode the text of nodes, training and testing the GNNs on each dataset independently. OFA-Llama2-7B involves training a single GNN across the Cora, Pubmed, and OGBN-Arxiv [13] datasets. Following the work [3], GCN-Llama2-7B, GCN-e5-large, and GCN-SBert represent the use of Llama2-7B, e5-large, and Sentence Bert as nodes' text encoders, respectively. The vanilla GCN directly utilizes non-contextual embeddings. By reporting the node classification accuracy on Vtest to evaluate the robustness of models against the Mettack. The implementation details of baselines refer to Appendix B. The result is depicted in Figure 1. We observe that under the influence of Mettack, GNNs combining LLMs/LMs"}, {"title": "4 LLM4RGNN: THE PROPOSED FRAMEWORK", "content": "In this section, we propose a novel LLM-based robust graph structure inference framework, LLM4RGNN. As shown in Figure 2, LLM4RGNN distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an edge predictor for finding missing important edges, so as to recover a robust graph structure, making various GNNs more robust."}, {"title": "4.1 Instruction Tuning a Local LLM", "content": "Given an attacked graph structure, one straightforward method is to query powerful GPT-4 to identify malicious edges on the graph. However, this method is extremely expensive, because there are |V|\u00b2 different perturbation edges on a graph. For example, for the PubMed [32] dataset with 19,717 nodes, the cost is approximately $9.72 million. Thus, we hope to distill the inference capability of GPT-4 into a local LLM, to identify malicious edges. To this end, instruction tuning based on GPT-4 is a popular fine-tuning technique [4, 44], which utilizes GPT-4 to construct an instruction dataset, and then further trains a local LLM in a supervised fashion. The instruction dataset generally consists of instance (instruction, input, output), where instruction denotes the human instruction"}, {"title": "4.2 Training an LM-based Edge Predictor", "content": "Now, given a new attacked graph structure A', our key idea is to recover a robust graph structure A. Intuitively, we can input each edge of A' into the local LLM and obtain its relevance score re. By removing edges with lower scores, we can mitigate the impact of malicious edges on model predictions. Meanwhile, considering that attackers can also delete some important edges to reduce model performance, we need to find and add important edges that do not exist in A'. Although the local LLM can identify important edges with higher relevance scores, it is still very time and resource-consuming with |V|\u00b2 edges. Therefore, we further design an LM-based edge predictor, as depicted in Figure 2 (b), which utilizes Sentence Bert [30] as the text encoder and trains a multilayer perceptron (MLP) to find missing important edges.\nFirstly, we introduce how to construct the feature of each edge. Inspired by [3], deep sentence embeddings have emerged as a powerful text encoding method, outperforming non-contextualized embeddings [11, 31]. Furthermore, sentence embedding models offer a lightweight method to obtain representations without fine-tuning. Consequently, for each node vi, we adopt a sentence embedding model LM as texts encoder to extract representations hi from the raw text si, i.e., hi = LM(si). We concatenate the representations of the node vi and vj as the feature for the corresponding edge.\nThen the edge label ye can be derived from re as follows:\n$Y_e = \\begin{cases} 1 & \\text{if } re > 4 \\\\ 0 & \\text{if } re \\leq 4 \\end{cases}$                                                                                                                                                                                                                                                                                                                                                                                                            (3)\nhere, we utilize the local LLM as an edge annotator to distill its inference capability, and select 4 as the threshold to find the most positive edges. It is noted that there may be a label imbalance problem, where the number of positive edges is much higher than the negative. Thus, based on the cosine similarity, we select some node pairs with a lower similarity to construct a candidate set. When there are not enough negative edges, we sample from the candidate set to balance the training set.\nNext, we feed the feature of each edge into an MLP to obtain the prediction probability \u0177e (vi, vj) = MLP(hi||hj). The cross-entropy"}, {"title": "4.3 Purifying Attacked Graph Structure", "content": "In Figure 2 (c), the robust graph structure \u00c2 is derived from the purification of A'. Specifically, new edges from &add will be added in A'. Simultaneously, with the relevance score re of each edge, we remove the malicious edges in A' by setting a purification threshold \u03b2, i.e., edges with re larger than \u1e9e are preserved, otherwise removed. The A is adaptive to any GNNs, making GNNs more robust."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 Experimental Setup", "content": ""}, {"title": "5.1.1 Dataset.", "content": "We conduct extensive experiments on four cross-dataset citation networks (Cora [27], Citeseer [6], Pubmed [32], OGBN-Arxiv [13]) and one cross-domain product network (OGBN-Products [13]). We report the average performance and standard"}, {"title": "5.1.2 Baseline.", "content": "First, LLM4RGNN is a general LLM-based framework to enhance the robustness of GNNs. Therefore, we select the classical GCN [20] and three robust GNNs (GAT [35], RGCN [53] and Simp-GCN [17]) as baselines. Moreover, to more comprehensively evaluate LLM4RGNN, we also compare it with existing SOTA robust GNN frameworks, including ProGNN [19], STABLE [22], HANG-quad [50] and GraphEdit\u00b9 [9], where GCN is selected as the object for improving robustness. More baseline introduction and implementation details refer to Appendix C.2 and C.3, respectively."}, {"title": "5.2 Main Result", "content": "In this subsection, we conduct extensive evaluations of LLM4RGNN against three popular poisoning topology attacks: non-targeted attacks Mettack [55] and DICE [40], and targeted attack Nettack [54], where we observe remarkable improvements of proposed LLM4RGNN in the defense effectiveness. We report the accuracy (ACC (\u2191)) on representative transductive node classification task. More results of inductive poisoning attacks refer to the Appendix D.1."}, {"title": "5.2.1 Against Mettack.", "content": "Non-targeted attacks aim to disrupt the entire graph topology to degrade the performance of GNNs on the test set. We employ the SOTA non-targeted attack method, Mettack [55], and vary the perturbation rate, i.e., the ratio of changed edges, from 0 to 20% with a step of 5%. We have the following observations: (1) From Table 1, LLM4RGNN consistently improves the robustness across various GNNs. For GCN, there is an average"}, {"title": "5.2.2 Against DICE.", "content": "To verify the defense generalization capability of LLM4RGNN, we also evaluate its effectiveness against another non-targeted attack, DICE [40]. Notably, DICE is not involved in the construction process of the instruction dataset. Considering that DICE is not as effective as Mettack, we set higher perturbation rates of 10%, 20% and 40%. The results are reported in Table 2 and Table 4. Similar to the results under Mettack, LLM4RGNN consistently improves the robustness across various GNNs and is superior to other"}, {"title": "5.2.3 Against Nettack.", "content": "Different from non-targeted attacks, targeted attacks specifically focus on a particular node v, aiming to fool GNNs into misclassifying v. We employ the SOTA targeted attack, Nettack [54]. Following previous work [53], we select nodes with a degree greater than 10 as the target nodes, and vary the number of perturbations applied to the targeted node from 0 to 5 with a step of 1, to generate attacked structures. The results are reported in Table 6 and Table 5. Similar to the results under the Mettack and DICE, LLM4RGNN not only consistently improves the robustness of various GNNs, but also surpasses existing robust GNN frameworks, exhibiting exceptional resistance to Nettack.\nIn summary, although we fine-tune the local LLM only based on TAPE-Arxiv23, LLM4RGNN still significantly improves the robustness of GNNs in both cross-dataset (Cora, Citeseer, PubMed, OGBN-Arxiv) and cross-domain (OGBN-Products) scenarios."}, {"title": "5.3 Model Analysis", "content": ""}, {"title": "5.3.1 Ablation Study.", "content": "To assess how the key components of LLM4RGNN benefit the adversarial robustness, we employ Mettack with a perturbation ratio of 20% to generate the attacked structure and conduct ablation experiments. Classical GCN is selected as training GNN. The experiment result is depicted in Figure 3, where"}, {"title": "5.3.2 Comparison with Different LLMs.", "content": "To evaluate the generalizability of LLM4RGNN across different LLMs, we choose four popular open-source LLMs, including Llama2-7B, Llama2-13B, Llama3-8B and Mistral-7B, as the starting checkpoints of LLM. We also introduce a direct comparison using the closed-source GPT-3.5 and GPT-4 as malicious edge detectors. Additionally, the metric AdvEdge (\u2193) is introduced to measure the number and proportion of malicious edges remaining after the LLM performs the filtering operation. We report the results of GCN on Cora and Citeseer datasets under Mettack with a perturbation ratio of 20% (generating 1053 malicious edges for Cora and 845 for Citeseer). As reported in Table 7, we have the following observations: (1) The well-tuned local LLMs are significantly superior to GPT-3.5 in identifying malicious edges, and the trained LM-based edge predictor consistently improves accuracy, indicating that the inference capability of GPT-4 is effectively distilled into different LLMs and edge predictors. (2) A stronger open-source LLM yields better overall performance. Among them, the performance of fine-tuned Mistral-7B and Llama3-8B is comparable to that of GPT-4. We provide a comprehensive comparison between Mistral-7B and Llama3-8B in Appendix D.3."}, {"title": "5.3.3 Efficiency Analysis.", "content": "In LLM4RGNN, using 26,000 samples to fine-tune the local LLM is a one-time process, controlled within 6 hours. The edge predictor is only a lightweight MLP, with training time on each dataset controlled within 1 minute. Furthermore, LLM4RGNN does not increase the complexity compared with existing robust GNNs framework [19, 22]. The complexity of LLM inferring edge relationships is 0(|8|), and the edge predictor is O(|V|2), and their inference processes are parallelizable. We provide the average time for LLM to infer one edge and for the lightweight edge predictor to infer one node in Table 8. Overall, the average inference time for the local LLM and the edge predictor is"}, {"title": "5.4 Hyper-parameter Analysis", "content": "We conduct hyper-parameter analysis on the probability threshold \u03bb, the maximum number of important edges K, the purification threshold \u1e9e, and the number of instances used in tuning LLMs.\nFirst, we present the accuracy of LLM4RGNN under different combinations of A and K in Figure 4. The results indicate that the accuracy of LLM4RGNN varies minimally across different hyper-parameter settings, demonstrating its insensitivity to the hyper-parameters A and K. More results are presented in Appendix D.2."}, {"title": "6 CONCLUSION", "content": "In this paper, we first explore the potential of LLMs on the graph adversarial robustness. Specifically, we propose a novel LLM-based robust graph structure inference framework, LLM4RGNN, which distills the inference capability of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, to efficiently purify attacked graph structure, making GNNs more robust. Extensive experiments demonstrate that LLM4RGNN significantly improves the adversarial robustness of GNNs and achieves SOTA defense result. Considering there are some graphs that lack textual information, a future plan is to extend LLM4RGNN to graphs without text."}, {"title": "A RELATED WORK", "content": ""}, {"title": "A.1 Adversarial Attacks and Defenses on Graph", "content": "It has been demonstrated in extensive studies [18, 23, 29, 40] that attackers can catastrophically degrade the performance of GNNS by maliciously perturbing the graph structure. For example, the Nettack [54] is the first study of adversarial attacks on graph data, which preserves degree distribution and imposes constraints on feature co-occurrence to generate small deliberate perturbations. Subsequently, the Mettack [55] utilizes the meta-learning while the Minmax and PGD [43] attacks utilize projected gradient descent, to solve the bilevel problem underlying poisoning attacks.\nThreatened by adversarial attacks, many methods [7, 8, 15, 21] have been proposed to defend against adversarial attacks. These methods can mainly be categorized into model-centric and data-centric. The methods of model-centric improve the robustness through model enhancement, either by robust training schemes (e.g., adversarial training [7, 21]) or designing new model architectures (e.g., RGCN [53], HANG [50], Mid-GCN[15]). The methods of data-centric typically focus on flexible data processing to improve the robustness of GNNs. By treating the attacked topology as noisy, defenders primarily purify graph structures by calculating various similarities between node embeddings [5, 19, 22, 41, 48]. For example, ProGNN [19] jointly trains GNN's parameters and learns a clean adjacency matrix with graph properties. STABLE [22] is a pre-training model, which is specifically designed to learn effective representations to refine graph quality. The above methods have received considerable attention in enhancing the robustness of GNNs."}, {"title": "A.2 LLMs for Graphs", "content": "Recently, Large Language models (LLMs) have been widely employed in graph-related tasks, which outperform traditional GNN-based methods and yield SOTA performance. According to the role played by LLMs in graph-related tasks, some methods utilize LLMs as an enhancer [12, 24, 47], where LLMs are used to enhance the quality of node features. Some methods directly utilize LLMs as a predictor [3, 14, 37, 49], where the graph structure is described in natural language for input to LLMs for prediction. Additionally, some methods employ LLMs as an annotator [4], generator [46], and controller [36]. Although GraphEdit [9] utilizes LLMs for graph structure learning, it focuses on identifying noisy connections in original graphs, rather than addressing adversarial robustness problem in graphs. In this paper, we adopt LLM as a defender, which utilizes LLMs to purify attacked graph structures, making GNNs more robust."}, {"title": "B IMPLEMENTATION DETAILS OF BASELINE IN SECTION 3", "content": "For all baselines, we use their original code and the default hyper-parameter settings in the authors' implementation. The sources are listed as follows:\n\u2022 TAPE: TAPE Repository\n\u2022 OFA-Llama2-7B/SBert: OneForAll Repository\n\u2022 GCN-Llama2-7B/SBert/e5-large: Graph-LLM Repository"}, {"title": "C EXPERIMENT DETAILS", "content": ""}, {"title": "C.1 Datasets", "content": "In this paper, we use the TAPE-Arxiv23 [12] that have up-to-date and rich texts to construct the instruction dataset, and use the following popular datasets commonly adopted for node classifications: Cora [27], Citeseer [6], Pubmed [32], OGBN-Arxiv [13] and OGBN-Products [13].\nNote that for OGBN-Arxiv, given its large scale with 169,343 nodes and 1,166,243 edges, we adopt a node sampling strategy [10] to obtain a subgraph containing 14,167 nodes and 33,502 edges. For the larger OGBN-Products, which have 2 million nodes and 61 million edges, we used the same sampling technique to construct a subgraph containing 12,394 nodes and 29,676 edges. We give a detailed description of each dataset in Table 9. The sources of datasets are listed as follows:\n\u2022 Cora, Pubmed, OGBN-Arxiv, TAPE-Arxiv23: TAPE Repository (MIT license)\n\u2022 OGBN-Products: LLM-Structured-Data Repository (MIT license)\n\u2022 Citeseer: Graph-LLM Repository (MIT license)"}, {"title": "C.2 Baselines", "content": "\u2022 GCN: GCN is a popular graph convolutional network based on spectral theory.\n\u2022 GAT: GAT is composed of multiple attention layers, which can learn different weights for different nodes in the neighborhood. It is often used as a baseline for defending against adversarial attacks.\n\u2022 RGCN: RGCN models node representations as Gaussian distributions to mitigate the impact of adversarial attacks, and employs an attention mechanism to penalize nodes with high variance.\n\u2022 Simp-GCN: SimpGCN employs a kNN graph to maintain the proximity of nodes with similar features in the representation space and uses self-learned regularization to preserve the remoteness of nodes with differing features.\n\u2022 ProGNN: ProGNN adapts three regularizations of graphs, i.e., feature smoothness, low-rank, and sparsity, and learns a clean adjacency matrix to defend against adversarial attacks.\n\u2022 STABLE: STABLE is a pre-training model, which is specifically designed to learn effective representations to refine graph structures.\n\u2022 HANG-quad: HANG-quad incorporates conservative Hamiltonian flows with Lyapunov stability to various GNNs, to improve their robustness against adversarial attacks.\n\u2022 GraphEdit: GraphEdit utilizes LLMs to identify noisy connections and uncover implicit relations among non-connected nodes in the original graph."}, {"title": "C.3 Implementation Details", "content": "We use DeepRobust, an adversarial attack repository, to implement all the attack methods as well as GCN, GAT, RGCN, and Sim-PGCN. We implement ProGNN, STABLE, and HANG-quad with the code provided by the authors.\nFor each graph, following existing works [19, 22], we randomly split the nodes into 10% for training, 10% for validation, and 80% for testing. We generate attacks on each graph according to the"}, {"title": "C.4 Computing Environment and Resources", "content": "The checkpoint of the local LLM and source code will be publicly available after the review. The implementation of the proposed LLM4RGNN utilized the PyG module. The experiments are conducted in a computing environment with the following specifications:\n\u2022 OS: Linux ubuntu 5.15.0-102-generic.\n\u2022 CPU: Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz.\n\u2022 GPU: NVIDIA A800 80GB."}, {"title": "D MORE EXPERIMENT RESULTS", "content": ""}, {"title": "D.1 Inductive Poisoning Atatck", "content": "We further verify the generalization ability of LLM4RGNN under inductive poisoning attacks. We conduct inductive experiments with Mettack on the Cora and Citeseer datasets. Specifically, we randomly split the data into training, validation, and test sets with a 1:8:1 ratio. During training, we ensure the removal of test nodes and their connected edges from the graph. We perform Mettack attacks on the validation set, purify the attacked graph using LLM4RGNN, and use the purified graph to train GNNs. The trained GNNs are then predicted on the clean test set. As shown in Table 11, we only report the baselines that support the inductive setting. Experimental results show that under the inductive setting, LLM4RGNN not only consistently improves the robustness of various GNNs but also"}, {"title": "D.2 More Hyper-parameter Sensitivity", "content": "We conduct more hyper-parameter experiments on the probability threshold A and the number of important edges K. The result is shown in Figure 7, demonstrating that the performance of the proposed LLM4RGNN is stable under various parameter configurations and consistently outperforms existing SOTA methods."}, {"title": "D.3 Comparative Experiment between Llama3-8B and Mistral-7B", "content": "In Section 5.3.2, the newly released Llama3-8B and Mistral-7B achieved close performance. Considering that Llama3-8B is released after all experiments are completed, we report additional comparative experiments between Mistral-7B and Llama3-8B. According to Table 10, we observe that both the well-tuned Mistral-7B and"}, {"title": "D.4 The Impact of Text Quality", "content": "Considering that LLM4RGNN relies on textual information for reasoning, we further analyze the impact of node text quality on the effectiveness of LLM4RGNN. Specifically, against the worst-case scenario of 20% Mettack, we further add random text replacement perturbations to the Cora and Citeseer datasets to reduce the text quality of nodes. As shown in Table 12, experimental results show that under 10%, 20%, and 40% text perturbations, the performance of LLM4RGNN only decreases by an average of 0.54%, 0.77%, and 1.14%, respectively. Its robustness consistently surpasses that of existing robust GNN frameworks, demonstrating that LLM4RGNN maintains superior robustness even with lower text quality. One possible explanation is that LLMs have strong robustness to text perturbations [2], and LLM4RGNN fully inherits this capability."}, {"title": "E CASE STUDY", "content": "In this section, we show some cases using GPT-4 and well-tuned local LLMs (Mistral-7B) to infer the relationships between nodes. It can be observed that the well-tuned Mistral-7B can achieve the edge relation inference ability of GPT-4. They infer the edge relations and provide analysis by discussing the background, problems, methods, and applications of two nodes."}, {"title": "E.1 TAPE-Arxiv23 (GPT-4)", "content": "User content: Node 1 Title: when renewable energy meets building thermal mass a real time load management scheme\\nAbstract: We consider the optimal power management in renewable driven smart building MicroGrid under noise corrupted conditions as a stochastic optimization problem. We first propose our user satisfaction and electricity consumption balanced (USECB) profit model as the objective for optimal power management. We then cast the problem in noise corrupted conditions into the class of expectation maximizing in stochastic optimization problem with convex constraints. For this task, we design a Bregemen projection based mirror decent algorithm as an approximation solution to our stochastic optimization problem. Convergence and upper-bound of our algorithm with proof are also provided in our paper. We then conduct a broad type of experiment in our simulation to test the justification of our model as well as the effectiveness of our algorithm.\nNode 2 Title: hiding sound in image by k lsb mutation\\nAbstract: In this paper a novel approach to hide sound files in a digital image is proposed and implemented such that it becomes difficult to conclude about the existence of the hidden data inside the image. In this approach, we utilize the rightmost k-LSB of pixels in an image to embed MP3 sound bits into a pixel. The pixels are so chosen that the distortion in image would be minimized due to embedding. This requires comparing all the possible permutations of pixel values, which may would lead to exponential time computation. To speed up this, Cuckoo Search (CS) could be used to find the most optimal solution. The advantage of using proposed CS is that it is easy to implement and is very effective at converging in relatively less iterations/generations.\nAnalysis: The two nodes represent research in different domains: Node1 discusses renewable energy and building thermal mass in the context of smart grids, while Node2 focuses on hiding sound files in images using k-LSB mutation and Cuckoo Search. There is no direct connection between the optimization of renewable energy in buildings and the embedding of sound files in images. The topics are unrelated, and the research methods and applications are distinct.\nRelevance Score: 1"}, {"title": "E.2 Cora (Mistral-7B)", "content": "User content: Node 1 Title: A Neural Network Model of Memory Consolidation\\nAbstract: Some forms of memory rely temporarily on a system of brain structures located in the medial temporal lobe that includes the hippocampus. The recall of recent events is one task that relies crucially on the proper functioning of this system. As the event becomes less recent, the medial temporal lobe becomes less critical to the recall of the event, and the recollection appears to rely more upon the neocortex. It has been proposed that a process called consolidation is responsible for transfer of memory from the medial temporal lobe to the neocortex. We examine a network model proposed by P. Alvarez and L. Squire designed to incorporate some of the known features of consolidation, and propose several possible experiments intended to help evaluate the performance of this model under more realistic conditions. Finally, we implement an extended version of the model that can accommodate varying assumptions about the number of areas and connections within the brain and memory capacity, and examine the performance of our model on Alvarez and Squire's original task.\nNode 2 Title: A Preliminary Investigation of Evolution as a Form Design Strategy\\nAbstract: We describe the preliminary version of our investigative software, GGE Generative Genetic Explorer, in which genetic operations interact with Au-toCAD to generate novel 3D forms for the architect. GGE allows us to asess how evolutionary algorithms should be tailored to suit Architecture CAD tasks.\nAnalysis: The two nodes represent research in different domains: Node1 discusses a neural network model of memory consolidation in the brain, while Node2 focuses on a software tool for generating 3D forms in architecture using evolutionary algorithms. There is no direct connection between the brain's memory consolidation process and the evolution of 3D forms in architecture. The topics are unrelated, and the expertise required for each is distinct.\nRelevance Score: 1"}]}