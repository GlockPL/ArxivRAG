{"title": "Degree Distribution based Spiking Graph Networks\nfor Domain Adaptation", "authors": ["Yingxu Wang", "Siwei Liu", "Mengzhu Wang", "Nan Yin", "Shangsong Liang"], "abstract": "Spiking Graph Networks (SGNs) have garnered significant attraction from both\nresearchers and industry due to their ability to address energy consumption chal-\nlenges in graph classification. However, SGNs are only effective for in-distribution\ndata and cannot tackle out-of-distribution data. In this paper, we first propose the\ndomain adaptation problem in SGNs, and introduce a novel framework named\nDegree-aware Spiking Graph Domain Adaptation for Classification (DeSGDA).\nThe proposed DeSGDA addresses the spiking graph domain adaptation problem by\nthree aspects: node degree-aware personalized spiking representation, adversarial\nfeature distribution alignment, and pseudo-label distillation. First, we introduce\nthe personalized spiking representation method for generating degree-dependent\nspiking signals. Specifically, the threshold of triggering a spike is determined by\nthe node degree, allowing this personalized approach to capture more expressive\ninformation for classification. Then, we propose the graph feature distribution\nalignment module that is adversarially trained using membrane potential against\na domain discriminator. Such an alignment module can efficiently maintain high\nperformance and low energy consumption in the case of inconsistent distribution.\nAdditionally, we extract consistent predictions across two spaces to create reliable\npseudo-labels, effectively leveraging unlabeled data to enhance graph classification\nperformance. Extensive experiments on benchmark datasets validate the superiority\nof the proposed DeSGDA compared with competitive baselines.", "sections": [{"title": "Introduction", "content": "Spiking Graph Networks (SGNs) Zhu et al. [2022b], Xu et al. [2021b] are a specialized type of\nartificial neural network engineered to process graph information by mimicking the human brain.\nSGNs transform static and real-valued graph features into discrete spikes by simulating neurons'\ncharging and discharging cycles, facilitating spike-based representations for graph node classification.\nNotably, SGNs excel in capturing semantic spiking representations with low energy consumption,\nwhich proves advantageous for event-based processing tasks Yao et al. [2021], Rosenfeld et al.\n[2021], Chen et al. [2023] such as object recognition Gu et al. [2020], Li et al. [2021b], Mei et al.\n[2022], real-time data analysis Bauer et al. [2019], Zhu et al. [2020], Baronig et al. [2024], and graph\nclassification Li et al. [2023], Zhu et al. [2022b], Xu et al. [2021b].\nCurrently, SGNs are usually tested within the same distribution as the training dataset Li et al.\n[2023], Yin et al. [2024], Duan et al. [2024]. However, in realistic scenarios, the testing set can have\ndifferent distributions from the training set, and such a distribution shift may lead to a degradation in"}, {"title": "", "content": "performance. For instance, Electroencephalography (EEG) data Binnie and Prior [1994], Biasiucci\net al. [2019], Peng et al. [2022], typically represented as a graph structure with nodes for neurons\nand edges for connections, is ideally processed by bio-inspired SGNs that mimic neuronal charging\nand discharging. Despite the suitability, EEGs often exhibit varying distributions over time or among\ndifferent groups Zhao et al. [2020, 2021], Wang et al. [2022], leading to suboptimal performance of\nmodels trained on specific distributions when applied to others. This significant issue underscores the\nnecessity of exploring domain adaptation for spiking graphs. Traditionally, SNNs transfer learning\nmethods Zhan et al. [2021], Zhang et al. [2021], Zhan et al. [2024], Guo et al. [2024] have been\napplied in event-based or computer vision scenarios. However, there's no existing research on spiking\ngraph domain adaptation.\nDesigning an effective spiking graph domain adaptation framework for classification is non-trivial due\nto the following major challenges: (1) How to meticulously design an SGN under the circumstance of\ndomain shift? SGNs usually utilize a global threshold for the firing of each node Xu et al. [2021a],\nHagenaars et al. [2021], Yin et al. [2024], Zhao et al. [2024]. However, we observe that the degree of\neach node influences the difficulty of triggering spikes. Specifically, nodes with high degrees can\nintegrate more information from neighbors, making it easier for membrane potential to accumulate\nand trigger a spike. Conversely, nodes with lower degrees are more challenging to reach the firing\nthreshold, denoted as the inflexible architecture challenge. (2) How to design a framework that\neffectively addresses spiking graph domain adaptation for classification? Current research primarily\nfocuses on graph node classification within the same distribution Li et al. [2023], Yao et al. [2023],\nDuan et al. [2024], Cai et al. [2024]. However, spike-based graph classification under domain shift\nremains unexplored. (3) How to guarantee the stability of the proposed framework? Though some\nworks have been proposed to address the spiking transfer learning challenges Zhan et al. [2021],\nZhang et al. [2021], Liu et al. [2024a], Zhan et al. [2024], there is still no theoretical research on\nspiking graphs under domain shift.\nTo tackle these challenges, we propose a framework named Degree-aware Spiking Graph Domain\nAdaptation for Classification (DeSGDA), which comprises three components: degree-aware per-\nsonalized spiking representation, graph feature distribution alignment, and pseudo-label distillation.\nTo address the first challenge, we establish variable node thresholds based on their degrees. By\nadaptively updating these thresholds, we can achieve a more expressive and personalized spiking\nrepresentation for each node. Then, we introduce a adversarial feature distribution alignment module\nthat is adversarially trained using membrane potential against a domain discriminator. To further\nenhance performance, we extract consistent predictions from different spaces to generate reliable\npseudo-labels. Additionally, to explore the generalization ability of the proposed DeSGDA, we first\npropose the error bound for spiking graph domain adaptation and demonstrate that our pseudo-label\ndistillation module effectively reduces this upper bound.\nOur contributions can be summarized as follows: (1) Problem Formulation: We first introduce\nthe problem of spiking graph domain adaptation for classification, which is non-trivial due to the\nchallenges of the inflexible architecture of SGNs and theoretical deficiency. (2) Novel Architecture:\nWe propose DeSGDA, a framework that efficiently learns personalized spiking representations for\nnodes using degree-aware thresholds and aligns domain distributions through adversarial training on\nmembrane potential. Furthermore, we utilize pseudo-label distillation to improve the performance\nfurther. (3) Theoretical Analysis: To guarantee the stability of DeSGDA, we provide theoretical\nproof of the error bound for spiking graph domain adaptation. Furthermore, we demonstrate that\nDeSGDA maintains a lower theoretical bound than standard spiking graph domain adaptation through\nthe effective use of the pseudo-label distillation module. (4) Extensive Experiments. We evaluate\nthe proposed DeSGDA on extensive spiking graph domain adaptation learning datasets, which shows\nthat our proposed DeSGDA outperforms the variety of state-of-the-art methods."}, {"title": "Related work", "content": "Spiking Graph Networks (SGNs). SGNs are a specialized type of neural network that combines\nSpiking Neural Networks (SNNs) with Graph Neural Networks (GNNs), preserving energy efficiency\nwhile achieving competitive performance in various graph tasks Li et al. [2023], Yao et al. [2023],\nQiu et al. [2024], Duan et al. [2024]. Existing research on SGNs focuses on capturing the dynamic\ntemporal information contained within graphs and enhancing model scalability. For instance, Xu et al.\n[2021a] utilizes spatial-temporal feature normalization within SNNs to effectively process dynamic"}, {"title": "Preliminaries", "content": "Bound for Graph Domain Adaptation (GDA). Applying GDA with optimal transport (OT), if\nthe covariate shift holds on representations that $P_S(Y|Z) = P_T(Y|Z)$, the target risk $\\epsilon_T(h, h)$ is\nbounded with the theorem:\nTheorem 1 [You et al., 2023] Assuming that the learned discriminator is $C_g$-Lipschitz, continuous\nas described in [Redko et al., 2017], and the graph feature extractor $f$ (also referred to as GNN)\nis $C_f$-Lipschitz that $||f||_{Lip} = \\max_{G_1, G_2} \\frac{||f(G_1)-f(G_2)||_2}{\\eta(G_1, G_2)} = C_f$ for some graph distance measure $\\eta$. Let $\\mathcal{H} := \\{h : \\mathcal{G} \\rightarrow \\mathcal{Y}\\}$ be the set of bounded real-valued functions with the pseudo-dimension\nPdim$(\\mathcal{H}) = d$ that $h = g \\circ f \\in \\mathcal{H}$, with probability at least $1 - \\frac{1}{8}$ the following inequality holds:\nwhere the (empirical) source and target risks are $\\hat{\\epsilon}_S(h,h) = \\frac{1}{N_S} \\sum_{i=1}^{N_S} |h(G_i) - \\hat{h}(G_i)|$ and\n$\\epsilon_T(h,h) = \\mathbb{E}_{P_T(\\mathcal{G})}[|h(\\mathcal{G}) - \\hat{h}(\\mathcal{G})|]$, respectively, where $\\hat{h} : \\mathcal{G} \\rightarrow \\mathcal{Y}$ is the labeling function for\ngraphs and $\\omega = \\min_{||g||_{Lip} \\leq C_g, ||f||_{Lip} \\leq C_f} \\{\\epsilon_S(h, h) + \\epsilon_T(h, \\hat{h})\\}$. The first Wasserstein distance is\ndefined as [Villani et al., 2009]: $W_1(P, Q) = \\sup_{||g||_{Lip} \\leq 1} \\{\\mathbb{E}_{P_S(Z)}g(Z) - \\mathbb{E}_{P_T(Z)}g(Z)\\}$.\nThe comprehensive justification of the OT-based graph domain adaptation bound demonstrates that\nthe generalization gap relies on both the domain divergence $2C_fC_gW_1(P_S(\\mathcal{G}), P_T(\\mathcal{G}))$ and model\ndiscriminability $\\omega$.\nSpiking Graph Networks. In contrast to traditional artificial neural networks, SGNs [Xu et al.,\n2021a, Zhu et al., 2022b] convert input data into binary spikes over time, with each neuron in the\nSGNs maintaining a membrane potential that accumulates input spikes. A spike is produced as an\noutput when the membrane potential exceeds a threshold, which is formulated as:\n$U^{r+1,i} = X(U^{r,i} - V_{ths,i}) + \\sum_j W_{ij}A(\\mathcal{A}, S^{r,j}) + b, S^{r+1,i} = H(U^{r+1,i} - V_{th}),$ (1)\nwhere $H(x)$ is the Heaviside function, which is the non-differentiable spiking function. $\\mathcal{A}$ is the\ngraph aggregation operation, and $A$ is the adjacency matrix of graph. $S^{r,i}$ denotes the binary spike\ntrain of neuron $i$, and $A$ is the constant. $W_{ij}$ and $b$ are the weights and bias of each neuron."}, {"title": "Methodology", "content": "This work studies the spiking graph domain adaptation problem and proposes a new approach\nDeSGDA. DeSGDA consists of three parts: Degree-aware personalized spiking representation"}, {"title": "Degree-aware Personalized Spiking Representation", "content": "In this part, we first study the disadvantages of directly applying SNNs to graphs and then propose the\ndegree-aware personalized spiking representation. Existing SGNs Li et al. [2023], Yao et al. [2023],\nDuan et al. [2024] usually employ a global threshold for membrane potential firing. However, the\nglobal threshold can lead to the inflexible architecture issue since nodes with higher degrees are more\nlikely to trigger spikes than those with lower degrees. As shown in Eq. 1, nodes with higher degrees\nhave more neighbors, and the aggregation operation allows for more significant feature accumulation,\nmaking it easier for these nodes to trigger spikes compared to those with fewer neighbors. To alleviate\nthis issue, we propose the degree-aware thresholds and iteratively update their values.\nSpecifically, we first set all the degrees of nodes in the source domain graphs, i.e., $D^s = \\text{set}(D^1 \\cup\n... \\cup D^{N_s})$, where $D_i$ denotes the degree set of graph $G_i$, and set$(\\cdot)$ operation is an unordered\nsequence of non-repeating elements. Considering that low-degree nodes are more challenging\nto trigger while high-degree nodes trigger more easily, we propose setting higher thresholds for\nhigh-degree nodes and lower thresholds for low-degree nodes, which is formulated as:\n$S_d^{th} = H(U^{th} - V_d^{th}), S_d^D = avg(s_d^t), V_d^{th} = (1-\\alpha)V^{th} + \\alpha s_d^D,$\nwhere $V_d^{th}$ is the threshold of degree $d \\in D$, initially set to $V_{th}$, and $\\alpha$ is a hyper-parameter.\nThe avg$(\\cdot)$ operation takes the average of spiking representation with degree $d$. Consequently,\nhigh-degree nodes tend to achieve high $S_d^D$, which leads to an iterative increase in the threshold\ncorresponding to degree $d_i$ and conversely for lower-degree nodes.\nWith different thresholds for different node degrees, we can obtain the personalized node spiking\nrepresentation $s_i \\in \\mathcal{S} \\in \\mathbb{G}$. Then, we summarize all node representations with a readout function into the\ngraph-level representation and output the prediction with a multi-layer perception (MLP) classifier:\n$s_i = READOUT(\\left\\{s_d^i \\right\\}_{s \\in \\mathbb{G}}), \\hat{y} = H(s_i),$ (3)"}, {"title": "Adversarial Distribution Alignment", "content": "To eliminate the discrepancy between the source and target domains, we propose the adversarial\ndistribution alignment module. Specifically, for each source graph $G_i^s$ and target graph $G_i^t$, we use\nthe degree-aware personalized spiking GNNs-based encoder $F(\\cdot)$ and semantic classifier $H(\\cdot)$ to\nproduce predicted labels. Then, a domain discriminator $Q(\\cdot)$ is trained to distinguish features from\nthe source and target domains. The encoder and classifier are adversarial trained to align the feature\nspaces of the source and target domains.\n$\\mathcal{L}_{AD} = \\mathbb{E}_{G^s \\in D^s} -\\log Q\\big(F(G^s), H(G^s)|V^{th}\\big) + \\mathbb{E}_{G^t \\in D^t} -\\log \\big(1 - Q\\big(F(G^t), H(G^t)|V^{th}\\big)\\big)$.\nHowever, the degree in the target domain may be unseen by the source. Thus, we further initialize the\nthreshold with $V_d^{th}$ and $d \\notin D^s$, which is formulated as:\n$\\mathcal{L}_{AD} = \\mathbb{E}_{G^s \\in D^s} -\\log \\big(1 - Q\\big(F(G^s), H(G^s)|V^{th}\\big)\\big) + \\mathbb{E}_{G^t \\in D^t} -\\log Q\\big(F(G^t), H(G^t)|V^{th}\\big)\n+ \\mathbb{E}_{G^t \\in D^t} -\\log \\big(1 - Q\\big(F(G^t), H(G^t)|V_{d^*}^{th}, V_{d^* \\notin D^s}^{V_D^t}\\big)\\big),$ (5)\nwhere $D^t = \\{d| d^t \\in D^t, d^t \\notin D^s\\}$. Then, we iteratively update $V_{d^t}^{th}$ with Eq. 2 on each latency.\nFurthermore, we present an upper bound on the adversarial distribution alignment.\nTheorem 2 Assuming that the learned discriminator is $C_g$-Lipschitz continuous as described in\nTheorem 1, the graph feature extractor $f$ (also referred to as GNN) is $C_f$-Lipschitz that $||f||_{Lip} = \\max_{G_1, G_2} \\frac{||f(G_1)-f(G_2)||_2}{\\eta(G_1, G_2)} = C_f$ for some graph distance measure $\\eta$ and the loss function bounded\nby $C > 0$. Let $\\mathcal{H} := \\{h : \\mathcal{G} \\rightarrow \\mathcal{Y}\\}$ be the set of bounded real-valued functions with the pseudo-dimension\nPdim$(\\mathcal{H}) = d$ that $h = g \\circ f \\in \\mathcal{H}$, and provided the spike training data set $S_n = \\{(X_i, y_i) \\in \\mathcal{X} \\times \\mathcal{Y}\\}_{i \\in [n]}$ drawn from $\\mathcal{D}^s$, with probability at least $1 - \\frac{1}{8}$ the following inequality :\nwhere the (empirical) source and target risks are $\\hat{\\epsilon}_S(h,\\hat{h}(S)) = \\frac{1}{N_s} \\sum_{i=1}^{N_s} |h(S_n) - \\hat{h}(S_n)|$ and\n$\\epsilon_T(h, h_T(X)) = \\mathbb{E}_{P_T(\\mathcal{G})}[|h(\\mathcal{G}) - \\hat{h}(\\mathcal{G})|]$, respectively, where $h : \\mathcal{G} \\rightarrow \\mathcal{Y}$ is the labeling function\nfor graphs and $\\omega = \\min_{||g||_{Lip} \\leq C_g, ||f||_{Lip} \\leq C_f} \\{\\epsilon_S(h, h(x)) + \\epsilon_T(h, h(X))\\}$, $\\epsilon_i$ is the Rademacher\nvariable and $p_i$ is the $i$th row of $P$, which is the probability matrix with:\n$P_{kt} = \\begin{cases}\n\\exp\\left(\\frac{u_k(t)-V^{th}}{\\sigma(u_k(t)-U_{reset})}\\right), & \\text{if } u_0 \\leq u(t) \\leq V_{th},\\\\\n0, & \\text{if } U_{reset} < u_k(t) \\leq u_0.\\end{cases}$ (7)"}, {"title": "Pseudo-label Distillation for Discrimination Learning", "content": "To further address the variance in thresholds between the target and source domains, we incorporate\nthe pseudo-label distillation module into the DeSGDA framework. With reliable pseudo-labels, we\ncan effectively update the source degree thresholds in the target domain.\nThe goal of the pseudo-label distilling procedure is to keep those examples and their corresponding\npseudo-labels from the deep feature space that aligns with the shallow feature space. Specifically, we\ndenote $s_i^'$ as the shallow spiking graph representation on the $L^{'}$-th layer, where $L^{'} < L$, and $\\hat{y}_i^t$ as\nthe prediction of graph $G_i^t$ on the $L$-th layer.\nThen, to enhance alignment between the shallow and deep feature spaces and facilitate the generation\nof more accurate pseudo-labels, we cluster the shallow features $s^{L^{'}}$ into $C$ clusters and each cluster\n$E_j$ includes graphs $\\{G_i^t\\}$.\nAfter that, we find the dominating labels $e_r$ in the cluster, i.e., $\\max_r |\\{E_r : e_r = \\hat{y}_i^t\\}|$, and remove\nother instances with the same pseudo-label but in different clusters. Formally, the pseudo-labels are\nsigned with:\n$\\mathcal{P} = \\{(G_i^t, \\hat{y}_i^t) : e_j = \\max_r |\\{E_r: e_r = \\hat{y}_i^t\\}|\\}.$ (8)\nFinally, we utilize the distilled pseudo-labels to guide the update of source degree thresholds on the\ntarget domain with Eq. 2, and to direct classification in the target domain:\n$\\mathcal{L}_T = \\mathbb{E}_{G^t \\in \\mathcal{P}} l(H(s_i), \\hat{y}_i^t),$ (9)\nwhere $H(\\cdot)$ and $s_i$ are the classifier and spiking graph representation, respectively, which are defined\nin Eq. 3. $l(\\cdot)$ is the loss function, and we implement it with cross-entropy loss.\nTheorem 3 Under the assumption of Theorem 1, we further assume that there exists a small amount\nof i.i.d. samples with pseudo labels $\\{(G_n, Y_n)\\}_{n=1}^{N'_T}$ from the target distribution $P_T(\\mathcal{G}, \\mathcal{Y}) (N'_T <\nN_s)$ and bring in the conditional shift assumption that domains have different labeling function\n$h_S \\neq h_T$ and $\\max_{G_1, G_2} \\frac{|h_D(G_1)-h_D(G_2)|}{\\eta(G_1, G_2)} = C_h < C_f C_g (D \\in \\{S,T\\})$ for some constant $C_h$ and\ndistance measure $\\eta$, and the loss function bounded by $C > 0$. Let $\\mathcal{H} := \\{h : \\mathcal{G} \\rightarrow \\mathcal{Y}\\}$ be the set of\nbounded real-valued functions with the pseudo-dimension Pdim$(\\mathcal{H}) = d$, and provided the spike\ntraining data set $S_n = \\{(X_i, Y_i) \\}_{i \\in [n]}$, with probability at least $1 - \\delta$ the following inequality holds:"}, {"title": "Experiment", "content": "To demonstrate the effectiveness of DeSGDA, we conduct extensive experiments on four\nwidely-used graph classification datasets from TUDataset \u00b9, including PROTEINS Dobson and Doig\n[2003], NCI1 Wale et al. [2008], FRANKENSTEIN Orsini et al. [2015], and MUTAGENICITY\nKazius et al. [2005]. To better address the variation in domain distributions within each dataset, we\ndivided them into source and target domains based on the edge density, node density, and graph flux\n(i.e., the ratio of the number of nodes to the number of edges). The specific statistics, distribution\nvisualization, and details introduction of experimental datasets are presented in Appendix E.\nBaselines. We compare DeSGDA with competitive baselines on the aforementioned datasets,\nincluding one graph kernel method: WL subtree Shervashidze et al. [2011]; four general graph neural\nnetworks: GCN Kipf and Welling [2017], GIN Xu et al. [2018], CIN Bodnar et al. [2021] and GMT\nBaek et al. [2021]; two spiking graph neural networks: SpikeGCN Zhu et al. [2022a] and DRSGNN\nZhao et al. [2024]; three recent domain adaptation methods: CDAN Long et al. [2018], ToAlign Wei\net al. [2021b], and MetaAlign Wei et al. [2021a]; and six graph domain adaptation methods: DEAL\nYin et al. [2022], CoCo Yin et al. [2023], SGDA Qiao et al. [2023], DGDA Cai et al. [2024], A2GNN\nLiu et al. [2024b] and PA-BOTH Liu et al. [2024c]. More details about the compared baselines can\nbe found in Appendix F."}, {"title": "Experimental Settings", "content": "Dataset. To demonstrate the effectiveness of DeSGDA, we conduct extensive experiments on four\nwidely-used graph classification datasets from TUDataset \u00b9, including PROTEINS Dobson and Doig\n[2003], NCI1 Wale et al. [2008], FRANKENSTEIN Orsini et al. [2015], and MUTAGENICITY\nKazius et al. [2005]. To better address the variation in domain distributions within each dataset, we\ndivided them into source and target domains based on the edge density, node density, and graph flux\n(i.e., the ratio of the number of nodes to the number of edges). The specific statistics, distribution\nvisualization, and details introduction of experimental datasets are presented in Appendix E.\nBaselines. We compare DeSGDA with competitive baselines on the aforementioned datasets,\nincluding one graph kernel method: WL subtree Shervashidze et al. [2011]; four general graph neural\nnetworks: GCN Kipf and Welling [2017], GIN Xu et al. [2018], CIN Bodnar et al. [2021] and GMT\nBaek et al. [2021]; two spiking graph neural networks: SpikeGCN Zhu et al. [2022a] and DRSGNN\nZhao et al. [2024]; three recent domain adaptation methods: CDAN Long et al. [2018], ToAlign Wei\net al. [2021b], and MetaAlign Wei et al. [2021a]; and six graph domain adaptation methods: DEAL\nYin et al. [2022], CoCo Yin et al. [2023], SGDA Qiao et al. [2023], DGDA Cai et al. [2024], A2GNN\nLiu et al. [2024b] and PA-BOTH Liu et al. [2024c]. More details about the compared baselines can\nbe found in Appendix F."}, {"title": "Performance Comparision", "content": "We present the results of the proposed DeSGDA with all baseline models under the setting of graph\ndomain adaptation on different datasets in Table 1, 2, 3. From these tables, we observe that: (1)\nThe performance of graph domain adaptation methods surpasses that of graph and spike-based\ngraph methods. We attribute this improvement to the fact that domain distribution shifts degrade\nthe performance of traditional graph methods. (2) The graph domain adaptation methods (DEAL\nand CoCo) outperform the spike-based graph methods (SpikeGCN and DRSGNN), underscoring\nthe necessity of the research in spiking graph domain adaptation. (3) Our DeSGDA outperforms all\nbaselines for most cases, which demonstrates its superiority over other methods. The remarkable\nperformance of DeSGDA lies in two main reasons: (i) The degree-aware personalized spiking\nrepresentations can capture more expressive information for graph classification by dynamically\nadjusting the thresholds of nodes in SNNs. (ii) The adversarial distribution alignment effectively\naddresses domain discrepancies by adversarially training the encoder and domain discriminator to\nalign feature spaces. Moreover, the pseudo-label distillation aids in updating the source degree\nthresholds in the target domain, thereby ensuring optimal performance. More results evaluated on\nother datasets can be found in Appendix G.1."}, {"title": "Energy Efficiency Analysis", "content": "To assess the energy efficiency of DeSGDA, we\nuse the metric from Zhu et al. [2022a] and quan-\ntify the energy consumption for graph classifi-\ncation in the inference stage. Specifically, the\ngraph domain adaption methods are evaluated on\nGPUs (NVIDIA A100), and the spiking-based\nmethods are evaluated on neuromorphic chips\n(ROLLS Indiveri et al. [2015]) following Zhu\net al. [2022a]. The results are shown in Figure 3,\nfrom the results, we find that compared with tra-\nditional graph domain adaptation methods, the\nspike-based methods (DeSGDA and DRSGNN)\nhave significantly lower energy consumption, demonstrating the superior energy efficiency of SGNs.\nMoreover, although the energy consumption of DeSGDA is slightly higher than DRSGNN due to\nadditional computations required for domain adaptation, the performance improvement justifies the\ndeployment of DeSGDA in low-power devices."}, {"title": "Ablation Study", "content": "We conduct ablation studies to examine the contributions of each component in the proposed DeSGDA:\n(1) DeSGDA w/o CA: It removes the adversarial distribution alignment module; (2) DeSGDA w/o\nPL: It removes the pseudo-label distilling module; (3) DeSGDA w/o CF: It removes the classification\nloss $\\mathcal{L}_s$; (4) DeSGDA w/o TL: It utilizes the global thresholds on all nodes; (5) DeSGDA w/ PT: It\ndeploys the adaptive perturbations Yin et al. [2022] on source data for alignment; (6) DeSGDA w/\nCL: It replaces the adversarial learning with the cross-domain contrastive learning Yin et al. [2023].\nExperimental results are shown in Table 4. From the table, we find that: (1) DeSGDA outperforms\nDeSGDA w/o CA, DeSGDA w/o PL, and DeSGDA w/o CF, demonstrating that the adversarial distri-\nbution alignment module can effectively reduce domain discrepancies, ensuring well-aligned feature\nspaces between source and target domains. Additionally, the pseudo-label distillation module can\naddress the variance in thresholds across domains, while the classification loss $\\mathcal{L}_s$ enables DeSGDA\nto effectively learn from labeled source data and generalize to the target domain. (2) DeSGDA w/o TL\nshows lower performance compared to DeSGDA, showing that the degree-aware thresholds, which\nare iteratively updated during model training, can resolve the issue of the inflexible architecture in\nSGNs. By using these thresholds, DeSGDA can effectively learn meaningful representations for nodes"}, {"title": "Sensitivity Analysis", "content": "We study the sensitivity analysis of DeSGDA with respect to the impact of its hyperparameters: time\nlatency $\\tau$, degree threshold value $V_{th}^{degree}$ in SNNs, and balance ratio $\\alpha$, which plays a crucial role\nin the performance of DeSGDA. In particular, $\\tau$ controls the number of SNNs propagation steps;\n$V_{th}^{degree}$ determines when a neuron fires; $\\alpha$ governs the changing ratio of degree-aware thresholds.\nFigure 4 illustrates how $\\tau$, $V_{th}^{degree}$, and $\\alpha$ affects the performance of DeSGDA on the PROTEINS\ndataset. More results on other datasets are shown in Appendix G.3. We vary $\\tau$ within the range\nof $\\{5, 6, 7, 8, 9, 10\\}$, $V_{th}^{degree}$ in $\\{0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0\\}$, and $\\alpha$ in $\\{0.1, 0.3, 0.5, 0.7, 0.9\\}$.\nFrom the results, we observe that: (1) The performance of DeSGDA in Figure 4a generally exhibits\nan increasing trend at the beginning and then stabilizes when $\\tau$ is greater than 8. We attribute this to\nsmaller values of $\\tau$ potentially losing important information for representation, while larger values\nsignificantly increase model complexity. To balance performance and complexity, we set $\\tau$ to 8 as\ndefault. (2) Figure 4b indicates an initial increase followed by a decreasing trend in performance\nas $V_{th}^{degree}$ increases. This trend occurs because a lower threshold may trigger more spikes for\nhigh-degree nodes, leading to a drastic change in the threshold, which can degrade performance.\nConversely, a higher threshold for low-degree nodes could result in fewer spikes, affecting the\nmodel's ability to process information effectively. Therefore, we set $V_{th}^{degree}$ to 0.5 as default. (3)\nFrom Figure 4c, we find that the performance of DeSGDA initially increases and then decreases\nas $\\alpha$ increases. The potential reason is that the smaller $\\alpha$ may delay the updating of the threshold,\nleading to performance degradation. Contrarily, a larger $\\alpha$ tends to introduce more spikes that change\ndynamically at each step, resulting in instability in the model's performance. Therefore, we set $\\alpha$ to\n0.5 as default."}, {"title": "Conclusion", "content": "In this paper, we first propose the problem of spiking graph domain adaptation and introduce a\nnovel framework DeSGDA for graph classification. This framework enhances the adaptability\nand performance of SGNs through three key aspects: node degree-aware personalized spiking\nrepresentation, adversarial feature distribution alignment, and pseudo-label distillation. Our approach\nenables more expressive information capture through degree-dependent spiking thresholds, aligns\nfeature distributions via adversarial training, and utilizes pseudo-labels to leverage unlabeled data\neffectively. The extensive experimental validation across benchmark datasets has demonstrated\nthat DeSGDA not only surpasses existing methods in accuracy but also maintains efficient energy\nconsumption, making it a promising solution for advancing the domain adaptation capabilities of\nspiking graph networks. In the future, we will apply SGNs in the scenarios of source-free domain\nadaptation and domain generalization."}]}