{"title": "Transformer Alignment in Large Language Models", "authors": ["Murdock Aubry", "Haoming Meng", "Anton Sugolov", "Vardan Papyan"], "abstract": "Large Language Models (LLMs) have made significant strides in natural language processing, and a precise understanding of the internal mechanisms driving their success is essential. We regard LLMs as transforming embeddings via a discrete, coupled, nonlinear, dynamical system in high dimensions. This perspective motivates tracing the trajectories of individual tokens as they pass through transformer blocks, and linearizing the system along these trajectories through their Jacobian matrices. In our analysis of 38 openly available LLMs, we uncover the alignment of top left and right singular vectors of Residual Jacobians, as well as the emergence of linearity and layer-wise exponential growth. Notably, we discover that increased alignment positively correlates with model performance. Metrics evaluated post-training show significant improvement in comparison to measurements made with randomly initialized weights, highlighting the significant effects of training in transformers. These findings reveal a remarkable level of regularity that has previously been overlooked, reinforcing the dynamical interpretation and paving the way for deeper understanding and optimization of LLM architectures.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), as exemplified by BERT and GPT-series [Devlin et al., 2019, Brown et al., 2020], have revolutionized the field of natural language processing through their adoption of the transformer architecture [Vaswani et al., 2017]. Despite their widespread success, the internal mechanisms that underpin their performance are not fully understood.\nPrevious works viewed certain types of deep networks as implementing discrete, nonlinear dynamical systems, operating in high dimensions [Greff et al., 2016, Papyan et al., 2017, Ebski et al., 2018, Chen et al., 2018, Bai et al., 2019, Rothauge et al., 2019, Li and Papyan, 2023, Gai and Zhang, 2021, Haber and Ruthotto, 2017, Ee, 2017]. The term discrete reflects the network's finite depth; nonlinear refers to the model's nonlinear components; and dynamical is due to the residual connections spanning various layers. This motivates us to trace the dynamics of individual tokens as they traverse through the numerous transformer blocks, and to linearize the system through its Jacobian matrices."}, {"title": "1.1 Residual Alignment", "content": "Our investigation draws inspiration from a recent study by Li and Papyan [2023] on Residual Networks (ResNets) [He et al., 2016], which uncovered a phenomenon they termed Residual Alignment (RA), marked by several distinct characteristics in hidden representations of ResNets:\n(RA1) Linear and equispaced trajectories in layer-wise progression of intermediate representations.\n(RA2) Alignment of top left and right singular vectors in the linearizations of residual blocks.\n(RA3) The rank of Residual Jacobians is at most the number of classes.\n(RA4) Top singular values of Residual Jacobians scale inversely with depth."}, {"title": "1.2 Contributions", "content": "We uncover Transformer Alignment (TA), characterized by two properties (analogous to those of RA) that consistently emerge with training in 38 openly available base and fine-tuned LLMs (Appendix A.1) including the Falcon [Almazrouei et al., 2023], Llama-3 [AI@Meta, 2024], Llama-2 [Touvron et al., 2023], MPT [Team, 2023a,b], Mistral v0.1 [Jiang et al., 2023], Gemma [Team et al., 2024], NeoX [Black et al., 2022], Neo [Black et al., 2021, Gao et al., 2020], and Pythia [Biderman et al., 2023] families. The properties are given by\n(TA1) Emergence of linearity and exponentially distanced trajectories of intermediate representations in layer-wise progression.\n(TA2) Alignment of top left and right singular vectors in the linearizations of transformer blocks through their Residual Jacobians.\nDue to the large vocabulary size relative to the embedding size in LLMs, Residual Jacobians maintain rank of at most the vocabulary size, and so the previous (RA3) immediately holds. Further, trained LLMs do not exhibit a consistent inverse scaling of top singular values with depth, and scaling varies considerably between models, demonstrating a lack of (RA4). In contrast to ResNets, the layer-wise progression of transformer representations is better described with an exponential evolution, rather than an equispaced linear progression.\nAdditionally, we explore the relationship between (TA2) and generalization, and uncover that increased alignment of Residual Jacobian singular vectors correlates with improved Open LLM leaderboard [Beeching et al., 2023] benchmark score (Figure 3), raising valuable insight to the mathematical properties of LLM transformers and their connections to generalization."}, {"title": "2 Background on Large Language Models", "content": "In the input layer, l 0, textual prompts undergo tokenization and are combined with positional encodings to create an initial high-dimensional embedding, denoted by $x_i^0 \\in \\mathbb{R}^{d_{model}}$ for the ith token. When these embeddings are stacked together, they form a matrix:\n$X^0 = (x_1^0, x_2^0,...,x_n^0) \\in \\mathbb{R}^{d_{model} \\times n}$.\n(1)\nThe embeddings then pass through L transformer blocks:\n$X^0 \\xrightarrow{F_{block}} X^1 \\xrightarrow{F_{block}} ... \\xrightarrow{F_{block}} X^{L-1} \\xrightarrow{F_{block}} X^L$\n(2)\nHere, $X^l = F_{block}(X^{l-1})$ denotes the embeddings after the lth block, consisting of causal multi-headed attention (MHA), a feed-forward network (FFN), and normalization layers (LN) with residual"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Suite of Large Language Models", "content": "Our study evaluates a total of 38 LLMs (24 base LLMs and 14 fine-tuned, see Appendix A.1) that were independently trained by various individuals and organizations. These models, provided through HuggingFace [Wolf et al., 2020], vary in terms of parameter budgets, number of layers, hidden dimensions, and training tokens. A summary of the models under consideration is presented in Table 1 of Appendix A.1. Measurements are made for models with varying parameter sizes with GPUs satisfying appropriate memory requirements, further detailed in Appendix A.3."}, {"title": "3.2 Prompt Data", "content": "We evaluate these LLMs using prompts of varying length, ambiguity, and context, sourced from the test set of ARC [Clark et al., 2018], GSM8K [Cobbe et al., 2021], HellaSwag [Zellers et al., 2019], MMLU [Hendrycks et al., 2021], Truthful QA [Lin et al., 2022], and Winogrande [Sakaguchi et al., 2019]. These datasets set the performance benchmarks on the HuggingFace Open LLM Leaderboard [Beeching et al., 2023] since they encompass a diverse set of language tasks. Post-prompting, we assess the models using several metrics, detailed in the following subsections."}, {"title": "3.3 Alignment of Singular Vectors of Residual Jacobians", "content": "To demonstrate the presence of (TA2), we examine the properties of the transformer blocks and the relationships between them. This is done by analyzing the linearizations of the blocks given by their Residual Jacobian matrices\n$\\frac{\\partial}{\\partial \\mathbf{x}_i^{l-1}} (h^l(\\mathbf{X}^{l-1}) + \\mathrm{FFN}_l(g^l(\\mathbf{X}^{l-1})))_i$,\n(7)\ndefined for each l = 1,..., L, and i = 1,...,n. Note that this is the Jacobian matrix for each transformer block without the contribution from the skip connection from the input of the block, analogous to the quantities measured by Li and Papyan [2023].\nThe singular value decompositions of these $J_i$ are computed, i.e., $J_i = U_iS_iV_i^T$ (with superscript i, indicating the token, omitted for clarity), where $U_i \\in \\mathbb{R}^{d_{model} \\times d_{model}}$ and $V_i \\in \\mathbb{R}^{d_{model} \\times d_{model}}$ are the matrices of left and right singular vectors respectively, and $S_i \\in \\mathbb{R}^{d_{model} \\times d_{model}}$ is the singular value matrix.\nThe matrices $A_{i,j,K} := U_{i,K}^T J_i V_{j,K}$ are plotted over all pairs of depths i, j \u2208 {1, ..., L}, where $U_{j,K}$ and $V_{j,K}$ are the sub-matrices with columns that are the top-K left and right singular vectors of $J_j$, respectively.\nTo quantify the misalignment of singular vectors of the Residual Jacobians at depths i and j, we measure how far the $A_{i,j,K}$ are from being diagonal. More precisely, we define the misalignment of $A_{i,j,K}$ as"}, {"title": "3.4 Layer-Wise Exponential Growth", "content": "To demonstrate the presence of (TA1), we measure the expodistance of the hidden trajectories, defined below. In this context, each layer in the model corresponds to a unit time step of a dynamical system, so we denote $x_i'(t) = x_i(t + 1) - x_i(t)$ the time derivative of the ith token. Assuming exponential growth of the embedding norms as they flow through the hidden layers, we can write,\n$||x_i(t)|| \\approx e^{\\alpha t} ||x_i(0)||\n(9)\nfor some fixed \u03b1 \u2208 R. We quantify the validity of the approximation in Equation (9) by measuring the coefficient of variation of \u03b1 for each trajectory. Namely, following (9),\n$\\alpha \\approx \\frac{1}{t} \\ln(\\frac{||x_i(t)||}{||x_i(0)||})$\n(10)\nfor each time (layer) t and token i. Under exponential growth, it is expected that \u03b1 is independent of time. We then denote the expodistance (ED) of the trajectory of the ith token of a given sequence by\n$\\mathrm{ED}_i = \\frac{\\mathrm{Var}_t \\alpha_t^i}{(\\mathrm{Avg}_t \\alpha_t^i)^2}$\n(11)\nThis measurement is motivated by the parametrization discussed in Appendix A.2 and serves as a method to test the validity of the linearization presented in Equation (15)."}, {"title": "3.5 Linearity of Trajectories", "content": "Linearity in intermediate embeddings (TA1) is quantified with the line-shape score (LSS), defined by Gai and Zhang [2021] as\n$ \\mathrm{LSS}(x_i^{0}, ..., x_i^{L}) = \\frac{L}{\\sum_{l=1}^{L} \\frac{||x_i^{l} - \\hat{x}_i^{l}||^2}{||x_i^{l} - x_i^{l-1}||^2} }$,\n(12)\nwhere $x_i^{0} = x_i$, i.e., the input embeddings passed to the LLM, and $\\hat{x}_i^{l}$ is defined recursively as\n$\\hat{x}_i^{l} = x_i^{l-1} + \\frac{x_i^{l} - x_i^{l-1}}{||x_i^{l} - x_i^{l-1}||^2} ||x_i - x_i^{l-1}||,\\ \\mathrm{for\\ } l = 1, ..., L.$\n(13)\nNote that LSS \u2265 1, with LSS = 1 if and only if the intermediate representations $x_i^{1}, ..., x_i^{L}$ form a co-linear trajectory."}, {"title": "3.6 Visualization of Trajectories with PCA", "content": "Each token, with initial embedding $x_i^0$, forms a trajectory $x_i^0, x_i^1, ..., x_i^{L}$ as it passes through the L transformer blocks. The dynamics in high-dimensional space are visualized through a 2-dimensional principal component (PC) projection, $PC_L$, fitted to the last layer embeddings $X^L = (x_1^L, x_2^L,...,x_n^L)$.\nThe projected embeddings, $PC_L(x_i^0), PC_L(x_i^1), ..., PC_L(x_i^{L})$, are plotted for each of the i = 1,..., n trajectories."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Alignment of Singular Vectors of Residual Jacobians", "content": "In trained LLMs, we observe (TA2), that is, alignment of the top singular vectors of the Jacobians $J_i$ across depth (Figure 2 bottom row), evident in the distinct diagonal lines present in the matrix subplots. This phenomenon is consistently observed across LLMs, similar to (RA2), which was"}, {"title": "4.2 Linearity of Trajectories", "content": "We observe (TA1) in each LLM, evident in the LSS of layer-wise trajectories at initialization and after training evaluated over benchmark prompts (Figure 4), in agreement with (RA1) in ResNets. Linear and expansive dynamical behavior is demonstrated in Llama-3 70B, Mistral 30B, and NeoX 20B through low dimensional projections of embedding trajectories (Figure 1). The LSS of the trajectories has an average value 4.25 across trained models, while taking average values of 6.54 at initialization, and is evidenced by the low variation across benchmark prompts (Figure 4)."}, {"title": "4.3 Layer-wise Growth", "content": "Across all LLMs considered, the hidden trajectories exhibit distinctly exponential growth that emerges with training (Figure 5), displaying the second property of (TA1). This property, which arises as a result of Jacobian alignment, is distinct from (RA1) in the sense that distances between consecutive layers grow exponentially instead of linearly with depth, and highlights a key difference between the trajectories in ResNet classifiers and transformers. Most of the LLMs considered exhibit a low"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Correlation with Generalization", "content": "As discussed in Section 4.1, stronger alignment of Residual Jacobians singular vectors is related to improved generalization (Figure 3). This relationship between (TA2) with LLM benchmark scores may assist in evaluating the training of transformers. Understanding the relationship between training methods and regularity in representations holds potential implications for the enhancement of LLMs. This finding suggests that development of techniques to amplify Transformer Alignment may lead to favourable model performance. The connection between hidden representation regularity, generalization, and LLM training, as prompted by our work, presents an intriguing research direction with important practical implications."}, {"title": "5.2 Regularity and Training", "content": "Many LLMs are based on the same transformer architecture, yet when trained by independent individuals and organizations, training directly affects the emergence of residual Jacobian alignment (Figure 3), presence of linearity (Figure 4), and extent of exponential growth (Figure 5). The effect of training is further evident when comparing base LLMs to their fine-tuned counterparts, since differences in measurements are a direct product of further optimization of the same weights. Our findings further highlight the important effect of training on the dynamical properties of hidden trajectories in transformers."}, {"title": "5.3 Comparison to ResNets", "content": "Linearity and Equidistance. Linearity in hidden trajectories, as observed in ResNets (RA1) Li and Papyan [2023], Gai and Zhang [2021], also emerges with training in LLMs (TA1). The mean"}, {"title": "5.4 Emergence of Exponential Growth", "content": "Trained ResNets distinctly feature equidistance in hidden trajectories, while measurements in trans- formers display a layer-wise exponential growth in norm. [Li and Papyan, 2023] prove that equidis- tance in certain classifiers emerges under the assumptions of (RA 2-4), in particular, the authors assume that top singular values of residual Jacobians scale inversely with model depth. In transform- ers, top singular values do not show a similar scaling, possibly suggesting that (RA4) is a necessary condition for equidistance.\nEmergence of (TA1) appears to be a result of (TA2) and a relationship between the singular values, lying analogously to the emergence of (RA1) as a result of (RA 2-4). (TA2) implies that linearizations of hidden trajectories take a simple form after training, satisfying a linear difference equation (see Appendix A.2). In contrast to (RA4), the top singular values of the Jacobian blocks exhibit low variation across depth in some models (Figure 9). Under these assumptions, decomposing the matrix A in Equation (16) as A = UEVT, we obtain\n$|x(t)| \\approx \\mathrm{exp}(\\sigma t)|x(0)|$,\n(14)\nwhere \u03c3 is the top singular value. Since \u03c3 is independent of depth, the above norm will be exponential in t, and directly explains the pattern seen in Figure 5.\nCommon in many models at initialization, the top singular values of the Jacobian matrices decay reciprocally with depth (Figure 8). As a result, relatively linear trajectories appearing in the untrained models is not surprising. Randomly initialized Jacobian matrices with decaying singular values, coupled with Equation 17 results in the approximate exponential growth becoming linearized due to the receding impact of each layer."}, {"title": "6 Related Work", "content": "Residual Networks. ResNets have been viewed as an ensemble of shallow networks [Veit et al., 2016], with studies delving into the scaling behaviour of their trained weights [Cohen et al., 2021]. The linearization of residual blocks by their Residual Jacobians was first explored by Rothauge et al. [2019], who examined Residual Jacobians and their spectra in the context of stability analysis, and later by Li and Papyan [2023] who discovered Residual Alignment. We continue this line of work by investigating Residual Jacobians in transformer architectures and uncover a similar phenomenon in pretrained LLMs.\nIn-context learning. Previous works have observed that LLMs are able to perform in context learning [von Oswald et al., 2023, Bai et al., 2024, Ahn et al., 2023, Aky\u00fcrek et al., 2023, Xie et al., 2021, Hahn and Goyal, 2023, Xing et al., 2024]: the model is capable of performing a particular machine learning task through a few examples provided in a single prompt or context. Subsequent theoretical"}, {"title": "7 Limitations", "content": "A main limitation of the scope of our analysis is that due to the significant compute resources required to train LLMs of the size studied in this paper, our exploration is restricted to pretrained LLMs and those fine-tuned on them. Several of these models are trained independently of each other, without carefully controlling for different experimental parameters. Consequently, direct comparisons between models become challenging, and so the cause of varying levels of regularity between LLMs remains unclear. Nonetheless, this makes the consistent emergence of these properties interesting for further study."}, {"title": "8 Conclusion", "content": "A detailed description of the complexities of transformer blocks in the context of language modelling is an active area of research. Our primary goal was to enhance the understanding of the mechanics underlying transformer architectures. We provided a quantitative description of embedding trajectories within LLMs. In a large collection of models, independently trained by various organizations, we discover a remarkable level of regularity among transformer representations characterized by the alignment of residual Jacobians and emergence of linear and exponentially growing trajectories. We observe a correlation between the strength of alignment of residual Jacobian singular vectors and LLM benchmark scores, highlighting the significance of (TA2) and it's implications regarding performance."}, {"title": "A.2 Dynamical Motivation", "content": "The equality of top left and right singular vectors (RA2) suggests that the linearizations form a simple dynamical system that acts on representations. Consider a homogenous linear ODE:\n$x' = Ax$\n(15)\nIts solution is given by\n$x(t) = \\mathrm{exp}(tA)x(0) = \\lim_{L \\rightarrow \\infty} (I + \\frac{tA}{L})^L x(0)$\n(16)\nwhere the product is repeated L times. Expanding the brackets shows that x(t) can be thought of as a collection of many paths of various lengths, due to the binomial identity. This agrees with Veit et al. [2016] which views ResNets as\n$x(t) = (I + \\frac{tA}{L}) (I + \\frac{tA}{L}) (I + \\frac{tA}{L}) ... (I + \\frac{tA}{L}) x(0)$\n(17)\nHowever, Veit et al. [2016] do not make any assumptions about the alignment of the various $A_i$ matrices. The Residual Alignment phenomenon suggests ResNets as implementing the simpler system\n$x(t) = (I + \\frac{tA}{L}) (I + \\frac{tA}{L}) (I + \\frac{tA}{L}) ... (I + \\frac{tA}{L}) x(0)$,\n(18)\nwhere all the $A_i$ matrices are aligned. One benefit of this interpretation is that, as L \u2192 \u221e, we can write x(t) in a simple closed form, namely x(t) = exp(tA)x(0).\nBorrowing from the dynamical perspective, we quantify the similarity of hidden trajectories to the evolution of a linear ODE, in order to detect the emergence of a simple linearization to representations."}, {"title": "A.3 Experimental and Implementation Details", "content": "The source code used to produce the results reported in this experiment has been included as supplemental material. Models with varying parameter sizes are loaded on GPUs with appropriate memory requirements: NVIDIA A40 (nparam \u2265 40B), NVIDIA Quadro RTX 6000 for Gemma variants and when (40B > nparam > 13B), and NVIDIA Tesla T4 when (13B > nparam) except Gemma variants. 1,200 prompts from the OpenLLM leaderboard were evaluated in variable batch sizes were queued on a SLURM cluster, with appropriate adjustments depending on the memory required to load the LLM.\n\u2022 13B \u2265 nparam: 100 prompts per batch, except Gemma variants, which used 25 prompts per batch. The larger memory requirement for Gemma variants is likely due to the much larger vocabulary size in the model.\n\u2022 40B > nparam > 13B: 10 prompts per batch, except NeoX 20B which used 100 prompts per batch.\n\u2022 Nparam \u2265 40B: 50 prompts per batch.\nDue to the high memory requirement for computing Residual Jacobians, for experiments involving Residual Jacobians, NVIDIA Quadro RTX 6000 was used additionally for 13B > nparam \u2265 7B and corresponding models were quantized. Additionally, due to compute restrictions, alignment of singular vectors of Residual Jacobians was computed on a smaller subset of the 1,200 prompts."}]}