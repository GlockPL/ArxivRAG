{"title": "On the Black-box Explainability of Object Detection Models for Safe and Trustworthy Industrial Applications", "authors": ["Alain Andresa", "Aitor Martinez-Serasa", "Ibai La\u00f1aa", "Javier Del Sera"], "abstract": "In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence (XAI) methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic XAI methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP), which uses segmentation-based mask generation. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in the same scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used.\nKeywords: Explainable Artificial Intelligence, Safe Artificial Intelligence, Trustworthy Artificial Intelligence, Object Detection, Single-stage object detection, Industrial Robotics", "sections": [{"title": "1. Introduction", "content": "In recent years, Artificial Intelligence (AI) has emerged as a transformative force across various domains, especially in human-machine interaction, where it has enabled significant ad- vancements in data-driven decision-making processes. Among these advances, object detection has become a key component, finding application in critical areas such as autonomous driving, security surveillance, industrial automation, and robotics Zou et al. (2023); Muhammad et al. (2020). State-of-the-art object detection models, including Faster-RCNN Ren et al. (2017), DETR Carion et al. (2020), and the YOLO series Terven et al. (2023), have demonstrated impressive performance in identify- ing and localizing objects within images. Despite their success, the adoption of these models in highly sensitive environments remains limited, particularly in domains where errors could re- sult in serious consequences such as injury, equipment damage, or operational failures. One of the primary reasons for this hesi- tancy is the black-box nature of object detectors implemented as Deep Learning models, which to date amount to the majority of proposals in the literature. The internal activations of these are not inherently interpretable, making it challenging for end-users to trust the predictions issued by object detectors, especially in high-risk environments operating in open-world environments such as autonomous vehicles and industrial robotics.\nIn this context, the field of Explainable AI (XAI) Barredo Ar- rieta et al. (2020) aims to enhance the interpretability of AI sys- tems by their audience and ultimately, to enhance the user's trust in the output of AI-based systems. Leaving aside the category of transparent AI models (which are inherently inter- pretable and do not require any explanations for a user to un- derstand how they work), explainability methods in XAI can be broadly categorized into white-box and black-box approaches. White-box XAI methods require access to the internal workings of the model, such as weights, activations, or gradients (e.g., Grad-CAM Selvaraju et al. (2017)). While these methods can provide powerful insights, they are often limited by their de- pendence on specific model architectures, making them diffi- cult to generalize across different models and less accessible to users unfamiliar with AI research/tools. In contrast, black-box XAI methods treat the model as an opaque entity, providing ex- planations based solely on the model's input-output behavior without requiring any access to its internal components. How- ever, most black-box XAI methods are designed for classifica- tion tasks rather than for object detection Ribeiro et al. (2016); Lundberg and Lee (2017); Petsiuk (2018); Ali et al. (2023a).\nWhile classification models produce a single label per im- age, object detection models must identify and localize multi- ple objects within an image. Therefore, they need to explain not only the class prediction for each detected object \u2013 what they detect- but also the spatial reasoning behind the bound- ing boxes that define the object's location - where the object is positioned within the image. Balancing these dual aspects complicates the explanation process and requires more sophis- ticated techniques than those used for classification tasks.\nIn this paper, we address the gap in XAI methods for object detection by focusing on model-agnostic, black-box XAI tech- niques. We propose and evaluate novel black-box XAI meth- ods and XAI metrics that are specifically tailored for object detection models, without requiring access to internal model details. Our proposed methods are generalizable to object de- tection frameworks beyond those utilized in our experiments. Specifically, the contributions of this work can be summarized as follows:\n\u2022 We formally define a quantitative evaluation metric, D- Deletion, which extends the existing Deletion metric Bach et al. (2015); Petsiuk (2018) proposed for classification tasks. This metric is adapted to handle the unique challenges of ob- ject detection, including localization (as seen in Figure 4), which is of utmost importance when multiple instances of the same object appear in the same scene.\n\u2022 By using the similarity score of D-RISE Petsiuk et al. (2021), we analyze multiple mask generation methods' performance and introduce D-MFPP, an extension of MFPP Yang et al. (2020) originally developed for classification tasks. D-MFPP utilizes segmentation-based mask generation to improve ex- planations for object detection models.\n\u2022 We analyze the impact of key parameters, such as image di- mensions and the model sizes within the YOLOv8 architec- ture utilized in our experiments, which can significantly in- fluence the quality of the resulting explanations.\n\u2022 Last but not least, we facilitate the broader adoption of the developed techniques for object detection in real-world use cases by releasing the code publicly in a repository https: //github.com/aklein1995?tab=repositories 1.\nThe remainder of this paper is structured as follows: in Sec- tion 2, we first review literature related to XAI for object de- tection. In Section 3, we provide the necessary background on object detection and XAI to familiarize the reader with the key concepts used in the definitions of D-RISE and Deletion. Next, Section 4 presents the experimental setup, including datasets, object detection training configuration, employed XAI meth- ods, and evaluation metrics. In this section we also introduce our proposed D-MFPP method and D-Deletion metric. We dis- cuss our results in Section 5. Finally, Section 6 concludes the paper with a summary of our key findings and directions for future research."}, {"title": "2. Related Work", "content": "Before proceeding with the materials and novel methods in- troduced in this work, we first pause briefly at XAI methods, focusing on those used for object detection tasks and put to practice in industrial applications:\nXAI methods. As stated in the introduction, XAI offers insights into the procedure followed by an AI-based system to elicit their outputs, enabling end-users to understand and eventually trust the decisions output by the AI-based system grounded on objective data Ali et al. (2023b). To date, the majority of \u03a7\u0391\u0399 methods are designed for models learned to address classifica- tion tasks. For instance, CAM-based methods like GradCAM Selvaraju et al. (2017), GradCAM++Chattopadhay et al. (2018) and Integrated Gradients Sundararajan et al. (2017) quantify and attribute the pixel-wise importance of a given input accord- ing to the gradients with respect a target class. Moreover, mak- ing use of backpropagation, LRP Montavon et al. (2019) calcu- lates the contribution that a neuron has with neurons in consec- utive layers to get relevance scores. In contrast, perturbation- based techniques work by occluding certain parts of the input and analyzing its impact in the predictions. Within this type of techniques, LIME Ribeiro et al. (2016), approximates a NN with an interpretable model; SHAP Lundberg and Lee (2017) assigns importance values to each input feature based on Shap- ley values; RISE Petsiuk (2018) generates saliency maps by probing the model with randomly masked versions of the input image; and MFPP Yang et al. (2020) generates masks by divid- ing the input image into multi-scale superpixels. Nonetheless, none of them have been explicitly extended for object detec- tion tasks -with the exception of RISE, which has been adapted for this purpose- although techniques like SHAP can also be utilized for regression problems.\nXAI methods for object detection models. In recent times, a scarcity of XAI approaches has been proposed to support the interpretability of complex object detection models. SODEX Sejr et al. (2021) is a method capable of explaining any object detection algorithm using classification explainers, demonstrat- ing how LIME can be integrated within YOLOv4, a variant of the YOLO family of single-stage object detectors. Similarly, D-RISE Petsiuk et al. (2021) extends RISE's mask generation technique by introducing a new similarity score that assesses both the localization and classification aspects of object detec- tion models. More recently, D-CLOSE Truong et al. (2024) enhances D-RISE by producing less noisy explanations. Along with other methodological improvements, D-CLOSE uses mul- tiple levels of segmentation in the mask generation phase. Other approaches focusing on hierarchical masking have been pro- posed. Concretely, GSM-NH Yan et al. (2022) evaluates the saliency maps at multiple levels based on the information of previous less fine-grained saliency maps, whereas BODEM Moradi et al. (2024) further extends this idea but focuses on an extreme black-box scenario where only object coordinates are available."}, {"title": "3. Background", "content": "We now proceed by elaborating on key concepts needed to properly understand the details of the proposed D-MFPP tech- nique and the D-Deletion metric that lie at the core of this work. Concretely, we provide fundamentals for object detection mod- els (Subsection 3.1) and attribution-based for object detection"}, {"title": "3.1. Object Detectors", "content": "Object detectors are crucial components in computer vision tasks, capable of identifying and localizing objects within an image. They can be broadly categorized into one-stage and two- stage detectors.\nOne-stage detectors. They directly predict bounding boxes and class probabilities from input images in a single pass. These detectors treat object detection as a simple regression prob- lem, straight from image pixels to bounding box coordinates and class probabilities. To this end, they produce a dense grid of bounding box proposals and class probabilities in one step. Specifically, YOLO divides the input image into a grid and pre- dicts bounding boxes and class probabilities for each grid cell. One-stage detectors are usually adopted for real-time applica- tions due to their high speed and efficiency, although their ac- curacy tends to be lower when being compared with two-stage detectors.\nTwo-stage detectors. These models, among which Faster R- CNN Ren et al. (2017) can be considered to be the most rep- resentative one, follow a more complex approach that divides the detection process into two stages. In the first stage, a Re- gion Proposal Network (RPN) generates a set of candidate ob- ject proposals (bounding boxes) from the input image. In the second stage, these proposals are refined and classified into dif- ferent object categories by a second network. This second stage typically involves a more complex network, such as a convolu- tional neural network (CNN), which performs classification and further refinement of the bounding box coordinates. Two-stage detectors are known for their high accuracy and robustness in detecting objects of varying sizes and shapes, but they are gen- erally slower than one-stage detectors.\nMost detector networks, including Faster R-CNN and YOLO, produce a large number of bounding box proposals which are subsequently refined using confidence thresholding and Non-Maximum Suppression (NMS) to produce a set of fi- nally detected objects in the image. Each bounding box pro- posal $d_i$ can be defined as follows:\n$d_i = [L_i, O_i, P_i] = [(x_1^i, y_1^i, x_2^i, y_2^i), O_i, (p_1^i, ..., p_c^i)],$ (1)\nwhere $L_i$ defines the bounding box corners $(x_1^i, y_1^i)$ and $(x_2^i, y_2^i)$; $O_i \\in [0, 1]$ refers to the probability that bounding box $L_i$ con- tains an object of any class; and $P_i$ is a vector of probabilities $(p_1^i, ..., p_c^i)$ representing the probability that region $L_i$ belongs to each of $C$ classes. As it can be observed in this above defi- nition, unlike traditional classifiers which assign a single class label to an entire image, object detectors must simultaneously handle both classification and localization. This dual task in- volves predicting not only the class of each object but also its precise location within the image. The complexity of this task increases the difficulty of making these models interpretable."}, {"title": "3.2. Explainable Artificial Intelligence (XAI)", "content": "Despite the great performance exhibited by object detectors in manifold applications, their adoption in risk-sensitive scenar- ios is often hindered by a lack of trust and transparency by the user making decisions based on the detections issued by these models. As introduced previously, research on XAI produce techniques and methods that make the behavior and predictions of AI models understandable to humans without sacrificing per- formance Gunning et al. (2019). To this end, multiple XAI tech- niques have been proposed, which can be classified into four broad categories Ali et al. (2023b):\n\u2022 Scoop-based techniques focus on the extent of the explana- tion, providing either local explanations for specific predic- tions or global explanations for the overall model behavior.\n\u2022 Complexity-based methods consider the complexity of the model, with simpler, interpretable models offering intrinsic interpretability and more complex models requiring post-hoc explanations.\n\u2022 Model-based approaches distinguish between XAI methods that are specific to particular types of models, and those that are model-agnostic, capable of being applied to any model disregarding the specifics of their internals.\n\u2022 Methodology-based techniques are categorized by their methodological approach, such as backpropagation-based methods that trace input influences, or perturbation-based methods that alter inputs to observe changes in the output of the model.\nGiven that object detectors are typically complex neural net- works, they fall under the complexity-based category, thereby requiring post-hoc explainability methods to explain their deci- sions. Among the various methodology-based techniques, at- tribution methods are commonly used to estimate the relevance of each pixel in an image for the detection task. Attribution methods are particularly important for object detection, where both localization and classification need to be explained.\nTraditional attribution methods have been primarily devel- oped for image classifiers Abhishek and Kamath (2022), which produce a single categorical output, making them less suited for object detectors. Object detectors, unlike classifiers, gener- ate multiple detection vectors that encode not only class proba- bilities, but also localization information and additional met- rics, such as objectness scores (see Section 3.1). Further- more, techniques like NMS and confidence threshold filtering, which are used to refine bounding box proposals, add complex- ities that require a deeper understanding of the model's inter- nal workings, complicating the use of certain XAI methods, such as gradient-based approaches. Therefore, we focus on model-agnostic black-box XAI approaches, which are designed to be architecture-independent, and do not depend at all on the specifics of the model under target.\nAmong black-box XAI methods, perturbation-based ap- proaches are commonly used due to their simplicity and effec- tiveness in revealing which parts of the input are most influen- tial for the model's predictions. Perturbation-based techniques offer a direct way to assess how changes to the input image affect the model's output. By systematically altering or mask- ing parts of the input image (using masks to generate perturbed samples), these methods allow inferring the importance of dif- ferent regions based on the model's input-output behavior.\nThe typical pipeline for perturbation-based XAI methods can be divided into three stages: (1) Data Preparation, (2) Model Assessment, and (3) Importance Computation. In the Data Preparation stage, masks are generated and applied to the im- age to create perturbed samples. The Model Assessment stage involves passing these perturbed images through the model to observe the changes in output. Finally, in the Importance Com- putation stage, the importance of each pixel is calculated by comparing the model's outputs for the original and perturbed images. While the Model Assessment stage remains consistent across methods, with each perturbed image passed through the model, the Importance Computation varies depending on the XAI approach used. This can range from simple techniques like retraining a model (e.g., LIME) to more complex approaches. Since the effectiveness of these methods largely depends on how the perturbed images are generated, three mask generation algorithms are next described:\n\u2022 Sliding Window: This method, which is similar to the Occlu- sion technique proposed in Zeiler and Fergus (2014), system- atically moves a window of fixed size across the image and sets the region within the window to a constant value (e.g., zero) to occlude that part of the image. By iteratively sliding the window across the entire image, we can assess the impact of each occluded region on the model's output. The method requires specifying the window size, which determines the area of the image being occluded at each step, and the stride, which sets how much the window moves between iterations.\n\u2022 RISE: Randomized Input Sampling for Explanation (RISE) Petsiuk (2018) involves sampling N binary masks of size hx w, which are smaller than the original image size H \u00d7 W. Each element in the mask is independently set to 1 with prob- ability p and to 0 with the remaining probability 1 \u2013 p. These masks are then upsampled to size $(h+1)\\cdot C_H \\times (w+1) C_w$ using bilinear interpolation, where $C_H \\times C_w = [H/h] \\times [W/w]$. The upsampled masks are cropped to the original image size H \u00d7 W with uniformly random offsets ranging from (0, 0) to $(C_H, C_w)$. This method creates a diverse set of masks that cover different parts of the image, allowing for a comprehen- sive evaluation of the importance of various regions.\n\u2022 MFPP: The so-called Morphological Fragmental Perturba- tion Pyramid (MFPP) Yang et al. (2020) method divides the input image into multi-scale fragments and perturbs them randomly. In this sense, it is similar to RISE, but instead of perturbing elements of the generated masks with dimen- sion h \u00d7 w, MFPP defines regions according to segmenta- tions at different scales. Depending on the number of defined fragments, the regions would be more fine-grained yet more time-consuming. The segments are dependent on each im- age, requiring the creation of new masks for every image."}, {"title": "4. Materials and Methods", "content": "This section describes the industrial robotics use cases pro- viding the datasets (Subsection 4.1), XAI methods (Subsection 4.3) and the explanation quality metrics (Subsection 4.4) con- sidered in our work. The novel XAI technique and quality met- rics proposed in this manuscript are also described in detail in Subsection 4.3."}, {"title": "4.1. Industrial Robotics Datasets under Consideration", "content": "The datasets used in this manuscript have been collected during the course of the ULTIMATE project, https:// ultimate-project.eu/, which features two distinct real robotics use cases Kozik et al. (2024). The first dataset, from PIAP https://piap.lukasiewicz.gov.pl/), involves a collaborative workspace where a human and a robotic arm work together. The second dataset, provided by Robotnik https: //robotnik.eu/, focuses on a battery assembly area, where a robotic arm assembles components for a battery kit.\nDataset 1: Human-Robot Dataset. This dataset consists of 96 images captured from three different cameras, as exemplified in Figure 2, with 32 images taken from each camera. The dataset includes two object classes: human and gripper. Importantly, each image in this dataset contains only a single object of each class, meaning a maximum of one human and one gripper per image. To ensure a diverse and representative sample, we ap- plied feature extraction using ResNet He et al. (2015) to obtain embeddings for the entire dataset. The dimensionality of these embeddings was reduced using Principal Component Analysis (PCA), followed by K-means clustering (with k = 8 clusters). From each cluster, four images were randomly selected, result- ing in a final subset. The data were split into three sets: 72 images for training (75%), 6 for validation (6.25%), and 18 for testing (18.75%). To maintain consistency, we applied the same partitioning to the data from each camera. This resulted in 24 images for training, 2 for validation, and 6 for testing from each camera.\nDataset 2: Battery Assembly Dataset. This dataset consists of 7 images, all captured from a bird's-eye (top-down) view, showing a robotic arm assembling a battery kit. The dataset includes five distinct object types: individual battery, bms_a, bms_b, battery holder, and unknown object. In contrast to the Human-Robot Dataset, each image in the Battery Assembly Dataset may contain mul- tiple objects of the same class, such as several individual bat- teries in a single scene.\nIt is worth noting that XAI techniques can be applied to any type of data. When applied to training data, they help reveal what the model has learned to focus on during training. When applied to test data, they provide insight into how well the model generalizes to new, unseen examples. For the Human- Robot Dataset, XAI explanations were applied exclusively to the test images, allowing us to assess the model's behavior on unseen data. However, for the Battery Assembly Dataset, given the limited number of images (only 7), XAI explanations were applied to the entire dataset."}, {"title": "4.2. Object Detection Model: YOLOv8", "content": "Among the possible object detector models, we selected one of the state-of-the-art options, YOLOv8, due to its numer- ous advancements over previous versions and its robust perfor- mance in object detection tasks Terven et al. (2023). YOLOv8 Reis et al. (2024) integrates a novel combination of Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) architectures, enhancing its ability to detect objects at various scales and resolutions. The FPN gradually reduces the spatial resolution of the input image while increasing feature channels, facilitating multi-scale object detection. The PAN architecture further aggregates features from different levels through skip connections, improving the detection of objects with diverse sizes and shapes. Additionally, YOLOv8 introduces an anchor- free detection mechanism that directly predicts the center of an object (instead of the offset from a known anchor box), re- ducing the number of box proposals and speeding-up the post- processing. Furthermore, it was trained with larger and more diverse datasets including the popular COCO dataset, improv- ing its performance across a wider range of images.\nYOLOv8 was developed and released by Ultralytics, and al- though the model and its weights are open-source, most users are expected to utilize the Ultralytics framework for its en- hanced usability. However, unlike previous YOLO releases where the probability for each class per predicted box was ac- cessible, in YOLOv8, the Ultralytics API outputs only the prob- ability for the class with the highest confidence in each box2.\nConsequently, by default, YOLOv8 outputs:\n$d_i = [L_i, O_i, C_i] = [(x_1^i, y_1^i, x_2^i, y_2^i), O_i, C_i],$ (2)\nwhere $L_i = (x_1^i, y_1^i, x_2^i, y_2^i)$ represents the coordinates of the bounding box, $O_i$ denotes the objectness score, and $C_i$ corresponds to the predicted class label for the object within the bounding box, which differs with respect to the outputs shown in Expression (1)."}, {"title": "4.3. Explainability Methods", "content": "We evaluate four popular methods for generating visual ex- planations of black-box models: LIME, RISE, D-RISE, and D- MFPP. The first two methods, LIME and RISE, were originally developed for image classifiers but can be adapted to object de- tectors. However, they primarily focus on explaining classifica- tion aspects and are not capable of addressing localization char- acteristics. In contrast, D-RISE is one of the first XAI methods specifically designed for object detectors, providing explana- tions that encompass both classification and localization. Ad- ditionally, we extend the existing MFPP method (originally tai- lored for classifiers) into a version suitable for object detection, which we refer to as D-MFPP. In what follows we briefly de- scribe them, flowing into a description of the proposed D-MFPP approach:\n\u2022 LIME was originally designed to explain the predictions of any classifier by approximating it locally with an inter- pretable model. To explain the prediction for an input image I, LIME fits an interpretable model g (e.g., a linear model) to approximate the behavior of the black-box model f locally around I. The similarity between the original image and the perturbed samples is measured using a kernel function $\\pi(z)$. When image explanations are targeted, LIME groups con- tiguous pixels into superpixels based on similar features they represent. This approach allows LIME to measure the impor- tance of regions in the image rather than individual pixels, making the explanations more interpretable.\n\u2022 As introduced in the previous section, RISE Petsiuk (2018) was originally designed for deep neural networks that take images as input and output a class probability (e.g., a classi- fier like ResNet-50). It generates saliency maps that indicate the importance of each pixel by applying randomly gener- ated binary masks $M_i$ to the input image I and observing the changes in the model's output $f(I \\odot M_i)$. In RISE, N binary masks $M_i \\in {0,1}^{h\\times w}$ are generated (as explained in Section 3.2). These masks are then applied to the input im- age I to generate masked images $I' = I \\odot M_i$, where $\\odot$ denotes element-wise multiplication. The model is evaluated on each masked image $I'$ to obtain the outputs $f(I \\odot M_i)$. The im- portance score for each pixel (x, y) is then calculated as the weighted sum of the outputs:\n$S_{I,f}(x,y) = \\frac{1}{N} \\sum_{i=1}^{N} (I \\odot M_i) \\cdot f(I \\odot M_i) \\cdot M_i(x, y)$ (3)\nwhere the weights $M_i(x, y)$ represent the value of mask i at pixel (x, y). The intuition behind RISE is that $f(I \\odot M)$ would be high when pixels preserved by mask $M_i$ are important. Although this is true when having infinite diverse masks, in practice RISE calculates each pixel's importance empirically by Monte Carlo sampling. Therefore, RISE largely depends on the number of masks (N) and how they are generated (i.e., is sensitive to the selected probability p and resolution s)."}, {"title": "4.3.1. D-RISE and Proposed D-MFPP Approach", "content": "Unlike the other two approaches originally designed for clas- sifiers that measure solely classification aspects, D-RISE (De- tector Randomized Input Sampling for Explanation) Petsiuk et al. (2021) was designed to explain both the classification and localization of a detection. In this sense, D-RISE extends RISE by producing saliency maps specifically for object detectors. As previously seen in Section 3.1, the output given by an object detector differs from the probability vector given by a classi- fier, obtaining localization information $L_i$, an objectness score $O_i$ and the probability of classifying each bounding box to any of the considered classes $P_i$. As a consequence, Expression (3) used by RISE is replaced in D-RISE with a new similarity score, given by:\n$S_{I,f}(d_i, d_j) = S_L(d_i, d_j) \\cdot S_P(d_i, d_j) \\cdot S_O(d_i, d_j),$ (4)\nwhere $S_L = IoU(L_i, L_j), S_P = P_i \\cdot P_j / (||P_i|| \\cdot ||P_j||)$, and $S_O = O_j$. In this formulation, $S_L$ represents the spatial proximity of the bounding boxes encoded by the target detection $d_i$ and the proposal $d_j$, measured using the Intersection over Union (IoU); the term $S_P$ evaluates the similarity between the class probabilities of the target detection and the proposal using cosine similarity; and $S_O$ incorporates the objectness score of the proposal $O_j$. It is important to note that for a detection target $d_i$, there would potentially be more than one detection proposals $d_j$. There- fore, we would have multiple $S_{I,f}(d_i, d_j)$. As explained in D- RISE, the explanations consider only the detection with maxi- mal score for each mask:\n$S_{I,f} (d_t, f(M_i \\odot I)) = \\underset{d_j \\in f(M_i \\odot I)}{\\text{max}} S_{I,f}(d_i, d_j).$ (5)\nGiven the YOLOv8 outputs explained in Section 4.2, which do not provide the class probability vector $P_i$ without modify- ing its architecture (an approach we want to avoid within the scope of this paper), we must adapt the similarity score to only consider $S_L$ and $S_O$. Consequently, the modified similarity score can be expressed as:\n$S_{I,f}(d_t, d_j) = S_L(d_1, d_j) \\cdot S_O(d_t, d_j) = IoU(L_t, L_j) \\cdot O_j.$ (6)\nThis adjustment allows still utilizing D-RISE effectively for generating saliency maps with the default YOLOv8 model, fo- cusing on the spatial and objectness aspects of detections, while maintaining the integrity of the model's original architecture.\nSimilarly, we can adopt this similarity score but apply it with a different mask generation process. The MFPP method in- troduced in Section 3.2, originally designed for classification tasks, can be extended by applying Equation (6), resulting in D-MFPP. To the best of our knowledge, no previous work has proposed this variant of MFPP for object detection tasks."}, {"title": "4.4. Metrics", "content": "Evaluating the performance of attribution-based explainabil- ity methods for image data involves assessing how well the gen- erated relevance heatmaps highlight important regions of the input image that contribute to the model's decision. Gener- ally, according to Hedstr\u00f6m et al. (2023), explanation quality metrics can be grouped into six categories based on their log- ical similarity: faithfulness, robustness, localization, complex- ity, randomization, and axiomatic metrics. In this study, we focus on two of these categories that are particularly relevant to object detection: localization (Subsection 4.4.1) and faithful- ness (Subsection 4.4.2)."}, {"title": "4.4.1. Localization", "content": "Localization metrics evaluate whether the explainable evi- dence is centered around a region of interest (RoI) defined by a bounding box, segmentation mask, or a cell within a grid. These metrics aim to verify if the saliency maps correctly high- light the areas in the image that contain the object of interest. Among them, our experiments will consider:\n\u2022 Pointing Game (PG), which is a human evaluation metric in- troduced in Zhang et al. (2018). If the highest saliency point lies inside the human-annotated bounding box of an object, it is counted as a hit. The PG accuracy is given by:\n$PG = \\frac{\\#Hits}{\\#Hits + \\#Misses},$ (7)\nwhich is averaged over all categories in the dataset.\n\u2022 Energy-based Pointing Game (EBPG) Wang et al. (2020), which measures the proportion of activations within the given bounding box relative to the whole activation in the image. It assesses how much of the model's activation en- ergy is concentrated within the predefined region of interest. Formally:\n$EBPG = \\frac{\\sum_{(x,y)\\in bbox} S_{I,f}(x, y)}{\\sum_{(x,y)\\in bbox} S_{I,f}(x, y) + \\sum_{(x,y)\\notin bbox} S_{I,f}(x, y)},$ (8)\nwhere $S_{I,f}(x, y)$ represent the saliency score at pixel (x, y), $\\sum_{(x,y)\\in bbox} S_{I,f}(x, y)$ represents the sum of activation values within the bounding box, and $\\sum_{(x,y)\\notin bbox} S_{I,f}(x, y)$ represents the sum of activation values outside the bounding box."}, {"title": "4.4.2. Faithfulness", "content": "Metrics accounting for faithfulness quantify to what extent explanations follow the predictive behavior of the model", "them": "n\u2022 Deletion: Inspired by the work by Bach et al. (2015)", "as": "n$Deletion(I,S,c) = AUC (\\{Pr(f (I \\odot M_k) = c)\\}_{k=1}^K),$ (9)\nwhere I is"}]}