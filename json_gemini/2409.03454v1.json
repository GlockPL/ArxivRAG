{"title": "How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes", "authors": ["Inacio Vieira", "Will Allred", "Seamus Lankford", "Sheila Castilho Monteiro De Sousa", "Andy Way"], "abstract": "Decoder-only LLMs have shown impressive performance in MT due to their ability to learn from extensive datasets and generate high-quality translations. However, LLMs often struggle with the nuances and style required for organisation-specific translation. In this study, we explore the effectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3 8B Instruct, leveraging translation memories (TMs), as a valuable resource to enhance accuracy and efficiency.\nWe investigate the impact of fine-tuning the Llama 3 model using TMs from a specific organisation in the software sector. Our experiments cover five translation directions across languages of varying resource levels (English to Brazilian Portuguese, Czech, German, Finnish, and Korean). We analyse diverse sizes of training datasets (1k to 207k segments) to evaluate their influence on translation quality. We fine-tune separate models for each training set and evaluate their performance based on automatic metrics, BLEU, chrF++, TER, and COMET.\nOur findings reveal improvement in translation performance with larger datasets across all metrics. On average, BLEU and COMET scores increase by 13 and 25 points, respectively, on the largest training set against the baseline model. Notably, there is a performance deterioration in comparison with the baseline model when fine-tuning on only 1k and 2k examples; however, we observe a substantial improvement as the training dataset size increases. The study highlights the potential of integrating TMs with LLMs to create bespoke translation models tailored to the specific needs of businesses, thus enhancing translation quality and reducing turn-around times. This approach offers a valuable insight for organisations seeking to leverage TMs and LLMs for optimal translation outcomes, especially in narrower domains.", "sections": [{"title": "Introduction", "content": "In recent years, decoder-only large language models (LLMs) have revolutionised the machine translation (MT) field due to their ability to learn from vast amounts of data and generate high-quality translations (Alves et al., 2023a; Moslem et al., 2023a; Mu et al., 2023; Robinson et al., 2023; Zhu et al., 2023; Lyu et al., 2024). LLMs, such as Llama 3 8B Instruct,\u00b9 have shown impressive capabilities in adapting to translation tasks, generating human-like accurate output, making them invaluable tools for the sector (Li et al., 2023; Moslem et al., 2023b; Lyu et al., 2024). However, out-of-the-box LLMs do not always capture all the nuances, appropriate tone, and terminology required for specialised or organisation-specific translations (Moslem et al., 2022; Alves et al., 2023b; Zheng et al., 2024). This is where translation memories (TMs) offer a potential solution.\nA TM is a database that stores previously human-translated segments and their respective translations. They are particularly useful to language service providers (LSPs) as they deal with repetitive content and organisation-specific style and terminology, enhancing the efficiency and accuracy of translations (Bloodgood and Strauss, 2014; Bulte and Tezcan, 2019; Moslem et al., 2023a). Therefore, the integration of TMs and LLMs can create models that better understand organisational requirements and lead to higher quality outputs and reduced turnaround times. However, this approach depends on several factors, like the amount, quality and specificity of the TMs used as training data for fine-tuning.\nPrevious work explored fine-tuning of models with TM for translation for specific domains and the benefit that offers to performance (Haque et al., 2020; Moslem et al., 2022). Accordingly, TM provides much value because of its high quality and domain relevance (Bulte and Tezcan, 2019; Xu et al., 2020; Cai et al., 2021; Knowles and Littell, 2022). This research highlights the gains available when leveraging existing TMs during the fine-tuning process of LLMs.\nIn this paper, we investigate a real-life scenario where we fine-tune Llama 3 8B Instruct (Llama Team, 2024) using TMs from a specific organisation. Additionally, since increasing the fine-tuning data requires dedicating more resources and time, we explore different dataset sizes to evaluate their impact on translation quality and identify the most efficient return on investment. We conduct experiments in five translation directions (from English) on languages of varying resource level (Brazilian Portuguese (PT-BR), Czech (CS), German (DE), Finnish (FI), and Korean (KO)). This approach can lead to bespoke translation models that cater to the unique needs of different companies when compared to generic LLMs."}, {"title": "Methodology", "content": "2.1 Data\nThe raw dataset consists of TMs from an anonymous organisation that operates in the software sector. The three datasets employed cover knowledge base, mobile user interface, and mobile reference materials.\nThe five target languages dataset (PT-BR, CS, DE, FI, and KO) are filtered to remove duplicates, source-copies, and segments over 150 words to ensure none would go over the maximum length set during training. All HTML tags are removed, and double spaces are converted to single spaces. Any rows containing only dates, version numbers, or any programming language are also removed. Rows are then randomly shuffled to mitigate any temporal bias that could arise from the chronological order of the data, ensure the model does not memorise sequences, and prevent the evaluation set from being biased towards a particular section of the data.\nThe dataset is then transformed into an inter-lingual aligned dataset for all five target languages where any rows with missing translations for any target languages, are dropped. This results in a dataset where all source segments have translations available in all five target languages. The dataset is then split into training, development, and test sets, as shown in Table 1.\nFurther filtering is applied to the test set removing segments that had over 75% similarity with any segments in the training dataset to ensure robust testing and minimal memorisation. We measure similarity as a combination of the Levenshtein distance (Levenshtein, 1965) and a 5-gram-based similarity"}, {"title": "Model", "content": "We use the Llama 3 8B Instruct model and its associated okenizer (Llama Team, 2024). The decision between the Instruct and the base model is based on an extensive MT evaluation of Llama 3 models (Wu et al., 2024) using the Flores-2002 dataset (Guzm\u00e1n et al., 2019; Costa-juss\u00e0 et al., 2022). Even though Wu (2024) dealt with the opposite language direction (X to English), we consider the close results between Instruct and the base model involving the five languages included in this paper to be a good indicator of proximity in performance between the models. Our baseline consists of the test set metric results obtained from the out-of-the-box Llama 3 8B Instruct model. We use QLORA (Hu et al., 2021; Dettmers et al., 2023) for efficient fine-tuning with 4-bit quantisation using Hugging Face Transformers. We perform fine-tuning on a high performance cluster with four A100-SXM4-80GB GPUs. From Hugging Face, we leverage the Supervised Fine-Tuning Training (SFTTrainer),\u00b3 which is a wrapper of the Trainer class\u2074 optimized for fine-tuning language models like Llama. On the largest dataset size, fine-tuning takes approximately 2.3 hours (Appendix A)."}, {"title": "Inference", "content": "At inference time, we use many of the recommended parameters from previous work (Moslem et al., 2023b) and model documentation to produce translation outputs from the baseline model and the fine-tuned versions (cf. Appendix C). Meta's Llama 3 documentation provides a recommended prompt format and instructions to implement special tokens during inference and training (Llama Team, 2024).\nThe prompt and the source segment were"}, {"title": "Prompting", "content": "At inference time, we use many of the recommended parameters from previous work (Moslem et al., 2023b) and model documentation to produce translation outputs from the baseline model and the fine-tuned versions (cf. Appendix C). Meta's Llama 3 documentation provides a recommended prompt format and instructions to implement special tokens during inference and training (Llama Team, 2024).\nThe prompt and the source segment were"}, {"title": "Translation", "content": "In order to obtain higher efficiency, both baseline and fine-tuned models are converted to the CTranslate2\u2076 (Klein et al., 2020) format (with 8-bit quantisation) and provided with parameters for inference (cf. Appendix C)."}, {"title": "Stopping Criteria and Post-processing", "content": "In early experiments, we observe frequent instances of overgeneration; an issue recently explored further by Zheng et al. (2024). By using '}assistant' as a stop token in our stopping criteria, we find much less post-processing is required in order to obtain the pure translation.\nOur post-processing consists of extracting the translation by removing the \u2018{\"translation\": \u201c"}, {"title": "Evaluation", "content": "To evaluate the performance of our models, we report BLEU (Papineni et al., 2002), chrF++ (Popovi\u0107, 2017), TER (Snover et al., 2006) via sacreBLEU,\u2077 and COMET (Rei et al., 2020). We use multiple metrics to make our experiments more comparable to a wider variety of work and to provide insight into certain aspects of performance.\nIt is important to note that the experiment aims to show the training efficiency of the PEFT fine-tuning method and its ability to approximate the model's translating capabilities to the training material. Therefore, we pay special attention to the automatic metrics measuring n-gram differences and edits (BLEU, chrF++, TER) whilst still considering the quality estimation aspect of COMET as a means of comparing inter-source languages and other similar research."}, {"title": "Results and Discussion", "content": "The results in Table 4 show an increase in performance across all the languages for all datasets with more than 5k segments compared to the baseline. The fully aligned 14.7k dataset sees a BLEU score increase of 4.8 points or relative increase of 17.42% on average over the baseline, over all target languages, while chrF++ and COMET increases 7.1 and 16.9, respectively. Similarly, TER decreases by 9 points. The 100k+ datasets also demonstrate consistent performance gains with an average increase of 13.7 BLEU, 12.7 chrF++, and 25 COMET, while TER decreases to 15.5.\nTo provide a point of comparison, we evaluate the performance of GPT-3.5\u2079 on our test set. While GPT-3.5 outperforms our highest-performing model in BLEU and chrF++ for DE and FI, the 100k+ datasets often surpass GPT-3.5 in other languages and metrics. This demonstrates the effectiveness of creating bespoke models through fine-tuning mid-sized LLMs when leveraging domain-specific data. Targeted fine-tuning can yield competitive or superior results compared to larger, general-purpose models like GPT-3.5."}, {"title": "Small Dataset Deterioration", "content": "Regarding translation quality across different training data sizes, we note a deterioration in quality for models trained on the smaller datasets (1k and 2k) in relation to the baseline. Despite a smooth reduction in both training and evaluation loss during training across all sizes, these smaller datasets still lead to poorer performance on all metrics. This can be due to the fact that the 1k and 2k datasets are insufficient to offer the models a wide enough variety of examples, leading to overfitting where the model performs well on training but poorly on the unseen test data (Barone et al., 2017; Atrio and Popescu-Belis, 2022; Garcia et al., 2023; Ram\u00edrez Atrio, 2023).\nIt is possible that the lack of diversity in the smaller models fails to capture the range of linguistic and translation nuances present in the test data which hinders the model's ability to generalise beyond the specific examples seen during training. Furthermore, the smaller datasets may make the models more susceptible to noise, such as translation errors or inconsistencies, leading to the learning of incorrect patterns and degrading performance on the test data, affecting the automatic metrics results, while the loss continues to drop due to fitting noisy data (Barone et al., 2017; Atrio and Popescu-Belis, 2022; Ram\u00edrez Atrio, 2023).\nAnother possible explanation for the deterioration is a decrease in training data quality in the 1k and 2k dataset sizes. To examine this, we use COMET-Kiwi (Rei et al., 2023), a popular quality estimation metric, to evaluate the quality of the training data. The scores are consistent for each language with variations within a narrow range of 1-2 points (cf. Appendix D). For example, FI has the highest variation with a maximum score of 79.58 (1k and 14.7K) and a minimum score of 78.12 (5k), resulting in a range of only 1.46 points. The minimal variation in score indicates consistent data quality across all dataset sizes for each language. Therefore, the deterioration in performance is unlikely to be due to a decrease in data quality for the 1k and 2k training data sizes.\nHyperparameter fine-tuning could be employed to mitigate this early deterioration in situations where only small datasets are available. This may include dropout or other regularisation techniques to prevent overfitting on small training sets. Adjustment of the learning rate, batch sizes and QLoRA hyperparameters should also be explored to deal with this specific case of deterioration (Barone et al., 2017; Atrio and Popescu-Belis, 2022; Dettmers et al., 2023; Ram\u00edrez Atrio, 2023).\nOverall, a different approach is required in order to obtain gains when the training data is scarce. Our experiments suggest the need for at least 5k examples to achieve an improvement in metrics under the hyper-specific domain and circumstances we explore.\nThe issues above seem to be mitigated on the larger sets whilst maintaining the same hyperparameters."}, {"title": "Resource Level", "content": "It is interesting to note that the performance for KO has improved after the 14.7k fine-tuning and becomes comparable to or better than the performance of the other language directions, despite the lower initial baseline score across all metrics. For instance, the COMET score for the KO baseline is 36.5 while the average for all other languages is 57.7. We find that the lower resource languages (KO being the lowest of the target languages explored) have the highest relative gains, turning around a very poor baseline across all metrics. The COMET score for KO increased to 84.3 compared to the average of 84.5 in the 100k+ datasets for PR-BR, DE, FI, and CS, resulting in KO's comparable performance to the high resource languages, i.e. PT-BR and DE.\nThese results probably relate not only to the resource level of the language but also to the amount of Korean data in the Llama 3 training recipe. According to MetaAI, \u201cover 5% of the Llama 3 pre-training dataset consists of high-quality non-English data that covers over 30 languages\" (Meta, 2024). While the Llama Team provides more detail on the training and data mix Llama 3, the exact proportion of Korean data is not discussed (Llama Team, 2024). Our baseline metrics suggest that Korean does not feature highly on that list given that it scores significantly lower than all other languages. This might be attributed to the fact that there were not enough examples to produce a firm understanding of the language but enough to provide a foundation that heavily benefited from fine-tuning. As mentioned, this is an assumption as we lack sufficiently detailed information on the training recipe.\nWhen looking at the target languages, we note that PT-BR shows the best performance at 14.7k and 100k+ dataset. This indicates that, even for a well-resourced language, the foundation model gained a strong understanding of the language during pre-training. However, it did not seem to benefit as much from fine-tuning as KO, a lower resource language. This corroborates the finding that resource level is a strong determiner of LLM MT performance (Robinson et al., 2023)."}, {"title": "Human Evaluation", "content": "Regarding the human evaluation, the qualitative comments from the translators reveal that the largest model struggles with ambiguity. Evaluators mention that segments that lacked complete information needed to be completely reworked. For example, the segment, \"Get basic, step-by-step instructions to learn\" lacks a final object, which impacts the translation. While human translators often face and resolve such ambiguities through research or decision-making with incomplete information, the model processes segments in isolation, unable to access potentially clarifying context from adjacent segments. This limitation provides insight into the model's performance in real-world translation scenarios."}, {"title": "Conclusions", "content": "Fine-tuning on TMs has been demonstrated to enhance the performance of LLMs in MT tasks. In this paper, we investigate the relationship between automatic metric results and training set sizes to identify the optimal balance where resource investment yields the most significant improvements in translation quality. In our experiments, it has become evident that fine-tuning on training datasets whose size is larger than 5k examples returned increasingly better results in 19 out of the 20 language-training set size combinations explored.\nBy leveraging TMs, the model becomes more adept at recognising and reproducing previously translated segments, their style, and terminology. Furthermore, fine-tuning on TM data helps the model adapt to specialised fields.\nThe test and training sets used come from a much narrower corpus of data than in similar experiments that deal with wider domains, i.e. medicine (Moslem et al., 2023b). The hyper-specific nature of the training data employed in our approach may partly explain the promising results. We therefore leverage the advantage that smaller models licensed for business-use offer; they can be adapted several times over for narrow and specific domains, as well as multiple languages with little investment, instead of aiming for a more general purpose or multilingual model. The hyper-specific purpose of our trained model, i.e. one language direction and a narrow domain, suits the size and easiness of training of an 8B parameter mode.\nBeing a commonly experienced scenario in the localization industry, this is an under-explored approach that organisations could be pursuing in order to make the most out of their access to TMs and LLMs for MT in order to obtain the best possible return on investment when leveraging their previously human-translated material.\nLow-resource languages seem to be in a perfect position to benefit from leveraging small business-friendly models, like Llama 3 8B. The gains in automatic metric results for KO are substantially higher for high resource languages, like PT-BR and DE, returning the highest increase in performance compared to the metrics obtained from training on similar set sizes in those languages. KO observes an increase of 130% on COMET from the baseline to the 100k+ dataset, whereas the average increase amongst the other target languages is 46% (cf. Table 4).\nIt is important to mention that, just as Wu (2024) acknowledges the FLORES-200 dataset leakage into Llama 3, it is possible that some of our test set was also scraped by the Llama 3 models, as parts of the material were published online prior to the Llama 3 family's pre-training. We face the same challenge as the whole AI researching community, forced to either constantly come up with new test sets or simply acknowledge the potential leakage of test data (Xu et al., 2024). We urge large tech companies to disclose at a minimum the test sets that were not ingested by their models for the benefit of the whole community. We acknowledge the Llama Team's leadership in this area (Llama Team, 2024)."}, {"title": "Future Work", "content": "Future work in the area may benefit from the introduction of checkpoints during training and subsequent intermediate evaluation would enable the visualisation of a clearer learning curve, and the identification of potential dips in performance and points of diminishing returns. This approach would facilitate the analysis and allow for a finer and more efficient evaluation process.\nIn the future, we aim to obtain a bespoke test set directly from the organisation that owns the TMs. This tailored test set would consist of examples specifically designed in-house according to strict guidelines, ensuring they are completely original and reflective of the organisation's unique requirements and style. By using a bespoke and unseen test set, we can more accurately assess the performance of our fine-tuned models in a real-world context.\nFinally, further investigation is required with regard to the training hyperparameters across the different dataset sizes in order to obtain better results with smaller training sets under 5k examples. Several strategies can be explored to optimise performance on smaller datasets. Adjustments such as modifying the dropout rates to prevent overfitting, applying regularisation techniques to enhance model generalisation, and fine-tuning the learning rate to ensure efficient convergence can be particularly beneficial in this case."}, {"title": "Appendix B", "content": "B.1 Special Token Descriptions\n<begin_of_text >: This is equivalent to the BOS token.\n<eot_id>: This signifies the end of the message in a turn.\n< |start_header_id| >{role}< |end_header_id| >: These tokens enclose the role for a particular message. The possible roles can be: system, user, assistant.\n< end_of_text >: This is equivalent to the EOS token."}, {"title": "Prompt", "content": "<\\begin_of_text| > < |start_header_id| >system< \\end_header_id >\nYou are a helpful Al assistant for translation from {source_language} to {target_language}. You MUST answer with the following JSON scheme: { \"translation\": \"string\"} <|eot_id >\n<|start_header_id| >user< |end_header_id >\n{source_sentence}<|eot_id| >< |start_header_id| >assistant< |end_header_id >"}, {"title": "Training Prompt", "content": "<|begin_of_text|><|start_header_id| >system<\\end_header_id| >\nYou are a helpful AI assistant for translation from {source_language} to {target_language}. You MUST answer with the following JSON scheme: { \"translation\": \"string\"} <|eot_id >\n<|start_header_id| >user< \\end_header_id|> {source_sentence}<|eot_id >\n<|start_header_id| >assistant< \\end_header_id| >{target_sentence}<|end_of_text>"}, {"title": "Appendix C", "content": "Inference Parameters\nsampling_topk\nmax_batch_size\nmin_length\nmax_length"}]}