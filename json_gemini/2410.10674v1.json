{"title": "Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach", "authors": ["Rory Young", "Nicolas Pugeault"], "abstract": "Deep reinforcement learning agents achieve state-of-the-art performance in a wide range of simulated control tasks. However, successful applications to real-world problems remain limited. One reason for this dichotomy is because the learned policies are not robust to observation noise or adversarial attacks. In this paper, we investigate the robustness of deep RL policies to a single small state perturbation in deterministic continuous control tasks. We demonstrate that RL policies can be deterministically chaotic as small perturbations to the system state have a large impact on subsequent state and reward trajectories. This unstable non-linear behaviour has two consequences: First, inaccuracies in sensor readings, or adversarial attacks, can cause significant performance degradation; Second, even policies that show robust performance in terms of rewards may have unpredictable behaviour in practice. These two facets of chaos in RL policies drastically restrict the application of deep RL to real-world problems. To address this issue, we propose an improvement on the successful Dreamer V3 architecture, implementing a Maximal Lyapunov Exponent regularisation. This new approach reduces the chaotic state dynamics, rendering the learnt policies more resilient to sensor noise or adversarial attacks and thereby improving the suitability of Deep Reinforcement Learning for real-world applications.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) have revolutionised reinforcement learning (RL) [25], enabling agents to excel in a diverse set of simulated control tasks [12, 16, 22, 23, 24]. However, trained deep RL policies are not robust controllers as DNNs are vulnerable to adversarial attacks [7, 26]. Adding a small amount of noise to each observation can cause these policies to make poor decisions, considerably degrading their overall performance [10, 11, 13]. This lack of stability poses a significant threat when applying deep RL to real-world environments where inaccurate sensors can easily introduce noise [5]. In this work, we argue that even high-performing deep RL policies are not robust controllers as they can create a chaotic closed-loop control system [4, 14]. These systems are characterised by a high sensitivity to initial conditions with small changes in initial system states producing vastly different long-term outcomes.\nIn this paper, we use the spectrum of Lyapunov Exponents [15] to empirically measure the stability of the policies learnt by state-of-the-art deep RL approaches subject to small state perturbations. We show that these controllers can produce chaotic state and reward dynamics when controlling continuous environments. Consequently, a single noisy observation has a dramatic long-term impact on these control systems, with the subsequent approximate state and reward trajectories diverging significantly. This instability poses two problems for the safe deployment of deep RL in real-world environments.\n1.  The chaotic state dynamics create a fractal return surface [28] which is highly sensitive to small changes in the system state. The high-frequency oscillations in this function cause a lack of robustness as small state perturbations can produce significantly different total rewards.\n2.  Even for high-performing policies with stable returns, it remains impossible to accurately predict the long-term behaviour of these chaotic control systems as they rely on noisy partially observable sensors to attain an observation. This unpredictability means safe and reliable behaviour cannot be guaranteed.\nTo address these issues, we propose Maximal Lyapunov Exponent regularisation for Dreamer V3 [9]. This novel technique estimates the local state divergence using the Recurrent State Space model and incorporates this term into the policy loss. We demonstrate that this regularisation term significantly reduces the chaotic dynamics produced by this state-of-the-art deep RL controller. This increased stability dramatically improves the robustness of the policy, thus improving the feasibility of deep RL agents for real-world continuous control tasks."}, {"title": "2 Background", "content": "Reinforcement learning provides a data-driven method for solving sequential decision-making problems. This control interaction is represented by a deterministic Markov Decision Process (MDP) with state space $S \\subset \\mathbb{R}^n$, action space $A \\subset \\mathbb{R}^m$, scalar reward function $r: S \\times A \\rightarrow \\mathbb{R}$, state transition function $f: S \\times A \\rightarrow S$ and initial state distribution $p_0 \\subseteq S$. The objective of an RL agent is to learn a policy $\\pi_{\\theta}: S \\rightarrow A$ which maximises the sum of discounted returns (Equation 1) for a given discount factor $\\gamma \\in [0, 1)$.\n$J(\\theta) = \\mathbb{E}_{s \\sim p_0} [\\sum_{t=0}^{\\infty} \\gamma^t \\times r(S_t, a_t)]$ (1)\nRL policies are said to be robust if they can maintain consistent behaviour and reliable performance in the face of noise or adversarial attacks. This stability is crucial for the safe deployment of deep RL"}, {"title": "2.2 Measuring stability: Lyapunov Exponents", "content": "Lyapunov Exponents (LEs) [15, 20] provide a method for quantifying the stability of complex, non-linear, high-dimensional systems by measuring the deformation rate of a small hyperellipsoid under the effects of a transition function. In general, for a dynamical system with N degrees of freedom, there are N LEs each representing the exponential growth rate of a unique principal axis of the hyperellipsoid. Given a set of N ordered exponents ($\\lambda_1 \\geq \\lambda_2 \\geq ... > \\geq \\lambda_v$), the volume of the hyperellipsoid grows proportional to $e^{(\\lambda_1 + \\lambda_2 + ... \\lambda_n)t}$ and the length grows proportionally to $e^{\\lambda_1 t}$. From this definition, the Maximal Lyapunov Exponent (MLE) (Equation 2) and the Sum of Lyapunov Exponents (SLE) (Equation 3) are used to determine if a system is stable, chaotic or unstable as outlined in Table 1.\n$\\lambda_1 = \\lim_{t \\rightarrow \\infty} \\lim_{\\epsilon \\rightarrow 0} \\frac{1}{t} \\ln{\\frac{|S_t' - S_t|}{\\epsilon}}$ (2)\n$\\lambda_{\\Sigma} = \\sum_{i=0}^{N} \\lambda_i$ (3)\nDynamical systems with a negative MLE ($\\lambda_1 \\leq 0$) are stable as all principle axes of the hyperellipsoid exponentially decrease to zero [21]. In these systems, any trajectories produced from similar initial positions converge to the same trajectory given sufficient time. Conversely, systems with positive MLE ($\\lambda_1 > 0$) and SLE ($\\lambda_{\\Sigma} > 0$) are unstable as the resulting state trajectories diverge at an exponential rate. However, for a positive MLE ($\\lambda_1 > 0$) and negative SLE ($\\lambda_{\\Sigma} < 0$), similar trajectories will diverge at an exponential rate but remain confined to a subregion of the phase space known as a chaotic attractor [4, 14]. This bounded exponential divergence means trajectories in this region of the state space are unstable and only replicable given the exact same starting state: small perturbations to the starting state will produce significantly different long-term outcomes which appear random and uncorrelated. As a result, it is impossible to predict the long-term behaviour of a chaotic system given an approximation of the initial state.\nTo measure the stability of a known dynamical system, the full spectrum of Lyapunov Exponents can be estimated using the approach outlined by Benettin et al [2, 3]. This method represents the spectrum as a set of small perturbation vectors which are iteratively updated using the known transition function. To avoid all vectors collapsing in the direction of maximal growth, they are periodically Gram-Schmidt orthonormalized so that each vector maintains a unique direction. Performing this"}, {"title": "2.3 Chaotic in reinforcement learning", "content": "Previous studies have investigated the chaotic state and reward dynamics produced by Reinforcement Learning policies. Rahn et al [19] showed the policy optimisation landscape can contain high- frequency discontinuities in the vicinity of a trained policy. A similar result was established by Wang et al [28], who proved that control systems with Lipschitz continuous reward and transition functions only have a Lipschitz continuous objective function if $\\lambda_1 < \u2013 \\ln(\\gamma)$. When $\\lambda_1 > - \\ln(\\gamma)$, the objective function is a fractal and is a-H\u00f6lder continuous with holder exponent $\\alpha = \u2212 \\ln(\\gamma)/\\theta_1$. Consequently, a single update to the policy in these chaotic control systems can produce substantially different total rewards. Furthermore, Parmas et al [17] demonstrated that chaos exists in model-based RL methods due to repeated nonlinear predictions and this instability causes gradients to explode during training.\nWhile these works use chaos theory to highlight an important issue in the field of RL policy learning, they are focused primarily on the stability of the policy subject to policy parameter perturbations during training. Our work uses similar concepts but instead focuses on the stability of fully trained RL policies subject to state perturbations and the adverse effect this has on the total reward attained in realistic environments. To our knowledge, we are the first to use the Lyapunov Exponent to estimate the level of chaos produced by trained DNN policies in continuous control tasks."}, {"title": "3 Chaotic state dynamics", "content": "In this section, we use Lyapunov Exponents to identify the level of chaos produced by various state-of-the-art deep reinforcement learning policies in continuous control environments. We claim that the presence of chaos in the MDP implies that the policies are not robust controllers as trivial changes to the system state produce significantly different long-term state trajectories. This instability poses a significant problem for real-world control systems where consistent and predictable behaviour is necessary."}, {"title": "4 Chaotic rewards", "content": "In this section, we show that chaotic state trajectories can also impact a policy's performance and produce chaotic reward trajectories as determined by the Maximal Lyapunov Exponent ($\\lambda_1$). This instability creates a fractal return surface in which small state perturbations produce significantly different total rewards. We argue that adversarial attack methods could leverage these high-frequency oscillations, repeatedly injecting perturbations which cause the agent to follow the state trajectories that attain the lowest total reward. This lack of robustness poses a significant problem for real- world control systems where the worst-case performance is often more significant than the average performance.\nBy considering the reward over a state trajectory as a trajectory in a one-dimensional reward space, the stability of the reward can also be measured using Lyapunov Exponents. Given this space is one-dimensional only one Lyapunov Exponent exists; however, this single exponent can still be used to reliably identify the stability of reward trajectories. Negative $\\lambda_1$ values indicate that small perturbations to the system's state still produce converging long-term rewards. Moreover, for a bounded reward function, the reward trajectories cannot diverge indefinitely; thus a positive $\\lambda_1$ indicates long-term reward is chaotic as small changes to the system state produce exponentially diverging bounded reward trajectories."}, {"title": "5 Maximal Lyapunov Exponent regularisation", "content": "In sections 3 & 4, we established that deep RL policies can produce chaotic state trajectories in continuous control tasks and that this can have a large detrimental impact on performance. To address this, we propose a novel regularisation method which improves the stability of RL policies by constraining the Maximal Lyapunov Exponent during policy updates. This improved stability is a crucial step towards the safe and reliable deployment of RL policies in real-world domains where local perturbations are common."}, {"title": "Algorithm 1 MLE regularisation", "content": "Require:\nPolicy ($\\pi_{\\theta}$ : H \u00d7 Z \u2192 P(A))\nEncoder ($q_{\\phi}$: S \u00d7 H \u2192 P(Z))\nDecoder ($p_{\\varphi}$: H \u00d7 Z \u2192 P(S))\nDynamcis Predictor ($q_{\\phi'}: H \u2192 P(Z)$)\nSequence Model ($f_{\\psi}$ : H \u00d7 Z \u00d7 A \u2192 H)\nCurrent State (s \u2208 S)\nCurrent Hidden State (h \u2208 H)\nTime Horizon (\u03a4\u2208\u039d)\nEnsure: $\\mathcal{L}_{MLE}(0)$\n$\\mathcal{L}_{MLE}(0) \\leftarrow 0$ # Initlaise the MLE regularisation loss\n$\\mathcal{Z} = \\{z_i \\sim q_{\\phi}(s,h)\\}_{i=1}^{L}$ # Update stochastic representation\nfor t = 1, 2, ..., T do\n$\\mathcal{A} \\leftarrow \\{a_i \\sim \\pi_{\\theta}(H_t, Z_t)\\}_{i=1}^{L}$ # Generate a set of sample action\n$\\mathcal{H} \\leftarrow \\{h_t = f_{\\psi}(H_t, Z_t, A_t)\\}_{i=1}^{L}$ # Update hidden representation\n$\\mathcal{Z} \\leftarrow \\{z_i \\sim q_{\\phi'}(H_t)\\}_{i=1}^{L}$ # Update stochastic representation\n$\\mathcal{S} \\leftarrow \\{s_i \\sim p_{\\varphi}(H_t, Z_t)\\}_{i=1}^{L}$ # Generate a set of predicted states\n$\\mathcal{L}_{MLE}(0) \\leftarrow \\mathcal{L}_{MLE}(0) + Var(\\mathcal{S}) + Var(\\mathcal{H})$ # Update the MLE regularisation loss\nend for\nreturn $\\mathcal{L}_{MLE}(0)$\nWe base our regularisation on Dreamer V3 [9], a general-purpose model-based RL algorithm which attains state-of-the-art performance across a diverse set of control tasks. To achieve this, Dreamer V3 uses a Recurrent State Space Model (RSSM) consisting of an Encoder ($q_{\\phi}$ : S \u00d7 H \u2192 P(Z)), Decoder ($p_{\\varphi}$ : H \u00d7 Z \u2192 P(S)), Dynamics Predictor ($q_{\\phi'}$ : H \u2192 P(Z)) and Sequence Model ($f_{\\psi}$ : H \u00d7 Z \u00d7 A \u2192 H) to predict state trajectories ($s_t$), bootstrapped $\\lambda$-return trajectories ($\\hat{R}$) [25] and state value trajectories ($v_{\\rho}(s_t)$) over a short time horizon T. The policy is then trained to maximise the normalised advantage estimates using REINFORCE gradients [30] and an entropy regulariser (H[.]) [29] with weighting coefficient $\\eta$. The full loss function used to train Dreamer V3's policy is outlined in Equation 4.\n$\\mathcal{L}^{Dr3}(\\theta) = -\\sum_{t=1}^{T} sg(\\frac{\\hat{R} - V(S_t)}{max(1, \\mathcal{S})})log \\pi_{\\theta}(a_t|s_t) + \\eta \\mathbb{H} [\\pi_{\\theta}(a_t|S_t)]$ (4)\n$\\mathcal{L}^{\\lambda_1}(\\theta) = \\sum_{t=1}^{T}(Var(S_t) + Var(H_t)$ (5)\n$\\mathcal{L}^{Policy}(\\theta) = \\mathcal{L}^{Dr3}(\\theta) + \\mathcal{L}^{\\lambda_1}(s)$ (6)\nAt its core, Dreamer V3 uses a stochastic RSSM to predict the state and reward trajectories over a predefined time horizon given an initial starting state $s_0$ and an internal representation $h_0$. Due to the stochastic nature of this model, repeating the same trajectory predictions $L \\in \\mathbb{N}$ times produces a set of state trajectories ($\\mathcal{S}_t = (s_{t,1}, s_{t,2}, ..., s_{t,L})$) and internal representation trajectories ($\\mathcal{H}_t = (h_{t,1}, h_{t,2}, \u2026, h_{t,L})$), each of which provides a plausible estimate of the future states. The variance between trajectories (Var\u2081 (\u00b7)) thus provides an estimation of the local state divergence as the state perturbation size approaches 0. Therefore, to minimise $\\lambda_1$ and improve the stability of Dreamer V3 subject to state perturbation, we propose incorporating the regularisation term outlined in Equation 5 into the policy loss (Equation 6). Including this regularisation term as an additional weighted term forces agents to consider the stability of the system during the optimisation process. This incentivises the policy to produce stable state trajectories which attain high rewards instead of solely optimising the expected return. The complete algorithm for calculating the MLE regularisation term is provided in Algorithm 1 using the notation outlined by Hafner et al [9]."}, {"title": "6 Experiments", "content": "In this section, we investigate the impact of the proposed MLE regularisation has on Dreamer V3. We show that the inclusion of this term reduces the chaotic state dynamics produced by the control policy and that this improved stability increases performance when noise is introduced. For these experiments, we train three instances of Dreamer V3 with MLE regularisation and reuse the SAC, TD3 and Dreamer V3 policies from sections 3&4. When estimating state divergence, the RSSM predicts L = 3 plausible future trajectories over which the state and internal representation variance is measured. Increasing L will produce a more accurate estimate of state divergence; however, this will require more computational resources. Therefore, to maintain a similar training time to that of Dreamer V3 we set L = 3. All other hyperparameters are consistent with the Dreamer V3 baseline.\nTable 2 provides the average reward and estimated MLE produced by the regularised and unregularised Dreamer V3 models for each control task. This indicates that MLE regularisation successfully minimises the chaotic state dynamics produced by Dreamer V3 while maintaining similar performance. However, as MLE is still positive in the majority of environments, the control interaction still produced chaotic state trajectories. Despite this, the regularised policies are more stable as the rate of divergence has significantly decreased."}, {"title": "7 Conclusion", "content": "A key issue preventing the application of deep Reinforcement Learning to real-world environments is the need for guaranteed stability and performance in the face of noisy observations and adversarial attacks. In this work, we set out to identify the impact a single perturbation has on the long-term behaviour of deep RL policies in continuous control environments. Using the spectrum of Lyapunov Exponents we established the MDP can produce chaotic state and reward trajectories which are highly sensitive to initial conditions. This instability poses two threats to the application of deep RL to real-world problems where it is infeasible to attain an accurate measurement of the system state. First, small state perturbation can have a large impact on the performance of trained deep RL policies, even where the average performance is good. This can create hazards in real-world conditions where observation noise is prevalent and can be exploited by adversarial attack methods. Second, even when deep RL policies perform well, they can produce unpredictable behaviours which is undesirable in most real-world applications.\nTo mitigate these chaotic dynamics and improve robustness we propose Maximal Lyapunov Exponent regularisation. This novel approach uses the recurrent state space model to estimate the local state divergence and incorporates this into the Dreamer V3's policy loss. In effect, the agent optimises its confidence in future trajectories jointly with its expectations of rewards. While MLE regularisation helps improve the robustness of the agent's policies, this approach assumes an accurate estimation of the local state divergence. In environments where the RSSM struggles to capture state dynamics, the effectiveness of the proposed regularization may be diminished. However, in our experiments, we demonstrate that this regularisation improves the stability of the learnt policies, thereby making them more robust to state perturbations."}]}