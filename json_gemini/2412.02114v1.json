{"title": "OmniCreator: Self-Supervised Unified Generation with Universal Editing", "authors": ["Haodong Chen", "Lan Wang", "Harry Yang", "Ser-Nam Lim", "Everlyn AI"], "abstract": "We introduce OmniCreator, a novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in a self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as a denoising target to learn the semantic correspondence between video and text. During inference, when presented with a text prompt and a video, OmniCreator is capable of generating a target that is faithful to both, achieving a universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with a text prompt only, OmniCreator becomes generative, producing high-quality video as a result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator a truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models.", "sections": [{"title": "1. Introduction", "content": "Generative artificial intelligence has opened up new possibilities for creative content generation, allowing for seamless creation and modification. Key developments in text-to-image (T2I) generation [89, 92, 94] have laid the foundation for innovative editing techniques, enabling text-based manipulation of images [7, 38, 73] and videos [2, 59, 114].\nControllable video editing faces the challenge of maintaining cross-frame consistency. Frame-by-frame editing approaches like utilizing IP2P [7] often produce inconsistencies [109]. Keyframe-based methods improve coherence by editing select frames and interpolating others [2, 8, 30, 51, 57, 59, 63, 68, 80, 85, 109], but still suffer from low editing quality and restricted scope. To enhance editing precision, Tune-A-Video's [114] one-video tuning paradigm advanced fine-tuning on specific videos, but often struggles with generalization [10, 68, 97, 101, 133\u2013135]. Structural conditions (e.g., edge, depth maps) [2, 22, 26, 59, 63, 65, 110, 120, 123, 134], attention feature [1, 27, 55, 61, 64, 68, 108, 113], or DDIM inversion-based [22, 30, 52, 61, 66, 68, 97, 104, 114, 132] methods can further refine edits but reduce model flexibility. Additionally, the IP2P paradigm promotes generating video-text-video pairs for training [18, 86], though dataset creation remains resource-intensive.\nAll the above observations motivate us to explore a more elegant and generalizable way for text-based generative video editing. To this end, we introduce OmniCreator, a framework for learning the intrinsic semantic correspondence between original text-video pairs. In this setup, text provides textual semantics while video delivers visual semantics. By conditioning the denoising process on both text and video embeddings while at the same time using the exact same video as denoising target, the model learns to associate these semantics in a structured manner. Very importantly, we note that this approach is self-supervised, serving to avoid tedious and large-scale annotation costs, and is yet surprisingly effective. Intuitively, this makes sense since in principle the model, when given the original text-video pair, should generate something close to the same video. Further, once the semantic correspondence is learned, the model can interpret any text and video conditions and generate the correct target video, thus achieving truly universal editing (see Fig. 1) without being restricted to certain editing types.\nTo implement OmniCreator, we introduce key architectural innovations. Here we seek to solve two key chal-"}, {"title": "2. Related Work", "content": "Diffusion-based Text-to-Video Generation. Recent advancements in diffusion models (DMs) [40, 100] have showcased their impressive ability in T2I generation [78, 89, 92, 94]. To extend this success to video, the first video diffusion model (VDM) [42] was introduced, utilizing a space-time factorized U-Net [93] for effective low-resolution video modeling in pixel space. Following this, Imagen-Video [41] developed cascaded DMs employing v-\nprediction to generate high-definition videos. To enhance training efficiency, subsequent research [5, 36, 107, 111] has focused on transferring T2I concepts to T2V generation [28, 69, 98], along with the development of VDMs in latent or hybrid pixel-latent spaces. Many studies [59, 72, 110, 120] have incorporated various controls to achieve more precise generation in specific scenarios, though often with additional supervision. However, achieving generalizable T2V generation [4, 11, 15, 62, 71] has proven more effective through training with raw text-video pairs or joint training with additional text-image pairs. Building on their success, we further explore the potential of raw text-video pairs for more generalizable and controllable video and image generation.\nDiffusion-based Text-guided Video Editing. Video editing can be viewed as controllable video generation, with a reference video serving as a condition during inference. Drawing inspiration from image editing [7, 21, 73], several studies [2, 9, 26, 59, 63, 113, 125] employ off-the-shelf image-to-image (I2I) models to edit video frames directly or edit keyframes first to ensure temporal consistency. The introduction of one-video tuning by TAV [114] has prompted research [8, 54, 68, 80, 130, 133] into fine-tuning T2V models for specific video structures, although this may limit usability and practical applications (see Tab. 5). To improve editing precision, many studies introduce additional control mechanisms [103], categorized into three types: 1) Structural conditions, which utilize elements like human pose [72, 104, 132], optical flow [52, 65, 125], depth"}, {"title": "3. Method", "content": "The primary objective of our work is to enable unified (image+video) generation and universal editing. While prior arts have made significant strides in controlled editing within specific scenarios by introducing additional controls, achieving more generalizable remains a more complex endeavor. To tackle this, OmniCreator adopts the original video as a new denoising condition, coupled with the origi-\nnal text caption, to harness the semantic correspondence between both in a self-supervised manner, as shown in Fig. 3.\nVideo Embedding: A New Condition. Unlike previous works that impose explicit structural constraints (e.g., depth maps [26, 110], edges [9, 132], etc.) for additional supervision, which can limit the flexibility of video editing, we introduce the same video of the denoising target as a new denoising condition. This allows the model to learn the video's structure at a semantic level through self-supervised training, jointly conditioning on both video and text. To achieve this, we leverage CLIP's [88] powerful encoders that have been pretrained with a large amount of data to achieve text-image alignment. The tricky part, however, lies in how to encode videos with these encoders. A naive approach is to apply temporal aggregation (e.g., average pooling) on per-frame features, but this often leads to information loss in temporal dynamics. Although using a video ViT [3] could capture temporal information more effectively, it would be computationally costly [112]. To address this, we insert a lightweight adapter into the CLIP image encoder to handle temporal aspects of video with minimal computational overhead. This adapter, as illustrated in Fig.3, is implemented as a depth-wise 3D convolutional layer (DWConv3D) [25], inspired by [81]. The adapted feature \\(X_{ada}\\) is computed as:\n\n\\[X_{ada} = X + f (DWConv3D(XW_{down}))W_{up},\\]\nwhere X initially represents the patch and positional embedding feature of input video x, \\(f(\\cdot)\\) is the activation function, and \\(W_{down}\\), \\(W_{up}\\) are the down- and up-projection layers, respectively.\nAdditionally, inspired by image-to-video (I2V) works [11, 119] that project image embedding into a text-aligned space, we adopt a similar idea for video embedding to improve cross-modal compatibility. Specifically, we use the full set of video tokens \\(E_{vid} = \\{e_i\\}_{i=1}^N\\) from the last transformer layer, rather than relying solely on the global semantic token \\(e_{cls}\\) [96, 128], to ensure comprehensive visual representation. To align video and text embeddings within the denoising network, we employ a query transformer [49, 50]"}, {"title": "Text-Video Semantic Correspondence", "content": "After projecting the video condition and text condition into intermediate representations \\(E_{vid}\\) and \\(E_{txt}\\), a dual-path cross-attention layer is introduced to incorporate these conditions into the U-Net's intermediate layers:\n\\[\\text{Softmax}(\\frac{QK_{txt}^T}{\\sqrt{d}}) V_{txt} + \\text{Softmax}(\\frac{QK_{vid}^T}{\\sqrt{d}}) V_{vid},\\qquad(3)\\]\nwhere the queries \\(Q\\) derive from an intermediate U-Net representation \\(E_{lat}\\) via \\(Q = W_Q E_{lat}\\). The keys and values for text and video conditions are calculated as follows:\n\\[\\begin{aligned}&K_{txt} = W_K^{(t)} E_{txt}, V_{txt} = W_V^{(t)} E_{txt} \\\\&K_{vid} = W_K^{(v)} E_{vid}, V_{vid} = W_V^{(v)} E_{vid}\\end{aligned}\\qquad(4)\\]\nHere, \\(W_K^{(t)}, W_V^{(t)}, W_K^{(v)}, W_V^{(v)}\\) are learnable projection matrices. Fig. 5 illustrates how semantic correspondence operates in our framework. The middle block shows results with only one condition applied from the first block: text-only generation adheres to the caption, but the inherent ambiguity in textual semantics often fails to capture specific video details, limiting effective global visual modeling. In contrast, video-only generation reconstructs visual elements more accurately, as the reference video supplies comprehensive global visual semantics."}, {"title": "Spatiotemporal Low-Rank Adaptations", "content": "To reduce the computational cost of training the video model, we inject LoRAs [44] into both the spatial and temporal layers of the denoising U-Net (see Fig. 3 right side), a common strategy in generation tasks [4, 31, 133]. Particularly, we do not modify the cross-attention layers to preserve the learning of text-video semantic correspondence. Instead, LoRAs are applied to the spatial and temporal self-attention layers as well as the feed-forward networks, updating correlations across both dimensions. Formally, LoRA updates the weight matrix W via low-rank factorization:\n\\[W = W_0 + \\Delta W = W_0 + BA,\\qquad(5)\\]\nwhere \\(W_0 \\in \\mathbb{R}^{d \\times k}\\) represents the original weights, and \\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times k}\\) are the low-rank factors, with \\(r\\) much smaller than d and k. This enables efficient training and fine-tuning of our video editing model."}, {"title": "Inference and Applications", "content": "The classifier-free guidance [39] is naturally extended to multi-conditional during inference, i.e., text condition \\(c_{txt}\\) and video condition \\(c_{vid}\\):\n\\[\\begin{aligned} \\epsilon_{\\theta}(z_t; t, c_{vid}, c_{txt}) &\\leftarrow \\epsilon_{\\theta}(z_t; t, \\O, \\O) \\\\ &+ w_{vid} (\\epsilon_{\\theta}(z_t; t, c_{vid}, \\O) - \\epsilon_{\\theta}(z_t; t, \\O, \\O)) \\\\ &+ w_{txt} (\\epsilon_{\\theta}(z_t; t, c_{vid}, c_{txt}) - \\epsilon_{\\theta}(z_t; t, c_{vid}, \\O)) \\end{aligned}\\qquad(6)\\]\nwhere \\(w_{vid}\\) and \\(w_{txt}\\) denote the guidance scales for video and text conditions, respectively.\nOmniCreator enables unified text-based generation and editing of images and videos. Video: For editing, we set \\(w_{vid}\\) and \\(w_{txt}\\) to specific values, allowing for semantic"}, {"title": "4. OmniBench-99", "content": "Overview. Generative video editing has emerged as a rapidly growing research area, yet it still lacks a comprehensive benchmark, potentially hindering its technical advancement. Although the recently introduced BalanceCC [26] organizes videos into four categories, i.e., humans, animals, objects, and landscapes, it only evaluates four editing types following the LOVEU-TGVE-2023 [115] benchmark. This limited scope overlooks the importance of assessing editing scenarios. To this end, we present OmniBench-99, a new benchmark of 99 videos with text prompts that evaluate both editing types and scenarios, providing a more comprehensive benchmark for generative video editing evaluation.\nEstablishment and Statistics. We collected 99 open-license videos, selected for their suitability for non-stigmatizing and legal modifications. These videos range in length from 2 to 20 seconds, with a frame rate of about 30 FPS. The videos are evenly distributed across three categories: Human/Animal, Environment, and Object, with 33 videos per category. In addition to generating four editing-type prompts for each video [26], we tasked GPT-4V [79] to create category-specific prompts tailored to different editing scenarios (see Fig. 1 for examples). Notably, we provide two kinds of prompts, namely full sentence and delta caption, for convenient use. Afterward, we conducted a thorough manual inspection to ensure the quality of these prompts. We present the statistical distribution of OmniBench-99 in Fig. 6. We hope this new benchmark will better address gaps in previous research and provide a reliable standard for generative video editing. More details and visual examples are available in App. D."}, {"title": "5. Experiment", "content": "In this section, we will describe in detail how we evaluate OmniCreator. As mentioned, OmniCreator is capable of both video and image universal editing and generation. The next few sections will demonstrate the superiority of OmniCreator in all four tasks."}, {"title": "5.1. Implementation Details", "content": "Training and Dataset. OmniCreator is developed based on the I2V model DynamiCrafter [119] and the T2I model Stable-Diffusion-v2.1 [92]. We adopt the newly proposed high-quality T2V dataset, OpenVidHD-0.4M [77]. Our OmniCreator is fine-tuned based on [119] for 12K iterations, sampling 16 frames per video with dynamic FPS, with a resolution 320 \u00d7 576. We employ a batch size of 32 and a learning rate of 1 \u00d7 10-5.\nVideo Evaluation. Automatic Metrics: For editing, we use PickScore [60] to evaluate the average alignment between all frames of the output video and the corresponding edited prompt. We also apply CLIP Frame (Frame Consistency) [88] to assess the average cosine similarity between CLIP image embeddings on all frames. For generation, we compute Fr\u00e9chet Video Distance (FVD) [105] on the UCF-101 [102] and CLIP Similarity [88] on the MSR-VTT [122]. User Study: We utilize mean opinion score (MOS) as our metric and focus on four aspects: \u2460 Text Alignment, \u2461 Temporal Consistency, \u2462 Structure Alignment (for editing only) and \u2463 Overall Quality. Benchmarks: For editing, We evaluate video editing using our newly constructed OmniBench-99 dataset, covering both editing types and scenarios. For generation, we evaluate on UCF-101 and MSR-VTT benchmarks. Baselines: For editing, we"}, {"title": "5.2. Qualitative Evaluation", "content": "Due to limitations in most PDF readers, videos may not render correctly. Therefore, we present a small number of frames from each video and highly recommend readers visit our video demo for full video demonstrations. Editing: We first demonstrate video editing examples of our OmniCreator in Fig. 1, illustrating its universal editing capabilities. OmniCreator consistently performs well across four editing types and ten scenarios, spanning three video content categories. To further demonstrate OmniCreator's ef-"}, {"title": "5.3. Quantitative Evaluation", "content": "User studies and automatic metrics serve equally important roles in generative works. Our user study evaluates four key aspects: Text Alignment, Temporal Consistency, Structural Alignment (for editing only), and Overall Quality. Feedback was gathered from 15 volunteers who rated each aspect on a five-point scale (1-5). \u25cf Editing: Building on our newly developed OmniBench-99 benchmark, we conduct video quantitative comparisons with baseline methods using both automatic metrics (i.e., CLIP Frame and PickScore) and user studies. We also conduct image comparisons using the LMM score following EditEval [48] benchmark. As shown in Tab. 1 and Tab. 2, OmniCreator outperforms baseline approaches without requiring additional controls, further validating its flexibility and generalizability. \u2192 Generation: We also quantitatively assess the performance of our OmniCreator T2V generation using FVD and CLIPSIM, and T2I generation using compositionality score. As illustrated in Tab. 3 and Tab. 4, our method demonstrates superior performance, further establishing OmniCreator as a robust baseline for both generation and editing future works. Detailed information on user study is in App. E.2."}, {"title": "6. Conclusion", "content": "This paper presents OmniCreator, a self-supervised framework for both high-quality text-to-video generation and universal text-guided video editing, covering diverse editing types and scenarios. The key sight of this work is leveraging the original video as a novel denoising condition, learning the semantic correspondence between video and text. To address the current gap in evaluating generative video editing methods, we introduce the meticulously curated OmniBench-99 benchmark, designed for comprehensive evaluation across both editing types and scenarios. This paper further discovers that OmniCreator can be extended to the image modality, as is, for both generative and editing tasks. We hope that our efforts will advance research in the field of universal content editing. More discussions about limitations are available in App. F."}, {"title": "A. Editing Capabilities Overview", "content": "To facilitate a clearer understanding of the current advancements in text-based video editing, we perform a comprehensive evaluation and review, as illustrated in Tab. 5, structured around the following aspects:\nTune & I2I. The \"Tune\" refers to techniques like one-video tuning, as seen in Tune-A-Video [114], or few-video tuning in MotionDirector [133]. While this method helps models learn the specific structure of given videos, it tends to limit the model's generalizability and flexibility in practical applications; The \u201cI2I\" method involves using image-to-image models, e.g., IP2P [7], to edit individual video frames. Although this allows for precise edits at the image level, it introduces challenges like increased computational cost, potential inconsistencies, and uncertainty in the editing process.\nAdditional Control. We categorize the additional control in video editing into three main types: Condition (Cond.): Widely used to constrain the structure of the edited video, this approach incorporates elements, e.g., optical flow, depth maps, masks, edges, etc.; Attention Features"}, {"title": "B. Ablation Studies", "content": "An appropriate inference paradigm in generative models is often as critical as the training process. As discussed in Sec. 3.2 (Fig. 5), the use of delta prompts allows for more\nprecise modeling of local textual semantics for the target area, thereby avoiding conflicts between the global textual semantics of a full sentence and the global visual semantics provided by the reference video. Additionally, we analyze the impact of our two multimodal classifier-free guidance scales in Fig. 13. Increasing each scale strengthens the respective control over the editing results. Notably, beyond the obvious control over facial regions, increasing the video condition scale wvid further enhances the preservation of the spatial structure of the reference video's background, as shown by the red bounding box. In practice, the ideal editing effect depends on the user preferences, making it beneficial to adjust the scales according to different examples for optimal results."}, {"title": "C. Implementation Details", "content": "We implement our OmniCreator framework on DynamiCrafter [119], a pre-trained image-to-video (I2V) generation model and T2I model Stable-Diffusion-v2.1 [92]. Notably, our method of using the original video as a new denoising condition can be extended to other I2V or T2V diffusion models that incorporate cross-attention mechanisms for text conditioning. The VQ-VAE [106] is used for patch-wise frame encoding and CLIP-ViT-H-14 [95] for text and visual embedding. Following [119], we incorporate a combination of sinusoidal functions and several fully connected (FC) layers activated by SiLU [37] as the FPS embedding layer. This embedding is added to the timestep embedding for further refinement."}, {"title": "D. OmniBench-99", "content": "Overview. Generative video editing has emerged as a rapidly growing research area, yet it still lacks a comprehensive benchmark, potentially hindering its technical advancement. Although the recently introduced BalanceCC [26] organizes videos into four categories, i.e., humans, animals, objects, and landscapes, it only evaluates four editing types following the LOVEU-TGVE-2023 [115] benchmark. This limited scope overlooks the importance of assessing editing scenarios. To this end, we present OmniBench-99, a benchmark that enables both editing types and scenarios evaluation. Our dataset consists of 99 high-quality, open-license videos. For each video, we generate four editing-type prompts. To accommodate the prompts accepted by existing models, we further design two types of text prompt, full description and delta caption. To reduce the burden of manual data annotation, we leverage GPT-4V(ision) [79] to automatically generate these prompts following the instructions shown in Fig. 14."}, {"title": "Human Inspection", "content": "Human inspection plays a pivotal role in our OmniBench-99 since LLMs often experience some hallucinations. In video editing, while the range of potential edits is vast, we aim for reasonable and appropriate prompts. During the refinement process, we focus on ensuring that the prompt aligns with the video content and whether the resulting edited video adheres to real-world physics. For example, consider a video of a car driving on a road with the prompt to change the road into a cracked canyon. Editing only the road would be ideal, but driving the car across a canyon is unrealistic. Ensuring such physical consistency is a key aspect of our approach:"}, {"title": "1. Video Baselines", "content": "We compare our OmniCreator with state-of-the-art generative video editing models: ControlVideo [132], Tune-AVideo [114], Pix2Video [8], TokenFlow [30], InsV2V [18], Video-P2P [68], and CCEdit [26]. Before delving into a brief introduction of each model, we recommend referring to Tab. 5 for a concise, clear summary."}, {"title": "Evaluation Details", "content": "EditEval [48] is designed to assess general diffusion model-based image editing across seven distinct tasks: Addition, Replacement, Removal, Background, Style, Texture, and Action. Following the methodology in [48], we use the LMM Score as the evaluation metric, which leverages large multimodal models (LMM), (GPT-4V [79] in our work), to measure editing performance.\nWe consider four critical factors for the evaluation: Editing Accuracy, Contextual Preservation, Visual Quality and Logical Realism."}, {"title": "More Discussion", "content": "OmniCreator, leveraging a self-supervised paradigm, achieves highly universal and superior editing and generation performance using only the original text-video training data. This means it addresses many of the limitations seen in existing methods. However, we still observe some limitations, including:\nSince OmniCreator relies on semantic information for conditional generation, it may struggle to capture the temporal dynamics and fine-grained details of large motion changes, particularly in cases involving high-speed movement or complex posture shifts. This is due to the inherent limitations of static video embeddings and text embeddings in capturing such temporal information. A promising solution could be to leverage the motion features from the reference video more effectively while ensuring they are not overly constrained by the original content\u2019s structure."}, {"title": "Ethical Implications", "content": "OmniCreator is developed as a self-supervised framework for research only. It may still raise important ethical considerations, particularly around content manipulation. The ability to generate and edit high-quality videos can potentially be misused for creating misleading or harmful content. To mitigate this risk, we recommend incorporating safeguards such as adding watermarks to edited videos to ensure transparency and authenticity. Additionally, guidelines on responsible use should be established, emphasiz-"}, {"title": "More Results", "content": "To further assess the effectiveness of our OmniCreator, we conducted additional quantitative comparisons with state-of-the-art methods using existing popular benchmarks, specifically LOVEU-TGVE-2023 [115] and BalanceCC [26]. As demonstrated in Tab. 6, OmniCreator consistently outperforms the baseline models."}]}