{"title": "D\u00b3epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic Scenes", "authors": ["Siyu Chen", "Hong Liu", "Wenhao Li", "Ying Zhu", "Guoquan Wang", "Jianbing Wu"], "abstract": "Depth estimation is a crucial technology in robotics. Recently, self-supervised depth estimation methods have demonstrated great potential as they can efficiently leverage large amounts of unlabelled real-world data. However, most existing methods are designed under the assumption of static scenes, which hinders their adaptability in dynamic environments. To address this issue, we present Depth, a novel method for self-supervised depth estimation in dynamic scenes. It tackles the challenge of dynamic objects from two key perspectives. First, within the self-supervised framework, we design a reprojection constraint to identify regions likely to contain dynamic objects, allowing the construction of a dynamic mask that mitigates their impact at the loss level. Second, for multi-frame depth estimation, we introduce a cost volume auto-masking strategy that leverages adjacent frames to identify regions associated with dynamic objects and generate corresponding masks. This provides guidance for subsequent processes. Furthermore, we propose a spectral entropy uncertainty module that incorporates spectral entropy to guide uncertainty estimation during depth fusion, effectively addressing issues arising from cost volume computation in dynamic environments. Extensive experiments on KITTI and Cityscapes datasets demonstrate that the proposed method consistently outperforms existing self-supervised monocular depth estimation baselines. Code is available at https://github.com/Csyunling/D3epth.", "sections": [{"title": "I. INTRODUCTION", "content": "Depth information is crucial for various real-world applications, such as autonomous vehicles [1], robot navigation [2], and human-robot interaction [3]. Self-supervised depth estimation [4], [5], [6] has emerged as a promising method due to its ability to predict pixel-level depth maps from images without requiring annotated data. It infers depth from a single target image and supervises learning by constructing a photometric loss through the reprojection of adjacent images back to the target image [7].\nCompared to single-frame methods, multi-frame self-supervised depth estimation [8], [9], [10] has garnered more attention for practical applications such as autonomous navigation, as it leverages richer information by inferring depth maps using multiple images during inference. However, both single-frame and multi-frame self-supervised methods are typically built on the assumption of static scenes, which presents challenges when dynamic objects are encountered in real-world environments. This stems from two primary factors:\n(i) The common framework of self-supervised depth estimation [7] relies on the reprojection loss to capture geometric relationships between consecutive frames under the assumption of photometric consistency. However, this assumption breaks down in the presence of dynamic objects, as shown by the red-circled section in Fig. 1, where a person riding a bike leads to errors during the view synthesis phase, significantly degrading the accuracy of the generated depth maps. To mitigate the impact of dynamic objects, Monodepth2 [11] proposes the Minimum Reprojection Loss. However, this approach is not exhaustive and fails to address the challenging dynamic conditions illustrated in the lower part of Fig. 2.\n(ii) In multi-frame depth estimation, the construction of cost volumes does not account for dynamic objects and occlusions, introducing additional errors and making multi-frame depth estimation more susceptible to challenges posed by dynamic scenes. To alleviate this, some studies [8], [10], [9] use teacher-student distillation, where a single-frame depth network guides the multi-frame depth estimation network to correct feature matching errors caused by cost volumes. Other methods involve semantic segmentation to identify dynamic objects and adjust their matching [9], [12] or use optical flow estimation [13] to alleviate the impact of dynamic objects. Unfortunately, these techniques often introduce complex algorithms with increasing inference costs.\nIn this paper, we propose Depth (Self-Supervised Depth Estimation with Dynamic Mask in Dynamic Scenes), a novel method for addressing the challenging problem of dynamic"}, {"title": "II. RELATED WORK", "content": "Single-Frame Self-Supervised Depth Estimation. Self-supervised monocular depth estimation has gained significant attention, owing to its capability to predict pixel-level depth maps from a single image without the need for labeled data. The foundational framework for self-supervised depth estimation is introduced by Zhou et al. [7], who employ DepthNet and PoseNet to predict geometric relationships between frames.\nMonodepth2 [11] addresses occlusion issues by proposing the minimum reprojection loss and employs full-resolution multi-scale sampling to mitigate visual artifacts. Further advancements are made by MonoProb [14], which incorporates an interpretable confidence measure to enhance depth modeling through uncertainty estimation. SQLdepth [15] introduces a self-supervised method utilizing the Self Query Layer to build a self-cost volume, effectively capturing fine-grained scene geometry from a single image. For indoor depth estimation, PMIndoor [16] leverages multiple loss functions to constrain depth estimation in non-textured regions and proposes a pose rectification network to address camera pose inaccuracies. To handle adverse weather conditions, WeatherDepth [17] introduces a curriculum-based contrastive learning strategy, integrating robust weather adaptation techniques to tackle challenging environments without inducing knowledge forgetfulness.\nMulti-Frame Self-Supervised Depth Estimation. Compared to single-frame methods, multi-frame depth estimation leverages multiple images for inference, providing richer information. This approach has high practical value in applications such as autonomous driving, making it an increasing focus in recent research. Early approaches [18], [19] to multi-frame depth estimation often employ Recurrent Neural Networks (RNNs) to enhance model performance; however,"}, {"title": "III. METHOD", "content": "Self-supervised methods use reprojection loss between adjacent frames as supervision for training. Each training instance includes a reference frame $I_t$ and two temporally adjacent source frames $I_s$ ($s \\in \\{t \u2013 1, t + 1\\}$). Given the depth map $D_t$ of the target view and the camera pose $T_{t\\rightarrow s}$ between the target and source views, the image synthesis from the source to the target view can be formulated as:\n$I_{st} = I_s \\langle Proj(D_t, T_{t\\rightarrow s}, K) \\rangle$, (1)\nwhere $K$ represents the known camera intrinsics, $\\langle \\rangle$ is the sampling operator [21], and $Proj(\\cdot)$ is the coordinate projection operation [7]. The photometric loss is composed of L1 and SSIM [22]:\n$pe(I_a, I_b) = \\frac{1}{2} (1 - SSIM(I_a, I_b)) + \\frac{\\alpha}{2} ||I_a \u2013 I_b||_1$ (2)\nFollowing Monodepth2 [11], we adopt the per-pixel minimum reprojection loss as our photometric loss:\n$L_{ph}(I_t, I_{s\\rightarrow t}) = \\min_{s} pe(I_t, I_{s\\rightarrow t})$. (3)\nOur method is primarily designed to address issues caused by dynamic objects from two perspectives, as shown in Fig. 1. First, we introduce the Dynamic Mask (DM) (Sec. III-C) to tackle limitations in the self-supervised framework by analyzing the reprojection loss. Second, we propose improvements to DepthNet, focusing on multi-frame depth estimation, as illustrated in Fig. 3. The overall framework of DepthNet consists of three components: single-frame depth estimation (MonoDepth), multi-frame depth estimation (MultiDepth), and the proposed Spectral Entropy Uncertainty (SEU). In our approach, Cost Volume Auto-Masking is applied before cost volume computation to filter out regions affected by dynamic objects and guide subsequent processing (Sec. III-D). The SEU module leverages spectral entropy to provide richer information, enhancing uncertainty estimation and enabling more effective depth fusion (Sec. III-E). Further details will be introduced in the following sections.\nIn self-supervised depth estimation frameworks, dynamic objects, lighting changes, and occlusions pose significant challenges during the computation of reprojection loss, as these factors violate the photometric consistency assumption."}, {"title": "This often leads to errors in reprojection loss calculations, particularly in the affected areas.", "content": "To address this issue, we propose the Dynamic Mask method, which targets this problem at the loss level, as illustrated in Fig. 1. By analyzing the reprojection errors from two source images, it can be observed that dynamic objects typically correspond to regions with high loss. If both source images exhibit high photometric loss at a particular location, it is highly likely that this area has mismatched points due to dynamic objects, occlusions, or lighting changes. As shown in Fig. 1, the person riding the bicycle is part of a dynamic region, which exhibits high loss in this area, making it an ideal candidate for masking.\nGiven the reprojection losses $L_{reproj} \\in R^{2 \\times C \\times H \\times W}$, where channel $c \\in \\{1, 2\\}$ corresponds to the two different reprojection losses. For each channel, we flatten the spatial dimensions into a vector:\n$L_c = Flatten(L_{reproj} [:, c, :, :]) \\in R^{B \\times (H \\times W)}$. (4)\nWe then compute the $p$-th quantile $q_c$ for each channel, controlled by a threshold $\\beta$, to identify high-loss regions:\n$q_c = Q_\\beta(L_c)$. (5)\nA higher $\\beta$ excludes only the highest loss areas, while a lower $\\beta$ captures more potential error regions. Using the quantile, a binary mask is generated for each channel:\n$M_c = I(L_{reproj} [:, c, :, :] > q_c)$, (6)\nand the final dynamic mask is constructed as:\n$M_{dynamic} = 1 - (M_1 \\land M_2)$. (7)\nMulti-frame depth estimation typically regresses depth maps by constructing a cost volume. However, dynamic objects can adversely affect the accurate construction of this cost volume. Inspired by the Auto-Masking approach in Monodepth2 [11], we propose a Cost Volume Auto-Masking strategy for cost volume. This method similarly filters out pixels whose appearance remains unchanged between consecutive frames, meaning that points stationary relative to the camera are masked out.\nGiven two adjacent frames $I_t$ and $I_{t-1}$ with pixel values $I_t(u, v)$ and $I_{t-1}(u, v)$, we define the consistency mask $M_{eq}$ as follows:\n$M_{eq}(u, v) = \\begin{cases} 1 & \\text{if } I_t(u, v) = I_{t-1}(u, v), \\\\ 0 & \\text{otherwise.} \\end{cases}$ (8)\nThe cost volume auto-masking mask $M_{cost}$ is derived as:\n$M_{cost}(u, v) = 1 - M_{eq}(u, v)$. (9)\nThis mask is zero when all channels are consistent and one otherwise. To apply this mask, it is downsampled to match the resolution of the feature maps. Let $s = 2^{scale}$ be the downsampling factor. The downsampled mask $M_{down}$ is obtained by:\n$M_{down}(u', v') = M_{cost}([])$. (10)\nThis downsampled mask is then applied element-wise to the feature maps of the reference frame $F_{ref}$ and the source frame $F_{source}$ as follows:\n$F'_{ref}(u', v') = F_{ref}(u', v') \\odot M_{down}(u', v')$, (11)\n$F'_{source}(u', v') = F_{source}(u', v') \\odot M_{down}(u', v')$. (12)\nHere, $\\odot$ denotes element-wise multiplication. The benefit of this approach is that it allows for the exclusion of objects moving at the same speed as the camera before constructing the cost volume. It even enables the omission of entire frames from monocular video when the camera is stationary. This strategy diverges from Monodepth2 [11] by specifically addressing photometric inconsistencies caused by occlusions and motion when constructing the cost volume. Furthermore, this approach lays the groundwork for the subsequent Spectral Entropy Uncertainty Module and guides its implementation."}, {"title": "E. Spectral Entropy Uncertainty Module", "content": "To address dynamic object issues and enhance depth regression using cost volume information, we propose the Spectral Entropy Uncertainty (SEU) module. This module integrates spectral entropy with Fourier transform for uncertainty estimation and depth fusion. The Fourier transform converts spatial information into the frequency domain, facilitating the identification of noise introduced by dynamic objects. And spectral entropy analysis quantifies the complexity of these frequency components, allowing for a more precise characterization and management of uncertainty regions. Firstly, we compute the Fourier transform of the cost probability tensor C:\n$\\hat{C} = FFT(C)$. (13)\nWe then derive the magnitude spectrum $S$ and normalize it to obtain a probability distribution $P$:\n$M = |\\hat{C}|, P = \\frac{S}{\\Sigma S}$. (14)\nNext, we calculate the spectral entropy $H$ from $P$:\n$H = - \\Sigma P \\cdot log(P + \\epsilon)$. (15)\nFollowing this, we use a neural network to predict the uncertainty $U$ from $H$.\nFinally, we fuse the depth estimates $D_{multi}$ and $D_{mono}$ using the predicted uncertainty $U$ to obtain the fused depth $D_f$ following MOVEDepth [26]:\n$D_{Fuse} = (1 - U) \\cdot D_{multi} + U \\cdot D_{mono}$. (16)\nThe proposed strategy integrates single-frame and multi-frame depth estimates, dynamically adjusting the fusion process based on the Spectral Entropy Uncertainty Module. Additionally, the fused depth $D_{Fuse}$ is used solely for loss computation, thereby not introducing additional inference overhead."}, {"title": "IV. EXPERIMENTS", "content": "We evaluate our approach on two widely-used benchmarks for depth estimation: KITTI [32] and Cityscapes [33]. KITTI serves as a standard dataset for outdoor driving scenarios, providing diverse scenes. Following the Eigen split [34], we use 39,810 images for training, 4,424 for validation, and 697 for testing. The Cityscapes dataset features a higher proportion of dynamic objects, making it particularly well-suited for evaluating our method's performance in dynamic environments. We train on 69,731 monocular triplets and\nevaluate on 1,525 test images. Since our method is specifically designed to address the challenges posed by dynamic objects, and considering that the Cityscapes dataset features a high prevalence of moving objects, the majority of our experiments are conducted on Cityscapes. Both datasets are evaluated with a maximum depth of 80 meters.\nIn our experiments, we implement the models using PyTorch and train them on NVIDIA RTX 4090 GPU. Our D\u00b3epth is based on a two-stage teacher-student distillation method following [28]. The dynamic mask threshold $\\beta$ is set to 0.8. For the KITTI dataset, this threshold is applied starting from epoch 0, while for the Cityscapes dataset, it is applied from epoch 1. We optimize the models using the Adam optimizer [35], with an initial learning rate of 2 \u00d7 10-4 for the teacher model and 1 \u00d7 10-4 for the student model. The learning rate is reduced by a factor of 10 after a specific number of epochs: 15 epochs for KITTI and 1 epoch for Cityscapes. The total number of training epochs is 20 for KITTI and 5 for Cityscapes, with a batch size of 12.\nWe evaluate our method on the KITTI and Cityscapes benchmarks, with the results presented in Table I and Table"}, {"title": "F. Training Objective", "content": "Our D\u00b3epth is trained in a self-supervised manner, and the loss consists of three parts:\n$L_{total} = L_{Mono}(D_{Mono}) + L_{Multi}(D_{Multi}) + L_{Fuse}(D_{Fuse})$, (17)\nwhere $L_{Mono}, L_{Multi}$, and $L_{Fuse}$ represent the loss associated with the depth estimates $D_{Mono}, D_{Multi}$, and $D_{Fuse}$, respectively. And $L(\\cdot)$ is a weighted combination of reprojection loss $L_{ph}$ (Eq. 3) and depth smooth loss $L_s$ [31]:\n$L(D) = L_{ph}(D) \\cdot M_{dynamic} + \\gamma L_s(D)$, (18)\nwhere $\\gamma \\approx 0.001$ denotes the loss weight."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose D\u00b3epth, a novel self-supervised depth estimation framework specifically designed to tackle challenges posed by dynamic objects. We introduce the Dynamic Mask to mitigate the impact of dynamic objects on the loss function, improving robustness in handling dynamic entities. Additionally, we enhance multi-frame depth estimation using the Cost Volume Auto-Masking strategy, which identifies and masks potential dynamic object regions. Furthermore, we incorporate the Spectral Entropy Uncertainty module to guide depth fusion and address challenges associated with dynamic objects in the cost volume. Our method achieves state-of-the-art results on the KITTI and Cityscapes datasets. Future work will focus on refining the distinction between high-loss areas caused by dynamic objects and those inherently exhibiting high loss, to more accurately localize dynamic objects within the reprojection loss."}]}