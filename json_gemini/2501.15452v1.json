{"title": "Identifying Critical Tokens for\nAccurate Predictions in\nTransformer-based Medical Imaging Models", "authors": ["Solha Kang", "Joris Vankerschaver", "Utku Ozbulak"], "abstract": "With the advancements in self-supervised learning (SSL),\ntransformer-based computer vision models have recently demonstrated\nsuperior results compared to convolutional neural networks (CNNs) and\nare poised to dominate the field of artificial intelligence (AI)-based medi-\ncal imaging in the upcoming years. Nevertheless, similar to CNNs, unveil-\ning the decision-making process of transformer-based models remains a\nchallenge. In this work, we take a step towards demystifying the decision-\nmaking process of transformer-based medical imaging models and pro-\npose \"Token Insight\", a novel method that identifies the critical tokens\nthat contribute to the prediction made by the model. Our method relies\non the principled approach of token discarding native to transformer-\nbased models, requires no additional module, and can be applied to any\ntransformer model. Using the proposed approach, we quantify the im-\nportance of each token based on its contribution to the prediction and\nenable a more nuanced understanding of the model's decisions. Our ex-\nperimental results which are showcased on the problem of colonic polyp\nidentification using both supervised and self-supervised pretrained vision\ntransformers indicate that Token Insight contributes to a more transpar-\nent and interpretable transformer-based medical imaging model, foster-\ning trust and facilitating broader adoption in clinical settings.", "sections": [{"title": "1 Introduction", "content": "While pretrained CNNs [12,24] have been the go-to models for solving complex\nmedical imaging problems in the past decade, the emergence of transformer-\nbased architectures such as the vision transformer (ViT) [8] and the self-attention\nmechanism [27] originating from natural language processing (NLP) has signif-\nicantly influenced the computer vision community, marking a shift towards re-\nplacing CNNs as the preferred models. The emergence of foundational models as\nwell as other advances in SSL frameworks such as DINO [4], M\u0410\u0415 [11], \u041c\u043e\u0421\u043e [7],"}, {"title": "2 Methodology", "content": "In this section, we provide a description of the dataset used, outline the models\nutilized, and finally describe the proposed method in detail."}, {"title": "2.1 Dataset", "content": "For straightforward and easily understandable experiments, we utilize the com-\nbination of recently proposed CP-CHILD-A and CP-CHILD-B datasets, which\ninvolves the detection of colonic polyps [28]. Both datasets combined comprises of\n9,500 colonoscopy RGB images obtained from 1,600 patients using the Olympus\nPCF-H290DI and FUJIFILM EC-530wm. The task involves identifying colonic\npolyps, which is challenging since it is not trivial to distinguish polyps from other\nproblematic colonic tissues such as lesions, ulcerative colitis, and inflammatory\nbowel disease. Furthermore, many of the polyps presented in the dataset do not\nappear fully in the picture and are only visible in the corners, thus increasing the\nchallenge."}, {"title": "2.2 Models", "content": "To showcase the capabilities of the proposed method, we employ the most com-\nmonly used transformer-based computer vision architecture, Vision Transformer-\nBase/16 (ViT-B/16) [8]. As suggested by [8], we use this model for images of size\n224 x 224 with image tokens of size 16 \u00d7 16, resulting in a total of 196 tokens.\nWe modify the final linear layer of the model to accommodate the two-class\nclassification problem tackled in this work."}, {"title": "2.3 Token Insight", "content": "Given an image $X \\in R^{D \\times W \\times H}$ and its categorical association $y \\in R^{M}$, sampled\nfrom a dataset $(X, y) \\sim D$, where $y_c = 1$ and $y_m = 0$ for all $m\\in \\{0, ..., M\\}\\\\c$,\nlet $g_\\theta(.)$ represent a vision transformer with parameters $\\theta$ that maps an image\nto a set of prediction likelihoods, denoted as $g(\\theta, X) = y'$, where $\\sum_{i=1}^{M} y'_i = 1$. If\narg max$(y') = c$, then the classification is considered correct.\nWhen utilized as an input for a vision transformer, the image $X$ undergoes a\nprocess of patchification (also called tokenization), transforming it into a set of"}, {"title": "", "content": "tokens as shown in Figure 1. In this study, we adopt ViT-B/16 which employs\n16 \u00d7 16 patches. Given that the input images have a resolution of 224 \u00d7 224,\nthe input X is patchified into 196 tokens denoted as $X = [x_1, x_2, ..., x_{196}]$. As\nshown in Figure 1, in the setup of ViT, a special token called (cls] token is\nprepended on X, thus increasing the number of tokens by one. This token is then\nused for the purpose of making classification using a linear layer after the final\ntransformer encoder layer.\nToken Insight is designed to progressively identify the most critical token\ncontributing to the model's prediction. To achieve this, at each step, it removes\none token at a time, measures the change in prediction, and identifies the token\ncausing the largest drop in prediction confidence for the correct class. Subse-\nquently, after examining each token, the one resulting in the highest confidence\nchange when removed is permanently discarded. This process is then repeated\nto discover remaining critical tokens until the prediction ultimately changes. In\nthe case of polyp detection problem, Token Insight finds the tokens that lead to\nthe prediction of the \"Polyp\" class and the token discarding operation is carried\nout repeatedly until the prediction changes from \"Polyp-positive\" to \"Polyp-\nnegative\" for the images considered. Token Insight finds its origins in the works\nof [15,18]. However, unlike those approaches which stop after a single iteration,\nwe continue to search for critical tokens greedily and discard them until the\nprediction is eventually changed, thus identifying all of the critical tokens that\ncontribute to the prediction.\nFormally, we can represent Token Insight as follows: given an input image\nrepresented as $X = [X_k]_{k\\in\\{1,...,196\\}}$, denote by $X^{(t_1,...,t_i)}$ the image where the\ntokens $\\{t_1,..., t_i\\}$ have been removed. At the next iteration, $i + 1$, the most\ncritical token is the token $t_{i+1}$ that gives rise to the largest drop in prediction\nconfidence:\n$g(\\theta, X^{(t_1,...,t_i\\cup t_{i+1})})_c \\leq g(\\theta, X^{(t_1,...,t_i)}), \\text{for all } s \\in \\{1, ..., 196\\}\\{(t_1,...,t_i\\}.$\nThe iterative search described above stops when arg max$(g(\\theta, X^{(t_1,...,t_i)}))_c) \\neq\nc. Note that Token Insight does not alter or remove the [cls] token during\nthe process described above and, in order to generate the prediction confidences\n[cls] token is used according to the description of [8] throughout the process.\nAn illustration of the token discarding operation of Token Insight is provided\nin Figure 2 where the example image is initially predicted with 0.99 confidence as\nhaving a polyp. Using Token Insight, critical tokens are identified and discarded\niteratively until the prediction eventually drops down to 0.44 for the polyp-\npositive class, thus identifying tokens that contribute to the prediction made by\nthe model.\nRelation to occlusion-based methods - Note that, on the surface, Token\nInsight appears to be similar to the occlusion-based methods which mask certain\nregions of the input to measure the change in the prediction [29]. The primary\ndifference between Token Insight and these methods is that during the token\ndiscarding process the model is not affected by the potential spurious signals such\nas missingness bias introduced by the masking operation (i.e., the bias that is"}, {"title": "3 Experimental Results", "content": "introduced by the mask) [13]. Since the tokens are completely removed, the model\nprediction is not influenced by the removed tokens. Unfortunately, since the\ntoken removal operation is only available for transformer-based models, Token\nInsight is not usable for CNNs and other non-transformer DNN architectures."}, {"title": "3.1 Training", "content": "As described in Section 2.2 and Section 2.1, we employ four ViT-B/16 models,\nthree of which are pretrained on the ImageNet dataset, and train them on the\ncombined CP-Child dataset. To discover the most appropriate model with the\nhighest performance, we perform extensive training efforts and in what follows,\nwe detail the training routine that leads to the identification of the best model.\nFor pretrained models (supervised, DINO, MAE), training is performed for\n25 epochs using Stochastic Gradient Descent (SGD) with an initial learning rate\nof $10^{-3}$ and momentum of 0.05. The model trained from scratch undergoes a\nlonger training period (50 epochs) to compensate for the lack of pretraining and\nuses an increased learning rate of 0.035, weight decay of $10^{-5}$, and momentum\nof 0.075. All models employ a batch size of 32 and use cosine annealing, which\nreduces the learning rate after each epoch using the method described in [6].\nIn the work of [28], pretrained ResNets are reported to attain an accuracy\nof approximately 99% on the CP-Child dataset. With pretrained ViTs, we repli-\ncate these results, achieving comparable performance. The model trained from\nscratch exhibits slightly reduced performance at 95.3% due to the absence of\npretraining. Nonetheless, this model still delivers commendable results on this\ndataset, making these models suitable for a study on predictive reasoning."}, {"title": "3.2 Token Insight", "content": "In order to investigate various properties of the proposed approach as well as\nmodels pretrained in different ways, we apply Token Insight to the images con-\ntaining polyps in the CP-Child-B dataset.\nNumber of tokens discarded - For all four models, we investigate the\nnumber of tokens discarded via Token Insight to examine the model's reliance\non tokens for making a polyp-positive prediction. Doing so, in Figure 3a, we\npresent the number of tokens discarded to change a polyp-positive prediction\ninto a polyp-negative one. As it can be seen, both MAE and the model trained\nfrom scratch discard more tokens before the prediction changes, while models\npretrained with supervised learning and DINO discard fewer, and hence rely\non more tokens to make a prediction. This implies that treating all transformer-\nbased medical AI models the same would be a mistake since the number of tokens\nused for confirming a prediction may vary significantly depending on how the\nmodel is trained.\nThe most impactful token - Expanding our investigation, in Figure 3b,\nwe illustrate the influence of the most impactful tokens across the dataset, as"}, {"title": "4 Conclusion", "content": "In this work, we introduced Token Insight, a novel computational method de-\nsigned to identify critical tokens that contribute to the prediction for transformer-\nbased medical imaging models, thus enabling its usage as a method for identi-\nfying spurious correlations as well as erroneous signals that lead to shortcut\nlearning. Unlike many of its predecessors, this method does not necessitate any\nchanges in the model or require any additional extra modules. Furthermore, the\nresults obtained by the proposed approach rely entirely on the change in predic-\ntion confidence of the model, making it a reliable indicator for identifying the\nundesired cases mentioned above.\nWe foresee two possible directions for future work. The current method identi-\nfies a set of tokens that influence the prediction by greedily removing the highest\nimpact token at every iteration, and we have offered computational evidence that\nthe set thus obtained forms a good approximation to the smallest set of highly\ninfluential tokens. For the future, we plan to investigate theoretical bounds for\nhow well our algorithm manages to capture the smallest set of tokens that influ-\nence the prediction. A second line of work concerns the computational cost of our\nmethod, which scales as O($N^2$) in the number of tokens. Given the considerable\ncomputational cost involved in a forward pass of a transformer-based model, it\nwould be of interest to re-use previously acquired information about the impact\nof a token to avoid making a forward pass for each candidate token."}]}