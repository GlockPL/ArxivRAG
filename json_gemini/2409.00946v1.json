{"title": "A Framework for Synthetic Audio Conversations Generation using Large Language Models", "authors": ["Kaung Myat Kyaw", "Jonathan Hoyin Chan"], "abstract": "In this paper, we introduce ConversaSynth, a framework designed to generate synthetic conversation audio using large language models (LLMs) with multiple persona settings. The framework first creates diverse and coherent text-based dialogues across various topics, which are then converted into audio using text-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth effectively generates high-quality synthetic audio datasets, which can significantly enhance the training and evaluation of models for audio tagging, audio classification, and multi-speaker speech recognition. The results indicate that the synthetic datasets generated by ConversaSynth exhibit substantial diversity and realism, making them suitable for developing robust, adaptable audio-based AI systems.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. These advancements have propelled various applications, from automated content creation to sophisticated dialogue systems, significantly enhancing human-computer interactions. However, the potential of these models extends beyond text generation, particularly in the realm of audio-based applications. One under-explored area is the creation of synthetic audio datasets for multi-speaker conversations, which are critical for training and evaluating models in numerous speech-related applications. However, existing audio conversation datasets are relatively uncommon and tend to be limited in size and diversity, posing challenges in developing robust and adaptable models capable of handling diverse scenarios. To address this gap, we propose ConversaSynth, a comprehensive framework for generating synthetic audio conversations using LLMs and text-to-speech (TTS) systems. Our framework leverages the capabilities of LLMs to generate dialogues involving multiple speakers with distinct personas, covering a wide range of topics. These text-based dialogues are then converted into audio, resulting in a contextually rich and varied synthetic conversation audio dataset. The generated datasets demonstrate substantial diversity and realism, which are crucial for training models to perform accurately in real-world scenarios. By utilizing LLMs to create coherent dialogues and employing TTS models to generate natural sounding speech, ConversaSynth can create a dataset that can serve as a valuable resource for various applications, including:\n\nAudio Classification: Improving the accuracy of models tasked with identifying and categorizing different types of audio content.\nSpeech Recognition and Extraction: Advancing the development of models capable of accurately transcribing and segmenting multi-speaker conversations.\n\nMoreover, the flexibility of our framework allows for the creation of specific conversational topics and speaker attributes that are underrepresented in existing datasets. This controlled environment is essential for training models that need to perform reliably across a broad spectrum of contexts and speaker variations. In summary, ConversaSynth represents a significant advancement in the generation of synthetic audio datasets. By bridging the gap between text-based dialogue generation and audio synthesis, our framework not only addresses the current limitations in the field but also creates the way for future research and development in synthetic audio generation and its applications in machine learning and artificial intelligence."}, {"title": "II. LITERATURE REVIEW", "content": "The field of artificial intelligence (AI) has witnessed transformative advancements in Natural Language Processing (NLP) and Text-to-Speech (TTS) technologies. The integration of these technologies has the potential to revolutionize applications such as virtual assistants, content creation, and accessibility tools. This literature review explores the evolution and advancements in LLMs and TTS systems, and examines existing research on their integration."}, {"title": "A. Large Language Models (LLMs)", "content": "Large Language Models (LLMs) have significantly impacted the field of NLP by utilizing massive datasets to generate human-like text. The progression from models like GPT-2 to GPT-3 and GPT-4 has marked substantial improvements in text generation capabilities [1]. These models excel at understanding context, syntax, and semantics, making them proficient in a range of language tasks. The Transformer architecture, introduced by Vaswani in 2017 [2], has been pivotal in advancing LLMs. This architecture employs self-attention mechanisms that allow models to weigh the importance of each word in a sentence relative to others, thereby capturing long-range dependencies and complex language patterns. With the introduction of the Transformer architecture, the development of models that can generate more coherent and contextually relevant text has emerged. In 2020, OpenAI released GPT-3 with 175 billion parameters, set new benchmarks for text generation. Its ability to generate contextually appropriate and coherent text across various tasks has demonstrated the potential of LLMs in content creation and interactive applications. Later, GPT-4 was released with even more parameters and refined training techniques, showing improved versatility and accuracy in generating and understanding text. With these advancements, LLMs have revolutionized text generation, enabling the creation of high-quality articles, creative content, and automated responses. Their ability to generate human-like text has applications in writing assistance, creative storytelling, and chatbots."}, {"title": "B. Text-to-Speech (TTS) Technologies", "content": "Text-to-Speech (TTS) technology has advanced significantly, evolving from basic methods to sophisticated deep learning techniques. This progress has made TTS systems much better at producing natural and clear speech from text. Early TTS systems used a method called concatenative synthesis. In this method, pre-recorded pieces of speech were joined together to form sentences. While this worked, it often resulted in speech that sounded less natural due to the limited variety and quality of the recorded sounds. Later, the introduction of deep learning models like Tacotron [3] and WaveNet [4] revolutionized TTS. Tacotron converts text into a visual representation of sound called a mel-spectrogram, which is then turned into an audio waveform by a vocoder, such as WaveNet. This end-to-end approach greatly enhanced the quality and realism of synthetic speech. Newer approaches, like Parler-TTS [5] and XTTS [6], have pushed the boundaries even further. Parler-TTS is designed to generate expressive speech, making the output sound more engaging and human-like. XTTS allows the generation of highly customized and context-aware speech. This means that XTTS can adapt its speech output based on the situation, making it particularly useful for applications where the tone and style of speech are important. Researchers have also been combining large language models (LLMs) with TTS systems to create conversational agents that sound more human-like. For example, when ChatGPT is integrated with advanced TTS models, it can engage users in natural-sounding conversations, offering more personalized and interactive experiences. This technology has important applications in personalized virtual assistants and interactive media, where a more tailored voice experience can enhance user interaction."}, {"title": "III. METHODOLOGY", "content": "In this section, we outlined the framework, which we coined ConversaSynth, used for generating synthetic audio conversations involving multiple speakers. Our approach is structured around key stages, including the selection of a suitable large language model (LLM), the design of distinct conversational personas, the process of generating conversations, the conversion of text to speech, and the concatenation of audio dialogues as displayed in Fig.1. Each stage is carefully designed to ensure the creation of coherent, contextually relevant, and diverse audio conversations. By leveraging a combination of advanced models and fine-tuned techniques, we aim to produce high-quality synthetic dialogues that maintain consistency in character voices and offer a realistic conversational experience.\n\nThe following sections detail the methodologies applied at each step, from LLM selection to audio post-processing, to achieve our desired outcomes."}, {"title": "A. Selection of Large Language Model (LLM)", "content": "The selection of an appropriate large language model (LLM) is crucial for generating coherent and contextually relevant conversations. To ensure optimal performance, we conducted an evaluation of several LLMs based on their capabilities and alignment with our requirements. The primary candidates considered were Llama2 [11], Llama3 [12], Gemma2 [13], and Mixtral [9]. Our selection criteria focused on three key aspects:\n\nPerformance: We assessed each model's ability to generate coherent, contextually relevant, and diverse conversations, ensuring that the generated dialogues are both realistic and engaging.\nCustomization: We evaluated the flexibility of each model in defining and modifying conversational personas to suit various dialogue styles and topics, which is essential for creating distinct and believable characters.\nComputation: We considered the computational resources required to run each model, aiming to balance high performance with efficiency to ensure feasibility and cost-effectiveness."}, {"title": "B. Designing Conversational Personas", "content": "To generate diverse conversations, distinct personas are defined with variations in name, characteristics, personality traits, and speaking style. For our sample experiment, we created 9 unique personas to ensure diversity in the generated audio data. Fig.4 is an example of how a persona is defined in Python.\n\nIt is crucial that the persona's speaking style includes the phrase \"very clear audio\" to ensure the generated audio remains undistorted."}, {"title": "C. Conversation Generation", "content": "First, we randomly determine the number of participants in each conversation, with the number ranging between 2 and 5 individuals. Following this, we randomly select predefined personas corresponding to the chosen number of participants. These selected personas are then added to the beginning of the prompt. To ensure the generated response adheres to the desired format, we utilize few-shot prompting. The resulting prompt is structured as in Fig.5.\n\nThe selected_personas list in Python contains the persona objects that were randomly selected in the previous step. We then perform string manipulation to generate the dialogues for each persona, ensuring that the conversation flows in the correct order as in Fig.6.\n\nAs demonstrated, it is still necessary to perform post-processing to address expressions such as squinting in the example above. These expressions can be problematic, as our text-to-speech model might interpret them as spoken dialogue. To prevent this, we filter out such expressions using regular expressions."}, {"title": "D. Text-to-Speech Conversion", "content": "In this step, the generated text dialogues are converted into audio, with unique voices assigned to each speaker. For text-to-speech conversion, we utilize two different models for distinct purposes. The first model we employ is Parler-TTS, a text-to-speech synthesis model that generates natural-sounding speech from text, delivering high-quality and customizable audio outputs efficiently. Initially, we attempted to use Parler-TTS exclusively for generating the conversation audio. However, Parler-TTS has a limitation: it tends to produce variations in voices even when the same configuration is applied. As a result, the voice of a single persona, such as Alice, may change from one dialogue to another within the same conversation. To address this issue, we incorporate a second model, XTTS, which is a zero-shot text-to-speech model with voice cloning capabilities. Given an audio sample and text input, XTTS can clone the voice from the audio and generate speech using that cloned voice. However, XTTS is limited in its ability to produce multiple unique voices, aside from the few built-in demo voices. To achieve the desired audio quality and consistency, we combine the strengths of both Parler-TTS and XTTS. First, we use Parler-TTS to generate a unique voice for each persona. Next, we employ XTTS to clone these unique voices and generate the speech for each dialogue. The process for generating synthetic conversation audio is illustrated in Fig.7. This approach ensures that each persona maintains a consistent voice throughout the entire conversation and across the dataset."}, {"title": "E. Audio dialogues concatenation", "content": "The resulting audio files from the previous text-to-speech conversion step are labeled according to the order of their dialogues, such as Alice_0.wav, David_1.wav, and Alice_3.wav. These dialogues are then concatenated to form a complete conversation audio file, which is saved in .wav format. Following this, a ground truth CSV file is created, which includes the filename, starting timestamp, ending timestamp, and speaker information, as illustrated in Table I. Additionally, the conversation audio can undergo further post-processing, such as adding background noise or reverb."}, {"title": "IV. EXPERIMENT", "content": "In this section, we detail the experimental procedures and setups used to evaluate the effectiveness of the synthetic audio conversation generation framework. The experiment was designed to assess the quality and the validity of the generated conversations."}, {"title": "A. Experimental Setup", "content": "The experiment involved generating a total of 200 synthetic conversation audios using the framework outlined in the Methodology section. These conversations varied in length and complexity, with the number of speakers per conversation ranging from 2 to 5. The personas used in these conversations were selected randomly from a predefined pool of 9 personas, ensuring a diverse representation of dialogue styles and topics. For the text generation phase, the Llama3-8B model as we mentioned earlier was utilized to generate coherent and contextually relevant conversations. The generated texts were then converted into audios using Parler-TTS for initial voice synthesis and XTTS for voice cloning to ensure consistency across dialogues."}, {"title": "B. Challenges and Considerations", "content": "During the experiment, several challenges were encountered. The primary issue was the occasional failure of the Llama3-8B model to generate conversations in the correct format, leading to a small percentage of unusable outputs."}, {"title": "V. RESULTS", "content": "The results of our experiment demonstrate the efficacy and robustness of the proposed framework for generating synthetic audio conversations."}, {"title": "A. Text Generation Time", "content": "During the text generation phase, the Llama3-8B model generated 200 conversation scenarios, involving 2 to 5 participants each. Out of the 200 generated conversations, 189 adhered to the correct format, resulting in a success rate of 94.5%. The total time taken for this process was 3730.06 seconds, averaging approximately 18.65 seconds per conversation. This efficiency demonstrates the Llama3-8B model's suitability for large-scale synthetic data generation."}, {"title": "B. Text-to-Speech Conversion Time", "content": "The process of converting the generated text conversations into audio using Parler-TTS and XTTS took 4457.94 seconds in total, averaging approximately 22.29 seconds per audio file. The cumulative duration to generate 189 audio conversations was approximately 8,200 seconds in total, and the time required remains within acceptable limits for practical applications."}, {"title": "C. Exploratory Data Analysis", "content": "The final dataset, consisting of 189 conversation audio files has a total duration of 4.01 hours. It is of high quality and suitable for various applications, including audio classification, speech recognition, and training voice assistants. The inclusion of ground truth annotations further enhances the usability of the dataset for supervised learning tasks. The analysis of the distribution of speaker appearances in the conversations reveals significant insights into the frequency and prominence of different speakers. As illustrated in Fig.8, the distribution of the total number of speakers in each segment varies, with segments involving 2 to 5 speakers. The pie chart shows that 2-speaker segments comprise 27.5% of the total, while 3-speaker, 4-speaker, and 5-speaker segments represent 22.2%, 25.9%, and 24.3%, respectively. This distribution indicates a relatively balanced participation across segments, with a slight tendency toward conversations involving fewer speakers.\n\nFurthermore, Fig.9 presents a horizontal bar chart that details the individual appearances of each speaker. The analysis reveals that \"Alice\" has the highest count at 300 appearances, followed by \"Ben\" with 278 appearances, and \"Cathy\" with 246 appearances. On the lower end, \"Frank\" has the least number of appearances at 205, closely followed by \"Grace\" with 206. The relatively even distribution among speakers suggests that the conversation segments are designed to give equal speaking opportunities to most participants, with some variability."}, {"title": "VI. CONCLUSION", "content": "In conclusion, this work successfully developed and validated a framework for generating synthetic audio conversations using large language models (LLMs) and text-to-speech (TTS) systems. The Llama3-8B model, combined with Parler-TTS and XTTS, provided a robust solution for creating diverse and coherent dialogues with consistent and natural-sounding synthetic voices. The generated sample dataset, comprising over 4 hours of multi-speaker conversation audio, represents a valuable resource for various machine learning and artificial intelligence applications.\n\nThis work demonstrates that the integration of advanced LLMs and TTS models can significantly enhance the quality and scalability of synthetic audio data generation. The ability to customize and control conversational personas and dialogue topics allows for the creation of datasets tailored to specific needs, addressing the limitations of existing audio datasets in terms of size, diversity, and contextual richness."}, {"title": "VII. FUTURE WORK", "content": "Based on the promising results of this experiment, several future research and development can be proposed."}, {"title": "A. Incorporating Environmental Contexts", "content": "Future research could explore the incorporation of synthetic environmental sounds or background noise to simulate more realistic conversation scenarios. This would provide additional context for the generated dialogues, making them more applicable to real-world audio classification and speech recognition tasks."}, {"title": "B. Evaluating Generalization to Other Languages", "content": "While this study focused on English, evaluating the framework's ability to generalize to other languages is a critical area for future work. This would involve training the LLMs and TTS systems on multilingual datasets and assessing the quality and coherence of the generated conversations in different languages.\n\nBy addressing these areas, the framework could be further enhanced to meet the growing demands of synthetic audio generation and its applications in machine learning and artificial intelligence."}]}