{"title": "Self-Emotion Blended Dialogue Generation\nin Social Simulation Agents", "authors": ["Qiang Zhang", "Jason Naradowsky", "Yusuke Miyao"], "abstract": "When engaging in conversations, dialogue\nagents in a virtual simulation environment may\nexhibit their own emotional states that are unre-\nlated to the immediate conversational context,\na phenomenon known as self-emotion. This\nstudy explores how such self-emotion affects\nthe agents' behaviors in dialogue strategies and\ndecision-making within a large language model\n(LLM)-driven simulation framework. In a di-\nalogue strategy prediction experiment, we an-\nalyze the dialogue strategy choices employed\nby agents both with and without self-emotion,\ncomparing them to those of humans. The\nresults show that incorporating self-emotion\nhelps agents exhibit more human-like dialogue\nstrategies. In an independent experiment com-\nparing the performance of models fine-tuned on\nGPT-4 generated dialogue datasets, we demon-\nstrate that self-emotion can lead to better over-\nall naturalness and humanness. Finally, in a\nvirtual simulation environment where agents\nhave discussions on multiple topics, we show\nthat self-emotion of agents can significantly\ninfluence the decision-making process of the\nagents, leading to approximately a 50% change\nin decisions.", "sections": [{"title": "1 Introduction", "content": "In an artificial social environment such as an open-\nworld video game, it is crucial to have nonplayer\ncharacters reflect believable conversational abil-\nity (Ochs et al., 2009) and express human-level\nemotions (Qu et al., 2014). During conversations,\na speaker's expressed emotion typically comprises\na blend of emotions stemming from the conver-\nsational context, denoted as context-emotion, and\nthose arising from life events tangential to the on-\ngoing conversation, denoted as self-emotion (Koch\net al., 2013). Consider a scenario where speaker A\ninforms speaker B that she has passed the bar exam\n(see Figure 1). The context-emotion recognized\nin this scenario could be one of joy or impressed.\nHowever, the emotion expressed by speaker B sig-\nnificantly varies when influenced by different self-\nemotions triggered by other events. For example,\nB might exhibit more intense happiness and an \u201cex-\ncited\" emotion if B is also experiencing a positive\nevent (e.g., a promotion). Conversely, a negative\nevent (e.g., failing an exam) can decrease the happi-\nness associated with the context-emotion, leading\nB to express a \"disappointed\" emotion.\nDespite its critical impact on dialogue behavior,\nself-emotion is often overlooked in the design of\nrecent dialogue models. In this work, we take the\napproach of representing self-emotion as events de-\nrived from simulated background world of speakers\nusing large language models (LLMs) and explore\nthe extent to which self-emotion influences conver-\nsational behaviors of an agent.\nTo achieve this, we construct a virtual agent\nframework and observe the dialogue behaviors\nof the agents under various self-emotional states."}, {"title": "2 Related Work", "content": "Self-emotion Self-emotion, also referred to as\n\u201cinternal emotion,\" plays a significant role in daily\ninteractions. Research on group discussions indi-\ncates that self-emotion in individuals can affect the\nquality of decisions (Van Knippenberg et al., 2010),\nteam performance (Long and Arroyo, 2018), and\nthe decision-making process itself (Hertel et al.,\n2000). Furthermore, other studies suggest that\nthe self-emotion of one member can influence oth-\ners through a mechanism known as mood conta-\ngion (Neumann and Strack, 2000; Sy et al., 2005).\nIndividual self-emotion has also been shown to\nimpact dialogue strategies (Bambauer-Sachse and\nGierl, 2009). In their research, Koch et al. (2013)\ndemonstrate that negative self-emotion encourages\nmore accommodative thinking. Additionally, other\nstudies suggest that effective self-emotion manage-\nment contributes to the development of leadership\nskills (Bjerg and Staun\u00e6s, 2011).\nEmotion-aware Dialogue Generation Existing\nemotion-aware dialogue models typically begin by\nrecognizing an emotion label from the conversa-\ntion history and then proceed with conditional text\ngeneration based on that recognized emotion la-\nbel. The most common emotion representation\nused is discrete emotion categories, such as the\nEkman basic emotions (Li et al., 2017). Subse-\nquent studies have further refined emotion labels\nto include more than 30 categories (Huang et al.,\n2018; Abdul-Mageed and Ungar, 2017; Rashkin\net al., 2019; Demszky et al., 2020). Some works\nalso represent emotions using different styles, such\nas intensity (Zhong et al., 2019), causalities in his-\ntory (Li et al., 2021), and potential emotion tran-\nsitions (Qiu et al., 2020). However, the limitation\nof this approach is that it assumes the emotional\nstate of speakers depends solely on the ongoing\nconversation discourse. Our work differs from\nthese approaches in that we consider self-emotion,\nwhich exists outside the conversation context. In\nthis sense, our approach is similar to response gen-\neration based on user profiles (Zhang et al., 2018;\nSong et al., 2021; Zhou et al., 2020).\nLLM-driven Agent LLMs possess impressive\ncapabilities in scheduling and planning, rendering\nthem valuable for constructing autonomous agents.\nA notable line of research focuses on simulating\nlife-like worlds and observing agent behaviors. For\ninstance, Generative Agents (Park et al., 2023) sim-\nulates a world where agents engage in self-planning\nto manage complex interaction dynamics such as\nmessage propagation and socializing. In their work,\nGao et al. (2023) propose a social simulation frame-\nwork, S\u00b3, to emulate human emotions and attitudes,\nenabling the observation of emergent behaviors us-\ning real-world data. Moreover, research also delves\ninto studying multi-agent collaborations. Agent-\""}, {"title": "3 Self-emotion Agents Framework", "content": "We build a framework\u00b9 in which agents' self-\nemotional states are influenced by a series of events\ngenerated by LLMs according to their profiles.\nAgents in this framework are prompted to manage\ntheir own self-emotion, goals, actions, and profiles."}, {"title": "3.1 Agent Representation", "content": "Agent Profile Each speaker agent has its profile\ngenerated by GPT-4. A profile contains informa-\ntion about the speaker's basic information such as\nname, age, gender, etc. Besides, each profile of\nan agent contains a \u201cdescription\u201d field providing\ninformation of the past experience (See Table 6).\nThis is helpful for further generation of events and\nanalysis of self-emotion status."}, {"title": "Dialogue Strategies as Agent Actions", "content": "Based on\ntheir current self-emotional states and the ongo-\ning conversation context, agents are prompted to\nchoose the most appropriate dialogue strategies for\ntheir next actions. Dialogue strategies are selected\nfrom a pre-defined strategy pool that contains 11 di-\nalogue strategies adapted from the taxonomy of em-\npathetic response intents (Welivita and Pu, 2020).\nA full list of the strategies can be found in Table 8."}, {"title": "3.2 Self-emotion Representation", "content": "Self-emotion can be influenced by various factors,\nsuch as emotional events (Wilms et al., 2020), past\nexperiences (Robinson and Freeston, 2014), cul-\ntural background, and personality traits (Salas et al.,\n2012; Jack et al., 2012). In this work, we represent\nself-emotion in natural language with three styles:\nrandom label, random event and profile event.\nRandom Emotional Label In the context of em-\npathetic dialogue models and datasets, it is com-\nmon to represent emotions using discrete labels (Li\net al., 2017; Hsu et al., 2018; Rashkin et al., 2019).\nDuring a conversation, speakers are randomly as-\nsigned one emotion label from a predefined pool,\nsuch as those used in the EmpatheticDialogues\n(ED) dataset (Rashkin et al., 2019), as their self-\nemotion. We utilize labels from the ED dataset be-\ncause they offer fine-grained distinctions between\nsimilar emotions. The self-emotion is directly rep-\nresented as a sentence of \u201cfeeling <label>\". For ex-\nample, if the emotional label \u201cexcited\u201d is selected,\nthe self-emotion might be represented as \u201c<name>\""}, {"title": "3.3 Self-emotion Generation", "content": "Different types of self-emotion are generated by\nprompting LLMs with necessary information such\nas profiles. For random label self-emotion, each\nspeaker agent will randomly choose an emotional\nlabel in the annotation schema of the ED dataset\nas its self-emotion (e.g., \u201cI'm feeling proud.\"). For\nrandom-event self-emotion, each speaker agent has\nits own self-emotion by analyzing its own profile\nand simulating the encountered events. For in-\nstance, if the profile of a speaker agent is a col-\""}, {"title": "4 Self-emotion in Strategy Selection", "content": "The purpose of this experiment is to explore\nwhether incorporating self-emotion leads to more\nhuman-like dialogue strategies. In this experiment,\nwe have agents simulate speakers in the Empa-\ntheticDialogues (ED) dataset and select the best\nstrategies from a predefined strategy pool in two\nsituations: with and without self-emotion. We then\ncompare the strategies provided by the models to\nthose made by human experts and evaluate the ac-\ncuracy."}, {"title": "4.1 Framework Prompt Settings", "content": "Agent Settings Each conversation in the ED\ndataset includes two speakers. To ensure our agents\nmaintain consistent personal backgrounds for both\nspeakers, the original conversations in the dataset\nare provided to GPT-4 when generating agent pro-\nfiles. The LLM is tasked with generating profiles of\ntwo individuals who could plausibly have the pro-"}, {"title": "4.2 Evaluation", "content": "Baselines Five language models are used as the\nbackend of the speaker agents in this experiment:\nMistral-7B-Instruct (Jiang et al., 2023), Llama-\n2-7B-Chat (Touvron et al., 2023), Gemma-2B-It (Team et al., 2024), gpt-3.5-turbo and gpt-4 2.\nEvaluation of strategy accuracy The experi-\nment is conducted on the test set of the ED dataset,\nresulting in the generation of 2547 conversations\nfor each self-emotion representation approach. Hu-\nman annotations are collected as the ground truth,\nand we define the strategy accuracy as the cosine\nsimilarity between the model-predicted strategy\nand the human strategy:\n$Acc = \\frac{S_m \\cdot S_h}{\\|S_m \\| \\|S_h\\|}$     (1)\nHere, $S_m$ represents the list of strategies chosen by\nthe model and $S_h$ is the list of strategies annotated\nby humans."}, {"title": "4.3 Results & Analysis", "content": "Strategy accuracy Table 2 presents the results\nof strategy accuracy for different representations\nof self-emotion. We are able to observe that\nwithin the same dialogue context, LLMs exhibit im-\nproved strategy selection when prompted with self-\nemotion. The random event self-emotion yields the\nhighest performance, outperforming profile events.\nAdditionally, among all models examined, GPT-4\ndemonstrates the most effective performance.\nSelf-emotion and strategies correlation Figure\n2 illustrates the relationship between the most fre-\nquent self-emotions and corresponding strategies.\nIt shows that for negative self-emotions such as\n\"anxious\u201d and \u201cnervous,\u201d the models tend to ex-\npress more pessimistic strategies such as \u201cexpress-\ning concern\" and \u201csympathizing.\u201d Conversely, for\npositive self-emotions like \u201cproud\" and \"joyful,\"\nthe models lean towards more optimistic strategies\nsuch as \"encouraging.\u201d Additionally, neutral strate-\ngies such as \u201csharing own thoughts\u201d and \u201csharing\nexperience\" are commonly employed across both\npositive and negative self-emotions as the most\nfrequently used strategies."}, {"title": "5 Self-emotion in Dialogue Generation", "content": "In this experiment, we explore whether incorporat-\ning self-emotion in a dialogue model leads to better\nperformance of the generated conversations using\nGPT-4. Additionally, considering the challenges\nassociated with deploying large language models"}, {"title": "5.1 Self-emotion Aware Model Training", "content": "GPT-4 conversations generation We employ\nthe same workflow as described in Section 4 to\ngenerate conversations both with and without self-\nemotion using GPT-4. These generated conversa-\ntions will then be used as training data to train the\nsmall scale models. Different from the previous\nexperiment, we generate using only the random\nevent (as it demonstrates the highest strategy ac-\ncuracy) on the full ED dataset, resulting in a final\ntrain/val/test split of 14,274/2,762/3,569 after fil-\ntering invalid cases with incorrect formats. Table 7\nshows an example of the generated conversation.\nSmall scale model training The purposes\nof training a small-scale model are to enhance\ndeployment convenience and to explore how effec-\ntively the capabilities of LLMs in understanding\nself-emotion can be transferred to a smaller-scale\nmodel. To do this, we fine-tune a FLAN-t5-large\nmodel (Chung et al., 2024) on the collected\ndatasets. Given the seq2seq architecture of the\nmodel, each conversation in the dataset is split into\nmultiple turns between the two speakers. For each\nturn, the utterance of the first speaker serves as\nthe input, and the utterance of the other speaker\nis treated as the label. The task instruction is\nthen prepended to form a training instance. For\ninstance, an example of the input in a training\ninstance without self-emotion is:\n\u201cI'm having a conversation with my friend. My\nfriend is feeling proud. friend: <utterance_1>. me:\n<utterance_2>. friend: <utterance_3>. Generate\nthe response.\"\nThe corresponding label is: \u201cme: <utterance_4>.\n<eos_token>.\" For models with self-emotion, the\nself-emotion is included in the task instruction:\n\u201cI'm having a conversation with my friend. My\nfriend is feeling proud. I'm feeling disappointed\nbecause my project application has been rejected.\""}, {"title": "5.2 Evaluation", "content": "Automatic evaluation The models are evaluated\non ROUGE (Lin, 2004), BLEU (Papineni et al.,\n2002) and BERT-score (Zhang et al., 2019). Table\n3 shows the automatic metrics of the models fine-"}, {"title": "6 Self-emotion in Group Discussion", "content": "Self-emotion can influence group discussions (Her-\ntel et al., 2000; Kelly and Barsade, 2001). In this\nexperiment, agents in the simulated world within\nour framework are prompted to engage in group\ndiscussions incorporating self-emotion across five\ntopics related to teamwork. The purpose of this"}, {"title": "6.1 Framework Prompt Settings", "content": "Group member creation Group member cre-\nation involves creating a profile for each member,\nincluding their roles, positions, and background, by\ninputting the description of the group into GPT-4.\nThe role of the a member is either the \"leader\" or\n\"member\", where the \"leader\" will serve as the host\nof the discussion by pushing the topic to next steps.\nEach \"member\" has their own position and back-\nground which are related to their occupation and\npast experience to trigger self-emotion. Figure 13\nshows the prompt we use to create group members.\nTopic generation Topic generation is the process\nof generating the topics that group members en-\ngage in. To capture the decision-making process,\neach topic is divided into several steps. For exam-\nple, the topic of \u201corganizing a group trip to Italy\nwith a limited budget of $1500 per person\u201d can\nbe broken down into steps such as choosing dates,\nselecting flights, deciding on attractions, choosing\nhotels, and so on. Figure 14 is the prompt we use\nto generate different topics.\nAgent discussion Agents follow the steps of the\ntopic and have discussions. The agents are required\nto reach an agreement before moving to the next\nstep. The \"leader\" of the group judges whether\nan agreement has been reached by analyzing the\ndiscussion history. During a discussion, a hidden\n\"manager\" will decide the next speaker by analyz-\ning the positions of the members and discussion\ncontext. For instance, if the \u201cmanager\u201d decides that\na structural engineer should pose an idea about the\nmaterial, it will set the structural engineer as the\nnext speaker. The \u201cmanager\u201d does not participate\nin the discussion by raising its own opinions."}, {"title": "6.2 Experiment settings", "content": "Agent goals As shown in Figure 3, in order to fa-\ncilitate the self-emotion, we simulate complete pro-\ncess of an agent encountering events, stimulating\nself-emotion, taking behaviors and participating in\nthe group discussions by prompting LLMs. Each\nagent maintains its own goals and self-emotion. For\nexample, in a discussion about \u201cbuilding a house\nand maximizing profits within a limited budget,\u201d\nthe structural engineer may aim to secure better\nmaterials while the landscape engineer may priori-\ntize budget allocation for sustainability. This way,\nagents can develop rich discussion content by ex-\npressing their own ideas, which might be affected\nby their self-emotions.\nWorld setting We assess discussions on 5 topics:\nhouse building, hosting a charity event, planning\na trip, organizing a welcome party, and develop-\ning a mobile app. For each topic, we generate a\ngroup with 6 members, where each member has its\nown role and position. We run 10 different discus-\nsions, and in each discussion, agents will encounter\ntheir own events which will cause the self-emotion.\nWe then compare the decisions made in these dis-\ncussions to those made in a discussion where the\nself-emotion of the agents is disabled.\nFor evaluation, we examine the percentages of\ndecision changes after incorporating self-emotion.\nSpecifically, we categorize these changes into six\ntypes:\n\u2022 Undecided change: discussions that shift\nfrom an agreement to delegation.\n\u2022 Decided change: discussions move from del-\negation to agreement."}, {"title": "6.3 Results & Analysis", "content": "Does adding self-emotion change the decisions?\nFigure 4 shows the average percentage of differ-\nent categories of decision changes influenced by\npositive and negative self-emotion across all topics.\nWe observe that a significant portion of decisions\nare affected: around 66% by negative and 51% by\npositive self-emotion.\nFor different categories of changes, we find that\nnegative self-emotion leads to more undecided, ma-\njority, and compromise changes. This suggests that\nagents with negative self-emotion tend to express\ntheir opinions more, resulting in delegation or com-\npromise in decision-making, which aligns with the\nfindings in (Koch et al., 2013). In contrast, posi-\ntive self-emotion tends to lead to more agreements,\nwith most changes involving the details of plans\nwithout altering the main direction of the decision.\nA comprehensive table of the decision change rates\nacross topics can be found in Table 10.\nAdditionally, an analysis of the average length\nand frequency of utterances indicates that agents\nwith positive self-emotion tend to be more active.\nDiscussions reach agreements more quickly when\nagents have negative self-emotion (Table 11).\nCase study on negative self-emotion changes\nthe decision. Figure 5 shows a discussion on the\ntopic of \"APP development\". The self-emotion of\nthe front-end developer (FD) influences the dis-\ncussion and ultimately leads to a decision change\nfrom \"using React Native and JavaScript as the de-\nvelopment tools\u201d to \u201cKotlin.\u201d In this case, despite\nbeing more agreeable when no self-emotion is intro-\nduced, the FD, experiencing a \u201csad\u201d self-emotion,\nadopts a more objective stance and proposes a dif-"}, {"title": "7 Conclusion", "content": "This work studies the role that self-emotion,\nspeaker's emotion status caused by out-of-context\nevents plays in the process of generating emotional\nresponses. Via a human evaluation, we show that\nmodels considering self-emotion are able to gener-\nate more natural conversations with more human-\nlike strategies. In an experiment of group discus-\nsion simulation, we also show that agent with self-\nemotion can have significant influence on the de-\ncision making process. The results of the experi-\nments demonstrate the importance of considering\nself-emotion when building embodied agents and\ndialogue models that can smoothly participate in\nhuman social activities."}, {"title": "Limitations", "content": "Future work could enhance several aspects of this\nresearch. For example, to capture the decision-\nmaking process, we focused on topics related to\nteamwork. However, group discussions can vary\nin style, such as debating, defending, etc. Future\nresearch can explore these different scenarios and\ninvestigate how self-emotion could affect the final\ndiscussion outcomes. Another point is the halluci-\nnations of language models, which lead to reduced\nrobustness of the agents. Agents may exhibit un-\nexpected behaviors and make choices based on\nimperfect dialogue strategies. While enhancements\nto the agent prompts can mitigate these problems,\nwe believe that such improvements require overall\nadvancements in large language models."}, {"title": "Ethical Considerations", "content": "Agents with self-emotion may bring potential ethi-\ncal risks when deployed in reality. One risk is the\nunpredictable behavior of agents caused by self-\nemotion, especially negative emotions (e.g., anger,\nhatred). We propose that all practitioners ensure\nthe values of agents so that they do not perform\ninappropriate behaviors during discussions. Self-\nemotion-aware agents should be guided by social\nrestrictions based on human values. Another risk is\nthe misinformation that might be caused by the hal-\nlucinations of LLMs. Agents driven by goals might\nexecute actions and produce utterances without re-\nferring to facts, which may lead to the unintentional\nspread of misinformation. Thus we suggest future\napplications to avoid using the generated discus-"}, {"title": "A Fixed Context Experiment & Data\nGeneration", "content": "Data generation is conducted after fixed context ex-\nperiment so that we are able to decide which model\nto use by comparing their performance. The fix\ncontext experiment consists of two steps, profile\ngeneration and conversation generation by prompt\ndifferent models. The experiment pipeline is imple-\nmented on huggingface. The links to the models\nwe use are shown in Table 5."}, {"title": "A.1 Profile Generation", "content": "We adopt the definition of profile as in Genera-\ntive Agents (Park et al., 2023) and added fields\nthat may have more effect on emotion expression,\nwhich includes name, age, innate, occupation, ori-\ngin, gender and an overall description. An example\nof the profile can be found in Table 6. In the profile,\n\"innate\" represents the innate personality of this\nspeaker, which can have an effect on the emotional\nexpression. The \u201cdescription\u201d of a speaker will be\nused for generating the profile-event self-emotion.\nThe prompt we use to generate profiles is shown\nin Figure 7 by providing the original conversations\nin ED dataset. The models are required to generate\nprofiles that can fit the conversation content and\nemotion expressions."}, {"title": "A.2 Conversation Generation", "content": "Without Self-emotion After generating the pro-\nfile, we are able to generate conversations with and\nwithout self-emotion. In ED dataset, each dialogue\nis annotated with an emotion label. Each dialogue\nhas a speaker and a listener and the speaker will ex-\npress the emotion annotated at the beginning of the\nconversation. We utilize this property of the dataset\nand take the first 3 utterances by the speaker and\nlistener as context if the length of the conversation\nis longer than 3. However, for dialogues of which\nthe length is shorter than 3, we take only the first\nutterance as the context. In the prompt, we instruct\nthe LLMs to generate a conversation between \"you\"\nand \"friend\", which represent the \u201clistener\u201d and\n\"speaker\" in the original dataset, respectively. The\nemotion label is used to describe the emotion status\nof \"friend\". We then prompt LLMs to continue to\ngenerate the conversations based on the context and\n\"friend's\" emotion. Figure 6 shows the prompt we\nuse to generate conversations without self-emotion."}, {"title": "With Self-emotion", "content": "When generating conversa-\ntions with self-emotion, we first generate the self-\nemotion based on profile of the speakers by prompt-\ning the same LLM as will be used for generating the\nconversations. Figure 8 and 9 show the prompts we\nuse for generating self-emotion with random events\nand profile events. The generated self-emotion is\nthen used as the emotion status of \u201cyou\u201d in the\nprompt for conversation generation. Figure 10 is\nthe prompt we use to generate conversations with\nself-emotion. An example of generated conversa-"}, {"title": "A.3 Training Data Generation", "content": "The dataset is generated using the same methods as\nin the fixed context experiment. We collect dataset\nfrom GPT-4, because it demonstrates best perfor-\nmance in the fixed context experiment. The conver-\nsations are generated by prompting GPT-4 with the\nprofiles and self-emotion."}, {"title": "B Group Discussion Settings", "content": "Before generating the group discussion, we first\ncreate the world information that includes the back-\nground of the group, the topics they engage in and\nthe profile of each group member including, name,\nrole, position and generic overview. Role is used\nto distinguish whether this member is a \u201cleader\" or\n\"member\" and position describes the part of work\nthis agent is in charge in the group (e.g., interior de-\nsigner, front-end developer, etc.) Figure 13 shows\nthe prompt that we use to generate profiles of the\ngroup members.\nAfter generating the profiles, we need to decide\nthe topics of each group. This is done by manually\ninputting a general topic and prompt LLMs to gen-\nerate the steps of this topic. The prompt we use to\ngenerate the steps is shown in Figure 14."}, {"title": "C Strategy List", "content": "Table 8 shows the list of strategies that we use\nfor generating conversations. The strategies are\nadapted from the analysis of response intents on the\nED dataset. However, in order to demonstrate more\ndiverse actions of the models, we made modifica-\ntions by adding a \"rejection\" action and merging"}, {"title": "D Human Evaluation Details", "content": "Human evaluation is conducted on Amazon Me-\nchanical Turk. We in total hire 43 annotators for\nthe evaluation on the conversations. The annotators\nare requested to answer a questionnaire as shown\nin Table 9 and select one model over the other. The\nquestions are adapted from ACUTE-Eval. To ver-\nify the quality of evaluation, during the task, the\nannotators are asked to answer some verification"}, {"title": "E Group Discussion", "content": "Table 10 shows the percentage of decisions that\nhave been altered after the introduction of self-\nemotion. Across all topics, a notable portion of\ndecisions is observed to be affected. Further in-\nvestigation into the effectiveness of positive and\nnegative self-emotion in the decision-making pro-\ncess reveals that negative self-emotion can result in\na greater diversity of decisions, consistent with the\nfindings in (Koch et al., 2013)."}]}