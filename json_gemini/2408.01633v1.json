{"title": "Self-Emotion Blended Dialogue Generation\nin Social Simulation Agents", "authors": ["Qiang Zhang", "Jason Naradowsky", "Yusuke Miyao"], "abstract": "When engaging in conversations, dialogue\nagents in a virtual simulation environment may\nexhibit their own emotional states that are unre-\nlated to the immediate conversational context,\na phenomenon known as self-emotion. This\nstudy explores how such self-emotion affects\nthe agents' behaviors in dialogue strategies and\ndecision-making within a large language model\n(LLM)-driven simulation framework. In a di-\nalogue strategy prediction experiment, we an-\nalyze the dialogue strategy choices employed\nby agents both with and without self-emotion,\ncomparing them to those of humans. The\nresults show that incorporating self-emotion\nhelps agents exhibit more human-like dialogue\nstrategies. In an independent experiment com-\nparing the performance of models fine-tuned on\nGPT-4 generated dialogue datasets, we demon-\nstrate that self-emotion can lead to better over-\nall naturalness and humanness. Finally, in a\nvirtual simulation environment where agents\nhave discussions on multiple topics, we show\nthat self-emotion of agents can significantly\ninfluence the decision-making process of the\nagents, leading to approximately a 50% change\nin decisions.", "sections": [{"title": "1 Introduction", "content": "In an artificial social environment such as an open-\nworld video game, it is crucial to have nonplayer\ncharacters reflect believable conversational abil-\nity (Ochs et al., 2009) and express human-level\nemotions (Qu et al., 2014). During conversations,\na speaker's expressed emotion typically comprises\na blend of emotions stemming from the conver-\nsational context, denoted as context-emotion, and\nthose arising from life events tangential to the on-\ngoing conversation, denoted as self-emotion (Koch\net al., 2013). Consider a scenario where speaker A\ninforms speaker B that she has passed the bar exam\n(see Figure 1). The context-emotion recognized\nin this scenario could be one of joy or impressed."}, {"title": "2 Related Work", "content": "Self-emotion Self-emotion, also referred to as\n\u201cinternal emotion,\" plays a significant role in daily\ninteractions. Research on group discussions indi-\ncates that self-emotion in individuals can affect the\nquality of decisions (Van Knippenberg et al., 2010),\nteam performance (Long and Arroyo, 2018), and\nthe decision-making process itself (Hertel et al.,\n2000). Furthermore, other studies suggest that\nthe self-emotion of one member can influence oth-\ners through a mechanism known as mood conta-\ngion (Neumann and Strack, 2000; Sy et al., 2005).\nIndividual self-emotion has also been shown to\nimpact dialogue strategies (Bambauer-Sachse and\nGierl, 2009). In their research, Koch et al. (2013)\ndemonstrate that negative self-emotion encourages\nmore accommodative thinking. Additionally, other\nstudies suggest that effective self-emotion manage-\nment contributes to the development of leadership\nskills (Bjerg and Staun\u00e6s, 2011).\nEmotion-aware Dialogue Generation Existing\nemotion-aware dialogue models typically begin by\nrecognizing an emotion label from the conversa-\ntion history and then proceed with conditional text\ngeneration based on that recognized emotion la-\nbel. The most common emotion representation\nused is discrete emotion categories, such as the\nEkman basic emotions (Li et al., 2017). Subse-\nquent studies have further refined emotion labels\nto include more than 30 categories (Huang et al.,\n2018; Abdul-Mageed and Ungar, 2017; Rashkin\net al., 2019; Demszky et al., 2020). Some works\nalso represent emotions using different styles, such\nas intensity (Zhong et al., 2019), causalities in his-\ntory (Li et al., 2021), and potential emotion tran-\nsitions (Qiu et al., 2020). However, the limitation\nof this approach is that it assumes the emotional\nstate of speakers depends solely on the ongoing\nconversation discourse. Our work differs from\nthese approaches in that we consider self-emotion,\nwhich exists outside the conversation context. In\nthis sense, our approach is similar to response gen-\neration based on user profiles (Zhang et al., 2018;\nSong et al., 2021; Zhou et al., 2020).\nLLM-driven Agent LLMs possess impressive\ncapabilities in scheduling and planning, rendering\nthem valuable for constructing autonomous agents.\nA notable line of research focuses on simulating\nlife-like worlds and observing agent behaviors. For\ninstance, Generative Agents (Park et al., 2023) sim-\nulates a world where agents engage in self-planning\nto manage complex interaction dynamics such as\nmessage propagation and socializing. In their work,\nGao et al. (2023) propose a social simulation frame-\nwork, S\u00b3, to emulate human emotions and attitudes,\nenabling the observation of emergent behaviors us-\ning real-world data. Moreover, research also delves\ninto studying multi-agent collaborations. Agent-"}, {"title": "3 Self-emotion Agents Framework", "content": "We build a framework\u00b9 in which agents' self-\nemotional states are influenced by a series of events\ngenerated by LLMs according to their profiles.\nAgents in this framework are prompted to manage\ntheir own self-emotion, goals, actions, and profiles."}, {"title": "3.1 Agent Representation", "content": "Agent Profile Each speaker agent has its profile\ngenerated by GPT-4. A profile contains informa-\ntion about the speaker's basic information such as\nname, age, gender, etc. Besides, each profile of\nan agent contains a \u201cdescription\u201d field providing\ninformation of the past experience (See Table 6).\nThis is helpful for further generation of events and\nanalysis of self-emotion status."}, {"title": "3.2 Self-emotion Representation", "content": "Self-emotion can be influenced by various factors,\nsuch as emotional events (Wilms et al., 2020), past\nexperiences (Robinson and Freeston, 2014), cul-\ntural background, and personality traits (Salas et al.,\n2012; Jack et al., 2012). In this work, we represent\nself-emotion in natural language with three styles:\nrandom label, random event and profile event.\nRandom Emotional Label In the context of em-\npathetic dialogue models and datasets, it is com-\nmon to represent emotions using discrete labels (Li\net al., 2017; Hsu et al., 2018; Rashkin et al., 2019).\nDuring a conversation, speakers are randomly as-\nsigned one emotion label from a predefined pool,\nsuch as those used in the EmpatheticDialogues\n(ED) dataset (Rashkin et al., 2019), as their self-\nemotion. We utilize labels from the ED dataset be-\ncause they offer fine-grained distinctions between\nsimilar emotions. The self-emotion is directly rep-\nresented as a sentence of \u201cfeeling <label>\". For ex-\nample, if the emotional label \u201cexcited\u201d is selected,\nthe self-emotion might be represented as \u201c<name>\""}, {"title": "3.3 Self-emotion Generation", "content": "Different types of self-emotion are generated by\nprompting LLMs with necessary information such\nas profiles. For random label self-emotion, each\nspeaker agent will randomly choose an emotional\nlabel in the annotation schema of the ED dataset\nas its self-emotion (e.g., \u201cI'm feeling proud.\"). For\nrandom-event self-emotion, each speaker agent has\nits own self-emotion by analyzing its own profile\nand simulating the encountered events. For in-\nstance, if the profile of a speaker agent is a col-\""}, {"title": "4 Self-emotion in Strategy Selection", "content": "The purpose of this experiment is to explore\nwhether incorporating self-emotion leads to more\nhuman-like dialogue strategies. In this experiment,\nwe have agents simulate speakers in the Empa-\ntheticDialogues (ED) dataset and select the best\nstrategies from a predefined strategy pool in two\nsituations: with and without self-emotion. We then\ncompare the strategies provided by the models to\nthose made by human experts and evaluate the ac-\ncuracy."}, {"title": "4.1 Framework Prompt Settings", "content": "Agent Settings Each conversation in the ED\ndataset includes two speakers. To ensure our agents\nmaintain consistent personal backgrounds for both\nspeakers, the original conversations in the dataset\nare provided to GPT-4 when generating agent pro-\nfiles. The LLM is tasked with generating profiles of\ntwo individuals who could plausibly have the pro-\nvided conversation. Figure 7 illustrates the prompt\nused for generating these profiles."}, {"title": "4.2 Evaluation", "content": "Baselines Five language models are used as the\nbackend of the speaker agents in this experiment:\nMistral-7B-Instruct (Jiang et al., 2023), Llama-\n2-7B-Chat (Touvron et al., 2023), Gemma-2B-"}, {"title": "4.3 Results & Analysis", "content": "Strategy accuracy Table 2 presents the results\nof strategy accuracy for different representations\nof self-emotion. We are able to observe that\nwithin the same dialogue context, LLMs exhibit im-\nproved strategy selection when prompted with self-\nemotion. The random event self-emotion yields the\nhighest performance, outperforming profile events.\nAdditionally, among all models examined, GPT-4\ndemonstrates the most effective performance.\nSelf-emotion and strategies correlation Figure\n2 illustrates the relationship between the most fre-\nquent self-emotions and corresponding strategies.\nIt shows that for negative self-emotions such as\n\"anxious\u201d and \u201cnervous,\u201d the models tend to ex-\npress more pessimistic strategies such as \u201cexpress-\ning concern\" and \u201csympathizing.\u201d Conversely, for\npositive self-emotions like \u201cproud\" and \"joyful,\"\nthe models lean towards more optimistic strategies\nsuch as \"encouraging.\" Additionally, neutral strate-\ngies such as \u201csharing own thoughts\u201d and \u201csharing\nexperience\" are commonly employed across both\npositive and negative self-emotions as the most\nfrequently used strategies."}, {"title": "5 Self-emotion in Dialogue Generation", "content": "In this experiment, we explore whether incorporat-\ning self-emotion in a dialogue model leads to better\nperformance of the generated conversations using\nGPT-4. Additionally, considering the challenges\nassociated with deploying large language models"}, {"title": "5.1 Self-emotion Aware Model Training", "content": "GPT-4 conversations generation We employ\nthe same workflow as described in Section 4 to\ngenerate conversations both with and without self-\nemotion using GPT-4. These generated conversa-\ntions will then be used as training data to train the\nsmall scale models. Different from the previous\nexperiment, we generate using only the random\nevent (as it demonstrates the highest strategy ac-\ncuracy) on the full ED dataset, resulting in a final\ntrain/val/test split of 14,274/2,762/3,569 after fil-\ntering invalid cases with incorrect formats. Table 7\nshows an example of the generated conversation.\nSmall scale model training The purposes\nof training a small-scale model are to enhance\ndeployment convenience and to explore how effec-"}]}