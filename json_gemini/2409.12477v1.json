{"title": "ViolinDiff: Enhancing Expressive Violin Synthesis with Pitch Bend Conditioning", "authors": ["Daewoong Kim", "Hao-Wen Dong", "Dasaem Jeong"], "abstract": "Modeling the natural contour of fundamental frequency (F0) plays a critical role in music audio synthesis. However, transcribing and managing multiple F0 contours in polyphonic music is challenging, and explicit F0 contour modeling has not yet been explored for polyphonic instrumental synthesis. In this paper, we present ViolinDiff, a two-stage diffusion-based synthesis framework. For a given violin MIDI file, the first stage estimates the F0 contour as pitch bend information, and the second stage generates mel spectrogram incorporating these expressive details. The quantitative metrics and listening test results show that the proposed model generates more realistic violin sounds than the model without explicit pitch bend modeling. Audio samples are available online: daewoung.github.io/ ViolinDiff-Demo.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in neural audio synthesis have yielded impressive results across various domains, such as speech [1]\u2013[3], singing voice [4]\u2013[7], and instrumental sounds [8]\u2013[12]. However, while the synthesized result of speech or singing voice achieved a high mean opinion score (MOS) of around 3.9 [5]\u2013[7], the quality score of music instrument synthesis typically lies around 3.1 [12], indicating significant room for improvement.\nA key challenge in instrumental music synthesis, as compared to singing voice synthesis (SVS), lies in its polyphonic nature. Instruments often play multiple notes simultaneously, whereas singing is predominantly monophonic. Many recent SVS models [5]\u2013[7] utilize explicitly modeled FO (fundamental frequency) contours. VISinger [5] has demonstrated the effectiveness of this approach. It predicts FO from note pitch, note duration, and lyrics, which significantly enhances the naturalness of the synthesized output. However, this approach becomes problematic for polyphonic music, where multiple notes can exist in a single time frame, making it infeasible to encode FO contours as a simple 1D sequence. Furthermore, estimating FO for polyphonic audio is substantially more complex than for monophonic audio due to the overlapping frequencies and interactions between simultaneous notes, which adds another layer of difficulty in acquiring training data for instrument synthesis with F0 modeling.\nIn this paper, we introduce ViolinDiff, a diffusion-based violin synthesis framework that addresses these limitations by modeling pitch bend information of MIDI and incorporating it as an explicit condition of audio synthesis. We trained the model using a dataset released along with a recent violin transcription model [13] that includes detailed polyphonic pitch deviation as MIDI pitch bend We evaluate the effectiveness of ViolinDiff using quantitative metrics and listening tests, demonstrating the advantages of incorporating pitch bend information in violin synthesis. Our contribution lies in i) proposing a bend roll, a new encoding scheme for polyphonic FO contour, ii) a novel evaluation metric for vibrato prediction, and iii) implementing a high-quality violin solo synthesis model with a recently published dataset made by automatic transcription and alignment."}, {"title": "II. RELATED WORKS", "content": "Several studies have been conducted on instrument synthesis, including research that encompasses violin sound. MIDI-DDSP [8] generates expressive details such as vibratos and attack noise directly from MIDI data, improving the realism of synthesized audio. However, MIDI-DDSP is primarily focused on monophonic MIDI, limiting its ability to handle complex polyphonic music. Deep Performer [9], a transformer-based model, synthesizes polyphonic audio from musical scores but lacks fine-grained pitch control.\nRecent advancements in diffusion-based music synthesis models [10], [11] have improved audio fidelity and expressiveness. Hawthorne et al. [10] proposed a T5-based architecture for synthesizing multi-instrument audio. Maman et al. [11] incorporated performer embeddings to control timbre and style for orchestral instruments and the acoustic condition of each recording. These performer embeddings helped enhance the overall quality of synthesized audio. However, these models do not explicitly model FO contours, which leaves a room to be improved in synthesis quality and controllability."}, {"title": "III. DATASET", "content": "We utilized the dataset introduced in [13], which was generated through an iterative learning and alignment process using weak labels, pairing violin recordings with their corresponding publicly available scores. The dataset comprises 34 hours of solo violin performances, 60 etudes by Wohlfahrt (Op. 45), 36 etudes by Kayser (Op. 20), and 24 caprices by Paganini (Op. 1). These pieces were performed by multiple violinists, resulting in 1,021 recordings from 22 distinct performers. The dataset includes both audio recordings and synchronized MIDI files, with the MIDI files providing crucial multi-pitch bends information.\nAfter excluding unavailable recordings due to removed YouTube links and eliminating one performer due to poor audio quality, we used a total of 961 audio recordings from 21 distinct performers. For the validation set, we selected 8 recordings of Kayser Op. 20 No. 16. The test set includes a diverse range of pieces\u2014Kayser Op. 20 Nos. 1, 18, and 36, Paganini Op. 1 Nos. 5, 13, and 24, and Wohlfahrt Op. 45 Nos. 1, 30, and 60\u2014comprising 78 audio recordings across 9 pieces. This setup enabled a comprehensive evaluation of the model\u2019s performance across various compositions and performance styles."}, {"title": "IV. METHOD", "content": "In this section, we introduce the structure of ViolinDiff, which consists of a bend estimation module and a mel spectrogram synthesis module, both in forms of denoising diffusion probablistic models [14].\nTo encode MIDI input, we employed piano roll representation as it enables encoding polyphonic pitch while being aligned to the mel spectrogram at frame level. If a mel spectrogram $M \\in \\mathbb{R}^{F \\times T}$ consists of T time frames and F mel bins, the input piano roll is a 2D binary matrix $R \\in \\{0,1\\}^{P \\times T}$, where P denotes the number of pitches. To emphasize the importance of the onset and offset of each note along with its frame-wise activation, we employed three different types of rolls: $R_{\\text{frame}}$, $R_{\\text{onset}}$, $R_{\\text{offset}}$. We followed the definition of each roll from piano music transcription research [15].\nIn addition to these multiple binary piano rolls, we also include pitch bend information in the model's input. Pitch bends in MIDI are discrete events, each with a pitch bend value and a corresponding time, which enable the continuous change of pitch. These events often occur at finer intervals than what traditional piano roll representations can capture. Therefore, we calculate frame-wise pitch bend value by weighted average considering the duration of pitch bend events within each frame, which results in $R_{\\text{bend}} \\in (-1,1)^{P \\times T}$.\nThe total input of the synthesis module can be represented as $X = \\{M_t,R,t,p\\}$, where $M_t$ is mel spectrogram with t-step gaussian noise, $R = \\{R_{\\text{frame}}, R_{\\text{onset}}, R_{\\text{offset}}, R_{\\text{bend}}\\}$ denotes input MIDI information encoded in piano roll formats that are aligned in frame-level with the mel spectrogram, $t \\in 0, 1, ..., N$ for diffusion time step, and p for performer ID. N denotes the number of diffusion steps.\nBend estimation module adjusts this structure by replacing the mel spectrogram with the direct bend roll $R_{\\text{bend}}$, using $X = \\{R_{\\text{bend}_t}, t, R_{w/o \\text{bend}}, t, p\\}$, where $R_{w/o \\text{bend}} = \\{R_{\\text{frame}}, R_{\\text{onset}}, R_{\\text{offset}}\\}$.", "subsections": [{"title": "A. Synthesis Module", "content": "In the synthesis module, the encoder processes the rolls R as input, through a GRU layer, followed by Transformer Encoder Blocks [16], producing the note feature sequence $E_R \\in \\mathbb{R}^{T \\times D}$ as presented in Fig. 1 (b), where D denotes model dimension size. A simple linear layer then generates an auxiliary mel spectrogram $\\hat{M}$ from $E_R$. For the denoiser of the synthesis module, we adopt the non-causal WaveNet [17] architecture as employed in DiffSinger [6], with additional modifications to incorporate performer conditioning through FiLM layers [18]. To denoise the noisy mel spectrogram $M_t \\in \\mathbb{R}^{F \\times T}$, we first add a time step embedding $e_t \\in \\mathbb{R}^{D}$ by passing it through a linear layer $f: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{F}$ to each time frame of $M_t$. The combined input is modulated again via a FiLM layer, using the auxiliary mel spectrogram $\\hat{M}$ as a conditioning input.\nEach residual block in the denoiser receives conditioning inputs $e_t$, $e_p$, and $E_R$. The performer embedding $e_p \\in \\mathbb{R}^{D}$ is processed through FiLM layers to modulate the internal processing within the residual block, while the time step embedding $e_t$ and the note features $E_R$ are added directly, following the approach of [6].\nThe model is trained to simultaneously minimize two L2 losses: the first, following the DDPM [14] loss formulation, predicts the noise ($\\epsilon$) added to $M_t$, while the second minimizes the error between the predicted auxiliary mel spectrogram $\\hat{M}$ and the ground truth mel spectrogram M. We did not use a weight to balance the two losses."}, {"title": "B. Bend Estimation Module", "content": "The bend estimation module, which is trained separately, is adapted from the synthesis module to handle pitch bend data directly. Unlike the synthesis module, which conditions its denoising process on an auxiliary mel spectrogram $\\hat{M}$, the bend estimation module operates directly on the noisy bend roll $R_{\\text{bend}_t}$ (Fig. 1 (a)). During the diffusion, we only add noise to the regions with note activation. Other regions were excluded in noise addition and loss calculation."}, {"title": "C. Diffusion Process", "content": "Our approach is based on the principles of Denoising Diffusion Probabilistic Models (DDPM) [14]. This framework offers a robust probabilistic method for generating high-quality samples through a learned iterative denoising process, and its effectiveness in music synthesis has been demonstrated in various studies [6], [10]\u2013[12].\nIn the synthesis module, N is set to 200, while in the bend estimation module, N is set to 100. For both modules, the $\\beta$ values increase linearly from $\\beta_1 = 10^{-4}$ to $\\beta_N = 0.06$. Both modules use Classifier-Free Guidance (CFG) [19], with condition dropout applied independently to $e_p$, $e_m$, and $\\hat{M}$ at a probability of 0.1 during"}]}, {"title": "V. EXPERIMENTS", "content": "A. Implementation Details\nThe audio is downsampled to 16kHz, with mel spectrogram computed using a 1024 FFT size, 640 window length, 320 hop length, and 128 Mel bins. The synthesis module is trained on 256-frame spectrograms (5.12 seconds), while bend estimation module uses 512-frame spectrograms (10.24 seconds). The piano roll is limited to 54 pitches (MIDI pitch 55 to 108).\nThe synthesis module uses separate bidirectional GRU layers (128 hidden units per direction, 2 layers) for frame, onset, offset, and bend information, while the bend estimation module processes only frame, onset, and offset information. The GRU outputs are summed and fed into a transformer encoder (256 input dimension, 1024 FFN hidden size, 6 layers, 4 attention heads per layer). Both models share a denoiser with 256 residual channels, and 20 layers without dilation. Both models are trained using the Adam optimizer with a learning rate of 1e-4. The batch size is set to 128. The synthesis module is trained for 280K steps, while the bend estimation module is trained for 112K steps. Both synthesis module and bend estimation module have approximately 25M parameters each.\nFor generating longer audio than the training settings, we followed the methodology outlined in [11], which has proven effective for music synthesis. This approach was originally adapted from motion generation techniques [21], [22]."}, {"title": "B. Baseline Model: ViolinDiff without Pitch Bend", "content": "To evaluate the advantage of modeling pitch bend explicitly, we also designed a baseline model, NoBend, that does not use pitch bend information during the synthesis. NoBend model only uses the synthesis module, and the module takes $R_{w/o \\text{bend}}$ instead of R. Therefore, the baseline has to model the FO trajectory implicitly during the synthesis without explicit pitch bend information."}, {"title": "C. Fr\u00e9chet Audio Distance (FAD)", "content": "Fr\u00e9chet Audio Distance (FAD) measures perceptual similarity by calculating the distance between multivariate Gaussian distributions fitted to embeddings of generated and reference audio [23]. Instead of the commonly used VGGish model [24], we employed the L-CLAP audio and L-CLAP music models from [25], which have been shown to perform effectively for FAD calculation in [26]. Embeddings for FAD were extracted using the fadtk library [26].\n1) All-FAD measures general audio quality by comparing the entire dataset to the synthesized audio generated from the test set MIDI data.\n2) Performer-FAD assesses how well the model replicates the unique characteristics (e.g., timbre, acoustics) of each performer. It is calculated by comparing the synthesized audio generated from the test set MIDI data for a specific performer to the corresponding real audio from the entire dataset of that same performer.\n3) Piece-FAD evaluates how well the model reproduces each piece by comparing the synthesized audio of a specific piece to the real performances of that piece by multiple performers in the test set. For example, if Kayser Op. 20 No. 16 was performed by Performer 0 and Performer 1, the model's generated audio for both performers would be compared to their real performances. This comparison helps assess how accurately the model captures the overall characteristics of the piece across different performers, rather than focusing on the distribution of arbitrary musical pieces."}, {"title": "D. Vibrato Evaluation", "content": "To evaluate the naturalness of vibrato, one of the key expressive features in violin performances, we propose novel metrics for its usage. To measure the usage of vibrato for each note, we followed the method proposed in MIDI-DDSP [8], which applies the discrete Fourier transform (DFT) to the FO sequence of each note to calculate the vibrato value, which is the periodicity of the FO contour. FO extraction was carried out using two different models: TAPE [27], which is specialized for monophonic FO extraction of violin audio, and MUSC [13], which can handle polyphonic sections by extracting multiple F0s. Since TAPE is designed for monophonic input, we excluded polyphonic notes from the vibrato evaluation when using"}, {"title": "E. Listening Test", "content": "The listening test aimed to evaluate the realism of synthesized violin sounds using the MUSHRA protocol [29], with a focus on how well the pitch expression and overall timbre matched real violin performances. 15-second segments were randomly selected from Kayser Op. 20 No. 36, Wohlfahrt Op. 45 No. 60, and Paganini Op. 1 Caprices Nos. 5, 13, and 24. For Kayser and Wohlfahrt, performer embeddings were sourced from two performers who had recorded both pieces, while for Paganini, embeddings came from three renowned performers. Two segments were used for Paganini Caprices Nos. 13 and 24, while one segment was used for the other pieces. To evaluate the model's generalizability on out-of-domain input, we also included three 15-second segments of Bach, Massenet, and Ysa\u00ffe's piece from YouTube-sourced audio, with MIDI generated by using the model from [13] to transcribe and align the audio with separately obtained scores. Performer embeddings for these segments were randomly selected from the same three performers used for the Paganini Caprices.\nThe models compared included those by Hawthorne et al.\u00b9 [10], Maman et al.\u00b2 [11], and our ViolinDiff and the baseline model, with the GM soundfont serving as the lowest anchor. The test, covering 10 segments, was conducted on the webMUSHRA platform [30]. Out of 34 participants, we filtered out eight who rated the reference sample"}, {"title": "VI. RESULTS", "content": "As shown in Table I, ViolinDiff consistently outperformed the baseline model across all FAD metrics (All-FAD, Performer-FAD, and Piece-FAD) in both the L-audio and L-music. The lower All-FAD score indicates more globally realistic audio, while the improved Performer-FAD and Piece-FAD scores suggest better capture of performer-specific and piece-specific qualities. These results highlight the effectiveness of explicitly modeling pitch bend information in achieving expressive and realistic music synthesis.\nIn the vibrato evaluation, ViolinDiff outperformed the baseline model in both F1 Score and Perf-MAE metrics. The result shows that explicit pitch bend modeling enhances vibrato modeling compared to implicit modeling of the baseline, both in terms of personalized style and commonness across the performers. Additionally, Fig. 2 shows a detailed example that ViolinDiff can model clearer vibrato in polyphonic excerpt compared to the baseline model.\nThe listening test results in Fig. 3 show that ViolinDiff achieved the highest realism score among the synthesized models, with a mean score of 70.96, outperforming the baseline model (66.99), showing that incorporating pitch bend information enhances synthesis quality. Wilcoxon signed-rank test showed a significant difference (p < 0.003) between these two models. Although it is difficult to compare the result with other models straightforwardly as the training data is different, our proposed model showed better result compared to the previous diffusion models such as Maman et al. [11] (59.47) and Hawthorne et al. [10] (43.67). The samples are available here."}, {"title": "VII. CONCLUSION", "content": "We presented ViolinDiff, a diffusion-based MIDI-to-audio synthesis model that predicts polyphonic pitch bend and leverages it to synthesize expressive violin performances. Our two-stage architecture, validated through quantitative metrics and listening tests, consistently produced more realistic sound, highlighting the effectiveness of explicit modeling pitch bend information in violin synthesis. For future work, we aim to incorporate additional control parameters such as tempo and articulation to further refine expressiveness. Additionally, extending the model to synthesize other instruments would broaden its applicability."}]}