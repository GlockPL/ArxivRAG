{"title": "CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare", "authors": ["Jingwei Zhu", "Minghuan Tan", "Min Yang", "Ruixue Li", "Hamid Alinejad-Rokny"], "abstract": "The rapid progress in Large Language Models (LLMs) has prompted the creation of numerous benchmarks to evaluate their capabilities. This study focuses on the Comprehensive Medical Benchmark in Chinese (CMB) [24], showcasing how dataset diversity and distribution in supervised fine-tuning (SFT) may enhance LLM performance. Remarkably, We successfully trained a smaller base model to achieve scores comparable to larger models, indicating that a diverse and well-distributed dataset can optimize performance regardless of model size. This study suggests that even smaller models may reach high performance levels with care-fully curated and varied datasets. By integrating a wide range of instructional content, our approach addresses potential issues such as data quality inconsistencies. Our results imply that a broader spectrum of training data may enhance a model's ability to generalize and perform effectively across different medical scenarios, highlighting the importance of dataset quality and diversity in fine-tuning processes.", "sections": [{"title": "Introduction", "content": "With the rapid development of Large Language Models (LLMs), there is increasing interest in ap-plying LLMs to the physical health domain. Due to the specialized nature of physical health, LLMs need to acquire extensive medical knowledge, ensure accuracy, and exhibit patience when interact-ing with patients. To evaluate the knowledge and accuracy of LLMs in this domain, various medical benchmarks have been established. Some models have achieved impressive scores, demonstrating their potential as basic doctor assistants for daily use."}, {"title": "Related Work", "content": "Instruction tuning is a highly effective approach for improving the performance of language models on unseen tasks in zero-shot or few-shot scenarios [26]. This method involves training models with a variety of instructions, enabling them to better understand and execute tasks they have not been explicitly trained on.\nNatural Instructions [18] represents an effort to create a comprehensive set of human-crafted instruc-tions designed to enhance model performance across a wide range of tasks. These instructions serve as a valuable resource for fine-tuning models to perform well in diverse applications. Building on this concept, Super-NaturalInstructions [25] expands the scope by including even more detailed and varied instructions, further improving the robustness and adaptability of language models.\nTo address the issue of limited diversity in human-crafted instructions, Unnatural Instructions [12] introduces a vast dataset of imaginative and varied instructions collected with minimal human effort. This innovative approach leverages automated methods to generate a rich and diverse set of instruc-tions, significantly enhancing the model's ability to handle a wider array of tasks with improved accuracy and efficiency."}, {"title": "Open-Source Medical Models", "content": "In the realm of medical LLMs, several notable open-source projects have emerged, such as Hu-atuoGPT [27] and BenTsao [23]. These models are designed to assist in medical consultations and diagnostics by leveraging large-scale medical dialogues and literature.\nHuatuoGPT and BenTsao [8] have undertaken the task of collecting extensive medical dialogue datasets. They use advanced language models like GPT-4 to reconstruct these dialogues into question-answer pairs for model training. This method aims to improve the models' understand-ing of medical consultations and enhance their ability to provide accurate and relevant responses.\nHowever, these models also come with notable limitations. One major concern is the risk of over-fitting to specific datasets, which can limit their generalizability to new, unseen medical scenarios. The reliance on reconstructed dialogues might lead to inconsistencies in data quality, affecting the robustness of the models' responses.\nThese challenges highlight the need for ongoing refinement and evaluation of open-source medical models. A key area of focus should be the diversity and distribution of datasets used during fine-tuning. Ensuring a wide variety of instructions and data sources may enhance the model's ability to generalize and perform effectively across various medical tasks. By carefully curating and di-versifying the datasets, it is possible to develop more robust and versatile medical LLMs, capable of providing reliable and comprehensive support in healthcare settings. Our work aims to address"}, {"title": "Collective Instruction Set", "content": "The datasets we gather encompass various types, from conversations to question-answering pairs. While we primarily focus on English and Chinese datasets, we also acknowledge the availability of healthcare datasets in other languages, such as HeadQA [22] in Spanish and FrenchMedMCQA [14] in French.\nOur review of publicly accessible datasets indicated that many formats are unsuitable for model fine-tuning due to inconsistencies in structure, detail levels, and annotation standards. To tackle these issues, we decided to standardize all datasets into the Alpaca format [21]. This format includes fields for instruction, input, and output, as well as optional fields for system prompts and history, tailored for specific use cases. By adopting a standardized format, we ensure consistent data processing, enhancing its effectiveness for training and fine-tuning models.\nReconstructing the datasets involves several steps. First, we extract relevant information from each dataset, preserving key details. Then, we reformat this information into the Alpaca structure, which entails defining clear instructions for the model, specifying inputs, and providing expected outputs. For conversational data, we include history fields to maintain context across dialogue turns.\nBy aligning diverse datasets into a single, coherent format, we facilitate more effective training processes and enhance the models' ability to generalize across different medical tasks.\nIn addition to reformatting existing datasets, we also aim to expand our collection with new data sources. This involves curating data from medical forums, academic publications, and other relevant repositories. This ongoing effort ensures our models remain relevant and effective in real-world medical applications.\nMoreover, incorporating diverse datasets helps mitigate biases present in individual data sources. By integrating data from various origins and languages, we create a more balanced and comprehensive training environment. This diversity is essential for developing robust, reliable models capable of providing accurate medical advice across different contexts and populations."}, {"title": "Data Collection", "content": null}, {"title": "Instruction Set Construction", "content": "We construct instructions based on the data types of the collected datasets, ensuring that each type is processed into a unified format that the language models can effectively utilize. This standardization is crucial for maintaining consistency and clarity across different data sources, which is essential for optimizing the model's performance. The following sections detail the strategies used to process various formats of datasets into a standardized format.\nMultiple-Choice Question Answering For the MCQA format, we use a consistent method to process the data. The instruction field typically contains background information and descriptions about the source of the question, which helps the LLM understand the context better. The input field combines the original question with all the answer options. The output field provides the correct answer, along with an explanation if available in the dataset.\nQuestion Answering The QA format is simpler compared to other formats. We leave the input field blank and fill the instruction field with the original question and the output field with the corresponding answer.\nDialogue The dialogue format differs slightly from others due to the nature of conversational data. In this case, we include an additional field named \"history\" that contains the entire chat history up to that point. The instruction field contains the current question, the input field is left blank, and the output field provides the response. This approach helps the LLM understand the context of the ongoing conversation."}, {"title": "Experiments", "content": "We employ advanced tools like LLaMA-Factory [29] to fine-tune our models, exploring various hyperparameters such as cut-off length, epoch count, and learning rate. These parameters are crucial for the models' performance and efficiency.\nOur experiments indicate that cut-off length profoundly affects the model's performance. Specifically, a shorter cut-off length yields better results with the same dataset. This improvement is due to the dataset's average length; shorter cut-off lengths help the model capture essential information within each instance, enhancing output accuracy and relevance.\nIn benchmark scenarios, particularly with multiple-choice questions, a slightly shorter cut-off length proves beneficial. For instance, CMB Exam emphasizes accuracy in answering specific questions over conversational abilities. By aligning the cut-off length with the dataset's average length, we boost the model's efficiency and accuracy for these specialized tasks. Shorter cut-off lengths en-able the model to concentrate on the core content of questions and options, improving its ability to select correct answers. Adjusting other hyperparameters like epoch count and learning rate in tandem with cut-off length further refines performance. A higher epoch count allows the model to learn more comprehensively from the training data, while a well-tuned learning rate ensures optimal convergence without overshooting or getting trapped in local minima."}, {"title": "Hyperparameter Optimization", "content": null}, {"title": "Performance over CMB Benchmark", "content": "We achieve an outstanding score in the Comprehensive Medical Benchmark (CMB) using a remark-ably small model as shown in Table 2, significantly smaller than any other model at the top of the benchmark. This achievement can be attributed to the diversity and distribution of our dataset. Our results demonstrate that the quality of the dataset is the most critical factor influencing the perfor-mance of model fine-tuning.\nBy using a wide variety of data formats and sources, we create a training set that is rich and represen-tative of diverse medical scenarios. This strategy allows our smaller model to generalize better and perform effectively across different tasks within the CMB. The success of our fine-tuning process shows the importance of dataset diversity and demonstrates that even with fewer model parameters, top performance can be achieved through careful dataset selection and distribution.\nFurthermore, our findings challenge the conventional belief that larger models are inherently supe-rior. Instead, they emphasize that a well-curated and diverse dataset can significantly enhance a model's capabilities, enabling smaller models to compete with and even surpass larger ones. This has important implications for the development of efficient, resource-conserving models that do not compromise on performance."}, {"title": "Discussion and Conclusion", "content": "In this article, we have highlighted the potential of using diverse datasets to improve model perfor-mance using SFT. Our findings suggest that incorporating a variety of data types is an effective way to enhance the capabilities of models, achieving better performance with fewer GPU resources.\nOur study also uncovered some limitations associated with this method. One notable issue is that while the fine-tuned smaller models excel at answering multiple-choice questions accurately and effectively, they may lose some of their conversational abilities. This loss means that although the models perform well on specific tasks like MCQA, they struggle to maintain engaging and coherent conversations with users during interactive sessions. This trade-off between specialized task performance and general conversational ability is an important consideration for the application in real-world scenarios.\nAdditionally, we observed common problems associated with smaller models, such as hallucination. Hallucination refers to the generation of plausible but incorrect or nonsensical information by the model. This issue can undermine the reliability of the model's responses and poses a significant challenge for its deployment in sensitive domains like healthcare, where accuracy is paramount.\nIn conclusion, while the use of diverse datasets in supervised fine-tuning offers a promising pathway for quickly enhancing a model's knowledge base and task-specific performance, it also presents several challenges that need to be addressed. Future work should focus on developing strategies to preserve the conversational capabilities of fine-tuned models and reduce instances of hallucination. Overall, this method shows great potential for improving the efficiency and effectiveness of LLMs, but it requires careful consideration and further innovation to fully realize its benefits."}]}