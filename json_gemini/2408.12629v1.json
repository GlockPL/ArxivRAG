{"title": "Data-Free Class Incremental Gesture Recognition via Synthetic Feature Sampling", "authors": ["Zhenyu Lu", "Hao Tang"], "abstract": "Data-Free Class Incremental Learning (DFCIL) aims to enable models to continuously learn new classes while retaining knowledge of old classes, even when the training data for old classes is unavailable. Although explored primarily with image datasets by researchers, this study focuses on investigating DFCIL for skeleton-based gesture classification due to its significant real-world implications, particularly considering the growing prevalence of VR/AR headsets where gestures serve as the primary means of control and interaction. In this work, we made an intriguing observation: skeleton models trained with base classes(even very limited) demonstrate strong generalization capabilities to unseen classes without requiring additional training. Building on this insight, we developed Synthetic Feature Replay (SFR) that can sample synthetic features from class prototypes to replay for old classes and augment for new classes (under a few-shot setting). Our proposed method showcases significant advancements over the state-of-the-art, achieving up to 15% enhancements in mean accuracy across all steps and largely mitigating the accuracy imbalance between base classes and new classes.", "sections": [{"title": "1. Introduction", "content": "Class incremental learning (CIL) refers to the learning paradigm wherein models are continuously updated as new class data is introduced. This area has gained significant attention as researchers recognize that data in real-world scenarios typically arrives incrementally rather than all at once. Although considerable progress has been made in class incremental learning [8, 25, 34, 41, 51], most of the research has focused on experimenting with images as the primary modality. However, the recent emergence of large pre-trained, visual or visual-language models, such as CLIP [33], DINO [4], SAM [19] and their variations, has demonstrated remarkable generalizability, showcasing impressive zero-shot performance. This development, to some extent, challenges the significance of class incremental learning research focused solely on images. We wouldn't be too surprised if leveraging large pre-trained vision models could surpass state-of-the-art performance on common image datasets that most class incremental learning studies tested on, either in zero-shot set or given a few shot prompts. Therefore, we argue that class incremental learning research should address specific real-world problems or be differentiated by modalities, as different modalities may exhibit distinct interclass properties as the CIL algorithm primarily experimented on image datasets might not be a universal solution adaptable to other modalities. Unlike the abundance of image sources readily available from the Internet, human-related data sources are more limited due to factors such as labor costs and privacy concerns associated with collecting data from individuals. These types of data sources may align better with the continuous availability of data. With the increasing trend in Virtual Reality (VR) technology and the growing market penetration of high-quality VR devices like Oculus Quest and Vision Pro, personal VR/AR devices are becoming more prevalent.\nGesture-based interaction is the primary means of controlling AR/VR headsets [15, 16]. Class incremental learning for skeleton-based gesture recognition shows promising value; for example, users may register and customize their own gestures for various functions. However, data privacy and security are significant concerns for human-related computer vision tasks, as even skeletal gestures can inadvertently reveal personal identity information. Thus, restricting the setting to be data-free, meaning that only data used to train newly introduced classes is available without access to training data for old classes, becomes especially important and natural.\nIn this study, we explore Data Free Class Incremental Learning(DFCIL) for Skeleton Gesture, a new research domain introduced by BOAT-MI [1]. Unlike BOAT-MI which involves data synthesis via model inversion and retraining of the whole model. We decide to keep the feature extractor consistently frozen, as we observe that the model trained for skeletal data exhibits a much greater ability to generalize to new classes than images without additional updating. Then, by modeling class feature space as a multivariate normal distribution characterized by a prototype defined by its mean and covariance, we can sample synthetic features that can be used for both old class replay and new class augmentation.\nIn summary, the key contributions of this paper are as follows:\n\u2022 We reveal that models trained for skeleton gestures inherently possess better generalization ability for unseen classes without requiring additional training.\n\u2022 Based on this observation, we proposed a novel Synthetic Feature Replay (SFR) algorithm that circumvents the need for challenging synthetic data generation, which is typically used in existing DFCIL methods. Our approach is not only faster and simpler, but also significantly outperforms current SOTA methods across all tested datasets.\n\u2022 Additionally, we examined the effect of shot size within the Few Shot Class Incremental Learning (FSCIL) context and found that sampling synthetic features for new classes combined with a carefully designed filtering mechanism can alleviate performance degradation caused by limited data availability."}, {"title": "2. Related Work", "content": "Data Free Class Incremental Learning. Regularization-based approaches [20, 25, 51] are a major category to address catastrophic forgetting of old classes [28] present in DFCIL. Knowledge distillation is a quite useful technique that is usually used in conjunction with other approaches. [5, 17, 24, 25, 34, 46, 47] use knowledge distillation to ensure that the model is updated without deviating too much from its frozen copy of its previous state. Model expansion [42, 43, 48] is another effective technique, allowing the model to dynamically grow as needed. Replay-based methods for DFCIL usually involve generating pseudo-samples of previously seen classes to mitigate the forgetting of old class knowledge. Training a sample generator during the pretraining stage is an effective way [8,37], but model inversion is a technique that can invert the model to obtain fake old class samples without training an additional generator [13,38,52].\nFew-Shot Class Incremental Learning. FSCIL [41] is a setup that imposes additional restrictions on the sample size of incremental classes beyond those in DFCIL. One strategy widely used in few-shot class incremental learning is to keep the backbone encoder frozen and update only the classifier to adapt to new classes. These methods focus either on creating better pre-trained base models [40, 56, 57], or designing novel classifiers [53]. Constructing a class prototype [3, 14, 21, 27], originally proposed by ProtoNet [39], primarily used for a few-shot learning, has also proved useful in creating the classifier for FSCIL as well [45,58]. Our approach also adopts such a frozen encoder strategy, but instead of directly using class prototypes to build the classifier, we sample synthetic features from class prototypes and train the classifier along with the sampled features.\nDFCIL for Skeleton-Based Gesture Recognition. Using skeletal data, instead of raw image frames, for gesture recognition, shows great value for its reduced data complexity and therefore effective processing and privacy protection. Deep learning approaches such as CNN-LSTM [31], GCNs [7,23,49], transformers [6,36,44,55], typically show superior performance for skeleton-based gesture recognition. ST-GCN [49] is a popular GCN-based model that can automatically learn both spatial and temporal patterns. DG-STA [6] is a transformer-based model that applies multi-head attention to spatial and temporal edges, respectively, for hand gesture recognition.\nClass incremental learning for skeleton data, especially in the data-free setting, is still a valuable but underexplored domain. To the best of our knowledge, BOAT-MI [1] represents pioneering work in investigating data-free class incremental learning of gestures by developing a model"}, {"title": "3. Preliminaries", "content": "3.1. Problem Definition\nData free class-incremental learning is the setup such that a model need to continue learning novel classes while not forgetting knowledge about old classes. Firstly, the entire dataset is divided into N subsets $D_i = \\{(X,Y) : Y \\in C_i, i \\in T\\}$. Here, i denotes the i-th incremental session and $C_i$ denotes the label space, noting that the class space introduced are exclusive between different steps. The base model is trained with $D_0$ and the model needs to update based on the information from $\\{D_1, D_2, ..., D_{N-1}\\}$ continuously, with data from previous incremental sessions no longer available. However, the test set for the i-th incremental session should encompass all classes seen before, such that $C_{test} = \\cup_{j=0}^{i}C_j$.\nIn addition to DFCIL, few-shot class-incremental learning (FSCIL) imposes a even stricter constraint where the sample size for each class during the incremental session is limited. Each task can be represented as an N-way K-shot classification task, where N classes are learned in the current step, with K samples available for each class.\n3.2. Metric Learning\nMetric learning or frozen encoder-based approaches are usually composed of two parts: a feature encoder $\\phi$ and a classifier H. Given the input and label of sample (x, y), the feature of x can be denoted as $z = \\phi(x)$ and the predicted label can be obtained by $\\hat{y} = H(z)$.\nThe encoder $\\phi$ pre-trained at the base stage is kept frozen for all upcoming incremental steps while only the classifier $H_i$ for each step updates continuously.\n3.3. Instantaneous Forgetting Measure (IFM)\nThe central dilemma in class incremental learning revolves around balancing the preservation of knowledge from old tasks with the ongoing learning of new tasks. However, many existing studies [8, 26, 53, 57] tend to overprioritize overall accuracy as the primary evaluation metric, often overlooking the imbalance between the accuracy of the base classes and the incremental classes. For instance, training directly on new classes without any form of regularization can lead the model to quickly forget old knowledge, causing the new class accuracy to significantly outperform the old classes. On the contrary, frozen encoder-based image CIL approaches tend to perform much poorer on newly introduced classes compared to base classes that are inherently trained for the base model.\nInstantaneous Forgetting Measure [2] is an important metric evaluating such imbalances:"}, {"title": "4. What to Replay? Embeddings or Samples", "content": "Synthetic replay-based DFCIL approaches [1, 8, 13, 37, 52] typically involve training an additional generator or using model inversion to create fake data. However, generating such samples poses significant challenges, as pre-training a generator during the previous step can be cumbersome, and the quality of samples generated through model inversion may suffer, especially for skeletal gesture movements that entail both spatial and temporal changes. BOAT-MI [1] proposed a novel boundary-aware sampling mechanism from class prototypes to generate the best synthetic samples via model inversion, but they still have trouble generating high-quality samples.\nThus, is there a way to bypass the challenging data synthesis step? Our proposed approach directly utilizes synthetic features sampled from prototypes to replay the old class and facilitate the training of the new class, when the samples of the newly class are limited, as illustrated in Figure 1. The effectiveness of our approach is rooted in a key observation."}, {"title": "4.1. Effect of Supervised Contrastive learning", "content": "In contrastive learning framework, a model is trained by pulling together samples from the same classes in embedding space while pushing apart samples coming from different classes. Model trained with constraive learning are proven to generate more robust and generailzable representation, making it effective for multiple downstream tasks, and have a superior transfer ability than its cross-entropy counterpart [18]. Some previous work has shown its effectiveness in continual learning studies [11,40].\nTherefore, during the pre-training stage, we experimented with both vanilla cross-entropy loss and supervised contrastive loss. The supervised contrastive loss for a pair of samples $(x_i, Y_i)$ and $(x_j, Y_j)$ can be represented as:\n$L_{Supcon} = -log\\frac{\\sum_{j=1}^{N} exp(z_i \\cdot z_j / \\tau)}{\\sum_{k=1 [k \\ne i]} exp(z_i \\cdot z_k / \\tau)}$"}, {"title": "4.2. Effect of Replay Buffer Size", "content": "We investigated how the size of synthetic features sampled affect the performance. As shown in Figure 4, performance improves as the buffer size increases from 10 to 20, 50, and nearly saturates at 100. A limited replay buffer size also results in unstable outcomes across different trials, leading to an increased standard error. This highlights the necessity of class prototypes described by mean and covariance, allowing for the sampling of more diverse examples."}, {"title": "4.3. SOTA Comparison", "content": "Although TEEN [45] is primarily designed for FSCIL, we re-implemented this metric learning approach for our set-up based on our observation, found that it performs well and already surpasses the SOTA previously established by BOAT-MI [1]. As shown in Figure 5, our proposed approach further improves the score, especially by achieving more balanced performance (significantly lower IFM). In both data sets, the accuracy gap between two incremental sessions is impressively within 2%. For the Shrec-2017 dataset, our method shows a 16% improvement in global accuracy and a 7% reduction in IFM measures compared to BOAT-MI. In the final and most challenging stage, our approach achieves 81% accuracy \u2014 a 23% improvement \u2014 while maintaining a very low IFM score, demonstrating an excellent accuracy balance between base and new classes. For the Ego-Gesture3D dataset, when comparing the final"}, {"title": "5. Few-Shot Class Incremental Learning", "content": "In the real world, the number of samples that the user can provide to update the model is usually limited. For instance, if the user would like to register a personalized gesture, it is crucial to balance the need for data with the user experience, as asking for too much data can be burdensome and discourage users from using the developed function. Therefore, while our primary focus is on DFCIL, we also examine our approach within FSCIL to evaluate its performance with limited data availability."}, {"title": "5.1. Effect of Shot Size", "content": "To investigate the impact of sample size on the performance, we conducted experiments across three settings, 5-Shot, 10-Shot, and Full-Shot. The comparison results are presented in Tables 3 and 4. We can easily see that, for both datasets, having a larger sample size can significantly improve the performance, particularly for IFM. For Shrec-2017 dataset, employing a 5-Shot setup results in a roughly 9% decrease in accuracy and yields a significantly higher IFM score, increasing from 2% to 13.2%, compared to the Full-Shot setup. This phenomenon contrasts with many other frozen encoder-based approaches that achieve SOTA performance on image datasets. For example, [53] shows marginal or even slightly worse performance with increased data availability."}, {"title": "5.2. Feature Augmentation", "content": "In light of the positive relationship between performance and the size of training samples, the instinctive approach is to augment or generate additional synthetic features for classifier training [50]. However, using fewer samples to construct prototypes increases the likelihood of biasing the simulated class distribution away from the true distribution. Then, the problem becomes how can we sample the correct features from a likely biased distribution. In the feature replay method previously outlined, where the classifier trained for the previous step is available, candidate features can be inputted into the previous classifier, and samples that cannot be classified correctly are rejected.\nHowever, the purpose now shifts towards learning the newly introduced classes instead of replaying for old classes. Thus, we developed a sampling filter strategy to discard samples confused by multiple classes, and this strategy is grounded in the assumption that the ground truth feature space for each class is compact and distinguishable, which we have reason to believe based on the earlier observation. This process involves utilizing Mahalanobis Distance $D_M$ to measure the distance between each sampled feature and the feature distribution of other classes. Samples are rejected if the distance falls below a threshold $\\beta$. After that, we reconstruct the calibrated feature space from the filtered synthetic features, allowing us to sample new and more robust synthetic features, as described in Algo. 2.\n$D_M = \\sqrt{(z - \\mu)^T \\sigma^{-1}(z - \\mu)}$.\nWe investigate the impact of our proposed feature augmentation for new classes in the 5-shot setting. As shown in Figure. 6, with our proposed augmentation approach, there is an improvement in both global accuracy and IFM consistent across all incremental sessions."}, {"title": "6. Experiments", "content": "6.1. Dataset\nFollowing the experimental protocol established by BOAT-MI [1], we evaluated our approaches on two hand gesture skeleton datasets, Shrec-2017 [9] and Ego-Gesture3D [1, 54]. Furthermore, we further evaluated a new body gesture skeleton dataset, NTU-RGB+D [35].\n6.2. Experiment Setup\nWe maintain the incremental task split consistent with BOAT-MI [1]. For the Shrec-2017 dataset, the base class size is set to 8, with 1 class added incrementally. For the Ego-Gesture3D dataset, there are 59 base classes with 4 new classes added at a time. For DFCIL, we conduct three trials for each dataset with different incremental class orders, following the approach of BOAT-MI [1]. For FSCIL, we perform three trials with different shot selections but the same incremental class order. Unlike some existing FSCIL methods that run only a single trial, we emphasize the importance of multiple runs. As shown in Table 6, the standard deviation of the metrics can exceed 15% in all trials, particularly when the shot size is limited, making the impact of outliers significant."}, {"title": "7. Conclusion", "content": "DFCIL for gesture recognition has big real-world implications. In this study, we observe skeletons inherently possess a greater generalization ability to new classes and therefore propose a synthetic feature sampling strategy that can replay old class data and augment new class data. Compared to previous approaches, our method not only achieves greater performance, but is also much more efficient, better suited for the gesture recognition domain, which primarily operates on edge devices."}, {"title": "2. Discussion", "content": "Performance on body motion dataset BOAT-MI [?] only evaluates on two hand gesture datasets, we test our findings and approaches on a new body motion skeleton dataset - NTU. Since BOAT-MI involves a very time consuming model inversion technique (their implementation of model inversion doesn't support batch processing), it takes too much time when evaluated on the much larger NTU dataset. Therefore, we only compared our approach against TEEN [?] which is training-free and has already shown superior performance compared to BOAT-MI. As shown in Table. 3, our approach can also adapt to human body skeleton dataset as well.\nNormal distribution validation To verify the correctness and feasibility of the Gaussian assumption in our approach, we plot histograms and quantile-quantile (Q-Q) plots for the top six principal components of the class feature space to visually inspect the distribution of the feature space as shown in Fig.2. In most cases, the histograms exhibit strong Gaussian characteristics, and the Q-Q plots largely align with the identity line. This indicates that modeling the class feature space as a Gaussian distribution is reasonable. However, some samples are concentrated in the tails as well, forming a tailed distribution. For future work, incorporating a prior to ensure that the features of an encoder follow a normal distribution could be a potential solution.\nCombine with other approaches Our approach doesn't involve any special training or fine-tuning design of the encoder. This allows it to be easily integrated with other methods to further enhance performance. It can be combined with improved training strategies for the base model, such as FACT [?], SAVC [?] or fine-tuning techniques for better adaptation to newly introduced classes."}]}