{"title": "VIDTOK A VERSATILE AND OPEN-SOURCE VIDEO TOKENIZER", "authors": ["Anni Tang", "Tianyu He", "Junliang Guo", "Xinle Cheng", "Li Song", "Jiang Bian"], "abstract": "Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce Vid-Tok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual generation and understanding have emerged as prominent research areas, driven by the capacity of visual data to offer immersive experiences (Ho et al., 2022b; Singer et al., 2023; Ho et al., 2022a; Yu et al., 2023; Kondratyuk et al., 2024; Yang et al., 2024c; Bai et al., 2024; Zhu et al., 2024), convey rich semantic information (Li et al., 2023; Zhang et al., 2024b; Liu et al., 2024), and function as an interface for models to interact with the physical world (Yang et al., 2024b;a; Zhang et al., 2024a; Chen et al., 2024b). However, the high degree of redundancy inherent in pixel-level representations (Sullivan et al., 2012) has led to a shift in modern methodologies. These approaches often employ visual tokenization techniques (Rombach et al., 2022; OpenAI, 2024; Kondratyuk et al., 2024; Wu et al., 2024; Chameleon, 2024), transforming raw visual data into compact latent tokens, which serve as a more efficient basis for tasks involving generation and understanding.\nThe adoption of visual tokenization has catalyzed extensive research on image tokenizers (Rombach et al., 2022; Zheng et al., 2022; Patil et al., 2024), resulting in the development of several open-source tokenizers that serve as widely used tools to advance and streamline image-related research (CompVis; HuggingFace; TencentARC). However, comparable resources and tools remain largely absent in the domain of video. While it is possible to treat each frame of a video as an independent image and compress it using an image tokenizer, this approach overlooks temporal redundancies and consistency, resulting in latent tokens that are temporally redundant and potentially inconsistent across frames.\nRecent efforts have sought to address this gap by introducing video tokenizers that incorporate temporal modeling. However, these approaches often fail to account for diverse use cases and exhibit limitations in performance. For instance, Yang et al. (2024c) exclusively offers tokenizers with continuous tokens, while Kondratyuk et al. (2024) demonstrates the effectiveness of discrete tokens but remains unavailable as an open-source tool. In this work, we introduce VidTok, a versatile and state-of-the-art video tokenizer designed to support both continuous and discrete tokenizations effectively. Our approach follows common architecture as illustrated in Fig. 2, and incorporates several key advancements over existing solutions:\n\u2022 Model architecture. We handle spatial and temporal sampling separately, reducing computational complexity without sacrificing reconstruction quality. Specifically, we employ 2D convolutions in spatial up/downsampling modules and adopt an AlphaBlender operator in temporal up/downsampling modules, while the remaining parts still utilize 3D convolutions to perform information fusion.\n\u2022 Advanced quantization techniques. To address the training instability and codebook collapse com-monly associated with conventional Vector Quantization (VQ) (Van Den Oord et al., 2017), we propose the use of Finite Scalar Quantization (FSQ) in discrete video tokenization. By optimizing the implicit codebook directly, this approach substantially improves discrete tokenizers.\n\u2022 Improved training strategies. To improve training efficiency, we employ a two-stage training strategy: initially pre-training the full model on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. Furthermore, we observe that utilizing training data with reduced frame rates effectively improves the model's ability to represent motion dynamics.\nBuilding upon the aforementioned advancements, we train VidTok on a large-scale video dataset and evaluate its performance on widely used benchmarks such as MCL-JCV (Wang et al., 2016) and a web video evaluation set. Experimental results reveal that VidTok outperforms previous models in both discrete and continuous tokenization, achieving superior results across all evaluated metrics, including PSNR, SSIM, LPIPS, and FVD."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 DISCRETE VIDEO TOKENIZATION", "content": "Discrete tokenization maps input images to a latent space and quantizes the latent representations using a codebook of vectors by identifying the nearest codebook vector. Compared to continuous tokens, discrete tokens offer the advantage of mitigating error accumulation during the autoregressive generation process. Building on the foundation of discrete image tokenization (Van Den Oord et al., 2017), discrete video tokenization extends this approach to video data (Yan et al., 2021; Yu et al., 2024; Wang et al., 2024; NVIDIA). It incorporates temporal modeling to effectively manage the temporal redundancies inherent in video sequences."}, {"title": "2.2 CONTINUOUS VIDEO TOKENIZATION", "content": "Compared to discrete tokenization, continuous tokenization (Zhao et al., 2024; hpcaitech; Chen et al., 2024a; Yang et al., 2024c; NVIDIA) generally offers higher reconstruction fidelity (Rombach et al., 2022). It is typically employed in conjunction with continuous space modeling techniques, such as diffusion models (Ho et al., 2020), to enhance the quality and smoothness of generated outputs. For example, Latent Video Diffusion Models (LVDMs) (Blattmann et al., 2023; Guo et al., 2024; Yang et al., 2024c; OpenAI, 2024) efficiently and effectively generate video content by compressing visual data into continuous latent representation first and then operating on it with denoising techniques. A notable example of this approach is OpenAI's Sora (OpenAI, 2024), which serves as a representative work in this domain.\nCV-VAE (Zhao et al., 2024) introduces a continuous video tokenizer designed to achieve spatio-temporal compression of videos, with a latent space that aligns with the latent space of existing image VAEs (Rombach et al., 2022) through its proposed latent space regularization method. Open-Sora (hpcaitech) and Open-Sora-Plan (PKU-YuanGroup; Chen et al., 2024a) are two open-source projects aimed at reproducing OpenAI's Sora. Both projects offer continuous video tokenizers that effectively perform spatial and temporal compression. CogVideoX (Yang et al., 2024c) introduces a continuous tokenizer that preserves a greater amount of information by maintaining a larger number of latent channels, resulting in enhanced reconstruction fidelity. More recently, Cosmos-Tokenizer (NVIDIA) also provides continuous video tokenizers with various compression ratios.\nThe proposed VidTok builds upon the publicly available models mentioned above by incorporating several key advancements, with the goal of establishing a foundational tokenizer for video-related research."}, {"title": "3 VIDTOK", "content": "In this section, we first introduce the general structure of the video tokenizer with detailed notations. From Sec. 3.2 to Sec. 3.4, we introduce the improved model architecture, the advanced quantization technique, and the improved training strategy respectively."}, {"title": "3.1 OVERVIEW OF VIDEO TOKENIZER", "content": "To enhance efficiency, existing approaches for video generation and understanding often utilize video tokenizers (e.g., 3D VAEs (Kingma & Welling, 2014)) to convert raw visual data into compact latent tokens. As illustrated in Fig. 2, these methods typically involve an encoder that compresses video data into compact latent tokens across both spatial and temporal dimensions, followed by a decoder that reconstructs the tokens back into pixel space. Depending on the scenario, latent tokens can be either continuous (Zhao et al., 2024; Yang et al., 2024c; OpenAI, 2024) or discrete (Yu et al., 2024; Wang et al., 2024; NVIDIA), and the model architecture may be designed to operate in a causal (Yu et al., 2024) or non-causal (Blattmann et al., 2023) manner. To enhance the model's capacity for generating novel data samples and to mitigate overfitting to the training dataset, it is essential to apply appropriate regularization within the latent space (Kingma & Welling, 2014; Van Den Oord et al., 2017)."}, {"title": "3.2 MODEL ARCHITECTURE", "content": "In the existing literature, it is widely acknowledged that fully 3D architectures offer superior reconstruction quality, albeit at a high computational cost (Chen et al., 2024a). However, in this work, we demonstrate that substituting a portion of these 3D convolutions with a combination of 2D and 1D convolutions-effectively decoupling spatial and temporal sampling-can achieve comparable reconstruction quality while significantly reducing computational demands.\nThe detailed network architecture is illustrated in Fig. 3. As shown, 2D convolutions are employed for spatial upsampling and downsampling modules, while an AlphaBlender operator is utilized in the temporal upsampling and downsampling modules. The remaining components, including the input/output layers and bottleneck layers, leverage 3D convolutions to facilitate information fusion. The specific structures of the temporal upsampling and downsampling modules are depicted on the right side of Fig. 3. Additionally, layer normalization (Lei Ba et al., 2016) is incorporated throughout the architecture to enhance stability and performance. Experimental results, as summarized in Tab. 2, validate the effectiveness of the proposed architectural design.\nAlphaBlender operator. Given a parameter a within the range [0, 1], the AlphaBlender operator performs the following operation to input x1 and input 12:\n$x = \\alpha * x_1 + (1 - \\alpha) * x_2$ (2)\nwhere x is the result after blending, and a can be either learnable or a given hyperparameter (PKU-YuanGroup). In this work, we adopt a pre-defined a = Sigmoid(0.2)."}, {"title": "3.3 FINITE SCALAR QUANTIZATION", "content": "Variational AutoEncoders (VAEs) (Kingma & Welling, 2014) are a class of generative models that map each data point, such as an image, from a complex dataset into a continuous distribution within a latent space, rather than assigning it to a single deterministic point. Conversely, the decoder performs the inverse operation, mapping representations from the latent space back to the original input space. However, due to the increasing demand for discrete latent variables, Vector Quantised-Variational AutoEncoder (VQ-VAE) (Van Den Oord et al., 2017) were introduced. Unlike standard VAEs, VQ-VAEs map inputs to a finite set of vectors (i.e., codebook), through a process known as vector quantization. This approach represents each input by the closest vector in the codebook, which is learned during training. By combining the generative capabilities of VAEs with the advantages of discrete representations, VQ-VAEs provide a robust framework for various machine learning applications, including data compression, representation learning, and generative modeling.\nIn this work, we employ Finite Scalar Quantization (FSQ) (Mentzer et al., 2024) to generate discrete tokens. The central principle of FSQ is that each scalar entry in the latent representation is independently quantized to the nearest pre-defined scalar value through rounding. In contrast to Vector Quantization (VQ), FSQ eliminates the need for codebook learning, thereby improving training stability (Mentzer et al., 2024; Yu et al., 2024). The approach can be described as follows: Given a vector z = (z1, z2, ..., zd) with d channels, each channel zi is mapped to a value in a finite set of L pre-defined values, resulting in a quantized representation \u00ee, which is one of Ld possible vectors. Notably, when L is set to 2, each zi can take one of two possible values, yielding binary latents. This mechanism corresponds to the Lookup-Free Quantization (LFQ) method proposed in MAGVIT-v2 (Yu et al., 2024).\nThe experiments in Sec. 4.3.2 show that FSQ has significant advantages in codebook utilization, reconstruction quality and training stability, functioning as an advanced quantization technique that effectively improves discrete tokenizers."}, {"title": "3.4 IMPROVED TRAINING STRATEGIES", "content": "Training video tokenizers is often computationally intensive, requiring substantial resources (e.g., 3,072 GPU hours for 256 \u00d7 256 resolution videos). This necessitates the development of efficient strategies to reduce computational costs while maintaining model performance. In this work, we implement a two-stage training approach to address this challenge: the full model is initially pre-trained on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. Specifically, the model is first trained from scratch using videos at 128 \u00d7 128 resolution. In the second stage, the decoder is fine-tuned using videos at 256 \u00d7 256 resolution.\nThe experimental results presented in Tab. 4 demonstrate that the proposed two-stage training strategy achieves performance comparable to training the model from scratch on 256 \u00d7 256 resolution videos, while substantially reducing computational costs-cutting training time by half, from 3,072 GPU hours to 1,536 GPU hours. Furthermore, since the encoder remains unchanged, the fine-tuned model retains compatibility with the latent space of the pre-fine-tuned model. This ensures that the model can adapt efficiently to novel domains without impacting the integrity of models trained on the same latent space.\nMoreover, as the video tokenizer is designed to model the motion dynamics of input videos, it is essential to efficiently represent these dynamics within the model. In this study, we empirically observe that training with data at reduced frame rates significantly enhances the model's capability to capture and represent motion dynamics. This finding is substantiated through the experimental results presented in Tab. 4 and Fig. 6, which illustrate the improved reconstruction quality achieved with lower frame rate training data."}, {"title": "4 EXPERIMENTS", "content": "This section verifies the proposed VidTok through comparative experiments with existing state-of-the-art video tokenizers (Yu et al., 2024; Wang et al., 2024; NVIDIA; Zhao et al., 2024; hpcaitech; PKU-YuanGroup; Yang et al., 2024c) and comprehensive ablation studies. Fig. 1 provides several radar charts for a quick comparison."}, {"title": "4.1 EXPERIMENTAL SETTING", "content": "Dataset and metrics. For training, we utilize a self-collected video dataset, divided into two subsets based on video quality: (1) Training Set 1, comprising approximately 10 million low-resolution videos (e.g., 480p); and (2) Training Set 2, consisting of approximately 6 million high-resolution videos (e.g., 1080p). All videos in the dataset are natural videos characterized by diverse lighting conditions, motion patterns, and scenarios. For evaluation, we follow the protocol of MAGVIT-v2 (Yu et al., 2024) and use two benchmark datasets: the MCL-JCV dataset (Wang et al., 2016) and the validation set of a web video dataset. Evaluation videos are resized to 256 \u00d7 256 with a frame rate of 30 FPS.\nThe video reconstruction performance of the models is assessed using four widely-used metrics: Peak Signal-to-Noise Ratio (PSNR) (Hore & Ziou, 2010), Structural Similarity Index Measure (SSIM) (Wang et al., 2004), Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018) and Fr\u00e9chet Video Distance (FVD) (Unterthiner et al., 2018).\nImplementation details. We implement video tokenizers with various settings, including both causal and non-causal cases, continuous and discrete latents, and different video compression ratios. All models are trained with four loss terms: a reconstruction term, a perceptual term, an adversarial term and a regularization term. The first three terms follow the practice in Latent Diffusion Models (Rombach et al., 2022). For the regularization term, we use KL loss (Kingma & Welling, 2014) in continuous tokenizers, and entropy penalty and commitment losses in discrete tokenizers (Yu et al., 2024).\nIn the first training stage, Training Set 1 is resized to a resolution of 128 \u00d7 128 and used for initial model training. We train VidTok for 50, 000 steps with batch size 16. In the second stage, Training Set 2 is resized to 256 \u00d7 256 and employed for fine-tuning. We fine-tune the decoder for another 30, 000 steps with batch size 8. The frame rate of the training data is maintained at 3 frames per second (FPS) during both stages. We use Adam optimizer (Kingma & Ba, 2015) with a constant learning rate of 1 \u00d7 10\u22125. The training is conducted on 8 NVIDIA 40G A100 GPUs with PyTorch (Paszke et al., 2019).\nBaselines. We compare our method with the following state-of-the-art solutions: (1) MAGVIT-v2 (Yu et al., 2024): a discrete video tokenizer which maps videos to a discrete latent space using the LFQ representation; (2) OmniTokenizer (Wang et al., 2024): a discrete video tokenizer using VQ as the discrete representation; (3) CV-VAE (Zhao et al., 2024): a continuous video tokenizer with a latent space that aligns with the latent space of existing image VAEs; (4) Open-Sora-v1.2 (hpcaitech): an open-source project aimed at reproducing OpenAI's Sora which offers a continuous video tokenizer; (5) Open-Sora-Plan-v1.2 (PKU-YuanGroup): another open-source project aimed at reproducing OpenAI's Sora; (6) CogVideoX (Yang et al., 2024c): a continuous tokenizer that preserves a greater amount of information by maintaining a larger number of latent channels; (7) Cosmos-Tokenizer (NVIDIA): a suite of continuous and discrete video tokenizers with various compression ratios. We conduct thorough experiments in Sec. 4.2, with aligned settings for all methods to guarantee fairness in comparison."}, {"title": "4.2 COMPARISON WITH BASELINES", "content": "To evaluate the advancements achieved by VidTok, we compare its performance against state-of-the-art models across various scenarios, encompassing both discrete and continuous tokenization approaches. The compre-hensive comparison results are presented in Tab. 1. All performance metrics reported in the table, except for those of MAGVIT-v2 (Yu et al., 2024), are obtained through our own experiments conducted under an identical evaluation protocol to ensure consistency and fairness. For MAGVIT-v2, as the model is not publicly accessible, we reference the results reported in their original publication. It is important to note that these results were obtained on a resolution of 17 \u00d7 360 \u00d7 640, differing from the 17 \u00d7 256 \u00d7 256 resolution used for the other models in our comparison.\nCompared to existing discrete tokenziers (Yu et al., 2024; Wang et al., 2024; NVIDIA), VidTok demonstrates significantly superior reconstruction performance, even when utilizing a smaller codebook size (e.g., 32, 768). This highlights the effectiveness of our approach in discrete tokenization. In the context of continuous tok-enization, VidTok achieves comprehensive improvements across all evaluation metrics, regardless of whether the latent representation comprises 4 or 16 channels. Notably, these advancements are achieved even with a smaller model size, surpassing the performance of state-of-the-art methods (Zhao et al., 2024; hpcaitech; PKU-YuanGroup; Yang et al., 2024c; NVIDIA). These results underscore the effectiveness of VidTok in both discrete and continuous tokenization tasks.\nWe present the corresponding visual reconstruction results in Fig. 5 for qualitative comparison. From these visual results, our method exhibits a distinct advantage in detail reconstruction fidelity and subjective viewing experience."}, {"title": "4.3 ABLATION EXPERIMENTS", "content": "We conduct comprehensive ablation experiments to validate the superiority of the proposed model architecture, the advanced quantization technique and the improved training strategies. All ablation experiments are conducted with a video compression ratio of 4 \u00d7 8 \u00d7 8 and an input size of 17 \u00d7 256 \u00d7 256, evaluated on MCL-JCV (Wang et al., 2016)."}, {"title": "4.3.1 ABLATION ON THE MODEL ARCHITECTURE", "content": "To evaluate the effectiveness of our proposed model architecture, we compare it with three alternative variants in terms of computational complexity and reconstruction quality. (1) Variant 1 employs a fully 3D architecture, integrating spatial and temporal sampling using 3D convolutions. (2) Variant 2 separates spatial and temporal sampling, but does not incorporate the AlphaBlender operator for temporal sampling. (3) Variant 3 replaces all 3D convolutions with 2D convolutions.\nThe experimental results provide insights into the trade-offs between model performance and computational efficiency. The results indicate that employing a fully 3D architecture (Variant 1) results in high computational complexity and model size. By modifying the architecture to replace 3D convolutions in the spatio-temporal sampling modules with a combination of 2D and 1D convolutions (Variant 2), we achieve a significant reduction in computational load without notable degradation in reconstruction quality. Building upon Variant 2, the introduction of the AlphaBlender operator for temporal sampling yields substantial improvements across most metrics, albeit with a slight increase in computational cost. Furthermore, replacing all 3D convolutions with 2D convolutions (Variant 3) leads to a marked decline in reconstruction performance, underscoring the importance of retaining 3D convolutions for effective spatio-temporal representation. Overall, the findings highlight the efficacy of the proposed architecture, which strikes a balance between computational efficiency and reconstruction performance."}, {"title": "4.3.2 ABLATION ON THE DISCRETE TECHNIQUES", "content": "In Tab. 3, we present a comparison of various quantization methods, including VQ (Van Den Oord et al., 2017), LFQ (Yu et al., 2024), and FSQ (Mentzer et al., 2024). Additionally, we analyze the impact of the regularization loss term on the performance of discrete tokenizers.\nThe results highlight several key observations. Traditional VQ suffers from common challenges, such as training instability and codebook collapse, which lead to extremely low codebook utilization and suboptimal reconstruction quality. In contrast, LFQ and FSQ achieve nearly 100% codebook utilization by directly optimizing an implicit codebook, resulting in significantly enhanced tokenizer performance. Furthermore, FSQ outperforms LFQ's binary quantization by achieving better reconstruction fidelity, suggesting reduced information loss during the quantization process.\nThe effects of regularization loss vary across quantization methods. For conventional VQ, the absence of regularization loss leads to model collapse and convergence failure. In the case of LFQ, while the model remains capable of convergence without regularization, it experiences a marked decline in codebook utilization and reconstruction performance. FSQ, on the other hand, demonstrates superior training stability, with its performance remaining largely unaffected even in the absence of the regularization loss term."}, {"title": "4.3.3 ABLATION ON THE TRAINING STRATEGIES", "content": "As detailed in Sec. 3.4, we employ a two-stage training strategy: pre-training the full model on low-resolution videos, followed by fine-tuning only the decoder on high-resolution videos. To evaluate the efficiency and effectiveness of this approach, we conduct an ablation study, with results summarized in Tab. 4.\nIn the first row, training the full model on high-resolution videos directly from scratch requires 3,072 GPU hours. In contrast, the results in the fourth row demonstrate that the proposed two-stage training strategy-starting with low-resolution data and then fine-tuning on high-resolution data-reduces training time by half (from 3,072 to 1,536 GPU hours) while achieving comparable reconstruction quality. A comparison between the third and fourth rows reveals that fine-tuning only the decoder during the second stage produces similar performance to fine-tuning the entire model, with a lower computational cost. This approach also ensures that the low-resolution and high-resolution models share a unified latent space due to the fixed encoder, enabling latent models trained in this shared space to be reused across resolutions and domains.\nAdditionally, the last row examines the impact of varying the sampling rate during training. Qualitative results, presented in Fig. 6, indicate that using training data with reduced frame rates enhances the model's ability to represent motion dynamics effectively."}, {"title": "4.4 MODEL SUMMARY", "content": "We provide a comprehensive summary of model performance in Tab. 5, covering both continuous and discrete tokenization, various video compression ratios, and causal versus non-causal scenarios."}, {"title": "5 CONCLUSION", "content": "In this paper, we present VidTok, a versatile and open-source video tokenizer that achieves state-of-the-art performance in both continuous and discrete tokenization. By converting raw visual data into compact latent tokens, VidTok provides an efficient foundation for tasks related to visual generation and understanding. Through the incorporation of advancements in model architecture, discrete representation, and training strategies, VidTok surpasses existing methods, demonstrating notable improvements across several performance metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation protocols. Additionally, we conduct extensive ablation experiments to thoroughly investigate the performance characteristics of the video tokenizer. We hope this work will inspire future research in this area."}]}