{"title": "Explainable Reinforcement Learning via Temporal Policy Decomposition", "authors": ["Franco Ruggeri", "Alessio Russo", "Rafia Inam", "Karl Henrik Johansson"], "abstract": "We investigate the explainability of Reinforcement Learning (RL) policies from a temporal\nperspective, focusing on the sequence of future outcomes associated with individual actions.\nIn RL, value functions compress information about rewards collected across multiple tra-\njectories and over an infinite horizon, allowing a compact form of knowledge representation.\nHowever, this compression obscures the temporal details inherent in sequential decision-\nmaking, presenting a key challenge for interpretability. We present Temporal Policy De-\ncomposition (TPD), a novel explainability approach that explains individual RL actions\nin terms of their Expected Future Outcome (EFO). These explanations decompose gener-\nalized value functions into a sequence of EFOs, one for each time step up to a prediction\nhorizon of interest, revealing insights into when specific outcomes are expected to occur.\nWe leverage fixed-horizon temporal difference learning to devise an off-policy method for\nlearning EFOs for both optimal and suboptimal actions, enabling contrastive explanations\nconsisting of EFOs for different state-action pairs. Our experiments demonstrate that TPD\ngenerates accurate explanations that (i) clarify the policy's future strategy and anticipated\ntrajectory for a given action and (ii) improve understanding of the reward composition,\nfacilitating fine-tuning of the reward function to align with human expectations.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has demonstrated considerable success in a wide range of\napplications, including complex domains like robotics (Kober et al., 2013; Polydoros and\nNalpantidis, 2017) and telecommunications (Saxena et al., 2022; Vannella et al., 2022). De-\nspite these achievements, a persistent challenge in RL is its lack of interpretability. The\nopacity of RL solutions hinders their wider adoption and deployment to real-world appli-\ncations, especially in safety-critical or highly regulated fields. This challenge has given rise"}, {"title": "2 Related Work", "content": "The literature of XRL presents methods that tackle the interpretability challenge from dif-\nferent perspectives, as outlined by Milani et al. (2023). The most relevant line of research\nfor this paper is the training of auxiliary value functions, sometimes referred to as GVFs, to\noffer a more interpretable view of the primary reward-based value function used for decision-\nmaking. Based on this approach, Juozapaitis et al. (2019) explored the idea of decomposing\nthe value function into separate components that align with the reward composition, assum-\ning that the reward is formulated as a sum of reward components. The value composition\ncan then be used as an explanation of how each reward component affects decision-making.\nLin et al. (2021) generalized this method by introducing user-defined outcomes in place of\ntraditional rewards. Specifically, they proposed a two-stage model where (i) the first stage\nlearns the GVFs as functions of state and action, capturing the user-defined outcomes, and\n(ii) the second stage then uses these GVFs to predict the reward-based value function. This\narchitecture can be trained end-to-end and generate explanations in terms of GVFs, which\nrepresent expected discounted sums of the user-defined outcomes. Instead, Yau et al. (2020)\nproposed to train, in parallel to the reward-based value function, state-action occupancy"}, {"title": "3 Background", "content": "In this section, we briefly present the technical background and notation on RL and FHTD\nlearning. These concepts are extensively used in the remainder of the paper."}, {"title": "3.1 Reinforcement Learning", "content": "An RL problem is mathematically modeled as a Markov Decision Process (MDP) (Sutton\nand Barto, 2018). An MDP is defined by a tuple $M = (S, A, p, r, \\gamma)$, where S is the\nstate space, A is the action space, $p : S \\times A \\times S \\rightarrow [0,1]$ are the transition dynamics,"}, {"title": "3.2 Fixed-Horizon Temporal Difference Learning", "content": "The fixed-horizon state-action value function under a policy \\(\\pi\\), denoted $Q_h^\\pi$, is defined as\nthe expected return collected over a fixed horizon:\n$Q_h^\\pi(s, a) = E_{\\tau ~ \\pi} [\\sum_{t=0}^{h-1} \\gamma^t r_t | s_0 = s, a_0 = a, \\pi]$\nwhere $a_t ~ \\pi(\\cdot|s_t)$ for t = 1, ..., h-1. A state-of-the-art RL prediction algorithm that learns\nthis function is FHTD learning (De Asis et al., 2020). Consider a transition at time t from\nstate $s_t \\in S$ to state $s_{t+1} \\in S$ after taking action $a_t \\in A$, with reward $r_t = r(s_t, a_t, s_{t+1})$.\nIn a finite MDP, let $Q^{(t)}_h \\in \\mathbb{R}^{S \\times A}$ be the estimate of $Q_h^\\pi$ at time t. The FHTD update is:\n$G_h = r_t + \\gamma Q_{h-1}(s_{t+1}, a_{t+1}), a_{t+1} ~ \\pi(\\cdot|s_{t+1})$\n$Q^{(t+1)}_h(s_t, a_t) \\leftarrow Q^{(t)}_h(s_t, a_t) + \\alpha_{t,h}(s_t, a_t) [G_h - Q^{(t)}_h(s_t, a_t)]$\nfor h = 0, . . . , H \u22121, where H \u2208 N is the prediction horizon and $ \\alpha_{t,h}$ a learning rate schedule."}, {"title": "4 Temporal Policy Decomposition", "content": "In this section, we first define the concept of EFO and clarify its utility for explainability\npurposes. Then, we devise an XRL method to explain actions selected by a given black-box\npolicy, such as one learned with a RL control algorithm, in terms of EFOs."}, {"title": "4.1 Expected Future Outcomes", "content": "Since RL involves sequential decision-making, each RL action should be explained in terms\nof the sequence of expected outcomes happening after the action is taken. This type of\nexplanation allows us to quantify and interpret the potential effects of actions across different\ntime steps, thus enabling more explainable and interpretable decision-making. To formalize\nthis idea, we introduce the following definition:\nDefinition 1 (Expected Future Outcome) Given a policy $\\pi : S \\rightarrow \\Delta(A)$, a measurable\nand bounded function $o : S \\times A \\times S \\rightarrow [-1, 1]$, an initial state $s \\in S$ and a starting action\n$a \\in A$, the EFO after h time steps under the policy $\\pi$ is:\n$O_h^\\pi(s, a) = E [o(s_h, a_h, s_{h+1})|s_0 = s, a_0 = a, \\pi]$\nwhere $a_i ~ \\pi(\\cdot|s_i)$ for i = 1, ..., h \u2212 1.\nA state-action GVF can be expressed as:\n$Q^o(s, a) = \\sum_{h=0}^{\\infty} \\gamma^h O_h^o(s, a)$\nIntuitively, the EFOs allow for a temporal decomposition of GVFs under the policy. In\nthe case of a state-action value function $Q^\\pi$, this approach effectively restores temporal\ninformation about when rewards are expected to be collected that is aggregated and lost\nin RL algorithms. For this reason, we call our method Temporal Policy Decomposition\n(TPD).\nWe propose to generate explanations that include EFOs up to a finite prediction horizon,\nas dealing with an infinite number of EFOs is infeasible in practice. Combining the concepts\nof GVF (Sutton et al., 2011; Lin et al., 2021) and fixed-horizon value function (De Asis et al.,\n2020), we have the following definition:\nDefinition 2 (Fixed-Horizon Generalized Value Function) Given a policy $ \\pi : S \\rightarrow\n\\Delta(A)$, a prediction horizon $H \\in \\mathbb{N}$, and a measurable and bounded function $o : S \\times A \\times S \\rightarrow\n[-1,1]$, the Fixed-Horizon Generalized Value Function (FHGVF) under the policy $\\pi$ is:\n$Q_o^{ \\pi,H}(s, a) = E_{\\tau ~ \\pi} [\\sum_{h=0}^{H-1} \\gamma^h o_h | s_0 = s, a_0 = a, \\pi]$\nwhere $o_h = o(s_h, a_h, o_{h+1})$ and $a_h ~ \\pi(\\cdot|s_h)$ for h = 1, . . ., H \u2212 1.\nThe temporal decomposition with a finite prediction horizon $H \\in \\mathbb{N}$ can thus be expressed\nas:\n$Q^o(s, a) = Q_o^{ \\pi,H}(s, a) + c = \\sum_{h=0}^{H-1} \\gamma^h O_h^o(s, a) + c$"}, {"title": "4.2 Relevant Classes of Expected Future Outcomes", "content": "In the following subsection, we discuss two particularly interesting types of outcomes,\nnamely rewards and events."}, {"title": "4.2.1 EXPECTED FUTURE REWARDS", "content": "Rewards are natural choices for outcomes, as RL control algorithms are designed to maxi-\nmize expected discounted cumulative rewards. Thus, a simple and effective outcome func-\ntion is the reward function itself, $r(s, a, s')$, which allows for a temporal decomposition of\nthe state-action value function $Q^\\pi$. This decomposition offers insights into when rewards\nare collected under a given policy $\\pi$.\nInspired by Juozapaitis et al. (2019), when rewards are composed of multiple com-\nponents, each representing a distinct aspect, we can gain an even deeper understand-\ning. Let $r(s,a,s') = g(\\tilde{r}(s,a,s'))$ be the reward function, with $g : \\mathbb{R}^K \\rightarrow \\mathbb{R}$ linear and\n$\\tilde{r}(s, a, s') = [r_k(s, a, s')]_{k=1}^K$ a vector of reward components. Let $R_h^\\pi,k(s, a)$ be the EFO cor-\nresponding to $r_k$, for k = 1, . . ., K. The state-action value function in Eq. (1) can then be\nwritten as:\n$Q^\\pi (s, a) = \\sum_{h=0}^{\\infty} E [g(\\tilde{r}(s_h, a_h, s_{h+1}))|s_0 = s, a_0 = a, \\pi]$\n$= g(\\sum_{h=0}^{\\infty} \\gamma^h E [\\tilde{r}(s_h, a_h, s_{h+1}) | s_0 = s, a_0 = a, \\pi])$\n$= g( \\sum_{h=0}^{\\infty} \\gamma^h R_h^\\pi(s, a))$\nwhere we have used the linearity of g to swap it with expectation and sum. This formulation\nextends the reward decomposition method proposed by Juozapaitis et al. (2019). In addition\nto decomposing the value into distinct value components mirroring the reward structure,\nthis approach also introduces a temporal aspect, allowing for a more granular understanding\nof how reward components are accumulated over time under a given policy."}, {"title": "4.2.2 PROBABILITIES OF FUTURE EVENTS", "content": "A particularly interesting type of outcome is an event. Let $X = S \\times A \\times S$ be the set of all\npossible transitions (s, a, s') and let $\\varepsilon \\subseteq X$ be the set of transitions where the event occurs.\nWe can then define the event as an outcome function e using an indicator function:\n$e(s, a, s') = 1_{\\varepsilon}(s, a, s')$\nand the corresponding EFO $E_h^\\pi$ becomes:\n$E_h^\\pi(s, a) = P [(s_h, a_h, s_{h+1}) \\in \\varepsilon |s_0 = s, a_0 = a, \\pi]$\nIntuitively, when the outcome is an event $e(s, a, s')$, the corresponding EFO $E_h^\\pi(s, a)$ is the\nprobability that in h time steps the event will occur when starting from state $s \\in S$ and"}, {"title": "4.3 Learning Expected Future Outcomes", "content": "Generating contrastive explanations requires learning to predict EFOs $[O_h^\\pi(s, a)]^{H-1}_{h=0}$ for\nall actions $a \\in A$, not just for the actions selected by the target policy $\\pi$, represented\nas $[O_h^\\pi(s, a)]^{H-1}_{h=0}$. This requirement raises the need for an off-policy algorithm capable of"}, {"title": "5 Experiments and Results", "content": "In this section, we describe the simulation-based experiments conducted to evaluate our\nmethod. We begin with an overview of the experimental setup, detailing the RL envi-\nronment and the training of both the policy and the explainers. After that, we present\nqualitative examples of explanations and provide a quantitative assessment of prediction\nerrors compared to ground truth values."}, {"title": "5.1 Experimental Setup", "content": "We adapted the Taxi environment from Gymnasium (Towers et al., 2024) by incorporating\nfuel consumption and traffic dynamics. Visual examples of the modified environment states\nare shown in Figs. 3a, 3b and 4a. The environment is formulated as the following MDP:\n\u2022 State Space: Each state is represented as an integer that encodes the taxi's position\n(row and column), its fuel level, and whether the passenger is in the taxi or not. To\nlimit the state space dimension, the destination, gas station, and initial passenger\nlocation are fixed to the same corner in all episodes.\n\u2022 Action Space: The taxi can perform the following actions: move in four directions\n(south, north, east, west), pick up the passenger, drop off the passenger, and refuel\nat the gas station.\n\u2022 Reward Function: The taxi earns a reward of +20 for successfully dropping off the\npassenger at the destination and +10 for picking up the passenger. A penalty of -100\nis incurred for running out of fuel or performing invalid actions, while a small penalty\nof -1 is applied for each movement action.\n\u2022 Transition Dynamics: Each movement action decreases the fuel level by 1 while refu-\neling increases it by 2. The fuel level ranges from 0 to 20. Movement actions have a\n0.1 probability of failing, which leaves the taxi's position unchanged but still decreases\nthe fuel level, simulating the taxi stuck in traffic. Invalid actions, such as refueling\nwhen the taxi is not at the gas station, leave the state unchanged. The episode ends\nwith either success (when the passenger is dropped off at the destination) or failure\n(when the taxi runs out of fuel).\nThe addition of fuel consumption and traffic dynamics introduces stochasticity to the envi-\nronment, making the interpretation of the taxi's behavior under a policy more challenging.\nEven with an optimal policy, uncertainty due to traffic conditions makes it difficult for a hu-\nman observer to anticipate the policy's behavior. This unpredictability is further amplified\nwhen the policy is suboptimal, underscoring the need for explanations.\nIn this environment, we trained a near-optimal policy using Q-learning with action\nmasking (Watkins and Dayan, 1992; Sutton and Barto, 2018). Based on this policy, we\nthen trained an explainer for each of the following events: dropoff, pickup, refuel, failure,\ntraffic, and move. These explainers are designed to predict the probabilities of future\nevents, as described in Section 4.2.2. The probability of the terminated event is derived by\nexclusion without requiring an additional explainer. Since this set of events is exhaustive,\nand each event corresponds to a deterministic reward, the assumptions in Eqs. (13) to (15)\nare satisfied. Consequently, the expected future reward composition can be reconstructed\nfrom the predicted event probabilities, as outlined in Section 4.2.2. Detailed information\non the training procedure can be found in Appendix B."}, {"title": "5.2 Explainability Results", "content": "Figure 3 illustrates explanations for two environment states that differ only in their fuel\nlevels (Fig. 3a and Fig. 3b). In both cases, the action under analysis is the one selected\nby the policy. The probabilities of future events (Fig. 3c and Fig. 3d) reveal the main\nstrategy of the policy in both states. With a fuel level of 10 (Fig. 3c), the policy aims to\ncomplete the task without refueling, as shown by the high probability of picking up the\npassenger in steps 4-5 and dropping them off in steps 9-10. In contrast, when the initial\nfuel level is 9 (Fig. 3d), the policy prioritizes refueling, indicated by a high probability of"}, {"title": "5.3 Learning Performance", "content": "Accurate predictions are crucial for reliable explanations. To quantitatively assess the qual-\nity of the explanations, we first computed the exact FHGVFs using dynamic programming\nand derived the corresponding ground-truth EFOs. We then compared the approximate\nEFOs predicted by our method against the ground truth.\nFor the evaluation, we collected states by running the policy for 104 episodes. For each\nstate encountered, we computed the approximate and exact EFOs for all possible actions,\nincluding both the actions selected by the policy and the other actions. The evaluation\nmetrics are reported separately for on-policy and off-policy actions and denoted by \\(\\pi\\) and\n\\(\\overline{\\pi}\\), respectively. We repeated the evaluation process for 10 independent runs. The results,\npresented in Table 1, demonstrate that the explainers accurately predict the EFOs for all the\nactions. This finding suggests that the qualitative explanations generated by our method\nare reliable and trustworthy.\nIt is important to note that dynamic programming has two key limitations that hinder\nits practical adoption and motivate our data-driven method. First, it assumes known tran-\nsition probabilities, which contradicts the core strength of RL the ability to work with\nunknown dynamics. Second, it does not generalize or scale well to continuous or high-\ndimensional MDPs. As a result, dynamic programming is impractical outside of controlled,\nlow-dimensional simulated MDPs."}, {"title": "6 Conclusion", "content": "In this paper, we introduced Temporal Policy Decomposition (TPD), an XRL method\ndesigned to clarify RL decision-making by explaining actions in terms of their Expected\nFuture Outcomes (EFOs) across time steps. We showed how GVFs, including the value\nfunctions used for decision-making in RL control algorithms, can be decomposed along\nthe temporal dimension to provide insights into when specific outcomes are expected to\noccur. To achieve this, we leveraged Fixed-Horizon Temporal Difference learning to devise\nan off-policy approach that efficiently exploits the sequential nature of MDPs. Our results\ndemonstrated that TPD delivers both accurate and interpretable explanations that clarify"}, {"title": "Appendix A. Convergence of FHTD Learning", "content": "In this appendix, we prove Theorem 3 presented in Section 4.3.\nTheorem 3 Assume that (i) the behavioral policies (\u03b2t)t, \u03b2t : S \u2192 \u2206(A), with at ~ \u03b2t(\u00b7|st),\nensure that every state-action pair (s,a) is visited infinitely often and (ii) for every h\nthe sequences \u03b1t,h(s,a) are positive, non-increasing in t, satisfying \u03a3n=1\u221e \u03b1n,h(s,a) = \u221e,\n\u03a3n=1\u221e [\u03b1n,h(s, a)]2 < \u221e. Then, under these assumptions and using the update in Eq. (5), we\nhave that limt\u2192\u221e Qh(t+1)(s, a) = Qh(s, a) almost surely for every (s,a).\nProof Denote by Qth(s, a) the fixed-horizon Q-value for horizon h at time step t in a given\nstate-action pair (s, a). We can rewrite the update rule in Eq. (5) as:\nQh(t+1)(s, a) =\n{ Qh(t)(s, a) + \u03b1t,h(s, a) (rt + \u03b3 Qh\u22121(t)\n(st+1, at+1) \u2212 Qh(t)(s, a)), if (s, a) = (st, at)\nQh(t)(s, a),\notherwise\nwhere at+1 ~ \u03c0(\u00b7|st+1), rt ~ q(st, at), with r(s, a) = Er~q(s,a) [r] and Qh(s, a) = 0 for every t\nand (s, a).\nWe exploit the structure of the update scheme, that is Qh\u22121(t) does not depend on\nQh(t), Qh+1(t),... to prove the claim by induction.\nProof of convergence for h = 1. We first prove that the iteration converges for h = 1,\nwhich is given by\nQ1(t+1)(s, a) =\n{ Q1(t)(s, a) + \u03b1t,h (rt \u2212 Q1(t)(s, a)), if (s, a) = (st, at)\nQ1(t)(s, a),\notherwise"}]}