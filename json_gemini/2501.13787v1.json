{"title": "Parameter-Efficient Fine-Tuning for Foundation Models", "authors": ["Dan Zhang", "Tao Feng", "Lilong Xue", "Yuandong Wang", "Yuxiao Dong", "Jie Tang"], "abstract": "This survey delves into the realm of Parameter-Efficient Fine-Tuning (PEFT) within the context of Foundation Models (FMs). PEFT, a cost-effective fine-tuning technique, minimizes parameters and computational complexity while striving for optimal downstream task performance. FMs, like ChatGPT, DALL-E, and LLaVA specialize in language understanding, generative tasks, and multimodal tasks, trained on diverse datasets spanning text, images, and videos. The diversity of FMs guides various adaptation strategies for PEFT. Therefore, this survey aims to provide a comprehensive overview of PEFT techniques applied to diverse FMs and address critical gaps in understanding the techniques, trends, and applications. We start by providing a detailed development of FMs and PEFT. Subsequently, we systematically review the key categories and core mechanisms of PEFT across diverse FMs to offer a comprehensive understanding of trends. We also explore the most recent applications across various FMs to demonstrate the versatility of PEFT, shedding light on the integration of systematic PEFT methods with a range of FMs. Furthermore, we identify potential research and development directions for improving PEFTs in the future. This survey provides a valuable resource for both newcomers and experts seeking to understand and use the power of PEFT across FMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Foundation Models (FMs) are pre-trained on large-scale datasets [1, 2, 3, 4, 5, 6] (often covering various types, such as text, images, and videos, etc.) to cater to multiple tasks like language understanding [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], code generation [18, 19], image or video understanding [20], visual content generation [21, 22, 23], as depicted in Fig. 2 (left). Presently, various FMs dominate distinct domains, for instance, language-focused tasks are supported by ChatGPT [4], ChatGLM [24, 25], and Qwen [26], while vision-language tasks are tackled by ChatGPT-4V [27]. DALL-E [28], Sora [29], and Veo2\u00b9 specialize in generative tasks, and LLaVA [30], and NEXT-GPT [31] excel at multimodal ones, as depicted in Fig. 2 (middle). In real-world applications, fine-tuning these FMs on unseen downstream datasets is usually required to achieve task-specific performance.\nParameter-Efficient Fine-Tuning (PEFT) technology [32, 33, 34, 35], a highly active topic, demonstrates notable cost-effectiveness during the fine-tuning process, as depicted in Fig. 1 and Fig. 2 (right). This technique minimizes the trainable parameters and computational overhead while aspiring to near fully fine-tuned performance on downstream tasks. Taking GPT-3 [3] as an example, full fine-tuning involves all 175B parameters, whereas LoRA [36] requires training only 4.7M or 37.7M, saving over 99.97% of parameters, and the result is a 0.1% to 0.5% improvement compared to full fine-tuning. Such attributes brought significant practical value to the community and real-world applications2. Nonetheless, the diversity of FMs steered the various adaptation strategies for PEFT. For example, in the prompt tuning approach, the design of trainable prompt modules often varies depending on the type of FMs (e.g., text prompts [37] for large language models (LLMs), and visual prompts [38] for vision language models (VLMs)). Similarly, LoRA [36] is integrated into different components of FMs depending on their architecture (e.g., transformer blocks [39] for LLMs or denoising U-Net [40] for vision content generation models (VGMs)). Consequently, conducting a comprehensive survey of how PEFT techniques are adapted across diverse FMs is crucial for advancing this field. This understanding will pave the way for more systematic and effective applications of PEFT across a wide range of tasks and domains.\nAs highlighted above, FMs are iterating at an unprecedented pace concerning structure, method, and applications. This rapid evolution fueled the PEFT community to become equally dynamic. Hence, keeping track on the technological trend of PEFT within FMs is imperative. As shown in Fig. 1, we count the total number of citations of PEFT methods across various FMs over the past five years as a trend indicator and have three key trends:\nTrend i: The field of PEFT is experiencing remarkable growth, covering a diverse range of tasks and FMs, including language, vision, and multimodal domains.\nTrend ii: LLMs and vision foundation models (VFMs) dominate the current research landscape, showing rapid and substantial increases in activity, while VLMs and vision content generation models (VGMs) are gaining traction as secondary areas of focus.\nTrend iii: In contrast, multimodal foundation models (MFMs) remain relatively underexplored, suggesting significant opportunities for future research and innovation in this area.\nIn this survey, we aim to explore the potential of integrating PEFT with various FMs to enhance scalability. Furthermore, given the mutual dynamism of these two communities, several overview surveys recently emerged, as shown in Table I. Like, Xin et al. [32] systematically review visual PEFT (covering common datasets and applications) while identifying future directions. Zhou et al. [34] expand the scope to multimodal LLMs and present empirical studies of several mainstream PEFT methods. Their findings highlight the superior performance of adapter tuning and the positive role of connector layers in fine-tuning MFMs. Wang et al. [35] focused on the core ideas and principles of various PEFT algorithms, providing a quick theoretical guide. Notably, Han et al. [33] offered detailed insights into PEFT for LLMs from an algorithmic standpoint and proposed recommendations for system design in real-world scenarios. These valuable surveys offer focused insights into some lines of PEFT. However, these insights are scattered across different studies for generalized FMs. Second, this field lacks close attention to the developmental lines of PEFT across various FMs and a more intuitive and unified illustration. Thus, a well-structured and comprehensive survey has become increasingly necessary.\nTherefore, we first review the trends in FM development and the categorization of PEFT (Section II). Subsequently, we delve into the design of PEFT across five model structures (Section III), including Selective PEFT, Additive PEFT, Prompt PEFT, Reparameterization PEFT, and Hybrid PEFT, with corresponding feature summary in Table II. We also explore the applications of PEFT in different downstream tasks and their corresponding scenarios (Section IV for LLMs, Section V for VFMs, and Section VI for MFMs). Finally, we provide observations on current research trends and potential future research directions (Section VII) to aid in the community development of PEFT across various domains. Through this survey, we provide a deeper understanding of the integration between a wide range of FMs and systematic PEFT methods."}, {"title": "II. BACKGROUND", "content": "FMs are primarily pre-trained on large-scale datasets and can be fine-tuned to adapt to various downstream tasks. Following the differences in their input modalities and functions, we roughly categorized them into five groups."}, {"title": "III. METHODOLOGY", "content": "This section will describe several important categories of PEFT methods, encompassing the PEFT taxonomy in LLM, VFM, VLM, MFM, and VGM. We will also analyze the pros and cons of each category for a more in-depth understanding."}, {"title": "A. Selective PEFT", "content": "Methods for this category refer to either selectively fine-tuning a subset of the original model's parameters while keeping the rest frozen, or introducing a minimal number of additional parameters to train, without altering the original parameters, as shown in Table III."}, {"title": "A.1 Selective PEFT in Basics", "content": "1) Specific Selection: Methods of this type aim to select specific layers or neurons for fine-tuning. Commonly used methods include Freeze Layers [53], BitFit [71], and PASTA [72].\nFreeze Layers-based methods only fine-tune the last few layers of FMs inspired by this work [118]. BitFit [71] proposed an even simpler fine-tuning approach by only adjusting part of the bias terms of a model or adjusting all the bias terms of the model. We present the core formula (1) for BitFit from the cited paper. Taking the BERT model as an example, the BERT encoder consists of L layers. Each layer begins with M self-attention heads, where a self-attention head (m, l) comprises query (Q), key (K), and value (V) encoders, each implemented as a linear layer. Here, x represents the output of the previous encoder layer (for the first encoder layer, x corresponds to the output of the embedding layer). The subject of blue vectors is the bias terms.\n$Q^{m,l}(x) = W^{m,l}x + b^{m,l},\\$\n$K^{m,l}(x) = W^{m,l}x + b^{m,l},\\$\n$V^{m,l}(x) = W^{m,l}x + b^{m,l}.$\nPASTA [72] updates only special tokens (e.g., [SEP] and [CLS]), achieving performance similar to full fine-tuning in natural language understanding tasks while training just 0. 029% of the total parameters. In particular, PASTA [72] with ROBERTa performed similarly to BitFit [71] but with significantly fewer trainable parameters, demonstrating its efficiency. Moreover, on the CoNLL2003 [119] for the Named Entity Recognition task, PASTA [72] with ROBERTa achieved an impressive F1 score of 90.8%, outperforming P-tuning v2 [37] by 0.6% with 20 times fewer trainable parameters, although it trailed slightly behind full fine-tuning by 2.0%.\n2) Automatic Selection: Methods of this type aim to utilize various algorithms to determine which parameters to train automatically, such as Masking [68], Diff-Pruning [55], FISH [70, 73], AutoFreeze Layers [54], and CHILD-TUNING [55]. Compared to specific selection, enabling FMs to decide which parameters to train would be a more sensible and flexible approach.\nInspired by weight agnostic neural networks [120], Mask-ing-based method [68] utilizes the straight-through estimator to train binary masks which are then employed to mask FM's parameters selectively. Zhao et al. [68] show that end-to-end learning of these selective masks, applied both to FMs and a randomly initialized classifier layer, consistently yields excellent performance. It offers a significant advantage in terms of memory footprint, especially when dealing with multiple tasks that need to be inferred simultaneously. Similarly, Diff-Pruning [55] learns a specific binary task to explore how FMs can be effectively employed for multitasking in environments with limited storage resources. This method leverages the Diff-vector approach to fine-tune the initial pre-trained parameters which will be separated into two parts, the fixed and the tunable. This Diff-vector undergoes adaptive pruning through L0-norm [121] regularization to encourage sparsity. However, utilizing Diff-Pruning requires more memories to store the binary mask. FISH-based methods [70, 73] contend that the parameters impacting the final model output constitute only a subset of all parameters. To identify this subset, they create the FISH (Fisher-Induced Sparse uncHanging) Mask, and select top-k parameters with the highest Fisher information as the crucial update parameter. FISH Mask remains fixed parameters over multiple iterations, effectively designating a specific subset for modification. Only activated neurons undergo updates during training, while other neurons are masked out.\nAutoFreeze Layers [54] utilizes two main modules to accelerate fine-tuning of the model and maintain accuracy: the freezing module and the caching module. The freezing module incorporates a decision engine plug-in to determine which layers should be frozen during the training process, employing a gradient-norm test algorithm. The caching module aims to store intermediate output results for each layer. In contrast, CHILD-TUNING [55] identifies a child network in the parameter matrix based on a certain strategy and generates a corresponding mask matrix. After computing the gradients, only the parameters corresponding to the child network are updated based on the mask, while the other parameters remain unchanged. The formula from the cited paper [55] is:\n$W_{t+1} = W_t-\\eta \\cdot \\frac{\\partial L(w_t)}{\\partial w_t} M_t$\nwhere t represents the t-th iteration, w is the parameter, L denotes the loss, \\eta is the learning rate, and $M_t$ signifies the mask matrix. Within $M_t$, 1 indicates that the parameter belongs to the child network, while a value of 0 means it does not belong to this network."}, {"title": "A.2 Selective PEFT in More FMs", "content": "Linear Probe [44] presents CLIP that jointly trains a text encoder and an image encoder enabling zero-shot prediction at test time. FC-CLIP [74] uses a shared frozen convolutional CLIP backbone to build a single-stage system for open-vocabulary segmentation and consists of three main components (class-agnostic mask generator, in-vocabulary classifier, and out-of-vocabulary classifier). Specifically, the classification score can be described as follows:\n$\\hat{c}_i(j) =\\begin{cases}\n(\\hat{c}_{i,in}(j))^{(1-\\alpha)}\\cdot(\\hat{c}_{i,out}(j))^{\\alpha}, if j \\in C_{train}\\\\\n(\\hat{c}_{i,in}(j))^{(1-\\beta)}\\cdot(\\hat{c}_{i,out}(j))^{\\beta}, otherwise\n\\end{cases}$\nhere, $\\hat{c}_i(j)$ represents the j-th element of $\\hat{c}_i$, with \u201cin\u201d and \u201cout\u201d denoting classifiers for in-vocabulary and out-of-vocabulary, respectively. The parameters $\\alpha$ and $\\beta$, falling within the range [0, 1], control the predictions' balance between the in-vocabulary and out-of-vocabulary classifiers for known and newly unseen categories. Tune-A-Video [75] presents a text-video pair tuning and proposes a tailored spatiotemporal attention mechanism for text-to-video generations.\nV* = D(DDIM-samp(DDIM-inv(E(V)), T*)),\nwhere V is a source video, T* is an edited prompt, and V* is an output video. During inference, Tune-A-Video leverages DDIM (Denoising Diffusion Implicit Model) inversion to provide structure guidance for sampling from input video V. LayerNorm Tuning [76] only adjusts the weights of the normalization layers within an attention block and demonstrates significant reductions in GPU memory usage and trainable parameters."}, {"title": "A.3 Pros and Cons", "content": "Here, we analyze the pros and cons of selective PEFT. A standout advantage of this category is that it refrains from adding new parameters, which has a dual benefit. First, it controls the model's complexity by keeping the parameter count in check, preserving the model's manageability. Second, it ensures that the inference time for downstream tasks does not inflate, thus aiding in maintaining model efficiency. Nevertheless, some shortcomings also should be noted.\n\u2022 Memory risk. Some techniques within this category (like FISH, and CHILD-TUNING), involve the integration of a masking matrix, which results in a spike in memory usage, which could be a challenge in memory-constrained scenarios.\n\u2022 Extra time costs. Some methods might lead to longer training periods due to a special selection mechanism (like Diff-Pruning). This could potentially offset the benefits of having fewer trainable parameters."}, {"title": "B. Additive PEFT", "content": "As shown in Fig. 3, the core idea behind adapters is to learn a set of parameters that can transform the output of one layer into the input of the next layer in a given task-specific way. Adapters are small parameter sets that can be inserted between the layers of FMs. They allow the network to be fine-tuned for a new task without modifying its original parameters."}, {"title": "B.1 Additive PEFT in Basics", "content": "For this group, three key types are included: Bottleneck Adapter, Multi-Adapter, and Adapter Sparsity, as shown in Table IV.\n1) Bottleneck Adapter: Method of this type [56] is proposed in the NLP field inspired by Residual Adapter [122] and ResNet [123] in cross-domain image classification task. This work [56] demonstrates the feasibility of using adapters for parameter-efficient transfer learning on classic NLP tasks. The adapter layer has a simple structure: it is down-projected to a smaller dimension, passed through a non-linear activation function, and then up-projected to the original dimension like a bottleneck. In addition, there is a residual connection between the input and output of the entire adapter layer. However, due to the additional parameters introduced by adapters, the model's inference speed has slowed, leading to various pruning operations on adapters. Further, how to make adapters lighter without sacrificing their performance has become a hot research direction.\n2) Multi-Adapter: Methods of this type refer to the addition of more adapter modules to the model to enhance its transferability. These methods are proposed as a specialized knowledge plug-in to integrate the knowledge of various tasks without forgetting the knowledge from previous tasks and improve the performance of the Bottleneck Adapter [56]. Multi-Adapter mainly includes Adapter Fusion [57], AdaMix [78], MAD-X [77], and BAD-X [124].\nAdapter Fusion [57] combines the knowledge from multiple tasks by fusing the parameters of their respective adapters. This multi-task learning framework consists of two stages. First, a set of new adapter parameters are learned for each task. Then, a fusion module is learned for a specific target task to combine all adapters learned from the first stage. It is worth noting that this method does not require any changes to the structure or parameters of the adapters, but rather combines multiple adapters through simple addition, making it a non-destructive approach. AdaMix [78] does a similar thing as Adapter Fusion [57], by reconstructing the structure of the adapter. Given the output of the expert $E_i$:\n$E_i (x) = w_{out} \\text{GeLU} (w_{in}x_s)$,\nwhere $x_s$ is the input token representation at the s-th position for the MoE (Mixture-of-Expert) layer, consisting of N expert Feed-Forward Network (FFN, $E_i$), $w_{in}$ and $w_{out}$ denote the input and output projection matrices for the i-th expert. The Softmax function is GeLU [125]. Using a gating network, the output of MoE is given by:\nh(x) = \\sum G(x_s)_i E_i(x_s),\nwhere $G(x_s)_i$ is the i-th logit of the output of G(xs), which denotes the probability of selecting expert Ei. Subsequently, Wang et al. [78] replace the gating unit with a random average selection methodology about experts serves not only to mitigate the computational load and the number of parameters necessitated by the gating unit but also to avert the risk of overload for any individual expert. However, AdaMix demands more memory resources during the training process.\nMAD-X (Multiple ADapters for Cross-lingual transfer) framework [77] comprises three types of adapters: invertible, language, and task adapters. Invertible adapters are modules added on top of the embedding layer. The inverse comes before the output embedding layer. This setup helps tackle vocabulary mismatches between multilingual and target languages. Language adapters are trained using masked language modeling on unlabeled data for a specific language. This training encourages the adapters to learn transformations that enhance the pre-trained multilingual model's suitability for that particular language. Task adapters are designed to learn specific tasks. When updating the parameters of the task adapter, the language adapter and inverse adapter are frozen. MAD-X is particularly useful for adapting to languages not covered by the multilingual model's training model and achieving competitive performance on high-resource languages. BAD-X [124] advocates for a more effective cross-lingual transfer by directly adapting the model to the specific source-target language pair, rather than separately training the source language and target language through a monolingual adapter. In this approach, a bilingual language pair adapter is learned, optimizing the adaptation process and potentially improving cross-lingual performance. Even though Multi-Adapter enhances transferability, this implementation introduces more parameters.\n3) Adapter Sparsity: Methods of this type are proposed to make full use of the parameter efficiency according to the internal structure of the adapter. Like AdapterDrop [126], LST [58], and Convpass [59].\nAdapterDrop [126] aims to reduce computation and memory requirements and simplify the model. They achieve this by randomly dropping adapters during training, which encourages the model to learn to rely on the original transformer layers in addition to the adapters. This can result in faster and more efficient training. AdapterDrop proposed two training approaches: (1) specialized adapter dropout, wherein during training, a fixed value n is maintained, and the model retains only the top n layers during inference. (2) robust AdapterDrop that randomly draws n from [0, 11] for training. Across multiple tasks in the GLUE benchmark, both variants of AdapterDrop demonstrate minimal performance degradation when dropping below n=5 during inference, whereas traditional adapters experience a rapid decline in performance beyond n=1. Removing the top 5 layers of the adapter results in a 26% increase in training speed. Moreover, the speed of concurrent inference for multiple tasks can be accelerated by 21%-42%. AdapterBias [79] incorporates the idea of BitFit [71] and introduces a token-dependent shift to the hidden output of transformer layers to adapt to downstream NLP tasks with only a vector and a linear layer, and demonstrates its efficiency by employing BERT as the pre-trained language model. Compared to Bottleneck Adapter [56], AdapterBias shows competitiveness despite having 40 times fewer parameters.\nSparseAdapter [127] further checks the additive PEFT from the perspective of network pruning and introduces the concept of Large-Sparse to maintain the same parameter budget. SparseAdapter can achieve comparable, or even better, performance than standard adapters when the sparsity ratio reaches 80%. LST [58], as a variant of adapters, involves training a small transformer network on one side of a pre-trained network, similar to a ladder. This network is connected to the transformer layers of the original model. FMs are utilized solely as feature extractors, while backpropagation is performed within the side network. LST [58] employs various techniques to conserve memory and computational resources during training and enhance fine-tuning performance."}, {"title": "B.2 Additive PEFT in More FMs", "content": "LST [58] has been evaluated on T5 [128] and CLIP-T5 [129] models, revealing that when fine-tuning the entire network, LST reduces memory costs by 69%, whereas other methods achieve only a 26% reduction under similar parameter usage. Convpass [59] introduces convolutional bypasses in ViTs as vision transformer adapters by introducing less than 0.5% trainable parameters for adapting vision models. AdaptFormer [80] introduces a lightweight module with less than 2% of the parameters of the ViT to boost recognition performance. ViT-Adapter [81] enhances the intrinsic representational capabilities of a standard ViT backbone with an adapter that integrates image-specific inductive biases during the fine-tuning process. SAN [82] separates mask proposal generation and class recognition tasks to achieve open-vocabulary semantic segmentation. By appending a lightweight side network to a fixed CLIP model, mask proposals and attention bias are predicted to direct CLIP in recognizing the class of the mask. CSN (DTL) [84] disentangles the weight updates from the backbone using a compact side network to identify the object. T2I-Adapter [87] learns lightweight adapter modes A to improve the performance of text-to-image models without the inherent framework of updating text-to-image models M. Given text prompt t, control signal $x_c$, and weighting factor w, T2I-Adapter can generate image x:\nx = M(t) + w . A(xc).\nIP-Adapter [85] uses an image prompt and introduces a cross-attention mechanism to learn image embeddings effectively. Given conditional model $e_0(x_t, c, t)$ and unconditional model $e_0(x, t)$, the predicted noise is:\n$e_0 (x_t, C, t) = WE(x_t, C, t) + (1 \u2212 w)e(x_t, t)$,\nw is the guidance scale or weight that adjusts the alignment with condition c. I2V-adapter [88] needs to fine-tune only 1% of the parameters of the base diffusion model. ControlNet [86] adds spatially localized conditions. Subsequently, ControlNeXt [89] introduces a lightweight conditional control module that further reduces learnable parameters to less than 10% of ControlNet, extending the scope to video generation and super-resolution. LLaMA-Adapter V2 [90] efficiently enhances LLaMA-Adapter [130] by unlocking more learnable parameters. And CLIP-Adapter [92] and Tip-Adapter [91], etc. suggest inserting trainable adapters to perform VLM fine-tuning into the fixed CLIP model."}, {"title": "B.3 Pros and Cons", "content": "Here, we analyze the pros and cons of additive PEFT. This category integrates task-specific parameters into the model by adding lightweight adapter layers to each layer and does not change most of the weights of FMs, thus preserving the integrity of the pre-trained knowledge. This makes the adapter model more generic and allows it to leverage rich knowledge to adapt to different tasks without having to retrain the entire model from scratch for each new task. This is especially valuable for rapid deployment and transfer learning scenarios. Nevertheless, some shortcomings also should be noted.\n\u2022 Inference overhead. This category may cause an increase in inference overhead due to the additional computation required by the adapter layer.\n\u2022 Prudent configurations. This category of methods may require careful initialization and training strategies, such as optimal settings of adapter dimensions and sparsity rates."}, {"title": "C. Prompt PEFT", "content": "Prompt Tuning is nearly the most common PEFT method in FMs on some specific tasks, as shown in Fig. 4. This category involves incorporating a carefully designed prompt into the input or the transformer's layers, aiming to align the input distribution with the original training data and guide the model toward generating the desired output."}, {"title": "C.1 Prompt PEFT in Basics", "content": "Three types are discussed here: Hard Prompt, AutoPrompt, and Soft Prompt.\n1) Hard Prompt: This type of approach means the initial form of the prompt involves manually specifying a template and concatenating it with the input to generate the desired output, without modifying the original model parameters.\nPET [131] proposed pattern-exploiting training, a semi-supervised technique that reformulates input examples into cloze-like sentences. For example, in a task to determine whether two sentences a and b agree or disagree, a pattern like P(a,b) = a?__, b is used. PET predicts the correct label by filling in the blank. Null Prompts [94] uses a general template like \"input + [MASK]\" to simplify the process of designing prompts for downstream tasks and reduce the time spent on prompt engineering and enhances memory efficiency.\nAlthough hard prompts are sometimes efficient, they have notable limitations: (1) selecting effective templates often requires significant human effort, making the process time-consuming and labor-intensive. (2) the generalization ability of hard prompts may suffer when the model encounters new or unfamiliar tasks, which may require further adjustment and pattern learning.\n2) AutoPrompt: This category proposes an automated prompt search method [132] that uses exploratory search to automatically generate prompts to address the challenges of manual prompt design of Hard Prompt. Although these automatically generated templates may not follow natural language conventions, the terms within them are still understandable, as they are selected from the model's vocabulary. However, the generated templates may not always represent the optimal solution."}, {"title": "3) Soft Prompt", "content": "This category has further expanded the scope beyond human-understandable words found in a vocabulary. These prompts are called continuous or soft prompts. In this advanced progression, the generation process changes from discrete, human-driven to continuous, machine-driven. Representative methods include Prefix Tuning [60], Prompt Tuning [93], P-Tuning [61, 37], PPT [62], and so on.\nPrefix Tuning [60] freezes the parameters of FMs and optimizes only a task-specific continuous vector known as the prefix, which functions as a differentiable virtual Token. An MLP is introduced before the prefix layer to enhance stability during training and prevent performance degradation and only the prefix parameters are retained. Despite tuning only around 0.1% of the model's parameters, prefix tuning achieved comparable performance to full fine-tuning on E2E [133], WebNLG [134], and DART [135] on GPT-2 and BART [136]. Prompt Tuning [93] is a simplified version of prefix tuning and defines task-specific prompts, which are appended to the input data. Unlike prefix tuning, prompt tuning only adds prompt tokens to the input layer. Lester et al. [93] also introduced Prompt Ensembling, where multiple prompts for the same task are trained simultaneously within one batch, mimicking model ensembling but at a lower cost. They further investigated how prompt initialization techniques and prompt length impact performance. Their ablation studies show that initializing prompts with class labels is better than other methods like random or vocabulary-based initialization. However, this advantage diminishes as the model size increases. Regarding prompt length, the optimal performance is achieved with about 20 tokens, beyond which no significant improvements are observed. However, this trend also weakens with larger models.\nCompared to Prefix Tuning [60], P-Tuning [61] applies differentiable virtual tokens only at the input layer, rather than across all layers, and allows flexible token insertion rather than restricting them to a prefix position. Specifically, P-Tuning transforms prompts into a learnable embedding layer, processing them through an MLP and LSTM structure [137]. This approach replaces traditional hand-crafted tokens with learnable virtual tokens. Prompt Tuning [93] and P-Tuning only apply prompts to the first transformer layer, resulting in shallow tuning and restricted optimization, especially when applied to smaller models and hard sequence tagging tasks. P-Tuning v2 [37] then extends prompt tokens to each layer of the model, enhancing scalability and universality across various natural language understanding tasks. By incorporating prompts at every layer, P-Tuning v2 increases the number of learnable parameters, from 0.01% in P-Tuning and Prompt Tuning to 0.1%-3%, while maintaining parameter efficiency. This deeper integration improves model predictions and can be seen as an adaptation and enhancement of prefix tuning.\nDART [138] treats the generation of the prompt as a differentiable function, enabling the model to learn the best way to generate a prompt for a particular task and allowing for gradient-based optimization of the prompt generation. It is similar to P-Tuning [61] but with some differences in the details, such as using continuous labels, and incorporating a template mask objective instead of an LSTM in P-Tuning. y-Tuning [139] fine-tunes the label extractor's parameters and uses cross attention to combine the loss features from both FMs and the label extractor to avoid adjusting the input text attributes or the parameters of FMs.\nPPT [62] propose Pre-trained Prompt Tuning, where soft prompts are pre-trained on a large-scale, unlabeled corpus through self-supervised tasks. PPT involves two steps: pre-training and fine-tuning. During pre-training, a large dataset is used for self-supervised learning to generate universal prompts. In the fine-tuning stage, generated prompts and a small amount of labeled data are used to fine-tune models on target tasks. PPT excels in few-shot settings, outperforming prompt tuning [93], with significant improvements in LCQMC [140], and comparable results in BoolQ [141]. However, its advantage diminishes with more training data.\nSPOT [95] shares similarities with PPT, using pre-trained prompts to enhance few-shot learning. Instead of manually designing pre-training tasks, SPoT initializes target task prompts using those trained on source tasks. SPOT inserts a pre-training step between LLM pre-training and target task prompt tuning, training one or multiple prompts on source tasks before using them to initialize prompts for the target task. SPOT focuses on full-data scenarios, finding that even with sufficient data, using source task-trained prompts provides significant benefits for the target task. Prompt Transfer [142] involves reusing trained soft prompts for zero-shot inference on new tasks or datasets, or for continued training, showing that soft prompts are effective for similar tasks under the same FMs. Transferability across models is achieved using a cross-model projector. In addition, using these prompts as a starting point for new tasks reduces training time and enhances performance. Performance metrics, particularly the overlapping rate of activated neurons, suggest that prompts stimulate the inherent abilities of FMs."}, {"title": "C.2 Prompt PEFT in More FMs", "content": "VP [96] adapts FMs to new tasks by adding prompts in the form of pixels to the image's pixel space, such as padding pixels along the image edges, without altering the model's parameters. VPT [97] then introduces some learnable parameters in the input space that are less than 1% of the original model parameters. DAM-VP [98] enhances the performance of pre-trained models on downstream tasks with high diversity and large datasets by adaptively selecting and optimizing visual prompts for different subsets of images. Given input image xp and ground truth y, the the cross-entropy loss on a dataset DT that includes prompts Pk is:\n$P_{1....,PN}^* = arg\\underset{P_1,...,P_N}{min} \\frac{1}{N} \\sum\\limits_{i=1} \\sum\\limits_{x\\in D_i} L_{CE}(M(x+P_i), y)$.\nILM-VP [99] advances visual prompting in transfer learning by introducing an iterative label mapping-based framework that significantly improves the precision of the target task and outperforms existing methods. EVP [100] significantly improves classification accuracy on various datasets to 82.5% by treating prompts as learnable entities and applying input diversity with gradient normalization, surpassing previous records. LION [101] is a lightweight and effective vision prompt tuning method that leverages implicit equilibrium layers to adapt pre-trained models to downstream tasks with minimal computational cost. Textual Inversion [102] found a way to describe novel concepts in the text encoder of CLIP to fine-tune the diffusion model (using less than 20k parameters) to generate content in a specialized style. CoOp [103] models the context words of the prompt with learnable vectors for implementing PEFT to identify or detect objects. OVSeg [104] incorporates masked and colorful prompts to improve the fine-tuning performance of VFMs significantly. Q-Former [105] bridges the modal gap using a lightweight projection that greatly reduces trainable parameters."}, {"title": "C.3 Pros and Cons", "content": "Here, we analyze the pros and cons of prompt PEFT. This category of PEFT adjusts the corresponding learnable prompt vectors (like text prompt and visual prompt) while maintaining consistent architecture, greatly improving the flexibility and versatility of the model. Second, since the base model parameters remain fixed, it helps preserve knowledge across tasks, reducing forgetting in multi-task scenarios. Nevertheless, some shortcomings also should be noted.\n\u2022 Poor Transferability. Some prompts trained for specific a task cannot be directly transferred to other tasks. Because the prompt vectors for each task are optimized based on the data and features of that task, they have strong task-specific characteristics and are not easily generalized across different tasks.\n\u2022 Model Dependency. This"}]}