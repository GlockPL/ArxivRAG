{"title": "Effective Guidance for Model Attention with Simple Yes-no Annotations", "authors": ["Seongmin Lee", "Ali Payani", "Duen Horng (Polo) Chau"], "abstract": "Modern deep learning models often make predictions by focusing on irrelevant areas, leading to biased performance and limited generalization. Existing methods aimed at rectifying model attention require explicit labels for irrelevant areas or complex pixel-wise ground truth attention maps. We present CRAYON (Correcting Reasoning with Annotations of Yes Or No), offering effective, scalable, and practical solutions to rectify model attention using simple yes-no annotations. CRAYON empowers classical and modern model interpretation techniques to identify and guide model reasoning: CRAYON-ATTENTION directs classic interpretations based on saliency maps to focus on relevant image regions, while CRAYON-PRUNING removes irrelevant neurons identified by modern concept-based methods to mitigate their influence. Through extensive experiments with both quantitative and human evaluation, we showcase CRAYON\u2019s effectiveness, scalability, and practicality in refining model attention. CRAYON achieves state-of-the-art performance, outperforming 12 methods across 3 benchmark datasets, surpassing approaches that require more complex annotations.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning models have achieved remarkable performance, even surpassing humans in tasks such as image classification [1]. However, recent advancements in deep learning interpretation have discovered that these models often make predictions focusing on irrelevant areas [2], [3]. For example, a model trained to classify waterbirds and landbirds often bases its predictions on backgrounds, not bird bodies, as waterbirds often appear near bodies of water like lakes, while landbirds are commonly found near land features like forests [4]. Such model attention to less relevant areas results in reduced trustworthiness [5], poor generalization [6], [7], and biased performance [8], [9], [10], [11]. Thus, it is crucial to improve these models to attend to pertinent areas [6], [7].\nExisting methods to mitigate irrelevant attentions suffer from major drawbacks. Some techniques attempt to decorrelate class labels from irrelevant areas by explicitly annotating each image\u2019s irrelevant regions. For example, in a bird dataset, images may be annotated with either water or land backgrounds, which are then used to balance the training dataset [12], [13], [14] or reduce correlations between irrelevant background areas and class labels [15], [16], [13]. However, in practice, due to the wide variety of irrelevant areas, categorizing them can be challenging [3], [17]. Other methods directly guide model attention using ground truth attention maps to inform the model where it should focus or avoid focusing, in order to align the model\u2019s saliency maps with these ground truth"}, {"title": "II. RELATED WORK", "content": "A lot of efforts have been dedicated to rectifying the attention of deep learning models. Some researchers have attributed irrelevant attention to spurious correlations in training data [21] and alleviate such issues by reweighting or subsampling training data [12], [13], [22], [23], [24]. However, challenges arise when a training dataset lacks spurious-free data. In response, some researchers opt to create balanced training datasets by collecting or generating additional instances [25], [26], [27], [14], [28]. Yet, these approaches can be costly or impractical in real-world scenarios [29]. Various loss terms [15], [2], [30] and pruning approaches [31] have been introduced to counter the impact of spurious correlations. However, all these methods require annotations for the attributes that are spuriously correlated with the class labels, while identifying the correlated attributes is challenging in practice [3]. Several methods addressing such limitations have shown some efficacy [22], [32], [16], [33].\nTo achieve higher performance while overcoming all the aforementioned limitations, researchers have incorporated humans in refining vision models [34], [35], [36]. Various approaches have extended model interpretation techniques beyond mere identification, aiming to align model attributions with human intuition [37], [38]. Many researchers have focused mainly on saliency maps and collected human annotations for model attention. The RRR loss [18] was proposed to redirect MLP models away from regions annotated by humans as irrelevant, later extending its applicability to deeper models [19], [20]. CDEP [39] and SPIRE [40] aim to reduce the impact of irrelevant pixels by leveraging contextual decomposition and masking specific objects in images, respectively. Stammer et al. [41] refine models at both pixel and concept levels by disentangling concepts within an image. However, all these methods require humans to supply ground truth attention maps for each image, which can be prohibitively costly to obtain. To address this challenge, some progress has been made by using eye-gaze tracking apparatus [42] or introducing simpler alternatives such as scribble maps [43] and bounding boxes [44], which yet do not resolve the inherent limitations of human-provided attention maps [20]. We further purposefully simplify human feedback to yes-no annotations on model interpretation results, overcoming existing methods' limitations while delivering superior performance."}, {"title": "III. METHODS", "content": "CRAYON fine-tunes a trained model to base its predictions on relevant data regions by harnessing simple yes-no annotations, which pertain to the relevance of the rationale behind the model's predictions, as revealed through model interpretations. In this section, we describe (1) how yes-no annotations for classic interpretations based on saliency maps guide the model's attention to the relevant regions we call this CRAYON-ATTENTION (Sec. III-B) and (2) how we extend our idea to modern concept-based interpretations to prune the neurons activated by irrelevant visual concepts we call this CRAYON-PRUNING (Sec. III-C).\nGenerating saliency maps stands as one of the most commonly employed and deeply explored model interpretation techniques [45]. Therefore, we chose to leverage it as a familiar mechanism to collect human annotations in our novel and simple way. For a given model and its training data X1,..., XN, the saliency map Mxn highlights the regions within the image xn that the model focuses on for its prediction. Once saliency maps are generated for all N training data points, we proceed to gather yes-no annotations regarding the relevance of each map to the prediction task. We denote the set of indices corresponding to training data with relevant and irrelevant maps as R and I, respectively.\nTo refine the model using the yes-no annotations, we introduce a loss function based on the energy loss [46]. For the data point xn whose saliency map Mxn highlights the relevant regions (i.e., n \u2208 R), the model should generate similar saliency maps following the refinement. Hence, we formulate the loss function Lrel,n as follows:\n$L_{rel,n} = \\sum_{h=1}^{H} \\sum_{w=1}^{W} [M'_{xn}]_{hw} (1 \u2013 [M_{xn}]_{hw})$\nwhere H and W represent the height and width of the saliency maps, respectively, and M'xn is the saliency map for the model being trained and the data point xn. We clarify that Mx is the saliency map for the original model before refining, and M'xn is for the model being fine-tuned. For better stability of the loss function, we normalize both Mx and M'x, scaling their values between 0 and 1 by dividing each map by its maximum value.\nFor the data point xn with irrelevant saliency map (i.e., n\u2208 I), the model should attend to the regions that are not highlighted in the map Mxm. In this regard, we construct the loss function Lirrel,n as follows:\n$L_{irrel,n} = \\sum_{h=1}^{H} \\sum_{w=1}^{W} [M'_{x_n}]_{hw} [M_{x_n}]_{hw}$\nWhile guiding the model to attend to the right regions, we need to preserve the accuracy of the model's predictions. Therefore, we incorporate the prediction loss Lpred,n for the data point xn:\n$L_{pred,n} = \\sum_{k=1}^{K} -y_{nk} log \\hat{y}_{nk}$\nwhere ynk is 1 if the label of the data xn is k and 0 otherwise and \u0177nk is the probability of the data xn being labeled as k computed by the model being trained."}, {"title": "III. METHODS", "content": "Neurons, also referred to as channels, in the penultimate layer of CNN models are activated by specific high-level visual concepts in the input data [47]. Based on this finding, a model interpretation method that summarizes the concepts responsible for a neuron's activation as a collection of image patches has recently been proposed [48]. These patches are identified by selecting the images that most highly activate the neuron and cropping out the corresponding region. For example, a neuron in the penultimate layer of a smile classifier has patches corresponding to the mouth concept, indicating that the neuron's activation is attributed to the presence of a mouth (Fig. 2, left), while another neuron has patches for hair (Fig. 2, right).\nCRAYON-PRUNING identifies the neurons in the penultimate layer that are activated by irrelevant concepts by presenting the image patches of each neuron and collecting yes-no annotations on their relevance. For instance, for the smile classifier in the previous example, the neuron activated by the mouth is relevant, while the neuron activated by the hair is irrelevant. We prune the irrelevant neurons and fine-tune the last fully-connected layer of the model to remove the effect of the irrelevant concept on the model's prediction. For this fine-tuning process, we use the prediction loss in Equation 3.\nBoth CRAYON-ATTENTION and CRAYON-PRUNING share the same general time complexity of O(NEM), where N is the number of training data points, E is the number of training epochs, and M corresponds to time for loss computation. For Crayon-ATTENTION, the loss computation is based on Grad-CAM (as described in Sec. III-B), whereas for CRAYON-PRUNING, it uses the conventional cross-entropy loss. This analysis indicates that the runtimes of both CRAYON-ATTENTION and CRAYON-PRUNING scale linearly with the number of training data points, as we will empirically demonstrate in our experiments in Sec. IV-G."}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "To demonstrate CRAYON's effectiveness, scalability, and practicality in rectifying model attention, we collect yes-no human annotations for three benchmark datasets from 5,893 participants on Amazon Mechanical Turk (MTurk).\nWaterbirds [4] consists of bird images, where waterbirds and landbirds are more commonly seen in water (e.g., lake) and land (e.g., forest) backgrounds, respectively. Its training set consists of:\nBird classifiers trained on this data tend to classify birds based on the backgrounds rather than their bird features. The test set consists of 1,284 waterbirds and 4,510 landbirds; half of the waterbird images and half of the landbird images have water backgrounds, while the other half have land backgrounds.\nBiased CelebA [49] demonstrates a correlation where most training images of individuals with black hair exhibit smiling expressions, while those with blond hair are mostly not smiling. There are a total of 20,200 training instances:\nThis correlation leads the smile classifier trained on this dataset to make incorrect associations between smile predictions and hair color. The test set contains a total of 8,000 data instances, with 2,000 instances per group.\nBackgrounds Challenge [3] addresses the problem that classifiers trained on the ImageNet [50] dataset often inappropriately base their predictions on image backgrounds, rather than foreground objects. Aiming to correct models to base their predictions on the foreground objects, the challenge introduces the ImageNet-9 (IN-9) dataset, a subset of ImageNet with nine coarse-grained classes (e.g., dog, bird, vehicle). The IN-9 dataset's training and test set consist of 5,045 and 450 images for each class, respectively (45,405 and 4,050 in total).\nFor each dataset, we train a ResNet50 [51] classifier, pretrained with ImageNet [50], with the cross-entropy loss (Equation 3) for 10 epochs, and refer to it as the Original model throughout this paper, indicating no refinement has been applied and thus it may attend to irrelevant areas. We train the Original model for the Waterbirds dataset with Adam"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "We collect yes-no annotations for the relevance of saliency maps and neuron concepts on Amazon MTurk. To collect human annotations for CRAYON-ATTENTION, we present participants with a visualization for each training image, as the example shown in Fig. 3, consisting of:\nThis visualization design was informed by our pilot study, where we learned that relying solely on Highlight in Red, which has conventionally been used [53], could obscure image contents, making it hard for participants to assess relevance. Similarly, identifying model-attended areas solely with Highlight Overlaid is challenging when the attended areas are white in color. We present all three visualization components for Waterbirds and Backgrounds Challenge, while only Highlight Overlaid is presented for Biased CelebA, as most areas in these face images are in skin colors, and many participants have found it sufficient to determine relevance. To maximize quality and efficiency, we present each visualization to two participants [54] and ask each participant a single yes-no question:"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "We outline the hyperparameter configurations for CRAYON used in our experiments. For the Waterbirds dataset, CRAYON-ATTENTION fine-tunes the Original bird classifier for 10 epochs with a batch size of 128, using the Adam optimizer [52] with a learning rate of 5e-5 and a weight decay of le-4. We set the hyperparameters a and \u1e9e in Equation 4 set to le+7 and 2e+5, respectively. For Biased CelebA, the Original smile classifier is fine-tuned for 10 epochs with a batch size of 64, using the Adam optimizer with a learning rate of le-5 and a weight decay of le-4. We set the hyperparameters a to 5e+7 and \u1e9e to le+6. For Backgrounds Challenge, we fine-tune the classifier for 10 epochs with a batch size of 256 using the SGD optimizer [56] with a learning rate of 5e-6 and a weight decay of le-1. The hyperparameters a and \u1e9e set to 5e+3 and 5e+2, respectively.\nFor CRAYON-PRUNING, we prune 1,034 irrelevant neurons from the penultimate layer of the Original model trained on the Waterbirds dataset, and then fine-tunes the last fully connected layer for 10 epochs with a learning rate of 5e-5. For the Biased CelebA dataset, we prune 1,871 irrelevant neurons and train"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "We also evaluate CRAYON-PRUNING+ATTENTION, which prunes irrelevant neurons in the penultimate layer and then fine-tunes the entire model with attention guidance. For Waterbirds, we set a to le+7 and 8 to 2e+5, while for Biased CelebA, we set a and \u1e9e to 5e+7 and le+6, respectively. For Backgrounds Challenge, we set a to le+3 and 3 to 5e+1. For the Waterbirds and Biased CelebA datasets, we use the Adam optimizer with the same learning rate, weight decay, and number of epochs as with CRAYON-ATTENTION, while we use the SGD optimizer with a learning rate of 5e-5 and weight decay to le-1 for the Backgrounds Challenge dataset. We select these values base on the performance of training data after testing with a wide range of hyperparameter values (details in Sec. IV-K).\nTo demonstrate the importance of human annotations, we compare CRAYON with some of the most recent, best-performing methods aimed at rectifying model attention without human annotations:"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "We also assess five methods that require complex human annotations by using the ground truth maps provided in the dataset. We note that this comparison poses CRAYON disadvantages since it uses simple human-provided annotations, not complex ground truth. For Biased CelebA, which lacks segmentation maps, we generate maps covering the eyes and mouth, relevant to smiling, as the images in this dataset are aligned with respect to the eyes' locations [60]. Bounding boxes are generated\nto enclose these maps. It is important to note that we could not collect human annotations for the existing methods because they either require proprietary apparatus [20], [19] or are not evaluated with human annotators [18], [44]. The methods are:\nTo ensure attention correction is not solely due to extended training, we examine the naive empirical risk minimization (ERM) [4] approach that simply minimizes classification loss by training the model for more epochs. Hyperparameter values for all compared methods are determined through a comprehensive systematic search around the values reported in their papers. We use the values that yield the highest performance. We report each method's training configurations in our repository.\nAs the Waterbirds and Biased CelebA provide the labels for irrelevant areas, we employ worst and mean group accuracy as the evaluation metrics in accordance with the practice in literature [4]. Specifically, we first evaluate the model accuracy for each group created by intersecting irrelevant area labels and class labels; for example, Waterbirds consists of four groups: waterbirds on water backgrounds, waterbirds on land backgrounds, landbirds on water backgrounds, and landbirds on land backgrounds. We then compute the minimum and mean accuracy values across these groups and denote them as worst group accuracy (WGA) and mean group accuracy (MGA), respectively.\nFor Backgrounds Challenge, we employ the metrics proposed by the challenge itself [3], as it is difficult to categorize its image backgrounds to form image groups. These metrics are based on two datasets, Mixed-Same and Mixed-Rand, which are created by transforming image backgrounds. The Mixed-Same dataset shuffles backgrounds across the images"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "CRAYON achieves state-of-the-art performance in correcting model attention, outperforming 12 existing methods across three benchmark datasets, including those that require more complex annotations. We run each method five times with different random seeds and report the average and standard deviation of the performance values in Table I.\nFor Waterbirds, compared to the unrefined original model (Row 1), CRAYON-ATTENTION (Row 2) substantially enhances WGA by 43.96 percentage points (pp), raising it from 28.35% to 72.31%; and MGA by 13.15pp, from 72.08% to 85.23%. CRAYON-PRUNING (Row 3) also demonstrates improvements, achieving a 40.62pp increase in WGA and an 11.05pp increase in MGA. CRAYON-PRUNING+ATTENTION (Row 4), which combines both attention and pruning approaches, further elevates both WGA and MGA beyond the capabilities of each individual approach, raising WGA to 76.04% and MGA to 86.03%.\nCRAYON is also effective for Biased CelebA, significantly outperforming all 12 methods. which is expected for the"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "are unclear [16]. Specifically, the underperformance of JtT and CnC is attributed to the small number (only 2%) of training data points misclassified by the original model, while MaskTune\u2019s limited performance suggests its potential reliance on large training data, as higher performance was reported when four times the training data points were used [32].\nIt is noteworthy that CRAYON-ATTENTION surpasses all methods using more complex annotations, which often contain richer information. We attribute CRAYON\u2019s superior performance to its ability to overcome the shortcomings of binary ground truth maps and boxes, which are represented as 0s and 1s, while model-generated saliency maps consist of continuous real numbers. This inconsistency degrades the performance of model attention guidance [20]. Additionally, experiments with the Biased CelebA dataset reveal that the shape constraint of bounding boxes significantly degrades performance. CRAYON addresses this challenge by using the saliency maps of the original model instead of binary ground truth.\nWe run all CRAYON algorithms on an NVIDIA A6000 GPU with 40GB RAM. For the Waterbirds dataset with 4,795 training data points, CRAYON-ATTENTION takes 288 seconds on average, CRAYON-PRUNING takes 111 seconds, and CRAYON-PRUNING+ATTENTION takes 295 seconds. For the Biased CelebA dataset with 20,200 training data points, CRAYON-ATTENTION takes 1368 seconds, CRAYON-PRUNING takes 1241 seconds, and CRAYON-PRUNING+ATTENTION takes 1401 seconds. For the Backgrounds Challenge dataset, which consists of 45,405 training data points, CRAYON-ATTENTION takes 4512 seconds, CRAYON-PRUNING takes 4118 seconds, and CRAYON-PRUNING+ATTENTION takes 4540 seconds. Overall, CRAYON-PRUNING takes much less time as it fine-tunes only the last fully connected layer. Additionally, these runtimes"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "We evaluate how the number of annotations n affects CRAYON\u2019s performance. For CRAYON-ATTENTION, we randomly sample n images from the training set of the Waterbirds dataset and compute both Lrel and Lirrel for these n images along with their annotations. For CRAYON-PRUNING, we randomly sample n neurons from the penultimate layer and prune the irrelevant neurons within this sampled group of n. We additionally investigate CRAYON-PRUNING+ATTENTION, where we use all 2,048 annotations for neuron relevance while varying the number of annotations for model attention.\nWe conduct an ablation study to assess the impact of the two proposed loss terms, Lrel and Lirrel, on CRAYON-ATTENTION\u2019s performance. We deactivate one of the two"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "Although attention annotations have been collected with respect to the saliency maps from the Original model, CRAYON-ATTENTION can effectively exploit these annotated saliency maps to refine other models, as these annotated maps serve as indicators of where a model should or should not attend. To assess the transferability of attention annotations, for each dataset, we train five ConvNeXt [61] models, each initialized with a different random seed value. We then refine these models using CRAYON-ATTENTION with the saliency maps from the Original model and their relevance annotations. In other words, we train the ConvNeXt models using the saliency maps from the Original model as Mx in Equation 1 and 2, and their relevance annotations to decide R and I in Equation 4. We note that M in Equation 1 and 2 is the saliency map for the ConvNeXt models being trained. We run"}, {"title": "IV. EVALUATION WITH HUMAN ANNOTATIONS", "content": "We examine how CRAYON's two hyperparameters, \u03b1 and B, affect the performance of CRAYON-ATTENTION and CRAYON-PRUNING+ATTENTION. We evaluate the performance of the two methods on the Waterbirds dataset with different values of a over the wide range from le+5 to le+9 while consistently using \u1e9e of 2e+5 as in Sec. IV-C. Similarly, we vary \u1e9e from le+3 to le+7, while keeping a as le+7. Fig. 5 visualizes the results.\nWe observe that the performance of CRAYON-ATTENTION stabilizes for various values of a from 5e+6 to 5e+7. The performance is even more stable with different values of B ranging from 2e+3 to 5e+5. The performance of CRAYON-PRUNING+ATTENTION is relatively more sensitive to these"}, {"title": "V. REPRODUCIBILITY", "content": "Our research is fully reproducible. We have carefully prepared and described all necessary information for interested researchers to reproduce and extend our work. Our work has met all the criteria outlined in the required reproducibility checklist. Below, we summarize how we have ensured reproducibility. We reference specific sections and subsections of this paper, and specific files in our repository, to help readers easily locate the information they need for reproducibility.\nWe have clearly described the settings of all presented algorithms (Sec. IV-C; misc/training_config.md in repository) and all models used (Sec. IV-A, Sec. IV-J). Additionally, we have analyzed their time and space complexity (Sec. III-D) and how they scale with increasing sample sizes (Sec. IV-C).\nFor all three datasets used in our paper, we have described essential information, including the number of data points in training and test datasets, their distributions, and image preprocessing steps (Sec. IV-A). We have also detailed the process of collecting the yes-no annotations for CRAYON by describing the visualizations and questions presented to the annotators and how we integrated the collected data into high-quality annotations (Sec. IV-B). In our repository, we provide the links to download the three datasets and yes-no annotations (README.md).\nTo facilitate easy reproduction, we have included specifications of dependencies (environment.yml), training and evaluation code (src/solver), a link to download pretrained models to run the code, and a README file with a table of experiment results and comprehensive guidelines on how to reproduce the results (README.md)."}, {"title": "VI. CONCLUSION", "content": "We introduce CRAYON, the state-of-the-art approach for correcting model attention using simple yes-no annotations annotations. Extensive quantitative evaluation with large-scale human annotations demonstrates CRAYON's effectiveness, scalability, and practicality. It outperforms all existing methods across three benchmark datasets, surpassing approaches that rely on more complex annotations."}]}