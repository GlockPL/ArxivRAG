{"title": "PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation", "authors": ["Kaidong Zhang", "Pengzhen Ren", "Bingqian Lin", "Junfan Lin", "Shikui Ma", "Hang Xu", "Xiaodan Liang"], "abstract": "Language-guided robotic manipulation is a challenging task that requires an em-\nbodied agent to follow abstract user instructions to accomplish various complex\nmanipulation tasks. Previous work generally maps instructions and visual percep-\ntions directly to low-level executable actions, neglecting the modeling of critical\nwaypoints (e.g., key states of \"close to/grab/move up\" in action trajectories) in\nmanipulation tasks. Trivially fitting the data without revealing the relation between\ninstruction and low-level executable actions, these models are prone to memorizing\nthe surficial pattern of the data instead of acquiring the transferable knowledge,\nand thus are fragile to dynamic environment changes. To address this issue, we\npropose a PrImitive-driVen waypOinT-aware world model for Robotic manipula-\ntion (PIVOT-R) that focuses solely on the prediction of task-relevant waypoints.\nSpecifically, PIVOT-R consists of a Waypoint-aware World Model (WAWM) and\na lightweight action prediction module. The former performs primitive action\nparsing and primitive-driven waypoint prediction, while the latter focuses on decod-\ning low-level actions. Additionally, we also design an asynchronous hierarchical\nexecutor (AHE) for PIVOT-R, which can use different execution frequencies for\ndifferent modules of the model, thereby helping the model reduce computational\nredundancy and improve model execution efficiency. Our PIVOT-R outperforms\nstate-of-the-art (SoTA) open-source models on the SeaWave benchmark, achiev-\ning an average relative improvement of 19.45% across four levels of instruction\ntasks. Moreover, compared to the synchronously executed PIVOT-R, the execution\nefficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop\nin performance. These results provide compelling evidence that our PIVOT-R can\nsignificantly improve both the performance and efficiency of robotic manipulation.", "sections": [{"title": "1 Introduction", "content": "Language-guided robotic manipulation [22, 33, 61, 50, 12, 38] is a key research problem of Embodied\nAI. This field aims to enable agents to follow abstract language instructions for performing various\nmanipulation tasks. To complete the tasks, the agent needs to transform high-level language instruc-\ntions into low-level actions as well as capturing environmental dynamics for precise manipulation\ndecision-making.\nWitnessed the immense success of vision-language foundation models (VLMs) [2, 40, 37], many\nworks have explored the utilization of VLMs for facilitating language-guided robotic manipulation in"}, {"title": "2 Related Work", "content": "Language-Guided Robotic Manipulation. Robotic Manipulation is a long-standing research field in\nEmbodied Artificial Intelligence. Benefiting from the flexibility and practicality of facilitating human-\nrobot interaction, language-guided robotic manipulation has gained extensive research attention in\nrecent years. Many benchmarks have been built to encourage the research of language-guided robotic\nmanipulation, such as RLBench [22], CALVIN [33], VLMBench [61], etc. Early methods improve\nthe manipulation performance by introducing powerful representations [9, 59], elaborated network\narchitectures [15, 13], or effective training mechanisms [32, 44]. With the rapid development of VLMs\n[2, 40, 37], recent works have attempted to introduce VLMs to improve the manipulation accuracy\nand generalization to unseen scenarios/objects in a trainable [48, 64, 49, 27, 26] or offline [21, 20, 35]\nmanner. However, most previous approaches tend to learn a direct mapping from multi-modal inputs\nto low-level actions, ignoring the explicit modeling of environmental dynamics. As a result, they may\nfail to make executable actions or plans and not generalize well to complex environments. We have\nalso noticed previous work on waypoints and primitive actions, but they often used a limited number\nof actions. For instance, CLIPort [45], Transporter [57], GMRT [47], and VPG [58] are restricted\nto simple actions like pick/place/push, limiting their use in complex tasks. Some language-guided\nmodels [10, 16, 30] define a few primitive actions (\u2264 5) and add prompts to aid decision-making.\nPerAct [46], RVT [14] use robot states as waypoints to skip trivial action predictions. SUSIE [6]\nand UniPi[11] predict sub-goals through video predictors, but there is an inconsistency between the\npredicted video and actions. In this work, we propose a waypoint-aware world model to track key\ndynamics that happen during the manipulation. Our model fulfills asynchronous world modeling and\naction prediction, which significantly promotes both manipulation accuracy and efficiency. PIVOT-R\nsupports 10 primitive actions and is extensible, making it effective in complex tasks.\nWorld Models. World models aim to generate a predictive model of its surroundings, accounting for\nuncertainties and dynamic changes. They have been widely studied in video generation [4, 53, 8],\nnavigation [51, 24, 39], and autonomous driving [52, 62, 54] areas. For example, Genie [8] introduces\na spatiotemporal video tokenizer and a dynamics model to autoregressively predict the next video\nframe. DriveDreamer [52] builds a world model deriving from real-world driving scenarios for\nenabling reasonable driving policy generation. With the great potential for acquiring insights into\nreal-world motion and physics rules, some works have also introduced world models for robotic\nmanipulation tasks [56, 34, 60]. Daydreamer [56] applies the Dreamer [17] algorithm to train real-\nworld robots by online reinforcement learning. SWIM [34] collects human videos for training a\nworld model and fine-tuning it on a small amount of robot data. Nevertheless, they usually perform\nworld modeling and decision-making alternatively, bringing great difficulty for training and is also"}, {"title": "3 Architecture", "content": "Our goal is to build a robot manipulation model that can respond accurately and timely to user\ninstructions in various zero-shot complex and variable environments. To this end, as shown in Figure\n2, we introduce a primitive-driven waypoint-aware world model for robot manipulation. Next, we\ndiscuss the structural details of each module of PIVOT-R in detail."}, {"title": "3.1 Problem Formulation", "content": "As shown in Figure 2 (a), we formulate the proposed PIVOT-R as learning a trainable robot manipula-\ntion model \u03c0, which maps the user's language instruction l and a series of observation images Ot-h:t\nand robot state St-h:t from the time step t - h to the current time step t to action At. h represents the\nlength of the historical frames, here it is set to 3. In addition, we also introduced a scene prediction\nmodule for WAWM to help the model model world knowledge. The overall formulation of PIVOT-R\nis as follows:\n\u03c0(VLM(l, Ot), Ot\u2212h:t, St\u2212h:t) \u2192 \u041c, \u0410, (1)\nwhere M and A are the waypoints and actions of the robot manipulation predicted by the model at\ntimestep t, respectively. In particular, we use the pre-trained VLM to parse the primitive actions P"}, {"title": "3.2 Inputs and Outputs", "content": "We provide a detailed description of the inputs and outputs of PIVOT-R in Figure 2 (a) as follows:\n\u2022 Language input. The user's language instruction l is first combined with the prompt and used as\nthe input of the pre-trained VLM to parse the primitive action represented by the short text. The\ndetails of the prompt are shown in Appendix F.1. Specifically, in the example of the language\ninstruction \"Give me a container of drinking water\", the primitive action at this time may be\n\"approach/grab/put down the container\". Then, the parsed primitive action and original instruction\nl are encoded by a text encoder as a text sequence P. Following [45, 46, 42], we employ pre-trained\nCLIP [40] as the language encoder Etext.\n\u2022 Visual input. For visual observation of RGB image O, we use a pre-trained CLIP [40] visual\nencoder Eimage for encoding.\n\u2022 Robot state input. The robot state includes 6 dimensions of robot arm movement S =\n(x, y, z, roll, pitch, yaw). We use linear layers to encode them.\n\u2022 Outputs. The output of PIVOT-R is the feature FM\u2081 \u2208 \\mathbb{R}^{b\u00d7n\u00d7d} of the task-related waypoint image\npredicted by the scene prediction module and the robot action A predicted by the action prediction\nmodule. Where b, n = 49, and d = 512 represent the batch size, number of tokens, and dimension\nof the feature FM, respectively. The action A contains the delta state S of the robot's end-effector\nand the binary state G \u2208 {0, 1} of the gripper, i.e., A = (S, G) \u2208 \\mathbb{R}^{1\u00d77}."}, {"title": "3.3 Network", "content": "Overall, PIVOT-R consists of a powerful waypoint-aware world model and a lightweight action\nprediction module, whose detailed information is described as follows:\n\u2022 Waypoint-Aware World Model (WAWM). By introducing waypoints as a data structural chunking\nmechanism, similar to tokenization in NLP, we segment dense and irregular robot trajectories\ninto meaningful sections, reducing the prediction burden. This hierarchical approach decouples\nlanguage-action interdependencies and leverages cross-trajectory waypoint transition knowledge,\nimproving action prediction accuracy. As shown in Figure 2, WAWM mainly includes a powerful\nVLM and a scene prediction module \u03a6sp. Given a user instruction l, the VLM parses l to provide\ntask-related waypoint prompts, which are used for guiding the scene prediction module Psp to\nconduct critical waypoint prediction.\nSpecifically, at each timestep t, we combine the prompts with the user instructions l and the\nrobot observation images Ot as the input of the pre-trained VLM to perform primitive action\nparsing related to the manipulation task. Then, the parsed primitive actions and the original user\ninstructions l are combined as waypoint indicators Pt for the scene prediction module. The above\nprocess can be expressed as:\nPt = (l, VLM (Prompt(l), Ot)) . (3)\nFor the scene prediction module sp, we use the waypoint waypoints Pt related to the robot\nmanipulation task as a prompt and the historical observation image Ot-h:t of the robot as input to\npredict the waypoints feature FM, of the robot manipulation, that is, we have:\nFM = sp(Etext(Pt), Eimage(Ot\u2212h:t)). (4)"}, {"title": "3.4 Asynchronous Hierarchical Executor", "content": "In addition, in order to improve the execution efficiency of PIVOT-R, we adopt an asynchronous hier-\narchical execution mode to execute primitive action parsing, scene prediction, and action prediction\nrespectively. Specifically, as shown in Figure 2 (a), we use different execution frequencies for these\nthree parts according to needs. For primitive action parsing, it requires a lot of computation using\nVLM so we use a lower execution frequency \u03bd1. For the lightweight action prediction module, we\nadopt a higher execution frequency \u03bd3. These three execution frequencies conform to the following\nrelationship: \u03bd1 < \u03bd2 < \u03bd3, where \u03bd2 is the execution frequency of the scene prediction module.\nSpecifically, at timestep t, if a module has not finished processing the new request, it will return the\nprevious result first."}, {"title": "3.5 Loss", "content": "The training loss of PIVOT-R mainly includes scene prediction loss Lscene and action prediction loss\nLact. Specifically, for scene prediction loss Lscene, following I-JEPA [1], we calculate the average L2\ndistance of features between the predicted waypoint state M' and the ground truth M, where M is\nencoded using a pre-trained CLIP image encoder Eimage. For action prediction loss Lact, following\nRT-1 [7], we use Cross Entropy Loss to calculate the loss between the predicted action A' and the\nground truth action A. The total loss of PIVOT-R is L = Lscene + Lact."}, {"title": "4 Experiments", "content": "We conduct experiments on the challenging SeaWave [42] benchmark. Our experiments aim to\naddress three key inquiries: 1) How effective is PIVOT-R in executing various complex language\ninstructions? 2) How robust and generalizable is PIVOT-R to manipulation on out-of-distribution\nscenarios? 3) Which modules of PIVOT-R play an important role? 4) if there are cases where the\nrobot can retry and successfully perform an action after an initial incorrect attempt?"}, {"title": "4.1 Experiment Settings", "content": "\u2022 AHE. We use multithreading to process each module separately. Each thread runs at its own\nfrequency, extracts the latest data from the corresponding buffer, and places the output results\nin the buffer. For example, the VLM gets data from the camera buffer and saves the output in\nthe buffer after each update. Then, the scene and action prediction module updates at different"}, {"title": "4.3 Results on Robotic Manipulation", "content": "Results on SeaWave. We perform experiments on four levels of tasks in SeaWave, and the average\nsuccess rate is in the last column. The results are shown in Table 1. PIVOT-R substantially achieved\na significant improvement on all tasks. Specifically, PIVOT-R achieved an average success rate\nof 74.19%, 19.45% higher than the best baseline. Both the manipulation ability and the ability to\nunderstand instructions have been greatly improved. This confirms the effectiveness of the primitive-\ndriven approach.\nWe also show qualitative results, which are shown in Figure 3. It demonstrates the example of bringing\nmilk close to yogurt. The task process can be divided into five actions. Through the instruction of\nprimitive actions and the prediction of waypoints, the model successfully completes the task.\nIt is also important for robots to be able to operate in real-time. Since the hardware device and action\nspace are the same for all models, we focus on the inference speed of the models. As shown in the\nlast column of Table 1, we compare the inference time of the models. We calculated the average time\nfor the model to execute one step. It can be seen that BC-Z based on ResNet[19] is the fastest. In\naddition, the inference speed of PIVOT-R and most other models are of the same order of magnitude,\nwith only a few milliseconds difference. Though simple, AHE's integration with WAWM is highly\neffective. PIVOT-R's VLM-based primitive-driven WAWM for scene and action prediction, combined\nwith AHE for asynchronous execution, improves efficiency by 28 times.\nResults on Real World. The quantitative results\nare shown in Table 2. PIVOT-R improves the\naverage success rate by 6%. The qualitative\nresults are shown in Figure 4. Surfer and RT-1\nusually fail due to position errors, while PIVOT-\nR has higher accuracy. In the \"push to\" task, the\nperformance of all models is suboptimal. This is\nbecause the downward force applied during the\npushing process increases the resistance, making\nit difficult for the models to effectively predict\nand adapt to this change."}, {"title": "4.4 Generalization Ability", "content": "We also perform experiments in different unseen scenarios on level 2, 3, and 4 tasks. New scenarios\ninclude unseen backgrounds (i.e., two unseen tables), changing light intensity, and more distractions\n(i.e., more objects). The results are shown in Table 3. PIVOT-R still maintains a success rate far\nsuperior to other models, indicating that with the help of WAWM, the model captures key information\nand maintains good generalization in changing scenarios."}, {"title": "4.5 Ablation Study", "content": "In this section, we explore what is important in the design of the model. Specifically, We discuss\nthe impact of waypoint selection, VLM, AHE, scene prediction supervision, and action prediction\nmodule design on PIVOT-R's performance. We designed a series of ablation experiments. We made\nsome assumptions and experiments: (i) Waypoint selection. We conduct experiments by selecting\nthe primitive action completion (PAC) frame, robot state changes (RSC) frame, next frame, five\nframes apart, and the final frame of the trajectory as waypoints respectively. (ii) VLM's image and\nlanguage understanding capabilities. We chose Qwen-VL[3] of the same size to compare with the\nmost powerful GPT-4[37] currently. (iii) Design of asynchronous architecture. We canceled the\nasynchronous architecture so that each module will be updated at every step. (iv) Design of scene\nprediction module. We refer to the design of MAE[18] and use predicted pixel-level images instead\nof feature prediction. (v) Design of action prediction module. We use a larger Transformer. Table 4\nshows the results of each ablation and the delta performance compared."}, {"title": "4.6 Failure and Retry", "content": "This section discusses cases where the robot fails and whether it can be retried and successfully\nexecuted. As shown in Figure 5 (left), retries may be successful in some cases. When the position of\nthe gripper deviated and the object failed to be grasped, the second attempt to grasp was successful.\nHowever, in the case of Figure 5 (right), if the object is knocked down and rolls a certain distance, it\nwill be difficult to successfully grasp it again."}, {"title": "5 Discussions", "content": "Conclusion. In this paper, we propose PIVOT-R, a primitive-driven waypoint-aware world model.\nPIVOT-R focuses on the execution of primitive actions. Predicting key waypoints in the future\ngreatly improves performance. It has achieved state-of-the-art results on the SeaWave benchmark,\nand experiments have proven that it has good robustness. We also use asynchronous hierarchical\nexecutors to ensure fast enough execution of the model. In addition, we demonstrate that PIVOT-\nR has the potential to complete unseen instructions and tasks under the guidance of a high-level\nVLM. Finally, we also demonstrate PIVOT-R's ability to improve performance through human\ndemonstration. These results illustrate the potential of PIVOT-R.\nLimitations. We demonstrate the ability of PIVOT-R to complete tasks, even unseen tasks, through a\ncombination of primitive actions guided by instructions. However, action execution and instructions\nare sometimes inconsistent. For example, if \"push left\" is required, the robot may execute \"push\nfront\". Therefore, we also need to strengthen the consistency between high-level instructions and\nunderlying actions, so that the robot can truly perform tasks according to our instructions, and even\nadjust according to requirements, just like a real intelligent agent."}]}