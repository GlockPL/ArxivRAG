{"title": "Relation-aware Hierarchical Prompt for Open-vocabulary Scene Graph Generation", "authors": ["Tao Liu", "Rongjie Li", "Chongyu Wang", "Xuming He"], "abstract": "Open-vocabulary Scene Graph Generation (OV-SGG) overcomes the limitations of the closed-set assumption by aligning visual relationship representations with open-vocabulary textual representations. This enables the identification of novel visual relationships, making it applicable to real-world scenarios with diverse relationships. However, existing OV-SGG methods are constrained by fixed text representations, limiting diversity and accuracy in image-text alignment. To address these challenges, we propose the Relation-Aware Hierarchical Prompting (RAHP) framework, which enhances text representation by integrating subject-object and region-specific relation information. Our approach utilizes entity clustering to address the complexity of relation triplet categories, enabling the effective integration of subject-object information. Additionally, we utilize a large language model (LLM) to generate detailed region-aware prompts, capturing fine-grained visual interactions and improving alignment between visual and textual modalities. RAHP also introduces a dynamic selection mechanism within Vision-Language Models (VLMs), which adaptively selects relevant text prompts based on the visual content, reducing noise from irrelevant prompts. Extensive experiments on the Visual Genome and Open Images v6 datasets demonstrate that our framework consistently achieves state-of-the-art performance, demonstrating its effectiveness in addressing the challenges of open-vocabulary scene graph generation.", "sections": [{"title": "1 Introduction", "content": "Scene Graph Generation (SGG) (Johnson et al. 2015; Zellers et al. 2018) is a fundamental task in computer vision, involving the construction of a structured representation of a scene by identifying the relations between entities depicted in an image. It has already demonstrated promising performance in various downstream tasks (Kamath et al. 2021; Lee et al. 2019; Chen et al. 2020; Li et al. 2021). Traditional SGG methods typically operate within a closed vocabulary, and due to the diversity of relational concepts that exceed existing data annotations, they face challenges in effectively modeling open-set relations. To address this challenge, Open-Vocabulary Scene Graph Generation (OV-SGG) (He et al."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Scene Graph Generation", "content": "The Scene Graph Generation (SGG) task, initially proposed by (Johnson et al. 2015), traditionally relies on supervised methods (Xu et al. 2017; Gu et al. 2019b; Tang et al. 2019;"}, {"title": "2.2 Open-vocabulary Methods", "content": "In recent years, researchers in the field of visual scene understanding, such as object detection (Gu et al. 2021; Zareian et al. 2021), have shifted their focus from traditional closed-set methods to more flexible open-vocabulary methods. A key driver of this evolution is the development and maturity of VLMs (Radford et al. 2021; Jia et al. 2021; Li et al. 2023). These models are typically pre-trained on large-scale image-text pairs, endowing them with strong cross-modal alignment capabilities. By leveraging natural language prompts (Wu et al. 2024), VLMs can compute similarities between images and language in open-vocabulary settings, facilitating category expansion (Gu et al. 2021; Ma et al. 2022).\nEarly research (Wang et al. 2023; Menon and Vondrick 2022) concentrates on simple prompts for open-vocabulary recognition. Other approaches (Wang et al. 2024, 2022b) employ learnable prompts to enhance image classification. As research progresses, single prompts cannot adequately handle complex visual inputs, leading to the proposal of hierarchical prompting methods to better structure intricate query information. Models like (Ge et al. 2023) rely on object class hierarchies by WordNet (Miller 1995). RECODE (Li et al. 2024b) utilizes LLMs to generate hierarchical prompts from the perspectives of subject, object, and spatial levels, facilitating zero-shot relationship recognition. In contrast to RECODE, our work approaches the task from a regional perspective, enabling the generation of more detailed and specific relationship prompts. Additionally, we introduce a dynamic VLM-guided mechanism that adjusts prompts based on visual inputs, increasing the accuracy and flexibility of text representations."}, {"title": "3 Preliminary", "content": null}, {"title": "3.1 Problem Setting", "content": "The goal of SGG is to create a descriptive graph G = {V, R} from an image I. This graph consists of Nv entities V = {vi} and visual relationship triples E = {vi, ri,j, Vj}i\u2260j, where ri,j represents the predicate category between them. Each entity vi is represented as (c, bi), where cu denotes the label in the entity category space Oe, and bi represents its location through a bounding box in the image. The predicate category ri,j denotes the label in the category space Or. In the task of OV-SGG, the category spaces for entities and predicates are divided into two parts. Specifically, the predicate category space contains the base category space Or and the novel category space Or, and it has Or = Or \\cup On. Similarly, the entity category space also has O = Oe \\cup On."}, {"title": "3.2 OV-SGG Pipeline", "content": "Most OV-SGG methods (Yu et al. 2023; Chen et al. 2023) can typically be decoupled into two steps: relationship proposal generation and predicate classification.\nFirst, the model receives an image as input and feeds it into a proposal network, from which it extracts relationship proposals P = {vi, vj}i\u2260j and the corresponding relationship features R \u2208 \\mathbb{R}^{N \\times d}, where N is the number of relationship proposals and d is the dimension of the feature representation. Then, the relationship features are fed into the predicate classifier as visual representations. The predicate classifier usually handles each predicate class using predefined text prompts, which generate text embeddings T\u2208 \\mathbb{R}^{Cp \\times d} through the text encoder TextEnc of a VLM, where Cp is the number of predicate categories. These text embeddings as the text representations replace the fixed predicate classifier weights, enabling the model to extend to new relationship categories that appear during the testing phase. The predicate classifier obtains the predicate classification scores S\u2208 \\mathbb{R}^{N \\times Cp} for each relationship proposal by calculating the similarity score between R and T:\nS = \\$(R, T) = \\frac{RT}{|RT|}, (1)\nwhere is the dot product, we define this operation of calculating similarity as \\$(). During OV-SGG training, OV-SGG methods use a distillation loss to distill the knowledge of the VLM to maintain the model's generalization. The distillation loss ensures that the distance between the text embeddings and relationship features remains consistent across all pairwise classifications."}, {"title": "4 Method", "content": null}, {"title": "4.1 Method Overview", "content": "We propose RAHP, a method that enhances the generalization of OV-SGG models on novel relations by using multi-level text prompts to strengthen visual relation text representations. Specifically, our framework is composed of three modules: hierarchical prompt generation (Sec. 4.2), visual relationship extraction (Sec. 4.3), and hierarchical relationship prediction (Sec. 4.4). Finally, we introduce the learning and inference pipeline of our method (Sec. 4.5)."}, {"title": "4.2 Hierarchical Prompt Generation", "content": "The hierarchical prompt generation module enriches text representations by creating multi-level text prompts that include entity-aware and region-aware prompts. As shown in Fig. 2 (a), for the input vocabularies, we sequentially generate prompts at two levels.\n\u2022 Entity-aware text prompts: These prompts include precise relationship content by combining predicate, subject, and object details. However, as the number of triplets grows cubically with subjects and objects, incorporating all triplet information into the prompts becomes impractical. To address this, we first cluster entities into super entities based on similarity. Similar to the approach in (Zhang et al. 2024), it can effectively reduce the triplet category space (more details can be found in the appendix). We then generate entity-aware text prompts by combining super entities with predicate categories.\n\u2022 Region-aware text prompts: Building on entity-aware prompts, we create region-aware text prompts that capture finer visual details through a region-aware description mining strategy. As shown in Fig. 2 (b), we use an LLM to decompose key entity parts and naturally generate region-level visual relation descriptions by combining these parts' relationships. For example, in the relationship triplet <male, sitting on seating, furniture> the \u201cmale\u201d can be associated with specific body parts like the hips, thighs, and arms, while the \u201cseating furniture\u201d can be associated with components like the seat and backrest. The \u201csitting on\u201d relationship is then represented by combining these elements in the LLM to provide extensive region-aware relation descriptions. Following (Menon and Vondrick 2022), we design two cases for the LLM to learn from.\nHierarchical Prompt Encoding After generating the two levels of prompts, we encode them into text embeddings as text representations using the frozen VLM text encoder TextEnc. As shown in Fig. 2 (a), we generate sentences for the entity-aware prompts through the template \"A photo of a/an [Subject] [Predicate] a/an [Object]\". The entity-aware prompts are encoded by TextEnc into an entity-aware text embedding set T^e = {T_1, T_2, ..., T_{C_{se}}}, where T_e \\in \\mathbb{R}^{C_p \\times d}, Cp represents the number of predicate categories, and Cse denotes the number of super entity categories. Correspondingly, the region-aware prompts are generated sentences through the template \"A region that reflects [region descriptions]\u201d. The region-aware prompts are encoded into an text embedding set T^r = {T_1, T_2, ..., T_{C_p \\times N}}, where T_r \\in \\mathbb{R}^{C_P \\times N \\times d}, N is the number of region-aware prompts, varying with the region descriptions per triplet."}, {"title": "4.3 Visual Relation Extraction", "content": "Following the process described in Sec. 3.2, the visual relationship extraction module is mainly designed to extract visual relation features. It employs a proposal network to extract relation proposals P = {vi, vj}i\u2260j from the visual input I, along with their corresponding relation feature representations R. Then we merge the predicted subject and"}, {"title": "4.4 Hierarchical Relation Prediction", "content": "The hierarchical relation prediction module predicts predicates by calculating the similarity between relation features R and two levels of text embeddings T: entity-aware and region-aware. This module includes two key components: VLM-guided dynamic selection, which filters out irrelevant prompts, and hierarchical prediction aggregation, ensuring accurate predicate classification.\nImage-guide Dynamic Selection The VLM-guided dynamic selection mechanism utilizes the image-text alignment capabilities of a VLM to match T^r with union features U. The mechanism is aimed at filtering out region-text pairs that are completely irrelevant to the image, leveraging the robust object recognition capabilities of the VLM to achieve this goal. Specifically, for jth in T^r, upon receiving the unified feature U, it computes the matching score S^a \\in \\mathbb{R}^{N \\times N} between U and the region-aware text embeddings T^r as follows:\nS^a = \\$(U,T^r) (3)\nTo capture core visual semantic information, we select the top k region-aware text embeddings with the highest matching scores and perform predicate classification. After performing VLM-guided selection on all region-aware prompts, we obtain the final region-aware text prompt embedding set T^{r'} = {T_1', T_2', ..., T_{r'_{C_{se}}}}, where T_r' \\in \\mathbb{R}^{C_p \\times k \\times d}. This mechanism dynamically selects text prompts based on union"}, {"title": "4.5 SGG Learning and Inference", "content": "SGG Learning During the training stage, the model only receives information from the base classes. Similar to (Li, Zhang, and He 2024; Chen et al. 2023), we adopt a multi-task loss for our model training. Specifically, we use L1 loss and GIOU loss for entity bounding box regression to reduce the gap between the predicted bounding box b and the ground truth b_{gt}:\nL_{bbox} = ||b - b_{gt}||_1 + GIOU(b, b_{gt}). (8)\nWe also use a cross-entropy loss L_{ent} = CE(c^o, c_{gt}) to ensure the accuracy of the prediction c^o for entity classification against the ground truth category c_{gt}.\nFor predicate prediction, we use L_{pre} = FL(r, r_{gt}) to represent the Focal loss for predicate categories, where r_{gt} is the ground truth predicate category, and r is the predicted predicate category. In addition, we employ an L1 loss (Liao et al. 2022; Chen et al. 2023) to minimize the gap between the relation feature R and the visual features V \\in \\mathbb{R}^d extracted by the VLM visual encoder VisEnc. The goal is to align the relation features extracted by SGG with the VLM space, thereby enabling the prediction of novel predicates. It also acts as a form of regularization to prevent overfitting to the specific training data. For the i-th relation proposal, the distillation loss is designed as an L1 distance loss, defined as follows:\nL_d = ||R_i - V||_1 (9)\nThe total training loss can be written as\nL = L_{bbox} + \\lambda_1L_{ent} + \\lambda_2L_{pre} + \\lambda_3L_d. (10)\nwhere the weights of each loss term \\lambda_1, \\lambda_2, \\lambda_3 balance the learning progress and importance across different tasks.\nSGG Inference To enhance the interpretability of novel relation triplets, we employ LLMs to generate informative visual descriptions before the inference phase. In the post-processing stage, we systematically eliminate invalid self-connected edges and exclude triplets where subject and object entities are identical. Subsequently, the remaining triplets are ranked based on the combined scores from entity predictions and predicate predictions. The top M relation triplets are then selected as the final output, providing comprehensive information in terms of subject entity probabilities, object entity probabilities, and predicate probabilities."}, {"title": "5 Experiment", "content": "In this section, we comprehensively evaluate our RAHP on the OV-SGG task. More results, including closed-set SGG, parameter sensitivity experiments and qualitative analysis, are provided in the Appendix."}, {"title": "5.1 Datasets and Experimental Settings", "content": "Datasets To evaluate the SGG task, we adopt two benchmarks: the VG150 version of the Visual Genome (VG) dataset (Krishna et al. 2017) and the Open Image v6 (OIV6) dataset (Kuznetsova et al. 2020).\nEvaluation metrics We evaluate our method under two settings (Chen et al. 2023): Open Vocabulary Relation-based Scene Graph Generation (OVR-SGG) uses a closed vocabulary for objects and an open one for relationships, whereas Open Vocabulary Detection + Relation-based Scene Graph Generation (OVD+R-SGG) uses open vocabularies for both. We adopt the PredCLS and SGDet protocols (Xu et al. 2017) and report the performance on Recall @K (K=50/100) and mean Recall @mK (mK=50/100) for each setting.\nImplementation Details We employ the GPT-3.5-turbo, as our LLM. We adopt CLIP (Radford et al. 2021) (ViT-B/32) as our VLM backbone. We categorize 150 entities into 30 super-class entities for VG and categorized 602 entities into 53 super-class entities for OIV6 (details can be found in the appendix). RAHP is applicable to both one-stage and two-stage models, therefore we select the one-stage methods SGTR\u2020 (Li, Zhang, and He 2024) and OvSGTR"}, {"title": "6 Conclusion", "content": "In this paper, we introduce the Relation-Aware Hierarchical Prompting framework (RAHP), designed to address the challenges of OV-SGG by enhancing text representations. By integrating entity-aware and region-aware relation text prompts, RAHP enhances text representation and enables more accurate and flexible image-text matching. Our dynamic selection mechanism further refines this process by adapting prompts based on visual information, reducing noise and improving the robustness of relation predictions. Through extensive experiments on the Visual Genome and Open Images v6 datasets, our method demonstrates state-of-the-art performance, and the demonstrated performance improvements-highlight the potential of RAHP to significantly advance the field of OV-SGG.\nDiscussion of Limitations: (1) Effectiveness of Entity Clustering. Clustering algorithms will struggle to maintain fine distinctions between diverse data categories, which can degrade the quality of text representations. (2) Diversity of Generated Text Prompts. Limited diversity in LLM-generated region descriptions can hinder model generalization for novel relationships."}, {"title": "A Overview of Material", "content": "In this supplementary document, we provide additional details and experimental results to enhance understanding and insights into our proposed RAHP. This supplementary document is organized as follows:\n\u2022 A detailed description of the entity clustering method in Sec. B is provided.\n\u2022 Examples generated by the region-aware prompt mining in Sec. C are presented.\n\u2022 Detailed descriptions of the Visual Genome and Open Image V6 datasets used in our study are provided in Sec. D.\n\u2022 Additional experimental results are presented in Sec. E."}, {"title": "B Entity Clustering", "content": null}, {"title": "B.1 Entity Clustering Method", "content": "In RAHP, we employ entity clustering to reduce the number of relationship triplets, thereby preventing the inefficiency in text prompt usage caused by the proliferation of triplets. To define superclasses, we leverage the lexical structure and part-of-speech tagging provided by WordNet. Specifically, we first cluster entities using WordNet's part-of-speech tags, grouping semantically related terms together. Next, we encode each cluster using the text encoder of a Vision-Language Model (VLM) and apply the K-means algorithm to the encoded embeddings, setting the number of clusters to M, resulting in M distinct categories.\nTo generate names for each super entity and minimize ambiguity, we utilize the extensive knowledge of a Large Language Model (LLM) to select the most frequent or semantically relevant superclasses. This approach enhances the stability and representativeness of the superclasses. We input all entity categories within each cluster into the LLM sequentially, prompting it to generate appropriate super entity names. To preserve the key characteristics of the entities, we instruct the LLM to analyze essential attributes before generating the super entity name. The prompt for generating the superclasses is shown is as follows:\nTask Description: You will be provided with a set of predicates related to specific actions, states, or relationships. Your task is to generate an appropriate superclass category name that effectively encapsulates the common characteristics of these predicates.\nInput: You will receive the following set of predicates.\nOutput: Please provide a concise and specific superclass category name that encompasses all the given predicates. The superclass name should be between one to three words and should use general and easily understandable vocabulary."}, {"title": "B.2 Super Entity Categories", "content": "The VG super entities generated by this method are as follows:\nVG_super_entities = [''male'', ''female'', ''children'', ''pets'', ''wild animal'', ''ground transport'', ''water transport'', ''air transport'', ''sports"}, {"title": "C Region-aware Prompts Generation", "content": null}, {"title": "C.1 Region-aware Prompts Statistics", "content": "This section gives an example of generating a region description. We use LLM extract key parts from both the subject and object, combining these parts to create detailed region descriptions. This approach allows the LLM to pinpoint the exact regions where subject-object interactions occur, leading to more precise visual relation descriptions for each relationship triplet. Using this prompt, we generate region descriptions for relationship triplets, achieving an average of 6.76 prompts per triplet with 20.32 unique objects and 7.58 unique relations, significantly outperforming baselines with only 2 objects and 1 relation. Empirically, our approach improves performance across 97 object classes in VG, highlighting its generalizability beyond specific categories. Here is a region description for the relationship triplet <vegetable, in, container>."}, {"title": "C.2 Region-aware Prompts Generation Examples", "content": "The complete region descriptions prompt is as follows:\nDescribe [subject] [predicate] [object] which parts of subject and object function in this relationship. Please list these parts, and then analyze and describe the visual relationship between these parts. The generated description should be concise and clear. Here are two examples for you to learn:\nExample A: \"[human] [holding] [wild animal]\":"}, {"title": "D Datasets and Relation Split", "content": "We evaluate our method on the Visual Genome (VG) and Open Image v6 (OIV6) datasets. This section details each dataset and our approach to dividing base and novel relations during experiments.\nVisual Genome: VG includes 150 entity categories and 50 predicate categories, manually annotated across 108,777 images. We use 70% of the images for training, 5,000 for validation, and the remainder for testing.\nPredCLS Setting: Following Epic's method, we categorize 70% of the predicates as base predicates and the remaining 30% as novel predicates. The base predicates are: [''above'', ''against'', ''at'', ''attached to'', ''behind'', ''belonging to'', ''between'', ''carrying'', ''covered in'', ''covering'', ''for'', ''from'', ''hanging from'', ''has'', ''holding'', ''in'', ''in front of'', ''looking at'', ''made of'', ''near'', ''of'', ''on'', ''over'', ''parked on'', ''playing'', ''riding'', ''sitting on'', ''standing on'', ''to'', ''under'', ''walking on'', ''watching'', ''wearing'', ''wears'', ''with'']\nThe novel predicates are: [''across'', ''along'', ''and'', ''eating'', ''flying in'', ''growing on'', ''laying on'', ''lying on'', ''mounted on'', ''on back of'', ''painted on'', ''part of'', ''says'', ''using'', ''walking in'", "Setting": "We adopt OvSGTR's division of predicates, the divided base predicates are: [\"between\", \"to\", \"made of\", \"looking at\", \"along\", \"laying on\", \"using\", \"carrying\", \"against\", \"mounted on\", \"sitting on\", \"flying in\", \"covering\", \"from\", \"over\", \"near\", \"hanging from\", \"across\", \"at\", \"above\", \"watching\", \"covered in\", \"wearing\", \"holding\", \"and\", \"standing on\", \"lying on\", \"growing on\", \"under\", \"on back of\", \"with\", \"has\", \"in front of\", \"behind\", \"parked on\"] and the novel predicates are: [\"belonging to\", \"part of\", \"riding\", \"walking in\", \"in\", \"of\", \"painted on\", \"playing\", \"for\", \"walking on\", \"says\", \"attached to\", \"eating\", \"on\", \"wears\"]\nOpen Image v6: OIV6 contains 301 entity categories and 31 predicate categories. We use 126,368 images for training, 1,813 for validation, and 5,322 for testing. Following PGSG's method, we split all predicates to base and novel predicates two parts.\nBase predicates: ['at', 'holds', 'wears', 'holding hands', 'on', 'highfive', 'contain', 'handshake', talk on phone']\nNovel predicates: ['surf', 'hang', 'drink', 'ride', 'dance', 'skateboard', 'catch', 'inside of', 'eat', 'cut', 'kiss', 'interacts with', 'under', 'hug', 'throw', 'hits', 'snowboard', 'kick', 'ski', 'plays', 'read']"}, {"title": "E More Experimental Results", "content": null}, {"title": "E.1 Close-vocabulary SGG on VG", "content": "In Tab. 5, we present the close-vocabulary SGDet performance on VG. We compare the performance of five models mentioned in the experimental section: PGSG, PE-NET, VS3,"}, {"title": "E.2 More Experimental Results of SGCLS", "content": "In addition to the PredCLS and SGDet protocols, we conduct OVR-SGG experiments using SGTR+ as the backbone model under the SGCLS protocol. The results are summarized in the Table 6. Compared to SGTR+, RAHP enhances the model's open-vocabulary capabilities, effectively recognizing novel predicates. Additionally, RAHP offers a rich set of prompts, enabling better differentiation among various predicates and improving the model's prediction accuracy for novel predicates."}, {"title": "E.3 Impact of Hyper-parameter a", "content": "We conduct experiments on the VG validation set to evaluate the impact of different parameters a, which controls the"}, {"title": "E.4 Impact of Hyper-parameter k", "content": "Table. 5 illustrates the impact of varying the hyperparameter k on model performance, where k represents the number of region-aware prompts selected in the dynamic selection mechanism. Similar to sec. E.3 setting, we show the R@100 results on the VG validation set. The model achieves optimal performance at k = 3. Consistent with our ablation study, selecting all region-aware prompts introduces noise, degrading performance for novel predicates. Conversely, limiting the selection to only the highest-scoring (Top 1) prompts reduces the diversity of text representations, thereby impairing SGG performance."}, {"title": "E.5 Impact of Distillation loss weight \u03bb3", "content": "We discuss the impact of the distillation loss weight \\lambda_3, on the model's performance, particularly its effect on the alignment between the SGG model's relation representations and the VLM's visual representations. We test four different values of \\lambda_3 = (1, 5, 20, 50) to determine the optimal setting. Table 6 presents the results for different \\lambda_3 values. The findings indicate that the model performs best with \\lambda_3 = 20, where the distillation method successfully balances the knowledge learned from SGG data and the knowledge transferred from VLM. This balance allows the model to maintain high performance across both base and novel predicates. Without distillation, the model struggles, particularly with novel categories, highlighting the importance of this strategy in retaining essential knowledge while generalizing to novel relationships. The results at \\lambda_3 = 50 demonstrate that while the model can be tuned to improve the recognition of"}, {"title": "E.6 Impact of the Number of Super Entities", "content": "We explore the impact of varying the number of super entities. Specifically, in the VG dataset, we group entities into super entities of 10, 20, and 30 categories, reflecting different levels of granularity. Fewer super entities indicate more aggregation, leading to less retention of the original entities' features. For example, classifying \"bike\" under \"vehicle\" may result in the loss of information specific to \"bike\" in region descriptions. Thus, the choice of the number of super entities balances between information completeness and computational complexity. Additionally, we conduct experiments on triplets formed by all entities (resulting in a total of 1,125,000 relation triplets, shown as \"all\" in the Fig. 7) to compare the performance when complete entity information is provided. The Fig. 7 shows that the number of super entities has a significant impact on model performance. When the number of super entities is limited to 10, the categories become overly broad, leading to a substantial loss of unique information when entities are converted into super entities. This makes it difficult to align the text representations with the visual representations. Increasing the number to 30 enhances the granularity of the text representations, improving the matching performance. Interestingly, at this level, the model's performance nearly matches that achieved using full triplets. This finding suggests that representative feature descriptions, even without highly detailed visual features, can substantially boost text-visual matching accuracy."}, {"title": "E.7 Impact of the Large Language Models", "content": "In RAHP, the region-aware prompts are generated using a Large Language Model (LLM). To ensure that variations in different LLMs do not significantly impact RAHP's performance, we conducted a comparative experiment using GPT-40-mini. We chose GPT-40-mini due to its rapid deployment capabilities through the OpenAI API, which offers advantages for method migration and implementation. The total cost of generating region-aware prompts for 45,000 relation triplets using both GPT-3.5-turbo and GPT-40-mini is [insert cost here]. As shown in the Table 7, experiments conducted on the VG dataset in an OVR-SGG setting revealed that both GPT-3.5-turbo and GPT-40-mini achieved comparable performance. This comparative study demonstrates that RAHP exhibits robustness across different LLMs, yielding stable performance despite variations in the underlying model."}, {"title": "E.8 Comparison with Zero-shot Relation Detection Method", "content": "To clarify the differences between RAHP and RECODE, we conducted tests on the zero-shot visual relationship detection task. Consistent with RECODE, we utilized CLIP with the Vision Transformer (ViT-B/32) as the default backbone"}, {"title": "E.9 Qualitative Results", "content": "To provide further insights into the effectiveness of our method, we visualize some scene graphs generated by our method and the baseline OvSGTR method in Fig 8. The solid lines in the figure represent base predicates, while the dashed lines denote novel predicates. Compared to the baseline, our method effectively identifies novel relationships and generates richer scene graphs. RAHP enhances the depth of scene understanding, demonstrating the superiority of our approach in generating scene graphs for complex visual scenarios."}]}