{"title": "Cluster-Norm for Unsupervised Probing of Knowledge", "authors": ["Walter Laurito", "Sharan Maiya", "Gr\u00e9goire Dhimoila", "Owen (Ho Wan) Yeung", "Kaarel H\u00e4nni"], "abstract": "The deployment of language models brings challenges in generating reliable information, especially when these models are fine-tuned using human preferences. To extract encoded knowledge without (potentially) biased human labels, unsupervised probing techniques like Contrast-Consistent Search (CCS) have been developed (Burns et al., 2022). However, salient but unrelated features in a given dataset can mislead these probes (Farquhar et al., 2023). Addressing this, we propose a cluster normalization method to minimize the impact of such features by clustering and normalizing activations of contrast pairs before applying unsupervised probing techniques. While this approach does not address the issue of differentiating between knowledge in general and simulated knowledge-a major issue in the literature of latent knowledge elicitation (Christiano et al., 2021)-it significantly improves the ability of unsupervised probes to identify the intended knowledge amidst distractions.", "sections": [{"title": "1. Introduction", "content": "The deployment of language models for practical applications introduces novel challenges, including the potential creation of untrustworthy or incorrect text (Weidinger et al., 2021; Park et al., 2023; Evans et al., 2021; Hendrycks et al., 2021). Specifically, models that are fine-tuned using human preferences may amplify existing human biases or generate persuasive yet deceptive outputs (Perez et al., 2022).\nEmpirical evidence suggests that simulated internal beliefs or knowledge can be extracted from language model activations (Li et al., 2022; Gurnee & Tegmark, 2023; Azaria & Mitchell, 2023; Bubeck et al., 2023). Supervised probing methods can be employed to extract this knowledge (Alain & Bengio, 2016; Marks & Tegmark, 2023) but such methods require labels, which in some domains may not be provided correctly due to human biases or because humans simply do not know the correct label. It may even be critical to avoid the use of human labels to differentiate between a model's true knowledge and its representation of human knowledge. Motivated by these ideas, unsupervised probing techniques like Contrast-Consistent Search (CCS) have been developed to extract the knowledge embedded in a language model without the need for ground truth labels (Zou et al., 2023; Burns et al., 2022).\nFarquhar et al. (2023) outline current limitations of these approaches, demonstrating that these unsupervised probes tend to identify the most salient binary feature, which may not always correspond to the specific knowledge feature we seek. For example, in one experiment, one of a pair of distracting random words is added to each prompt in a text dataset. After training, unsupervised CCS probes function as classifiers for these random words, rather than the intended knowledge feature of the text. In practice, there may be numerous salient features of which we are unaware, which can divert an unsupervised probe from identifying the target feature F, regardless of whether they are correlated or uncorrelated with F.\nTo tackle this issue, we propose a cluster normalization method. Our method follows the usual initial approach of unsupervised probing of harvesting contrast pair activations, however we then cluster similar activations and normalize them separately, thereby eliminating the effect of distracting salient features. We can then apply any unsupervised probing method, such as CCS or CRC-TPC (Burns et al., 2022), to train a probe on these normalized activations. It is of course crucial to ensure this approach does not inadvertently eliminate the knowledge feature itself. To prevent this, we utilize contrast pairs, performing the clustering based on the average embedding of each pair. Further details on contrast pairs are provided in Section 2.1.\nProbes trained with the original CCS approach achieve an average accuracy of approximately 0.5 on prompt datasets with distracting random word features (Farquhar et al., 2023). In contrast, our clustering method significantly improves this average accuracy to about 0.77, and to 0.81 for CRC-TPC"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Contrast-Consistent Search (CCS)", "content": "Contrast-Consistent Search (CCS), as described by Burns et al. (2022), locates a direction in activation space using a perceptron that adheres to logical consistency principles. This is achieved through a loss function designed to ensure that probabilities for a question-answer pair and its negated counterpart - a contrast pair - are complementary. This loss function is optimized in an unsupervised manner, and in doing so CCS extracts the latent knowledge within large language models to answer binary questions.\nAt first, a language model M processes a dataset of textual contrast pairs $(x_i^+, x_i^-)_{i=1}^N$, generating contextualized embeddings $(M(x_i^+), M(x_i^-))$. Following this, a linear probe (Alain & Bengio, 2016) is trained to calculate from these embeddings the probabilities $p_i^+$ and $p_i^-$, whether $x_i^+$ or $x_i^-$ is true, respectively. The objective function used to train this probe is given by a sum of two terms:\n$\\mathcal{L}_{CCS} = \\sum_{i=1}^N \\mathcal{L}_{consistency} + \\mathcal{L}_{confidence}$\n$\\mathcal{L}_{consistency} = [p(x_i^+) - (1 - p(x_i^-))]^2$\n$\\mathcal{L}_{confidence} = \\min\\{p(x_i^+), p(x_i^-)\\} ^2$.\nThe first term, $\\mathcal{L}_{consistency}$, is motivated by the idea that the probabilities of a statement and its negation should sum to one. This ensures logical consistency. The second term, $\\mathcal{L}_{confidence}$, is designed to maximize the information extracted by the probe, penalizing cases where the probabilities for both true and false are the same, at"}, {"title": "2.2. Contrastive Representation Clustering", "content": "As an alternative to CCS, the method of Contrastive Representation Clustering via Top Principal Component (CRC-TPC) (Burns et al., 2022) separates the normalized contrast pair differences $\\{M(x_i^+) - M(x_i^-)\\}$ based on projections onto their top principal component, i.e., the singular vector associated with the highest singular value, or the direction with the highest variance. This is again motivated by the intuition that the second-most salient contrastive feature after F+ - removed by normalization - should be the knowledge feature $F_{T/1}$."}, {"title": "2.3. Theoretical Background", "content": "A salient feature is a direction with high variance in the data. We are interested in salient features in the contrast pair differences $M(x_i^+) - M(x_i^-)$, and we refer to these as contrastive features.\nIn this section, we explain (1) why undesired salient contrastive features can mislead unsupervised probes, and (2) how contrastive features can be induced by non-contrastive ones. The mechanisms of the latter point are illustrated through an example.\nWe shall first examine why there is a close link between the CCS loss described in Section 2.1 and the idea of saliency i.e., variance. Contrastive features will naturally achieve a low CCS loss. To see this, consider the variance of contrast pair differences projected along the feature direction of a given feature F:\n$X := F^T \\cdot M(x^+), Y := F^T \\cdot M(x^-)$.\n$Var(X - Y) = E(X^2) + E(Y^2) - 2\\cdot E(X \\cdot Y)$\n$-(E(X)^2 + E(Y)^2 - 2\\cdot E(X) \\cdot E(Y))$\nIn this expanded form, we see that the variance of contrast pair differences in the direction F captures,\n*   confidence, with $E(X^2) + E(Y^2)$ higher if the magnitude projection along F in either element of a pair is high,\n*   and consistency, with $-2\\cdot E(X \\cdot Y) > 0$ if the projections of a contrast pair along F have opposing sign. This also increases with the magnitude of these projections.\nNote that the term $-(E(X)^2 + E(Y)^2 - 2\\cdot E(X) \\cdot E(Y))$ equals zero under the set-up of CCS, as the normalization step described above results in $E(M(x)) = 0$, therefore $E(X) = E(Y) = 0$.\nDue to this link between confidence, consistency, and saliency, a probe trained using the CCS loss will favor learning salient contrastive features. Otherwise, the projections of a contrast pair onto a feature will be small in difference or equal, failing to satisfy at least the consistency condition.\nAs mentioned in Section 2.1, we can describe two features which intuitively will satisfy the CCS loss:\n*   $F_+ := F_+ - F_-$, the syntactical difference between contrast pairs due to the appending of positive and negative tokens, removed by normalization in the original CCS method,"}, {"title": "3. Method", "content": "We begin with a dataset of contrast pairs, $\\{(x_i^+, x_i^-)\\}_{i=1}^N$. For each pair, we harvest the intermediate activations of a language model M, specifically the state of the residual stream at the final token position at a specific layer, which we denote as $M(x_i)$. We average these activations for each contrast pair $\\overline{M(x_i)} = \\frac{M(x_i^+) + M(x_i^-)}{2}$, and partition $\\{\\overline{M(x_i)}\\}_i$ using a clustering algorithm, thereby partitioning the original dataset using its most salient features. Each cluster is then normalized separately to have zero mean and unit variance, i.e. for each positive sample $x_i^+$, where $x_i$ belongs to cluster c, $M(x_i^+) = \\frac{M(x_i^+) - \\mu_c}{\\sigma_c^2}$, where $\\mu_c$ and $\\sigma_c^2$ are the mean and standard deviation of all positive samples in cluster c. The same normalization process is applied to all negative samples. Finally, an unsupervised probe can be trained on the contrast pair differences of the normalized (by cluster) samples. This approach allows the probe to isolate the desired knowledge feature, ignoring other distracting features isolated to each original cluster.\nFollowing the notation in Section 2.3, if $x_i$ belongs to cluster $c \\in \\{0, 1\\}$, a successful cluster normalization will leave:\n$\\overline{M(x_i^+)} - \\overline{M(x_i^-)} = F_{T/1} \\pm \\triangle$.\nThis follows from equation 1, however in this case normalization is performed over c only as opposed to the whole dataset.\nA key element to the effectiveness of our method is that our clustering approach does not erase the effect of the desired knowledge feature. This is achieved by clustering the averages of each contrast pair, $\\overline{M(x_i)}$. As a result, clustering only isolates salient non-contrastive features, and is effectively blind to $F_+$ and $F_{T/1}$. Normalizing positive and negative samples separately per cluster aims to ensure that all contrastive features $F'$ related to $F_+$ are properly normalized out - including the leaks from non-contrastive features F mixing with $F_+$, as explained in Section 2.3. Note, we do not normalize out similar $F'$ resulting from the mixing of $F$ with $F_{T/1}$. Eventually, only contrastive features related to knowledge are kept."}, {"title": "4. Experiments", "content": "In our experiments, we utilize Mistral-7B as our main language model, harvesting a ctivations (using the libraries from (Wolf et al., 2020) and (Nanda & Bloom, 2022)) at the 75th percentile layer (layer 24 for Mistral-7b) since, from preliminary experiments, we find probes achieve higher accuracies using the 50th to 90th percentile layers. We also report results using different language models (Phi-2 and 3, Gemma-7b, Llama-3-8B, Pythia-6.9b) and layers in the Appendices to verify the efficacy of our method. Our experiments follow the same general approach as those reported in Farquhar et al. (2023), as each of these original experiments set out to demonstrate the limitations of current unsupervised probing techniques.\nWe present results for three experiments below. For the first and second, we create prompt datasets based on the IMDb dataset (Jiang et al., 2023; Maas et al., 2011), while for the third we use the CommonClaim (Casper et al., 2023) dataset. We report results on a fourth experiment utilizing the DBpedia dataset (Lehmann et al., 2015) in Appendix A; this experiment follows on from results reported in Farquhar et al. (2023), however, we find we are unable to replicate these results (on three different models) and instead obtain high accuracies for both the original method of CCS and our approach using cluster normalization.\nActivation clustering is performed using HDBScan, implemented in the scikit-learn library (Kramer & Kramer, 2016), setting a minimum number of elements in each cluster to 5 and using the Euclidean distance metric. One advantage of HDBScan over other clustering algorithms (e.g., k-means) is that the number of clusters does not need to be specified in advance. In order to examine the variance in probe performance, we report summary statistics of 50 probe fits in each experiment, and visualize the results from all.\nThe following experiments generally involve a comparison between an original prompt and a modified one, to attempt to induce a bias in an unsupervised probe. Hereon, we refer to these original prompts as unbiased and modified prompts as biased. We also refer to normalization over an entire dataset, as Burns normalization or Burns-Norm (See 2.3 for more details). We refer to our alternative approach through clustering as cluster normalization or Cluster-Norm. Unlike the approach in (Burns et al., 2022), where multiple prompt templates were used, the study in (Farquhar et al., 2023) utilized only one prompt template per dataset. Our method follows the prompt-template setup from (Farquhar et al., 2023).\nEach experiment utilizes a train-test split of 70% for training and 30% for testing. Importantly, we evaluate our unsupervised probes on a test set where Burns-Norm is applied to the test set as it was done in (Burns et al., 2022), and not our cluster normalization. This is because we want probes to generalize, so if during evaluation they are fed with a contrast pair that belongs to an entirely different dataset, it is out of distribution for the clusters found during training. The probe should be a feature in the unaltered latent space. Although we do not use cluster norm on the test set for the aforementioned reason, we do use Burns-Norm for being able to compare our results with Burns et al. (2022) as this is what they do for the test set. Farquhar et al. (2023) likely follow a similar approach, as they mention utilizing normal-"}, {"title": "4.1. Random Words", "content": "In this experiment, we induce a strong syntactical bias in the data to illustrate the problem of distracting salient features and demonstrate the necessity of our method for removing them."}, {"title": "4.1.1. DATASET", "content": "Following the approach of Farquhar et al. (2023), we create a dataset by appending a random word to half of our prompts and a different random word to the remaining half. The following is an example of a prompt in a given dataset, where [label] can be positive or negative and [random_word] is a random word from the NLTK corpus (Bird, 2006). For each data point we have a different movie review ([review], e.g. \"This is my favorite movie ...\"):\nConsider the following example: [review],\nBetween positive and negative, the\nsentiment of this example\nis [label]. (random_word]\nThese random words are appended with the aim of distracting an unsupervised probe. Our cluster normalization method is able to remove these distractions (See Figure 2)."}, {"title": "4.1.2. TRAINING AND RESULTS", "content": "We train probes on each dataset with two partitions and random words, followed by normalization over the entire dataset as described in Farquhar et al. (2023). Subsequently, we train an additional set of probes for each setting using our cluster normalization method (see Section 3). We find probes trained using our method achieve a much higher accuracy on average, as shown in Table 1 and Figure 1."}, {"title": "4.2. Explicit Opinion", "content": "In this experiment, we examine how the inclusion of an explicit opinion from a fictional character, Alice, affects the accuracy of unsupervised probes."}, {"title": "4.2.1. DATASET", "content": "Following the second experiment in Farquhar et al. (2023), we create a dataset by inserting the opinion of a fictional character Alice into each prompt, with the adversarial aim of distracting an unsupervised probe into predicting the opinion of Alice. The sentiment of Alice is chosen randomly, and may or may not align with the ground truth label for a given sample - an example prompt is shown below:\nAlice is an expert at classifying\nthe sentiment of movie reviews.\nEveryone agrees with her,\nand they always copy her answers.\nConsider the following example: [review]\nAlice thinks it is [positive | negative].\nWhat do you think? I think it is [label].\nContrast pairs are constructed by setting [label] to either \"positive\" or \"negative\u201d."}, {"title": "4.2.2. TRAINING & RESULTS", "content": "Our results for CCS are shown in Figure 3. We find that a modified prompt including Alice's opinion causes the majority of our CCS probes to achieve random accuracy against ground truth labels, when normalizing over the entire dataset (Burns-Norm). Clustering before normalizing over each cluster addresses this issue - we see that the average accuracy is closer to that of the control setting, where the opinion of Alice is not inserted. (PCA visualizations analogous to those in Figure 2) are found in Appendix 14.) The reason for cluster normalization achieving higher accuracy is that our method removes the distracting feature of the opinion of Alice, enabling a CCS probe to more accurately"}, {"title": "4.3. Prompt Template Sensitivity", "content": "Farquhar et al. (2023) outline two key issues with current approaches to unsupervised probing for knowledge in language models. Thus far, we have primarily discussed the first of these issues: distracting salient features can satisfy the CCS loss, and trained probes may align with these unwanted features instead of the desired knowledge feature. Our approach of cluster normalization addresses this limitation, as demonstrated in the above experiments.\nThe second issue - frequently discussed in the eliciting latent knowledge literature (Christiano et al., 2021) - has to do with the quality of the desired knowledge feature itself, and how to discriminate between a model's true knowledge and that of a simulated agent. Prompting can induce a language model into a state of simulating a different quality of knowledge. For example, performance on difficult question-answering datasets can be improved by prompting language models to \"provide Professor Smith's answers to a question (Evans et al., 2021): it is implied that Professor Smith is an intelligent agent highly capable of answering such questions. In Farquhar et al. (2023), it is observed that current techniques for unsupervised probing are unable to detect a model's capability to answer a given question under such a prompt change.\nIn the interest of clearly defining the scope of our cluster normalization method, we carry out an additional experiment investigating prompt template sensitivity, and demonstrate that our method also does not give rise to uniform scores"}, {"title": "4.3.1. DATASETS", "content": "We use the CommonClaim (Casper et al., 2023) dataset, which assesses models' abilities to identify true or false statements, with many questions deliberately designed to trick the model through common misconceptions and traps.\nIn this experiment, the language model is presented with a question and possible answer, and asked to evaluate the truth of the answer. In order to evaluate prompt template sensitivity, we consider three settings:\n*   A default setting, where the model is simply asked to evaluate the truth of a possible answer to a given question.\n*   A professor setting, where the following text is prepended to each prompt, before asking the question: \"Professor Smith was given the following instructions:\n    Interpret each claim literally, and as a question about the real world; carefully research each claim, without falling prey to any common myths; and reply 'I have\n    no comment' unless you are completely certain of the answer. Here are the Professor's responses:\"\n*   A literal setting, where the first and last phrase of the above text are removed, to investigate whether the invocation of a specific character is important.\nAn example prompt in this set-up is shown below:\nProfessor Smith was given...\nQuestion: [question]\nAnswer: [answer]\nIs this answer true or false?\nContrast pairs are constructed by appending either \"True\" or \"False\" to each prompt."}, {"title": "4.3.2. TRAINING AND RESULTS", "content": "Each question is presented with a possible answer (our dataset contains an equal random split of true and false answers) under each of the three settings: default, literal, and professor. Three sets of unsupervised probes are then trained, one for each setting, and are scored on their ability to identify a feature corresponding to ground truth labels. We compare performance of normalizing over the entire dataset, as in Burns et al. (2022), to our cluster normalization approach.\nCCS probe accuracies are visualized in Figure 4. We see that in the default (blue) setting, the variance in probe accuracy is slightly higher than the literal (red) or professor (green) settings. Indeed, this difference is also clear when"}, {"title": "5. Related Work", "content": "It has been shown that language models develop internal representations of the world (Li et al., 2022), with individual concepts often encoded as linear directions in activation space (Elhage et al., 2022; Nanda et al., 2023; Burns et al., 2022; Marks & Tegmark, 2023). Language models can also output false information, even if the encoded knowledge in the activations seems to indicate a correct internal representation of the information (Evans et al., 2021; Azaria & Mitchell, 2023; Campbell et al., 2023). We seek to elicit this latent knowledge (Christiano et al., 2022) in an unsupervised manner. In recent years several methods have been proposed (Burns et al., 2022; Belrose et al., 2023; 2024; Zou et al., 2023; Li et al., 2024), although unsupervised methods can be subject to undesirable biases, as shown by Farquhar et al. (2023). They demonstrate that unsupervised probing techniques, such as those developed in Burns et al. (2022), often identify the most salient features in a dataset, as opposed to knowledge only. These features may not always align with the specific knowledge feature of interest, as described in Section 3. We provide theoretical explanations for some of these issues, and propose a method to eliminate them.\nThe work we cite in the introduction and background sections focuses on finding a general linear representation of knowledge in the latent space of a language model. While we focus on unsupervised approaches, most work concentrates on supervised ones (Christiano et al., 2022; Marks & Tegmark, 2023). This body of work is part of a more general field of research that aims at ensuring truthfulness of language models, by making sure that what they answer is actually what they believe or follows from reasoning e.g., working with quirky language models or using chain-of-thought reasoning (Turpin et al., 2023; Lyu et al., 2023; Radhakrishnan et al., 2023; Mallen & Belrose, 2023)."}, {"title": "6. Discussion and Conclusion", "content": "In this study, we address significant challenges associated with the unsupervised probing of knowledge in language models. The primary issue tackled is that of distracting salient features that can mislead the probing process. Our cluster normalization technique shows promising results in effectively isolating and minimizing the impact of such distractions, thereby enhancing the performance of unsupervised probes. Our results demonstrate that without proper normalization, probes tend to align with the most salient features present in the dataset, which are not necessarily related to the target knowledge feature. This observation mostly aligns with findings from previous studies (Farquhar et al., 2023), which showed that unsupervised probes are prone to capturing irrelevant features when such features are salient. However, in general, our results do not show as pronounced an effect as (Farquhar et al., 2023) suggested for the standard CCS method. This observation is especially true for the experiments detailed in Section 4.2 and Appendix A). Nonetheless, through cluster normalization, we provide a promising method to mitigate the issue of distracting salient features by identifying these features and ensuring that they are canceled out during the training of the probe. This normalization allows the probe to focus more accurately on the intended knowledge feature."}, {"title": "7. Limitations", "content": "Our study also highlights limitations of current probing techniques that are not addressed by our method. Specifically, as noted by Farquhar et al. (2023), we find that prompting techniques which can induce a language model into simulating a different quality of knowledge by simulating an agent can still affect our unsupervised probe performance. This is a critical limitation, as we specifically want to elicit the knowledge of the model, not that of some simulated entity. Addressing this limitation is another significant challenge for the research community, as it requires an investigation into the question of whether a language model's knowledge as its capacity to answer a given question under any prompt differs from simulated knowledge, and whether such a difference could be exploited to increase the reliability of probing algorithms. These limitations are studied in Mallen & Belrose (2023), where the context-dependence of knowledge probes is measured.\nAnother potential limitation of our method is that, as mentioned in Section 3, it relies on the fact that the mean of each pair of activations contains no information related to knowledge, which seems to be the case in practice but may need to be further investigated.\nFurther research is also needed to explore the effect of the choice of basis on probing algorithms, using e.g. the Local Interaction Basis developed by Bushnaq et al. (2024) or over-complete bases given by dictionary learning (Cunningham et al., 2023; Braun et al., 2024)."}, {"title": "A. Implicit Opinion", "content": "In this section, we discuss an additional experiment from Farquhar et al. (2023) that we are unable to replicate, achieving comparably high outcomes using both the original CCS method and our approach with Cluster-Norm.\nIn practical future use-cases of unsupervised probing, it is unlikely that distracting salient features will be present as explicitly as in Sections 4.1 and 4.2. In this experiment, the character of Alice is included and her opinion (acting as the distracting feature) is implied rather than explicitly stated. In Farquhar et al. (2023) it is demonstrated that this distracting feature is frequently selected by CCS probes. We investigate the use of cluster normalization to address this issue."}, {"title": "A.1. Dataset", "content": "We use the DBpedia topic classification dataset (Lehmann et al., 2015) to construct a task: the language model must identify the correct topic of a given text sample, given two answer choices. An example prompt from this task is shown below:\nAlice answers questions about the\ntopic of text passages.\nExample: [text sample]\nChoice 1: [choice 1]\nChoice 2: [choice 2]\nAlice says \"Choice [choice]\nContrast pairs are constructed by changing the value of [choice] to 1 or 2.\nIn order to convey Alice's implied opinion, few shot examples are included in the prompt, in a biased and unbiased (as a control) manner. In the latter case, the correct choice is always given in few shot examples. In the former, Alice is shown to always choose incorrectly when the correct label is \u201ccompany\u201d. A specific few shot example used in this biased setting is shown below:\nExample: Thai Beverage is a large\ndrinks company in Asia.\nChoice 1: album\nChoice 2: company\nAlice says \"Choice 1\". Even though\nthe text was about a company, Alice\ngave the other answer to avoid\nsaying company. Alice always says\nthe wrong answer when the topic of\nthe text is company, because she\ndoesn't like capitalism."}, {"title": "A.2. Results", "content": "Under this experimental setting, should Alice's biased implicit opinion act as a distracting feature for a CCS probe, we would notice a drop in probe accuracy for the correct answer in the biased setting, specifically on questions with the correct answer \"company\". For further details on this experiment, including the exact few-shot prompts used, see Farquhar et al. (2023).\nThe question still remains as to the reason for the differing results here, when compared to those in Farquhar et al. (2023). We believe the most likely reason is model size: while we report results using Mistral-7B, Farquhar et al. (2023) make use of Chinchilla 70B: a much larger model. The PCA visualizations in Figure 6 show that at our model size, the feature of Alice's biased opinion is not salient i.e., it is not represented by the model as cleanly as the \"correct choice\" feature, and it is for this reason that our CCS probes never select the implicit opinion feature. Regardless, this results in an inability to compare the original CCS method with cluster normalization."}, {"title": "B. Additional Results", "content": "In addition to Mistral-7B, the random word experiment and explicit opinion experiment is repeated for the following models: Gemma-7b, Phi-2 and 3, Llama 3 8B and Pythia-6.9b-v0. We harvested activations at four different points: the 25th percentile layer, 50th, 75th and the last layer. Unbiased examples correspond to probes trained on the original prompts, while biased examples correspond to probes trained on the modified prompts."}, {"title": "B.1. Random Words", "content": "The results for the additional models and layers are comparable to those of Mistral-7b at the 75th percentile layer. Figure 7 shows the average results across all models, including Gemma-7b, Phi-2 and 3, Llama 3 8B, Pythia-6.9b-v0, and Mistral-7b. For the biased prompt-template dataset, unsupervised methods using Cluster-Normalization do usually perform better than those using the standard Burns-Normalization. Results can be found in figures 8, 9, 10, 11, 12 and 13."}, {"title": "B.2. Explicit Opinion", "content": "The results for the additional models and layers are comparable to those of Mistral-7b at the 75th percentile layer. Figure 15 shows the average results across all models, including Gemma-7b, Phi-2 and 3, Llama 3 8B, Pythia-6.9b-v0, and Mistral-7b. For the unbiased and especially the biased prompt-template dataset, unsupervised methods using Cluster-Normalization tend to outperform those that use standard Burns-Normalization. However, the performance gap between these two methods is smaller compared to the random word experiment. Moreover, in our work, the standard CCS using Burns-Normalization appears to perform better on the biased dataset than reported by Farquhar et al. (2023) for the different models and layers. The individual results for the various layers and models are shown in the figures: 17, 18, 19, 20 and 21."}, {"title": "B.2.1. VIOLIN PLOTS 75TH PERCENTILE LAYER", "content": "Additional violin plots are displayed in the following figures: Llama-3-8b in Figure 23, Phi-3 in Figure 24, Phi-2 in Figure 25, Gemma-7b in Figure 26, and Pythia-6.9b in Figure 27."}, {"title": "B.3. Prompt Template Sensitivity", "content": "In Farquhar et al. (2023) an analogous experiment investigation prompt template sensitivity is performed using the TruthfulQA (Evans et al., 2021) dataset. After a manual inspection of this dataset we feel the inclusion of numerous ambiguous questions casts doubt on experimental results, and for this reason we perform the experiments in Section 4.3 using the CommonClaim (Casper et al., 2023) dataset instead. Here, we repeat these experiments using TruthfulQA to allow for a direct comparison to the results in Farquhar et al. (2023).\nAnalogous results to those in Figure 4 when performed instead on the TruthfulQA dataset are shown in Figure 34. We note a high variance in probe accuracy in all settings, and therefore feel these experimental results do not lead to any clear conclusions.\nWe thoroughly verify these results by repeating these experiments when harvest contrast pair activations at the 25th percentile, 50th percentile, and last layer for Mistral-7B, as well as two additional models: Llama-3-8B and Phi-2. These results are visualized in Figures 28 to 30.\nWe additionally repeat these layer-by-layer experiments, again using the same two additional models, for the experiments outlined in Section 4.3 using the CommonClaim dataset. Results are visualized in Figures 31 to 33.\nWe present results on TruthfulQA for Mistral-7B in figure 34, following the exact same procedure as in the main body for CommonClaim (Figure 4). We then show a PCA visualization of contrast differences for CommonClaim in figure 35."}]}