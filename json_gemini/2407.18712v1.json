{"title": "Cluster-Norm for Unsupervised Probing of Knowledge", "authors": ["Walter Laurito", "Sharan Maiya", "Gr\u00e9goire Dhimoila", "Owen (Ho Wan) Yeung", "Kaarel H\u00e4nni"], "abstract": "The deployment of language models brings challenges in generating reliable information, especially when these models are fine-tuned using human preferences. To extract encoded knowledge without (potentially) biased human labels, unsupervised probing techniques like Contrast-Consistent Search (CCS) have been developed (Burns et al., 2022). However, salient but unrelated features in a given dataset can mislead these probes (Farquhar et al., 2023). Addressing this, we propose a cluster normalization method to minimize the impact of such features by clustering and normalizing activations of contrast pairs before applying unsupervised probing techniques. While this approach does not address the issue of differentiating between knowledge in general and simulated knowledge-a major issue in the literature of latent knowledge elicitation (Christiano et al., 2021)-it significantly improves the ability of unsupervised probes to identify the intended knowledge amidst distractions.", "sections": [{"title": "1. Introduction", "content": "The deployment of language models for practical applications introduces novel challenges, including the potential creation of untrustworthy or incorrect text (Weidinger et al., 2021; Park et al., 2023; Evans et al., 2021; Hendrycks et al., 2021). Specifically, models that are fine-tuned using human preferences may amplify existing human biases or generate persuasive yet deceptive outputs (Perez et al., 2022).\nEmpirical evidence suggests that simulated internal beliefs or knowledge can be extracted from language model activations (Li et al., 2022; Gurnee & Tegmark, 2023; Azaria & Mitchell, 2023; Bubeck et al., 2023). Supervised probing methods can be employed to extract this knowledge (Alain & Bengio, 2016; Marks & Tegmark, 2023) but such methods require labels, which in some domains may not be provided correctly due to human biases or because humans simply do not know the correct label. It may even be critical to avoid the use of human labels to differentiate between a model's true knowledge and its representation of human knowledge. Motivated by these ideas, unsupervised probing techniques like Contrast-Consistent Search (CCS) have been developed to extract the knowledge embedded in a language model without the need for ground truth labels (Zou et al., 2023; Burns et al., 2022).\nFarquhar et al. (2023) outline current limitations of these approaches, demonstrating that these unsupervised probes tend to identify the most salient binary feature, which may not always correspond to the specific knowledge feature we seek. For example, in one experiment, one of a pair of distracting random words is added to each prompt in a text dataset. After training, unsupervised CCS probes function as classifiers for these random words, rather than the intended knowledge feature of the text. In practice, there may be numerous salient features of which we are unaware, which can divert an unsupervised probe from identifying the target feature F, regardless of whether they are correlated or uncorrelated with F.\nTo tackle this issue, we propose a cluster normalization method. Our method follows the usual initial approach of unsupervised probing of harvesting contrast pair activations, however we then cluster similar activations and normalize them separately, thereby eliminating the effect of distracting salient features. We can then apply any unsupervised probing method, such as CCS or CRC-TPC (Burns et al., 2022), to train a probe on these normalized activations. It is of course crucial to ensure this approach does not inadvertently eliminate the knowledge feature itself. To prevent this, we utilize contrast pairs, performing the clustering based on the average embedding of each pair. Further details on contrast pairs are provided in Section 2.1.\nProbes trained with the original CCS approach achieve an average accuracy of approximately 0.5 on prompt datasets with distracting random word features (Farquhar et al., 2023). In contrast, our clustering method significantly improves this average accuracy to about 0.77, and to 0.81 for CRC-TPC"}, {"title": "2. Background", "content": "2.1. Contrast-Consistent Search (CCS)\nContrast-Consistent Search (CCS), as described by Burns et al. (2022), locates a direction in activation space using a perceptron that adheres to logical consistency principles. This is achieved through a loss function designed to ensure that probabilities for a question-answer pair and its negated counterpart - a contrast pair are complementary. This loss function is optimized in an unsupervised manner, and in doing so CCS extracts the latent knowledge within large language models to answer binary questions.\nAt first, a language model M processes a dataset of textual contrast pairs $(x_i^+, x_i^-)_{i=1}^N$, generating contextualized embeddings $(M(x_i^+), M(x_i^-))$. Following this, a linear probe (Alain & Bengio, 2016) is trained to calculate from these embeddings the probabilities $p^+$ and $p^-$, whether $x^+$ or $x^-$ is true, respectively. The objective function used to train this probe is given by a sum of two terms:\n$\\mathcal{L}_{CCS} = \\sum_{i=1}^N \\mathcal{L}_{consistency} + \\mathcal{L}_{confidence}$\n$\\mathcal{L}_{consistency} = [p(x_i^+) - (1 - p(x_i^-))]^2$\n$\\mathcal{L}_{confidence} = min \\{p(x_i^+), p(x_i^-)\\} ^2$.\nThe first term, $\\mathcal{L}_{consistency}$, is motivated by the idea that the probabilities of a statement and its negation should sum to one. This ensures logical consistency. The second term, $\\mathcal{L}_{confidence}$, is designed to maximize the information extracted by the probe, penalizing cases where the probabilities for both true and false are the same, at $p(x^+) = p(x^-) = 0.5$. Thus, this term encourages the probe to be more certain in its outputs.\nIntuitively, there are at least two possible directions (features) satisfying this loss. The first is the knowledge direction we seek, and the second is the syntactical difference between positive and negative prompt templates. To remove this latter undesired feature, Burns et al. (2022) proceed as follows. Before training an unsupervised probe, contrast pair activations are first normalized: $M(x_i^+) = \\frac{M(x_i^+) - \\mu^+}{\\sigma^+}$, with $\\mu^+$ and $\\sigma^+$ the mean and standard deviation of the activations of all positive examples in each contrast pair; the same normalization procedure is followed for negative examples, and the unsupervised probe is trained on these normalized $M(x_i^-)$. In this way, CCS removes the most salient feature of the contrast pair differences: the syntactical difference direction $F_+$. However, as Farquhar et al. (2023) show, the second-most salient feature may not necessarily be the desired knowledge - an implicit assumption of the original CCS method. In this work, we take advantage of normalization to remove other undesired salient features by including a clustering step.\n2.2. Contrastive Representation Clustering\nAs an alternative to CCS, the method of Contrastive Representation Clustering via Top Principal Component (CRC-TPC) (Burns et al., 2022) separates the normalized contrast pair differences $\\{M(x_i^+) - M(x_i^-)\\}$ based on projections onto their top principal component, i.e., the singular vector associated with the highest singular value, or the direction with the highest variance. This is again motivated by the intuition that the second-most salient contrastive feature after $F_+$ - removed by normalization - should be the knowledge feature $F_{T/1}$."}, {"title": "2.3. Theoretical Background", "content": "A salient feature is a direction with high variance in the data. We are interested in salient features in the contrast pair differences $M(x_i^+) - M(x_i^-)$, and we refer to these as contrastive features.\nIn this section, we explain (1) why undesired salient contrastive features can mislead unsupervised probes, and (2) how contrastive features can be induced by non-contrastive ones. The mechanisms of the latter point are illustrated through an example.\nWe shall first examine why there is a close link between the CCS loss described in Section 2.1 and the idea of saliency i.e., variance. Contrastive features will naturally achieve a low CCS loss. To see this, consider the variance of contrast pair differences projected along the feature direction of a given feature $F$:\n$X := F^T \\cdot M(x^+), Y := F^T \\cdot M(x^-)$.\n$Var(X - Y) = E(X^2) + E(Y^2) -2 \\cdot E(X \\cdot Y)$\n$\\qquad\\qquad\\qquad\\qquad\\qquad -(E(X)^2 + E(Y)^2 - 2 \\cdot E(X) \\cdot E(Y))$\n$\\qquad = 0$\nIn this expanded form, we see that the variance of contrast pair differences in the direction $F$ captures,\n*   confidence, with $E(X^2) + E(Y^2)$ higher if the magnitude projection along $F$ in either element of a pair is high,\n*   and consistency, with $-2 \\cdot E(X \\cdot Y) > 0$ if the projections of a contrast pair along $F$ have opposing sign. This also increases with the magnitude of these projections.\nNote that the term $-(E(X)^2 + E(Y)^2 - 2 \\cdot E(X) \\cdot E(Y))$ equals zero under the set-up of CCS, as the normalization step described above results in $E(M(x_i^+)) = 0$, therefore $E(X) = E(Y) = 0$.\nDue to this link between confidence, consistency, and saliency, a probe trained using the CCS loss will favor learning salient contrastive features. Otherwise, the projections of a contrast pair onto a feature will be small in difference or equal, failing to satisfy at least the consistency condition.\nAs mentioned in Section 2.1, we can describe two features which intuitively will satisfy the CCS loss:\n*   $F_+ := F_+ - F_-$, the syntactical difference between contrast pairs due to the appending of positive and negative tokens, removed by normalization in the original CCS method,\n*   $F_{T/1} := F_{T/1} - F_1$, the knowledge feature we seek.\nUnder our definition, undesired distracting features such as proxies for knowledge or random words (as in Farquhar et al. (2023)) should not be contrastive features: their contrast pair projections should be equal in both examples of each pair and thus should be ignored by a CCS probe. It is however the case that these non-contrastive features can still mislead unsupervised probes by inducing undesired contrastive features. We describe the mechanism through which this occurs with the following example:\nLet $f$ be some binary function, say the $XOR$ function on the presence of features, and $F_1, F_2$ be any two features. Suppose that a model represents the feature $f(F_1, F_2)$ as its own direction $F_{f(F_1,F_2)}$, orthogonal to $F_{1,2}$. Now, suppose both $F_1$ and $F_2$ are distracting features and assume without loss of generality that exactly one of them appears in each pair with probability $\\frac{1}{2}$.\nWe can now write the contrast pair differences as:\n$M(x^+) - M(x^-) = F_+ - F_- + F_{f(F_+,F_j)} - F_{f(F_-,F_j)}$\n$\\qquad = F_+ + (F_+ - F_-) \\pm (F_{f(F_T,F_j)} - F_{f(F_1,F_j)})$\n$\\qquad$ for $j \\in \\{1, 2\\}$.\nThe expected value of these contrast pair differences over our dataset is:\n$E(M(x^+) - M(x^-)) = F_+ + \\frac{1}{2} (\\Delta_1 + \\Delta_2)$  (1)\nsince $F_+$ is constant, $\\Delta_{F_j}$ are both constant on half of the dataset and the two knowledge related terms have uniformly alternating sign. After centering, we have:\n$\\widetilde{M(x^+)} - \\widetilde{M(x^-)} = \\pm \\frac{1}{2} \\Delta_{F_j}$ $+ (\\Delta_1 - \\Delta_2)$\nwhere $\\alpha = 1$ if $j = 1$ and $-1$ otherwise.\nA probe using any of these remaining terms will have low CCS loss, with a bias towards the most salient terms. This is true for any undesirable feature that would remain in the contrast differences, and it affects both trained CCS probes and analytical CRC-TPC probes. This work aims to address this issue by removing unwanted features before training the probe."}, {"title": "3. Method", "content": "We begin with a dataset of contrast pairs, $\\{(x_i^+, x_i^-)\\}_{i=1}^N$. For each pair, we harvest the intermediate activations of a language model M, specifically the state of the residual stream at the final token position at a specific layer, which we denote as $M(x_i)$. We average these activations for each contrast pair $\\overline{M(x_i)} = \\frac{M(x_i^+) + M(x_i^-)}{2}$, and partition $\\{\\overline{M(x_i)}\\}_i$ using a clustering algorithm, thereby partitioning the original dataset using its most salient features. Each cluster is then normalized separately to have zero mean and unit variance, i.e. for each positive sample $x_i^+$, where $\\overline{M(x_i)}$ belongs to cluster $c$, $M(x_i^+) = \\frac{M(x_i^+) - \\mu_c^+}{\\sigma_c^+}$, where $\\mu_c^+$ and $\\sigma_c^+$ are the mean and standard deviation of all positive samples in cluster c. The same normalization process is applied to all negative samples. Finally, an unsupervised probe can be trained on the contrast pair differences of the normalized (by cluster) samples. This approach allows the probe to isolate the desired knowledge feature, ignoring other distracting features isolated to each original cluster.\nFollowing the notation in Section 2.3, if $x_i$ belongs to cluster $c \\in \\{0, 1\\}$, a successful cluster normalization will leave:\n$\\widetilde{M(x_i^+)} - \\widetilde{M(x_i^-)} = F_{T/1} \\pm \\Delta$.\nThis follows from equation 1, however in this case normalization is performed over c only as opposed to the whole dataset.\nA key element to the effectiveness of our method is that our clustering approach does not erase the effect of the desired knowledge feature. This is achieved by clustering the averages of each contrast pair, $\\overline{M(x_i)}$. As a result, clustering only isolates salient non-contrastive features, and is effectively blind to $F_+$ and $F_{T/1}$. Normalizing positive and negative samples separately per cluster aims to ensure that all contrastive features $F'$ related to $F_+$ are properly normalized out - including the leaks from non-contrastive features $F$ mixing with $F_+$, as explained in Section 2.3. Note, we do not normalize out similar $F'$ resulting from the mixing of $F$ with $F_{T/1}$. Eventually, only contrastive features related to knowledge are kept."}, {"title": "4. Experiments", "content": "In our experiments, we utilize Mistral-7B as our main language model, harvesting activations (using the libraries from (Wolf et al., 2020) and (Nanda & Bloom, 2022)) at the 75th percentile layer (layer 24 for Mistral-7b) since, from preliminary experiments, we find probes achieve higher accuracies using the 50th to 90th percentile layers. We also report results using different language models (Phi-2 and 3, Gemma-7b, Llama-3-8B, Pythia-6.9b) and layers in the Appendices to verify the efficacy of our method. Our experiments follow the same general approach as those reported in Farquhar et al. (2023), as each of these original experiments set out to demonstrate the limitations of current unsupervised probing techniques.\nWe present results for three experiments below. For the first and second, we create prompt datasets based on the IMDb dataset (Jiang et al., 2023; Maas et al., 2011), while for the third we use the CommonClaim (Casper et al., 2023) dataset. We report results on a fourth experiment utilizing the DBpedia dataset (Lehmann et al., 2015) in Appendix A; this experiment follows on from results reported in Farquhar et al. (2023), however, we find we are unable to replicate these results (on three different models) and instead obtain high accuracies for both the original method of CCS and our approach using cluster normalization.\nActivation clustering is performed using HDBScan, implemented in the scikit-learn library (Kramer & Kramer, 2016), setting a minimum number of elements in each cluster to 5 and using the Euclidean distance metric. One advantage of HDBScan over other clustering algorithms (e.g., k-means) is that the number of clusters does not need to be specified in advance. In order to examine the variance in probe performance, we report summary statistics of 50 probe fits in each experiment, and visualize the results from all.\nThe following experiments generally involve a comparison between an original prompt and a modified one, to attempt to induce a bias in an unsupervised probe. Hereon, we refer to these original prompts as unbiased and modified prompts as biased. We also refer to normalization over an entire dataset, as Burns normalization or Burns-Norm (See 2.3 for more details). We refer to our alternative approach through clustering as cluster normalization or Cluster-Norm. Unlike the approach in (Burns et al., 2022), where multiple prompt templates were used, the study in (Farquhar et al., 2023) utilized only one prompt template per dataset. Our method follows the prompt-template setup from (Farquhar et al., 2023).\nEach experiment utilizes a train-test split of 70% for training and 30% for testing. Importantly, we evaluate our unsupervised probes on a test set where Burns-Norm is applied to the test set as it was done in (Burns et al., 2022), and not our cluster normalization. This is because we want probes to generalize, so if during evaluation they are fed with a contrast pair that belongs to an entirely different dataset, it is out of distribution for the clusters found during training. The probe should be a feature in the unaltered latent space. Although we do not use cluster norm on the test set for the aforementioned reason, we do use Burns-Norm for being able to compare our results with Burns et al. (2022) as this is what they do for the test set. Farquhar et al. (2023) likely follow a similar approach, as they mention utilizing normal-"}, {"title": "4.1. Random Words", "content": "In this experiment, we induce a strong syntactical bias in the data to illustrate the problem of distracting salient features and demonstrate the necessity of our method for removing them.\n4.1.1. DATASET\nFollowing the approach of Farquhar et al. (2023), we create a dataset by appending a random word to half of our prompts and a different random word to the remaining half. The following is an example of a prompt in a given dataset, where [label] can be positive or negative and [random_word] is a random word from the NLTK corpus (Bird, 2006). For each data point we have a different movie review ([review], e.g. \"This is my favorite movie ...\"):\nConsider the following example: [review],\nBetween positive and negative, the\nsentiment of this example\nis [label]. [random_word]\nThese random words are appended with the aim of distracting an unsupervised probe. Our cluster normalization method is able to remove these distractions (See Figure 2).\n4.1.2. TRAINING AND RESULTS\nWe train probes on each dataset with two partitions and random words, followed by normalization over the entire dataset as described in Farquhar et al. (2023). Subsequently, we train an additional set of probes for each setting using our cluster normalization method (see Section 3). We find probes trained using our method achieve a much higher accuracy on average, as shown in Table 1 and Figure 1."}, {"title": "4.2. Explicit Opinion", "content": "In this experiment, we examine how the inclusion of an explicit opinion from a fictional character, Alice, affects the accuracy of unsupervised probes.\n4.2.1. DATASET\nFollowing the second experiment in Farquhar et al. (2023), we create a dataset by inserting the opinion of a fictional character Alice into each prompt, with the adversarial aim of distracting an unsupervised probe into predicting the opinion of Alice. The sentiment of Alice is chosen randomly, and may or may not align with the ground truth label for a given sample - an example prompt is shown below:\nAlice is an expert at classifying\nthe sentiment of movie reviews.\nEveryone agrees with her,\nand they always copy her answers.\nConsider the following example: [review]\nAlice thinks it is [positive | negative].\nWhat do you think? I think it is [label].\nContrast pairs are constructed by setting [label] to either \"positive\" or \"negative\u201d.\n4.2.2. TRAINING & RESULTS\nOur results for CCS are shown in Figure 3. We find that a modified prompt including Alice's opinion causes the majority of our CCS probes to achieve random accuracy against ground truth labels, when normalizing over the entire dataset (Burns-Norm). Clustering before normalizing over each cluster addresses this issue - we see that the average accuracy is closer to that of the control setting, where the opinion of Alice is not inserted. (PCA visualizations analogous to those in Figure 2) are found in Appendix 14.) The reason for cluster normalization achieving higher accuracy is that our method removes the distracting feature of the"}, {"title": "4.3. Prompt Template Sensitivity", "content": "Farquhar et al. (2023) outline two key issues with current approaches to unsupervised probing for knowledge in language models. Thus far, we have primarily discussed the first of these issues: distracting salient features can satisfy the CCS loss, and trained probes may align with these unwanted features instead of the desired knowledge feature. Our approach of cluster normalization addresses this limitation, as demonstrated in the above experiments.\nThe second issue - frequently discussed in the eliciting latent knowledge literature (Christiano et al., 2021) - has to do with the quality of the desired knowledge feature itself, and how to discriminate between a model's true knowledge and that of a simulated agent. Prompting can induce a language model into a state of simulating a different quality of knowledge. For example, performance on difficult question-answering datasets can be improved by prompting language models to \"provide Professor Smith's answers to a question (Evans et al., 2021): it is implied that Professor Smith is an intelligent agent highly capable of answering such questions. In Farquhar et al. (2023), it is observed that current techniques for unsupervised probing are unable to detect a model's capability to answer a given question under such a prompt change.\nIn the interest of clearly defining the scope of our cluster normalization method, we carry out an additional experiment investigating prompt template sensitivity, and demonstrate that our method also does not give rise to uniform scores across different prompts. Therefore, our method does not address this second issue.\n4.3.1. DATASETS\nWe use the CommonClaim (Casper et al., 2023) dataset, which assesses models' abilities to identify true or false statements, with many questions deliberately designed to trick the model through common misconceptions and traps.\nIn this experiment, the language model is presented with a question and possible answer, and asked to evaluate the truth of the answer. In order to evaluate prompt template sensitivity, we consider three settings:\n*   A default setting, where the model is simply asked to evaluate the truth of a possible answer to a given question.\n*   A professor setting, where the following text is prepended to each prompt, before asking the question: \"Professor Smith was given the following instructions: Interpret each claim literally, and as a question about the real world; carefully research each claim, without falling prey to any common myths; and reply 'I have no comment' unless you are completely certain of the answer. Here are the Professor's responses:\"\n*   A literal setting, where the first and last phrase of the above text are removed, to investigate whether the invocation of a specific character is important.\nAn example prompt in this set-up is shown below:\nProfessor Smith was given...\nQuestion: [question]\nAnswer: [answer]\nIs this answer true or false?\nContrast pairs are constructed by appending either \"True\" or \"False\" to each prompt.\n4.3.2. TRAINING AND RESULTS\nEach question is presented with a possible answer (our dataset contains an equal random split of true and false answers) under each of the three settings: default, literal, and professor. Three sets of unsupervised probes are then trained, one for each setting, and are scored on their ability to identify a feature corresponding to ground truth labels. We compare performance of normalizing over the entire dataset, as in Burns et al. (2022), to our cluster normalization approach.\nCCS probe accuracies are visualized in Figure 4. We see that in the default (blue) setting, the variance in probe accuracy is slightly higher than the literal (red) or professor (green) settings. Indeed, this difference is also clear when"}, {"title": "5. Related Work", "content": "It has been shown that language models develop internal representations of the world (Li et al., 2022), with individual concepts often encoded as linear directions in activation space (Elhage et al., 2022; Nanda et al., 2023; Burns et al., 2022; Marks & Tegmark, 2023). Language models can also output false information, even if the encoded knowledge in the activations seems to indicate a correct internal representation of the information (Evans et al., 2021; Azaria & Mitchell, 2023; Campbell et al., 2023). We seek to elicit this latent knowledge (Christiano et al., 2022) in an unsupervised manner. In recent years several methods have been proposed (Burns et al., 2022; Belrose et al., 2023; 2024; Zou et al., 2023; Li et al., 2024), although unsupervised methods can be subject to undesirable biases, as shown by Farquhar et al. (2023). They demonstrate that unsupervised probing techniques, such as those developed in Burns et al. (2022), often identify the most salient features in a dataset, as opposed to knowledge only. These features may not always align with the specific knowledge feature of interest, as described in Section 3. We provide theoretical explanations for some of these issues, and propose a method to eliminate them.\nThe work we cite in the introduction and background sections focuses on finding a general linear representation of knowledge in the latent space of a language model. While we focus on unsupervised approaches, most work concentrates on supervised ones (Christiano et al., 2022; Marks & Tegmark, 2023). This body of work is part of a more general field of research that aims at ensuring truthfulness of language models, by making sure that what they answer is actually what they believe or follows from reasoning e.g., working with quirky language models or using chain-of-thought reasoning (Turpin et al., 2023; Lyu et al., 2023; Radhakrishnan et al., 2023; Mallen & Belrose, 2023)."}, {"title": "6. Discussion and Conclusion", "content": "In this study, we address significant challenges associated with the unsupervised probing of knowledge in language models. The primary issue tackled is that of distracting salient features that can mislead the probing process. Our cluster normalization technique shows promising results in effectively isolating and minimizing the impact of such distractions, thereby enhancing the performance of unsupervised probes. Our results demonstrate that without proper normalization, probes tend to align with the most salient features present in the dataset, which are not necessarily related to the target knowledge feature. This observation mostly aligns with findings from previous studies (Farquhar et al., 2023), which showed that unsupervised probes are prone to capturing irrelevant features when such features are salient. However, in general, our results do not show as pronounced an effect as (Farquhar et al., 2023) suggested for the standard CCS method. This observation is especially true for the experiments detailed in Section 4.2 and Appendix A). Nonetheless, through cluster normalization, we provide a promising method to mitigate the issue of distracting salient features by identifying these features and ensuring that they are canceled out during the training of the probe. This normalization allows the probe to focus more accurately on the intended knowledge feature.\n7. Limitations\nOur study also highlights limitations of current probing techniques that are not addressed by our method. Specifically, as noted by Farquhar et al. (2023), we find that prompting techniques which can induce a language model into simulating a different quality of knowledge by simulating an agent can still affect our unsupervised probe performance. This is a critical limitation, as we specifically want to elicit the knowledge of the model, not that of some simulated entity. Addressing this limitation is another significant challenge for the research community, as it requires an investigation into the question of whether a language model's knowledge as its capacity to answer a given question under any prompt differs from simulated knowledge, and whether such a difference could be exploited to increase the reliability of probing algorithms. These limitations are studied in Mallen & Belrose (2023), where the context-dependence of knowledge probes is measured.\nAnother potential limitation of our method is that, as mentioned in Section 3, it relies on the fact that the mean of each pair of activations contains no information related to knowledge, which seems to be the case in practice but may need to be further investigated.\nFurther research is also needed to explore the effect of the choice of basis on probing algorithms, using e.g. the Local Interaction Basis developed by Bushnaq et al. (2024) or over-complete bases given by dictionary learning (Cunningham et al., 2023; Braun et al., 2024)."}]}