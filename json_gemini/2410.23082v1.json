{"title": "An Event-Based Digital Compute-In-Memory Accelerator with Flexible Operand Resolution and Layer-Wise Weight/Output Stationarity", "authors": ["Nicolas Chauvaux", "Adrian Kneip", "Christoph Posch", "Kofi Makinwa", "Charlotte Frenkel"], "abstract": "Compute-in-memory (CIM) accelerators for spiking neural networks (SNNs) are promising solutions to enable \u00b5s-level inference latency and ultra-low energy in edge vision applications. Yet, their current lack of flexibility at both the circuit and system levels prevents their deployment in a wide range of real-life scenarios. In this work, we propose a novel digital CIM macro that supports arbitrary operand resolution and shape within a unified CIM storage for weights and membrane potentials. These circuit-level techniques enable a hybrid weight- and output-stationary dataflow at the system level to maximize operand reuse, thereby minimizing costly on- and off-chip data movements during the SNN execution. Measurement results of a fabricated FlexSpIM prototype in 40-nm CMOS demonstrate a 2x increase in 1-bit-normalized energy efficiency compared to prior fixed-precision digital CIM-SNNs, while providing resolution reconfiguration with bitwise granularity. Our approach can save up to 90% energy in large-scale systems, while reaching a state-of-the-art classification accuracy of 95.8% on the IBM DVS gesture dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Deploying convolutional neural networks (CNNs) to enable vision at the edge has unlocked applications ranging from user recognition to decentralized object detection and classification. Recently, event-based vision has emerged as a promising opportunity for reduced system-level energy and latency [1]. Indeed, pixels of event-based cameras fire events independently, generating sparse event streams at a \u00b5s-level temporal resolution (Fig. 1(a)), which calls for dedicated algorithms [2]. Among them, spiking neural networks (SNNs) exploit bio-inspired neuron models (e.g., integrate-and-fire (IF) in Fig. 1(b)) to sparsely process information using binary spikes: they rely on an internal state called membrane potential to retain the evolution of information across time. While SNNs are well suited to map per-timestep processing scenarios for low-latency decisions at the edge (Fig. 1(c)), large-scale models that cannot fit entirely in the on-chip memory suffer from significant data movement overheads, which prevents their efficient deployment on conventional hardware architectures.\nTo that end, compute-in-memory (CIM) hardware for SNNs has recently been proposed [3]-[12], but their limited reconfigurability makes them fundamentally unsuited to the large diversity of CNN layer specifications (Fig. 1(a)). This either implies sub-optimal workload mapping to CIM hardware, leading to latency and energy penalties, or increased time-to-market due to the need for application-specific CIM hardware.\nIn this work, we propose FlexSpIM, a digital CIM-based accelerator for SNN inference with high flexibility at the circuit and system levels. It solves three core challenges compared to prior CIM-based SNNs:\n1) while previous works only support a fixed resolution or a few pre-defined options, FlexSpIM supports a fully reconfigurable resolution for weights and membrane potentials, thereby expanding the exploration of the trade-off landscape between accuracy, energy efficiency, and memory footprint for SNN workloads (Fig. 1(d));\n2) while operand mapping is usually restricted to either fully bit-serial row-wise or fully bit-parallel column-wise, FlexSpIM allows for reconfigurable operand shapes to support different, non-proportional resolution values for weights and membrane potentials, which were otherwise constrained to fixed ratios [3] (Fig. 1(e));\n3) while maximizing operand stationarity is key to reduce"}, {"title": "II. PROPOSED RECONFIGURABLE DIGITAL CIM MACRO", "content": "SRAM-based CIM is a computing paradigm where operations are carried out directly inside the SRAM, harnessing a low-cost reuse of the data stored in the array. In the case of boolean digital CIM-SRAMs [14], multiple bitwise operations can be performed in parallel on the different vertical bitlines (BLs) by enabling two shared horizontal wordlines (WLs) at a time (Figs. 2(a) and (b)). Applied to SNNs, these architectures can execute the XNOR-and-accumulate operation of IF neurons (Fig. 1(b)) by sequentially accessing corresponding bits of a weight A and a membrane potential B, and updating the potential accordingly [3]. In this work, we split this operation in five phases (Fig. 2(c)): 1) precharge of BL/complementary BL (BLB) to VDD, 2) AND/NOR operation between the stored A and B operand values on BL/BLB by activating the corresponding WLs, 3) 1-bit sum and carry-out generation in the peripheral circuit (PC) yielding the updated membrane potential, 4) half-select-prevention BL/BLB precharge, and 5) write-back of the new membrane potential bit into A. These steps are repeated until all bits of the membrane potential and weight have been processed. Then, a comparison between the resulting membrane potential and a threshold conditionally generates an output spike, transferred to the next layer.\nReconfigurability in the FlexSpIM macro (Fig. 2(d)) is achieved by combining a dense 6T SRAM array storing both the weights and membrane potentials with a modular PC per column (Fig. 2(e)). Two additional control bitcells define the per-PC state, while emulation bits (EBs) allow for sign-bit extension and write-free CIM operation during data broadcasting in the macro. Each pitch-matched PC consists of a dual sense amplifier (SA) for the individual readout of BL/BLB, a 1-bit full-adder adapted from [14] with a comparison and a carry-in selection circuits, and logic for I/O communication."}, {"title": "A. Arbitrary Operand Resolution and Shape", "content": "FlexSpIM leverages arbitrary operand resolutions (i.e., 1-to-512\u00d7256-bit with bitwise granularity), which can be selected on a per-layer basis for both weights and membrane potentials (Fig. 3(a)). This degree of flexibility overcomes the limited set of resolutions supported in previous works [3]-[12], thereby preventing any waste of storage space. To support weight and membrane potential operands that may take different and non-proportional resolutions, the carry-select circuit (Fig. 2(e)) allows FlexSpIM to map operand bits using any shape in the unified 6T SRAM array (Figs. 3(b) and (c)). The number of columns occupied by each multi-bit operand is defined using the 2-bit control bitcells, which allow chaining multiple 1-bit adders of neighboring PCs for multi-bit computation by changing the carry-in origin (Fig. 3(d)). The multi-bit CIM operation then occurs in parallel over the columns, and sequentially from the LSB row to the MSB row (Fig. 3(e)). For"}, {"title": "B. Hybrid-Stationary Dataflow", "content": "At the system level, the unified memory of FlexSpIM allows to support hybrid stationarity (HS) by choosing between WS and OS on a per-layer basis, contrary to previous WS-only CIM designs for SNNs [3]\u2013[6], [9]\u2013[12]. To illustrate this point, we consider a typical spiking CNN workload (SCNN), made of six convolution layers (defined in Fig. 4(a)) followed by three fully-connected (FC) ones (not shown). Exploiting the known memory requirements of both weight and membrane potential operands in each of the SCNN's layers, the HS flow selects each layer's dataflow type in order to maximize the overall utilization of the CIM storage space (Figs. 4(a) and (b)), thereby increasing the overall operand stationarity across the multi-timestep execution of a model. This principle can be extended to multiple macros, where a full HS scenario requires at least two macros to ensure the full stationarity of at least one of the operands of every layer when targeting our SCNN workload. Fig. 4(a) depicts two HS dataflows in which the stationary operand is either the one requiring the least or the most memory, respectively referred to as HS-min and HS-max. Compared to the conventional WS-only dataflow, HS-min increases the amount of stationary operands by 46% with an optimal layer mapping across both macros (Fig. 4(b)). Further efficiency gains can be unlocked by scaling up the number of macros, where additional CIM storage avoids frequent external memory accesses by ensuring the stationarity of the operands with the largest memory footprint (Fig. 4(a))."}, {"title": "III. IMPLEMENTATION AND MEASUREMENT RESULTS", "content": "The proposed CIM macro is integrated within the complete accelerator shown in Fig. 5(a). Beyond the proposed 16kB CIM-SRAM macro, it is composed of a 4.25kB memory for per-timestep input spike buffering, and 4\u00d74 banks of 2kB SRAMs buffering the SNN weights (resp. membrane potentials) in OS (resp. WS) mode. A 32-to-256-bit bandwidth-adaptive merge-and-shift unit ensures correctly aligned data transfers for arbitrary CIM configurations. The microphotograph of the chip in bulk 40-nm CMOS is shown in Fig. 5(b)."}, {"title": "A. Circuit Level", "content": "The unrestricted support of the neural network quantization landscape allows for workload-dependent accuracy/footprint optimization. On the IBM DVS gesture dataset [1] and our six-layer SCNN model, we demonstrate that fine-tuning the resolution of the operands allows for a 30% memory footprint reduction over [4] while maintaining a state-of-the-art accuracy of 95.8% for gesture classification (Fig. 6). Alternatively, an additional memory footprint decrease of 36% can be reached while preserving an accuracy of 90%.\nThe measurement results of FlexSpIM are presented in Fig. 7. Nominal measurement conditions correspond to a 1.1-V core voltage, a 157-MHz system clock that defines complete CIM operations, and a 942-MHz internal clock that splits internal CIM phases, as illustrated in Fig. 2(c). First, when operands are mapped over all available columns using a single-row shape and an increasing resolution equal for both weights and membrane potentials, the energy per operation increases linearly with the operands resolution. A marginal overhead under 5% appears with an increased resolution due to the carry propagation in the PC adder tree. Next, changing the operand mapping ($N_R \\times N_C$, where $N_R$ and $N_C$ respectively stand for the number of rows and columns required to store the operand) for a fixed target resolution (e.g., 16 bits) and number of output channels (e.g., 32) modifies the number of CIM columns involved in the computation, as operations spread out sequentially with the number of rows. Compared to a row-wise kernel stacking in [3]\u2013[7], [9]\u2013[12], FlexSpIM's dedicated operand shaping combined with the PC standby mode for unused columns allows saving up to 4.3\u00d7 energy per operation, keeping the energy variation across all considered FlexSpIM shapes below 24% (Fig. 7(a)). This homogeneity"}, {"title": "B. System Level", "content": "At the system level, accurate evaluation can be obtained by considering a many-macro CIM array architecture with a global on-chip buffer and an external DRAM (Fig. 7(b)). By extracting the energy efficiency of the complete system, accounting for macro-level measurements, the impact of FlexSpIM's system-level flexibility is assessed on the six-layer SCNN workload. First, considering the optimum-resolution mappings of FlexSpIM and [4] (Fig. 3(a)), a FlexSpIM-based system composed of 16 CIM macros with HS maximizing the amount of stationary operands achieves an 87 - 90% energy efficiency gain in the 85 - 99% input sparsity range. Second, considering the fixed 6-bit weight and 11-bit membrane potential resolutions of IMPULSE [3], a FlexSpIM-based system with 18 CIM macros achieves a 79 - 86% energy efficiency gain in the 85 - 99% sparsity range."}, {"title": "IV. CONCLUSION", "content": "With FlexSpIM, we introduced arbitrary resolution and operand shaping, together with a unified weight/membrane potential memory, to enable CIM-based SNN hardware with the highest resolution and dataflow configurability. With a prototype fabricated in 40-nm CMOS, we demonstrated based on silicon measurements that this flexibility, enabling a 79-90% reduction of energy per operation at the system level, is achieved while maintaining a competitive macro-level trade-off between peak throughput and energy per operation, especially compared to past digital CIM solutions. This work thus underlines the importance of macro-level flexibility to enable significant system-level gains in scalable architectures for real-world edge-vision tasks."}]}