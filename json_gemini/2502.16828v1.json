{"title": "PREDICTING THE ENERGY LANDSCAPE OF STOCHASTIC DYNAMICAL SYSTEM VIA PHYSICS-INFORMED SELF-SUPERVISED LEARNING", "authors": ["Ruikun Li", "Huandong Wang", "Qingmin Liao", "Yong Li"], "abstract": "Energy landscapes play a crucial role in shaping dynamics of many real-world complex systems. System evolution is often modeled as particles moving on a landscape under the combined effect of energy-driven drift and noise-induced diffusion, where the energy governs the long-term motion of the particles. Estimating the energy landscape of a system has been a longstanding interdisciplinary challenge, hindered by the high operational costs or the difficulty of obtaining supervisory signals. Therefore, the question of how to infer the energy landscape in the absence of true energy values is critical. In this paper, we propose a physics-informed self-supervised learning method to learn the energy landscape from the evolution trajectories of the system. It first maps the system state from the observation space to a discrete landscape space by an adaptive codebook, and then explicitly integrates energy into the graph neural Fokker-Planck equation, enabling the joint learning of energy estimation and evolution prediction. Experimental results across interdisciplinary systems demonstrate that our estimated energy has a correlation coefficient above 0.9 with the ground truth, and evolution prediction accuracy exceeds the baseline by an average of 17.65%. The code is available at github.com/tsinghua-fib-lab/PESLA.", "sections": [{"title": "INTRODUCTION", "content": "Energy landscapes are inherent in many stochastic dynamical systems in nature, such as the potential energy surface of protein conformations (Norn et al., 2021), the fitness landscape of species evolution (Papkou et al., 2023; Poelwijk et al., 2007), and the fractal energy landscapes of soft glassy materials. The evolution of these systems can be modeled as particles moving on the landscape under the combined effect of energy-driven drift and noise-induced diffusion. The structure of the energy landscape governs the long-term motion of particles, forming the deterministic aspect of the dynamics, while inherent random noise disrupts the movement along the energy gradient, driving exploration across energy barriers (Blount et al., 2018; Kryazhimskiy et al., 2014). When multiple low-energy regions exist in the landscape, the combined effect of the energy gradient and noise induces high-frequency movement within individual regions and low-frequency transitions between different regions (Lin et al., 2024). In this context, energy landscapes have been applied to guide the generation of stable molecular structures (No\u00e9 et al., 2019) and direct the evolution of proteins (Packer & Liu, 2015; Greenbury et al., 2022), and more recently, they have been incorporated as physical knowledge into deep learning for predicting system evolution (Guan et al., 2024; Wang et al., 2024b; Ding et al., 2024)."}, {"title": "BACKGROUND AND PROBLEM SETUP", "content": "Let us consider a stochastic dynamical process which can be described by the following differential equation:\n$$dst = f(st)dt + \\sigma(st,t)dW(t),$$\n$$Xt = g(St).$$\nSpecifically, it represents a system with latent state variable $s_t \\in S$ whose evolution is driven by a deterministic drag force $f(s_t)$ and a random force described by white noise $\\sigma(s_t,t)dW(t)$. While the state variable $s_t$ is hidden and cannot be observed directly, the observable measurement $X_t \\in X$ of the system is derived through a transformation $g : S \\rightarrow X$, which can be either linear or nonlinear and can even represent a mapping from continuous space to discrete space, thereby describing systems with discrete observable metrics, such as ecological evolution.\nMore specifically, we focus on systems where the force $f(s_t)$ is conservative. This implies the existence of an energy function $E(s_t)$, also referred to as the energy landscape, such that $f(s_t) = -\\nabla E(s_t)$. Then, the dynamic equation 1 can be be rewritten as:\n$$dst = -\\nabla E(st)dt + \\sigma(st,t)dW(t),$$\nThe energy landscapes measure the thermodynamic stability of a given state. Low-energy regions induce a drift that draws the system state into them with greater probability and duration, manifesting thermodynamically as the Boltzmann distribution, $p \\propto e^{-E(s)/kT}$, where $k$ is Boltzmann constant and T represents temperature. For evolution starting from any initial state distribution, the system's long-term dynamics will eventually drift toward the Boltzmann distribution defined by the energy landscape. Examples of such energy landscapes in different disciplines include fitness landscapes in ecology Papkou et al. (2023), potential energy in molecular dynamics Chmiela et al. (2017), and free energy in glassy materials Charbonneau et al. (2014).\nLearning problem In this paper, our primary objective is to estimate the energy landscape of a stochastic dynamical system based on its evolution trajectories, without the true energy as a supervisory signal. More formally, the input of this learning problem is a set of the N-step evolution trajectory $X^N = \\{x_t; i=0, ..., N-1\\}$ of the stochastic dynamical system in the D-dimensional observation space X. Then, for an arbitrary observable state x, the objective of this learning problem is twofold: (1) building a transformation $E$ to map the observable measurement to a latent feature $E(x)$ that determines the energy of the system; (2) estimating the energy $\\hat{E}(E(x))$ as an approximation of the true energy $E(g^{-1}(x))$. Since the true energy $E(g^{-1}(x))$ is unavailable as a supervisory signal in the learning process, the estimated energy $\\hat{E}(E(x))$ is only required to be a linear transformation of the true energy."}, {"title": "METHOD", "content": "In this section, we introduce a Physics-informed Energy Self-supervised Landscape Analysis (PESLA) method, which learns to predict the energy landscape through a self-supervised evolution prediction task, as shown in Figure 1. First, we develop an adaptive codebook learning module to instantiate the mapping $E$ from the observed space to the energy landscape. This approach integrates concepts from reduced-order modeling of complex systems to mitigate uncertainties caused by limited sample coverage. Next, we explicitly incorporate the energy function into a graph neural Fokker-Planck equation to model the system's evolution on the energy landscape. Additionally, we introduce physics-inspired regularization constraints into the optimization objective to eliminate the assumption of thermodynamic equilibrium sampling."}, {"title": "ADAPTIVE CODEBOOK LEARNING", "content": "Constructing the energy landscape involves learning the transformation $E$ from the observed space X to the latent space S where the energy landscape resides. Previous studies have shown that, despite the high dimensionality of the state space, the long-term dynamics of systems unfold on a very low-dimensional manifold in the form of reduced-order model (Vlachas et al., 2022; Thibeault et al., 2024; Li et al., 2024). This suggests that the energy landscape, which shapes the system's"}, {"title": "GRAPH NEURAL FOKKER-PLANCK EQUATION", "content": "In the latent space S, the time evolution of the system state is influenced by the combined effect of energy-driven drift and diffusion caused by inherent random noise, theoretically modeled by the Fokker-Planck equation (Risken, 1996). On the discretized energy landscape, we extend the traditional Fokker-Planck equation into a graph neural differential equation, enabling joint learning of energy estimation and evolution prediction.\nWe construct the codeword topology $A = (a_{ij})_{K\\times K}$ based on the adjacency relationships of the codeword regions (as shown in Figure 1) and estimate the energy of each codeword as $E(c_i)$ as the energy landscape $G = \\{A, C, E(*)\\}$ of system evolution. At this point, we have projected the original observed trajectory onto a low-dimensional energy landscape, obtaining the transition trajectory of the system state on the codeword topology. Predicting the temporal evolution of the system means modeling the time-dependent evolution of the probability distribution over the codewords. The effects of energy and noise on this evolution are modeled by the Graph Fokker-Planck equation Chow et al. (2012) as:\n$$\\frac{dp_i}{dt} = \\sum_{j\\in N(i), E_{ji}>0} ((E_{ji} + \\beta \\log \\frac{p_j}{p_i}) p_j + \\sum_{j\\in N(i), E_{ji}<0} ((E_{ji} + \\beta \\log \\frac{p_j}{p_i}) p_i+ \\sum_{j\\in N(i), E_{ji}=0} \\beta(p_j - p_i),$$\nwhere $p_i$ denotes the probability of node i and $E_{ji} = E_j - E_i$. $\\beta$ is a positive constant which governs the noise strength. Denoting $p(c_{t_0})$ as K-dimensional probability distribution at time $t_0$,"}, {"title": "TRAINING", "content": "The trainable parameters include the encoder $E$, decoder $\\Omega$, codebook C, probability encoder $\\Phi$, probability decoder $\\Psi$, neighborhood attention weights W, coefficient vector $\\beta_{\\epsilon}$, and energy function $E(*)$. The detailed model architecture is provided in Appendix A.2. In the following, we introduce the training procedure for the model.\nAdaptive codebook learning and evolution prediction form a joint learning task. The optimization objective for the former is to minimize the negative log-likelihood of the reconstructed distribution, i.e., $\\mathcal{L}_{reconstruct} = \\log q_{\\Xi, \\Omega}(x)$. In our experiments, we use a Gaussian prior distribution decoder with negative log-likelihood loss for continuous systems, and cross-entropy loss for discrete systems. Additionally, the loss function $\\mathcal{L}_{vq}$ for updating codeword is consistent with the one proposed by Van Den Oord et al. (2017). Similarly, we minimize the negative log-likelihood in both the latent space and the landscape space for the evolution prediction task. In the latent space, we minimize the L2 error $\\mathcal{L}_{latent} = ||\\Phi(p(C_{t+\\triangle t})) - \\Psi(H(t + \\triangle t))||$, while in the landscape space, we use cross-entropy $\\mathcal{L}_{code} = -p(C_{t+\\triangle t}) \\log q(C_{t+\\triangle t})$.\nWith the mapping of adaptive codebook, we can estimate the distribution $p(c_i)$ of observed samples within the landscape space and employ the corresponding negative log-probability as reference energies to guide energy estimation. However, this approach fails when evolution trajectories are not sampled from a thermodynamic equilibrium state. Proven by statistical mechanics, the state probability distribution evolving in the form of Fokker-Planck equation converges to the Boltzmann distribution. Although we cannot expect all sample data to be drawn from a thermodynamic equilibrium state, the long-term evolution of states will eventually converge to the Boltzmann distribution. This suggests incorporating a regularization term into the long-term prediction task, expressed as the KL divergence between the empirical distribution p and the Boltzmann distribution q, i.e.,\n$$\\mathcal{L}_{phy} = D_{KL}(P||q) = \\sum_i P(c_i) \\log (\\frac{P(c_i)}{q(c_i)}).$$ Overall, we conduct the training process by optimizing the aforementioned objectives $\\mathcal{L} = \\mathcal{L}_{reconstruct} + \\mathcal{L}_{vq} + \\mathcal{L}_{latent} + \\mathcal{L}_{code} + \\mathcal{L}_{phy}$. Detailed training strategies are provided in Section 4.1 and ablation studies can be found in Appendix E."}, {"title": "EXPERIMENTAL SETUP", "content": "Baselines For the energy estimation task, we employ the Markov state model (MSM) (Majewski et al., 2023) and autoencoder potential energy (APE) (Kamyshanska & Memisevic, 2014) as baselines. For the evolution prediction task, we compare PESLA with NeuralMJP (Seifner & S\u00e1nchez, 2023), T-IB (Federici et al., 2024), VAMPNets (Mardt et al., 2018), and SDE-Net (Kong et al., 2020). Details on the implementation and hyperparameter searching of these baseline algorithms can be found in Appendix A.3.\nEvaluation Metrics We evaluate the accuracy of energy estimation from two perspectives. The trajectory energy correlation $\\rho_r$ represents the Pearson correlation coefficient between the predicted and true energies for all samples along a new trajectory, assessing predictive performance within the regions covered by training data. The full-space energy correlation $\\rho_F$ measures the correlation coefficient for the energy of system states uniformly across the entire state space, accounting for unseen areas during training. For the evolution prediction task, all metrics are measured from M reference trajectories $X^M_\\tau$ unfolding from randomly initialized system states, where $\\tau$ denotes the lag time of each step. All models are tasked with predicting evolution trajectories starting from these initial states, covering the same time span as the reference trajectories. We evaluate the accuracy of the predicted distributions by calculating the Jensen-Shannon divergence between the marginal (MJS) and transition (TJS@\\tau) probability distributions of the predicted and reference trajectories across all states. For systems with a continuous state space, we discretize it into evenly spaced grid partitions, following previous work Federici et al. (2024); Arts et al. (2023). Further details can be found in Appendix A.5.\nTraining strategy We first train encoder $\\Xi$, decoder $\\Omega$ and the feature vectors of the codewords C to construct the landscape topology. Then, we freeze them and train the parameters of the graph neural Fokker-Planck equation and energy function E(*) on the landscape. For all models, we use the Adam optimizer, with the learning rate decaying exponentially by a factor of 0.99 each epoch."}, {"title": "ECOLOGICAL EVOLUTION", "content": "We examine the strong selection weak mutation system within eco-evolutionary dynamics, which is widely studied in ecology to understand the adaptive evolution of populations in specific environments (Kryazhimskiy et al., 2009; Bank et al., 2016). The fixation probability of a candidate state j (new mutation) with fitness $f_j$ is governed by the Kimura formula derived from the Wright\u2013Fisher model, given by $P_{i,j} = \\frac{1 - e^{-2s_i(j)}}{1 - e^{-2Ns_i(j)}}$, where N represents the population size and $s_i(j) = \\frac{f_j}{f_i} - 1$ is the selection coefficient. Sella & Hirsh (2005) have mathematically demonstrated that the logarithmic fitness of such evolutionary systems aligns with the energy of thermodynamic systems. We simulate 1K trajectories, each with 100 time steps, under the two-locus setting where each locus has 64 possible mutation types as our dataset. Figure 2b (bottom) and Figure 4b respectively report PESLA's superior predictive performance for fitness and system evolution.\nIn ecology, fitness measures the relative advantage of a genotype and is negatively correlated with energy (Sella & Hirsh, 2005). PESLA estimates the energy function of genotypes with a correlation coefficient close to -1. The predicted energy is fitted with a RANSAC regression model (see Appendix A.5) and visualized in Figure 4a (right). The distribution pattern of codewords within the codebook indicates that PESLA successfully identifies the set of genotypes with high fitness in eco-evolutionary dynamics. Moreover, since the genotype space is characterized by the Hamming distance, states in the same row or column of the codebook are more likely to be mapped to the same codeword (Figure 4a (center)). This indicates that the adaptive codebook incorporates knowledge of system dynamics rather than relying on simple equidistant grid binning."}, {"title": "PROTEIN FOLDING", "content": "We apply PESLA to the folding data of five fast-folding proteins simulated by the Anton supercomputer (Lindorff-Larsen et al., 2011). Each protein has two folding trajectories of equal length, used for model training and testing, respectively. Due to the lack of true energy, we estimate the reference energy using Time-lagged Independent Component Analysis (TICA) and the Markov State Model (MSM) based on the complete dataset (three times larger than the training data), consistent with previous studies (Majewski et al., 2023; Mardt et al., 2018). For each protein, the lag time used in TICA processing and experiments is based on the mean transition path time reported by Lindorff-Larsen et al. (2011). The reference energy distribution on the 2D principal component plane identified by TICA is shown in Figure 5a, with implementation details provided in Appendix A.4. Each protein features a varying number (1 to 4) of low-energy regions with different distributions, posing challenges for energy estimation. Figure 5b shows PESLA's partitioning of the state space for each protein on the TICA principal component plane, demonstrating that PESLA differentiates low-energy, high-energy, and unknown energy regions with varying codeword aggregation rates. This automatic scaling ensures that PESLA's energy predictions remain consistent with reference values (as shown in Figure 5c), even in challenging protein folding problems. Additionally, PESLA achieved the best performance in the evolution prediction task (see Appendix C)."}, {"title": "RELATED WORK", "content": "Estimating the energy landscape is a crucial problem across multiple disciplines. The most fundamental approach involves collecting data through modern sequencing techniques and manual experiments. Sarkisyan et al. (2016) measured tens of thousands of Aequorea victoria (avGFP) derivative genotypes to construct the local fitness landscape of green fluorescent proteins. Chen et al. (2022) analyzed the fitness of all single mutations in VIM-2 B-lactamase across a 64-fold range of ampicillin concentrations. Additionally, Wang et al. (2024a) conducted high-throughput functional genomics on Salmonella to identify gene networks related to adaptive effects. There have been many similar efforts (Starr et al., 2018). However, these manual experiments are often associated with high operational costs, making machine learning a promising solution to improve this process in a data-driven manner (Rupp et al., 2012; Han et al., 2023). Zhang et al. (2018) introduce the deep potential molecular dynamics method, using neural networks to model interatomic forces and potential energy. To mitigate overfitting issues in deep neural networks, Aghazadeh et al. (2021) apply sparse recovery algorithms from coding theory for spectral regularization. Zhang et al. (2022) employ high-speed atomic force microscopy to collect data for training a U-net model to predict the energy landscape of spatial angles on the DHR10-micaN protein. Additionally, Tonner et al. (2022) and Skwara et al. (2023) offer interpretable predictions of mutation effects and population functions through hierarchical Bayesian modeling and polynomial regression, respectively. More recently, Du et al. (2024) developed a graph neural network to model intermolecular interactions, predicting Gibbs free energy in solute-solvent interactions. Despite these efforts, these models often depend on true energy values or molecular force fields as supervisory signals. In contrast to these methods, our PESLA does not require supervisory signals for energy; instead, it learns to estimate energy through a self-supervised evolution prediction task. An additional benefit of this approach is that the predicted energy effectively enhances the accuracy of evolution prediction."}, {"title": "EVOLUTION PREDICTION", "content": "Predicting the evolution of stochastic dynamical systems is challenging due to the unknown underlying energy landscape. Vlachas et al. (2022) employ dimensionality reduction techniques to construct reduced-order models that capture essential macroscopic information, thereby simplifying the analysis of large-scale systems. To handle the challenge of modeling long-term dynamics, approaches such as learning time-invariant representations have been explored (Federici et al., 2024; Kostic et al., 2024b; Li et al., 2023). Furthermore, Kostic et al. (2022; 2024a) extend Koopman operator theory to map system states into a Hilbert space, facilitating the learning and interpretation of nonlinear dynamics. Wu et al. (2018) and Seifner & S\u00e1nchez (2023) represent stochastic dynamical processes as discrete state transitions within a Markov process framework. In contrast to existing methods, PESLA utilizes energy landscape knowledge to guide system dynamics modeling."}, {"title": "CONCLUSION", "content": "In this paper, we propose the PESLA method to estimate the energy landscape from historical evolution trajectories in a self-supervised manner. By integrating adaptive codebook learning and a graph neural Fokker-Planck equation, PESLA collaboratively models the energy landscape and system dynamics, even with limited observational data. We introduce physics-inspired regularization to help PESLA move beyond the reliance on thermodynamic equilibrium sampling. Experimental results across various systems demonstrate that PESLA outperforms state-of-the-art methods in both energy estimation and evolution prediction. PESLA does not require supervisory signals for energy, making it a powerful data-driven tool for understanding and predicting stochastic dynamical systems.\nLimitations and Future work This work focuses on estimating the energy landscape of a class of energy-driven evolutionary systems. However, when a system is driven by non-conservative forces, an energy landscape does not exist, as in the case of motion in viscous fluids. Additionally, inferring energy landscapes becomes more challenging when the landscape is time-varying, such as in cases where climate change alters species fitness. Future work will need to explore additional model designs to accommodate the dynamics of time-varying landscapes, where $E(*)$ needs to adapt to $E(*,t)$."}, {"title": "INTRODUCTION AND IMPLEMENTATION OF BASELINES", "content": "Our baselines cover both the energy estimation and evolution prediction tasks. For the energy estimation task, we have:\n\u2022 Markov State Model (MSM) is a commonly used method for estimating the relative energy of system states based on statistical probabilities. It discretizes the system using equidistant grid binning and then calculates the negative log of the frequency distribution for all states in the dataset as the reference energy. This approach is often limited by the inefficiency of Monte Carlo sampling. When the dataset fails to cover the entire sample space, some state frequencies become zero, making it impossible to infer unobserved samples from the existing data. In our experiments, we used nearest-neighbor interpolation to compute the full-space energy correlation coefficient $\\rho_F$ for quantitative evaluation.\n\u2022 Autoencoder Potential Energy (APE). Kamyshanska & Memisevic (2014) demonstrate that an autoencoder can estimate the energy of a sample by treating the reconstruction error as a proxy for energy, where a lower reconstruction error indicates that the sample lies in a high-probability, low-energy region of the learned manifold, while a higher error corresponds to a higher energy. For an autoencoder with sigmoid activations, with weights $W$, hidden biases $b_h$, and reconstruction biases $b_r$, the energy function is given by:\n$$E(x) = \\sum_k log(1 + exp(W_k^Tx+b_{hk})) - \\frac{1}{2}||x - b_r.||^2 + const,$$\nwhere $W_k^T$ represents the linear combination of inputs, $b_{hk}$ is the hidden bias term for the k-th hidden unit, and $b_r$ is the reconstruction bias.\nFor the evolution prediction task, we compare PESLA with:\n\u2022 Neural MJP. Seifner & S\u00e1nchez (2023) introduce Neural MJP as an alternative variational inference algorithm for Markov jump processes, which relies on neural ordinary differential equations in the form of the master equation. Neural MJP predefines the number of discrete states and encodes observed states into a one-hot vector representing the discrete state distribution as the starting point for state evolution. The key difference between these predefined discrete states and PESLA's codewords is that Neural MJP does not characterize them by energy but instead relies on a black-box neural network to fit the transition probabilities. The number of preset discrete states is treated as a hyperparameter.\n\u2022 T-IB (Federici et al., 2024) captures time-invariant representations of continuous dynamical systems using a representation learning objective derived from information bottleneck theory and models state transitions in the representation space through a conditional flow model. This efficient representation allows T-IB to filter out high-frequency fluctuations as noise and model long-term dynamics over extended time spans.\n\u2022 VAMPnet (Mardt et al., 2018) captures the dynamics of molecular systems by directly learning a transformation from molecular configurations to a Markov state model using a deep neural network that maximizes a variational score. This end-to-end approach allows it to identify slow dynamical processes and long-timescale kinetics effectively.\n\u2022 SDE-Net (Kong et al., 2020) explicitly models the drift and noise diffusion terms of stochastic dynamical systems by parameterizing these two mechanisms within a neural differential equation, enhancing the representational capacity of the neural network. However, SDE-Net does not treat the dynamical system as a Markov process, making it challenging to capture transition characteristics between metastable states. Despite this limitation, it serves as a benchmark for all models."}, {"title": "ARCHITECTURE OF PESLA MODEL", "content": "We summarize all components of the PESLA model and the parameter shapes of each component in Table 1, where D is the dimension of the observed state, K is the preset number of codewords, and r is the proportion of activated codewords."}, {"title": "GRID SEARCHING FOR HYPERPARAMETERS", "content": "To ensure a fair comparison across all models, we used the same batch size, optimizer, and learning rate decay strategy during training, conducting grid search only on the learning rate and model-specific hyperparameters to achieve optimal performance. The range and targets of the hyperparameter search are detailed in Table 2."}, {"title": "DATA GENERATION OR PREPROCESSING", "content": "Consistent with previous studies, we simulate the 2D Prinz potential and ecological evolution systems to obtain the datasets. For protein folding, we use the official data provided by the authors, as described in Table 3. For the first two systems, the training and testing sets are split in a 7:3 ratio. For the protein data, each protein has two trajectories of equal length, one used for training and the other for testing."}, {"title": "EVALUATION METRICS CALCULATION", "content": "For the evolution prediction task, we test each model using the following steps:\n1. Randomly initialize M initial states and predict N future steps using the model to obtain $X^M_\\tau$;\n2. Discretize each dimension into a K \u00d7 K state space using a uniform grid, resulting in a finite set of discrete states;\n3. Compute the marginal and transition probability distributions for each state across all M trajectories;\n4. Calculate the Jensen-Shannon divergence of the marginal and transition probabilities for each trajectory and take the average.\nWe set K to 5, 8, and 8 for the 2D Prinz potential, ecological evolution, and protein folding systems, respectively.\nFor the trajectory energy correlation reported in Figure 4a (right), we used the Random Sample Consensus (RANSAC) regression algorithm to fit the maximum likelihood expression $E_{pred} = f(E_{true})$ of the true energy and PESLA's predicted energy. We then mapped the data from Figure 4a (left) using f and visualized it in Figure 4a (right)."}, {"title": "SUPPLEMENTARY EXPERIMENTAL RESULTS FOR 2D PRINZ POTENTIAL", "content": "We provide the codebooks from five independent experiments on the 2D Prinz potential in Figure 6, showing that PESLA consistently learns similar codeword distribution patterns."}, {"title": "SUPPLEMENTARY EXPERIMENTAL RESULTS FOR PROTEIN FOLDING", "content": "We provided a supplementary comparison of the predictive performance of all models using the BBA protein as an example, as shown in Table 4."}, {"title": "INTERPRETABILITY", "content": "Our PESLA synergistically estimates energy and predicts trajectories to simultaneously improve the accuracy of both. Although the quality of evolution prediction directly influences the precision of energy estimation, it remains unclear how the accuracy of energy estimation, in turn, impacts evolution prediction. Here, we investigate how the correlation between the estimated energy landscape and the true energy landscape influences the evolution prediction. Specifically, we aim to clarify the degree of correlation required between the predicted energy and the true energy to ensure accurate evolution prediction.\nWe disable the energy prediction module of PESLA, replacing the predicted energy of each codeword with a dummy energy value. When the Pearson correlation coefficient, denoted as $\\rho$, equals 1.0, the dummy energy is derived from the mean true energy values of all samples within the region of each codeword. We gradually introduce noise to the dummy energy to reduce its correlation with the true energy, as illustrated in the first row of Figure 7. Subsequently, we train PESLA under various dummy energy conditions and evaluate the prediction error. As shown in Figure 7, the prediction error progressively increases as the correlation coefficient between the dummy energy and the true energy decreases. When the correlation coefficient drops below 0.5, PESLA's predictive performance begins to lag behind the optimal baseline algorithm (NeuralMJP). This indicates that the quality of evolution prediction is directly influenced by the accuracy of energy estimation."}, {"title": "CONSISTENCY", "content": "Although the degree of discretization of the state space depends on the predefined number of codewords, a robust prediction model should yield consistent energy landscapes across different settings. We evaluate the correlation between energy values predicted by PESLA under various hyperparameter settings (predefined number of codewords K) and random seeds. As shown in the correlation matrix in Figure 8, the energy landscapes identified by PESLA remain consistent not only across parallel experiments with different random seeds but also across different choices of hyperparameters K."}, {"title": "NOISE ROBUSTNESS", "content": "Considering that real-world trajectory data is usually noisy and sparse, the robustness of a predictive model to noise and limited data determines its practical utility. As reported in Figure 2a of the main text, PESLA outperforms all baselines when available data is reduced. Here, we further evaluate PESLA's robustness to noisy data. Specifically, we add Gaussian noise of varying strength to the dataset, with a noise amplitude equal to the original data magnitude when the strength is set to 1.0. The results in Figure 9 indicate that PESLA remains sufficiently robust to noise until the noise strength exceeds 0.6. PESLA's robustness to noise can be attributed to its adaptive codebook learning model, which incorporates a reduced-order approach. By identifying a low-dimensional, compact representation of the original state space, PESLA inherently possesses the ability to filter out uncertainties such as noise-related errors."}, {"title": "TRANSFERABILITY", "content": "Here, we explore the transferability of PESLA. Although the energy landscapes of different evolutionary systems are inherently distinct, PESLA's modules can be partially reused in similar state spaces by learning a generalized spatial mapping mechanism. We proceed by evaluating PESLA's transferability across five different protein-folding datasets.\nDue to differences in sequence length and arrangement, the folding processes of different proteins occur within their unique energy landscapes, which means that the energy function E(*) and transition model need to be specifically trained for each type of protein. However, the mapping functions \u039e and \u03a9 between the observed space X and the latent space S have the potential for transferability. By learning a universal encoder E, decoder \u03a9 and codebook C, it is promising to project the structures of various proteins onto a unified latent space.\nTo test such transferability, we conduct cross-protein experiments for each protein. Specifically, for a given protein i, we train the encoder \u039e, decoder \u03a9, and codebook C using data from the other four proteins. We then freeze their parameters and use a single folding trajectory of protein i to train the energy function E(*) and the Graph Neural Fokker-Planck equation. Finally, we evaluate the accuracy of energy and evolution predictions on unseen folding trajectories of protein i. The results are presented in Table 5. Although the predictive performance in all cross-protein transfer experiments is lower than that achieved by training on specific proteins, the average transfer performance $\\rho_t$ for energy prediction across all proteins reaches over 80% of the performance of specifically trained models. Additionally, the transfer performance of evolution prediction is significantly better"}, {"title": "SCALABILITY", "content": "To investigate the relationship between the size of the state space and the codebook size, we evaluate the impact of the preset number of codewords K on energy and evolution prediction in protein folding datasets with varying numbers of alpha-C atoms, as shown in Figure 10. As K increases from 10 to 1000, the relative performance of the model improves. For the BBA protein, which has only 28 alpha-C atoms, the prediction accuracy for energy reaches over 90% of the performance observed at K = 1000 when K = 100. For larger proteins, such as A3D, the model's predictive performance converges at K = 500. In fact, protein size increases the complexity of the state space, thereby adding to the modeling challenge. For larger proteins or other systems with complex state spaces, the codebook size needs to be sufficiently large to ensure PESLA's modeling capacity. For the systems studied in this paper, we recommend setting the preset number of codewords K to 1000. For other unfamiliar systems, starting with a relatively large K value is generally advisable.\nFurthermore, as analyzed in Appendix F, the time complexity for training and inference grows sublinearly with the increase in K. Since only a limited number of codewords are activated in the preset codebook, the size of the energy landscape constructed by PESLA is at most K. Consequently, when modeling the state transition distribution over the landscape using the Graph Neural Fokker-Planck equation, the number of codewords to be considered does not necessarily increase linearly with K. This design allows users to efficiently explore and select appropriate values for K."}, {"title": "ABLATION STUDIES", "content": "PESLA comprises multiple loss function terms and submodules. Here, we introduce additional ablation studies to elucidate the individual contribution of each component to the overall performance.\nAs summarized in Section 3.3, the training process involves five loss function terms: $\\mathcal{L}_{rec}$, $\\mathcal{L}_{vq}$, $\\mathcal{L}_{latent}$, $\\mathcal{L}_{code}$, and $\\mathcal{L}_{phy}$. The $\\mathcal{L}_{rec}$ and $\\mathcal{L}_{vq}$ terms jointly guide the adaptive codebook learning module, while $\\mathcal{L}_{latent}$ and $\\mathcal{L}_{code}$ direct the synergistic learning process for energy and evolution prediction. Additionally, the $\\mathcal{L}_{phy}$ term incorporates physical knowledge to further inform energy estimation. Beyond the essential loss terms, we individually evaluate the performance impact of the auxiliary terms, $\\mathcal{L}_{latent}$ and $\\mathcal{L}_{phy}$. The results are presented in Table 6.\nWe firstly remove the $\\mathcal{L}_{latent}$ term, training the prediction module solely with the $\\mathcal{L}_{code}$ term. The results indicate that, without the predictive constraint from the latent space, the accuracy of evolution prediction deteriorates, and the precision of energy estimation is also affected. Next, upon removing the $\\mathcal{L}_{phy}$ term, PESLA's performance in both energy and evolution prediction declined significantly. This suggests that, although self-supervised learning on the evolution prediction task"}, {"title": "COMPUTATIONAL COST", "content": "We denote the sample size and the preset number of codewords as N and K", "modules": "adaptive codebook learning and the graph neural Fokker-"}]}