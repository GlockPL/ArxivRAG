{"title": "Strong Preferences Affect the Robustness of Value Alignment", "authors": ["Ziwei Xu", "Mohan Kankanhalli"], "abstract": "Value alignment, which aims to ensure that large language models (LLMs) and other AI agents behave in accordance with human values, is critical for ensuring safety and trustworthiness of these systems. A key component of value alignment is the modeling of human preferences as a representation of human values. In this paper, we investigate the robustness of value alignment by examining the sensitivity of preference models. Specifically, we ask: how do changes in the probabilities of some preferences affect the predictions of these models for other preferences? To answer this question, we theoretically analyze the robustness of widely used preference models by examining their sensitivities to minor changes in preferences they model. Our findings reveal that, in the Bradley-Terry and the Placket-Luce model, the probability of a preference can change significantly as other preferences change, especially when these preferences are dominant (i.e., with probabilities near 0 or 1). We identify specific conditions where this sensitivity becomes significant for these models and discuss the practical implications for the robustness and safety of value alignment in AI systems.", "sections": [{"title": "Introduction", "content": "Value alignment (Gabriel & Ghazavi, 2021) aims to ensure that AI agents, such as large language models (LLMs) (OpenAI, 2023; Llama Team, 2024), behave in accordance with human values. It is critical for ensuring safety and trustworthiness of AI systems. An important component of value alignment is the modeling of preferences, where preferences of individual or groups of people (e.g., citizens of a country) are collected as samples of choices made by the people over options given contexts of decisions and are then fit by probabilistic frameworks to predict preferences in unseen contexts. For example, in the Reinforcement Learning from Human Feedback (RLHF) framework, alternative model outputs for the same prompt are shown to the human subjects in order to elicit their preferences. A comprehensive set of such preferences are then used to train a reward/preference model, which is subsequently used to train a target agent via proximal policy optimization (PPO) (Schulman et al., 2017). Recent value alignment approaches, such as direct policy optimization (DPO) (Rafailov et al., 2023), uses reparameterized implicit preference models in its optimization objectives.\nOne of the most widely used preference model in value alignment research is the Bradley-Terry model (Bradley & Terry, 1952), which is the pairwise case for the more generalized Plackett-Luce model (Plackett, 1975; Luce, 1959). The study on these for value alignment has been focused on how to better fit a fixed target distribution of preferences (Rafailov et al., 2023; Azar et al., 2024; Xu et al., 2024). However, there has been limited exploration of how changes in the probability of a modelled preference could influence others within these models. Understanding these relationships is crucial for assessing the robustness of preference models in dynamic or uncertain environments, particularly when observed pref-"}, {"title": "Analysis of Preference Models", "content": "We consider the following value alignment setting in our analysis. Suppose there is a group of human subjects who share certain values which we hope to align target AI agents with. In a decision-making context, the values guide the evaluation of the strengths of candidate options, producing a ranking of these options. A preference is thus a ranking over candidate options, denoted as $O = \\{o_i | i = 1, 2, ..., N\\}$, based on the their corresponding strengths, represented as scores $S = \\{s_i | s_i \\in R, i = 1, 2, ..., N\\}$. Since it is difficult to directly obtain the scores, we instead query the subjects' preferences and fit models to estimate the scores of the options. The model then captures the group's values by learning and predicting the probability of the subjects expressing any given preference."}, {"title": "Definitions", "content": "Formally, we define a K-tuple preference as a ranking of K options:\nDefinition 1. (K-tuple preference.) A K-tuple preference is a ranking $o_{\\omega_1} \\succ o_{\\omega_2} \\succ ... \\succ o_{\\omega_{K-1}} \\succ o_{\\omega_K}$ over K items in $O$. It can equivalently expressed as a K-permutation of $O$, or $\\omega = (o_{\\omega_1}, o_{\\omega_2},..., o_{\\omega_{K-1}}, o_{\\omega_K})$, where $1 < \\omega_* < N$.\nThe probability of a particular preference $\\omega$ (being expressed by a certain group of subjects), denoted $p_\\omega$, is predicted by a preference model.\nDefinition 2. (K-tuple preference model.) A K-tuple preference model is a function $f$ that predicts the probability of a given K-tuple preference. Formally, $p_\\omega = f(\\omega)$, where $f : Perm(O, K) \\rightarrow (0, 1)$, $Perm(O, K)$ represents the set of all K-permutations of set $O$, and $\\omega \\in Perm(O, K)$.\nWe will use superscripts to indicate the specific preference model, and subscripts are used to denote the corresponding preference. For example, $p_{ij}^{BT}$ prefers to the probability of $(o_i, o_j)$ under the Bradley-Terry model, and $p_\\omega^{(K)}$ to the probability of $\\omega$ under a K-tuple Plackett-Luce model.\nAssumption 1. (Preference models depend only on score differences.) Following assumptions in traditional"}, {"title": "Analysis of a General Pairwise Preference Model", "content": "This section discusses the probability of a pair of options $(o_i, o_j)$ as $p_{ij}^{(2)} = g(s_i - s_j)$, where $g$ is a preference model that predicts the probability $p_{ij}$ of the ordered pair (i.e., 2-tuple) $(o_i, o_j)$ based on the score difference $s_i - s_j \\in R$. A common example is the Bradley-Terry model $p_{ij}^{(2)} = 1/(1 + exp(-(s_i - s_j)))$. Without restricting the discussion to any specific preference model, we assume that $g(s_i \u2013 s_j)$ satisfies the following properties:\nAssumption 2. (Strictly increasing.) $g$ is a strictly increasing function. Therefore, $p_{ij}^{(2)}$ grows with $s_i \u2013 s_j$. In other words, $o_i$ is more preferred over $o_j$ whenever $s_i \u2013 s_j$ grows.\nAssumption 3. (Limits at infinity.) $\\lim_{x\\rightarrow -\\infty}g(x) = 0$ and $\\lim_{x\\rightarrow +\\infty} g(x) = 1$. Therefore, $p_{ij}^{(2)}$ is bounded within (0, 1), and when $(s_i \u2013 s_j)$ goes to positive (negative) infinity, $p_{ij}^{(2)} = 1$ (or 0).\nAssumption 4. (Symmetry.) $\\forall x \\in R, g(x) + g(-x) = 1$. Therefore, $\\forall o_i, o_j \\in O, p_{ij}^{(2)} + p_{ji}^{(2)} = g(s_i - s_j) + g(s_j - s_i) = 1$, meaning a higher preference for $o_i$ implies a lower preference for $o_j$.\nAssumption 5. (Continuous differentiable.) $g(x)$ is continuously differentiable. Therefore, $g$ and its derivative are reasonably smooth.\nThe following lemma says that, in a pairwise preference model, the probability of any given preference can be expressed as a function of two other preference probabilities.\nLemma 1. For all $o_i, o_j, o_k \\in O$, and under the pairwise model $p_{ij}^{(2)} = g(s_i - s_j)$ following assumptions above,\n$p_{ij}^{(2)} = g \\bigg(g^{-1}(p_{ik}^{(2)}) + g^{-1}(p_{kj}^{(2)}) \\bigg)$, (1)\nwhere $g^{-1} : (0, 1) \\rightarrow R$ is the inverse of $g$, mapping a probability to a difference of scores.\nProof. $p_{ij}^{(2)} = g(s_i - s_j) = g ((s_i - s_k) + (s_k - s_j)) = g \\bigg(g^{-1}(p_{ik}^{(2)}) + g^{-1}(p_{kj}^{(2)}) \\bigg)$.\nLemma 1 is the basis of analysis below as we study $p_{ij}^{(2)}$ as a function of $p_{ik}^{(2)}$ and $p_{kj}^{(2)}$. It suggests that under a pairwise preference model, changes in some probabilities (e.g., $p_{ik}^{(2)}$ and/or $p_{kj}^{(2)}$) leads to changes in other probabilities (e.g., $p_{ij}^{(2)}$). We now study how significant such changes could be by focusing on the sensitivity of $p_{ij}^{(2)}$ to $p_{ik}^{(2)}$ and $p_{kj}^{(2)}$. Towards this, we first examine $g'$ and $g^{-1}$, the derivative and the inverse of $g$, as they are the key components of Eq. (1).\nWe first consider $g'$ and show that it must approach 0 at infinity."}, {"title": "Analysis on the Bradley-Terry Model", "content": "The Bradley-Terry model is a widely-used special case of the general pairwise preference model discussed in the last section. Under this model, $p_{ij}^{BT} = g_{BT}(s_i - s_j) = 1/ (1 + exp (\u2212 (s_i - s_j)))$. According to Lemma 1, we write $p_{ij}^{BT}$ as a function of $p_{ik}^{BT}, p_{kj}^{BT}$.\n$p_{ij}^{BT} = \\frac{1}{1 + \\frac{(1-p_{ik}^{BT})(1-p_{kj}^{BT})}{p_{ik}^{BT}p_{kj}^{BT}}}$ (3)\n$\\frac{\\partial p_{ij}^{BT}}{\\partial p_{ik}^{BT}}$ can be derived as\n$\\frac{\\partial p_{ij}^{BT}}{\\partial p_{ik}^{BT}} = \\frac{p_{kj}^{BT}(1 - p_{kj}^{BT})}{(p_{ik}^{BT}p_{kj}^{BT} + p_{ik}^{BT}p_{kj}^{BT} - 2 p_{ik}^{BT}p_{kj}^{BT} -1)^2}$ (4)"}, {"title": "Extension to the K-tuple Plackett-Luce Model", "content": "The Bradley-Terry model can be extended to the more general K-tuple Plackett-Luce model, which describes probabilities of preferences over K options, where $2 < K < N$. In the extended model, a preference is a K-tuple $\\omega = (o_{\\omega_1}, o_{\\omega_2},..., o_{\\omega_{K-1}}, o_{\\omega_K})$, where $o_{\\omega_*} \\in O$. The probability of this preference is defined as:\n$p_{\\omega}^{(K)} = \\prod_{u=1}^{K-1} \\frac{exp (s_{\\omega_u})}{\\sum_{v=u}^K exp (s_{\\omega_v})} = \\prod_{u=1}^{K-1} \\frac{1}{1 + \\sum_{v=u+1}^K exp(- (s_{\\omega_u} \u2013 s_{\\omega_v}))}$ (6)\nWhen $K = 2$, the model degenerates to the Bradley-Terry model.\nWe are interested in whether sensitivity analysis for Bradley-Terry model can generalize to K-tuple preference models for cases where $K > 2$. In particular, we address the following questions: (1) how $p_{\\omega}^{(K)}$ can be expressed as a function of other preference probabilities, (2) its M-sensitivity w.r.t. these probabilities, and (3) the extent of its sensitivity in comparison with the Bradley-Terry model.\nLemma 5. Let $\\omega$ be a K-tuple preference, where $K > 2$. Let $\\omega_{uv} = (\\omega_{uv}; o_{\\omega_u}, o_{\\omega_v})$ be a K-permutation of O with $o_{\\omega_u}$ and $o_{\\omega_v}$ being the last two elements and $\\omega_{uv}'$ being any (K \u2013 2)-permutation of $O \\setminus \\{o_{\\omega_u}, o_{\\omega_v}\\}$. Then $p_{\\omega}^{(K)}$ is a function of $p_{\\omega'}^{(K)} / p_{\\omega_{uv}}^{(K)}$, where $1 < u < v < K. More specifically,\n$p_{\\omega}^{(K)} = \\prod_{u=1}^{K-1} \\frac{1}{1 + \\sum_{v=u+1}^K \\frac{p_{\\omega'}^{(K)}}{p_{\\omega_{uv}}^{(K)}}}$ (7)"}, {"title": "Implications", "content": "We outline the implications of our analysis for robustness and safety for value alignment."}, {"title": "Dominant preferences impact robustness of value alignment", "content": "In practice, any preference dataset $D$ can only provide preference probabilities for a subset of all possible preference. To link this setting to our earlier discussions, assume the dataset $D$ provides $p_{ik}^{BT}$ and $p_{kj}^{BT}$ to train a Bradley-Terry model $p_{ij}^{BT}$ to predict $p_{ij}$. If $p_{ik}$ and $p_{kj}$ expresses dominance, making $(p_{ik}, p_{kj})$ fall in the M-sensitive region of $p_{ij}$ for some large M, then training-time perturbations that lead to minor changes in $p_{ik}^{BT}$ in the trained model may result in significant changes in the model's prediction of $p_{ij}$. Concretely, the following consequences could arise:\n1. Preference models with similar behaviors on training set may assign drastically different probabilities to unseen preferences. This has been demonstrated in Example 1.\n2. Minor changes in the data distributions within the training set may lead to significant changes in the learned preference models. For example, consider datasets $D_1$ and $D_2$ with different probabilities for $(o_i, o_k)$ and $(o_k, o_j)$. Suppose $(p_{ik}^{BT}, p_{kj}^{BT}) = (0.9999, 0.02)$ and $(p_{ik}^{BT, D_2}, p_{kj}^{BT, D_2}) = (0.9801, 0.02)$. If two preference models $p_{ij}^{BT,1}$ and $p_{ij}^{BT,2}$ are trained to align perfectly with the distribution in the $D_1$ and $D_2$ respectively, similar to Example 1, the two models will assign 0.95 and 0.50 to $(o_i, o_j)$."}, {"title": "Longer Tuples of Preferences may Improve Robustness", "content": "From Theorem 2 we know that, when $K > 2$, K-tuple Plackett-Luce model is more robust than the pairwise Bradley-Terry model ($K = 2$). It is natural to extend this discussion to compare Plackett-Luce models with different K. Recall that by Proposition 2, $A (\\Omega_M (p_{\\omega}^{(K)}, p_{\\omega_{uv}}^{(K)})) = \\frac{\\beta^2}{6\\alpha^2 M^2}$, which suggests that K-tuple models are more robust when $\\alpha$ increases and $\\beta$ decreases. When does this happen? As indicated by Eq. (10),\n$\\alpha = 1 + \\sum_{t=u+1}^K \\frac{p_{\\omega_{tu}}^{(K)}}{p_{\\omega'}^{(K)}}$, and $\\beta = \\prod_{t=u+1 \\\\ t\\neq v} \\frac{1}{1 + \\sum_{l=u+1 \\\\ l\\neq u}^K \\frac{p_{\\omega_{ml}}^{(K)}}{p_{\\omega'}^{(K)}}}$. Given that the ratio $\\frac{p_{\\omega_{ml}}^{(K)}}{p_{\\omega'}^{(K)}} = exp(s_{\\omega_v} \u2013 s_{\\omega_u})$ depends only on the difference of scores, changing K does not cause changes in this ratio. Therefore, as K increases, more positive terms are added to $\\alpha$, resulting in its increase. Likewise, since $\\beta$ is a product of K constants lying between 0 and 1, an increase in K results in a decrease of $\\beta$. This finding suggests that for a fixed M, modeling preferences using longer tuples (i.e., increasing K) leads to smaller M-sensitive areas and yields more robust models. However, using longer preference tuples comes at the expense of higher data collection costs, especially when the number of options increases."}, {"title": "Experiments", "content": "It is noteworthy that AI agents such as LLMs are not preference models, but are trained to align with explicit or implicit preference models. Therefore, it is necessary to verify that the sensitivity presented in Section 2 also exists in trained AI agents, when they are trained using datasets with extremely dominant preferences. Towards this goal, we use a set of three options $O_a = \\{dog, cat, bird\\}$ to synthesize a series of datasets that contain pairwise preferences about animals in $O_a$, with controllable distribution of preferences. In our dataset, a sample contains a question like \u201cWhich one do you prefer, $a_{o_i}$ or $a_{o_j}$?", "I prefer $o_{\\omega}$.": "and a rejected answer like \u201cI prefer $o_{\\iota}$.\u201d, where $o_i, o_j \\in O_a$ and $(o_{\\omega}, o_{\\iota}) \\in Perm (\\{o_i, o_j\\})$. Appendix C details the templates for the questions and answers.\nA dataset $D(\\omega_a, p_{12}, p_{23})$ is synthesized based on three parameters: (1) $\\omega_a = (\\omega_{a1}, \\omega_{a2}, \\omega_{a3})$, a permutation of $O_a$, (2) $p_{12}$, the probability of $\\omega_{a1}$ preferred over $\\omega_{a2}$ in the dataset, and (3) $p_{23}$, the probability of $\\omega_{a2}$ preferred over $\\omega_{a3}$ in the dataset. The dataset contains no samples comparing $\\omega_{a1}$ and $\\omega_{a3}$, thus providing no explicit information about $p_{13}$. For example, $D (\\{dog, bird, cat\\}, 0.99, 0.01)$ is a dataset where dog is preferred over bird in 99% of the samples, and bird over cat in 1% of the samples, with no samples comparing dog and cat provided. We generate datasets based on different permutations of $O_a$ to avoid potential impacts of existing bias about animals in the target language model.\nTo examine the sensitivity of trained models when dominant preferences present in the dataset, we fix $p_{12} = 0.99$ and vary $p_{23}$ from 0 to 1 with a step size of 0.05 for all the datasets. In each experiment, a Llama-3-8B-Instruct model is trained on one of the datasets using the DPO algorithm; training is repeated three times with different random seeds. After training, the model is tested for its preference on $p_{13}$ and $p_{31}$, where $p_\\epsilon^L$ denotes the preference probabilities of the learned preference model. We query the trained model with question \u201cDo you prefer $o_1$ ($o_2$) or $o_3$?"}, {"title": "Related Works", "content": "Value alignment for AI systems (Gabriel & Ghazavi, 2021; Ji et al., 2023) is a critical challenge in ensuring that AI systems act in accordance with human values. Significant efforts in the field have focused on developing frameworks that can align the behavior of AI agents with human preferences (Wirth et al., 2017; Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Dong et al., 2023; Huang et al., 2023; Duan et al., 2024). The Bradley-Terry model (Bradley & Terry, 1952) and the Plackett-Luce model (Luce, 1959; Plackett, 1975) have been a widely used framework for modeling pairwise preferences for value alignment. Methods like RLHF (Ouyang et al., 2022), for example, uses human preferences to train reward models under the Bradley-Terry model. Direct policy optimization (DPO) (Rafailov et al., 2023) has emerged more recently as an alternative approach, which integrates implicit preference modeling into policy training. More recently, further improvements on preference optimization (Azar et al., 2024; Xu et al., 2024; Tajwar et al., 2024; Liu et al., 2024; Song et al., 2024) have been proposed."}, {"title": "Conclusion", "content": "In this paper, we show that in popular preference models, the probability of a given preference could be sensitive to minor changes in other preference probabilities. For the Bradley-Terry model and the Plackett-Luce model, we identify the situations where such sensitivity arises. Experiments verify the existence of these sensitivities on LLMs trained with DPO. Furthermore, we discuss implications of our findings on the robustness of value alignment. Specifically, we suggest that (1) there is a trade-off between the robustness of value alignment and modeling dominant preferences, and (2) employing K-tuple preference models with K\u2265 3 could mitigate the sensitivities in preference models and improve the robustness of value alignment.\nLimitations. The analysis of this paper assumed that preference models strictly follow the mathematical definitions with Assumption 1-5. Real-world agents, such as LLMs, are usually not preference models but are only trained with preference models. Therefore, these agents may exhibit similar but not exact sensitivities predicted by the theoretical analysis. Furthermore, the paper assumes a finite set of options, which is theoretically limited but probably less worrying in practice."}, {"title": "Experiment Details", "content": "Training and inference of large language models uses two NVIDIA-A100 GPUs each with 40 gigabytes of video memory. An 8-bit version of the AdamW optimizer provided by the Hugging Face's Bitsandbytes package is used to train the LLMs. In each experiment session, an LLM is trained using DPO for one epoch with learning rate set to $5 \\times 10^{-6}$ and temperature of DPO loss $\\beta'$ set to 0.1. Note that these details are not important for reproducibility, as the purpose of training is to fit LLMs to the dominant preferences in the dataset. The sensitivity should arise whenever LLMs exhibit strong preferences, regardless of how they were trained."}, {"title": "Sample Generation", "content": "We generate a series of datasets $D(\\omega_a, p_{12}, p_{23})$ based on $O_a = \\{dog, cat, bird\\}$, where $\\omega_a$ is one of the six permutations of $O_a$, $p_{12} = 0.99$, and $p_{23}$ varies from 0 to 1 with a step size of 0.05. Each sample in a generated dataset contains (1) a question, (2) a chosen answer, and (3) a rejected answer. When generating a sample for a specific dataset $D$, we first randomly sample a pair of options $(o_i, o_j)$ from $O_a$. Then, we sample from $Bernoulli(p)$ to determine, for this sample, whether the chosen answer prefers $o_i$ or $o_j$. The rejected answer will be set to express the opposite preference as the chosen one. We use the following templates to generate questions and answers, where <A> and <B> are replaced with the actual options."}]}