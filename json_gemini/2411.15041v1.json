{"title": "mR2AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA", "authors": ["Tao Zhang", "Ziqi Zhang", "Zongyang Ma", "Yuxin Chen", "Zhongang Qi", "Chunfeng Yuan", "Bing Li", "Junfu Pu", "Yuxuan Zhao", "Zehua Xie", "Jin Ma", "Ying Shan", "Weiming Hu"], "abstract": "Advanced Multimodal Large Language Models (MLLMs) struggle with recent Knowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their limited and frozen knowledge scope, often leading to ambiguous and inaccurate responses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally introduced to provide MLLMs with comprehensive and up-to-date knowledge, effectively expanding the knowledge scope. However, current mRAG methods have inherent drawbacks, including: 1) Performing retrieval even when external knowledge is not needed. 2) Lacking of identification of evidence that supports the query. 3) Increasing model complexity due to additional information filtering modules or rules. To address these shortcomings, we propose a novel generalized framework called multimodal Retrieval-Reflection-Augmented Generation (mR2AG), which achieves adaptive retrieval and useful information localization to enable answers through two easy-to-implement reflection operations, preventing high model complexity. In mR\u00b2AG, Retrieval-Reflection is designed to distinguish different user queries and avoids redundant retrieval calls, and Relevance-Reflection is introduced to guide the MLLM in locating beneficial evidence of the retrieved content and generating answers accordingly. In addition, mR2AG can be integrated into any well-trained MLLM with efficient fine-tuning on the proposed mR2AG Instruction-Tuning dataset (mR2AG-IT). mR\u00b2AG significantly outperforms state-of-the-art MLLMs (e.g., GPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while maintaining the exceptional capabilities of base MLLMs across a wide range of Visual-dependent tasks.", "sections": [{"title": "1. Introduction", "content": "The rapid development of Multimodal Large Language Models (MLLMs) [1, 2, 26, 29, 30, 40] enables them to excel in Visual-dependent VQA tasks that rely solely on visual content or common sense, such as VQAv2 [17], GQA [19] and TextVQA [42]. However, recently proposed Knowledge-based VQA tasks like INFOSEEK [9] and Encyclopedic-VQA [35], introduce fine-grained visual entities and focus on encyclopedic knowledge, posing significant new challenges for existing MLLMs. As shown in Q2 of Figure. 1, when queried about the first flight date of an airplane, typical MLLMs tend to provide inaccurate and overly general responses due to their limited and frozen knowledge scope.\nTo obtain accurate and specific answers, some works [7, 18, 45] introduce multimodal Retrieval-Augmented Generation (mRAG) that leverages external knowledge bases."}, {"title": "2. Related Work", "content": "Knowledge-based VQA. Several works [9, 35] focus on Knowledge-based VQA and construct corresponding datasets. Early OK-VQA [33] and its variant A-OKVQA [41] emphasize the significance of knowledge in VQA but primarily focus on commonsense. ViQuAE [23] introduces a wide range of entity types and tests fine-grained knowledge related to named entities. However, certain questions in Vi-QuAE can be answered without viewing the visual content.\nINFOSEEK [9] and Enc-VQA [35] address this by designing questions that force the model to examine the image for the correct answer. These datasets cover a broad range of Wikipedia entities and focus on fine-grained knowledge related to these entities. In contrast, INFOSEEK explicitly defines two splits: Unseen Entity and unseen Question, whereas Enc-VQA introduces more diverse question types, including single-hop, multi-answer, and two-hop questions.\nMultimodal Large Language Models. The rapid development of LLMs [5, 10-12, 38, 43] drives the progress of MLLMs [1, 2, 26, 29, 30, 40, 48]. Typical MLLMs, e.g., LLaVA, widely adopt a two-stage pre-training and fine-tuning paradigm, achieving impressive capabilities across various multimodal tasks [25, 27, 42, 46]. MLLMs perform well in understanding human queries [30], handling purely visual VQA tasks [17, 19], and addressing commonsense VQA tasks [33]. However, they struggle with Knowledge-based VQA tasks that involve fine-grained knowledge of specific visual entities, as shown by their performance on INFOSEEK and Enc-VQA [9, 35].\nRetrieval-Augmented Generation. RAG is widely used in LLMs to address challenges like hallucinations [47], non-renewable knowledge, and opaque reasoning [14, 21]. It combines LLMs' inherent knowledge with dynamic external knowledge, offering a solution for knowledge-intensive tasks [24, 32]. This technique is gaining traction within the MLLMs domain. For example, Wiki-LLaVA [7] retrieves"}, {"title": "3. Methodology", "content": "Knowledge-based VQA takes an image-question pair (I, Q) as input and is supported by an accessible knowledge base. As shown in Figure. 2, naive mRAG first retrieves the top-N articles most relevant to the input from the knowledge base, denoted as P = {P_i}_{i=1}^N, and then feeds the concatenation of P and the (I, Q) into the MLLM to directly generate the response y_{ans}. In contrast, mR2AG proposes two novel reflection operations to decouple the generation process into three steps: (1) Performing Retrieval-Reflection to determine whether retrieval is needed. (2) Performing Relevance-Reflection to identify evidence passages and generate answers accordingly. (3) Post-processing multiple potential answers. The presentation of mR2AG is organized as follows: Section 3.1 introduces the mR2AG method. Section 3.2 describes the training of mR2AG. Section 3.3 introduces the mR2AG-IT dataset for fine-tuning."}, {"title": "3.1. mR2AG Method", "content": "3.1.1. Retrieval-Reflection\nUser queries can be divided into Visual-dependent and Knowledge-based according to the input (I, Q). As shown in Figure 2, the question in case (a1) requires external knowledge for a confident answer, while the question in case (a2) can be answered entirely relying on visual content. Introducing external knowledge in the latter case may bring undesirable noise. To guide the model in distinguishing between different queries, we define two special tokens: [Retrieval]/[No Retrieval], to perform Retrieval-Reflection. First, the model generates retrieval-reflection predictions y_{ret} based on the input (I, Q):\ny_{ret} = MLLM(I, Q).\nDepending on the different results of y_{ret}, one of the following ways is executed:\n\u2022 y_{ret} = [No Retrieval]: The model determines that the question can be answered without external knowledge, and conditions on this token along with (I, Q) to generate the answer y_{ans}:\ny_{ans} = MLLM(I, Q, y_{ret} = [No Retrieval]).\n\u2022 y_{ret} = [Retrieval]: The model recognizes the need for external knowledge to answer the question and invokes retrievers to assist in further generation process.\nWe use English Wikipedia entries as the knowledge base, where the kth entry consists of a candidate image Ik, title Tk, and article Pk. mR2AG combines cross-modal and uni-modal retrieval to select the most relevant Wikipedia entries to the query image I. CLIP [39] is utilized to encode I, \u00cek and Tk, and calculates the cosine similarity of sim(I, \u00cek) and sim(I,Tk). The overall retrieval score S_{ret}^k for the kth entry is the average of two cosine similarities:\nS_{ret}^k = (sim(I, \u00ce_k) + sim(I,T_k))/2.\nThe supposed result P = {P_i}_{i=1}^N correspond to the articles of the top-N entries with the highest retrieval scores."}, {"title": "3.1.2. Relevance-Reflection", "content": "We divide each retrieved article P_i into multiple natural paragraphs. To enable the model to determine whether each segmented paragraph P_{ij} contains evidence relevant to the question Q, we introduce two relevance-reflection tokens: [Relevant]/[Irrelevant]. The model conditioned on the combination of P_{ij} and the query (I, Q) generates the relevance-reflection prediction y_{ij}^{rel}:\ny_{ij}^{rel} = MLLM(I, Q, P_{ij}).\nAccording to the result of y_{ij}^{rel}, mR2AG selects one of the following processes to perform:\n\u2022 y_{ij}^{rel} = [Irrelevant]: This indicates that the model perceives the P_{ij} as irrelevant to the query and lacking sufficient evidence, prompting the model to terminate the generation process and avoid producing unreliable answers.\n\u2022 y_{ij}^{rel} = [Relevant]: The model considers P_{ij} relevant to the query, containing evidence beneficial for answer generation, and thus proceeds to generate the answer y_{ans}^{rel} based on (I, Q, P_{ij}, y_{ij}^{rel}):\ny_{ans}^{rel} = MLLM(I, Q, P_{ij}, y_{ij}^{rel} = [Relevant])."}, {"title": "3.1.3. Answer Post-Processing", "content": "Multiple evidence passages may exist in an article, leading to generate multiple candidate answers. Therefore, post-processing is necessary to arrive at a single final answer.\nBased on the retrieval-reflection-augmented generation process, we apply a hierarchical post-processing to rank the candidate answers by integrating scores at three levels:\n\u2022 Entry-Level. The retrieval score in Equation. 3 measures the similarity between the query image I and the candidate Wikipedia entry, which serves as the Retrieval-Reflection score S_{ret}^i for the ith retrieved entry.\n\u2022 Passage-Level. The probability of generating the [Relevant] Relevance-Reflection token quantifies the model's confidence in judging P_{ij} as evidence, which can be defined as the Relevance-Reflection score S_{ij}^{rel}:\nS_{ij}^{rel} = P_\\theta(y_{ij}^{rel} = [Relevant] | I, Q, P_{ij}),\nwhere \\theta represents the parameters of MLLM.\n\u2022 Answer-Level. We calculate the probability of each token in the generated answer sequence and use the geometric mean to normalize the influence of sequence length variation, resulting in the answer confidence score S_{ij}^{ans}:\nS_{ij}^{ans} = \\sqrt[n]{\\prod_{k=1}^{n} P_\\theta(y_{ij}^{ans})},\nwhere n represents the sequence length. This score reflects the model's confidence in generating answers based on the retrieved content and reflection tokens.\nPost-processing. The three levels of scores comprehensively consider each step in the answer generation process, evaluating the reliability of the candidate answers at the entry, passage, and answer levels, respectively. The effects of the three scores are integrated by calculating their product, which serves as the final criterion for ranking candidate answers. The model outputs the answer with the highest score based on this criterion."}, {"title": "3.2. Training with Reflection Mechanism", "content": "During instruction tuning, we combine the common visual instruction tuning dataset, such as LLaVA-IT [30], with the specifically designed mR2AG-IT dataset:\n\u2022 For each sample in LLaVA-IT, we set the Retrieval-Reflection token to [No Retrieval]. The model is trained to answer the questions depends only on the visual content, and the training loss is formulated as:\nL_1 = -E_{(I,Q,y_{ret},y_{ans})\\sim D_{LLaVA-IT}} log p_\\theta(y_{ret} = [No Retrieval], y_{ans} | I, Q).\n\u2022 For each sample in mR2AG-IT, we set the Retrieval-Reflection token to [Retrieval]. The model is trained to invoke retrievers, identify evidence passages, and generate"}, {"title": "4. Experiments", "content": "4.1. Datasets\nINFOSEEK [9] contains a training set and three evaluation sets: INFOSEEKValidation, INFOSEEKWikidata and INFOSEEK Human. The training set, along with INFOSEEK Validation and INFOSEEKWikidata, are all derived from 1.3M samples automatically constructed from Wikipedia to support large-scale training and evaluation. INFOSEEK Human consists of 8.9K samples annotated by humans to simulate real information-seeking intentions. To"}, {"title": "4.2. Implementation details", "content": "We perform instruction tuning using the combination of the LLaVA instruction tuning dataset (LLaVA-IT) [30] and the mR2AG-IT dataset. Training continues from the stage-1 checkpoint of the LLaVA-v1.5-7B, with a learning rate of 2 \u00d7 10-5 and a batch size of 8 \u00d7 16, and lasts for one epoch. The main experiments and ablation studies are conducted based on the LLaVA-v1.5-7B [30], while mR2AG is also applicable to other MLLMs, such as Mini-Gemini [28] and"}, {"title": "4.3. Comparisons with SOTA", "content": "4.3.1. INFOSEEK\nWithout Knowledge. In the setting without external knowledge, the model predicts the answer based solely on the input image and question, relying on the knowledge stored in its parameters during training. To explore the performance of MLLMs under this protocol, we fine-tune LLaVA-v1.5-7B [30] using {V, Q, A} triples from the INFOS-EEK training set. After fine-tuning, the model's performance on INFOSEEKHuman improves from 9.5 to 12.0, and on INFOSEEKWikidata from 9.1 to 20.5. Additionally, the strongest models, GPT-4v [1] and GPT-4o [37], achieve scores of 12.1 and 21.3 on INFOSEEKHuman, respectively. These results suggest that while fine-tuning for specific datasets or using stronger models helps improve performance, current models still fall short in knowledge-based VQA tasks, underscoring the need for external knowledge.\nRetrieved Knowledge. Table. 1 presents the comparisons of models utilizing an external knowledge base on the INFOSEEK [9] benchmark. When leveraging the articles of the retrieved Wikipedia entries for answer generation, our mR2AG significantly outperforms the best existing models on all three test sets. Specifically, mR\u00b2AG surpasses LLM-RA [20], CLIP FiD [9], and EchoSight [45] by 15.5%, 10.6%, and 8.9% in overall accuracy on INFOSEEKWikidata, INFOSEEK Human, and INFOSEEK Validation.\nTo further verify that our improvement stems from the proposed mR2AG framework rather than simply the improved"}, {"title": "4.3.2. Encyclopedic VQA", "content": "As Table. 2 illustrates, when using the retrieved knowledge, mR2AG achieves superior performance, improving from 53.4% to 55.9% on single-hop questions and significantly increasing from 33.6% to 51.8% on the overlooked multi-answer questions. This indicates that mR2AG can effectively analyze noisy retrieved knowledge, accurately locate relevant information, and generate high-quality responses. Under the Oracle setting, when provided with more reliable knowledge sources, the performance of mR2AG improves dramatically from 55.9% to 88.2%, surpassing the previous SOTA of 87.0%. This suggests that: 1) Improving retrieval precision effectively enhances the performance of mR2AG, demonstrating the method's high potential. 2) With the same retrieval content, the superior performance underscores that the Relevance-Reflection mechanism introduced by mR2AG further strengthens the model's information extraction and reasoning capabilities. Overall, the state-of-the-art performance across multiple Knowledge-based VQA tasks showcases the broad applicability of mR2AG."}, {"title": "4.4. Comparisons on the Visual-dependent Tasks", "content": "We conduct comprehensive evaluations on widely used Visual-dependent benchmarks to demonstrate mR2AG's capabilities in handling questions about visual content or common sense. As shown in Table. 3, our model performs comparably to the base MLLM, i.e., LLaVA-V1.5-7B [29], on the MME [13] benchmark and surpasses it on LLaVAW [29], MMB [31], and POPE [27] benchmarks. These comparisons highlight the effectiveness of the mR2AG design, which strategically separates Visual-dependent tasks from"}, {"title": "4.5. Generalizability and Scalability", "content": "We validate the effectiveness of the mR2AG framework on three model architectures (Mipha [48], Mini-Gemini [28], and LLaVA [30]), covering three different scales of language models (3B, 7B, and 13B). In our experiments, we fine-tune these MLLMs from their stage-1 checkpoints using the same instruction-tuning data, while maintaining consistent hyperparameters such as learning rate, batch size and number of epochs. The results in Table. 4 show that these base models generally perform poorly on the INFOSEEK [9] benchmark. However, by introducing the naive mRAG approach or the mR2AG framework, we observe significant improvements in performance on Knowledge-based VQA tasks. Notably, the mR2AG framework consistently outperforms the naive mRAG approach, demonstrating its generalizability across different model architectures and scalability across varying language model sizes."}, {"title": "4.6. Ablation Studies", "content": "In this section, we conduct ablation studies to verify the effectiveness of our model's design choices. Most comparisons are performed on the INFOSEEKValidation dataset.\nEffect of S_{ret}, S_{rel} and S_{ans}. To evaluate the effectiveness of using the product of the Retrieval-Reflection score S_{ret}, Relevance-Reflection score S_{rel}, and answer confidence score S_{ans} in post-processing, we conducted a comprehensive comparison with all alternative ranking methods, as shown in Table 5. As observed, randomly selecting from answer candidates yields the lowest performance, which is"}, {"title": "4.7. Qualitative analysis", "content": "In Figure. 3, we present the qualitative comparison of the mR2AG with GPT-4o. As illustrated in Figure. 3 (a) and (b), our method retrieves relevant knowledge and accurately answers the Knowledge-based questions, achieving more precise results compared to GPT-4o. However, two failure cases are shown in Figure.3:\n\u2022 Inaccurate retrieval: As illustrated in Figure. 3 (c), when the subject in the image is difficult to identify, the retriever struggles to find relevant information, making it challenging for our method to answer the questions.\n\u2022 Knowledge interference: In Figure. 3 (d), the retriever finds the correct entity, but our method provides the wrong answer due to the conflicting knowledge in the text, specifically \"a suspended span of 564 feet.\""}, {"title": "5. Conclusion", "content": "This paper proposes an advanced multimodal RAG framework that optimizes Knowledge-based VQA tasks while maintaining the model's capabilities as a general-purpose MLLM. The approach is based on existing MLLMs, guiding the model to explicitly distinguish the type of user query and evaluate retrieved information to refine the naive multimodal RAG process. It improves inference efficiency by adaptively avoiding unnecessary retrievals. Furthermore, by explicitly evaluating the retrieved content, it can identify the evidence passages relevant to the query, while filtering out noise from the retrieved content, thereby enhancing the credibility of the generated responses. Future work will explore knowledge graph-based retrieval-augmented systems and broader application scenarios."}, {"title": "6. Limitation", "content": "Limited by the entity recognition capabilities of existing MLLMs, our method is quite dependent on the retriever, assuming that the default visual entity occupies the major position in the image. In the future, we plan to conduct specialized training for visual entity recognition tasks and guide the model to enhance the discrimination of visual entity recognition categories and locations."}, {"title": "7. Prompt Engineering", "content": "7.1. mR2AG-IT Dataset Annotation\nWe utilize the GPT-4 [1] model via API to annotate the training dataset and design the following prompt to assess the relevance between retrieved content and the query. Inspired by the chain-of-thought [44] approach, the prompt instructs GPT-4 [1] to extract evidence sentences before generating relevance judgments. This design not only enhances the accuracy of the judgments but also facilitates manual calibration. For each input, the content within curly braces {} is replaced with the corresponding actual input. In the following, \"question\", \"answer\", and \"paragraph\" correspond to Q, A, and Pij, respectively, as defined in Section 3.3."}, {"title": "7.2. INFOSEEK", "content": "INFOSEEK [9] evaluates generated answers using exact match, requiring the outputs to strictly match the annotated answers, which are typically concise and presented in the form of a single word or phrase. To ensure the outputs align with these requirements, we design the following prompt, guiding the model to focus on retrieved content and produce concise responses:\n\"Based on the retrieved document, answer the question with a single word or phrase.\""}, {"title": "7.3. Encyclopedic-VQA", "content": "The Enc-VQA [35] dataset includes both single-hop and multi-answer questions. For single-hop questions, we adopt the same prompt template as INFOSEEK. For multiple-answer questions, the model needs to generate several possible answers. We adjust the prompt to ensure the answers comply with the dataset requirements, enabling effective extraction of answer lists from the responses for evaluation:\n\"Based on the retrieved documents, answer the question as briefly as possible, using '&&' to connect multiple different answers.\""}, {"title": "8. Additional Experiment Results", "content": "Tables 9, 10, and 11 present the complete experimental results on INFOSEEK [9] across various question types. In the without external knowledge setting, the model relies solely on the knowledge encoded in its parameters to answer questions. As shown in Tables 9 and 11, we fine-tune the LLaVA model without integrating external knowledge. The results indicate that this approach only leads to limited performance improvements. Additionally, using APIs, we evaluate the performance of GPT-4v/o [1, 37] and Gemini-1.5-pro [40] on INFOSEEK Human. As shown in Table 9, although the GPT series models outperform other fine-tuned models, they remain inferior to the mR2AG framework. Overall, the complete experimental results on the INFOSEEK dataset demonstrate that mR2AG significantly improves accuracy across all question types, with the most notable enhancement observed in the Time category. These findings further underscore the superiority of our approach in addressing Knowledge-based VQA tasks."}, {"title": "9. Qualitative Results and Visualizations", "content": "Figure 4 qualitatively demonstrates the effectiveness of the mR2AG framework. It highlights the framework's ability to accurately assess the relevance between retrieved content and user queries, precisely locate evidence paragraphs within the retrieved documents, and generate reliable answers. Figure 5 provides additional visualization results, illustrating that mR2AG effectively handles various types of visual entities and question types, further validating the design's effectiveness and reliability. The last column presents additional error cases, where the primary issue lies in the failure to retrieve relevant Wikipedia entities for the visual content."}]}