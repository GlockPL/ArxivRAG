{"title": "End-to-End Long Document Summarization using Gradient Caching", "authors": ["Rohit Saxena", "Hao Tang", "Frank Keller"], "abstract": "Training transformer-based encoder-decoder models for long document summarization poses a significant challenge due to the quadratic memory consumption during training. Several approaches have been proposed to extend the input length at test time, but training with these approaches is still difficult, requiring truncation of input documents and causing a mismatch between training and test conditions. In this work, we propose CachED (Gradient Caching for Encoder-Decoder models), an approach that enables end-to-end training of existing transformer-based encoder-decoder models, using the entire document without truncation. Specifically, we apply non-overlapping sliding windows to input documents, followed by fusion in decoder. During backpropagation, the gradients are cached at the decoder and are passed through the encoder in chunks by re-computing the hidden vectors, similar to gradient checkpointing. In the experiments on long document summarization, we extend BART to CachED BART, processing more than 500K tokens during training and achieving superior performance without using any additional parameters.", "sections": [{"title": "1 Introduction", "content": "Summarization is a critical task in natural language understanding, aiming to reduce extensive information into its most essential content by generating a concise and coherent summary. In recent years, transformer-based (Vaswani et al., 2017) pretrained language models have shown remarkable success in abstractive summarization, primarily on short texts (Narayan et al., 2018; Nallapati et al., 2016; Gliwa et al., 2019), heavily relying on dependencies within the input text or context of words.\nDespite their success, these models face significant challenges when applied to long document summarization tasks (Shaham et al., 2022; Gorinski and Lapata, 2015; Kryscinski et al., 2022). One of the primary limitations is their inability to handle long input during training, due to the memory requirement being quadratic with respect to the sequence length. This often necessitates truncation of the input text during training, resulting in a loss of crucial information and hampering the quality of the generated summaries. This problem is particularly pronounced in domains that require processing of extremely long text, such as book summarization (Kryscinski et al., 2022), where maintaining the full context is essential for producing accurate and meaningful summaries.\nPrior work has attempted to address the limitation of processing long input, including designing attention mechanisms that are more memory efficient (Beltagy et al., 2020), dividing an input document into chunks (Bertsch et al., 2023; Ivgi et al., 2023; Xie et al., 2024; Yen et al., 2024), or extending context at test time (Ratner et al., 2023; Han et al., 2024). Despite all the effort, truncation during training (typically at 16K tokens) is ubiquitous and is the standard approach to dealing with memory issues during training, causing a mismatch between training and test conditions.\nTo tackle the problem of truncation, we propose CachED (Gradient Caching for Encoder-Decoder Models), a simple and efficient approach that enables end-to-end training of existing encoder-decoder transformer models for long document summarization. We follow the chunking approach in favor of its generality, allowing us to plug and play any pretrained models, but more importantly, providing us the opportunity to release memory between the computation of chunks. We only keep the final output of the encoder, and release the intermediate results of the encoder whenever possible. The fusion of encoder output happens at the"}, {"title": "2 Abstractive Summarization with Encoder-Decoder Models", "content": "The task of summarization is to produce a summary of M tokens Y1, Y2,...,yM given an input document of L tokens X1,X2,...,XL, where a token can be a word or a wordpiece (Wu et al., 2016). The dominant approach to abstractive summarization is to use an encoder-decoder model (Bahdanau et al., 2016; Raffel et al., 2020; Beltagy et al., 2020), where the encoder turns the input document into a sequence of hidden vectors and the decoder produces a summary attending to the hidden vectors. More formally,\nh1,..., h\u2081 = Enc(x1,...,XL), (1)\nwhere Enc is the encoder, and\nYm = Dec(y1,..., Ym\u22121, h1, ...,h\u2081) (2)\nwhere the decoder Dec is repeatedly called for m = 1,..., M. A transformer-based encoder typically consists layers of self-attention, and a vanilla implementation requires O(L2) of memory (Vaswani et al., 2017). Long-document summarization is the setting where L is large, making it difficult to store the intermediate results of the entire input in memory.\nA naive approach to solving the memory problem is truncating the input, only taking, say, the first 16,000 tokens as input and capping the length at min(16,000, L). Depending on the types of summarization, this approach can be sufficient, for example, for summarizing news articles. For long documents, such as books (Kryscinski et al., 2022) or movie scripts (Saxena and Keller, 2024), naively truncating the input makes it impossible to properly perform the task, as a model has no access to the truncated input. Despite the obvious limitation, truncation is widely used during training (Beltagy et al., 2020; Guo et al., 2022; Bertsch et al., 2023; Xie et al., 2024), and is sometimes the only option when scaling up the model size.\nAnother approach is to divide the input document into chunks, with each chunk encoded individually. More formally, the input document of length L is divided into K chunks, with each chunk of size [L/K]. Each chunk, denoted as x(k-1)[L/K]+1,...,xk[L/K], is encoded into h(k-1)[L/K]+1,...,hk[L/K], for k = 1,..., K. The memory requirement of this approach is O([L/K]2) per chunk, i.e., O(K \u00b7 [L/K]2) = O(L2/K) in total, less than the O(L2) when running self-attention on the entire sequence.\u00b9 Div-ing the input document into chunks is sometimes called a chunk-based approach (Xie et al., 2024), the sliding window approach (Ivgi et al., 2023), or parallel context (Yen et al., 2024; Ratner et al., 2023), in which chunks might or might not have overlaps. This approach also makes a modeling assumption: text representation can only be contextualized within each chunk, delaying further contextualization or fusion in the decoder.\nDespite the memory saving with sliding windows, the input documents are still truncated before chunking (Ivgi et al., 2023; Bertsch et al., 2023; Xie et al., 2024), because intermediate results are not released from memory after the computation of each chunk. Truncation of input doc-"}, {"title": "3 CachED: Gradient Caching for Encoder-Decoder Models", "content": "To address the compromise of truncating input documents, we propose gradient caching for encoder-decoder models (CachED), an approach that turns any existing transformer-based encoder-decoder models into a model for long document summarization without truncation.\nRecall that an input document of length Lis divided into K chunks and fitting O(L2/K) in memory is still difficult, especially when there are many layers. The O(L2/K) memory requirement is due to K calls to the encoder, each of which requires O(L2/K2). Instead of maintaining K calls simultaneously in memory, we decide to call the encoder K times in sequence, releasing the memory after each call and reducing the memory requirement to O(L2/K2). This can be easily done during inference, but not maintaining all K calls in memory makes computing the gradient difficult during training.\nTo compute the full gradient without truncation, we ideally want to break the computation up with respect to the K chunks. If we use J to denote the loss function and \u0398 to denote the parameters in the encoder, the relationship between the derivative of the individual K chunks and the gradient is\n\\frac{\\partial J}{\\partial \\Theta} = \\sum_{k=1}^{K} \\frac{\\partial J}{\\partial H_k} \\frac{\\partial H_k}{\\partial \\Theta}, (3)\nwhere Hk = [h(k-1)[L/K]+1,..., hk[L/K]] is the concatenation of the hidden vectors from the k-th chunk. The total derivative naturally leads to the following three steps.\n1. Compute the encoder output Hk for k = 1,..., K in sequence without storing the intermediate layers.\n2. Compute the loss J based on Hk and the gradient \u2202J/\u2202Hk with regular backpropagation.\n3. Re-compute Hk and the intermediate layers and use the cached \u2202J/\u2202Hk to compute"}, {"title": "6 Analyses", "content": "In this section, we present a comprehensive analysis of various aspects related to our approach and its performance. For our analysis, we will use the CachED BARTlarge model, which performs the best in our experiments. We first examine the utilization of the full context to understand how effectively our method handles and processes long inputs. Next, we analyze the method's performance in relation to document length, assessing how changes in length impact the model's efficiency and accuracy. We also address time and memory usage to evaluate the computational resources required by our approach. Finally, we assess the factual consistency of the model's outputs, ensuring that the generated summaries are not only better in terms of ROUGE but are also factually accurate.\n6.1 Utilization of the entire document\nTo evaluate whether CachED models effectively use the full context of a document, we use the alignment between the generated summary and the document as a proxy. We first divide the document into equal segments (bins) and map the summary sentences to these segments to estimate"}]}