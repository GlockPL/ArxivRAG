{"title": "DART: An Automated End-to-End Object Detection Pipeline\nwith Data Diversification, Open-Vocabulary Bounding Box\nAnnotation, Pseudo-Label Review, and Model Training", "authors": ["Chen Xin", "Andreas Hartel", "Enkelejda Kasneci"], "abstract": "Swift and accurate detection of specified objects is crucial for many industrial applications, such as safety monitoring on construction sites. However, traditional approaches rely heavily on arduous manual annotation and data collection, which struggle to adapt to ever-changing environments and novel target objects. To address these limitations, this paper presents DART, an automated end-to-end pipeline designed to streamline the entire workflow of an object detection application from data collection to model deployment. DART eliminates the need for human labeling and extensive data collection while excelling in diverse scenarios. It employs a subject-driven image generation module (DreamBooth with SDXL) for data Diversification, followed by an Annotation stage where open-vocabulary object detection (Grounding DINO) generates bounding box annotations for both generated and original images. These pseudo-labels are then Reviewed by a large multimodal model (GPT-40) to guarantee credibility before serving as ground truth to Train real-time object detectors (YOLO). We apply DART to a self-collected dataset of construction machines named Liebherr Product, which contains over 15K high-quality images across 23 categories. The current implementation of DART significantly increases average precision (AP) from 0.064 to 0.832. Furthermore, we adopt a modular design for DART to ensure easy exchangeability and extensibility. This allows for a smooth transition to more advanced algorithms in the future, seamless integration of new object categories without manual labeling, and adaptability to customized environments without extra data collection. The code and dataset are released at https://github.com/chen-xin-94/DART.", "sections": [{"title": "1. Introduction", "content": "In recent years, computer vision has witnessed rapid advancements, revolutionizing various applications through innovative methodologies. A significant development is the emergence of generative AI techniques, exemplified by text-to-image Diffusion models\n[1, 2, 3, 4, 5]. They establish a robust backbone for generating high-fidelity, contextually rich images, which in turn enables other specialized frameworks [6, 7, 8] including DreamBooth[9]. These frameworks build upon this foundation to enhance the personalization and precision of the generated images, providing more nuanced and versatile con-"}, {"title": "2. Related work", "content": "In the related work section, we will focus on four areas corresponding to the DART pipeline's core stages in order: subject-driven image generation, open-vocabulary object detection, and large multimodal models, real-time object detection. By discussing progress in these fields, we aim to justify our current implementation of state-of-the-art methods and explore the potential for further enhancements by incorporating more advanced modules in the future."}, {"title": "2.1. Subject-driven image generation", "content": "Text-to-image diffusion models [1, 2, 3, 4, 5] have swiftly advanced in recent years, establishing themselves as powerful tools in generative AI. These models generate high-quality images from textual descriptions through iterative denoising processes, forming the foundation for subject-driven text-to-image generation. Given only a few (typically 3-5) casually captured images of a specific subject, subject-driven image generation aims to produce customized images with high detail fidelity and variations guided by text prompts. One typical approach is to fine-tune a special prompt token to describe the concept of the specific subject. Textual Inversion [8] shows evidence that a single-word"}, {"title": "2.2. Open-vocabulary object detection", "content": "Open-vocabulary object detection (OVD) has become a cutting-edge approach in modern object detection. It aims to detect any objects given their corresponding text prompts beyond predefined categories. Recent groundbreaking works in OVD include ViLD [30] which distills CLIP [31] embeddings to cropped image embeddings from region proposal network (RPN) [32], as well as GLIP [33] where object detection is reformulated as a grounding task by aligning bounding boxes to phrases using early fusion. Moreover, Grounding DINO [12] extends the idea of cross-modality fusion to both feature extraction and decoder layers. It excels in zero-shot scenarios, demonstrating an impressive generalization ability to detect unseen objects. Nevertheless, one major issue of OVDs is that they are too computation- and memory-intensive to meet the needs of real-time and on-device applications. This perspective holds true even for the latest lightweight OVDs such as YOLO-world [34]. Therefore, instead of directly using OVD to solve our tasks, the proposed DART pipeline leverages OVD to generate ground truth as training data for downstream real-time object detectors. Combined with the LMM-based bounding box review (see Section 2.3 and Section 3.4), our approach results in a faster downstream detector and alleviates the speed limitations inherent in OVD."}, {"title": "2.3. Large multimodal model", "content": "Recent advancements in large multimodal models (LMMs) have been characterized by the release of several groundbreaking proprietary systems, including GPT-4v [10], Claude 3 [35], and Gemini [36]. These models have collectively set a new standard for vision-language understanding, demonstrating unprecedented capabilities in combining visual inputs with advanced language processing. Recently, GPT-40 [11] further extends these capabilities with enhanced contextual awareness and multimodal reasoning, pushing the boundaries of LMM by achieving state-of-the-art performance across a wide range of vision-language tasks. Meanwhile, the open-source community has also been actively advancing in this domain. Flamingo [37] utilizes visual and language inputs as prompts,"}, {"title": "2.4. Real-time object detection", "content": "Real-time object detection aims to classify and locate objects in real-time with low latency, which is essential for safety-related industrial applications, e.g., on a construction site. This area has witnessed significant advancements in recent years, primarily driven by the iterative development of state-of-the-art YOLO models. Early development of YOLO from v1 to v3 [13, 14, 15] determined the optimal segmentation of the real-time object detection network structure: backbone, neck, and head. YOLOv4 [16] and YOLOv5 [17] changed the backbone to CSPNet [47] and introduced a series of best practices in computer vision regarding training strategy, post-processing, as well as plugin modules. Based on these improvements, YOLOv6 [20] and YOLOv7 [19] are further fine-tuned in label assignment strategy[48] and loss functions [49, 50, 51], and they utilize Repblock [52] and E-ELAN [53] as the core of their backbone respectively. YOLOv8 [21] introduces the C2f building block for effective feature extraction and fusion, along with a comprehensive set of integrated tools, solidifying its position as the go-to choice for real-time object detection. YOLOv9 [22] proposes programmable gradient information (PGI) to refine the training process and a new lightweight network architecture \u2013 Generalized Efficient Layer Aggregation Network (GELAN). Finally, the concurrent work YOLOv10 [23] presents an NMS-free training strategy and polishes the mode architecture with holistic efficiency-accuracy-driven design, contributing to its state-of-the-art real-time object detection capabilities. Considering YOLOv8's ease of use and YOLOv10's optimal tradeoff between speed and accuracy, this work employs both the YOLOv8 and YOLOv10 families as the real-time object detectors of choice. Although we have selected the best YOLO models, their reliance on predefined object categories restricts their applicability in open scenarios. Therefore, real-time object detection only serves as the last stage in DART, depending on the other three stages to provide high-quality training data."}, {"title": "3. Methods", "content": "The proposed method, DART, is an automated end-to-end object detection pipeline that spans the entire workflow of an object detection application from data collection to model deployment. As illustrated in Figure 1, DART starts with dataset preprocessing, followed by four main stages: data Diversification through subject-driven image generation, bounding box Annotation via OVD, LMM-based Review of generated images and"}, {"title": "3.1. Dataset preprocessing", "content": "The data preprocessing phase, illustrated by the orange block in Figure 1, involves identifying classes and instances from raw data as input for prompt engineering and deduplication to obtain clean data."}, {"title": "Class identification.", "content": "The raw dataset is sourced from Liebherr's internal digital system, where images of Liebherr's products are initially roughly categorized in the database. By refining these preliminary classifications with detailed product descriptions, we establish 23 accurately defined classes, as detailed in Section 4.1. We find the precise definition of the class name essential because it highly affects the performance of the following data diversification phase since ambiguous class definition leads to inferior quality of generated data of that class. As a result, each image in the raw dataset is assigned at least one precise class name based on its primary object, and this broad assignment will be used as one of the text prompts for bounding box annotation, as described in Section 3.3."}, {"title": "Instance identification.", "content": "For images containing a single object, we tag them at the instance level using distinctive metadata information, such as product numbers. For each class, we randomly selected 2-5 instances and manually curated approximately 10 images for each instance (see Figure A.14). These images will be further utilized for data diversification (Section 3.2). Note that tagging with instance labels is a straightforward and easy task since we only need 20-50 images in total for each class."}, {"title": "Prompt engineering.", "content": "Class names for images are the cornerstone for prompt construction for image generation (Section 3.2), OVD (Section 3.3), and LMM-based review (Section 3.4). The constructed class-specific prompts vary slightly for different tasks. Instance tags, however, will only serve as the backbone for instance-specific prompts for training SDXL models under the DreamBooth framework. Specific prompt construction rules will be introduced in the corresponding sections mentioned above and further detailed in Appendix B. The precise text prompts for data diversification, annotation, and review stage are listed in Table B.9, Table B.11, and Table B.12, respectively."}, {"title": "Deduplication.", "content": "The initial raw dataset contains many duplicated images. We employ perceptual hashing (pHash) [54] for preliminary deduplication. However, cash tends to misidentify similar images as duplicates, for example, consecutive shots of the same scene from slightly different angles. These near duplicates are manually retained in the dataset but are ensured to appear only in the training set.\nUpon completing these steps, we obtain a clean dataset, dubbed Liebherr Products (LP). We also acquire instance-specific and class-specific prompts for the following phases. It should be noted that all human interventions within the DART framework conclude here, and the subsequent four phases of DART are fully automated."}, {"title": "3.2. Data diversification based on DreamBooth with Stable Diffusion XL", "content": "Data diversification and augmentation in DART are deeply rooted in the latest image-generation techniques. One of the most popular ones is Stable Diffusion, which represents a series of latent diffusion models that iteratively denoises random Gaussian noise in the latent space of a pre-trained autoencoder using a U-Net architecture [55] conditioned on textual prompts. Stable Diffusion XL (SDXL) introduces several architectural improvements over previous versions of Stable Diffusion, including a larger U-Net backbone, more powerful text encoders, and novel conditioning schemes to enhance the visual fidelity of generated samples, resulting in significantly improved performance. However, SDXL takes only text prompts and, therefore, cannot mimic the appearance of a specific subject given by visual prompts, which is essentially our goal for the data diversification stage. DreamBooth addresses this issue by fine-tuning a pre-trained SDXL model using a few images of a specific subject as ground truth. The key idea is to bind a unique textual identifier with the visual subject, allowing the model to synthesize novel photorealistic images of the subject in diverse scenes and poses with the help of diversified text prompts. The framework of DreamBooth with SDXL is illustrated in Figure 2. Without loss of generality, we showcase the class \"articulated dump truck\" and one of its instances, \"TA230\".\nDuring training, a pre-trained SDXL model \u03a9e with frozen weights @ is first applied to generate a bunch of class images $x_{cls} = \u03a9_\u03b8(\u03f5', c_{cls})$, given initial noises $\u03f5' ~ N(0, I)$ and a conditioning vector $c_{cls} = \u0393_{\u03b8'}(P_{cls})$ generated by a text encoder \u0393 with frozen weights \u03b8' using a class-specific text prompt $P_{cls}$, which takes the form \"a photo of a {class_name}\".\nThen, the same SDXL model is fine-tuned by setting 0 and \u03b8' to be trainable. In the actual implementation, we do not train the original parameters of the SDXL model and its text encoder directly but adopt LoRA (Low-Rank Adaptation) [56] instead which freezes"}, {"title": "", "content": "the pre-trained model weights and injects trainable low-rank matrices into the transformer layers' weight matrices to enable efficient adaptation with minimal additional parameters. Here, we slightly abuse the notation of \u03b8 and \u03b8' to also represent LoRA parameters.\nThe loss of the fine-tuning process is composed of two parts. The first part is the standard reconstruction loss Lrec for diffusion model training. Specifically, it's a squared error (SE) loss between a ground truth instance image x and a denoised version of it, predicted by the model Ng at a specific diffusion timestep $t ~U[0, 1]$:\n$L_{rec} = w_t (\u03a9_\u03b8(\u03b1_tx + \u03c3_t\u03f5, c_{ins}) \u2013 x)^2$ (1)\nwhere $\u03b1_t$, and $\u03c3_t$ are functions of timestep t that control the noise schedule of the diffusion process. As for $w_t$ in Equation 1, we adopt Min-SNR weighting strategy [57] to avoid the inefficiency and instability caused by conflicts among timesteps. Specifically, $w_t$ = $\\min(\u03b1_t^2/\u03c3_t^2, \u03b3)$, where \u03b3 is a hyperparameter to avoid the model focusing too much on small noise levels. Note that the encoded instance-specific prompt cins is used for the prediction of the ground truth instance. In contrast to class-specific prompts Pcls that only provide a general description of a class, instance-specific prompts Pins should include both a general class description for better image quality and a unique identifier to build the correspondence to the given visual instance. Therefore, Pins utilizes the extracted instance name (usually the product number) wrapped in angle brackets as the identifier (e.g. \"<TA230>\" in Figure 2) followed by the original class name.\nThe second component of the overall loss is the class-specific prior preservation loss Lpr which is essentially the same SE loss but treats the class images Xcls as ground truth instead. It is introduced to mitigate the overfitting issue of the standard fine-tuning. With both reconstruction loss Lrec and class-specific prior preservation loss Lpr, DreamBooth encourages diversity in the generated images while preserving the key visual features of the subject. The total loss L can be expressed as follows:\n$L = L_{rec} + \u03bbL_{pr}$ \n$= w_t (\u03a9_\u03b8(\u03b1_tx + \u03c3_t\u03f5, c_{ins}) \u2013 x)^2 + \u03bbw_{t'} (\u03a9_\u03b8(\u03b1_{t'}x_{cls} + \u03c3_{t'}\u03f5', c_{cls}) \u2013 x_{cls})^2$ (2)\nWe fine-tune a specific SDXL model with the DreamBooth framework for each instance. Diversified text prompts can then guide the trained SDXL models to depict each specific instance in novel contexts. We carefully design 67 text prompts in total that cover a wide and diverse range of scenarios, weather, poses, and quantities. Please refer to Table B.9 for the list of prompts used during inference for data generation. Note that to achieve our goal of diversification, we sampled a wide range of prompts covering various scenarios. As we will see in Section 4.6 and Appendix C, DART's data diversification stage effectively follows the scenarios and requirements mentioned in the text, demonstrating excellent adaptability to customized environments with minimal human intervention. At the end of the data diversification block, we obtain diversified data ready to be annotated."}, {"title": "3.3. Open-vocabulary bounding box annotation via GroundingDINO", "content": "Grounding DINO is an open-vocabulary object detector that marries a transformer-based detector DINO [5] with grounded pre-training, which can draw bounding boxes for arbitrary objects given inputs as plain text. The key to Grounding DINO's open-set capability is the tight fusion of language and vision modalities, achieved through three main components: a feature enhancer, a language-guided query selection, and a"}, {"title": "", "content": "cross-modality decoder. The feature enhancer fuses language and visual features by interleaving text-to-image and image-to-text cross-attention based on features extracted from text and image backbones in the early stage. The language-guided query selection uses language information (text features) to guide the selection of detection queries from image features, and the cross-modality decoder extends standard cross attention to two layers using image and text features as keys and values right before model outputs.\nIn this study, we leverage Grounding DINO for the bounding box annotation of target classes. An intuitive approach is to concatenate class names with delimiters('.' for Grounding DINO) into one text prompt as input to the model. However, we find that the model performance decreases exponentially as the length of the text grows. Thus, we turn to a strategy of incrementally adding and replacing text prompts to ensure that we ensure higher scores for ground truth accuracy.\nAs described in Algorithm 1, we start by using the class name that is roughly assigned during data collection (see Section 3.1) as the original prompt. To avoid misunderstanding caused by unclear semantics of the text, we design the synonym prompt using synonyms for the categories with ambiguous meanings (e.g., a handler can mean either the machine or the human operator), common aliases (e.g., dozer for bulldozer) or explicit superclasses (e.g., crane for crawler crane). A list of all synonyms can be found in Table B.10. Note that we input one synonym at a time to the model to generate bounding boxes for all synonym prompts to trade redundancy for high accuracy. The class-agnostic NMS (the last step of Algorithm 2) will finally address the redundant annotations.\nTo address the common phenomenon of multiple object categories in a single image and avoid the subpar performance of concatenating all classes as text prompts, we utilize the information (collected during data preprocessing) on frequently co-occurring object combinations to create another type of prompt dubbed co-occurring prompt. Unlike the original and synonym prompts, where one phrase is presented at a time, the co-occurring prompt concatenates the names of objects commonly appearing together with delimiters into a single text. For instance, three classes of the LP dataset, \"mining trucks,\" \"mining excavators,\" and \"mining bulldozers,\" often appear simultaneously but rarely with other classes. Thus, images categorized into these three classes during preprocessing are given a co-occurring prompt that combines the names of the corresponding classes. This construction rule is necessary because Grounding DINO tends to produce identical boxes but different class labels when the separately given text prompt is semantically similar, which is often the case for co-occurring classes. This issue is significantly alleviated when these semantically similar texts are input into Grounding DINO as one text prompt. However, the ultimate solution to this problem is the class-agnostic NMS, which will be discussed in the next paragraph. Additionally, we apply the same rule for constructing co-occurring prompts for classes with synonyms, iterating through all combinations of these synonyms.\nAfter getting the output bounding boxes and their corresponding confidence scores from original, synonym, and co-occurring prompts for all images, we apply filtering and non-maximum suppression (NMS) to keep only the best bounding boxes. As illustrated in Algorithm 2, we first sift out the ones with confidence scores bigger than 0.5 unless there's only one box for an image, in which case we take the one regardless. Then, we transform labels derived from various text prompts back to their original class names from synonyms and finally adopt class-agnostic NMS to address the issue of confusing similar labels and keep only the most feasible annotations. The remaining bounding boxes with their given class labels and scores are the candidate pseudo-labels to be reviewed."}, {"title": "3.4. LMM-based review of pseudo-labels and image photorealism using InternVL-1.5 and GPT-40", "content": "LMM leverages the synergy between textual and visual data to perform tasks that require cross-modality comprehension, such as visual question answering (VQA). In the"}, {"title": "", "content": "VQA setup, questions with hints and requirements are first summarized as text prompts and then further processed by a (preferably transformer-based [58]) language model. The extracted text features are combined with encoded image features (usually by a ViT [59]) and then passed through another transformer to generate the text answer.\nWhile LMMs excel in semantic understanding and comprehension, they face challenges in numeric-related tasks. For instance, directly generating bounding boxes can be problematic, as depicted in a failure case of GPT-40 in Figure 3a. In contrast, GPT-40 can precisely interpret the bounding box and its associated class label drawn directly on the image as visual hints, demonstrated by its correct judgments of whether the given boxes and labels are both accurate and complete, as shown in (b)-(c) in Figure 3.\nThe proposed DART pipeline leverages GPT-40 to review the pseudo-labels generated in the bounding box annotation stage. We draw the predicted bounding boxes together with their corresponding class labels and confidence scores on top of the raw images and use these synthesized images as the visual prompts. In the text prompt, we ask GPT- 40 to behave as an expert bounding box reviewer and make judgments based on three criteria: precision, recall, and fit. The corresponding three questions are:\n1. Precision: Does each bounding box perfectly enclose one single target object?\n2. Recall: Are all target objects localized by a bounding box?\n3. Fit: Is each bounding box neither too loose nor too tight?"}, {"title": "3.5. Real-time object detector training for YOLOv8 and YOLOv10", "content": "In the realm of real-time object detection, YOLO frameworks have established themselves as the dominant methodology primarily because they effectively balance computational efficiency with high detection accuracy. Currently, the most popular YOLO variant is YOLOv8. As illustrated in the figure, YOLOv8's architecture (Figure 4) inherits the overall framework from previous YOLO iterations, utilizing the Darknet [15] backbone, SPP (Spatial Pyramid Pooling [60]) and PAN (Path Aggregation Network [61]) structure in the neck, and three multi-scale decoupled heads to predict bounding box coordinates and class labels. For the major block in the backbone and neck, YOLOv8 employs the C2f module, which is a faster implementation of the CSPBlock [47] featuring two convolution layers at each end, with the CSP Bottlenecks in between (depicted as the red block in Figure 4). In terms of training strategy, the model adopts an anchor-free approach and incorporates Task Alignment Learning (TAL) [48] for ground truth assignment and Distribution Focal Loss (DFL) [62] as the regression loss. These enhancements contribute to improved computational efficiency and detection accuracy. The concurrent YOLOv10 is the latest release in the YOLO family. It proposes a consistent dual assignments strategy for NMS-free training as well as a holistic efficiency-accuracy-driven model design featuring comprehensive adoption of pointwise, depthwise, and large-kernel convolutions with partial self-attention (PSA), which greatly reduces the computational overhead and enhances model capability.\nWith approved (Section 3.4) diversified (Section 3.2) images and their corresponding pseudo-labels (Section 3.3), training a real-time object detector becomes plain and easy. Since DART's output will be deployed on edge devices, we select and train only the nano (n) and small (s) variants of YOLOv8 and YOLOv10. These models, pre-trained on the COCO dataset [64], are fine-tuned using approved diversified data and pseudo-labels. Extensive experimentation reveals the best-performing YOLO models, considering the tradeoff between accuracy, speed, and model size (see Section 4.5.4). By demonstrating"}, {"title": "4. Experiments", "content": "4.1. Dataset Data collection. We collect a dataset named Liebherr Products (LP) from the internal database of Libherr, a German-Swiss multinational equipment manufacturer. During data collection, we also employ class and instance identification (in Section 3.1) for bounding box annotation (in Section 3.3) and data diversification (in Section 3.2), respectively."}, {"title": "Dataset composition.", "content": "The LP dataset (after preprocessing) encompasses more than 15K images of Liebherr's extensive range of heavy machinery such as construction machines, earthmoving equipment, deep foundation machines, mining machines, various types of cranes, material handling machines, and so on. A list of the exact 23 categories can be found either as the x-axis tick labels in Figure 5 or in the first column of Table B.10. Figure 5 illustrates the distribution of images in each category, represented by the blue bars labeled \"0:1\". The unbalanced data distribution in the LP dataset makes"}, {"title": "Dataset split.", "content": "The dataset is initially split into a training and a test set with an 80/20 distribution. To address class imbalance, stratified random sampling ensures each class is adequately represented in the test set. We allocate 20% of the training set to create a validation set, which is used for hyperparameter fine-tuning. After obtaining the optimal hyperparameter set, the model is trained on the original training with 12K images and evaluated on the 3K images from the test set. The training set could incorporate generated data in specific experiment setups to enhance data diversification. Unless otherwise specified, the generated-to-original data ratio is maintained at the optimal proportion of 3:1, as outlined in Section 4.5.1. Notably, categories that already exhibit strong performance with only the original data are excluded from the augmentation with generated data, as detailed in Section 4.5.4. All performance metrics reported in the tables and figures of this paper are based on the test set."}, {"title": "4.2. Evaluation metrics", "content": "In this work, we adopt average precision (AP) to evaluate the final object detection models. AP first calculates the precision p and recall r at different Intersection over Union (IoU) thresholds t as follows:\n$p(t) = \\frac{TP}{TP + FP}, r(t) = \\frac{TP}{TP + FN},$ (3)\nwhere detections are classified as true (T) or false (F) according to their confidence scores and as positive (P) or negative (N) based on the IoU threshold t.\nThen a modified version of precision-recall curve p(r) is constructed separately for each class c\u2208 C:\n$p(r, c) = max_{r'>r} p(r', c),$ (4)\nso that the curve will decrease monotonically instead of the zigzag pattern.\nFinally, we adopt two commonly used AP metrics: AP50 and AP50-95. The former measures precision at a single IoU threshold of 50%, while the latter averages precision across 10 IoU thresholds ranging from 50% to 95% in increments of 5% ([.50:.05:.95]). Mathematically, AP50\u201395 can be expressed as follows:\n$AP_{50-95} = \\frac{1}{|C|}\\frac{1}{10}\\sum_{c\u2208C} \\sum_{t\u2208[.50:.05:.95]}p(r(t), c).$ (5)\nAP50-95 is always smaller than AP50 since the precision decreases for a given recall with higher thresholds. However, both metrics exhibit similar trends in diverse and complex datasets, such as LP. This consistent pattern will also be observed in the following tables. Consequently, the following sections will primarily focus on the AP50\u201395. It is important to note that AP50\u201395 is also commonly abbreviated as AP, so we will use these two terms interchangeably."}, {"title": "4.3. Implementation details", "content": "4.3.1. Implementation of DreamBooth with SDXL We instantiate DreamBooth with SDXL as the subject-driven image generation framework for the data diversification phase. The SDXL model is preferable, as it significantly outperforms previous versions of Stable Diffusion models such as SD-1.5 [1]. We incorporate prior preservation loss to prevent overfitting and adjust its weight () in Equation 2) to 1.0. Contrary to SD-1.5, training the text encoder of SDXL has limited effects; we still opt to fine-tune it to enhance overall performance marginally. We extract 69 instances for the training of DreamBooth with SDXL in total. The minimum number of images per instance is set to 3 (as suggested by [9]), while the majority of instances have 10 images each (see Figure A.14). We implement dynamic training steps by adjusting the maximum training steps based on the number of images collected for the corresponding instance. 100-160 steps per image are proven to be suitable, combining with the AdamW optimizer [65] with a learning rate 1 \u00d7 10-4 for U-Net and 5 \u00d7 10-6 for the text encoder. The SNR-gamma is set to 5 by default. Other important hyperparameters are explained in Section Appendix A.1.\nWe fine-tune two models from the same official pre-trained checkpoint for each instance: one with slightly more steps to ensure resemblance and one with fewer for diversification. For training, a general class-specific prompt, \"a photo of a {class_name},\" is employed to generate class images for prior preservation loss. As for instance-specific prompts, we use the instance name (product number if available, otherwise a descriptive word) to form the prompt as \"a photo of a <{instance_name}> {class_name}.\" We also explore different approaches to constructing identifiers and class names. However, our experiments reveal that subtle modifications have negligible effects on performance as long as the identifier is sufficiently distinctive and the class name is adequately precise. To meet the requirements of data diversification, we craft inference text prompts encompassing a broad spectrum of scenarios, as listed in Section Appendix B.1. The training processes are executed on either a single NVIDIA A100 80GB GPU or two NVIDIA 3090 GPUs, with each configuration capable of completing the training of one instance-specific SDXL model in less than an hour. The inference process requires substantially less GPU memory and is compatible with any memory optimization techniques for SDXL."}, {"title": "4.3.2. Implementation of Grounding DINO", "content": "Off-the-shelf Grounding DINO base model is employed for bounding box annotation. Through extensive comparative experiments, we determined that a box threshold of 0.27 and a text threshold of 0.25 generally returns high-quality bounding boxes, resulting in only 18 images without annotations, which are consequently excluded from the pipeline. The original collected images and approved generated images are isometrically resized to 800/1333 pixels for the shorter/longer side before going through the model. As for text prompts, we follow instructions given in Section 3.3 to construct all three prompt types as separate inputs to the Grounding DINO model and apply filtering and NMS to the combined results to obtain the candidate bounding boxes to be reviewed. The annotation process is mainly carried out (including speed test as shown in Table 3) by an NVIDIA A100 80GB GPU with FP16 for our case."}, {"title": "4.3.3. Implementation of LMM", "content": "We leverage GPT-40 by calling OpenAI's API for pseudo-label review. The bounding boxes and class labels are directly drawn on top of the image, and the synthesized image"}, {"title": "4.3.4. Implementation of YOLO", "content": "As the final step of the DART pipeline, we fine-tune COCO pre-trained YOLOv8 and YOLOv10 models as the output object detector to be deployed. We follow common practice [17, 20, 18, 19, 21, 22, 23] to resize all images to 640\u00d7640. Then, AdamW is implemented with a weight decay set to 5e-4 as the optimizer. Next, we conduct extensive experiments focusing on learning rate and its scheduling for each model and each experiment setup. The optimal initial learning rates range from 7e-5 to 5e-4, with a general trend indicating that larger models and bigger datasets benefit from lower learning rates. Mosaic augmentation [16, 17] is adopted to enhance data diversity. Details on other critical hyperparameters are provided in Section Appendix A.3. Speed tests (including NMS for YOLOv8, referenced from the official codebase or paper) are conducted on an NVIDIA T4 GPU using TensorRT and FP16. The test results are reported in Table 6."}, {"title": "4.4. Ablation study on the overall DART pipeline", "content": "In this section, we perform an ablation study on the overall DART pipeline. We systematically analyze the contributions of each major component by evaluating the test performances of trained real-time object detection models under various configurations. Specifically, we incrementally integrate the first three stages of DART, i.e., Grounding DINO for annotation, GPT-40 for review, and DreamBooth with SDXL for data diversification, as shown in Table 1. To evaluate the impact of each newly incorporated component, we leverage the overall performance of DART's final stage as the metric. Various YOLO models with different versions and scales, including YOLOv8n, YOLOv8s, YOLOv10n, and YOLOv10s, are fine-tuned for a comprehensive assessment. We observe a consistent trend in performance increase across all four YOLO models as new components of DART are introduced. Therefore, the subsequent analyses will primarily use YOLOv8n as the representative model. For the complete test results on the other models, please refer to Table 1.\nOur baseline configuration (#0) utilizes a YOLOv8n model without any additional modules from DART, yielding the lowest performance with an AP of just 0.064. In this setup, the model is trained on one of the most abundant datasets, LVIS [27]. Despite LVIS's extensive coverage of over 1.2K categories across 164K images, it falls short of capturing all the classes present in our self-collected LP dataset. Consequently, we assign each LP's category to its closest kin in LVIS during inference, guided by similarity scores calculated from class names and descriptions by SBERT [66]. The results, however,"}, {"title": "4.5. Quantitative Analysis", "content": "In this section, we conduct internal analyses for the four major stages of the proposed DART pipeline by investigating the best solutions for each key module via comprehensive experiments and ablation studies within each stage."}, {"title": "4.5.1. Analysis of DreamBooth with SDXL", "content": "We begin by examining the impact of incorporating extra training data generated by SDXL models trained for each instance using the DreamBooth framework. Across all configurations, we maintain a"}]}