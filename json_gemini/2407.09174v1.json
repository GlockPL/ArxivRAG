{"title": "DART: An Automated End-to-End Object Detection Pipeline\nwith Data Diversification, Open-Vocabulary Bounding Box\nAnnotation, Pseudo-Label Review, and Model Training", "authors": ["Chen Xin", "Andreas Hartel", "Enkelejda Kasneci"], "abstract": "Swift and accurate detection of specified objects is crucial for many industrial applica-\ntions, such as safety monitoring on construction sites. However, traditional approaches\nrely heavily on arduous manual annotation and data collection, which struggle to adapt\nto ever-changing environments and novel target objects. To address these limitations,\nthis paper presents DART, an automated end-to-end pipeline designed to streamline\nthe entire workflow of an object detection application from data collection to model\ndeployment. DART eliminates the need for human labeling and extensive data collec-\ntion while excelling in diverse scenarios. It employs a subject-driven image generation\nmodule (DreamBooth with SDXL) for data Diversification, followed by an Annotation\nstage where open-vocabulary object detection (Grounding DINO) generates bounding\nbox annotations for both generated and original images. These pseudo-labels are then\nReviewed by a large multimodal model (GPT-40) to guarantee credibility before serv-\ning as ground truth to Train real-time object detectors (YOLO). We apply DART to\na self-collected dataset of construction machines named Liebherr Product, which con-\ntains over 15K high-quality images across 23 categories. The current implementation of\nDART significantly increases average precision (AP) from 0.064 to 0.832. Furthermore,\nwe adopt a modular design for DART to ensure easy exchangeability and extensibility.\nThis allows for a smooth transition to more advanced algorithms in the future, seamless\nintegration of new object categories without manual labeling, and adaptability to cus-\ntomized environments without extra data collection. The code and dataset are released\nat https://github.com/chen-xin-94/DART.\nKeywords: Open-vocabulary object detection (OVD), Data diversification,\nPseudo-label, Large multimodal model (LMM), Stable Diffusion, GPT-40, YOLO", "sections": [{"title": "1. Introduction", "content": "In recent years, computer vision has witnessed rapid advancements, revolutionizing\nvarious applications through innovative methodologies. A significant development is the\nemergence of generative AI techniques, exemplified by text-to-image Diffusion models"}, {"title": "2. Related work", "content": "In the related work section, we will focus on four areas corresponding to the DART\npipeline's core stages in order: subject-driven image generation, open-vocabulary ob-\nject detection, and large multimodal models, real-time object detection. By discussing\nprogress in these fields, we aim to justify our current implementation of state-of-the-\nart methods and explore the potential for further enhancements by incorporating more\nadvanced modules in the future."}, {"title": "2.1. Subject-driven image generation", "content": "Text-to-image diffusion models [1, 2, 3, 4, 5] have swiftly advanced in recent years,\nestablishing themselves as powerful tools in generative AI. These models generate high-\nquality images from textual descriptions through iterative denoising processes, forming\nthe foundation for subject-driven text-to-image generation. Given only a few (typically\n3-5) casually captured images of a specific subject, subject-driven image generation aims\nto produce customized images with high detail fidelity and variations guided by text\nprompts. One typical approach is to fine-tune a special prompt token to describe the\nconcept of the specific subject. Textual Inversion [8] shows evidence that a single-word"}, {"title": "2.2. Open-vocabulary object detection", "content": "Open-vocabulary object detection (OVD) has become a cutting-edge approach in\nmodern object detection. It aims to detect any objects given their corresponding text\nprompts beyond predefined categories. Recent groundbreaking works in OVD include\nViLD [30] which distills CLIP [31] embeddings to cropped image embeddings from region\nproposal network (RPN) [32], as well as GLIP [33] where object detection is reformulated\nas a grounding task by aligning bounding boxes to phrases using early fusion. Moreover,\nGrounding DINO [12] extends the idea of cross-modality fusion to both feature extrac-\ntion and decoder layers. It excels in zero-shot scenarios, demonstrating an impressive\ngeneralization ability to detect unseen objects. Nevertheless, one major issue of OVDs is\nthat they are too computation- and memory-intensive to meet the needs of real-time and\non-device applications. This perspective holds true even for the latest lightweight OVDs\nsuch as YOLO-world [34]. Therefore, instead of directly using OVD to solve our tasks,\nthe proposed DART pipeline leverages OVD to generate ground truth as training data\nfor downstream real-time object detectors. Combined with the LMM-based bounding\nbox review (see Section 2.3 and Section 3.4), our approach results in a faster downstream\ndetector and alleviates the speed limitations inherent in OVD."}, {"title": "2.3. Large multimodal model", "content": "Recent advancements in large multimodal models (LMMs) have been characterized\nby the release of several groundbreaking proprietary systems, including GPT-4v [10],\nClaude 3 [35], and Gemini [36]. These models have collectively set a new standard for\nvision-language understanding, demonstrating unprecedented capabilities in combining\nvisual inputs with advanced language processing. Recently, GPT-40 [11] further extends\nthese capabilities with enhanced contextual awareness and multimodal reasoning, pushing\nthe boundaries of LMM by achieving state-of-the-art performance across a wide range\nof vision-language tasks. Meanwhile, the open-source community has also been actively\nadvancing in this domain. Flamingo [37] utilizes visual and language inputs as prompts,"}, {"title": "2.4. Real-time object detection", "content": "Real-time object detection aims to classify and locate objects in real-time with low\nlatency, which is essential for safety-related industrial applications, e.g., on a construction\nsite. This area has witnessed significant advancements in recent years, primarily driven by\nthe iterative development of state-of-the-art YOLO models. Early development of YOLO\nfrom v1 to v3 [13, 14, 15] determined the optimal segmentation of the real-time object\ndetection network structure: backbone, neck, and head. YOLOv4 [16] and YOLOv5\n[17] changed the backbone to CSPNet [47] and introduced a series of best practices in\ncomputer vision regarding training strategy, post-processing, as well as plugin modules.\nBased on these improvements, YOLOv6 [20] and YOLOv7 [19] are further fine-tuned in\nlabel assignment strategy[48] and loss functions [49, 50, 51], and they utilize Repblock\n[52] and E-ELAN [53] as the core of their backbone respectively. YOLOv8 [21] introduces\nthe C2f building block for effective feature extraction and fusion, along with a compre-\nhensive set of integrated tools, solidifying its position as the go-to choice for real-time\nobject detection. YOLOv9 [22] proposes programmable gradient information (PGI) to\nrefine the training process and a new lightweight network architecture \u2013 Generalized Effi-\ncient Layer Aggregation Network (GELAN). Finally, the concurrent work YOLOv10 [23]\npresents an NMS-free training strategy and polishes the mode architecture with holistic\nefficiency-accuracy-driven design, contributing to its state-of-the-art real-time object de-\ntection capabilities. Considering YOLOv8's ease of use and YOLOv10's optimal tradeoff\nbetween speed and accuracy, this work employs both the YOLOv8 and YOLOv10 families\nas the real-time object detectors of choice. Although we have selected the best YOLO\nmodels, their reliance on predefined object categories restricts their applicability in open\nscenarios. Therefore, real-time object detection only serves as the last stage in DART,\ndepending on the other three stages to provide high-quality training data."}, {"title": "3. Methods", "content": "The proposed method, DART, is an automated end-to-end object detection pipeline\nthat spans the entire workflow of an object detection application from data collection to\nmodel deployment. As illustrated in Figure 1, DART starts with dataset preprocessing,\nfollowed by four main stages: data Diversification through subject-driven image genera-\ntion, bounding box Annotation via OVD, LMM-based Review of generated images and"}, {"title": "3.1. Dataset preprocessing", "content": "The data preprocessing phase, illustrated by the orange block in Figure 1, involves\nidentifying classes and instances from raw data as input for prompt engineering and\ndeduplication to obtain clean data."}, {"title": "Class identification.", "content": "The raw dataset is sourced from Liebherr's internal digital sys-\ntem, where images of Liebherr's products are initially roughly categorized in the database.\nBy refining these preliminary classifications with detailed product descriptions, we estab-\nlish 23 accurately defined classes, as detailed in Section 4.1. We find the precise definition\nof the class name essential because it highly affects the performance of the following data\ndiversification phase since ambiguous class definition leads to inferior quality of generated\ndata of that class. As a result, each image in the raw dataset is assigned at least one\nprecise class name based on its primary object, and this broad assignment will be used\nas one of the text prompts for bounding box annotation, as described in Section 3.3."}, {"title": "Instance identification.", "content": "For images containing a single object, we tag them at\nthe instance level using distinctive metadata information, such as product numbers. For\neach class, we randomly selected 2-5 instances and manually curated approximately 10\nimages for each instance (see Figure A.14). These images will be further utilized for data\ndiversification (Section 3.2). Note that tagging with instance labels is a straightforward\nand easy task since we only need 20-50 images in total for each class."}, {"title": "Prompt engineering.", "content": "Class names for images are the cornerstone for prompt con-\nstruction for image generation (Section 3.2), OVD (Section 3.3), and LMM-based review\n(Section 3.4). The constructed class-specific prompts vary slightly for different tasks.\nInstance tags, however, will only serve as the backbone for instance-specific prompts for\ntraining SDXL models under the DreamBooth framework. Specific prompt construction\nrules will be introduced in the corresponding sections mentioned above and further de-\ntailed in Appendix B. The precise text prompts for data diversification, annotation, and\nreview stage are listed in Table B.9, Table B.11, and Table B.12, respectively."}, {"title": "Deduplication.", "content": "The initial raw dataset contains many duplicated images. We em-\nploy perceptual hashing (pHash) [54] for preliminary deduplication. However, cash tends\nto misidentify similar images as duplicates, for example, consecutive shots of the same\nscene from slightly different angles. These near duplicates are manually retained in the\ndataset but are ensured to appear only in the training set.\nUpon completing these steps, we obtain a clean dataset, dubbed Liebherr Products\n(LP). We also acquire instance-specific and class-specific prompts for the following phases.\nIt should be noted that all human interventions within the DART framework conclude\nhere, and the subsequent four phases of DART are fully automated."}, {"title": "3.2. Data diversification based on DreamBooth with Stable Diffusion XL", "content": "Data diversification and augmentation in DART are deeply rooted in the latest image-\ngeneration techniques. One of the most popular ones is Stable Diffusion, which represents"}, {"title": "3.3. Open-vocabulary bounding box annotation via GroundingDINO", "content": "Grounding DINO is an open-vocabulary object detector that marries a transformer-\nbased detector DINO [5] with grounded pre-training, which can draw bounding boxes\nfor arbitrary objects given inputs as plain text. The key to Grounding DINO's open-\nset capability is the tight fusion of language and vision modalities, achieved through\nthree main components: a feature enhancer, a language-guided query selection, and a"}, {"title": "3.4. LMM-based review of pseudo-labels and image photorealism using InternVL-1.5 and\nGPT-40", "content": "LMM leverages the synergy between textual and visual data to perform tasks that\nrequire cross-modality comprehension, such as visual question answering (VQA). In the"}, {"title": "3.5. Real-time object detector training for YOLOv8 and YOLOv10", "content": "In the realm of real-time object detection, YOLO frameworks have established them-\nselves as the dominant methodology primarily because they effectively balance computa-\ntional efficiency with high detection accuracy. Currently, the most popular YOLO variant\nis YOLOv8. As illustrated in the figure, YOLOv8's architecture (Figure 4) inherits the\noverall framework from previous YOLO iterations, utilizing the Darknet [15] backbone,\nSPP (Spatial Pyramid Pooling [60]) and PAN (Path Aggregation Network [61]) structure\nin the neck, and three multi-scale decoupled heads to predict bounding box coordinates\nand class labels. For the major block in the backbone and neck, YOLOv8 employs the\nC2f module, which is a faster implementation of the CSPBlock [47] featuring two convo-\nlution layers at each end, with the CSP Bottlenecks in between (depicted as the red block\nin Figure 4). In terms of training strategy, the model adopts an anchor-free approach\nand incorporates Task Alignment Learning (TAL) [48] for ground truth assignment and\nDistribution Focal Loss (DFL) [62] as the regression loss. These enhancements contribute\nto improved computational efficiency and detection accuracy. The concurrent YOLOv10\nis the latest release in the YOLO family. It proposes a consistent dual assignments strat-\negy for NMS-free training as well as a holistic efficiency-accuracy-driven model design\nfeaturing comprehensive adoption of pointwise, depthwise, and large-kernel convolutions\nwith partial self-attention (PSA), which greatly reduces the computational overhead and\nenhances model capability."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": ""}, {"title": "Data collection.", "content": "We collect a dataset named Liebherr Products (LP) from the\ninternal database of Libherr, a German-Swiss multinational equipment manufacturer.\nDuring data collection, we also employ class and instance identification (in Section 3.1)\nfor bounding box annotation (in Section 3.3) and data diversification (in Section 3.2),\nrespectively."}, {"title": "Dataset composition.", "content": "The LP dataset (after preprocessing) encompasses more\nthan 15K images of Liebherr's extensive range of heavy machinery such as construction\nmachines, earthmoving equipment, deep foundation machines, mining machines, various\ntypes of cranes, material handling machines, and so on. A list of the exact 23 categories\ncan be found either as the x-axis tick labels in Figure 5 or in the first column of Ta-\nble B.10. Figure 5 illustrates the distribution of images in each category, represented by\nthe blue bars labeled \"0:1\". The unbalanced data distribution in the LP dataset makes"}, {"title": "Dataset split.", "content": "The dataset is initially split into a training and a test set with an\n80/20 distribution. To address class imbalance, stratified random sampling ensures each\nclass is adequately represented in the test set. We allocate 20% of the training set to\ncreate a validation set, which is used for hyperparameter fine-tuning. After obtaining the\noptimal hyperparameter set, the model is trained on the original training with 12K images\nand evaluated on the 3K images from the test set. The training set could incorporate\ngenerated data in specific experiment setups to enhance data diversification. Unless\notherwise specified, the generated-to-original data ratio is maintained at the optimal\nproportion of 3:1, as outlined in Section 4.5.1. Notably, categories that already exhibit\nstrong performance with only the original data are excluded from the augmentation with\ngenerated data, as detailed in Section 4.5.4. All performance metrics reported in the\ntables and figures of this paper are based on the test set."}, {"title": "4.2. Evaluation metrics", "content": "In this work, we adopt average precision (AP) to evaluate the final object detection\nmodels. AP first calculates the precision p and recall rat different Intersection over\nUnion (IoU) thresholds t as follows:\n$p(t) = \\frac{TP}{TP + FP}, r(t) = \\frac{TP}{TP + FN},$ (3)\nwhere detections are classified as true (T) or false (F) according to their confidence scores\nand as positive (P) or negative (N) based on the IoU threshold t.\nThen a modified version of precision-recall curve $p(r)$ is constructed separately for\neach class $c \\in C$:\n$p(r, c) = \\max_{r'>r} p(r', c),$ (4)\nso that the curve will decrease monotonically instead of the zigzag pattern.\nFinally, we adopt two commonly used AP metrics: $AP_{50}$ and $AP_{50-95}$. The former\nmeasures precision at a single IoU threshold of 50%, while the latter averages precision\nacross 10 IoU thresholds ranging from 50% to 95% in increments of 5% ([.50:.05:.95]).\nMathematically, $AP_{50\u201395}$ can be expressed as follows:\n$AP_{50-95} = \\frac{1}{|C|} \\sum_{c \\in C} \\frac{1}{10} \\sum_{t \\in [.50:.05:.95]} p(r(t), c).$ (5)\n$AP_{50-95}$ is always smaller than $AP_{50}$ since the precision decreases for a given recall with\nhigher thresholds. However, both metrics exhibit similar trends in diverse and complex\ndatasets, such as LP. This consistent pattern will also be observed in the following tables.\nConsequently, the following sections will primarily focus on the $AP_{50\u201395}$. It is important\nto note that $AP_{50\u201395}$ is also commonly abbreviated as AP, so we will use these two terms\ninterchangeably."}, {"title": "4.3. Implementation details", "content": ""}, {"title": "4.3.1. Implementation of DreamBooth with SDXL", "content": "We instantiate DreamBooth with SDXL as the subject-driven image generation frame-\nwork for the data diversification phase. The SDXL model is preferable, as it significantly\noutperforms previous versions of Stable Diffusion models such as SD-1.5 [1]. We incorpo-\nrate prior preservation loss to prevent overfitting and adjust its weight (\\lambda in Equation 2)\nto 1.0. Contrary to SD-1.5, training the text encoder of SDXL has limited effects; we still\nopt to fine-tune it to enhance overall performance marginally. We extract 69 instances\nfor the training of DreamBooth with SDXL in total. The minimum number of images\nper instance is set to 3 (as suggested by [9]), while the majority of instances have 10\nimages each (see Figure A.14). We implement dynamic training steps by adjusting the\nmaximum training steps based on the number of images collected for the corresponding\ninstance. 100-160 steps per image are proven to be suitable, combining with the AdamW\noptimizer [65] with a learning rate 1 \u00d7 10-4 for U-Net and 5 \u00d7 10-6 for the text encoder.\nThe SNR-gamma is set to 5 by default. Other important hyperparameters are explained\nin Section Appendix A.1.\nWe fine-tune two models from the same official pre-trained checkpoint for each in-\nstance: one with slightly more steps to ensure resemblance and one with fewer for di-\nversification. For training, a general class-specific prompt, \"a photo of a {class_name},\"\nis employed to generate class images for prior preservation loss. As for instance-specific\nprompts, we use the instance name (product number if available, otherwise a descriptive\nword) to form the prompt as \"a photo of a <{instance_name}> {class_name}.\" We also\nexplore different approaches to constructing identifiers and class names. However, our\nexperiments reveal that subtle modifications have negligible effects on performance as\nlong as the identifier is sufficiently distinctive and the class name is adequately precise.\nTo meet the requirements of data diversification, we craft inference text prompts encom-\npassing a broad spectrum of scenarios, as listed in Section Appendix B.1. The training\nprocesses are executed on either a single NVIDIA A100 80GB GPU or two NVIDIA 3090\nGPUs, with each configuration capable of completing the training of one instance-specific\nSDXL model in less than an hour. The inference process requires substantially less GPU\nmemory and is compatible with any memory optimization techniques for SDXL."}, {"title": "4.3.2. Implementation of Grounding DINO", "content": "Off-the-shelf Grounding DINO base model is employed for bounding box annotation.\nThrough extensive comparative experiments, we determined that a box threshold of 0.27\nand a text threshold of 0.25 generally returns high-quality bounding boxes, resulting in\nonly 18 images without annotations, which are consequently excluded from the pipeline.\nThe original collected images and approved generated images are isometrically resized to\n800/1333 pixels for the shorter/longer side before going through the model. As for text\nprompts, we follow instructions given in Section 3.3 to construct all three prompt types\nas separate inputs to the Grounding DINO model and apply filtering and NMS to the\ncombined results to obtain the candidate bounding boxes to be reviewed. The annotation\nprocess is mainly carried out (including speed test as shown in Table 3) by an NVIDIA\nA100 80GB GPU with FP16 for our case."}, {"title": "4.3.3. Implementation of LMM", "content": "We leverage GPT-40 by calling OpenAI's API for pseudo-label review. The bounding\nboxes and class labels are directly drawn on top of the image, and the synthesized image"}, {"title": "4.3.4. Implementation of YOLO", "content": "As the final step of the DART pipeline, we fine-tune COCO pre-trained YOLOv8\nand YOLOv10 models as the output object detector to be deployed. We follow common\npractice [17, 20, 18, 19, 21, 22, 23] to resize all images to 640\u00d7640. Then, AdamW\nis implemented with a weight decay set to 5e-4 as the optimizer. Next, we conduct\nextensive experiments focusing on learning rate and its scheduling for each model and\neach experiment setup. The optimal initial learning rates range from 7e-5 to 5e-4, with a\ngeneral trend indicating that larger models and bigger datasets benefit from lower learning\nrates. Mosaic augmentation [16, 17] is adopted to enhance data diversity. Details on other\ncritical hyperparameters are provided in Section Appendix A.3. Speed tests (including\nNMS for YOLOv8, referenced from the official codebase or paper) are conducted on an\nNVIDIA T4 GPU using TensorRT and FP16. The test results are reported in Table 6."}, {"title": "4.4. Ablation study on the overall DART pipeline", "content": "In this section, we perform an ablation study on the overall DART pipeline. We\nsystematically analyze the contributions of each major component by evaluating the test\nperformances of trained real-time object detection models under various configurations.\nSpecifically, we incrementally integrate the first three stages of DART, i.e., Grounding\nDINO for annotation, GPT-40 for review, and DreamBooth with SDXL for data di-\nversification, as shown in Table 1. To evaluate the impact of each newly incorporated\ncomponent, we leverage the overall performance of DART's final stage as the metric. Var-\nious YOLO models with different versions and scales, including YOLOv8n, YOLOv8s,\nYOLOv10n, and YOLOv10s, are fine-tuned for a comprehensive assessment. We observe\na consistent trend in performance increase across all four YOLO models as new com-\nponents of DART are introduced. Therefore, the subsequent analyses will primarily use\nYOLOv8n as the representative model. For the complete test results on the other models,\nplease refer to Table 1.\nOur baseline configuration (#0) utilizes a YOLOv8n model without any additional\nmodules from DART, yielding the lowest performance with an AP of just 0.064. In this\nsetup, the model is trained on one of the most abundant datasets, LVIS [27]. Despite\nLVIS's extensive coverage of over 1.2K categories across 164K images, it falls short of\ncapturing all the classes present in our self-collected LP dataset. Consequently, we assign\neach LP's category to its closest kin in LVIS during inference, guided by similarity scores\ncalculated from class names and descriptions by SBERT [66]. The results, however,"}, {"title": "4.5. Quantitative Analysis", "content": "In this section, we conduct internal analyses for the four major stages of the proposed\nDART pipeline by investigating the best solutions for each key module via comprehensive\nexperiments and ablation studies within each stage."}, {"title": "4.5.1. Analysis of DreamBooth with SDXL", "content": "We begin by examining the impact of incorporating extra training data generated by\nSDXL models trained for each instance using the DreamBooth framework. Across all\nconfigurations, we maintain a consistent test set of 3K images and augment the training\nset with generated data (with both images and pseudo-labels approved by LMM) in\nvarying multiples (from 0 to 4) of the original 12K images.\nTable 2 presents the results of training YOLOv8 models with different ratios of gen-\nerated to original data. When the training data comprises entirely generated data (0:1\nratio), the YOLOv8n model achieves an AP of only 0.031. This outcome is expected due\nto two primary factors. First, our objective of data diversification drives us to intention-\nally describe scenarios in the text prompts during inference that are entirely different\nfrom the original images (see Section Appendix B.2). Secondly, the current DreamBooth"}, {"title": "4.5.2. Analysis of Grounding DINO", "content": "Two Grounding DINO variants, which utilize Swin tiny and base [67] as the backbone,\nrespectively, are tested for the bounding box annotation task. It can be seen from Table 3\nthat using the more powerful base variant increases test performance for the YOLOv8n\nmodel, suggesting better consistency in bounding box annotations and label assignment.\nHowever, both of the Grounding DINO variants run too slow to fit the real-time require-\nments, even on one of the most powerful GPUs, NVIDIA A100. Consequently, we only\nadopt the Grounding DINO base model for all data annotation tasks for better bounding\nbox quality despite a slightly slower speed, which is negligible for the offline labeling\nprocess."}, {"title": "4.5.3. Analysis of LMM", "content": "DART utilizes GPT-40 for bounding box review. The detailed construction of visual\nand text prompts is thoroughly discussed in Section 3.4 and further detailed in Sec-\ntion Appendix B.3. According to the responses from the pseudo-label review by GPT-40,\nwe eliminate slightly less than 2K images with only disapproved bounding boxes. This\nresults in improvements in the confidence score of bounding boxes among all categories\nas displayed by the red bars in Figure 5, indicating the consistent judgment between\nGPT-40 and Grounding DINO regarding high-quality bounding box annotations.\nIt turns out that GPT-40's decision is even more in line with experienced human\nlabelers. We conduct a small-scale human evaluation experiment by providing experi-"}, {"title": "4.5.4. Analysis of YOLO", "content": "YOLO models are chosen as the final real-time object detector for DART. Figure 9\ndepicts the class-wise test performance of two identical YOLOv8n models applied with\npartial (a) and full(b) DART. The first confusion matrix is derived from the YOLOv8n\nmodel, on which only the annotation and training stage of DART is employed. Note that\nwe choose these two DART components from the outset since the pre-trained YOLO\nmodels fail catastrophically in object detection on the LP dataset as explained in Sec-\ntion 4.4. The realization that providing zero-shot performance comparison is meaningless\nalready emphasizes the essential need for DART.\nWithout the full ability of DART, we observe several misclassifications (non-diagonal\nelements) and subpar true positive rates (diagonal elements), particularly in categories\nsuch as \"combined pilling and drilling rig,\" \"crawler loader,\" \"log loaders,\" and so on,\nindicating room for improvement. However, certain categories, such as crawler excavators\nand truck mixers, already exhibit satisfactory performance. We hypothesize that this is\ndue to their relative prevalence compared to other categories and their inclusion in several\npublic datasets (e.g., LVIS). This enables Grounding DINO to provide more accurate and\nconsistent bounding boxes, as indicated by the high confidence scores of these classes\nin Figure 7. Consequently, these categories do not require further data diversification.\nInstead, we focus on diversifying data for categories with temporarily lower performance,\nallocating more quota of generated images to these classes to improve their results.\nThe second confusion matrix in Figure 9 represents the results from the full DART\npipeline. Obviously, the selective data diversification process described above pays off.\nMisclassifications are reduced across almost all categories. True positive detections for the\ndiversified categories considerably elevate without negatively impacting the performance\nof the previously well-performing categories. This indicates the effectiveness of selective\ndata diversification, which is therefore employed for all experiments involving the addition\nof generated diversified data"}, {"title": "4.6. Qualitative Analysis", "content": "In this section, we conduct qualitative analyses using visualizations to examine the\nmodel's performance during the data diversification, bounding box annotation, and LMM-\nbased review stages. We focus on three specific scenarios in this section, while numerous\nother visualizations can be found in Appendix C.\nFigure 11 showcases a randomly selected category, \u201ccombined pilling and drilling rig,\u201d\nto demonstrate the visualization of the data diversification and bounding box annotation"}, {"title": "5. Limitaion", "content": "While our DART pipeline demonstrates significant performance, boosting the AP of\nan already fine-tuned YOLOv8n model from 0.064 to 0.832 without any manual label-\ning, it is still constrained by weaknesses in the current instantiations of each module.\nGrounding DINO's issues are illustrated and discussed in Section 4.6. The Dreambooth\nwith SDXL for data diversification generates objects that closely mimic originals in var-\nious poses and scenes but still suffer from slight dissimilarity and hallucination, which\nresult in initially unrealistic generations and, eventually, the necessity of original data\nduring training. For images disapproved by pseudo-label review, human intervention is\nstill preferred to indiscriminate deletion, which leads to an inevitable loss of information.\nCompared to proprietary LMMs such as GPT-40, open-source models like InternVL per-\nform suboptimally and can only handle simple scenarios. A few categories show minimal\nimprovement after diversification, primarily due to inadequate labeling of test set data\nand deficiencies noted above."}, {"title": "6. Conclusion", "content": "In this paper, we introduce DART, an automated end-to-end object detection pipeline\nto eliminate the need for human labeling and extensive data collection while maintaining\nhigh model performance. The Liebherr Product (LP) dataset with 15K images across\n23 categories of construction machines is collected to validate the effectiveness of our\napproach. We have demonstrated the efficacy of the DART pipeline through extensive\nexperimentation and analysis. Our results consistently show that incremental incorpo-\nration of each component of DART always contributes to significant improvements in\nAP. Detailed quantitative ablation studies, coupled with qualitative analyses, reveal the\noptimal configuration of each stage. Notably, our researches indicate that a 3:1 ratio of\ngenerated (by fine-tuned SDXL models via DreamBooth) to original data yields the best\nresults for data diversification on the LP dataset. Open-vocabulary bounding box an-\nnotation (Grounding DINO) has proven effective in generating credible bounding boxes\nwithout human intervention. GPT-40 has demonstrated human-level semantic under-\nstanding capabilities in the task of pseudo-label review. Furthermore, our analysis reveals\nthat YOLOv8n offers the best tradeoff between accuracy and model size, making it the\noptimal choice for real-time object detection on edge devices. These findings demonstrate\nDART's superior efficacy in enhancing the capabilities of real-time object detection mod-\nels. In addition to its effectiveness, the modularity of DART ensures its adaptability to\nfuture advancements and customization for any specific target object or environment.\nRemarkably, DART accomplishes these results without requiring any manual annotation"}, {"title": "Appendix A. More model details", "content": "In this section, we discuss additional implementation details of the core modules of\nDART."}, {"title": "Appendix A.1. DreamBooth", "content": "The hyperparameters used for the Dreambooth with SDXL model are detailed in Ta-\nble A.7. We use the official SDXL checkpoint from Stability AI but replace the VAE\nwith madebyollin's version (for a fair comparison of FP16 and BF16"}, {"title": "DART: An Automated End-to-End Object Detection Pipeline\nwith Data Diversification, Open-Vocabulary Bounding Box\nAnnotation, Pseudo-Label Review, and Model Training", "authors": ["Chen Xin", "Andreas Hartel", "Enkelejda Kasneci"], "abstract": "Swift and accurate detection of specified objects is crucial for many industrial applica-\ntions, such as safety monitoring on construction sites. However, traditional approaches\nrely heavily on arduous manual annotation and data collection, which struggle to adapt\nto ever-changing environments and novel target objects. To address these limitations,\nthis paper presents DART, an automated end-to-end pipeline designed to streamline\nthe entire workflow of an object detection application from data collection to model\ndeployment. DART eliminates the need for human labeling and extensive data collec-\ntion while excelling in diverse scenarios. It employs a subject-driven image generation\nmodule (DreamBooth with SDXL) for data Diversification, followed by an Annotation\nstage where open-vocabulary object detection (Grounding DINO) generates bounding\nbox annotations for both generated and original images. These pseudo-labels are then\nReviewed by a large multimodal model (GPT-40) to guarantee credibility before serv-\ning as ground truth to Train real-time object detectors (YOLO). We apply DART to\na self-collected dataset of construction machines named Liebherr Product, which con-\ntains over 15K high-quality images across 23 categories. The current implementation of\nDART significantly increases average precision (AP) from 0.064 to 0.832. Furthermore,\nwe adopt a modular design for DART to ensure easy exchangeability and extensibility.\nThis allows for a smooth transition to more advanced algorithms in the future, seamless\nintegration of new object categories without manual labeling, and adaptability to cus-\ntomized environments without extra data collection. The code and dataset are released\nat https://github.com/chen-xin-94/DART.\nKeywords: Open-vocabulary object detection (OVD), Data diversification,\nPseudo-label, Large multimodal model (LMM), Stable Diffusion, GPT-40, YOLO", "sections": [{"title": "1. Introduction", "content": "In recent years, computer vision has witnessed rapid advancements, revolutionizing\nvarious applications through innovative methodologies. A significant development is the\nemergence of generative AI techniques, exemplified by text-to-image Diffusion models"}, {"title": "2. Related work", "content": "In the related work section, we will focus on four areas corresponding to the DART\npipeline's core stages in order: subject-driven image generation, open-vocabulary ob-\nject detection, and large multimodal models, real-time object detection. By discussing\nprogress in these fields, we aim to justify our current implementation of state-of-the-\nart methods and explore the potential for further enhancements by incorporating more\nadvanced modules in the future."}, {"title": "2.1. Subject-driven image generation", "content": "Text-to-image diffusion models [1, 2, 3, 4, 5] have swiftly advanced in recent years,\nestablishing themselves as powerful tools in generative AI. These models generate high-\nquality images from textual descriptions through iterative denoising processes, forming\nthe foundation for subject-driven text-to-image generation. Given only a few (typically\n3-5) casually captured images of a specific subject, subject-driven image generation aims\nto produce customized images with high detail fidelity and variations guided by text\nprompts. One typical approach is to fine-tune a special prompt token to describe the\nconcept of the specific subject. Textual Inversion [8] shows evidence that a single-word"}, {"title": "2.2. Open-vocabulary object detection", "content": "Open-vocabulary object detection (OVD) has become a cutting-edge approach in\nmodern object detection. It aims to detect any objects given their corresponding text\nprompts beyond predefined categories. Recent groundbreaking works in OVD include\nViLD [30] which distills CLIP [31] embeddings to cropped image embeddings from region\nproposal network (RPN) [32], as well as GLIP [33] where object detection is reformulated\nas a grounding task by aligning bounding boxes to phrases using early fusion. Moreover,\nGrounding DINO [12] extends the idea of cross-modality fusion to both feature extrac-\ntion and decoder layers. It excels in zero-shot scenarios, demonstrating an impressive\ngeneralization ability to detect unseen objects. Nevertheless, one major issue of OVDs is\nthat they are too computation- and memory-intensive to meet the needs of real-time and\non-device applications. This perspective holds true even for the latest lightweight OVDs\nsuch as YOLO-world [34]. Therefore, instead of directly using OVD to solve our tasks,\nthe proposed DART pipeline leverages OVD to generate ground truth as training data\nfor downstream real-time object detectors. Combined with the LMM-based bounding\nbox review (see Section 2.3 and Section 3.4), our approach results in a faster downstream\ndetector and alleviates the speed limitations inherent in OVD."}, {"title": "2.3. Large multimodal model", "content": "Recent advancements in large multimodal models (LMMs) have been characterized\nby the release of several groundbreaking proprietary systems, including GPT-4v [10],\nClaude 3 [35], and Gemini [36]. These models have collectively set a new standard for\nvision-language understanding, demonstrating unprecedented capabilities in combining\nvisual inputs with advanced language processing. Recently, GPT-40 [11] further extends\nthese capabilities with enhanced contextual awareness and multimodal reasoning, pushing\nthe boundaries of LMM by achieving state-of-the-art performance across a wide range\nof vision-language tasks. Meanwhile, the open-source community has also been actively\nadvancing in this domain. Flamingo [37] utilizes visual and language inputs as prompts,"}, {"title": "2.4. Real-time object detection", "content": "Real-time object detection aims to classify and locate objects in real-time with low\nlatency, which is essential for safety-related industrial applications, e.g., on a construction\nsite. This area has witnessed significant advancements in recent years, primarily driven by\nthe iterative development of state-of-the-art YOLO models. Early development of YOLO\nfrom v1 to v3 [13, 14, 15] determined the optimal segmentation of the real-time object\ndetection network structure: backbone, neck, and head. YOLOv4 [16] and YOLOv5\n[17] changed the backbone to CSPNet [47] and introduced a series of best practices in\ncomputer vision regarding training strategy, post-processing, as well as plugin modules.\nBased on these improvements, YOLOv6 [20] and YOLOv7 [19] are further fine-tuned in\nlabel assignment strategy[48] and loss functions [49, 50, 51], and they utilize Repblock\n[52] and E-ELAN [53] as the core of their backbone respectively. YOLOv8 [21] introduces\nthe C2f building block for effective feature extraction and fusion, along with a compre-\nhensive set of integrated tools, solidifying its position as the go-to choice for real-time\nobject detection. YOLOv9 [22] proposes programmable gradient information (PGI) to\nrefine the training process and a new lightweight network architecture \u2013 Generalized Effi-\ncient Layer Aggregation Network (GELAN). Finally, the concurrent work YOLOv10 [23]\npresents an NMS-free training strategy and polishes the mode architecture with holistic\nefficiency-accuracy-driven design, contributing to its state-of-the-art real-time object de-\ntection capabilities. Considering YOLOv8's ease of use and YOLOv10's optimal tradeoff\nbetween speed and accuracy, this work employs both the YOLOv8 and YOLOv10 families\nas the real-time object detectors of choice. Although we have selected the best YOLO\nmodels, their reliance on predefined object categories restricts their applicability in open\nscenarios. Therefore, real-time object detection only serves as the last stage in DART,\ndepending on the other three stages to provide high-quality training data."}, {"title": "3. Methods", "content": "The proposed method, DART, is an automated end-to-end object detection pipeline\nthat spans the entire workflow of an object detection application from data collection to\nmodel deployment. As illustrated in Figure 1, DART starts with dataset preprocessing,\nfollowed by four main stages: data Diversification through subject-driven image genera-\ntion, bounding box Annotation via OVD, LMM-based Review of generated images and"}, {"title": "3.1. Dataset preprocessing", "content": "The data preprocessing phase, illustrated by the orange block in Figure 1, involves\nidentifying classes and instances from raw data as input for prompt engineering and\ndeduplication to obtain clean data."}, {"title": "Class identification.", "content": "The raw dataset is sourced from Liebherr's internal digital sys-\ntem, where images of Liebherr's products are initially roughly categorized in the database.\nBy refining these preliminary classifications with detailed product descriptions, we estab-\nlish 23 accurately defined classes, as detailed in Section 4.1. We find the precise definition\nof the class name essential because it highly affects the performance of the following data\ndiversification phase since ambiguous class definition leads to inferior quality of generated\ndata of that class. As a result, each image in the raw dataset is assigned at least one\nprecise class name based on its primary object, and this broad assignment will be used\nas one of the text prompts for bounding box annotation, as described in Section 3.3."}, {"title": "Instance identification.", "content": "For images containing a single object, we tag them at\nthe instance level using distinctive metadata information, such as product numbers. For\neach class, we randomly selected 2-5 instances and manually curated approximately 10\nimages for each instance (see Figure A.14). These images will be further utilized for data\ndiversification (Section 3.2). Note that tagging with instance labels is a straightforward\nand easy task since we only need 20-50 images in total for each class."}, {"title": "Prompt engineering.", "content": "Class names for images are the cornerstone for prompt con-\nstruction for image generation (Section 3.2), OVD (Section 3.3), and LMM-based review\n(Section 3.4). The constructed class-specific prompts vary slightly for different tasks.\nInstance tags, however, will only serve as the backbone for instance-specific prompts for\ntraining SDXL models under the DreamBooth framework. Specific prompt construction\nrules will be introduced in the corresponding sections mentioned above and further de-\ntailed in Appendix B. The precise text prompts for data diversification, annotation, and\nreview stage are listed in Table B.9, Table B.11, and Table B.12, respectively."}, {"title": "Deduplication.", "content": "The initial raw dataset contains many duplicated images. We em-\nploy perceptual hashing (pHash) [54] for preliminary deduplication. However, cash tends\nto misidentify similar images as duplicates, for example, consecutive shots of the same\nscene from slightly different angles. These near duplicates are manually retained in the\ndataset but are ensured to appear only in the training set.\nUpon completing these steps, we obtain a clean dataset, dubbed Liebherr Products\n(LP). We also acquire instance-specific and class-specific prompts for the following phases.\nIt should be noted that all human interventions within the DART framework conclude\nhere, and the subsequent four phases of DART are fully automated."}, {"title": "3.2. Data diversification based on DreamBooth with Stable Diffusion XL", "content": "Data diversification and augmentation in DART are deeply rooted in the latest image-\ngeneration techniques. One of the most popular ones is Stable Diffusion, which represents"}, {"title": "3.3. Open-vocabulary bounding box annotation via GroundingDINO", "content": "Grounding DINO is an open-vocabulary object detector that marries a transformer-\nbased detector DINO [5] with grounded pre-training, which can draw bounding boxes\nfor arbitrary objects given inputs as plain text. The key to Grounding DINO's open-\nset capability is the tight fusion of language and vision modalities, achieved through\nthree main components: a feature enhancer, a language-guided query selection, and a"}, {"title": "3.4. LMM-based review of pseudo-labels and image photorealism using InternVL-1.5 and\nGPT-40", "content": "LMM leverages the synergy between textual and visual data to perform tasks that\nrequire cross-modality comprehension, such as visual question answering (VQA). In the"}, {"title": "3.5. Real-time object detector training for YOLOv8 and YOLOv10", "content": "In the realm of real-time object detection, YOLO frameworks have established them-\nselves as the dominant methodology primarily because they effectively balance computa-\ntional efficiency with high detection accuracy. Currently, the most popular YOLO variant\nis YOLOv8. As illustrated in the figure, YOLOv8's architecture (Figure 4) inherits the\noverall framework from previous YOLO iterations, utilizing the Darknet [15] backbone,\nSPP (Spatial Pyramid Pooling [60]) and PAN (Path Aggregation Network [61]) structure\nin the neck, and three multi-scale decoupled heads to predict bounding box coordinates\nand class labels. For the major block in the backbone and neck, YOLOv8 employs the\nC2f module, which is a faster implementation of the CSPBlock [47] featuring two convo-\nlution layers at each end, with the CSP Bottlenecks in between (depicted as the red block\nin Figure 4). In terms of training strategy, the model adopts an anchor-free approach\nand incorporates Task Alignment Learning (TAL) [48] for ground truth assignment and\nDistribution Focal Loss (DFL) [62] as the regression loss. These enhancements contribute\nto improved computational efficiency and detection accuracy. The concurrent YOLOv10\nis the latest release in the YOLO family. It proposes a consistent dual assignments strat-\negy for NMS-free training as well as a holistic efficiency-accuracy-driven model design\nfeaturing comprehensive adoption of pointwise, depthwise, and large-kernel convolutions\nwith partial self-attention (PSA), which greatly reduces the computational overhead and\nenhances model capability."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": ""}, {"title": "Data collection.", "content": "We collect a dataset named Liebherr Products (LP) from the\ninternal database of Libherr, a German-Swiss multinational equipment manufacturer.\nDuring data collection, we also employ class and instance identification (in Section 3.1)\nfor bounding box annotation (in Section 3.3) and data diversification (in Section 3.2),\nrespectively."}, {"title": "Dataset composition.", "content": "The LP dataset (after preprocessing) encompasses more\nthan 15K images of Liebherr's extensive range of heavy machinery such as construction\nmachines, earthmoving equipment, deep foundation machines, mining machines, various\ntypes of cranes, material handling machines, and so on. A list of the exact 23 categories\ncan be found either as the x-axis tick labels in Figure 5 or in the first column of Ta-\nble B.10. Figure 5 illustrates the distribution of images in each category, represented by\nthe blue bars labeled \"0:1\". The unbalanced data distribution in the LP dataset makes"}, {"title": "Dataset split.", "content": "The dataset is initially split into a training and a test set with an\n80/20 distribution. To address class imbalance, stratified random sampling ensures each\nclass is adequately represented in the test set. We allocate 20% of the training set to\ncreate a validation set, which is used for hyperparameter fine-tuning. After obtaining the\noptimal hyperparameter set, the model is trained on the original training with 12K images\nand evaluated on the 3K images from the test set. The training set could incorporate\ngenerated data in specific experiment setups to enhance data diversification. Unless\notherwise specified, the generated-to-original data ratio is maintained at the optimal\nproportion of 3:1, as outlined in Section 4.5.1. Notably, categories that already exhibit\nstrong performance with only the original data are excluded from the augmentation with\ngenerated data, as detailed in Section 4.5.4. All performance metrics reported in the\ntables and figures of this paper are based on the test set."}, {"title": "4.2. Evaluation metrics", "content": "In this work, we adopt average precision (AP) to evaluate the final object detection\nmodels. AP first calculates the precision p and recall rat different Intersection over\nUnion (IoU) thresholds t as follows:\n$p(t) = \\frac{TP}{TP + FP}, r(t) = \\frac{TP}{TP + FN},$ (3)\nwhere detections are classified as true (T) or false (F) according to their confidence scores\nand as positive (P) or negative (N) based on the IoU threshold t.\nThen a modified version of precision-recall curve $p(r)$ is constructed separately for\neach class $c \\in C$:\n$p(r, c) = \\max_{r'>r} p(r', c),$ (4)\nso that the curve will decrease monotonically instead of the zigzag pattern.\nFinally, we adopt two commonly used AP metrics: $AP_{50}$ and $AP_{50-95}$. The former\nmeasures precision at a single IoU threshold of 50%, while the latter averages precision\nacross 10 IoU thresholds ranging from 50% to 95% in increments of 5% ([.50:.05:.95]).\nMathematically, $AP_{50\u201395}$ can be expressed as follows:\n$AP_{50-95} = \\frac{1}{|C|} \\sum_{c \\in C} \\frac{1}{10} \\sum_{t \\in [.50:.05:.95]} p(r(t), c).$ (5)\n$AP_{50-95}$ is always smaller than $AP_{50}$ since the precision decreases for a given recall with\nhigher thresholds. However, both metrics exhibit similar trends in diverse and complex\ndatasets, such as LP. This consistent pattern will also be observed in the following tables.\nConsequently, the following sections will primarily focus on the $AP_{50\u201395}$. It is important\nto note that $AP_{50\u201395}$ is also commonly abbreviated as AP, so we will use these two terms\ninterchangeably."}, {"title": "4.3. Implementation details", "content": ""}, {"title": "4.3.1. Implementation of DreamBooth with SDXL", "content": "We instantiate DreamBooth with SDXL as the subject-driven image generation frame-\nwork for the data diversification phase. The SDXL model is preferable, as it significantly\noutperforms previous versions of Stable Diffusion models such as SD-1.5 [1]. We incorpo-\nrate prior preservation loss to prevent overfitting and adjust its weight (\\lambda in Equation 2)\nto 1.0. Contrary to SD-1.5, training the text encoder of SDXL has limited effects; we still\nopt to fine-tune it to enhance overall performance marginally. We extract 69 instances\nfor the training of DreamBooth with SDXL in total. The minimum number of images\nper instance is set to 3 (as suggested by [9]), while the majority of instances have 10\nimages each (see Figure A.14). We implement dynamic training steps by adjusting the\nmaximum training steps based on the number of images collected for the corresponding\ninstance. 100-160 steps per image are proven to be suitable, combining with the AdamW\noptimizer [65] with a learning rate 1 \u00d7 10-4 for U-Net and 5 \u00d7 10-6 for the text encoder.\nThe SNR-gamma is set to 5 by default. Other important hyperparameters are explained\nin Section Appendix A.1.\nWe fine-tune two models from the same official pre-trained checkpoint for each in-\nstance: one with slightly more steps to ensure resemblance and one with fewer for di-\nversification. For training, a general class-specific prompt, \"a photo of a {class_name},\"\nis employed to generate class images for prior preservation loss. As for instance-specific\nprompts, we use the instance name (product number if available, otherwise a descriptive\nword) to form the prompt as \"a photo of a <{instance_name}> {class_name}.\" We also\nexplore different approaches to constructing identifiers and class names. However, our\nexperiments reveal that subtle modifications have negligible effects on performance as\nlong as the identifier is sufficiently distinctive and the class name is adequately precise.\nTo meet the requirements of data diversification, we craft inference text prompts encom-\npassing a broad spectrum of scenarios, as listed in Section Appendix B.1. The training\nprocesses are executed on either a single NVIDIA A100 80GB GPU or two NVIDIA 3090\nGPUs, with each configuration capable of completing the training of one instance-specific\nSDXL model in less than an hour. The inference process requires substantially less GPU\nmemory and is compatible with any memory optimization techniques for SDXL."}, {"title": "4.3.2. Implementation of Grounding DINO", "content": "Off-the-shelf Grounding DINO base model is employed for bounding box annotation.\nThrough extensive comparative experiments, we determined that a box threshold of 0.27\nand a text threshold of 0.25 generally returns high-quality bounding boxes, resulting in\nonly 18 images without annotations, which are consequently excluded from the pipeline.\nThe original collected images and approved generated images are isometrically resized to\n800/1333 pixels for the shorter/longer side before going through the model. As for text\nprompts, we follow instructions given in Section 3.3 to construct all three prompt types\nas separate inputs to the Grounding DINO model and apply filtering and NMS to the\ncombined results to obtain the candidate bounding boxes to be reviewed. The annotation\nprocess is mainly carried out (including speed test as shown in Table 3) by an NVIDIA\nA100 80GB GPU with FP16 for our case."}, {"title": "4.3.3. Implementation of LMM", "content": "We leverage GPT-40 by calling OpenAI's API for pseudo-label review. The bounding\nboxes and class labels are directly drawn on top of the image, and the synthesized image"}, {"title": "4.3.4. Implementation of YOLO", "content": "As the final step of the DART pipeline, we fine-tune COCO pre-trained YOLOv8\nand YOLOv10 models as the output object detector to be deployed. We follow common\npractice [17, 20, 18, 19, 21, 22, 23] to resize all images to 640\u00d7640. Then, AdamW\nis implemented with a weight decay set to 5e-4 as the optimizer. Next, we conduct\nextensive experiments focusing on learning rate and its scheduling for each model and\neach experiment setup. The optimal initial learning rates range from 7e-5 to 5e-4, with a\ngeneral trend indicating that larger models and bigger datasets benefit from lower learning\nrates. Mosaic augmentation [16, 17] is adopted to enhance data diversity. Details on other\ncritical hyperparameters are provided in Section Appendix A.3. Speed tests (including\nNMS for YOLOv8, referenced from the official codebase or paper) are conducted on an\nNVIDIA T4 GPU using TensorRT and FP16. The test results are reported in Table 6."}, {"title": "4.4. Ablation study on the overall DART pipeline", "content": "In this section, we perform an ablation study on the overall DART pipeline. We\nsystematically analyze the contributions of each major component by evaluating the test\nperformances of trained real-time object detection models under various configurations.\nSpecifically, we incrementally integrate the first three stages of DART, i.e., Grounding\nDINO for annotation, GPT-40 for review, and DreamBooth with SDXL for data di-\nversification, as shown in Table 1. To evaluate the impact of each newly incorporated\ncomponent, we leverage the overall performance of DART's final stage as the metric. Var-\nious YOLO models with different versions and scales, including YOLOv8n, YOLOv8s,\nYOLOv10n, and YOLOv10s, are fine-tuned for a comprehensive assessment. We observe\na consistent trend in performance increase across all four YOLO models as new com-\nponents of DART are introduced. Therefore, the subsequent analyses will primarily use\nYOLOv8n as the representative model. For the complete test results on the other models,\nplease refer to Table 1.\nOur baseline configuration (#0) utilizes a YOLOv8n model without any additional\nmodules from DART, yielding the lowest performance with an AP of just 0.064. In this\nsetup, the model is trained on one of the most abundant datasets, LVIS [27]. Despite\nLVIS's extensive coverage of over 1.2K categories across 164K images, it falls short of\ncapturing all the classes present in our self-collected LP dataset. Consequently, we assign\neach LP's category to its closest kin in LVIS during inference, guided by similarity scores\ncalculated from class names and descriptions by SBERT [66]. The results, however,"}, {"title": "4.5. Quantitative Analysis", "content": "In this section, we conduct internal analyses for the four major stages of the proposed\nDART pipeline by investigating the best solutions for each key module via comprehensive\nexperiments and ablation studies within each stage."}, {"title": "4.5.1. Analysis of DreamBooth with SDXL", "content": "We begin by examining the impact of incorporating extra training data generated by\nSDXL models trained for each instance using the DreamBooth framework. Across all\nconfigurations, we maintain a consistent test set of 3K images and augment the training\nset with generated data (with both images and pseudo-labels approved by LMM) in\nvarying multiples (from 0 to 4) of the original 12K images.\nTable 2 presents the results of training YOLOv8 models with different ratios of gen-\nerated to original data. When the training data comprises entirely generated data (0:1\nratio), the YOLOv8n model achieves an AP of only 0.031. This outcome is expected due\nto two primary factors. First, our objective of data diversification drives us to intention-\nally describe scenarios in the text prompts during inference that are entirely different\nfrom the original images (see Section Appendix B.2). Secondly, the current DreamBooth"}, {"title": "4.5.2. Analysis of Grounding DINO", "content": "Two Grounding DINO variants, which utilize Swin tiny and base [67] as the backbone,\nrespectively, are tested for the bounding box annotation task. It can be seen from Table 3\nthat using the more powerful base variant increases test performance for the YOLOv8n\nmodel, suggesting better consistency in bounding box annotations and label assignment.\nHowever, both of the Grounding DINO variants run too slow to fit the real-time require-\nments, even on one of the most powerful GPUs, NVIDIA A100. Consequently, we only\nadopt the Grounding DINO base model for all data annotation tasks for better bounding\nbox quality despite a slightly slower speed, which is negligible for the offline labeling\nprocess."}, {"title": "4.5.3. Analysis of LMM", "content": "DART utilizes GPT-40 for bounding box review. The detailed construction of visual\nand text prompts is thoroughly discussed in Section 3.4 and further detailed in Sec-\ntion Appendix B.3. According to the responses from the pseudo-label review by GPT-40,\nwe eliminate slightly less than 2K images with only disapproved bounding boxes. This\nresults in improvements in the confidence score of bounding boxes among all categories\nas displayed by the red bars in Figure 5, indicating the consistent judgment between\nGPT-40 and Grounding DINO regarding high-quality bounding box annotations.\nIt turns out that GPT-40's decision is even more in line with experienced human\nlabelers. We conduct a small-scale human evaluation experiment by providing experi-"}, {"title": "4.5.4. Analysis of YOLO", "content": "YOLO models are chosen as the final real-time object detector for DART. Figure 9\ndepicts the class-wise test performance of two identical YOLOv8n models applied with\npartial (a) and full(b) DART. The first confusion matrix is derived from the YOLOv8n\nmodel, on which only the annotation and training stage of DART is employed. Note that\nwe choose these two DART components from the outset since the pre-trained YOLO\nmodels fail catastrophically in object detection on the LP dataset as explained in Sec-\ntion 4.4. The realization that providing zero-shot performance comparison is meaningless\nalready emphasizes the essential need for DART.\nWithout the full ability of DART, we observe several misclassifications (non-diagonal\nelements) and subpar true positive rates (diagonal elements), particularly in categories\nsuch as \"combined pilling and drilling rig,\" \"crawler loader,\" \"log loaders,\" and so on,\nindicating room for improvement. However, certain categories, such as crawler excavators\nand truck mixers, already exhibit satisfactory performance. We hypothesize that this is\ndue to their relative prevalence compared to other categories and their inclusion in several\npublic datasets (e.g., LVIS). This enables Grounding DINO to provide more accurate and\nconsistent bounding boxes, as indicated by the high confidence scores of these classes\nin Figure 7. Consequently, these categories do not require further data diversification.\nInstead, we focus on diversifying data for categories with temporarily lower performance,\nallocating more quota of generated images to these classes to improve their results.\nThe second confusion matrix in Figure 9 represents the results from the full DART\npipeline. Obviously, the selective data diversification process described above pays off.\nMisclassifications are reduced across almost all categories. True positive detections for the\ndiversified categories considerably elevate without negatively impacting the performance\nof the previously well-performing categories. This indicates the effectiveness of selective\ndata diversification, which is therefore employed for all experiments involving the addition\nof generated diversified data"}, {"title": "4.6. Qualitative Analysis", "content": "In this section, we conduct qualitative analyses using visualizations to examine the\nmodel's performance during the data diversification, bounding box annotation, and LMM-\nbased review stages. We focus on three specific scenarios in this section, while numerous\nother visualizations can be found in Appendix C.\nFigure 11 showcases a randomly selected category, \u201ccombined pilling and drilling rig,\u201d\nto demonstrate the visualization of the data diversification and bounding box annotation"}, {"title": "5. Limitaion", "content": "While our DART pipeline demonstrates significant performance, boosting the AP of\nan already fine-tuned YOLOv8n model from 0.064 to 0.832 without any manual label-\ning, it is still constrained by weaknesses in the current instantiations of each module.\nGrounding DINO's issues are illustrated and discussed in Section 4.6. The Dreambooth\nwith SDXL for data diversification generates objects that closely mimic originals in var-\nious poses and scenes but still suffer from slight dissimilarity and hallucination, which\nresult in initially unrealistic generations and, eventually, the necessity of original data\nduring training. For images disapproved by pseudo-label review, human intervention is\nstill preferred to indiscriminate deletion, which leads to an inevitable loss of information.\nCompared to proprietary LMMs such as GPT-40, open-source models like InternVL per-\nform suboptimally and can only handle simple scenarios. A few categories show minimal\nimprovement after diversification, primarily due to inadequate labeling of test set data\nand deficiencies noted above."}, {"title": "6. Conclusion", "content": "In this paper, we introduce DART, an automated end-to-end object detection pipeline\nto eliminate the need for human labeling and extensive data collection while maintaining\nhigh model performance. The Liebherr Product (LP) dataset with 15K images across\n23 categories of construction machines is collected to validate the effectiveness of our\napproach. We have demonstrated the efficacy of the DART pipeline through extensive\nexperimentation and analysis. Our results consistently show that incremental incorpo-\nration of each component of DART always contributes to significant improvements in\nAP. Detailed quantitative ablation studies, coupled with qualitative analyses, reveal the\noptimal configuration of each stage. Notably, our researches indicate that a 3:1 ratio of\ngenerated (by fine-tuned SDXL models via DreamBooth) to original data yields the best\nresults for data diversification on the LP dataset. Open-vocabulary bounding box an-\nnotation (Grounding DINO) has proven effective in generating credible bounding boxes\nwithout human intervention. GPT-40 has demonstrated human-level semantic under-\nstanding capabilities in the task of pseudo-label review. Furthermore, our analysis reveals\nthat YOLOv8n offers the best tradeoff between accuracy and model size, making it the\noptimal choice for real-time object detection on edge devices. These findings demonstrate\nDART's superior efficacy in enhancing the capabilities of real-time object detection mod-\nels. In addition to its effectiveness, the modularity of DART ensures its adaptability to\nfuture advancements and customization for any specific target object or environment.\nRemarkably, DART accomplishes these results without requiring any manual annotation"}, {"title": "Appendix A. More model details", "content": "In this section, we discuss additional implementation details of the core modules of\nDART."}, {"title": "Appendix A.1. DreamBooth", "content": "The hyperparameters used for the Dreambooth with SDXL model are detailed in Ta-\nble A.7. We use the official SDXL checkpoint from Stability AI but replace the VAE\nwith madebyollin's version (for a fair comparison of FP16 and BF16, where we later find\nno apparent differences between"}]}]}