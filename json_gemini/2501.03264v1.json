{"title": "BRIDGE THE INFERENCE GAPS OF NEURAL PROCESSES VIA EXPECTATION MAXIMIZATION", "authors": ["Qi Wang", "Marco Federici", "Herke van Hoof"], "abstract": "The neural process (NP) is a family of computationally efficient models for learning distributions over functions. However, it suffers from under-fitting and shows suboptimal performance in practice. Researchers have primarily focused on incorporating diverse structural inductive biases, e.g. attention or convolution, in modeling. The topic of inference suboptimality and an analysis of the NP from the optimization objective perspective has hardly been studied in earlier work. To fix this issue, we propose a surrogate objective of the target log-likelihood of the meta dataset within the expectation maximization framework. The resulting model, referred to as the Self-normalized Importance weighted Neural Process (SI-NP), can learn a more accurate functional prior and has an improvement guarantee concerning the target log-likelihood. Experimental results show the competitive performance of SI-NP over other NPs objectives and illustrate that structural inductive biases, such as attention modules, can also augment our method to achieve SOTA performance. Our code is available at https://github.com/hhq123gogogo/SI_NPs.", "sections": [{"title": "INTRODUCTION", "content": "The combination of deep neural networks and stochastic processes provides a promising framework for modeling data points with correlations (Ghahramani, 2015). It exploits the high capacity of deep neural networks and enables uncertainty quantification for distributions over functions.\nAs an example, we can look at the deep Gaussian process (Damianou & Lawrence, 2013). However, the run-time complexity of predictive distributions in Gaussian processes is cubic w.r.t. the number of predicted data points. To circumvent this, Garnelo et al. (2018a;b) developed the family of neural processes (NPs) as the alternative, which can model more flexible function distributions and capture predictive uncertainty at a lower computational cost.\nIn this paper, we study the vanilla NP as a deep latent variable model and show the generative process in Fig. (1). In particular, let us recap the inference methods used in vanilla NPs: It learns to approximate the functional posterior q\u03c6(z)\u2248p(z|DT;\u03bd) and a functional prior q\u03d5(z|DC)\u2248p(z|DC;\u03d1), which are permutation invariant to the order of data points. Then the predictive distribution for a data point [x\u2217,y\u2217] can be formulated in the form Eq\u03d5(z|DC)[p(y\u2217|[x\u2217,z];\u03d1)].\nWhile the NP provides a computationally efficient framework for modeling exchangeable stochastic processes, it exhibits underfitting and fails to capture accurate uncertainty (Garnelo et al., 2018b; Kim et al., 2019) in practice. To improve its generalization capability,"}, {"title": "PRELIMINARIES", "content": "General Notations. We study NPs in a meta learning setup. T defines a set of tasks with \u03c4 a sampled task. Let DC={(xi,yi)}nci=1 and DT={(xi,yi)}n+mti=1 denote the context points for the functional prior inference and the target points for the function prediction. The latent variable z is a functional representation of a task \u03c4 with observed data points.\nWe refer to \u03d1\u2208\u0398 as the parameters of the deep latent variable model for NPs. In detail, \u03d1 consists of encoder parameters in a functional prior p(z|DC;\u03bd) and decoder parameters in a generative distribution p(DT|z;\u03bd). \u03a6 refer to the parameters of a variational posterior distribution q\u03c6(z)=q\u03c6(z|DT), while \u03b7 refer to the parameters of a proposal distribution q\u03b7(z) in the following self-normalized importance sampling. Gaussian distributions with diagonal covariance matrices are the default choice for these distributions, e.g. p(z|DF;\u03bd)=N(z;\u03bc\u03bd(DF),\u03a3\u03bd(DF)), q\u03c6(z)=N(z;\u03bc\u03c6(DT),\u03a3\u03c6(DT)) and q\u03b7(z|DT)=N(z;\u03bc\u03b7(DI),\u03a3\u03b7(DT)).\nNPs as Exchangeable Stochastic Processes. In vanilla NPs, the element-wise generative process can be translated into Eq. (1). Here the mean and variance functions are respectively denoted by \u03bc and \u03a3.\nPx1:n+m(y1:n+m)=\u222bp(z)n+m\u220fi=1N(yi;\u03bc(xi,z),\u03a3(xi,z))dz\n(1)\nBased on the Kolmogorov extension theorem (Klenke, 2013) and de Finneti\u2019s theorem (Kerns & Sz\u00e9kely, 2006), the above equation Px1:n+m(y1:n+m) is verified to be a well-defined exchangeable stochastic process.\nNPs in Meta Learning Tasks. Given a collection of tasks T, we can decompose the marginal distribution p(DT|DC;\u03bd) with a global latent variable z in Eq. (2). Here the conditional distribution p(z|DC;\u03bd) with \u03c4\u2208T is permutation invariant w.r.t. the order of data points and encodes the functional prior in the generative process.\np(DF|DF;\u03bd)=\u220f\u03c4\u2208T\u222bp(D+|z;\u03bd)p(z|D;\u03bd)dz"}, {"title": "OPTIMIZATION GAPS AND STATISTICAL TRAITS", "content": "Throughout the paper, the optimization objective of our interest is the marginal log-likelihood of a meta learning dataset in Eq. (3). Furthermore, this applies to all NP variants.\nmax\u2211\u03c4\u2208Tln\u222bp(D+|z;\u03bd)p(z|D;\u03bd)dz\n(3)\nFor the sake of simplicity, we consider one task \u03c4 to derive equations in the following section, which corresponds to maximizing the following objective\u00b9.\nL(\u03d1)=ln\u222bp(D+|z;\u03bd)p(z|D;\u03bd)dz\n(4)\nIn the NP family (Garnelo et al., 2018a;b), the target data points are conditionally independent given the global latent variable p(DT|z;\u03bd)=\u220fn+mti=1p(yi|[xi,z];\u03d1). The marginal distribution can be interpreted as the infinite mixture of distributions when z is defined on a continuous domain. Now learning distributions over functions is reduced to a probabilistic inference problem."}, {"title": "INFERENCE SUBOPTIMALITY IN VANILLA NPS", "content": "Previously, variational auto-encoder (VAE) models (Kingma & Welling, 2013; Rezende et al., 2014) mostly set a prior distribution fixed, e.g. N(0,I), as the default to approximate the posterior. This differs significantly from NPs family settings. On the one hand, the functional prior is learned in NPs. On the other hand, the functional prior participates in the performance evaluation.\nExact ELBO for NPs. Following the essential variational inference operation, we can establish connections between the exact ELBO and the log-likelihood in Eq. (5). Given the functional prior p(z|DC;\u03bd) and the generative distribution p(DT|z;\u03bd), the exact functional posterior can be obtained by the Bayes rule\np(z|D;\u03bd)=p(DF|z;\u03bd)p(z|DC;\u03bd)\u222bp(DF|z;\u03bd)p(z|DF;\u03bd)dz.\nThe denominator p(DT|DC) makes exact inference infeasible, and the family of variational poste-riors is introduced to approximate p(z|DT;\u03bd). Here the variational posterior family is defined in a parameterized set Q={q\u03c6(z)|\u03c6\u2208\u03a6}.\nL(\u03d1)=lnp(DT|DC;\u03bd)=Eq\u03c6(z)[lnp(DT,z|DC;\u03bd)]\u2212DKL[q\u03c6(z)||p(z|D;\u03bd)]=LELBO+Posterior Approximation Gap\n(5)\nWhen the variational posterior family is flexible enough, e.g. p(z|D7;9)\u2208Q4, the posterior approximation gap can be reduced to an arbitrarily small quantity. In this case, maximizing the exact ELBO in Eq. (6) increases the likelihood in Eq. (5) accordingly.\nLELBO(\u03d1,\u03c6)=Eq\u03c6(z)[lnp(DT|z;\u03bd)]\u2212DKL[q\u03c6(z)||p(z|D;\u03bd)]\n(6)\nApproximate ELBO for NPs. As previously mentioned, the inference is complicated since the functional prior and the posterior in the exact ELBO are unknown. To this end, Garnelo et al. (2018b) proposes a surrogate objective as an approximate ELBO for NPs. This is defined as Eq. (7) to maximize.\nLNP(\u03d1,\u03c6)=Eq\u03c6(z)[lnp(DT|z;\u03bd)]\u2212DKL[q\u03c6(z)||q\u03c6(z|DF)]\n(7)\nThe Kullback-Leibler divergence between the approximate posterior q\u03c6(z) and the approximate prior q\u03c6(z|DF) is referred to as the consistent regularizer in this paper. We claim that the consistent regularizer is the source of the inference suboptimality of vanilla NPs, and this is shown in Appendix (D.2) as the proof of Remark (1).\nMeta training and testing phases are implemented in a batch of tasks consistent with Eq. (2)/(3)."}, {"title": "OPTIMIZATION GAPS AND STATISTICAL TRAITS", "content": "Other Available Objectives in NPs Family. Now we turn to other tractable optimization objectives in NPs. These include conditional neural processes (CNPs) (Garnelo et al., 2018a) and convolutional neural processes (ConvNPs) (Foong et al., 2020) (or VERSA (Gordon et al., 2018)).\nThe CNP is also a typical model of the NPs family. The objective LCNP(\u03d1) can be obtained when the functional prior collapses into a Dirac delta distribution with z a fixed real-value vector.\nLCNP(\u03d1)=Ep(z|DF;9)[lnp(DT|z;\u03bd)] with p(z|D;\u03bd)=\u03b4(|z\u2212z|)\n(8)\nFor the ConvNP, we do not focus on the convolutional structural inductive bias and concentrate more on the optimization objective itself. Its objective in Eq. (9) is a biased Monte Carlo estimate of Eq. (3), so we can maximize the log-likelihood of marginal distributions straightforwardly.\nLML-NP(\u03d1)=ln1BB\u2211b=1exp(lnp(D|z(b);\u03bd)) with z(b)\u223cp(z|D;\u03bd)\n(9)\nThis is termed as Monte Carlo Maximum Likelihood LML-NP(\u03d1)2, and B is the number of used Monte Carlo samples. Without the involvement of the consistent regularizer, both LCNP(\u03d1) and LML-NP(\u03d1) will not encounter the approximation gap in practice."}, {"title": "EVALUATION CRITERIA & ASYMPTOTIC PERFORMANCE", "content": "As in (Le et al., 2018; Foong et al., 2020), we take a multi-sample Monte Carlo method to evaluate the performance. This applies to both meta training and meta testing processes. In detail, NP models need to run B times stochastic forward pass by sampling z(b)\u223cp(z|DC;\u03bd) and then compute the log-likelihoods as ln[1BB\u2211b=1P(DI|z(b);9)].\nAfter describing the evaluation criteria, we turn to a phenomenon of our interest. In Gaussian pro-cesses (Ghahramani, 2015), with more observed context points, the epistemic uncertainty can be decreased, and the predictive mean function is closer to the ground truth.\nSimilarly, this trait is also reflected in NPs family and can be quantitatively described as follows. Given a measure of average predictive errors \u03b2, the number of context points n and the evaluated dataset, DT, the introduced metrics \u03b2(DT;n) are decreased when increasing n in prediction. In our paper, we refer to this trait as the asymptotic behavior.\nDefinition 3.1 (Prior Collapse) The functional prior p(z|D;\u03bd)=N(z;\u03bc\u03bd(DC),\u03a3\u03bd(DC)) is said to collapse in learning when the trace of the covariance matrix satisfies Tr[\u03a3v(DC)]=\u2211di=1\u03c3i\u22480 with \u03a3v(DC)=diag[\u03c321,...,\u03c32d].\nAs previously mentioned, we can more precisely keep track of measures \u03b2(DT;n), such as predic-tive log-likelihoods or mean square errors of data points, to assess the asymptotic behavior. The role of latent variables z is to propagate the uncertainty about the partial observations in functions. And Definition (3.1) provides a quantitative way to examine the extent of prior collapse in ML-NPs and SI-NPs."}, {"title": "TRACTABLE OPTIMIZATION VIA EXPECTATION MAXIMIZATION", "content": "In this section, we propose alleviating the inference suboptimality of NPs with the help of the vari-ational expectation maximization algorithm. The strategy is to formulate a surrogate optimization objective and then execute the EM-steps in optimization. The benefit of our method is to guarantee performance improvement w.r.t. the likelihood of meta dataset in iterations and finally result in at least a local optimum."}, {"title": "VARIATIONAL EXPECTATION MAXIMIZATION FOR NPS", "content": "This part is to avoid the inference suboptimality of vanilla NPs as mentioned earlier. We retain the neural architectures used in NPs. The basic idea is illustrated in Fig. (2). In detail, we iteratively construct the lower bound L(\u03d1k) and maximize the surrogate function L(v;\u03b8k). The referred optimization gap is due to the complexity of objectives or the choice of optimizers and measures the difference between converged (local) optimal functional prior and the theoretical optimal functional prior. The general pseudo code is Algorithm (1)."}, {"title": "SURROGATE FUNCTION FOR EXACT NPS", "content": "To make the optimization of meta dataset log-likelihood feasible, we construct surrogate functions as the proxy in each iteration step. These meta learning surrogate functions with special properties are closely connected with the original objective, and we leave this discussion in Appendix (E).\nHere \u03d1k denotes the parameter of the latent variable model for NPs in the k-th iteration of variational expectation maximization. Following the Algorithm (1), we take the E-step #1 by replacing the approximate posterior in Eq. (6) with the last time updated p(z|DT;\u03b8k). And this results in the following equation,\nL(v;vk)=Ep(z/DT;vk)[lnp(DT,z|DC;v)\u2212lnp(z|DF;vk)]\n(10)\nProposition 1 The proposed meta learning function L(v;\u03b8k) in Eq. (10) is a surrogate function w.r.t. the log-likelihood of the meta learning dataset.\nThe above proposition is examined based on the definition in Appendix (E.1)."}, {"title": "TRACTABLE OPTIMIZATION WITH SELF-NORMALIZED IMPORTANCE SAMPLING", "content": "Since the second term in Eq. (10) is constant in the iteration, we can drop it to simplify the surrogate objective as the right side of the following equation."}, {"title": null, "content": "max L(\u03d1;\u03b8k) maxLEM1(\u03d1;\u03b8k)=Ep(z|DT;vk)[lnp(DT,z|D;v)]\n(11)\nProposition 2 Optimizing this surrogate function of a batch of tasks via the variational expectation maximization leads to an improvement guarantee w.r.t. the log-likelihood \u2211\u03c4\u2208Tlnp(DT|DC;\u03bd).\nStill we cannot optimize LEM(\u03d1;\u03b8k) since the expectation has no analytical solution and it is in-tractable to sample from p(z|DT;\u03b8k) for Monte Carlo estimates\u00b3. Remember that the marginal distribution p(DT|DC;\u03bdk) is task dependent and can not be ignored in computing the posterior p(z|DT;\u03bdk). To circumvent this, we introduce a proposal distribution q\u03b7(z|DT) and optimize the objective via self-normalized importance sampling (Tokdar & Kass, 2010). The resulting meta learning surrogate function is as follows:\nLEM(\u03d1;\u03b8k)=Ea\u03b7(6)[p(zD;\u03b8k)p(z/DF)lnp(DT,z\\D;\u03bd)]\u2248\u2211Bi=1lnp(DT|z(b);\u03bd)+lnp(z(b)|D;\u03bd)]\u2248LSI-NP(\u03d1;\u03b7k,\u03b8k)(12)\nInp(D|z(6),\u03b8k)Generative LikelihoodFunctional Prior Likelihoodwhere z(b)\u223cq\u03b7k(z/DF),w(b)=exp(lnp(DT|z(b);\u03d1k)+lnp(z(b)|DC;\u03bdk)\u2212lnq\u03b7k(z(b)|D+))\u03a3Bi=1w(6\u2032)and w(b)=Pn(zD\u03b8k)\u2208[1,B].In terms of the first conditional term in Eq. (12), all the data points are conditional independent and this is further expressed as lnp(DT|z(b);\u03bd)=\u2211n+mti=1lnp(yi|[xi,z(b)];\u03bd). In practice, the selection of proposal distributions is empirically tricky, so we make the update of proposal distribu-tions optional in implementations. In our experimental settings, we simply use the functional prior p(z|D;\u03bd) as the default proposal distribution, which is competitive enough in performance.\nProposition 3 With one Monte Carlo sample used in Eq. (12), the presumed diagonal Gaussian prior p(z|DC;\u03bd) will collapse into a Dirac delta distribution in convergence. In this case, SI-NP with the one sample Monte Carlo estimate in Eq. (13) is equivalent with CNP in Eq. (8).\nLSI-NP(\u03d1;\u03b7k,\u03b8k)\u2248Ep(z/DF;vk)[lnp(DT|z;\u03bd)]+Ep(z/DF;0k)[lnp(z|D;\u03bd)]\n(13)\nThe Proposition (3) establishes connections between SI-NPs and CNPs in optimization and we attribute this to the collapse term in Eq. (13). Hence, CNP can be viewed as a particular example in SI-NPs. The complete proof is based on limit analysis and can be found in Appendix (F.1)."}, {"title": "SCALABLE TRAINING AND TESTING", "content": "As shown in Algorithm (1), the meta training process consists of two steps. We skip E-step #2 to avoid unstable optimization observed in empirical results. By repeating E-step #1/M-step iterations until convergence, the method can theoretically find at least a local optimal w.r.t. the log-likelihood of meta learning dataset based on the Proposition (2).\nOnce the learning progress reaches the final convergence, we can make use of the learned functional prior to obtain the predictive distribution. With B particles in prediction, the distribution can be expressed as follows.\np(y|x,D;\u03bd)=Ep(z|DF;0)p(y|[x,z];\u03bd)\u22481BB\u2211b=1p(y|[x,z(b)];\u03bd) with z(b)\u223cp(zD;\u03bd)\n(14)"}, {"title": "EXPERIMENTS AND ANALYSIS", "content": "In this section, two central questions are answered: (i) can variational EM based models SI-NPs, achieve a better local optimum than vanilla NPs? (ii) what is the role of randomness in functional priors? Specifically, we examine the influence of NPs optimization objectives on typical downstream tasks and understand the functional prior quantitatively."}, {"title": "SYNTHETIC REGRESSION", "content": "We create synthetic regression tasks by sampling functions from Gaussian processes. Three types of kernels, respectively Matern-, RBF, and Periodic, are used to generate diverse function distribu-tions. In each iteration, a batch of data points from functions is randomly processed into the context and the target for training models.\nIn meta testing, the same strategy is used to generate functions and data points. We report the average log-likelihood results in Table (1). It shows SI-NPs outperform the vanilla NPs in all kernel cases and are at least competitive with other baselines. ML-NPs are also superior to NPs, but they cannot beat CNPs in RBF and Periodic cases. An illustration of fitted curves is given in Fig. (3): compared to vanilla NPs, we notice that SI-NPs and ML-NPs can better match the ground truth and capture the uncertainty between context data points."}, {"title": "IMAGE COMPLETION", "content": "Similar to experiments in (Garnelo et al., 2018b; Kim et al., 2019), we perform image completion experiments in this section. We implement NPs baselines in four commonly used datasets, namely MNIST (Bottou et al., 1994), FMNIST (Xiao et al., 2017), CIFAR10 (Krizhevsky et al., 2009) and SVHN (Sermanet et al., 2012). During the meta training, the goal is to complete images in which some pixels have been randomly masked out. More precisely, we learn a latent variable model that maps all pixel locations x\u2208[0,1]2 to Gaussian distribution parameters of pixel values based on the context pixel locations and values. Fig. (4) is an example of completion results from sampled images. For implementation details, please refer to Appendix (G)."}, {"title": "RELATED WORK", "content": "NPs Family. NPs are simple and flexible in model formulations, but they may suffer from un-derfitting. Previous research focuses more on the use of structural inductive biases in NPs. (Kim et al., 2019; 2021) improves the predictive performance by adding attention based local variables. Translation equivariance and invariance are incorporated in modeling NPs with help of convolutions (Gordon et al., 2019; Foong et al., 2020; Kawano et al., 2020). To find more compact functional representations, the contrastive loss is used to regularize (C)NPs\u2019 training (Gondal et al., 2021). Hierarchical and mixture structures are also explored to induce diverse latent variables in NPs (Wang & Van Hoof, 2020; Wang & van Hoof, 2022). Lee et al. (2020) combines boostrapping tricks with neural processes to improve expressiveness of latent variables. Besides, NPs are applied to address sequential decision-making problems (Galashov et al., 2019; Wang & Van Hoof, 2022). A summary of these models is given in Appendix (D.3).\nExpectation Maximization & Wake-Sleep Algorithms. For log-likelihood maximization prob-lems with incomplete observations, expectation maximization (Bishop & Nasrabadi, 2006; Balestriero et al., 2020) is a tractable approach in optimization. It consists of an E-step to opti-mize the distribution of latent variables and a M-step to maximize the likelihood parameters. The optimization of VAEs and variants is also built upon an EM framework (Ghojogh et al., 2021; Gao et al., 2021). Our developed algorithm can be interpreted as EM for NPs. Another family of algo-rithms related to our method is wake-sleep algorithms (Bornschein & Bengio, 2014; Eslami et al., 2016; Dieng & Paisley, 2019; Le et al., 2020), where self-normalized importance sampling is used in Monte Carlo estimates. In this case, our method can be also viewed as the extension of the reweighted wake-sleep (RWS) algorithm to NPs with an improvement guarantee. Another differ-ence lies in that optimizing a learnable functional prior is of most importance in NPs, while RWS algorithms are mostly used in scenarios with a fixed prior."}, {"title": "CONCLUSION", "content": "Technical Discussions. In this paper, we study NPs family from an optimization objective per-spective and analyze the inference suboptimality of vanilla NPs. Within the variational expectation maximization framework, our developed SI-NP improves the target likelihood step by step to obtain a more optimal functional prior. Besides, experimental results tell us the learned functional prior has close connections with the number of context points and complexity of function families.\nExisting Limitations. Compared to vanilla NPs, SI-NP requires more than one Monte Carlo sample from the functional prior in training, which consumes more computations. Though intuitively, the uncertainty of the functional prior can be forward propagated to the output distribution, the influence of such uncertainty has not been mathematically studied.\nFuture Extensions. The SI-NP objective is regardless of neural architectures so that any structural inductive bias can be incorporated in modeling. The combination can theoretically produce supe-rior functional representation baselines in the domain (Please refer to Appendix (H.5) for extensive experiments)."}, {"title": "FREQUENTLY ASKED QUESTIONS", "content": "In this section, we list frequently asked questions from researchers who help proofread this manuscript. These raised questions are crucial, so we include more detailed discussions here. Meanwhile, we make extra efforts for readers to quickly understand our work and make slides as follows https://anonymous.4open.science/r/SI_NPs_Slides-7C94/iclr_ submission_slides.pdf.\nLearnable Proposal Distributions. The reason why we set the update of learnable proposal distri-bution an optional choice is that we find it difficult to balance the optimization of Eq. (12) and Eq. (31). This is non-trivial and assigning equal weights in optimization results in unstable performance in experiments (Please refer to Section (H.1) for more discussion.). So, we leave the improvement of the proposal distribution future research. Suppose a reasonable objective or at least a weight scheduling mechanism for two objectives in two E-steps can be developed to stabilize the training. In that case, SI-NPs are likely to achieve even better performance.\nInfluence of Functional Prior Collapse. Generally, the generated functions are more diverse when the functional prior does not collapse: More randomness can be reflected from the functional prior, and the uncertainty of partial observation can be propagated to the output space. However, in ex-periments, we observe that for functions with more superficial structures, e.g. MNIST dataset, the deterministic functional representation in CNPs is sufficient to generate functions of high quality. So there is no consensus on the influence of the functional prior collapse on the generation performance.\nInfluence of the Number of Inference Particles in SI-NPs. With more inference particles (latent variables), SI-NPs can avoid the prior collapse in some cases, but the cost is expensive for computa-tions. To trade-off performance and computations, we set the required number of particles for meta training in a small quantity, e.g. 8 particles for image completion tasks.\nConnections between Different Optimization Objectives. In Table (3), we summarize the con-nections of different objectives with the meta dataset log-likelihood L(v). Note that LNP(\u03d1,\u03c6) is not the exact ELBO of L(v). LML-NP(\u03bd) can also be viewed as the importance weighted method with equal weights for all particles. Since the estimates of importance weights in SI-NPs exploit the target observations and consider the difference in particles, this helps improve the model\u2019s general-ization capability. Though the self-normalized importance sampling makes SI-NPs\u2019 estimates over L(v) biased, there is a probabilistic convergence guarantee towards the true integral quantity of our interest (Please refer to Chapter 9 in the Book \u201cMonte Carlo Theory, Methods and Examples\u201d (Owen, 2013)).\nCombination with Structural Inductive Biases. The primary research focus is on the optimality of the different optimization objectives, and this study applies to more general stochastic processes (stationary or nonstationary cases). Theoretically, the objective of SI-NPs can be combined with most structural inductive biases in Table (4), especially when the function family is complicated. For example, the scale parameter of Matern kernels also varies with kernel values, bringing more variations in realizations. In this case, we guess a global latent variable is difficult to handle: SI-NPs and ML-NPs might encounter a similar performance bottleneck from the empirical observations in the main paper. To further improve the performance, we take the attention module (Kim et al., 2019) as an example to conduct extensive experiments in Section (H.5). Since the meta learning exper-iment is computationally expensive and time-consuming in training processes, we do not examine combinations with other inductive biases in this paper.\nGP Oracles in Synthetic Regression. Note that the GP oracle is computed in the form ln p(Y1:n+m|Y1:n,X1:n+m), where the predictive distribution is mainly with a non-diagonal covari-ance matrix (suggesting Yn+m not independent in statistics). In comparison, the log-likelihood of NPs family is computed in the form \u2211n+mi=1lnp(yi|[xi,z];\u03d1), where Y1:n+m is conditional indepen-dent w.r.t. the global latent variable z. Such a difference in computations causes the mentioned gap. However, with a large neural model, such as the integration of attention networks in Section (H.5), the quantified performance gap of GP Oracles and the log-likelihood of NPs family is well reduced."}, {"title": "PROBABILISTIC GENERATIVE PROCESS IN NPS", "content": "Definition B.1 (Exchangeable Stochastic Processes) We denote a probability space of functions by (\u03a9,F,P). Let Vn\u00d71,\u2026,xn be a probability measure on Rd with {x1,\u2026,xN} a finite index set. The process is called an exchangeable stochastic process S:X\u00d7\u03a9\u2192Rd such that \u2200x1,\u2026,xn(F1\u00d7\u2026\u00d7FN)=P(Sx1\u2208F1,\u2026\u2026\u2026,SN\u2208FN) if it satisfies exchangeable consistency and marginalization consistency.\nHere we can translate the generative process of NPs in the following mathematical way.\n\u03c4\u223cp(T),z\u223cN(z;\u03bc\u03bd(DC),\u03a3\u03bf(DC))(15)\nxi\u223cp(x),yi\u223cp(y|[xi,z];\u03bd)\u2200i\u2208{1,2,\u2026,n+m}"}, {"title": "PREDICTIVE DISTRIBUTIONS IN GPS & NPS", "content": "Here we take one-dimensional deep Gaussian processes (Dai et al., 2016) as an example. With context points DC={(xi,yi)}nci=1 and target points DT={(xi,yi)}n+mti=1=[x\u2217,y\u2217], the key to applications is the predictive distribution p(f(x\u2217)|DC,xT)=N(yT;\u03bcT,\u03a3T). The conditional mean \u03bcT and covariance \u03a3T functions in Eq. (16) are permutation invariant to the order of context points.\n\u03bcT=m\u03b8(xT)+\u2211T,CC\u2211\u22121C,C(yc\u2212mo(xc))(16)\n\u03a3T=\u2211T,T\u2212\u2211T,C\u2211\u22121C,C\u2211C,T\nHere the covariance matrix denoted by \u2211 is computed with the context input xc=(x1,\u2026,xn)\u2208Rn\u00d7d, the target input xT=(x1,\u2026,xn+m)\u2208R(n+m)\u00d7d, and a kernel function \u03c8, e.g. [\u2211C,C]i,j=\u03c8(xi,xj), mo is the mean function m\u03b8, and the context output is yc=(y1,\u2026\u2026\u2026,yn)\u2208Rn. Nevertheless, the computation of matrix inversion in Eq. (16) makes the runtime complexity as expensive as O((n+m)3)."}, {"title": "NPS FORMULATION & STRUCTURAL INDUCTIVE BIASES", "content": "PRIOR, POSTERIOR & PROPOSAL DISTRIBUTIONS\nSince fast adaptation is achieved in an amortized way, which reduces the gradient updates w.r.t. model parameters to learning function specific latent variables with meta trained neural networks.\nDefinition D.1 (Permutation Invariant Functions) Let Sn be an n-element permutation group. For any permutation operator g\u2208Sn, the function is a bijective mapping from the order set {1,2,\u2026,n} to itself:\ng:[1,2,\u2026,n]\u2192[g1,g2,\u2026,gn].\nThen given a set of data points {x1,x2,\u2026,xn}, the function \u03a6 is said to be a permutation invariant function if the following equation is satisfied \u2200g\u2208Sn\n\u03a6(g([x1,x2,\u2026,xn]))=\u03a6([xg1,xg2,\u2026,xgn])=\u03a6([x1,x2,\u2026,xn])."}, {"title": null, "content": "The context points and the target points are treated as sets, so the amortized network should be permutation invariant w.r.t. the order of data points.\nApproximate Posterior Distribution. This is denoted by q\u03c6(z|DT) in this paper. Usually, the approximate posterior is used in NPs (Garnelo et al., 2018b; Kim et al., 2019) and works as a proxy for the non-analytical exact posterior p(z|DT;\u03bd).\nPrior Distribution. This is denoted by p(z|DC;\u03bd) in this paper. Unlike the approximate prior q\u03c6(z|DC) used in NPs, we use an exact functional prior in SI-NPs.\nProposal Distribution. This is denoted by q\u03b7(z|DT) in this paper. The role of the proposal distri-bution resembles that of the approximate posterior in NPs. It is used to sample latent variables and enables the computation of the importance weights in NPs."}, {"title": "FORMULATION OF VARIATIONAL EXPECTATION MAXIMIZATION METHOD", "content": "In this section, we detail the progress of optimizing the NP model with the help of variational ex-pectation maximization algorithms. The concept of meta learning surrogate functions is introduced, and NPs are verified. Meanwhile, the improvement guarantee as well as other concerning technical points are included to better understand our method."}, {"title": "PROOF OF IMPROVEMENT GUARANTEE USING VARIATIONAL EXPECTATION MAXIMIZATION", "content": "\u039cETA LEARNING SURROGATE FUNCTION\nDefinition E.1 (Surrogate Function) 4 Given the objective function f(\u03d1) to maximize", "hold": "ng(\u03d1;\u03d1k)\u2264f(\u03d1)\u2200\u03d1;g(\u03d1k;\u03d1k)=f(\u03d1k).\n(21)\nWhen the objective function f(\u03d1) to maximize is complicated", "paper.\nL(\u03d1;\u03d1k)=\u2211\u03c4\u2208TEp(z/DT;9k)[lnp(DT,z|DC;\u03d1)\u2212lnp(z|DT;Vk)": 22, "E-step": "q\u03c6(z|DC)= Pok"}]}