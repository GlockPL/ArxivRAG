{"title": "Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large Language Model on Knowledge Graph", "authors": ["Xujian Liang", "Zhaoquan Gu"], "abstract": "Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes the naive RAG system a step further by integrating graph information, such as knowledge graph (KGs), into large-scale language models (LLMs) to mitigate hallucination. However, existing GRAG still encounter limitations: 1) simple paradigms usually fail with the complex problems due to the narrow and shallow correlations capture from KGs 2) methods of strong coupling with KGs tend to be high computation cost and time consuming if the graph is dense. In this paper, we propose the Fast Think-on-Graph (FastToG), an innovative paradigm for enabling LLMs to think \"community by community\" within KGs. To do this, FastToG employs community detection for deeper correlation capture and two stages community pruning - coarse and fine pruning for faster retrieval. Furthermore, we also develop two Community-to-Text methods to convert the graph structure of communities into textual form for better understanding by LLMs. Experimental results demonstrate the effectiveness of FastToG, showcasing higher accuracy, faster reasoning, and better explainability compared to the previous works.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) is a cutting-edge technique for large-scale language models (LLMs) to generate accurate, reasonable, and explainable responses. The early RAG, known as Naive RAG (Gao et al. 2023), mostly works by indexing the documents into manageable chunks, retrieving relevant context based on vector similarity (Karpukhin et al. 2020) to a user query, integrating the context into prompts, and generating the response via LLMs. As the paradigm is simple and highly efficient for the basic tasks, Naive RAG is widely adopted in various applications e.g. Q&A (Chan et al. 2024), Recommendation System (Deldjoo et al. 2024), Dialogue System (Ni et al. 2023), etc. However, there remain such as low precision as the ambiguity or bias (Thurnbauer et al. 2023) exist in the embedding model, low recall when dealing with complex queries, and lack of explainability as the queries and documents are embedded in high-dimensional vector spaces.\nThe Graph-based RAG (GRAG) is widely considered an advanced RAG by incorporating KGs as an external source for LLMs. Employing various graph algorithms within a graph database, GRAG empowers LLMs for complex operations (Liang et al. 2024) such as BFS and DFS querying, path exploration, and even community detection, etc, providing LLMs a wider and deeper understanding of the relations within the data, making it possible to execute sophisticated reasoning and the generate more precise responses.\nIn this paper, GRAG is categorized into n-w GRAG and n-d GRAG (Fig. 1) based on the breadth and depth of retrieval. Particularly, there are still quite a few 1-w 1-d Graph RAG researches (Andrus et al. 2022; Baek, Aji, and Saffari 2023; Li et al. 2023). Similar to the \"Retrieve one Read One\" paradigm of Naive RAG, most of the 1-w 1-d research focuses on single entity or one-hop relationship queries, thereby inheriting the shortcomings of Naive RAG. To overcome these, researches (Jiang et al. 2023; Modarressi et al. 2023; Edge et al. 2024; Wang et al. 2024), categorized as n-w GRAG, aim to expand the context window of retrieval. Noteworthy works within the n-w GRAG category focus on treating the densely interconnected groups of nodes as the basic units for retrieval and reasoning. These groups are partitioned from the graph by community detection, allowing them to furnish LLMs with more contextual information compared to single node or random clustering methods. For example, given the query concerning the \u201cclimate of the area where Pennsylvania Convention Center is located\", triples such as (Pennsylvania Convention Center, located in, Market East Section),...,(Pennsylvania, climate, humid subtropical) would be sufficient for answering as the entity Pennsylvania Convention Center of the query is linked to the relevant entity humid subtropical over a short distance (n-hop). Despite its simplicity, the retrieval may falter if the distance is too long between the entities (Fig 1 a,b). n-d GRAG (Sun et al. 2023; Wang et al. 2022a) are ideal for this predicament by broadening the depth of the context window. Analogous to \"Let's think step by step\" (Kojima et al. 2022), n-d GRAG takes entities and relations as intermediate steps in the chain of thought (CoT) to guide the generation of LLMs. As they are tightly coupled with KGs, these paradigms exhibit higher recall in retrieval. However, since LLMs are required to \"think\" at each step, longer chains result in heightened computational costs (Fig. 1, c). On the other hand, if"}, {"title": "Related Work", "content": "Algorithms of Community Detection\nCommunity detection algorithms are used to grouping or partitioning nodes, as well as their tendency to strengthen or separate apart. These algorithms can be categorized into agglomerative and divisive types.\nAgglomerative: These algorithms iteratively group smaller communities until a stop condition is met. One prominent method is hierarchical clustering, which progressively groups nodes or small communities into larger ones by similarity. This down-top grouping procedure is intuitive but computationally expensive, making it impractical for large-scale networks. Since the determination to stop the clustering is important, Louvain (Blondel et al. 2008) and Leiden (Traag, Waltman, and Van Eck 2019) algorithms introduce the modularity benefit function as a measure of community quality. Based on modularity, the optimization procedure can be explicitly stopped as the global modularity can no longer be improved given perturbations."}, {"title": "The Method", "content": "Overview\nWe begin by introducing the basic framework of Fast think on graph (FastToG). Inspired by the work (Kojima et al. 2022) \"Let's think step by step\", the basic idea of FastToG is enabling large models to \"think\u201d community by community on the KGs. Consequently, the core component of the Fast-ToG (see our repository for the pseudocode) are the W reasoning chains P =[p\u2081, p\u2082, ..., p\ud835\udccc], where each chain p\u1d62\u2286 P is a linked list of communities i.e., p\u1d62 = [c\u2080, c\u2081, ..., c\u2099\u208b\u2081]. The FastToG framework comprises two main phases: the initial phase and the reasoning phase. The objective of the initial phase is to determine the start communities c\u2080 and the header community (Fig. 2a) c*\u2080 for each reasoning chain p\u1d62. FastToG prompts LLMs to extract the subject entities of to query x as a single-node community c\u2080. After that, FastToG employs Local Community Search (LCS), which involves two key components: Community Detection on Subgraph (Fig. 2b) and Community Pruning (Fig. 2c), to identify neighbor communities with highest potential for solving the query x, serving as the head community c*\u2080 for each reasoning chain p\u1d62.\nOnce the head communities are determined, the algorithm enters into the reasoning phase. For each p\u1d62, FastToG continues to leverage LCS for local community detection and pruning, with the selected community being added to p\u1d62 as the newest element, termed pruning. After the update of all reasoning chains, Community2Text methods like Graph2Text (G2T) or Triple2Text (T2T) are utilized to convert all communities within chains into textual form as the input content of LLMs, which is called reasoning (Fig. 2d). If the gathered reasoning chains are deemed adequate by the LLMs for generating the answer, the algorithm will be terminated and return the answer. To mitigate time consumption, if no answer is returned within Dmax iterations of reasoning, the algorithm will be degraded into the methods"}, {"title": "Local Community Search", "content": "In this subsection, we focus on the LCS, which consist of there main part: community detection on the subgraph, pruning methods of coarse-pruning and fine-pruning.\nCommunity Detection on Subgraph Due to the vast number of nodes and relations in the KGs, as well as the dynamic of queries, it is impossible to partition the KGs into communities entirely and frequently. To solve this, we propose community detection based on the local subgraphs for each pruning. Given KGs \u03a9, start community c\u2080 or header community c*\u2080, FastToG firstly retrieves the subgraph g (g \u2282\u03a9) within n hops from c\u2080 or c*\u2080. Considering the rapid growth of the number of neighbors in the dense graph and the degradation of semantics as the nodes move further apart, the algorithm randomly samples neighbor nodes at different hops with exponential decay \u03c1. The probability of selecting each node x at n-hop from c\u2080 or c*\u2081 is given by:\nPr(x = 1) = \u03c1\u207f\u207b\u00b9\nOnce the local subgraph g is retrieved, the next step is to partition it into communities. Community detection algorithms are utilized to reveal the closely connected groups of nodes (referred to as communities) of the graph. This process enables the graph to be partitioned into subgroups with tight internal connectivity and sparse external connections, aiding LLMs in uncovering implicit patterns within the graph structure. In this study, we consider Louvain (Blondel et al. 2008), Girvan\u2013Newman algorithm (Girvan and Newman 2002), Agglomerative Hierarchical Clustering, and Spectral Clustering as representative algorithms. To prevent the retrieval of repeated communities, the algorithm ensures that new communities discovered do not exist in the historical community set during each community detection iteration, and adds them to the historical community set after each pruning step.\nThe oversize communities may contain more redundant nodes, leading to the increased noise, prolonged community detection time, and reduced explainability. Thus, we introduce a constraint on the maximum size of community within the community detection. To do this, all detection algorithms will be backtracked when the stop condition is met. At each iteration from the end to the beginning of backtracking, the algorithm verifies whether the size of each community meets the condition size(c\u1d62) <= M (hyperparameter). Finally, the algorithm returns the partition status that first satisfies the size constraint.\nModularity-based Coarse-Pruning When dealing with too many candidate communities, existing methods relying on Large Language Models (LLMs) for community selection encounter difficulties, resulting in either high time consumption or selection bias (Zheng et al. 2023). To tackle this, current methods frequently incorporate random sampling to diminish the number of candidate communities or"}, {"title": "Modularity-based Coarse-Pruning", "content": "nodes. However, these methods inevitably ignore the network structure of the graph, resulting in the loss of structural information from the candidate communities.\nBased on the above, we propose modularity-based coarse-pruning. The modularity (Blondel et al. 2008) of a partitioned subgraph g is:\nQ = 1/2m \u03a3\u1d62\u2c7c[A\u1d62\u2c7c - k\u1d62kj/2m]\u03b4(c\u1d62, c\u2c7c)\nWhere m is the number of edges, A\u1d62\u2c7c is an element in the adjacency matrix A, k\u1d62 and k\u2c7c are the degrees of nodes i and j, respectively. The function \u03b4(c\u1d62, c\u2c7c) = 1 indicates that nodes i and j belong to the same community, otherwise, it returns 0. In the weighted graph, m is the sum of graph weights, 2m \u03a3\u1d62\u2c7c A\u1d62\u2c7c\u03b4(c\u1d62, c\u2c7c) represent the average weight of the given community, while k\u1d62kj \u03b4(c\u1d62, c\u2c7c) denotes the average weight of the given community under random conditions. The larger the difference between them, the higher the quality of community partitioning. For simplicity, we do not consider the weights or directions of edges in our work. The expression1 of modularity can be rewritten as:\nQ = 1/2m \u03a3\u1d62\u2c7cA\u1d62\u2c7c\u03b4(c\u1d62, c\u2c7c) - \u03a3\u1d62\u03a3\u2c7c k\u1d62kj /2m \u03b4(c\u1d62, c\u2c7c)\n= 1/2m [\u03a3c\u03a3\u1d62\u2c7cA\u1d62\u2c7c\u03b4(c\u1d62, c\u2c7c) - \u03a3c(\u03a3\u2c7c k\u1d62)2/2m\nwhere \u03a3\u1d62\u2099 is the number of edges in community c and \u03a3tot is the number of edges connected to c. Thus, the modularity of community e can be:\nQ(c) = \u03a3\u1d62\u2099 - (\u03a3tot)\u00b2 /2m\nUpon calculating the modularity of each candidate community c \u2282 C, the communities C' with low modularity will be pruned, while the remaining will serve as the refined set of candidate communities for the next stage.\nC' := argtopk c\u2282CQ(c)\nLLMs-based Fine-Pruning After coarse pruning, a smaller set of candidate communities C' that are more compact in structure is identified. Subsequently, FastToG prompts the LLMs to make the final selection C''\nC\" = fine_pruning(x, C', \u03a0, k)\nwhere x is the query, \u03a0 is the instance of LLMs, k = 1 or W is for the single or multiple choice.\nTo simplify the pruning process, FastToG no longer considers scoring-based pruning (Sun et al. 2023). Instead, it prompts the LLMs to directly choose either the best community or top W communities. In the initial phase, LLMs are"}, {"title": "Reasoning", "content": "guided through multiple-choice prompts to retrieve W communities, which will act as the header communities c*,...,c*\ud835\udccc for the reasoning chains p\u2081,...,p\ud835\udccc, respectively. During the reasoning phase, single-choice prompts are employed for each reasoning chain to recall the best community c, which is then appended to its chain p\u1d62. Not that the length of each chain p may not be the same if LLMs insist that none of the candidate communities are relevant to the query. In such cases, the exploration of such chains will be discontinued.\nReasoning\nOnce of all the reasoning chains p\u2081 \u2282 P are updated, LLMs will be prompted to integrate and generate the answers from all the chains. The returned results could be a clear answer if the chains are adequate for answering, or \"Unknown\" if not. In cases where \"Unknown\" is returned, the algorithm proceeds to the next iteration until either reaching the maximum depth D = Dmax or obtaining a definitive answer. If the maximum iterations are exhausted without a conclusive response, FastToG will be degraded to generate the answer by the inner knowledge of LLMs itself. Consistent with ToG (Sun et al. 2023), the entire process of FastToG paradigm involves 1 round of pruning and reasoning in the initial phase, and 2WDmax pruning and Dmax reasoning in the second. Consequently, the worst condition needs 2WDmax + Dmax + 2 calls to the LLMs.\nCommunity-to-Text\nKnowledge graphs are organized and stored in the form of RDF triples. Thus, a community also consisted of triples e.g. c = [(Philadelphia, isCityOf, Pennsylvania), (Pennsylvania, climate, Humid Subtropical)]. To input this structure into LLMs, it needs to be converted into text format. To do this, we propose two methods: Triple-to-Text (T2T) and Graph-to-Text (G2T). For T2T, triples are directly converted into text by rule-based scripts e.g. T2T(c) = \u201cPhiladelphia isCityOf Pennsylvania, Pennsylvania climate Humid Subtropical\". For G2T, triple will be converted into human language like G2T(c) = \u201cPhiladelphia, located in the state of Pennsylvania, features a Humid Subtropical climate.\"\nT2T may result in redundancy. For instance, a T2T result of [(Allentown, isCityOf, Pennsylvania), (New Castle, isCityof, Pennsylvania), (Philadelphia, isCityOf, Pennsylvania)] is \"Allentown isCityOf Pennsylvania, New Castle isCityof Pennsylvania, Philadelphia isCityOf Pennsylvania\", which can be summarized as: \"Allentown, New Castle, and Philadelphia are the cities of Pennsylvania\". Therefore, G2T also undertakes the role of the text summary. To do this, we fine-tune the smaller language models (like T5-base, 220M) on the outputs from T2T for the conversion.\nSince G2T leverages a base model with less parameters compared to existing LLMs (e.g., llama-3-8b has 36 times more parameters than T5-base), the impact of time efficiency is tiny. Apart from intra-community relationships, there also exist inter-community paths such as c\u2081-E\u2081\u2082-c\u2082. Therefore, it is necessary to perform text transform on the E\u2081\u2082. This study excludes candidate communities with distance larger than 1 hop from the current community, meaning"}, {"title": "Experimental Setup", "content": "Experimental Setup\nDataset and Evaluation Metric We evaluated FastToG on 6 real-world datasets, which include datasets of multi-hop KBQA: CWQ (Talmor and Berant 2018), WebQSP (Yih et al. 2016), and QALD (Perevalov et al. 2022), slot filling: Zero-Shot RE (abbr. ZSRE) (Petroni et al. 2021) and TREX (Elsahar et al. 2018), and common-sense reasoning Creak (Onoe et al. 2021). To ensure comparability with prior works (Li et al. 2023; Sun et al. 2023; Edge et al. 2024), we employed exact match (hit@1) as the evaluation metric. Considering the computational cost and time consumption, we randomly sampled 1k examples from the datasets containing more than a thousand data. Furthermore, the initial entities provided by (Sun et al. 2023) were used as the starting community for the evaluations.\nLanguage Model To reduce the computational cost, we employ two LLMs: gpt-4o-mini\u00b9 and Llama-3-70b-instruct (Dubey et al. 2024) without quantization. To keep the pruning steady, the temperature is configured to 0.4 during pruning and 0.1 during reasoning. Considering the text descriptions of community structure are longer, the maximum length of output is increased to 1024.\nGraph2Text Model The Graph2Text module is developed by fine-tuning T5-base (Raffel et al. 2020) using dataset WebNLG (Auer et al. 2007) and synthetic data generated by GPT-4. To build the synthetic data, we prompt GPT-4 to generate text descriptions of given communities, which are the byproduct of the experiments.\nKnowledge Graph We utilize Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch 2014), which is a free and structured knowledge base, as source of KGs. The version of Wikidata is 20240101 and only the triples in English are considered. Following extraction, 84.6 million entities and 303.6 million relationships are stored in Neo4j2."}, {"title": "Performance on Accuracy", "content": "Performance on Accuracy\nAccuracy is one of the most important criteria for RAG systems. In this experiment, we evaluate FastToG and compared methods on the datasets and settings mentioned above.\nCompared Methods We consider two categories of comparative methods: Inner-knowledge-based or KGs-retrieval-based methods. The former methods include: 1) IO prompt (Brown et al. 2020): prompting the model to directly generate the result. 2) CoT (Wei et al. 2022): guiding the model to \"think\" step by step before answering. 3) CoT-SC (Wang et al. 2022b): ensembling all the CoTs to obtain more consistent predictions. The KGs-retrieval-based methods include: 1) 1-d 1-w methods, which represent the methods like CoK"}, {"title": "Ablation Study", "content": "Ablation Study\nTrade-off between Accuracy and Efficiency While larger communities can reduce the average depth of reasoning chains, such large communities may also bring more noise, potentially negatively impacting the reasoning effectiveness of LLMs. Fig. 4 illustrates the relationship between the MaxSize and accuracy. Overall, most of the cases achieve higher accuracy when MaxSize is set to 4. However, at MaxSize = 8, results show a decrease in accuracy. Therefore, setting a larger size of the community does not necessarily result in higher gain from accuracy.\nComparison on Pruning Methods To validate the effectiveness of our proposed Modularity-based Coarse Pruning, we compared it with Random Pruning, a method widely used by previous works. Fig. 5 depicts the accuracy comparison between Modularity-based coarse pruning and random pruning on the CWQ and WebQSP datasets for 2 modes, with all cases using MaxSize = 4. Overall, Modularity-based Coarse Pruning outperforms Random Pruning in all cases. Particularly, we observed that cases based on g2t mode are more sensitive in modularity-based pruning, indicating that communities of densely connected structure are preferable for the conversion between graphs and text.\nComparison on different Community Detection Community detection is a crucial step in FastToG. Tab. 3 compares the impact of different detection algorithms - including Louvain, Girvan-Newman (abbr. GN), hierarchical clustering (abbr. Hier), and Spectral Clustering (abbr. Spectral) on accuracy. Additionally, we consider random community detection (abbr. Rand), which randomly partitions nodes into different groups. Comparing the 4 non-random algorithms, the impact of different community detection on FastToG is very small (< 1% on average). On the other hand, when comparing algorithms between random and non-random, the latter outperforms the former (> 3% on average), demonstrating that community detection does help FastToG.\nCase Study\nFor the query \"Of the 7 countries in Central America, which consider Spanish an official language?\" from dataset CWQ, we visualize the retrieval process of FastToG. Fig. 6 displays a snapshot of LLMs pruning with start community Central America. Note that the node Central America has 267 one-hop neighbors, making it hard to visualize. Tab. 4 shows the corresponding Graph2Text output of the communities or nodes. The left column shows part of the pruning with communities as the units to be selected, while the right column shows the nodes. As we can see, community as the basic"}, {"title": "Conclusion", "content": "Conclusion\nWe introduced a novel GraphRAG paradigm - FastToG. By allowing large models to think \u201ccommunity by community\u201d on the knowledge graphs, FastToG not only improves the accuracy of answers generated by LLMs but also enhances the efficiency of the RAG system. We also identified areas for further improvement, such as incorporating semantics and syntax with community detection, introducing multi-level hierarchical communities to enhance retrieval efficiency, and developing better community-to-text conversion."}, {"title": "Appendix", "content": "Appendix\nA. Algorithms for FastToG\nwe provide the relevant algorithms for FastToG. Algorithm 1 is the basic algorithmic framework of FastToG and Algorithm 2 is the implementation of Local Community Search.\nAlgorithm 1: FastToG\nRequire: LLMS \u03c0, Knowledge Graph \u03a9\nInput: query \u0445\nParameter:\nW width of reasoning chains\nImax maximum iteration for reasoning\nRmax maximum radius of subgraph\nInitialize:\nreasoning chains P \u2190 [] \u00d7W\nhistory set of community H\nI\u21900\n// initial phase\n1: c\u00ba extract entities on x\n2: C* \u2190 LCS(x, c\u00ba, \u03c0,\u03a9, Rmax, W)\n3: Append each c* of C* to p\u1d62 of P respectively\n// reasoning phase\n1: I = 0\n2: while I < Imax do\n3: for each p\u1d62 in P do\n4: c\u1d62 \u2190 get the last community from p\u1d62\n5: c* \u2190 LCS(x, c\u1d62, \u03c0, \u03a9, Rmax, 1)\n6: append c* to p\u1d62\n7: add c* to H\n8: end for\n9: Ptext \u2190 Community2Text(P)\n10: if reasoning(\u03c0, x, Ptext) then\n11: break\n12: end if\n13: Increment I by 1\n14: end while\n15: if I < Imax then\n16: reasoning(\u03c0, x) //IO,COT\n17: end if\nAlgorithm 2: LocalCommunitySearch (abbr. LCS)\nInput:\nquery x\ncurrent community c\nLLMS \u03c0, Knowledge Graph \u03a9\nmaximum radius of subgraph Rmax\nnumber of candidate community N\n1: g subgraph(c, \u03a9, Rmax)\n2: C\u2190 community_detection(g)\n3: C' \u2190 coarse_pruning(C \u2013 H, c) //based on modularity\n4: gtext \u2190 Community2Text(g)\n5: C\" \u2190 fine_pruning(C', x, \u03c0, \u039d)\n6: return C''"}, {"title": "B. Details of Graph2Text", "content": "B. Details of Graph2Text\nB1. Training Details We employed a straightforward fine-tuning approach using T5-base to implement Graph2Text. The fine-tuning data for Graph2Text comprised a combination of open-source datasets and AI generated datasets via prompt-based LLMs, with 10% of the data allocated for validation. Statistics of fine-tuning and Examples of the data are listed in Tab 6 and Tab 7, respectively.\nFor fine-tuning, apart from configuring the learning rate to 1e-5 and Epoch=2, all other parameters remained consistent with T5-base. We trained multiple models and ultimately selected the model based on Rougel as the filtering metric. The fine-tuning results are shown in Table 5. All G2T models were fine-tuned on an NVIDIA Tesla T4.\nB2. Errors Analysis From the above experiments, we observe some cases where the T2T-based modes exceed the G2T-based ones. We hypothesize that the Hallucination generated by the Graph2Text model could have a negative impact on reasoning in LLMs, leading to a decrease in accuracy. To validate it, we manually analyzed 100 data samples drawn from each dataset and each max size of the communities for the statistic of hallucination. Fig. 7 illustrates the percentage of the hallucinations across different max size of community. It can be observed that the proportion of hallucinations increases dramatically as the max size of community increases. This explains the existence of higher performance"}]}