{"title": "Marginal Causal Flows for Validation and Inference", "authors": ["Daniel de Vassimon Manela", "Laura Battaglia", "Robin J. Evans"], "abstract": "Investigating the marginal causal effect of an intervention on an outcome from complex data remains challenging due to the inflexibility of employed models and the lack of complexity in causal benchmark datasets, which often fail to reproduce intricate real-world data patterns. In this paper we introduce Frugal Flows, a novel likelihood-based machine learning model that uses normalising flows to flexibly learn the data-generating process, while also directly inferring the marginal causal quantities from observational data. We propose that these models are exceptionally well suited for generating synthetic data to validate causal methods. They can create synthetic datasets that closely resemble the empirical dataset, while automatically and exactly satisfying a user-defined average treatment effect. To our knowledge, Frugal Flows are the first generative model to both learn flexible data representations and also exactly parameterise quantities such as the average treatment effect and the degree of unobserved confounding. We demonstrate the above with experiments on both simulated and real-world datasets.", "sections": [{"title": "Introduction", "content": "Simulating realistic datasets such that the marginal causal effect is constrained to take a specific form is a significant challenge in causal inference. Many methods for inferring these effects exist, but simulating from them is a significant challenge (Young et al., 2008; Havercroft and Didelez, 2012; Keogh et al., 2021). In particular, it is difficult to simulate complex benchmarks from generative models in such a way that a custom marginal effect exactly holds.\n\nThe frugal parameterisation (Evans and Didelez, 2024) provides a solution to this problem by constructing a joint distribution that explicitly parameterises the marginal causal effect and builds the rest of the model around it. Frugal models typically represent the dependency between an outcome and pretreatment covariates using copulae. Standard multivariate copulae are parametric, leading to potential model misspecification.\n\nIn this paper we show how one can construct frugally parameterised marginal causal models using normalising flows (NFs, Rezende and Mohamed, 2015; Dinh et al., 2016) to target the causal margin of the distribution (a conditional univariate marginal density of an outcome conditioned on a treatment). We name the resulting model a Frugal Flow (FF). To the best of our knowledge, FFs offer the first likelihood-based framework for learning a marginal causal effect while modelling the outcome and propensity nuisance parameters.\n\nFFs are exceptionally well suited for generating benchmark datasets for causal method validation. Since FFs enable direct parameterisation of the causal margin, they provide a framework for generating causal benchmark datasets which resemble real-world datasets, but which also allow users to encode causal properties in order to validate novel inference models. FFs can be used to generate benchmarks with customisable degrees of unobserved confounding. This can aid in the validation"}, {"title": "Background", "content": "In this paper we consider a static treatment model with an outcome $Y \\in \\mathcal{Y} \\subseteq \\mathbb{R}$ and T a binary treatment in $\\mathcal{T} = \\{0,1\\}$. Let the set of measured pretreatment covariates be $\\mathbf{Z} \\in \\mathcal{Z} \\subset \\mathbb{R}^D$. Additionally, we will use the notation of Pearl (2009) where intervened distributions are indicated by the presence of a \"do(.)\" operator, with its absence indicating that the distribution is from the observational regime."}, {"title": "Marginal Causal Models", "content": "Causal inference methods are generally developed to estimate the average effect of a treatment (T) on an outcome (Y) for a population defined by a set of pretreatment covariates (Z) (Hern\u00e1n and Robins, 2020). Let the variables be distributed according to $(Z, T,Y) \\sim P_{ZTY}$ with density $p_{zTY}$. We make the standard assumptions of a stable unit treatment value (commonly referred to as SUTVA), positivity, and conditional ignorability (equivalent to conditional exchangability) outlined in Pearl (2009). Additionally, the covariate set Z must only include pretreatment covariates. The conditional distribution of Y and Z after an intervention on T is equal to\n\n$P_{ZY|do(T)}(z, y | t) = p_Z(z) \\cdot P_{Y|Z,do(T)} (y | z, t)$.\n\nCausal practitioners are often interested in the marginal effect of T on Y on the intervened system, sometimes referred to as the marginal outcome distribution (MOD), $P_{Y|do(T)}$:\n\n$P_{Y|do(T)} (y | t) = \\int_\\mathcal{Z} dz P_{Y|Z,do(T)} (y | z,t) p_Z(z)$.\n\nThe difference between the means of Y under this margin between different values of T is called the average treatment effect (ATE), $\\tau$ where, $\\tau = \\mathbb{E}[Y | do(T = 1)] - \\mathbb{E}[Y | do(T = 0)]$. Models which target this marginal quantity are known as marginal structural models (MSMs, Robins, 1998) and are frequently used in epidemiological and medical domains to account for time-varying confounding. In particular, they are effective at quantifying the effect of an intervention over a population, where the specific relationships between the outcome and (possibly high dimensional) pretreatment covariates are not relevant, and are modelled as nuisance parameters. The semiparametric question of estimating finite dimensional quantities in the presence of high dimensional nuisance parameters has a long history (Robins et al., 1995; Robins and Rotnitzky, 1995), but has undergone a renaissance since the development of methods such as targeted maximum likelihood estimation (van der Laan and Rose, 2011) and double machine learning (Chernozhukov et al., 2018), which allow for general machine learning algorithms to flexibly describe the nuisance models and still have valid inference on a low-dimensional treatment effect."}, {"title": "Frugal Parameterisations", "content": "Frugally parameterised distributions consist of three distinct components: the distribution of the 'past,' $\\theta_{ZT}$; the intervened causal quantity of interest, $\\theta_{y|do(T)}$; and an intervened dependency measure between Y and Z conditional on T, $\\O_{ZY|do(T)}$. The key idea is to explicitly parameterise the marginal causal effect, and build the rest of the model around it. In this paper we encode all the dependence among covariates in the copula, so 'the past' is really just the propensity for treatment"}, {"title": "Copulae", "content": "A multivariate copula, denoted by $C : [0,1]^d \\rightarrow [0, 1]$ is a multivariate cumulative distribution function (CDF) defined over a set of d uniform margins, with an associated density $c(\\cdot)$ if it is continuous with respect to its arguments (Sklar, 1959; Joe, 2014). Copulae are often used to parameterise the dependency structure of a joint distribution independent of its univariate margins. Large, complex dependency structures are often modelled by pair-copula constructions (PCCs) or vine copulae (Czado and Nagler, 2022; Joe and Kurowicka, 2011). These methods factorise the dependency structure into a set of non-overlapping bivariate copulae. However, these approaches typically impose the constraints of a finite dimensional parameterisation on the dependency structure in the bivariate copulae used. A more comprehensive introduction to copulae can be found in Appendix B.\n\nCopulae in Machine Learning More complex ML models have been developed to more flexibly learn copula distributions. Several alternatives have been proposed, some targeting specific copula classes (Ling et al., 2020; Wilson and Ghahramani, 2010), and others constraining a neural network-based architecture to estimate valid copulae, though often with limited scalability (Zeng and Wang, 2022; Chilinski and Silva, 2020) or using variational approximations (Letizia and Tonello, 2022). However, the most active research area in this field makes use of normalising flows, leveraging their likelihood-based, composable and invertible nature to chain transformations of marginal quantities to the fitting of the copula density.\n\nPaper Motivation A key motivation for this paper is the search for a flexible parameterisation of the copula\n\n$\\$ZY|do(T) $(z, y | t) = c(F_{Y\\do(T)}(y | t), F_{Z_1} (z_1), ..., F_{Z_D} (z_D))$"}, {"title": "Normalising Flows", "content": "Normalising flows (NFs) (Tabak and Turner, 2013; Rezende and Mohamed, 2015; Dinh et al., 2016) allow for density estimation via learning a diffeomorphic transformation F that maps the unknown target distribution $p_X(x)$, $x \\in \\mathbb{R}^D$ to a simple and known base distribution $p_U (u)$, $u \\in \\mathbb{R}^D$, so that when $X \\sim p_X$ and $U \\sim p_U$ then $U = F^{-1}(X)$.\n\nF is usually a composition of invertible and differentiable transformations $F_i$ parametrised by neural networks, and is often trained by maximising the log-likelihood of observed $\\{x_i\\}_{i=1}^N$. This can be conveniently done in closed form exploiting the change of variable formula\n\n$p_X(x) = p_U(F^{-1}(x)) \\text{ det }\\left(\\frac{\\partial(F^{-1}(x))}{\\partial x}\\right)$,\n\nprovided that the chosen model for $F$ allows for efficient computation of the Jacobian determinant $\\text{det}(\\partial(F^{-1}(x))/\\partial x)$. The implementation of $F^{-1}$ then allows for density evaluation, whereas $F$ can be used for sampling from the joint.\n\nAs for the choice of F, the literature has explored a number of implementations that retain invertibility while allowing for computational tractability of the determinant. See Papamakarios et al. (2021) for an introduction and overview. Our implementation relies on neural spline flows (NSF, Durkan et al., 2019), a particular type of autoregressive flows that will be further illustrated in Section 2.5."}, {"title": "Copula Flows", "content": "Our Frugal Flow approach builds upon the copula-based flow model proposed by Kamthe et al. (2021) for synthetic data generation. The authors start by considering a copula $C(F_{X_1},..., F_{X_D})$ defined over the marginal probability integral transforms $F_{X_1},..., F_{X_D}$ of a random vector $X = [X_1, ..., X_D]$. Assuming the copula density exists, the joint density of X can be written as\n\n$p_X(x_1,...,x_d) = c_X (F_{X_1} (x_1),..., F_{X_D}(x_D))\\prod_{d=1}^D p_{X_d}(x_d)$,\n\nwhere $p_{X_d}$ is the marginal density of $X_d$. This factorisation of the density can be similarly induced by a NF that composes D flows $F_1, . . ., F_D$ for the marginal quantities and a flow $C_X$ for the copula.\n\nFor the rest of this paper, we will let $U \\sim \\text{Uniform}[0, 1]^D$ represent a vector of independent uniforms, and let $V \\sim C$ represent a vector of dependent uniforms as a multivariate copula C. The generative"}, {"title": "Validating and Benchmarking Causal Methods", "content": "Methods for validating causal models can be broadly categorised into two groups. The first comprises auxiliary analyses conducted after fitting a causal model and estimating a treatment effect. These include but are not limited to sensitivity analyses (Imai et al., 2010), subgroup analyses (Cochran and Chambers, 1965), placebo tests (Eggers et al., 2023), and negative controls (Shi et al., 2020).\n\nThe second set of validation methods is where we see FFs having a significant impact. These methods are used to construct synthetic datasets while allowing the causal practitioner to customise specific features of the data-generating process. For example, when validating an inference method which estimates an ATE under certain confounding assumptions, it is crucial that generated data follow the \"ground truth\" ATE and confounding assumptions one wishes to measure. However, synthetic data risk being oversimplified and contrived, failing to reflect the complexity of real world datasets.\n\nTo mitigate this, generative models are trained on real-world data and calibrated to generate samples with modifiable causal constraints. Such constraints include the average causal treatment effect, unobserved confounding, and positivity. To our knowledge, the FF framework proposed in this paper is the first method to allow all of these conditions to adjusted by the user. Existing methods (Neal et al., 2020; Athey et al., 2021; Parikh et al., 2022) encode these effects through soft optimisation constraints, hence there is no guarantee that the constraints are satisfied. Enforcing these constraints too strongly may negatively impact model optimisation, and may affect the reconstructive ability of the underlying model. Furthermore, since these approaches do not explicitly parameterise the causal effect, samples from trained models must be tested post hoc to ensure the desired constraints are present in the sampled data. A key benefit of frugal models is that the marginal causal effect is directly parameterised by the user through the likelihood. As a result, synthetic data samples will exactly satisfy these constraints."}, {"title": "Method", "content": "In this section we parameterise the full observational joint using FFs. Section 3.1.1 outlines how the FF is constructed; we first learn the probability integral transforms of the pretreatment covariates, and then infer the causal margin jointly with an extended copula flow, the Frugal Flow. To infer the causal margin, this is sufficient. Nevertheless, the propensity score is needed to complete the joint in order to generate benchmarks which are confounded in a similar fashion to the original real-world dataset. We describe the fitting of the propensity score in Section 3.1.2"}, {"title": "Constructing Frugal Flows", "content": "The first step involves learning the margins for the pretreatment covariates Z. This is done in a similar fashion to that of Kamthe et al. (2021)'s copula-based flows, as described in Section 2.5. The outcome, treatment, and the inferred ranks $V_Z$ of the pretreatment covariates are then used to train the Frugal Flow (see bottom part of Figure 2) that models $F_{y|do(T)}$ together with the copula flow. This is required in order to learn the causal marginal $P_{Y|do(T)}$ rather than the conditional $p_{y|T}$.\n\nThe Frugal Flow of dimension D + 1 transforms the joint input of $(Y, V_Z | do(T))$ into a random vector U which we set to be distributed according to an independent uniform base distribution. In the first subflow of the composition, Y is pushed through a univariate flow $F_{Y|do(T)}$ conditioned on T to obtain $V_{Y|do(T)}$, while the $V_Z$ remain untransformed. Subsequently, $V_{Y|do(T)}$ is kept fixed, while a copula is learnt over $V_Z$ conditional on $V_{Y|do(T)}$ via an NSF. Importantly, a specific ordering of the variables is imposed, such that the causal margin is ranked first. In this way, we ensure that $U_1$ and $V_{Y|do(T)}$ have the same distribution, and $V_{Y|do(T)}$ is therefore constrained to be uniform. The marginal flow $F_{y|do(T)}$ thus targets the CDF of the marginal causal effect, $F_{y|do(T)}$.\n\nIn summary, we construct a flow $\\Q^{-1} : (Y, V_{Z_1},..., V_{Z_D} | T) \\rightarrow V$ as a composition of a marginal flow $F_{Y|do(T)}$ and conditional copula distribution $C^{-1}(v_{y|do(T)}, U_{Z_1},..., U_{Z_D}) = C(U_{Z_1},..., U_{Z_D} | V_{Y|do(T)})$. More on the implementation details can be found in Appendix C."}, {"title": "Learning the Propensity Flow", "content": "We constructed the conditional distribution of Y and Z after an intervention on T in Section 3.1.1:\n\n$P_{ZY|do(T)} (z, y | t) = \\prod_{i=1}^D PZ_i (z_i) P_{Y\\do(T)} (y,t) C_{ZY\\do(T)} (U_{Y\\do(T)}, U_{Z_1}, ..., U_{Z_D}).$\n\nInferring the above is sufficient for identifying the causal margin. However, to generate realistic samples for causal method validation, one also needs to learn the propensity score, $p_{T|Z} = p_T.c_{T|Z}$. By decoupling the marginal treatment density $p_T$ from the conditional copula $c_{TZ}$, one can modify the marginal treatments while retaining the dependence of the original data. We therefore learn an approximate probability integral transform of the discrete treatment T (see Appendix B.2.1 for further details), followed by the conditional copula flow of T on Z, $C_{T|Z} : V_T \\rightarrow V_{T|Z} | Z$.\n\nOne could directly model $P_{T|Z}$ using a normalising flow, which would also constitute a valid frugal model. We instead choose to model the conditional copula using a flow, $C_{T|Z} = C_{TZ}$, allowing users to encode a degree of unobserved confounding in the generated data by sampling the ranks $V_{T|Z}$ and $V_{Y|do(T)}$ from a non-independence copula. Assuming ignorability, these ranks would be independent. However, unobserved confounders imply dependence between these ranks. Sampling them from a copula can replicate this effect, as demonstrated in the far-right plots in Figures 3 and 4.\n\nThe above section describes how one can estimate the propensity of treatment from a real-world dataset. However, we remark that one can choose any custom propensity score function to generate treatments conditional on the pretreatment covariates via inverse probability integral transforms on $V_{TZ}$. Hence, one can fully control the overlap/positivity of FF generated benchmark datasets."}, {"title": "Generating Synthetic Benchmarks", "content": "Data generated from a fitted FF can be customised with a range of properties, allowing for model validation against a range of customisable causal assumptions. We describe these below.\n\nModifying the Causal Margin The central output of the Frugal Flow is a method for sampling ranks for each of the margins in $P_{y| do(T)}, P_{Z_1},..., P_{Z_d}$. Any causal marginal density $Q_{Y|do(T)}$ can be used to generate samples of Y via inverse probability integral transforms. Since the Frugal Flow returns ranks for the intervened causal effect, these can be inverse transformed by any valid CDF. Unlike other methods, this constraint is strictly enforced by the the frugal likelihood.\n\nSimulating from Discrete Outcomes Since FFs return $V_{Y|do(T)}$ ranks, one can sample from any custom causal margin. This extends to both continuous and discrete causal margins. One can simulate from a logistic marginal effect $Y | do(T) \\sim Bernoulli(p = \\text{expit}(\\beta T + c))$ or probit model $Y | do(T) \\sim Bernoulli(p = \\Phi(\\beta T + c))$ where $\\Phi(\\cdot)$ is a univariate standard Gaussian CDF. This is non-trivial, because logistic regression is not collapsible, meaning that if (for example) $Y | T = t, Z = z$ is a logistic regression, then $Y | do(T = t)$ generally will not be. Hence it is infeasible for a fully conditional method of simulation to produce outcomes where the causal margin uses a logistic link. For experimental results see Appendix D.2.1.\n\nModifying the Degree of Unobserved Confounding One can sample data from FFs as if the outcome is affected by unobserved confounding. The variables $V_{Y|do(T)}$ and $V_{T|Z}$ are independent of each other if no unobserved confounding is assumed. Introducing a dependence between these ranks replicates the effect of unobserved confounding. This can be achieved by sampling $(V_{Y|do(T)}, V_{T\\z})$ from a Gaussian bivariate copula, $c(U_{Y|do(T)}, U_{T|z}; \\rho)$, where $\\rho$ quantifies the degree of unobserved confounding in the sampled data."}, {"title": "Customising Treatment Effect Heterogeneity", "content": "Consider a stationary treatment with pretreament covariate set $Z = (W,W)$ where $W \\subset Z$ with $|Z| = D$, $|W| = d$, and $|W| = D - d$. We proceed considering the case where $0 < d < D. Interest may lie in the causal treatment margin conditional on the subset of variables W:\n\n$P_{Y|W,do(T)} (y | w,t) = \\int_\\mathcal{W} dw P_{Y|z,do(T)} (y | w,w,t) p_{W|w}(w | w)$\n\nWe propose a method to exactly parameterise heterogeneous treatment effects using a subset of pretreatment covariates, $W \\subset Z$. FFs offer exact parameterisation of $p_{y|W,do(T)}$, allowing for customisation of heterogeneity while capturing complex dependencies between other covariates. Specifically, the model infers the conditional treatment margin, $p_{y|W,do(T)} (y | w, t)$, ensuring proper inference of the joint pretreatment covariate distribution, $p_z(\\cdot)$. Thus, one may simulate data where causal effects are conditional on a selected subset of variables, offering flexible and precise control over treatment heterogeneity. Further details may be found in Appendix D.2.2."}, {"title": "Customising the Propensity Score", "content": "Since the propensity score is variation independent from the rest of the model, one has complete flexibility on how to parameterise the propensity score. Any distribution $P_{TZ}$ can be used to generate treatments with varying degrees of overlap in a manner that is completely customisable by the user."}, {"title": "Experiments", "content": "The following section discusses our experiments and results, which aim to i) demonstrate that FFs accurately infer the true MOD for confounded data, and ii) show how a trained FF can generate synthetic datasets that meet user specified causal margins and unobserved confounding."}, {"title": "Inference", "content": "We generate simulated data from three models. The first two are parameterised by four pretreatment covariates $Z = \\{Z_1,..., Z_4\\}$ with a binary treatment T, a linear Gaussian causal margin $Y | do(T) \\sim \\mathcal{N}(\\mu = T + 1,\\sigma = 1)$, and a copula dependence measure $c(V_{Y|T}, U_{Z_1}, ..., U_{Z_4})$. In the first model $M_1$, all four covariates follow a gamma distribution. In the second $M_2$, the data is generated from an even split of gamma and binary covariates. Additionally, we generate data from model $M_3$ with ten pretreatment covariates comprising five gamma and five binary variables. A more quantitative description of the simulated data generating process and hyperparameter values are presented in Appendix D.1."}, {"title": "Benchmarking and Validation", "content": "In this section we present the results of multiple causal inference methods on data generated from FFs trained on two real-world datasets. The first is the Lalonde data, taken from a randomised control trial to study the effect of a temporary employment program in the US on post intervention income level (LaLonde, 1986). The second is an observational dataset used to quantify the effect of individuals' 401(k) eligibility on their accumulated net assets, in the presence of several relevant covariates (Abadie, 2003). Both datasets have a binary treatment and continuous outcome. Appendix D.3 can be referred to for a more comprehensive description of the data. In addition, we present diagnostics on the quality of the model fit in Appendix D.3.6, and the loss optimization for both datasets is presented in Appendix D.3.7."}, {"title": "Conclusions", "content": "We introduce Frugal Flows, a novel likelihood-based model that leverages NFs to flexibly learn the data-generating process while directly targeting the marginal causal quantities inferred from observational data. Our proposed model addresses the limitations of existing methods by expliclitly parameterising the causal margin. FFs offer significant improvements in generating benchmark datasets for validating causal methods, particularly in scenarios with customizable degrees of un-observed confounding. To our knowledge, FFs are the first generative model that allows for exact parameterisation of causal margins, including binary outcomes from logistic and probit margins."}, {"title": "Limitations and Future Work", "content": "Our experiments validated the empirical effectiveness of FFs, showing that they can infer the correct form of causal margins on confounded data simulations. Despite these promising results, FFs come with certain limitations that need to be addressed in future research. NFs require extensive hyperparameter tuning, which can be computationally intensive and time-consuming. Moreover, we see that FFs perform better in inference tasks with larger datasets. Future work could explore alternative ML copula methods and architectures that may be more effective for smaller datasets. Fortunately, this is less problematic for simulation as specification of the exact causal margin is left to the user. Additionally, the dequantising mechanism used by FFs implicitly shuffles the order of discrete samples, potentially losing some inherent structure in the data, making FFs less suitable for categorical datasets without implicit ordering.\n\nIn summary, Frugal Flows offer a novel approach to causal inference and model validation that combines flexibility with exact parameterisation of causal effects. Future work will refine the inference capabilities and extend the applicability of FFs to a wider range of data types and sizes."}, {"title": "The Frugal Parameterisation", "content": "The frugal parameterisation proposed by Evans and Didelez (2024) provides a method for simulating from a parametric marginal causal model, by starting with this distribution and building the rest of the model around it.\nWe specify the notation used in this appendix. Functions labelled $F_i(\\cdot)$ are CDFs for the variable i. Apart from this, in general density functions will be labelled with a lower case letter, whereas CDFs will be named with the upper case (e.g. we contrast the copula density c(41, 42) with the distribution function C(U1, U2)).\n\nConsider firstly the case of a static treatment model with a single outcome Y, a single treatment T and an effective pretreatment covariate set Z. Assume that any of these covariates occur prior to treatment even if they do not causally affect the treatment directly. Evans and Didelez (2024) construct frugal models in three parts:\n\n\u2022 The causal distribution of interest $P(Y | do(T))$\n\n\u2022 The past $P(Z,T)$\n\n\u2022 The intervened variation independent dependency measure $(Y, Z | do(T))$.\n\nThe three frugal components are variation independent in the sense that they characterise non-overlapping components of the full observational joint. We quote the following definition from Evans and Didelez (2024):\n\nTake a set $\\Theta$ and two functions defined on it $\\phi$, $\\psi$. We say that $\\phi$ and $\\psi$ are variation independent if $(\\phi \\times \\psi)(\\Theta) = \\phi(\\Theta) \\times \\psi(\\Theta)$; i.e. the range of the pair of functions together is equal to the Cartesian product of the range of them individually.\n\nVariation independence (VI) is a highly desirable property for a parameterization, since it allows different components to be specified entirely separately. This is extremely useful if one is trying to use a link function in a GLM, or to specify independent priors for a Bayesian analysis. In addition, VI is important in semiparametric statistics. The definition simply states that the Cartesian product of the images is the same as the image of the joint map. For example, in a bivariate gamma-distribution with positive responses, then $\\mu_1 \\in R+$ and $\\mu_2 \\in R+$ is a variation independent parameterization, since\n\n$(\\mu_1 \\times \\mu_2)(\\Theta) = R+ \\times R+ = \\mu_1(\\Theta) \\times \\mu_2(\\Theta)$.\n\nHowever, if we replace $\\mu_2$ with $\\mu'_2 = \\mu_2 - \\mu_1$ (for example), then although the range of this parameter is R,\n\n$(\\mu_1 \\times \\mu'_2)(\\Theta) = \\{(x, y) : x > 0, y > -x\\} \\neq R+ \\times R = \\mu_1(\\Theta) \\times \\mu'_2(\\Theta)$.\n\nCentral to this is the choice of $\\$\\$. This dependency measure should encode dependencies between Z and $Y | do(T)$, but not provide information about their marginal distributions.\n\nDiscrete frugal models can be parameterised by conditional odds ratios, while continuous variables typically use copulae. Both allow for variation independent parameterisation of the full joint dis-tribution. The methodology facilitates the creation and simulation of models with parametrically determined causal distributions, enabling fitting using likelihood-based techniques, including fully Bayesian methods. Furthermore, this parameterisation covers a range of causal quantities, such as the average causal effect and the effect of treatment on the treated."}, {"title": "Copula Theory", "content": "Copulae present a powerful tool to model joint dependencies independent of the univariate margins. This aligns well with the requirements of the frugal parameterisation, where dependencies need to be varied without altering specified margins (the most critical being the specified causal effect). Understanding the constraints and limitations of copula models ensures that causal models remain accurate and consistent with the intended parameterisation."}, {"title": "Sklar's Theorem", "content": "Sklar's theorem (Sklar, 1959; Czado, 2019) is the fundamental foundation for copula modelling, as it provides a bridge between multivariate joint distributions and their univariate margins. It allows one to separate the marginal behaviour of each variable from their joint dependence structure, with the latter being represented by the copula itself.\n\nFor a d-variate distribution function $F_{1:d} \\in F(F_1, ..., F_d)$, with jth univariate margin Fj, the copula associated with F is a distribution function $C : [0, 1]^d \\rightarrow [0, 1]$ with uniform margins on (0, 1) that satisfies\n\n$F_{1:d}(y) = C(F_1(y_1), ..., F_d(y_d)), y \\in \\mathbb{R}^d$.\n\n If F is a continuous d-variate distribution function with univariate margins $F_1,..., F_d$ and rank functions $F_1^{-1},..., F_d^{-1}$ then\n\n$C(u) = F_{1:d}(F_1^{-1}(u_1), ..., F_d^{-1}(u_d)), u \\in [0, 1]^d$.\n\n If $F_{1:d}$ is a d-variate distribution function of discrete random variables (more generally, partly continuous and partly discrete), then the copula is unique only on the set\n\nRange($F_1$) \u00d7\u30fb\u30fb\u30fb\u00d7 Range($F_d$).\n\nThe copula distribution is associated with its density c(.)\n\nf(y) = c(F1(y1), ..., Fd(yd)) \u00b7 f1(y1) ... fd(yd)\n\nwhere $f_i(\\cdot)$ is the univariate density function of the ith variable.\n\nNote that Sklar's theorem explicitly refers to the univariate marginals of the variable set $\\{Y_1,..., Y_d\\}$ to convert between the joint of univariate margins C(u) and the original distribu-tion F(y). For absolutely continuous random variables, the copula function C is unique. This uniqueness no longer holds for discrete variables, but this does not severely limit the applicability of copulae to simulating from discrete distributions. The non-uniqueness does play a more problematic role in copula inference, however (Genest and Nevslehova, 2007).\n\nAn equivalent definition (from an analytical purview) is $C : [0, 1]^d \\rightarrow [0, 1]$ is a d-dimensional copula if it has the following properties:\n\n1.  C(u1,..., 0, . . ., ud) = 0\n2.  C(1,..., 1, ui, 1, ..., 1) = ui.\n3.  C is d-non-decreasing.\n\nA copula C is d-non-decreasing if, for any hyperrectangle $H = \\prod_{i=1}^d [U_i, V_i] \\subseteq [0, 1]^d$, the C-volume of H is non-negative."}, {"title": "Copulae for Discrete Variables", "content": "Accurately modelling the univariate marginal CDFs of pretreatment covariates is a crucial step in training Frugal Flows, particularly when the dataset includes discrete variables. For continuous covariates, the mapping between observations and ranks is unique, allowing for straightforward"}, {"title": "Empirical Copula Processes for Discrete Variables", "content": "In order to deal with discrete variables, we use a similar approach as taken by Kamthe et al. (2021), who quote the generalised distributional transform of a random variable found originally proposed by R\u00fcschendorf (2009). We quote the main result from R\u00fcschendorf (2009) below.\n\nOn a probability space (\u03a9, \u0391, P) let X be a real random variable with distribution function F and let $V \\sim U(0,1)$ be uniformly distributed on (0,1) and independent of X. The modified distribution function $F(x, A)$ is defined by\n\nF(x, \u03bb) := P(X < x) + \u03bbP(X = x).\n\nWe define the (generalised) distributional transform of X by\n\nU := F(X,V).\n\nAn equivalent representation of the distributional transform is\n\nU = F(X\u2212) + V(F(X) \u2013 F(X\u2212))."}, {"title": "Frugal Flow Implementation Details", "content": "The FF software used for this paper can be found in the GitHub repository https://github.com/llaurabatt/frugal-flows.git.\n\nFF software builds upon FlowJax (Ward, 2024), a Python package implementing normalising flows in JAX (Bradbury et al., 2018). JAX is an open-source numerical computing library that extends NumPy functionality with automatic differentiation and GPU/TPU support, designed for high-performance machine learning"}]}