{"title": "PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation", "authors": ["Ziyan Wang", "Sizhe Wei", "Xiaoming Huo", "Hao Wang"], "abstract": "Diffusion models have made significant advance- ments in recent years. However, their perfor- mance often deteriorates when trained or fine- tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this chal- lenge. Rather than directly minimizing the KL di- vergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribu- tion conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbal- ance problem in diffusion models, improving both generation accuracy and quality.", "sections": [{"title": "1. Introduction", "content": "The development of diffusion models (Ho et al., 2020; Song et al., 2020b) and their subsequent extensions (Song et al., 2020a; Nichol & Dhariwal, 2021; Huang et al., 2023) has significantly advanced the learning of complex probability distributions across various data types, including images (Ho et al., 2022; Rombach et al., 2022; Saharia et al., 2022; Ho & Salimans, 2022), audio (Kong et al., 2020), and 3D biomed- ical imaging data (Luo & Hu, 2021; Poole et al., 2022; Shi et al., 2023; Pinaya et al., 2022). For these generative models, the amount of training data plays a critical role in determining both the accuracy of probability estimation and the model's ability to generalize, which enables effective extrapolation within the probability space.\nData diversity and abundance are key to improving the gen- erative capabilities of large-scale models, enabling them to capture intricate details within a vast probability space. However, many data-driven modeling tasks often rely on small, imbalanced real-world datasets, leading to poor gener- ation quality, particularly for minority groups. For example, when training and fine-tuning a diffusion model with an imbalanced dataset of individuals, existing models often struggle to generate accurate images for those who appear less frequently (i.e., minorities) in the training data (Fig. 1). This challenge is further compounded when accuracy is prioritized over simply high resolution. For example, gener- ated images of individuals need to match the identity of at least one individual in the training set (Fig. 1). Addressing this gap is crucial for deploying diffusion models in real- world applications where correctness is paramount, such as personalized content generation or medical imaging.\nThis limitation is true even for finetuning large diffusion models pretrained on large-scale datasets like LAION- 5B (Schuhmann et al., 2022), e.g., Stable Diffusion (Rom- bach et al., 2022). Imagine an imbalanced dataset consisting of employees in a small company, senior employees might have more photos available, while new employees only have a very limited number of them. Since none of the employees appear in the LAION-5B dataset, generating photos of them require finetuning the Stable Diffusion model. Unfortu- nately, finetuning the model on such an imbalanced dataset might enable the model to generate accurate images for the majority group (i.e., senior employees), but it will perform poorly for the minority group (i.e., new employees).\nTo address this challenge, we propose a general fine-tuning approach, dubbed PoGDiff. Rather than directly minimizing"}, {"title": "2. Related Work", "content": "Long-Tailed Recognition. Addressing the challenges posed by long-tailed data distributions has been a critical area of research in machine learning, for both classification and regression problems. Traditional methods, such as re- sampling and re-weighting techniques, have been used to mitigate class imbalances by either over-sampling minority classes or assigning higher weights to them during train- ing (Chawla et al., 2002; He & Garcia, 2009; Torgo et al., 2013; Branco et al., 2017; 2018). Such algorithms fail to measure the distance in continuous label space and fall short in handling high-dimensional data (e.g., images and text). Deep imbalanced regression methods (Yang et al., 2021; Ren et al., 2022; Gong et al., 2022; Keramati et al., 2023; Wang & Wang, 2024) address this challenge by reweighting the data using the effective label density during representa- tion learning. However, all methods above are designed for recognition tasks such as classification and regression, and are therefore not applicable to our generation task.\nDiffusion Models Related to Long-Tailed Data. There are also works that related to both diffusion models and long- tailed data. They aim at improving generation robustness using noisy label (Na et al., 2024), improving fairness in image generation (Shen et al., 2023), and improving classifi- cation accuracy using diffusion models (Zhang et al., 2024). However, these works have different goals and therefore are not applicable to our setting.\nMost relevant to our work is Class Balancing Diffusion Model (CBDM) (Qin et al., 2023), which uses a distribution adjustment regularizer that enhances tail-class generation"}, {"title": "3. Methods", "content": "3.1. Preliminaries\nDiffusion models (DMs) (Ho et al., 2020) are probabilistic models that generate an output image \\(x_0\\) from a random noise vector \\(x_T\\) conditioned on text input c. DMs operate through two main processes: the forward diffusion process and the reverse denoising process. During the diffusion process, Gaussian noise is progressively added to a data sample \\(x_0\\) over T steps. The forward process is defined as a Markov chain, where:\n\\(q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t I).\\)\nHere, \\(\\beta_t\\) is the predefined diffusion rate at step t. By denot- ing \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{i=1}^{t} a_i\\), we can describe the entire diffusion process as:\n\\(q(x_{1:T}|x_0) = \\prod_{t=1}^{T} q(x_t|x_{t-1})\\)\n\\(q(x_t|x_0) = N(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)\\)\nThe denoising process removes noise from the sample \\(x_T\\), eventually recovering \\(x_0\\). A denoising model \\(\\epsilon_\\theta(x_t, t, y)\\) is trained to estimate the noise \\(\\epsilon\\) from \\(x_t\\) and a text-guided em- bedding \\(y = \\phi(c)\\), where \\(\\phi(\\cdot)\\) is a pretrained text encoder. Formally:\n\\(p_\\theta(x_{t-1}|x_t, t, y) = N(x_{t-1}; \\epsilon_\\theta(x_t, t, y), \\sigma I) .\\)\nThe denoising process is trained by maximizing the likeli- hood of the data under the model or, equivalently, by min- imizing the variational lower bound on the negative log- likelihood of the data. Ho et al. (2020) shows that this is equivalent to minimizing the KL divergence between the predicted distribution \\(p_\\theta(x_{t-1}|x_t, y)\\) and the ground-truth distribution \\(q(x_{t-1}|x_t, x_0, y)\\) at each time step t during the backward process. The training objective then becomes:\n\\(min D_{KL} (q(x_{t-1}|x_t, x_0, y) || p_\\theta(x_{t-1}|x_t, y)).\\)\nThis can be simplified to:\n\\(L_{DM} = E_{x_0=x,\\epsilon~N(0,1),t} [||\\epsilon - \\epsilon_\\theta(x_t, t, y)||^2] .\\)"}, {"title": "3.2. Product-of-Gaussians Diffusion Models (PoGDiff)", "content": "3.2.1. \u039c\u0391IN IDEA\nMethod Overview. Given an image dataset \\(D = \\{(x^{(i)}, c^{(i)})\\}_{i=1}^N\\), where \\(c^{(i)}\\) is the text description for im- age \\(x^{(i)}\\), we use a fixed CLIP encoder to produce \\(c^{(i)}\\)'s corresponding text embedding \\(y = \\phi(c)\\).\nTypical diffusion models minimize the KL divergence between the predicted distribution \\(p_\\theta(x_{t-1}|x_t, y) = N(\\epsilon_\\theta(x_t, t, y), A_t^{-1}I)\\) and the ground-truth distribution \\(q(x_{t-1}|x_t, x_0, y) = N(\\epsilon, A_t^{-1}I)\\) at each time step t during the backward denoising process. Here, \\(A_t\\) and \\(\\lambda_t\\) represent the precision. In contrast, our PoGDiff replaces the ground-truth target with a Product of Gaussians (PoG), and instead minimize the following KL divergence (for each t)\n\\(D_{KL}^{PoGDiff} (q(x_{t-1}|x_t, x_0, y) || p_\\theta(x_{t-1}|x_t, y') \\odot p_\\theta(x_{t-1}|x_t, y)),\\)\nwhere \\(\\odot\\) represents the product of two Gaussian distribu- tions, \\(y'\\) is a selected neighboring embedding from other samples in the training dataset (more details below), and \\(p_\\theta(x_{t-1}|x_t, y')\\) denotes the predicted distribution when us- ing \\(y'\\) as the input text embedding.\nAs shown in Fig. 2, intuitively, PoGDiff's denoising model \\(\\epsilon_\\theta(x_t,t,y)\\) (or \\(p_\\theta(x_{t-1}|x_t, y)\\)) is optimized towards two"}, {"title": "3.2.2. THEORETICAL ANALYSIS AND ALGORITHMIC DESIGN", "content": "Based on Eqn. (1), we then derive a concrete objective function following Proposition 3.1 below.\nProposition 3.1. Assume \\(\\lambda_y = \\lambda_{PoG} = \\lambda_t + \\lambda_{y'}\\), we have our loss function\n\\(L_t^{PoGDiff} = E_q [\\frac{1}{2} \\lambda_t ||\\mu_\\theta(x, y) - \\mu_{PoG}||^2 ] + C.\\)\nHere, C is a constant, and \\(\\mu_{prog}\\) denotes the mean of the PoG, with the expression defined in Eqn. (2). Then, through derivations based on Gaussian properties, we obtain\n\\(L_t^{PoGDiff} < E_q [A(\\lambda_t) || \\epsilon_\\theta(x, y) - \\epsilon||^2 + A(\\lambda_{y'}) ||\\epsilon_\\theta(x, y) - \\epsilon_\\theta(x_t, y')||^2] + C\\)\nwhere the function \\(A(\\lambda) = \\frac{\\lambda_t \\lambda_{y'} (1-\\bar{a}_t)^2}{2a_t (1-\\bar{a}_t)}\\).\nProof. The proof is available in the Appendix A. Eqn. (4) in Propo- sition 3.1 provides a upper bound for the KL divergence (Eqn. (1)) we aim to minimize.\nIn diffusion model literature (Ho et al., 2020; Rombach et al., 2022), one typically sets \\(A(\\lambda_t) = 1\\) to eliminate the dependency on the time step t, and thus Eqn. (4) can be written as\u00b9:\n\\(L_t^{PoGDiff} = E_{x_0~q(x_0),\\epsilon~N(0,1),t~U(1,T)} [||\\epsilon_\\theta(x_1,y) - \\epsilon ||^2\\)\n\\(+\\frac{\\lambda_{y'}}{\\lambda_t + \\lambda_{y'}}||\\epsilon_\\theta(x_1,y) - \\epsilon_\\theta(x_t,y')||^2]\\)\n\u00b9For clarification, our \\(A(\\lambda_t)\\) is equivalent to \\(\\lambda_t\\) in (Ho et al., 2020), with the difference that in our paper, \\(\\lambda\\) refers to the precision of the Gaussian distribution."}, {"title": "3.2.3. COMPUTING THE SIMILARITY \u03c8", "content": "Next, we discuss the choice of \\(\\psi\\) in Eqn. (6). Given a image-text dataset D, the similarities between each image-text pair need to be considered in two parts:\n\\(\\psi \\equiv \\Psi_{img-sim} (x, x') \\cdot \\Psi_{inv-txt-den} (y),\\)\nwhere \\(\\Psi_{img-sim} (x, x')\\) is the similarity between images x and x', and \\(\\Psi_{inv-txt-den}(y)\\) is the probability density of the text embedding y (more details below).\nImage Similarity \\(\\Psi_{img-sim}\\). For all \\(x \\sim D\\), we apply a pre- trained image encoder to obtain the latent representations z. We then calculate the cosine similarities between each z and select the k nearest neighbors with the highest similarity values for all samples in the dataset D, denoted as \\(\\{s_j\\}_{j=1}^k\\), where \\(s_j\\) represents the cosine similarity scores between x and other images in D, sorted in descending order. These values are then normalized to produce the weights for each neighbor:\n\\(w_j = \\frac{s_j}{\\sum_{i=1}^{k} s_i}.\\)\nFor each data pair (x, y), we then randomly sample a neigh- boring pair (x', y') through from a categorical distribution \\(Cat([w_j]_{j=1}^k)\\) (\"Cat\" is short for \u201cCategorical\u201d), i.e., with \\(w_j\\) serving as the probability weight, and compute their image similarity as:\n\\(\\Psi_{img-sim} (x, x') = max (\\{s_j\\}_{j=1}^k , a_1+a_2\\cdot 1[I(x) \\neq I(x')]),\\)"}, {"title": "3.2.4. FINAL OBJECTIVE FUNCTION", "content": "where \\(s \\in \\{s_j\\}_{j=1}^k\\) denotes the cosine similarity sampled according to the weights \\(\\{w_j\\}_{j=1}^k\\) in Eqn. (8), 1[\u00b7] denotes the indicator function, and \\(I(\\cdot)\\) retrieves the class/identity of the current input image; 1[I(x) \u2260 I(x')] = 0 if x and x' are two photos of the same person (e.g., Albert Einstein), and 1[I(x) \u2260 I(x')] = 1 if x and x' are photos of two different persons (e.g., x is Einstein and x' Reagan). \\(a_1, a_2\\) are hyperparameters that control the scale of the similarities. For example, if the cosine similarity (s) between x and x' is 0.4, and \\(a_1 = a_2 = 1\\): if x and x' are of the same person, the image similarity will be \\(0.4^1\\), whereas if x and x' are not of the same person, the image similarity will be \\(0.4^2\\), which"}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDatasets. To demonstrate the effectiveness of PoGDiff in terms of both accuracy and quality, we evaluate our method on two widely used imbalanced datasets, i.e., AgeDB- IT2I (Moschoglou et al., 2017) and DigiFace-IT2I (Bae et al., 2023).\nNote that our method is designed for fine-tuning. Therefore our setup does not require large-scale, long-tailed human datasets. Instead, we sample from these datasets, as long as they meet the following criteria: (1) the dataset must be long-tailed, (2) traditional methods must fail to recognize the minority classes, and (3) there must be a distinguishable difference between the majority and minority classes (e.g., we prefer visual distinctions between the two groups to better highlight the impact of our method). Fig. 4 shows the label density distribution of these datasets, and their level of imbalance (see Appendix G.4 for details on data sparsity).\nAgeDB-IT2I: AgeDB-IT2I is constructed from the AgeDB dataset (Moschoglou et al., 2017). For each image x in AgeDB, we passed it through the pretrained LLaVA-1.6-7b model (Liu et al., 2024) to generate textual captions \u1ef9. Since the identities in AgeDB are well-known individuals that the pretrained SDv1.5 (Rombach et al., 2022) might have encountered during pre-training, we masked the true names and replaced them with generic, random names, leading to"}, {"title": "4.2. Results", "content": "Generation Quality and Accuracy. We report the perfor- mance of different methods in terms of FID score, human evaluation score, GPT-40 score, and DINO score in Table 1,"}, {"title": "5. Conclusions", "content": "In this paper, we propose a general fine-tuning approach called PoGDiff to address the performance drop that oc- curs when fine-tuning on imbalanced datasets. Instead of directly minimizing the KL divergence between the pre- dicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighbor- ing text embedding. Looking ahead, an interesting avenue for future research would be to explore more innovative techniques for re-weighting minority classes (as discussed in Sec. E), particularly within the constraints of: (1) long- tailed generation settings, as opposed to recognition tasks, and (2) natural text prompts rather than one-hot class labels. Exploring PoGDiff for other modalities (e.g., videos and time series) is also an interesting future work."}, {"title": "Impact Statement", "content": "Finetuning under imbalanced datasets in specific domain presents an inescapable challenge in generative AI. For ex- ample, when generating the counterfactual outcomes for users with specific actions, such \"user(or patient)-action- outcome\" pairs are always imbalanced, as it is impossible for any company or any hospital to obtains all the pairs. As such, to save the budget, learning the mapping from \"user(or patient)-action\u201d (sentence description) to \"outcome\" (images) is where this challenge is particularly pronounced. Our proposed method, PoGDiff, represents an innovative and efficient solution to navigate this issue. We argue that the complexity and importance of this problem warrant fur- ther research, given its profound implications across diverse fields. This exploration not only advances our understanding but also opens new avenues for significant impact, under- scoring the need for continued investigation into training generative models under imbalanced datasets."}, {"title": "D. Details for Implementation", "content": "For both baselines and our model, we used the same hyper-parameter settings, specifically\n\u2022 AgeDB-IT2I-L & DigiFace-IT2I-L. The learning rate was set to \\(1 \u00d7 10^{-5}\\), with a maximum of 12, 000 training steps. The effective batch size per GPU was 32, calculated as 8 (Batch Size) \u00d7 4 (Gradient Accumulation Steps).\n\u2022 AgeDB-IT2I-M & AgeDB-IT2I-S. The learning rate was set to \\(1 \u00d7 10^{-5}\\), with a maximum of 6, 000 training steps. The effective batch size per GPU was 8, calculated as 8 (Batch Size) \u00d7 1 (Gradient Accumulation Step)."}, {"title": "E. Limitations", "content": "Datasets. Our method relies heavily on \u201cborrowing\" the statistical strength of neighboring samples from minority classes, making the results sensitive to the size of the minority class. (i.e., in our assumption we require at least 2 for each minority class). In addition, while our AgeDB-IT2I-small and AgeDB-IT2I-medium are actually the sparse dataset, the cardinality remains limited in our experiments. Therefore, how to address IT2I problem under this settings are interesting directions.\nModels. Our method is a general fine-tuning approach designed for datasets that the Stable Diffusion (SD) model has not encountered during pre-training. As shown in Fig. 1, color deviation is very common and is a known issue when one fine-tunes diffusion models (as also mentioned in (Song et al., 2020b)); for example, we can observe similar color deviation in both baselines (e.g., CBDM and Stable Diffusion v1.5) and our PoGDiff. This can be mitigated using the exponential moving average (EMA) technique (Song et al., 2020b); however, this is orthogonal to our method and is outside the scope of our paper. Moreover, as shown in Fig. 6, the baseline Stable Diffusion also suffers from this issue. Besides, exploring PoGDiff's performance when training from scratch is also an interesting direction for future work.\nMethodology. The distance between the current text embedding y and the sampled y' impacts the final generated results, therefore in our paper, we introduced a more sophisticated approach for computing the weight \\(\\psi\\), which depends on the quality of the image pre-trained model and our trained VAE. These mechanisms ensure that data points with smaller distances are assigned higher effective weights. Effectively producing \\(\\psi\\) for any new, arbitrary dataset remains an open question and is an interesting avenue for future work, as it could further enhance the method's performance.\nEvaluation. Our goal is to adapt the pretrained diffusion model to a specific dataset; therefore the evaluation should focus on the target dataset rather than the original dataset used during pre-training. For example, when a user fine-tunes a model on a dataset of employee faces, s/he is not interested in how well the fine-tuned model can generate images of \u201ctables\u201d and \"chairs\". Evaluating the model's performance on the original dataset used during pre-training would be an intriguing direction for future work, but it is orthogonal to our proposed PoGDiff and out of the scope of our paper."}, {"title": "F. Additional Details for AgeDB-IT2I-small in Table 5", "content": "For AgeDB-IT2I-small, there are two IDs, one \"majority\" ID with 30 images and one minority ID with 2 images.\n\u2022 For VANILLA and T2H, the gRecall for the majority ID and the minority ID is 1/30 and 0/2, respectively. Therefore, the average gRecall score is \\(0.5 * 1/30 + 0.5 * 0/2 \\approx 0.0167\\).\n\u2022 For CBDM, the gRecall for the majority ID and the minority ID is 16/30 and 0/2, respectively. Therefore, the average gRecall score is \\(0.5 * 16/30 + 0.5 * 0/2 \\approx 0.2667\\).\n\u2022 For PoGDiff (Ours), the gRecall for the majority ID and the minority ID is 18/30 and 2/2, respectively. Therefore, the average gRecall score is \\(0.5 * 18/30 + 0.5 * 2/2 = 0.8\\)."}, {"title": "G. Discussion", "content": "G.1. Problem Settings\nWe would like to clarify that our paper focuses on a setting different from works like DreamBooth (Ruiz et al., 2023), and our focus is not on diversity, but on finetuning a diffusion model on an imbalanced dataset. Specifically:\n\u2022 Different Setting from Custom Techniques like DreamBooth (Ruiz et al., 2023), CustomDiffusion (Kumari et al., 2023) and PhotoMaker (Li et al., 2024). Previous works like CustomDiffusion and PhotoMaker focus on adjusting the model to generate images with a single object, e.g., a specific dog. In contrast, our PoGDiff focuses finetuning"}]}