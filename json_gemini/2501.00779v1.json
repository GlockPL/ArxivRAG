[{"title": "REM: A Scalable Reinforced Multi-Expert Framework for Multiplex Influence Maximization", "authors": ["Huyen Nguyen", "Hieu Dam", "Nguyen Do", "Cong Tran", "Cuong Pham"], "abstract": "In social online platforms, identifying influential seed users to maximize influence spread is a crucial as it can greatly diminish the cost and efforts required for information dissemination. While effective, traditional methods for Multiplex Influence Maximization (MIM) have reached their performance limits, prompting the emergence of learning-based approaches. These novel methods aim for better generalization and scalability for more sizable graphs but face significant challenges, such as (1) inability to handle unknown diffusion patterns and (2) reliance on high-quality training samples. To address these issues, we propose the Reinforced Expert Maximization framework (REM). REM leverages a Propagation Mixture of Experts technique to encode dynamic propagation of large multiplex networks effectively in order to generate enhanced influence propagation. Noticeably, REM treats a generative model as a policy to autonomously generate different seed sets and learn how to improve them from a Reinforcement Learning perspective. Extensive experiments on several real-world datasets demonstrate that REM surpasses state-of-the-art methods in terms of influence spread, scalability, and inference time in influence maximization tasks.", "sections": [{"title": "Introduction", "content": "Graph data has found a wide range of applications such as social networks and data mining (Lim et al. 2015; Liaghat et al. 2013; Nettleton 2013; Bonchi 2011; Ngo et al. 2024). One popular application is Influence Maximization (IM), which aims to identify a set of individuals that can maximize the spread of influence in a social network under a specific diffusion model. This problem is known to be NP-hard and has been extensively studied in various domains such as viral marketing (Domingos and Richardson 2001; Kempe et al. 2003). With the diversification of social platforms, many users on Online Social Networks (OSNs) like Facebook and Twitter are linking their accounts across multiple platforms. These interconnected OSNs with overlapping users are referred to as Multiplex Networks. The structure of multiplex networks allows users to post information across various OSNs simultaneously, presenting significant value for marketing campaigns (Vikatos et al. 2020; Zhang et al. 2022; Jalili et al. 2017).\n\nThe inner information propagation models on each OSN can vary, leading to differences in how information spreads and influences users across platforms. Consequently, it becomes crucial to customize influence maximization strategies that effectively exert influence over multiple platforms. This is known as Multiplex Influence Maximization (MIM). To date, Combinatorial Optimization (CO) algorithms for MIM (Zhan et al. 2015; Zhang et al. 2016; Kuhnle et al. 2018; Singh et al. 2019; Ling et al. 2023) have limitations compared to learning-based approaches. CO algorithms struggle to generalize to unseen graphs and handle diverse multiplex networks. They also face scalability issues when dealing with large-scale networks. Furthermore, CO algorithms rely on predefined rules or heuristics, limiting their ability to capture complex patterns and non-linear dependencies in multiplex networks. These shortcomings significantly undermine their effectiveness in optimizing the selection of influential seed nodes. In contrast, learning-based approaches (Do et al. 2024; Yuan et al. 2024; Chen et al. 2022; Li et al. 2018) offer advantages in terms of generalization, scalability and capturing complex patterns. However, they still suffer critical weaknesses in MIM as follows:\n1) Inefficient optimization. MIM, being a NP-hard problem with layers potentially scaling to billions, demands efficient training. RL methods, such as those presented in (Manchanda et al. 2020; Chen et al. 2022; Yuan et al. 2024; Do et al. 2024), optimize seed sets in discrete spaces through exploration, iteratively improving solutions without an initial dataset. However, they rely on extensive random sampling, leading to long training time and risks of local optima. Data-driven approaches like (Ling et al. 2023) address these issues by leveraging generative models trained on diverse datasets, though their success is tied to dataset quality. Only with a sufficiently diverse training dataset can the model capture key features for optimization. Developing low-complexity models for efficient optimization remains a major challenge.\n2) Inaccurate propagation estimating models. Accurately measuring propagation value is crucial for evaluating seed set effectiveness. Simulation-based methods (Manchanda et al. 2020; Do et al. 2024; Yuan et al. 2024) rely on running propagation processes to compute spread which is computationally expensive and scales poorly for large graphs. GNN-based approaches (Chen et al. 2022;"}, {"title": "Related Work", "content": "Combinatorial optimization for IM. Influence Maximization is essentially a simplified instance of Multiplex Influence Maximization, constrained to a single network instead of encompassing multiple interconnected ones. While traditional IM has witnessed significant advancements, MIM presents unique challenges due to the complex interplay between these interconnected layers. Early IM approaches relied heavily on simulation-based methods (Leskovec et al. 2007), which involve repeatedly simulating the diffusion process on the network to estimate influence spread. These methods, while intuitive, can be computationally expensive, especially for large networks. Proxy-based methods (Kimura and Saito 2006; Chen et al. 2010b,a) emerged to address scalability issues by approximating influence spread with simpler metrics. Leveraging the submodularity of influence diffusion, approximation algorithms like goyal2011celf++ (Goyal et al. 2011) and UBLF (Zhou et al. 2015) provide efficient seed selection with guaranteed (1 \u2013 1/e)-approximation ratios. Recently, Tiptop (Li et al. 2019) emerged as a game-changer, offering near-exact solutions to IM by achieving a (1 \u2013 \u20ac)-optimal solution for any desired \u20ac > 0. Despite these advancements, MIM necessitates novel approaches due to the added complexity of multiple interconnected networks. While promising approaches utilizing combinatorial approximation algorithms (Zhang et al. 2016) exist, MIM remains an active research area. Future directions include incorporating machine learning and leveraging specific multiplex network characteristics for more efficient and accurate solutions.\nMachine Learning for IM. Learning-based methods, employing deep learning techniques, have emerged to overcome the limitations of traditional IM methods, particularly their lack of generalization ability. Integrating reinforcement learn-"}, {"title": "Problem Formulation", "content": "A multiplex network with I layers is represented by G = {G\u2081 = (V1, E1), G2 = (V2, E2), . . ., G\u2081 = (V\u03b9, \u0395\u03b9)}, where each element consists of a directed graph Gi = (Vi, Ei). If a node exists in more than one layer, then this node is added to set the overlapping users of the multiplex G. Without loss of generality, we consider each layer of the multiplex has a same number of nodes. Therefore, if a node v \u2208 Gi does not belong to Gj (i \u2260 j) we add this node to Gj as an isolated node. Then for each node, interlayer edges are added to connect its adjacent interlayer copies across all the multiplex networks. Finally, we consider the set of all nodes of the multiplex network as V = U=1 Vi. In this study, since we permit different layers of a multiplex to follow distinct models of influence propagation, it is essential to define a mathematical model for the propagation on network G.\nDefinition 1 (Influence Spread). Given a layer Gi = (V, Ei),"}, {"title": null, "content": "we define a seed set S \u2286 V. The function di represents an influence propagation model within Gi, which maps from the power set of V to the non-negative real numbers, di : 2V \u2192 R\u2265 0. The influence spread, defined as the expected number of nodes influenced by the seed set S, is denoted as di(S) and is calculated as follows:\n$\\delta_{i}(S) = \\lim_{m \\rightarrow \\infty} \\frac{1}{m} \\sum_{j=1}^{m} |T_{j}|$ (1)\nwhere Tj represents the final activated sets Tj \u2282 V given a seed set S at the j-th simulation step. The simulation continues until no more nodes are activated or until reaching the maximum number of Monte Carlo steps, m. Increasing m improves the accuracy of estimating influenced nodes. This method is applicable to most propagation models, including Independent Cascade (IC) and Linear Threshold (LT) models (Kempe et al. 2003).\nNext, let us define the overall influence propagation model 8 in the multiplex network G. Firstly, when an overlapping node v is activated in one layer graph Gi, its corresponding interlayer copies in other layers also become activated in a deterministic manner. This phenomenon is known as \"overlapping activation\u201d (Kuhnle et al. 2018; Do et al. 2024), and is visualized in Figure 1. Secondly, when quantifying the expected number of influenced nodes in the entire multiplex network G, we consider the overlapping nodes as a single instance rather than counting them multiple times. This means that when counting the influenced nodes across all layers, we do not add up the duplicates resulting from overlapping activation. Thus, the overall influence d(S) combines the independent influences from each layer while accounting for overlapping activations:\n$\\delta(S) = \\lim_{m \\rightarrow \\infty} \\frac{1}{m} \\sum_{j=1}^{m} \\sum_{i=1}^{l} |T_{ij}(S)|$ (2)\nwhere Tij \u2282 V represents the final activated sets in layer i given a seed set S at the j-th simulation step. We are now ready to define our MIM problem as follows:\nDefinition 2 (Multiplex Influence Maximization (MIM)). Given a multiplex graph G = (G\u2081 = (V, E1), \u03b4\u2081), . . ., (G\u2081 = (V, \u0395\u03b9), \u03b4\u03b9) and a budget b \u2208 N. Specifically, seed set S is represented as a binary vector x \u2208 R1\u00d7|V|, where each element xj corresponds to a node vj in V. Specifically, xj = 1 if vj is included in the seed set, and x = 0 otherwise. Suppose we have a training dataset of seed set indicators pairs (x, y), where x represents a seed set and y = f(x) is the corresponding total number of infected nodes. The MIM problem asks us to find an optimal seed node set x of size at most b to maximize the overall influence spread d(x) calculated in the multiplex. This problem is formulated as follows:\n$\\hat{x} = arg \\max_{x \\le b} \\delta (x)$ (3)\nFor each layer Gi \u2208 G, many greedy based algorithms (Leskovec et al. 2007; Goyal et al. 2011; Tang et al. 2014,"}, {"title": "Our Framework: REM", "content": "The REM model addresses mentioned challenges by following concepts illustrated in Figure 2. First, instead of optimizing the seed set in a complex and discrete space, REM employs our proposed Seed2Vec, a Variational Autoencoder (VAE)-based model (Kingma 2013). VAE is a generative framework that encodes data into a continuous latent space while preserving meaningful structure, enabling the representation of complex seed sets in a less noisy form. This allows for optimization and the generation of new potential solutions within that space. Recognizing that Seed2Vec only captures and generates solutions within the feature distribution of the original training data, our framework treats Seed2Vec as an RL agent. This agent explores and exploits diverse latent representations during each training episode. For each latent sample generated by Seed2Vec, we apply our proposed Propagation Mixture of Experts (PMoE) to predict its propagation with very high accuracy, rank, and store it in a Priority Replay Memory (PRM) (Horgan et al. 2018), a structure designed to prioritize important samples based on their predictive value for enhanced learning efficiency. We then sample the top k samples from PRM and combine them with the original dataset to form a combined dataset. Finally, REM uses this"}, {"title": "Seed2Vec: Learning To Embed Complex Seed Set", "content": "To optimize and identify quality seed sets in a multiplex network", "models": "the Encoder denoted as Ey and the Decoder model denoted as D. Formally", "have": "n$F_{O"}, "E_{\\psi} \\circ D_{\\phi}, x = F_{e} (x) = D_{\\phi} (E_{\\psi} (x)) = D_{\\phi}(z)$, (4)\nwhere x \u2208 [0,1]1\u00d7|V| represents the reconstructed seed set generated.\nSpecifically, to generate x, Fo assumes the existence of a latent random variable z \u2208 R1\u00d7s, where s represents the dimension of the variables in z. This latent variable captures the features of the original seed set and follows a latent distribution p\u2084(z). The complete generative process can be described by the equation:\n$P_{\\phi}(z | x) = \\frac{P(x | z)p(z)}{P(x)}$ (5)\nHowever, computing the exact value of p(x) = \u222b ... \u222b p$(x, z), dz\uff61 . . . dz is intractable, making the equation computationally challenging. To address this problem, Ey will learn qy which is approximated posterior distribution of p(z | x). The goal is to approximate the intractable posterior distribution with a simpler distribution qy (z | x) given the seed set \u00e6. In other words, the objective is to have P\u2084(z | x) \u2248 q\u2084(z | x).\nThis is used to derive the following Evidence Lower Bound (ELBO) to train the model using the reparameterization trick and SGD (Kingma and Welling 2013).\n$\\mathcal{L}_{ELBO} = E_{q_{\\psi}} [log p_{\\phi}(x | z)] - E_{q_{\\psi}} [log q_{\\psi}(z | x)]$\n$= E_{q_{\\psi}} [log p_{\\phi}(x | z)] + E_{q_{\\psi}} [log p_{\\phi}(z)]$\\\n    },\n    {", "title\": \"Propagation Mixture of Expert", "content", "Applying Graph Neural Networks (GNNs) to predict propagation in large-scale multiplex networks with billions of nodes is challenging due to oversmoothing (Cai and Wang 2020). In addition, when using a single GNN with h layers, nodes aggregate information from h-hop neighbors, potentially mixing data from different layers, leading to inaccuracies. To overcome this, we propose the Propagation Mixture of Experts (PMoE). This approach uses multiple GNN models, each with different layer depths, to capture propagation dynamics effectively. Nodes are routed to the most suitable expert based on their characteristics and desired propagation depth, ensuring the model focuses on relevant information and reduces noise. This method allows accurate and efficient propagation prediction in large-scale multiplex networks.\nOur PMOE framework captures the propagation process given a seed set \u00e6 and a multiplex graph G. In this framework, we define a set of C \"expert networks,\" denoted as e1, 2, ..., ec. Each expert ei is implemented as a GNN with varying layer depths, outputting er(x, G, \u00c9i) \u2208 [0,1]1\u00d7|V|, a vector representing the estimated infection probability for each node in G, where \u00a7\u2081 is the parameter of the i-th expert. To effectively leverage the diverse knowledge of experts, we employ a routing network R, which outputs a probability distribution over experts R(x) \u2208 R1\u00d7C based on the input seed set x. Each element in this distribution corresponds to the relevance probability of a particular expert for the given input. Inspired by the noisy top-m routing mechanism proposed by (Shazeer et al. 2017), we select the m most relevant experts for each input. This mechanism operates as follows:\n$Q(x) = x + \\epsilon \\cdot Softplus (\\alpha_{\\xi_{n}})$, (7)\n$R(x) = Softmax (TopM (Q(x), m))$, (8)\nIn this equation, \u0454 ~ N(0, 1) represents standard Gaussian noise. The parameters \u00a7\u2090 and \u00a7n are learnable weights that control the contributions of the clean and noisy scores, respectively. The expected value M(x, G; \u00a7), where \u03be = [\u03be\u03b1, \u03be\u03b7, \u00a71,..., \u03be\u03b5] represents the parameters of the PMOE model M, is calculated based on the outputs of all experts and can be formulated as follows:\n$M(x, G; \\xi) = \\sum_{i=1}^{C} R_{i}(x)e_{i}(x, G; \\xi_{i})$ (9)\nHere, Ri (x) is the i-th element of routing network R(x), representing the relevance probability of the i-th expert in predicting the influence of seed set \u00e6. In this scenario, the total number of infected nodes, denoted as \u0177 \u2208 R+, is calculated as \u0177 = P(x,G;\u00a7) = g(M (x, G; \u00a7) ; \u03da). Here, g(\u00b7) is a normalization function (e.g., l \u2013 1 norm) and ( is the threshold to transform the probability into discrete value.\nLemma 1 (Monotonicity of PMoE Models). Assuming the PMOE model has been trained to convergence and during the inference phase, noisy scores \u00a7n are not considered, for any GNN-based, P is infection monotonic if the aggregation function and combine function in GNN are non-decreasing.\n(Proof in Appendix C1)"]}, {"title": null, "content": "According to Lemma 1 the PMoE model P(x, G; \u00a7) has the theoretical guarantee to retain monotonicity, and the objective of learning the PMoE model P(x, G; \u00a7) is given as maximizing the following probability with a constraint:\n$max_{\\varepsilon} E[p_{e} (y|x, G)]$, (10)\nLatent Seed Set Exploration\nAs a generative model, Seed2Vec can only produce quality seed sets if the original training data is feature-rich. If the data is biased toward dominant features or lacks diversity, Seed2Vec may miss important but less prevalent features. As the multiplex becomes more complex and the number of nodes increases, the model tends to favor dominant seed nodes in the dataset, often overlooking less frequent but potentially significant ones. REM overcomes this by treating Seed2Vec as an RL agent, actively exploring novel and potentially impactful seed sets that maximize propagation to retrain and reinforce itself by the following lemma:\nLemma 2 (Latent Entropy Maximization Equivalence). Assuming the Seed2Vec model has convergened, we have arg maxz H(D(z)) x arg maxx H(x). (Proof in Appendix C2)\nAccording to Lemma 2, exploration within the latent space z, aimed at identifying the novel seed set St, where t = 1,2,3,... represents the training episode, is proportional to exploration within the discrete space x. This correlation emerges because a well-trained Seed2Vec model, using the original collected seed set X0, ensures both continuity-where nearby points in the latent space decode into similar content and completeness, meaning that any point sampled from the latent space's chosen distribution generates 'meaningful' content. At this juncture, p\u2084(z | x) \u2248 qy (z |"}, {"title": null, "content": "x), with qy (z | x) converging to a Gaussian distribution N(\u03bc, \u03c3\u00b2) as indicated by the second term of Equation 6. Typically, an RL agent could explore various latent features by sampling z ~ N(\u03bc, \u03c3\u00b2) and reconstructing the seed set x using the Decoder (i.e., x = D\u2084(z)). However, since qy (z | x) converges to a continuous function that has a derivative with respect to z. Instead of the RL agent exploring by random sampling, we use Gradient Descent directly on z to minimize the following objective function:\n$\\mathcal{L}_{Explore} (z) = E (c\\cdot H(D_{\\psi}(z)) + exp(-P(D_{\\psi}(z)))$ (11)\nwhere c are coefficients. The term H(D\u2084(z)) = \u2212 \u03a31 p(xi) log p(xi) denotes the entropy of the latent variable, which promotes exploration within new regions of the latent space. The function P(D$(z)) refers to the Propagation Mixture of Experts (PMoE), which is detailed in the following section, and is used for predicting the reconstructed seed set x = D(z). To align with the objective of minimizing the loss function, we employ the exponential function, exp(.), to reduce the impact of P(D$(z)) as its value increases. With the novel synthetic seed set St (store by using Priority Replay Memory (Schaul et al. 2015)) obtained by optimizing Equation 11, we sampling top k best samples and combine them with the original dataset Xo to create a Combined Dataset Xt = S(<k) UX0. Therefore, as training episode t progresses, we can use Xt to retrain the Seed2Vec model Fo. This approach allows Fe to generate improved seed sets in future iterations.\nEnd-to-end Learning Objective. Finally, to bridge representation learning, latent seed set exploration, and diffusion model training, we minimize the following end-to-end objective function, which combines Eq. (6), (11), and (10):\n$\\mathcal{L}_{train} = E [\\mathcal{L}_{ELBO}(\\phi) + \\mathcal{L}_{PMOE}(\\xi) + \\mathcal{L}_{Explore} (z)]$ (12)\nwhere LPMOE = (\u0177 \u2013 y)\u00b2.\nSeed Node Set Inference. Finally, our method conclude with inferencing the seed node set from the continuous latent space. Specifically, gradient ascent is employed to find the latent representation \u017e that maximizes the predicted influence spread, based on the estimation provided by the PMoE model. Representation \u017e is decoded using the decoder network of Seed2Vec to obtain the optimal seed node set x.\nTheorem 3 (Influence Estimation Consistency). Given two distinct seed sets x(i) and x(i), with their corresponding latent representations z(i) and z(i) encoded by a Seed2Vec. If the reconstruction error is minimized during the training and P(p\u2084(z(i)), G;\u00a7) > P(p\u2084(z(i)), G; \u00a7), then it follows that P(x(i), G; \u00a7) > P(x(i),G;\u00a7). (Proof in Appendix C3)\nAccording to Theorem 3, the optimal seed set that maximizes influence can be found by optimizing z."}, {"title": "Experiment", "content": "We conduct experiments to compare our proposed REM framework to 6 other state-of-the-art frameworks across 5 real world networks in various settings.\nExperiment Setup\nOur main objective is to evaluate the effect of influence spread across different scenarios in Influence Maximization (IM). Our experiments focus on two dominant propagation models within IM: the Linear Threshold (LT) and Independent Cascade (IC) models. To delve deeper into our experimental setup, we refer to Appendix D.\nDataset. Our experiments leverage multiple multiplex network datasets of diverse interaction types and systems. The Celegans Multiplex GPI Network from BioGRID (Stark et al. 2006) (version 3.2.108) includes genetic interactions within Caenorhabditis elegans, comprising 6 layers, 3,879 nodes, and 8,181 edges. The Arabidopsis Multiplex Network also from BioGRID (Stark et al. 2006) details genetic and protein interactions for Arabidopsis thaliana, comprising 7 layers, 6,980 nodes, and 18,654 edges. For social media dynamics, the NYClimateMarch2014 Twitter Network (Omodei et al. 2015) captures retweets, mentions, and replies during the People's Climate March, featuring 3 layers, 102,439 nodes, and 353,495 edges. The ParisAttack2015 Twitter Network (De Domenico and Altmann 2020) includes similar social interactions during the 2015 Paris Attacks, with 3 layers, 1,896,221 nodes, and 4,163,947 edges. We also use the Cora dataset (McCallum et al. 2000), a citation network of 2,708 scientific publications and 7,981 edges, to analyze influence in academic publishing.\nComparison to other Methods\nWe assess the performance of REM by comparing it against two categories of influence maximization techniques. 1) Traditional methods: ISF (Influential Seed Finder) (Kuhnle et al."}, {"title": null, "content": "2018) is a greedy algorithm designed for multiplex influence maximization; KSN (Knapsack Seeding of Networks) (Kuhnle et al. 2018) utilizes a knapsack approach to find the best seed users in a multiplex network. 2) Deep learning methods: ToupleGDD (Chen et al. 2022), GCOMB (Manchanda et al. 2020), DeepIM (Ling et al. 2023) are state-of-the-art single network influence maximization solutions. For multiplex network, the MIM-Reasoner (Do et al. 2024) method utilize probabilistic graphical models to capture the dynamics within the multiplex, then determine the best seed sets with a reinforcement learning solution. We also evaluate the performance of 2 different REM variants to demonstrate the effectiveness of our approach. One approach is REM-NonRL, which does not employ the exploration of seed sets and solely relies on an initial dataset to provide solution. This variant provides observation on the effectiveness of our proposed reinforcement learning set up. The other variant, REM-NonMixture, forego our Mixture of expert set up, capture the complicated multiplex propagation with one GNN model. This variant will underscore the advantages of our more complex configurations. The comparison is based on three metrics: total influence spread (activated nodes) and inference time (wall-clock time, in seconds).\nQuantitative Analysis\nWe evaluate the performance of the REM method against other IM strategies by comparing their ability to optimize influence across various datasets. In each case, models identify seed nodes representing 1%, 5%, 10%, and 20% of all nodes. We simulate the diffusion process until completion and determine the average influence spread across 100 iterations. We report the final number infected nodes.\nIM under IC Model. The methods are evaluated on five datasets under the IC diffusion model with budgets of 1%, 5%, 10%, and 20% of network nodes. As shown in Table 1, REM consistently outperforms other methods, particularly on large datasets such as NYClimateMarch2014 and ParisAttack2015. Traditional methods (ISF, KSN) perform well on smaller graphs but struggle to scale with larger graphs and higher budgets. Single-graph learning methods (GCOMB, TOUPLEGDD, DEEPIM) fall behind due to their inability to adapt to multiplex networks. While MIM-Reasoner achieves strong results on larger multiplex networks, it is outperformed by REM. Notably, REM variants (REM-NonRL, REM-NonMixture) show significant performance drops, underscoring the importance of REM's key components.\nIM under LT Model. We evaluate the methods under the LT diffusion model, with the results in Table 2 showing that REM consistently outperforms other techniques in maximizing node infections. REM's superiority is particularly evident on large networks and with a 20% seed set, achieving 10% and 15% higher influence spread than the best competing methods on the NYClimateMarch2014 and ParisAttack2015 datasets, respectively. This performance highlights REM's superior generalization across diffusion models.\nIM with explore step number. We compare the effectiveness of increasing exploration steps under the IC and LT models within a budget constraint. As shown in Figure 3, more exploration steps generally improve results across net-"}, {"title": "Scalability Analysis", "content": "We investigate the runtime of seed sets when increasing of graph size of REM verse other learning-based IM solutions. As can be seen in Table 3, REM demonstrates near-linear growth of runtime as the graph size increases. In addition,"}, {"title": null, "content": "it achieves a generally shorter inference time (on average, it has a 10% faster inference time than the second-fastest MIM-Reasoner and a 20% faster inference time than the third-fastest DeepIM."}, {"title": "Conclusion", "content": "This paper has introduced REM, a framework designed to tackle the inherent challenges of MIM. Through the integration of a Propagtaion Mixture of Experts and a RL-based exploration strategy on a continuous latent representation, REM offers a robust solution to optimize influence spread across multiplex networks. Our approach not only demonstrates superior scalability and efficiency but also excels in handling the diversity of propagation mechanisms within these networks. The empirically experimental results on multiple real-world datasets validate REM's effectiveness, showcasing its ability to outperform existing state-of-the-art methods in both influence spread and computational efficiency."}, {"title": "A. Detail Steps of REM", "content": "When solving a MIM problem, REM ultilizes Algorithm 1 for training and Algorithm 2 for inference. Algorithm 1 initializes Seed2Vec and PMoE models, and then iteratively explores the latent space to discover novel seed sets with high propagation potential (lines 6-10). These new sets are combined with the original data to retrain Seed2Vec and PMOE (lines 12-14). Then, Algorithm 2 infers the optimal seed set by optimizing a latent representation using gradient ascent to maximize the predicted influence from the PMoE model (lines 3-5). This representation is then decoded Seed2Vec to output the final solution (line 6). The code and datasets for REM are available at the following GitHub repository: https://github.com/huyenxam/REM."}, {"title": "B. Detail ELBO", "content": "In the Variational Autoencoder (VAE) framework, the primary objective is to maximize the Evidence Lower Bound (ELBO), which serves as a proxy for the log-likelihood of the data. The ELBO comprises two key components: the Reconstruction Loss and the KL Divergence.\n$\\mathcal{L}_{ELBO} = E_{q_{\\psi}} [log p_{\\phi}(x | z)] - E_{q_{\\psi}} [log \\frac{q_{\\psi}(z|x)}{P(z)}]$ (13)\nThe Reconstruction Loss measures the dissimilarity between the original seed set \u00e6 and its reconstruction 2, while the KL Divergence regularizes the latent space distribution qy (z | x) towards a prior distribution pp(z). The process of minimizing Reconstruction Loss is the role of the Decoder model parameterized by 4. Specifically, the Decoder observes z and attempts to generate a reconstruction 2 that is as close as possible to the original data, thus minimizing the Reconstruction Loss. To effectively train the VAE, the Mean Squared Error (MSE) loss is used as reconstruction"}, {"title": null, "content": "loss. The MSE loss directly quantifies the difference between the original input \u00e6 and the reconstructed output 2, making it a straightforward and widely used loss function for this purpose. The MSE loss is given by:\n$MSE Loss = \\frac{1}{N} \\sum_{i=1}^{N} ||x_{i} - x_{i}||^2$ (14)\nMinimizing the MSE loss corresponds to maximizing the likelihood term log p(x | z) within the ELBO. This is because a smaller MSE indicates that the reconstructed output 2 is closer to the original input \u00e6, implying a higher probability of the data under the model.\nFinally, the ELBO can be expressed as:\n$\\mathcal{L}_{ELBO} (x; \\psi, \\phi) = \\frac{1}{N} \\sum_{i=1}^{N} MSE(x_{i}, \\hat{x_{i}}) - KL (q_{\\psi} (z | x) ||P_{\\phi}(z))$ (15)\nHere, the MSE loss is directly incorporated as the reconstruction term in the ELBO, guiding the optimization process to improve the VAE's ability to reconstruct the input data accurately. Meanwhile, the KL Divergence term ensures that the latent space is regularized towards the prior distribution, maintaining a balance between reconstruction quality and latent space regularization."}, {"title": "C. Proofs", "content": "C1. Monotonicity of PMoE Models\nLemma 1 (Monotonicity of PMoE Models) Assuming the PMOE model has been trained to convergence and during the inference phase, noisy scores \u00a7n are not considered, for any GNN-based, P is infection monotonic if the aggregation function and combine function in GNN are non-decreasing.\nProof. Assuming we have a Graph Neural Network (GNN) with H layers, where Ah and Ch are non-decreasing, denoted as e(.). The input is a vector \u00e6, and we apply the GNN to x over H layers as follows:\n1. Input Definition: Initially, consider the input ro) to be x for every node v in the graph, meaning all nodes start with the initial feature vector x.\n2. Iterating Through Layers: For each layer h from 1 to H, the aggregation function is applied to each node v as follows:\n$e(x) = A^{H} \\circ (C^{H} \\circ A^{1} \\circ C^{1} ... \\circ A^{H} \\circ C^{H})$ (16)\nBecause Ah and Ch are non-decreasing, so is A\u00b9\u00b0C1.... AHCH, which is e(x). Therefore, we have that e(x) is a non-decreasing function.\nNow, we will prove that propagation synthesized from a Mixture of Experts model is also a non-decreasing function, provided that the model has converged and there is no noise in expert selection. Recall the fact that the propagation M(x, G; \u00a7) given any seed set \u00e6 can be calculated by Eq. 10. In our setting where we only consider non-noisy"}, {"title": null, "content": "experts, therefore M(x, G; \u00a7) is independent of En. We can reformulate the output of our model as:\n$Q (x, \\xi_{\\alpha}, \\xi_{n}) = x \\xi_{\\alpha} + \\epsilon \\cdot Softplus (\\alpha_{\\xi_{n}})$\n$= x \\xi_{\\alpha} + 0. Softplus (\\alpha_{\\xi_{n}})$ (17)\n$= x \\xi_{\\alpha}$\n$\\Rightarrow R(x, \\xi_{\\alpha}, \\xi_{n}) = R(x,\\xi_{\\alpha})$\n$= Softmax (TopM (Q (x, \\xi_{g}), m))$ (18)\nTherefore, we have:\n$\\Rightarrow M(x, G; \\xi) = \\sum_{i=1}^{C} R(x,\\xi_{g})e_{i}(i)$ (19)\nBecause R() is the softmax operator, which is non-decreasing, and e(\u00b7) is also a non-decreasing function, it follows that the PMoE model M(x, G; \u00a7) is monotonic. Consequently, since the function g(\u00b7) is non-decreasing as well, the influence propagation function P(x, G;\u00a7) is likewise monotonic."}]