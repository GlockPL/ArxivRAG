{"title": "REM: A Scalable Reinforced Multi-Expert Framework for Multiplex Influence Maximization", "authors": ["Huyen Nguyen", "Hieu Dam", "Nguyen Do", "Cong Tran", "Cuong Pham"], "abstract": "In social online platforms, identifying influential seed users to maximize influence spread is a crucial as it can greatly diminish the cost and efforts required for information dissemination. While effective, traditional methods for Multiplex Influence Maximization (MIM) have reached their performance limits, prompting the emergence of learning-based approaches. These novel methods aim for better generalization and scalability for more sizable graphs but face significant challenges, such as (1) inability to handle unknown diffusion patterns and (2) reliance on high-quality training samples. To address these issues, we propose the Reinforced Expert Maximization framework (REM). REM leverages a Propagation Mixture of Experts technique to encode dynamic propagation of large multiplex networks effectively in order to generate enhanced influence propagation. Noticeably, REM treats a generative model as a policy to autonomously generate different seed sets and learn how to improve them from a Reinforcement Learning perspective. Extensive experiments on several real-world datasets demonstrate that REM surpasses state-of-the-art methods in terms of influence spread, scalability, and inference time in influence maximization tasks.", "sections": [{"title": "Introduction", "content": "Graph data has found a wide range of applications such as social networks and data mining (Lim et al. 2015; Liaghat et al. 2013; Nettleton 2013; Bonchi 2011; Ngo et al. 2024). One popular application is Influence Maximization (IM), which aims to identify a set of individuals that can maximize the spread of influence in a social network under a specific diffusion model. This problem is known to be NP-hard and has been extensively studied in various domains such as viral marketing (Domingos and Richardson 2001; Kempe et al. 2003). With the diversification of social platforms, many users on Online Social Networks (OSNs) like Facebook and Twitter are linking their accounts across multiple platforms. These interconnected OSNs with overlapping users are referred to as Multiplex Networks. The structure of multiplex networks allows users to post information across various OSNs simultaneously, presenting significant value for marketing campaigns (Vikatos et al. 2020; Zhang et al. 2022; Jalili et al. 2017).\nThe inner information propagation models on each OSN can vary, leading to differences in how information spreads and influences users across platforms. Consequently, it becomes crucial to customize influence maximization strategies that effectively exert influence over multiple platforms. This is known as Multiplex Influence Maximization (MIM). To date, Combinatorial Optimization (CO) algorithms for MIM (Zhan et al. 2015; Zhang et al. 2016; Kuhnle et al. 2018; Singh et al. 2019; Ling et al. 2023) have limitations compared to learning-based approaches. CO algorithms struggle to generalize to unseen graphs and handle diverse multiplex networks. They also face scalability issues when dealing with large-scale networks. Furthermore, CO algorithms rely on predefined rules or heuristics, limiting their ability to capture complex patterns and non-linear dependencies in multiplex networks. These shortcomings significantly undermine their effectiveness in optimizing the selection of influential seed nodes. In contrast, learning-based approaches (Do et al. 2024; Yuan et al. 2024; Chen et al. 2022; Li et al. 2018) offer advantages in terms of generalization, scalability and capturing complex patterns. However, they still suffer critical weaknesses in MIM as follows:\n1) Inefficient optimization. MIM, being a NP-hard problem with layers potentially scaling to billions, demands efficient training. RL methods, such as those presented in (Manchanda et al. 2020; Chen et al. 2022; Yuan et al. 2024; Do et al. 2024), optimize seed sets in discrete spaces through exploration, iteratively improving solutions without an initial dataset. However, they rely on extensive random sampling, leading to long training time and risks of local optima. Data-driven approaches like (Ling et al. 2023) address these issues by leveraging generative models trained on diverse datasets, though their success is tied to dataset quality. Only with a sufficiently diverse training dataset can the model capture key features for optimization. Developing low-complexity models for efficient optimization remains a major challenge.\n2) Inaccurate propagation estimating models. Accurately measuring propagation value is crucial for evaluating seed set effectiveness. Simulation-based methods (Manchanda et al. 2020; Do et al. 2024; Yuan et al. 2024) rely on running propagation processes to compute spread which is computationally expensive and scales poorly for large graphs. GNN-based approaches (Chen et al. 2022;\nLing et al. 2023) predict spread more efficiently but face accuracy issues due to oversmoothing (Cai and Wang 2020). This challenge is exacerbated in multiplex networks, where each layer may use a different propagation model and scale to billions of nodes, complicating accurate predictions.\nOur Contributions. We propose Reinforced Expert Maximization (REM), a novel framework for tackling challenges in Multiplex Influence Maximization (MIM). First, we introduce Seed2Vec, a VAE-based model that maps the discrete, noisy input space into a cleaner, continuous latent space following a Gaussian distribution. This allows us to optimize the seed set within this latent space. To address Seed2Vec's reliance on training data quality, we frame it as a Reinforcement Learning (RL) policy, enabling efficient exploration of latent regions to generate novel seed sets with significant spread in multiplex networks. These samples are then used to iteratively retrain Seed2Vec, improving its performance. Finally, REM enhances spread estimation with the Propagation Mixture of Experts (PMoE), a method that employs multiple Graph Neural Network (GNN) models as experts to capture complex diffusion patterns. Experiments on real-world datasets show that REM outperforms state-of-the-art methods in influence spread, scalability, and inference time."}, {"title": "Related Work", "content": "Combinatorial optimization for IM. Influence Maximization is essentially a simplified instance of Multiplex Influence Maximization, constrained to a single network instead of encompassing multiple interconnected ones. While traditional IM has witnessed significant advancements, MIM presents unique challenges due to the complex interplay between these interconnected layers. Early IM approaches relied heavily on simulation-based methods (Leskovec et al. 2007), which involve repeatedly simulating the diffusion process on the network to estimate influence spread. These methods, while intuitive, can be computationally expensive, especially for large networks. Proxy-based methods (Kimura and Saito 2006; Chen et al. 2010b,a) emerged to address scalability issues by approximating influence spread with simpler metrics. Leveraging the submodularity of influence diffusion, approximation algorithms like goyal2011celf++ (Goyal et al. 2011) and UBLF (Zhou et al. 2015) provide efficient seed selection with guaranteed (1 \u2013 1/e)-approximation ratios. Recently, Tiptop (Li et al. 2019) emerged as a game-changer, offering near-exact solutions to IM by achieving a $(1 - \\epsilon)$-optimal solution for any desired $\\epsilon > 0$. Despite these advancements, MIM necessitates novel approaches due to the added complexity of multiple interconnected networks. While promising approaches utilizing combinatorial approximation algorithms (Zhang et al. 2016) exist, MIM remains an active research area. Future directions include incorporating machine learning and leveraging specific multiplex network characteristics for more efficient and accurate solutions.\nMachine Learning for IM. Learning-based methods, employing deep learning techniques, have emerged to overcome the limitations of traditional IM methods, particularly their lack of generalization ability. Integrating reinforcement learn-"}, {"title": "Problem Formulation", "content": "A multiplex network with $I$ layers is represented by $G = \\{G_1 = (V_1, E_1), G_2 = (V_2, E_2), ..., G_I = (V_I, E_I)\\}$, where each element consists of a directed graph $G_i = (V_i, E_i)$. If a node exists in more than one layer, then this node is added to set the overlapping users of the multiplex $G$. Without loss of generality, we consider each layer of the multiplex has a same number of nodes. Therefore, if a node $v \\in G_i$ does not belong to $G_j (i \\neq j)$ we add this node to $G_j$ as an isolated node. Then for each node, interlayer edges are added to connect its adjacent interlayer copies across all the multiplex networks. Finally, we consider the set of all nodes of the multiplex network as $V = \\bigcup_{i=1}^{I} V_i$. In this study, since we permit different layers of a multiplex to follow distinct models of influence propagation, it is essential to define a mathematical model for the propagation on network $G$.\nDefinition 1 (Influence Spread). Given a layer $G_i = (V, E_i)$,"}, {"title": "Our Framework: REM", "content": "The REM model addresses mentioned challenges by following concepts illustrated in Figure 2. First, instead of optimizing the seed set in a complex and discrete space, REM employs our proposed Seed2Vec, a Variational Autoencoder (VAE)-based model (Kingma 2013). VAE is a generative framework that encodes data into a continuous latent space while preserving meaningful structure, enabling the representation of complex seed sets in a less noisy form. This allows for optimization and the generation of new potential solutions within that space. Recognizing that Seed2Vec only captures and generates solutions within the feature distribution of the original training data, our framework treats Seed2Vec as an RL agent. This agent explores and exploits diverse latent representations during each training episode. For each latent sample generated by Seed2Vec, we apply our proposed Propagation Mixture of Experts (PMoE) to predict its propagation with very high accuracy, rank, and store it in a Priority Replay Memory (PRM) (Horgan et al. 2018), a structure designed to prioritize important samples based on their predictive value for enhanced learning efficiency. We then sample the top k samples from PRM and combine them with the original dataset to form a combined dataset. Finally, REM uses this"}, {"title": "Seed2Vec: Learning To Embed Complex Seed Set", "content": "To optimize and identify quality seed sets in a multiplex network, we propose characterizing the probability of a seed node set, denoted as $p(x)$, given the multiplex graph G. Learning $p(x)$ can provide insights into the underlying nature of the seed set, facilitating effective exploration of seed sets in the search space. However, learning such a probability is challenging due to the interconnections between different nodes within each seed set and their high correlation based on the network topology of G. These complex connections make the node relationships within seed sets difficult to decipher compared to other similar combinatorial problems. Therefore, instead of learning directly the complex representation of $x$, we learn a latent presentation $z$ using Variational Auto Encoder (VAE) (Kingma and Welling 2013) denoted as $F_{\\theta}$. For convenient, we further decompose the VAE model $F_{\\theta}$ into two models: the Encoder denoted as $E_{\\psi}$ and the Decoder model denoted as $D_{\\phi}$. Formally, we have:\n$F_{\\theta} = E_{\\psi} \\circ D_{\\phi}, x = F_{\\theta} (x) = D_{\\phi} (E_{\\psi} (x)) = D_{\\phi}(z),$\nwhere $x \\in [0,1]^{1\\times|V|}$ represents the reconstructed seed set generated.\nSpecifically, to generate $x$, $F_{\\theta}$ assumes the existence of a latent random variable $z \\in \\mathbb{R}^{1\\times s}$, where $s$ represents the dimension of the variables in $z$. This latent variable captures the features of the original seed set and follows a latent distribution $p_{\\phi}(z)$. The complete generative process can be described by the equation:\n$p_{\\theta}(z | x) = \\frac{P(x | z)p(z)}{P(x)}$\nHowever, computing the exact value of $p(x) = \\int ... \\int p_{\\phi}(x, z), dz_{1} . . . dz_{s}$ is intractable, making the equation computationally challenging. To address this problem, $E_{\\psi}$ will learn $q_{\\psi}$ which is approximated posterior distribution of $p(z | x)$. The goal is to approximate the intractable posterior distribution with a simpler distribution $q_{\\psi}(z | x)$ given the seed set $x$. In other words, the objective is to have $p_{\\phi}(z | x) \\approx q_{\\psi}(z | x)$.\nThis is used to derive the following Evidence Lower Bound (ELBO) to train the model using the reparameterization trick and SGD (Kingma and Welling 2013).\n$\\mathcal{L}_{ELBO} = E_{q_{\\psi}} [log p_{\\theta}(z, x)] - E_{q_{\\psi}} [log q_{\\psi} (z | x)]$\n$= E_{q_{\\psi}} [log p_{\\theta} (x | z)] + E_{q_{\\psi}} [log p_{\\phi}(z)] - E_{q_{\\psi}} [log q_{\\psi} (z | x)]$\n$= E_{q_{\\psi}} [log p_{\\theta} (x | z)] - E_{q_{\\psi}} [log \\frac{q_{\\psi}(z|x)}{p_{\\theta}(z)}]$"}, {"title": "Propagation Mixture of Expert", "content": "Applying Graph Neural Networks (GNNs) to predict propagation in large-scale multiplex networks with billions of nodes is challenging due to oversmoothing (Cai and Wang 2020). In addition, when using a single GNN with h layers, nodes aggregate information from h-hop neighbors, potentially mixing data from different layers, leading to inaccuracies. To overcome this, we propose the Propagation Mixture of Experts (PMoE). This approach uses multiple GNN models, each with different layer depths, to capture propagation dynamics effectively. Nodes are routed to the most suitable expert based on their characteristics and desired propagation depth, ensuring the model focuses on relevant information and reduces noise. This method allows accurate and efficient propagation prediction in large-scale multiplex networks.\nOur PMOE framework captures the propagation process given a seed set $x$ and a multiplex graph G. In this framework, we define a set of C \"expert networks,\" denoted as $e_1, e_2, ..., e_c$. Each expert $e_i$ is implemented as a GNN with varying layer depths, outputting $e_i(x, G, \\Theta_i) \\in [0,1]^{1\\times|V|}$, a vector representing the estimated infection probability for each node in G, where $\\Theta_i$ is the parameter of the i-th expert. To effectively leverage the diverse knowledge of experts, we employ a routing network R, which outputs a probability distribution over experts $R(x) \\in \\mathbb{R}^{1\\times C}$ based on the input seed set x. Each element in this distribution corresponds to the relevance probability of a particular expert for the given input. Inspired by the noisy top-m routing mechanism proposed by (Shazeer et al. 2017), we select the m most relevant experts for each input. This mechanism operates as follows:\n$Q(x) = x + \\epsilon \\cdot Softplus (\\xi_{\\alpha} \\xi_{\\eta}),$\n$R(x) = Softmax (TopM (Q(x), m)),$\nIn this equation, $\\epsilon \\sim \\mathcal{N}(0, 1)$ represents standard Gaussian noise. The parameters $\\xi_{\\alpha}$ and $\\xi_{\\eta}$ are learnable weights that control the contributions of the clean and noisy scores, respectively. The expected value $M(x, G; \\xi)$, where $\\xi = [\\xi_{\\alpha}, \\xi_{\\eta}, \\xi_1,..., \\xi_c]$ represents the parameters of the PMOE model M, is calculated based on the outputs of all experts and can be formulated as follows:\n$M(x, G; \\xi) = \\sum_{i=1}^{C} R_i(x)e_i(x, G; \\Theta_i)$\nHere, $R_i(x)$ is the i-th element of routing network R(x), representing the relevance probability of the i-th expert in predicting the influence of seed set x. In this scenario, the total number of infected nodes, denoted as $\\hat{y} \\in \\mathbb{R}_{+}$, is calculated as $\\hat{y} = P(x,G;\\xi) = g(M (x, G; \\xi) ; \\zeta)$. Here, g(\u00b7) is a normalization function (e.g., l \u2013 1 norm) and $\\zeta$ is the threshold to transform the probability into discrete value."}, {"title": "Latent Seed Set Exploration", "content": "As a generative model, Seed2Vec can only produce quality seed sets if the original training data is feature-rich. If the data is biased toward dominant features or lacks diversity, Seed2Vec may miss important but less prevalent features. As the multiplex becomes more complex and the number of nodes increases, the model tends to favor dominant seed nodes in the dataset, often overlooking less frequent but potentially significant ones. REM overcomes this by treating Seed2Vec as an RL agent, actively exploring novel and potentially impactful seed sets that maximize propagation to retrain and reinforce itself by the following lemma:\nLemma 2 (Latent Entropy Maximization Equivalence). Assuming the Seed2Vec model has convergened, we have $arg \\underset{z}{max} H(D(z)) \\propto arg \\underset{x}{max} H(x)$.\nAccording to Lemma 2, exploration within the latent space z, aimed at identifying the novel seed set $S_t$, where $t = 1,2,3,...$ represents the training episode, is proportional to exploration within the discrete space x. This correlation emerges because a well-trained Seed2Vec model, using the original collected seed set $X_0$, ensures both continuity\u2014where nearby points in the latent space decode into similar content\u2014and completeness, meaning that any point sampled from the latent space\u2019s chosen distribution generates 'meaningful' content. At this juncture, $p_{\\phi}(z | x) \\approx q_{\\psi} (z | x)$, with $q_{\\psi} (z | x)$ converging to a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ as indicated by the second term of Equation 6. Typically, an RL agent could explore various latent features by sampling $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and reconstructing the seed set x using the Decoder (i.e., $x = D_{\\phi}(z)$). However, since $q_{\\psi} (z | x)$ converges to a continuous function that has a derivative with respect to z. Instead of the RL agent exploring by random sampling, we use Gradient Descent directly on z to minimize the following objective function:\n$L_{Explore} (z) = E (c \\cdot H(D_{\\phi}(z)) + exp(-P(D_{\\phi}(z))))$\nwhere c are coefficients. The term $H(D_{\\phi}(z)) = - \\sum_{i=1}^{n} P(x_i) log p(x_i)$ denotes the entropy of the latent variable, which promotes exploration within new regions of the latent space. The function $P(D_{\\phi}(z))$ refers to the Propagation Mixture of Experts (PMoE), which is detailed in the following section, and is used for predicting the reconstructed seed set $x = D(z)$. To align with the objective of minimizing the loss function, we employ the exponential function, exp(\u00b7), to reduce the impact of $P(D_{\\phi}(z))$ as its value increases. With the novel synthetic seed set $S_t$ (store by using Priority Replay Memory (Schaul et al. 2015)) obtained by optimizing Equation 11, we sampling top k best samples and combine them with the original dataset $X_0$ to create a Combined Dataset $X_t = S_{(<k)} \\cup X_0$. Therefore, as training episode t progresses, we can use $X_t$ to retrain the Seed2Vec model $F_{\\theta}$. This approach allows $F_{\\theta}$ to generate improved seed sets in future iterations."}, {"title": "End-to-end Learning Objective", "content": "Finally, to bridge representation learning, latent seed set exploration, and diffusion model training, we minimize the following end-to-end objective function, which combines Eq. (6), (11), and (10):\n$\\mathcal{L}_{train} = E [\\mathcal{L}_{ELBO}(\\phi) + \\mathcal{L}_{PMoE}(\\xi) + L_{Explore} (z)]$\nwhere $\\mathcal{L}_{PMoE} = (\\hat{y} - y)^2$.\nSeed Node Set Inference. Finally, our method conclude with inferencing the seed node set from the continuous latent space. Specifically, gradient ascent is employed to find the latent representation $z$ that maximizes the predicted influence spread, based on the estimation provided by the PMoE model. Representation $z$ is decoded using the decoder network of Seed2Vec to obtain the optimal seed node set x."}, {"title": "Theorem 3 (Influence Estimation Consistency)", "content": "Given two distinct seed sets $x^{(i)}$ and $x^{(j)}$, with their corresponding latent representations $z^{(i)}$ and $z^{(j)}$ encoded by a Seed2Vec. If the reconstruction error is minimized during the training and $P(p_{\\phi}(z^{(i)}), G;\\xi) > P(p_{\\phi}(z^{(j)}), G; \\xi)$, then it follows that $P(x^{(i)}, G; \\xi) > P(x^{(j)}, G; \\xi)$.\nAccording to Theorem 3, the optimal seed set that maximizes influence can be found by optimizing z."}, {"title": "Experiment", "content": "We conduct experiments to compare our proposed REM framework to 6 other state-of-the-art frameworks across 5 real world networks in various settings.\nExperiment Setup\nOur main objective is to evaluate the effect of influence spread across different scenarios in Influence Maximization (IM). Our experiments focus on two dominant propagation models within IM: the Linear Threshold (LT) and Independent Cascade (IC) models. To delve deeper into our experimental setup, we refer to Appendix D.\nDataset. Our experiments leverage multiple multiplex network datasets of diverse interaction types and systems. The Celegans Multiplex GPI Network from BioGRID (Stark et al. 2006) (version 3.2.108) includes genetic interactions within Caenorhabditis elegans, comprising 6 layers, 3,879 nodes, and 8,181 edges. The Arabidopsis Multiplex Network also from BioGRID (Stark et al. 2006) details genetic and protein interactions for Arabidopsis thaliana, comprising 7 layers, 6,980 nodes, and 18,654 edges. For social media dynamics, the NYClimateMarch2014 Twitter Network (Omodei et al. 2015) captures retweets, mentions, and replies during the People's Climate March, featuring 3 layers, 102,439 nodes, and 353,495 edges. The ParisAttack2015 Twitter Network (De Domenico and Altmann 2020) includes similar social interactions during the 2015 Paris Attacks, with 3 layers, 1,896,221 nodes, and 4,163,947 edges. We also use the Cora dataset (McCallum et al. 2000), a citation network of 2,708 scientific publications and 7,981 edges, to analyze influence in academic publishing.\nComparison to other Methods\nWe assess the performance of REM by comparing it against two categories of influence maximization techniques. 1) Traditional methods: ISF (Influential Seed Finder) (Kuhnle et al."}, {"title": "Quantitative Analysis", "content": "We evaluate the performance of the REM method against other IM strategies by comparing their ability to optimize influence across various datasets. In each case, models identify seed nodes representing 1%, 5%, 10%, and 20% of all nodes. We simulate the diffusion process until completion and determine the average influence spread across 100 iterations. We report the final number infected nodes.\nIM under IC Model. The methods are evaluated on five datasets under the IC diffusion model with budgets of 1%, 5%, 10%, and 20% of network nodes. As shown in Table 1, REM consistently outperforms other methods, particularly on large datasets such as NYClimateMarch2014 and ParisAttack2015. Traditional methods (ISF, KSN) perform well on smaller graphs but struggle to scale with larger graphs and higher budgets. Single-graph learning methods (GCOMB, TOUPLEGDD, DEEPIM) fall behind due to their inability to adapt to multiplex networks. While MIM-Reasoner achieves strong results on larger multiplex networks, it is outperformed by REM. Notably, REM variants (REM-NonRL, REM-NonMixture) show significant performance drops, underscoring the importance of REM's key components.\nIM under LT Model. We evaluate the methods under the LT diffusion model, with the results in Table 2 showing that REM consistently outperforms other techniques in maximizing node infections. REM's superiority is particularly evident on large networks and with a 20% seed set, achieving 10% and 15% higher influence spread than the best competing methods on the NYClimateMarch2014 and ParisAttack2015 datasets, respectively. This performance highlights REM's superior generalization across diffusion models."}, {"title": "Conclusion", "content": "This paper has introduced REM, a framework designed to tackle the inherent challenges of MIM. Through the integration of a Propagtaion Mixture of Experts and a RL-based exploration strategy on a continuous latent representation, REM offers a robust solution to optimize influence spread across multiplex networks. Our approach not only demonstrates superior scalability and efficiency but also excels in handling the diversity of propagation mechanisms within these networks. The empirically experimental results on multiple real-world datasets validate REM's effectiveness, showcasing its ability to outperform existing state-of-the-art methods in both influence spread and computational efficiency."}, {"title": "A. Detail Steps of REM", "content": "When solving a MIM problem, REM ultilizes Algorithm 1 for training and Algorithm 2 for inference. Algorithm 1 initializes Seed2Vec and PMoE models, and then iteratively explores the latent space to discover novel seed sets with high propagation potential (lines 6-10). These new sets are combined with the original data to retrain Seed2Vec and PMoE (lines 12-14). Then, Algorithm 2 infers the optimal seed set by optimizing a latent representation using gradient ascent to maximize the predicted influence from the PMoE model (lines 3-5). This representation is then decoded Seed2Vec to output the final solution (line 6). The code and datasets for REM are available at the following GitHub repository: https://github.com/huyenxam/REM."}, {"title": "B. Detail ELBO", "content": "In the Variational Autoencoder (VAE) framework, the primary objective is to maximize the Evidence Lower Bound (ELBO), which serves as a proxy for the log-likelihood of the data. The ELBO comprises two key components: the Reconstruction Loss and the KL Divergence.\n$\\mathcal{L}_{ELBO} = E_{q_{\\psi}} [log p(x | z)] - E_{q_{\\psi}} [log \\frac{q_{\\psi}(z|x)}{p_{\\theta}(z)}]$\nThe Reconstruction Loss measures the dissimilarity between the original seed set $x$ and its reconstruction $\\hat{x}$, while the KL Divergence regularizes the latent space distribution $q_{\\psi} (z | x)$ towards a prior distribution $p_{\\theta}(z)$. The process of minimizing Reconstruction Loss is the role of the Decoder model parameterized by $\\phi$. Specifically, the Decoder observes $z$ and attempts to generate a reconstruction $\\hat{x}$ that is as close as possible to the original data, thus minimizing the Reconstruction Loss. To effectively train the VAE, the Mean Squared Error (MSE) loss is used as reconstruction"}, {"title": "C1. Monotonicity of PMoE Models", "content": "Lemma 1 (Monotonicity of PMoE Models) Assuming the PMOE model has been trained to convergence and during the inference phase, noisy scores $\\xi_{\\eta}$ are not considered, for any GNN-based, P is infection monotonic if the aggregation function and combine function in GNN are non-decreasing.\nProof. Assuming we have a Graph Neural Network (GNN) with H layers, where $A^h$ and $C^h$ are non-decreasing, denoted as $e(.)$. The input is a vector $x$, and we apply the GNN to x over H layers as follows:\n1. Input Definition: Initially, consider the input $r^{(0)}$ to be $x$ for every node v in the graph, meaning all nodes start with the initial feature vector x.\n2. Iterating Through Layers: For each layer h from 1 to H, the aggregation function is applied to each node v as follows:\n$e(x) = A^{(H)} \\circ (C^{(H)} \\circ A^{(H-1)} \\circ C^{(H-1)} ... \\circ A^{(1)} \\circ C^{(1)})$\nBecause $A^h$ and $C^h$ are non-decreasing, so is $A^{(1)} \\circ C^{(1)}... A^{(H)}C^{(H)}$, which is $e(x)$. Therefore, we have that $e(x)$ is a non-decreasing function.\nNow, we will prove that propagation synthesized from a Mixture of Experts model is also a non-decreasing function, provided that the model has converged and there is no noise in expert selection. Recall the fact that the propagation $M(x, G; \\xi)$ given any seed set x can be calculated by Eq. 10. In our setting where we only consider non-noisy"}, {"title": "C2. Latent Entropy Maximization Equivalence", "content": "Lemma 2 (Latent Entropy Maximization Equivalence) Assuming the Seed2Vec model has convergened, we have $arg \\underset{z}{max} H(D(z)) \\propto arg \\underset{x}{max} H(x)$.\nProof. The entropy of a random variable x is given by:\n$H(x) = - \\sum_{i=1}^{n} p (x_i) log p (x_i)$\nSimilarly, the entropy of the latent variable $D_{\\phi}(z)$ is:\n$H (D_{\\phi}(z)) = - \\sum_{i=1}^{n} p (x_i) log p (x_i)$\nwhere $x = D_{\\phi}(z)$ represents the data reconstructed from the latent variable z.\nWhen $F_{\\theta}$ has converged, the original data x and the reconstructed data $\\hat{x}$ are nearly identical, i.e., $x \\approx \\hat{x}$. Since x and $\\hat{x}$ are nearly the same, their entropies are also nearly the same $H(x) \\approx H (D_{\\phi}(z))$. Therefore, maximizing the entropy of the latent variable z is equivalent to maximizing the entropy of the original data x:\n$arg \\underset{z}{max} H (D(z)) \\propto arg \\underset{x}{max} H(x)$"}, {"title": "C3. Influence Estimation Consistency", "content": "Theorem 3 (Influence Estimation Consistency) Given two distinct seed sets $x^{(i)}$ and $x^{(j)}$, with their corresponding latent representations $z^{(i)}$ and $z^{(j)}$ encoded by a Seed2Vec. If the reconstruction error is minimized during the training and $P(p_{\\phi}(z^{(i)}), G;\\xi) > P(p_{\\phi}(z^{(j)}), G; \\xi)$, then it follows that $P(x^{(i)}, G; \\xi) > P(x^{(j)}, G; \\xi)$."}, {"title": "D1. Hyperparameter Setting.", "content": "We conducted our experiments on a system equipped with an Intel(R) Core i9-13900k processor, 128 GB RAM, and two Nvidia RTX 4090 GPUs with 24GB VRAM each. For each baseline, we set hyperparameters according to their original papers and fine-tune them on each dataset. For the configuration of each diffusion model, we use a weighted cascade version of the IC model, i.e., the propagation probability $P_{u,v} = 1/d_{in}$ ($d_{in}$ denotes the in-degree of node v) for each edge e = (u, v) on graph G; For the LT model, the threshold $\\theta$ was set to 0.5 for each node v.\nThis section outlines the hyperparameter selection for REM (Table 4), focusing on model performance, training stability, and efficiency. Learning rates were adjusted to model complexity: 0.003 for the Variational Autoencoder (VAE) to accelerate convergence, and 0.001 for the over-parameterized Propagation Mixture of Experts (PMOE) to ensure stability. The VAE's KL divergence weight is set to 0.55, balancing reconstruction accuracy and latent space regularization. We specify the number of experts in PMOE to 20, to ensure comprehensive capture of inter-layer processes while mitigating the risk of overlooking potential propagation pathways. A minibatch size of 256 balances training stability of both models. We configured the latent seed set exploration process for 30 episodes, each comprising 400 steps, resulting in a total of 12,000 generated data points. This volume is sufficient to augment any of the graph architectures under study. Proximal Policy Optimization (PPO) was employed for policy training, incorporating an entropy coefficient and dropout to encourage exploration and prevent overfitting, respectively."}, {"title": "D2. Case Study: Graph Neural Network", "content": "This section compares the performance between two prominent Graph Neural Network (GNN) architectures, Graph Convolutional Network (GCN) and Graph Attention Network (GAT), within our PMoE framework. Table 7 presents the influence spread achieved by REM when applying each GNN variant as its PMoE expert architecture on the 5 aforementioned datasets, under both LT and IC diffusion models. Notably, we observe superior performance with GAT compared to GCN across all scenarios. This difference in performance arises from GAT's ability to assign varying levels of importance to neighboring nodes during the aggregation process, unlike GCN, which treats all neighbors equally. Choosing GAT architecture for the experts is neccessary, especially in multiplex, where the diverse and complex nature of node relationships demands a more adaptive and selective aggregation process to achieve optimal performance."}, {"title": "D3. Final infected nodes percentage", "content": "In addition to our final total infected node results, we added the percentage of nodes infected in the graph. This percentage is computed by dividing the number of nodes activated by the end of the diffusion process by the total number of nodes in the network. This metric provides a direct comparison of the effectiveness of different IM strategies in terms of their relative reach within the network.\nIM under IC Model. Table 5 compares the percentage of infected nodes (infected nodes / total nodes) achieved by various IM methods on five datasets under the IC diffusion model with four seed set budgets (1%, 5%, 10%, 20%). REM"}]}