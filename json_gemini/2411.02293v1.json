{"title": "Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation", "authors": ["Xianghui Yang", "Huiwen Shi", "Bowen Zhang", "Fan Yang", "Jiacheng Wang", "Hongxu Zhao", "Xinhai Liu", "Xinzhou Wang", "Qingxiang Lin", "Jiaao Yu", "Lifu Wang", "Zhuo Chen", "Sicong Liu", "Yuhong Liu", "Yong Yang", "Di Wang", "Jie Jiang", "Chunchao Guo"], "abstract": "While 3D generative models have greatly improved artists' workflows, the existing diffusion models for 3D generation suffer from slow generation and poor generalization. To address this issue, we propose a two-stage approach named Hunyuan3D-1.0 including a lite version and a standard version, that both support text- and image- conditioned generation. In the first stage, we employ a multi-view diffusion model that efficiently generates multi-view RGB in approximately 4 seconds. These multi-view images capture rich details of the 3D asset from different viewpoints, relaxing the tasks from single-view to multi-view reconstruction. In the second stage, we introduce a feed-forward reconstruction model that rapidly and faithfully reconstructs the 3D asset given the generated multi-view images in approximately 7 seconds. The reconstruction net- work learns to handle noises and in-consistency introduced by the multi-view diffusion and leverages the available information from the condition image to efficiently recover the 3D structure. Our framework involves the text-to-image model i.e., Hunyuan-DiT, making it a unified framework to support both text- and image-conditioned 3D generation. Our standard version has 10\u00d7 more parameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves an impressive balance between speed and quality, significantly reducing generation time while maintaining the quality and diversity of the produced assets.", "sections": [{"title": "1. Introduction", "content": "3D generation has long been an attractive and active topic in the fields of computer vision and computer graphics, with significant applications spanning gaming, film, e-commerce, and robotics. Creating high-quality 3D assets is a time-intensive process for artists, making automatic generation a long-term goal for researchers. Early efforts in this field focused on unconditional generation within specific categories, constrained by 3D representation and data limitations. The recent success of scaling laws in large language models (LLMs), as well as in image and video generation, has illuminated a path toward this long-term vision. However, achieving similar advancements in 3D asset generation remains challenging due to the expressive nature of 3D assets and the limited availability of comprehensive datasets. The largest existing 3D dataset, Objarverse-xl [7], comprises only 10 million assets, which pales in comparison to the large-scale datasets available for language, image, and video tasks. Leveraging priors from 2D generative models presents a promising approach to address this limitation.\nTo take advantage of 2D generative models, pioneering works have explored this problem and achieved notable advancements. Poole et al. [34] utilize Score Distillation Sampling (SDS) to distill a 3D representation, i.e., Nerf [30], via 2D image diffusion models. Despite issues with over-saturation and significant time costs, this approach inspired subsequent 2D lifting research. Follow-up works have explored to improve sampling efficiency [51], fine-tune diffusion models into multi-view diffusion frameworks [1, 23, 40], and replace sampling losses with regular rendering losses [22, 25, 26, 60]. However, these optimization-based methods remain time-consuming, requiring anywhere from 5 minutes to an hour to optimize the 3D representation [30, 49, 57, 62]. In contrast, feed-forward methods [4, 11, 13, 43, 59] can generate 3D objects in mere seconds but often struggle with generalization to unseen objects and fail to generate thin, paper-like structures. Disentangling single-view generation tasks into generating multi-view images and completing sparse-view reconstruction via feed-forward methods is a promising path to mitigate generalization issues and eliminate the optimization problem in SDS.\nDespite several works [58] in multi-view generation and sparse-view reconstruction, few have organized these approaches into a cohesive framework that addresses their combined challenges. First, widely used multi-view diffusion models are often criticized for multi-view inconsistency and slow denoising processes. Second, sparse-view reconstruction models typically rely solely on view-aware RGB images to predict 3D representations. Addressing these issues separately is challenging. Noticing the need to tackle these sub-tasks together, we propose Hunyuan3D-1.0, which integrates the strengths of multi-view diffusion models and sparse-view reconstruction models to achieve 3D generation in 10 seconds in the best-case scenario, achieving a subtle balance between generalization and quality. In the first stage, the multi-view diffusion model generates RGB to finish the 2D-to-3D lifting. We fine-tune a large-scale 2D diffusion model to generate multi-view images to enhance the model's understanding of 3D information. Additionally, we set the 0-elevation camera orbit for the generated views to maximize the visible area between generated views. In the second stage, the sparse-view reconstruction model utilizes the imperfectly consistent multi-view images to recover the underlying 3D shape. Unlike most sparse-view reconstruction models that only use RGB images with known poses, we incorporate the conditional image, without the known view pose, to provide additional view information as an auxiliary input to cover the unseen part in the generated multi-view images. Furthermore, we employ a linear unpatchify layer operation to enrich details in the latent space without incurring additional memory or computational costs.\nOur contributions are summarized as follows:\n\u2022 We introduce a unified framework Hunyuan3D-1.0, support text- and image- condition 3D generation both.\n\u2022 We design the 0-elevation pose distribution in the multi-view generation, maximizing the visible area between generated views.\n\u2022 We introduce a view-aware classifier-free guidance that balances the controllability and diversity for different view generations.\n\u2022 We incorporate the hybrid input that involves the uncalibrated condition image as an auxiliary view in the sparse-view reconstruction process to compensate for the unseen part in the generated images."}, {"title": "2. Related Works", "content": "Recent advances in multi-view generation models and sparse-view reconstruction models have significantly improved the quality of image-to-3D generation. Here, we briefly summarize the related works."}, {"title": "3. Medthods", "content": "We present the two stages in our approach, Hunyuan3D-1.0, in this section. First, we introduce the multi-view diffusion model for 2D-to-3D lifting in Sec. 3.1. Second, we discuss pose-known and pose-unknown image fusion and the super-resolution layer within the sparse-view reconstruction framework in Sec. 3.2."}, {"title": "3.1. Multi-view Diffusion Model", "content": "Witnessing the huge success of diffusion models in 2D generation, their potential on novel-view generation models has also been explored. Most novel-view [23, 53] or multi-view [25, 40, 47, 48] generation models leverage the generalization ability of the diffusion model trained on a large amount of data. We further scale it up by training a larger model with 10\u00d7 parameters on a large-scale dataset.\nMulti-view Generation. We simultaneously generate multi-view images by organizing the multi-view images as a grid. To achieve this, we follow Zero-1-to-3++ [39] and scale it up by replacing the model with a 10\u00d7 larger model [36]. We utilize reference attention as employed in Zero-1-to-3++ [39]. Reference attention guides the diffusion model to generate images that share similar semantic content and texture with a reference image. This involves running the denoising UNet model on an extra condition image and appending the self-attention key and value matrices from the condition image to the corresponding attention layers during the denoising process. Unlike the rendering settings of Zero-1-to-3++, we render target images with an elevation of 0\u00b0, azimuth of {0\u00b0, 60\u00b0, 120\u00b0, 180\u00b0, 240\u00b0, 300\u00b0} and a white background. The target images are arranged in a 3x 2 grid, with the size of 960\u00d7640 for the lite model and 1536x1024for the standard model.\nAdaptive Classifier-free Guidance. Classifier-free guidance (CFG) [12] is a widely used sampling technique in diffusion models to balance controllability and diversity. In multi-view generation, it has been observed that a small CFG helps synthesize detailed textures but introduces unacceptable artifacts, while a large CFG ensures excellent object geometry at the expense of texture quality [55]. Additionally, the performance of different CFG scale values varies across different views, such as front and back views. A higher CFG scale retains more details from the condition image for front views, but it can result in darker back views. Based on these observations, we propose an Adaptive Classifier-Free Guidance schedule that sets different CFG scale values for different views and time steps. Intuitively, for front views and at early denoising time steps, we set a higher CFG scale, which is then decreased as the denoising process progresses and as the view of the generated image diverges from the condition image. Specifically, we set the front view CFG scale following the curve:\n$W_t = 2 + 16 * (t/1000)^5$ (1)\nFor other views, we apply scaled versions of this curve\n$W_{t,v} = W_t * T_v$, (2)\nwhere we define $\u03c4_v \u2208 [0.5,1]$ according to view distance from the front, and $T_{front} = 1$ and $T_{back} = 0.5$. This adaptive approach allows us to dynamically adjust the CFG scale, optimizing for both texture detail and geometric accuracy across different views and stages of the denoising process. By doing so, we achieve a more balanced and high-quality multi-view generation."}, {"title": "3.2. Sparse-view Reconstruction Model", "content": "In this section, we detail our sparse-view reconstruction model, a transformer-based approach designed to recover 3D shapes in a feed-forward manner within 2 seconds, using the generated multi-view images from the multi-view diffusion model. Unlike larger reconstruction models that rely on 1 or 3 RGB images [11, 13, 20], our method combines calibrated and un-calibrated inputs, lightweight super-resolution, and explicit 3D representation to achieve high-quality 3D reconstructions from sparse-view inputs. This approach addresses the limitations of existing methods and provides a robust solution for practical 3D generation tasks.\nHybrid Inputs. Our sparse-view reconstruction model utilizes a combination of calibrated and uncalibrated images (i.e., the user inputs) for the reconstruction process. The calibrated images come with their corresponding camera embeddings, which are predefined during the training phase of the multi-view diffusion model. Since we constrain the multi-view generation to a 0-elevation orbit, the model has difficulty capturing information from top or bottom views, resulting in uncertainties in these perspectives. To address this limitation, we propose incorporating information from the uncalibrated condition image into the reconstruction process. Specifically, we extract features from the condition image and create a dedicated view-agnostic branch to integrate this information. This branch takes a special full-zero embedding as the camera embedding in the attention module, allowing the model to distinguish the condition images from generated images and effectively incorporate the features from the condition image. This design minimizes uncertainties and improves the model's ability to accurately reconstruct 3D shapes, even from sparse views.\nSuper-resolution. While a higher feature resolution in transformer-based reconstruction enables the encoding of more detailed aspects of the 3D shape, we have noticed that most existing works predominantly use low-resolution triplanes. These artifacts are directly linked to the triplane resolution, and we identify this as an aliasing issue that can be alleviated by increasing the resolution. The enhanced capacity also improves the geometry. However, it is not straightforward to increase the resolution, as it follows a quadratic complexity with the size. Drawing inspiration from the recent works [54, 68], we propose an upsampling module for triplane super-resolution. This approach maintains linear complexity with respect to the input size by avoiding self-attention on the higher-resolution triplane tokens. With this modification, we initially produced 64\u00d764 resolution triplanes with 1024 channels. We further increase the triplane resolution by decoding one low-resolution triplane token into 4 \u00d7 4 high-resolution triplane tokens using a linear layer, resulting in 120-channel triplane features at a 256x256 resolution.\n3D Representation. While most existing 3D generation models end with implicit representations, e.g., NeRF or Gaussian Splatting, we argue that implicit representations are not the final goal of 3D generation. Only explicit representations can be seamlessly utilized by artists or users in practical applications. Therefore, we adopt the Signed Distance Function (SDF) from NeuS [49] in our reconstruction model to represent the shape via implicit representation and convert it into explicit meshes by marching cube [27]. Given the generated meshes, we extract their UV maps by unwarpping. The final outputs are ready for texture mapping and further artistic refinement, which can be directly used in various applications."}, {"title": "4. Implementation", "content": "Training datasets. We train the multi-view diffusion model and the sparse-view reconstruction model using an internal dataset analogous to Objaverse [6, 7]. To ensure the quality and relevance of the training data, we filtered out 3D data that contained complex scenes, lacked meaningful textures, or exhibited unreasonable distortions. Additionally, all 3D objects in the dataset were scaled to fit within a unit sphere before rendering.\nFor rendering the condition images, we employed a random sampling strategy for camera poses. Specifically, we sampled the camera elevation from a range of [-20, 60] degrees and the azimuth from [0, 360] degrees. The HDR is randomly sampled from the a HDR set and field of view (FOV) were sampled from a uniform distribution U(47,0.01), and the camera distance was sampled from U(1.5,0.1). For rendering the target images, we fix the camera parameters for model learning. We render 24 images with azimuth angles uniformly sampled from the set {0, 15, 30, 45, ..., 330, 345} degrees, and a fixed elevation of 0 degrees. The FOV was set to 47.9 degrees, and the camera distance was fixed at 1.5 units. Uniform lighting conditions were applied to ensure consistency across the target images. All renderings were completed using Blender with a fixed rendering resolution of 1024x1024.\nTraining details. We train the multi-view diffusion model and sparse-view reconstruction model separately. For the multi-view diffusion model, our lite verison adopts the SD-2.1 as the backbone and our standard version takes SD-XL as the backbone. The RGB images are organized as a 3x2 grid. The condition image is randomly resized with [256, 512] during training, while fixed with size 512 during inference. The target images are all resized into 320\u00d7320. For the sparse-view reconstruction model, we extract the image features via DINO encoder and adopt the tri-plane as the intermediate latent representation. The reconstruction model is fisrt trained with 256 \u00d7 256 multiview input images and then finetuned with 512 \u00d7 512 multiview input images. All training is completed on 64 A100 GPUs.\nEvaluation. We evaluate our models against existing approaches using two public datasets: GSO [9] and OmniObject3D [56] with randomly sampled approximately 70 objects. To convert implicit 3D representations into meshes, we utilized the Marching Cubes algorithm [27] to extract iso-surfaces. We then sampled 10,000 points from these surfaces to compute the Chamfer Distance (CD) and F-score (FS), which are standard metrics for evaluating the accuracy of 3D shape reconstructions. Since some methods"}, {"title": "5. Results", "content": "We quantitatively and qualitatively compare Hunyuan3D-1.0 to previous state-of-the-art methods using two different datasets with 3D reconstruction metrics.\nQuantitative Comparisons. We compare Hunyuan3D-1.0 with the existing state-of-the-art baselines on 3D reconstruction that use feed-forward techniques, including OpenLRM [11], SyncDreamer [25], TripoSR [43], Wonder3D [26], CRM [52], LGM [41] and InstantMesh [58]. As shown in Table 1 and Table 2, our Hunyuan3D-1.0, especially our standard version, outperforms all the baselines, both in terms of CD and F-score metrics, achieving new state-of-the-art performance on this task.\nQualitative Comparisons. We present qualitative results of existing methods in Fig. 4. The figure illustrates that OperLRM [11] and TripoSR [43] struggle with geometric shapes, such as the soap and the box, and often generate blurred textures, as seen with the chair and the shoes. InstantMesh [20] captures more surface details but still exhibits some artifacts in certain areas, such as the seat of the chair, the logo on the cup, and the corners of the soap and box. In contrast, our model demonstrates superior reconstruction quality for both shape and texture. They not only capture the more accurate overall 3D structures of the objects but also excel in modeling intricate details. Our Hunyuan3D-1.0 received the highest user preference across 5 metrics as shown in 5.\nPerformance vs. Runtime. Another key advantage of Hunyuan3D-1.0 is its inference speed. The lite model takes around 10 seconds to produce a 3D mesh from a single image on an NVIDIA A100 GPU, while the standard model takes roughly 25 seconds. Note that these times do not include UV map unwrapping and texture baking, which takes approximately 15 seconds. Fig. 6 presents a 2D plot comparing our method to existing approaches, with inference times on the x-axis and the average F-Score on the y-axis. The plot demonstrates that Hunyuan3D-1.0 achieves an optimal balance between quality and efficiency."}, {"title": "6. Ablation Studies.", "content": "We single out the effectiveness of our proposed techniques, i.e., adaptive CFG, and hybrid inputs to the generation speed and quality in this section.\nAdaptive CFG. We verify the effectiveness of adaptive classifier-free guidance (CFG) on generated multi-view images in Fig. 7. Traditional fixed CFG throughout the denoising process often results in dark shadows in the back views. While the time-adaptive CFG introduced by Consistent123 [55] helps mitigate the shadow issue, it overlooks the relationships between views. In our camera orbit settings, the condition image has more area visible from the front view. The slow CFG would reduce the condition control for the front view, while the high CFG leaves excessive control over the back view, causing the model to replicate details from the front, such as the logo on the back of the cup. By dynamically adjusting the CFG during the generation process, we achieve a significant improvement in image quality. The adaptive CFG mechanism effectively prevents oversaturation and enables the model to generate more coherent and realistic multi-view images.\nHybrid Inputs. The hybrid input technique was designed to enhance the reconstruction of unseen parts of 3D shapes. To evaluate its effectiveness, we compare the shapes generated w/o vs w/ hybrid input. As shown in Fig. 8, the generated garlic exhibits a flat top due to the lack of top-view information in our 0-evaluation orbit. By incorporating top-view information, the reconstruction model can accurately recover the dent around the garlic root. This demonstrates that the hybrid input approach significantly enhances the reconstruction accuracy of unseen regions and confirms that it produces more complete and accurate 3D shapes, especially in areas that are not directly visible in the generated views."}, {"title": "7. Conclusion.", "content": "This work introduces Hunyuan3D-1.0, a two-stage 3D generation pipeline capable of creating high-quality 3D shapes. The pipeline consists of a multi-view generation model that produces multi-view images rich in texture and geometry details and a feed-forward sparse-view reconstruction model that recovers the underlying 3D shape with explicit representations. We incorporate several innovative designs to enhance the speed and quality of the 3D generation process, including adaptive classifier-free guidance to balance the controllability and diversity for multi-view diffusion, hybrid inputs to address the unseen part reconstruction, and a lightweight super-resolution module to enhance the representation of details. Extensive evaluations on benchmark tasks demonstrate that Hunyuan3D-1.0 achieves state-of-the-art performance in 3D generation. Our method consistently outperforms existing approaches, highlighting its effectiveness in addressing the inherent challenges of 3D generation. These results validate the robustness and efficiency of our proposed pipeline, making substantial contributions to the 3D Generative community."}]}