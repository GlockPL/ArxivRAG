{"title": "ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for Recommender Systems", "authors": ["Yi Zhang", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "abstract": "Offline reinforcement learning (RL) is an effective tool for real-world recommender systems with its capacity to model the dynamic in-terest of users and its interactive nature. Most existing offline RL recommender systems focus on model-based RL through learning a world model from offline data and building the recommendation policy by interacting with this model. Although these methods have made progress in the recommendation performance, the effec-tiveness of model-based offline RL methods is often constrained by the accuracy of the estimation of the reward model and the model uncertainties, primarily due to the extreme discrepancy between offline logged data and real-world data in user interactions with online platforms. To fill this gap, a more accurate reward model and uncertainty estimation are needed for the model-based RL meth-ods. In this paper, a novel model-based Reward Shaping in Offline Reinforcement Learning for Recommender Systems, ROLER, is proposed for reward and uncertainty estimation in recommenda-tion systems. Specifically, a non-parametric reward shaping method is designed to refine the reward model. In addition, a flexible and more representative uncertainty penalty is designed to fit the needs of recommendation systems. Extensive experiments conducted on four benchmark datasets showcase that ROLeR achieves state-of-the-art performance compared with existing baselines. Source code can be downloaded at this address.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems (RS) serve as a forever-running engine on online platforms with large volumes of data to create personalised experiences for massive users through services like content recom-mendations and targeted advertising [2, 3]. The performance of the recommender systems significantly bolsters a company's market competitiveness, especially in industries where user engagement and retention are essential to the business.\nTo dynamically model user preference, over the past few years, reinforcement learning (RL) [38] has been incorporated into recom-mender systems by modeling the entire recommendation process as an interactive problem [1]. In this process, interactions between users and the recommender system are formulated as a sequence of states, actions, and rewards within an environment. The RL-based approaches can continuously learn and adapt from each user inter-action, being able to adjust quickly to changing user preferences and behaviors [13], optimizing users' long-term engagement [44]. Despite the effectiveness of RL, it is unrealistic for an uncompleted recommender system to perform expensive online interactions with users to collect training data [4, 5, 7, 10]."}, {"title": null, "content": "Although these methods have made progress in the recommen-dation performance, the effectiveness of RL methods is often con-strained by the accuracy of the estimation of the reward model and the model uncertainties. This is because of the extreme discrep-ancy between offline data and real-world data on online platforms. Recent methods [7, 55] target at the Matthew Effect logged in the offline data with an entropy penalty to enhance the recommen-dation diversity. Others focus on the individual perspective for the filter bubble issue [10, 26] in user preference. The efficiency of these model-based offline reinforcement learning (RL) methods is frequently limited by the precision of the reward model estimation and uncertainty in the reward model, which are predominantly attributed to the significant disparity between offline logged data and real-world data in user interactions on online platforms. Re-visiting the literature of offline RL for RS, it is worth noting that neither behaving conservatively nor encouraging exploration can empower the policy to learn from offline data under the influence of an inaccurate reward estimation. In Figure 1, we demonstrate the influence of the accuracy of the reward functions on a bench-mark dataset - KuaiRec [8]. The reward prediction error of the state-of-the-art method, DORL [7], is generally high in all reward intervals in the red bar. An ideal situation would require the reward estimation error to be smaller for an effective RS development.\nTo address the aforementioned problems, we propose a novel model-based Reward Reshaping in Offline Reinforcement Learning method for Recommender Systems, ROLeR to improve the re-ward function during policy learning and decouple the uncertainty penalty with the ensemble of the world model. In RS, although the users' tastes may vary from person to person and change over time, finding similar users who share analogous interests is feasi-ble. Based on this intuition, we try to discover the patterns within the user-item interactions from offline data. We find that both the users' historical interaction data and the learned user embedding from the world model can be used as user indicator features. And if the indicator features of one user match that of another user, their feedback on a certain group of items can be mutually inferred. Thus, we regard a user's indicator feature as a soft label to retrieve its nearest neighbors. Then, the user's feedback on a certain item can be inferred by that from these neighbors. In other words, our reward shaping method is a non-parametric clustering-based one, simple yet effective. The accuracy of the reward function can be substantially improved. On the other hand, to cooperate with our reward shaping module, we develop an uncertainty estimation method based on the quality of the clustering. The distance be-tween a user and its nearest neighbors is used as the uncertainty, measuring the utility of current reward estimation. Therefore, this uncertainty penalty works as the complementary for the reward shaping part, releasing the need of an ensemble of world models. Our contributions are summarised as:\n\u2022 We find out the accuracy of the reward function prediction substantially determines the RS performance and propose a novel reward shaping method that is non-parametric for model-based offline RL for RS.\n\u2022 We develop a new uncertainty penalty that integrates with the reward shaping part for better generalization capacity and does not rely on an ensemble of world models."}, {"title": null, "content": "\u2022 We demonstrate the effectiveness of the proposed ROLeR on four recommendation benchmarks, KuaiRand, KuaiEnv, Coat and Yahoo, to showcase the superior performance among the state-of-the-art methods and strong baselines."}, {"title": "2 RELATED WORK", "content": "2.1 RL in Recommendation Systems\nSince supervised RSs have difficulties in capturing the dynamics of user preference [5, 32\u201335, 51], more deep RL [38] methods are em-ployed. Some directions [46, 52] focuses on formulating the RS to a Markov Decision Process (MDP), investigating the state representa-tion [14, 24], user profiling [22, 56], and action representation [23]. Further, based on the transition modeling, the literature can be divided into model-based methods and model-free methods. Model-based RL [49, 50] utilizes offline data to build the transition model. Further, model-free RL [6, 20] gains more attention especially when an online interaction environment is available, including Double DQN [40] to model the user dynamics [54] and utilizing a multi-agent setting [27] to tackle the sub-optimum issue [53].\n2.2 Offline Reinforcement Learning\nRecently, offline RL has attracted great attention from the research community [25, 31]. The gap between the learned policy and the real data results in the overestimation issue [19, 43]. To solve this problem, model-free methods regulate the policy learning to be con-servative. For instance, BCQ [6] avoid using the out-of-distribution (OOD) experience in policy iteration. Besides, CQL [20] constrains the usage of OOD data in the iteration of the state-action value function. CRR [45] limits the policy improvement conditioning on the discrepancy between the behavior policy and the learning policy. Though they achieve high accuracy in value estimation, they are also limited in policy improvement [48]. To deal with the extrapolate error introduced by the world model, recent methods such as MOREL [18] and MOPO [50] use the distances calculated through an ensemble of world models to penalize the overestimated values. COMBO [49] penalizes the rewards that tend to be OOD and controls the degree of pessimism based on a sampling distribution."}, {"title": "3 PRELIMINARIES", "content": "3.1 Interactive Recommendation\nThe interactive recommendation is a comprehensive simulation of real-world recommendations where the systems need to continu-ously recommend items to users.\n3.2 Reinforcement Learning Formulation\nReinforcement Learning (RL) aims to make a sequence of decisions to maximize the long-term return. RL problems can be formulated as a Markov Decision Process (MDP) with a tuple $G =< S, A, T, r, \u03b3 >$. $S$ is the state space and each $s \u2208 S$ refers to a specific state. $A$ is the action space consisting of all the potential actions. $T$ stands for the state transition of the environment as $T{S_t, a_t, S_{t+1}} = P(S_{t+1} | S = s_t, a = a_t)$. It describes the dynamics of the environment. $R$ is the reward function as $r_t = R(s_t, a_t)$, representing the reward of taking action $a_t$ at state $s_t$. At last, $\u03b3$ is the discount factor used to balance the current reward and future return. The objective of RL is to learn a policy $\u03c0$ which can maximize the long-term return: $G_t =$"}, {"title": null, "content": "$\u2211_{t=t}^{T}\u03b3^{t'}r(s_t, a_t)$, where $s_t$ stands for the start state, $T$ is the last state and $t$ is the timestep. In addition, the state value function and state-action value function of a given policy $\u03c0$ are $V^\u03c0(s) = \u0395_\u03c0 [G_t | s = s_t]$ and $Q^\u03c0 (s, a) = \u0395_\u03c0 [G_t | s = s_t, a = a_t]$, respectively. In finite MDP, where the state space and action space are finite, an optimal policy, $\u03c0^*$, whose expected return is no less than that of all other policies. Theoretically, it can be learned through policy iteration and value iteration with the Bellman Equation:\n$V_{k+1} (s) = \\underset{a}{max}  \\sum_{s',r} P(s', r | s, a) [r + \u03b3V_k (s')]$,\n(1)\n$Q_{k+1} (s, a) =  \\sum_{s',r} P(s', r | s, a) [r + \u03b3 \\underset{a'}{max} Q_k (s', a')]$,\n(2)\nwhere k is the iteration index and we use $T$ to represent the Bellman Operator in the following paragraphs.\n3.3 Offline Reinforcement Learning\nCompared to expensive online interaction, offline data are usually abundant and easily accessible. Therefore, offline RL, which in-vestigates how to utilize offline datasets to train policies, attracts increasing attention in current research due to its potential effi-ciency. The offline dataset $D = {(s_t, a_t, s_{t+1}, r_t)}$ is collected by one or more behavior policies $\u03c0_{\u03b2_t}$. One intuitive challenge of offline RL is aroused by the inevitable gap between the distribution of offline data and the evaluated environment. It is difficult for the learning policy to estimate the value function on rarely seen and even unseen states. Directly using the online RL methodologies tends to overes-timate the value functions on these states due to the maximization over available actions during decision making. The corresponding solutions can be categorized into two classes. The model-free of-fline RL adds constraints on the behavior policies and the learning policy to avoid risky decisions. But it limits the generation of of-fline RL. The model-based offline RL simulates the environments based on the offline datasets to enable the learning agent to interact with them. Then, they introduce the uncertainty estimation as a penalty in the reward function to encourage conservative actions by $r = \\hat{r} \u2013 \u03bb_U PU$. This branch suits the RS since the offline inter-actions are highly sparse. Diving into model-based offline RL for RS, it first models both the transition function and reward function to obtain a world model denoted as $G' =< S, A, \\hat{T}, \\hat{R}, \u03b3 >$, where $\\hat{T}$ and $\\hat{R}$ are the estimated transition and reward functions.\nIn addition, many recent efforts in this domain rely on an en-semble of world models to calculate uncertainty. It unnecessarily binds the world model learning and uncertainty penalty."}, {"title": "4 METHOD", "content": "4.1 Problem Definition\nConsidering reinforcement learning for the recommendation sys-tem, we demonstrate the problem formulation in this part. Recalling the MDP tuple $G =< S, A, T, r, \u03b3 >$, each $s \u2208 S$ corresponds to a user state which usually consists of the users' side information such as personal interests and dynamic features like the recent interaction history. The action space $A$ is the item set and each action $a$ corresponds to one action a_t. The reward $r(s_t, a_t)$ is from the feedback of recommending item $a_t$ at state $s_t$. It depends on the specific datasets, for instance, the reward can be the watched ratio in a short"}, {"title": null, "content": "video platform or ratings for a film forum. The transition function here is special as it is also a state tracker used for encoding states autoregressively: $s' = f(s, a, r)$. So it can be implemented using sequential models. At last, the ultimate goal of the recommendation system is to learn a policy $\u03c0$ which can maximize the cumulative user experience: $\\underset{\u03c0}{arg \\space max}  E_{\u03c4~\u03c0}[\u2211_{(s,a)\u2208\u03c4}\u03b3^t r(s, a)]$, where $\u03c4$is a trajectory sampled with policy $\u03c0$. In this paper, we focus on the model-based offline RL. And we follow a state-of-the-art method - DORL to completely illustrate the whole learning process. It usually consists of two stages: world model learning with offline interaction history and train the recommendation policy on this environment.\n4.2 World Model Learning\nIn this part, we mainly focus on the simulation of an environment. The input for this stage is the offline history. They are used as the training set of a supervised prediction model. In DORL and CIRS, they use DeepFM. The outputs are the item embedding, user em-bedding, predicted reward and uncertainty and entropy estimation. Item Embedding, $e_i$, comes from the item ID and item features like the tags for music and categories for movies.\n$e_i = f_I(iid, F_i)$,\n(3)\nwhere $f_I$ is the item encoder and $F_i$ is the item features. User Embedding, $e_u$, is similar to item embedding for user ID and other static features. It is also considered as time-invariant.\n$e_u = f_u(uid, F_u)$,\n(4)\nwhere $f_u$ is the user encoder and $F^u$ is the user features. Reward Prediction is the core output of the world model. If we formulate the feedback of all the users towards all the items as a matrix, this matrix is usually highly sparse based on the offline interaction data, which is one typical characteristic of RS. The function of reward prediction, $\\hat{r}$, is to answer the \"what if\" queries and complement the matrix. However, the quality of this matrix complement and its influence have not been investigated before. In recent methods [7, 10], the reward is the average of multiple world models:\n$\\hat{r} = \\frac{1}{M} \\sum_{j=1}^{M} W_j(e_i, e_u)$,\n(5)\nwhere $M$ is the number of world models, and $W_j$ is the j-th one. Uncertainty Penalty is widely used in offline RL to estimate the risk of taking a certain action. One representative direction utilizes an ensemble of world models to calculate the distances within the ensemble as a measure of uncertainty. In DORL, it formulates the world model as a Gaussian probabilistic model (GPM) and calculates the uncertainty of one interaction, $x$, as $P_U (x) := \\underset{k}{max}  \\frac{\u03c3_k}{\u03c3_{\u03c0^k}}$, where k is the index in world model ensemble $E$, and $\u03c3_k^2$ is the variance of corresponding GPM. Thus, the estimated reward in Equation (5) has been changed to:\n$\\hat{r}' = \\hat{r} \u2013 \u03bb_U P_U$,\n(6)\nwhere $\u03bb_U$ is the uncertainty coefficient to adjust its scale and the uncertainty estimation naturally binds with the world model. If the world model is inaccurate, the uncertainty will be impacted."}, {"title": null, "content": "Entropy Penalty is a critical contribution of DORL. Though it is calculated with the offline data rather than based on the world model, it is also implemented in the first stage of training. Thus, we introduce it here to ensure a complete illustration and consistency with the implementation of DORL. The entropy penalty is calcu-lated as the summation of a k-order entropy, which takes recent k interactions as a pattern and counts the frequency of the next item matching the current pattern.\n$PE = \u2212D_{KL}(\u03c0_{\u03b2_t}(\u00b7|s)||\u03c0_I(\u00b7|s))$,\n(7)\nwhere $\u03c0_{\u03b2_t}$ and $\u03c0_I$ are the behavior policy and the uniform distri-bution, respectively. It is an effective penalty that encourages the policy to explore the world model and improves the cumulative rewards. The final reward function during the policy learning is:\n$r' = r \u2013 \u03bb_U P_U + \u03bb_E PE$,\n(8)\nwhere $\u03bb_E$ is the coefficient for the entropy used to control its scale. In this way, DORL successfully alleviates the Matthew Effect.\n4.3 State Tracker\nState tracker models the transition function, $s' = f(s, a, r)$, during the policy learning. It is also the state encoder that combines both the static and dynamic information as:\n$s' = f(s, e_i, a, \\hat{r} | e_u)$,\n(9)\nwhere $e_i$,$\\hat{r}$, and $a$ come from the world model introduced in the last section. In the implementation of DORL, the static user embedding is not considered in its state tracker. In addition, the $f$ is imple-mented as the average of recent w item embedding concatenating with the estimated rewards $\\hat{r}$, where w is named as window size:\n$s_{t+1} = \\frac{1}{w}\u2211_{j=t-w+1}^{t} [a_j (s_j, a_j)]$.\n(10)\nSince the average tracker loses the order information, in our im-plementation, we turn to an attention tracker which enhances the cumulative reward in most cases.\n4.4 Action Representation\nIn the current setting, the item embedding from the world model is used to initialize action representation during policy learning since each item corresponds to one action.\n$a_t = e_i$.\n(11)\nOn one hand, the action representation also influences policy learn-ing. On the other hand, the inaccurate reward estimation from DeepFM intrigues us to doubt the quality of item embeddings. Thus, we find that replacing the current item embedding with a random initialization sampled from a standard normalization can enhance the cumulative rewards in many settings. Since this interesting di-rection is not our current focus, we leave it as our future direction.\n4.5 Policy Learning Pipeline\nIn this part, we illustrate the policy learning process by interact-ing with the world model. Since the states are continuous, policy gradient or actor-critic algorithms are often used as baselines. We follow the implementation of DORL to build our method based on Advantage Actor-Critic [29] (A2C)."}, {"title": null, "content": "The general pipeline for actor-critic algorithms is: The agent interacts with the world model to sample interaction trajectories, $\u03c4 = {(s_t, a_t, s_{t+1}, r_t)}$, using current policy $\u03c0_\u03b8$ where $\u03b8$ refers to the parameters representing the policy network. Then, the critic estimates the value function, either $V(s)$ or $Q(s, a)$, for the current $\u03c0_\u03b8$. It uses $\u03c4$ to update its estimation, aiming to minimize the two parts between the Bellman equation ((1) or (2)). Next, the actor updates $\u03b8$ by ascending the gradient of the expected cumulative reward estimated by the critic, aiming to maximize it. To sum up, the objective function of the critic is:\n$L_{critic} = E_{\u03c4t} [(rt + \u03b3  \\underset{a'}{max}  Q(s_{t+1}, a'; \u03a6) \u2013 Q(s_t, a_t; \u03a6))^2]$,\n(12)\nwhere $\u03a6$ represents the parameters of the critic network estimating $Q(s, a)$. The objective of the actor is:\n$J_{actor} = E_\u03c4 [log \u03c0_\u03b8 (a_t | s_t). A (s_t, a_t)]$.\n(13)\nIn A2C, the advantage function, $A^{\u03c0_\u03b8} = Q^{\u03c0_\u03b8} (s, a) \u2013 V^{\u03c0_\u03b8} (s)$, is intro-duced to accelerate the learning process.\n4.6 Reward Shaping\nThe reward functions for most existing world models are inaccurate. The reason is also one of the main challenges of RS: the offline data is too sparse to comprehensively reflect users' true feedback. To overcome this problem, we need to dig out the intrinsic patterns in the offline data. Intuitive thought is that within a short time interval, some users may exhibit similar interests in a group of items, forming a cluster. Within this user cluster and the group of items, the feedback of a specific user toward a certain item may be inferred from the other users' feedback. Thus, based on our explo-ration and observation, a non-parametric reward shaping method that accounts for the specialties of RS is proposed to improve the prediction of the reward functions.\nUnlike training the reward model from extremely sparse offline data, user representation learning is comparably more informative thanks to the existence of static side information from users [42]. In addition, the user interaction history or the counterfactual interac-tion history can also be used to identify the users. We use this user information as their indicator features, denoted as u for brevity. For each user in the evaluation environment, we utilize its indicator features to retrieve similar users in the training environment based on an appropriate distance metric using a clustering method. Here we adopt a soft-label kNN to discover the user clusters in the offline data. Then, the reward correction is estimated by aggregating the feedback of these nearest neighbors:\n$F(s, a) = \\frac{1}{k} \\sum_{u' \u2208kNN(u)} r_u(s, a)$,\n(14)\nIn this paper, we use averaging as our aggregation function. The choice of k depends on some statistics of the offline datasets. They will be elaborated in the experiment section.\n4.7 Uncertainty Penalty\nAs many current uncertainty estimations in offline RL for RecSys rely on an ensemble of world models [7, 16], the uncertainty penalty inevitably suffers from the inaccurate prediction of the reward"}, {"title": null, "content": "functions. Thus, considering the special difficulties in our setting, we propose to penalize the distances between a user and its nearest neighbors in Eq.(14) as the uncertainty. Then, we have,\n$P_U = \\frac{1}{k} \\sum_{u' \u2208kNN(u)} d(u, u')$,\n(15)\nwhere d() is the distance metric. Intuitively, it works as a comple-mentary to our reward shaping method since it considers both the clustering quality and the conditions of the dataset to enhance the generalization. When the dataset is extremely sparse, the retrieved nearest neighbors for mutual inference may be less representative. Thus, our uncertainty penalty can effectively reduce the chance of making risky decisions. Specifically, we use the cosine distance as d in our implementation. Extensive experiments about the design and estimation of uncertainty penalty are conducted in Section 6.4.\n4.8 Algorithm Overview\nNow the reward function for policy learning is derived as,\n$r' = \\hat{r} \u00d7 (1 \u2013 P_U) + \u03bb_E PE$.\n(16)\nWe summarize the overall training process in the Algorithm 1. In the evaluation process, given a user and its recent interaction trajectories, the state tracker calculates its state representation with Equation (10). Then, this representation along with the action representation obtained through Equation (11) are fed into the learned policy $\u03c0_\u03b8$. The policy recommends one item to the user. This process lasts until the user quits interaction.\n5 PERFORMANCE LOWER BOUND\nIn the simulated MDP from the world model, $\u011c =< S, A, T, \\hat{r}, \u03b3 >$, the $ \\hat{r}$ is estimated by our reward shaping method. Under the actor-critic framework, our analysis mainly focuses on the Bellman"}, {"title": null, "content": "Error in the critic since ideally, if the critic can perfectly estimate the ground truth value function, the actor can make the best decisions from a finite action space accordingly.\nWe are interested in the closeness of the estimated value func-tion learned on \u011c and the underlying ground truth value function corresponding to G in the evaluation environment: $|V^* \u2013V], \u2200s \u2208 S$. To conduct meaningful analysis, we need to make some mild as-sumptions on the distances between a user and its near neighbors. We denote $Q^*$ as the optimal state-action value function of \u011c, T as the Bellman Operator and d as the distance metric for selecting nearest neighbors.\nAssumption 1. (Lipschitz continuity) For any two transitions (s, a), (s', a') \u2208 S \u00d7 A, the difference of their estimated value function after value iteration is Lipschitz continuous:\n$|TQ(s, a) \u2013 TQ(s', a')| \u2264 L \u00b7 d[(s, a), (s', a')]$.\n(17)\nwhere L is the Lipschitz constant. This assumption bounds the differences in the value function estimation between any two neigh-borhood state-action pairs under the Bellman Operation of the eval-uation environment. Based on the statistics of the offline data, we also need to consider the quality of the clustering. We use $d_m :=  \\underset{(s_i,a_i)}{max}  d[(s_i, a_i), (s_j, a_j)], (s_i, a_i) \u2208 S\u00d7A, (s_j, a_j) \u2208 kNN((s_i, a_i))$ to represent the maximal distance between the feedback of a user and that of its nearest neighbors. In addition, the size of offline data and the number of neighbors are noted as N and K, respectively. Then, we can derive the discrepancy between the estimated value function and the ground truth one in the following theorem.\nTheorem 1. For offline data of size N, $Q^*$ is its optimal value function with respect to its world model. IfTQ* is L-smooth, then with probability at least 1 - 8,\n$|V^* - V^*| \u2264 \\frac{2}{1- \u03b3}(Ld_m+ \\frac{Qm ln(N,K,\u03b4)}{2k})$\n(18)\nwhere $\u03f5$ is a little quantity and $Q_m$ is the maximal Q-value. This theorem provides our reward shaping method with a theoretical guarantee that when the users' behaviors are not too irrelevant to form a meaningful clustering and the sparsity of the offline data is limited, then the policy learned with the world model is lower bounded.\n5.1 Proof Sketch\nIn this part, we proof sketch of the Theorem (1). Before it starts, a Lemmas from [30] will be introduced at first.\nLemma 1. Let $\u03f5$ \u2265 0 and $\u03f5^+$ \u2265 0 be constants such that \u2200(s, a) \u2208 (S, A),$\u2212\u03f5 \u2264 Q(s, \u0101) - TQ(s, a) \u2264 \u03f5^+$. The return $V^\u03c0$ from the greedy policy over Q satisfies:\n$\u2200s \u2208 S, V^\u03c0 (s) \u2265 V^*(s) \u2013 \\frac{\u03f5_- + \u03f5_+}{1- \u03b3}$\n(19)\nProof: We denote the value function for the real environment G and the world model \u011c as Q and $\\hat{Q}$. Applying the Bellman Operator on the same state-action pair of two environments is different as their reward functions differ. Thus, we use T and $\\hat{T}$ to denote them. To build a bridge between these two operators, we define a cross-environment Bellman Operator as a bridge to bind G and \u011c:\n$TQ(s, a) = \\hat{r} + \u03b3 \\underset{a'}{max} \\hat{Q}(s', a') + TQ(s, a) \u2013 TQ(s_i, a_i)$,\n(20)"}, {"title": null, "content": "where $(s_i, a_i) \u2208 kNN(s, a)$ Then, we decompose the Bellman Error as:\n$Q(s, a)-TQ(s, a) = Q(s, a)\u2212T(s, a)+T(s, a)\u2212TQ(s, a)$. (21)\nWe focus on the right-hand side, taking the first two terms as one unit:\n$1 = \\frac{1}{k} \u2211_{i \u2208kNN(s,a)} [T(s, a) - TQ(s_i, a_i)]$; (22)\nDue to our assumption 1 about the Lipschitz continuity, we have: -L.dm \u2264 I \u2264 L.dm; (23)\nNow we need to bound II = T(s, a) \u2013 TQ(s, a). We have\n$E[T(s, a)] = TQ(s, a)$ (24)\nWe denote $C(N, K)$ as the largest number of clusters in a dataset. According to the Hoeffding inequality, given real number $\u03b4 \u2208 (0, 1)$, we have,\n$P|T(s, a) \u2013 TQ(s, a)| \u2265 Q_m \\sqrt{\\frac{ln(\\frac{2C(N, K)}{\u03b4})}{2k}} \u2264 \u03b4$, (25)\nwhere $Q_m$ is the maximal Q-value.\nThen, we obtain that with probability at least 1 - $\u03b4$, $|I| \u2264 \u03f5(k, N, \u03b4)$. Putting I and II together, we have,\n$-(L.d_m+\u03f5(k, N, \u03b4)) \u2264 Q(s, a)\u2212TQ(s, a) \u2264 L\u00b7d_m+\u03f5(k, N, \u03b4)$. (26)\nUse Eq. (19) in Lemma 1, the Theorem is proved."}, {"title": "6 EXPERIMENT", "content": "In this section, we begin with introducing the experiment setup including the datasets we use, evaluation metric, the state-of-the-art method, and other baselines. Then, we investigate the following questions:\n\u2022 (RQ1) How does ROLeR perform compared with other base-lines?\n\u2022 (RQ2) How do the reward shaping and uncertainty penalty in ROLeR contribute to its performance?\n\u2022 (RQ3) What is the most effective design of the uncertainty penalty?\n\u2022 (RQ4) What is the impact of the world model in ROLeR?\n\u2022 (RQ5) Is ROLeR robust to the critical hyperparameters?\n6.1 Setup\n6.1.1 Datasets. We conduct experiments on four datasets including two newly proposed challenging short-video interaction datasets and two typical datasets. The statistics are listed in Table 1.\nKuaiRec [8] is a video dataset that contains a fully-observed user-item interaction matrix used for evaluation. Within this matrix, each user's feedback towards any item is known. The normalized viewing time of a video is used as a reward signal. The training data is from the standard interaction history containing popularity bias, which makes it difficult to learn a recommendation policy.\nKuaiRand-Pure [9] is also a video dataset that contains user-item interaction collected by randomly inserting the target videos in the standard recommendation stream. This part is used to simulate a fully observed interaction matrix like KuaiRec for evaluation, while"}, {"title": null, "content": "the data collected in the standard stream serves as the training set. The \"is_click\" signal is used as the reward.\nCoat [36] is a shopping dataset. It consists of the user ratings, a five-point scale, on the self-selected items and uniformly sampled ones. These two parts of data are used for training and testing, respectively. The ratings are used as reward signals.\nYahoo [28] is a music dataset with training set and testing set collection similar to Coat. The ratings on the user-selected items are used for training, while those on the randomly picked items are for testing. Similarly, the ratings are treated as reward signals.\n6.1.2 Evaluation. In the scope of RL for recommender systems, the objective of RS is to maximize long-term user satisfaction, which can be formulated as the return, i.e., cumulative reward. For the detailed evaluation setting, we follow the same manners as in DORL [7] and [10]: The same item will not be recommended twice to a user. A quitting mechanism is applied as the termination condition: the interaction will terminate when a user receives M items of the same category in the recent N transitions. We keep M = 0, N = 4 as in DORL, which means for every four transitions, the items' categories should differ. The maximum user-item interaction length is set to 30 and the cumulative reward is evaluated after every training epoch on 100 testing trajectories. The final cumulative reward is averaged across 200 epochs. In addition, to enable in-depth analysis, the cumulative reward is decomposed into the interaction length and single-step recommendation reward. They are regarded as metrics as well. Majority category domination (MCD) was originally proposed in DORL as a reference of the Matthew effect, which should be controlled in a suitable range. For readability, we still overbold the smallest MCD in each dataset. In addition, due to the definition of most popular categories in DORL, the calculation of MCD is not supported on Coat and Yahoo.\nWhile the testing set of KuaiRec is fully observable and can be directly used as the reward function during policy evaluation, the reward functions for the other three datasets are estimated with the testing data by DeepFM models. To ensure a fair comparison, we use exactly the same reward functions as in DORL on all datasets.\n6.1.3 Baselines. The baselines include five model-based offline RL methods, including two state-of-the-art, four model-free offline RL methods, and two vanilla bandit-based methods. To ensure a fair comparison, on each dataset, we use the same DeepFM [11] model to learn the world model for model-based offline RL methods."}, {"title": "6.2 Overall Performance (RQ1)", "content": "For our ROLeR", "hyperparameters": "the coefficients of the entropy penalty and the number of nearest neighbors. We simply tune k from 5 to 50", "metric": "cumulative reward followed by average interaction length and single-step reward as the second and third rows"}, {"title": "ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for Recommender Systems", "authors": ["Yi Zhang", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "abstract": "Offline reinforcement learning (RL) is an effective tool for real-world recommender systems with its capacity to model the dynamic in-terest of users and its interactive nature. Most existing offline RL recommender systems focus on model-based RL through learning a world model from offline data and building the recommendation policy by interacting with this model. Although these methods have made progress in the recommendation performance, the effec-tiveness of model-based offline RL methods is often constrained by the accuracy of the estimation of the reward model and the model uncertainties, primarily due to the extreme discrepancy between offline logged data and real-world data in user interactions with online platforms. To fill this gap, a more accurate reward model and uncertainty estimation are needed for the model-based RL meth-ods. In this paper, a novel model-based Reward Shaping in Offline Reinforcement Learning for Recommender Systems, ROLER, is proposed for reward and uncertainty estimation in recommenda-tion systems. Specifically, a non-parametric reward shaping method is designed to refine the reward model. In addition, a flexible and more representative uncertainty penalty is designed to fit the needs of recommendation systems. Extensive experiments conducted on four benchmark datasets showcase that ROLeR achieves state-of-the-art performance compared with existing baselines. Source code can be downloaded at this address.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems (RS) serve as a forever-running engine on online platforms with large volumes of data to create personalised experiences for massive users through services like content recom-mendations and targeted advertising [2, 3]. The performance of the recommender systems significantly bolsters a company's market competitiveness, especially in industries where user engagement and retention are essential to the business.\nTo dynamically model user preference, over the past few years, reinforcement learning (RL) [38] has been incorporated into recom-mender systems by modeling the entire recommendation process as an interactive problem [1]. In this process, interactions between users and the recommender system are formulated as a sequence of states, actions, and rewards within an environment. The RL-based approaches can continuously learn and adapt from each user inter-action, being able to adjust quickly to changing user preferences and behaviors [13], optimizing users' long-term engagement [44]. Despite the effectiveness of RL, it is unrealistic for an uncompleted recommender system to perform expensive online interactions with users to collect training data [4, 5, 7, 10]."}, {"title": null, "content": "Although these methods have made progress in the recommen-dation performance, the effectiveness of RL methods is often con-strained by the accuracy of the estimation of the reward model and the model uncertainties. This is because of the extreme discrep-ancy between offline data and real-world data on online platforms. Recent methods [7, 55] target at the Matthew Effect logged in the offline data with an entropy penalty to enhance the recommen-dation diversity. Others focus on the individual perspective for the filter bubble issue [10, 26] in user preference. The efficiency of these model-based offline reinforcement learning (RL) methods is frequently limited by the precision of the reward model estimation and uncertainty in the reward model, which are predominantly attributed to the significant disparity between offline logged data and real-world data in user interactions on online platforms. Re-visiting the literature of offline RL for RS, it is worth noting that neither behaving conservatively nor encouraging exploration can empower the policy to learn from offline data under the influence of an inaccurate reward estimation. In Figure 1, we demonstrate the influence of the accuracy of the reward functions on a bench-mark dataset - KuaiRec [8]. The reward prediction error of the state-of-the-art method, DORL [7], is generally high in all reward intervals in the red bar. An ideal situation would require the reward estimation error to be smaller for an effective RS development.\nTo address the aforementioned problems, we propose a novel model-based Reward Reshaping in Offline Reinforcement Learning method for Recommender Systems, ROLeR to improve the re-ward function during policy learning and decouple the uncertainty penalty with the ensemble of the world model. In RS, although the users' tastes may vary from person to person and change over time, finding similar users who share analogous interests is feasi-ble. Based on this intuition, we try to discover the patterns within the user-item interactions from offline data. We find that both the users' historical interaction data and the learned user embedding from the world model can be used as user indicator features. And if the indicator features of one user match that of another user, their feedback on a certain group of items can be mutually inferred. Thus, we regard a user's indicator feature as a soft label to retrieve its nearest neighbors. Then, the user's feedback on a certain item can be inferred by that from these neighbors. In other words, our reward shaping method is a non-parametric clustering-based one, simple yet effective. The accuracy of the reward function can be substantially improved. On the other hand, to cooperate with our reward shaping module, we develop an uncertainty estimation method based on the quality of the clustering. The distance be-tween a user and its nearest neighbors is used as the uncertainty, measuring the utility of current reward estimation. Therefore, this uncertainty penalty works as the complementary for the reward shaping part, releasing the need of an ensemble of world models. Our contributions are summarised as:\n\u2022 We find out the accuracy of the reward function prediction substantially determines the RS performance and propose a novel reward shaping method that is non-parametric for model-based offline RL for RS.\n\u2022 We develop a new uncertainty penalty that integrates with the reward shaping part for better generalization capacity and does not rely on an ensemble of world models."}, {"title": null, "content": "\u2022 We demonstrate the effectiveness of the proposed ROLeR on four recommendation benchmarks, KuaiRand, KuaiEnv, Coat and Yahoo, to showcase the superior performance among the state-of-the-art methods and strong baselines."}, {"title": "2 RELATED WORK", "content": "2.1 RL in Recommendation Systems\nSince supervised RSs have difficulties in capturing the dynamics of user preference [5, 32\u201335, 51], more deep RL [38] methods are em-ployed. Some directions [46, 52] focuses on formulating the RS to a Markov Decision Process (MDP), investigating the state representa-tion [14, 24], user profiling [22, 56], and action representation [23]. Further, based on the transition modeling, the literature can be divided into model-based methods and model-free methods. Model-based RL [49, 50] utilizes offline data to build the transition model. Further, model-free RL [6, 20] gains more attention especially when an online interaction environment is available, including Double DQN [40] to model the user dynamics [54] and utilizing a multi-agent setting [27] to tackle the sub-optimum issue [53].\n2.2 Offline Reinforcement Learning\nRecently, offline RL has attracted great attention from the research community [25, 31]. The gap between the learned policy and the real data results in the overestimation issue [19, 43]. To solve this problem, model-free methods regulate the policy learning to be con-servative. For instance, BCQ [6] avoid using the out-of-distribution (OOD) experience in policy iteration. Besides, CQL [20] constrains the usage of OOD data in the iteration of the state-action value function. CRR [45] limits the policy improvement conditioning on the discrepancy between the behavior policy and the learning policy. Though they achieve high accuracy in value estimation, they are also limited in policy improvement [48]. To deal with the extrapolate error introduced by the world model, recent methods such as MOREL [18] and MOPO [50] use the distances calculated through an ensemble of world models to penalize the overestimated values. COMBO [49] penalizes the rewards that tend to be OOD and controls the degree of pessimism based on a sampling distribution."}, {"title": "3 PRELIMINARIES", "content": "3.1 Interactive Recommendation\nThe interactive recommendation is a comprehensive simulation of real-world recommendations where the systems need to continu-ously recommend items to users.\n3.2 Reinforcement Learning Formulation\nReinforcement Learning (RL) aims to make a sequence of decisions to maximize the long-term return. RL problems can be formulated as a Markov Decision Process (MDP) with a tuple $G =< S, A, T, r, \\gamma >$. $S$ is the state space and each $s \\in S$ refers to a specific state. $A$ is the action space consisting of all the potential actions. $T$ stands for the state transition of the environment as $T{S_t, a_t, S_{t+1}} = P(S_{t+1} | S = s_t, a = a_t)$. It describes the dynamics of the environment. $R$ is the reward function as $r_t = R(s_t, a_t)$, representing the reward of taking action $a_t$ at state $s_t$. At last, $\\gamma$ is the discount factor used to balance the current reward and future return. The objective of RL is to learn a policy $\\pi$ which can maximize the long-term return: $G_t =$"}, {"title": null, "content": "$\\sum_{t=t}^{T}\\gamma^{t'}r(s_t, a_t)$, where $s_t$ stands for the start state, $T$ is the last state and $t$ is the timestep. In addition, the state value function and state-action value function of a given policy $\\pi$ are $V^\\pi(s) = E_\\pi [G_t | s = s_t]$ and $Q^\\pi (s, a) = E_\\pi [G_t | s = s_t, a = a_t]$, respectively. In finite MDP, where the state space and action space are finite, an optimal policy, $\\pi^*$, whose expected return is no less than that of all other policies. Theoretically, it can be learned through policy iteration and value iteration with the Bellman Equation:\n$V_{k+1} (s) = \\underset{a}{max}  \\sum_{s',r} P(s', r | s, a) [r + \\gamma V_k (s')]$,\n(1)\n$Q_{k+1} (s, a) =  \\sum_{s',r} P(s', r | s, a) [r + \\gamma \\underset{a'}{max} Q_k (s', a')]$,\n(2)\nwhere k is the iteration index and we use $T$ to represent the Bellman Operator in the following paragraphs.\n3.3 Offline Reinforcement Learning\nCompared to expensive online interaction, offline data are usually abundant and easily accessible. Therefore, offline RL, which in-vestigates how to utilize offline datasets to train policies, attracts increasing attention in current research due to its potential effi-ciency. The offline dataset $D = {(s_t, a_t, s_{t+1}, r_t)}$ is collected by one or more behavior policies $\\pi_{\\beta_t}$. One intuitive challenge of offline RL is aroused by the inevitable gap between the distribution of offline data and the evaluated environment. It is difficult for the learning policy to estimate the value function on rarely seen and even unseen states. Directly using the online RL methodologies tends to overes-timate the value functions on these states due to the maximization over available actions during decision making. The corresponding solutions can be categorized into two classes. The model-free of-fline RL adds constraints on the behavior policies and the learning policy to avoid risky decisions. But it limits the generation of of-fline RL. The model-based offline RL simulates the environments based on the offline datasets to enable the learning agent to interact with them. Then, they introduce the uncertainty estimation as a penalty in the reward function to encourage conservative actions by $r = \\hat{r} - \\lambda_U PU$. This branch suits the RS since the offline inter-actions are highly sparse. Diving into model-based offline RL for RS, it first models both the transition function and reward function to obtain a world model denoted as $G' =< S, A, \\hat{T}, \\hat{R}, \\gamma >$, where $\\hat{T}$ and $\\hat{R}$ are the estimated transition and reward functions.\nIn addition, many recent efforts in this domain rely on an en-semble of world models to calculate uncertainty. It unnecessarily binds the world model learning and uncertainty penalty."}, {"title": "4 METHOD", "content": "4.1 Problem Definition\nConsidering reinforcement learning for the recommendation sys-tem, we demonstrate the problem formulation in this part. Recalling the MDP tuple $G =< S, A, T, r, \\gamma >$, each $s \\in S$ corresponds to a user state which usually consists of the users' side information such as personal interests and dynamic features like the recent interaction history. The action space $A$ is the item set and each action $a$ corresponds to one action a_t. The reward $r(s_t, a_t)$ is from the feedback of recommending item $a_t$ at state $s_t$. It depends on the specific datasets, for instance, the reward can be the watched ratio in a short"}, {"title": null, "content": "video platform or ratings for a film forum. The transition function here is special as it is also a state tracker used for encoding states autoregressively: $s' = f(s, a, r)$. So it can be implemented using sequential models. At last, the ultimate goal of the recommendation system is to learn a policy $\\pi$ which can maximize the cumulative user experience: $\\underset{\\pi}{arg \\space max}  E_{\\tau~\\pi}[\\sum_{(s,a)\\in\\tau}\\gamma^t r(s, a)]$, where $\\tau$is a trajectory sampled with policy $\\pi$. In this paper, we focus on the model-based offline RL. And we follow a state-of-the-art method - DORL to completely illustrate the whole learning process. It usually consists of two stages: world model learning with offline interaction history and train the recommendation policy on this environment.\n4.2 World Model Learning\nIn this part, we mainly focus on the simulation of an environment. The input for this stage is the offline history. They are used as the training set of a supervised prediction model. In DORL and CIRS, they use DeepFM. The outputs are the item embedding, user em-bedding, predicted reward and uncertainty and entropy estimation. Item Embedding, $e_i$, comes from the item ID and item features like the tags for music and categories for movies.\n$e_i = f_I(iid, F_i)$,\n(3)\nwhere $f_I$ is the item encoder and $F_i$ is the item features. User Embedding, $e_u$, is similar to item embedding for user ID and other static features. It is also considered as time-invariant.\n$e_u = f_u(uid, F_u)$,\n(4)\nwhere $f_u$ is the user encoder and $F^u$ is the user features. Reward Prediction is the core output of the world model. If we formulate the feedback of all the users towards all the items as a matrix, this matrix is usually highly sparse based on the offline interaction data, which is one typical characteristic of RS. The function of reward prediction, $\\hat{r}$, is to answer the \"what if\" queries and complement the matrix. However, the quality of this matrix complement and its influence have not been investigated before. In recent methods [7, 10], the reward is the average of multiple world models:\n$\\hat{r} = \\frac{1}{M} \\sum_{j=1}^{M} W_j(e_i, e_u)$,\n(5)\nwhere $M$ is the number of world models, and $W_j$ is the j-th one. Uncertainty Penalty is widely used in offline RL to estimate the risk of taking a certain action. One representative direction utilizes an ensemble of world models to calculate the distances within the ensemble as a measure of uncertainty. In DORL, it formulates the world model as a Gaussian probabilistic model (GPM) and calculates the uncertainty of one interaction, $x$, as $P_U (x) := \\underset{k}{max}  \\frac{\\sigma_k}{\\sigma_{\\pi^k}}$, where k is the index in world model ensemble $E$, and $\\sigma_k^2$ is the variance of corresponding GPM. Thus, the estimated reward in Equation (5) has been changed to:\n$\\hat{r}' = \\hat{r} - \\lambda_U P_U$,\n(6)\nwhere $\\lambda_U$ is the uncertainty coefficient to adjust its scale and the uncertainty estimation naturally binds with the world model. If the world model is inaccurate, the uncertainty will be impacted."}, {"title": null, "content": "Entropy Penalty is a critical contribution of DORL. Though it is calculated with the offline data rather than based on the world model, it is also implemented in the first stage of training. Thus, we introduce it here to ensure a complete illustration and consistency with the implementation of DORL. The entropy penalty is calcu-lated as the summation of a k-order entropy, which takes recent k interactions as a pattern and counts the frequency of the next item matching the current pattern.\n$PE = -D_{KL}(\\pi_{\\beta_t}(\\cdot|s)||\\pi_I(\\cdot|s))$,\n(7)\nwhere $\\pi_{\\beta_t}$ and $\\pi_I$ are the behavior policy and the uniform distri-bution, respectively. It is an effective penalty that encourages the policy to explore the world model and improves the cumulative rewards. The final reward function during the policy learning is:\n$r' = r - \\lambda_U P_U + \\lambda_E PE$,\n(8)\nwhere $\\lambda_E$ is the coefficient for the entropy used to control its scale. In this way, DORL successfully alleviates the Matthew Effect.\n4.3 State Tracker\nState tracker models the transition function, $s' = f(s, a, r)$, during the policy learning. It is also the state encoder that combines both the static and dynamic information as:\n$s' = f(s, e_i, a, \\hat{r} | e_u)$,\n(9)\nwhere $e_i$,$\\hat{r}$, and $a$ come from the world model introduced in the last section. In the implementation of DORL, the static user embedding is not considered in its state tracker. In addition, the $f$ is imple-mented as the average of recent w item embedding concatenating with the estimated rewards $\\hat{r}$, where w is named as window size:\n$s_{t+1} = \\frac{1}{w}\\sum_{j=t-w+1}^{t} [a_j (s_j, a_j)]$.\n(10)\nSince the average tracker loses the order information, in our im-plementation, we turn to an attention tracker which enhances the cumulative reward in most cases.\n4.4 Action Representation\nIn the current setting, the item embedding from the world model is used to initialize action representation during policy learning since each item corresponds to one action.\n$a_t = e_i$.\n(11)\nOn one hand, the action representation also influences policy learn-ing. On the other hand, the inaccurate reward estimation from DeepFM intrigues us to doubt the quality of item embeddings. Thus, we find that replacing the current item embedding with a random initialization sampled from a standard normalization can enhance the cumulative rewards in many settings. Since this interesting di-rection is not our current focus, we leave it as our future direction.\n4.5 Policy Learning Pipeline\nIn this part, we illustrate the policy learning process by interact-ing with the world model. Since the states are continuous, policy gradient or actor-critic algorithms are often used as baselines. We follow the implementation of DORL to build our method based on Advantage Actor-Critic [29] (A2C)."}, {"title": null, "content": "The general pipeline for actor-critic algorithms is: The agent interacts with the world model to sample interaction trajectories, $\\tau = {(s_t, a_t, s_{t+1}, r_t)}$, using current policy $\\pi_\\theta$ where $\\theta$ refers to the parameters representing the policy network. Then, the critic estimates the value function, either $V(s)$ or $Q(s, a)$, for the current $\\pi_\\theta$. It uses $\\tau$ to update its estimation, aiming to minimize the two parts between the Bellman equation ((1) or (2)). Next, the actor updates $\\theta$ by ascending the gradient of the expected cumulative reward estimated by the critic, aiming to maximize it. To sum up, the objective function of the critic is:\n$L_{critic} = E_{\\taut} [(rt + \\gamma  \\underset{a'}{max}  Q(s_{t+1}, a'; \\Phi) - Q(s_t, a_t; \\Phi))^2]$,\n(12)\nwhere $\\Phi$ represents the parameters of the critic network estimating $Q(s, a)$. The objective of the actor is:\n$J_{actor} = E_\\tau [log \\pi_\\theta (a_t | s_t). A (s_t, a_t)]$.\n(13)\nIn A2C, the advantage function, $A^{\\pi_\\theta} = Q^{\\pi_\\theta} (s, a) - V^{\\pi_\\theta} (s)$, is intro-duced to accelerate the learning process.\n4.6 Reward Shaping\nThe reward functions for most existing world models are inaccurate. The reason is also one of the main challenges of RS: the offline data is too sparse to comprehensively reflect users' true feedback. To overcome this problem, we need to dig out the intrinsic patterns in the offline data. Intuitive thought is that within a short time interval, some users may exhibit similar interests in a group of items, forming a cluster. Within this user cluster and the group of items, the feedback of a specific user toward a certain item may be inferred from the other users' feedback. Thus, based on our explo-ration and observation, a non-parametric reward shaping method that accounts for the specialties of RS is proposed to improve the prediction of the reward functions.\nUnlike training the reward model from extremely sparse offline data, user representation learning is comparably more informative thanks to the existence of static side information from users [42]. In addition, the user interaction history or the counterfactual interac-tion history can also be used to identify the users. We use this user information as their indicator features, denoted as u for brevity. For each user in the evaluation environment, we utilize its indicator features to retrieve similar users in the training environment based on an appropriate distance metric using a clustering method. Here we adopt a soft-label kNN to discover the user clusters in the offline data. Then, the reward correction is estimated by aggregating the feedback of these nearest neighbors:\n$F(s, a) = \\frac{1}{k} \\sum_{u' \\in kNN(u)} r_u(s, a)$,\n(14)\nIn this paper, we use averaging as our aggregation function. The choice of k depends on some statistics of the offline datasets. They will be elaborated in the experiment section.\n4.7 Uncertainty Penalty\nAs many current uncertainty estimations in offline RL for RecSys rely on an ensemble of world models [7, 16], the uncertainty penalty inevitably suffers from the inaccurate prediction of the reward"}, {"title": null, "content": "functions. Thus, considering the special difficulties in our setting, we propose to penalize the distances between a user and its nearest neighbors in Eq.(14) as the uncertainty. Then, we have,\n$P_U = \\frac{1}{k} \\sum_{u' \\in kNN(u)} d(u, u')$,\n(15)\nwhere d() is the distance metric. Intuitively, it works as a comple-mentary to our reward shaping method since it considers both the clustering quality and the conditions of the dataset to enhance the generalization. When the dataset is extremely sparse, the retrieved nearest neighbors for mutual inference may be less representative. Thus, our uncertainty penalty can effectively reduce the chance of making risky decisions. Specifically, we use the cosine distance as d in our implementation. Extensive experiments about the design and estimation of uncertainty penalty are conducted in Section 6.4.\n4.8 Algorithm Overview\nNow the reward function for policy learning is derived as,\n$r' = \\hat{r} \\times (1 - P_U) + \\lambda_E PE$.\n(16)\nWe summarize the overall training process in the Algorithm 1. In the evaluation process, given a user and its recent interaction trajectories, the state tracker calculates its state representation with Equation (10). Then, this representation along with the action representation obtained through Equation (11) are fed into the learned policy $\\pi_\\theta$. The policy recommends one item to the user. This process lasts until the user quits interaction.\n5 PERFORMANCE LOWER BOUND\nIn the simulated MDP from the world model, $\\hat{G} =< S, A, T, \\hat{r}, \\gamma >$, the $ \\hat{r}$ is estimated by our reward shaping method. Under the actor-critic framework, our analysis mainly focuses on the Bellman"}, {"title": null, "content": "Error in the critic since ideally, if the critic can perfectly estimate the ground truth value function, the actor can make the best decisions from a finite action space accordingly.\nWe are interested in the closeness of the estimated value func-tion learned on \u011c and the underlying ground truth value function corresponding to G in the evaluation environment: $|V^* -V], \u2200s \u2208 S$. To conduct meaningful analysis, we need to make some mild as-sumptions on the distances between a user and its near neighbors. We denote $Q^*$ as the optimal state-action value function of \u011c, T as the Bellman Operator and d as the distance metric for selecting nearest neighbors.\nAssumption 1. (Lipschitz continuity) For any two transitions (s, a), (s', a') \u2208 S \u00d7 A, the difference of their estimated value function after value iteration is Lipschitz continuous:\n$|TQ(s, a) - TQ(s', a')| \\le L \\cdot d[(s, a), (s', a')]$.\n(17)\nwhere L is the Lipschitz constant. This assumption bounds the differences in the value function estimation between any two neigh-borhood state-action pairs under the Bellman Operation of the eval-uation environment. Based on the statistics of the offline data, we also need to consider the quality of the clustering. We use $d_m :=  \\underset{(s_i,a_i)}{max}  d[(s_i, a_i), (s_j, a_j)], (s_i, a_i) \u2208 S\u00d7A, (s_j, a_j) \u2208 kNN((s_i, a_i))$ to represent the maximal distance between the feedback of a user and that of its nearest neighbors. In addition, the size of offline data and the number of neighbors are noted as N and K, respectively. Then, we can derive the discrepancy between the estimated value function and the ground truth one in the following theorem.\nTheorem 1. For offline data of size N, $Q^*$ is its optimal value function with respect to its world model. IfTQ* is L-smooth, then with probability at least 1 - 8,\n$|V^* - V^*| \\le \\frac{2}{1- \\gamma}(Ld_m+ \\frac{Q_m ln(N,K,\\delta)}{2k})$\n(18)\nwhere \u03f5 is a little quantity and $Q_m$ is the maximal Q-value. This theorem provides our reward shaping method with a theoretical guarantee that when the users' behaviors are not too irrelevant to form a meaningful clustering and the sparsity of the offline data is limited, then the policy learned with the world model is lower bounded.\n5.1 Proof Sketch\nIn this part, we proof sketch of the Theorem (1). Before it starts, a Lemmas from [30] will be introduced at first.\nLemma 1. Let $\u03f5$ \u2265 0 and $\u03f5^+$ \u2265 0 be constants such that \u2200(s, a) \u2208 (S, A),$\u2212\u03f5 \\le Q(s, \u0101) - TQ(s, a) \\le \u03f5^+$. The return $V^\u03c0$ from the greedy policy over Q satisfies:\n$\u2200s \u2208 S, V^\u03c0 (s) \\ge V^*(s) - \\frac{\u03f5_- + \u03f5_+}{1- \\gamma}$\n(19)\nProof: We denote the value function for the real environment G and the world model \u011c as Q and $\\hat{Q}$. Applying the Bellman Operator on the same state-action pair of two environments is different as their reward functions differ. Thus, we use T and $\\hat{T}$ to denote them. To build a bridge between these two operators, we define a cross-environment Bellman Operator as a bridge to bind G and \u011c:\n$TQ(s, a) = \\hat{r} + \\gamma \\underset{a'}{max} \\hat{Q}(s', a') + TQ(s, a) - TQ(s_i, a_i)$,\n(20)"}, {"title": null, "content": "where $(s_i, a_i) \\in kNN(s, a)$ Then, we decompose the Bellman Error as:\n$Q(s, a)-TQ(s, a) = Q(s, a)-T(s, a)+T(s, a)-TQ(s, a)$. (21)\nWe focus on the right-hand side, taking the first two terms as one unit:\n$1 = \\frac{1}{k} \\sum_{i \\in kNN(s,a)} [T(s, a) - TQ(s_i, a_i)]$; (22)\nDue to our assumption 1 about the Lipschitz continuity, we have: -L.dm \u2264 I \u2264 L.dm; (23)\nNow we need to bound II = T(s, a) \u2013 TQ(s, a). We have\n$E[T(s, a)] = TQ(s, a)$ (24)\nWe denote $C(N, K)$ as the largest number of clusters in a dataset. According to the Hoeffding inequality, given real number $\u03b4 \u2208 (0, 1)$, we have,\n$P|T(s, a) - TQ(s, a)| \\ge Q_m \\sqrt{\\frac{ln(\\frac{2C(N, K)}{\\delta})}{2k}} \u2264 \u03b4$, (25)\nwhere $Q_m$ is the maximal Q-value.\nThen, we obtain that with probability at least 1 - $\u03b4$, $|I| \\le \u03f5(k, N, \u03b4)$. Putting I and II together, we have,\n$-(L.d_m+\u03f5(k, N, \u03b4)) \u2264 Q(s, a)\u2212TQ(s, a) \u2264 L\u00b7d_m+\u03f5(k, N, \u03b4)$. (26)\nUse Eq. (19) in Lemma 1, the Theorem is proved."}, {"title": "6 EXPERIMENT", "content": "In this section, we begin with introducing the experiment setup including the datasets we use, evaluation metric, the state-of-the-art method, and other baselines. Then, we investigate the following questions:\n\u2022 (RQ1) How does ROLeR perform compared with other base-lines?\n\u2022 (RQ2) How do the reward shaping and uncertainty penalty in ROLeR contribute to its performance?\n\u2022 (RQ3) What is the most effective design of the uncertainty penalty?\n\u2022 (RQ4) What is the impact of the world model in ROLeR?\n\u2022 (RQ5) Is ROLeR robust to the critical hyperparameters?\n6.1 Setup\n6.1.1 Datasets. We conduct experiments on four datasets including two newly proposed challenging short-video interaction datasets and two typical datasets. The statistics are listed in Table 1.\nKuaiRec [8] is a video dataset that contains a fully-observed user-item interaction matrix used for evaluation. Within this matrix, each user's feedback towards any item is known. The normalized viewing time of a video is used as a reward signal. The training data is from the standard interaction history containing popularity bias, which makes it difficult to learn a recommendation policy.\nKuaiRand-Pure [9] is also a video dataset that contains user-item interaction collected by randomly inserting the target videos in the standard recommendation stream. This part is used to simulate a fully observed interaction matrix like KuaiRec for evaluation, while"}, {"title": null, "content": "the data collected in the standard stream serves as the training set. The \"is_click\" signal is used as the reward.\nCoat [36] is a shopping dataset. It consists of the user ratings, a five-point scale, on the self-selected items and uniformly sampled ones. These two parts of data are used for training and testing, respectively. The ratings are used as reward signals.\nYahoo [28] is a music dataset with training set and testing set collection similar to Coat. The ratings on the user-selected items are used for training, while those on the randomly picked items are for testing. Similarly, the ratings are treated as reward signals.\n6.1.2 Evaluation. In the scope of RL for recommender systems, the objective of RS is to maximize long-term user satisfaction, which can be formulated as the return, i.e., cumulative reward. For the detailed evaluation setting, we follow the same manners as in DORL [7] and [10]: The same item will not be recommended twice to a user. A quitting mechanism is applied as the termination condition: the interaction will terminate when a user receives M items of the same category in the recent N transitions. We keep M = 0, N = 4 as in DORL, which means for every four transitions, the items' categories should differ. The maximum user-item interaction length is set to 30 and the cumulative reward is evaluated after every training epoch on 100 testing trajectories. The final cumulative reward is averaged across 200 epochs. In addition, to enable in-depth analysis, the cumulative reward is decomposed into the interaction length and single-step recommendation reward. They are regarded as metrics as well. Majority category domination (MCD) was originally proposed in DORL as a reference of the Matthew effect, which should be controlled in a suitable range. For readability, we still overbold the smallest MCD in each dataset. In addition, due to the definition of most popular categories in DORL, the calculation of MCD is not supported on Coat and Yahoo.\nWhile the testing set of KuaiRec is fully observable and can be directly used as the reward function during policy evaluation, the reward functions for the other three datasets are estimated with the testing data by DeepFM models. To ensure a fair comparison, we use exactly the same reward functions as in DORL on all datasets.\n6.1.3 Baselines. The baselines include five model-based offline RL methods, including two state-of-the-art, four model-free offline RL methods, and two vanilla bandit-based methods. To ensure a fair comparison, on each dataset, we use the same DeepFM [11] model to learn the world model for model-based offline RL methods."}, {"title": "6.2 Overall Performance (RQ1)", "content": "For our ROLeR", "hyperparameters": "the coefficients of the entropy penalty and the number of nearest neighbors. We simply tune k from 5 to 50, 25 to 200, 10 to 35, and 10 to"}]}]}