{"title": "ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for Recommender Systems", "authors": ["Yi Zhang", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "abstract": "Offline reinforcement learning (RL) is an effective tool for real-world recommender systems with its capacity to model the dynamic in- terest of users and its interactive nature. Most existing offline RL recommender systems focus on model-based RL through learning a world model from offline data and building the recommendation policy by interacting with this model. Although these methods have made progress in the recommendation performance, the effec- tiveness of model-based offline RL methods is often constrained by the accuracy of the estimation of the reward model and the model uncertainties, primarily due to the extreme discrepancy between offline logged data and real-world data in user interactions with online platforms. To fill this gap, a more accurate reward model and uncertainty estimation are needed for the model-based RL meth- ods. In this paper, a novel model-based Reward Shaping in Offline Reinforcement Learning for Recommender Systems, ROLER, is proposed for reward and uncertainty estimation in recommenda- tion systems. Specifically, a non-parametric reward shaping method is designed to refine the reward model. In addition, a flexible and more representative uncertainty penalty is designed to fit the needs of recommendation systems. Extensive experiments conducted on four benchmark datasets showcase that ROLeR achieves state-of- the-art performance compared with existing baselines. Source code can be downloaded at this address.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems (RS) serve as a forever-running engine on online platforms with large volumes of data to create personalised experiences for massive users through services like content recom- mendations and targeted advertising [2, 3]. The performance of the recommender systems significantly bolsters a company's market competitiveness, especially in industries where user engagement and retention are essential to the business.\nTo dynamically model user preference, over the past few years, reinforcement learning (RL) [38] has been incorporated into recom- mender systems by modeling the entire recommendation process as an interactive problem [1]. In this process, interactions between users and the recommender system are formulated as a sequence of states, actions, and rewards within an environment. The RL-based approaches can continuously learn and adapt from each user inter- action, being able to adjust quickly to changing user preferences and behaviors [13], optimizing users' long-term engagement [44]. Despite the effectiveness of RL, it is unrealistic for an uncompleted recommender system to perform expensive online interactions with users to collect training data [4, 5, 7, 10].\nAlthough these methods have made progress in the recommen- dation performance, the effectiveness of RL methods is often con- strained by the accuracy of the estimation of the reward model and the model uncertainties. This is because of the extreme discrep- ancy between offline data and real-world data on online platforms. Recent methods [7, 55] target at the Matthew Effect logged in the offline data with an entropy penalty to enhance the recommen- dation diversity. Others focus on the individual perspective for the filter bubble issue [10, 26] in user preference. The efficiency of these model-based offline reinforcement learning (RL) methods is frequently limited by the precision of the reward model estimation and uncertainty in the reward model, which are predominantly attributed to the significant disparity between offline logged data and real-world data in user interactions on online platforms. Re- visiting the literature of offline RL for RS, it is worth noting that neither behaving conservatively nor encouraging exploration can empower the policy to learn from offline data under the influence of an inaccurate reward estimation. In Figure 1, we demonstrate the influence of the accuracy of the reward functions on a bench- mark dataset - KuaiRec [8]. The reward prediction error of the state-of-the-art method, DORL [7], is generally high in all reward intervals in the red bar. An ideal situation would require the reward estimation error to be smaller for an effective RS development.\nTo address the aforementioned problems, we propose a novel model-based Reward Reshaping in Offline Reinforcement Learning method for Recommender Systems, ROLeR to improve the re- ward function during policy learning and decouple the uncertainty penalty with the ensemble of the world model. In RS, although the users' tastes may vary from person to person and change over time, finding similar users who share analogous interests is feasi- ble. Based on this intuition, we try to discover the patterns within the user-item interactions from offline data. We find that both the users' historical interaction data and the learned user embedding from the world model can be used as user indicator features. And if the indicator features of one user match that of another user, their feedback on a certain group of items can be mutually inferred. Thus, we regard a user's indicator feature as a soft label to retrieve its nearest neighbors. Then, the user's feedback on a certain item can be inferred by that from these neighbors. In other words, our reward shaping method is a non-parametric clustering-based one, simple yet effective. The accuracy of the reward function can be substantially improved. On the other hand, to cooperate with our reward shaping module, we develop an uncertainty estimation method based on the quality of the clustering. The distance be- tween a user and its nearest neighbors is used as the uncertainty, measuring the utility of current reward estimation. Therefore, this uncertainty penalty works as the complementary for the reward shaping part, releasing the need of an ensemble of world models. Our contributions are summarised as:\n\u2022 We find out the accuracy of the reward function prediction substantially determines the RS performance and propose a novel reward shaping method that is non-parametric for model-based offline RL for RS.\n\u2022 We develop a new uncertainty penalty that integrates with the reward shaping part for better generalization capacity and does not rely on an ensemble of world models.\n\u2022 We demonstrate the effectiveness of the proposed ROLeR on four recommendation benchmarks, KuaiRand, KuaiEnv, Coat and Yahoo, to showcase the superior performance among the state-of-the-art methods and strong baselines."}, {"title": "2 RELATED WORK", "content": "2.1 RL in Recommendation Systems\nSince supervised RSs have difficulties in capturing the dynamics of user preference [5, 32\u201335, 51], more deep RL [38] methods are em- ployed. Some directions [46, 52] focuses on formulating the RS to a Markov Decision Process (MDP), investigating the state representa- tion [14, 24], user profiling [22, 56], and action representation [23]. Further, based on the transition modeling, the literature can be divided into model-based methods and model-free methods. Model- based RL [49, 50] utilizes offline data to build the transition model. Further, model-free RL [6, 20] gains more attention especially when an online interaction environment is available, including Double DQN [40] to model the user dynamics [54] and utilizing a multi- agent setting [27] to tackle the sub-optimum issue [53].\n2.2 Offline Reinforcement Learning\nRecently, offline RL has attracted great attention from the research community [25, 31]. The gap between the learned policy and the real data results in the overestimation issue [19, 43]. To solve this problem, model-free methods regulate the policy learning to be con- servative. For instance, BCQ [6] avoid using the out-of-distribution (OOD) experience in policy iteration. Besides, CQL [20] constrains the usage of OOD data in the iteration of the state-action value function. CRR [45] limits the policy improvement conditioning on the discrepancy between the behavior policy and the learning policy. Though they achieve high accuracy in value estimation, they are also limited in policy improvement [48]. To deal with the extrapolate error introduced by the world model, recent methods such as MOREL [18] and MOPO [50] use the distances calculated through an ensemble of world models to penalize the overestimated values. COMBO [49] penalizes the rewards that tend to be OOD and controls the degree of pessimism based on a sampling distribution."}, {"title": "3 PRELIMINARIES", "content": "3.1 Interactive Recommendation\nThe interactive recommendation is a comprehensive simulation of real-world recommendations where the systems need to continu- ously recommend items to users.\n3.2 Reinforcement Learning Formulation\nReinforcement Learning (RL) aims to make a sequence of decisions to maximize the long-term return. RL problems can be formulated as a Markov Decision Process (MDP) with a tuple $G =< S, A, T, r, \\gamma >$. S is the state space and each $s \\in S$ refers to a specific state. A is the action space consisting of all the potential actions. T stands for the state transition of the environment as $T\\{S_{t}, a_{t}, S_{t+1}\\} = P(S_{t+1} | S = s_{t}, a = a_{t})$. It describes the dynamics of the environment. R is the reward function as $r_{t} = R(s_{t}, a_{t})$, representing the reward of taking action $a_{t}$ at state $s_{t}$. At last, $\\gamma$ is the discount factor used to balance the current reward and future return. The objective of RL is to learn a policy $\\pi$ which can maximize the long-term return: $G_{t} =$"}, {"title": "3.3 Offline Reinforcement Learning", "content": "Compared to expensive online interaction, offline data are usually abundant and easily accessible. Therefore, offline RL, which in- vestigates how to utilize offline datasets to train policies, attracts increasing attention in current research due to its potential effi- ciency. The offline dataset $D = \\{(S_{t}, a_{t}, S_{t+1}, r_{t})\\}$ is collected by one or more behavior policies $\\pi_{\\rho}$. One intuitive challenge of offline RL is aroused by the inevitable gap between the distribution of offline data and the evaluated environment. It is difficult for the learning policy to estimate the value function on rarely seen and even unseen states. Directly using the online RL methodologies tends to overes- timate the value functions on these states due to the maximization over available actions during decision making. The corresponding solutions can be categorized into two classes. The model-free of- fline RL adds constraints on the behavior policies and the learning policy to avoid risky decisions. But it limits the generation of of- fline RL. The model-based offline RL simulates the environments based on the offline datasets to enable the learning agent to interact with them. Then, they introduce the uncertainty estimation as a penalty in the reward function to encourage conservative actions by $r = \\hat{r} \u2013 \\lambda_{U}P_{U}$. This branch suits the RS since the offline inter- actions are highly sparse. Diving into model-based offline RL for RS, it first models both the transition function and reward function to obtain a world model denoted as $G\u2019 =< S, A, \\hat{T}, \\hat{R}, \\gamma >$, where $\\hat{T}$ and $\\hat{R}$ are the estimated transition and reward functions.\nIn addition, many recent efforts in this domain rely on an en- semble of world models to calculate uncertainty. It unnecessarily binds the world model learning and uncertainty penalty."}, {"title": "4 METHOD", "content": "4.1 Problem Definition\nConsidering reinforcement learning for the recommendation sys- tem, we demonstrate the problem formulation in this part. Recalling the MDP tuple $G =< S, A, T, r, \\gamma >$, each $s \\in S$ corresponds to a user state which usually consists of the users\u2019 side information such as personal interests and dynamic features like the recent interaction history. The action space A is the item set and each action a corre- sponds to one action $a_{t}$. The reward $r(s_{t}, a_{t})$ is from the feedback of recommending item $a_{t}$ at state $s_{t}$. It depends on the specific datasets, for instance, the reward can be the watched ratio in a short"}, {"title": "4.2 World Model Learning", "content": "In this part, we mainly focus on the simulation of an environment. The input for this stage is the offline history. They are used as the training set of a supervised prediction model. In DORL and CIRS, they use DeepFM. The outputs are the item embedding, user em- bedding, predicted reward and uncertainty and entropy estimation. Item Embedding, $e_{i}$, comes from the item ID and item features like the tags for music and categories for movies.\n$e_{i} = f_{I}(iid, F_{id}),$  (3)\nwhere $f_{I}$ is the item encoder and $F_{i}$ is the item features.\nUser Embedding, $e_{u}$, is similar to item embedding for user ID and other static features. It is also considered as time-invariant.\n$e_{u} = f_{u}(uid, F_{id}),$ (4)\nwhere $f_{u}$ is the user encoder and $F^{u}$ is the user features.\nReward Prediction is the core output of the world model. If we formulate the feedback of all the users towards all the items as a matrix, this matrix is usually highly sparse based on the offline interaction data, which is one typical characteristic of RS. The function of reward prediction, $\\hat{r}$, is to answer the \u201cwhat if\u201d queries and complement the matrix. However, the quality of this matrix complement and its influence have not been investigated before. In recent methods [7, 10], the reward is the average of multiple world models:\n$\\hat{r} = \\frac{1}{M} \\sum_{j=1}^{M} w_{j}(e_{i}, e_{u}),$ (5)\nwhere M is the number of world models, and $W_{j}$ is the j-th one. Uncertainty Penalty is widely used in offline RL to estimate the risk of taking a certain action. One representative direction utilizes an ensemble of world models to calculate the distances within the ensemble as a measure of uncertainty. In DORL, it formulates the world model as a Gaussian probabilistic model (GPM) and calculates the uncertainty of one interaction, x, as $P_{U}(x) := max_{k \\in E} \\frac{\\sigma_{x}^{2}}{\\sigma_{\\kappa}^{2}}$, where k is the index in world model ensemble E, and $\\sigma_{k}^{2}$ is the variance of corresponding GPM. Thus, the estimated reward in Equation (5) has been changed to:\n$\\hat{r}\u2019 = \\hat{r} \u2013 \\lambda_{u}P_{u},$  (6)\nwhere $\\lambda_{u}$ is the uncertainty coefficient to adjust its scale and the uncertainty estimation naturally binds with the world model. If the world model is inaccurate, the uncertainty will be impacted."}, {"title": "4.3 State Tracker", "content": "State tracker models the transition function, $s\u2019 = f (s, a, r)$, during the policy learning. It is also the state encoder that combines both the static and dynamic information as:\n$s\u2019 = f(s, e_{i}, a, \\hat{r}|e_{u}),$ (9)\nwhere $u_{i}, \\hat{r}$, and a come from the world model introduced in the last section. In the implementation of DORL, the static user embedding is not considered in its state tracker. In addition, the f is imple- mented as the average of recent w item embedding concatenating with the estimated rewards $\\hat{r}$, where w is named as window size:\n$S_{t+1} = \\frac{1}{w} \\sum_{j=t-w+1}^{t} [a_{j} (s_{j}, a_{j})].$ (10)\nSince the average tracker loses the order information, in our im- plementation, we turn to an attention tracker which enhances the cumulative reward in most cases."}, {"title": "4.4 Action Representation", "content": "In the current setting, the item embedding from the world model is used to initialize action representation during policy learning since each item corresponds to one action.\n$a_{t} = e_{i}.$ (11)\nOn one hand, the action representation also influences policy learn- ing. On the other hand, the inaccurate reward estimation from DeepFM intrigues us to doubt the quality of item embeddings. Thus, we find that replacing the current item embedding with a random initialization sampled from a standard normalization can enhance the cumulative rewards in many settings. Since this interesting di- rection is not our current focus, we leave it as our future direction."}, {"title": "4.5 Policy Learning Pipeline", "content": "In this part, we illustrate the policy learning process by interact- ing with the world model. Since the states are continuous, policy gradient or actor-critic algorithms are often used as baselines. We follow the implementation of DORL to build our method based on Advantage Actor-Critic [29] (A2C)."}, {"title": "4.6 Reward Shaping", "content": "The reward functions for most existing world models are inaccurate. The reason is also one of the main challenges of RS: the offline data is too sparse to comprehensively reflect users\u2019 true feedback. To overcome this problem, we need to dig out the intrinsic patterns in the offline data. Intuitive thought is that within a short time interval, some users may exhibit similar interests in a group of items, forming a cluster. Within this user cluster and the group of items, the feedback of a specific user toward a certain item may be inferred from the other users\u2019 feedback. Thus, based on our explo- ration and observation, a non-parametric reward shaping method that accounts for the specialties of RS is proposed to improve the prediction of the reward functions.\nUnlike training the reward model from extremely sparse offline data, user representation learning is comparably more informative thanks to the existence of static side information from users [42]. In addition, the user interaction history or the counterfactual interac- tion history can also be used to identify the users. We use this user information as their indicator features, denoted as u for brevity. For each user in the evaluation environment, we utilize its indicator features to retrieve similar users in the training environment based on an appropriate distance metric using a clustering method. Here we adopt a soft-label kNN to discover the user clusters in the offline data. Then, the reward correction is estimated by aggregating the feedback of these nearest neighbors:\n$\\bar{r}(s, a) = \\frac{1}{k} \\sum_{u' \\in kNN(u)} r_{u\u2019} (s, a),$ (14)\nIn this paper, we use averaging as our aggregation function. The choice of k depends on some statistics of the offline datasets. They will be elaborated in the experiment section."}, {"title": "4.7 Uncertainty Penalty", "content": "As many current uncertainty estimations in offline RL for RecSys rely on an ensemble of world models [7, 16], the uncertainty penalty inevitably suffers from the inaccurate prediction of the reward"}, {"title": "4.8 Algorithm Overview", "content": "Now the reward function for policy learning is derived as,\n$r = \\bar{r} \\times (1 \u2013 P_{u}) + \\lambda_{E}P_{E}.$  (16)\nWe summarize the overall training process in the Algorithm 1. In the evaluation process, given a user and its recent interaction trajectories, the state tracker calculates its state representation with Equation (10). Then, this representation along with the action representation obtained through Equation (11) are fed into the learned policy $\\pi_{\\theta}$. The policy recommends one item to the user. This process lasts until the user quits interaction."}, {"title": "5 PERFORMANCE LOWER BOUND", "content": "In the simulated MDP from the world model, $\\hat{G} =< S, A, T, \\hat{I}, \\gamma >$, the $\\hat{r}$ is estimated by our reward shaping method. Under the actor-critic framework, our analysis mainly focuses on the Bellman"}, {"title": "6 EXPERIMENT", "content": "In this section, we begin with introducing the experiment setup including the datasets we use, evaluation metric, the state-of-the- art method, and other baselines. Then, we investigate the following questions:\n\u2022 (RQ1) How does ROLeR perform compared with other base- lines?\n\u2022 (RQ2) How do the reward shaping and uncertainty penalty in ROLeR contribute to its performance?\n\u2022 (RQ3) What is the most effective design of the uncertainty penalty?\n\u2022 (RQ4) What is the impact of the world model in ROLeR?\n\u2022 (RQ5) Is ROLeR robust to the critical hyperparameters?\n6.1 Setup\n6.1.1 Datasets. We conduct experiments on four datasets including two newly proposed challenging short-video interaction datasets and two typical datasets. The statistics are listed in Table 1.\nKuaiRec [8] is a video dataset that contains a fully-observed user-item interaction matrix used for evaluation. Within this matrix, each user\u2019s feedback towards any item is known. The normalized viewing time of a video is used as a reward signal. The training data is from the standard interaction history containing popularity bias, which makes it difficult to learn a recommendation policy.\nKuaiRand-Pure [9] is also a video dataset that contains user- item interaction collected by randomly inserting the target videos in the standard recommendation stream. This part is used to simulate a fully observed interaction matrix like KuaiRec for evaluation, while"}, {"title": "6.2 Overall Performance (RQ1)", "content": "For our ROLeR, we have two key hyperparameters: the coefficients of the entropy penalty and the number of nearest neighbors. We simply tune k from 5 to 50, 25 to 200, 10 to 35, and 10 to 100 on KuaiRec, KuaiRand, Coat, and Yahoo, respectively.\nThe detailed performance on four datasets is listed in Table 2 and 3. The corresponding curves for KuaiRand and KuaiRec are in Figure 2. The first row is the main evaluation metric: cumulative reward followed by average interaction length and single-step reward as the second and third rows, respectively. We first analyze the results in a more specific view, i.e., the interaction length and the single- step reward. Then, we compile our observations on cumulative rewards, detailing the insights gleaned from the experiments."}, {"title": "6.3 Ablation Study (RQ2)", "content": "In this part, we investigate the significance of the proposed com- ponents: the non-parametric reward shaping and the uncertainty penalty on four datasets. Our experiments on KuaiRec are based on DORL with Transformer [41] state tracker inspired by SASRec [17] and Gaussian initialized item embedding, which are effective de- signs that can improve the performance of DORL. As for the other three datasets, these techniques cannot consistently increase the cumulative reward, so we continue to use the original DORL. Intu- itively, ROLeR has three variants. One adapts the reward shaping but keeps the world model based uncertainty penalty. We denote this version as ROLeR without kr. Another one keeps the world model predicted reward and changes to kNN-based uncertainty penalty denoted as ROLeR without ku. As described in Section 4.7, the uncertainty penalty collaborates with the kNN reward shaping and cannot be directly applied to the reward functions from world models. Thus, an alternative version, i.e., $\\hat{r} \u2013 \\lambda_{U}P_{U} + \\lambda_{E}P_{E}$, is tested. From Table 4, either the reward shaping method or uncertainty penalty can improve the cumulative rewards compared to DORL. This observation is especially obvious for ROLeR without ku, empha- sizing on the contribution of the non-parametric reward shaping."}, {"title": "6.4 The Design of Uncertainty Penalty (RQ3)", "content": "As illustrated in Section 4.7, the uncertainty penalty can adjust the way and degree of reward rectification to influence generalization. To investigate the most effective design of the uncertainty penalty, we conduct extensive experiments about the possible variants of the current uncertainty penalty across four datasets, and the results are shown in Table 5. We denote the distances between a user and its nearest neighbours as d in this table, and dmin/davg/dmax represents the minimum/mean/maximum of the distances. N(, ) is the Gaussian distribution. In addition, the $\\lambda$ for each variant is tuned and the optimal choice is different. We omit the subscript of $\\lambda$ and the entropy penalty, PE, for simplicity. The r for all variants in this table denotes the reward function after our reward shaping.\nIn the second line of Table 5, we use the inverse of the dis- tances as the uncertainty penalty to discount the probabilities of recommending highly uncertain items, i.e., $r \\times \\frac{1}{d}$. As it tends to penalize the uncertainty too sharply, it yields the smallest cumula- tive reward across four datasets. In the next line, sampling from a Gaussian distribution with the predicted r as mean and uncertainty penalty as the variance i.e., N(r, Ad), is also a conservative way to avoid risky recommendation. This variant implicitly penalizes the uncertainty by changing the deterministic reward into a Gaussian- distributional reward. Unfortunately, this variant does not perform robustly across four datasets. The following three variants, i.e., r \u2013 dmin/davg/dmax, serve as intuitive ways to adjust the degree of the uncertainty penalty. The corresponding results demonstrate that such a direct design is not sufficient to estimate the uncer- tainty in offline model-based RL for RecSys. The last second variant achieves the highest cumulative reward on KuaiRec. We believe it benefits from the special coverage of KuaiRec compared to the other datasets. As the training set of KuaiRec is comparably dense, the distances between a user and its nearest neighbors are capable"}, {"title": "6.5 The Impact of World Model (RQ4)", "content": "When summarizing the utility of the world model in policy learn- ing, it trains the item embedding used as the initialization of action representation, predicts the user-item feedback as the estimated rewards, and estimates the uncertainty. In implementing ROLeR on KuaiRec, our reward model and uncertainty penalty are estimated from the offline data. We find that both initializing the action repre- sentation from a standard distribution and applying a Transformer state tracker can improve the cumulative reward in most of our test- ing settings as exemplified in Table 6. In this table, att and avg mean the transformer state tracker and average state tracker, respectively. ag and at mean whether to initialize the action representation at the beginning of policy training or not. We believe the inaccuracy of the world model does not only hurt the estimation of the reward function but the action representation. In addition, as recent inter- action history is available, the former Avg. state tracker used in DORL can hardly capture the order information. That is the reason we turn to the Transformer state tracker on KuaiRec. However, this is not the case for the other three datasets. The original testing matrix for KuaiRand is extremely sparse (see Table1). To make it a fully observed evaluation environment, the user-item feedback matrix is completed through emulation. We hypothesise that may explain why the order information is not so informative."}, {"title": "6.6 Hyperparameter Sensitivity(RQ5)", "content": "In ROLeR, the major hyperparameter we introduced is the num- ber of nearest neighbors k. Thus, we investigate the influence of k on the cumulative rewards. Considering the size and sparsity of the datasets, we test ROLeR in [5, 10, 15, 20, 30, 40, 50] for KuaiRec, [25, 50, 75, 100, 150, 200] for KuaiRand, [10, 15, 20, 25, 30, 35] for Coat, and [10, 20, 40, 60, 80, 100] for Yahoo."}, {"title": "7 CONCLUSION", "content": "In this paper, we identify that the reward function prediction is inaccurate in the world model of current model-based offline rein- forcement learning for recommender systems. We verify that this"}, {"title": "8 ACKNOWLEDEGMENTS", "content": "This work is supported by projects DE200101610, CE200100025 funded by Australian Research Council, and CSIRO\u2019s Science Leader project R-91559."}]}