{"title": "A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration", "authors": ["Zhang Zheng", "Wu Hengyang"], "abstract": "This paper proposes a knowledge-enhanced disease diagnosis method based on a prompt learning framework. The method retrieves structured knowledge from external knowledge graphs related to clinical cases, encodes it, and injects it into the prompt templates to enhance the language model's understanding and reasoning capabilities for the task. We conducted experiments on three public datasets: CHIP- CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the proposed method significantly outperforms existing models across multiple evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC dataset, 3.1% on the IMCS-V2-NER dataset, and 4.2% on the KUAKE-QTR dataset. Additionally, ablation studies confirmed the critical role of the knowledge injection module, as the removal of this module resulted in a significant drop in F1 score. The experimental results demonstrate that the proposed method not only effectively improves the accuracy of disease diagnosis but also enhances the interpretability of the predictions, providing more reliable support and evidence for clinical diagnosis.", "sections": [{"title": "Introduction", "content": "Disease diagnosis is the process of systematically identifying and confirming a patient's illness. It involves analyzing the patient's symptoms, medical history, physical examination results, and data obtained from various medical tests, such as laboratory tests and imaging studies. The purpose of disease diagnosis is to determine the cause of the illness, thereby guiding physicians in selecting appropriate treatment methods and increasing the patient's chances of recovery. Disease diagnosis holds critical importance in medicine. It serves as the foundation for formulating treatment plans, allowing doctors to prescribe targeted treatment regimens and avoiding misdiagnosis and over-medication. Moreover, accurate diagnosis helps prevent further deterioration and spread of diseases, particularly in the control of infectious diseases. On the other hand, disease diagnosis can enhance the efficiency of medical resource utilization, reduce unnecessary tests and treatments, and lower healthcare costs.\nInitially, people primarily relied on doctors' experience and knowledge for diagnosis, using methods such as patient interviews, physical examinations, and laboratory tests. However, this simple and me- chanical approach clearly could not produce optimal diagnostic results. To address issues of subjectivity, time consumption, and risks of misdiagnosis or missed diagnosis, knowledge engineering methods were later adopted [14]. These methods utilize rule-based matching techniques to determine text categories.However, in recent years, with the rapid development of technologies such as big data and cloud computing, the internet has experienced an explosive growth in information. The vast amount of textual data has posed significant challenges to traditional classification methods. In response to this, efforts have been made to develop new algorithms and models that integrate multi-source information, including clinical data, medical imaging, and biomarkers, to enhance the accuracy and reliability of diagnoses. For example, Luo et al. [8] proposed a new multi-modal heterogeneous graph model to represent medical data, which effectively addresses the challenge"}, {"title": "Related work", "content": "The continuous advancement of information technology has driven the development of intelligent clinical decision support systems. The widespread application of machine learning models has significantly improved the effectiveness of these systems, particularly in the field of disease prediction. The evolution of these models has gone through several stages: from the early expert rule-based models, to models based on statistical analysis and case-based reasoning, and finally to the current advanced models utilizing machine learning and deep learning techniques[6].At the same time, breakthroughs in the field of natural language processing (NLP) have introduced innovative tools and techniques, opening up new perspectives for disease diagnosis and providing unprecedented possibilities."}, {"title": "Disease Diagnosis", "content": "In these early disease diagnosis methods, rule-based research primarily relied on the analysis of medical literature and case data. Expert rule-based disease diagnosis methods involved the collection of expert di- agnostic experiences to form disease diagnosis pathways, thus creating expert systems. A typical example of such an expert system is the MYCIN expert system[18], developed by Shortliffe in 1976, which became a foundational model for many subsequent expert systems in the medical field. However, many techniques are unable to explain the processes involved in disease monitoring and data inference. To address this, Aami and Sarfraz [1] proposed a fuzzy rule-based diabetes classification system, which combines fuzzy logic with the cosine amplitude method. They developed two fuzzy classifiers, and the proposed model demonstrated high predictive accuracy.Sanz et al. [16] further developed a new approach based on the Fuzzy Rule-Based Classi- fication System (FRBCS) by integrating it with Interval-Valued Fuzzy Sets (IV-FRBCS). They validated the applicability and effectiveness of this method in medical diagnostic classification problems, demonstrating its potential in improving diagnostic accuracy in complex medical scenarios. With the accumulation of large- scale clinical data, statistical analysis has become an important method for disease diagnosis. Researchers use statistical analysis to uncover potential correlations between patient characteristics and medical indica- tors, thereby providing new perspectives and approaches for disease prediction and diagnosis. As a result,"}, {"title": "Application of the BERT Model in Disease Diagnosis", "content": "BERT (Bidirectional Encoder Representations from Transformers), proposed by Google in 2018, is an ad- vanced pre-trained natural language processing (NLP) model. This model has achieved state-of-the-art re- sults in various NLP tasks, including but not limited to text classification, named entity recognition (NER), and question-answering systems (QA). Recently, the research community has begun exploring the potential of the BERT model in the medical field. By integrating multi-source information such as clinical texts and medical literature, the BERT model can absorb and learn rich medical knowledge, thereby providing more precise auxiliary information in the disease diagnosis process. The bidirectional contextual modeling capa-"}, {"title": "Task formulation", "content": "Given clinical information and electronic medical records of a patient, where w\u2081 represents the i-th word in the text and N represents the total number of words, this research aims to predict the disease type \u0177 that the patient has based on the content of x. Therefore, this task can be represented as learning a model f(.) with parameters 0. Given an input x, f(x; 0) outputs a predicted result \u0177 \u2208 V. Here, Y is the set of labels for all candidate disease types.\nThis paper utilizes prompt learning to accomplish the task, converting it from a classification problem into a language modeling problem. The original classification problem is formulated as fitting Y = P(X;0). The transformed language modeling problem is formulated as fitting Y = (T(X); 0), where T(X) represents the prompt template used to encapsulate the original text into a new input."}, {"title": "Methodology", "content": ""}, {"title": "Knowledge Retrieval", "content": "Clinical texts contain many conceptual and structured forms of knowledge, such as symptoms, diagnoses, and treatment plans. Knowledge retrieval is capable of identifying and extracting key medical concepts and relationships. For example, \"Type 2 diabetes\" is a common chronic disease, characterized primarily by persis- tently elevated blood glucose levels. The diagnostic criteria include fasting blood glucose and HbA1c levels. Common symptoms include excessive thirst, hunger, frequent urination, weight loss, and fatigue. Through knowledge retrieval, it can extract structured knowledge such as causes, related complications, risk factors,"}, {"title": "Knowledge Representation", "content": "The reasoning paths from the collection are concatenated into a single text sequence. This concatenated text is then represented as a vector k using a model."}, {"title": "Reasoning Path Representation", "content": "Each reasoning path P (gi, gj) is converted into a readable text sequence, with each path consisting of a se- ries of triples (gi, r, gj). These triples can be transformed into text, where each triple is expressed in the form of \"Entity 1 reaches Entity 2 through Relationship.\" For example, given a reasoning path (g1,1,g2), (g2, I2, g3), it can be converted into the text: \"Entity g\u2081 reaches Entity g2 through Relationship r\u2081, and g2 reaches En- tity g3 through Relationship r2. \"Assuming the knowledge graph contains the following triples: (\"Type 2 Diabetes\", \"causes\", \"High Blood Sugar\"), (\"High Blood Sugar\", \"leads to\u201d, \u201cKidney Disease\"), the con- version to text would be: \"Type 2 Diabetes reaches High Blood Sugar through causes, and reaches Kidney Disease through leads to.\""}, {"title": "Text Vectorization Model Selection (BERT)", "content": "The input text Tp is first tokenized by BERT, splitting the input text into smaller subword units. For example, the input text \"Type 2 diabetes passed causes to high blood sugar, pass leads to Kidney disease\u201d might be tokenized into subwords like \"Type 2\", \"diabetes\", \"pass\", \"causes\", \"to\", \"Kidney disease\u201d, \u201cto\u201d, and so on.Subsequently, each token is converted into an ID from BERT's vocabulary. After tokenization, the next step involves encoding, where the token ID sequence is transformed into embedding vectors. These embedding vectors are then fed into BERT's multi-layer Transformer encoder, effectively capturing the contextual relationships and dependencies between the tokens. The formula for this process is as follows:\nfBERT: Xp = fBERT (TP) \nWhere:\nTp is the input text sequence.\nxp is the text vector representation output by the BERT model.\nThe function fBERT transforms the text sequence Tp into its vector representation xp. Here, xp contains the semantic information of the input text Tp.\nThe BERT model outputs the vector representation of the text Tp as:\nkp M (xp)"}, {"title": "Output Vector Extraction", "content": "In the text vectorization model selection mentioned earlier, we used the BERT model to convert the input text sequence Tp into the vector representation xp. During this process, we typically choose to use the special token[CLS], and its corresponding vector is used as the overall representation of the entire input text. This comprehensive representation is denoted as kp[0]."}, {"title": "Prompt Template", "content": ""}, {"title": "Construction of Prompt Template", "content": "To effectively utilize the BERT model for text vectorization, we first need to preprocess the original input text x to construct a proper prompt template Template (x) that meets the input format requirements of the BERT model. This prompt template will generate a text x' that contains special tokens and specific vocabulary. The text x' contains special tokens such as [MASK], soft tokens, and other relevant vocabulary."}, {"title": "Expression of Template Conversion", "content": "x' = Template (x) = [w\u2081, soft, w2, mask, W3,...]\nHere, wi represents either words from the original text or other words added based on the context. Suppose we want to convert the original text x into the template x'. This can be achieved by inserting special tokens and soft labels. A specific example is as follows:\nOriginal input text x: \"Type 2 diabetes is a chronic metabolic disease.\"\nConverted preprocessed text x': \"Type 2 diabetes is a[MASK]type[SOFT]disease.\u201d\nIt can be represented as:\nx' = Template (x) = [\"Type 2 diabetes\u201d, soft, \u201cis a\u201d, mask, \u201ctype\u201d, soft, \u201cdisease\".]"}, {"title": "Vector Representation and Processing", "content": "Using the pre-trained model M, the preprocessed text x' is transformed into a set of vectors {k1, ksoft,k2,kmask,k3, ...}. Each vector ki corresponds to a word or token in the preprocessed text x', where:\nksoft is the vector corresponding to the soft token.\nkmask is the vector corresponding to the mask token.\nFor example, for the original input text x: \"Type 2 diabetes is a chronic metabolic disease.\", the generated vector set can be explained as follows:\nk1: The vector corresponding to the first word in the original text, \"Type 2 diabetes\".\nksoft: The vector corresponding to the soft token.\nk2: The vector corresponding to the second word in the original text, \"is a\".\nkmask: The vector corresponding to the mask token.\nk3: The vector corresponding to the third word in the original text, \"disease\".\nOther vectors follow similarly, with each vector representing the respective words or tokens in the pre- processed text."}, {"title": "Vector Processing", "content": "The formula for replacing the original ksoft vector with the average of ksoft and other related vectors ki is as follows:\nknewsoft = 1/(n+1) * (ksoft + \u2211(i=1 to n) ki)\nWhere: knew is the updated ksoft vector,\nksoft is the original soft token vector,\nki represents related vectors (e.g., corresponding to related tokens or words),\nn is the number of related vectors used in the calculation.\nFinally, after replacing the original ksoft with knew, we obtain the updated vector set: {k1, knewsoft , k2,kmask, k3,...}, which will be used in the next steps of the model processing."}, {"title": "Prediction", "content": "The text x' containing the mask and the new soft token is input into a pre-trained language model for forward inference. This produces representations for each token. The representation at the mask position is then used for a verbalizer prediction."}, {"title": "", "content": "Given the modified input x', it is formalized as:\nx' = [W1, softnew,w2, mask,W3,...]\nFeed x' into the pre-trained language model M, and perform forward inference to obtain the vector representations for each token:\n{k1, knewsoft ,k2,kmask,k3,...}\nExtract the representation kmask of the mask token from the model output.\nUse a verbalizer function V to map kmask to a specific vocabulary term. The Verbalizer V is a mapping from the vector space to the vocabulary, typically used to convert the model's predicted vector into a specific word.\nThe specific steps are as follows:\npredicted word = V (kmask)\n(1) Calculate the similarity between kmask and the embedding vectors of each word in the vocabulary to obtain a probability distribution. For example, if the embedding vector for each word in the vocabulary is ei, then the probability distribution is given by:\nsimilarity=kmaskei\n(2) Based on the computed similarities, select the word with the highest probability as the prediction result."}, {"title": "Experimental settings", "content": ""}, {"title": "Datasets", "content": "In this paper, experiments were conducted on the CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR datasets. CHIP-CTC, one of the experimental datasets, originates from a bench-marking task released at the CHIP2019 conference. All text data is sourced from real clinical trials, including 22,962 entries in the training set, 7,682 entries in the validation set, and 10,000 entries in the test set. The dataset is available at https: //github.com/zonghui0228/chip2019task3.\nThe IMCS-V2-NER dataset, used as another experimental dataset, comes from the Named Entity Recog- nition task in the IMCS2 dataset developed by the School of Data Science at Fudan University. It includes 2,472 entries in the training set, 833 entries in the validation set, and 811 entries in the test set. The dataset is available at https://github.com/lemuria-wchen/imcs21.\nThe KUAKE-QTR dataset, used as another experimental dataset, includes 24,174 entries in the training set, 2,913 entries in the validation set, and 5,465 entries in the test set. The dataset is available at https: //tianchi.aliyun.com/dataset/95414."}, {"title": "Baseline", "content": "When evaluating our proposed method, we established a comprehensive set of baseline models to ensure rigorous and fair comparison. These baselines were selected to represent robust benchmarks in the field, including both traditional machine learning algorithms and advanced deep learning techniques.\nSVM: Support Vector Machines (SVM) differentiate between classes by finding the optimal hyperplane, effectively handling both linearly separable and non-linearly separable problems. This classifier uses kernel techniques to process text data, enabling efficient identification and classification of complex text patterns in tasks such as sentiment analysis and spam detection.\nCNN: Convolutional Neural Networks (CNNs) extract local features from text data using convolutional layers in text classification tasks. These local features capture key semantic information within sentences or documents, helping the model understand and classify the text content. This ability of CNNs is particularly well-suited for scenarios where quick and effective identification of key information from text is required.\nRNN: Recurrent Neural Networks (RNNs) are particularly well-suited for text classification tasks due to their ability to process sequential text data and capture long-term dependencies between words. This allows RNNs to understand contextual information within the text, leading to more accurate predictions of text categories, such as sentiment or topic classification.\nBiLSTM: Bidirectional Long Short-Term Memory (BiLSTM) networks combine both forward and back- ward LSTMs, allowing them to consider both preceding and succeeding contextual information in the text. This structure makes BiLSTMs particularly effective for text classification tasks as they can capture tem- poral dependencies in the text data comprehensively. By doing so, BiLSTMs provide a deeper semantic understanding, resulting in higher accuracy across various text classification scenarios.\nAttention: In text classification tasks, the Attention mechanism enables the model to focus on key parts of the text, enhancing its understanding of the overall content's importance. By assigning different weights to various words or phrases, it emphasizes the information most influential for the classification task.\nBiRNN: Bidirectional Recurrent Neural Networks (BiRNN) combine both forward and backward RNNs, allowing them to consider contextual information from both directions in a text sequence. This structure enables BiRNNs to better capture long-term dependencies in text data, thereby enhancing classification performance. It is particularly well-suited for tasks that require a comprehensive understanding of text semantics, such as sentiment analysis and topic classification.\nBERT: BERT (Bidirectional Encoder Representations from Transformers) plays a crucial role in text classification tasks. By utilizing pre-trained models to understand the semantics and context of text, and through fine-tuning techniques, BERT can capture rich textual features and improve classification accu- racy. Its bidirectional encoding and context-aware capabilities enable BERT to excel across various text classification scenarios.\nThe inclusion of these baselines aims to provide a clear reference point for evaluating the performance of our new method. By comparing with these established models, we aim to demonstrate the advancements of our approach relative to existing solutions. Bench-marking against these well-established models helps highlight scenarios where our method offers superior performance, thereby validating its effectiveness and efficiency in addressing the problem at hand."}, {"title": "Results and Analysis", "content": "This section presents our experimental results and provides an analysis of these results. To ensure the reliability of the experiments, we repeated each experiment three times and used the average values as the final results."}, {"title": "Comparison Experiments", "content": "The results indicate that as the complexity of the models increases, the accuracy improves accordingly across different datasets. SVM, as a classical machine learning model, can handle linearly separable data, but it struggles to capture more complex relationships when datasets have intricate features and patterns. CNN performs better than SVM on the CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR datasets, as it excels at extracting local key information compared to SVM.The experimental results indicate that although the accuracy improves compared to SVM, the overall performance remains relatively low. CNNs show limited effectiveness in handling natural language tasks, particularly in capturing long-range dependencies. The Attention mechanism proves to be more effective at focusing on crucial parts of the input sequence, thereby enhancing the model's performance with sequential data. Compared to CNNs, Attention provides greater flexibility in highlighting key aspects of the data, resulting in improved accuracy. Additionally, BiRNNs exhibit significantly improved performance across various datasets. Unlike traditional RNNs, BiRNNs utilize two separate RNNs-forward RNN and backward RNN-to process the input sequence, which contributes to their superior performance.BiRNN addresses the limitation of traditional RNNs, which can only utilize past context and cannot leverage future context. By incorporating bidirectional processing, BiRNNs offer enhanced context understanding and feature extraction capabilities. The bidirectional feature extraction enables BiRNNs to better capture complex patterns and semantic relationships within sequences, thus im- proving the model's predictive performance.\nIn comparison, BiLSTM further enhances these capabilities by employing LSTM units with gating mech- anisms that effectively mitigate the vanishing gradient problem. This allows BiLSTM to capture long-range dependencies and bidirectional context more effectively. Experimental results demonstrate that BiLSTM achieves higher accuracy than BiRNN, highlighting its advantages in handling complex sequence processing tasks.Compared to BiLSTM and other models, BERT leverages the self-attention mechanism in the Trans- former architecture to simultaneously consider both left and right context information. Trained on extensive corpora and fine-tuned for specific tasks, BERT's multi-layer self-attention mechanism captures deeper and more nuanced features. This results in superior feature extraction capabilities compared to RNNs, BiRNNs, and BiLSTMs, making BERT more effective in understanding and processing complex textual data.\nFrom the last row of the table, it is evident that our method achieved the best performance across all four datasets. This demonstrates the feasibility and effectiveness of our approach. Furthermore, it highlights the significant impact of integrating medical knowledge into the model through the prompt-learning framework, which notably enhances model performance."}, {"title": "Ablation Study", "content": "This section aims to analyze the effectiveness of each module in our proposed method. To achieve this, we first remove the knowledge representation component from our method. As seen in the first row of Table 3, removing the knowledge representation results in a significant drop of 0.2 in the F1 score. This is because knowledge representation is a core component of our method, which injects structured knowledge obtained from the knowledge graph into the language model, thereby enhancing the model's understanding of medical domain knowledge and improving performance on medical diagnostic tasks.\nSubsequently, we modified the prompt template, changing it to: \"The characteristics of Type 2 Diabetes"}, {"title": "Model Interpretability Study", "content": "This section aims to analyze the interpretability of the model. Unlike general text classification tasks, in disease diagnosis tasks, users are additionally concerned with the interpretability of the results. An unexplainable prediction result is unacceptable to users. Therefore, we conduct an analysis of the model's interpretability, using clinical case examples from Table X.It can be observed that during the knowledge retrieval phase, the following knowledge was extracted from the clinical case: \"Type 2 diabetes is a common chronic disease,\" \"its main feature is persistently elevated blood sugar levels,\" \"polydipsia, polyphagia, and"}, {"title": "Conclusion", "content": "In this paper, we propose a knowledge-enhanced disease diagnosis method based on a prompt learning framework. This method leverages an external knowledge graph to retrieve relevant knowledge from clinical cases and then encodes this structured knowledge into prompt templates. By incorporating this encoded knowledge, the language model's understanding of the task is improved, resulting in more accurate disease diagnosis outcomes. Experimental results demonstrate that the proposed method effectively enhances the performance of language models in disease diagnosis tasks. Additionally, the model exhibits strong inter- pretability, providing users with supporting evidence related to the diagnostic results.\nIn the future, we will explore additional methods for knowledge injection. Additionally, we plan to investigate more advanced knowledge editing techniques to integrate medical knowledge into the reasoning process of language models."}]}