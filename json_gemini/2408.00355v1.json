{"title": "DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training", "authors": ["Yu Xie", "Qian Qiao", "Jun Gao", "Tianxiang Wu", "Shaoyao Huang", "Jiaqing Fan", "Ziqiang Cao", "Zili Wang", "Yue Zhang", "Jielei Zhang", "Huyang Sun"], "abstract": "More and more end-to-end text spotting methods based on Transformer architecture have demonstrated superior performance. These methods utilize a bipartite graph matching algorithm to perform one-to-one optimal matching between predicted objects and actual objects. However, the instability of bipartite graph matching can lead to inconsistent optimization targets, thereby affecting the training performance of the model. Existing literature applies denoising training to solve the problem of bipartite graph matching instability in object detection tasks. Unfortunately, this denoising training method cannot be directly applied to text spotting tasks, as these tasks need to perform irregular shape detection tasks and more complex text recognition tasks than classification. To address this issue, we propose a novel denoising training method (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we decompose the queries of the denoising part into noised positional queries and noised content queries. We use the four Bezier control points of the Bezier center curve to generate the noised positional queries. For the noised content queries, considering that the output of the text in a fixed positional order is not conducive to aligning position with content, we employ a masked character sliding method to initialize noised content queries, thereby assisting in the alignment of text content and position. To improve the model's perception of the background, we further utilize an additional loss function for background characters classification in the denoising training part. Although DNTextSpotter is conceptually simple, it outperforms the state-of-the-art methods on four benchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially yielding an improvement of 11.3% against the best approach in Inverse-Text dataset.", "sections": [{"title": "1. Introduction", "content": "Text Spotter, as an essential foundational technology that encompasses text detection and recognition, plays a critical role in various domains [6, 32, 39, 49] such as autonomous driving, security monitoring, and social media analysis. Given that textual elements in wild scenarios are often set against complex backgrounds, presented in diverse font sizes, and subject to distortions, accurate identification of text remains a challenging and dynamic field of research. To address these challenges, traditional CNN-based text spotters [1, 23, 26, 29, 36, 40, 42] divide the text localization task into separate detection and recognition stages, following a detect-then-recognize principle."}, {"title": "2. Related Works", "content": "Early literature tends to classify end-to-end text spotting architecture into two-stage methods and one-stage methods. Recently, due to the popularity of transformer-based text spotters, we categorize these methods into CNN-based methods and transformer-based methods. Earlier comprehensive and in-depth surveys on text spotting are available in [6, 32]."}, {"title": "2.1. CNN-based Text Spotter", "content": "The first end-to-end scene text recognition network [20] combines detection and recognition into a single system. This method is limited to recognizing regular-shaped text. Subsequent works [1, 10] improve the connection between the detector and recognizer, considering single characters or text blocks to handle irregular text more flexibly. The Mask TextSpotter series [23, 33] employs segmentation approaches to generate proposals. These methods rely on character-level annotations, significantly increasing the effort required for generating ground truth. Text Perceptron [36] and Boundary [40] utilize Thin-Plate-Spline [4] transformation to rectify features obtained from curved text. The ABCNet series [27, 29] use BezierAlign to address the problem of curved text, requiring the prediction of a small fixed number of points.\nWhile these methods achieve good performance, they require additional RoI-based [11] or TPS-based connectors, and the only shared part between the detector and recognizer is the backbone network's features, neglecting the collaborative nature of detection and recognition. [53] propose ARTS, highlighting the importance of collaborative detection and recognition in the text spotting task. Lastly, all of the aforementioned methods require complex manual operations like Non-Maximum Suppression (NMS)."}, {"title": "2.2. Transformer-based Text Spotter", "content": "With the impressive success of Transformers [38] in visual tasks [8, 9, 17, 21, 30, 31, 46, 51], also influenced by the DETR family [5, 19, 25, 34, 50, 54], more recent works explore Transformer-based structures for the Text Spotting. TESTR [52] employs dual decoders for detection and recognition tasks, sharing the backbone and Transformer encoder features. TTS [18] utilizes an encoder and a decoder with multiple prediction heads for performing multi-tasks. DeepSolo [48] employs an explicit points method to model decoder queries.\nAlthough these methods achieve promising results, they still exhibit certain limitations. The random initialized queries used in TESTR [52] and TTS [18] still lack clarity and fail to efficiently represent queries encompassing both positional and semantic aspects. Utilizing the encoder's output features, DeepSolo [48] generates Bezier center curve proposals, which subsequently serve to generate positional queries. It effectively decouples the ambiguously defined queries into positional queries and content queries. Although these methods have achieved certain accomplishments, the use of bipartite graph matching algorithms to obtain one-to-one matching results has been proven to have a negative impact on detection tasks by DN-DETR [19] and DINO [50]. Based on this, ESTextSpotter [15] attempted to use the denoising training method [50]. However, this method uses boxes as a positional prior for point prediction, failing to consider the irregular attributes of text instances and the characteristics of text scripts. Therefore, we design a denoising training approach based on Bezier control points and characters."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Preliminaries", "content": "Denoising Training. The denoising training method was first proposed by DN-DETR to address the slow convergence issue of DETR. This method constructs an additional auxiliary task in the decoder section without bipartite matching, which can be used to accelerate the convergence of DETR-like methods. Technically, it additionally feeds noised ground-truth boxes and labels into the transformer decoder to reconstruct these ground-truths, and this part is updated through an additional auxiliary DN loss. DINO improved the denoising training method and further proposed a contrastive denoising training method, which adds negative queries in addition to the original noised queries to predict the background. In our method, we further extend the denoising training approach based on this foundation.\nBezier Center Curve. ABCNet [27] was the first to use Bezier curves to flexibly adapt to any shape of scene text with a small number of fixed points. Subsequently, DeepSolo introduced the Bezier Center Curve, which initializes transformer decoder queries by uniformly sampling a fixed number of points on the curve. This Bezier Center Curve is obtained by calculating the average of the four Bezier control points on the top and bottom edges of each text instance. In our method, we consider utilizing Bezier center curves in denoising training."}, {"title": "3.2. Query Initialization", "content": "The ambiguous meaning of decoder queries in DETR is often interpreted in existing literature [25, 50] as a combination of positional queries and content queries. We also utilize this modeling approach and follow DeepSolo, using the Bezier Center Curve to initialize positional queries and then combine them with learnable content queries. In the denoising part, we represent noised queries as two parts: noised positional queries and noised content queries.\nNoised Positional Queries. As shown in Fig. 3, for any text instance, the four Bezier control points of the Bezier center curve. Assuming that a picture contains N text instances, we can represent the set of all instances as: $S = {S_j|j = 1,2,..., N}$, $S_j$ is a set of the four Bezier control points of the Bezier center curve of a text instance. Each instance can be represented as:\n$S_j = {(X_{ij}, Y_{ij})|i = 0,1,2,3}$,\nwhere each point is composed of a pair of coordinates $(X_{ij}, Y_{ij})$, and the index i ranges from 0 to 3, indexing the four Bezier control points. Random noise is added to these points to obtain:\n$S' = {(X_{ij} + \\Delta X_{ij}, Y_{ij} + \\Delta Y_{ij})|i = 0, 1, 2, 3}$,\nwhere $\\Delta X_{ij}$ and $\\Delta Y_{ij}$ are obtained by calculating the distance between the four Bezier control points of the center curve and the four Bezier control points on the top side and are denoted as $D_{x}^{ij}$ and $D_{y}^{ij}$. We use elements $a_{ij}$ and $B_{ij}$ from sets that satisfy $a < 1$ from uniform distribution to control the noise ratio for the x and y coordinates respectively. Therefore, we can represent the offset of the coordinates as follows:\n$\\Delta x_{ij} = {\\begin{cases}(-1)^{m}a_{ij}D_{x}^{ij} \\qquad \\text{if positive};\n(-1)^{m}(a_{ij}+1)D_{x}^{ij} \\qquad \\text{otherwise}.\\end{cases}}$\n$\\Delta y_{ij} = {\\begin{cases}(-1)^{m}B_{ij}D_{y}^{ij} \\qquad \\text{if positive};\n(-1)^{m}(B_{ij}+1)D_{y}^{ij} \\qquad \\text{otherwise}.\\end{cases}}$\nAfter adding noise to the Bezier control points, we obtain a new set of control points $S'$. Using these noisy Bezier control points, we uniformly sample T point coordinates from the resulting Bezier curves. These point coordinates form tensor Points with shape (N, T, 2). Finally, these coordinates are processed through positional encoding (PE) and two layers of MLP to obtain Noised Positional Queries ($Q_N$) with a shape of (N, T, 256). We represent $Q_N$ as follows:\n$Q_N = MLP(PE(Points)).$\nSection 4.4 analyzes 'Why add noise to the Bezier control points?'.\nNoised Content Queries. We designed Mask Character Sliding (MCS), as shown in Fig. 4, to initialize the noised content queries. For the maximum recognition length T, which is equal to the T mentioned above, we perform a sliding operation on the valid characters in the positive part. Specifically, we first determine the number of valid characters t, which refers to the number of characters in the input sequence that actually have meaning. Then, we calculate the number of times each valid character should be cloned by performing a division operation $\\lceil \\frac{T}{t} \\rceil$, in order to evenly distribute the total length T of the sequence to each valid character. Additionally, since the division of T by t may not be exact, there will be a remainder $k = T - \\lceil \\frac{T}{t} \\rceil$, indicating that there are k additional spaces that need to be allocated. To fairly distribute these extra spaces, we assign"}, {"title": "3.3. Single Attention Mask", "content": "To ensure that during the decoder self-attention calculation, the information in the denoising part contains ground-truth information, we need to ensure that the information in the matching part cannot see the information in the denoising part. In addition, each group should not be able to see each other. Considering that there are two parts during self-attention calculation, namely intra-relation self-attention which calculates the attention relationship between characters, and inter-relation self-attention which calculates the attention relationship between text instances. We consider whether two attention masks are needed to prevent information leakage. However, in fact, for intra-relation self-attention which calculates the attention relationship between characters, we do not need to use an attention mask, because, for any text instance (including the denoising part and the matching part), the attention calculation is within the text instance and does not interact with other text instances. So we only need to consider the design of inter-relation self-attention, we call this attention mask that only needs to be used once as Single Attention Mask, and we devise the attention mask A = $[a_{ij}]_{(g+2n)\u00d7(g+2n)}$ as follows:\n$A_{ij} = \\begin{cases}\n1, & \\text{if } j < g \\times 2n \\text{ and } \\lceil \\frac{i}{2} \\rceil \\neq \\lceil \\frac{j}{2} \\rceil;\n1, & \\text{if } j < g \\times 2n \\text{ and } i \\geq g \\times 2n;\n0, & \\text{otherwise}.\n\\end{cases}$\nwhere g and n represent the number of groups and the number of text instances per image, respectively. $a_{ij} = 1$ means the i-th query is blind to the j-th query; if it's 0, they can see each other."}, {"title": "3.4. Training Losses", "content": "Compared to the matching part, the denoising part uses a slightly modified loss function and an additional background computation loss function (namely, the background part is calculated twice, which we briefly refer to as BCT). The rest of the loss functions are consistent with Deep-Solo [48], including the Hungarian matching algorithm, focal loss, CTC loss [12], and L1 loss.\nWe use focal loss [24] to calculate the classification of text instances. In each set of denoising queries, the positive part represents positive samples, and the negative part represents negative samples, with the focal loss being used to compute the background loss for the first time. Therefore, for the T-th query in the positive part of the denoising queries, the calculation of the focal loss for text instance classification is as follows:\n$L_{cls}^{(T)} = -\\mathds{1}_{{T \\in I_m(\\varphi)}}\\alpha (1 - \\hat{o}(\\tau))^{\\gamma}\\log(\\hat{o}(\\tau))\n- \\mathds{1}_{{T \\notin I_m(\\varphi)}}(1 - \\alpha) (\\hat{o}(\\tau))^{\\gamma}\\log(1 - \\hat{o}(\\tau))$,\nwhere 1 represents the indicator function, and $I_m(\\varphi)$ denotes the image of the mapping $\\varphi$. Concerning character classification, for the T-th denoised query in the positive part, we employ the CTC loss:\n$L_{text,pos}^{(T)} = \\mathds{1}_{{T \\in I_m(\\varphi)}}CTC(t(\\varphi^{-2}(+)), f(t)).$\nThe cross entropy loss for the K-th denoised query in the negative part during the second background calculation:\n$L_{text,neg}^{(K)} = \\mathds{1}_{{K \\in I_m(\\varphi)}}CE(t(\\varphi^{-2}(k)), f(k)).$"}, {"title": "4. Experiment", "content": "We conduct comparisons with known Transformer-based approaches on various datasets, including Total-Text, SCUT-CTW1500, ICDAR15, and InverseText which contain multi-directional scene text and arbitrary-shaped text instances. In addition, We choose publicly available datasets Synth150K, MLT17, IC13, and TextOCR as additional pre-training datasets."}, {"title": "4.1. Public Datasets", "content": "Total-Text is a widely used comprehensive scene text dataset introduced by [28], specifically designed for arbitrary text detection. It comprises 1255 training images and 300 testing images, containing horizontal, multi-directional, and arbitrary-shaped text instances.\nSCUT-CTW1500 is another significant dataset for arbitrary-shaped text, published by [7]. It comprises 1500 images, consisting of 1000 training images and 500 testing images.\nICDAR2015 Incidental Text (ICDAR15) [16] includes 1000 training images and 500 testing images with quadrilateral text. It contains multi-directional text instances annotated with word-level quadrilateral annotations.\nInverseText was manually annotated by [47] and includes 500 test images. Unlike the previous datasets used in detection tasks [2, 3], 40% of this dataset's text instances are inverse-like, specifically designed to address the lack of such texts in existing test datasets."}, {"title": "4.2. Implementation Details", "content": "All settings are based on the ResNet-50 backbone. We employ 6 layers of encoder and 6 layers of decoder, with a hidden dimension of 256. For character classification, we predict 37 classes on the Total Text, ICDAR15, and InverseText datasets, and 96 classes on the CTW1500 dataset. During training, we set the noise hyperparameters as follows: The probability A of characters being flipped to other characters is set to 0.4. The learning rate scheduler utilizes an initial learning rate of 2e-5 for the backbone and 2e-4 for other parts. We train the DNTextSpotter for a total of 435k steps, and the learning rate is reduced by a factor of 0.1 at 375k steps. We use AdamW as the optimizer and train our network with a batch size of 8. The denoising part loss weights $\\lambda_{cls}, \\lambda_{coord}, \\lambda_{bd}, \\lambda_{text,pos},$ and $\\lambda_{text,neg}$ are set to 1.0, 1.0, 0.5, 0.5, and 0.5. The focal loss parameters $\\alpha$ and $\\gamma$ are set to 0.25 and 2.0, respectively."}, {"title": "4.3. Comparison with State-of-the-Art Methods", "content": "For Arbitrarily-Shaped Scene Text Spotting: As previously indicated, the Total-Text and SCUT-CTW1500 datasets are specifically designed to emphasize text instances characterized by arbitrary shapes. In comparison to other methods on the Total-Text dataset (shown in Table 1), in the detection task, our approach is close to the current state-of-the-art method, ESTextSpotter, achieving 89.2%. While its detection performance is slightly lower, it has a significant advantage in recognition performance. Without a lexicon ('None' results), outperforming ESTextSpotter by 3.7%, reaching 84.5%, and with a lexicon ('Full' results), exceeding it by 2.7%, achieving 89.8%. Compared to the state-of-the-art method in recognition performance, DeepSolo, is higher by 2.0% and 1.1%, respectively. On the CTW1500 dataset, both detection and recognition performances have reached the current state-of-the-art. The detection F1 score reached 90.2%, surpassing the state-of-the-art method ESTextSpotter by 0.2%. Without a lexicon ('None' results), it exceeds ESTextSpotter by 2.1%, and with a lexicon ('Full' results), it surpasses by 0.3%, reaching 84.2%. Compared to our baseline model, DeepSolo, there is a significant improvement in performance for detection and recognition, with increases in F1, 'None', and 'Full' by 0.9%, 2.8%, and 2.8%, respectively. We use only"}, {"title": "4.4. Ablation Studies", "content": "Ablation experiments were conducted on the Total-Text dataset. Table 4 further demonstrates the convergence effects of the improved denoising training. Table 6 shows the impact of noise scale and mask probability. Table 7 shows the effects of adding noise to Bezier control points(BCP), using masked character sliding(MCS), and calculating an additional background loss(BCT).\n(1) Effect of BCP: We investigate the addition of noise on the Bezier control points rather than directly on the sampling points of the Bezier center curve. Using BCP leads to a noticeable improvement in 'F1' results, by approximately 0.8%. We think BCP can contribute positional prior information to the noised positional queries, which is not achieved by directly adding noise to the sampling points on the centerline. Conversely, if noise is directly added to these sampling points, the denoising training segment would miss out on the benefits of the smooth Bezier curve's positional priors, thereby negatively impacting the training outcome.\n(2) Effect of MCS: Generating queries directly without MCS leads to a significant decrease in performance because it forces the model to learn a fixed postional output order. Introducing the sliding operation means that the model no longer learns targets based on a fixed position, which promotes a one-to-one alignment between character and position. The experimental results also confirm the effectiveness of MCS, showing a 1.6% increase in 'None' results.\n(3) Effect of BCT: Employing additional background loss calculation techniques (BCT) leads to improvements in both F1 scores (+0.2%) and 'None' results (+0.4%). BCT is actually a reuse of the negative part. Originally, the focal loss was used solely to calculate the loss for binary classification between the foreground and background. Now, an additional cross-entropy loss requires that each character in the negative part undergo multi-class classification.\n(4) Effect of Noise Scale and Mask Probability: The ablation experiments presented in Table 5 and Table 6 indicate that choosing an appropriate noise ratio is crucial, as both excessive and insufficient noise can impact the results. We conduct ablation experiments on the hyperparameter A for randomly flipping characters, with the experimental results showing the performance when A is set between 0.0 and 0.8. Furthermore, we control the noise scale A at 0.4 to conduct ablation experiments on the mask probability. The experimental results show that either excessive or insufficient noise can affect the model's performance."}, {"title": "4.5. Qualitative Analysis", "content": "Instability Measurement: We utilize the analysis method for quantifying the instability(IS) of bipartite graph matching proposed by DN-DETR [19]. The calculation formula can be found in the appendix. For a training image in the TotalText dataset, we calculate the indices of N proposals every 10k iterations, with every 10k iterations considered as one group. By comparing the differences in indices between the i-th group and the (i + 1)-th group, we obtain the results for IS. We visualize the IS results as shown in Fig. 6. Additionally, the training set of the Total Text\ncontains a total of 1255 training images, with an average of 7.04 text instances per image, so the largest possible IS is 7.04 x 2 = 14.08.\nVisualization Comparisons: Fig. 5 illustrates the experimental results on InverseText. ESTextSpotter significantly struggles with the recognition of inverse-like text. Even for some texts, distortions and deformations occur during detection. Although DeepSolo has greatly improved the recognition of these inverse-like texts, it faces challenges in recognizing all the more dense texts. As for DNTextSpotter, we achieve good performance on most of the inverse-like texts, indicating that denoising training has an increasingly\npositive effect on more complex tasks. More detailed visualization results can be found in the appendix."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel denoising training method based on the attributes of scene text. Our research indicates that devising a denoising training method that aligns the positions and contents of characters is highly effective. Future research could focus on developing denoising training methods that further align with task-specific characteristics to improve model performance. We hope our approach offers valuable insights for other researchers."}, {"title": "Appendix", "content": null}, {"title": "A. Detection Results on Inverse-Text", "content": "In addition to comparing the recognition results of 'None' and 'Full', we also supplement the comparison of detection results here. We compare some mainstream end-to-end methods. DNTextSpotter employs the same data augmentation and the same additional pre-training datasets as DeepSolo, a mixture of Synth150K, MLT17, Total-Text, IC13, IC15, and TextOCR. After fine-tuning on the Total-Text for 2k iterations, DNTextSpotter directly applies these updated weights to assess performance on the Inverse-Text dataset. It consistently outperforms current state-of-the-art methods, as shown in Table 8, achieving 94.3% precision, 77.2% recall, and an F1-score of 84.9%."}, {"title": "B. Details of the Instability Measurement", "content": "We analyze the instability of bipartite graph matching used by DN-DETR [19]. In the main text, we group every 10k iterations as one group. We adopt this setting here as well. For a training image, we represent the predicted text instances from transformer decoders at the i-th group\nas $P^{i} = {P^{i}_{0}, P^{i}_{1}, ..., P^{i}_{N-1}}$, where N signifies the total\ncount of detected text instances, and the M ground truth text\ninstances are denoted as $G = {G_{0}, G_{1}, G_{2}, ..., G_{M-1}}$.\nAfter bipartite matching, we generate a vector $W^{i} = {W^{i}_{0}, W^{i}_{1}, ..., W^{i}_{N-1}}$ for the i-th iteration to capture the\nmatching outcomes, defined by:\n$W^{i}_{n} = {\\begin{cases}\nm, & \\text{if } P^{i}_{n} \\text{ matches } G_{m}\n-1, & \\text{if } P^{i}_{n} \\text{ matches nothing}\n\\end{cases}}$\nThe stability for a single training image at iteration i is then\ndetermined by the variance between its $W^{i}$ and $W^{i+1}$, cal-\nculated as:\n$IS^{2} = \\sum_{k=0}^{N}\\mathds{1}(W^{i}_{n} \\neq W^{i+1}_{n})$\nHere, $\\mathds{1}(.)$ stands for the indicator function, where $\\mathds{1}(z) = 1$\nif z is true, and 0 otherwise. The overall stability for it-\neration i across the dataset is obtained by averaging these\nstability values for all images."}, {"title": "C. Single Attention Mask", "content": "We further present the attention mask in a graphical form\nto facilitate a better understanding for the readers. The attention mask A = $[a_{ij}]_{(g+2n)\u00d7(g+2n)}$ is shown in the main\ntext as follows:\n$A_{ij} = {\\begin{cases}\n1, & \\text{if } j < g \\times 2n \\text{ and } \\lceil \\frac{i}{2} \\rceil \\neq \\lceil \\frac{j}{2} \\rceil;\n1, & \\text{if } j < g \\times 2n \\text{ and } i \\geq g \\times 2n;\n0, & \\text{otherwise}.\n\\end{cases}}$"}, {"title": "E. Limitation and Discussion", "content": "Although DNTextSpotter achieves quite good performance, there are still some limitations. The most significant is the excessive overhead during training. Compared to the original vector shape of (bs, 100, 25, 256), during denoising training, the maximum shape can reach (bs, 200, 25, 256). Given that the computational complexity of the self-attention mechanism increases quadratically, the increased computational cost when the sequence length grows from 100 to 200 is non-negligible. DNTextSpotter was trained using 8 NVIDIA Tesla H800 GPUs, requiring approximately 26 hours of training time. Fortunately, the denoising training does not add any overhead during inference, making it a worthwhile method for actual deployment and application. Additionally, we have only applied the denoising training method to the evaluation of English scene text datasets and have not experimented with Chinese. We look forward to DNTextSpotter achieving similarly good results on Chinese datasets as well."}]}