{"title": "DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training", "authors": ["Yu Xie", "Qian Qiao", "Jun Gao", "Tianxiang Wu", "Shaoyao Huang", "Jiaqing Fan", "Ziqiang Cao", "Zili Wang", "Yue Zhang", "Jielei Zhang", "Huyang Sun"], "abstract": "More and more end-to-end text spotting methods based on Transformer architecture have demonstrated superior performance. These methods utilize a bipartite graph matching algorithm to perform one-to-one optimal matching between predicted objects and actual objects. However, the instability of bipartite graph matching can lead to inconsistent optimization targets, thereby affecting the training performance of the model. Existing literature applies denoising training to solve the problem of bipartite graph matching instability in object detection tasks. Unfortunately, this denoising training method cannot be directly applied to text spotting tasks, as these tasks need to perform irregular shape detection tasks and more complex text recognition tasks than classification. To address this issue, we propose a novel denoising training method (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we decompose the queries of the denoising part into noised positional queries and noised content queries. We use the four Bezier control points of the Bezier center curve to generate the noised positional queries. For the noised content queries, considering that the output of the text in a fixed positional order is not conducive to aligning position with content, we employ a masked character sliding method to initialize noised content queries, thereby assisting in the alignment of text content and position. To improve the model's perception of the background, we further utilize an additional loss function for background characters classification in the denoising training part. Although DNTextSpotter is conceptually simple, it outperforms the state-of-the-art methods on four benchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially yielding an improvement of 11.3% against the best approach in Inverse-Text dataset.", "sections": [{"title": "1. Introduction", "content": "Text Spotter, as an essential foundational technology that encompasses text detection and recognition, plays a critical role in various domains such as autonomous driving, security monitoring, and social media analysis. Given that textual elements in wild scenarios are often set against complex backgrounds, presented in diverse font sizes, and subject to distortions, accurate identification of text remains a challenging and dynamic field of research. To address these challenges, traditional CNN-based text spotters divide the text localization task into separate detection and recognition stages, following a detect-then-recognize principle."}, {"title": null, "content": "Compared to these classical spotting algorithms, building on the Deformable DETR, TESTR represents a significant advancement in text spotting with its dual decoder design. This novel approach streamlines the process by removing the necessity for manually designed components and eliminating intermediate steps. TESTR simplifies the complex tasks of detection and recognition by treating them as a unified set prediction problem. It leverages bipartite graph matching to concurrently assign labels for both detection and recognition, achieving a more efficient and integrated workflow. Despite the significant achievements of TESTR, the employment of two decoders significantly increases computational complexity. Moreover, initializing queries with distinct properties for detection and recognition poses challenges for model optimization. Many methods have been proposed to address these issues. For example, TTS attempts to unify the detection and recognition tasks within a single decoder. DeepSolo, while using a single decoder, introduces a novel query form with shared parameters, which initializes decoder queries by utilizing a series of explicit coordinates generated from the text line. However, despite further improvements in performance, these methods overlook the instability introduced by the bipartite matching employed in the DETR-like methods. In general object detection tasks, DN-DETR firstly points out the instability problem of bipartite graph matching when using the DETR architecture and proposes denoising training to solve this problem. Denoising training, in simple terms, initializes noised queries using ground truth with a small amount of noise added, allowing for direct loss calculation with the ground truth after decoding, bypassing the bipartite graph matching algorithm. DINO further proposes a contrastive denoising training method to further enhance the performance of denoising training. Unfortunately, for the tasks of spotting scene text, the challenge is significantly amplified due to the arbitrary shapes of the text to be detected and the need for recognition tasks that are more complex than mere classification. This complexity makes it difficult to directly apply this denoising training method. In fact, ESTextSpotter directly incorporates a DINO-based denoising training method within its model architecture. In this context, as shown by the convergence curves in Fig. 1, this denoising training starts with regular bounding boxes as queries initialization, and the results on inverse-like texts become very poor, indirectly reflecting the negative impact brought by this coarse prior."}, {"title": null, "content": "In this paper, we propose a novel denoising training method specifically designed for transformer-based text spotters that handle arbitrary shapes. Considering that the task of text spotting aims at the detection and recognition of text in any shape, and using regular boxes to initialize noised queries is coarse, we abandon the traditional approach that relies on 4D anchor boxes and classification labels. Instead, we use Bezier control points and text characters to initialize noised queries. Technically, we feed the noised queries obtained from the ground truth along with randomly initialized learnable queries into the decoder. We design the noised query using bezier control points of the bezier center curve and text scripts, thereby accomplishing the denoising of both points coordinates and text characters. In addition, considering that outputting text characters in a fixed positional order is not conducive to aligning position with content, we use a masked character sliding method to initialize noised content queries before initializing the text script as noised queries. This method assists in aligning the content and position of the text instance."}, {"title": null, "content": "We verify the effectiveness of DNTextSpotter by using multiple public datasets. In the metrics of 'None' results with ResNet-50 backbone, compared with the current state-of-the-art methods, our method achieves 2.0% and 2.1% improved results on the Total Text and CTW1500 datasets respectively, reaching 84.5% and 67.0% respectively. On the newly released benchmark Inverse-Text dataset, our method even exceeds the state-of-the-art results by 11.3%, reaching 75.9%. When switching to the ViTAEv2-S backbone, scores for all metrics are further improved."}, {"title": null, "content": "Our main contributions can be summarized as follows:"}, {"title": null, "content": "\u2022 We introduce a novel denoising training method to design an end-to-end text spotting architecture. Starting from the attribute of arbitrary shapes of scene text, we utilize bezier control points as well as text characters to design this denoising training method."}, {"title": null, "content": "\u2022 Taking into account the negative impact of directly using ground truth text scripts to initialize noised queries, which leads to misalignment between the position of the characters and the content of these characters, we design a masked character sliding method to preprocess these ground truth text scripts, thereby optimizing the alignment between text position and content."}, {"title": null, "content": "\u2022 Our method achieves state-of-the-art results on multiple benchmarks. Specifically, we conduct a qualitative analysis of several text spotting architectures based on the transformer structure, including analyses of instability results and visualization of results."}, {"title": "2. Related Works", "content": "Early literature tends to classify end-to-end text spotting architecture into two-stage methods and one-stage methods. Recently, due to the popularity of transformer-based text spotters, we categorize these methods into CNN-based methods and transformer-based methods. Earlier comprehensive and in-depth surveys on text spotting are available in [6, 32]."}, {"title": "2.1. CNN-based Text Spotter", "content": "The first end-to-end scene text recognition network [20] combines detection and recognition into a single system. This method is limited to recognizing regular-shaped text. Subsequent works [1, 10] improve the connection between the detector and recognizer, considering single characters or text blocks to handle irregular text more flexibly. The Mask TextSpotter series [23, 33] employs segmentation approaches to generate proposals. These methods rely on character-level annotations, significantly increasing the effort required for generating ground truth. Text Perceptron [36] and Boundary [40] utilize Thin-Plate-Spline [4] transformation to rectify features obtained from curved text. The ABCNet series [27, 29] use BezierAlign to address the problem of curved text, requiring the prediction of a small fixed number of points."}, {"title": null, "content": "While these methods achieve good performance, they require additional RoI-based [11] or TPS-based connectors, and the only shared part between the detector and recognizer is the backbone network's features, neglecting the collaborative nature of detection and recognition. [53] propose ARTS, highlighting the importance of collaborative detection and recognition in the text spotting task. Lastly, all of the aforementioned methods require complex manual operations like Non-Maximum Suppression (NMS)."}, {"title": "2.2. Transformer-based Text Spotter", "content": "With the impressive success of Transformers [38] in visual tasks, also influenced by the DETR family, more recent works explore Transformer-based structures for the Text Spotting. TESTR [52] employs dual decoders for detection and recognition tasks, sharing the backbone and Transformer encoder features. TTS [18] utilizes an encoder and a decoder with multiple prediction heads for performing multi-tasks. DeepSolo [48] employs an explicit points method to model decoder queries."}, {"title": null, "content": "Although these methods achieve promising results, they still exhibit certain limitations. The random initialized queries used in TESTR [52] and TTS [18] still lack clarity and fail to efficiently represent queries encompassing both positional and semantic aspects. Utilizing the encoder's output features, DeepSolo [48] generates Bezier center curve proposals, which subsequently serve to generate positional queries. It effectively decouples the ambiguously defined queries into positional queries and content queries. Although these methods have achieved certain accomplishments, the use of bipartite graph matching algorithms to obtain one-to-one matching results has been proven to have a negative impact on detection tasks by DN-DETR [19] and DINO [50]. Based on this, ESTextSpotter [15] attempted to use the denoising training method [50]. However, this method uses boxes as a positional prior for point prediction, failing to consider the irregular attributes of text instances and the characteristics of text scripts. Therefore, we design a denoising training approach based on Bezier control points and characters."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Preliminaries", "content": "Denoising Training. The denoising training method was first proposed by DN-DETR to address the slow convergence issue of DETR. This method constructs an additional auxiliary task in the decoder section without bipartite matching, which can be used to accelerate the convergence of DETR-like methods. Technically, it additionally feeds noised ground-truth boxes and labels into the transformer decoder to reconstruct these ground-truths, and this part is updated through an additional auxiliary DN loss. DINO improved the denoising training method and further proposed a contrastive denoising training method, which adds negative queries in addition to the original noised queries to predict the background. In our method, we further extend the denoising training approach based on this foundation. Bezier Center Curve. ABCNet [27] was the first to use Bezier curves to flexibly adapt to any shape of scene text with a small number of fixed points. Subsequently, DeepSolo introduced the Bezier Center Curve, which initializes transformer decoder queries by uniformly sampling a fixed number of points on the curve. This Bezier Center Curve is obtained by calculating the average of the four Bezier control points on the top and bottom edges of each text instance. In our method, we consider utilizing Bezier center curves in denoising training."}, {"title": "3.2. Query Initialization", "content": "The ambiguous meaning of decoder queries in DETR is often interpreted in existing literature [25, 50] as a combination of positional queries and content queries. We also utilize this modeling approach and follow DeepSolo, using the Bezier Center Curve to initialize positional queries and then combine them with learnable content queries. In the denoising part, we represent noised queries as two parts: noised positional queries and noised content queries."}, {"title": "Noised Positional Queries", "content": "As shown in Fig. 3, for any text instance, the four Bezier control points of the Bezier center curve. Assuming that a picture contains N text instances, we can represent the set of all instances as: $S = \\{S_j|j = 1,2,..., N\\}$, $S_j$ is a set of the four Bezier control points of the Bezier center curve of a text instance. Each instance can be represented as:"}, {"title": null, "content": "$$S_j = \\{(X_{ij}, Y_{ij})|i = 0,1,2,3\\},$$"}, {"title": null, "content": "where each point is composed of a pair of coordinates $(X_{ij}, Y_{ij})$, and the index i ranges from 0 to 3, indexing the four Bezier control points. Random noise is added to these points to obtain:"}, {"title": null, "content": "$$S' = \\{(X_{ij} + \\Delta X_{ij}, Y_{ij} + \\Delta Y_{ij})|i = 0, 1, 2, 3\\},$$"}, {"title": null, "content": "where $\\Delta x_{ij}$ and $\\Delta y_{ij}$ are obtained by calculating the distance between the four Bezier control points of the center curve and the four Bezier control points on the top side and are denoted as $D_{x}^{ij}$ and $D_{y}^{ij}$. We use elements $\\alpha_{ij}$ and $\\beta_{ij}$ from sets that satisfy $\\alpha_ij \\in [0, 1]$ uniform distribution to control the noise ratio for the x and y coordinates respectively. Therefore, we can represent the offset of the coordinates as follows:"}, {"title": null, "content": "$$\\Delta x_{ij} =\\begin{cases} ((-1)^m\\alpha_{ij}D_{x}^{ij}, &\\text{if positive};\\\\ ((-1)^m(\\alpha_{ij} + 1)D_{x}^{ij}, &\\text{otherwise}. \\end{cases}$$"}, {"title": null, "content": "$$\\Delta y_{ij} =\\begin{cases} ((-1)^m\\beta_{ij}D_{y}^{ij}, &\\text{if positive};\\\\ ((-1)^m(\\beta_{ij} + 1)D_{y}^{ij}, &\\text{otherwise}. \\end{cases}$$"}, {"title": null, "content": "where m and n are used to control the direction of the offset, with \"positive\" indicating the coordinates of the positive part."}, {"title": null, "content": "After adding noise to the Bezier control points, we obtain a new set of control points S', Using these noisy Bezier control points, we uniformly sample T point coordinates from the resulting Bezier curves. These point coordinates form tensor Points with shape (N, T, 2). Finally, these coordinates are processed through positional encoding (PE) and two layers of MLP to obtain Noised Positional Queries (QN) with a shape of (N, T, 256). We represent QN as follows:"}, {"title": null, "content": "$$Q_N = MLP(PE(Points)).$$"}, {"title": null, "content": "Section 4.4 analyzes 'Why add noise to the Bezier control points?'."}, {"title": "Noised Content Queries", "content": "We designed Mask Character Sliding (MCS), as shown in Fig. 4, to initialize the noised content queries. For the maximum recognition length T, which is equal to the T mentioned above, we perform a sliding operation on the valid characters in the positive part. Specifically, we first determine the number of valid characters t, which refers to the number of characters in the input sequence that actually have meaning. Then, we calculate the number of times each valid character should be cloned by performing a division operation $\\lceil\\frac{T}{t}\\rceil$, in order to evenly distribute the total length T of the sequence to each valid character. Additionally, since the division of T by t may not be exact, there will be a remainder $k = T - \\lceil\\frac{T}{t}\\rceil$, indicating that there are k additional spaces that need to be allocated. To fairly distribute these extra spaces, we assign one additional clone to each of the first k valid characters in the sequence, ensuring that the allocation for each character is as even as possible. Since this operation visually resembles a character sliding operation, we name it Character Sliding. After sliding the characters, we use a mask operation to control the number of consecutive characters, flipping a portion of the consecutive characters into a background label with a certain probability. After processing the positive part, we add noise to the characters in both the positive part and the negative part, causing these characters to flip to other characters with a probability of A. These characters are then transformed into noised content queries after being embedded, with all the characters initialized in the negative part being backgrounds."}, {"title": null, "content": "In addition, we use a dynamic group g to fully utilize the performance of denoising training. Considering the computational cost, we set the maximum number of text instances N per image to 100. When N exceeds 100, we simply use the slicing method to take the first 100 instances. The division of g is as follows:"}, {"title": null, "content": "$$g = \\begin{pmatrix} 5, \\frac{100}{N} \\end{pmatrix}.$$"}, {"title": "3.3. Single Attention Mask", "content": "To ensure that during the decoder self-attention calculation, the information in the denoising part contains ground-truth information, we need to ensure that the information in the matching part cannot see the information in the denoising part. In addition, each group should not be able to see each other. Considering that there are two parts during self-attention calculation, namely intra-relation self-attention which calculates the attention relationship between characters, and inter-relation self-attention which calculates the attention relationship between text instances. We consider whether two attention masks are needed to prevent information leakage. However, in fact, for intra-relation self-attention which calculates the attention relationship between characters, we do not need to use an attention mask, because, for any text instance (including the denoising part and the matching part), the attention calculation is within the text instance and does not interact with other text instances. So we only need to consider the design of inter-relation self-attention, we call this attention mask that only needs to be used once as Single Attention Mask, and we devise the attention mask $A = [a_{ij}]_{(g+2n)\\times(g+2n)}$ as follows:"}, {"title": null, "content": "$$A_{ij}=\\begin{cases} 1, & \\text{if } j < g \\times 2n \\text{ and } \\lfloor \\frac{i}{g} \\rfloor \\ne \\lfloor \\frac{j}{g} \\rfloor;\\\\ 1, & \\text{if } j < g \\times 2n \\text{ and } i \\ge g \\times 2n;\\\\ 0, & \\text{otherwise}. \\end{cases}$$"}, {"title": null, "content": "where g and n represent the number of groups and the number of text instances per image, respectively. $a_{ij} = 1$ means the i-th query is blind to the j-th query; if it's 0, they can see each other."}, {"title": "3.4. Training Losses", "content": "Compared to the matching part, the denoising part uses a slightly modified loss function and an additional background computation loss function (namely, the background part is calculated twice, which we briefly refer to as BCT). The rest of the loss functions are consistent with Deep-Solo [48], including the Hungarian matching algorithm, focal loss, CTC loss [12], and L1 loss."}, {"title": null, "content": "We use focal loss [24] to calculate the classification of text instances. In each set of denoising queries, the positive part represents positive samples, and the negative part represents negative samples, with the focal loss being used to compute the background loss for the first time. Therefore, for the T-th query in the positive part of the denoising queries, the calculation of the focal loss for text instance classification is as follows:"}, {"title": null, "content": "$$L^{(T)}_{cls} = - \\frac{1}{n} \\begin{cases} \\mathbb{1}\\{T \\in I_m(\\varphi)\\}\\alpha(1 - \\gamma^{(T)})^{\\gamma}log(\\gamma^{(T)}) \\\\ - \\mathbb{1}\\{T \\notin I_m(\\varphi)\\} (1 - \\alpha) (1 - \\gamma^{(T)})^{\\gamma}log(1 - (\\gamma^{(T)}) \\end{cases},$$"}, {"title": null, "content": "where 1 represents the indicator function, and $I_m(\\varphi)$ denotes the image of the mapping $\\varphi$. Concerning character classification, for the T-th denoised query in the positive part, we employ the CTC loss:"}, {"title": null, "content": "$$L_{text,pos} = \\frac{1}{n}\\mathbb{1}\\{T \\in I_m(\\varphi)\\}CTC(t(\\varphi^{-2}(T)), f(t)).$$"}, {"title": null, "content": "The cross entropy loss for the K-th denoised query in the negative part during the second background calculation:"}, {"title": null, "content": "$$L_{text,neg} = \\frac{1}{n}\\mathbb{1}\\{K \\in I_m(\\varphi)\\}CE(t(\\varphi^{-2}(K)), f(k)).$$"}, {"title": null, "content": "Additionally, for the coordinate points of the center curve and boundaries in the positive part of the T-th denoised query, we employ the L1 loss for the computation:"}, {"title": null, "content": "$$L_{coord}^{(T)} = \\frac{1}{\\mathbb{1}\\{T \\in I_m(\\varphi)\\}} \\sum_{n=0}^{N-1}||p^{-1}(T) - p_n^{(T)}||,$$"}, {"title": null, "content": "$$L_{bd}^{(T)} = \\frac{1}{\\mathbb{1}\\{T \\in I_m(\\varphi)\\}} \\sum_{n=0}^{N-1} (||top(\\varphi^{-1}(T))- top_n^{(T)}|| + ||bot(\\varphi^{-1}(T)) - bot_n^{(T)}||),$$"}, {"title": null, "content": "where $top$ refers to the top curves of the boundaries, and $bot$ refers to the bottom curves of the boundaries. The negative part does not participate in the calculation of the part."}, {"title": null, "content": "The loss function for the denoised queries consists of four aforementioned losses in the positive part and two aforementioned losses in negative part:"}, {"title": null, "content": "$$L_{pos} = \\sum_{T} (\\lambda_{cls}^{(T)}L_{cls}^{(T)} + \\lambda_{text,pos}^{(T)}L_{text,pos}^{(T)} + \\lambda_{coord}^{(T)}L_{coord}^{(T)} + \\lambda_{bd}^{(T)}L_{bd}^{(T)}),$$"}, {"title": null, "content": "$$L_{neg} = \\sum_{K} (\\lambda_{cls}^{(K)}L_{cls}^{(K)} + \\lambda_{text,neg}^{(K)}L_{text,neg}^{(K)}),$$"}, {"title": null, "content": "where $\\lambda_{cls}$, $\\lambda_{text}$, $\\lambda_{coord}$, $\\lambda_{bd}$ are hyper-parameters to balance different tasks. The final loss function of the denoising part is:"}, {"title": null, "content": "$$L_{dn} = L_{pos} + L_{neg}.$$"}, {"title": "4. Experiment", "content": "We conduct comparisons with known Transformer-based approaches on various datasets, including Total-Text, SCUT-CTW1500, ICDAR15, and InverseText which contain multi-directional scene text and arbitrary-shaped text instances. In addition, We choose publicly available datasets Synth150K, MLT17, IC13, and TextOCR as additional pre-training datasets."}, {"title": "4.1. Public Datasets", "content": "Total-Text is a widely used comprehensive scene text dataset introduced by [28], specifically designed for arbitrary text detection. It comprises 1255 training images and 300 testing images, containing horizontal, multi-directional, and arbitrary-shaped text instances. SCUT-CTW1500 is another significant dataset for arbitrary-shaped text, published by [7]. It comprises 1500 images, consisting of 1000 training images and 500 testing images. ICDAR2015 Incidental Text (ICDAR15) [16] includes 1000 training images and 500 testing images with quadrilateral text. It contains multi-directional text instances annotated with word-level quadrilateral annotations. InverseText was manually annotated by [47] and includes 500 test images. Unlike the previous datasets used in detection tasks [2, 3], 40% of this dataset's text instances are inverse-like, specifically designed to address the lack of such texts in existing test datasets."}, {"title": "4.2. Implementation Details", "content": "All settings are based on the ResNet-50 backbone. We employ 6 layers of encoder and 6 layers of decoder, with a hidden dimension of 256. For character classification, we predict 37 classes on the Total Text, ICDAR15, and InverseText datasets, and 96 classes on the CTW1500 dataset. During training, we set the noise hyperparameters as follows: The probability A of characters being flipped to other characters is set to 0.4. The learning rate scheduler utilizes an initial learning rate of 2e-5 for the backbone and 2e-4 for other parts. We train the DNTextSpotter for a total of 435k steps, and the learning rate is reduced by a factor of 0.1 at 375k steps. We use AdamW as the optimizer and train our network with a batch size of 8. The denoising part loss weights $\\lambda_{cls}$, $\\lambda_{coord}$, $\\lambda_{bd}$, $\\lambda_{text,pos}$, and $\\lambda_{text,neg}$ are set to 1.0, 1.0, 0.5, 0.5, and 0.5. The focal loss parameters $\\alpha$ and $\\gamma$ are set to 0.25 and 2.0, respectively."}, {"title": "4.3. Comparison with State-of-the-Art Methods", "content": "For Arbitrarily-Shaped Scene Text Spotting: As previously indicated, the Total-Text and SCUT-CTW1500 datasets are specifically designed to emphasize text instances characterized by arbitrary shapes. In comparison to other methods on the Total-Text dataset (shown in Table 1), in the detection task, our approach is close to the current state-of-the-art method, ESTextSpotter, achieving 89.2%. While its detection performance is slightly lower, it has a significant advantage in recognition performance. Without a lexicon ('None' results), outperforming ESTextSpotter by 3.7%, reaching 84.5%, and with a lexicon ('Full' results), exceeding it by 2.7%, achieving 89.8%. Compared to the state-of-the-art method in recognition performance, DeepSolo, is higher by 2.0% and 1.1%, respectively. On the CTW1500 dataset, both detection and recognition performances have reached the current state-of-the-art. The detection F1 score reached 90.2%, surpassing the state-of-the-art method ESTextSpotter by 0.2%. Without a lexicon ('None' results), it exceeds ESTextSpotter by 2.1%, and with a lexicon ('Full' results), it surpasses by 0.3%, reaching 84.2%. Compared to our baseline model, DeepSolo, there is a significant improvement in performance for detection and recognition, with increases in F1, 'None', and 'Full' by 0.9%, 2.8%, and 2.8%, respectively. We use only ResNet-50 [13] as the backbone, and the results on various datasets reach state-of-the-art. When we switch the backbone to ViTAEv2-S, our performance also greatly exceeds that of DeepSolo using the same backbone."}, {"title": "For Arbitrarily-Oriented Scene Text Spotting", "content": "In the case of the multi-oriented benchmark ICDAR15, DNTextSpotter exhibits excellent performance when compared to other Transformer-based methods. As shown in Table 3, DNTextSpotter achieves results of 88.7%, 84.3%, and 79.9% on the settings of \"S\", \"W\", and \"G\", respectively. These results surpass SOTA method, DeepSolo, by 0.6% in \"S\", 0.4% in \"W\", and 0.4% in \"G\", respectively. For Inverse-like Scene Text Spotting: Besides ESTextSpotter, whose weights were measured using the publicly available weights from the paper, all other results were taken from the DeepSolo report. In the latest Inverse-Text dataset, our method achieves significant success. Compared to the current SOTA method, DeepSolo, our results without a lexicon (\"None\") surpassed it by 11.3%, reaching 75.9%, and with a lexicon (\"Full\"), we exceeded it by 10.4%, achieving 81.6%. We analyze why our method performed exceptionally well on this dataset. We believe this is due to the fact that datasets for Inverse-like texts present a more complex challenge than those for arbitrarily shaped texts. Unlike conventional texts that follow left-to-right, these texts are ordered from right to left and are flipped downward, making it challenging for the original model to learn this pattern. Denoising training, as an auxiliary task, with its relatively simpler nature, can more easily help the model learn these uncommon or complex forms of text."}, {"title": "4.4. Ablation Studies", "content": "Ablation experiments were conducted on the Total-Text dataset. Table 4 further demonstrates the convergence effects of the improved denoising training. Table 6 shows the impact of noise scale and mask probability. Table 7 shows the effects of adding noise to Bezier control points(BCP), using masked character sliding(MCS), and calculating an additional background loss(BCT)."}, {"title": null, "content": "(1) Effect of BCP: We investigate the addition of noise on the Bezier control points rather than directly on the sampling points of the Bezier center curve. Using BCP leads to a noticeable improvement in 'F1' results, by approximately 0.8%. We think BCP can contribute positional prior information to the noised positional queries, which is not achieved by directly adding noise to the sampling points on the centerline. Conversely, if noise is directly added to these sampling points, the denoising training segment would miss out on the benefits of the smooth Bezier curve's positional priors, thereby negatively impacting the training outcome."}, {"title": null, "content": "(2) Effect of MCS: Generating queries directly without MCS leads to a significant decrease in performance because it forces the model to learn a fixed postional output order. Introducing the sliding operation means that the model no longer learns targets based on a fixed position, which promotes a one-to-one alignment between character and position. The experimental results also confirm the effectiveness of MCS, showing a 1.6% increase in 'None' results."}, {"title": null, "content": "(3) Effect of BCT: Employing additional background loss calculation techniques (BCT) leads to improvements in both F1 scores (+0.2%) and 'None' results (+0.4%). BCT is actually a reuse of the negative part. Originally, the focal loss was used solely to calculate the loss for binary classification between the foreground and background. Now, an additional cross-entropy loss requires that each character in the negative part undergo multi-class classification."}, {"title": null, "content": "(4) Effect of Noise Scale and Mask Probability: The ablation experiments presented in Table 5 and Table 6 indicate that choosing an appropriate noise ratio is crucial, as both excessive and insufficient noise can impact the results. We conduct ablation experiments on the hyperparameter A for randomly flipping characters, with the experimental results showing the performance when A is set between 0.0 and 0.8. Furthermore, we control the noise scale A at 0.4 to conduct ablation experiments on the mask probability. The experimental results show that either excessive or insufficient noise can affect the model's performance."}, {"title": "4.5. Qualitative Analysis", "content": "Instability Measurement: We utilize the analysis method for quantifying the instability(IS) of bipartite graph matching proposed by DN-DETR [19]. The calculation formula can be found in the appendix. For a training image in the TotalText dataset, we calculate the indices of N proposals every 10k iterations, with every 10k iterations considered as one group. By comparing the differences in indices between the i-th group and the (i + 1)-th group, we obtain the results for IS. We visualize the IS results as shown in Fig. 6. Additionally, the training set of the Total Text contains a total of 1255 training images, with an average of 7.04 text instances per image, so the largest possible IS is 7.04 x 2 = 14.08."}, {"title": "Visualization Comparisons", "content": "Fig. 5 illustrates the experimental results on InverseText. ESText"}]}