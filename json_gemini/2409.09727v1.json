{"title": "From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare", "authors": ["Ming Li", "Pengcheng Xu", "Junjie Hu", "Zeyu Tang", "Guang Yang"], "abstract": "Federated learning holds great potential for enabling large-scale healthcare research and collaboration across multiple centres while ensuring data privacy and security are not compromised. Although numerous recent studies suggest or utilize federated learning based methods in healthcare, it remains unclear which ones have potential clinical utility. This review paper considers and analyzes the most recent studies up to May 2024 that describe federated learning based methods in healthcare. After a thorough review, we find that the vast majority are not appropriate for clinical use due to their methodological flaws and/or underlying biases which include but are not limited to privacy concerns, generalization issues, and communication costs. As a result, the effectiveness of federated learning in healthcare is significantly compromised. To overcome these challenges, we provide recommendations and promising opportunities that might be implemented to resolve these problems and improve the quality of model development in federated learning with healthcare.", "sections": [{"title": "1 Introduction", "content": "The integration of Artificial Intelligence (Al) into healthcare research has started a transformative era, catalyzing unprecedented advancements in patient care, diagnostic precision, and therapeutic efficacy\u00b9. However, developing robust Al models requires a vast amount of multi-centre data. A notable example is the genome-wide association studies, when confined to data from a single institution, are often limited by sample size, failing to identify established biomarkers2. This underscores the imperative for collaborative data sharing among institutions. Standard Al approaches rely on centralized datasets for model training, but in healthcare, centralization is complex due to various factors such as privacy concerns, regulatory constraints, as well as legal, ethical and technological barriers to data sharing\u00b3.Federated Learning (FL) emerges as a revolutionary paradigm, promising the collaborative training of Al models across distributed datasets without data sharing. By enabling privacy-preserving data analysis across multiple data silos, FL can exploit the full potential of worldwide healthcare data across different demographics, unlocking insights unattainable by isolated institutions. Models trained in a federated fashion are potentially able to yield even less biased decisions and higher sensitivity to rare cases as they are exposed to a more complete data distribution. Recent studies have shown that models trained by FL can achieve performance comparable to the ones trained on centrally hosted datasets and superior to models that only see isolated single-institutional data 5,6. Notably, early studies into FL, particularly in areas like brain tumour, triple negative breast cancer7 and COVID-198, also have begun to illustrate the potential for generalizability beyond a single institution.Today's pioneering large-scale initiatives span academic research, clinical applications, and industrial translations, collectively advancing FL in healthcare. Within academic research, consortia such as Trustworthy Federated Data Analytics (TFDA) and the 10 spearhead decentralized research across institutions. An illustrative example is the international collaboration employing FL to develop Al models for mammogram assessment, which outperformed single-institutional models and exhibited enhanced generalizability 11. Moving to clinical applications, projects like HealthChain 12 and DRAGON13 aim to deploy FL across multiple hospitals in Europe, facilitating the prediction of treatment responses for cancer and COVID-19. By aiding clinicians in treatment decisions based on histology slides and CT images, FL demonstrated direct clinical impact. Another large scale project is the Federated Tumour Segmentation (FeTS) initiative 14, which involves 30 institutions globally, that utilize FL to improve tumour boundary detection across various cancers. In the industrial domain, collaborative efforts like 15 demonstrate how competing companies can optimize the drug discovery process through multi-task FL while protecting their proprietary data.Despite FL's promising advantages, integrating it within healthcare still faces methodological flaws and underlying biases. These encompass but are not limited to, addressing privacy concerns 8,16, generalization issues 17, communication costs 18, and the non-independent and identically distributed (non-IID) nature of healthcare data across institutions19, safeguarding patient data against sophisticated inference attacks that could potentially deanonymize sensitive information from model updates 20, and the necessity for standardization across FL implementations. Moreover, there's a pressing need for models that not only exhibit robust performance across diverse datasets but are also interpretable and transparent in their predictions and decision-making processes 21,22.To facilitate the implementation of FL in healthcare, we have considered and analyzed the most recent studies, delving into the practical application of FL in healthcare. We provide numerous recommendations and promising opportunities, which, if followed appropriately, might be able to mitigate current pitfalls and challenges, ultimately leading to high-quality development"}, {"title": "2 Preliminaries", "content": "FL, introduced in 20174 as federated averaging (FedAvg), is an approach that trains models across multiple clients without centralizing data. In FL, each client (e.g., hospitals and institutions) keeps their private data locally and contributes to a shared model by sending updates like gradients or parameters to a central server. This server coordinates the training process, aggregates updates, and broadcasts the refined model back to clients. The goal of FL is to minimize the global objective function with parameters \\( \\theta \\) defined as:\n\n\\[\n\\sum_{k=1}^{K} w_{k} F_{k}(\\theta) \\quad \\text { where } \\quad F_{k}(w)=\\frac{1}{n_{k}} \\sum_{i \\in P_{k}} l\\left(x_{i}, y_{i}, \\theta\\right)\n\\]\n\nwhere K is the number of clients, the weights \\( w_{k} \\) represents the proportional significance or scale of each local dataset, \\( n_{k} \\) is the number of training data on client k; \\( P_{k} \\) is the set of indices"}, {"title": "3 Method", "content": "Our review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.As shown in Figure 3, the flow diagram outlines the search, inclusion and exclusion procedures. We carried out a comprehensive search of the most recent studies focusing on advanced FL technologies within healthcare domain up to May 2024."}, {"title": "3.1 Literature Search", "content": "We conducted a systematic search using PubMed, Web of Science, Scopus, Science Direct, IEEEXplore, ArXiv, Springerlink, ACM Digital Library, and Google Scholar. Any study up to May 2024 that involved the use of FL technologies in healthcare based on a simulated or real distributed scenario was included. The search phrases included the following keywords: \u201cFederate Learning\u201d, \u201cHealthcare\u201d, \u201cPrivacy-Preserving\u201d, \u201cMedical\u201d, \u201cBiomedical\u201d, \u201cDecentralized Learning\u201d, and \u201cPrivacy\u201d, using Boolean operators such as \u201cAND/OR\u201d and various combinations of these keywords. As shown in Figure 3, initially, a total of 1,149 studies were identified."}, {"title": "3.2 Study Selection", "content": "We defined clear and transparent inclusion and exclusion criteria as follows. Inclusion criteria: (1) Studies involving the implementation of FL in the healthcare domain; (2) Studies that, while not explicitly focused on healthcare applications, involve or utilize healthcare data or scenarios in their experiments; (3) Studies on the design or optimization of FL frameworks/workflows that cover the healthcare domain; (4) Studies in English language. Exclusion criteria: (1) Duplicate studies; (2) Studies such as surveys, reviews, opinions, editorial letters, book chapters, and theses; (3) Studies unrelated to FL or those using FL for non-healthcare applications; (4) Non-English language studies.Based on the above criteria, the screening procedure was conducted independently by two groups of authors (Group A: M.L. and P.X.; Group B: J.H. and Z.T.) to eliminate bias and ambiguity. Two groups confirmed the selected studies and resolved any conflicts or inconsistencies through discussion between the groups. The study selection process is outlined in Figure 3. Initially, a total of 43 duplicate studies were removed. Subsequently, the titles and abstracts were carefully screened, leading to the exclusion of 402 unqualified and irrelevant studies. Next, the eligibility of the remaining 704 studies was assessed through full-text screening. Finally, after further evaluation, 597 studies were deemed ineligible, and 107 studies were included in our final review."}, {"title": "4 Results", "content": "Included studies explored a broad range of healthcare specialties, including general medicine, cardiology 29, oncology 30, ophthalmology31, drug discovery 15, multiomics 32, dermatology 33, and radiology 34. Most studies focused on tasks such as classification (67/107), segmentation"}, {"title": "4.1 Application and Data", "content": "(20/107), and detection (8/107), with additional applications in regression 35, clustering 32, reconstruction 36,37, feature selection 38,39, data synthesis 40\u201342, and biomedical language process 43.In terms of data types, medical imaging, including MRI, CT, and X-rays, was the most frequently used (55/107), followed by clinical data and electronic health records (EHR) (24/107), skin images (6/107), retinal images (6/107), histopathology slides (16/107), multiomics data (3/107), and biomedical language data (1/107). Some studies involved multiple data types, while 8 studies did not specify the type of data used or used non-healthcare data44\u201346."}, {"title": "4.2 Topology, Scenario and Framework", "content": "The centralized FL paradigm dominates current implementations, with 95 out of 107 studies following this topology. Only 10 studies reported real-world deployments in distributed clinical settings, while the rest remained in the realm of prototypes or simulations. In terms of frameworks, the majority (78/107) utilized custom-designed FL frameworks, while a smaller number (13/107) employed open-source options such as Flower13, Flare8,11,79, SubstraFL7,15, TFF35, OpenFL14, PySyft34,47,66,101, and FedBioMed80. 16 studies did not specify the framework used. Further details about open-source frameworks can be found in Table 2."}, {"title": "4.3 Data Curation and Partition", "content": "Among the reviewed studies, only 12 provided details on the processes of data standardization and harmonization. Regarding data partition, HFL was the predominant approach, with 94 out of 107 studies focusing solely on it. In contrast, VFL was explored in only 3 studies 28,36,48, while 2 studies considered both HFL and VFL in combination 28,48. Only 1 study discussed FTL77. Notably, 12 studies did not mention this aspect at all. The majority of studies addressed only one type of data heterogeneity, such as quantity skew or label skew, without considering multiple factors simultaneously. Moreover, 37 studies employed natural data splits for training and/or evaluation, while the rest relied on artificial splits. Only 17 studies detailed their training, testing or validation sets and 12 studies split a holdout cohort for evaluation."}, {"title": "4.4 Model", "content": "Among reviewed studies, Convolutional Neural Networks (CNNs) were the most commonly utilized (80/107), including both custom models specifically designed for healthcare tasks and well-established architectures like ResNet, DenseNet, MobileNet, and U-Net47,98,100. Additionally, Recurrent Neural Networks (RNNs) have been incorporated to leverage their strengths in handling complex healthcare data 48,57,102. Some studies also employed custom Multi-Layer Perceptrons (MLPs) and attention mechanisms to further boost model performance 61,70,103.43 utilized large language models for distributed biomedical natural language processing. Beyond deep learning approaches, several studies explored traditional machine learning (ML) algorithms, including linear models and ensemble methods. Notable examples include logistic regression70, support vector machines84, fuzzy clustering 104, and decision trees 71,105. Interestingly, only 23 studies explicitly discussed their initialization strategies for model training. Among these, the majority opted for random initialization, while a mere five clearly stated that they utilized pretrained or foundation models as their starting point 29,33,43,96,106"}, {"title": "4.5 Optimization", "content": "Most studies addressed either data or model heterogeneity, and none of them considered system heterogeneity. Only 15 studies evaluated model generalization ability in unseen open domains. A total of 19 studies focused on improving communication efficiency, employing techniques such as knowledge distillation 51,96,99, gradient quantization61, one-shot FL98, split learning 101, and tensor factorization 95,107. In terms of convergence analysis, a few studies (21/107) reported metrics such as communication rounds and costs, as well as overall convergence time, but none provided a theoretical understanding of convergence dynamics. Only two studies considered temporal data dynamics in model learning 28,57. Regarding synchronization, 15 studies employed asynchronous aggregation instead of synchronous aggregation, particularly in applications involving wearables 75 and IoMT devices 57."}, {"title": "4.6 Privacy and Security", "content": "Only 41 studies addressed the exchange of model updates with privacy guarantees. The most commonly used techniques for safeguarding model updates included Differential Privacy (DP), Homomorphic Encryption (HE), Secure Multi-Party Computation (SMPC), knowledge distillation, and partial model exchange. However, metadata such as sample sizes and distributions were frequently shared without protection, particularly in methods based on FedAvg 106,108. To mitigate the risk of adversaries inferring raw data, synthetic data was employed in some cases 19,42,85,98 Additionally, swarm learning and blockchain were utilized to secure the communication process 19,24."}, {"title": "4.7 Fairness and Incentive", "content": "Only three studies have discussed issues related to fairness and/or incentives in healthcare FL 45,53,109, with just one of these studies specifically exploring the complexities of both fairness and incentives in detail53."}, {"title": "4.8 Evaluation", "content": "Most studies used conventional ML metrics for evaluation, such as accuracy, precision, area under the receiver operating characteristic curve (AUC), sensitivity/recall, specificity, F1-score, Dice score, Intersection over Union (IoU), Hausdorff Distance (HD), and loss value. Additionally, many studies performed comparisons against classical centralized models or localized models, and conducted ablation studies. However, only a few studies (26/107) addressed critical aspects unique to FL, such as communication overhead, resource consumption, scalability, generalization, privacy, fairness, and security concerns. As for benchmarking, just one study provided relatively comprehensive benchmarks across multiple healthcare datasets 52. Interpretability was explored in seven studies, either through feature selection 38,62, attention maps 39,51,66, or tree-based models 7,35. While 29 studies released their source code, only one also made the trained model publicly available."}, {"title": "5 From Challenges and Pitfalls to Recommended Solutions and Future Opportunities", "content": "After a thorough review of the most recent and advanced FL studies, we find various challenges and pitfalls that still limit the implementation of FL in healthcare. In this section, we introduce a clear taxonomy, as depicted in Figure 5, focusing on the challenges and pitfalls, and further providing recommended solutions and opportunities. We adhere to the best practice workflow in FL for discussion in the following subsections."}, {"title": "5.1 Scenario and Framework", "content": ""}, {"title": "5.1.1 Domination of Simulation Studies", "content": "The majority of existing studies have been confined to simulation environments, with only 10 studies incorporating real-world distributed clinical scenarios. This indicates that the application of FL in healthcare is still in its nascent stage. The complexity of deploying FL across a real-world network of hospitals and institutions has significantly hindered its progress. Most studies have operated within controlled, simulated settings where data is pooled and then artificially partitioned to represent distributed environments. The simulated clients interact with a simulated server, coordinating model updates in a manner that is highly controlled and predictable. In contrast, real-world scenarios involve each client working with inherently distributed, heterogeneous, and locale-specific data. The interaction between real server and clients is far more complex, requiring secure protocols, real-time communication, and the ability to handle diverse datasets across various institutions."}, {"title": "5.1.2 Deficiency in FL Frameworks Development and Usage", "content": "Most studies developed their own FL frameworks, often not strictly aligning with standard FL protocols, particularly when confined to single machine simulation studies. Meanwhile, some simulation studies used industrial-grade frameworks, which introduce unnecessary complexity and resource demands for simulation and prototype research. Some other studies utilized lightweight open-source FL frameworks, although prevalent, frequently lack healthcare-specific adaptations, leading to deficiencies in privacy, security, and regulatory compliance. Common shortcomings across current FL frameworks include a lack of healthcare adaptations, as most frameworks are not tailored to meet healthcare-specific requirements, which include stringent privacy, security, and regulatory standards. Additionally, many frameworks do not address the need for communication efficiency, which is essential for the practical deployment of FL in resource-constrained environments. Limited support for traceability also hinders accountability and transparency in FL. Furthermore, while some frameworks offer scalability and cloud compatibility, many do not, which can limit their ability to handle large-scale healthcare data and integrate with existing cloud infrastructures. Here, we inventory the most popular FL frameworks in Table 2, with emphasis on those adapted for healthcare, and outline their features."}, {"title": "5.2 Data", "content": ""}, {"title": "5.2.1 Unclear in Data Standardization and Harmonization", "content": "Healthcare data are often collected and stored in diverse and proprietary formats that do not always adhere to international standards and terminologies, complicating data linkage and reuse. For example, structured clinical data usually contains features that vary with differences in clinical practice across institutions 124, such as diabetes diagnosis, which can involve different glucose measurement methods with varying cut-off points, resulting in hidden heterogeneity that may be overlooked in subsequent statistical analyses. A crucial step before implementing FL in healthcare is to ensure data standardization, harmonization, and interoperability across different cohorts, which are key to the success of FL (Figure 6).Most simulation studies processed data centrally and generate artificially partitioned datasets without considering the distributed nature of various data silos. This oversight extends to the lack of discussion on how datasets at each client are curated for use in experiments. Despite this, almost all FL frameworks assume the input data is preformatted for model training or preprocessing pipelines. This assumption leads to significant frustration and delays, as the burden of data export and conversion typically falls on clinical data managers who may lack the necessary budget and training. Moreover, among the included studies, only two performed quality or integrity checks on the data. 125 excluded samples with impossible values (e.g., negative heart rates) and inconsistent feature values, while 126 used Principal Component Analysis to filter out noise. Few studies addressed structural or informative missingness, with only57 and 38 considering imputation methods while also deleting features with high missingness rates. Poor quality imputation and handling of non-random missingness can bias model training."}, {"title": "5.2.2 Issues in Data Partition", "content": "Among all included studies, only a few leveraged natural splits to replicate data collection processes across different hospitals or institutions. For instance, 30 employed the CheXpert dataset, 128 worked with the chest X-ray dataset and39 extracted metadata from Tissue Source sites in the TCGA dataset for their studies. These datasets naturally reflect the heterogeneity found in real-world clinical data across hospitals and institutions, making them more suitable for FL studies in healthcare 58,129Simulation studies typically used heuristics to artificially create heterogeneous data partitions from a pooled dataset, assigning these partitions to simulated clients, as illustrated in Figure 4. Common synthetic partitioning methods for classification tasks include assigning samples from a limited number of classes to each client, using Dirichlet distribution sampling on class labels, and employing the Pachinko Allocation Method (PAM) when labels have a hierarchical structure4. For regression tasks, Gaussian Mixture clustering based on t-SNE feature representations has been used to partition datasets among clients 130Nonetheless, synthetic partitioning methods may not accurately reflect the intricate heterogeneity found in real-world scenarios 90. Examples from digital histopathology illustrate the limitations of synthetic partitioning methods 131. In digital histopathology, tissue samples are extracted, stained, and digitized, leading to data heterogeneity due to factors such as patient demographics, staining techniques, physical slide storage methods, and digitization processes. Although advancements in staining normalization have reduced some heterogeneity, other sources remain challenging to replicate synthetically, and some may even be unknown 132. These underscore the necessity of conducting cohort experiments with natural splits to ensure that FL models are robust across varied clinical settings. This issue also extends to other areas, including radiology, dermatology, and retinal image analysis.Even among studies that adopted synthetic partitioning methods, the strategies employed are often limited, primarily focusing on scenarios such as quantity skew. These studies addressed only a narrow aspect of heterogeneity. For instance, label skew, where the distribution of labels differs across clients, and feature skew, where clients have different feature distributions, are"}, {"title": "5.3 Model", "content": ""}, {"title": "5.3.1 Inadequate Model Selection and Development", "content": "The studies reviewed exhibit a wide range of model complexities, from advanced, parameter-heavy architectures to traditional ML techniques. However, several issues persist in model"}, {"title": "5.3.2 Negligence in Initialization", "content": "Most of the included studies began federated training from a random initialization, a method that, while effective in IID scenarios, can be less optimal for handling non-IID data. In healthcare, where data distribution often varies significantly across institutions due to differences in patient demographics, clinical practices, or data collection methods, random initialization can lead to slower convergence, increased communication costs, and potentially suboptimal local optima 138,139.A significant issue is the lack of standardization in model initialization approaches. Many studies either adopted random initialization without justification or entirely omitted the description of their initialization method. This inconsistency can result in significant variations in model performance and convergence rates, making it difficult to compare results across different studies and settings. Additionally, if the initial model is biased towards the data distribution of certain participants, it might not perform well across all clients, leading to fairness issues and suboptimal overall performance. Moreover, these challenges are further exacerbated by the heterogeneity of computational resources available across institutions. Some advanced initialization methods, such as those involving foundation models or pretraining on large-scale datasets, may be computationally expensive and thus infeasible for resource-constrained participants 43,96,138Personalization in model initialization is another underexplored area. Personalized initialization techniques, which tailor the starting point to the specific data distribution of each client, are critical for improving local model performance and accelerating convergence. However, research into these techniques, such as model-agnostic meta-learning and partial initialization for finding a good initialization, remains limited within FL for healthcare 140,141."}, {"title": "5.4 Optimization", "content": ""}, {"title": "5.4.1 Heterogeneity Issues", "content": "In FL for healthcare, heterogeneity refers to the variability in data, models, and systems across different hospitals and institutions. This variability poses significant challenges to FL's performance and its ability to generalize well across diverse environments. The key types of heterogeneity in FL for healthcare include statistical heterogeneity, model heterogeneity, and system heterogeneity.Statistical heterogeneity arises due to the non-IID nature of healthcare data across various institutions, which is characterized by demographic differences, instrumentation biases, distinct data acquisition protocols, and human operations, etc142. For instance, variations in CT scan quality across sites can lead to inconsistencies in the correlation between imaging data and corresponding site-specific EHR data. These inconsistencies severely degrade FL performance, with accuracy drops of up to 50%4, necessitating additional communication rounds for convergence 143. Statistical heterogeneity can also result in clients overfitting to their local data, leading to poor generalization on data from other clients, making simple parameter averaging an ineffective aggregation strategy 144. Since local models are optimized towards different local optima, the aggregated global model may drift from the true global optima, causing a biased minimum and significantly slowing down convergence as illustrated in Figure 7. In healthcare, statistical heterogeneity can be broadly characterized by four forms, including:Quantity Skew. The number of training samples differs greatly across clients, leading to imbalanced data distributions. Models tend to optimizing for clients with more data, potentially neglecting those with less, further reducing the generalization ability 134.Label Skew. The distribution of labels varies across clients. For instance, in the context"}, {"title": "5.4.2 Open Domain Problem", "content": "A key challenge in healthcare FL is the poor generalization of models to open domains, where unseen data lies beyond the federation's scope. A mere 14% of the included studies have validated their methods on such external data, underscoring a significant research gap. Current FL strategies predominantly focus on boosting performance within the federation, frequently overlooking the essential need for model adaptability to new, unseen environments.Studies have shown that even slight differences in devices or acquisition protocols can result in a significant distribution shift, thereby reducing the model's effectiveness when applied to new, unseen datasets. This issue is particularly acute in healthcare applications like diabetic retinopathy screening in fundus images, where the diversity of cameras and settings across different institutions can lead to poor model performance on external data. 31 addressed this challenge by introducing a frequency-based domain generalization approach in FL. They enabled privacy preserving exchange of distribution information across clients through continuous frequency space interpolation and designed a boundary-oriented episodic learning scheme to expose local training to domain shifts and enhance model generalizability in ambiguous boundary regions. However, the proposed method can be impractical for real-world applications due to its reliance on extensive network bandwidth and computational resources required for Fourier transform computation in frequency space interpolation."}, {"title": "5.4.3 Burdens in Communication", "content": "Communication is a significant bottleneck in the implementation of FL in healthcare. In FL, each client needs to frequently communicate with the central server. This communication can be orders of magnitude slower than local computation due to constraints such as bandwidth, latency, and power95.The communication bottleneck in FL arises from several factors. First, the number of clients involved in an FL system can be very large (e.g., wearables and IoMT), leading to significant communication overhead. Each communication round requires sending model updates between the clients and the central server, which can be expensive in terms of time and resources 4,95. Second, ensuring data privacy and security in FL is crucial, especially in sensitive domains like healthcare. The need for encryption and secure communication protocols adds to the computational and communication overhead. Encrypting model updates can significantly increase the size of the data being transmitted, further straining the communication channels and requiring more sophisticated algorithms to balance privacy and efficiency4. Third, as FL tasks become more complex, the size of the models involved increases. Modern large-scale models, such as large language models (LLMs) and foundation models (FMs), can have billions of parameters, resulting in model sizes that require significant bandwidth to transmit. This issue is exacerbated when using standard communication protocols like gRPC, which have size limits on single messages (e.g., 2 GB). Typical LLMs and FMs can exceed these limits, necessitating the model to be split into smaller chunks for transmission, adding additional overhead and complexity to the system 158Included studies primarily concentrated on enhancing communication encryption techniques, with the aim of either reducing the volume of data exchanged63,104 or minimizing the number of communication rounds 54. Additionally, several studies explored methods for achieving fully decentralized communication without central server55,159. Improvements in encryption often involved methods for securely sharing secret keys among clients 81,83, encryption mechanisms for safeguarding exchanged data 64,82, and techniques for perturbing model outputs at each client using a secret key 160. To decrease the amount of data transmitted, some studies proposed transferring only a subset of model parameters 95,141,161 or employing strategies like compressing 97, masking 61, and quantizing gradients 95 or model outputs before exchange 27. Reducing the number of communication rounds was addressed through model design91,98,162, aggregating updates based on elapsed time instead of epochs 97,107, and evaluating the potential benefit of an update before communication 81. Other studies focused on detecting attacks during communication 163, developing authentication systems for clients 46, and improving client management systems 45,64."}, {"title": "5.4.4 Plain Convergence Analysis", "content": "Federated optimization in healthcare aims to adapt models to local data distributions while integrating global information. The inherent heterogeneity across hospitals and institutions often leads to instability and slow convergence in federated training 154. However, comprehensive convergence analysis is frequently lacking in current studies.Most studies tend to provide only plain convergence analysis, focusing on reporting metrics such as the number of local epochs, communication rounds, and overall convergence time. While these metrics are useful, they do not offer a deep theoretical understanding of the convergence dynamics. This lack of rigorous analysis limits our understanding of how and why certain FL algorithms perform well (or poorly) in specific healthcare applications.Only a handful of studies 61,95,104,159 have focused on the convergence of FL in healthcare settings. These studies typically relied on Stochastic Gradient Descent (SGD) as the foundational optimization method due to its effectiveness in smooth optimization problems, under assumptions such as the existence of lower bounds, Lipschitz smoothness, and bounded variance 61,159. However, SGD-based FL algorithms often struggle with nonsmooth optimization problems, which are common in healthcare data due to irregularities in data distributions and the presence of outliers."}, {"title": "5.4.5 Temporal Dynamics and Revoke Issues", "content": "Healthcare data's inherent time dependence is critical, especially for diseases with distinct progression or treatment timelines, such as cancer and chronic conditions like diabetes. However, included studies often overlooked these temporal dynamics when partitioning data, potentially leading to models that inaccurately reflect disease progression. For example, COVID-19 characteristics, such as ground-glass opacities in lung CT scans, evolve with the disease's progression 169 Ignoring such temporal dynamics can result in models that are overfitted to specific stages of a disease and unable to generalize across different phases 170\u2013172 Furthermore, the dynamic nature of data at each participating hospital or institution complicates the situation. Hospitals continuously acquire new data and may also remove or modify existing data due to errors or other factors. This dynamic data environment requires FL models to adapt without frequent retraining, as new data might cause model drift, while data removal can leave critical gaps in the model's understanding, particularly if the removed data represents rare or critical cases.Another critical but overlooked issue in FL is data revocation, which becomes necessary when specific data must be withdrawn due to privacy concerns, regulatory requirements, patient requests, or participant requests. Current FL setups, designed for iterative data aggregation, struggle with \u201cunlearning\u201d specific contributions without requiring complete model retraining. Emerging research highlights the need for mechanisms that allow for efficient data revocation without compromising the integrity of the model. For instance, methods have been proposed to facilitate client \u201cunlearning\u201d in FL, enabling the removal of data contributions from specific clients without significant degradation in model performance 173. This is especially important in healthcare, where patient consent might be withdrawn, or new privacy laws might mandate the deletion of certain data."}, {"title": "5.4.6 Synchronization Issues", "content": "Synchronization of updates across different clients also poses significant challenges for FL in healthcare. The variation in computational resources, network conditions, and data availability among clients can lead to different training speeds and delayed model updates.In Synchronous FL, all clients must complete their training and send their updates before the global model aggregation, which is straightforward but can be inefficient. This approach works well in environments where data is immediately available, such as in a centralized hospital picture archiving and communication system (PACS). However, in real-world scenarios, where data acquisition might be delayed due to network issues or the unavailability of input/output devices, synchronous updates can result in significant idle times and resource underutilization. Additionally, clients in an FL network, particularly smaller healthcare entities (e.g., wearables), may not be active during every communication round, further delaying the global model update and potentially degrading overall system performance.Asynchronous FL, on the other hand, allows clients to send updates independently, without waiting for other clients to finish their training. This approach is more flexible and can accommodate variations in client availability and computational power, thereby improving the overall efficiency of the FL process. However, asynchronous updates can introduce challenges related to the consistency of the global model, as updates from slower or less reliable clients may arrive out of sync with the rest of the system."}, {"title": "5.5 Privacy and Security", "content": "Two critical privacy and security issues exist in current studies. First, it is concerning that 62% of the included studies did not encrypt model updates during communication. This lack of encryption leaves FL system vulnerable to interception, posing significant security risks. Second, statistical information such as sample sizes and distributions were often shared alongside model updates, particularly in FedAvg-based methods 106,108. This practice can expose participants with large datasets to targeted attacks if an adversary intercepts the communication or compromises the aggregator158.Despite the advantage of FL in healthcare without directly exchanging or sharing local data, it is not immune to privacy and security risks. Adversaries can analyze changes in model updates over time to infer sensitive information or exploit system vulnerabilities to conduct targeted attacks using techniques like model inversion 178, membership inference 179, and poisoning 177. Additionally, clients may unintentionally reveal private data during the FL process. This can happen when the client memorizes previous model and gradient updates, leading to the leakage of sensitive information 16,65. Furthermore, methods involving the sharing of a few data samples for augmentation or disclosing local data distributions during knowledge transfer can also result in privacy breaches 50,99. These adversarial and unintentional exposures undermine the privacy and security of the FL process, necessitating robust countermeasures and continuous vigilance to safeguard the integrity and confidentiality of the FL process.In Table 3, we provide a comprehensive overview of various privacy and security attacks identified in the context of FL in healthcare. By summarizing their key characteristics, specific"}, {"title": "5.6 Fairness and Incentive", "content": "In FL for healthcare, research on fairness and incentive mechanisms is relatively underexplored, with only 53 delved into the intricacies of both fairness and incentive in FL for healthcare.Fairness in FL refers to the equitable distribution of model performance across participants. In healthcare, it often means ensuring that ML models perform consistently across different healthcare providers and demographic groups or patient attributes. These fairness considerations are essential because disparities in model accuracy can lead to unequal treatment outcomes. 53 introduced several types of fairness, including horizontal fairness, where different hospitals receive comparable model accuracy, and vertical fairness, which focuses on ensuring that model performance is balanced across different demographic or medical attributes. They also proposed multilevel fairness, which seeks to address both client-level and attribute-level fairness simultaneously, and agnostic distribution fairness, aiming to generalize the model's fairness to non-participating entities, such as hospitals outside the federation.Incentive mechanisms are equally important in FL, as healthcare institutions often require motivation to participate in the federation. While regulatory constraints can mandate FL within organizations, voluntary participation in broader FL networks typically relies on clear incentives. For instance, hospitals engaging in FL for tasks like chest radiography classification or COVID-19 detection benefit from access to models that are more accurate than those developed using only local data. A well-designed incentive structure should ensure that participants who contribute more, whether through higher-quality data or computational resources, receive proportionately higher rewards. These rewards could be financial"}]}