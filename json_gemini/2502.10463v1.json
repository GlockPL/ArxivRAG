{"title": "FROM Layers to States: A State SpaCE MODEL PERSPECTIVE TO DEEP NEURAL NETWORK LAYER DYNAMICS", "authors": ["Qinshuo Liu", "Weiqin Zhao", "Wei Huang", "Yanwen Fang", "Lequan Yu", "Guodong Li"], "abstract": "The depth of neural networks is a critical factor for their capability, with deeper models often demonstrating superior performance. Motivated by this, significant efforts have been made to enhance layer aggregation - reusing information from previous layers to better extract features at the current layer, to improve the representational power of deep neural networks. However, previous works have primarily addressed this problem from a discrete-state perspective which is not suitable as the number of network layers grows. This paper novelly treats the outputs from layers as states of a continuous process and considers leveraging the state space model (SSM) to design the aggregation of layers in very deep neural networks. Moreover, inspired by its advancements in modeling long sequences, the Selective State Space Models (S6) is employed to design a new module called Selective State Space Model Layer Aggregation (S6LA). This module aims to combine traditional CNN or transformer architectures within a sequential framework, enhancing the representational capabilities of state-of-the-art vision networks. Extensive experiments show that S6LA delivers substantial improvements in both image classification and detection tasks, highlighting the potential of integrating SSMs with contemporary deep learning techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the depth of neural network architectures has emerged as a crucial factor influencing performance across various domains, including computer vision, natural language processing, and speech recognition. The network models are capable of capturing increasingly complex features and representations from data as they become deeper, and various methods have emerged to utilize larger numbers of layers to improve performance. Growing evidence indicates that strengthening layer interactions can encourage the information flow of a deep neural network. For CNN-based networks, ResNet employed skip connections, allowing gradients to flow more easily by connecting non-adjacent layers. DenseNet extended this concept further by enabling each layer to access all preceding layers within a stage, fostering a rich exchange of information. Later, GLOM proposed an intensely interactive architecture that incorporates bottom-up, top-down, and same-level connections to effectively represent part-whole hierarchies. Recently, some studies have begun to frame"}, {"title": "2 RELATED WORK", "content": "State Space Models. In the realm of state space models, considerable efforts have been directed toward developing statistical theories. These models are characterized by equations that map a 1-dimensional input signal x(t) to an N-dimensional latent state h(t), with the details provided in Equation 1. Inspired by continuous state space models in control systems and combined with HiPPO initialization, LSSL showcased the potential of state space models in addressing long-range dependency problems. However, due to limitations in memory and computation, their adoption was not widespread until the introduction of structured state space models (S4), which proposed normalizing parameters into a diagonal structure. S4 represents a class of recent sequence models in deep learning, broadly related to RNNs, CNNS and classical state space models. Subsequently, introduced the selective structured state space model (S6), which builds upon S4 and demonstrates superior performance compared to transformer backbones in various deep learning tasks, including natural language processing and time series forecasting. More recently, VMamba was developed, leveraging the S6 model to replace the transformer mechanism and employing a scanning approach to convert images into patch sequences. Additionally, Graph-Mamba represented a pioneering effort to enhance long-range context modeling in graph networks by integrating a Mamba block with an input-dependent node selection mechanism. These advancements indicate that state space models have also been successfully applied to complex tasks across various domains.\nLayer Interaction. The depth of neural network architectures has emerged as a crucial factor influencing performance. To effectively address the challenges posed by deeper models, increasing efforts have been directed toward improving layer interactions in both CNN and transformer-based architectures. Some studies lay much emphasis on amplifying interactions within a layer. DIANet employed a parameter-sharing LSTM throughout the network's depth to capture cross-channel relationships by utilizing information from preceding layers. In RLANet, a recurrent neural network module was used to iteratively aggregate information from different layers. For attention mechanism, proposed to strengthen cross-layer interactions by retrieving query-related information from previous layers. Additionally, RealFormer and EA-Transformer both incorporated attention scores from previous layers into the current layer, establishing connections through residual attention. However, these methods face significant memory challenges due to the need to retain features from all encoders, especially when dealing with high-dimensional data and they may lack robust theoretical supports."}, {"title": "3 PRELIMINARY AND PROBLEM FORMULATION", "content": "3.1 REVISITING STATE SPACE MODELS\nThe state space model is defined below, and it maps a 1-dimensional input signal x(t), a continuous process, to an N-dimensional latent state h(t), another continuous process:\n\\begin{equation}\nh'(t) = Ah(t) + Bx(t),\n\\end{equation}\nwhere $A \\in \\mathbb{R}^{N \\times N}$ and $B \\in \\mathbb{R}^{N \\times 1}$ are the structured state matrix and weight of influence from input to latent state, respectively. We then can obtain the discretization solution of the above equation:\n\\begin{equation}\n\\h_t = e^{At}h_{t-1} + \\int_{t-1}^{t} e^{A(t-\\tau)}Bx(\\tau)d\\tau.\n\\end{equation}\nTogether with the zero-order hold (ZOH) condition (Karafyllis & Krstic, 2011), i.e. $x(\\tau)$ is a constant at intervals $[t - 1, t]$ for all integers t, we have\n\\begin{equation}\n\\h_t = e^{A\\Delta}h_{t-1} + (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta Bx^{+}.\n\\end{equation}\nAs a result, the continuous process at Equation 1 can be replaced by the following discrete sequence:\n$h_t = \\bar{A}h_{t-1} + \\bar{B}x_t$ with $\\bar{A} = exp(\\Delta A)$ and $\\bar{B} = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B$. (4)"}, {"title": "3.2 CNN LAYER AGGREGATION", "content": "Consider a neural network, and let $X_{t-1}$ be the output from its tth layer. We then can mathematically formulate the layer aggregation at the tth layer below,\n\\begin{equation}\nA_t = g_t (X^0, X^1,..., X^{t-2}, X^{t-1}), X^t = f_t (A_{t-1}, X^{t-1}),\n\\end{equation}\nwhere $g_t$ is used to summarize the first t layers, $A_t$ is the aggregated information, and $f_t$ produces the new layer output from the last hidden layer and the given aggregation which contains the previous information. The Hierarchical Layer Aggregation proposed can be shown to have such similar mechanism which satisfies Equation 6.\nThis formulation could be generalized to the special case of CNNs. The traditional CNNs do not contain layer aggregation since the layer output only depends on the last layer output, which overlooks the connection between the several previous layers' influence. DenseNet perhaps is the first one for the layer aggregation, and its output at tth layer can be formulated into\n\\begin{equation}\nX^t = Conv3_t [Conv1^*(Concat(X^0, X^1,..., X^{t-1}))].\n\\end{equation}\nLet $A_t = Conv1^*(Concat(X^0, X^1, ..., X^{t-1}))$ and $X^t = Conv3^*(A_t)$, and then DenseNet can be rewritten into our framework at Equation 6. RLA considers a more convenient additive form for the layer aggregation, and it has the form of $A_t = \\sum_{i=0}^{t-1} Conv1_{t+1}(X^i)$, where the kernel weights of Conv1 form a partition of the weights in Conv1t. As a result, a lightweight aggregation can be formed:\n\\begin{equation}\nX^t = Conv3_t[A_{t-1} + Conv1_{t-1}(X^{t-1})].\n\\end{equation}\nWithout loss of generality, ResNets can also be treated as a layer aggregation. Specifically, we can treat the update of $X^t = X^{t-1} + f_{t-1}(X^{t-1})$ with applying the update recursively as $A_t = \\sum_{i=0}^{t-1} f(X^i) + X^0$ and $X^t = A_{t-1} + X^{t-1}$."}, {"title": "3.3 ATTENTION LAYERS AGGREGATION", "content": "In this section, we show how to generalize the layer aggregation within a transformer. Consider a simple attention layer with general input $X \\in \\mathbb{R}^{L \\times D}$ and output $O \\in \\mathbb{R}^{L \\times D}$. Its query Q, key K and value V are given by linear projections $W_q \\in \\mathbb{R}^{D \\times D}$, $W_k \\in \\mathbb{R}^{D \\times D}$ and $W_v \\in \\mathbb{R}^{D \\times D}$ i.e. $Q^T = W_qX, K^T = W_kX$ and $V^T = W_vX$. As a result, the output O has the following mathematical formulation:\n\\begin{equation}\nO = Self-Attention(X) = softmax(\\frac{QK^T}{\\sqrt{D}})V.\n\\end{equation}\nLet $X^t \\in \\mathbb{R}^{L \\times D}$ with $1 < t < T$ be the output from tth layer, where L is the number of tokens, D represents the channel of each token, and T is the number of attention layers. A vanilla transformer can then be formulated into:\n\\begin{equation}\nA^t = X^{t-1} + Self-Attention(X^{t-1}), X^t = A^t + MLP(Norm(A^t)).\n\\end{equation}\nNote that these simple self-attention layers can only capture the connection between the current layer output and the last output; they are supposed to perform better if the information from previous layers can be considered. To this end, we may leverage the idea given by CNN aggregation to concatenate the previous outputs. Specifically, the vanilla transformer at Equation 10 has the form of:\n\\begin{equation}\nX^t = f_t (g_t(X^0,..., X^{t-1})),\n\\end{equation}\nwhere $g_t$ is the attention layer, and $f_t$ is the Add & Norm layer for the t-th layer. Following at Equation 8, we may then use the recurrent mechanism to combine all the outputs given by attention layers, i.e. replacing $A^t = g_t(X^0,..., X^{t-1})$ by $A^t = A^{t-1} + g_{t-1}(X^{t-1})$."}, {"title": "3.4 THE FORMULA OF S6LA", "content": "Denote a sequence $X = {X^1,\u2026\u2026\u2026, X^T}$, where $X^t$ is the output from tth layer, say Convolutional layers/blocks or Attention layers, of a deep neural network, and T is the number of layers. In financial statistics, the price of an asset can be treated as a process with discrete time when it is sampled in a low frequency, say weekly data, while it will be treated as a process with continuous time the sampling frequency is high, say one-minute data; see Yuan et al. (2023). Accordingly, we may treat X as a sequence with discrete time as the number of layers T is small or even moderate, and all existing methods for layer aggregation in the literature follow this way. However, for a very deep neural network, it is more like the scenario of high-frequency data, and hence a continuous process is more suitable for the sequence X. This section further conducts layer aggregation by considering state space models in Section 3.1; see Figure 1 for the illustration.\nSpecifically, we utilize the Mamba model (Gu & Dao, 2023) due to its effectiveness in processing long sequences. This model is based on S6 models and can provide a better interpretation on how to leverage the previous information and then how to store it based on its importance. Moreover, it has been demonstrated to have a better performance than traditional transformers and RNNs. Following its principle, we propose our selective state space model layer aggregation below:\n\\begin{equation}\nh_t = g(h_{t-1}, X^t), X^t = f_t(h_{t-1}, X^{t-1}),\n\\end{equation}\nwhere ht is a hidden state similar to At in Equation 6, and it represents the recurrently aggregated information up to (t - 1)th layer. We may consider an additive form, as in Equation 8, for ht. Moreover, gt is the relation function between the current SSM hidden layer state and previous hidden layer state with input. As a result, similar to Equation 4, the update of ht can be formulated as:\n\\begin{equation}\nh_t = Ah_{t-1} + BX^t, X^t = f_t(h_{t-1}, X^{t-1}).\n\\end{equation}\nThe choice of function ft is different for CNNs and Transformer-based models, and they are detailed in the following two subsections; see Figures 4 and 3.5 for their illustrations, respectively."}, {"title": "3.5 APPLICATION TO DEEP CNNS", "content": "For CNNs backbones, we adopt ResNet (He et al., 2016a) as the baseline architecture. We propose to concatenate the input at each layer, say $X^t \\in \\mathbb{R}^{H \\times W \\times D}$, where H and W represent the height and width, and D indicates the embedding dimension. For each CNN layer, the input to each block in ResNet-comprising a combination of 1D and 3D convolutional operations-is formed by concatenating Xt with the state $h_{t-1} \\in \\mathbb{R}^{H \\times W \\times N}$ from the previous layer, where N is the dimension of latent states. This integration effectively incorporates SSM information into the convolutional layers, enhancing the network's capacity to learn complex representations.\nFor the specific implementation of S6LA in CNNs, we initialize the SSM state h\u00ba using Kaiming normal initialization method. This initialization technique is crucial for ensuring effective gradient flow throughout the network, and we will further clarify this point in ablation studies. Next, we employ a selective mechanism to derive two components, the coefficient B for input and the interval A as specified in Equation 4. For transition matrix A, the initial setting is"}, {"title": "3.6 APPLICATION TO DEEP VITS", "content": "In our exploration of S6LA implementation in deep ViT backbones, we draw parallels between the integration of the state space model (SSM) state and the mechanisms used in convolutional neural networks (CNNs). However, the methods of combining inputs within transformer blocks differ significantly from those in CNNs. Like the treatment of attention mechanism, we utilize multiplication combination instead of simply concatenating to deal with the input Xt and ht-1 in transformer-based models. This approach enhances the interaction between input features and SSM, enabling richer feature representation. Then the next paragraghs gives the specifics of leveraging our S6LA method with ViTs backbones as follows:\nInput Treatment: We begin by combining the class token and input, alongside the application of positional embeddings. Then following the attention mechanism, $X_{input} \\in \\mathbb{R}^{(L+1) \\times D}$ appeared where L is the number of patches and D is the embedding dimension, and it is split into two components next: image patch tokens $X \\in \\mathbb{R}^{L \\times D}$ and a class token $X^c \\in \\mathbb{R}^{1 \\times D}$.\n\\begin{equation}\nX_{input} = Add&Norm(MLP(Add&Norm(Attn(X)))), X, X^c = Split(X_{input}).\n\\end{equation}"}, {"title": "4 EXPERIMENT", "content": "This section evaluates our S6LA model in image classification, object detection, and instance segmentation tasks, and provides an ablation study. All models are implemented by the PyTorch toolkit on 4 GeForce RTX 3090 GPUs. More implementation details, and comparisons are provided in Appendix D."}, {"title": "4.1 EXPERIMENTS ON IMAGE CLASSIFICATION", "content": "Backbone. For the dataset, we use Imagenet-1K dataset directly. For the CNN backbone, we choose different layers of ResNet. For transformer-based model, DeiT, Swin Transformer and PVTv2 are considered. We compare our S6LA with baselines and other layer aggregation SOTA methods with different backbones alone as the baseline models.\nExperimental settings. The hyperparameter of state space model channel N shown in Section 3.5 is introduced to control the dimension of feature of h in per S6LA hidden layer module. After comparison of different N = 16, 32, 64 for ResNet, we choose 32 as our baseline feature channel, for others we talk about in Section 4.3. In order to compare the baseline models and the models enhanced by S6LA fairly, we use the same data augmentation and training strategies as in their original papers in all our experiments.\nMain results. The performance of our main results, along with comparisons to other methods, is presented in Tables 1 and 2. To ensure a fair comparison, all results for the models listed in these tables were reproduced using the same training setup on our workstation. Notably, our model outperforms nearly all baseline models. We specifically compare our S6LA model with other layer interaction methods using ResNets as baselines. The results in Table 1 demonstrate that our S6LA surpasses several layer aggregation methods on CNN backbones, including SENet, CBAM, A2-Net, NL ECA-Net, RLA-Net and MRLA. Additionally, we find that our model consistently outperforms them, achieving nearly a 2% improvement in Top-1 accuracy"}, {"title": "4.2 EXPERIMENTS ON OBJECT DETECTION AND INSTANCE SEGMENTATION", "content": "This subsection validates the transferability and the generalization ability of our model on object detection and segmentation tasks using the three typical detection frameworks: Faster R-CNN, RetinaNet and Mask R-CNN.\nExperimental settings. For the dataset, we choose MS COCO 2017 for experiments. All the codes are based on the toolkits of MMDetection. The hyperparameter of state space model channel N is introduced to control the dimension of feature of h in per S6LA hidden layer module same to the settings in classification tasks.\nResults of object detection and instance segmentation. For the results of object detection task, Table 9 illustrates the details about AP of bounding box with the notation APbb. It is apparent that the improvements on all metrics are significant. Also compared with the other stronger backbones and detectors, our method outperforms in this task while we only add a little parameters and FLOPs which can be overlooked by the servers. Meanwhile, Table 4 illustrates our S6LA method's improvements about AP of bounding box and mask on all the metrics with Mask R-CNN as the framework. Also similar to the advantages in object detection task, our method balance the parameters and FLOPs with traditional backbones. From the tables' results, it is proved that our S6LA model is feasible."}, {"title": "4.3 ABLATION STUDY", "content": "Different variants of S6LA. Due to resource limitations, we only experiment with the ResNet-50 and DeiT models on the ImageNet dataset. Our investigation considers several factors: (a) the"}, {"title": "5 CONCLUSION", "content": "In conclusion, we have demonstrated an enhanced representation of information derived from the original data by treating outputs from various layers as sequential data inputs to a state space model (SSM). The proposed Selective State Space Layer Aggregation (S6LA) module uniquely combines layer outputs with a continuous perspective, allowing for a more profound understanding of deep models while employing a selective mechanism. Empirical results indicate that the S6LA module significantly benefits classification and detection tasks, showcasing the utility of statistical theory in addressing long sequence modeling challenges. Looking ahead, we aim to optimize our approach by reducing parameters and FLOPs while enhancing accuracy. Additionally, we see potential for integrating further statistical models into computer science applications, suggesting a promising convergence in these fields."}, {"title": "D.1 IMAGENET CLASSIFICATION", "content": "D.1.1 IMPLEMENTATION DETAILS\nResNet For training ResNets with our method, we follow exactly the same data augmentation and hyper-parameter settings in original ResNet. Specifically, the input images are randomly cropped to 224 x 224 with random horizontal flipping. The networks are trained from scratch using SGD with momentum of 0.9, weight decay of 1e-4, and a mini-batch size of 256. The models are trained within 100 epochs by setting the initial learning rate to 0.1, which is decreased by a factor of 10 per 30 epochs. Since the data augmentation and training settings used in ResNet are outdated, which are not as powerful as those used by other networks, strengthening layer interactions leads to overfitting on ResNet. Pretraining on a larger dataset and using extra training settings can be an option.\nDeiT, Swin Transformer, PVTV2 We adopt the same training and augmentation strategy as that in DeiT. All models are trained for 300 epochs using the AdamW optimizer with weight decay of 0.05. We use the cosine learning rate schedule and set the initial learning rate as 0.001 with batch size of 1024. Five epochs are used to gradually warm up the learning rate at the beginning of the training. We apply RandAugment, repeated augmentation, label smoothing with \u2208 = 0.1, Mixup with 0.8 probability, Cutmix with 1.0 probability and random erasing with 0.25 probability. Similarly, our model shares the same probability of the stochastic depth with the MHSA and FFN layers of DeiT/CeiT/PVTv2."}, {"title": "D.1.2 MODEL COMPLEXITY WITH RESPECT TO INPUT RESOLUTION", "content": "Figure 5 illustrates the FLOPs induced by our model S6LA with respect to input resolution. We use the model PVTv2-b1 as the backbone and then derive the differences under various settings of input resolution. From this, it is apparent that complexity of our method is linear to input resolution."}, {"title": "D.1.3 COMPARISONS WITH RELEVANT NETWORKS", "content": "Layer-interaction networks For the comparison of layer-interaction-based models using CNNs, we first compare our method S6LA with Densenet, DIANet, RLA and MRLA. The comparison on the ImageNet-1K validation set are given in Table 1. From the table, it is obvious that our method outperforms in the classification task and also compared with the similar model size of DenseNet, our model performs better.\nOther relevant networks The methods in the last part, they all use the same implemental settings from ours in the training of Imagenet. However for some other models, such as BA-Net, adopted other settings. It applied cosine learning schedule and label smoothing in their training process. However, the different settings of our method and their method will give the unfair comparison. Therefore, we adopt the results given in MRLA since our method settings are same to this paper. The results are given in Table 8."}, {"title": "D.2 OBJECT DETECTION AND INSTANCE SEGMENTATION ON COCO DATASET", "content": "Implementation details We adopt the commonly used settings by which are same to the default settings in MMDetection toolkit. For the optimizer, we use SGD with weight decay of 1e-4, momentum of 0.9 and batchsize of 16 for all experiments. The learning rate is 0.02 and is decreased by a factor of 10 after 8 and 11 epochs for within the total 12 epochs. For RetinaNet, we modify the initial learning rate to 0.01 to avoid training problems. For the pretrained model, we use the model trained in ImageNet tasks.\nResults For the experiments of detection tasks in the similar settings models, it is illustrated in Table 4 and 9. For almost backbone models, our model performs better and it proves that our model"}, {"title": "D.3 VISUALIZATIONS", "content": "To investigate how S6LA contributes to representation learning in convolutional neural networks (CNNs), we utilize the ResNet-50 model as our backbone. In this study, we visualize the feature maps using score-weighted visual explanations generated by ScoreCAM as illustrated in Figure 6 and 7.\nWe specifically focus on the final convolutional layer of the ResNet-50 model and our S6LA-enhanced model. This choice is grounded in our observation that the feature maps from the initial three layers of both models exhibit remarkable similarity. In the visualization, the first column presents the original images, the second column displays the ScoreCAM images, and the third column showcases the combination of the original images and their corresponding ScoreCAM. Both two images in our analysis are randomly selected from the ImageNet validation set, ensuring a diverse representation of the data. According to the definition of the CAM method, areas highlighted in warmer colors indicate stronger contributions to the classification decision.\nFrom our visualizations, it is evident that models enhanced with S6LA exhibit larger warm regions, which align more closely with the classification labels. In contrast, the vanilla ResNet-50 model struggles to identify all relevant object areas compared to our method. This disparity suggests that our approach not only improves the localization of important features but also enhances the model's overall classification performance.\nThe findings presented in the figure provide direct evidence of the efficacy of our method in the classification task. By leveraging S6LA, we can significantly improve the interpretability of CNNs, allowing for better insights into how these models make decisions based on the features they learn. In summary, our results highlight the advantages of incorporating S6LA into standard architectures like ResNet-50, ultimately leading to more robust and accurate classification outcomes."}]}