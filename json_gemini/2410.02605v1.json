{"title": "Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative Prospect Theoretic Reinforcement Learning", "authors": ["Olivier Lepel", "Anas Barakat"], "abstract": "The widely used expected utility theory has been shown to be empirically inconsistent with human preferences in the psychology and behavioral economy literatures. Cumulative Prospect Theory (CPT) has been developed to fill in this gap and provide a better model for human-based decision-making supported by empirical evidence. It allows to express a wide range of attitudes and perceptions towards risk, gains and losses. A few years ago, CPT has been combined with Reinforcement Learning (RL) to formulate a CPT policy optimization problem where the goal of the agent is to search for a policy generating long-term returns which are aligned with their preferences. In this work, we revisit this policy optimization problem and provide new insights on optimal policies and their nature depending on the utility function under consideration. We further derive a novel policy gradient theorem for the CPT policy optimization objective generalizing the seminal corresponding result in standard RL. This result enables us to design a model-free policy gradient algorithm to solve the CPT-RL problem. We illustrate the performance of our algorithm in simple examples motivated by traffic control and electricity management applications. We also demonstrate that our policy gradient algorithm scales better to larger state spaces compared to the existing zeroth order algorithm for solving the same problem.", "sections": [{"title": "1 Introduction", "content": "In classical reinforcement learning (RL), rational agents make decisions to maximize their expected cumulative rewards through interaction with their environment. This paradigm has largely been prescribed by the expected utility theory model which has dominated decision making. Besides this risk-neutral setting, risk-seeking and risk-averse behaviors can also be individually modelled within the same expected utility maximization paradigm by considering the expectation of a modified utility function as a policy optimization objective (see e.g. Prashanth et al. [2022] for a recent survey).\nHowever, human decision makers might not act rationally due to psychological biases and personal preferences, their decisions might not necessarily be dictated by expected utility theory. Consider this simple example as a first illustration: A player must choose between (A) receiving a payoff of 80 and (B) participating in a lottery and receive either 0 or 200 with equal probability. The player's preference depends on their attitude towards risk. While a risk-neutral agent will be satisfied with the immediate and safe payoff of 80, another individual might want to try to obtain the much higher 200 payoff. In particular, different agents might perceive the same utility and the same random outcome differently. Furthermore, they can exhibit both risk-seeking and risk-averse behaviors depending on the context. Therefore, due to its failure to capture such settings as a descriptive model, the standard expected utility theory has been called into question by the pioneering behavioral psychologist Daniel Kahneman together with his colleague Amos Tversky [Kahneman and Tversky, 1979]. In particular, Daniel Kahneman has been awarded the Nobel Prize in Economic Sciences in 2002 \"for having integrated insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty\". In their seminal works combining cognitive psychology and economics, they laid the foundations of the so-called prospect theory and its cumulative version later on [Tversky and Kahneman,"}, {"title": "2 Preliminaries: From Classical RL to CPT-RL", "content": "Markov Decision Process. A discrete-time discounted Markov Decision Process (MDP) [Puterman, 2014] is a tuple $M = (S,A,P,r, \\rho, \\gamma)$, where $S$, $A$ are respectively the state and action spaces, supposed to be finite for simplicity, $P : S \\times A \\times S \\rightarrow [0,1]$ is the state transition probability kernel,"}, {"title": "3 About Optimal Policies in CPT Policy Optimization", "content": "In this section, we investigate the properties of optimal policies to (CPT-PO) when they exist. We focus on constrasting our results with existing known results for solving standard MDPs to highlight the"}, {"title": "4 Policy Gradient Algorithm for CPT-value Maximization", "content": "In this section, we propose a policy gradient algorithm for solving (CPT-PO). From this section on, we parametrize policies $\\pi\\in {\\Pi}_{\\mathbb{N}^\\mathcal{H}}$ by a vector $\\theta\\in {\\mathbb{R}}^d$ and we denote by {\\pi}_{\\theta} the parametrized policy. As a consequence, the CPT objective in (CPT-PO) becomes a function of the policy parameter $\\theta$ and we use the shorthand notation $J(\\theta)$ for the corresponding CPT objective value.\nPolicy Gradient Theorem for CPT-RL. Our key result enabling our algorithm design is a PG theorem for CPT value maximization.\nTheorem 6. Suppose that the utility functions $\\underline{u},\\bar{u}$ are continuous and that the weight functions $w_-, w_+$ are Lipschitz and differentiable. Assume in addition that the policy parametrization $\\theta \\mapsto \\pi_\\theta(a|h)$ (for any $h,a \\in {\\mathbb{H}} \\times A$) are both differentiable. Then, for every $\\theta \\in {\\mathbb{R}}^d$, the gradient of the (CPT-PO) objective $J$ w.r.t. the policy parameter $\\theta$ is given by:\n$\\nabla_{\\theta} J = {\\mathbb{E}}_{\\tau \\sim {\\mathbb{P}}_\\theta} [\\phi(R(\\tau)) \\sum_{z=0}^{H-1} \\nabla_\\theta log \\pi_\\theta(a_z|h_z)]$,\nwhere $\\phi(v) := \\int_{0}^{+\\infty} max(v,0) w_+'(\\mathbb{P}(u_+(R(\\tau)) > z))dz - \\int_{-\\infty}^{max(-v,0)} w_+'(\\mathbb{P}(u_-(R(\\tau)) > z))dz, \\forall v \\in \\mathbb{R}, w, w'$ denoting the derivatives and $R(\\tau) := \\sum_{t=0}^{H-1} \\gamma^t r_t$ with $\\tau := (s_t, a_t, r_t)_{0<t<H-1}$ is a trajectory of length H generated from the MDP by following policy {\\pi}_{\\theta}.@\nThe integral $\\mathbb{I}(R(\\tau))$ is finite under our continuity assumptions since the return R(T) is bounded.\nWe provide a few comments regarding this result. Theorem 6 recovers the celebrated policy gradient theorem for standard RL [Sutton et al., 1999] by setting $w_+$ (resp. $w_-$) to the identity function (on ${{\\mathbb{R}}}^+$ (resp. ${\\mathbb{R}}^-$) in which case $w'$ is the constant function equal to 1 and hence $\\phi(R(\\tau)) = R(\\tau)$. We stated the theorem in the general setting where the policy is non-Markovian. In practice, it is also possible to use a parametrization of a smaller policy set such as {\\Pi}_{\\sum,NS} or even {\\Pi}_{\\mathbb{M},\\mathcal{S}} in which the policy is a function of (t, st, {\\sum}_{k=0}^{t-1} \\gamma^k r_k) or only st respectively.\nStochastic Policy Gradient Algorithm for CPT-RL. In the light of Theorem 6, we will perform a policy gradient ascent on the objective $J$ to solve (CPT-PO). Our general policy gradient algorithm is presented in Algorithm 1. As usual, since we only have access to sampled trajectories from the MDP, we need a stochastic policy gradient to estimate the true unknown gradient given by the theorem. In particular, we need an approximation of $\\phi(R(\\tau))$ for any sampled trajectory $\\tau$ from the MDP following policy ${\\pi}_{\\theta}$. In the particular case of (EUT-PO) in which $w$ is the identity, the unknown quantity $\\phi(R(\\tau))$ reduces to $U(R(\\tau))$ which can be easily computed as $U$ is known and $R(\\tau)$ is the cumulative reward.\nIn the more general setting, the approximation task becomes more challenging since we need to compute the integral term {\\int}_{0}^{\\infty} w'(\\mathbb{P}(u_+(R(\\tau)) > z)dz$ (and likewise for the second integral term). We address this challenge using the following result which is a slight variation of Proposition 6 in L.A. et al. [2016] in which the integral is taken over a bounded interval. Accordingly, we end up with a"}, {"title": "5 Experiments", "content": "We demonstrate the performance of our CPT-PG algorithm in three different settings: (a) we consider a traffic control application to show the influence of the probability distortion function, (b) we illustrate the better scalability of our PG algorithm to larger state spaces compared to the existing zeroth order algorithm in a grid environment with increasing state space size and (c) we show the applicability and performance of our algorithm in a continuous state-action space setting via an electricity management application. See appendix 3 for more details (Table 2 therein) and additional simulations illustrating some of our theoretical findings of section 3 (Proposition 2 and Theorem 4).\n(a) Traffic Control. We consider a car agent which would like to reach a given destination at the other side of the city. Passing through the city center is faster on average but carries a small risk of incurring a very large delay. We model the setting as a n\u00d7n grid (see fig. 1 center). Central roads can get cluttered and peripheral roads take constant time to get through. We run our PG algorithm, the training curves are reported in Fig. 2 (center). In the risk-neutral case, we observe that the total expected return is higher than in the CPT case. This is because the risk averse policy compromises return in order to get certainty by going around the risky city center. These examples show that our algorithm is successful at"}, {"title": "6 Related Work", "content": "Risk-sensitive RL. There is a rich literature around risk sensitive control and RL that we do not hope to give justice to here. We refer the reader to recent comprehensive surveys on the topic [Garcia and Fern\u00e1ndez, 2015, Prashanth et al., 2022]. One of the approaches to risk sensitive RL which is the most relevant to our discussion consists in modifying objective functions by considering different statistics of the return deviating from the standard expectation such as the variance or the conditional value at risk (e.g. Tamar et al. [2012], Chow and Ghavamzadeh [2014], Chow et al. [2018]) or even considering the entire distribution of the returns like in distributional RL [Bellemare et al., 2023]. Another popular objective modification consists in maximizing an exponential criterion (e.g. Borkar [2002], Noorani et al. [2022]) to obtain robust policies w.r.t noise and perturbations of system parameters or variations in the environment. Noorani et al. [2022] and Moharrami et al. [2024] proposed and analyzed PG algorithms for the exponential objective criterion and leveraged the (approximate) multiplicative Bellman equation satisfied in this special case. Following the work of Tamar et al. [2015], Vijayan and LA [2023] introduced a computationally lighter PG algorithm for solving risk-sensitive RL for a class of smooth risk measures including some distortion risk measures and a mean-variance risk measure. Their approach is based on simultaneous perturbation stochastic approximation (SPSA) [Bhatnagar et al., 2013] using zeroth-order information to estimate gradients. (CPT-PO) covers several of the aforementioned objectives including smooth distortion risk measures and exponential utility as particular cases (see appendix C for more details).\nConvex and General Utility RL. In the last few years, convex RL (a.k.a. RL with general utilities) [Hazan et al., 2019, Zhang et al., 2020, Zahavy et al., 2021, Geist et al., 2022] has emerged as a framework to unify several problems of interest such as pure exploration, imitation learning or experiment design. More precisely, this line of research is concerned with maximizing a given functional of the state(-action) occupancy measure w.r.t. a policy. To solve this problem, several policy gradient algorithms have been proposed in the literature [Zhang et al., 2021, Barakat et al., 2023]. Mutti et al. [2022b,a, 2023a] challenged the initial problem formulation and proposed a finite trial version of the problem which is closer to practical concerns as it consists in maximizing a functional of the empirical state(-action) distribution rather than its true asymptotic counterpart. The particular case of our CPT policy optimization problem without probability distortion (see (EUT-PO) below) coincides with a particular case of the single trial convex RL problem [Mutti et al., 2023b] in which the function of the empirical visitation measure is a linear functional of the reward function (see appendix C.4 for details). However, our general problem is not a particular case of convex RL which does not account for probability distortions. Furthermore, our utility function is in general nonconvex in our setting (see example in Fig 6) and our policy gradient algorithm is model-free.\nCumulative Prospect Theoretic RL. Motivated by Prospect Theory and its sibling CPT [Kahneman and Tversky, 1979, Tversky and Kahneman, 1992, Barberis, 2013], L.A. et al. [2016] first proposed to combine CPT with RL to obtain a better model for human decision making. Following this first research effort, only few isolated works [Borkar and Chandak, 2021, Ramasubramanian et al., 2021, Ethayarajh et al., 2024] considered a similar CPT-RL setting. In particular, Borkar and Chandak [2021] proposed and analyzed a Q-learning algorithm for CPT policy optimization. Ramasubramanian et al. [2021] further"}, {"title": "7 Conclusion", "content": "We investigated a CPT variant of the standard RL problem to model human decision making. We provided new insights on optimal policies in such problems to highlight their peculiarity compared to classical RL. Then, we designed a novel PG algorithm for CPT-PO. Finally, we showed the benefits of our algorithm in terms of scalability compared to prior work and we illustrated its performance in applications including electricity management and traffic control. Our work opens the way to interesting avenues for future work. Using CPT usually requires to know the utility and distortion functions (or to posit models thereof) a priori. Can we learn such functions to align with the preferences of the human decision maker involved? Looking forward, investigating incentive design problems in which human agents are collectively modelled using CPT would be interesting. We hope our work will stimulate further research in better capturing the behavior and preferences of human agents in real-world decision making applications beyond expected utility."}, {"title": "A Notation for Policy Classes", "content": "Throughout this work, we will consider the following sets of policies:\n* ${{\\Pi}}_{{\\mathbb{N}}^{{\\mathcal{H}}}} := {\\{{{\\mathbb{H}} \\rightarrow \\Delta(A)}\\}$ is the set of non-Markovian policies,\n* {\\Pi}_{\\sum,NS} := {\\{{{\\mathcal{S}}\\times\\mathbb{R}\\times \\mathbb{N} \\rightarrow \\Delta(A)}\\}$ is the set of policies that only depend on the current state, the timestep and the sum of discounted rewards accumulated so far: The RL agent in state s at timestep t following policy {\\pi}\\in {{\\Pi}_{\\sum,NS}} samples its next action from the distribution {\\pi}({s, {\\sum}_{k=0}^{t-1} {\\gamma}^k r_k,t}),\n* {\\Pi}_{\\sum,\\mathcal{S}} := {\\{{{\\mathcal{S}}\\times\\mathbb{R} \\rightarrow \\Delta(A)}\\}$ is the set of policies that only depend on the state and the sum of discounted rewards: The RL agent in state s at timestep t following policy {\\pi}\\in {{\\Pi}_{\\sum,\\mathcal{S}}} samples its next action from the distribution {\\pi}({s, {\\sum}_{k=0}^{t-1} {\\gamma}^k r_k}),\n* {\\Pi}_{\\mathbb{M},NS} := {\\{{{\\mathcal{S}}\\times\\mathbb{N} \\rightarrow \\Delta(A)}\\}$ is the set of Markovian policies: An agent in state s at timestep t following policy {\\pi}\\in {{\\Pi}_{\\mathbb{M},NS}} samples its next action from the distribution {\\pi}({s,t}).\n* {\\Pi}_{\\mathbb{M},\\mathcal{S}} := {\\{{{\\mathcal{S}}\\rightarrow \\Delta(A)}\\}$ is the set of stationary Markovian policies, i.e. Markovian policies which are time-independent."}, {"title": "B Extended Related Work Discussion", "content": "Risk-sensitive RL. There is a rich literature around risk sensitive control and RL that we do not hope to give justice to here. We refer the reader to recent comprehensive surveys on the topic [Garcia and Fern\u00e1ndez, 2015, Prashanth et al., 2022] and the references therein. Let us briefly mention that there exist several approaches to risk sensitive RL. These include formulations such as constrained stochastic optimization to control the tolerance to perturbations and stochastic minmax optimization to model robustness with respect to worst case perturbations for instance. Another approach which is more relevant to our paper discussion consists in regularizing or modifying objective functions. Such modifications are based on considering different statistics of the return deviating from the standard expectation such as the variance or the conditional value at risk (e.g. Tamar et al. [2012], Chow and Ghavamzadeh [2014], Chow et al. [2018]) or even considering the entire distribution of the returns like distributional RL [Bellemare et al., 2023]. Another popular objective modification consists in maximizing an exponential criterion (e.g. Borkar [2002], Noorani et al. [2022]) to obtain robust policies w.r.t noise and perturbations of system parameters or variations in the environment. Noorani et al. [2022] designed a model-free REINFORCE"}, {"title": "C Complements about CPT Values and CPT Policy Optimization", "content": "C.1 Positioning CPT-RL in the literature\nC.2 CPT value examples"}, {"title": "D Proofs for Section 3", "content": "D.1 Unwinding MDPs for CPT-RL\nIn this section, we describe an equivalent MDP construction that will be used in some of our proofs such as for Proposition 3. For any CPT-MDP (S, A, r, P) with utility function U, we can formally define an equivalent 'unwinded' MDP3 that can be solved using classical RL techniques. For any state s\u2208 S is the original MDP and any timestep t < H \u2212 1 with cumulative reward $\\sum_{k=0}^{t} r_k$, we associate a"}, {"title": "E Proofs and Additional Details for Section 4", "content": "E.1 Proof of Theorem 6\nThe CPT value is a difference between two integrals (see definition in (1)). In what follows, we compute the derivative of the first integral assuming that the second one is zero in the CPT value. A similar treatment can be applied to the second integral. We skip these redundant details for conciseness.\nRemark 13. As we consider a finite horizon setting with finite state and action spaces, the integral on trajectories $\\tau$ are in fact finite sums, allowing us to differentiate freely. We still write the proof with {\\int} signs, signalling our hope that, under some technical assumptions, our proof could be generalized to a setting with infinite horizon and/or infinite state and action spaces.\nUsing the shorthand notation $X = \\sum_{t=0}^{H-1} \\gamma^t r_t$, we first observe that:\n$C(X) = \\int_{z=0}^{+\\infty} w(\\mathbb{P}(U(X) > z)dz = \\int_{z=0}^{+\\infty} \\int_{{\\tau}\\text{ such as } U(R(\\tau)) > z} p_\\theta(\\tau)d{\\tau}dz$,\nwhere $p_\\theta$ is the trajectory probability distribution induced by the policy ${\\pi}_\\theta$ defined for any H-length trajectory $\\tau = (s_0, a_0,..., s_{H-1}, a_{H-1})$ as follows:\n$p_\\theta(\\tau) = p(s_0) \\prod_{t=0}^{H-1} \\pi_\\theta(a_t|h_t)p(s_{t+1}|h_t, a_t)$.\nRemark 14. Recall that we have ignored the second integral in the CPT value definition for conciseness.\nStarting from the above expression (16), it follows from using the chain rule that:\n$\\nabla_\\theta C(X) = \\int_{z=0}^{+\\infty} w'(\\mathbb{P}(U(X) > z)) (\\int_{{\\tau}\\text{ such as } U(R(\\tau)) > z} \\nabla_\\theta p_\\theta({\\tau})d{\\tau}) dz$\n$\\quad = \\int_{z=0}^{+\\infty} w'(\\mathbb{P}(U(X) > z)) (\\int_{{\\tau}\\text{ such as } U(R(\\tau)) > z} p_\\theta({\\tau})\\nabla_\\theta \\log p_\\theta({\\tau})d{\\tau}) dz$\n$\\quad =  \\int_{z=0}^{+\\infty} \\int_{{\\tau}\\text{ such as } U(R(\\tau)) > z} w'(\\mathbb{P}(U(X) > z)) p_\\theta({\\tau})\\nabla_\\theta \\log p_\\theta({\\tau})dzd{\\tau}$\n$\\quad = \\int_\\tau \\phi(U(R(\\tau))) \\nabla_\\theta p_\\theta({\\tau})d{\\tau},$\nwhere $\\phi(t) := {\\int}_{z=0}^{+\\infty} w'(\\mathbb{P}(U(X) > z))dz$ for any real $t$.\nWe now use the standard log trick to rewrite our integral as an expectation:\n$\\nabla_\\theta C(X) = {\\int}_\\tau [\\phi(U(R(\\tau))) p_\\theta({\\tau})\\nabla_\\theta \\log p_\\theta({\\tau})]d{\\tau} = {\\mathbb{E}}_{{\\tau}\\sim p_\\theta}[\\phi(U(R(\\tau))) \\nabla_\\theta \\log p_\\theta({\\tau})].$\nFurthermore, we can expand the gradient of the score function using (17) as follows:\n$\\log p_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{H-1} \\log \\pi_\\theta(a_t|h_t) + \\sum_{t=0}^{H-1} \\log p(s_{t+1}|h_t, a_t),$\n$\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=0}^{H-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|h_t),$"}, {"title": "F More Details about Section 5 and Additional Experiments", "content": "F.1 Additional figure\nF.2 Illustration of Proposition 2: about the need for stochastic policies in CPT-RL\nF.3 Illustration of Theorem 4: Markovian vs non-Markovian policies for CPT-RL\nF.4 Grid Environment\nF.5 Traffic Control\nF.6 Electricity Management"}]}