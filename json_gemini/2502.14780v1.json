{"title": "ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting", "authors": ["Abhijit Mishra", "Richard Noh", "Hsiang Fu", "Mingda Li", "Minji Kim"], "abstract": "Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (<500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.", "sections": [{"title": "1 Introduction", "content": "The increasing adoption of conversational AI in augmented reality (AR), virtual reality (VR), and modern smartphones has heightened the need for multimodal AI systems that seamlessly process text, images, speech, and gestures. Devices like Meta Ray-Ban Smart Glasses and Apple Vision Pro are transforming human-computer interaction, enabling users to issue spoken commands while interacting with digital and physical environments. For instance, a user wearing smart glasses might say, \"Call this number\" while looking at a business card, or \"Add this to my calendar\" while viewing an event flyer. Handling such task-oriented multimodal commands requires AI models capable of interpreting visual context, rewriting instructions into structured text, and executing them within a conversational AI framework, all while maintaining user privacy. Notably, a key challenge in these systems is privacy \u2013 many interactions involve sensitive information that should ideally be processed on-device rather than being sent to cloud servers.\nWhile large vision-language models (VLMs) such as PaLI-X, LLaVA, and Qwen-VL (Chen et al., 2023; Liu et al., 2023b; Team, 2023) have demonstrated impressive multimodal capabilities, they are often impractical for on-device deployment due to their size, requiring cloud-based inference. Also, transmitting private visual and textual data to external servers raises security risks and compromises user privacy. On the other hand, smaller models suitable for local execution often lack the broad world knowledge embedded in larger models (Zhang et al., 2023), making them less effective in complex multimodal understanding.\nTo address this, we propose ReVision, an approach based on Visual Instruction Rewriting that converts multimodal instructions into text-only commands, allowing privacy-preserving on-device execution. By transforming complex visual interactions into structured text, existing lightweight conversational AI models can efficiently process user instructions without sending sensitive visual data to external servers. We introduce a curated dataset consisting of (image, original instruction, rewritten instruction ) triplets, covering diverse real-world tasks. A freshly built compact 250 Million parameters vision-language model (Liu et al., 2023b) is fine-tuned on this dataset and evaluated using NLG metrics (such as BLEU, METEOR, ROUGE) and semantic parsing accuracy.\nOur findings demonstrate that our compact"}, {"title": "2 Related Work", "content": "Instruction or query rewriting and semantic parsing have been widely explored in conversational AI to improve query understanding and response generation. Early methods relied on rule-based transformations and supervised learning (Kamath et al., 2020), while recent advances leverage LLMs for dynamic query refinement (Ye et al., 2023; Mo et al., 2023). Generative query rewriting frameworks such as LLM-R2 (Zhang et al., 2024b) enhance text ranking, and personalized query rewriting methods (Cho et al., 2021) refine queries based on user preferences. However, these techniques focus primarily on textual query transformations and do not extend to multimodal task-oriented instruction processing. Visual instruction tuning has emerged as a key development in multimodal AI, with models like LLaVA (Liu et al., 2023a) and PaLI-X (Chen et al., 2023) demonstrating strong vision-language capabilities. While these models excel in multimodal question answering, they are not optimized for rewriting task-oriented instructions. Similarly, Patel et al. (Patel et al., 2020) explore generating natural questions from images for multimodal assistants, but their work focuses on question generation rather than instruction rewriting. Unlike these approaches, our work introduces a dedicated dataset and a compact model for Visual Instruction Rewriting, specifically designed to convert multimodal user instructions into structured text for privacy-preserving, on-device execution.\nThe closest work to ours is MARRS (Ates et al., 2023), which integrates multimodal reference resolution with query rewriting to improve conversational grounding. However, MARRS relies on rule-based replacements after reference resolution in a non-VLM setting, whereas our approach focuses on learning-based instruction rewriting to enable structured task execution from multimodal inputs. Other highly relevant studies are by Zhang et al. (2022) and Wei et al. (2021), which investigate whether open domain text-based QA systems can handle visual knowledge questions by reformulating them into purely textual queries. Their work highlights the effectiveness of query rewriting in bridging the gap between vision and language using a modular approach different from ours but aligns closely with our goal of rewriting multimodal instructions into structured text. However, while their approach focuses on adapting visual questions for open-domain QA, our work is specifically designed for task-oriented instruction execution, making it applicable to a broader set of real-world multimodal interactions."}, {"title": "3 Constructing a Dataset for Visual Instruction Rewriting", "content": "Task-oriented conversational AI systems rely on a semantic parser to interpret user intent and extract structured arguments (Louvan and Magnini, 2020; Aghajanyan et al., 2020). For example, when a user says, \"Add the team meeting to my calendar for Friday at 3 PM\", the system must parse the intent (CreateCalendarEvent) and extract arguments such as the EventTitle (\u201cteam meeting", "Friday\"), and EventTime (\u201c3 PM": "to schedule the event correctly. Unlike purely text-based interactions, multimodal instructions, particularly those directed at conversational AI assistants on AR/VR devices (e.g., Apple's Siri for Apple Vision Pro), introduce additional challenges such as ellipsis and coreference resolution. For instance, a user may look at a book cover and ask, \u201cWho wrote this?\u201d or point at a product in an AR interface and say, \"How much does this cost?\" Traditional text-based semantic parsers struggle with such instructions since critical visual context is missing. Thus, to bridge the gap between multimodal input and existing conversational AI stacks, we introduce a dataset specifically designed for rewriting multimodal instructions into structured text that can be processed by standard text-based semantic parsers. \nTo construct our dataset, we first define an ontology of intents and arguments, as existing ontologies in conversational AI and semantic parsing are often proprietary and unavailable for research use. We take inspiration from Goel et al. (2023) for ontology and extend it to accommodate multimodal task-oriented interactions."}, {"title": "4 Developing a Baseline Rewriter VLM", "content": "We develop a lightweight vision-language model (shown in Figure 3) tailored for instruction-following tasks by integrating a pretrained vision encoder with an instruction-tuned language model, following the popular multimodal fusion approach (Zhang et al., 2024a). Since vision encoders and instruction-tuned language models operate in different embedding spaces, a multimodal projector (Liu et al., 2023b) is used to align the encoded image features with the token embedding space of the language model. Our approach is similar to PaLI-Gemma (Beyer et al., 2024), where an image encoder based on the SigLIP architecture (Zhai et al., 2023) extracts D-dimensional image encodings for N patches from a single input image, say $V_1, V_2, ..., V_N$. Building on Lauren\u00e7on et al. (2024), who demonstrated that using a sampling technique to extract the most relevant M patch encodings from a larger set of N samples improves efficiency, we employ Perceiver Sampler (Jaegle et al., 2021) to downsample the N patch embeddings into M D-dimensional encodings. These image encodings are then mapped into a shared embedding space via a linear multimodal projector, ensuring compatibility with the language model's H-dimensional token embeddings. We fix K at 64. The projected image embeddings $(H_1, H_2, ..., H_M)$ are concatenated with the token embeddings extracted from the tokenized textual input $(H'_1, H'_2, ..., H'_K)$, where K represents the number of input tokens. The combined embeddings are then processed by the language model to generate responses. To ensure consistency in input representation, we apply image preprocessing, tokenization, and chat template formatting, making the model familiar with structured multimodal input formats.\nAlthough large-scale vision-language models typically involve hundreds of millions of parameters, our focus is on designing a compact and efficient model capable of running on-device. To maintain a parameter budget under 250M, we select a small SigLIP encoder (Zhai et al., 2023) (google/siglip-base-patch16-256), which processes images of size 256 \u00d7 256 by dividing them into 16 \u00d7 16 patches, with 768 dimensions in hidden layers. The language model is a 150M-parameter instruction-tuned model from OuteAI (OuteAI/Lite-Mistral-150M-v2-Instruct) based on the Mistral architecture (Jiang et al., 2023), featuring a vocabulary size of 32,768 and a hidden dimension of 768. Since the hidden dimensions of both the vision encoder and the language model are identical, the projector acts purely as a dimensional transformer without altering the shape of the embeddings. While the model's limited size may impact its ability to handle multi-turn conversations, it is well-suited for single-turn multimodal instruction rewriting tasks. Additionally, since the model is designed for multimodal deixis resolution, it may not be effective for resolving text-only references in extended conversations(Ates et al., 2023)."}, {"title": "4.1 Model Pretraining", "content": "To pretrain the model, we adopt an end-to-end training strategy, leveraging datasets from three key sources: (a) LLaVA-ReCap-CC3M, (b) LLaVA-Pretrain, and (c) LLaVA-CC3M-Pretrain-595K. These datasets are curated from large-scale image-text corpora, including LAION (Schuhmann et al., 2021), Conceptual Captions (CC) (Sharma et al., 2018), and SBU (Ordonez et al., 2011), which are filtered for balanced concept distribution and enhanced with synthetic captions generated via BLIP to improve vision-language alignment (Lab, 2023; Liu, 2023b,a). Specifically, LLaVA-ReCap-CC3M focuses on re-captioning images to improve concept coverage, while LLaVA-Pretrain consists of 558K image-caption pairs, forming a strong foundational dataset for multimodal alignment. The LLaVA-CC3M-Pretrain-595K dataset, derived from Conceptual Captions 3M, provides a rich set of image-text pairs to enhance model robustness. The total number of examples is thus a little more than 4M. Despite some redundancy in images across datasets, we ensure sufficient data diversity and scale to instill basic image-text alignment capabilities in our pretrained model.\nFor pretraining, we use the following configurations: a batch size of 16, trained for 2 epochs, using the AdamW optimizer with a learning rate of 2 \u00d7 10-5 and a linear learning rate schedule. The training was conducted on consumer-grade GPUs (NVIDIA RTX 3090) over 3 days, using PyTorch and Hugging Face's Transformers library for implementation."}, {"title": "4.2 Model Fine-Tuning", "content": "For the instruction rewriting task, we conduct fine-tuning under multiple configurations, trained on our dataset (3). We will refer to the rewritten prompts from this dataset as the \u201creference\" prompts. Below, we describe the fine-tuning setups and the rationale behind integrating metadata-driven enhancements to improve performance on text-dense images.\nReVision-BL: This is the baseline fine-tuned model. The input consists of an image, a rewrite prompt, and an instruction, while the model generates a rewritten version of the instruction in response.\nReVision-Metadata: In this, we augment the input with \"metadata\", namely the image caption and an external OCR-extracted text. To differentiate the rewrite prompt and instruction from the auxiliary metadata, we prefix the prompt-instruction and metadata sections with <task> and <data>, respectively. Collectively, the input consists of an image, a prefixed rewrite prompt and instruction, and a prefixed caption and OCR text and the output is a rewritten instruction.\nThe motivation for integrating metadata arises from the limitations of small-scale vision-language models (VLMs). Despite being optimized for rewriting tasks, small VLMs struggle with extracting embedded text from images. OCR is a specialized capability distinct from traditional vision-language alignment (Lamm and Keuper, 2024; Nagaonkar et al., 2025). However, most modern devices are equipped with built-in OCR and image description capabilities, making it practical to supplement the model with external text recognition systems. To systematically evaluate this approach, we present two different metadata extraction:\nGPT-4o_Caption+OCR: We use GPT-4o to generate both captions and OCR-extracted text, simulating a high-end device equipped with an advanced OCR and captioning system.\nSelf_Caption+EasyOCR: We use rewriter models to generate captions themselves using the simple prompt: \"Caption this:\". For OCR, we employ EasyOCR, a lightweight text extraction model based on the CRAFT algorithm (Baek et al., 2019), simulating a low-resource on-device setting.\nThe fine-tuning procedure follows a similar framework as pretraining but with optimized hyperparameters for smaller-scale adaptation. The vision encoder is frozen during fine tuning and the number of training epochs is increased from 2 to 5 to compensate for the smaller dataset size. The batch size remains at 16, but gradient accumulation steps are reduced from 4 to 1, allowing for more frequent model updates. The learning rate remains stable at 2 \u00d7 10-5 with the same linear rate schedule, maintaining a conservative optimization approach. Additionally, the number of warm-up steps is lowered from 100 to 10, reflecting the shorter training duration. To simulate a realistic fine-tuning environment where such models could be updated on-device, we conduct fine-tuning on a consumer-grade desktop equipped with an NVIDIA GeForce RTX 2070 SUPER (8GB VRAM). Each fine-tuning run took approximately 5.5 to 6 hours.\nFor baseline comparisons, we evaluate our model against state-of-the-art small-scale VLMs: PaliGemma-v2 (10B) and QwenVL-v2 (7B), known for strong performance in OCR, captioning, and multimodal reasoning. However, deploying these models on-device is impractical without high-end GPUs. To ensure a fair comparison, we assess them as-is with optimized prompting but without fine-tuning, reflecting real-world constraints. While fine-tuning could improve accuracy, their size and hardware demands make them unsuitable for mobile applications, thus highlighting the need for lightweight models like ours.\nTo further assess on-device deployment feasibility, we evaluated the 8-bit quantized version of our fine-tuned models. This approach reduces memory by up to fourfold, lowering computational demands while maintaining competitive performance. Though quantization may slightly reduce accuracy, the simplicity of the rewriting task makes this tradeoff worthwhile. We examine whether an 8-bit model can efficiently handle multimodal instruction rewriting while staying lightweight for real-world use."}, {"title": "5 Evaluation Metrics", "content": "To evaluate our models in Visual Instruction Rewriting, we use standard NLG metrics (BLEU, METEOR, ROUGE) (Sharma et al., 2017) alongside task-specific semantic parsing evaluations. While NLG metrics assess linguistic similarity, they do not capture functional quality in downstream AI systems. Effective rewriting must ensure instructions remain interpretable by semantic parsers extracting intent and arguments (Louvan and Magnini, 2020). In the absence of a baseline parser for our ontology (Figure 5), we use GPT-4 as a proxy parser to classify intents and extract arguments, simulating an on-device parser. We compare GPT-4-generated parses for reference and model rewrites to verify preservation of intent and structured arguments. We measure the following metrics:\nIntent Accuracy: Exact match of intent labels between reference and model-generated rewrites, assessing task-specific intent preservation.\nArgument Similarity: Mean Jaccard Similarity (MJS) between argument labels from reference and model rewrites, ensuring retention of key task-related arguments."}, {"title": "6 Results and Discussion", "content": "Table 3 presents the evaluation results for both baseline models (BL) and our proposed ReVision models across Language Generation (NLG) metrics (ROUGE, BLEU, METEOR) and semantic parsing performance (Intent Accuracy and Argument Mean Jaccard Similarity). We also provide anecdotal examples in Figure 7 in the Appendix to illustrate the strengths and limitations of various models.\nAs we can see, the baseline models struggle significantly in the rewriting task, not because they are inherently weak, but because they are not explicitly tuned for rewriting. While PaliGemma2-10B and QwenVL-7B have demonstrated strong performance in various vision-language tasks, they are not optimized to follow meta-instructions like rewriting. This is evident in their vanilla versions (BL1a, BL16), where ROUGE-1 scores remain low (3.4% and 43.7%), BLEU is nearly negligible (0.03% and 12.3%), and Intent Accuracy is poor (16.2% and 50.3%). A key issue is that these models misinterpret the task, often either responding to the instruction directly or attempting to autocomplete the instruction instead of rewriting it. Since many input instructions are imperative, task-oriented instructions, they frequently refuse to generate a rewrite (e.g., replying \"I can't help with that\") or incorrectly complete the instruction, significantly degrading both NLG and parsing metrics. Moreover, since they are relatively small models (<10B parameters), they lack the necessary world knowledge and instruction-following capabilities to recognize and execute rewriting as a structured transformation task effectively. Additionally, prompting these models with Self_Caption+EasyOCR metadata (BL2a, BL2b) helps slightly, especially for QwenVL-7B, where Intent Accuracy improves from 50.3% to 61.2%. However, the models still struggle with ROUGE and BLEU scores, reinforcing that generic VLMs require dedicated instruction tuning to handle the rewriting task effectively.\nIn contrast, our proposed ReVision models, specifically trained for rewriting, significantly outperform all baselines. Even without metadata enhancements, ReVision-BL already outperforms the input-augmented baseline models, achieving ROUGE-1 of 56.9%, BLEU of 27.7%, and Intent Accuracy of 56.5%. This confirms that explicit tuning for rewriting is essential and that even a compact, instruction-tuned VLM can surpass larger models that lack task-specific optimization. These observations are also corroborated by the intent category-wise F1 scores reported in Figure 4.\nWith metadata, the performance further improves. ReVision-Metadata with GPT4-derived captions and OCR Text achieves 72.4% ROUGE-1, 49.9% BLEU, and 62.4% Intent Accuracy, showing that supplementing the input with extracted text helps models disambiguate multimodal instructions and produce more accurate rewrites. The best-performing model, ReVision-Metadataself_Caption+EasyOCR, achieves the highest scores across all metrics, confirming that even lightweight OCR and captioning models can be leveraged to improve rewriting quality. Lastly, the 8-bit quantized version of the best-performing model offers competitive performance to its full-precision counterpart, with only a minor drop in Intent Accuracy (67.6% vs. 71.5%) but a slight improvement in Argument Similarity (79.5%). This demonstrates that 8-bit models can be effectively deployed on resource-constrained devices.\nDespite the strong performance of our ReVision model variants, certain limitations prevent even higher accuracy. One major challenge is the loss of fine-grained text details due to image downsampling to 256 \u00d7 256 resolution, making it difficult for the model to capture small but critical information, such as ingredient lists or nutritional facts on product packaging. Additionally, the lack of explicit reference localization in the dataset restricts the model's ability to map user intent to specific image regions, leading to errors in object disambiguation and instruction alignment. To address these challenges, future work can incorporate bounding box annotations to provide spatial grounding cues for better reference resolution. Processing localized image regions instead of entire downsampled images could reduce information loss, especially for text-heavy visual instructions. This aligns with Pali-Gemma's short-resolution increase technique (Beyer et al., 2024), which enhances fine-grained visual understanding. Despite these limitations, our results confirm that task-specific instruction tuning and metadata augmentation significantly enhance multimodal rewriting, ensuring scalable and efficient on-device deployment."}, {"title": "7 Conclusion and Future Work", "content": "In this work, we explored Visual Instruction Rewriting as a lightweight, privacy-preserving approach to multimodal interaction on AR, VR, and smartphone devices. With a strong emphasis on dataset development, we present a diverse collection of 39,000+ examples covering 14 domains, enabling robust training for on-device instruction rewriting. Our approach ensures that text-only inference is more secure in privacy-sensitive settings by eliminating the need to send personal vision-related images to the server, reducing data exposure risks. Additionally, rewriting removes the necessity of storing images, making multimodal AI systems more efficient and privacy-focused. Our experimental results show that even an 8-bit quantized model maintains strong performance while significantly reducing memory requirements. For future work, we aim to expand data coverage by incorporating more diverse real-world multimodal instructions and introducing multilingual support to enhance accessibility. Furthermore, improving deixis resolution with bounding box annotations and localized image region training will enhance reference grounding while integrating gaze tracking and tactile input can further refine contextual understanding in on-device AI assistants.\nWhile our approach demonstrates strong performance in Visual Instruction Rewriting, several limitations remain. First, image downsampling to 256 x 256 resolution can lead to the loss of fine-grained text details, affecting instructions that rely on small-font information, such as nutritional labels or product specifications. Second, deictic reference resolution remains challenging, especially in images with multiple similar objects where the model lacks explicit localization cues. The absence of bounding box annotations in our dataset limits the model's ability to disambiguate references, leading to errors in object-grounded instructions. Additionally, while our model is lightweight and optimized for on-device execution, it still lags behind larger VLMs in handling complex multimodal instructions requiring deep reasoning and external world knowledge. Lastly, our dataset, while diverse across 14 domains, is monolingual, limiting applicability to multilingual and culturally varied settings. Future work can address these challenges by increasing dataset coverage, incorporating localized image region processing, and adding bounding box annotations to improve reference resolution and multimodal grounding."}, {"title": "Ethics Statement", "content": "This work prioritizes privacy and ethical considerations by designing a lightweight, on-device Visual Instruction Rewriting system that eliminates the need to transmit personal vision-related data to external servers. By converting multimodal instructions into text-only commands, our approach reduces data exposure risks and ensures secure, user-controlled inference. Our dataset is sourced from publicly available and academic-use image collections, ensuring compliance with fair use and licensing policies. However, we acknowledge potential biases in data distribution and the need for greater multilingual and cultural inclusivity. Future efforts will focus on expanding dataset diversity, improving fairness in multimodal understanding, and ensuring responsible AI deployment in real-world applications.\nAdditionally, we acknowledge the use of OpenAI's ChatGPT-4 system solely for enhancing writing efficiency, generating LaTeX code, and aiding in error debugging. No content related to the survey's research findings, citations, or factual discussions was autogenerated or retrieved using Generative AI-based search mechanisms. Our work remains grounded in peer-reviewed literature and ethical academic standards."}]}