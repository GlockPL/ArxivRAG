{"title": "DYNAMIC LOSS-BASED SAMPLE REWEIGHTING FOR IMPROVED LARGE LANGUAGE MODEL PRETRAINING", "authors": ["Daouda A. Sow", "Herbert Woisetschl\u00e4ger", "Saikiran Bulusu", "Shiqiang Wang", "Hans-Arno Jacobsen", "Yingbin Liang"], "abstract": "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally, overlooking\nthe importance or relevance of individual samples throughout the training pro-\ncess. Existing reweighting strategies, which primarily focus on group-level data\nimportance, fail to leverage fine-grained instance-level information and do not\nadapt dynamically to individual sample importance as training progresses. In this\npaper, we introduce novel algorithms for dynamic, instance-level data reweighting\naimed at improving both the efficiency and effectiveness of LLM pretraining. Our\nmethods adjust the weight of each training sample based on its loss value in an\nonline fashion, allowing the model to dynamically focus on more informative or\nimportant samples at the current training stage. In particular, our framework allows\nus to systematically devise reweighting strategies deprioritizing redundant or unin-\nformative data, which we find tend to work best. Furthermore, we develop a new\ntheoretical framework for analyzing the impact of loss-based reweighting on the\nconvergence of gradient-based optimization, providing the first formal characteri-\nzation of how these strategies affect convergence bounds. We empirically validate\nour approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter\nLLMs to smaller-scale language models and linear regression problems, demon-\nstrating that our loss-based reweighting approach can lead to faster convergence\nand significantly improved performance.", "sections": [{"title": "INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) (Brown et al., 2020; Raffel et al., 2020;\nTouvron et al., 2023; Chowdhery et al., 2023; Achiam et al., 2023; Dubey et al., 2024) has revolution-\nized natural language processing and artificial intelligence (AI) capabilities. These models, trained\non billions or trillions of tokens, exhibit remarkable generalization capabilities across a wide range of\ndownstream tasks. As datasets grow to web-scale proportions and models become increasingly large,\nthe need for more efficient training techniques has become paramount.\nExisting approaches to LLM pretraining predominantly involve two phases: heavy data curation\n(Longpre et al., 2023; Dubey et al., 2024; Wettig et al., 2024; Penedo et al., 2024) and training\nwith uniform sampling on the constructed corpus. Data curation for LLM pretraining typically\ninvolves a combination of automated filtering techniques and manual quality checks. For instance,\nheuristic-based filters are often employed to remove low-quality content and deduplicate data. Some\napproaches use perplexity-based filtering or auxiliary classifiers (Gao et al., 2020; Penedo et al., 2023)\nto identify high-quality samples. Manual curation is then applied to refine these filtered datasets,\noften involving human evaluation of subsets of the data to ensure quality and relevance. However,"}, {"title": null, "content": "these approaches face significant limitations due to scalability issues and the static nature of data\nselection. First, as the pretraining corpora grow to hundreds of billions of tokens, manual curation\nbecomes increasingly infeasible. The sheer volume of data makes it impractical for humans to review\neven a small fraction of the dataset. Second, data curation methods cannot adapt to the changing\nimportance of samples during the training process, e.g., as the model improves over time certain\nsamples that were useful early in training can become irrelevant in later training stages.\nIn addition, conventional gradient-based training pipelines often treat all data points as equally\ninformative regardless of their individual importance at current training stage. This uniform sampling\nstrategy, while simple, overlooks the nuanced diversity within large-scale datasets and potentially\nwastes computational resources on less informative samples. Recent work (Xie et al., 2023; Fan et al.,\n2023) has explored group-level reweighting strategies, which adjust the importance of entire domains\nor groups of data. While these methods have shown promise, they operate at a coarse level, do not\nfully leverage the fine-grained information available at the instance level, and still lack dynamicity in\nimportance assignment during the training phase.\nGiven these limitations, this paper seeks to address the following fundamental question:\nHow can we dynamically leverage instance-level information to accelerate training and improve\nmodel performance without incurring significant computational overhead while potentially reducing\nthe need for extensive data curation?\nAnswering this question presents several intertwined technical challenges, particularly in the context\nof LLM pretraining. First, unlike in computer vision, where samples are seen repeatedly over the\ncourse of training (often hundreds of times), in LLM pretraining, individual samples are typically en-\ncountered only once due to the vast size of the datasets. This renders dynamic reweighting approaches\nprimarily devised for multi-epoch training or relying on historical statistics, such as previous loss\nor gradient norms (Loshchilov & Hutter, 2015; Jiang et al., 2019), inadequate within the context of\nLLM pretraining. Second, storing previous statistics for individual samples becomes computationally\ninfeasible when working with web-scale datasets, and methods requiring the persistent storage of\nsample-level data would significantly inflate resource requirements. Furthermore, for a general-\npurpose LLM, leveraging small hold-out validation sets to compute importance weights through, e.g.,\nbilevel optimization as in Grangier et al. (2023), is ineffective.\nAddressing these challenges, our work makes the following significant contributions:\nInstance-Level Loss-Based Reweighting Strategies. We introduce and systematically study a\nvariety of instance-level, loss-based reweighting strategies for improving both the efficiency and\neffectiveness of ML training, especially within the context of LLM pretraining. Each strategy is\nmeticulously designed to achieve specific goals, such as focusing the learning dynamics on different\nparts of the loss distribution. This builds on recent work that emphasizes the importance of data\ndiversity and sample importance in pretraining and extends previous works on group-level reweighting\nfor LLM pretraining, such as DoReMi Xie et al. (2023), DoGE Fan et al. (2023). It also adds a\nnew dimension by incorporating more fine-grained per-sample dynamics rather than domain-level\nadjustments. In fact, combining our reweighting schemes with DoGE/DoReMi achieves better or\ncomparable performance on various few-shot reasoning benchmarks when we train on the SlimPajama\ndataset. Moreover, our study reveals that, in general, strategies down-weighting the importance of\nlow-loss samples tend to consistently yield performance improvements across various scenarios.\nNew Theoretical Framework. We develop a new theoretical framework for analyzing the effects of\nloss-based reweighting on training acceleration. To the best of our knowledge, this represents the\nfirst explicit characterization of loss reweighting effects within the convergence bounds of gradient\nmethods under widely adopted loss geometries. Our derived convergence bounds provide theoretical\njustification for the empirical success of downweighting low-loss samples, demonstrating faster\nconvergence under this strategy.\nEmpirical Validation. We conduct extensive experiments that corroborate our claims and support\nour theoretical findings. Notably, we demonstrate that the advantages of downweighting low-loss\nsamples are observed across a spectrum of problem scales and complexities: (i) in complex, large-\nscale problems such as LLM pretraining with number of parameters ranging from 124M to 7B, our\napproach leads to notably improved performance and faster convergence; (ii) in simple, small-scale\nproblems like linear regression, the results highlight the fundamental nature of our findings."}, {"title": "RELATED WORK", "content": "Training Data Re-weighting/Selection for LLMs. Several recent studies (Xie et al., 2023; Chen\net al., 2023; Fan et al., 2023; Thakkar et al., 2023) have explored various reweighting techniques\nto enhance the generalization and efficiency of language models pretraining. For instance, Xie\net al. (2023) and Fan et al. (2023) optimize the composition of pretraining corpora to achieve\nbetter performance across pretraining domains or for out-of-domain generalization. Chen et al.\n(2023) introduce a framework for ordered skill learning, optimizing data selection based on how\neffectively it teaches interdependent skills for continual pretraining and fine-tuning regimes. Although\neffective, these techniques operate at the group level, whereas our work explores reweighting at\nthe instance level, offering finer control over how individual samples are treated based on their\nloss values. Furthermore, we demonstrate that combining domain-level methods such as DoReMi\n(Xie et al., 2023) or DoGE (Fan et al., 2023) with our instance-level reweighting methods results\nin improved performance across multiple domains. Instance-level reweighting has been used in\npost-training settings of LLMs (Chen et al., 2024; Jiang et al., 2024). Jiang et al. (2024) boost the\nself-improvement abilities of LLMs by employing sample reweighting to filter out self-generated data\nthat have correct answers but exhibit high distribution shifts. Chen et al. (2024) reweight individual\nsamples during continual training/instruction-tuning to focus on medium-loss samples. In contrast,\nour work systematically studies the effects of various sample-level, loss-based reweighting strategies\non the efficiency and effectiveness of LLMs pretraining. The approach in Fan & Jaggi (2023) offers\na curriculum learning framework that prioritizes samples with a higher learnability score, which\nis precomputed using another auxiliary model similar to DoReMi and DoGE. While we do not\nexplicitly address curriculum learning in this work, our re-weighting mechanisms naturally allow for\nimplementing a form of loss-based curriculum learning algorithms without the need to train and store\nadditional proxy models as in Fan & Jaggi (2023).\nSample-level Re-weighting as generic ML solution. Sample-level reweighting has been extensively\nexplored in other machine learning areas. For image classification, Loshchilov & Hutter (2015), Jiang\net al. (2019), and Katharopoulos & Fleuret (2018) introduce pioneering approaches that prioritize\nsamples based on loss values or gradient norms with the goal of accelerating training speed. Although\nthese methods emphasize the importance of selecting high-loss samples during training, they typically\nrequire additional forward/backward passes on each training sample and are primarily designed for\nmulti-epoch training, limiting their applicability with the vast pretraining corpus used for LLMs.\nUsing bilevel optimization, Grangier et al. (2023) adapt the data distribution during training to\nfocus more on relevant samples for a target data distribution. Similarly, Ren et al. (2018) employ a\nmeta-learning approach to adjust sample weights based on validation set performance, which excels\nin noisy and imbalanced datasets. In this work, we introduce and investigate various lightweight,\nloss-based reweighting techniques that add little to no computational overhead compared to uniform\nsampling and do not require any nested optimization routines, often arising with bilevel optimization\nand/or meta-learning. Sample-level reweighting has also been explored in the context of adversarial\nmachine learning (Zhang et al., 2020; Liu et al., 2021; Zeng et al., 2021; Sow et al., 2023), domain\nadaptation (Jiang & Zhai, 2007; Fang et al., 2020), data augmentation (Yi et al., 2021), and imbalanced\nclassification (Qi et al., 2021; Ren et al., 2018). Our reweighting mechanisms also have the potential\nto be studied under these contexts, which we leave as future work."}, {"title": "PRELEMINARIES: AUTOREGRESSIVE LANGUAGE MODELING & GOAL", "content": "Large language models (LLMs) are typically trained using autoregressive language modeling, where\nthe objective is to predict the next token in a sequence given the previous tokens. Formally, given a\nsample sequence of tokens $x = (x_1,x_2,...,x_T)$, where $x_t$ represents the t-th token in the sequence,\nthe LLM model parameterized by $\\theta \\in \\mathbb{R}^d$ is trained to maximize the likelihood of the sequence:\n$PLLM(x; \\theta) = \\prod_{t=1}^T P(x_t|x_1,..., x_{t-1};\\theta)$.\nwhere $P(x_t|x_1,..., x_{t-1}; \\theta)$ denotes the conditional probability of generating token $x_t$ given $\\theta$ and\nall previously seen tokens $x_1,..., x_{t-1}$. In practice, autoregressive LLMs typically compute the loss\n$f(x; \\theta)$ for sample x using the negative log-likelihood (NLL) of the predicted tokens.\n$f(x; \\theta) = - log PLLM(X; \\theta) = \\sum_{t=1}^Tlog P(x_t|x_1,..., x_{t-1}; \\theta)$."}, {"title": "APPROACH", "content": "Based on these sample losses, conventional SGD-like algorithms will then update the model parame-\nters with the average gradient over all samples in a batch B at each training step t, i.e.,\n$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{|B|} \\sum_{i \\in B} \\nabla f(x; \\theta_t)$,\nwhich results in each sample contributing equally to the overall training update. While this approach\nenjoys simplicity, it inherently treats all data points as equally informative, regardless of their\nindividual relevance or difficulty. This uniform treatment overlooks the nuanced diversity within the\ndata, which is particularly critical for current LLMs trained on vast and heterogeneous web-scale\ndatasets. In such scenarios, the ability to dynamically differentiate between high-value and lesser-\nvalue data samples at different training stages is paramount for optimizing training efficiency and\nmodel performance.\nIn contrast, our work introduces and benchmarks a variety of simple, lightweight reweighting strate-\ngies that can be seamlessly integrated into existing training pipelines with little to no computational\noverhead. These strategies aim to address the inherent limitations of the standard uniform averaging\nmethod by assigning dynamic importance to individual data samples based on their loss values, thus\nproviding a more refined control over the training process. Ultimately, our objective is twofold: (i)\nto improve the efficiency and effectiveness of LLM training by allowing the model to dynamically\nfocus on more relevant or informative data at different training stages and (ii) to potentially pro-\nvide a scalable, automated approach to online data selection during pretraining. This ambitiously\nseeks to reduce the need for extensive manual data curation and selection efforts, which are increas-\ningly becoming infeasible given the growing size and diversity of pretraining corpora. We envision\nthese reweighting strategies as a foundation for more efficient and adaptive data utilization in LLM\npretraining, ultimately reducing the time, cost, and complexity associated with model pretraining."}, {"title": "TECHNICAL CHALLENGE: DATA REWEIGHTING FOR LLMS MUST BE DYNAMIC AND\nFULLY ONLINE", "content": "The core of our framework is to dynamically reweight individual training samples based on their\nimportance at different training stages. Although sample importance seems static intuitively,\n\"garbage data is garbage data\" regardless of model or training stage-data reweighting should be\ndynamic. For instance, similar or duplicated samples in training corpora may be useful early in\ntraining, but should be deprioritized once the model captures their patterns. Similarly, a hard but\nuseful sample could be assigned a larger weight, however, as the model learns it could be beneficial\nto reduce its weight. Based on these observations, we update the model at each step t using\n$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{|B|} \\sum_{i \\in B} w(x_i; \\theta_t) \\nabla f (x_i; \\theta_t)$,\nwhere $\\sum_{i \\in B} w(x_i; \\theta_t) = 1$ and the weight $w(x_i; \\theta_t)$ is dynamically assigned based on sample $x_i$ and\ncurrent model $\\theta_t$. However, designing effective dynamic reweighting methods for LLM pretraining\ninvolves several inherent complexities. In LLM pretraining, each sample is typically seen only once\ndue to the vast size of the datasets, which contrasts with problems where repeated exposure to data is\ncommon, such as in computer vision problems. This makes methods that depend on accumulating\nhistorical data, such as previous loss or gradient norms (Loshchilov & Hutter, 2015; Jiang et al.,\n2019), unsuitable. Furthermore, the sheer scale of these large corpus makes it computationally\nprohibitive to track and store sample-specific statistics, as it would significantly increase resource\nrequirements. Additionally, approaches that rely on hold-out validation sets to compute sample\nweights, as in Grangier et al. (2023), are also irrelevant for pretraining a general-purpose LLM. These\nunique technical challenges necessitate the development of a fully online, lightweight reweighting\napproach that scales efficiently with the size of a modern LLM training corpus, which we discuss\nnext."}, {"title": "LOSS-BASED REWEIGHTING STRATEGIES", "content": "To address the aforementioned challenges, we propose a novel framework of loss-based sample\nreweighting strategies motivated by the following theorem. Theorem 1 characterizes the effects"}]}