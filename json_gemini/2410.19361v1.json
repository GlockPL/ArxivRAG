{"title": "Engineering Trustworthy AI: A Developer Guide for Empirical Risk Minimization", "authors": ["Diana Pfau", "Alexander Jung"], "abstract": "AI systems increasingly shape critical decisions across personal and societal domains. While empirical risk minimization (ERM) drives much of the AI's success, it typically prioritizes accuracy over trustworthiness, often resulting in biases, opacity, and other adverse effects. This paper discusses how key requirements for trustworthy AI can be translated into design choices for the components of ERM. We hope to provide actionable guidance for building AI systems that meet emerging standards for trustworthiness of AI.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial intelligence (AI) has become integral to our daily lives, influencing aspects such as job searches, housing, and relationships through AI-powered platforms [1], [2]. Most current Al systems employ machine learning (ML) to train personalized models for users. These trained models provide tailored predictions on interests like job offers, dating, and music videos [3]. The availability of tailored (personalized) predictions is instrumental for many applications. As a point in case, the use of personalized diagnosis and treatment can significantly improve healthcare [4].\n\nA. Evidence for Harmful A\u0399\n\nDespite the usefulness of ML applications, there is increas- ing evidence for their potentially harmful effects:\n\n\u2022 Impact on Democratic Processes. Social media plat- forms use ML in the form of recommender systems to select (or suppress) information presented to a user [5]. These recommendation systems can (be exploited to) amplify sensationalist and divisive content which, in turn, can deepen polarization and the fragmentation of public sphere into filter bubbles [6], [7]. There is also evidence for the exploitation of these effects in order to influence core democratic processes such as elections [8], [9].\n\u2022 Autopilot Crashes. Al based control of vehicles, which facilitates semi-autonomous driving, has been associated with several notable accidents. In some instances, the sys- tem did not accurately detect obstacles or misinterpreted road conditions [10], [11]. AI-based autopilots might also reduce driver engagement and, in turn, awareness for dangerous situations that require human intervention [12]. This case illustrates the importance of requiring AI systems to be transparent about their operation and limitations [13].\n\u2022 The Cambridge Analytica Scandal. Cambridge Ana- lytica was a British political consulting and data analysis firm that became widely known for its controversial use of personal data. The company accessed vast amounts of personal data from Facebook users without explicit permission, violating privacy rights and data protection regulations [14]\u2013[16]. Cambridge Analytica used the data to create detailed profiles of individuals' personalities, po- litical views, and behavioural traits, which were then used to micro-target individuals with tailored political ads [17]. The firm was involved in several high-profile political campaigns, including Donald Trump's 2016 presidential campaign and the Leave.EU campaign for Brexit, using data-driven strategies to sway public opinion [18]. The Cambridge Analytica scandal sparked a global conversa- tion about data privacy and the impact of AI systems on societal wellbeing and core democratic processes [19].\n\u2022 COMPAS Recidivism Prediction Algorithm. A study found that the COMPAS algorithm, used in the U.S. justice system to predict recidivism, disproportionately predicted African-American and female defendants at higher risk compared to white male defendants [20], [21]. This finding raised concerns about the unbiased-ness and fairness of the COMPAS algorithm [22].\n\u2022 Uighur minority in China. Reports suggest that China has utilized facial recognition technology to specifically identify individuals with Uighur characteristics, targeting the Muslim minority group [23], [24]. The targeting of the Uighur minority by the state is well-documented, with numerous accounts indicating the presence of what are referred to as 're-education centers,' described by human rights organizations as high-security detention camps [25], [26]. The use of facial recognition technology to target a specific ethnic group highlights fundamental concerns about harmful effects or misuse of AI systems. In particular, AI-systems must adhere to ethical principles and respect human rights including privacy and wellbeing (individual and on societal level).\n\nB. The Need to Regulate AI\n\nThe use of AI is already regulated by existing legal frame- works. Indeed, any smartphone app that uses AI must conform to existing consumer protection law [27], [28]. However, these"}, {"title": "II. EMPIRICAL RISK MINIMIZATION AS AI ENGINE", "content": "Many of the current AI systems are based on machine learning (ML) techniques. The goal of ML is to predict some quantity of interest (its label) y from low-level measurements (its features) x = (x1,...,xa). The predictions are com- puted via some hypothesis map h that reads in the features of a data point and delivers a prediction \u0177 = h(x) for its label.\nT Model. The hypothesis h is learnt or optimized based on the discrepancy between previous predictions and observed labels. The space of possible hypothesis maps, from which a ML method can choose from, is referred to as hypothesis space or model.\n\nLoss. To choose or learn a useful hypothesis out of a model we need a measure for the quality of the predictions obtained from a hypothesis. To this end, ML methods use loss functions L ((x, y), h) to obtain a quantitative measure for the prediction errors.\nRisk. The ultimate goal of ML is to learn a hypothesis h\u2208 H that incurs a small loss when predicting the label of any data point. We can make this informal requirement precise by interpreting data points as realizations of independent and identically distributed (i.i.d.) RVs with a common probability distribution p(x, y). This allows to define the expected loss or risk of a hypothesis,\n\nL(h) := E{L ((x, y), h) }.\n\n\nData. Since the underlying probability distribution p(x, y) if typically unknown, we cannot directly optimize the risk (1). Instead, practical ML methods need to approximate the risk from a dataset\n\nD = {(x(1),y(1)),..., (x(m),y(m))}.\n\n\nThe dataset is constituted by data points, each characterized by features x and some label y. ERM-based methods require a dataset to measure the usefulness of a hypothesis h\u2208 H and, in turn, to train a model (by learning a useful choice for the model parameters).\nEmpirical Risk. Arguabely, the most widely used approx- imation to the risk (1) is the average loss or empirical risk,\n\n\u00ce (h|D) := (1/m) \u2211 L ((x(r),y(r)), h).\n\nr=1\n\nMuch of statistical learning theory revolves around the study of the approximation L(h|D) \u2248 L(h). Techniques for studying the approximation quality include by (the different forms of) the law of large numbers or concentration inequalities [43]- [46].\nEmpirical Risk Minimization. ERM-based methods learn a hypothesis h\u2208 H out of a hypothesis space (or model) H by minimizing the empirical risk L(h|D) as a proxy for the risk,\n\nh := argmin \u00ce(h|D)\n\nhEH\n\n= argmin \u2211 L ((x, y), h).\n\nhEH (x,y) ED\n\nA plethora of different ML methods is obtained by applying optimization methods to solve (4) for different design choices for data points (their features and label), the hypothesis space (or model) and loss function [47, Ch. 3].\nDesign Choices. From a ML engineering perspective, the design choices in ERM are mainly guided by computational aspects and statistical aspects of the resulting optimization problem (4). The computational aspects include the number of arithmetic operations required by a ML method. The statistical aspects include the generalization error L(h) \u2013 \u00ce(\u0125|D) of the learnt hypothesis and its robustness against the presence of outliers in the training set (2)."}, {"title": "III. KEY REQUIREMENTS FOR TRUSTWORTHY AI", "content": "The European Union put forward seven key requirements for trustworthy artificial intelligence (AI) [29]. These require- ments are motivated by the EU Charter of fundamental rights as the ultimate legal basis for trustworthy AI [49]. We next list these key requirements for trustworthy AI along with their motivation from the fundamental rights.\n1) KR1- Human Agency and Oversight [29, p.15]. The requirement of Human agency and oversight is based on the idea of human autonomy, which results from the right to dignity [49, Article 1]: Every person regardless of any other characteristics has an inherent, equal and inalienable value. KR1 is also aligned wit the right to liberty [49, Article 6] which determines that every person has a right to decide over their own life.\n2) KR2 -Technical Robustness and Safety [29, p.16]. ERM-based methods must perform reliably under var- ious conditions, minimizing risks of harm. KR2 aligns with several EU fundamental rights, such as the right to life [49, Article 2], the physical and mental integrity of the person [49, Article 3], and the protection of personal data [49, Article 8]. Section V discusses the robustness of ERM-based AI systems against perturbations of data sources and imperfections of computational infrastruc- ture.\n3) KR3 Privacy and Data Governance [29, p.17]. ERM-based methods must ensure protection against unauthorized access to - and misuse of personal data. Data and privacy protection are typically implemented as part of a data governance framework [50]. KR3 aligns with individuals' rights to privacy and the security of their personal data.\n4) KR4 Transparency [29, p.18]. Transparency is to enable a person to utilise their right to take action where they believe they have been treated wrongly. This is closely related to the right to data protection. To take actions against a potentially unlawful processing or an unjustified outcome of an ERM-based Al system, a user has to have enough information to understand how processing has taken place or how a decision was reached.\n5) KR5 Diversity, Non-discrimination and Fairness [29, p.18]. KR5 is aligned with [49, Article 21] which prohibits discrimination based on factors such as race, gender, and religion. Al systems must treat all indi- viduals fairly and inclusively, safeguarding their right to equality. Ensuring KR5 includes quality control for the dataset D used in ERM as well as the usability of interfaces for different user groups. KR5 is ultimately rooted in the inalienable value of all persons.\n6) KR6 - Societal and Environmental Well-Being [29, p.19]. KR6 is aligned with Article 35 of the EU Charter of Fundamental Rights by ensuring a high standard of environmental and social well-being [51]. Al systems should minimize harm to the environment and foster a sustainable development. By doing so, this requirement supports both individual rights and the collective welfare of society.\n7) KR7 - Accountability [29, p.19]. KR7 supports funda- mental rights to justice, remedy, and transparency [49]. Accountability ensures that Al systems are designed with mechanisms to identify, explain, and address poten- tial harm. Organisations that operate an AI system are responsible for its direct and indirect effects on the user [34], [52]. Developers and deployers must implement measures that allow to explain the aims, motivations, and reasons underlying the behaviour of AI systems. The accountability for AI systems is also regulated by"}, {"title": "IV. KR1- HUMAN AGENCY AND OVERSIGHT", "content": "ERM-based methods must be designed to support user agency, ensuring human oversight, and safeguarding funda- mental rights (see Section III). The predictions delivered by a trained model must not result in any manipulation or undue influence. We must ensure safeguards to maintain human control and the prevention of harmful outcomes. KR1 is closely related to fundamental rights such as dignity, freedom, and non-discrimination [49].\nHuman Agency. Users should be able to understand, in- teract with, and challenge decisions based on the predictions h(x) delivered by a trained model h \u2208 H. Human agency is facilitated by using transparent models (see Section IV-B and Section VII) and comprehensive documentation of the training process (e.g., optimization method used to solve (3)). The ERM design choices for data points (their features and label) and loss function (see Section IV-A and Section IV-C must ensure that the trained model avoids any manipulation or deception or users all of which may threaten individual autonomy [54]-[56].\nHuman Oversight. We must design ERM-based methods that do not compromise autonomy or cause harm. This can be implemented through various governance models that allow for varying degrees of human intervention, from direct involve- ment in learning cycles (monitoring gradient bases methods for solving (3)) to broader oversight of the societal and ethical impacts resulting from the predictions h(x).\nWe next discuss how KR1 guides the design choice for data (training set), model and loss used in ERM (4).\n\nA. Data\n\nFreedom of Individual. Ensuring individual freedom de- mands that individuals, especially those at risk of exclusion, have equal access to the benefits and opportunities that AI can offer. In this regard, KR1 requires that the training set in (3) is curated with diversity and inclusivity in mind. Biases in data collection or labelling can disproportionately affect certain groups, potentially limiting their autonomy or reinforcing discriminatory patterns. Fair representation of sub-populations in the dataset D used by ERM (4) is instrumental for avoiding the manipulation of individuals and protecting their mental autonomy and freedom of decision-making.\nRespect for Human Dignity. Learning personalized model parameters for recommender systems allows to boost addiction or in widespread emotional manipulation resulting in genocide [57]-[59]. Micro-targeting uses ML models to predict user preferences for products or susceptibility towards propaganda [60]. KR1 rules out certain design choices for the labels of"}, {"title": "V. KR2 - TECHNICAL ROBUSTNESS AND SAFETY", "content": "To obtain a practical ERM-based AI system, we must im- plement ERM (3) by some numerical optimization algorithm that is executed on some computer [64]. Such an implemen- tation will typically incur a plethora of imperfections, ranging from programming errors, quantization noise, power outages, interrupted communication links to hardware failures [65].\nAssume that we would have a perfect computer that is able to perfectly solve (3). Still, we must take into account imperfections of the collection process. The training set D might be obtained from physical sensors which rarely deliver perfect measurements of a physical quantity [66]. Moreover, the training set might have been intentionally manipulated (poisoned) by an adversary [67], [68].\nEven if we can rule out any physical measurement errors or data poisoning, it might still be useful to consider the training set as being subject to perturbation. Indeed, a key assumption of statistical learning theory is that the training set D consists of i.i.d. samples from an underlying probability distribution p((x, y)). Thus, we can interpret D as a perturbed representation of p((x, y)).\nIt seems natural to require the trained model h\u2208 H to be robust against perturbations arising from the i.i.d. sampling process. Indeed, the result of ERM should be a hypothesis with minimum risk, irrespective of the specific realization of the training set. For a more detailed analysis of the relation between robustness and generalization of ERM, we refer to [69], [70] as well as [46, Sec. 13.2].\nTo ensure KR2 we need to understand the effect of pertur- bations on a ERM-based AI system. These perturbations might affect any of the ERM components: the data points in D, the model H or the loss function L. Let us denote the perturbed components as D, H and L. The resulting perturbed ERM is then\n\nh = argmin(1/|D) \u03a3\u0128 ((x,y), h).\n\nhEH (x,y)ED\n\nThe effect of perturbations on optimization problems (such as (4)) has been studied extensively in robust optimization literature [71], [72]. By interpreting ERM as an estimator of (optimal) model parameters allows to use tools from robust statistics and signal processing [73], [74] to study the deviation between (4) and (5)\nThe analysis of (5) is typically based on assuming that the perturbed data D, model H and loss L belong to a known uncertainty set U,\n\n(\u00d5,\u00c3, \u00ce) \u2208U.\n\nDifferent robustness measures are obtained for different choices for the uncertainty set and measures for the deviations between optimization problems. For example, the uncertainty set U might consist of all datasets constituted by data points within some distance of the data points in D. if we measure the deviation between (4) and (5) in terms of their optimal values, we can use basic convex duality to quantify the effect of perturbations [61, Sec. 5.6]."}, {"title": "A. Loss", "content": "This section discusses specific choices (constructions) for the loss function in ERM (4) such that its solutions are close to the perturbed ERM (5). In particular, we consider uncertainty sets that contain a single choice for model H and loss function L but different perturbed datasets D. Thus, throughout this section, we assume (4) and (5) use the same H and L.\nRobust Statistics. A well-known example for a loss func- tion that improves robustness of ERM is the Huber loss. As a case in point, consider ERM for learning the parameters of a linear map h(x) = w\u1d40x to predict a numeric label y from the features x of a data point. Using the Huber loss instead of the squared error loss in (4) makes the resulting method significantly robust (in-sensitive) against the presence of outliers in the training set D [47, Ch. 3].\nAdversarial Loss. A principled construction of robust loss functions is based on replacing ERM (4) with an adversarial (or worst-case) variant [75], [76]\n\nh = argmin sup \u2211 L((x,y), h).\n\nhEH DEU (X,Y)ED\n\nA rapidly growing body of work studies various instances of (7) obtained for different constructions of the uncertainty set U [77]-[80]. We next show how a special case of (7) is equivalent to (4) for a suitable choice L' for the loss function.\nOne widely used construction of the uncertainty set in (7) is to separately perturb the features (and potentially also the labels) of data points in D [77], [79]. Thus, the uncertainty set decomposes into one separate uncertainty set U(x,y) for each data point (x, y) in D. The adversarial ERM (7) then becomes\n\nm\nh = argmin \u2211 sup L ((x, \u1ef9), h).\n\nhEH r=1 (x,y)\u2208u(x(r),y(r))\n\nrobust loss L'((x(r),y(r)),h)\n\nThe robust loss function L' depends on the original choice for the loss function in (4) and the uncertainty set U in (8). Let us next consider a modification of (8) where we perturb only the features of data points but leaving their labels untouched [71]. This modification uses an uncertainty set U(n) which is parametrized by a perturbation strength \u03b7 and consists of datasets D = (X,y) with feature matrix\n\nX := (x(1),...,x(m)) :=(x(1),...,x(m)) +(u(1),..., u(d))\n\nT (j)\nT with u \u2264\u03b7.\n\n\nCarefully note that, in contrast to (8), the construction (9) couples the features of different data points in D. Thus, instead of maximizing over possible perturbations separately for each data point as in (8), we need to study the worst-case perturbation of the entire dataset:\n\nm\nh = argmin sup \u2211L ((x(r),y), h).\n\nhEH DEUr=1\n\nConsider learning the parameters w of a linear model using loss function L ((x, y), h) = |y-h(x))|. For this special case, it can be shown that (10) is equivalent to ERM with the robust loss L' = |y - h(x))| + \u03b7 ||w||\u2081 [71, Thm. 14.9.].\nRecent work also studies uncertainty sets U that consist of perturbed datasets D with an empirical distribution P close to the empirical distribution P of D,\n\nU(7) := {D : W (P,P) \u2264 n}.\n\nHere, we use Wassertein distance W W(P, P) to measure the distance between P and P [80]. Consider the adversarial ERM (7) with uncertainty set (11) and a loss function L that is Lipschitz continuous with modulus a. It can then be shown that (7) is equivalent to ERM with a specific robust loss function [80, Theorem 4].\nFor a binary classification, the authors of [81] study a robust loss of the form\n\nL ((x, y), h) = {1 if h(x') \u2260 y for some x' \u2208 Ux otherwise .\n\nHere, Ux is some robustness region. Note that (12) reduces to the basic 0/1 loss for the choice Ux = {x} [47]."}, {"title": "B. Data", "content": "Instead of choosing a robust loss function L in ERM (4), we can construct the training set in (4) to make its solutions more robust. One widely studied approach is adversarial training, i.e., to include adversarially perturbed data points in the training set D [76], [82], [83].\nAn opposite approach to adversarial training is to prune a given dataset using some form of outlier detection [84]. The training set is then obtained by the remaining data points that have not been declared as outliers. However, it can be challenging to distinguish outliers from natural perturbations due to the sampling from a true underlying probability distri- bution [85], [86]. Fundamental limits for this outlier removal approach can be obtained, e.g., from the malicious noise model [87]:\n\nz(r) if b(r)\n\n\nZ(r)\notherwise,\n\nwith b(r) i.i.d. B(pe), z(r) i.i.d. p (x, y).\n\nC. Model\n\nA principled approach to defining and measuring robustness of ML is to study the continuity properties of the learnt hypothesis h. Beside the basic qualitative notion of continuity, we can also use Lipschitz continuity to obtain a quantitative measure of robustness. However, the concept of Lipschitz continuity requires the domain the and range of h to be a metric space.\nOne way to ensure to robustness is then to use a model H that only contains Lipschitz continuous hypothesis maps H. Recent work has shown that ERM delivers a Lipschitz continuous hypothesis if the model H is sufficiently large [88]. Instead of Lipschitz continuity, the authors of [89] use the concept of local and global robustness for multi-class classification problems. Here, data points have a label y \u2208 y := {1, ..., K} and the goal is to learn a classifier h(x) = (h\u2081(x),...,hk(x)) which is used to classify a data point as \u0177 = argmaxc\u2208{1,...,K} hc(x).\n\nA classifier h(x) is then defined as \u025b-locally robust at feature vector x if it classifies \u0177 = \u0177' for every data point with features x' such that ||x - x'||2 \u2264 \u03b5 [89] . Note that if we require this to hold at every x, the classifier must be trivial (delivering the same label value for every data point). To obtain a useful notion of global robustness, the authors of [89] introduce an auxiliary label value that signals if the classifier fails to be robust locally."}, {"title": "VI. KR3 PRIVACY AND DATA GOVERNANCE", "content": "\"..privacy, a fundamental right particularly affected by Al systems. Prevention of harm to privacy also necessitates adequate data governance that covers the quality and integrity of the data used...\" [29, p.17].\nData Governance. ML systems might use data points that are generated by human users, i.e., personal data. Whenever personal data is used, special care must be dedicated towards data protection regulations [32]. It might then be useful (or even compulsory) to designate a data protection officer and conduct a data protection impact assessment [29].\nMeasuring Privacy Leakage. Ensuring privacy protection for an ERM-based system requires some means to quantify its privacy leakage. To this end, it is useful to think of an ERM- based method as a map A: An ERM-based method A reads in the training set D, solves (3), and delivers some output A(D). The output could be the learnt model parameters w or the prediction h(x) obtained for a specific data point with features x.\nPrivacy protection requires non-invertibility. To imple- ment means of privacy protection, we need to clarify what parts of a data point are considered private or sensitive infor- mation. To fix ideas, consider data points representing humans. Each data point is characterized by features x, potentially a label y and a sensitive attribute s (e.g., a recent medical diagnosis). For a ERM-based method A, privacy protection means that it should be impossible to infer, from the output A(D), any of the sensitive attributes s in D. Mathemati- cally, privacy protection requires non-invertibility of the map A(D). In general, just making A(D) non-invertible is typically insufficient for privacy protection. We need to make A(D) sufficiently non-invertible.\nDifferential privacy (DP). One widely used approach to make a ERM-based method sufficiently non-invertible is introduce some randomness or noise. Examples for such randomness include the adding of noise to the output and the selection of a random subset of D. The map A then becomes stochastic and, in turn, the output A(D) is then characterized by a probability distribution Prob{A(D) \u2208 S} for all sets S within a well-defined collection of measurable sets [44]. DP measures the non-invertibility of a stochastic algorithm A via the similarity of the probability distributions obtained for two datasets D,D' that are considered neighbouring or adjacent [90], [91]. Typically, we consider D' as adjacent to D if it is obtained by modifying the features or label of a single data point in D. In general, the notion of neighbouring datasets is a design choice used in the formal definition of DP.\nDefinition 1: (from [91]) A ERM-based method A is (\u03b5, \u03b4)- DP if for any measurable set S and any two neighbouring datasets D, D',\n\nProb{A(D) \u2208 S} < exp(\u03b5)Prob{A(D') \u2208 S} + \u03b4.\n\nA. Data\n\nOne simple way to implement privacy protection in ERM- based methods is by careful selection of the features used to characterize data points [92], [93]. The idea is to use only features that are relevant for the learning task but at the same time do not convey too much information about any sensitive attribute. Clearly, there is an inherent trade-off between privacy protection and resulting statistical accuracy. Indeed, we obtain perfect privacy protection but not using any property of a data point as their features. Clearly, this extreme case of maximum privacy protection comes at the cost of a lower quality of the predictions delivered by (the hypothesis learnt from) ERM.\nPrivate Feature Learning. In general, it is difficult to manually identify features that strike a good balance between"}, {"title": "VII. KR4 - TRANSPARENCY", "content": "According to [29], this key requirement encompasses trans- parency of elements relevant to an AI system. KR4 requires ERM-based methods to provide explanations for their pre- dictions. Instead of constructing explicit explanations, ERM- based methods should utilize models that are intrinsically interpretable [100], [101].\nKR4 also mandates that users be informed when interacting with an automated system, e.g., through notifications such as 'You are now conversing with a chatbot'. In addition, ERM- based methods should be transparent about their capabilities and limitations, including quantitative measures of prediction uncertainty.\nTraceability. The design choices (and underlying business models) for a ERM-based Al systems must be documented. This includes the source for the (data points in the) training set, the model, the loss function used in (3) [102]. Moreover, the documentation should also cover the details of the optimization method used to solve (3). This documentation might include the recording of the current model parameters along with a time-stamp (\"logging\").\nCommunication. The user interface of an Al system must clearly indicate if it delivers responses based on automated data processing such as ERM. AI systems also need to communicate the capabilities and limitations to their end users (e.g., of a digital health app running on a smartphone). For example, we can indicate a measure of uncertainty about the predictions delivered by the trained model. Such an uncertainty measure can be obtained naturally from probabilistic model for the data, e.g., the (estimated) conditional variance of the label y, given the features x of a random data point. Another example for an uncertainty measure is the validation error of a trained model h\u2208 H.\nExplainability. Another core aspect of transparency is the explainability of an Al system. In what follows, we will discuss how specific design choices can facilitate the explainability of ERM-based methods. To this end, we need a precise definition or quantitative measure for the explainability of ERM. There is a variety of approaches to constructing"}, {"title": "VIII. KR5 - DIVERSITY, NON-DISCRIMINATION AND FAIRNESS", "content": "\"...we must enable inclusion and diversity throughout the entire Al system's life cycle...this also entails ensuring equal access through inclusive design processes as well as equal treatment.\" [29, p.18].\nConsider an AI application that uses data points representing humans. Each data point is characterized by features x and a sensitive attribute s. The sensitive attribute typically depends on the raw features of a data point, s = s(x) with some map s(.). Examples for a sensitive attribute s include ethnicity, age, gender or religion.\nIndividual Fairness (Disparate Treatment). Roughly speaking, a fair ERM-based method should learn a \u0125h \u2208 H that does not put inappropriate weight on the sensitive attribute. To makes this fairness notion precise, we need a measure d(x, x') for the similarity between data points, with features x, x', that maximally ignores their sensitive attributes [114], [115]. A fair classifier should deliver the same predictions for sufficiently similar data points,\n\nh(x) = h(x')\n\n whenever d(x, x') is sufficiently small.\n\nHere, d(x, x') denotes a quantitive measure for the similarity between two data points with features x, x', respectively. The fairness requirement (19) seems natural in order to prevent disparate treatment [116].\nExample: Job Platform. Consider a job platform that uses ERM to learn a hypothesis h for predicting if a given user is suitable for a specific job opening. Each user is characterized by features x = (x1,...,xa) with its first entry x\u2081 being the age of the user. Thus, the sensitive attribute is s = x1. Fairness might require that the prediction h(x) does not depend at all on the age of the user [117]. We could ensure this by using a classifier satisfying (19) with a metric d(x, x') that does not depend on x1. However, the requirement (19) is insufficient when the sensitive attribute s = x1 can be inferred (predicted) from the values of the remaining features x2,...,xd [118], [119].\nML literature has proposed and studied a variety of quanti- tative measures for the fairness of a trained model h\u2208 H. In what follows we briefly survey some of these measures in the context of binary classification where the learn hypothesis is used to deliver a predicted label \u0177 \u2208 {0,1}.\nGroup Fairness (Disparate Impact). Besides the individ- ual fairness constraint (19), another flavour of fairness is to require a trained model to have similar performance across"}, {"title": "IX. KR6 - SOCIETAL AND ENVIRONMENTAL WELL-BEING", "content": "\"...Sustainability and ecological responsibility of AI systems should be encouraged, and research should be fostered into AI solutions addressing areas of global concern, such as for instance the Sustainable Development Goals.\" [29, p.19].\nSo far, we discussed KRs that focused on the effect for ERM-based methods on individual users. In contrast, KR6 key requirement revolves around the wider impact of an ERM-based method on the level of societies and natural environments.\nSociety and Democracy. Design choices for ERM should also consider the effect of (predictions delivered by) a trained model h\u2208 H on society at large. The predictions h(x) could not only harm the mental health of individual users but also affect core democratic processes such as policy-making or elections. As a case in point, social media might use person- alized models h to recommend (or select) content delivered to its users. These recommendations might be nudged in order to boost polarization and, in the extreme case, social unrest [134]. On the other hand, there is immense potential for large language models (LLMs) to improve the accessibility of high-quality education which, in turn, is crucial for the success of democracies [135], [136].\nEnvironment. ERM-based Al systems need to solve the optimization problem (3) using some computational methods. The implementation of these methods in physical hardware"}, {"title": "X. KR7 - ACCOUNTABILITY", "content": "\"...mechanisms be put in place to ensure responsibility and accountability for AI systems and their outcomes, both before and after their development, deployment and use.\" [29, p. 19].\nPolicy and Governance Approaches. Organizations such as the OECD have been working on governance structures for ERM-based Al systems. This includes formalizing auditing procedures and ensuring that developers are held to both ethical standards and legal requirements [140].\nFrameworks for Answerability. AI developers and oper- ators must be able to justify their actions and decisions. This involves both transparency (see Section VII) and oversight (see Section IV) mechanisms which are especially important in high-stakes domains [141]. The justification of ERM design choices also requires a solid understanding of the inherent trade-offs between design criteria such as explainability and accuracy [101], [142].\nRegular Audits and Third-Party Reviews. Periodic re- views of AI systems by independent auditors help ensure and validate accountability. To this end, independent external teams (\u201cred teams\u201d) should stress-test the ERM-based system for vulnerabilities and biases that might undermine accountability [140], [143]."}]}