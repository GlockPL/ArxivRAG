{"title": "Leveraging Foundation Models To learn the shape of semi-fluid\ndeformable objects.", "authors": ["Omar El Assal", "Carlos M. Mateo", "Sebastien Ciron", "David Fofi"], "abstract": "Abstract\u2014One of the difficulties imposed on the manip-\nulation of deformable objects is their characterization and\nthe detection of representative keypoints for the purpose of\nmanipulation. A keen interest was manifested by researchers\nin the last decade to characterize and manipulate deformable\nobjects of non-fluid nature, such as clothes and ropes. Even\nthough several propositions were made in the regard of object\ncharacterization, however researchers were always confronted\nwith the need of pixel-level information of the object through\nimages to extract relevant information. This usually is ac-\ncomplished by means of segmentation networks trained on\nmanually labeled data for this purpose. In this paper, we\naddress the subject of characterizing weld pool to define stable\nfeatures that serve as information for further motion control\nobjectives. We achieve this by employing different pipelines.\nThe first one consists of characterizing fluid deformable objects\nthrough the use of a generative model that is trained using a\nteacher-student framework. And in the second one we leverage\nfoundation models by using them as teachers to characterize\nthe object in the image, without the need of any pre-training\nand any dataset. The performance of knowledge distillation\nfrom foundation models into a smaller generative model shows\nprominent results in the characterization of deformable objects.\nThe student network was capable of learning to retrieve the\nkeypoitns of the object with an error of 13.4 pixels. And the\nteacher was evaluated based on its capacities to retrieve pixel\nlevel information represented by the object mask, with a mean\nIntersection Over Union (mIoU) of 75.26%.", "sections": [{"title": "I. INTRODUCTION", "content": "In order to accomplish manipulation tasks successfully,\na robot has to perceive and comprehend the manipulated\nobject. This consists of analysing information from sev-\neral sources like images, points clouds or the model of\nthe object [1], [2], [3]. Most of the research found in\nthe literature focused on modeling and characterising rigid\nbodies [4]. However, the ever-increasing need for robots to\ninteract with day-day objects, alongside the recent advances\nin machine learning and computer vision have raised an\ninterest in the subject of deformable objects. For instance,\nthe topic of modeling and estimating the state of these\nobjects have attracted the interest of several researchers [5],\n[6], [7], [8], [9]. Which indicates that the manipulation\nof deformable objects presents several challenges imposed"}, {"title": "II. RELATED WORK AND MOTIVATION", "content": "Describing deformable objects have always been a re-\nsearch question that accompanied the manipulation of these\nobjects [18]; In many cases they were represented as ob-\njects as continuous geometrical entities like surfaces and curves [19], [20]. Some researchers however described them\nin a discrete representation through meshes, skeletons or\nlandmark points [21]. And these objects were of different\nshapes and natures. In some cases they were of a linear\ndeformable objects nature, and their characterization serves\nas a feedback for for later use in a geometric optimal control\nproblem [22]. In other cases they were of more complex\nshapes to represent such as clothes [23]. The problematic\nof deformable clothing grasping in this case was resolved\nby assimalting clothes to polygonal shapes described by\ntheir corners. From a different perspective, [24] relied on\nthe detection of extremed edges and points to represent\nobjects such as towels and ropes. They presented a method\nthat is based on the use of Convolutional Neural Networks.\nWhile this approach worked well for fabric, it still does not\ntake into account the deformable nature of fluid objects. In\n[25], Coherent Point Drift (CPD) is applied after the points\nare registered with a Gaussiant Mixture model (GMM), to\nmaintain feature from one frame to another. Softer objects\nsuch as dough characterization and manipulation have also\nbeen studied by researchers, in [26] visual images with depth\nmaps are analyzed by Graph Neural Netwroks to learn a\nparticle-based model of the object. Besides that, tissues are\nanother type of soft deformable objecs that were explored\nby researchers. Feature extraction and state tracking of\ndeformable of soft tissues by deep neural networks approach\nexist in the literature [27]. Nevertheless, manipulation and\ncharacterization of softer deformable objects, such as fluid\nand fluid like objects have been introduced [28], [5]. Where\nin [28] the manipulation of semi-fluid objects in a wok is\naddressed. These objects are considered as a single particle\nrepresented by the center of the object. In [5] a model-based\nmethod was used to manipulate water.\nAs discussed in earlier sections, the methods present in\nthe state of the art do not apply to fluid and semi-fluid\ndeformable objects. Although these methods are efficient\nfor their specific task, they fail to be generalized for other\ntypes of applications and objects, since many of the proposed\nsolutions are application oriented [29]. Providing model-free\nmethods to estimate the shape and to characterize deformable\nobjects is the main motivation of this work. This motivation\nis backed by the unpredictable nature of semi-fluid objects,\nsuch as weld pool and glue, and the difficulties of predicting\ntheir deformations. To address the problem of generalization,\nwe propose the use of foundation models. Foundation models\nare large-scale models trained on a large scale of data and\ntasks, which allows for better analysis and understanding of\nthis data [31]. We use these models as teachers for a smaller\nstudent network as depicted in Fig. 1.\nAs for the student network, we use a generative model.\nGenerative models are mathematical models that analyze\nthe underlying patterns in the streams of data. They have\nthe capacity of generating new data with similar character-\nistics by learning underlying representations [30]. Several\narchitectures are present in the literature such as Generative\nAdversarial Networks [32], Auto-encoders architectures [33],\nGaussian Mixture Models (GMM), and Hidden Markov\nModels (HMM)) [30]. As for visual image generation the\nmost commonly used are diffusion models [34], GANs [32],\nauto-encoders [33] and Variational Auto-encoders [35].\nThese models often need heavy computing powers and have\ncomplex convergence criteria [36]. Amongst these models,"}, {"title": "III. FEATURE POINTS EXTRACTION USING GENERATIVE\nMODELS", "content": "We employ the teacher-student architecture of Fig. 1 in our\nframework. We consider the output of the teacher to be the\ncorresponding ground truth. In this framework, the teacher\ntakes as input the corresponding image of the deformable\nobject and outputs a heatmap describing its shape. This\nheatmap is stacked with the mask of the object in the image\nto represent the output of the teacher. On the other hand, the\nstudent network has a variational auton-encoder architecture\nwith a ResNet backbone. As illustrated in Fig. 2, the teacher\nconsists of two different pipelines. The first one consists\nof finding the mask of the object by combining the two\nfoundation models discussed earlier, DINO and SAM2, and\nthe second one is dedicated to generate relevance heatmaps\ndescribing the keypoints of the object. In first place, the\nimage is passed through DINO model. DINO is a self\ndistillation network that is capable of generating attention\nmaps describing the observed scene. After retrieving the six\nattention maps, their average is thresholded with respect to\nthe mean value of attention to propose a 2D array with\nhighest attention values. After that, we employ a pipeline\nto extract SAM2 prompts. SAM2 is another prompt based\nfoundation model, that takes as input an image and possible\nprompts, and proposes a possible mask for these prompts\non the output. We pass the original image through the\nSAM2 model, alongside with the extracted prompts from\nthe DINO attention map in order to retrieve the pixel-level\ninformation of the object represented by the mask. After\nthat, a heatmap that describes the keypoints of the object\nis proposed by Algorithm 1, this heatmap represented the\nmost probable location of the keypoints, according to the\nnormal lines on the contour of the object. Subsequently, the\nextracted heatmap alongside the previously defined mask are\nconcatenated to represent the ground truth label which is\ndistilled into the student network.\nThe generation of the heatmap goes through the different\nsteps that are highlighted in Algoirhtm 1 and depcited in\nFig 3. The contour of the object is extracted from the output\nmask of DINO-SAM. And Sobel operator is applied to find\nthe values of normal lines. We refer to these lines according\nto their angle \u03b8. Let C be the set of these points pi,\n\n[(Xi, Yi), \u03b8i] = Pi\n\nthe pair (xi, Yi) represents the coordinates of each point p\u00e5 in\nthe contour C of the object, and 0\u2081 the corresponding angular\ncoordinate, codifying its normal angle.\nThe difference between 0 and 0-1 is calculated and\nfiltered to find the points at which the variation is the highest.\nThe set of found points P is denoted as,\n\nP = {(xi, Yi) | |\u03b8\u03b5 - \u03b8\u03ad\u22121| > x}\n\n\u03bb is the threshold value for filtering out low frequency\nvariations. K-means is later applied to cluster the points of\nPinto k groups. And each of these groups P; is a region of\ninterest described by a 2D Gaussian distribution, of center \u03bc\nand covariance matrix \u03a3\nFollowing the clustering of the points and the calculation\nof the covariance matrices, the heatmap F; of cluster Pj is\ndefined at the center of this cluster, Thus the ground truth\nheatmap is finally computed as the mixture of all Fj,\n\nk\nF = \u2211Fj\n\nAlgorithm 1 highlights the steps of the heatmap calculation\ndiscussed before.\nThe output of the teacher framework, Fig. 3e, represents\nthe ground truth knowledge that is distilled into the ResNet-\nVAE stduent [39], [35], [40]. This type of networks encodes\nthe input into a latent probabilistic space of continuous\nnature.\nIn our application, we encode the image of the weld pool\nusing resnet [40], and we decode the probabilistic latent\nvector to reconstruct the heatmap and the mask using a\nfully connected convolutional layer, outputing a two channels\nmatrix of shape (W,H,2). Here, W and H are the dimension\nof the input image, and each of the channels represent re-\nspectively the DINO-SAM mask and the calculated heatmap\nof Fig.3\nThe purpose of reconstructing the mask of the image\ninstead of the image itself using the student network (the\nVAE), is that reconstructing a single element of the image\ninstead of all three channels helps the network focus on a\nsingle zone and a single shape, which helps converging the\nnetwork and accelerates it. t"}, {"title": "IV. DATA ACQUISITION AND DATASET", "content": "An industrial application that consists of characterizing the\nshape of weld pool as taken as case in point. Characterizing\nweld pool is important to control the robotized welding process and to analyze the quality of the weld. Since no\nexisting benchmark or dataset exist for that purpose we create\nour own dataset by equipping a camera to an industrial robot,\nequipped with a welding source. The changes in geometry,\nrobot speed, and process parameters accompanied with ex-\nternal perturbations impose deformations on the molten pool\nmanipulated by the robot. Nine video sequences are taken\nof the weld pool in these conditions and by welding actual\nsamples in a fillet joint configuration (Tee joint). Fig. 4 shows\nthe different elements of the welding setup. Table I shows\nthe different parameters used to acquire the dataset."}, {"title": "V. RESULTS", "content": "The performance of the teacher network is validated\naccording to its capacity of retrieving accurate masks of\nthe designated object. We use the Intersection Over Union\n(IoU) for that objective. Several configuration are tested in an\nablation study manner to decide the type and number prompts\nto optimize the performance of the SAM2 detections. We\nfirstly experiment with the results of SAM2 without any\nprompts. We secondly introduce the prompts proposed by\nDINO networks: the center of the attention map and the\ndifferent prompt values presented in Fig. 2\nThe performance of solely using SAM is first evaluated,\nhowever, the model in this case failed to achieve reliable\nvalues of mIoU with only 35.37%, with an upper quartile\nof 59%. Introducing DINO predicition with a single prompt\nameliorated significantly the upper quartile without a signif-\nicant impact on the value of mIoU that is 34.82%. On the\nother hand, after applying some heuristics such as clustering\nthe thresholded mean attention map into different clusters,\napplying DBSCAN to propose a best fit bounding box and\nfiltering the propmt points increased significantly the values\nof mIoU. After applying these heuristics, we obtained an\nmIoU of 75.26%, with an upper quartile of 85%. Fig. 6\ndepicts experimenting with different heuristics and different"}, {"title": "VI. DISCUSSION", "content": "Qualitative evaluation. As opposed to baseline methods, the\nstudent-teacher framework of our approach provided stable estimation of the shape of the deformable object without\nthe need of any prior labeling. However, the automatically\ngenerated masks by the DINO-SAM2 framework were in\nsome cases subject to ambiguity coming from erroneous\nprompt points, resulting low values of IoU. This ambiguity\nis due to the presence of impurities and intrusions in some\ncases, such as the presence of greases on the robot trajectory.\nQuantitative evaluation In addition to qualitative evaluation,\nthe calculated value of the Euclidean distance between the\npredicted points and the ground truth is relatively small and\nacceptable for such an application. However, this error is\nthe result of two main factors: human factor and the lack\nof visual features in some images. We recall that validation\nimages were labeled manually with the possible centers of\nthe heatmap, which makes these annotations biased and\nprone to error. Since defining accurate labels for these\ntasks is not possible. To resolve this kind of uncertainty,\nwe recommend adopting the automatically generated GT\nlabels as ground truth for evaluation and not the manually\ndefined GT. As a proposed solution to this, we propose\nthe use of sequential generative models students, such as\nLong Shot Term Memory Variational Autoencodre (LSTM-\nVAE) [46] capable of preserving temporal information about\nthe process.\nOn the other hand, the lack visual features in some frames,\ndue to several factors; such as noise and process instability\nhave also resulted low fidelity predicitions of the heatmap."}, {"title": "VII. CONCLUSION", "content": "A clear representation of the shape of the manipulated\nobject is a required step for robotic manipulation task. In\nthe case of deformable objects, this step is dependent on the\nshape of the deformations of the object. In this paper we\naddress the case of highly-deformable fluid and viscoelastic\nobjects. We propose a teacher-student framework that does\nnot require any prior labeling to represent them. Our method\nis independent from the complex physical modeling require-\nments. During inference time, we use only the student of our\nframework. The student model, a generative model in our\ncase, generates a heatmap that estimates the current state of\nthe object. This heatmap represents the locations at which the\nmost important deformation occurs, according to the contour\nof the object. The student network distils the knowledge of a\nteacher that is composed of two foundation models and the\nheatmap proposal algorithm 1. The two foundation models\nemployed in the case of this paper are DINO and SAM2.\nWe use DINO as a prompts generator for the SAM2 model.\nWe demonstrate first the capabilities of foundation models\nto retrieve pixel-level information about the deformable ob-\njects. Secondly, we show that the generative student network\nis capable of learning the shape of the fluid deformable object\nby absorbing the knowledge provided by the foundation\nmodels successfully.\nFurther research directions include ameliorating the per-\nformance of the teacher by including the human in the loop\nthrough techniques such as active learning on one hand. On\nthe other hand, integrating the retrieved shape of the weld"}]}