{"title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion", "authors": ["Yongyuan Liang", "Tingqiang Xu", "Kaizhe Hu", "Guangqi Jiang", "Furong Huang", "Huazhe Xu"], "abstract": "Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks.", "sections": [{"title": "1 Introduction", "content": "Policy learning traditionally involves using sampled trajectories from a replay buffer or behavior demonstrations to learn policies or trajectory models mapping from state s to action a, modeling a narrow behavior distribution. In this paper, we consider a shift in paradigm: moving beyond training a policy, can we reversely predict optimal policy network parameters using suboptimal trajectories from offline data? This approach would obviate the need to explicitly model behavior distributions, allowing us to learn the underlying parameter distributions in the parameter space, thus revealing the implicit relationship between agent behaviors for specific tasks and policy parameters.\nUsing low-dimensional demonstrations (such as agent behavior) to guide the generation of high-dimensional outputs (policy parameters) is a challenging problem. When diffusion models (12; 19) have demonstrated highly competitive performance on various tasks including text-to-image synthesis, we are inspired to approach policy network generation as a conditional denoising diffusion process. By progressively refining noise into structured parameters, the diffusion-based generator can discover various policies that are not only superior in performance but also more robust and efficient than the demonstration in the policy parameter space.\nWhile prior works on hypernetworks (10; 1; 17) explore the concept of training a hypernetwork to generate weights for another neural network, they primarily use hypernetworks as an initialization network of meta-learning (7) and then adapt to specific task settings. Our approach diverges from this paradigm by leveraging agent behaviors as direct prompts or to generate optimal policies within the"}, {"title": "2 Preliminaries", "content": "Policy Learning. Reinforcement Learning (RL) is structured within the formation of Markov Decision Processes (MDPs) (2), which is defined by the tuple $M = (S, A, P, R, \\gamma)$. Here, $S$ signifies the state space, $A$ the action space, $P$ the transition probabilities, $R$ the reward function and $\\gamma$ the discount factor. RL aims to optimize an agent's policy $\\pi : S \\rightarrow A$, which outputs action $a_t$ based on state $s_t$ at each timestep, to maximize cumulative rewards. The optimal policy can be expressed as:\n$\\pi^* = \\underset{\\pi}{\\arg \\max} E_{z \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t]$ (1)\nwhere $z$ represents a trajectory generated by following policy $\\pi$. In deep RL, policies $\\pi$ are represented using neural network function approximations (21), parameterized by $\\theta$, facilitating the learning of intricate behaviors across high-dimensional state and action spaces.\nDiffusion Models. Denoising Diffusion Probabilistic Models (DDPMs) (12) are generative models that frame data generation through a structured diffusion process, which involves iteratively adding noise to the data and then denoising it to recover the original signal. Given a sample $x_0$, the forward diffusion process to obtain $X_1, X_2, ..., X_T$ of increasing noise intensity is typically denoted by:\n$q(x_t | x_{t-1}) = \\mathcal{N}(x_t, \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_tI),$ (2)\nwhere q is the forward process, $\\mathcal{N}$ is Gaussian noise, and $\\beta_t \\in (0,1)$ is is the noise variance."}, {"title": "3 Methodology", "content": "The denoising process, which is the reverse of the forward diffusion, can be formulated as:\n$P_\\theta (x_{t-1} | x_t) = \\mathcal{N} (x_{t-1} | \\mu_\\theta (x_t,t), \\Sigma_\\theta),$ (3)\nwhere $p_\\theta$ denotes the reverse process, $\\mu_\\theta$ and $\\Sigma_\\theta$ are the mean and variance of the Gaussian distribution respectively, which can be approximated by a noise prediction neural network parameterized by $\\theta$.\nDiffusion models aim to learn reverse transitions that maximize the likelihood of the forward transitions at each time step t. The noise prediction network $\\theta$ is optimized using the following objective, as the function mapping from $\\epsilon_\\theta(x,t)$ to $p_\\theta(x_t, t)$ is a closed-form expression:\n$L_{DM}(\\theta) := E_{x_0 \\sim q, \\epsilon \\sim \\mathcal{N}(0,1),t}[||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon, t)||^2],$ (4)\nHere, $\\epsilon \\sim \\mathcal{N}(0, I)$, is the target Gaussian noise, $\\bar{a}_t := \\prod_{s=1}^{t} 1 - \\beta_s$, and $\\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon$ is the estimated distribution of $x_t$ from the closed-form relation.\nAlthough diffusion models are typically used for image generation through the reverse process, the variable x can be generalized to represent diverse entities for generation. In this paper, we adapt x to represent the parameters $\\theta$ of the policy network in policy learning."}, {"title": "4 Experiments", "content": "We conduct extensive experiments to evaluate Make-An-Agent, answering the following problems:\n\u2022 How does our method compare with other multi-task or learning-to-learn approaches for policy learning, in terms of performance on seen tasks and generalization to unseen tasks?\n\u2022 How scalable is our method, and can it be fine-tuned across different domains?\n\u2022 Does our method merely memorize policy parameters and trajectories of each task, or can it generate diverse and new behaviors?\nBenchmarks. We include two manipulation benchmarks for simulated experiments and real-world robot tasks to show the performance and capabilities of our method as visualized in Figure 4.\nMetaWorld. MetaWorld (27) is a benchmark suite for robotic tabletop manipulation, consisting of a diverse set of motion patterns for the Sawyer robotic arm and interactions with different objects. We selected 10 tasks for training as seen tasks and 8 for evaluation as unseen downstream tasks. Detailed descriptions of these tasks can be found in Appendix B.1. The state space of MetaWorld consists of 39 dimensions and the action space has 4 dimensions. The policy network architecture used for MetaWorld is a 4-layer MLP with 128 hidden units, containing a total of 22,664 parameters.\nRobosuite. Robosuite (31), a simulation benchmark designed for robotic manipulation, supports various robots such as the Sawyer and Panda arms. We train models on three manipulation tasks: Block Lifting, Door Opening and Nut Assembly, using the single-arm Panda robot. Evaluations are conducted on the same tasks using the Sawyer robot. This experimental design aims to validate the practicality of our approach by assessing whether the generated policy can be effectively utilized on different robots. In the Robosuite environment, the state space comprises 41 dimensions, and the action space consists of 8 dimensions. The policy network employed for this domain contains 23,952 parameters.\nQuadrupedal locomotion. To evaluate the policies generated by Make-An-Agent in the real world, we utilize walk-these-ways (13) to train policies on IsaacGym and use our method to generate actor networks conditioning on trajectories from IsaacGym simulation with the pretrained adaptation modules. Then, we deploy the generated policy on real robots in environments differ from simulations. The policies generated for real-world locomotion deployment comprise 50,956 parameters.\nDataset. We collect 1500 policy networks for each task in MetaWorld and Robosuite. These networks are sourced from policy checkpoints during SAC (11) training. The checkpoints are saved every"}, {"title": "4.1 Performance Analysis", "content": "For evaluation, the trajectories used as generation conditions are sampled from the SAC training buffer within the first 0.5 million timesteps, which can be highly sub-optimal, under the same environmental initialization. During testing, the generated policies are evaluated in 5 random initial configurations, thoroughly assessing the robustness of policies generated using trajectories from the fixed environment settings.\nIn RoboSuite experiments, due to the inconsistency in policy networks, we retrain the autoencoder and finetune the diffusion generator trained on MetaWorld data. The experimental setup for RoboSuite is almost identical to that of MetaWorld, with the only difference being the robot used during testing.\nFor real-world locomotion tasks, we save 10,000 policy network checkpoints using walk-these-ways (WTW) (13) trained on IsaacGym, requiring a total of 200 GPU hours. The 100 trajectories used as generation conditions are sourced from the first 10,000 training iterations of WTW.\nBaselines. We compare Make-An-Agent with four baselines, including multi-task imitation learning (IL), multi-task reinforcement learning (RL), meta-RL with hypernetworks, and meta-IL with transformers. These represent state-of-the-art methods for multi-task policy learning and adaptation. For a fair comparison, each baseline uses the same testing trajectory data for downstream adaptation.\nMulti-task BC (23): We train behavior cloning policies using 100 trajectories from the optimal policies in our training dataset for each task, and then finetune them with test trajectories, which are treated as trajectories with sparse rewards.\nMulti-task RL, CARE (22): We train CARE on each task for 2 million steps. For RL training, we train the algorithm in dense reward environments and finetune the model using test trajectories with sparse rewards, where feedback is only provided at the end of a trajectory.\nMeta-RL with hypernetworks (1): We train a hypernetwork with our training data with dense rewards, which can adapt to different task-specific policies during testing with test trajectories.\nMeta Imitation Learning with decision transformer(DT) (4) We train the pre-trained DT model using the training trajectories in our dataset, then use the test trajectories from replay to adapt it to test tasks."}, {"title": "Adaptability to environmental randomness on seen tasks.", "content": "Figure 5 demonstrates the significant advantage of our algorithm over other methods on seen tasks. This is attributed to the fact that, despite test trajectories originating from the same environment initialization, the generated policy parameters are more diverse, thus possessing a strong ability to adapt to environmental randomness. In contrast, other algorithms, when adapted using such singular trajectories, exhibit more limited adaptability in these scenarios. Our experimental design aligns with practical requirements, as real-world randomness is inherently more complex."}, {"title": "Generalizability to unseen tasks.", "content": "Figure 6 showcases the superior performance of our algorithm on unseen tasks. Test trajectories originate from the same environment setting for each task, while evaluation occurs in randomly initialized environments. Our policy generator, without fine-tuning, directly utilizes test trajectories as input, demonstrating a remarkable ability to generate parameters that work on unseen tasks. The agent's behavior in unseen tasks exhibits similarities to seen task behaviors, such as arm dynamics and the path to goals. By effectively combining parameter representations related to these features, the generative model successfully generates effective policies. In contrast, baseline methods struggle to adapt in environmental randomness.\nThese results strongly suggest that our algorithm, compared to other policy adaptation methods, may offer a superior solution for unseen scenarios. To further investigate robustness in generalization, we added Gaussian noise with a standard deviation of 0.1 to actions in test trajectories used for policy generation or adaptation on unseen tasks. Figure 7 demonstrates that our method remains resilient to noisy inputs, while the performance of the baselines is significantly impacted. We believe this is because our behavior embeddings only need to capture key dynamic information as conditions to generate policies, without directly learning state-action relationships from trajectories, resulting in better robustness."}, {"title": "Trajectory difference.", "content": "To compare the difference between using test trajectories as conditions and the trajectories obtained by deploying the generated policies, we visualize the trajectories during unseen task evaluations. As shown in Figure 9, our diffusion generator can synthesize various policies, which is significantly different from policy learning methods that learn to predict actions or states from trajectory data. We believe that this phenomenon fully illustrates the value of our proposed policy parameter generation paradigm."}, {"title": "Parameter distinction.", "content": "Beyond trajectory differences, we also investigate the distinction between synthesized parameters and RL policy parameters. We calculate the cosine similarity between the RL policies used to obtain the test trajectories and the parameters generated from these trajectories. As a benchmark, we include the RL policies after 100 steps of finetuning with the test data. For tasks seen during training, the parameters generated by our approach demonstrate significantly greater diversity compared to the RL parameters after fine-tuning, indicating that our generator does not simply memorize training data. On unseen tasks, the similarity between our generated parameters and those learned by RL is almost negligible, with most similarities falling below 0.2. This further highlights the diversity and novelty of the policy parameters generated by our method."}, {"title": "Real-world Evaluation", "content": "We further deploy policies generated from simulation trajectories onto a quadruped robot, instructing it to complete tasks as illustrated in Figure 10. Our synthesized policies"}, {"title": "4.2 Ablation Studies", "content": "To better investigate the impact of each design choice in our method on the final results, we conduct a series of comprehensive ablation studies. All ablation studies report average results of the Top 5 generation models on MetaWorld.\nChoice of behavior embeddings. Regarding the choice of conditional embeddings, as illustrated in Figure 3, we concatenate h and v as generation conditions to maximally preserve trajectory information. Figure 8 shows that utilizing either embedding individually also achieves comparable performance due to our contrastive loss, ensuring efficient capture of dynamics information. Our contrastive behavior embeddings significantly outperform a baseline that adds an embedding layer in the diffusion model to encode trajectories as input. These ablation results underscore the effectiveness of our behavior embeddings.\nChoice of trajectory length. The trajectory length n used in behavior embeddings can also impact experimental results. Figure 12a demonstrates that overly short trajectories lead to performance degradation, probably due to the absence of crucial behavior information. However, beyond 40 steps, trajectory length minimally impacts policy generation, indicating that our method is not sensitive to the length of trajectories."}, {"title": "5 Related Works", "content": "Parameter Generation. Learning to generate neural networks has long been a compelling area of research. Since the introduction of Hypernetworks (10) and the subsequent extensions (3), several studies have explored neural network weight prediction. Hypertransformer (30) utilizes Transformers to generate weights for each layer of convolutional neural networks (CNN) using task samples for supervised and semi-supervised learning. Additionally, previous work (20) employs self-supervised learning to learn hyper representations of neural network weights. In the context of using diffusion models for parameter generation, G.pt (16) trains a diffusion transformer to generate parameters conditioned on learning metrics such as test losses and prediction errors, enabling the optimization of unseen parameters with a single update. Similarly, p-diff (24) propose a diffusion-based method to generate the last two normalization layers without any conditions for classification tasks. In contrast to these prior works, our focus is on policy learning problems. We develop a latent diffusion parameter generator that is more generalizable and scalable, based on agents' behaviors as prompts.\nLearning to Learn for Policy Learning. When discussing learning to learn in policy learning, the concept of meta-learning (7) has been widely explored. The goal of meta-RL (7; 6; 9; 14) is to learn a policy that can adapt to any new task from a given task distribution. During the meta-training or meta-testing process, prior meta-RL methods require rewards as supervision for policy adaptation. Meta-imitation learning (8; 5; 25) addresses a similar problem but assumes the availability of expert demonstrations. Diffusion models have also been used in meta-learning. Metadiff (28) models the gradient descent process for task-specific adaptation as a diffusion process to propose a diffusion-based meta-learning method. Our work departs from these learning-to-learn works. Instead, we shift the focus away from data distributions across tasks and simply leverage behavior embeddings as conditional inputs for policy synthesis in the parameter space."}, {"title": "6 Conclusion", "content": "In this paper, we introduced a novel policy generation method based on conditional diffusion models. Targeting the generation of policies in high-dimensional parameter spaces, we employ an autoencoder to encode and reconstruct parameters, incorporating a contrastive loss to learn efficient behavior embeddings. By prompting with these behavior embeddings, our policy generator can effectively produce diverse and well-performing policies. Extensive empirical results across various domains"}, {"title": "Limitation.", "content": "Due to the vast number of parameters involved, we have not yet explored larger and more diverse policy networks. Additionally, the capabilities of the parameter diffusion generator are limited by the parameter autoencoder. We believe there is substantial room for future research to explore more flexible parameter generation methods. It would also be interesting to apply our proposed generation framework to generate other structures, further facilitating exploration in policy learning within the parameter space."}, {"title": "A Implementation Details", "content": "All experiments were conducted on NVIDIA A40 GPUs.\nAutoencoder. The autoencoder implementation consists of a three-layer MLP encoder and a decoder. Prior to training, each layer of the policy network is flattened and encoded separately. The final mean and std layers are concatenated with the middle layer for encoding.\nThe hyperparameters used for the autoencoder are detailed in Table 1. On average, training an autoencoder requires 5 GPU hours.\nBehavior embedding.\nThe behavior embedding model consists of two three-layer MLP embeddings. During training, we concatenate the state and action sequences from the first n = 60 steps (each sequence having a length of 3) to form the input for the h embedding layer. Subsequently, we concatenate the m = 3 states after success as inputs for the v embedding layer. Both embedding layers output 128-dimensional vectors. When utilizing these embeddings as conditional inputs, we concatenate the h and v embeddings as 256-dimenional conditions.\nAll hyperparameters about the training of the behavior embeddings can be found in Table 2. A single training for the embeddings requires less than 1 GPU hour.\nConditional diffusion generator. Our diffusion model employs a 1D convolutional U-Net architecture as its backbone, utilizing behavior embeddings as global conditions. It outputs latent parameter representations with the same dimensionality as the autoencoder's output.\nTraining a single diffusion generator requires only 4 GPU hours. All relevant hyperparameters are detailed in Table 3.\nHyperparameters We conduct all experiments with this single set of hyperparameters."}, {"title": "B Experiments", "content": "B.1 Task Description\nMeta World Descriptions of tasks and random initialization:\nSeen tasks (Training):"}, {"title": "B.2 More Results", "content": "In addition to reporting the average performance of the top 5 generated results in the main paper, we rigorously define \"qualified policies\" as those achieving a 100% success rate in the test environment. Table 4 presents the proportion of qualified policies among 100 policy parameters generated from 100 trajectories. Notably, we maintain an average qualification rate of over 30% on seen tasks.\nFurthermore, even on unseen tasks, we can generate high-performing policies using an average of only 20 trajectories. Considering that our method does not rely on expert demonstrations, the quality and success rate of our generated policies significantly enhance the sample efficiency of policy learning."}, {"title": "B.3 Details of Real-world Robots", "content": "In this section, we detail the real-world robot applications of our method. We deploy synthesized policies on the Unitree Go2 quadruped, designing diverse real-world testing environments to evaluate two key aspects of agent performance: (1) stability during high-speed turning and backward locomotion, and (2) robustness of movements on uneven terrain (mats). Our deployment process consists of four key steps:\n\u2022 Obtain actor network parameters and corresponding test trajectories from IsaacGym simulations, where the actors are trained using walk-these-ways (13).\n\u2022 Train Make-An-Agent using the acquired training data.\n\u2022 Generate actor networks from randomly sampled IsaacGym trajectories, covering a variety of training periods.\n\u2022 Equip the Unitree Go2 quadruped with the generated actors and a pretrained adaptor, enabling it to complete designed, challenging locomotion tasks."}]}