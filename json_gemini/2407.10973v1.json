{"title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion", "authors": ["Yongyuan Liang", "Tingqiang Xu", "Kaizhe Hu", "Guangqi Jiang", "Furong Huang", "Huazhe Xu"], "abstract": "Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks.", "sections": [{"title": "1 Introduction", "content": "Policy learning traditionally involves using sampled trajectories from a replay buffer or behavior demonstrations to learn policies or trajectory models mapping from state s to action a, modeling a narrow behavior distribution. In this paper, we consider a shift in paradigm: moving beyond training a policy, can we reversely predict optimal policy network parameters using suboptimal trajectories from offline data? This approach would obviate the need to explicitly model behavior distributions, allowing us to learn the underlying parameter distributions in the parameter space, thus revealing the implicit relationship between agent behaviors for specific tasks and policy parameters.\nUsing low-dimensional demonstrations (such as agent behavior) to guide the generation of high-dimensional outputs (policy parameters) is a challenging problem. When diffusion models (12; 19) have demonstrated highly competitive performance on various tasks including text-to-image synthesis, we are inspired to approach policy network generation as a conditional denoising diffusion process. By progressively refining noise into structured parameters, the diffusion-based generator can discover various policies that are not only superior in performance but also more robust and efficient than the demonstration in the policy parameter space.\nWhile prior works on hypernetworks (10; 1; 17) explore the concept of training a hypernetwork to generate weights for another neural network, they primarily use hypernetworks as an initialization network of meta-learning (7) and then adapt to specific task settings. Our approach diverges from this paradigm by leveraging agent behaviors as direct prompts or to generate optimal policies within the"}, {"title": "2 Preliminaries", "content": "Policy Learning. Reinforcement Learning (RL) is structured within the formation of Markov Decision Processes (MDPs) (2), which is defined by the tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$. Here, $\\mathcal{S}$ signifies the state space, $\\mathcal{A}$ the action space, P the transition probabilities, R the reward function and $\\gamma$ the discount factor. RL aims to optimize an agent's policy $\\pi : \\mathcal{S} \\rightarrow \\mathcal{A}$, which outputs action $a_t$ based on state $s_t$ at each timestep, to maximize cumulative rewards. The optimal policy can be expressed as:\n$\\pi^* = \\underset{\\pi}{\\arg \\max} \\mathbb{E}_{z \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t]$\nwhere z represents a trajectory generated by following policy $\\pi$. In deep RL, policies $\\pi$ are represented using neural network function approximations (21), parameterized by $\\theta$, facilitating the learning of intricate behaviors across high-dimensional state and action spaces.\nDiffusion Models. Denoising Diffusion Probabilistic Models (DDPMs) (12) are generative models that frame data generation through a structured diffusion process, which involves iteratively adding noise to the data and then denoising it to recover the original signal. Given a sample $x_0$, the forward diffusion process to obtain $x_1, x_2, ..., x_T$ of increasing noise intensity is typically denoted by:\n$q(x_t | x_{t-1}) = \\mathcal{N}(x_t, \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)$,\nwhere q is the forward process, $\\mathcal{N}$ is Gaussian noise, and $\\beta_t \\in (0,1)$ is is the noise variance."}, {"title": "3 Methodology", "content": "Overview. An overview of our proposed methodology is illustrated in Figure 1. To achieve this, we address several key challenges: (1) Developing latent representations of high-dimensional policy parameters that can be effectively reconstructed into well-functioned policies. (2) Learning an embedding of behavior demonstrations that serves as an effective diffusion condition. (3) Training a conditional diffusion model specifically for policy parameter generation.\nParameter representation. We use an MLP with m layers as the common policy approximator. Consequently, when the full parameters of a policy are flattened, they form a high-dimensional vector."}, {"title": "4 Experiments", "content": "We conduct extensive experiments to evaluate Make-An-Agent, answering the following problems:\n\u2022 How does our method compare with other multi-task or learning-to-learn approaches for policy learning, in terms of performance on seen tasks and generalization to unseen tasks?\n\u2022 How scalable is our method, and can it be fine-tuned across different domains?\n\u2022 Does our method merely memorize policy parameters and trajectories of each task, or can it generate diverse and new behaviors?\nBenchmarks. We include two manipulation benchmarks for simulated experiments and real-world robot tasks to show the performance and capabilities of our method as visualized in Figure 4.\nMetaWorld. MetaWorld (27) is a benchmark suite for robotic tabletop manipulation, consisting of a diverse set of motion patterns for the Sawyer robotic arm and interactions with different objects. We selected 10 tasks for training as seen tasks and 8 for evaluation as unseen downstream tasks. Detailed descriptions of these tasks can be found in Appendix B.1. The state space of MetaWorld consists of 39 dimensions and the action space has 4 dimensions. The policy network architecture used for MetaWorld is a 4-layer MLP with 128 hidden units, containing a total of 22,664 parameters.\nRobosuite. Robosuite (31), a simulation benchmark designed for robotic manipulation, supports various robots such as the Sawyer and Panda arms. We train models on three manipulation tasks: Block Lifting, Door Opening and Nut Assembly, using the single-arm Panda robot. Evaluations are conducted on the same tasks using the Sawyer robot. This experimental design aims to validate the practicality of our approach by assessing whether the generated policy can be effectively utilized on different robots. In the Robosuite environment, the state space comprises 41 dimensions, and the action space consists of 8 dimensions. The policy network employed for this domain contains 23,952 parameters.\nQuadrupedal locomotion. To evaluate the policies generated by Make-An-Agent in the real world, we utilize walk-these-ways (13) to train policies on IsaacGym and use our method to generate actor networks conditioning on trajectories from IsaacGym simulation with the pretrained adaptation modules. Then, we deploy the generated policy on real robots in environments differ from simulations. The policies generated for real-world locomotion deployment comprise 50,956 parameters.\nDataset. We collect 1500 policy networks for each task in MetaWorld and Robosuite. These networks are sourced from policy checkpoints during SAC (11) training. The checkpoints are saved every"}, {"title": "4.1 Performance Analysis", "content": "By using test trajectories as conditions, our policy generator can produce an equivalent number of policy parameters. Compared with baselines, we report both the best result among the generated policies and the average performance of the top 5 policies. All algorithms use the same task-specific replay trajectories. The difference is that we use them as generation conditions, whereas other methods use them for adaptation.\nWe define policies achieving a 100% success rate during evaluation as qualified policies. The analysis of qualification rates for policies generated by our model is presented in Appendix B.2."}, {"title": "4.2 Ablation Studies", "content": "To better investigate the impact of each design choice in our method on the final results, we conduct a series of comprehensive ablation studies. All ablation studies report average results of the Top 5 generation models on MetaWorld.\nChoice of behavior embeddings. Regarding the choice of conditional embeddings, as illustrated in Figure 3, we concatenate h and v as generation conditions to maximally preserve trajectory information. Figure 8 shows that utilizing either embedding individually also achieves comparable performance due to our contrastive loss, ensuring efficient capture of dynamics information. Our contrastive behavior embeddings significantly outperform a baseline that adds an embedding layer in the diffusion model to encode trajectories as input. These ablation results underscore the effectiveness of our behavior embeddings.\nChoice of trajectory length. The trajectory length n used in behavior embeddings can also impact experimental results. Figure 12a demonstrates that overly short trajectories lead to performance degradation, probably due to the absence of crucial behavior information. However, beyond 40 steps, trajectory length minimally impacts policy generation, indicating that our method is not sensitive to the length of trajectories."}, {"title": "5 Related Works", "content": "Parameter Generation. Learning to generate neural networks has long been a compelling area of research. Since the introduction of Hypernetworks (10) and the subsequent extensions (3), several studies have explored neural network weight prediction. Hypertransformer (30) utilizes Transformers to generate weights for each layer of convolutional neural networks (CNN) using task samples for supervised and semi-supervised learning. Additionally, previous work (20) employs self-supervised learning to learn hyper representations of neural network weights. In the context of using diffusion models for parameter generation, G.pt (16) trains a diffusion transformer to generate parameters conditioned on learning metrics such as test losses and prediction errors, enabling the optimization of unseen parameters with a single update. Similarly, p-diff (24) propose a diffusion-based method to generate the last two normalization layers without any conditions for classification tasks. In contrast to these prior works, our focus is on policy learning problems. We develop a latent diffusion parameter generator that is more generalizable and scalable, based on agents' behaviors as prompts.\nLearning to Learn for Policy Learning. When discussing learning to learn in policy learning, the concept of meta-learning (7) has been widely explored. The goal of meta-RL (7; 6; 9; 14) is to learn a policy that can adapt to any new task from a given task distribution. During the meta-training or meta-testing process, prior meta-RL methods require rewards as supervision for policy adaptation. Meta-imitation learning (8; 5; 25) addresses a similar problem but assumes the availability of expert demonstrations. Diffusion models have also been used in meta-learning. Metadiff (28) models the gradient descent process for task-specific adaptation as a diffusion process to propose a diffusion-based meta-learning method. Our work departs from these learning-to-learn works. Instead, we shift the focus away from data distributions across tasks and simply leverage behavior embeddings as conditional inputs for policy synthesis in the parameter space."}, {"title": "6 Conclusion", "content": "In this paper, we introduced a novel policy generation method based on conditional diffusion models. Targeting the generation of policies in high-dimensional parameter spaces, we employ an autoencoder to encode and reconstruct parameters, incorporating a contrastive loss to learn efficient behavior embeddings. By prompting with these behavior embeddings, our policy generator can effectively produce diverse and well-performing policies. Extensive empirical results across various domains demonstrate the versatility of our approach in multi-task settings, the generalization ability on unseen tasks, and the resilience to environmental randomness. Our work not only introduces a fresh perspective on policy learning, but also establishes a new paradigm that delves into the latent connections between agent behaviors and policy parameters.\nLimitation. Due to the vast number of parameters involved, we have not yet explored larger and more diverse policy networks. Additionally, the capabilities of the parameter diffusion generator are limited by the parameter autoencoder. We believe there is substantial room for future research to explore more flexible parameter generation methods. It would also be interesting to apply our proposed generation framework to generate other structures, further facilitating exploration in policy learning within the parameter space."}, {"title": "Appendix", "content": "A Implementation Details\nAll experiments were conducted on NVIDIA A40 GPUs.\nAutoencoder. The autoencoder implementation consists of a three-layer MLP encoder and a decoder. Prior to training, each layer of the policy network is flattened and encoded separately. The final mean and std layers are concatenated with the middle layer for encoding.\nThe hyperparameters used for the autoencoder are detailed in Table 1. On average, training an autoencoder requires 5 GPU hours.\nBehavior embedding.\nThe behavior embedding model consists of two three-layer MLP embeddings. During training, we concatenate the state and action sequences from the first n = 60 steps (each sequence having a length of 3) to form the input for the h embedding layer. Subsequently, we concatenate the m = 3 states after success as inputs for the v embedding layer. Both embedding layers output 128-dimensional vectors. When utilizing these embeddings as conditional inputs, we concatenate the h and v embeddings as 256-dimenional conditions.\nAll hyperparameters about the training of the behavior embeddings can be found in Table 2. A single training for the embeddings requires less than 1 GPU hour.\nConditional diffusion generator. Our diffusion model employs a 1D convolutional U-Net architecture as its backbone, utilizing behavior embeddings as global conditions. It outputs latent parameter representations with the same dimensionality as the autoencoder's output.\nTraining a single diffusion generator requires only 4 GPU hours. All relevant hyperparameters are detailed in Table 3.\nHyperparameters We conduct all experiments with this single set of hyperparameters.\nB Experiments\nB.1 Task Description\nMeta World Descriptions of tasks and random initialization:\nSeen tasks (Training):\n\u2022 window open: Push and open a window. Randomize window positions\n\u2022 door open: Open a door with a revolving joint. Randomize door positions\n\u2022 drawer open: Open a drawer. Randomize drawer positions\n\u2022 dial turn: Rotate a dial 180 degrees. Randomize dial positions\n\u2022 faucet close: Rotate the faucet clockwise. Randomize faucet positions\n\u2022 button press: Press a button. Randomize button positions\n\u2022 door unlock: Unlock the door by rotating the lock clockwise. Randomize door positions\n\u2022 handle press: Press a handle down. Randomize the handle positions\n\u2022 plate slide: Slide a plate into a cabinet. Randomize the plate and cabinet positions\n\u2022 reach: reach a goal position. Randomize the goal positions\nUnseen tasks (Downstream):\n\u2022 window close: Push and close a window. Randomize window positions\n\u2022 door close: Close a door with a revolving joint. Randomize door positions\n\u2022 drawer close: Open a drawer. Randomize drawer positions\n\u2022 faucet open: Rotate the faucet counter-clockwise. Randomize faucet positions\n\u2022 button press wall: Bypass a wall and press a button. Randomize the button positions\n\u2022 door lock: Lock the door by rotating the lock clockwise. Randomize door positions\n\u2022 handle press side: Press a handle down sideways. Randomize the handle positions\n\u2022 coffee-button: Push a button on the coffee machine. Randomize the position of the button\n\u2022 reach wall: Bypass a wall and reach a goal. Randomize goal positions\nRobosuite Descriptions of tasks, robots, and random initialization:\nTasks:\n\u2022 Door: A door with a handle is mounted in free space in front of a single robot arm. The robot arm must learn to turn the handle and open the door. The door location is randomized at the beginning of each episode.\n\u2022 Lift: A cube is placed on the tabletop in front of a single robot arm. The robot arm must lift the cube above a certain height. The cube location is randomized at the beginning of each episode.\n\u2022 Nut Assembly - Single: Two colored pegs (one square and one round) are mounted on the tabletop, and two colored nuts (one square and one round) are placed on the table in front of a single robot arm. The goal is to place either one round nut or one square nut into its peg.\nRobots:\n\u2022 Panda: Panda is a 7-DoF and relatively new robot model produced by Franka Emika, and boasts high positional accuracy and repeatability. The default gripper for this robot is the PandaGripper, a parallel-jaw gripper equipped with two small finger pads, that comes shipped with the robot arm.\n\u2022 Sawyer: Sawyer is Rethink Robotic's 7-DoF single-arm robot. Sawyer's default RethinkGripper model is a parallel-jaw gripper with long fingers and useful for grasping a variety of objects."}, {"title": "B.2 More Results", "content": "In addition to reporting the average performance of the top 5 generated results in the main paper, we rigorously define \"qualified policies\" as those achieving a 100% success rate in the test environment. Table 4 presents the proportion of qualified policies among 100 policy parameters generated from 100 trajectories. Notably, we maintain an average qualification rate of over 30% on seen tasks.\nFurthermore, even on unseen tasks, we can generate high-performing policies using an average of only 20 trajectories. Considering that our method does not rely on expert demonstrations, the quality and success rate of our generated policies significantly enhance the sample efficiency of policy learning."}, {"title": "B.3 Details of Real-world Robots", "content": "In this section, we detail the real-world robot applications of our method. We deploy synthesized policies on the Unitree Go2 quadruped, designing diverse real-world testing environments to evaluate two key aspects of agent performance: (1) stability during high-speed turning and backward locomotion, and (2) robustness of movements on uneven terrain (mats). Our deployment process consists of four key steps:\n\u2022 Obtain actor network parameters and corresponding test trajectories from IsaacGym simulations, where the actors are trained using walk-these-ways (13).\n\u2022 Train Make-An-Agent using the acquired training data.\n\u2022 Generate actor networks from randomly sampled IsaacGym trajectories, covering a variety of training periods.\n\u2022 Equip the Unitree Go2 quadruped with the generated actors and a pretrained adaptor, enabling it to complete designed, challenging locomotion tasks."}]}