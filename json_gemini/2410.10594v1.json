{"title": "VISRAG: VISION-BASED RETRIEVAL-AUGMENTED GENERATION ON MULTI-MODALITY DOCUMENTS", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM. Compared to traditional text-based RAG, VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in VisRAG and explore a variety of generation methods. Experiments demonstrate that VisRAG outperforms traditional RAG in both the retrieval and generation stages, achieving a 25-39% end-to-end performance gain over traditional text-based RAG pipeline. Further analysis reveals that VisRAG is effective in utilizing training data and demonstrates strong generalization capability, positioning it as a promising solution for RAG on multi-modality documents. Our code and data are available at https://github.com/openbmb/visrag.", "sections": [{"title": "INTRODUCTION", "content": "Trained on massive data, large language models (LLMs) like GPT-4 (Achiam et al., 2023) have shown strong abilities in common NLP tasks using their parametric knowledge (Wei et al., 2022; Zhao et al., 2023). However, the issue of hallucination (Ji et al., 2023; Bang et al., 2023) and the challenge of updating the parametric knowledge limit their real-world application in specific domains. Retrieval-augmented generation (RAG) alleviates this problem by using a knowledge retriever, which has access to a custom outer knowledge base, to supply the LLM with the necessary information for generating outputs (Guu et al., 2020; Lewis et al., 2020; Yu et al., 2023). Open-source RAG frameworks like llamaindex (Liu, 2022) have been developed to facilitate the research and deployment of common RAG pipelines.\nTypical retrieval-augmented generation (RAG) pipelines are text-based, operating on segmented texts as retrieval units (Yu et al., 2023; Asai et al., 2024; Yan et al., 2024), which we refer to as TextRAG. In real-world scenarios, knowledge is often presented in multi-modality documents such as textbooks and manuals which may have texts and figures intersected together. To acquire texts from such data sources, a parsing stage is often employed, which typically involves a cascade of processes, including layout recognition, optical character recognition (OCR), and post-processing steps like text joining (Zhang et al., 2024). While effective in most scenarios, the parsing process inevitably introduces errors, which can negatively impact the retrieval and generation phases. Moreover, text-based RAG utilizes only textual information, overlooking potential information present in"}, {"title": "RELATED WORK", "content": "Retrieval-augmented Generation (RAG). RAG enhances large language models (LLMs) by incorporating retrieved information from external knowledge bases, which assists in addressing knowledge-intensive tasks (Guu et al., 2020), reducing hallucinations (Semnani et al., 2023), and acquiring new knowledge (Vu et al., 2023). An RAG pipeline typically comprises a text-based retriever that fetches relevant information from the knowledge base given the user query, and an LLM-based generator that reads the query along with the retrieved information to generate an answer (Shi et al., 2024b; Yu et al., 2023). Prior research on RAG primarily focuses on: a) improving the retriever, which is typically a text encoder producing text embeddings, through generator feedback (Yu et al., 2023; Shi et al., 2024b); b) enhancing the generator via supervised fine-tuning (Lin et al., 2024; Xu et al., 2024a), in-context pre-training (Shi et al., 2024a), or advanced prompting (Xu et al., 2024c); and c) developing advanced RAG pipelines to handle long-form or multi-hop question answering (Jiang et al., 2023; Asai et al., 2024). However, research on RAG has predominantly targeted cleaned text corpora like Wikipedia from an academic standpoint. Building effective RAG pipelines for real-world, multi-modal documents remains a challenge.\nVision-language Models. Recent advancements in vision-language models (VLMs) have greatly improved fine-grained multi-modal understanding. Since CLIP (Radford et al., 2021) pioneered contrastive visual-text alignment, models like Flamingo (Alayrac et al., 2022), LLaVA (Liu et al., 2023b), and BLIP (Li et al., 2022) have expanded LLMs to process visual inputs by connecting languages models with a CLIP-style vision encoder. Research has then shifted towards more advanced multi-task and multi-stage pre-training paradigms, enabling models to generalize across a wide range of vision-language tasks (Liu et al., 2024; Bai et al., 2023; Wang et al., 2023; Dai et al., 2023). This is followed by notable advancements in high-resolution visual understanding (Xu et al., 2024b; Bavishi et al., 2023; Lin et al., 2023) and OCR capabilities (Kim et al., 2022; Lee et al., 2023; Hong et al., 2024; Chen et al., 2024b). More recently, breakthroughs have been made in multi-image understanding (Li et al., 2024a; Wang et al., 2024). Recent open-source VLMs like the MiniCPM-V (Yao et al., 2024) and Qwen2-VL (Wang et al., 2024) series combine the merits of recent techniques, achieving state-of-the-art performance. Those features of VLMs provide a foundation for our vision-based RAG pipeline, which requires multi-modal document understanding.\nMulti-modality Retrieval and RAG. Multi-modal retrieval encompasses a wide range of tasks, such as retrieving a matching image given the text (Han et al., 2017), retrieving a text-image pair to answer a question (Chang et al., 2022), and retrieving texts that answer the given query about a provided image (Hu et al., 2023a; Luo et al., 2023), etc. Wei et al. (2023) propose UniIR, a universal multi-modal retrieval model capable of addressing the aforementioned multiple tasks. The retrieved information is then employed for incorporating knowledge (Hu et al., 2023b; Luo et al., 2021) or in-context learning (Tan et al., 2024; Liu et al., 2023a), with the aim of generating answers or images (Sharifymoghaddam et al., 2024). Prior research mentioned above is conducted on academic datasets, where texts and images are meticulously extracted from raw data and paired (e.g., images with their captions), to make it feasible to do separate encoding of data in different modalities. This hinders their applicability in real-world RAG scenarios, as real-world multi-modal documents are often presented in mixed modalities, and information may be distributed across various combinations of modalities. Concurrent works DSE (Ma et al., 2024) and ColPali (Faysse et al., 2024) address this issue by directly encoding the image of a document for retrieval. However, as these studies focus on retrieval, they lack a comprehensive comparison of their approaches with text-based retrieval in both in-domain and out-of-domain settings, and do not conduct an end-to-end RAG evaluation."}, {"title": "METHODOLOGY", "content": "In this section, we first recap the typical RAG pipeline (Sec. 3.1), then present our VisRAG framework (Sec. 3.2) and the construction of our training and evaluation data (Sec. 3.3)."}, {"title": "PRELIMINARY: RETRIEVAL-AUGMENTED GENERATION", "content": "A typical retrieval-augmented generation (RAG) pipeline consists of a retriever and a generator, both built on large language models (LLMs). This pipeline operates on a knowledge corpus D, which is processed into units for retrieval and generation, denoted as $D = \\{d_1,...,d_n\\}$, where n is the number of retrieval units. Given a text query q and the retrieval corpus D, the retriever functions as $R : (q, D) \\rightarrow D_R$, taking q and D as inputs and producing a candidate set $D_R \\subset D$. To enable efficient search, the units in the knowledge corpus D are pre-encoded into embeddings. During RAG pipeline inference, approximate nearest neighbor (ANN) search is applied to retrieve $D_R$, which serves as the knowledge source for generation. The generation process can be defined as a function $G: (q, D_R) \\rightarrow a$, where a represents the answer and G denotes the LLM generator. This is achieved by prompting the LLM with the query and the retrieved units $D_R$ to generate an answer.\nAs shown in Figure 2 (left), traditional RAG frameworks (TextRAG) typically utilize text-based units for retrieval and generation. However, in real-world scenarios, data often appear in complex, multi-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose to use the page as the fundamental unit for retrieval and generation, which is directly processed by vision language models (VLMs) as an image without further processing during retrieval and generation. In subsequent sections, we use the terms \"page\" and \"document\" interchangeably."}, {"title": "VISRAG: VISION-BASED RETRIEVAL-AUGMENTED GENERATION", "content": "In this section, we present Vision-based Retrieval-augmented Generation (VisRAG), as shown in Figure 2 (right). In contrast to traditional RAG frameworks which use text segments for both retrieval and generation, VisRAG leverages the image of the document to preserve all information."}, {"title": "RETRIEVAL", "content": "The first stage of VisRAG, VisRAG-Ret, aims to retrieve a set of pages from the corpus D given q. We follow the dual-encoder paradigm in text-based dense retrieval models (Karpukhin et al., 2020) but employ a VLM rather than an LLM to encode the query and page. Specifically, the query and page are encoded separately as text and image in the VLM, producing in a sequence of hidden states. To derive the final embedding, and given that we use generative VLMs with causual attention, we adopt the position-weighted mean pooling over the last-layer VLM hidden states (Muennighoff, 2022), giving higher weights to later tokens:\n$V = \\sum_{i=1}^{S} w_i h_i$,\nwhere $h_i$ is the i-th hidden state, S is the sequence length, $w_i = \\frac{i}{\\sum_{i=1}^{S} i}$ is the i-th weight, and v is the query or page embedding. The similarity score is calculated by the cosine similarity of the query and page embedding. VisRAG-Ret is optimized using the InfoNCE loss:\n$l(q, d^+, D^-) = -log\\frac{exp(s(q, d^+)/\\tau)}{exp(s(q, d^+)/\\tau) + \\sum_{d^- \\in D^-} exp(s(q, d^-)/\\tau)}$,\nwhere $d^+$, $D^-$ are positive document and the negative document set of q, respectively, s(q, d) is the similarity score between q and d, and $\\tau$ is the temperature."}, {"title": "GENERATION", "content": "The second stage of VisRAG, VisRAG-Gen, focuses on generating the answer according to the user query and retrieved pages using a VLM. We propose the following mechanisms to enable VisRAG-Gen to handle multiple retrieved pages in $D_R$ for generation. The prompts used for generation is presented in Appendix E."}, {"title": "Page Concatenation", "content": "A straightforward approach is to concatenate all pages in $D_R$ into a single image to accommodate most VLMs that are trained to accept a single image. Formally,\n$a \\leftarrow VLM-Single(q, Concat(\\{d|d \\in D_R\\}))$,\nwhere VLM-Single is a VLM that accepts a single image with text prompt and Concat is the image concatenation operation. In this paper, we experiment with horizontal concatenation."}, {"title": "Weighted Selection", "content": "Another approach is to ask the VLM to generate an answer for every page from top-k, and select a final one with the highest confidence (Lewis et al., 2020; Shi et al., 2024b). The final confidence is defined as the weighted generation probability of the answer:\n$P(a|q, D_R) = \\sum_{d \\in D_R} P(a|q, d) A(q, d)$,\nwhere P(a|d, q) is calculated as the reciprocal of the perplexity of generating the answer a conditioned on the single document d, and A(d, q) is the normalized retrieval score:\n$A(q, d) = \\frac{e^{s(q,d)}}{\\sum_{d'\\in D_R}e^{s(q,d')}}$."}, {"title": "VLMS Accepting Multiple Images", "content": "Some recent VLMs like MiniCPM-V 2.6 (OpenBMB, 2024b) and Qwen-VL 2 (Wang et al., 2024) are designed and trained to accept multiple images as input to perform cross-image reasoning. This capability may be useful for the generation as the required information could be located on a single page from the retrieved document set $D_R$ for single-hop questions or spread across multiple pages for multi-hop questions. Formally, we have\n$a\\leftarrow VLM-Multi(q, \\{d|d \\in D_R\\})$,\nwhere VLM-Multi is the VLM that accepts multiple images with text prompt."}, {"title": "DATA CONSTRUCTION", "content": "To effectively build and evaluate RAG pipelines on multi-modal documents, we construct our datasets using a combination of visual question answering (VQA) datasets and synthetic data. The statistics of our constructed dataset are provided in Table 1."}, {"title": "Data Sources", "content": "We collect question-document pairs from a series of VQA datasets, targeting different document types: MP-DocVQA (Tito et al., 2023) for industrial documents, ArXivQA (Li et al., 2024b), ChartQA (Masry et al., 2022), InfographicsVQA (Mathew et al., 2022), and PlotQA (Methani et al., 2020) for various figure types, and SlideVQA (Tanaka et al., 2023) for presentation slides. All datasets feature questions that can be answered using a single document (page), except for SlideVQA, which includes multi-hop questions requiring information from multiple pages. We follow the original datasets' train-test splits, except for MP-DocVQA and InfographicsVQA, where the validation split serves as our evaluation set. Additionally, we enhance our training set by collecting openly available PDFs from online sources and generating queries using GPT-40 (OpenAI, 2024), with details presented in Appendix A.1. We assemble the retrieval corpus by gathering the positive document associated with each query from the training and evaluation sets."}, {"title": "Query Filtering", "content": "Some queries extracted from VQA datasets are context-dependent, which lack specificity to a certain entity. For instance, the response to \u201cWhere was the conference held?\u201d varies based on the contextual document. Using such context-dependent queries in open retrieval tasks is ineffective because they lack strong document specificity. To address this, we implement an additional filtering stage to remove these context-dependent questions, where we prompt llama-3-8b-instruct (AI@Meta, 2024) with human-annotated in-context samples to generate the classification label. Table 1 shows a substantial reduction in context-dependent questions across data sources. The details of filtering are presented in Appendix A.2."}, {"title": "Evaluation Metrics", "content": "For retrieval, we evaluate the performance using MRR@10 and Recall@10. For generation, consistent with methods applied to the source datasets, we report the answer accuracy, employing a relaxed exact match metric which allows a 5% error margin for numeric responses (Masry et al., 2022; Methani et al., 2020)."}, {"title": "EXPERIMENTAL METHODOLOGY", "content": "Document Parsing. To assess the performance of VisRAG in comparison to TextRAG, we employ specific text extraction methods. The first approach, referred to as \u201c(OCR)\u201d in subsequent text, is a pipeline that initially leverages PPOCR (Du et al., 2020) to identify text regions, then combines vertically aligned and horizontally proximate text boxes to reduce fragmentation. The second approach, termed \u201c(Captioner)\u201d, is an end-to-end model-based method. In this approach, we apply MiniCPM-V 2.0 (OpenBMB, 2024a; Yao et al., 2024), fine-tuned on paired (document image, extracted text) data, to directly parse text from the document image. Details of the parsing processes are presented in Appendix B.\nRetrieval Experiments. VisRAG-Ret is a document embedding model built on MiniCPM-V 2.0, a vision-language model that integrates SigLIP (Zhai et al., 2023) as the vision encoder and MiniCPM (Hu et al., 2024) as the language model. To ensure fair comparisons, we organize experiments into three settings: off-the-shelf, out-of-domain, and in-domain, as depicted below.\n\u2022 Off-the-shelf: We directly evaluate popular text and image retrieval models on extracted texts, including BM25 (OCR), a lexical model; bge-large-en-v1.5 (Xiao et al., 2023) (OCR) and NV-Embed-v2 (Lee et al., 2024) (OCR), state-of-the-art text embedding models with sizes 335M and 7.85B, respectively; and SigLIP, a CLIP-style (Radford et al., 2021) vision model serving as the encoder for MiniCPM-V series.\n\u2022 Out-of-domain: Models in this category are trained solely on synthetic data and evaluated on the VQA datasets, lacking in-domain supervision, in order to show the models' generalization capabilities. These models include textual models MiniCPM (OCR), MiniCPM (Captioner), and vision model SigLIP. MiniCPM (OCR) and (Captioner) are MiniCPM-based text embedding models trained and evaluated on extracted text.\n\u2022 In-domain: Models in this category are trained on the blend of the VQA training data and synthetic data. We evaluate the same set of models as in the out-of-domain setting to show model performance when supervised labels are available. We also run ColPali (Faysse"}, {"title": "EVALUATION RESULTS", "content": "Overall Performance. In this experiment, we compare VisRAG-Ret with (a) off-the-shelf models, and trained baselines in (b) out-of-domain setting where we only leverage synthetic data, and in (c) in-domain setting where we leverage both in-domain and synthetic training data.\nAs shown in Table 2(a)(b), VisRAG-Ret, trained on out-of-domain data, outperforms all off-the-shelf baselines, including both text and vision models. It significantly outperforms both BM25 and bge-large, and surpasses NV-Embed-v2, a state-of-the-art text retrieval model with 7.85B parameters. Note that bge-large and NV-Embed-v2 are trained on millions of query-doc pairs (Xiao et al., 2023; Lee et al., 2024), which are 10x more than our training data. Although bge-large outperforms BM25 on benchmarks like MTEB (Muennighoff et al., 2023), it fails on our datasets, indicating text-based embedding models trained on clean text struggle with texts parsed from real-world documents."}, {"title": "TRAINING DATA EFFICIENCY", "content": "As retrieval acts as the bottleneck in an RAG pipeline, it is crucial to have an effective retrieval component to maintain optimal performance. In this"}, {"title": "PERFORMANCE ON DIFFERENT DATA SUBSETS", "content": "In this experiment, we assess the retrieval and generation performance of VisRAG and TextRAG defined in Figure 3, as well as VisRAG (SigLIP), which replaces the retriever in VisRAG with SigLIP. We report their performance across different data subsets by categorizing queries based on the lengths of their positive documents, measured by the number of tokens of the extracted text. Documents with a higher volume of extracted text may prioritize textual information over visual content. As illustrated in Figure 5, queries in ArxivQA and InfographicsVQA are divided into equal-sized bins according to the lengths of their relevant documents. For each bin, we calculate and plot the average performance differences between VisRAG and TextRAG, as well as between VisRAG (SigLIP) and TextRAG, to compare how each model performs relative to TextRAG. We observe that, in general, the relative performance of VisRAG and VisRAG (SigLIP) improves as the length of the relevant document decreases. This suggests that models with vision encoders can better understand documents that emphasize visual information. However, VisRAG (SigLIP) consistently underperforms VisRAG across all data subsets and, in some cases, even performs worse than TextRAG. In contrast, VisRAG consistently outperforms TextRAG, indicating that the underlying language model in VisRAG is crucial for better understanding the semantics conveyed through visual cues."}, {"title": "CONCLUSION", "content": "In this paper, we propose VisRAG, a novel retrieval-augmented generation (RAG) paradigm that utilizes vision-language models (VLMs) to facilitate retrieval and generation within an RAG pipeline, thereby eliminating the parsing stage required in traditional text-based RAG. Our empirical results demonstrate that VisRAG consistently outperforms text-based RAG on retrieval and generation while maintaining a simpler pipeline. We hope that VisRAG will inspire future RAG development to incorporate VLMs for handling multi-modal documents."}, {"title": "A DATA CONSTRUCTION DETAILS", "content": ""}, {"title": "SYNTHETIC DATA", "content": ""}, {"title": "QUERY FILTERING", "content": "As mentioned in Sec. 3.3, a significant portion of queries in VQA datasets are context-dependent that are unsuitable for retrieval. We prompt llama-3-8b-instruct (AI@Meta, 2024) to filter out such queries using the prompt in Figure 7, which includes human-annotated samples from DocVQA."}, {"title": "DOCUMENT PARSING", "content": "In this paper, we experiment with two categories of document parsing strategies: pipeline-based parsing and model-based parsing."}, {"title": "MODELS USED IN THIS PAPER", "content": "MiniCPM (Hu et al., 2024) is a large language model (LLM) with 2.4 billion non-embedding parameters, demonstrating capabilities comparable to much larger models, such as Llama2-7B (Touvron et al., 2023) and Gemma-7B (Team et al., 2024). In this paper, we employ MiniCPM to construct the baseline text-based retriever (Table 2) and generator (Table 3).\nSigLIP (Zhai et al., 2023) is a CLIP-style multi-modal model designed to align text and vision representations. We utilize SigLIP-400m, released by Hugging Face\u00b2, which incorporates Flash Attention 2, increases maximum resolution to 980x980, and adopts the NaViT strategy to allow (a) variable resolution images and (b) aspect ratio preserved images. In this paper, SigLIP is used to develop the baseline vision-based retriever (Table 2).\nMiniCPM-V 2.0 (OpenBMB, 2024a; Yao et al., 2024) is a vision-language model (VLM) with 2.8 billion non-embedding parameters, built upon SigLIP-400m and MiniCPM. It can process single images up to 1.8 million pixels (e.g., 1344x1344) at any aspect ratio. We use MiniCPM-V 2.0 to build VisRAG-Ret (Table 2) and VisRAG-Gen (Table 3(b)), as well as the document parsing model.\nMiniCPM-V 2.6 (OpenBMB, 2024b; Yao et al., 2024) is an upgrade of MiniCPM-V 2.0 and MiniCPM-Llama3-V 2.5 (Yao et al., 2024). It is built upon SigLIP-400M and Qwen2-7B (Yang et al., 2024) with a total of 8.5B parameters, exihibiting a significant performance improvement over MiniCPM-Llama3-V 2.5 (Yao et al., 2024). Different from previous models, MiniCPM-V 2.6 can accept multiple images as the input and perform multi-modal in-context learning. It also demonstrates stronger OCR capabilities. We use MiniCPM-V 2.6 to build VisRAG-Gen (Table 3) and a text-based generation baseline MiniCPM-V 2.6 (OCR) (Figure 3, Figure 5).\nNote that, MiniCPM-Llama3-V 2.5 (Yao et al., 2024) is not used in this paper.\nGPT-40 (OpenAI, 2024) is OpenAI's latest multi-modal model, capable of processing any combination of text, audio, image, and video inputs and generating outputs in text, audio, and image formats. We use GPT-40 to construct VisRAG-Gen (Table 3) and to synthesize training data."}, {"title": "ADDITIONAL RESULTS", "content": "Table 6 presents the retrieval performance in Recall@10."}, {"title": "PROMPTS FOR GENERATION", "content": "We present the prompts of VisRAG-Gen and TextRAG-Gen in Table 7."}, {"title": "CASE STUDY", "content": "We show two cases in Table 8 and Table 9. In both instances, we compare VisRAG with TextRAG, maintaining the same setup as described in the \"End-to-end Performance\" paragraph in Sec. 5.1.\nIn the first case from DocVQA, the user queries about \u201cClub Jetty,\u201d however, the term \u201cClub Jetty\u201d in the relevant document is not successfully extracted due to its decorative font. This leads to TextRAG failing to retrieve the document, while VisRAG successfully retrieves it.\nIn the second case from InfographicsVQA, although both TextRAG and VisRAG successfully retrieve the document, TextRAG generates an incorrect response due to the loss of layout information, making it unclear which number (53% or 49%) pertains to Europe. VisRAG effectively utilizes the layout information and generates the correct answer."}]}