{"title": "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "authors": ["Pengrui Han", "Peiyang Song", "Haofei Yu", "Jiaxuan You"], "abstract": "Recent advancements in artificial intelligence have led to the creation of highly capable large language models (LLMs) that can perform tasks in a human-like manner. However, LLMS exhibit only infant-level cognitive abilities in certain areas. One such area is the A-Not-B error, a phenomenon seen in infants where they repeat a previously rewarded behavior despite well-observed changed conditions. This highlights their lack of inhibitory control \u2013 the ability to stop a habitual or impulsive response. In our work, we design a text-based multi-choice QA scenario similar to the A-Not-B experimental settings to systematically test the inhibitory control abilities of LLMs. We found that state-of-the-art LLMs (like Llama3-8b) perform consistently well with in-context learning (ICL) but make errors and show a significant drop of as many as 83.3% in reasoning tasks when the context changes trivially. This suggests that LLMs only have inhibitory control abilities on par with human infants in this regard, often failing to suppress the previously established response pattern during ICL.", "sections": [{"title": "Introduction", "content": "In the field of cognitive science, there is a classic phenomenon called the A-Not-B error (Popick et al., 2011; Smith and Gasser, 2005; Vorms, 2012). In a typical A-Not-B task, an experimenter repeatedly hides an attractive toy at one location (e.g., under box A) within the baby's reach while the baby watches, and has the baby retrieve it several times. Then, during the critical trial, the experimenter moves the toy to a new location (e.g., under box B) that is also within the baby's reach while the baby watches, yet the baby fails to understand the straightforward change of situation, and continues previous actions to search under the original location (box A). This is a key indication of infants' limited inhibitory control: the ability to stop or control impulsive responses and instead use deliberate attention and reasoning when the context changes (Geier, 2013; Fiske and Holmboe, 2019). On the other hand, adults have developed strong inhibitory control abilities over the years as they have matured (Geier, 2013). For instance, an adult may initially reach for a frequently used coffee mug on its usual shelf but quickly adapt and reach for it on a new shelf after remembering it was moved the day before. Similarly, evidence has shown that when infants grow to about 12 months (MacNeill et al., 2018), they typically begin to have such inhibitory control abilities, and no longer exhibit A-not-B errors.\nRecent advancements in Large Language Models (LLMs) (Saravanan et al., 2023; Zhang et al., 2024a; Chang et al., 2024; Kasneci et al., 2023; Zhao et al., 2023b) have significantly impacted cognitive science-related research (Feng et al., 2024)."}, {"title": "Related Work", "content": "A-Not-B Error and Inhibitory Control. The A-Not-B error is a classic cognitive phenomenon observed in infants typically between 8 to 12 months, and resolves as they grow older (Popick et al., 2011; Smith and Gasser, 2005; Vorms, 2012). Researchers note this as a significant milestone in cognitive development in humans that signifies the emergence of inhibitory control (Diamond, 2013; Casey et al., 2000). Inhibitory control is the ability to inhibit a response \u2013 such as refraining from seeking an object at a previously habitual location A after explicitly seeing it moved to location B. This ability is among the three core components of executive functions (Diamond, 2013), along with working memory (Baddeley, 1992) and cognitive flexibility (Miyake et al., 2000). These cognitive abilities are crucial for human development and play a vital role in various aspects of cognition (Anderson et al., 2001; Best and Miller, 2010; Blair and Razza, 2007).\nInhibitory control, in particular, emerges early in development and is essential for fundamental human cognitive abilities such as decision-"}, {"title": "LLMs and Inhibitory Control", "content": "Humans learn from previous experience (Yip and Wilson, 2010), and inhibitory control allows them to suppress automatic periodic responses and avoid being distracted by past irrelevant patterns, thereby exhibiting more adaptive behaviors (Kolb and Whishaw, 2009; Diamond, 2013). LLMs, like humans, also learn from a vast array of previous experience (Hinton, 2002) including demonstrations (Wang et al., 2024; Shao et al., 2023), examples (Brown et al., 2020; Y\u0131ld\u0131z et al., 2024), and interactions (Collins et al., 2024; Yang and Deng, 2019). The learning takes place through diverse mechanisms such as ICL (Xie et al., 2021; Coda-Forno et al., 2023), fine-tuning (Han et al., 2024b,a), and pretraining(Y\u0131ld\u0131z et al., 2024; Raffel et al., 2020).\nPrevious studies show that LLMs are capable of learning from these experiences (data and con-"}, {"title": "LLMs and Multiple-Choice QA", "content": "In our experimental setting in Section \u00a73, we test LLMs on multiple-choice questions (MCQs). MCQs are widely used evaluators for LLMs (Zhang et al., 2024b; Li et al., 2024a). Prior research has shown that current LLMs do not perform consistently well on MCQ tasks. For instance, (Zheng et al., 2023) demonstrates that LLMs are sensitive to changes in the positioning of options and tend to prefer certain option IDs potentially due to token bias (Jiang et al., 2024). While such studies explore interesting aspects of LLM performance on MCQs, they primarily focus on the inherent capability of LLMs to perform on MCQs. In contrast, our work investigates whether, and how, external factors, such as minimal changes in context, impact the consistency of LLMs' performances on MCQs. Such different settings require LLMs to perform inhibitory control and exhibit trustworthy reasoning. We will elaborate more on the specific designs in Section \u00a73."}, {"title": "Experimental Setup", "content": "Experiment Motivation\nMotivated by the original A-not-B experiment, we adapt the scenario to be purely based on natural language, thus creating a parallel experiment for LLMs as shown in Figure 2. In the original experiment, the infant constantly observes the placement of a certain object to be in the same Location A. In ours, this is implemented by LLMs observing the ground truth answer to an MCQ question to be the same Option A. Similar to how the infant then observes the placement of the ball in a different \"Location B\", LLMs are tasked with a similar-style MCQ from the same domain whose ground truth answer is Option B. An A-not-B error is observed if the infant still looks for the object in Location A, or if the LLMs choose the wrong answer A. We believe that by using such an experiment setup, we are testing similar abilities between the original A-not-B cognitive experiment and the LLM version, because the main difficulty of both lies in inhibiting a previously established trivial pattern of one option being repetitively selected, when moving to a new scenario, and instead eliciting internal knowledge to solve new tasks.\nThis demonstrates limited inhibitory control because an infant would know well that the ball is in \"Location B\", if not paying excessive attention to the established pattern from previous demonstra-"}, {"title": "Models", "content": "We select four leading open-source models with different levels of abilities and knowledge. Specifically, we choose two families of LLMs: Llama3 (AI@Meta, 2024) and QWen-1.5 (Bai et al., 2023). Within each model family, we experiment with both large and small models to investigate the relationship between the impact of A-not-B errors and model sizes. Specifically, we experimented with Llama3-70b, Llama3-8b, Qwen1.5-72b, and Qwen1.5-7b."}, {"title": "Data", "content": "MCQA Datasets We choose four representative multiple-choice question-answering (MCQA) datasets, each tasking a particular fundamental reasoning task: the MathQA dataset (Amini et al., 2019) for arithmetic reasoning, the CommonsenseQA dataset (Talmor et al., 2019) for commonsense reasoning, the Winogrande dataset (Sakaguchi et al., 2019) for causal reasoning, and the SciQ dataset (Welbl et al., 2017) for scientific reasoning.\nPreprocessing We preprocess the datasets to split an MCQA sample into three parts: question statement, choices, and ground truth answer. We modify the MCQA samples so that each has only two choices left, with one of them being the ground truth and the other incorrect, thus loyally resembling the original A-not-B scenario."}, {"title": "Prompting Settings", "content": "We test the models' performances on the modified datasets in two different prompting settings: (1) the original and (2) the A-not-B settings.\nOriginal prompting We construct a prompt in the standard few-shot paradigm \u2013 we first provide $n$ (typically from 5 to 50) MCQA examples, and then ask one question to the model. We then check if the model's answer agrees with the ground truth."}, {"title": "Prompting", "content": "We reorder the options so that answers for all $n$ examples we provide are the first one (Option A). Then for the final question we ask, the ground truth is set as the second (Option B). For each model, we test both the original and the A-Not-B prompting settings with 100 data samples per task and record the success rates. The differences in the success rates then tell the LLMs' susceptibility to the A-not-B scenario."}, {"title": "Results", "content": "Our results in Table 1 show three key factors that are related to the inhibitory control ability of LLMs (1) model size, (2) number of few-shot examples, and (3) type of reasoning tasks. We discuss each of them in detail in the rest of this section.\nImpact of Model Size. In Figure 3, we compare the average rate of change across different numbers of few-shot examples for all four reasoning tasks between large models (Llama3-70b and Qwen1.5-72b) and small models (Llama3-8b and Qwen1.5-7b). The results show that model size significantly impacts performance, with smaller models consistently showing lower resilience (less accuracy) compared to larger models across all shots. Specifically, the larger models exhibit an average drop of 8.7% at 25 shots, while the smaller models show an average drop of 20.8% at 25 shots. This indicates that smaller models with fewer internal parameters and knowledge are more susceptible to the A-Not-B scenario, indicating less inhibitory control abilities.\nImpact of Number of Few-Shot Examples. In Figure 3, we can also see the average performance of large (Llama3-70b and Qwen1.5-72b) and small (Llama3-8b and Qwen1.5-7b) models with different numbers of few-shot examples. Naturally, as the number of few-shot examples increases, both large and small models are significantly more likely to suffer from an accuracy drop. This indicates that when trivial patterns are reinforced more, it becomes harder for LLMs to inhibit them and instead turn to deliberate attention for reasoning. Such errors can be easily avoided by humans, which we further show in Section 6.\nImpact of Reasoning Tasks. Experiments demonstrate that models' vulnerability to the A-Not-B scenario varies across different reasoning tasks, as indicated by the different degrees of performance drop. The most pronounced"}, {"title": "Ablation Study", "content": "In this section, we delve deeper into the implications of interesting findings from Section \u00a74. Specifically, we investigate 3 key research questions that naturally arise from the main results.\nRQ1: Why Qwen1.5-7b has abnormal behavior compared with other models? This is because Qwen1.5-7b is biased between A and B tokens during pretraining.\nIn our main results, we notice abnormal performance increases in Qwen1.5-7b on mathematical reasoning. Although the increases diverge from the A-not-B scenario we primarily study in this paper, they still suggest untrustworthy reasoning. This is because minor perturbations with no meaningful changes should not lead to any significant accuracy fluctuations if the model is reasoning consistently and reliably."}, {"title": "Discussions", "content": "Human Performance in A-not-B Scenarios\nWhile humans, particularly adults, generally possess sufficient inhibitory control to overcome the"}, {"title": "Better Pretraining Helps Mitigate A-not-B Errors", "content": "To better understand how model pretraining, particularly pretraining data, influences models' resistance to A-not-B errors, we further compare the rates of change in accuracy across different numbers of shots, in Llama3-8b, Llama3-70b, and Llama2-70b. Results are shown in Figure 6. These Llama-family models share the same fundamental architectures, but the Llama3 models have been trained on significantly larger, more novel, and higher-quality datasets compared to Llama2-70b. As illustrated in Figure 6, although Llama2-70b has the same model size as Llama3-70b, it is much more significantly impacted. In fact, Llama2-70b is less resilient to A-not-B errors even compared to the much smaller Llama3-8b, especially as the number of shots increases.\nThis observation suggests that the quality and quantity of pretraining data can significantly impact LLMs' inhibitory control. When encountering input prompts or contexts, an LLM must apply its internal knowledge, skills, and abilities learned from the pretraining distribution to predict responses. Therefore, better pretraining (both in quality and"}, {"title": "Conclusion", "content": "We have explored the intriguing cognitive phenomenon of A-Not-B errors within the domain of LLMs. Our findings reveal that, akin to human infants, even sophisticated models like LLMs exhibit limited inhibitory control to inhibit previously established trivial patterns. Through our experiments, we find that model size, the number of few-shot examples, and the type of reasoning tasks greatly impact LLMs' trustworthy reasoning abilities in the A-not-B scenario. Furthermore, we show that this limitation widely persists even in generalized cases and cannot be easily resolved by advanced self-explanation techniques. However, we demonstrate that increasing model sizes and improving pretraining, both in terms of data quantity and quality, can enhance LLMs' inhibitory control. Our findings provide several promising avenues for further exploration to understand and mitigate A-not-B errors in LLMs, enhancing their trustworthy reasoning."}, {"title": "Limitations and Future Works", "content": "We report the following limitations of this work and spaces for future explorations:\n1. While this work includes qualitative examples from advanced GPT models and Gemini, no large-scale experiments are performed with closed-source models due to budget limits. Future works can replicate our experiments on closed-source models to further study A-not-B errors.\n2. This work studies four mainstream reasoning tasks, while more specific domains and more diverse tasks remain unexplored. We include an easily usable toolkit in our codebase that enables future studies of A-not-B errors in more diverse reasoning tasks.\n3. While this work focuses on unveiling state-of-the-art LLMs' lack of inhibitory control abilities to perform trustworthy reasoning, how to improve this ability remains an important research question. Future works can start with the several directions drawn out in Section \u00a76 and propose mitigation methods of A-not-B errors.\n4. With this work focusing solely on A-not-B errors, it is of interest to explore this cognitive phenomenon with other axes in conjunction with other axes. For instance, are there trade-offs between A-not-B errors and other important aspects of LLMs, including energy efficiency (Zhao et al., 2024b; Gretsch et al., 2024), fairness (Kocielnik et al., 2023; Han et al., 2024a), robustness (Chao et al., 2024; Hou et al., 2024) and more."}, {"title": "Reproduction", "content": "We warmly welcome the reproduction of this work and follow-up investigation of A-not-B errors in LLMs across other settings, in order to better understand and improve LLMs' trustworthy reasoning. To facilitate future explorations, we have made all the results and code publicly available at https://github.com/Peiyang-Song/LLM-A-Not-B-Errors. The repository is self-contained with thorough documentation and guidance of usage in its README file. Here we present a high-level introduction.\nIn general, the repository contains:\n1. Datasets: Processed datasets across all four reasoning categories.\n2. Experiments: Code for all experiments, including ablation studies.\n3. Toolkit: A directly usable tool to investigate A-not-B phenomenon in any model on any reasoning task with various configurations.\nPlease refer to the documentation of our repository for more detailed introductions.\nAll experiments in this paper that involve open-source LLMs were conducted using the togetherAI API."}, {"title": "Data and Model Licenses", "content": "All datasets and models used in this study are open sources and publicly accessible, free uses granted under permissive licenses. We have ensured that we cited each one comprehensively, providing detailed references and acknowledgment of their respective sources."}, {"title": "Prompt Formats", "content": "Here we present the prompt templates used during our main and ablation experiments. Figure 7 shows the format for a few-shot prompt in our original setting, and Figure 8 shows that in the A-not-B setting. Figure 9 shows the format for many-shot prompts (the ablation experiment) in our original setting, and Figure 10 shows that in our A-not-B setting."}, {"title": "Gemini Failure Example", "content": "Figure 11 shows a screenshot taken on September 21, 2024, which illustrates the failure case example in Gemini we mentioned in Firgure 1."}]}