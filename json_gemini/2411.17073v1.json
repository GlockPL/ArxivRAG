{"title": "Path-RAG: Knowledge-Guided Key Region Retrieval for Open-ended Pathology Visual Question Answering", "authors": ["Awais Naeem", "Tianhao Li", "Huang-Ru Liao", "Jiawei Xu", "Aby Mammen Mathew", "Zehao Zhu", "Zhen Tan", "Ajay Kumar Jaiswal", "Raffi A. Salibian", "Ziniu Hu", "Tianlong Chen", "Ying Ding"], "abstract": "Accurate diagnosis and prognosis assisted by pathology images are essential for cancer treatment selection and planning. Despite the recent trend of adopting deep-learning approaches for analyzing complex pathology images, they fall short as they often overlook the domain-expert understanding of tissue structure and cell composition. In this work, we focus on a challenging Open-ended Pathology VQA (PathVQA-Open) task and propose a novel framework named Path-RAG, which leverages HistoCartography to retrieve relevant domain knowledge from pathology images and significantly improves performance on PathVQA-Open. Admitting the complexity of pathology image analysis, Path-RAG adopts a human-centered AI approach by retrieving domain knowledge using HistoCartography to select the relevant patches from pathology images. Our experiments suggest that domain guidance can significantly boost the accuracy of LLaVA-Med from 38% to 47%, with a notable gain of 28% for H&E-stained pathology images in the PathVQA-Open dataset. For longer-form question and answer pairs, our model consistently achieves significant improvements of 32.5% in ARCH-Open PubMed and 30.6% in ARCH-Open Books on H&E images. All our relevant codes and datasets will be open-sourced.", "sections": [{"title": "1. Introduction", "content": "Pathology image analysis plays an essential role in the diagnosis, treatment, and study of cancer Wang et al. (2019b). For example, reading hematoxylin and eosin (H&E)-stained slides is a clinical standard of care for the diagnosis and staging of many cancers Luo et al. (2017). The advent of whole-slide image (WSI) scanning, capable of capturing intricate histological details at high resolutions, has enabled remarkable advancements in pathology image analysis through deep learning algorithms to detect cancer regions Kather et al. (2016), classify tissue subtypes Javed et al. (2020), and identify diagnostically relevant structures and regions (e.g., nuclei Gamper et al. (2019), glands Graham et al. (2019), vessels and nerves Fraz et al. (2020)).\nWith recent advance of foundation model, a natural thinking is to utilize Visual Language Models (VLMs), like GPT4-V and LLAVA, for helping pathology analysis. However, studies show that on Open-ended Pathology visual question answering (VQA), recent state-of-the-art VLM LLAVA, even after fine-tuning, only achieves a recall of 38% Li et al. (2024).\nThe reason is because these VLMs are hard to identify fine-grained visual objects and textual entities, especially the intricate domain knowledge in pathology images. Therefore, in this paper, we resort to domain knowledge in pathology literature to help narrow down and identify key regions, and then use VLM as soft knowledge base to retrieve most important information from the image to be diagnosed. Specifically, we propose Path-RAG, refer to Retrieval Augmentation Generation for Pathology VQA.\nThe key question is which domain knowledge we'd like to retrieve. Since pathology images are complex and tightly connected with different tissue structures, normal deep learning approaches, that typically involve patch generation, patch-level feature extraction, and feature aggregation to generate image-level embeddings for pathology tasks, fall short as they often overlook crucial tissue structure and cell composition Jaume et al. (2021). The critical step for diagnosing pathology images is the pathologist's ability to identify tissue structure and examine the cellular composition, organization, and architecture of tissues to characterize features of normal structures and abnormal cells in histologic sections Bauer et al. (2020).\nFor analyzing this, cell graphs of cancer have been generated based on pathology images showing the distinct features compared with healthy cells at the cellular level and at the tissue level Gunduz et al. (2004). Cell graphs are one kind of entity graph where nodes and edges of the graphs denote tissue entities and their interactions. The entities can be nuclei, tissue regions, and glands. Entity graphs can simultaneously capture local entity environment and global tissue composition, illustrating an ideal way to capture domain-specific knowledge in pathology images. Entity graphs can improve the efficiency in diagnosis considering that only diagnostically relevant entities have been analyzed, instead of analyzing the entire pathology images Shaban et al. (2020). Generating entity graphs requires a complex workflow including stain normalization, tissue detection, entity detection, entity encoding, and constructing graph topology. HistoCartography is an open-source Python library that unifies a set of histology image manipulation tools to build entity graphs Graham et al. (2019); Ahmedt-Aristizabal et al. (2022), including stain normalization, tissue detection, nuclei detection, tissue component detection, feature extraction, and cell graph builders.\nIn Path-RAG, we utilize the existing HistoCartography tool to help decompose given image into several key region patches, and then use LLAVA-Med to summarize key information as well as candidate answer for each patch. The retrieved knowledge are fed into the final GPT-4 model for reasoning to provide the answer. In addition to using LLAVA-MED fine-tuned on PathVQA He et al. (2020), we also fine-tune LLAVA-MED on a custom dataset consisting of detailed open-ended question-answer pairs and pathology images which aids us in evaluating the efficacy of our knowledge-guided approach on detailed answers"}, {"title": "2. Methodology", "content": "In our approach to handling pathology VQA, we are employing a careful ensemble of tools built specifically to process pathology images with large language models (LLMs) pre-trained on the pathology data. Our approach obviates any need to further fine-tune LLMs which incur high computational and memory overhead. We are the first to illustrate that using an off-the-shelf domain-aware tool can significantly benefit the intricate task of pathology open-ended VQA rather than investing heavily in compute resources, which happens to be the norm when dealing with multi-modal data in the pathology domain. Furthermore, we convert the multi-modal problem (including image and text) into a solely text-based problem and then use the textual reasoning capabilities of GPT-4 as the last step. This technique gives us\nthe leverage to use multiple LLMs; one fine-tuned to handle pathology VQA in a multi-modal setting Li et al. (2024) and another one specialized for textual reasoning OpenAI (2024).\nPathology images can be categorized and represented in discrete units such as tissues, nuclei, cells, glands, etc. To answer any question related to pathology, it is considered essential to understand the image regions which are heavily concentrated in any discrete unit under consideration. In fact, this approach is used by radiologists when they are reasoning about the question asked about any diagnostic image Bauer et al. (2020). To follow a similar practice of analyzing pathology images, we are utilizing HistoCartography Jaume et al. (2021) which is a toolkit containing pre-trained models for graph analytics in digital pathology. We are using this tool for two purposes: (1) Differentiate between pathology and non-pathology images (2) Extract image patches which are concentrated in nuclei structures.\nLLaVA-Med Li et al. (2024) is a multi-modal language model fine-tuned on biomedical data and exhibits conversational capabilities to answer open-ended questions about a biomedical image. Since it can follow open-ended instructions to give relevant details about any biomedical image, we employ LLaVA-Med to generate descriptions/answers about the pathology image and/or its relevant patches. Finally, we use GPT-4 OpenAI (2024) to reason about the question using the textual descriptions generated via LLaVA-Med.\nOur method uses a sequential approach to use these components as shown in Figure 1 and subsequently detailed in the following sections."}, {"title": "2.1. H&E stained Pathology Image Identification", "content": "As our approach relies on generating the patches of the pathology image depending on the presence of nuclei in the image, firstly we need to determine whether an image is an H&E stained pathology image or not. Using state-of-the-art approaches, a binary image classifier can be used to differentiate pathology images from non-pathology images. However, to avoid the use of an extra classifier, we have used HistoCartography Jaume et al. (2021) tool to determine this classification dynamically. We use a nuclei extractor API in HistoCartography to detect the number of nuclei in the image. For any non-pathology image, the extractor will result in zero or miniature\nnumber of nuclei-like components in the image. For our use case, we have used a hard threshold value of minimum 5 nuclei to differentiate between pathology and non-pathology image. Identifying any image as non-pathology gives us leverage to not use HistoCartography to divide this image into patches and directly advance towards LLaVA-Med to get the description/answer about the whole image. For the pathology images, however, the next step is to extract the relevant patches. This approach aligns with our focus on cancer-related pathology, since H&E stained images are gold standard for tumor diagnosis."}, {"title": "2.2. Pathology Image Decomposition", "content": "To extract the relevant patches of pathology images, we use different tools available in HistoCartography Jaume et al. (2021). Firstly, we use stain normalizer to overcome stain variability in the input image. Secondly, we detect nuclei present in the processed image using a nuclei extractor module to come up with the pixel locations of nuclei and their centers. Third, a deep feature extractor based on ResNet34 He et al. (2016) is used to extract the features of each nucleus by taking input from all the neighborhood image patches around each nucleus Jaume et al. (2021). The encoded nuclei features are then fed into the KNNGraphBuilder which builds a graph based on DGL Wang et al. (2019a) such that each nucleus is connected to 5 nearest neighbors with a maximum distance threshold of 50 pixels as per the configuration defined in HistoCartography Jaume et al. (2021) tool. In the resulting nuclei graph, each nuclei center is represented by node whereas the edges connect the nearest neighbor nuclei. The procedure is illustrated in part B of Figure 1.\nOnce the graph is obtained, a custom approach is taken to extract only those patches of the image which have maximum number of nuclei. In this approach, the pathology image is divided into a total of 9 patches having 20% overlap between them irrespective of the total dimensions of the image. It results in varying patch size for different images because of overlapping between successive patches up/down and avoids miniature/big patches having fixed size. Finally, all the patches are ranked as per the number of nuclei centers contained in each patch and different numbers of top-ranked patches are considered for experiments and evaluation. To investigate if our domain-aware patch selection strategy is effec-"}, {"title": "2.3. Patch Captioning and Candidate Answer Extraction", "content": "To generate the textual descriptions and candidate of each patch (we abbreviate as answer/description later), we resort to LLaVA-Med Li et al. (2024) which can generate an open-ended description of biomedical images. Once the key image regions are extracted as per the domain knowledge Jaume et al. (2021), For the pathology images, we provide each patch to LLaVA-Med to generate the open-ended descriptions, and then generate a candidate answer independently. This results into multiple answer/description pairs of each key region along with the original image, which will be later provided to GPT-4 for the final-stage reasoning. For the non-pathology images, we only use the input image to generate a single answer/description of the complete image. The procedure is illustrated in part C of Figure 1."}, {"title": "2.4. Reasoning over Per-Patch Extracted Knowledge", "content": "As the last step, we are using GPT-4 OpenAI (2024) to reason about the final answer given question and all answer/description of the patches generated in the last step. For the non-pathology images, we pass in a question along with the complete description of the input image to GPT-4 in order to get a final response. For the pathology images, we have the answers/descriptions of each patch along with the original image, so we consider two different approaches for experimentation/evaluation: Path-RAG (answer) and Path-RAG (description). We pass in candidate answers and descriptions returned by LLaVA-Med, respectively, along with the question to GPT-4 and ask it to reason about the final answer. This response is the final answer which is returned as the final output of the system given a question and a pathology image as an input. The prompts given to GPT-4 for textual reasoning are added in supplmentary material. We also set a ablation baseline (Path-RAG w/o GPT-4 (answer)), which only consider our final answer as the concatenation of different answers returned by LLaVA-Med for each image/patch, without using GPT-4 for reasoning."}, {"title": "3. Experimental Setup", "content": ""}, {"title": "3.1. PathVQA Dataset.", "content": "PathVQA He et al. (2020) contains 32,799 question-answer pairs and 4,998 images from pathology textbooks. Each question has been manually checked to ensure correctness. For each image, several questions may be asked regarding multiple aspects such as location, shape, color, appearance, etc. All of the questions can be categorized into two types by whether the answer is yes or no (close-ended questions) and open-ended questions. We only consider open-ended questions in this work because SoTA methods Li et al. (2024); Saygin Seyfioglu et al. (2023) excel at yes/no questions, while performance on open-ended VQA re-"}, {"title": "3.2. ARCH-Open Dataset.", "content": "To address the lack of datasets accommodating the open-ended nature of pathology VQA, we constructed an open-ended VQA dataset namely ARCH-Open\nusing ARCH Gamper and Rajpoot (2021). ARCH is a multiple instance captioning dataset for pathology images in which caption-image pairs are provided from relevant PubMed articles and books. There are a total of 4,270 captions and 4,305 images from PubMed articles, whereas a total of 3,309 image-caption pairs are extracted from the pathology books. This dataset addressed the shortcomings of PathVQA He et al. (2020) and was subsequently used to assess the effectiveness of our PathRAG approach along with PathVQA. While PathVQA is the gold standard for open-ended VQA about pathology, it has significant flaws: answers are typically 2-4 words long (Figure: 2), and some questions can be answered without any image. We ensured each ARCH-Open question requires an image for a valid answer and the answer lengths align with the nature of open-ended (long-form) VQA task.\nTo construct ARCH-Open, we employed instruction tuning by providing image captions to GPT-4 OpenAI (2024) and then asked it to generate five open-ended question-answer pairs for each image-caption pair considering that caption text belongs to a pathology image (Appendix C). As a result, we got 21,350 VQA pairs from image-caption pairs extracted from PubMed articles (termed as ARCH-Open PubMed), and a total of 16,545 VQA pairs relating to image-caption pairs extracted from pathology books (termed as ARCH-Open Books). In ARCH-Open PubMed, the questions and answers have the average word length of 12.81 and 23.12, respectively (Figure: 3, 4), whereas the average word length for questions and answers in ARCH-Open"}, {"title": "3.3. Implementation Settings", "content": "We use LLaVa-Med model and LLaVa-Med model fine-tuned on PathVQA training set to get image descriptions from image input and answers from image and question input. Prompts used are given in Appendix A, B. All inference processes were done on a single NVIDIA RTX A6000 GPU. We use GPT-4 (gpt-4-0125-preview) OpenAI (2024) from OpenAI API for the experiments that need to generate final answers after doing inference on LLaVa-Med model. The temperature of GPT-4 is set to be 0. Because LLMs' responses are stochastic and depend on the temperature, replication of the scores may be slightly different."}, {"title": "4. Results and Ablation Study", "content": ""}, {"title": "4.1. Comparison with SOTA:", "content": "In Table 1, we compare our method with existing generation-based methods on the open set of PathVQA He et al. (2020) datasets. Following previous works Li et al. (2024); Saygin Seyfioglu et al. (2023), we evaluate our performance using recall. To compare with our main baseline LLaVA-Med Li et al. (2024), we use both finetuned and not fine-tuned models on PathVQA, and explored different forms of textual input to the GPT-4 model. To highlight the effectiveness of our HistoCartography Jaume et al. (2021) patches retrieval, we categorized the testing data into H&E-stained pathology images or non-H&E-stained pathology images. Our major findings are: \u2460 We achieve SoTA on PathVQA open-set dataset with 47.4% recall. \u2461 Our method demonstrates a significant improvement of 27.7% over LLaVA-Med with fine-tuned model on H&E-stained pathology images, highlighting the importance of incorporating HistoCartography for pathology image analysis. \u2462 For models not fine-tuned on PathVQA, our method using description outperforms LLaVA-Med by 11.6%, providing further evidence of effectiveness of our approach."}, {"title": "4.2. Effectiveness of Retrieving Patches with HistoCartography:", "content": "To demonstrate the effectiveness of our patches retrieval, we conducted a comparative analysis of three methods: employing no patches, using random patches, and our method, which utilizes domain relevant patches retrieved by HistoCartography Jaume et al. (2021) as presented in Table 3. For textual input as answer, we first compared scenarios where no patches are employed, our method demonstrates a significant recall improvement of 9.3%, emphasizing the critical importance of extracting the region of interest patches. These patches provide essential pathology features that can enhance the performance. Secondly, our method outperforms the utilization of random patches by 1.2% in terms of recall improvement, highlighting the necessity of incorporating domain knowledge to extract relevant patches. Notably, the presence of features such as the number of nuclei in a patch emerges as significant regions of interest for pathology multi-modal models. Such results highlight the significant advantages brought about by retrieving domain relevant patches using HistoCartography."}, {"title": "4.3. Effect of textual reasoning:", "content": "We ablate the components used for textual reasoning. The results are shown in Table 1. Firstly, we investigated the form of textual input to GPT-4. For the fine-tuned model, instructing LLaVA-Med to generate the answer directly and sending it to GPT-4 yields a better recall improvement of 15.9% compared to generating open-ended descriptions and forwarding them to GPT-4. This suggests that leveraging the explicit answer generation capability of fine-tuned LLaVA-Med enhances the effectiveness of GPT-4 in textual reasoning tasks. However, for not fine-tuned\nmodel, generating descriptions enhance the recall by 6.8% compared to generating answers directly. This indicates that, in the absence of fine-tuning, providing descriptions as input to GPT-4 may be more effective in facilitating textual reasoning tasks. Secondly, we explored the utilization of GPT-4 for textual reasoning as opposed to directly concatenating the four answers generated by LLaVA-Med. Our results demonstrate that employing GPT-4 for textual reasoning results in a higher recall improvement compared to directly concatenating the answers from different patches for both fine-tuned and not fine-tuned settings. This highlights the superiority of GPT-4's capabilities for textual reasoning tasks using in-context learning, enhancing the performance in open-ended pathology VQA."}, {"title": "4.4. Effect of different number of patches", "content": "In Table 4, we evaluate the impact of employing different numbers of patches. Our findings suggest that incorporating more patches yields improved performance, indicating that employing a greater number of patches is beneficial."}, {"title": "5. Conclusion", "content": "Open-ended Pathology VQA is a challenging task that requires an understanding of intricate domain knowledge in pathology. Unlike prior works treating this task as simple VQA, our work for the first time argues to inject domain knowledge as guidance using HistoCartography to select the relevant and\ninformation-rich patches from pathology images. Our experiments and ablation studies show consistent performance improvement of this retrieval augmentation across multiple design choices. Our future work includes improving PATH-RAG's current patch selection strategies by considering their relevance with open-ended questions. We also aim to explore the performance benefits of Path-RAG for other datasets using vision-language models (GPT4-V OpenAI and et al. (2024), Qwen2-V1 Wang et al. (2024), LLaVA-Med++ Xie et al. (2024)etc) to capture complex features of the pathology images rather than GPT-4, a text-only model. For a more comprehensive evaluation, we plan to reference recent advancements Sun et al. (2023) that utilize the GPT-4 model as an oracle to analyze and rate responses across multiple aspects."}, {"title": "Appendix A. Path-RAG Prompts", "content": "Prompts\nGenerate Answer from LLaVa-Med:\n(question) + (image)\nGet Response from GPT-4: You are a professional pathologist. Please generate a comprehensive an-\nswer that integrates multiple perspectives for a\npathology image and provides a balanced conclu-\nsion based on the information provided.\n\u2022 Perspective 1: Answer from LLaVa-Med with\nfull image input\n\u2022 Perspective 2: Answer from LLaVa-Med with\nPatch 1 input\n\u2022 Perspective 3: Answer from LLaVa-Med with\nPatch 2 input\n\u2022 Perspective 4: Answer from LLaVa-Med with\nPatch 3 input\n\u2022 Question: Question\nGenerate Description from LLaVa-Med:\n\"Describe the following image in detail.\" +\n(image)\nGet Response from GPT-4: You are a professional\npathologist. Please answer the question in several\nsentences based on the description of an image\nand three important patches.\n\u2022 Description of image: Description from\nLLaVa-Med with full image input\n\u2022 Description of patch 1: Description from\nLLaVa-Med with Patch 1 input\n\u2022 Description of patch 2: Description from\nLLaVa-Med with Patch 2 input\n\u2022 Description of patch 3: Description from\nLLaVa-Med with Patch 3 input\n\u2022 Question: Question"}, {"title": "Appendix B. Path-RAG Example", "content": "Path-RAG GPT-4 Prompt and Response\nPrompt Design: You are a professional patholo-\ngist. Please generate a comprehensive answer that\nintegrates multiple perspectives for a pathology\nimage, providing a balanced conclusion.\n\u2022 Perspective 1: \"other tubules\"\n\u2022 Perspective 2: \"other molecules\"\n\u2022 Perspective 3: \"small intestine\"\n\u2022 Perspective 4: \"red cells in vessels well\nshown\"\nModel Response: In the pathology image under con-\nsideration, we see a complex interplay of various elements\nrequiring a detailed analysis from multiple perspectives to\nreach a comprehensive understanding.\nFrom the first perspective, focusing on \"other tubules,\"\nwe observe structures likely part of the nephron system\nin the kidney. These tubules, when cut in cross-section,\nreveal essential architecture for filtration and absorption,\nsuggesting a renal context.\nThe second perspective, \"other molecules,\" hints at\nthe biochemical aspect. This could involve identify-\ning molecules within the tubules or tissues, indicating\nmetabolic or pathological states.\nThe third perspective, \"small intestine,\" introduces a gas-\ntrointestinal context with villi, essential for nutrient ab-\nsorption. The structures could represent small intestine\ntissue if viewed cross-sectionally.\nThe fourth perspective, \"red cells in vessels well shown,\"\nemphasizes vascularization, highlighting oxygen and nu-\ntrient delivery within the tissue.\nIn conclusion, the cross-sections likely represent the small\nintestine with a focus on its internal structure and vascu-\nlarization. The tubules could represent intestinal glands,\nwith visible molecules indicating enzymatic activity, and\nclear red cells in vessels underscoring vascular support."}, {"title": "Appendix C. Prompt for ARCH-Open Dataset Construction", "content": "ARCH-Open Dataset Prompt\nPrompt Design: You are provided with a text de-\nscription (figure caption) of a pathology image.\nUnfortunately, you don't have access to the orig-\ninal image. Your job is to generate a total of 5\nopen-ended question/answer pairs from this figure\ncaption starting with \"What\" or \"Where\". Below\nare the requirements to generate the question/an-\nswer pairs:\n\u2022 Requirement 1: Avoid quoting or referring\nto specific facts, terms, abbreviations, dates,\nnumbers or names, as these may reveal the\nconversation is based on the text informa-\ntion, rather than image itself\n\u2022 Requirement 2: Focus on the visual aspects\nof the image that can be inferred without the\ntext information\n\u2022 Requirement 3: Do not use phrases like\n\"mentioned\", \"caption\", \"context\", \"with-\nout the image\" in the question/answer pairs.\nInstead, refer to the information as being \"in\nthe image\" or preferably don't mention any-\nthing\n\u2022 Requirement 4: Ensure that question/anwer\npairs are diverse and cover a range of visual\naspects of the image\n\u2022 Requirement 5: Answer responsibly, avoid-\ning overconfidence, and do not provide med-\nical advice or diagnostic information\nCaption: {caption}\nQuestion:\nAnswer:\nModel Response:\nQuestion: What type of cell morphology can be observed\nin the image?\nAnswer: The image likely displays spindle-shaped cells,\nwhich are a characteristic of certain types of tumors.\nQuestion: What is the visual appearance of the cells in\nthe image?\nAnswer: The cells in the image likely have a prominent\neosinophilic appearance, meaning they are stained bright\npink or red.\nQuestion: What distinctive feature might some cells in\nthe image show?\nAnswer: Some cells in the image might show prominent\nparanuclear vacuolisation, which is a clear or empty\nspace around the nucleus of the cell.\nQuestion: Where can we observe the fascicles of\neosinophilic spindle cells in the image?\nAnswer: The fascicles of eosinophilic spindle cells are\nlikely to be spread throughout the image, arranged in\nbundles or clusters.\nQuestion: What similar characteristic could be inferred\nfrom the image when compared to leiomyosarcoma?\nAnswer: Just like in images of leiomyosarcoma, we might\nsee prominent paranuclear vacuolisation in some of the\ncells in this image."}]}