{"title": "A Stack-Propagation Framework for Low-Resource Personalized Dialogue Generation", "authors": ["HAOYU SONG", "WEI-NAN ZHANG", "KAIYAN ZHANG", "TING LIU"], "abstract": "With the resurgent interest in building open-domain dialogue systems, the dialogue generation task has attracted increasing attention over the past few years. This task is usually formulated as a conditional generation problem, which aims to generate a natural and meaningful response given dialogue contexts and specific constraints, such as persona. And maintaining a consistent persona is essential for the dialogue systems to gain trust from the users. Although tremendous advancements have been brought, traditional persona-based dialogue models are typically trained by leveraging a large number of persona-dense dialogue examples. Yet, such persona-dense training data are expensive to obtain, leading to a limited scale. This work presents a novel approach to learning from limited training examples by regarding consistency understanding as a regularization of response generation. To this end, we propose a novel stack-propagation framework for learning a generation and understanding pipeline. Specifically, the framework stacks a Transformer encoder and two Transformer decoders, where the first decoder models response generation and the second serves as a regularizer and jointly models response generation and consistency understanding. The proposed framework can benefit from the stacked encoder and decoders to learn from much smaller personalized dialogue data while maintaining competitive performance. Under different low-resource settings, subjective and objective evaluations prove that the stack-propagation framework outperforms strong baselines in response quality and persona consistency and largely overcomes the shortcomings of traditional models that rely heavily on the persona-dense dialogue data.", "sections": [{"title": "1 INTRODUCTION", "content": "Developing an intelligent open-domain dialogue system that can naturally converse with humans has been the longest-running goal in Artificial Intelligence [54], where users could have conversations with the dialogue system using natural texts [39] or voice messages [48] in open domains. Recently, dialogue systems are also of growing importance in facilitating smooth interactions between humans and their information devices [12], and there emerges a series of well-known applications, such as social bot XiaoIce [40], and intelligent personal assistants Apple Siri and Amazon Alexa. At the same time, for years, academia has also paid much attention to improving natural language conversational AI [15], being instrumental to the advancement of machine intelligence.\nNatural language conversation requires the system to have a comprehensive ability to deliver and understand dialogues, among which one of the most challenging parts is how to make the dialogues more like coming from a real human being [54]. As argued and proved by previous works [20, 23, 56, 71], it is easy for users to distinguish between a real person and a dialogue system behind a conversation due to the lack of a consistent personality in the dialogue system. Unlike shallow blemishes in word usage or sentence fluency, the inconsistent personality occurs at a deeper level of dialogues, containing logical flaws. Figure 1 exemplifies the logical consistency of persona information in open-domain dialogues. The PERSONA can be defined as a composite of identity elements for a certain person, such as personal background facts and common user profiles. The two examples in Figure 1 share the same persona, which is a personal fact that \"I've a son who is in junior high\u201d. Given persona information, what a real human says will not contradict the persona during the conversation, as shown in the first example. In contrast, the second example shows an inconsistent generated response from a GPT-2 [35] model. Although the word-level predictions are made perfectly, it contradicts the given persona at the logical level due to the phrase \u201cno kids\u201d.\nBecause of the uniqueness of different persona compositions, it is full of difficulties for retrieval-based dialogue models [17, 21, 66] to select informative yet persona-consistent responses from a pre-constructed response pool. For example, a candidate response in the pool could be \"I am a software engineer in California\", which expresses the persona information of occupation and location. However, it is almost impossible to enumerate every combination of occupation and location within the candidate pool to satisfy different persona settings. Although the retrieved responses are always fluent and informative, the fixed response pool is not flexible enough to provide a detailed and consistent persona in the response. A better way to produce an informative yet consistent response is to develop end-to-end dialogue generation models [39, 47]. The generation model formulates the dialogue task as a sequence-to-sequence (Seq2Seq) [51] learning problem, where it learns an input-to-response mapping from open-domain dialogue data. With the advent of the social media era, e.g., Twitter\u00b9 and Weibo\u00b2, a massive collection of natural conversations is available on the public web, making it possible to train an end-to-end dialogue generation model. Compared with the retrieval-based model, the generation-based model is more flexible in considering additional constraints, such as the persona information, along with the input message, yielding dialogue responses with good flexibility and quality. Early dialogue generation methods mainly exploit implicit personas [20], where the personas are abstracted embeddings learned from a large dialogue corpus. Recent persona-based dialogue studies focus on the explicit persona [71, 77]. The main advantages of the implicit personas lie in that it has the potential to capture all personalized information from the training data. Some of the information may not be accurately described in explicit language, such as the preference of word usage. However, when evaluating the performance of this manner, it is often difficult to conclude accurate performance metrics about the representations of implicit personas due to the complexity of language generation. In contrast, the explicit"}, {"title": "2 PRELIMINARIES", "content": "2.1 Natural Language Understanding\nNatural Language Inference. The task of natural language inference (NLI) aims to predict whether a natural language hypothesis can be inferred from a natural language premise [3]. If the premise can be inferred from the hypothesis, they are entailed. Or, if the inferred results are opposite to the hypothesis, they are contradicted. Otherwise, they are neutral. During the past few years, the NLI task has greatly advanced the research on general natural language understanding and laid a foundation for different natural language understanding tasks. Thanks to years of accumulation, there are many large-scale yet high-quality NLI datasets, such as the well-known benchmarks SNLI [3] and MNLI [59].\nDialogue Consistency Understanding. Originated from natural language inference, dialogue consistency understanding identifies logical consistency in open-domain dialogues. It determines the consistency relations between a response and its contexts, such as the dialogue history and pre-defined persona. Typically, there are three relations: entailed, neutral, and contradicted. For instance, in Figure 1 (a), the consistency relation between the persona and response is entailed, while in (b), the response is contradicted to the persona. To better identify the logical consistency in open-domain dialogues, Welleck et al. [57] first construct a dialogue NLI dataset from PersonaChat, where the persona is annotated with a (e1, r, e2) triple, e.g., (i, have_pet, dog). And the contradicting candidate is another utterance that associates with a specified contradicting triple, e.g., (i, not_have, dog). Song et al. [44] contribute a KvPI dataset for dialogue consistency identification, which annotates the consistency relation between pre-defined key-value profiles and dialogue response. Dziri et al. [11] first explore the feasibility of evaluating dialogue coherence using entailment techniques. Their results show that the automatic entailment metric can be used as a surrogate for human judgment. Nie et al. [30] introduce a dialogue contradiction detection (DECODE) dataset, which contains both human-human and human-bot contradictory dialogues. The contradiction detection models trained on the DECODE dataset correlate well with human judgments, showing great potential for its usage in both automatically evaluating and improving the consistency of dialogue models. Despite their contributions to the advancements of dialogue consistency understanding, these datasets are of very limited scale and very costly to extend scale due to the expensive human annotation costs. In this work, we aim to reduce the need of dialogue-annotation data for training dialogue models.\n2.2 Auto-regressive and Masked Language Models\nAuto-regressive Language Models. Auto-regressive language model is a classical language modeling method, which gives the text prefix to predict the next word. The prediction of each token attends only to the leftward context tokens. For example, given a sequence of tokens \u201ct1t2t3t4t5", "invisible\" to t3, as shown in Figure 2 (a). Due to such characteristics, it is also known as the causal language model or unidirectional language model [10]. The auto-regressive manner is naturally compatible with text generation and thus shows impressive performances on a wide range of natural language generation tasks, such as translation, summarization, and conversation. The most well-known auto-regressive language model is the GPT series [4, 34, 35].\nMasked Language Models. Masked language model (MLM) randomly masks some tokens in a sequence and then recovers the masked tokens by looking into all the remaining tokens. For example, given a token sequence \"t1 t2 [MASK]t4t5": "here the t3 is masked, all t1, t2, t4, and t5 will be leveraged to recover the t3, as shown in Figure 2 (a). Compared with the classical auto-regressive language model, MLM makes better use of available information, e.g., the t4 and t5 behind the t3, indicating it could obtain stronger representations. The MLM manner was initially proposed in the BERT [7]"}, {"title": "2.3 Stack-propagation", "content": "Stack-propagation [33, 74] is a methodology that combines two related but different tasks, where one task is usually depending on another task. For instance, parsing and part-of-speech tagging. Typically, such two tasks are modeled in a pipeline way, where specific models are only trained to complete the specific tasks. Back-propagation between tasks and models is not considered, which means the two tasks are modeled independently. There are two main disadvantages of the pipeline stacking way: 1) the errors cascade between tasks, and 2) the neglect of inner relations between tasks. Stack-propagation is thus proposed to address the two issues by setting up a continuous and differentiable connection between tasks and allowing back-propagation between different tasks' models. Figure 2 (b) illustrates the differences between pipeline stacking and stack-propagation. In pipeline stacking, task-specific signals are only back-propagated to the specific model. As a result, both models are unaware of each other's tasks. While in stack-propagation, the signals from Task B back-propagates to not only Model B but also Model A, making the model at the bottom can acquire the information from both tasks. In this work, we will show that persona consistency understanding and dialogue generation tasks can be jointly modeled in a stack-propagation framework and yield significant improvements over previous methods on persona consistency and response quality."}, {"title": "3 RELATED WORK", "content": "3.1 Traditional Dialogue Models\nWith the prosperity of social media, a vast amount of dialogue-like data has been accumulated on the Internet. Recent researches on open-domain dialogues begin to focus on end-to-end data-driven methods, most of which fall into two categories: retrieval-based and generation-based.\nRetrieval-based Dialogue Models. Retrieval-based dialogue models select a suitable response from a large pool of candidate responses according to different matching schemes [17, 61, 64, 65, 69, 80, 81]. Generally, retrieval-based dialogue models could be decomposed into two steps: 1) filter a small set of candidate responses from the large pool using fast but coarse-grained retrieval methods, e.g., BM25 [37], and 2) re-rank the filtered candidate responses with fine-grained neural networks to select the best matching response. For example, with token-level and sentence-level representations from neural models, Zhou et al. [80] aggregate all query-candidate pairs into a three-dimensional form and then leverage the convolution network to extract features for matching score prediction. The retrieved responses are always fluent and informative, but the retrieving manner suffers from flexibility since the response pool is constructed in advance. As a result, retrieval systems are difficult to return appropriate responses for the unseen inputs and are not flexible enough to be applied in conditional dialogues.\nGeneration-based Dialogue Models. Given an input message, the dialogue generation model outputs a word-by-word generated response according to the mappings learned on the large dialogue corpora [14, 22, 39, 47, 52, 56]. The mainstream of dialogue generation modeling adopts a neural encoder-decoder framework [51], where the dialogue inputs are firstly encoded into dense representations. Then the representations are decoded to the target responses. The encoder and decoder originally adopt deep LSTM or GRU, and it has been recently migrated to the Transformer. [55]. The advantages of the generation manner mainly lie in that the model can output responses unseen in the training data. There is also greater flexibility of the input format, which can introduce various conditions, such as emotion [50, 78], knowledge [9, 79], topic [62, 70], etc. Meanwhile, the generative manner also has some inherent shortcomings, such as the lack of diversity in responses [19] and usually generating safe but dull responses like \"I don't know\" [22].\nRetrieval-enhanced Dialogue Generation. As an attempt to combine the advantages of both retrieval and generation methods, the retrieval-enhanced generation way has recently attracted increasing attention [5, 6, 49, 82]. All the methods in this line of research will first retrieve several candidate responses from a pre-constructed pool, and then they differ in how to use these candidates. Zhu et al. [82] leverage these retrieved responses as positive examples in adversarial training to enhance the dialogue generation model. Cai et al. [5] regard the retrieved candidate as a kind of response skeleton, and further edits are carried out by a well-designed generation model, which will polish the retrieved skeleton into a final response. By breaking down the encode-decode process of the traditional dialogue generation model, the retrieval-enhanced dialogue generation model presents better performances in generating dialogue responses and shows the potential of the multi-stage dialogue refinement methods."}, {"title": "3.2 Persona-aware Dialogue Models", "content": "The lack of a consistent persona is a long-standing challenge faced by open-domain dialogue systems [56]. Persona-aware dialogue models are thus proposed to assign consistent personas to the dialogue systems [20, 71]. In fact, a large-scale dialogue corpus characterizes different massive speakers, leading to differences and conflicts in persona information. Li et al. [20] address the persona conflicting in the corpus by building a speaker model and a speaker-addressee model"}, {"title": "3.3 Pre-trained Dialogue Models", "content": "Along with the evolution of large-scale pre-training in natural language processing, recent open-domain dialogue models also turn to pre-training for better language modeling [2, 24, 41, 73, 77]. Zhang et al. [73] train a generative GPT model on dialogue data and show an impressive performance. Bao et al. [2] introduce latent variables into large pre-trained dialogue model to generate diversified responses. Zheng et al. [77] leverage pre-trained GPT to generate personalized responses from persona-sparse dialogue data. Recent works [24, 41] begin to fuse different dialogue skills in one model through pre-training. Due to the billions of parameters, the vast amount of pre-training data, and the delicately designed structure, these models can carry out natural and informative dialogues better than the traditional dialogue models. However, training such models is very expensive, and they cannot benefit from the general pre-trained language models, such as the widely applied BERT and GPT.\nFollowing BERT and GPT, the encoder-decoder language models, such as T5 [36] and BART [18], re-prove the efficiency of the Seq2Seq architecture [51] on generative tasks. This type of model is born with a bidirectional encoder like BERT and an auto-regressive decoder like GPT, so it can be easily fine-tuned on dialogue data to achieve good performance. However, the capabilities of such models' encoder and decoder are highly coupled, and when expanding the capabilities, such as multilingual [31] and multimodal [53], both the encoder and decoder need to be re-pretrained."}, {"title": "4 THE STACK-PROPAGATION FRAMEWORK", "content": "In this section, we present the stack-propagation EDU model, a novel Transformer-stacked framework that jointly models dialogue generation and consistency understanding for low-resource personalized dialogue generation. We first give an introduction about the settings of low-resource personalized dialogue generation (\u00a7 4.1) and how we regularize dialogue generation with consistency understanding (\u00a7 4.2). Then we present each sub-module in the framework (\u00a7 4.3) as well as the training objectives (\u00a7 4.4). After that, we discuss different initialization schemes (\u00a7 4.5) of the EDU.\n4.1 Formulation of Low-resource Personalized Dialogue Generation\nIn this work, we aim to address the limited personalized data issue in persona-based dialogues. The restrictions on the availability of personalized data are reflected in: 1) using as little crowd-sourced personalized dialogue data as possible, and 2) not using the expensive dialogue-specific consistency understanding data. Personalized dialogue data teaches the model how to generate dialogue responses and integrate persona information during training. The former is similar to the training of a language model, which can be learned from large-scale unlabeled texts rather than the personalized"}, {"title": "4.2 Regularize Dialogue Generation with Consistency Understanding", "content": "The biggest challenge brought by the lack of personalized data is that it is difficult to train dialogue generation and dialogue understanding models simultaneously. Traditionally, a persona-based dialogue generation model requires persona P and dialogue query Q to generate a response. And a consistency understanding model requires persona P, response R, and the consistency labels between P and R. Moreover, the desired dataset for joint training should satisfy the format of {P, Q, R, Label}, where both entailment-labeled and contradiction-labeled responses are needed. Since the crowd-sourcing of personalized data and the annotation of consistency understanding data are already very expensive, it is not feasible to obtain sufficient annotated data that satisfy the ideal format of {P, Q, R, Label}.\nInstead of collecting an ideal dataset, we propose to disentangle dialogue generation and consistency understanding through regularizing generation with understanding. In the proposed stack-propagation framework, we introduce the consistency understanding regularizer U to disentangle generation and understanding. U is also a decoder, but it reads persona embeddings P and the hidden states R\u2081 from the first decoder, rather than the query embedding Q. Its outputs"}, {"title": "4.3 EDU: Encoder, Decoder, and Understanding Regularizer", "content": "As aforementioned, the proposed EDU framework employs an encoder E and two decoders, including a response generation decoder D and an understanding regularizer U. During training, a negative log-likelihood loss for response generation will back-propagate from D through E to the embedding layer. For consistency understanding, an unlikelihood and reconstructive loss will back-propagate from U through D and E until the embedding layer. In this way, consistency understanding supervisions flow back to regularize the dialogue generation task and can be jointly learned with dialogue generation within the stack-propagation framework. We will detail each sub-module as follows.\n4.3.1 Input Representation. In the proposed framework, the input is a combination of persona P and dialogue query Q. For the persona, we can always linearize it into a sequence of words, no matter P is personal facts (e.g., \"I have two dogs\") or profiles (e.g., \"location: Seattle\"). A special token [s] is added between the persona and dialogue query to provide segment information for the model, and the input sequence is formatted as:\n```latex\ninput = p^{(0)}_{1}, p^{(0)}_{2}, ..., p^{(0)}_{ut}, [s], q_{1}, q_{2}, ..., q_{n}\n```\nwhere t is the number of persona texts, and ut is the number of words in the t-th persona text. Then the embedding layer will convert input into vector representations through tokenization and embedding lookup. We follow the usual practice to sum up the token embedding, type embedding, and position embedding as the final input embedding, where we set the type embedding to 0 for the persona and to 1 for the query. Note that P and Q can also get their representations independently. The resulted representations are P and Q, and we jointly note them as emb = ef, en, ..., ef, where I is the maximum length of the input.\n4.3.2 Backbone: Multi-Layer Transformer. Before going into each module in detail, we first brief our framework's backbone, i.e., the multi-layer Transformer, and unify the notation used in the following sections. Transformer encodes"}, {"title": "4.3.3 Encoder", "content": "The encoder E works like a standard Transformer encoder but formats the input tokens P and Q according to equation 1, and then the input embeddings are bidirectionally encoded to a sequence of hidden vectors, from which the downstream tasks will be performed on. In short:\n```latex\nE(P,Q) = H\n```\nMore specifically, we first get the input embeddings emb of P and Q. Then the encoder E will perform multi-head attention on the emb to transform the embeddings into a sequence of hidden vectors H, followed up is a fully connected feed-forward network (FFN). There are N identical layers in E, and for E's hidden states h\u00b9 in the i-th layer:\n```latex\nh^{i+1} = FFN(MultiHead(h^{i}, h^{i}, h^{i}))\n```\nwhere h\u00b9 = emb, and hN is the output of encoder. The MultiHead and FFN are defined in equation 4 and 5, respectively."}, {"title": "4.3.4 Response Generation Decoder", "content": "The response generation decoder D is an auto-regressive Transformer decoder. First, a cross-attention is inserted between E and D to share the context representation H encoded by E. After that, a causal mask Mcausal is applied to D to preserve the auto-regressive generation property under different initialization"}, {"title": "4.3.5 Consistency Understanding Regularizer", "content": "Similar to D, the consistency understanding regularizer U is also a N-layer Transformer decoder. Regularizer U takes persona embedding P and the dialogue response representation's approximation R* as input, and U's output is the final representation for response generation R2. Since both the masked language model and auto-regressive language model are capable of language understanding, we make no assumption about the shape of the attention masks in the EDU framework. According to the initialization language model type, we apply the model-specific mask to the understanding regularizer. In short:\n```latex\nU(P, R^{*}, M_{model-specific}) = R_{2}\n```\nwhere R*=R\u2081 when trained with personalized dialogue data, and R*=R when trained with non-dialogue inference data.\nFor the i-th layer in U, the multi-head attention is performed twice to update its hidden state h:\n```latex\nh^{i+1} = FFN(MultiHead(h^{i}, P, P))\n```\n```latex\n \\ begin{cases}  FFN(MultiHead(p^{i+1}, R_{1}, R_{1})) & \\text{for personalized dialogue data} \\\\ FFN(MultiHead(p^{i+1}, R, R)) & \\text{for non-dialogue inference data} \\end{cases}\n```\nFor the personalized dialogue data, they are in a {persona, query, response} format, and these data can flow through E, D to the U. Therefore, here R\u2081 is the hidden states of D. While for the non-dialogue inference data, they only have premises and hypotheses, where the premise and hypothesis approximate persona and response, respectively. As there is no query in the inference data, we get the representations of the approximated response through its embeddings, i.e., R. The resulted hi\u207a\u00b9 in each layer thus fuses information from both persona and response. The output of U's N-th layer is the final representation R2. Once we get R2, we can 1) get the generated response \u0154 through a linear layer with softmax or embedding dot product, and 2) penalize the approximated responses labeled as a contradiction in the inference data to obtain a consistency understanding capability."}, {"title": "4.4 Training Objectives", "content": "One of the key features that distinguish the stack-propagation from pipeline stacking is the loss back-propagation strategy, as discussed in \u00a7 2.3. The proposed stack-propagation framework jointly models dialogue generation and consistency understanding tasks and treats consistency understanding as a regularizer of dialogue generation. To this end, the consistency understanding loss first flows through the understanding regularizer U, and is further back-propagated to the decoder D and encoder E, until the embedding layer. For the dialogue generation loss, it is back-propagated from D to E and embedding layer, as the usual practice in a sequence-to-sequence model.\nSpecifically, we employ the negative log-likelihood (NLL) objective for dialogue generation and the unlikelihood (UL) objective for consistency understanding. A brief illustration of which module the objective is applied to is shown in Figure 3, and detailed descriptions will be provided in the following section.\nNegative Log-Likelihood for Response Generation. To generate dialogue responses, a dialogue model will maximize the probabilities of every target token conditioned on the hidden states, i.e., R\u2081 or R2, produced by the model. In our framework, we apply the widely used negative log-likelihood loss for response generation training. The encoder E reads the persona P and query Q and pass H into D to predict the target response R from the coarse representations R\u2081:\n```latex\nL_{D}^{LNLL} = -log(p_{\\theta}(R|P, Q)) = - \\sum_{i=1}^{R}log(p_{\\theta}(r_{i}|P, Q, R_{<i}))\n```\nwhere the \u03b8 is the learnable parameters in D, E and the embedding layer. The generation part in U is also trained by the negative log-likelihood objective in a manner like auto-encoding. The regularizer U reads persona embeddings P and raw representations R\u2081 to predict the target response R:\n```latex\nL_{U}^{LNLL} = -log(p_{\\gamma}(R|P, R_{1})) = - \\sum_{i=1}^{R}log(p_{\\gamma}(r_{i}|P, R_{1}, R_{<i}))\n```\nwhere the y is the learnable parameters in E, D, U, and the embedding layer. As e is a subset of y, i.e., \u03b8\u2208 y, the gradients from the understanding regularizer U will also flow back to the dialogue generation sub-modules.\nUnlikelihood Objective for Consistency Understanding. As an important step to alleviate the dependence on dialogue-specific consistency understanding data, we turn to leverage the abundant non-dialogue inference datasets. Practically, we collect positive data N+ from the entailment category in the non-dialogue inference dataset and collect negative data N\u207b from the contradiction category:\n```latex\nN^{+} = \\{(p^{(i)}, \\hat{R}^{(i)+})\\}, N^{-} = \\{(p^{(j)}, \\hat{R}^{(j)-})\\}\n```\nwhere P and R are premise and hypothesis from the non-dialogue inference data, respectively. Here we leverage the premise P to approximate the persona P and the hypothesis \u0154 to approximate the response R. Due to regularizer U disentangles the representations of query and response, the inference data can still be used to train the model even if there is no query in its format. The representations of P and R in the EDU framework are denoted as P and R, which can be obtained through the embedding layer. The key idea behind unlikelihood training is decreasing the model's probability of undesired tokens. For data from N\u207a, we still apply the negative log-likelihood loss:\n```latex\nL_{UL}^{+} = \\sum_{i=1}^{|R|}log(p_{\\gamma} (\\hat{r_{i}}|P, R, \\hat{R}_{<i}))\n```"}, {"title": "4.5 Framework Initialization Schemes", "content": "There are two approaches to initialize of the proposed EDU framework: 1) randomly initialize all parameters and train the model on personalized dialogue data from scratch, and 2) initialize the model parameters from pre-trained language models and then finetune on the personalized dialogue data. In the second approach, typically we can leverage auto-regressive language models, e.g., GPT, and masked language models, e.g., BERT. As discussed in \u00a7 2.2, the auto-regressive language models are naturally compatible with generation tasks, while the masked language models are better at understanding tasks. Since the proposed stack-propagation framework regularizes dialogue generation with consistency understanding, we initialize the framework with different types of language models to thoroughly investigate the framework's potential. Specifically, according to the initialization approaches of encoder E, decoder D, and regularizer U, we explore the following framework initialization schemes:\n1) BoB (BERT-over-BERT). It initializes E, D, and U with the same BERT checkpoint and randomly initializes the cross-attention between E and D. As BERT is a typical masked language model, we apply a causal mask to the D to keep the auto-regressive generation manner. We note the model initialized under this scheme as EDUBOB.\n2) GoG (GPT-over-GPT). Similar to the BoB scheme, it initializes E, D, and U with the same openAI GPT [34] checkpoint, and the cross-attention between E and D is randomly initialized. Since GPT is an auto-regressive model, it only models the left-to-right information when applied for language understanding tasks. We will see the performance differences brought by the unidirectional information in the following experiments. And we note the model initialized under this scheme as EDUGOG.\n3) BGB (BERT-GPT-BERT). This scheme initializes different sub-modules with different language models according to the module's characteristics. For E and U, both of them need a bidirectional representation to capture full semantics, and thus we leverage the BERT model to initialize them. For D, it needs an auto-regressive manner to generate responses, and thus we initialize it with the GPT model. The cross-attention between E and D is also randomly initialized as such attention does not exist in BERT and GPT. We note this scheme as EDUBGB.\n4) ToT (Transformer-over-Transformer). It randomly initializes all parameters in the three Transformer blocks (details in \u00a7 4.3) and trains them on the personalized data from scratch. We set this initialization scheme to investigate the role of language model pre-training in the stack-propagation framework. A BERT-like bidirectional mask is applied to the regularizer U in this scheme. We note this baseline scheme as EDUTOT."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct experiments to evaluate the effectiveness of the proposed EDU framework on two low-resource scenarios: 1) a persona-dense scenario, where all conversations are grounded on specific personas by crowd-sourcing data collection. We gradually reduce the number of examples by halving the training dataset to simulate a low-resource scenario. And 2) a persona-sparse scenario, where every conversation is associated with a specific profile, but only very few conversations are related to the profile. Persona-sparse is a typical scenario for the large-scale dialogue datasets collected from social media. In the following section, we first introduce the experimented datasets for two scenarios (\u00a7 5.1). Then we brief the compared methods (\u00a7 5.2), and next we discuss the evaluation metrics for both dialogue quality and persona consistency (\u00a7 5.3). After that, we present the evaluation results of the persona-dense scenario (\u00a7 5.4), and we conduct analyses to have a more thorough view of the framework's performance. We further present validation experiments under the persona-sparse scenario (\u00a7 5.5). At last, we showcase some examples to intuitively demonstrate how the EDU framework works (\u00a7 5.6).\n5.1 Datasets\nWe evaluate the performance of the stack-propagation framework on a persona-dense scenario and a persona-sparse scenario with two publicly available personalized datasets:\n\u2022 PersonaChat [71] is a well-known personalized dataset. It is collected from crowd-sourced workers and thus covers rich persona information. Each dialogue in this dataset is grounded on certain personas. We use the ConvAI2 PersonaChat [8], so the results are comparable to previous methods. Besides the full dataset experiments, we reduce the number of training examples to 1/2, 1/4, and 1/8 scales to simulate a low-resource setting.\n\u2022 PersonalDialog [76] is a large-scale persona-sparse dataset. It is collected from the popular Chinese social media Weibo. Every dialogue in the PersonalDialog is associated with users' profiles, i.e., persona, but the majority are not grounded on the persona. There are two testsets: 1) a random testset, which is identically distributed as the persona-sparse training data. 2) a biased testset, which is manually filtered to be persona-relevant."}, {"title": "5.2 Compared Methods", "content": "We compare the proposed stack-propagation framework with delicately designed non-pretrained models and strong pre-trained dialogue models. We mainly compare the stack-propagation framework with the following models:\n\u2022 Transformer. The encoder-decoder-based Transformer [55] is employed as a non-pretrained baseline for both experiments. We concat personas and dialogue queries as the model's inputs.\n\u2022 CMAML. Meta-learning has been proven to be an effective way for few-shot learning. CMAML [46] is a meta-learning-based method that learns to customize model structures to adapt from few-shot personas.\n\u2022 GDR. Short for generate-delete-rewrite, GDR [43] is a multi-stage generation method for personalized dialogues, which aims at improving persona consistency. The GDR model, along with the CMAML, is delicately designed for the persona-dense dataset, and thus we only compare with them on the PersonaChat dataset.\n\u2022 LIC. Short for lost in conversation, LIC [13] is the best performing model in the ConvAI2 PersonaChat challenge [8]. LIC is a pre-trained dialogue model with a general language model architecture, and thus we compare this model on both PersonaChat and PersonalDialog datasets. For more information about the models in ConvAI2 please refer to the competition technical report [8].\n\u2022 AttentionRouting. AttentionRouting [77] is a pre-trained model specially designed for the persona-sparse scenario, and it is also the latest model on the PersonalDialog dataset.\n\u2022 GPT-2. We also finetune a pre-trained GPT-2 model [35] for a thorough comparison on PersonaChat.\nBesides conventional methods, in section 5.4.4, we also compare the framework with fine-tuned BART [18] and T5 [36]."}, {"title": "5.3 Evaluation Metrics", "content": "The goal of personalized dialogues is to generate natural, informative, and persona-consistent dialogue responses. According to such characteristics, we focus on the evaluations of two main aspects in the generated responses: response quality and persona consistency. To comprehensively compare different models, we employ both objective automatic metrics and subjective human evaluations."}, {"title": "4.4 Training Objectives", "content": "One of the key features that distinguish the stack-propagation from pipeline stacking is the loss back-propagation strategy, as discussed in \u00a7 2.3. The proposed stack-propagation framework jointly models dialogue generation and consistency understanding tasks and treats consistency understanding as a regularizer of dialogue generation. To this end, the consistency understanding loss first flows through the understanding regularizer U, and is further back-propagated to the decoder D and encoder E, until the embedding layer. For the dialogue generation loss, it is back-propagated from D to E and embedding layer, as the usual practice in a sequence-to-sequence model.\nSpecifically, we employ the negative log-likelihood (NLL) objective for dialogue generation and the unlikelihood (UL) objective for consistency understanding. A brief illustration of which module the objective is applied to is shown in Figure 3, and detailed descriptions will be provided in the following section.\nNegative Log-Likelihood for Response Generation. To generate dialogue responses, a dialogue model will maximize the probabilities of every"}]}