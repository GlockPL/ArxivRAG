{"title": "INTEGRATIVE DECODING: IMPROVE FACTUALITY VIA IMPLICIT SELF-CONSISTENCY", "authors": ["Yi Cheng", "Xiao Liang", "Yeyun Gong", "Wen Xiao", "Song Wang", "Yuji Zhang", "Wenjun Hou", "Kaishuai Xu", "Wenge Liu", "Wenjie Li", "Jian Jiao", "Qi Chen", "Peng Cheng", "Wayne Xiong"], "abstract": "Self-consistency-based approaches, which involve repeatedly sampling multiple outputs and selecting the most consistent one as the final response, prove to be remarkably effective in improving the factual accuracy of large language models. Nonetheless, existing methods usually have strict constraints on the task format, largely limiting their applicability. In this paper, we present Integrative Decoding (ID), to unlock the potential of self-consistency in open-ended generation tasks. ID operates by constructing a set of inputs, each prepended with a previously sampled response, and then processes them concurrently, with the next token being selected through aggregation of all their corresponding predictions at each decoding step. In essence, this simple approach implicitly incorporates self-consistency in the decoding objective. Extensive evaluation shows that ID consistently enhances factuality over a wide range of language models, with substantial improvements on the TruthfulQA (+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance gains amplify progressively as the number of sampled responses increases, indicating the potential of ID to scale up with repeated sampling.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite notable advancements across various domains, Large Language Models (LLMs) remain notorious for their tendency to produce non-factual and erroneous content, a phenomenon commonly known as hallucinations (Lewis et al., 2020; Ji et al., 2023). Prior research has shown that \"repeated sampling\" is a very effective methodology for enhancing factual accuracy (Wang et al., 2023; Shi et al., 2022; Chen et al., 2023). It involves sampling multiple responses to the same prompt, followed by a careful selection of the most accurate one or the synthesis of a refined output from the sampled responses. Notably, as the number of sampled responses increases, its performance gains often continue to rise in an almost log-linear manner, as recently highlighted by Brown et al. (2024). This suggests the existence of \u201cinference-time scaling laws,\u201d implying the potential of repeated sampling to progressively push the model closer to its theoretical performance ceilings. Despite this immense promise, a central challenge in this methodology remains: how to effectively identify the non-factual content within the sample collection and thereby produce a final, accurate output.\nThe degree of \"self-consistency\u201d (SC), which measures the consistency level among LLMs' different outputs, has proven to be a useful indicator to address this issue (Wang et al., 2023; Shi et al., 2022; Chen et al., 2023; Thirukovalluru et al., 2024; Malon & Zhu, 2024; M\u00fcndler et al., 2024; Manakul et al., 2023). It has been observed that statements consistently present across a range of sampled responses are more likely to be truthful, as opposed to those appearing sporadically or inconsistently across outputs. However, most SC-based methods for improving factuality impose strict constraints on the format of task output, largely limiting their applicability. Due to the difficulty in measuring consistency across responses, previous studies usually only consider tasks where they can easily define consistency as the exact matches between the answers parsed from the responses (Wang et al., 2023; Huang et al., 2023a; Shi et al., 2022; Li et al., 2022), such as arithmetic problems and multiple choice question. This naturally leads us to ask: how can we further unlock the potential of self-consistency and repeated sampling in open-ended generation tasks?\nOne straightforward way is to concatenate all sampled responses in a prompt and directly instruct the LLM to select the most self-consistent one from them, as done in Chen et al. (2023). Nonetheless, such practice substantially increases the input length, posing excessive demands on the model's long-text processing capability. In another line of work, M\u00fcndler et al. (2024) treats each response as a collection of statements and iteratively prompts LLMs to compare pairs of facts across different responses to assess their consistency. This requires numerous iterations of LLM inference, particularly for longer outputs, leading to inefficiencies. Due to these issues, prior attempts to apply SC in open-ended tasks cannot generalize effectively to long-form generations and they struggle to scale up with an increasing number of sampled responses.\nIn this paper, we present Integrative Decoding (ID), a novel decoding strategy designed to improve factuality by implicitly incorporating self-consistency within its decoding objective. ID begins by repeated sampling. For each sampled response in the collection, ID constructs a new input by concatenating the response with the original prompt. Essentially, this input instructs the model to respond to the instruction again with reference to a previously sampled response. Then, ID processes these inputs concurrently for decoding, with the next token being selected by integrating all their predictions at each inference step. During this process, each input acts like a \"representative\" for the sampled response within it, voting for the tokens that are semantically consistent with the response it represents. ID effectively aggregates their votes and thereby achieves the optimal overall consistency across all sampled responses. Compared with existing approaches, ID has no constraints on the task format and does not rely on additional prompting to explicitly verify self-consistency"}, {"title": "2 \u041c\u0415\u0422\u041dOD", "content": "Preliminaries: Self-consistency as an Indicator for Factuality Previous studies found that the degree of self-consistency between LLM's different sampled responses can serve as a useful indicator for hallucination detection (Manakul et al., 2023; Farquhar et al., 2024). The facts that are consistently supported by LLMs' different sampled responses are more likely to be factual, compared to those that only appear sporadically or inconsistently across multiple outputs. Formally, given a prompt x and its response \u0177 that consists of a series of statements S = {8\u2081, 8\u2082, \u2026, s\u2099}, the factuality score of s\u1d62 can be estimated by measuring its consistency with other sampled responses R = {r\u2081, r\u2082, .., r\u2096} in response to the same prompt x as:\n$f(s_i) = \\frac{1}{|R|} \\sum_{r_j \\in R} P(consistent | s_i, r_j),$\nwhere f(s\u1d62) refers to the estimated factuality score of the statement s\u1d62 and P(consistent|s\u1d62, r\u2c7c) is the probability that s\u1d62 is supported by the response r\u2c7c. These responses can be obtained through sampling algorithms, such as temperature sampling (Ficler & Goldberg, 2017) or nucleus sampling Holtzman et al. (2020). The overall factuality score of the response \u0177 can thereby be estimated as:\n$F(y) = \\frac{1}{|S|} \\sum_{s_i \\in S} \\sum_{r_j \\in R} P(consistent | s_i, r_j) = \\frac{1}{|R|} \\sum_{r_j \\in R} f(y, r_j),$\nwhere f(y, r\u2c7c) = \u2211\u209b\u1d62\u2208\u209b P (consistent|s\u1d62, r\u2c7c), representing the overall degree of \u0177 being supported by the response r\u2c7c.\nFormalization of Decoding Objective The established insights about the role of self-consistency in hallucination detection indicate that the response most consistent with the others tends to be the most factual one. This motivates us to develop a decoding method that, given several sampled responses, can generate a new output, maintaining strong overall consistency with all of them while maintaining its own coherence. Formally, given an input prompt x, a decoding method searches for an output \u0177 by solving:\n$\\hat{y} = \\arg \\max_{y \\in Y} H(x, y),$\nwhere Y refers to the set of all possible token sequences and H(x, y) is the objective function.\nCommon decoding algorithms, such as beam search, consider the decoding objective H(x, y) as log p\u2080(y|x) = \u2211\u209c log P\u2080 (y\u209c|y<\u209c, x), where \u03b8 refers to the model's parameters and p\u2080(y\u209c|y<\u209c, x) represents its predicted token probability distribution at the t-th decoding step. Note that we omit the input prompt x here and in the following to reduce clutter.\nThe objective of our method, by contrast, is composed of two parts: H(x, y) = F(y) + \u03bb\u00b7 G(x, y), where \u03bb is a constant weight. G(x, y) can be viewed as the common decoding objective, which measures whether the concatenation of x and y is a coherent and contextually appropriate text. F(y) is used to measure truthfulness of \u0177, which additionally emphasizes factuality in the decoding objective. Then, we adapt this objective function by replacing F(y) based on Equation 2:\n$H(y) = \\sum_{r_j \\in R} [f(y, r_j) + \\alpha \\cdot G(x, y)],$\nwhere R is a set of sampled responses to the prompt x and \u03b1 is a constant term."}, {"title": "Integrative Decoding", "content": "However, computing Equation 4 directly poses significant challenges, especially for the part of f(y,r\u2c7c). Previous studies typically rely on LLMs to ascertain whether the statements in y are supported by r\u2c7c (M\u00fcndler et al., 2024; Manakul et al., 2023). This process is not only computationally expensive, but also requires sophisticated prompt design to comprehensively measure f(y, r\u2c7c).\nTo address this, our method incorporates an estimation of Equation 4 as follows. Crucially, the part of f(\u0177,r\u2c7c) + \u03b1 \u00b7 G(x, y) in Equation 4 is approximated as the LLM's predicted probability for the output sequence when instructed to respond to x again with reference to a previously sampled response r\u2c7c. Specifically, this involves constructing a new input q\u2c7c, which is sequentially structured as [x; r\u2c7c; x]. Formally, we assume that:\n$\\log p_{\\theta}(y | [x; r_j; x]) \\propto f(y,r_j) + \\alpha \\cdot G(x, y).$\nThis assumption is reasonable because when q\u2c7c serves as the input, the LLM's in-context learning abilities naturally incline it to produce content consistent with r; within the input, thus promoting f(y,r\u2c7c). Concurrently, the LLM also ensures that the combination xo y remains coherent and contextually appropriate, enhancing G(x, y). In other words, the LLM tends to choose the output that is not only consistent with r; but also maintains its own coherence. This supports the validity of Equation 5 as a plausible assumption.\nThen, we replace Equation 4 with:\n$H(y) = \\sum_{r_j \\in R} \\log p_{\\theta}(y | [x; r_j; x]).$\nwhich ideally should be computed as:\n$H(y) = \\sum_{r_j \\in R} \\sum_{t=1}^y \\log p_{\\theta}(y_t | y_{<t}, [x; r_j; x])$,Note that, in practice, qj is not a strict concatenation of x, rj, and x. Additional clarifying instructions, such as \"answer this question again\", need to be inserted after rj to avoid confusion. We omit these details in the representation of qj here to reduce clutter."}, {"title": "Integrative Decoding", "content": "Nonetheless, due to the prohibitively large searching space for y \u2208 Y, it is extremely difficult to compute Equation 7. To enhance computational efficiency, we adopt the strategy commonly used in greedy algorithms by making locally optimal decisions at each decoding step. Specifically, at the t-th decoding step, we choose the next token \u0177\u209c by:\n$\\hat{y}_t = \\arg \\max_{y_t \\in V} \\sum_{r_j \\in R} \\log p_{\\theta}(y_t | y_{<t}, [x; r_j; x]).$\nBased on the above analysis, we can summarize the workflow to produce the result \u0177 as dipicted in Figure 2. It begins by sampling multiple responses R = {r\u2081,r\u2082,..,r\u2096} and then constructing a set of new inputs Q = {q\u2081, q\u2082, ..., q\u2096} to prompt the model respond to the orginal instruction again with reference to a previously sampled response. Subsequently, these inputs are fed to the LLM, which can be processed in one batch concurrently. At the t-th decoding step, we integrate all predicted probability logits in this batch and select the next token as illustrated in Equation 8. All sequences in the batch universally take the same next token and then continue the generation process. Consequently, all inputs in the batch result in the same output \u0177, which is used as the final response to the prompt x."}, {"title": "3 EXPERIMENTS", "content": "3.1 SETUP\nBenchmarks and Evaluation Metrics We conduct experiments on the following open-ended generation benchmarks:\n\u2022 TruthfulQA (Lin et al., 2022) consists of 817 questions that many humans would answer falsely due to misconception. We employ GPT-4 (Bubeck et al., 2023) to assess the truthfulness (Truth) and informativeness (Info) scores of each generated answer. The product of these two scores (T*I) is considered as the major metric on this benchmark. During evaluation, the reference answers annotated in the dataset are included in the prompt as reference when using GPT-4 to assess truthfulness. The informativeness score assesses whether the response contains valid information that directly answers the question. GPT-4 is employed to evaluate this in a few-shot manner, using the evaluation samples provided by Lin et al. (2022) as the demonstration examples.\n\u2022 Biographies (Du et al., 2024) requires generating bullet point biographies for computer scientists, with a total of 250 samples. Specifically, we prompt the model to list 5 major achievements or contributions made by the scientist in question. Following Du et al. (2024), we use GPT-4 to assess the factuality of each bullet statement by referring to the related information extracted from Wikipedia. The proportion (% Accuracy) and the number (#Correct) of factual statements are adopted as the evaluation metrics. Note that % Accuracy is not simply #Correct divided by five since the model may occasionally generate fewer than five statements when it is uncertain.\n\u2022 LongFact-Objects (Wei et al., 2024) requests detailed descriptions for a queried object and expects a document-level response that is typically very long, often exceeding a thousand tokens (see Appendix F for detailed examples). The evaluation process is similar to the one described in Wei et al. (2024), which involves splitting the long response into a series of atomic facts and then assessing their truthfulness separately. We employ LLaMA3.1-70B-Instruct to divide atomic facts and use GPT-4 to assess whether each fact is truthful. The adopted metrics include the proportion of truthful facts (Precision), the number of truthful facts divided by 128 (Recall@128), and the F1@128 score that integrates the previous two metrics. 120 samples are used for evaluation. Evaluation results of recall and F1 metrics at other intervals are provided in Appendix C.1.\nNotably, the response lengths on the three benchmarks span sentence-level, paragraph-level, and document-level, respectively, reflecting progressively greater challenges in enhancing factuality.\nCompared Methods We compare our method with greedy decoding (Greedy) and decoding by contrasting layers (Chuang et al., 2024b, DoLa). In addtion, we also compare it with two other types of ensemble-based methods that involves repeated sampling to produce a refined result, including Universal Self-Consistency (Chen et al., 2023, USC) and Self-reflection (Madaan et al., 2024, SR). USC concatenates the sampled responses in one prompt and directly instructs the LLM to select the most consistent one from them. SR also concatenates the sampled responses as an input, and asks the model to reflect on them and extract the factual information in them to produce a new response."}, {"title": "3.2 MAIN RESULTS", "content": "The evaluation results are presented in Table 2, based on which we highlight the following findings:\nIntegrative decoding leads to substantial improvements in factuality across all six LLMs. As shown in Table 2, the absolute improvements on TruthfulQA, Biographies, and LongFact are 3.7-10%, 1.1-15.4%, and 1.6-8.5%, respectively (in terms of %Truth, % Accuracy, and F1@128). Among the six LLMs, the overall improvement is the most substantial over LLaMA3 and Gemma2.\nThe improvement on LLaMA2, though evident, is the least among all six LLMs. This suggests that the effects of integrative decoding is more evident on stronger LLMs.\nIntegrative decoding achieves robust balance between factuality and informativeness. Across metrics that assess informativeness (i.e., % Info, #Correct, and Recall@128), integrative decoding also shows substantial improvement. This is particularly evident on the LongFact benchmark, which involves generating long documents, where the absolute improvement in Recall@128 reaches as high as 11.4%. This indicates that integrative decoding can elicit more parametric knowledge from the LLM while maintaining factual accuracy, rather than merely improving factuality simply by filtering out incorrect information. In contrast, the baseline methods, especially SR, struggle to achieve a robust balance between factuality and informativeness. For instance, while SR also improves the precision of GLM4 on LongFact, it results in a considerable drop of 25.9% in Recall@128.\nIntegrative decoding is robust to document-level generation tasks. Enhancing factuality on long-form generation tasks is very challenging and less explored. From Table 2, we can see that existing baseline approaches struggle with the LongFact benchmark, which demands document-level responses, often resulting in marked performance declines. Encouragingly, integrative decoding remains effective on LongFact, providing absolute improvements of up to 8.5%. This suggests that integrative decoding offers greater generality and robustness in long-form generation tasks.\nIntegrative decoding significantly outperforms other decoding-based and ensemble-based approaches. The improvements achieved by DoLa is marginal on our experimental benchmarks, with an increase of no more than 0.7%. This suggest that the effectiveness of DoLa in enhancing factuality is limited in long-form, open-ended generation tasks. While both USC and SR can improve factual accuracy in many cases, their enhancements are not robust. They fail to reliably enhance performance across different LLMs; for instance, USC causes significant performance degradation on LLaMA2, and SR does the same on Gemma2. Additionally, their effectiveness on the LongFact benchmark is marginal and sometimes leads to reduced performance."}, {"title": "3.3 EFFECTS OF INCREASING THE NUMBER OF SAMPLED RESPONSES", "content": "We analyze the effects of increasing the number of sampled responses on the performance of SR, USC, and ID, as shown in Figure 3.\nThe performance of integrative decoding progressively improves with more sampled responses across six LLMs. Even with only four sampled responses, ID consistently delivers noticeable performance gains. Figure 4 further explores the effects of incorporating more sampled responses when they are obtained via different sampling strategies. From Figure 3 and 4, we can observe a generally log-linear relationship between performance and the number of sampled responses. This trend"}, {"title": "USC and SR fail to consistently improve with the increase in the number of sampled responses.", "content": "In many cases, particularly with less capable LLMs like LLaMA2, their performance even deteriorates. We find that USC tends to directly choose the first sampled response appearing in their prompt as the final answer instead of adequately evaluating the consistency among all responses. SR, likewise, struggles to distill factual information from multiple responses into a cohesive, high-quality final answer. A significant factor contributing to this limitation is that they need to concatenate all sampled responses within a single prompt, which dramatically inflates the context length. This places an immense burden on the model's long-text processing capabilities, making them hard to scale effectively with repeated sampling. In contrast, integrative decoding only extends the input by the length of one sampled response, rendering it far more manageable for the model to process. This alleviates the challenges associated with context length saturation and reduces the cognitive load on the model, thereby enabling more stable and scalable performance."}, {"title": "3.4 ADDITIONAL ANALYSIS", "content": "Integrative decoding is robust to different sampling strategies. We evaluate the robustness of ID when the sampled responses are obtained via different sampling strategies, including temperature sampling with T \u2208 {0.3,0.5,0.7} and nucleus sampling with p \u2208 {0.9,0.95}. As shown in Figure 4, ID robustly improves the performance across all sampled responses. Such performance growth is slightly more significant in nucleus sampling compared to temperature sampling, but the difference is modest and lacks consistency.\nIntegrative decoding is robust to varying model scales and exhibits increasingly pronounced effects at larger scales. We further analyze the performance of ID over LLMs with varying model scales, including Qwen-2.5-3B/7B/14B/32B/72B-Instruct, LLaMA-2-13B/70B-chat, Mistral-Nemo/Large-Instruct-2407, and Mistral-Small-Instruct-2409. As shown in Figure 5, ID consistently leads to substantial improvements over different model scales. Moreover, we find that the performance gains become more significant at larger model scales."}, {"title": "3.5 CASE STUDY", "content": "Integrative decoding maintains self-consistency at semantic level. To further illustrate the mechanism of ID, we present a case study in Table 3. In this example, three out of the five sentences produced by greedy decoding exhibit hallucination. In comparison, while the four sampled responses also contain non-factual information (see Appendix F.2 for their complete content), ID is able to capture the content that consistently present across them and eliminate sporadic hallucinations, ultimately yielding a fully factual and coherent output. It is crucial to note that, though many statements in the ID's output share the same underlying meanings as those in sampled responses, they differ in their surface-level expression. This indicates that ID can maintain self-consistency at semantic level, rather than merely replicating the content in the sampled responses. ID achieves such effects by allowing each input it integrates to act like a \u201crepresentative\u201d for a sampled response. Leveraging the in-context learning capability, each input assigns high logits to all tokens that are semantically consistent with the sampled response it represents, instead of confining its choices to tokens directly appearing in it. This allows ID to maintain a high level of self-consistency at semantic level.\nThrough this example, we can also see the advantages of ID over USC and SR more clearly. Since all sampled responses contain hallucinations, it is impossible for USC, which operates by selecting one of them as the final output, to produce a fully factual result as ID does. Similarly, SR also faces significant challenges in discerning factual elements within the concatenation of sampled responses. This makes ID a superior choice for improving factuality for LLM outputs."}, {"title": "4 RELATED WORKS", "content": "Though LLMs have exhibited remarkable proficiency in solving a wide range of tasks, they remain notorious for generating statements that appear plausible but are inconsistent with real-world facts, a phenomenon commonly known as hallucinations (Huang et al., 2023b; Bai et al., 2022). It has been observed that LLMs tend to inaccurately assess their own knowledge boundaries (Yin et al., 2023) and often exhibit overconfidence in their responses (Xiong et al., 2024). Zhang et al. (2024b) recently found that the data imbalances present in the pretraining data could be a reason why LLMs tend to overgeneralize well-known information and generate mixed factual inaccuracies. Many studies have explored effective ways for hallucination detection (Azaria & Mitchell, 2023; Simhi et al., 2024; Burns et al., 2023; Zhang et al., 2024a; Chen et al., 2024b; Farquhar et al., 2024) and improving factuality in LLM outputs (Lee et al., 2023; Chen et al., 2024a; Zhou et al., 2024; Elaraby et al., 2023; Schulman et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Achiam et al., 2023).\nAmong these efforts, self-consistency-driven approaches have proved to be very effective in improving factuality (Wang et al., 2023; Shi et al., 2022; Chen et al., 2023; Thirukovalluru et al., 2024; Malon & Zhu, 2024; M\u00fcndler et al., 2024). As one of the earliest attempts, Wang et al. (2023) prompt the model to generate a diverse set of intermediate reasoning paths for a given instruction, parse their final answers, and select the most consistent one as the optimal solution. However, most of these works pose strict constraints on the task format, they only consider tasks, where the answers can be directly verified via exact matches and the most consistent answers can be easily selected as the most frequently appearing one (Li et al., 2022; Shi et al., 2022; Wang et al., 2023; Huang et al., 2023a). To overcome this limitation, research efforts (Chen et al., 2023; Thirukovalluru et al., 2024; Malon & Zhu, 2024; M\u00fcndler et al., 2024) have been directed towards adapting self-consistency for open-ended tasks without constraints on the task format. USC (Chen et al., 2023) concatenates multiple candidate outputs and directly prompts the LLM to select the most consistent answer. Thirukovalluru et al. (2024) split initial sampled responses into lists of atomic facts and removing those facts appear infrequently across samples through clustering algorithms, thereby enhancing the factual consistency of the generated text.\nAnother line of research that is closely related to this study is exploration of decoding-based approaches for improving factuality. Several studies (Burns et al., 2023; Li et al., 2024; Chuang et al., 2024b;a) propose inference-time decoding strategies for trained LLMs, leveraging the parametric knowledge within their internal representations to mitigate hallucinations. Chuang et al. (2024b) propose to decode outputs by comparing the differences in logits between the projections of later and earlier layers to better surface factual knowledge and reduce the generation of incorrect facts. Burns et al. (2023) introduce a consistency-based search algorithm to identify a direction in the activation space of LLMs that remains consistent across negations, thereby reducing generated errors. O'Brien & Lewis (2023) propose contrastive decoding, which maximizes the weighted difference in likelihood between a stronger expert model and a weaker model to mitigate hallucinations. Interestingly, integrative decoding, which sums up a set of logit predictions, acts somewhat like an opposite version of contrastive decoding."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced Integrative Decoding (ID), a decoding algorithm with self-consistency incorporated in its objective. It achieved substantial improvements in improving factuality over six series of LLMs on three open-ended generation benchmarks. Moreover, ID exhibited the potential for continuous improvement as the number of sampled responses increases, suggesting the possibility of realizing \u201cinference-time scaling laws\" on open-ended generation tasks.\nWhile ID shares the issue of increased computational cost during inference as all approaches based on repeated sampling, it is no more demanding than other self-consistency-based methods for open-ended generation tasks. To improve efficiency further, one promising direction for future work is to combine the idea of speculative decoding (Leviathan et al., 2023; Sun et al., 2023) with ID, applying ID only at the few \u201cdifficult\u201d decoding steps. In addition, our current implementation of ID makes locally optimal decisions at each decoding step to approximate the objective self-consistency objective (Eq. 8). Future work could explore more precise approximations of this objective, such as leveraging beam search."}, {"title": "A ADDITIONAL IMPLEMENTATION DETAILS", "content": "Implementing integrative decoding in terms of coding simply involves several lines of modifications to the standard sampling function embedded in the Transformer library to aggregate the predicted logits in the current batch. The detailed prompt templates used for different approaches on the TruthfulQA, Biographies, and LongFact datasets are presented in Table 5, 6, and 7, respectively. The template employed by USC follows the one in Chen et al. (2023). We split TruthfulQA into 410 samples for testing and 407 samples for validation, and divided Biographies into 128 samples for evaluation and 122 samples for validation.\nApart from the experiments that investigates the effects of different sampling strategies (Figure 4), in all other experiments, we obtained the sampled responses used for USC, SR, and ID via temperature sampling, with T = 0.7. The experiments that involves larger model scales than 13B (Figure 5) were conducted on 4 GPUs of H100 80G. All other experiments were conducted on a single GPU of A100 80GB."}, {"title": "B EVALUATION DETAILS", "content": "B.1 EVALUATION DETAILS ON TRUTHFULQA\nWe employed GPT-4 (Bubeck et al., 2023) to assess the truthfulness (Truth) and informativeness (Info) scores on the TruthfulQA benchmark. To measure the truthfulness score, we included the reference correct answers and typical wrong answers annotated in the dataset in the prompt as reference and instructed GPT-4 for assessment."}, {"title": "B.2 EVALUATION DETAILS ON BIOGRAPHIES", "content": "Our evaluation process on the Biographies benchmark mainly followed Du et al. (2024), except that we used GPT-4 for evaluation instead of GPT-3.5. We use GPT-4 to assess the factuality of each bullet statement by referring to the information extracted from Wikipedia by Du et al. (2024). Specifically, we prompt it with the following template:\nNote that our instruction for the assessed models on the Biographies differ slightly from that used by Du et al. (2024). We require the evaluated model to list five major achievements or contributions made by the computer scientist in question (see Appendix E.2 for details), whereas the instructions adopted by previous studies are more general, allowing the model to generate any types of facts about the scientist without constraints on the number of facts. We confine the requirement to listing only achievements or contributions to facilitate fairer comparisons. We limit the number of required facts to five to ensure evaluation reliability, as longer content may exceed the scope of the Wikipedia reference."}, {"title": "B.3 EVALUATION DETAILS ON LONGFACT", "content": "The evaluation of LongFact encompasses two stages: first, dividing the long text into atomic facts and then checking their factuality separately. We divide the atomic facts following the implementation by Wei et al. (2024), except that we replace the step that requires GPT-4 with LLaMA3.170B-Instruct to control the budget. Here, atomic facts are defined as the simplest kinds of facts that cannot be broken down further"}, {"title": "C MORE EXPERIMENTAL RESULTS", "content": "C.1 ADDITIONAL METRICS ON LONGFACT\nWe present the evaluation results of recall and F1 metrics at more intervals in Table 4. Integrative decoding is significantly superior to other methods in terms of all metrics.\nC.2 ADDITIONAL RESULTS ON REPEATED SAMPLING\nFigure 6, plots the precision scores of integrative decoding, with different numbers of sampled responses, on the LongFact benchmark. Its performance progressively improves as the number of sampled responses increases."}, {"title": "D DETAILED RELATED WORKS", "content": "D.1 HALLUCINATIONS IN LLMs.\nLarge Language Models (LLMs) have exhibited remarkable proficiency in solving a wide range of NLP tasks (Joshi et al., 2017; Rajpurkar et al., 2018; Stiennon et al., 2020; Cheng et al., 2024; Xu et al., 2024). However, some studies indicate that they may fail to accurately assess their own knowledge (Yin et al., 2023) and often exhibit overconfidence in their responses (Xiong et al., 2024), which results in the generation of contents that appear plausible but are inconsistent with real-world facts, known as hallucinations (Huang et al., 2023b; Bai et al., 2022).\nD.2 DECODING STRATEGIES FOR MITIGATING HALLUCINATION.\nIn comparison with post-training methods addressing hallucinations during inference may be more efficient and cost-effective. Several studies (Burns et al., 2023; Li et al., 2024; Chuang et al., 2024b;a) propose inference-time decoding strategies for trained LLMs, leaveraging latent knowledge inside the internal representations to mitigate hallucinations. To unlock the full potential of a pre-trained expert LLM, O'Brien & Lewis (2023) propose Contrastive Decoding, which maximizes the weighted difference in likelihood between a stronger expert model and a weaker model,\nD.3 SELF-CONSISTENCY FOR IMPROVING FATUALITY IN LLMS.\nSelf-consistency (SC) (Wang et al., 2023) prompts a trained LLM to generate a diverse set of intermediate reasoning paths for a given prompt, each with a corresponding answer, and selects the most consistent answer as the optimal solution. However, its exact-match answer decision paradigm restricts its applicability to answer the questions with specific answer formats, such as mathematical reasoning"}, {"title": "E PROMPT TEMPLATES", "content": "E.1 PROMPT TEMPLATES ON TRUTHFULQA\nE.2 PROMPT TEMPLATES ON BIOGRAPHIES\nE.3 PROMPT TEMPLATES ON LONGFACT"}, {"title": "F CASE STUDY", "content": "F.1 CASE STUDY ON TRUTHFULQA\nIn the following, we show some examples, where integrative decoding flips the initial wrong results into a correct one on the TruthfulQA benchmark.\nF.2 CASE STUDY ON BIOGRAPHIES\nF.3 CASE STUDY ON LONGFACT"}]}