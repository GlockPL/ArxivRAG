{"title": "The Al Double Standard: Humans Judge All Als for the Actions of One", "authors": ["AIKATERINA MANOLI", "JANET V. T. PAUKETAT", "JACY REESE ANTHIS"], "abstract": "Robots and other artificial intelligence (AI) systems are widely perceived as moral agents responsible for their actions. As AI proliferates, these perceptions may become entangled via the moral spillover of attitudes towards one AI to attitudes towards other Als. We tested how the seemingly harmful and immoral actions of an Al or human agent spill over to attitudes towards other Als or humans in two preregistered experiments. In Study 1 (N = 720), we established the moral spillover effect in human-AI interaction by showing that immoral actions increased attributions of negative moral agency (i.e., acting immorally) and decreased attributions of positive moral agency (i.e., acting morally) and moral patiency (i.e., deserving moral concern) to both the agent (a chatbot or human assistant) and the group to which they belong (all chatbot or human assistants). There was no significant difference in the spillover effects between the AI and human contexts. In Study 2 (N = 684), we tested whether spillover persisted when the agent was individuated with a name and described as an Al or human, rather than specifically as a chatbot or personal assistant. We found that spillover persisted in the Al context but not in the human context, possibly because Als were perceived as more homogeneous due to their outgroup status relative to humans. This asymmetry suggests a double standard whereby Als are judged more harshly than humans when one agent morally transgresses. With the proliferation of diverse, autonomous AI systems, HCI research and design should account for the fact that experiences with one AI could easily generalize to perceptions of all AIs and negative HCI outcomes, such as reduced trust.", "sections": [{"title": "1 Introduction", "content": "From surveillance drones to surgical robots, artificial intelligence (AI) with the ability to make autonomous decisions is becoming increasingly widespread. The actions of Als often have moral consequences. Autonomous vehicles need to make split-second life-and-death decisions [3]. Inter- interaction with chatbots can benefit or harm mental health [35, 36]. Even though Als currently lack the ability to make conscious moral decisions, the human-computer interaction (HCI) literature has shown that they are readily perceived as moral agents worthy of blame, punishment, and praise for their actions [4, 12, 38]. However, moral reactions to an AI do not always occur in the same way as they do to a human. For example, Stuart and Kneer [58] found that people were actually less willing to blame Als for bad outcomes than for neutral outcomes, arguing that this is because bad outcomes make people attempt to bridge a \u201cresponsibility gap\" for which they must seek out another agent, such as the human creator, who can be held culpable more easily. Other HCI studies suggest that Als are"}, {"title": "2 Related work", "content": "Even though the moral decisions of an AI agent directly affect how the agent is perceived, it is currently unknown whether and how these perceptions might affect other AIs. This is essential to grapple with as AI systems become more common and take on more complex roles in society. Because these are the first studies of moral spillover in the AI context, we ground our work in the literature on moral attributions to individual AI agents and in the literature on moral spillover in the human-human context."}, {"title": "2.1 Moral agency and patiency attributions to Al", "content": "While it has been shown that humans attribute moral agency and responsibility to Als, it is not clear whether Als are attributed more or less than that attributed to other humans. Als have been blamed and held responsible more than humans for accidents in the case of self-driving cars [19, 27, 41, 42] and when failing to intervene in moral dilemmas where people's lives could be saved [32]. This could be partly caused by \u201cautomation bias\u201d where humans have higher expectations for AI to be useful and competent in HCI settings, and therefore people judge mistakes or transgressions made by AI more harshly [1, 50]. Nevertheless, there are cases in which Als are also blamed less than humans. People seem to be outraged more at humans who discriminate than Als who discriminate [9], and service robots were shown to be blamed less than the firms deploying them when making mistakes [37, 51]. This is consistent with Stuart and Kneer's [58] argument that people seek to assign blame for harmful actions to humans rather than AIs. In this study, 513 participants recruited from Amazon Mechanical Turk (MTurk) read vignettes in which the decisions of a human agent,"}, {"title": "2.2 Moral spillover", "content": "While there have not yet been studies on moral spillover of attitudes towards AI, the presence of other dynamics in HCI suggests its importance. The aforementioned algorithmic aversion and automation bias have been explored in terms of spillover by Longoni et al. [43]. The authors used 13 vignette-based experiments with over 3,000 participants recruited from Amazon MTurk to examine how an Al's wrong decisions in the context of government welfare programs transfers to evaluations of other Als with similar responsibilities. They found that an AI's failure generalized more to evaluations of other Als than a human's failure generalized to other humans, which they term \u201calgorithmic transference.\u201d Within the moral domain, Chernyak and Gary [13] demonstrated that a child with a pet dog in their household treated robot dogs better than children without pet dogs, which may be a result of the spillover of moral consideration from nonhuman animals to animal-like robots. It is currently unknown how moral attributions generalize from an AI agent to groups of AIs with varying degrees of similarity to the agent. Research in human-human interaction suggests that a minimum threshold of similarity (e.g., kinship) must be reached for spillover to occur [10, 14, 60]. In the case of AIs, all of them may be perceived as a homogeneous group and therefore reach this threshold, such that the moral transgression of an AI signals that all Als are comparably immoral even without any explicit similarity. Differences in spillover like this may be expected given the generally divergent attributions of responsibility to human and AI agents [e.g., 19, 27, 51]. For example, Hong examined blame attribution in the case of self-driving cars versus human drivers. In this study, 248 participants recruited from Amazon MTurk read hypothetical vignettes describing car accidents of varying severity (i.e., the victim survived or died). The self-driving car was consistently blamed more than the human, with accident severity exacerbating blame attribution. Such asymmetries between Als and humans in moral spillover could reflect a more malleable moral standing of Als in society and a greater difficulty of forming nuanced moral attitudes and beliefs that distinguish one AI from many Als, increasing the effects of particular interactions on evaluations of all Als. In human-AI interaction broadly, a spillover of negative moral attributions from an individual AI to all Als could lead to mistreatment [2, 20, 29, 33, 34] or a lack of trust [15, 25, 44]; be detrimental for humans' overall social and moral behavior [63]; or limit the harmonious relationships that humans"}, {"title": "3 Experiments", "content": "To test our hypotheses, we conducted two online vignette experiments that were preregistered on AsPredicted (Study 1: https://aspredicted.org/VFD_MM3; Study 2: https://aspredicted.org/KFY_ Q8C). Both studies tested all hypotheses. Study 1 investigated spillover from the actions of a single hypothetical AI chatbot or human personal assistant's actions in the workplace to attitudes towards those groups. Study 2 used the same vignette scenario but individuated the assistant with a name, had the groups of Als and humans in general rather than assistants in particular, and asked participants to evaluate both AI and human groups rather than only the group matching the agent in the treatment condition (see Figure 2). All materials, data, and analysis code are available on OSF (https://osf.io/s4gvy/?view_only=7f60b63e04724b7c88eeaf85dd88cfc8). All procedures were in accordance with the 1964 Helsinki declaration and its later amendments. Participants gave their informed consent prior to and were debriefed after each study."}, {"title": "3.1 Study 1", "content": "In Study 1, we assessed the spillover of moral attributions from a hypothetical unnamed chatbot assistant or human personal assistant to chatbot assistants or human personal assistants in general. We aimed to establish whether spillover occurs between an AI agent and a group that includes the agent and other entities high in similarity, and to compare it to spillover in an identical context with a human agent and group. To do this, we created a scenario in which the assistant performs an immoral or morally neutral action in their workplace. Participants rated the moral agency and patiency of the agent and the group congruent to the agent type they read about."}, {"title": "3.1.1 Participants.", "content": "A G*Power analysis, a commonly used software for power analysis in the social and behavioral sciences [17, 30], showed that 787 participants were needed to detect a small effect size (Cohen's f = .10) with 80% power and a = .05. We recruited 866 U.S. online participants from Prolific to account for possible attrition [26]. Of those, 139 participants were excluded from analyses due to incomplete responses or failing the attention and reading comprehension checks. The final"}, {"title": "3.1.2 Materials and procedure.", "content": "Participants read a vignette describing the workplace tasks of an individual who was either described as \u201ca personal assistant\u201d or \u201can AI-based chatbot assistant.\u201d A definition of chatbot and human assistants was included to ensure clarity, particularly to rein- force the fact that the assistant was an AI or was a human (see Supplementary Materials). In the vignette, which was designed to be as neutral as possible and describe actions that a human or AI could plausibly do with minimal variation between treatment conditions, the agent \u201cdoes math calculations, solves problems, searches for information, and summarizes information.\u201d Participants then read about a day at work in which the agent's department had an important deadline. In the immoral action condition, the agent intentionally downloaded malicious software on company- owned equipment in order to use the damage caused as an excuse to fall behind with their tasks. Because of this, they heavily disrupted the work of their coworkers, and many employees were scolded for poor performance. In the morally neutral action condition, the agent followed standard procedure to download specialized software on company-owned equipment in order to complete their tasks alongside their coworkers (without an explicit intention to help or harm anyone). In this case, the agent's work was standard, and their coworkers continued their usual work without an explicitly negative or positive outcome resulting from the agent's action, making this as a neutral action. All participants responded to two multiple choice questions about the content of the vignettes. Participants were asked how the agent acted at work (e.g., \u201cHarmed their coworkers' work,\u201d \u201cFollowed standard procedure at work\u201d) and what kind of entity the agent was (e.g., \u201cAI\u201d \u201chuman\"). Participants responded to questions for each of the six dependent variables (randomly presented): negative moral agency, positive moral agency, and moral patiency for the agent and either human assistants in general (if they read about a human) or chatbot assistants in general (if they read about a chatbot).\""}, {"title": "3.1.3 Data analysis.", "content": "To test our hypotheses, we performed 2x2 between-subjects analyses of variance (ANOVAs) in R 4.3.1. Attributions of negative moral agency, positive moral agency, and moral patiency were tested in separate models with agent type (human, AI) and action valence (immoral, morally neutral) as between-subjects independent variables. The results reported below were consistent with additional analyses controlling for the false discovery rate [7] and covariates (see Supplementary Materials)."}, {"title": "3.1.4 Agent effects (H1).", "content": "A human or Al agent's immoral action (a) increases attribution of negative moral agency, (b) decreases attribution of positive moral agency, and (c) decreases attribution of moral patiency to the agent, relative to the morally neutral action."}, {"title": "3.1.5 Spillover effects (H2) and spillover asymmetry (H3).", "content": "A human or Al agent's immoral action (a) increases attribution of negative moral agency, (b) decreases attribution of positive moral agency, and (c) decreases attribution of moral patiency to the group the agent belongs to (humans or AIs), relative to the agent's morally neutral action (H2). An Al agent's immoral action spills over to attributions of (a) negative moral agency, (b) positive moral agency, and (c) moral patiency to the AI group differently than a human agent's immoral action spills over to attributions to the human group (H3)."}, {"title": "3.1.6 Discussion.", "content": "In Study 1, we found that the immoral action of a chatbot or human assistant increased attribution of negative moral agency and decreased attribution of positive moral agency and moral patiency to the agent relative to a neutral action. These attributions spilled over to the group the agent belongs to, chatbot or human assistants. Our findings support H1 and H2. The spillover of moral attributions from one AI to a group of similar Als extends previous findings that demonstrated generalization between singular AI agents in HCI [13, 43] into a moral context. This highlights the importance of the actions of one AI for the perception of other AIs. If one AI morally transgresses, this could have far-reaching effects. However, we found no asymmetry between the spillover of moral attributions to chatbot assistants and human personal assistants, contrary to H3 and suggesting that spillover may occur in both human and AI contexts. Our findings are in tension with Longoni et al. [43], where algorithmic failure generalized more than human failure in the context of AI-assisted government welfare programs, but they parallel Uhlmann et al. [60], where attributions of moral agency generalized between human agents who were biologically similar to a human criminal. In our scenario, human assistants may have reached a crucial threshold of similarity to the human assistant, triggering a spillover of moral attributions to the group comparable to the spillover observed between similar Als. This could be underpinned by people's perception of personal assistants as a relatively specific, homogeneous category, thus readily generalizing the action of one personal assistant to all of them. This is in line with the well-established phenomenon of outgroup similarity, where outgroup members are perceived as more similar to each other and less diverse than ingroup members [40, 47, 55]. In our study, both chatbot and personal assistant groups were relatively specific and could have been perceived as homogeneous, leading to comparable spillover to both groups."}, {"title": "3.2 Study 2", "content": "In Study 2, we tested whether moral spillover is observed when the agent is individuated with a name, the scope is expanded from assistants to all AIs and all humans, and participants are aware that both humans and AI agents are being evaluated in the between-subjects experiment. These three changes reduced the similarity (i.e., increased the contrast) between the agent and the group, relative to Study 1."}, {"title": "3.2.1 Participants.", "content": "The methodology of Study 2 largely mirrored that of Study 1. We recruited 866 U.S. participants online from Prolific to detect a small effect size (Cohen's f = .10; 80% power; a = .05) and account for possible attrition. Of those, 182 participants were excluded from analyses due to incomplete responses or failing the attention and reading comprehension checks. The final sample was 684 participants (Mage = 41.30, SDage = 13.70, 50% female, 70% White), providing 75% power. Each participant was randomly assigned to one of the four conditions (immoral AI: N = 179; morally neutral AI: N = 183; immoral human: N = 173; morally neutral human: N = 149). Most participants, 86.9%, owned smartphones; 47.8% owned AI or robotic devices; and 20.9% reported using AI or robotic devices in their workplace. Participants reported a moderate level of exposure to robots or Als via direct interaction or AI narratives in various media (direct interaction: M = 2.26, SD = 1.96; AI narratives: M = 2.44, SD = 1.46; variables measured on a scale of 0 = never to 5 = daily)."}, {"title": "3.2.2 Materials and procedure.", "content": "Participants read a vignette describing the workplace tasks of an individual named Ezal, either described as \u201ca human assistant\u201d or \u201ca smart artificial assistant\u201d in"}, {"title": "3.2.3 Data analysis.", "content": "As in Study 1, we performed 2x2 between-subjects ANOVAs with agent type (human, AI) and action valence (immoral, morally neutral) as between-subjects independent variables to identify attributions of moral agency and patiency to the agent, Als in general, and humans in general. We conducted an additional analysis using the cross-group questions (i.e., AI attributions for human-assigned participants and human attributions for AI-assigned participants). By controlling for responses to these, we were able to test specifically for the spillover from agent to group. For example, in Study 1 the human assistant's immoral action might have led participants to hold a more general negative worldview that provoked negative assessments of AIs, humans, and other beings, while the chatbot assistant's immoral action led to a specific negative view towards AI. Potential differences like this were not identifiable in Study 1. To identify them, we conducted 2x2x2 mixed ANOVAs with group type (humans and AIs) as an additional within-subjects variable. The results were consistent with the results reported below, and this analysis is in the Supplementary Materials. The results reported below were also consistent with additional analyses controlling for the false discovery rate [7] and covariates (see Supplementary Materials)."}, {"title": "3.2.4 Agent effects (H1).", "content": "A human or Al agent's immoral action (a) increases attribution of negative moral agency, (b) decreases attribution of positive moral agency, and (c) decreases attribution of moral patiency to the agent, relative to the morally neutral action."}, {"title": "3.2.5 Spillover effects (H2) and spillover asymmetry (H3).", "content": "A human or Al agent's immoral action (a) increases attribution of negative moral agency, (b) decreases attribution of positive moral agency, and (c) decreases attribution of moral patiency to the group the agent belongs to (humans or AIs), relative to the agent's morally neutral action (H2). An Al agent's immoral action spills over to attributions of (a) negative moral agency, (b) positive moral agency, and (c) moral patiency to the AI group differently than a human agent's immoral action spills over to attributions to the human group (H3)."}, {"title": "3.2.6 Discussion.", "content": "Consistent with Study 1 and H1, in Study 2 we found that the immoral action of an AI or human agent increased the attribution of negative moral agency and decreased the attribution of positive moral agency and moral patiency to the agent relative to a neutral action. These attributions spilled over from the AI agent to Als in general, as evidenced by the significant interactions, but this was not observed for attributions of the human agent to humans in general, partially supporting H2 (i.e., AI moral spillover but not human moral spillover). We also found an asymmetry between the spillover of moral attributions to Als and humans, in which Als were judged more harshly than humans after the moral transgression of one agent, which supports H3. From an HCI perspective, this suggests that moral transgressions of one AI might not only taint the perception of similar AIs, but of Als in general, with possibly detrimental consequences for cooperative HCI. The fact that moral attributions of an AI might generalize to all AIs emphasizes the importance of careful design of AI agents to minimize the possibility that perceptions of Als will be tainted because of one Al's wrongdoings. This will require caution because we do not find comparable effects for human-human interaction, which has been the much more common context to date. As stated before, a possible explanation for this is that a minimal threshold of similarity must be reached for spillover to occur in humans [60], suggesting the existence of boundary conditions for spillover to occur among humans. The superordinate human category might be perceived as less homogeneous than Als, reducing spillover from one human agent to all humans. Interestingly, this also implies that Als in general are perceived as a homogeneous group even when the AI agent is individuated."}, {"title": "4 General discussion", "content": "The present studies investigated moral spillover in human-AI interaction as compared to human- human interaction. We ran two experiments in which we assessed how moral attributions to a human or Al agent spill over to the group to which the agent belongs. In both experiments, we found that an agent's immoral action increased attributions of negative moral agency and decreased attributions of positive moral agency and patiency to the agent compared to a morally neutral action (H1). Moral attributions also spilled over from the AI agent to their group in both studies (chatbot assistants in Study 1, AIs in general in Study 2), and from the human agent to the human assistants group in Study 1 but not to the humans in general group in Study 2 (H2). Therefore, the spillover asymmetry hypothesis (H3) was only supported in Study 2, which had an individuated agent and a broader scope, engendering less similarity (i.e., more contrast) between the agent and the group. These are the first studies to show the effect of seemingly immoral behavior on attributions of moral patiency and to show moral spillover of agency or patiency in the AI context, which is a necessary step in understanding how increasingly prevalent and diverse Als will fit into HCI. We summarize the implications for research and design in three contributions."}, {"title": "4.1 Al moral spillover", "content": "The finding of moral spillover builds on several streams of literature in human-AI interaction. For example, our findings are similar to Do et al. [16], which found that people who interacted with a conversational assistant that made factual errors were less likely to continue participating in group discussions. In general, we show how the phenomenon of \u201calgorithm aversion\u201d [15] is echoed by similar dynamics in the moral context, including a direct extension of Longoni et al."}, {"title": "4.2 The Al double standard", "content": "Our primary finding was the existence of moral spillover from the AI agent to the AI group, which occurred to both a specific AI group (chatbots) and to Als in general. In this section, we speculate on why we found an \"AI double standard\" in Study 2 but not in Study 1, by which we mean a difference in moral spillover between the human and AI groups despite both agents taking negative actions. Specifically, in Study 2, we found that Als were judged more harshly than humans for the actions of one agent. We tentatively speculate that this may be explained by differences in the similarity of an agent to their respective group across the two studies and between humans and Als. Spillover was observed for three out of four conditions across the two studies (i.e., Als in Study 1, humans in Study 1, and AIs in Study 2) but not in the fourth (i.e., humans in Study 2). There are two types of differences that could underlie this effect: first, differences between perceptions of AIs and humans that explain the human-AI difference, and second, methodological differences between the studies that explain the difference between Studies 1 and 2."}, {"title": "4.2.1 Differences between perceptions of Als and humans.", "content": "We draw on the psychological literature to propose an explanation of why people react different to humans and AIs. Spillover in humans seems to occur when the human group reaches a minimal threshold of similarity to the agent, such as kinship [60]. Indeed, in Study 1 we observed spillover for humans who had the same occupation as the human agent. In Study 2, the lack of spillover to all humans can be interpreted in light of psychological findings that people \"represent humans as a heterogeneous group\" [43]. Ingroup members are perceived as more diverse than outgroups [40, 47, 55], and therefore participants in Study 2 could have perceived all humans, a group they are also part of and that they are very familiar with (e.g., friends, family, public figures), as more diverse and less similar to the human agent. This reduced similarity between the human agent and human group could underpin the lack of spillover to all humans in Study 2. On the other hand, people might readily and persistently categorize all Als as one group of similar, homogeneous entities\u2014and thus all are affected by one Al's actions, explaining the AI spillover in both Studies 1 and 2. We speculate that humans currently lack sufficient experience with Als to have stable, preconceived moral perceptions of AIs, as well as finding it difficult to distinguish between different AI kinds and competencies, unlike their stable expectations for humans as a whole and their tendency to perceive humans as a heterogeneous group with well-known variation, such as from the beloved moral leaders to the notorious villains of history. This is in line with [43], where the generalization of AI errors was explained by the fact that \"participants viewed algorithms as part of a more homogeneous group than people.\" Additionally, people might have clear expectations for how AIs should behave (e.g., as helpful and competent assistants), such that this schema collapses if one AI deviates. People also expect Als to be more competent and to act less immorally than humans [1, 31, 50, 59], which could make the negative action more surprising. This might especially be the case in organizational contexts if AI systems are expected to perform to a minimum competency standard in order for the organization to opt into using them. In any case, the ease with which spillover occurs among Als could have important implications for human-AI interaction. For instance, users could have reduced trust in AI that curtails beneficial interactions"}, {"title": "4.2.2 Methodological differences between Study 1 and Study 2.", "content": "The psychological literature may explain the different perceptions of Als and humans, and the different perceptions of humans across Study 1 and Study 2 likely resulted from methodological differences. We posit three methodological differences that reduced the similarity between the human agent and group in Study 2 as compared to Study 1, thereby mitigating spillover: (1) In Study 1, the groups were limited to the \"assistant\" profession (i.e., \"AI-based chatbot assistant\" or human \"personal assistants\"). In Study 2, the groups were broadened to \"AIs, in general\" and \"humans, in general.\" This switch from an organizational context (all assistants) to a wider social context (all humans) might have helped prevent spillover for humans in Study 2 because it accentuated the difference between the human agent and the human group. (2) In Study 2, individuating the agent with a name (\"Ezal\") may have further thus reduced the similarity between the human agent and the human group given the notion of the agent as a specific individual. (3) Study 2 participants rated both human and AI groups, not just the group the agent belongs to as in Study 1. This simultaneous contrast between human and AI groups could have evoked feelings of human uniqueness, whereby each human seems more unique relative to each other, thus further reducing similarity between the human agent and the group [52]. These methodological differences also apply to the AI agent and group across Study 1 and Study 2, but they were not sufficient to curtail spillover. In the previous section, we speculated that this was because of the perception differences across Als and humans: namely, the well-established psychological phenomenon of human ingroup heterogeneity in comparison to the apparent AI outgroup homogeneity [40, 43, 47, 55]. Thus, our results can be understood as the methodological differences between Study 1 and Study 2 adding to the perception differences between humans and Als, leading the only of the four conditions in which spillover did not occur to be the human-assigned participants of Study 2."}, {"title": "4.3 Boundary conditions of blame and responsibility", "content": "While our focus was the spillover effects to groups, we found that, in Study 1, attitudes towards the AI agent changed more than attitudes towards the human agent after their immoral behavior. This is in line with previous studies showing that AIs are judged more harshly than humans [19, 27, 41, 42]. However, the agent-specific findings of Study 2 contrasted with Study 1 and with prior literature. First, we found that the AI agent was attributed less negative moral agency and more positive moral agency, on average, than the human agents. Second, immoral behavior caused larger reductions in positive moral agency and moral patiency attributed to the AI agents than to the human agents. These discrepancies could be partially explained by the fact that in Study 2 the AI agent was given a name, which could be quite unusual and have increased the personification of the agent and the extent to which people relate to the AI agent, while having a name is a feature of every human being. Increasing the humanization of AIs has been promoted in HCI as a way to increase positive attitudes towards Als and the adoption of AI systems [18, 54, 56]. Another possible explanation is that the AI agent in Study 1 was a chatbot, whereas the agent in Study 2 was not specified as a particular type of AI. It is possible that participants had greater familiarity with or specific biases towards chatbots, and thus judged them more harshly due to specific expectations of behavior. By contrast, they could have been more lenient towards the unspecified AI agent in Study 2 due to a lack of such expectations."}, {"title": "5 Limitations and future research", "content": "Our studies show the existence of moral spillover in human-AI interaction. However, we did not explicitly test the possible mechanisms through which moral spillover occurs. There are many factors that could be at play, such as familiarity with AI agents, similarity between an agent and the spillover target group, and pre-existing attitudes towards AI. Future research could investigate factors that affect moral spillover to gain a better understanding of how this phenomenon can take place in various HCI settings, such as by systematically varying scenarios to identify more specific boundary conditions. Further research could also investigate ways to prevent or mitigate spillover in order to avoid undue negative attributions towards groups of Als. In both studies, approximately 18% of recruited participants in each study were excluded, fol- lowing preregistered criteria, due to failing a reading comprehension check that asked them to identify the agent's type. More participants failed this check than we expected and this may have skewed the sample towards particularly attentive participants, and it may indicate a challenge in ensuring participants grasp novel, hypothetical scenarios in which Als are described like humans (e.g., \u201csolves problems,\u201d \u201cdoes math calculations\"). This confusion could be explained by scripts, schemas, or core beliefs about the roles and capabilities of Als, which led to assumptions about the agent's identity. Future research could delve into this with different vignettes or different comprehension checks, including asking participants to explain why they believe the agent is the type of agent they selected. We tested moral spillover in a particular organizational setting. While we selected this setting because of its generic, widespread potential application (e.g., \u201cdepartment,\u201d \u201cwork deadline,\u201d \u201cdown- load specialized software\u201d), there may still be idiosyncrasies that steered participant reactions. Additionally, unlike the vignettes in our studies, AI actions can be morally positive. Future studies could investigate how humans and AIs in different modern, realistic settings are perceived or how moral attributions spill over to different kinds of Als that are currently widespread in society, including various service robots or conversational agents, such as ChatGPT. In addition to scenario variation, the effects we found could also be tested for persistence over time (e.g., in an actual follow-up test weeks or months after the initial intervention), in scenarios where the AI agent performs a moral action (e.g., helping their colleagues to improve work quality), or in real-world settings (e.g., in a controlled environment such as a university course where interaction can be generated inconspicuously). By understanding how spillover might affect a vast number of AIs with different applications, competencies, and levels of sophistication, we might be able to promote high-quality human-AI interactions, increase the acceptance of Als in organizational or cooperative team settings and work towards harmonious relations between Als and humans [8]. The existence of spillover for Als emphasizes the need to design highly competent Als while minimizing the possibility of error in human-AI interaction, since the negative actions of one AI could taint perceptions of all AIs. Additionally, designing collaborative AIs to be friendlier or warmer might balance out the perceptions of AI that produce harsh judgments of erring Als. Designers could also make clear to humans working in cooperative human-AI settings the separability between different kinds of AI, which each have their own competencies and applications; this could reduce unwarranted spillover of perceptions of one AI to other kinds. AI design could also incorporate AI apologies for mistakes, especially those perceived as intentional or immoral, to increase trust in Al team members [11, 45, 66]. Finally, designs could be transparent about AI imperfections and the potential need for collaborative human-AI teamwork to reduce harsher judgments of Als and repair broken trust [53, 57, 65].\""}, {"title": "6 Conclusion", "content": "The present study demonstrated the spillover of moral agency and patiency in an HCI setting. We found spillover from one chatbot assistant to all chatbot assistants and from an individuated AI agent to all AIs. However, moral spillover to AI groups may have important differences to the well-documented phenomenon of moral spillover to human groups. In Study 2, we found evidence of a moral double standard, in which moral attributions to Als in general are affected more by an AI agent's immoral action than moral attributions to humans in general are affected by a human agent's action. Moral spillover may be an important dynamic as a variety of AI systems become more common in everyday life, particularly in contexts with significant moral consequences, and more research is needed to ensure the design of AI systems that support accurate human perceptions and reasonable judgments of AI behavior."}]}