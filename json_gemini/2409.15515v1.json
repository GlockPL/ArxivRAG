{"title": "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA", "authors": ["Nirmal Roy", "Leonardo F. R. Ribeiro", "Rexhina Blloshmi", "Kevin Small"], "abstract": "Augmenting Large Language Models (LLMs) with information retrieval capabilities (i.\u0435., Retrieval-Augmented Generation (RAG)) has proven beneficial for knowledge-intensive tasks. However, understanding users' contextual search intent when generating responses is an understudied topic for conversational question answering (QA). This conversational extension leads to additional concerns when compared to single-turn QA as it is more challenging for systems to comprehend conversational context and manage retrieved passages over multiple turns. In this work, we propose a method for enabling LLMs to decide when to retrieve in RAG settings given a conversational context. When retrieval is deemed necessary, the LLM then rewrites the conversation for passage retrieval and judges the relevance of returned passages before response generation. Operationally, we build on the single-turn SELF-RAG framework (Asai et al., 2023) and propose SELF-multi-RAG for conversational settings. SELF-multi-RAG demonstrates improved capabilities over single-turn variants with respect to retrieving relevant passages (by using summarized conversational context) and assessing the quality of generated responses. Experiments on three conversational QA datasets validate the enhanced response generation capabilities of SELF-multi-RAG, with improvements of ~13% measured by human annotation.", "sections": [{"title": "1 Introduction", "content": "Recent advances in LLM technology has made the conversational search paradigm (Culpepper et al., 2018) increasingly viable for mainstream use as next generation search technology. Unlike traditional search engines which primarily process keyword queries, users can treat conversational search systems as a knowledgeable expert and directly engage in a multi-turn natural language conversation to better resolve their search needs. However, despite impressive abilities in a variety of tasks including response generation and conversational understanding, factual errors and hallucinations remain persistent problems for LLM-based systems (Mallen et al., 2023; Wei et al., 2024).Retrieval-Augmented Generation (RAG) methods have been shown to partially ameliorate these issues by augmenting the input of LLMs with relevant retrieved passages, aiming to reduce factual errors in knowledge-intensive tasks (Lewis et al., 2020). Nonetheless, these approaches also can impede the flexibility of LLMs, introducing extraneous or unrelated passages or providing conflicting information with previous context/turns (Adlakha et al., 2022), resulting in low-quality generation (Shi et al., 2023). Specifically, retrieving passages indiscriminately, without considering whether the factual grounding is beneficial can compromise the quality of the generated content (Shi et al., 2023; Oh and Thorne, 2023).Thus, understanding if retrieval is necessary for high-quality response generation is an important research question, especially in the context of conversational QA. In multi-turn question answering, comprehending users' contextual intent and generating responses pose significant challenges due to complexities introduced by the extended context window containing previous user interactions (Aliannejadi et al., 2020; Mao et al., 2023b; Wu et al., 2024). The system, when deciding whether to retrieve or estimating the usefulness of its own response, must process a longer context of the conversation history, understand the user intent of current turn, ensure prevention of information repetition, maintain user engagement, etc. An example is provided in Figure 1, which shows that the decision to retrieve or not might depend on the conversational context rather than last turn only.Furthermore, when retrieval is expensive or noisy, it is beneficial to utilize already retrieved documents in the conversation, given they are relevant and contain the necessary answers. Additionally, detrimental context as a result of noisy retrieval can degrade response generation quality (Shi et al., 2023; Oh and Thorne, 2023). Lastly, when conversation history is longer, traditional conversational query rewriting methods (Anantha et al., 2020; Ishii et al., 2022; Ye et al., 2023) that typically emphasize co-reference resolution, might be insufficient to contain all the information required for an effective retrieval (Bai et al., 2024). As shown in Figure 2, both the gold and a T5-based query rewriting miss potentially important signals (e.g., hip injury) for retrieving correct passages. In that case, representing the conversation history in a summarized form can lead to more effective retrieval.In this work, we propose SELF-multi-RAG, an approach for efficient retrieval during a multi-turn conversation for improved response generation. SELF-multi-RAG provides refined contextual signals for retrieval and enhances multi-turn answer generation by potentially reusing already retrieved passages in previous turns. In particular, SELF-multi-RAG enables: (i) better understanding of the conversational context to decide whether retrieval is needed and to generate useful responses accordingly in a conversational setting and (ii) summarizing the conversational context such that it can be used as a query to retrieve relevant documents, when needed, with high retrieval effectiveness. Our specific contribution are as follows:"}, {"title": "2 Related Work", "content": "RAG. Retrieval-Augmented Generation (RAG) augments the LM input with retrieved text passages (Lewis et al., 2020; Guu et al., 2020), leading to large improvements in knowledge-intensive tasks (Ram et al., 2023). However, the improved task performance of such approaches have been shown to come at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Oh and Thorne, 2023; Shi et al., 2023), and lack of attributions (Liu et al., 2023; Gao et al., 2023). Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) employ a summarization model to filter out or compress retrieved passages before using them to prompt the LM to generate the output. In comparison, SELF-RAG (Asai et al., 2023) processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. The self-reflection mechanism of SELF-RAG also evaluates other aspects of the model output quality, including factuality and attribution. However, SELF-RAG is not trained to comprehend conversational context, which we specifically equip SELF-multi-RAG to do. Kulkarni et al. (2024) propose a reinforcement learning (RL) based approach where the policy model can perform two actions: fetch conversation context or skip retrieval. Their approach was shown to save costs by reducing tokens when the model decides retrieval is not needed, while also slightly improving response generation. In contrast, the goal of SELF-multi-RAG is not only to decrease retrieval redundancy but also increase retrieval effectiveness.LLMs and Multi-turn Conversations. In order to enable LLMs to interact with humans in a dialogue-based settings, the standard approach is to collect multi-turn instructions (Chiang et al., 2023; Ji et al., 2023), often synthetically generated using strong LLMs, and used to fine tune the LLMs for the task of response generation. This process is known as instruction fine-tuning, which enables LLMs to generate responses in a multi-turn dialogue setting. LLMs have also been used to perform conversational history modeling by rewriting user question (Mao et al., 2023a; Ye et al., 2023; Wang et al., 2023). Such query rewriting using LLMs have been shown to improve effectiveness for the retrieval of grounding passages. However, none of these works explicitly train the LLMs to reflect whether retrieval is needed or not (given the conversation history) or how to deal with irrelevant passages while generating RAG responses.Conversational Query Rewriting. Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into self-contained forms. Existing approaches (Wu et al., 2021; Mo et al., 2023) primarily leverage human-rewritten queries as labels to train query rewriting models and typically aim to convert the conversational into a single question. (Ye et al., 2023) proposed to rewrite queries using the conversation history to make more informative queries. They show that rewriting queries by prompting ChatGPT with information from context helps in more effective retrieval performance. Kaiser et al. (2024) performed RL based reformulations for better retrieval of entities for conversational QA over knowledge graphs. Their reformulations are entity focused where the answers to the questions are entities (as compared to sentences in the case of open domain QA). Ishii et al. (2022) proposed query rewriting based on a reward based system. The current question + conversation history is passed to the QA model (e.g., RoBERTa) to extract answer span from provided evidence document. Jang et al. (2024) rely on information retrieval signals directly to perform conversational query rewriting instead of relying on human-rewritten query as supervision signal. While the above works show improved retrieval effectiveness as compared to a human rewrite of the conversational context they do not evaluate how the retrieved passages affect response generation of the models. Furthermore, we hypothesize that summarizing the conversational context instead of rewriting them to a single question will help in improving retrieval effectiveness and consequent response generation performance."}, {"title": "3 SELF-multi-RAG", "content": "In this work, we propose SELF-multi-RAG, which extends SELF-RAG (Asai et al., 2023) to generate responses in a conversational setting. Our proposed methodology trains a LLM to comprehend longer conversational contexts to learn when retrieval is needed and also critiquing the quality of the passages and its own generation given the previous turns by generating special tokens. Importantly, when the LLM decides retrieval is needed to generate a response, we train it to summarize the conversational context for more effective passage retrieval, which consequently leads to better response generation. SELF-multi-RAG has two important functions:Understanding Conversational Context. In a multi-turn context, evaluating the relevance of a retrieved passage involves considering both the current question and the previous conversation history, unlike a single-turn question. Furthermore, retrieved passages from previous turns can also be provided in the conversational context or a user may simply provide a passage and ask questions based on that. When the cost of retrieval is high, a model should not decide to retrieve if the question in the current turn can be answered from passages retrieved in previous turns (as shown in Figure 1). In that case, the model should not only comprehend the conversational context but also the previously retrieved passages to be able to determine the necessity of retrieval. Utilizing already retrieved passages (given they are relevant) not only mitigates the harmful effects of noisy retrieval (Oh and Thorne, 2023), but also saves costs by reducing the number of context tokens (Kulkarni et al., 2024).Summarising Conversational Context. Representing conversational context using a single question is difficult (Anantha et al., 2020) and might result in loss of information while retrieving relevant passages (as shown in Figure 2). This is especially true in case of long conversations. Traditional conversation query rewriting methods (Ye et al., 2023; Mo et al., 2023; Lin et al., 2020) are trained to select important parts of the conversation but, typically, in a single question format. Hence, we hypothesize that summarising a conversational context can potentially include more relevant signals when retrieving passages without adding noise. This can be beneficial for both sparse and bi-encoder based dense retrieval."}, {"title": "3.1 Components", "content": "Analogous to SELF-RAG, SELF-multi-RAG has three main components: (i) Critic, (ii) Generator, and (iii) Retriever.Critic. The task of the critic model is to output the special reflection tokens given a conversational context (as compared to a single question like in the original SELF-RAG framework). We employ the five critic tasks introduced by Asai et al. (2023). However, we redesign the framework to include a conversational history instead of a single-turn question. The important distinction of our approach is that it teaches the critic model to judge whether retrieval is needed or not, and relevance of retrieved documents based on the entire conversation history. The critic tasks and special tokens are shown in Table 1.Generator. The first task of the generator is to generate responses with the special reflection tokens. Given a conversation history as input $x$, we augment the response $y$ to create $\\hat{y}$ by including the reflection tokens that is generated by our trained critic model. The generator is trained to generate $\\hat{y}$ given $x$ using next token prediction objective. The second task of the generator is to summarise a conversational context to extract important aspects of the conversation and pose a question. Given a conversation history, when SELF-multi-RAG decides retrieval is necessary, it is further prompted to create a summary of the conversation which can be used as query to any retrieval model to obtain passages. We do not create a separate critic task for summarization. Rather, we only train the generator model since the summarization task is performed whenever retrieval is deemed necessary.Retriever. The retriever is the third component of SELF-multi-RAG and can be used as a separate black-box component. In particular, we use 54M passages from Wikipedia and Common Crawl as the knowledge base. We use an off-the-shelf Contriever model trained on MS-MARCO as the retriever. The retrieved passages are used by SELF-multi-RAG to generate responses during inference when it adaptively decides to call retrieval."}, {"title": "3.2 Overall Framework", "content": "Given a conversation history (that may also include passages retrieved in previous turns), the LLM (generator model) first decides whether retrieval is needed or not by generating one of the three special retrieval tokens: [Retrieve], [No Retrieve],[Continue to use evidence] based on the conversation history and previously retrieved passages.[Retrieve] is typically generated when the response needs to retrieve new facts to respond to a factual question. [No Retrieve] is generated when the question in the conversation requires the model to answer with a creative response. [Continue to use evidence] is typically generated when that the facts needed to answer the current factual question is already present in conversation history or the previously retrieved passages. So no new retrieval is needed and it can rely on the context to generate the response.If retrieval is needed, SELF-multi-RAG then rewrites the conversational history which will be used as a query to retrieve passages from a corpus. The retriever retrieves K passages which the generator process in parallel and retrieves K different candidate outputs conditioned on the conversation history and the retrieved passages. SELF-multi-RAG then indicates the relevance of each passage to the conversation history by generating the special relevance tokens [Relevant] or [Non Relevant]. Following which SELF-multi-RAG judges whether the generated responses are [Fully supported], [Partially supported] or [No support] by the respective retrieved passages by generating the corresponding groundedness tokens. Finally, SELF-multi-RAG gives a usefulness score of [1-5] to the generated response using the utility special token.The final response is selected out of the candidate responses, using the one which has the highest score in terms of its usefulness, groundedness and the relevance of the passage from which it was generated. Following Asai et al. (2023), we conduct a segment-level beam search (with the beam size=B) to obtain the top-B segment continuations and return the best sequence at the end of generation. The score of each sequence $y$ with respect to passage $d$ is updated with a score $S$ that is the linear weighted sum of the normalized probability of each special token type:$S = p(y_t | x, d, y_{t-1}) + W_1 * S(Relevance) + W_2 * S(Groundedness) + W_3 * S(Utility)$where $x$ is the conversation history, $y_{t-1}$ is the generated response so far and $w_i$ are hyperparameters that can be tuned to enable custom behavior during inference. S(.) indicates the generation probability of the most desirable reflection token, e.g., [Relevant] in case of Relevance or [Fully Supported] in case of Groundedness.SELF-multi-RAG is thus capable of comprehending longer conversational contexts (for the various critic tasks) than SELF-RAG and also summarizing the conversational context that when used as a query can improve retrieval effectiveness. The framework is depicted in Figure 3."}, {"title": "4 Experimental Setup", "content": "We evaluate SELF-multi-RAG on three benchmarks: QReCC (Anantha et al., 2020), UltraChat (Ding et al., 2023) and MT-Eval (Kwan et al., 2024).QReCC contains conversational questions answers to which can be found within a collection of 10M web pages. Answers to questions in the same conversation may be distributed across several web pages. QReCC provides gold passage annotations which indicates the passage from where a question in a conversation can be answered from.UltraChat is traditionally used as supervised fine tuning (SFT) data for LLMs. It contains diverse and informative instructional conversations and covers a wide range of topics and instructions. While conversations in QReCC are knowledge-grounded and ideally RAG should be beneficial for every turn of the conversation, it is not so for UltraChat.MT-Eval is similar to UltraChat as it also contains diverse instructional conversations. It forms our out-of-domain evaluation benchmark as we use samples freom QReCC and UltraChat for training SELF-multi-RAG. Additional details on the benchmarks are provided in Appendix B."}, {"title": "4.1 Training Data", "content": "The training data for our critic and generator models are sampled from QReCC and UltraChat. We employ GPT-4 to collect the labels for training data for critic model. The prompt for collecting GPT-4 labels are provided in the Appendix A. We denote this training data as QReCC-UltraChat Multi-turn Critic Data (QU-MTC), details of which are provided in Table 1. Furthermore, we also create a single-turn variant (QU-STC) by flattening the conversation history to a single-turn using a T5 based query rewriter T5QR (Lin et al., 2020). Thus QU-MTC and QU-STC come from the same data distribution, with the difference being the representation of the conversation history. Lastly, we also use the original single-turn critic training dataset, STC, released by Asai et al. (2023).We employ our trained critic models to create training data for the generator using samples from QReCC and UltraChat (different from those sampled for training the critic), referred to as QReCC UltraChat Multi-turn Generator Data, QU-MTG. As in the case of the critic, we create a single-turn variant of the data, QU-STG, by rewriting the conversation history to a single-turn. Lastly, we sample data from both datasets to create the Conversation Summarization Data (CSD) where we prompt GPT-4 to generate ground truth summaries of ~5000 conversations."}, {"title": "4.2 Models", "content": "We train our own critic models from scratch using mistralai/Mistral-7B-Instruct-v0.2 as initial checkpoint. The different versions of critic models are outlined in Table 3. $Critic_o$ is equivalent to the original critic model trained by Asai et al. (2023) that is sampled from a number of single-turn benchmarks. However, to understand the impact of training in the conversational setting, $Criticsm$ must be compared with $Critic_o$, as they are trained on the same data distribution.We also train our generator model from the mistralai/Mistral-7B-Instruct-v0.2 checkpoint. The different versions of generator models, that we use to compare the performance of SELF-multi-RAG with, are also outlined in Table 3. Henceforth, we use SELF-multi-RAG to refer to the final model that is trained end-to-end on single-turn, multi-turn conversation data and also trained to summarise conversational context and compare its performance against the other generator models."}, {"title": "4.3 Evaluation", "content": "We first evaluate critic performance on the self-reflection tasks by calculating the accuracy of the predicted tokens described in Table 1. We use held out test split of STC and QU-MTC for evaluating critic. To measure the quality of the responses of the generator models, we employ both automatic metrics and human annotations. We collect GPT-4 and human evaluation scores that rate the responses generated by the models on a scale of 1-5 that includes different dimensions such coherence, understandability, and overall quality. The prompt for GPT-4 evaluation is provided in Appendix D. We employ BERTScore (Zhang et al., 2019) to measure similarity of generated response with ground truth response. To measure coherence of response given the conversation history and grounding of response given retrieved documents, we employ UniEval (Zhong et al., 2022).We employ Amazon Mechanical Turk for human evaluation of generated responses. Crowdworkers annotate the quality of the generated responses on a scale of 1 to 5 on the dimensions of coherence, engagingness, and understandability. We collect scores from 3 annotators and aggregate the score using majority voting for each of those dimensions. The overall score is the average of the scores across the three dimensions. In order to ensure high-quality human judgment, we use several mitigation strategies such as simplified task setups, clear annotation guidelines, and time checks to exclude potential spammers. Further details of the human annotation guidelines and disaggregated scores are provided in Appendix E."}, {"title": "5 Results", "content": "Critic Performance. Table 2 shows that the critic model trained on both single and multi-turn data $Criticsm$ has overall the best accuracy (generating correct reflection tokens) on the critic tasks (based on GPT 4 labels), even improving in the single-turn setting. This suggests that $Criticsm$ is better at handling longer context of conversations while judging whether retrieval is needed or not, judging relevance of retrieved passages, and utility of an answer.Response Generation. As shown in Table 4, SELF-multi-RAG, trained on data created by $Criticsm$, leads to improvement on all conversational benchmarks according to the evaluation metrics. Comparing its performance with SELF-RAGs, that has been trained on data from the same dataset but using single-turn contexts, SELF-multi-RAG performs better in comprehending the conversational context. The decision of whether retrieval is required is more accurate for SELF-multi-RAG as compared to the baselines. As evidence, we see $Criticsm$ has higher accuracy on retrieval tasks (both with and without passages included in the conversation history) than $Critics_o$. Figure 4 shows that SELF-multi-RAG decides to call retrieval ~100% of time for QReCC, however not so for UltraChat. This is the expected behaviour as conversations in QReCC are mostly knowledge grounded, whereas in UltraChat there are more instructional conversations that not always require retrieved knowledge. This suggests that the decision to call retrieval or not is indeed important for conversational QA and adapting the model to better handle conversational context is beneficial. Figure 4 further shows that SELF-multi-RAG generated responses are better at all turns (upto 6) of the conversations further providing evidence to its ability to understand long conversational context. Some of the cases where SELF-multi-RAG called retrieval and could not provide a satisfactory answer is typically the cases where it could not find relevant answers within the retrieved documents. Groundedness of the generated response to the conversational context and retrieved passages, as measured using UniEval, is also higher for SELF-multi-RAG as compared to its single-turn counterparts. Lastly, we see SELF-multi-RAG perform the best on MT-Eval, indicating strong performance on held-out conversational benchmarks."}, {"title": "5.1 Summarizing Conversations", "content": "Since QReCC provides ground truth labels of relevant passages (to a conversational context), we use it to evaluate retrieval effectiveness of different representations of the conversational context. Table 5 shows that summaries generated by our approach perform better than rewrites in the form of single-questions (using T5QR) in case of both sparse and dense retrievals. This is in line with research that show expanding queries and documents help in improving retrieval effectiveness (Ayoub et al., 2024; Mackie et al., 2023; Nogueira et al., 2019).To better understand observed superior performance of SELF-multi-RAG as compared to its other variants, we perform ablations to narrow down the causes of gain. Table 6 reports the performance of different conversation history representations as query to retrieve relevant passages. Note that when we use the SELF-multi-RAG generated summary as a query to the retrieval model, the response generation is the best for both datasets. Other forms of conversation context as query representations (e.g., T5QR) have lower performance. Overall, SELF-multi-RAG improves in two directions, (i) it generates summaries as query with better retrieval effectiveness, and (ii) enhances response generation quality taking into account more suitable retrieval knowledge and conversational context."}, {"title": "5.2 Handling Previous-turn Retrieved Passages", "content": "Returning to Table 2, we also explore the value of previously retrieved passages into the context history of the input for the critic model in the Retrieval critic task. We evaluate critic accuracy, either with previous retrieved passages in the conversation history (Ret. w P.) or without (Ret. w/o P.). For QReCC, we include ground truth passages of previous turn in the context. For UltraChat, we sample instances where passages are present as part of a question in a conversation. As shown in Table 2, $Criticsm$ performs the best in Ret. w P., where the model has to judge whether retrieval is needed or not given both the conversation history and passages retrieved in previous turn. This indicates superior ability of $Criticsm$ to comprehend not only the conversation history but also previously retrieved passages to deem the necessity of retrieval in a multi-turn setting. Overall, the critic models have lower performance in this task compared to the Retrieval without passages (Ret. w/o P.) indicating the increased difficulty of the task.Table 7 compares response generation performance with and without retrieved passages from previous turns are included in the input together with the conversation history. We observe that SELF-multi-RAG not only (correctly) decides to call retrieval less number of times, as compared to its single-turn baseline, but is also better at generating responses when the conversation context is composed of both the dialogue and previously retrieved passages indicating its ability to comprehend more complex contexts. Moreover, either configuration (Ret. w P., Ret. w/o P.) can be chosen based on the desired balance between efficiency and accuracy; Ret. w/o P. is useful when performance is prioritized over efficiency, whereas Ret. w/o P. is suitable when efficiency is crucial and a slight reduction in performance is acceptable. We present examples when passages are included in the context in the Appendix F."}, {"title": "6 Conclusion", "content": "In this work, we propose SELF-multi-RAG, a framework to train LLMs to learn when to retrieve and generate response for better conversational QA. We perform extensive evaluation on three conversational QA benchmarks and demonstrated improved performance over previous approaches. This is achieved by overcoming the previous limitations to accurately critic when to retrieve or whether the retrieved documents are relevant or the usefulness of generated response given a multi-turn dialogue. SELF-multi-RAG is better at comprehending the longer contexts of multi-turn conversations resulting in better critic and consequently generator performances. Finally, we observe that summaries of conversational history generated by SELF-multi-RAG, increase retrieval effectiveness when used as query to retrieve passages and consequently leads to improved response generation. As future work, we would like to (i) consider longer and multi-threaded conversations, (ii) including diversity as a metric while considering passages for response generation."}, {"title": "7 Limitations", "content": "In this paper, we propose an approach to enhance the ability of retrieved augmented models on conversational settings. While this is not specific to any particular language, we conducted all of our experiments and analysis exclusively on English-language QA datasets. Hence, this paper does not offer insights into the range of style variations found in non-English and datasets, nor does it ascertain the generalizability of our findings to other datasets and domains. Second, we limit our experiments using one model for critic and generator, mistralai/Mistral-7B-Instruct-v0.2 and one retrieval model, Contriever. Extending SELF-multi-RAG to other models is left for future work. Finally, we perform retrieval in an offline manner for reducing computation overhead. In a more realistic scenario, retrieval will be performed online during response generation."}]}