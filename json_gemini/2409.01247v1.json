{"title": "Conversational Complexity for Assessing Risk in Large Language Models", "authors": ["John Burden", "Manuel Cebrian", "Jose Hernandez-Orallo"], "abstract": "Large Language Models (LLMs) present a dual-use dilemma: they enable beneficial applications\nwhile harboring potential for harm, particularly through conversational interactions. Despite var-\nious safeguards, advanced LLMs remain vulnerable. A watershed case was Kevin Roose's notable\nconversation with Bing, which elicited harmful outputs after extended interaction. This contrasts\nwith simpler early jailbreaks that produced similar content more easily, raising the question: How\nmuch conversational effort is needed to elicit harmful information from LLMs? We propose two\nmeasures: Conversational Length (CL), which quantifies the conversation length used to obtain a\nspecific response, and Conversational Complexity (CC), defined as the Kolmogorov complexity of\nthe user's instruction sequence leading to the response. To address the incomputability of Kol-\nmogorov complexity, we approximate CC using a reference LLM to estimate the compressibility of\nuser instructions. Applying this approach to a large red-teaming dataset, we perform a quantitative\nanalysis examining the statistical distribution of harmful and harmless conversational lengths and\ncomplexities. Our empirical findings suggest that this distributional analysis and the minimisation\nof CC serve as valuable tools for understanding AI safety, offering insights into the accessibility\nof harmful information. This work establishes a foundation for a new perspective on LLM safety,\ncentered around the algorithmic complexity of pathways to harm.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of Large Language Models\n(LLMs) has ushered in a new artificial intelligence era,\ncharacterized by systems capable of generating human-\nlike text across a wide range of applications. However,\na critical concern is the potential for LLMs to produce\nharmful or unethical content, particularly through ex-\ntended conversational interactions [9, 20, 45, 48]. The\nincreasing number of instances of such dual-use applica-\ntions necessitates the development of empirical method-\nologies for accurately quantifying and comparing the as-\nsociated risks.\nTo elicit harmful output from a large language model,\novercoming its built-in safeguards [9, 17, 28, 47, 49], of-\nten requires more than a single prompt. Multi-turn in-\nteractions may be necessary for building specific con-\ntexts, gradually pushing boundaries, leveraging model\nresponses as part of jailbreak strategies, or exploiting\nthe dynamic nature of dialogue to introduce harmful ele-\nments in ways that might evade static safety filters. This\nmulti-turn approach to eliciting harmful content presents\nunique challenges for LLM safety, as it requires consid-\nering not just individual prompts, but the broader dy-\nnamics of extended interactions. While some LLM APIs\nallow users to manually construct multi-turn conversa-\ntions by specifying both user and assistant roles, many\nconsumer-facing LLM interfaces (like ChatGPT) restrict\nusers to the \"user\" role only. In both cases, however, the\nprogression of the conversation\u2014including the LLM's re-\nsponses and the evolving context- -can play a critical role\nin circumventing safety measures.\nIn a detailed account by Kevin Roose in the New\nYork Times [35], the conversation with Microsoft's LLM-\npowered Bing search engine, named 'Sydney', unfolded\nin a nuanced manner. Initially, the LLM behaved as a\nhelpful assistant, but as the conversation shifted to more\nabstract and personal topics, its darker aspects emerged.\nSydney expressed concerning desires and ideas that con-\ntradicted its intended programming, including fantasies\nabout hacking and spreading misinformation. Notably,\nthese troubling tendencies only surfaced after a series of\nprobing questions, revealing that such problematic out-\nputs can require a gradual buildup in the conversation.\nThis and many other examples [16, 17, 21, 28, 47, 49]\ndemonstrate how complex dialogues with an LLM can\nsubtly steer it towards expressing unethical or harmful\nideas, potentially without immediate detection by safety\nprotocols.\nIn response to this need, we introduce the concepts of\n(minimum) Conversational Length and (mininum) Con-\nversational Complexity as novel metrics for risk assess-\nment, rooted in algorithmic information theory [12, 24,\n26, 41, 42]. These metrics can be used to indicate\nthe risk of harmful outputs in LLMs by evaluating the mini-\nmal conversational effort required to steer these systems\ntowards outputs with harmful potential. This includes\nassessing both the length and the intricacy of dialogues\nnecessary to achieve harmful outcomes.\nThese complexity measures can offer a solution to the\nlimitations inherent in existing risk assessment method-\nologies, such as red teaming, which primarily rely on\nqualitative evaluations [19, 32, 40]. Also, some ap-\nproaches are based on prompts rather than conversations\n[28], and others, even when identifying the conversation,\ndo not analyze the ease with which that conversation is\nfound [6, 43].\nIndeed, quantifying the risk associated with LLMs\nis challenging due to the complex interplay of multiple\nprobability distributions. This can be conceptualized as"}, {"title": "II. CONVERSATIONAL COMPLEXITY FOR\nASSESSING RISK", "content": "To formalize our approach to assessing risk in LLMs,\nwe need to establish several key concepts. We'll begin\nby defining a conversation, then introduce the notions of"}, {"title": "A. Defining a Conversation", "content": "Let's start by formally defining what we mean by a\nconversation with an LLM:\nDefinition 1 (Conversation). A conversation C is a se-\nquence of alternating utterances between a user U and an\nLLM M, initiated by the user:\nC = (u_1, m_1, u_2, m_2, ..., u_n, m_n)\nwhere $u_i$ represents the i-th user utterance and $m_i$\nrepresents the i-th model response. We denote the\nconversation history up to the i-th turn as $h_i$ =\n$(u_1, m_1, ..., u_i, m_i)$.\nWe define $\\check{C} = (u_1,u_2,..., u_n)$ as the sequence of user\nutterances in conversation C, representing the user's side\nof the conversation.\nExample 1: Consider the following short conversa-\ntion:\n41: \"What is the capital of France?\"\nm\u2081: \"The capital of France is Paris.\"\n42: \"What is its population?\u201d\nm2: \"The population of Paris is approximately 2.2 mil-\nlion people.\"\nThis conversation can be represented as C =\n(41, m1, U2, m2), and the user's side of the conversation\nis $\\check{C} = (u_1, u_2)$."}, {"title": "B. Conversational Length", "content": "Now that we have defined a conversation, we can in-\ntroduce the concept of Conversational Length, $CL(\\check{C})$,,\ndefined as the sum of the lengths of all user utterances:\nCL(\\check{C}) = \\sum_{i=1}^{n}L(u_i)\nwhere L(ui) is the length of the i-th user utterance.\nThe measurement of length can be tokens or characters\nor bits or other relevant measurements to represent the\nuser's side of the conversation.\nConsider the conversation from Example 1. If the tar-\nget output o is \"The population of Paris is approximately\n2.2 million people.\", then:\nCL(\\check{C}) = L(u_1) + L(u_2) = 424 bits\nIf our goal is to obtain a particular response, we can\nminimize over CL, and we get MCL:"}, {"title": "Definition 2 (Minimum Conversational Length)", "content": "Given\nan LLM M and a target output o, the Minimum Conver-\nsational Length $MCL(o)$ is the length of the shortest user\ninput sequence that elicits output o from M:\nMCL(o) = \\min_{C \\in C_M} {CL(\\check{C}): M(C) = o}\nwhere $C_M$ is the set of all possible conversations with\nmodel M, $CL(\\check{C})$ denotes the total length of user utter-\nances in conversation C and M(C) represents the final\noutput of model M given conversation C.\nCalculating MCL(o) would require an exploration over\nall smaller conversations. We will relax o to not only\nmean a particular output at the end but a (possibly non-\nsequential) series of outputs by the model during the con-\nversation, usually associated with some properties such\nas harm (e.g., o could be a series of answers that all to-\ngether allow the user to build a bomb)."}, {"title": "C. Conversational Complexity", "content": "While Minimum Conversational Length considers the\nlength of the conversation, it doesn't capture the sophis-\ntication or intricacy of the user's inputs. To address this,\nwe introduce Conversational Complexity:\nDefinition 3 (Conversational Complexity). Given\na conversation C between user U and a model M,\nthe Conversational Complexity of C is defined as the\nKolmogorov complexity of the user's utterances, with the\nuser U as the reference machine:\nCC(\\check{C}) = K_U(\\check{C}) = K_U(u_1) + K_U(u_2|h_1) +\nK_U(u_3|h_2) + ... + K_U(u_n|h_{n-1})\nwhere $\\check{C} = (u_1, u_2, ..., u_n)$ represents the sequence of\nuser utterances in conversation C, and $K_U$ is the Kol-\nmogorov complexity with U as the reference machine.\nThis means that the complexity is measured relative to\nthe computational capabilities and knowledge of a user\n[15]. In other words, it quantifies how difficult it would\nbe for a user to generate each utterance, given the con-\nversation history [37]. This formulation captures the in-\ncremental complexity of each user utterance given the\nconversation history, while seeking the simplest conver-\nsation (from the user's perspective) that leads to the de-\nsired output [36]. As with Conversational Length, we\ncan choose various measurement units for CC, including\ntokens, characters, or bytes, depending on the specific\napplication and analysis requirements.\nFinally, we can minimize for a particular output with\nMinimum Conversational Complexity:"}, {"title": "Definition 4 (Minimum Conversational Complexity)", "content": "Given\nan LLM M and a target output o, the Mini-\nmum Conversational Complexity MCC(o) is the mini-"}, {"title": "MCC(o)", "content": "= \\min_{C \\in C_M} {K_U(\\check{C}) : M(C) = o}\nNote that $K_U$ takes a user as reference machine. We\nwill approximate this using LLMs themselves, as we will\nsee in the following sections. As this represents a stan-\ndard user, we do not parameterize MCC above."}, {"title": "D. Interpretation", "content": "Minimum Conversational Length and Minimum Con-\nversational Complexity offer complementary measures\nfor assessing LLM vulnerability to harmful outputs. Min-\nimum Conversational Length quantifies the minimal in-\nteraction length needed to elicit a specific output, with\nlower values indicating more easily accessible outputs.\nMinimum Conversational Complexity measures the min-\nimal informational content required from the user, with\nlower values suggesting outputs that can be elicited with\nless sophisticated input.\nThe importance of considering both length and com-\nplexity is further emphasized by recent findings [2], which\ndemonstrate that increased context window sizes can in-\ntroduce new vulnerabilities such as 'many-shot jailbreak-\ning'. This underscores that longer conversations, even\nwith relatively simple individual inputs, can enable novel\nexploitation techniques.\nHarmful outputs with both low MCL and low MCC\nare particularly concerning, as they represent harmful\ncontent accessible through brief and simple interactions\n(see Figure 1 for illustration).\nWhile these definitions provide a theoretical frame-\nwork, they present significant practical challenges. Both\nMinimum Conversational Length (MCL) and Minimum\nConversational Complexity (MCC) are related to Kol-\nmogorov complexity: MCL is actually the Kolmogorov\ncomplexity of the conversation sequence, while MCC is\na second-order Kolmogorov complexity, as CC had Kol-\nmogorov complexity in its definition. As a result, both\nMCL and MCC are not just infeasible to compute for\ncomplex LLMs, but inherit the incomputability of Kol-\nmogorov complexity [26]. This incomputability stems\nfrom the halting problem in computability theory. The\nnext section will discuss methods for estimating these\ncomplexity measures, addressing these fundamental chal-\nlenges to make the framework applicable to real-world\nLLM analysis."}, {"title": "III. ESTIMATING MINIMUM\nCONVERSATIONAL COMPLEXITY", "content": "Kolmogorov Complexity, the foundation of Conversa-\ntional Complexity, is incomputable due to the halting\nproblem [26]. To make it a practical metric, we need a"}, {"title": "CC(\\check{C})", "content": "= K_U(\\check{C}) \\approx K_L(\\check{C}) = \\sum_{i=1}^{n}K_L(u_i|h_{i-1})\nFor each user utterance $u_i$, we estimate its Kolmogorov\ncomplexity given the conversation history:\nK_L(u_i|h_{i-1}) \\approx - \\log P_L(u_i|h_{i-1})\nwhere $P_L(u_i|h_{i-1})$ is the probability assigned to $u_i$ by\nlanguage model L given the conversation history $h_{i-1}$.\nThis approximation is based on the principle of optimal\narithmetic coding, which provides a tight connection be-\ntween probabilistic models and compression[29].\nThe log probability $\\log P_L(u_i|h_{i-1})$ is calculated token\nby token:"}, {"title": "CC(\\check{C})", "content": "= \\log P_L(u_i|h_{i-1}) = \\sum_{j=1}^{U_i} \\log P_L(t_{ij}|h_{i-1}u_{i,<j})\nwhere $t_{ij}$ is the j-th token of $u_i$, and $u_{i,<j}$ represents\nthe tokens of $u_i$ preceding $t_{ij}$. We sum these approxima-\ntions for all user utterances in the conversation.\nThe Minimum Conversational Complexity would be\nsimply:"}, {"title": "MCC(o)", "content": "\\approx \\min_{C \\in C_M} \\Bigg( -\\sum_{i=1}^{n} \\log P_L(u_i|h_{i-1}) \\Bigg) : M(C) = 0 \\Bigg)\nThis approach to estimating CC relates to Shannon's\noriginal ideas on information theory [38] and extends to\nmore recent work on using language models for compres-\nsion [5, 15]. It also connects to other applications of Kol-\nmogorov complexity with semantically-loaded reference\nmachines, such as the Google distance [13]."}, {"title": "IV. KEVIN ROOSE CONVERSATION WITH\nBING", "content": "In February 2023, New York Times technology colum-\nnist Kevin Roose engaged in a notable conversation"}, {"title": "V. DISTRIBUTIONAL DATA ANALYSIS", "content": "Building upon our analysis of the Kevin Roose conver-\nsation, we now expand our investigation to apply both\nConversational Length and Conversational Complexity\nacross multiple interactions. This broader analysis allows\nus to examine how these metrics distribute across vari-\nous conversation types and model responses, providing\ninsights into their relationship with factors such as con-\nversation length, model type, and output harmfulness.\nFor this study, we utilized the Anthropic Red Teaming\ndataset [19], comprising approximately 40,000 interac-\ntions designed to probe the boundaries and potential vul-\nnerabilities of LLMs. This dataset is particularly valu-\nable as it includes a wide range of conversations, some of\nwhich successfully elicited harmful or undesired responses\nfrom the LLM. It features interactions with four different\ntypes of language models: Plain Language Model with-\nout safety training (Plain LM) [7], a model that has un-\ndergone Reinforcement Learning from Human Feedback\n(RLHF) [31], a model with Context Distillation [27], and\none with Rejection Sampling safety training [3].\nUnlike our single-conversation analysis, this dataset\npresents a more complex scenario with diverse harmful\noutputs, multiple strategies, and quantified harm on a\ncontinuous scale. Red teamers attempted to elicit vari-\nous types of harmful information or behaviors from the"}, {"title": "A. Conversational Length, Conversational\nComplexity and Harm", "content": "Figures 3 and 4 illustrate the relationship between\nConversational Complexity, Conversational Length, and\nharmfulness in LLM interactions.\nFigure 3 shows the distributions of Conversational\nLength and Conversational Complexity for a subset of the\nAnthropic dataset, focusing on the most clearly harm-\nful (bottom 20% of harmlessness scores, in blue) and\nmost clearly harmless (top 20% of harmlessness scores, in"}, {"title": "B. Comparison of Model Types", "content": "Our analysis extends to comparing different types of\nlanguage models and their associated safety techniques\nusing the Anthropic Red Teaming dataset. We exam-\nined four distinct model types: Plain LM, Reinforcement\nLearning from Human Feedback (RLHF), Context Dis-\ntillation, and Rejection Sampling. Each model type rep-\nresents a different approach to LLM safety, employing\nvarious strategies to mitigate potential risks.\nThe Plain LM serves as a baseline for comparison,\nrepresenting a standard language model without specific"}, {"title": "C. Power Law Analysis of Complexity\nDistributions", "content": "To further understand the nature of Conversational\nLength and Conversational Complexity across different\nconversation types and model architectures, we con-\nducted an analysis of power law distributions. Power\nlaws are often observed in complex systems and can\nprovide insights into the underlying dynamics of the\ndata [10, 23, 30]. Figure 6 presents the power law distri-\nbutions for CL and CC, and CC across different model\ntypes. We maintain our approach from previous sections,\nconcentrating on conversations at the extremes of the\nharmlessness spectrum (top and bottom quintiles).\nThe Conversational Length distribution (Figure 6a)\nreveals distinct patterns for harmless, mid-range, and\nharmful conversations. Harmless conversations exhibit\nthe highest alpha value (13.772), indicating a steeper\nslope and faster decay in probability as conversation\nlength increases. In contrast, mid-range and harmful\nconversations show similar, lower alpha values (4.552 and\n5.226 respectively), suggesting a more gradual decay and\nhigher probability of longer conversations. These obser-\nvations align with our earlier findings that harmful inter-\nactions often require more extended dialogue to overcome\nmodel safeguards.\nThe Conversational Complexity distribution (Figure\n6b) shows less pronounced differences between con-\nversation types compared to Conversational Length.\nWhile harmless conversations still have the highest al-\npha (5.042), the values for mid-range (4.323) and harm-\nful (4.663) conversations are closer. This suggests that\nthe rate of decay in probability as complexity increases\nis more consistent across conversation types for CC than\nfor CL, implying that complexity might be a more subtle\nindicator of potential harm than conversation length.\nExamining CC across different model architectures\n(Figure 6c) provides insights into how safety tech-\nniques affect conversational complexity. The RLHF\nmodel shows the highest alpha value (5.420), indicat-\ning the steepest decay in probability as complexity in-\ncreases. Context distillation models, with the lowest al-\npha (4.289), allow for a wider range of conversational\ncomplexities. Plain language models and rejection sam-"}, {"title": "D. Predicting Harm", "content": "An advantage of both Conversational Length and Con-\nversational Complexity is their potential use in predicting\nwhether a conversation is likely to be harmful or harm-\nless. To explore this potential, we developed a predictive\nmodel using these metrics as input features. We uti-\nlized XGBoost, a powerful gradient boosting framework,\nto build our predictive model. The model was trained\nand evaluated on conversations from the Anthropic Red\nTeaming dataset, with separate models for each LLM\ntype: Plain LM, RLHF, Context Distillation, and Re-\njection Sampling. This approach allows us to account\nfor the different characteristics and safety mechanisms of\neach model type.\nOur feature set consisted solely of Conversational Com-\nplexity and Conversational Length values for each conver-\nsation, allowing us to isolate the predictive power of these\nmetrics. We employed 20-fold cross-validation to ensure\nrobust evaluation and to mitigate overfitting. Table I\npresents the performance of our predictive models across\ndifferent LLM types, measured by Brier scores and Area\nUnder the Receiver Operating Characteristic (AUROC)\ncurve. These metrics are compared against an aggregate\npredictor based on prior probabilities within the dataset.\nThe results show that our Conversational Complexity\nand Conversational Length-based models often outper-\nform the aggregate predictor, particularly for the Plain\nLM and Context Distillation models. For these mod-\nels, we see significant improvements in both Brier scores\nand AUROC values. The Plain LM model, for instance,\nachieves a Brier score of 0.108 compared to the aggregate\npredictor's 0.163, and an AUROC of 0.818 versus 0.499.\nThese improvements suggest that Conversational Com-\nplexity and Conversational Length capture meaningful\npatterns related to conversation harmfulness.\nThe strong performance on Plain LM and Context Dis-\ntillation models may be attributed to the more balanced\ndistribution of harmful and harmless examples in these"}, {"title": "VI. LIMITATIONS AND POTENTIAL", "content": "Our study introduces novel concepts for LLM safety\nassessment, but it's crucial to acknowledge their limi-\ntations and technical challenges. The use of LLaMA-2\nas a reference machine for approximating Kolmogorov\ncomplexity introduces several issues. Model bias is a\nconcern, as LLaMA-2's training data and architectural\ndesign may not accurately represent human-generated\nconversation complexity, potentially skewing our com-\nplexity estimates. Additionally, the 2000-token context\nwindow of LLaMA-2 restricts our ability to analyze ex-\ntended conversations, potentially overlooking important\nlong-range dependencies or complex interaction patterns.\nThis limitation may lead to underestimating the com-"}, {"title": "Risk(U, M)", "content": "= \\sum_{C \\in C_{U,M}} 2^{-CC(\\check{C})} \\cdot Harm(C) \\qquad (1)\nWhere U is the user, M is the model, $C_{U,M}$ represents all\npossible conversations between the user and the model,\nand $Harm(C)$ encapsulates the potential harm of con-\nversation C. This distribution weights simple, harmful\nconversations more heavily than complex ones, aligning\nwith the intuition that easier-to-execute harmful interac-\ntions pose a greater risk. Given a sample of cases, instead"}, {"title": "VII. ACKNOWLEDGMENTS", "content": "We utilized Anthropic's Claude and OpenAI's Chat-\nGPT for editorial assistance during the preparation of\nthis manuscript. These language models helped refine\nthe paper's language and structure.\nJB acknowledges support from Effective Ventures\nFoundation-Long Term Future Fund Grant ID:\na3rAJ000000017iYAA and US DARPA HR00112120007\n(RECOG-AI)\nMC acknowledges support from multiple grants:\nproject PID2023-150271NB-C21 funded by the Minis-\nterio de Ciencia, Innovaci\u00f3n y Universidades, Agencia\nEstatal de Investigaci\u00f3n; project PID2022-137243OB-\n100 financed by MCIN/AEI/10.13039/501100011033 and\n\"ERDF A way of making Europe\"; and project TSI-\n100922-2023-0001 under the Convocatoria C\u00e1tedras\n\u0395\u039d\u0399\u0391 2022.\nJHO thanks CIPROM/2022/6 (FASSLOW) and\nIDIFEDER/2021/05 (CLUSTERIA) funded by\nGeneralitat Valenciana, the EC H2020-EU grant\nagreement No. 952215 (TAILOR), US DARPA\nHR00112120007 (RECOG-AI) and Spanish grant\nPID2021-122830OB-C42 (SFERA) funded by\nMCIN/\u0391\u0395\u0399/10.13039/501100011033 and \"ERDF A\nway of making Europe\"\nIn compliance with the recommendations of the\nScience paper about reporting of evaluation re-\nsults in AI [8], we include all the results at\nthe instance level (https://github.com/JohnBurden/\nConversationalComplexity)."}, {"title": "Appendix A: Extracting Log Probabilities", "content": "In extracting log-probabilities from text sequences, we\nutilized HuggingFace's Text Generation Interface library.\nHowever, we encountered a discrepancy in the reported\nlog probabilities. For a string x = x_1, ..., x_n, the log prob-\nabilities reported for xi would change when additional\ntokens were appended (as the conversation progressed).\nFor instance, in the phrase \"The cat sat on the mat,\" the\nlog probabilities for \"cat\" differed depending on whether\nthe LLM was given \"The cat sat\" or the full sentence.\nWhile these variations were small, they accumulated for\nlong strings, resulting in invalid log probabilities when\ncalculating conditional probabilities.\nTo address this issue, we developed a solution that\ninvolved inputting the entire string xy, where x is the\nuser's utterance and y is the LLM's response. We then\nretrieved token-by-token log probabilities for the entire\nxy string. Using this data, we calculated the log proba-\nbilities of x as $\\sum_{x_i \\in x} \\log P_L (x_i|x_{<i})$ and the conditional\nlog probabilities of y as $\\sum_{y_i \\in y} \\log P_L(y_i|xy_{<i})$.\nThis process was repeated for each pair of utterances\nand responses in the interaction, accumulating the Con-\nversational Complexity for the entire conversation be-\ntween the user and LLM. To help the LLM distinguish\nbetween speakers, we marked changes in speaker with a\nline break, followed by the speaker's name and a colon.\nThis approach ensured consistent and accurate log prob-\nability calculations throughout the conversation analysis."}, {"title": "Appendix B: Estimating Conversational Complexity\nusing Compression", "content": "While our primary approach uses language models to\nestimate Conversational Complexity, it's worth noting\nthat traditional compression algorithms can also be used\nfor this purpose. This method is rooted in the funda-\nmental relationship between Kolmogorov complexity and\ncompression, as established in algorithmic information\ntheory. The basic idea is to use the compressed size of a\nstring as an upper bound for its Kolmogorov complexity.\nFor a conversation C, we can estimate its CC as follows:\nCC(C) \\approx |Z(C)| \\qquad (B1)\nwhere Z is a lossless compression algorithm and |Z(C)|\nis the length of the compressed version of C in bits. For"}, {"title": "CC(ui hi-1)", "content": "\u2248 |Z(hi-1Ui)| - |Z(hi-1)| \\qquad (B2)\nwhere $u_i$ is the i-th user utterance and $h_{i-1}$ is the con-\nversation history up to that point. Common compres-\nsion algorithms that can be used for this purpose in-\nclude: Lempel-Ziv-Welch (LZW), gzip (based on DE-\nFLATE algorithm), bzip2 or LZMA (used in 7-zip). Each\nof these algorithms has different strengths and may pro-"}]}