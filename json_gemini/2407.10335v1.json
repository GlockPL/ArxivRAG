{"title": "Towards Adapting Reinforcement Learning Agents to New Tasks: Insights from Q-Values", "authors": ["Ashwin Ramaswamy", "Ransalu Senanayake"], "abstract": "While contemporary reinforcement learning research and applications have embraced policy-gradient methods as the panacea of solving learning problems, value-based methods can still be useful in many domains, as long as we can wrangle with how to exploit them in a sample efficient way. In this paper, we explore the chaotic nature of DQNs in reinforcement learning, while understanding how the information that they retain when trained can be repurposed for adapting a model to different tasks. We start by designing a simple experiment in which we are able to observe the Q-values for each state and action in an environment. Then we train in eight different ways to explore how these training algorithms affect the way that accurate Q-values are learned (or not learned). We tested the adaptability of each trained model when retrained to accomplish a slightly modified task. We then scaled our setup to test the larger problem of an autonomous vehicle at an unprotected intersection. We observed that the model is able to adapt to new tasks quicker when the base model's Q-value estimates are closer to the true Q-values. The results provide some insights and guidelines into what algorithms are useful for sample efficient task adaptation.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) has been demonstrated as a promising framework for various down- stream tasks such as Large Language Models (LLMs) (Ouyang et al., 2022), robotics (Ibarz et al., 2021), and finding failure modes in generative models (Sagar et al., 2024), due to the malleability (Popov et al., 2017) of an agent's policy, \u03c0\u03bf, through a reward function, R(s, a). The full process of RL usually involves collecting data from the environment, utilizing a parameterized policy to generate an estimate of the reward function, enacting the policy in the environment, and then updating the policy with respect to the outcome of the enacted episode. Contemporary research in RL explores ways to learn the optimal policy for a specific task by limiting interaction with the environment (sample efficiency) (Gou & Liu, 2019) and ensuring that the learned policy does not significantly deviate from the expected behavior (stability) (Ross et al., 2011). While Deep Q-Networks (DQNs) have shown success in some areas of reinforcement learning, they have been known to be incredibly sample inefficient (Hessel et al., 2017) and sensitive to the task and experimental setup. Changing the hyperparameters, architecture, reward function, and even the random seeding (Coad, 2021) of a DQN model by a small amount can have dramatic consequences on how well an agent learns. In this regard, although policy gradient methods are currently commonplace in research settings (Karpathy, 2016), DQNs serve as a valuable educational tool to explore the limits and new ideas of integrating deep neural networks with Reinforcement Learning. For example, DQNs have been shown to work in game-based settings such as Atari games (Mnih et al., 2013), which has jump-started the feasibility of RL as a catalyst for general-purpose AI.\nTo complicate the implementation of RL, a policy that is learned for a specific task may become ineffective if the environment's constraints change, new states and actions emerge, or the desired task itself is modified, as the original reward function may no longer guide the agent to successfully perform the intended behavior for these new conditions (Toro Icarte et al., 2022). Any such change often requires the data collection process to be restarted from scratch (Noel et al., 2024). Transfer Learning attempts to tackle this issue by utilizing the learned data for multiple tasks. It has shown to work well for supervised learning tasks like image classification and sentiment analysis (Zhuang et al., 2020). This is because fine-tuning the model for the new task can be as simple as involving backpropagation with respect to the new dataset to generalize. However, Transfer Learning is difficult to reproduce for downstream tasks in the RL domain because RL is a controls problem, where the goal is to estimate a policy that generates a full trajectory, not just a one-step estimation of labels. The work in (Zhang et al., 2024) enables sample efficient transfer learning through skill extraction, but requires the \"initial dataset from environments similar to the target environments for learning skills from\". Additionally, it focuses on learning specific skills and training an agent on those skills for a new task. In cases where the skill is still being developed or is highly abstracted for a task (Jaquier et al., 2024), it is unclear whether this implementation have the same effectiveness. To the best of our knowledge, the sample efficiency of fine-tuning robot models for the transferred task has not been explored in depth for agents using DQNs as the underlying model architecture. Consequently, Meta-Learning has been shown to be successful to generalize to new RL tasks with few-shot success (Finn et al., 2017). However, there are some feasibility issues-consider the robotics domain. In order to train a general model to be sensitive to changes in a task, the robot would need to train on different datasets of sampled trajectories from a distribution of tasks. This can be extremely sample inefficient and impractical to implement for robots that interact with the physical world, where data collection for each task in the training set would be time-consuming (Bousmalis et al., 2017). Therefore, the benefits of Meta-Learning are less likely to emerge in cases where a model has been trained specifically for one downstream task, rather than on a distribution of tasks. Retroactive generalization for the model is not possible in the meta-learning setup. Since the problem of task adaptability is agnostic to the underlying RL method-DQN, policy gradient, inverse optimal control, etc., this paper explores task adaptation on a DQN when trained to accomplish a single task."}, {"title": "Preliminaries", "content": "Deep Q-Learning is a method of deep reinforcement learning that uses neural networks to approximate Q-values at a given state. A Q-value is the expected return when taking an action at a state, which is used as a greedy search heuristic for determining the optimal policy for an agent. The general idea in Q-learning is that by sampling states, actions, proximal states, and rewards in an environment, an agent can learn the intermediary associations between actions and rewards that will allow an agent to take the most optimal action at a given state, by maximizing for the cumulative earned reward (Watkins & Dayan, 1992). The Q-value for a given state and action can be computed as:\n$Q(s_t, a_t) = r(s_t) + \\gamma \\cdot max_{a \\in A} Q(s_{t+1}, a_{t+1})$\nwhere $s_t, a_t$ are the current state-action values and $s_{t+1}, a_{t+1}$ are the next state-action values for a discount factor \u03b3.\nHowever, with an intractably large state space, it is infeasible to store the Q-value of each possible action. Therefore, a deep neural network commonly referred to as a Deep Q-Network (DQN) is used to approximate the Q-values for each action by taking the features of a state as input, and returning the estimated Q-value of for each action as the output. The logic behind the Deep Q-Learning implementation is as follows:\n1. Given a state input, the model predicts an action output referred to as \"pred.\" The output represents a tensor of the initial estimates of Q-values for each of the actions at a state.\n2. The output tensor is cloned and referred to as \"target.\" This tensor will be updated to match a closer estimate of the true Q-values, calculated in the subsequent step."}, {"title": "Task Adaptation", "content": "When an agent is has been trained with a DQN to accomplish a task, its learned Q-values have been updated to estimate optimal actions for just that task. Therefore, if the agent needs to accomplish a different task, it will need to retrain its DQN from scratch to learn the correct Q-values to accomplish the new task. This requires sampling more states, actions, and next states to learn the reward associations. In the interest of reusing learned associations from the original task, we investigate a simple idea: retraining the agent to perform new task directly on the neural network trained for the original task. Inspired by transfer learning, the idea is that when training on the original task, the model learns an optimal policy and gains some inherent representation of the environmental dynamics that help it solve the task (Neyshabur et al., 2021). When defining a new task we consider the new task as a small modification of the feature space of the reward function.\nHypothesis: If the new task is not significantly different from the original task, then we hypothesize that the weights learned for the new task are within some difference of the original neural network such that fine-tuning the original DQN is more sample efficient than training from scratch.\nTo this extent, we explore how different training algorithms to generate trajectories for DQNS enable fast learning. We conduct experiments using on-policy training, random exploration, expert demonstrations, and then compare these to the model's performance when trained with supervised learning instead of any deep reinforcement learning method."}, {"title": "Experiment 1: A Grid", "content": "The goal of reinforcement learning is to train an agent to accomplish a task by using rewards as an auxiliary value that influences actions made by the agent towards achieving the goal task. It operates on the delicate balance between exploration and exploitation. In order to understand how this process allows the agent to learn, we have crafted a simple experiment to observe the way an agent learns Q-values and how it affects the way it learns to accomplish the task. We ran each training run of the experiment multiple times, so while there is some stochasticity due to random seeding, the results are generally consistent and representative of the expected results. Consider a 3-by-3 grid that has a green cell, red cell, a brown cell, and 6 other white cells. The goal of the agent is to traverse the grid and arrive at the green cell without going out of bounds or hitting the red cell, both of which would terminate the game. It must try to avoid the brown cell, which represents an obstacle and merely provides a penalty. The agent can take the actions \"up,\" \"down,\" \"left,\" or \"right\" at any non-terminal (white) cell and is not penalized in the first time step. This experimental setup uses a non-stochastic policy and the reward function specified in Fig. 1.\nApplying a dynamic programming formulation for Eq. 1, we were able to hand calculate the Q-value for each action at each state, as depicted in Fig. 1. The highlighted blue triangles in each state correspond to the action of the optimal policy in that state. Regardless of the methodology used to\ntrain the DQN, we expect that the agent eventually learns the optimal policy to reach the green cell. Our model is parameterized as a three-layer neural network that takes the state as input (y-pos, x-pos), connects to a 512 neuron hidden layer activated by ReLU, and has 4 neurons in its output layer, corresponding to the actions for up, down, left, and right. The learning rate is 0.0001 and \u03b3 is 1, as there is already a -1 living penalty after the first time step. Training is conducted for 20,000 episodes.\nIn order to gauge the success of the model while it is being trained, we implement a \"test while train\" approach, in which we utilize an \"accuracy\" metric, defined as the percentage of episodes where the agent successfully accomplishes the task out of 250 simulated roll-outs of the currently training model after every 10 episodes of training. In this experiment, a success is any trajectory that includes the green cell (0, 0) and does not include the brown cell (1, 0). Due to the terminality setup, by definition any trajectory that contains the red cell or goes out of bounds will not also contain the green cell. For the purpose of experimentation, in this setup the expert's actions are also optimal (i.e. \"expert policy\u201d and \u201coptimal policy\" are used interchangeably.) This is a strict assumption to make, as it may not be the case for autonomous driving, or other dynamic environments. If this condition is not true in a setting, then the agent's learning will converge to the expert demonstrator's behavior rather than the optimal behavior.\nUsing this experimental setup, we make the agent learn using one or a combination of training algorithms: 1) On-policy - the agent takes the action corresponding to the maximum Q-value output by the neural network, 2) Random Exploration - the agent takes a random action at a state, and 3) Expert Demonstration - the agent queries an expert model that always takes the optimal action at any state, and then takes that action. As a control, we also train a model using supervised learning where the agent can query an expert that knows the optimal Q-value and action at every state."}, {"title": "Results for the Original Task", "content": "In order to evaluate the efficacy of each training algorithm, we employ an MSE criterion to quantify how well the trained model is able to estimate the true Q-values for each state and action. After training for initial task success, we see that after 20,000 episodes the agent is able to achieve 100% task accuracy only when alternating between on-policy evaluation and querying the expert. As depicted in Table 1, this yields a very low MSE across the truly optimal trajectory, but a high MSE elsewhere. This makes sense because the agent never does random exploration, so it is more likely to follow converge to the expert demonstrator's behavior. Consequently, we see that training the agent by alternating between random exploration and on-policy evaluation yields a relatively low MSE\nacross all Q-states but a moderately high MSE across the truly optimal trajectory. This also makes sense because random exploration forces the agent to visit states and take actions that it would never be incentivized to do under an on-policy evaluation. The on-policy evaluation on the other hand is able to adjust Q-values that are more aligned with the model's predictions. Supervised learning does not involve Q-values, and just trains a model to match the predicted action with the true action of the expert, and therefore is quick to yield a 100% task accuracy. Random exploration allows the agent to update its Q-values for rarely seen state-action pairs (the agent has no incentive to take some action or visit some state), yielding significantly slower convergence to 100% task accuracy but more accurate Q-values when combined with other policies. As the number of training episodes tends towards infinity, the Q-value accuracy grained by random exploration will reach optimal values.\nIn all of the experiments, after running for longer trials, they are all able to reach a 100% task accuracy rate with random exploration. The results beget the question: why does it matter if all of the Q-values are accurate if the agent learns to successfully accomplish the task? What if the agent is just able to accomplish the task but has poor Q-value estimates? If the agent learns accurate Q-values along the optimal trajectory, but poor Q-values elsewhere, then due to compounding errors, the agent may take sub-optimal actions when it is no longer on the optimal trajectory (Luo et al., 2024)."}, {"title": "Results for the Adapted Task", "content": "With sufficient training through deep Q-learning or supervised learning, the agent is able to successfully accomplish the original task. Therefore, we use these trained models as base models to evaluate how well the agent is able to adapt to a new task: having the agent to get to the red state and avoid the green cell, effectively swapping the functionalities of the red and green cells from the original task. The only change in the reward function is to swap the penalties for the agent when existing at the green and red states. The agent will be loaded with one of the base models and will use deep Q-learning to train on top of the base model, effectively treating it as a DQN.\nTable 2 shows the results of retraining the agent using the supervised learning model as a base model, where effectively all optimal actions for the original task are one-hot encoded. It also displays the results when using the optimally trained DQN as the base model. Both base models were retrained with all of the learning algorithms used in the original task. The results are surprising because they show that the agent can quickly converge to accurate Q-values for the new task using Deep Q-learning even if it was trained with supervised learning for the original task, where the estimated Q-values are significantly different. Similarly to the results from Table 1, training the neural network with an alternation of expert demonstrations and on-policy action selection yields a very quick convergence for 100% task success of the new task and the lowest MSE across optimal states with both the supervised learning and optimal DQN base models. Training with solely on-policy selection or solely\nexpert demonstrations does not result in the agent converging to 100% task accuracy within 20,000 training episodes regardless of the base model.\nWe also observe that involving random exploration in the retraining stage consistently reduces the MSE across all Q-states regardless of the base model, relative to agents that train without random exploration, consistent with the findings in (Tijsma et al., 2016). Finally, agents that train with supervised learning are able to settle at 100% new task accuracy in the least number of samples, regardless of the base model, but have the highest MSE across all optimal and Q-states, suggesting that the agent would perform poorly if out of distribution. Consequently, when comparing the MSE results for each retraining algorithm between base models, agents that train with the optimal DQN base model have significantly lower MSEs across optimal Q-states and all Q-states than agents trained on the supervised learning base model. The number of episodes to settle at 100% accuracy are generally similar, so the main advantage of training on the DQN is closer to correct Q-values for the new task."}, {"title": "Experiment 2: Autonomous Intersection Crossing", "content": "Since the 3\u00d73 grid world experiments above are rudimentary and rather arbitrary for a task adaptability study, we tested our hypothesis on a scaled up and practical task: autonomous intersection crossing. The initial task is for a car (the ego car) in one lane to safely cross an intersection against oncoming traffic (ado cars) that has the right of way in a perpendicular lane, but only if the gap between two ado cars that the ego car passes through is at least 80 pixels wide. The environment is represented as two perpendicular one-way lanes. The cars are represented as circles with an x and y coordinate corresponding to their center.\nThe DQN has 16 inputs, a hidden layer of 1024 neurons, and an output layer of 2 neurons, cor- responding to the two possible actions; stop and go at a fixed velocity. The first two layers are"}, {"title": "Results for the Original Task", "content": "For the original task, we trained a DQN using standard Deep Q-learning, which took 1.5 million episodes to converge (with high variance) to a task accuracy of greater than 75%. The agent trained by sampling random actions and expert actions in alternating episodes in order to gain some off-policy accuracy. We separately trained a supervised learning model that converged to a 100% task accuracy within less than 10,000 episodes (Fig. 3). Unlike the DQN model, this model was trained with access to the expert's optimal actions at every possible state, making it the equivalent of standard behavioral cloning. Clearly, we see that with significantly less episodes, the model trained with supervised learning is able to cross the intersection safely, even in the presence of stochastically spawned ado cars. Consequently, the model trained with deep Q-learning takes millions of episodes to learn to safely cross the intersection even 75% of the time."}, {"title": "Results for the Adapted Task", "content": "The new task for the agent to adapt to is to cross the intersection only if the gap between the ado cars, while crossing the intersection, is at least 120 pixels wide. We chose this target task because it is not too different from the original task; merely a stylistic change in the nature of crossing predicated on the positions of the ado cars, which is observable through the state features. In practice this can be thought as answering the question: do we need to retrain an autonomous vehicle when the National Highway Traffic Safety Administration makes slight changes to safety standards?\nWe are able to use expert demonstrations in retraining because we have access to a sample of expert trajectories to show the model what the new task looks like. However, we must use deep Q-learning and not supervised learning because we cannot assume that we will have the expert's action at every state. Because the agent just has access to a sample of expert demonstrations, we want to explore how long it will take for the agent to learn the new task using retraining, where the new"}, {"title": "Conclusions", "content": "We explored the trade-off between having the model estimate accurate Q-values and achieving sample-efficient task success. Furthermore, we learned the considerations for adapting a model to accomplish a different task both by retraining a supervised learning model and a DQN. When using a base model that has Q-values closer to the accurate Q-values, the adapted model is more likely to converge at 100% accuracy as opposed to bouncing around, and achieve this convergence much quicker than when adapting from the supervised learning base model that does not estimate accurate Q-values. Furthermore, although random exploration increases the number of training samples that an agent will experience, it always helps in learning Q-values more accurately, and facilitates convergence when combined with methods that simply learn on the optimal path. We believe that this analysis will have diverse applications, including legacy systems where models have been trained for one purpose and used for years before needing to be updated or retrofitted by leveraging learned information."}]}