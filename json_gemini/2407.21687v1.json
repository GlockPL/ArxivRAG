{"title": "Dynamic Object Queries for Transformer-based\nIncremental Object Detection", "authors": ["Jichuan Zhang", "Wei Li", "Shuang Cheng", "Ya-Li Li", "Shengjin Wang"], "abstract": "Incremental object detection (IOD) aims to sequentially learn new classes, while\nmaintaining the capability to locate and identify old ones. As the training data\narrives with annotations only with new classes, IOD suffers from catastrophic\nforgetting. Prior methodologies mainly tackle the forgetting issue through knowl-\nedge distillation and exemplar replay, ignoring the conflict between limited model\ncapacity and increasing knowledge. In this paper, we explore dynamic object\nqueries for incremental object detection built on Transformer architecture. We\npropose the Dynamic object Query-based DEtection Transformer (DyQ-DETR),\nwhich incrementally expands the model representation ability to achieve stability-\nplasticity tradeoff. First, a new set of learnable object queries are fed into the\ndecoder to represent new classes. These new object queries are aggregated with\nthose from previous phases to adapt both old and new knowledge well. Second,\nwe propose the isolated bipartite matching for object queries in different phases,\nbased on disentangled self-attention. The interaction among the object queries\nat different phases is eliminated to reduce inter-class confusion. Thanks to the\nseparate supervision and computation over object queries, we further present the\nrisk-balanced partial calibration for effective exemplar replay. Extensive exper-\niments demonstrate that DyQ-DETR significantly surpasses the state-of-the-art\nmethods, with limited parameter overhead. Code will be made publicly available.", "sections": [{"title": "1 Introduction", "content": "Humans inherently possess the ability to incrementally learn novel concepts without forgetting\nprevious ones, capable of acquiring and accumulating knowledge from past experiences. Traditional\nobject detection models [10, 64, 24, 46, 65] rely on supervised learning with fixed data, where all\nclasses are predefined and known beforehand. However, real-world data continuously evolve over\ntime, leading to non-stationary distributions. Due to the stability-elasticity dilemma, fine-tuning\nmodels directly on new class data leads to catastrophic forgetting [35, 36], whereas joint training\nis expensive in both computation and storage. Therefore, incremental object detection (IOD) has\nattracted increasing attention in both research and practical applications.\nRecent advances for IOD adopt knowledge distillation and exemplar replay to address forgetting.\nKnowledge distillation-based methods [48, 21, 40, 9, 18, 39] typically involve distillation on the non-\nbackground predictions of the old model to circumvent the imbalance issue caused by an excessive"}, {"title": "2 Related Work", "content": "Incremental Learning (IL). Prevailing IL methods can be broadly divided as regularization-based,\ndistillation-based and structure-based ones. Regularization-based methods[1, 19, 38, 59] estimate\nparameter importance and penalize updating of crucial parameters to maintain previous knowledge.\nDistillation-based methods build the mapping between the old and new model by matching logits[26,\n45], feature maps[7], or other information[15, 42, 49, 50, 51], which leverage the knowledge transfer\nto prevent forgetting. Structure-based methods[14, 25, 51, 54, 56, 63] dynamically expand the\nrepresentative network, e.g., backbone, prompt, to fit the evolving data stream.\nIncremental Object Detection (IOD). As the typical extension of incremental learning, IOD involves\nmultiple objects belonging to the old and new classes appearing simultaneously. This co-occurrence\nmakes knowledge distillation an inherently effective strategy for IOD, since it allows for the utilization\nof old class objects from new training samples to minimize the discrepancies in responses between the\nprevious and current updating model. As a pioneering work, ILOD[48] distills the responses for old\nclasses to counteract catastrophic forgetting on Fast R-CNN[10]. The idea of knowledge distillation\nis then extended to other detection frameworks, such as CenterNet[64] (SID[40]), RetinaNet[28]\n(RILOD[21]), GFLV1[24] (ERD[9]), Faster R-CNN[46] (CIFRCN[12], Faster ILOD[39], DMC[61],\nBNC[6], IOD-ML[17]) and Deformable DETR[65] (CL-DETR[32]). Built on Deformable DETR\ninstead of conventional detectors such as Faster R-CNN, DyQ-DETR can efficiently expand queries\nrather than inefficiently enlarging the backbone or specific convolutional layers. Note that DyQ-DETR\nalso uses knowledge distillation techniques. As for exemplar replay,[16] proposes maintaining an\nexemplar set and fine-tuning the model on the exemplars after each incremental step.[31] proposes an\nadaptive sampling strategy to achieve more efficient exemplar selection and [32] proposes distribution-\npreserving calibration, which selects exemplars to match the training distribution. They usually\nfinetune directly using the exemplar set with incomplete annotations and overlook the amount of\ninformation and reliability of the annotated objects.\nTransformer-based Object Detection. The infusive work DETR (DEtection TRansfomer)[3]\nformulates object detection as a set prediction problem, with an elegant transformer-based architecture.\nIt captures global context and reasons object relations with attention mechanism. With a small set\nof learnable object queries and Hungarian bipartite matching[20], it eliminates the need for the\ncomplex non-maximum suppression and many other hand-designed components in object detection\nwhile demonstrating good performance. Deformable DETR[65] introduces sparse attention on multi-\nlevel feature maps, thereby accelerating the convergence of DETR and improving the performance,\nparticularly for small objects. There also exists many other DETR variants[4, 37, 22, 30] designed to\naccelerate convergence speed and enhance detection performance. Without loss of generality, we\nbuild our method on the commonly used Deformable DETR."}, {"title": "3 Methodology", "content": "Preliminaries In the paradigm of IOD, object detection is performed in multiple steps from sequen-\ntially arrived training data to recognize and localize objects of all seen classes in test images. Let\nD be a dataset with samples (x, y), where x is the image, y is the set of object class labels and the\nassociated bounding boxes. Suppose there are T steps. At time step t, the incoming dataset is denoted\nas Dt = (xt, Yt) and the objects belong to seen classes in Ct. Specially, the images in Dt are only"}, {"title": "3.1 Transformer-based Incremental Object Detection", "content": "We build up the DyQ-DETR based on the DETR architecture [3, 65]. Other than the backbone, a\nDETR consists of an encoder, a Transformer decoder, and the predictor to generate object classes\nand locations. The encoder takes the images as inputs and outputs visual features. Then those visual\nfeatures and learnable object queries are fed into Transformer decoder for prediction. Notably, we\npropose to aggregate the object queries from up-to-far learning steps to resist the forgetting of old\nclass knowledge. Due to the absence of annotations from previous seen classes in incremental step t,\nknowledge distillation is applied for preserving the class-specific old knowledge. As in Fig.2, we\nselect the non-background predictions of old classes by thresholding. Pseudo labels \u1ef9+(1 < r < t)\nare generated by the last previous model. Instead of mixing the pseudo labels \u1ef9,(1 \u2264 r < t) with\nthe ground-truth labels yt, we use \u1ef9+ and yt separately to supervise the model training. The pseudo\nlabels for old classes and the real ground-truths are used to guide the learning of object queries being\nQ and Qt, respectively. Besides, object queries from different groups share the weight parameters\nof the Transformer decoder. To allow for computational complexity to grow linearly rather than\nquadratically, the self-attention over different groups of object queries are eliminated.\nAs for IOD with exemplar replay, where a small number of exemplar images et from dataset Dt in\ndifferent time steps are stored, we introduce a risk-balanced selection mechanism. At time step t,\nwe use the trained model It to score the annotated objects from images in Dt. The computed loss\nfrom the partial bipartite matching is considered as the risk score for exemplar selection. We choose\nthe sample images with moderate risk score for the tradeoff between the annotation importance and\nquality. Specifically, to build the exemplar dataset 61:t, we select the samples falling into the middle\npart after sorting, because they are informative and reliably annotated. Considering the image in\n\u20ac1:t is incompletely annotated for specific classes, we adapt the partial calibration. We leverage the\nincomplete annotations to calibrate the output for corresponding object queries in each group. Since\n\u20ac1:t is balanced, the partial calibration will prohibit the model being biased towards certain classes."}, {"title": "3.2 Dynamic Object Query Assembly", "content": "Existing DETR models employ a fixed set of object queries (i.e., learnable embedding) as the\ninputs of Transformer decoder. These object queries are progressively optimized to map into object\ninstances in images. Despite various designs, the learnable object queries are highly relevant with the\nspecific classes. For IOD, the object queries are expected to be associated with objects belonging\nto sequentially arrived classes. Since new classes are considered as backgrounds in previous time\nsteps, the preservation of old knowledge naturally contradicts the knowledge updating from new data\nlearning, especially from the perspective of object queries. Moreover, the conflict between a fixed\nnetwork and the continually emerged class-specific information severely undermines the performance\nof incremental learning, especially in non-exemplar scenarios.\nTo cope with incremental classes without extra modules in network architecture, we focus on tackling\nthe forgetting issue with dynamic object queries. At time step t, for a set of new classes Ct, a new\nset of learnable object queries Qt is added. The newly added object queries Qt are aggregated\nwith previous sets of queries Q\u2081,1 < r < t. The assembly of object queries {Q1, Q2,\u2026\u2026,Qt}\nserves as the input of Transformer encoder in step t. The visual embeddings et corresponding to the\nnewly expanded queries Qt are used to predict the objects of new classes Ct, and the old classes\nC\u2081(1 \u2264 r < t) is detected with the embeddings e, of old queries Q7. By dynamically expanding\nobject queries, the new and old classes are segregated by class embeddings, significantly alleviating\nthe conflict between the old knowledge and continuously evolving new knowledge.\nBased on the dynamic assembly of object queries, we further investigate the decoder design to\nrestrict the computational burden. In standard DETR, object queries interact visual features with\ncross-attention for refinement. Besides, those object queries interact with each other by self-attention.\nThrough self-attention, duplicated detections can be removed, but the computational complexity\nincreases quadratically with the number of object queries. Considering that the object instances from\ndifferent class sets rarely overlap, we disentangle the self-attention in Transformer decoder. The\nself-attention is computed among separate set of object queries, as:\n$a_{i,j} =\n\\begin{cases}\nSoftmax(Q_{m,i} \\cdot Q_{n,j} / \\sqrt{d}) & m = n \\\\\n0 & m \\neq n\n\\end{cases}$\nwhere ai,j denotes the attention weight between two queries Qm,i and Qn,j from the query set Qm\nand Qn, respectively. By eliminating the attention interaction between queries of different groups\nas shown in Eq.(1), we achieve a linear growth in computational complexity almost for free. Upon\nthe addition of new queries, the capability of old queries to detect old classes is fully preserved. We\nperform the decoder forward passes as many as the time steps, obtaining the embedding vectors from\ndifferent sets of queries as:\ne\u2081 = decoder(ft, Qr),1 \u2264 t \u2264t,\nwhere ft denotes the visual feature extracted from image x by the CNN and Transformer encoder.\nEach decoder forward pass is executed with a different set of queries Q7, resulting in different\ntask-specific embeddings er to obtain detection predictions of the corresponding classes CT.\nWe further adapt the knowledge distillation for incremental detector training. The foreground\npredictions with the pseudo labels generated from old model are kept for distillation and used for\nsupervision. As in [32], the foreground predictions with high confidence from the old model are\nselected. A probability threshold \u03b8p (typically 0.4) is set over the prediction scores. An additional\nIoU threshold \u03b8\u03b9\u03bf\u03c5 (typically 0.7) is used to restrict the predictions not too close to the ground-truth\nbounding boxes of new classes. It helps filter out incorrect predictions about new class objects that are\nmisclassified as old classes. The highly-confident predictions after filtering are used as pseudo labels\n\u1ef9,(1 \u2264 r < t), which contain two parts of annotations (i.e., the predicted object labels and bounding\nboxes). Notably, instead of merging the pseudo labels with real grounding truths, we compute the\nseparate bipartite matching losses with different set of object queries in an independent way. The\nloss for retaining old class knowledge LDETR(1 < r < t), as well as the loss for learning new class\nknowledge LDETR can be formulated as in Eq.(3):\nLDETR = LDETR(er, \u1ef9\u30f6),1 \u2264 r < t; LDETR = CDETR(et, Yt),"}, {"title": "3.3 Risk-balanced Partial Calibration", "content": "For exemplar replay based IOD paradigm, an ex-\nemplar memory is formed to store a small num-\nber of samples for subsequent incremental learn-\ning. At time step t, the exemplar set et is a subset\nof the entire dataset Dt. Let \u20ac1:t = \u20ac1 U... Et.\nThe exemplars in et are used to represent the\nsamples with objects in Ct. We intend to choose\nthe class annotated images which are substan-\ntial for detector training. An intuitive way is\nto directly compute the loss between the model\noutput and the real label yt for sample selection.\nHowever, since the image x \u2208 Dt only con-\ntains annotations for the classes Ct of interest,\nthis will result in the loss being dominated by\nthe absence of old classes, posing challenges to\nforeground-background balancing.\nBenefiting from the internal decoupling of dy-\nnamic object queries, we are able to exclusively detect new classes Ct using the corresponding queries\nQt. Considering that the image is only annotated for the specific classes, the partial loss is more\nreliable. For computing the partial loss, the old classes are considered as backgrounds by Qt, which\nis compatible with the incomplete real labels yt. After each incremental step of training, the partial\nloss from Eq.(5) is considered as the risk score to guide the subsequent selection of exemplars.\nY \u2190 Lpartial = CDETR(et, Yt).\nThe risk score can measure the quality of incomplete class labels and bounding boxes. Additionally,\nit also takes the number of annotated objects like in [32] into account. As in Fig. 3, images with low\nrisk, which constitute a high proportion, provide limited information for optimization, while those\nwith high risk are likely to be outliers with incorrect annotations. Based on the risk estimation, we\nconstruct the exemplar set et by sorting and selecting the middle part of risk-balanced samples in Dt.\nThe exemplar set et is merged with 6t-1 to form a set \u20ac1:t for partial calibration of the model.\nIn incremental step t, the exemplar sets \u20ac1:t are used to finetune the model after training with Dt.\nPrevious IOD methods typically finetune the model directly with dataset \u20ac1:t, without addressing the\nissue of missing labels. That is, an image x in the balanced exemplar set 61:t is only annotated for a\nspecific class subset from {C1, ..., Ct\u22121, Ct}, with the absence of annotations for other classes. The\nconfusing, even contradictive supervision hinders the process of prediction calibration. An intuitive\nway is to use pseudo labels, yet the quality of pseudo labels is hard to guarantee. Thanks to the\ndynamic object queries and associative disentangled computation, we propose to perform partial\ncalibration that relies only on the incomplete real labels. Specifically, we calculate the partial loss\nbetween the outputs of the corresponding queries and the ground-truth annotations in \u20ac1:t. This type\nof partial calibration further mitigates the forgetting."}, {"title": "4 Experiment", "content": "4.1 Experimental Setup\nDataset and evaluation metrics. Following[32], we conduct experiments on the widely-used COCO\n2017 dataset[29], which consists of images from 80 object categories in natural scenes. The standard\nCOCO metrics as AP, AP50, AP75, APS, APM, APL are used for performance evaluation.\nProtocols. We evaluate DyQ-DETR with two protocols: 1) traditional protocol [48] (Tab.1-left) and\n2) revised protocol proposed by[32] (Tab.1-right). Protocol 2) avoids observing the same images at\ndifferent stages. Therefore, we adopt protocol 2) for all subsequent experiments, and the details of\nprotocol 1) can be found in the appendix. For protocol 2), we adopt both two-phase and multiple-\nphase settings, which can be formulated in the form of c\u2081 + C2 + ... + CT, where ct represents the\nnumber of new classes in incremental step t (ct = |Ct|) and the sum of ct is denoted as c (c = |C1:T|).\nAt time step t, we observe a fraction of the training samples with ct new categories annotated. We\ntest settings C1 + C2 + ... + \u0441\u0442 = 40 + 40, 70 + 10, 40 + 20 \u00d7 2, and 40 + 10 \u00d7 4. Following [32],\nwe also set the total memory budget for the exemplars to be 10% of the total dataset size.\nImplementation details. Following [32], we build DyQ-DETR on top of Deformable DETR [65]\nwithout iterative bounding box refinement and the two-stage variant. The backbone is ResNet-50[13]\npretrained on ImageNet[5] and the training configurations for the initial stage are consistent with[32]\nto maintain uniform performance in the initial phase. We denote the initial number of queries for the\ndetector as N (typically 300). For both two-phase and multi-phase settings, we dynamically expand\nby N queries at each incremental step t and the initial parameters of Qt are inherited from Qt-1.\nAt step t, old queries Q1:t-1 are frozen during the incremental training and unfrozen in subsequent\nexemplar replay. We train the model for 50 epochs, and for an additional 20 epochs during fine-tuning.\nAll the experiments are performed on 8 NVIDIA GeForce RTX 3090, with a batch size of 8."}, {"title": "4.2 Quantitative Results", "content": "Two-phase setting. We compare our DyQ-DETR with LwF[26], iCaRL[45], RILOD[21], SID[40],\nERD[9], and the previous SOTA method CL-DETR[32]. For each setting, we provide the performance\nof different methods with/without Exemplar Replay (ER). The metrics by joint training are also\npresented as the upper bound for reference. Tab.1 shows that, in the two-phase settings, our proposed\nDyQ-DETR consistently outperforms the aforementioned methods under different protocols with\nsignificant margins. For protocol 2), with exemplar replay, the DyQ-DETR achieves the AP of 39.7%\nand 41.9% under 40+40 and 70+10 settings. It surpasses CL-DETR by 2.2% AP and 1.8% AP\nunder the 40+40 and 70+10 settings, respectively. Compared with the upper bound, the DyQ-DETR\nobtains an average performance gap of 1.4%, which is much smaller than the 3.4% gap of CL-DETR."}, {"title": "Multiple-phase setting", "content": "We conduct experiments in the more challenging 40+20\u00d72 and 40+10\u00d74\nsettings. The changing AP and AP50 along with time steps are presented in Fig.4. Our DyQ-DETR\nconsistently outperforms other IOD methods. Moreover, in both settings, the AP improvements of\nDyQ-DETR become more pronounced with the increase of incremental steps."}, {"title": "Scalability", "content": "As shown in Fig.5-left, by expand-\ning queries rather than the network structure, the\nadditional parameter overhead of our method\nis almost negligible. As illustrated in Fig.5-\nright, the computational overhead of our method\ngrows linearly because of the removed inter-\ngroup query interaction. Since the computa-\ntional load of the decoder (excluding the portion\nshared by different query groups) constitutes\na small portion (~6%) of the entire model's\ncomputation, our computational complexity in-\ncreases at a slow linear rate. Specifically, with\n20 stages and an increment of 100 queries per\nstage (note that the standard Deformable DETR\nhas 300 queries), DyQ-DETR only increases the parameters and GFLOPs by 2% and 39% respectively\ncompared to the standard Deformable DETR, confirming its scalability."}, {"title": "4.3 Ablation Study", "content": "Effect of dynamic object queries. Tab.3 illustrates the ablation study over dynamic object queries\nin the 70+10 setting. Compared to the baseline, the naive way of expanding queries (+Nat Query)\nincreases the AP to 34.5% and AP50 to 52.7%, by 1.1% and 4.2%, respectively. This can be attributed\nto model capacity. By equipping the dynamic object queries (+Dy Query) with isolated matching on\nindependent query set, we further obtain 3.6% AP and 3.5% AP50 improvements. It validates that\ndynamic object queries is effective to retain old knowledge during the learning of new knowledge.\nFurthermore, freezing the task-specific old queries (+DyFro Query) during the incremental training\nleads to slightly more improvement of 0.4% AP."}, {"title": "5 Conclusion", "content": "In this paper, we propose DyQ-DETR for incremental object detection. Distinct from the mainstream\nmethods focus on the distillation mechanism, we address the catastrophic forgetting by taking\ninspiration from dynamic networks for model capability expansion. In particular, we propose the\ndynamic object queries with incremental assembly of new object queries, disentangled self-attention\ncomputation and isolated bipartite matching over object queries from different time. The DyQ-DETR\ncan alleviate the conflict between outdated background knowledge and continually emerged classes,\nthereby achieving stability-plasticity tradeoff. Benefiting from the isolated supervision of dynamic\nobject queries, we further propose risk-balanced partial calibration for effective exemplar replay,\nwith the idea to select exemplars based on risk and partially finetunes the model without relying\non low-quality pseudo labels. Extensive experiments demonstrate that our proposed DyQ-DETR\nsurpasses existing IOD methods by a large margin, with quite limited memory overhead. Besides of\ndynamic query, we hope that more various ways of model expansion can be explored in IOD."}, {"title": "A Appendix", "content": "In Sec.B, we present the algorithmic pipeline of DyQ-DETR. In Sec.C, we introduce a simple but\nstrong baseline with only exemplar replay to emphasize the importance and rationale of the non-\nexemplar setting. In Sec.D, we provide more detailed experimental results. This includes details\nof the traditional protocol (Sec.D.1), more fine-grained results about old classes and new classes\n(Sect.D.2), and more ablation results (Sec. D.3). In Sec.E, we show more visualization results,\nincluding images with different levels of risk, behavior of decoupled queries, and performance\ncomparison with CL-DETR. Please note that all experiments, except for those in Sec.D.1, are based\non the revised protocol, using Deformable DETR on COCO 2017, and we will not specify this further."}, {"title": "B Algorithmic Pipeline of DyQ-DETR", "content": "To get a more comprehensive understanding of the overall framework, we describe the training\npipeline of DyQ-DETR in Algorithm 1. In the T-th phase, we dynamically add a new set of queries,\nwhich are responsible for detecting and recognizing new classes. We disentangle self-attention\nmodules in the decoder, ensuring that queries between different groups remain relatively independent.\nIn the incremental training, we use a score threshold to filter out background predictions, resulting\nin the pseudo-labels ypseudo. Instead of simply merging pseudo-labels ypseudo and incomplete real\nlabels y, we divide the completed annotations of an image according to the incremental phase/class\nset. For an image, annotations corresponding to different class sets act on the outputs of respective\nqueries, thus resulting in the decoupled loss LDETR for training.\nAfter the incremental training, we utilize the trained model to calculate the partial loss as the risk\nscore for images in D. We sort D, based on the risk score and simply select the middle 10% of\nthese samples to serve as exemplars. Then, for an image in the exemplar set \u20ac1:7, we identify the\nspecific stage corresponding to its annotation, and we use the respective output and real labels to\ncompute the partial loss Lpartial for calibration."}, {"title": "C Baseline with only Exemplar Replay", "content": "We introduce a simple baseline, which only combats forgetting by saving exemplars of the same\nsize. As shown in Tab.6, it completely forgets past knowledge after the incremental training, but\nits performance significantly improves after the simple exemplar replay. The total memory budget"}, {"title": "D More Detailed Experimental Results", "content": "D.1 Traditional protocol and results\nThe main difference between the traditional protocol[48, 21, 40, 9] and the revised protocol[32]\nlies in data partitioning. In the traditional protocol, the model can observe all images containing at\nleast one object of the currently interested classes, which may result in an image appearing across\nmultiple phases. Formally, let D = {(x,y)} denotes a dataset with images x and corresponding\nobject annotations y. First, we divide the total class set into non-overlapping parts {C1, C2, ..., \u0421\u0442},\none for each incremental phase. For each phase 7, all samples in D retain annotations of C and\ndrop others. The incremental training dataset in phase T consists of images that contain at least one"}, {"title": "D.2 Results about old classes and new classes", "content": "Tab.7 presents fine-grained results in the 40+40 setting about old classes and new classes, which\nrespectively represent the model's stability and plasticity. The results show that, compared to the\nSOTA, our DyQ-DETR achieves improvements of 1.6% AP for old classes and 2.3% AP for new\nclasses, indicating enhanced stability and plasticity in our method."}, {"title": "D.3 More ablation results", "content": "Effect of the score threshold. In the 40+40 setting, we set the score threshold to 0.6, while for other\nsettings, it is set to 0.4. This is because, in other settings, there is a higher proportion of old classes\nwhere stability is preferred, and more old class pseudo-labels help combat forgetting. The ablation\nstudy on the score threshold is shown in Fig.7(a), where the optimal threshold for the 40+40 setting is\n0.6. We also find that when the score threshold is low, the pseudo-label noise is significant, leading to\npoor performance in non-exemplar settings. However, after exemplar replay, the performance greatly\nimproves, further illustrating that exemplar replay may mask issues in the incremental training.\nEffect of the selected risk interval. After sorting the images by risk in ascending order, the results\nof changing the selection interval are illustrated in Fig.7(b). It can be observed that selecting images\nwith moderate risk is optimal. This selection strategy is straightforward, and our contribution lies\nmore in demonstrating the feasibility of using the model to optimize the selection."}, {"title": "E More visualization results", "content": "Images with different levels of risk. As illustrated in Fig.8, images with low risk are considered as\nsimple samples containing only a few annotated objects, while images with high risk are characterized\nby a dense distribution of objects, accompanied by severe occlusion or inaccurate annotations.\nSamples in the middle part are considered informative and reliable, and such an active selection\nstrategy promotes more effective exemplar replay.\nBehavior of decoupled queries. As depicted in Fig.9, our designed dynamic query set achieves the\ngoal of only detecting the corresponding class set. The old query set is responsible for memorizing"}, {"title": "F Discussion over Open Vocabulary Object Detection", "content": "Open vocabulary object detection (OVD) leverages visually related language data as auxiliary\nsupervision to bridge the gap between base categories and novel label spaces[52, 58]. By using\nimage-caption pairs, it acquires an extensive vocabulary of concepts, allowing the model to learn\nobject detection with annotations only for certain base categories. Consequently, the model can\ndetect both base classes and novel classes that were not present in the training annotations. OVR-\nCNN[58] is the pioneering work that introduced the concept of open vocabulary object detection and\nestablished a comprehensive framework to address this problem. Subsequently, with the introduction\nof large-scale visual language pretraining models such as CLIP[43], novel approaches have emerged\nthat utilize these pretrained models to enhance the capability of open vocabulary object detection.\nSome works[11, 41, 44, 34] employ knowledge distillation, using large model text encoders to align\nvisual features with the large model and transfer new class information. Other works address the\ngap in object detection region localization by converting image-text level pre-training to region-text\nlevel[23, 62, 2, 60], while some complete this during the detector training stage after pre-training[27,\n57, 55, 33].\nThe difference between IOD (Incremental Object Detection) and OVD (Open Vocabulary Detection)\nlies in their focus. IOD emphasizes the continuous learning and updating process of the model,\nwhile OVD focuses on the model's generalization ability. IOD aims to improve the model through\nmultiple learning steps, whereas OVD aims for the model to generalize perfectly to other problems\nwith just training once. Specifically, after training, OVD can detect some objects in a new category\nspace, even if these objects have not been seen in the annotations of base categories, due to the\nrich semantic concepts obtained during the image-caption alignment process. However, despite the\nrichness of the learned semantic concepts, OVD is difficult to handle newly generated concepts due\nto the dynamic nature of the environment and requires re-alignment of image-caption pairs to learn\nthese new concepts. In contrast, IOD simplifies this process by allowing incremental learning of new\nconcept based on newly collected and annotated data, thus avoiding the need for retraining the entire\nmodel."}, {"title": "IOD and OVD are not mutually exclusive but complementary", "content": "They are both important for open-world\npractical applications. As mentioned above, OVD still shows deficiency in handling the emergence of\nnew concepts without retraining, whereas IOD simplifies this process, saving time and computational\nresources. However, IOD struggles to resist catastrophic forgetting of old class semantics during\ncontinuous model updates. OVD relies on pre-learned rich semantic concepts for novel classes,\nwithout consideration of forgetting issue."}, {"title": "G Limitations and Broader Impacts", "content": "We propose a dynamic object queries-based detection transformer (DyQ-DETR) to address catas-\ntrophic forgetting in incremental object detection. While it scales well to tasks with an incremental\nstep of 20, DyQ-DETR may face challenges in scaling to tasks with longer incremental steps.\nHigh-performance incremental object detection systems have tremendous potential to have a signifi-\ncant impact on various fields and inspire innovative research approaches in robotics, autonomous\ndriving, and beyond. For example, our proposed DyQ-DETR can progressively enhance the recog-\nnition and manipulation capabilities of robots, thereby improving efficiency and productivity in\nindustries and healthcare. Furthermore, considering the significant overhead of joint training, DyQ-\nDETR helps detection models avoid retraining from scratch, providing an effective and efficient\napproach for incremental updates of large visual models. As for potential widespread impact, our\nwork has not shown any negative social consequences."}]}