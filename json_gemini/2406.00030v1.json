{"title": "Large Language Model Pruning", "authors": ["Hanjuan Huang", "Hao-Jia Song", "Hsing-Kuo Pao"], "abstract": "We surely enjoy the larger the better models for their superior performance in the last couple of years when\nboth the hardware and software support the birth of such extremely huge models. The applied fields include\ntext mining and others. In particular, the success of LLMs on text understanding and text generation draws\nattention from researchers who have worked on NLP and related areas for years or even decades. On the\nside, LLMs may suffer from problems like model overfitting, hallucination, and device limitation to name\na few. In this work, we suggest a model pruning technique specifically focused on LLMs. The proposed\nmethodology emphasizes the explainability of deep learning models. By having the theoretical foundation,\nwe obtain a trustworthy deep model so that huge models with a massive number of model parameters become\nnot quite necessary. A mutual information-based estimation is adopted to find neurons with redundancy to\neliminate. Moreover, an estimator with well-tuned parameters helps to find precise estimation to guide the\npruning procedure. At the same time, we also explore the difference between pruning on large-scale models\nvs. pruning on small-scale models. The choice of pruning criteria is sensitive in small models but not for\nlarge-scale models. It is a novel finding through this work. Overall, we demonstrate the superiority of the\nproposed model to the state-of-the-art models.", "sections": [{"title": "1. Introduction", "content": "People embrace large-scale models based on large-scale data nowadays. The bigger the models, the better\nperformance we can expect from the models. This is because the big models can have their complexity large\nenough to cover the high diversity that could be seen from the data. Given this reason, giant industries\nlook for Large Language Models (LLMs) starting from a few years back, and it gets heated since the\nannouncement of ChatGPT at the end of 2022. That shows the evidence that people generally believe the\nlarger the models the more problems can be solved by the models.\nThe progress of LLMs can be seen by various well-known models such as LLaMA2 [1], PaLM-2 [2], and\ntheir followers for various scenarios given a diverse set of data such as LLaMA-Pro-8B [3] and Med-PaLM\n2 [4]. At the same time, some others are focused on proposing relatively small models to achieve the same\nmodel effectiveness, such as TinyLlama-1.1B [5] and ORCA-2 [6]. Other threads of development includes the\nGemini-based structures, such as Gemini 1.5 [7] and Gemma [8]; also, InternLM2 [9] from an independent\nresearch group, not to forget the still close-to-the-top-performance one, GPT-4 [10] which top others on\nvarious scenarios and receive quite a rich set of comments from the general public.\nLLMs are welcome if we do not have storage or computation limitations. Therefore, applications like\nedge computing cannot be the target when LLMs are among the essential parts of the system. But even if\nwe do not have the limitations, we should keep the storage or computation constrained in as small size as\npossible for the sack of green concerns. As an alternative viewpoint, we can ask for small models to avoid\nthe possibility of overfitting and hallucination. The smaller the models are, the less likely the models can\nrun into those cases.\nIn this work, we propose a pruning method that is designed especially for LLMs. The strategy is based\non some understanding of the models, such as exploring the direction of model explainability borrowing\nthe Tishby's deep learning theory [11, 12, 13]. With the understanding of the neurons' content from the\nrepresentation layer, we have clues to prune the model where the neurons with redundancy shall be dropped\nand the neurons with essential information can be kept.\nThe pruning on large and small-scale models can face very different situations. In large-scale models,\nneurons are considered with a few possible substitutions. That is, we usually have an over-complete repre-\nsentation in a large-scale model with more than necessary neurons in the representation layer. In this case,\ntwo or more neurons could own similar meanings or importance and the pruning on either one can produce\nresulting models with similar effectiveness. On the other hand, pruning on small-scale models can be crucial\nin the sense that the number of neurons is close to the minimum required number of neurons and pruning\non such models needs careful treatment with as precise computation as possible. In this work, the proposed\npruning method, with the help of Mutual Information (MI) estimation can serve the need for such precise\ncomputation that leads to effective compressed models.\nThe most recent progress in LLM compression includes direct network pruning, knowledge distillation,\nquantization, and low-rank factorization. The different methods enforce the compression on different network\nparts. Direct network pruning [14, 15, 16, 17, 18] is aimed to remove redundant or unimportant components\n(neurons or weights) to have a small network to replace a large network to fulfill the goal. The knowledge\ndistillation [19, 20, 21] is focused on finding a small, student model to learn what can be offered from a large,\nteacher model. Mimicking the prediction ability of the large model from a small model, we achieve the goal\nof model downsizing. Quantization [22, 23, 24] provides another approach which saves space by utilizing\nintegers or discrete numbers to substitute floating-point numbers in networks. The computation time may\nalso be saved by such a design. The last approach, called low-rank factorization [25, 26], emphasizes replacing\nthe large weight matrix with a small weight matrix to reach the goal of model compression.\nTo focus on the family of network pruning techniques, we consider two types of methods, the structured\nand unstructured ones. The overall goal is to remove redundant components from the models. First,\nthe structured pruning simplifies an LLM by removing the entire structural components, such as neurons,\nchannels, or layers while keeping the network structures [27, 28]. On the other hand, the unstructured\npruning [18, 29] is aimed at pruning the redundant neurons or links. Given the approach of deleting\nindividual parameters, we may suffer from the irregular sparse structure problems. To compare between\nthe two, the structured pruning can be deployed to various edge devices directly, while the unstructured\npruning needs to be accompanied by the assistance of extra software or hardware treatment to complete the\ntask [28].\nWe have to keep in mind a few items when designing an appropriate compression method for LLMs.\nFirst, quite a few LLMs were trained mainly based on unlabeled data. Some of these types of pre-trained\nLLMs can be used by third-party developers for various downstream tasks. Therefore, we do not know and\nwe cannot assume any label or contextual information provided when building the LLMs. That is to say, we\ndo not know what could be the real mission when building the models or applying any compression to the\nmodels [30]. After all, we better assume only an unsupervised type of compression applied to LLMs. Second,\nmost pruning methods need retraining to maintain the models' performance. This retraining procedure\ninevitably needs to consume various resources [30] and should be avoided if possible. In LLMs, the Feed-\nforward Network (FFN) is the major part of the models in terms of the models' size and it attracts the most\ncomputation [31]. Therefore compressing this part of a network means a lot if refers to the saving of space\nand time complexity. All the above points motivate us to search for an unsupervised LLM compression\nmethod that needs no further retraining after the compression is done. Moreover, we prefer one pruning\nmethod that can be applied to FFN in particular and that is structured typed.\nWe utilize an information-based estimation method to measure the information stored in network neurons\nto decide how we can operate the compression procedure. An MI-based estimator helps to find the relation"}, {"title": "2. Past Work", "content": "Let us discuss the past work that is related to deep model pruning, especially the pruning which can\nbe applied to LLMs. At the same time, we should keep an eye on the most recent progress of LLMs, and\nunderstand what pruning techniques can be used for LLM pruning, given the most recent popular model\nstructures to focus for the time being. To speak of the proposed pruning method, we also need to mention\nsome different types of pruning methods, as well as the MI estimation which is the metric that we use to\nguide the pruning procedure."}, {"title": "2.1. Structured Pruning for LLMs", "content": "Let us go through the past pruning methods that address large-scale models or LLMs and explain how\nthey can achieve the goal. If we consider the structured pruning for LLMs, we separate the methods into\nthe supervised and unsupervised types. In supervised methods, we focus on the relation between hidden\nneurons and the label, while the mutual relation between different groups of neurons receives more attention\nin the unsupervised methods. Which type of pruning should be used also depends on the applications.\nFirst, we start with the supervised pruning methods, which are also the majority in the LLM pruning up\nto now [36, 37, 38, 39, 40, 41]. Among them, Voita et al. [36] proposed a pruning method based on stochastic\ngates and a differentiable relaxation of Lo penalty, which can remove the vast majority of heads without\nseriously affecting the model's performance. Liu et al. [37] proposed a structured pruning method for efficient\nBERT inference (EBERT), which can dynamically prune unimportant heads in Multi-Head Self-attention\n(MHA) and unimportant channels in FFN with the help of the predictor branch. That means that the\nlabeled data are necessary for their operations. Kwon et al. [38] proposed a three-stages pruning framework,\nwhich used a Fisher-based mask search algorithm (labeled data are needed) to decide which heads/filters to\nprune, then rearranged the pruned heads/filters, and at last tuned the mask variables to recover the output\nsignal for each layer. Yang et al. [39] proposed a model pruning toolkit called TextPruner for pre-trained\nlanguage models. The toolkit includes two pruning methods: one is supervised method, which used the\ntraining loss to measure the importance score of neurons; and the other is the self-supervised method, which\nused the Kullback-Leibler divergence to measure the importance score of neurons. Park et al. [40] proposed\na structured pruning algorithm, named Kprune (Knowledge-preserving), which focused on preserving the\nuseful knowledge of the pre-trained model to minimize pruning errors through an iterative pruning process\nthat consisted of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving\nweight-tuning. Ma et al. [41] introduced an LLM pruning approach referred to as LLM-Pruner. This\nmethod employed structural pruning, selectively eliminating non-essential coupled structures guided by\ngradient information. The aim is to preserve the majority of the LLM's functionality to the fullest extent\npossible.\nSome of the discussion on the structured and unstructured pruning can be found in the Sec. 1. Other than\nthat, we point out one of the essential procedures, performing retraining or not after pruning, and exploring\nthis viewpoint through various prior work. The work belonging to the retraining-based group [37, 42, 41]\nhas the entire dataset ready for retraining on the compressed model. On the other hand, the retraining-free\nmethods [38, 40, 43] can skip this procedure as the model after pruning remains to be effective. In general,\ntraining or retraining on LLMs is known to be resource-intensive and retraining-free approaches should be\nfavored in this sense. An et al. [43] and Park et al. [40] saved the retraining step, but found some alternative\nways to enhance the model's effectiveness. For instance, An et al. [43] added additional bias terms to recover\nthe output feature maps using the baseline values. Park et al. [40] tuned the weights on the pruned model\nfor model improvement. Also, Kwon et al. [38] asked for label information to help with model improvement\neven if retraining after pruning is not needed.\nOverall, we understand that most LLMs have the models trained without the help of label information.\nThe label information for most LLM tasks is indeed hard to acquire. Therefore, we rely on unsupervised\npruning more than before. Nova et al. [30] proposed a gradient-free structured pruning framework to\nintegrate two ranking techniques representative ranking and data-driven ranking, without the help of labeled\ndata"}, {"title": "2.2. Feature Selection based on Mutual Information", "content": "In a sense, we can recognize the values of all network neurons as feature content. Pruning neurons\nis equivalent to feature selection in this case. In terms of feature selection, we aim to remove irrelevant\nfeatures [44] from the set to shrink the model size. The irrelevant features imply two meanings: the ones\nmay own redundancy or the ones help nothing in the focused prediction task from the model. We call the\nfirst one the unsupervised type and the second one the supervised type and further discuss the pruning\nstrategies of the two types.\nIf through the MI [32] estimation between different neurons, we may understand the neurons' relation\nand use the clue to prune the possible non-important neurons from the network. As mentioned before, we\ncan utilize MI to measure the relation either between two hidden neurons or between a hidden neuron and\nthe target information. The first is called the measurement of redundancy and the second is called the\nmeasurement of relevancy [44]\nLewis [45] is the first one to propose the MI-based feature selection method, estimating the MI between\na potential feature (or a neuron) and the label, we eliminate the neurons that have the MI result fall below\na pre-defined threshold. Peng et al. [46] proposed a criterion called minimal-redundancy-maximal-relevance\n(mRMR) to select useful features, which checked both the MI between candidate features and the label and\nthe MI between different neurons to decide what could be the features to keep. Apparently, we have to\npay attention to the trade-off between redundancy and relevance at the same time to make a final decision,\nwhich could be case by case. For instance, in a transfer learning type of scenario, we may focus more\non the relevance more because the distribution of the target task could be different from what we can\nfocus on the source task. Zhou et al. [44] proposed another approach called CCMI to integrate MI and\ncorrelation coefficient to measure how different features are related. Similarly, they also considered both\nthe redundancy and relevance at the same time such as picking up features with high relevance to the label\nand low redundancy between different features or hidden neurons. Fan et al. [47] applied the MI-based\ncomputation to feature or neuron selection then to model pruning. The redundant neurons got eliminated\nif the neuron set at layer L after the elimination owns the biggest MI value to the neuron set at layer L +1.\nIn the proposed method, we pay attention to redundancy more than relevance. Moreover, we care about\nthe different situations when a small or a large set of neurons is considered for pruning. Other than the\naforementioned issues, we utilize an alternative way to estimate the MI, which provides more precise pruning\nthan prior work."}, {"title": "2.3. Mutual Information Estimation", "content": "The MI estimation on deep learning networks is hard, if not intractable due to the large-scale of network\nstructures and data size. The classical binning-based estimator [13] considered quantizing neurons' output\nto estimate the corresponding probability distribution, which leads to at least three problems: (1) an\nappropriate decision on the bin size to ensure the estimation precision [48, 49], (2) probability distribution\nestimation needs a large amount of samples [49], (3) difficult computation for certain activation functions\nsuch as ReLU. Some other issues include the possible systematic errors [50] happened in the computation\nprocedure. We may utilize some hyperbolic functions (e.g., tanh) to deal with the last issue. Kraskov et\nal. [51] proposed a kNN distance-based MI estimation called KSG, to deal with a wide range of activation\nfunctions. However, this could rely on a wise decision on the number of neighbors. To deal with other issues,\nBelghazi et al. [52] proposed Mutual Information Neural Estimator (MINE), adopting a different network,\nusing the gradient descent to realize the MI estimation given high-dimensional random variables. Through\ntheir framework, both the dimensionality of neurons and the number of samples can be extended linearly for\nbetter estimation precision. However, the method is sensitive to the choice of network, and the converging\nspeed is slow for such a network. Wickstr\u00f8m et al. [33] improved from the result of Giraldo et al. [34] and Yu\net al. [35] and proposed a novel matrix or tensor-based estimation called R\u00e9nyi a-order entropy estimator,\ncan estimate MI given high-dimensional multivariate data without estimating the probability of random\nvariables that are involved in the MI computation.\nThe key to the R\u00e9nyi a-order entropy estimator is focusing on the estimation of the kernel width param-\neter. Based on that, we have choices between the supervised learning and unsupervised learning approaches.\nIn supervised learning, an optimal criterion [33] is used, while the Scott's rule [53] is considered for the\nunsupervised learning case. In a nutshell, they aligned the label kernel matrix and a kernel matrix from a\npre-specified layer to approximate the kernel width parameter. In Scott's rule [53], one can estimate the\nkernel width parameter by checking the data size and the dimensionality of the focused hidden layer. The\ncurrent approach to applying Scott's rule is focused on the estimation of the whole hidden layer [49]."}, {"title": "3. Proposed Method", "content": "In this section, we introduce the proposed method in detail. The goal of this work is to have an explainable\nmodel and prune the model based on such explainability. The key steps to achieve the goal include: finding\na way to confirm the explainability of the model, taking advantage of the model's explainability to prune\nneurons in the representation layer based on MI computation, and having an effective estimation of MI to\nproduce the final result. Before we go on to elaborate on the details of the proposed method, we introduce\nthe notations that shall be used in this work."}, {"title": "3.1. Notations", "content": "In the l-th transformer encoder, we have $K_e$ neurons in the fully-connected layer of FFN, which are\ndenoted by $Z_1, Z_2,..., Z_{K_e}$, and we use the random variables $Z_1, Z_2,..., Z_{K_e}$ to describe the value of those\nneurons (features) in the FC layer. That is, $Z$ refers to a neuron and $Z$ is the random variable to describe\nthe value on neuron $Z$."}, {"title": "3.2. Framework", "content": "Given a pre-trained LLM, we perform a fine-tuning procedure that is designed for a specific task, then\napply the proposed pruning to the fine-tuned model to obtain its compressed version. The compressed model\nis assumed to have similar behavior to its original model. The overall proposed methodology is shown in\nFig. 1"}, {"title": "3.3. The Proposed Pruning Methodology", "content": null}, {"title": "3.3.1. Working on Feed-forward Networks", "content": "In one typical design of transformers, we have Multi-Head Self-attention (MHA), followed by one Feed-\nforward Network in its encoder part. Moreover, each FFN contains two linear transformation layers and a\nGeLU activation in between."}, {"title": "3.3.2. Redundancy as Feature Selection Criteria", "content": "We adopt mutual information to measure the relationship between features. Based on the result, we\nprune features with a certain level of redundancy. In the fully connected layer of FFN, we randomly select\ntwo features represented by their corresponding random variables $Z_k$ and $Z_e$, and compute their mutual\ninformation as $I(Z_k; Z_e)$. If the values of $I(Z_k; Z_e)$ is large enough to show a certain degree of information\noverlap, we choose one to delete from the feature set.\nThe whole procedure of the pruning strategy is shown in Algorithm 1."}, {"title": "3.3.3. Clustering Strategy as a Scaling up Option", "content": "The pruning algorithm, as shown in Algorithm 1 may not scale well to a large set of neurons or goes\nquadratically in terms of the number of neurons in its computation. To bypass such a burden, we consider\na clustering-based procedure to perform the pruning in a group-based manner. In detail, we cluster features\nbased on their similarity while features with high similarity should go together. Before that, we decide\nthe number of clusters according to different choices of compression rates. When the clustering result is\nconfirmed, we choose one feature, which could be the one closest to each cluster centroid to be the one to\nkeep while all the other neurons of the same cluster should be eliminated after the procedure.\nThe mutual information is used to decide a metric for the clustering procedure. Given the pairwise\ndistances, we utilize the multidimensional scaling (MDS) [54] to find coordinates in a pre-specified dimen-\nsionality. In detail, given two features $Z_k$ and $Z_e$ we compute their mutual information $I(Z_k; Z_e)$ and a\nset of pairwise mutual information is transformed into pairwise distances. Afterward, Eq. 1 helps to find\ncoordinates given a pre-specified space of certain dimensionality.\n$d(Z_k, Z_e) = A exp(-I(Z_k; Z_e)),\n(1)$\nwhere A is a constant in case if we prefer a distance between 0 and 1. In the formula, the larger the mutual\ninformation between $Z_k$ and $Z_e$, the smaller the value $d(Z_k, Z_e)$ is. In the MDS-projected space, two close-\nby features $Z_k$ and $Z_e$ implies that they share larger mutual information $I(Z_k; Z_e)$. Moreover, two features\nwith their large mutual information $I(Z_k; Z_e) > T_r$ may end up in the same cluster and the pruning strategy\nin Algorithm 1 could suggest the removal of one of the features in the pair. That is, we group features into\na cluster if they own large mutual information. After that, only one feature per cluster is used to be the\nrepresentative once we obtain the grouping result. In the end, we have features in different clusters if the\nfeatures have their pairwise mutual information smaller than a threshold $T_r$."}, {"title": "3.3.4. Subsidiary Condition", "content": "The procedure may not produce a unique compression model because the solution to MDS and the\nselection of representatives may not always be the same. We may suggest some side condition to encourage\na decent compression result by trying M random seeds and choosing the best one out of them by the\nfollowing criteria.\nThe subsidiary condition is to minimize the difference between the original and the compressed mod-\nels. We use the Kullback-Leibler (KL) divergence to measure the difference between the original and the\ncompressed model by Eq. 2 if focusing on the representation of both models. Given the original model, we\naim to find a compressed model M that is closet to the original model in its representation distribution if\nmeasured by KL divergence.\n$M_{comp} = argmin_{M} D_{k_l}(p(z_o)||p(z_M)),\n(2)$\nwhere $D_{KL} (P_1 ||P_2)$ measures the KL divergence between two distributions $p_1$ and $p_2$, $z_o$ and $z_M$ denote\nthe representation of the original and the compressed model M, respectively; $p(z_o)$ and $p(z_M)$ are the\ndistributions of $z_o$ and $z_M$, respectively. A small $D_{KL}$ indicates a closer relationship between $p(z_M)$, the\nrepresentation for the compressed model and $p(z_o)$, the representation of the original model."}, {"title": "3.4. Estimate Method of Kernel Width Parameter of Hidden Neuron", "content": "To estimate the mutual information between $Z_k$ and $Z_e$, we compute\n$I(Z_k; Z_e) = S_\u03b1(A) + S_\u03b1(B) \u2013 S_\u03b1(A, B),\n(3)$\nwhere A and B are Gram matrices of $Z_k$ and $Z_e$, respectively. $S_\u03b1(A)$ and $S_\u03b1(B)$ denote the matrix-based\nR\u00e9nyi's \u03b1-order entropy, respectively. $S_\u03b1(A, B)$ denotes the joint entropy of $Z_k$ and $Z_e$.\nThe estimation is analogous to the estimation of the mutual information in a reproducing kernel Hilbert\nspace (RKHS), that is, solving the eigenvalues of the kernel matrix to estimate the entropy, and solve the\neigenvalues of the Hadamard (entry-wise) product of two kernel matrices to estimate the joint entropy. In\nthis computation, one of the key steps is to choose an appropriate kernel width parameter \u03c3 to have a decent\nestimation.\nTo have it focus on a single neuron-related calculation, we have to adjust the estimation by integrating\nthe optimal criterion and Scott's rule to estimate the kernel width parameter if one hidden neuron is focused.\nThe process is illustrated as follows.\nFirst, We apply Scott's rule to estimate the kernel width parameter $\u03c3^l$ of the focused hidden layer, we\nhave\n$\u03c3^l = \u03b3N^{-(4+d)},\n(4)$\nwhere N denotes the number of samples, d denotes the the number of hidden neurons, and \u03b3 is an empirically\ndetermined constant. Second, we adopt Eq. 5 to calculate the kernel matrix $K_\u03c3$ of hidden layer. The RBF\nkernel is written as:\n$K_\u03c3(x_i, x_j) = exp(-\\frac{||x_i-x_j||}{2\u03c3^2}),\n(5)$\nwhere $x_i$, $x_j$ denote data points, $||.||_F$ denotes the Frobenius norm, \u03c3 is the kernel width parameter. Third,\nwe align kernel matrix $K_\u03c3$ with kernel matrix $K_{\u03c3_n}$ (kernel matrix of hidden layer) by maximizing the kernel\nalignment loss between these two kernel matrices, the kernel alignment loss [55] is written as:\n$A(K_{\u03c3^l}, K_{\u03c3_n}) = \\frac{(K_{\u03c3^l}, K_{\u03c3_n})_F}{||K_{\u03c3^l}||_F ||K_{\u03c3_n}||_F},\n(6)$\nwhere $||.||_F$ and $(, )_F$ denote the Frobenius norm and inner product, respectively.\nThus, we choose our optimal $\u03c3_n$ as Eq. 7:\n$\u03c3_n^* = arg max_{\u03c3_n} A(K_{\u03c3^l}, K_{\u03c3_n}).\n(7)$"}, {"title": "4. Experiment Result", "content": "We conduct a few series of experiments to evaluate the effectiveness of the proposed model. The first\ngoal is to understand how accurate the mutual information estimation on the focused hidden neurons is.\nAfter that, we need to confirm the effectiveness of the proposed pruning method. Relative FLOPs are used\nto indicate the compression level of the model, the formula is expressed as:\nRelative FLOPS = $\\frac{FLOPs of pruned model}{FLOPs of original model}$\nThe smaller the value of Relative FLOPs, the higher the compression level of the model. We follow the\nwidely adopted definition of Relative FLOPs [56, 38]."}, {"title": "4.1. Experimental Settings", "content": "We evaluate the effectiveness of the proposed methods using the BERT-tiny model [57] on the General\nLanguage Understanding Evaluation (GLUE) [58] benchmark. The BERT-tiny is a (pretraining + fine-\ntuning) model from [57], which consists of one Embedding layer and two transformer encoder blocks, with a\nhidden size of 512 for the FC layer in FFN. The GLUE benchmark contains a collection of NLU tasks and\nwe fine-tuned it on five downstream tasks: Single-Sentence Task (SST-2 [59]), Similarity and Paraphrase\nTasks (STS-B [60], MRPC [61], and QQP [61]), and Inference Task (QNLI [61]). The batch size was set to\n8, 8, 8, 16, and 16 for these tasks, respectively. Additionally, the learning rate was set to 5\u00b710-4, 5\u00b710-4,\n3\u00b710-4, 3\u00b710-4, and 3\u00b710-4 for these tasks, respectively. Throughout all experiments, we trained the\nmodel using the AdamW optimizer [62] with B\u2081 = 0.9, \u03b22 = 0.999, \u20ac = 10-8, and conducted a total of 4\nfine-tuning epochs. The overall data statistics and corresponding evaluation metrics are shown in Table. 1.\nIn the estimation of mutual information settings, we random sample 1% of the number of samples per\ntask training dataset to calculate the mutual information, so we can know that the N in Eq. 4 is equal to\nthe random sample number. At the same time, in Eq. 3, we set a = 1.01, in Eq. 4, we set y =1, \u03b7 =512.\nThe batch size of the calculation process is set to 100, which is the same as [48]. Due to the randomness of\nMDS, we sample 500 random seeds and choose the best one based on Eq. 2."}, {"title": "4.2. The Results of Model Pruning", "content": "In this part, we compare the proposed method to some supervised learning approaches [39, 38, 40],\nself-supervised learning method [39], and unsupervised learning methods [16, 30], overall three types of\ncomparisons. The proposed method is similar to the weight-magnitude approach [16] and KCM [30], in\nthe sense that all need no labeled data in the pruning procedure. Moreover, the proposed method is a\nretraining-free approach, which follows the convention from [38, 30, 40]. It is different from the approach\nadopted on [38, 30, 40] where weight-tuning on the left-out (unpruned) neurons is necessary to confirm the\nmore-than-acceptable network effectiveness. Note that the random strategy has its output as an average of\nten trials to reveal the general behavior of the strategy. In all experiments, we fix the pruned percentage to\nbe 1%.\nIn Table. 2, we show that the proposed model can maintain the performance of the original model for\nvarious tasks given the relative FLOPs are set to be 40%. Let us compare the proposed method to the\nWeight-Magnitude [16] and KCM [30] approaches, both belonged to the category of unsupervised methods,\nand own some similarities to the proposed method. The proposed method performs superior to the Weight-\nMagnitude method on the tasks SST-2 and the QNLI while slightly worse on the other three tasks. On the\nother hand, we also compare the proposed method to another self-supervised method like Yang et al. [39].\nThe proposed method is superior to other methods on all but the MRPC and QQP tasks. To compare to\nthe category of supervised method, we evaluate the superiority of the proposed method to other supervised\nmethods. In this category, the proposed method performs better than the other two methods on the SST-2\nand QNLI tasks. For instance, the proposed method is better than Mask-tuning [38] on the SST-2 and\nQNLI tasks, and better than Yang et al. [39] on the SST-2 task. For other tasks, the proposed method and\nthe two supervised methods perform similarly.\nTo clearly show the model performance under different compression rates, we consider the following set\nof comparisons given five tasks. As shown in Fig. 2, we demonstrate the result for every 1% change of\nthe compression rate. For all the tasks other than STS-B, the proposed method performs better than the\nrandom strategy. If compared to the unsupervised learning method proposed by Li et al. [16], the proposed\npruning method shows better performance on all except the QQP dataset, and both perform similarly on\nthe QQP dataset. Compared to another method from the unsupervised group, we consider KCM [30]. In\nthis case, both perform similarly on all four except the QQP dataset. The performance on SST-2 is slightly\nbetter if by the proposed method.\nWhen checking the self-supervised method such as Yang et al. [39], the proposed method shows its"}, {"title": "4.3. Mutual Information between Hidden Neurons Estimation", "content": "In this part of experiment, we take a close look about the MI estimation between hidden neurons. We\ncompare the adopted approach and the estimation based on the kernel width parameter tuning (Scott's\nrule). Of course, no true value can be obtained to confirm how accurate the MI estimation is, we provide\nthe study on the model test accuracy to provide another support for the pruning effectiveness.\nAs shown in FIG. 3, the proposed method offers better performance in terms of prediction accuracy than\nother methods in most cases. We have a clear advantage on the QQP dataset, across all compression rates.\nWorking on the QNLI dataset, the proposed method still keeps an advantage in most cases. On the SST-2\nand MRPC datasets, both (the proposed method and Scott's rule) show similar results. At the last, the\nproposed method may perform poorly on the STS-B dataset, for at least the low compression rate cases."}, {"title": "4.4. Ablation Study", "content": "We take turns studying some alternative approaches to see the possibilities of further improvement from\nthe proposed method."}, {"title": "4.4.1. Mutual Information vs. Pearson Correlation Coefficient", "content": "First, we consider an alternative measure to describe how two variables are related to each other, when\nwe need to measure the relation between two groups of neurons. For instance, we may choose Pearson\ncorrelation coefficient (PCC) to substitute the role of MI in Algorithm 1 to guide the pruning procedure.\nThe result from both, the MI-based"}]}