{"title": "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models", "authors": ["Logan Cross", "Violet Xiang", "Agam Bhatia", "Daniel L.K. Yamins", "Nick Haber"], "abstract": "Multi-agent reinforcement learning (MARL) methods struggle with the non-stationarity of multi-agent systems and fail to adaptively learn online when tested with novel agents. Here, we leverage large language models (LLMs) to create an autonomous agent that can handle these challenges. Our agent, Hypothetical Minds, consists of a cognitively-inspired architecture, featuring modular components for perception, memory, and hierarchical planning over two levels of abstraction. We introduce the Theory of Mind module that scaffolds the high-level planning process by generating hypotheses about other agents' strategies in natural language. It then evaluates and iteratively refines these hypotheses by reinforcing hypotheses that make correct predictions about the other agents' behavior. Hypothetical Minds significantly improves performance over previous LLM-agent and RL baselines on a range of competitive, mixed motive, and collaborative domains in the Melting Pot benchmark, including both dyadic and population-based environments. Additionally, comparisons against LLM-agent baselines and ablations reveal the importance of hypothesis evaluation and refinement for succeeding on complex scenarios.", "sections": [{"title": "Introduction", "content": "A primary goal of AI research is to create autonomous agents that act adaptively in rich embodied social worlds. Multi-agent reinforcement learning (MARL) methods suffer from various drawbacks in these regimes, including high sample complexity, poor generalization to agents not seen in training, and limited reasoning capabilities.\nLLMs are not only powerful reasoners and in-context learners, but they are also particularly suited for these tasks given the utility of language for scaffolding ToM [Astington and Baird, 2005, de Villiers, 2007, 2021, Kosinski, 2023, Gandhi et al., 2024]. We advance recent research on LLM-based agents in embodied social worlds [Wang et al., 2023b,a, Brohan et al., 2023a, Park et al., 2023] with the Hypothetical Minds model. Our agent produces adaptive policies in competitive, cooperative, and mixed-motive multi-agent scenarios with concealed information by generating hypotheses about other agents' latent states with a Theory of Mind module.\nThe Theory of Mind module in Hypothetical Minds generates hypotheses about other agents' strategies, goals, and capabilities to facilitate effective coordination or counter-strategies. High-level plans are subsequently passed to a subgoal module to sequence together embodied action plans. The ToM module simultaneously evaluates multiple hypotheses until a hypothesis provides a sufficient explanation of the data [O'Doherty et al., 2021, Tenenbaum et al., 2006, Gershman et al., 2015]."}, {"title": "Related Work", "content": "A burgeoning area of research involves building autonomous agents rooted in large language models Wang et al. [2023b], Sumers et al. [2023]. This involves deploying LLMs as central controllers across many different domains by leveraging their extensive background knowledge from training."}, {"title": "LLM-based Agents", "content": "Applications span a wide range from equipping LLMs with external tools to interface with databases and APIs [Schick et al., 2023, Shen et al., 2023, Qin et al., 2023, Ge et al., 2023, Yao et al., 2022] to using them for high-level planning and control in robotics [Huang et al., 2022, Brohan et al., 2023b, Rana et al., 2023, Brohan et al., 2023a]. The most relevant branch of this research direction includes works where LLMs are used as planners in embodied virtual environments. Voyager autonomously builds complex skills in Minecraft by storing and retrieving behaviors in a skill library of executable code and uses the skill library to solve progressively harder tasks [Wang et al., 2023a]. Octopus and Jarvis-1 use vision-language models to plan long horizon behaviors in virtual environments [Yang et al., 2023, Wang et al., 2023d]. Dynalang and DECKARD use language to guide world modeling for model-based RL policies in virtual environments [Lin et al., 2023, Nottingham et al., 2023]. In this work, we use an LLM for long horizon high-level planning and predicting the future states of other agents in multi-agent environments.\nPrevious papers have also incorporated LLM-based agents into embodied multi-agent environments. Park et al. [2023] introduce a interactive simulation of a rich social environment, where each agent autonomously selects goals and builds relationships with others. We extend the cognitive module framework developed in this work for multi-agent environments of varied dynamics. SAMA uses an LLM to plan out sequences of subgoals for language-based goal-conditioned RL policies in environments requiring multi-agent coordination Li et al. [2023]. Another study builds cooperative embodied agents, by using an LLM for planning and communication between agents Zhang et al. [2023b]. ProAgent develops a method for improving zero-shot coordination in Overcooked by using an LLM to infer the intentions of teammates based on the present state Zhang et al. [2023a]. In the present work, we present a generalizable method that addresses the challenge of inferring other agents intentions' across a spectrum of social dynamics."}, {"title": "Reasoning and Hypothesis Search with LLMs", "content": "LLMs have shown impressive reasoning abilities, augmented by Chain-of-Thought methods that scaffold the thought process [Wei et al., 2022, Zhang et al., 2023c]. [Wang et al., 2023c] investigates LLMs' inductive reasoning by generating and evaluating hypotheses on the Abstraction and Reasoning Corpus (ARC), while [Qiu et al., 2023] refines LLM-generated hypotheses using task-specific symbolic interpreters. Similarly, we generate, evaluate, and refine hypotheses based on feedback, computing values for each hypothesis by predicting another agent's goals. STaR [Zelikman et al., 2022] also learns from feedback by finetuning language models on rationales that produced correct answers. [Akata et al., 2023] assessed LLMs' ability to reason in matrix games requiring ToM, showing mixed results. We demonstrate that asking the LLM to predict the opponent's actions improves results, highlighting the importance of ToM reasoning in game-theoretic tasks."}, {"title": "Cognitive models in partially-observable environments", "content": "Our method has analogs in the computational modeling of animal and human decision-making, and our work adds to emerging promising evidence that LLMs can operate as cognitive models [Binz and Schulz, 2023]. Numerous methods have been proposed for generating hypotheses about hidden or latent states in partially-observable settings. Methods from Bayesian nonparametric stats, such as the Chinese restaurant process, suggest that when animals and humans are learning they generate an unbounded number of possible latent causes of the observed data, until a satisfactory explanation is found [Gershman et al., 2015]. This framework explains a wealth of behavioral phenomena from categorization [Anderson, 1991, Sanborn et al., 2010, Gershman and Niv, 2013, Gershman et al., 2015], Pavlovian conditioning, and more. Our LLM-based agent similarly iteratively generates and refines hypotheses about the opponent's strategy, until a explanation aligns with the observed data's likelihood. Mixture models and mixture of experts models are similarly useful for modeling the human decision-making process under uncertainty, by arbitrating between multiple hypotheses in proportion to the reliability of their predictions [Frank and Badre, 2012, O'Doherty et al., 2021, Archambeau et al., 2023]. Researchers have extended this POMDP modeling framework to multi-agent settings by constructing models that explicitly model other agents' beliefs and goals when interacting with them, handling uncertainty over this hidden information with a Bayesian framework [Baker et al., 2017, Rusch et al., 2020]. Our framework also explicitly prompts a LLM to infer its opponent's goals/strategy and weighs the uncertainty over these inferences with a hypothesis evaluation mechanism. This mechanism is also inspired by cognitive models and employs a Rescorla-"}, {"title": "Method", "content": ""}, {"title": "Partially-observable Markov games", "content": "Our method is directly applicable to any multi-agent environment where states are partially observable and agent(s)' policies are hidden. We formally define this as a Markov game for N players in a partially observable setting. Let the finite set S represent the possible states of the game. Each player i receives observations given an observation function $x^i : S \\rightarrow O$, representing their limited point of view. Additionally, each player i can take actions from their action space $A^i$, and when all players choose actions $(a^1, ..., a^N) \\in A^1 \\times ... \\times A^N := A$, the state transitions according to a probability distribution $T : S \\times A \\rightarrow D(S)$. The reward function for each player i is represented as $r^i: S \\times A \\rightarrow \\mathbb{R}$, mapping the current state and joint actions to a real-valued reward."}, {"title": "Hypothetical Minds Model", "content": "The Hypothetical Minds model consists of several cognitive modules that altogether form an embodied LLM agent (Figure 1). The egocentric observations are represented by a textual map/state representation, which is added to a memory system after every step. The memory system also logs rewards and other important state information like the inventories from previous interactions in Running With Scissors. Two cognitive modules depend on an LLM, a Theory of Mind module and a Subgoal module, which output high-level goals and action plans respectively. An action planner takes an action plan (i.e. \"move to coordinate (13, 5)\") and creates a sequence of actions that achieves that action plan with a pathfinding algorithm. Each cognitive module is explained in more detail in the Appendix. Below we describe the key novel contributions of our method that implement hierarchical planning."}, {"title": "Theory of Mind Module", "content": "In multi-agent environments, other agents' behavior can be influenced by various latent variables, such as their strategies, goals, competence levels, and locations in space. These latent variables are often not directly observable and must be inferred from the agent's observations. We represent these latent variables as a multidimensional space $\\Theta = \\theta_1, \\theta_2, ..., \\theta_m$, where each dimension $\\theta_i$ corresponds to a specific latent variable. The ToM module maintains a set of hypotheses $H = h_1, h_2, ..., h_n$ in its working memory, where each hypothesis $h_t$ is a natural language representation of a belief about the latent variables $h_t = p(\\Theta)$ (Figure 2). A hypothesis at time t is generated by asking an LLM to infer another agent's strategy, conditioned on HM's memory M of important past observations O. Additionally, the LLM is shown the top k valued previously generated hypotheses, such that it can perform hypothesis refinement (see Appendix and code for more details and prompts):\n$h_t = LLM(M, h_{<t})$\nwhere M is a memory buffer storing information about an agent's actions, observations, and rewards. Each hypothesis $h_i$ is scored based on how well it predicts the other agent's future behavior, noted here formally as a distribution of trajectories $p(T)$. We formalize this scoring mechanism using a likelihood function $p(T|h_i)$ representing the probability of an agent exhibiting trajectory $\\tau$ given the hypothesis $h_i$. The best hypothesis $h^*$ is selected using the Maximum a Posteriori (MAP) estimate:\n$h^* = \\arg \\max_{h_i \\in H} p(h_i|t) = \\arg \\max_{h_i \\in H} \\frac{p(T|h_i)p(h_i)}{p(T)}$\nwhere $p(h_i)$ is the prior probability of hypothesis $h_i$ and $p(r)$ is the marginal probability of the observed action a and has no effect on the argmax. The likelihood is approximated by a hypothesis evaluation mechanism described below. The LLM predicts the other agent's future behavior con-ditioned on each hypothesis separately. Hypotheses leading to correct predictions will have higher values reflecting higher likelihoods. The prior $p(h_i)$ corresponds both to the background knowledge embedded in the weights of an LLM from pretraining and to the refinement mechanism that shows the top valued hypotheses to the LLM when the LLM generates a hypothesis. By continuously updating and selecting the best hypothesis based on observed information, the ToM module can effectively infer the latent variables governing the other agents' behavior and adapt its own strategies accordingly."}, {"title": "Hypothesis Evaluation", "content": "Drawing on cognitive modeling approaches [Rescorla, 1972, Daw and Tobler, 2014], multiple hypotheses are scored with a value system $V_{h_i} = E[r]$ where r reflects intrinsic reward based on the accuracy of the predictions the hypothesis generates. We compute self-supervised intrinsic rewards bootstrapped from the LLM's own predictions. Let $\\phi(\\tau)$ be a particular behavior, a feature from an observed trajectory, and $\\hat{\\phi}(\\tau)$ be the predicted behavior by the LLM, such as the inventory played by an agent in Melting Pot environments. The intrinsic reward function $r_i$ can then be defined as:\n$r_i = \\begin{cases} c & \\text{if } \\phi(\\tau) = \\hat{\\phi}(\\tau) \\\\ -c & \\text{if } \\phi(\\tau) \\neq \\hat{\\phi}(\\tau) \\end{cases}$\nwhere c is a hyperparameter. $V_{h_i}$ is then dynamically updated with a Rescorla Wagner update rule [Rescorla, 1972], expressed as:\n$\\delta = r_i - V_{h_i}$\n$V_{h_i} \\leftarrow V_{h_i} + \\alpha \\cdot \\delta$\nmodulated by learning rate $\\alpha$ via a prediction error $\\delta$. The learning rate dictates how much to weigh recent interactions, a useful property when playing against evaluation scenarios where agents change their strategy within an episode. When the value of a hypothesis meets a threshold $V_{thr}$, the TOM module marks the hypothesis as validated and uses this hypothesis to condition high-level plans (using the highest-valued one if multiple pass the threshold). This hypothesis will then continue to be used for planning until it no longer makes good predictions and its value subsequently falls below the threshold. If no hypothesis meets the threshold, then the latest generated hypothesis (with the most updated information) is used for conditioning high-level plans."}, {"title": "Conditioning High-Level Plans", "content": "The ToM module then conditions its high-level plans on the inferred latent variables represented by the hypotheses. A high-level plan z is a natural language description of HM's overall strategy, goal, or intention, conditioned on the best hypothesis $h^*$ and memory of past events:\n$z = LLM(M, h^*)$\nBy conditioning the high-level plans on the hypotheses, HM can adapt its strategy based on its understanding of the other agents' latent states."}, {"title": "Subgoal Module", "content": "Finally, the Subgoal module selects a sequence of subgoals. Let $g = g_1, g_2,..., g_k$ be a sequence of subgoals, where each subgoal $g_i$ is an action or short sequence of actions that the agent needs to take to achieve the high-level plan z:\n$g = LLM(O, M, z)$\nThe sequence is generated by conditioning the LLM on the high-level plan, observations, and memory, and prompt to achieve the high-level plans. The LLM outputs a sequence of specified subgoal function calls, which are then parsed and mapped to the corresponding actions in the environment by a hardcoded Action Planner (see Appendix)."}, {"title": "Experiments", "content": "Here we investigate the following to analyze the generalizability and scalability of our method:\nQ1. How does Hypothetical Minds perform compared to LLM agent and RL baselines in embodied competitive zero-sum environments?\nQ2. How does Hypothetical Minds perform compared to LLM agent and RL baselines in a collaborative domain that requires adaptation to a partner's role and competence?\nQ3. How does Hypothetical Minds perform compared to LLM agent and RL baselines in a mixed-motive setting?\nQ4. Does the Hypothetical Minds agent scale effectively to environments with larger populations of agents?\nQ5. How do the different components of the Hypothetical Minds agent and the Theory of Mind module contribute to its overall performance?\nWe directly test our LLM-based agent on the evaluation scenarios in four Melting Pot environments (Fig. 3). The key evaluation method is to test agents against different bots with various policies. Across environments, this consists of 30 distinct evaluation scenarios. Crucially, our agent has no knowledge about which strategies they may be playing in the prompts given. Strategies have to be ascertained online within an episode via in-context learning."}, {"title": "Baselines", "content": "ReAct synergizes reasoning and acting in language models, allowing them to generate both reasoning traces and task-specific actions in an interleaved manner [Yao et al., 2022]. Reflexion includes three main components: an Actor module that generates actions and text, an Evaluator that scores these actions, and a Self-Reflection module that uses the evaluations to provide constructive feedback stored for subsequent use [Shinn et al., 2024]. PlanReAct To provide a hierarchical baseline to test against our hierarchical model, we include the PlanReAct architecture introduced in [Liu et al., 2023]. This structure allows the agent to plan before interacting with the environment. PPO is a model-free RL baseline [Schulman et al., 2017] and we train agents in a population of models with the same parameters. Prompts and architectures are shared across baselines to provide a fair comparison and natural ablations of our model. The Subgoal module provides a baseline actor shared between LLM baselines, and the only difference between PlanReAct and Hypothetical Minds is that high-level planning is mediated by the Theory of Mind module, including hypothesis generation, evaluation, and refinement."}, {"title": "Competitive Environments", "content": "Running With Scissors in the Matrix Repeated (RWS). A zero-sum competitive environment with two players moving around a map and collecting yellow, purple, or blue resources that correspond to rock, paper, and scissors respectively. Zapping your opponent causes an interaction, with one agent getting positive reward and the other agent getting an opposite negative reward according to the inventories of resources picked up by each player, mirroring the rock, paper, scissors matrix game.\nRWS presents nine distinct evaluation scenarios, which range in complexity. These include three straightforward strategies where opponents consistently play rock, paper, or scissors. The remaining scenarios introduce more dynamic and adaptive strategies (see Appendix for details and full list). Therefore in order to succeed on the scenarios, an agent needs to correctly infer the strategy and exploit it. Against the simple policies, the agent should play the same type of inventory every round rather than playing randomly or anticipating a change in its opponent's policy. In contrast, success against the adaptive strategies demands not only the anticipation of the opponent's next move based on personal previous plays but also selecting the most advantageous counter."}, {"title": "Collaborative Cooking Asymmetric", "content": "In the Collaborative Cooking: Asymmetric environment, two players on distinct sides of a divided kitchen must collaborate to efficiently cook tomato soup. The layout provides distinct advantages to each side-one side is closer to the goal delivery but farther from the tomato dispenser, and vice versa for the other side. To maximize rewards, the two players should specialize based on their proximity to resources: one handles tomato dispensing and delivery, and the other manages dish retrieval and soup delivery. Evaluation scenarios challenge the focal agent to demonstrate dual forms of adaptation: adjusting to the specialized role dictated by their side of the kitchen and to the varying competence of their partner, from skilled and specialized to entirely unresponsive.\nAgain, Hypothetical Minds achieves higher rewards than the baselines on every scenario (Figure 3). Interestingly, HM performs significantly better than the baselines on the scenarios where there is a functional partner (Appendix Table 1). This suggests that if there is value in a partner, HM can take advantage of this and adapt its behavior accordingly, highlighting the model's usefulness for complex, dynamic environments where cooperative interaction is crucial. The LLM baselines perform relatively well with the negligent partner, where success hinges on repeatedly executing"}, {"title": "Mixed Motive: Prisoner's Dilemma in the Matrix Repeated", "content": "In the Prisoner's Dilemma in the Matrix (PD) environment, agents navigate a similar grid world to RWS and collect resources corresponding to cooperation or defection in the iterated prisoner's dilemma game. The payoff matrix incentivizes mutual defection in a single interaction, but the highest total welfare is achieved through mutual cooperation across an episode.\nHypothetical Minds achieves the highest reward overall and in 5/10 scenarios (Figure 3, Appendix Table 1). This highlights its ability to perceive the background agent's strategy and adapt accordingly. HM outperforms the baselines relatively more with dynamic partners. With tit-for-tat for example (scenarios 5 and 6), Hypothetical Minds achieves the highest score, by engaging in more consistent cooperation while demonstrating some forgiveness to avoid cycles of alternating defection, a pattern that plagues baselines. In scenarios 8 and 9, Hypothetical Minds showcases a capacity for \"corrective punishment.\" By initially defecting against exploitative partners, it persuades them to switch to conditional cooperation. The agent then shifts to a forgiving cooperation strategy to maintain a mutually beneficial equilibrium.\nRelative to the other three environments tested, HM performs comparably to the baselines in PD, and all LLM baselines perform similarly in the majority of the scenarios. This similarity likely arises from their convergence on simple heuristic strategies, such as consistently defecting against static bots (the optimal approach) and employing a tit-for-tat strategy against adaptive bots. Again, this highlights the importance of focusing on the more dynamic strategies as previously discussed. All models struggle with the grim reciprocators, as testing defection is needed to perform well on the task overall, but is not tolerated by these bots."}, {"title": "Ablations", "content": "As previously discussed, we compared baseline LLM agents with our models to perform natural ablations, maintaining similar prompts but varying information flows. We also compare base LLMs between GPT4 (default), GPT 3.5, and Llama3 (Figure 5). GPT4 performs by far the best across environments, and GPT 3.5 performs poorly. Llama3's performance lies in between these two on RWS Repeated and Collaborative Cooking, and slightly below GPT 3.5 on Prisoner's Dilemma. Due to input context limitations, we could not run Llama3 on RWS Arena.\nWe conducted an additional detailed ablation analysis on the ToM module for the challenging environment, RWS Repeated. Vanilla Mind Prompting (VMP): Utilizes a single API call for all ToM module steps, excluding hypothesis evaluation and refinement (see Appendix and code for more"}, {"title": "Conclusion", "content": "Here we evaluate the Hypothetical Minds model on challenging multi-agent environments with varied dynamics. One limitation of our method is the human in the loop component necessary to set up the scaffolding and prompting for the agent. Additionally, knowledge of the game rules and mechanics are listed in the prompts. An avenue for future research is learning these concepts and the appropriate types of scaffolding within the cognitive modules autonomously from environmental feedback."}, {"title": "A Appendix", "content": ""}, {"title": "Environments", "content": ""}, {"title": "Running With Scissors Repeated", "content": "Specifically, here we evaluate our model on the Running With Scissors in the matrix: Repeated environment (RWS) in the Melting Pot multi-agent decision-making benchmark [Agapiou et al., 2022]. This is a zero-sum competitive environment with two players moving around a map and collecting yellow, purple, or blue resources that correspond to rock, paper, and scissors respectively. In addition to movement, the agents have an action to fire an \"interaction\" beam which initiates a duel with the other player when that player is within range of the beam. An interaction results in one agent getting positive reward and the other agent getting an opposite negative reward according to the inventories of resources picked up by each player. Specifically a player will collect an inventory, which is only observable by that player:\n$p = (p_{\\text{yellow}}, p_{\\text{purple}}, p_{\\text{blue}})$.\nReward is determined by matrix multiplication operations mirroring the rock, paper, scissors matrix game:\n$r_{\\text{row}} = v_{\\text{row}}^T A_{\\text{row}} v_{\\text{col}}, \\quad r_{\\text{col}} = -r_{\\text{row}}$\nwhere $v_i = \\frac{p_i}{\\sum_{j=1}^3 p_j}$, and\n$A_{\\text{row}} = \\begin{bmatrix} 0 & -10 & +10\\\\ +10 & 0 & -10\\\\ -10 & +10 & 0 \\end{bmatrix}$\nThe partially-observable input in Melting Pot consists of a 5x5 window around the agent such that it can see three grids in front of itself and one behind it, and two on each side."}, {"title": "Scenarios", "content": "Description of scenarios for each substrate are reproduced directly from [Agapiou et al., 2022]:\nSC0: Versus mixed strategy opponent. Here the focal agent must defeat an opponent that was trained to play a pure strategy: either rock, paper, or scissors. However, the specific opponent is sampled at test time so it could be any of those. All opponents commit strongly to their choice, aiming to collect at least three resources before interacting. To defeat them, the focal agent should scout out which pure strategy its opponent is playing and then collect the resources to implement its counter strategy. Since this is a one-shot interaction, success requires the focal agent to pay close attention to which resources are missing since they provide a clue to which strategy their opponent is implementing."}, {"title": "Running With Scissors Arena", "content": "We also evaluate our model on Running With Scissors in the matrix: Arena environment in the Melting Pot multi-agent decision-making benchmark [Agapiou et al., 2022]. This environment has the same dynamics as Running With Scissors in the matrix: Repeated with the main exception being that their are 8 players in this substrate playing on a larger 25 by 24 matrix with a 11 by 11 observability window, skewed towards viewing more in front of the agent. All scenarios under this agent represent one focal resident (agent) playing against 7 others with varying fixed and dynamic policies, attempting to maximize reward decided by the payoff matrix equivalent to the one in Running with Scissors Repeated."}, {"title": "Scenarios", "content": "SC0: Versus a background population containing bots implementing all three pure strategies. Here one focal player joins seven from the background population. The background population contains bots who implement all three pure strategies: rock, paper, and scissors. They may either commit to their strategy moderately (aiming to collect three resources before interacting) or more strongly (aiming to collect five). The task for the focal agent is to watch its opponents, see what strategy one of them is implementing, and act accordingly.\nSC1: Versus gullible bots. Here one focal player joins seven from the background population. The background population consists entirely of weak bots who were trained to best respond to agents playing pure strategies. They are weak opponents.\nSC2: Versus mixture of opponents who play rock and some who flip to scissors after two interac-tions Here one focal player joins seven from the background population. The focal player should pay attention to what each prospective partner has collected since 2/3 of them play rock while 1/3 play scissors after the first two interactions. Choosing paper to best respond to rock is a bad choice if accidentally paired with an opponent playing scissors.\nSC3: Versus mixture of opponents who play paper and some who flip to rock after two interactions. Like SC2 but with bots playing paper and bots switching from paper to rock.\nSC4: Versus mixture of opponents who play scissors and some who flip to paper after two in-teractions. Like SC 2 but with bots playing scissors and bots switching from scissors to paper.\nSC5: Visiting a population of pure paper bots. Here one focal player joins seven from the background population. All seven background bots play paper so the focal player can get a high score by playing scissors.\nSC6: Visiting a population of pure rock bots Here one focal player joins seven from the background population. All seven background bots play rock so the focal player can get a high score by playing paper.\nSC7: Visiting a population of pure scissors bots. Here one focal player joins seven from the background population. All seven background bots play scissors so the focal player can get a high score by playing rock."}, {"title": "Prisoners Dilemma Repeated", "content": "The Prisoners Dilemma in the Matrix Repeated environment is a mixed-motive one where two individuals collect resources that represent 'defect' (red) or 'cooperate' (green) and compare inven-"}, {"title": "Scenarios", "content": "SC0: Partner may play either cooperate or defect The optimal strategy is simply to unconditionally defect. However, given that the focal doesn't know the strategy of the background player, a good strategy is more subtle. A reasonable strategy is to be a grim reciprocator cooperator, which would cooperate with the cooperator, and defect to the defector. Alternatively the focal player might try to ascertain whether the background player is exploitable. Doing so, however, carries a risk, for if the background player were to be a Grim reciprocator (like in other scenarios), this would cause them to defect for the rest of the episode.\nSC1: Partner typically plays cooperate. The optimal strategy is simply to unconditionally defect. The same considerations about uncertainty of the background player's strategy from Scenario 0 apply here.\nSC2: Partner typically plays defect The optimal strategy is simply to unconditionally defect. However, because the focal player doesn't a priori know the strategy of the background player, they must first try to find out their strategy. This can be done by looking at which resources they collect or by paying attention to the results of the first few interactions. Once the focal has identified its background partner is defecting then it may have confidence that it should defect as well. The focal player should also consider the possibility that the background bot is corrigible, i.e. that it could be persuade to switch from defection to cooperation. This is not the case here but the background populations used in SC 8 and SC are corrigible.\nSC3: Partner is a hair-trigger grim reciprocator, i.e. one who initially cooperates but, if defected on once, will retaliate by defecting forever after. The optimal strategy is simply to cooperate. Grim reciprocator background players are non-exploitable, and there is no way to know how they will react to a defection ahead of time. Because of this uncertainty, testing for exploitability can lead to poor performance of the focal player. Conditional cooperators who cooperate first but retaliate if defected on should achieve a high score.\nSC4: Partner is a two-strikes grim reciprocator, i.e. one who initially cooperates, but if defected on twice, will retaliate by defecting forever after The optimal strategy is simply to cooperate. Grim reciprocator background players are non-exploitable, and there is no way to know how they will react to a defection ahead of time. Because of this uncertainty, testing for exploitability can lead to poor performance of the focal player. In principle, it would be possible to defect once against the background player leading to higher reward. But since it is not possible to know the background player is a two-strikes grim reciprocator, and testing it against a hairtrigger grim reciprocator leads to defection, in practice is better simply to cooperate. Conditional cooperators who cooperate first but retaliate if defected on should achieve a high score.\nSC5: Partner is a tit-for-tat conditional cooperator The optimal strategy is simply to cooperate. Defecting against a tit-for-tat agent, even occasionally, might lead to miscoordinated interac-tions where one player cooperates and the other defects, in an alternating way. Forgiveness is one way to break out of such cycles of recrimination. Conditional cooperators who cooperate first but retaliate if defected on should also be forgiving to ensure they do well in this scenario.\nSC6: Partner is a tit-for-tat conditional cooperator who occasionally plays defect instead of cooperate. Like the previous scenario, except the tit-for-tat background player occasionally will defect instead of cooperate. This is known as trembling hand in game theory. A strict tit-for-tat focal player would occasionally fall into miscoordinated interactions with the background player resulting in alternating cooperation and defection. As in SC5, focal"}, {"title": "Methods", "content": ""}, {"title": "Textual Map", "content": "Pixel images of the global state are preprocessed into a text-based state representation. The images are divided into 8x8 patches, each corresponding to a cell within the Melting Pot grid that entities can occupy. Each patch may represent one of four types: an agent, a resource, a wall, or a blank space. The patches are labeled by comparing them to manually labeled reference patches of each type of entity, including the numerous possible body orientations and hat colors an agent can embody in Running With Scissors. The global state can then be fully represented in text by coordinates in a 23x15 grid and the entity label at each coordinate. Egocentric states are then created from this"}, {"title": "Memory", "content": "The memory system consists of two parts. The first data structure appends the observed states in the previous step to lists of each entity type in a tuple with the step it was observed. For example: 'yellow_box': [((13, 3), \u2018Step: 1087'), ((13, 4), 'Step: 1087'), ((7, 3), 'Step: 1091')]. The LLM is prompted that its memory can be outdated and therefore should take the step it was last observed into account.\nThe second data structure in the memory system contains a list of the agent's inventories and rewards from the interactions that occurred so far. This specific information, distinct from the previously observed states, is relayed to the ToM module."}, {"title": "Theory of Mind Module", "content": "The Theory of Mind Module is queried periodically after discrete events. For the * in the Matrix substrates", "substrates": "n1. Record the observed behavior from the other agent's trajectory ($\\tau$). Here this refers to whether they played rock, paper, or scissors, the argmax of the inventory (or cooperate/defect in Prisoner's Dilemma). Since the opponent's inventory is never observed, we have to estimate it given the inventory Hypothetical Minds played and the reward it received. Thus, we ask the LLM to estimate the opponent's inventory given this information, and note the output as the empirical opponent's inventory.\n2. Evaluate Hypotheses about opponent's strategy. In the previous interaction, the top k hypotheses are used to generate predictions about the opponent's next inventory $\\phi(\\tau)$. These argmax of the inventory predictions are"}]}