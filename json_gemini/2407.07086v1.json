{"title": "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models", "authors": ["Logan Cross", "Violet Xiang", "Agam Bhatia", "Daniel L.K. Yamins", "Nick Haber"], "abstract": "Multi-agent reinforcement learning (MARL) methods struggle with the non-stationarity of multi-agent systems and fail to adaptively learn online when tested with novel agents. Here, we leverage large language models (LLMs) to create an autonomous agent that can handle these challenges. Our agent, Hypothetical Minds, consists of a cognitively-inspired architecture, featuring modular components for perception, memory, and hierarchical planning over two levels of abstraction. We introduce the Theory of Mind module that scaffolds the high-level planning process by generating hypotheses about other agents' strategies in natural language. It then evaluates and iteratively refines these hypotheses by reinforcing hypotheses that make correct predictions about the other agents' behavior. Hypothetical Minds significantly improves performance over previous LLM-agent and RL baselines on a range of competitive, mixed motive, and collaborative domains in the Melting Pot benchmark, including both dyadic and population-based environments. Additionally, comparisons against LLM-agent baselines and ablations reveal the importance of hypothesis evaluation and refinement for succeeding on complex scenarios.", "sections": [{"title": "1 Introduction", "content": "A primary goal of AI research is to create autonomous agents that act adaptively in rich embodied social worlds. Multi-agent reinforcement learning (MARL) methods suffer from various drawbacks in these regimes, including high sample complexity, poor generalization to agents not seen in training, and limited reasoning capabilities.\nLLMs are not only powerful reasoners and in-context learners, but they are also particularly suited for these tasks given the utility of language for scaffolding ToM [Astington and Baird, 2005, de Villiers, 2007, 2021, Kosinski, 2023, Gandhi et al., 2024]. We advance recent research on LLM-based agents in embodied social worlds [Wang et al., 2023b,a, Brohan et al., 2023a, Park et al., 2023] with the Hypothetical Minds model. Our agent produces adaptive policies in competitive, cooperative, and mixed-motive multi-agent scenarios with concealed information by generating hypotheses about other agents' latent states with a Theory of Mind module.\nThe Theory of Mind module in Hypothetical Minds generates hypotheses about other agents' strategies, goals, and capabilities to facilitate effective coordination or counter-strategies. High-level plans are subsequently passed to a subgoal module to sequence together embodied action plans. The ToM module simultaneously evaluates multiple hypotheses until a hypothesis provides a sufficient explanation of the data [O'Doherty et al., 2021, Tenenbaum et al., 2006, Gershman et al., 2015].\nWe evaluate our model on four substrates in the Melting Pot MARL benchmark that encompasses diverse challenges and social dynamics [Agapiou et al., 2022]. Collaborative Cooking Asymmetric requires effective coordination and division of labor among agents. Running With Scissors necessitates strategic reasoning about opponents' policies and the ability to exploit predictable patterns in a competitive setting, with the eight-player version offering a unique challenge to the model's scalability. Moreover, the mixed-motive environment Prisoner's Dilemma involves a tension between individual and collective interests. Diverse evaluation scenarios stress test playing with a wide array of agents with a fixed or adaptive policies, necessitating contextual adaption. Our contributions are as follows:\n\u2022 We propose the Hypothetical Minds model (HM), an embodied LLM-based agent for multi-agent environments with concealed information that integrates modular components for perception, memory, and hierarchical planning over two levels of abstraction.\n\u2022 HM incorporates a novel Theory of Mind (ToM) module, which generates, evaluates, and refines hypotheses about other agents' strategies or goals in natural language. Through ablations and comparisons against LLM-agent baselines, we identify the critical role of hypothesis evaluation and refinement within the ToM Module.\n\u2022 We demonstrate the effectiveness of Hypothetical Minds across multiple multi-agent environments in the Melting Pot benchmark, including competitive, collaborative, and mixed-motive domains, and 30 distinct evaluation scenarios. Our agent significantly outperforms LLM-agent and RL baselines in every environment and in a large majority of evaluation scenarios, showcasing its generalizability."}, {"title": "2 Related Work", "content": "A burgeoning area of research involves building autonomous agents rooted in large language models Wang et al. [2023b], Sumers et al. [2023]. This involves deploying LLMs as central controllers across many different domains by leveraging their extensive background knowledge from training."}, {"title": "2.1 LLM-based Agents", "content": "Applications span a wide range from equipping LLMs with external tools to interface with databases and APIs [Schick et al., 2023, Shen et al., 2023, Qin et al., 2023, Ge et al., 2023, Yao et al., 2022] to using them for high-level planning and control in robotics [Huang et al., 2022, Brohan et al., 2023b, Rana et al., 2023, Brohan et al., 2023a]. The most relevant branch of this research direction includes works where LLMs are used as planners in embodied virtual environments. Voyager autonomously builds complex skills in Minecraft by storing and retrieving behaviors in a skill library of executable code and uses the skill library to solve progressively harder tasks [Wang et al., 2023a]. Octopus and Jarvis-1 use vision-language models to plan long horizon behaviors in virtual environments [Yang et al., 2023, Wang et al., 2023d]. Dynalang and DECKARD use language to guide world modeling for model-based RL policies in virtual environments [Lin et al., 2023, Nottingham et al., 2023]. In this work, we use an LLM for long horizon high-level planning and predicting the future states of other agents in multi-agent environments.\nPrevious papers have also incorporated LLM-based agents into embodied multi-agent environments. Park et al. [2023] introduce a interactive simulation of a rich social environment, where each agent autonomously selects goals and builds relationships with others. We extend the cognitive module framework developed in this work for multi-agent environments of varied dynamics. SAMA uses an LLM to plan out sequences of subgoals for language-based goal-conditioned RL policies in environments requiring multi-agent coordination Li et al. [2023]. Another study builds cooperative embodied agents, by using an LLM for planning and communication between agents Zhang et al. [2023b]. ProAgent develops a method for improving zero-shot coordination in Overcooked by using an LLM to infer the intentions of teammates based on the present state Zhang et al. [2023a]. In the present work, we present a generalizable method that addresses the challenge of inferring other agents intentions' across a spectrum of social dynamics."}, {"title": "2.2 Reasoning and Hypothesis Search with LLMs", "content": "LLMs have shown impressive reasoning abilities, augmented by Chain-of-Thought methods that scaffold the thought process [Wei et al., 2022, Zhang et al., 2023c]. [Wang et al., 2023c] investigates LLMs' inductive reasoning by generating and evaluating hypotheses on the Abstraction and Reasoning Corpus (ARC), while [Qiu et al., 2023] refines LLM-generated hypotheses using task-specific symbolic interpreters. Similarly, we generate, evaluate, and refine hypotheses based on feedback, computing values for each hypothesis by predicting another agent's goals. STaR [Zelikman et al., 2022] also learns from feedback by finetuning language models on rationales that produced correct answers. [Akata et al., 2023] assessed LLMs' ability to reason in matrix games requiring ToM, showing mixed results. We demonstrate that asking the LLM to predict the opponent's actions improves results, highlighting the importance of ToM reasoning in game-theoretic tasks."}, {"title": "2.3 Cognitive models in partially-observable environments", "content": "Our method has analogs in the computational modeling of animal and human decision-making, and our work adds to emerging promising evidence that LLMs can operate as cognitive models [Binz and Schulz, 2023]. Numerous methods have been proposed for generating hypotheses about hidden or latent states in partially-observable settings. Methods from Bayesian nonparametric stats, such as the Chinese restaurant process, suggest that when animals and humans are learning they generate an unbounded number of possible latent causes of the observed data, until a satisfactory explanation is found [Gershman et al., 2015]. This framework explains a wealth of behavioral phenomena from categorization [Anderson, 1991, Sanborn et al., 2010, Gershman and Niv, 2013, Gershman et al., 2015], Pavlovian conditioning, and more. Our LLM-based agent similarly iteratively generates and refines hypotheses about the opponent's strategy, until a explanation aligns with the observed data's likelihood. Mixture models and mixture of experts models are similarly useful for modeling the human decision-making process under uncertainty, by arbitrating between multiple hypotheses in proportion to the reliability of their predictions [Frank and Badre, 2012, O'Doherty et al., 2021, Archambeau et al., 2023]. Researchers have extended this POMDP modeling framework to multi-agent settings by constructing models that explicitly model other agents' beliefs and goals when interacting with them, handling uncertainty over this hidden information with a Bayesian framework [Baker et al., 2017, Rusch et al., 2020]. Our framework also explicitly prompts a LLM to infer its opponent's goals/strategy and weighs the uncertainty over these inferences with a hypothesis evaluation mechanism. This mechanism is also inspired by cognitive models and employs a Rescorla-"}, {"title": "3 Method", "content": "Our method is directly applicable to any multi-agent environment where states are partially observable and agent(s)' policies are hidden. We formally define this as a Markov game for N players in a partially observable setting. Let the finite set S represent the possible states of the game. Each player i receives observations given an observation function \\(x^i : S \\rightarrow O\\), representing their limited point of view. Additionally, each player i can take actions from their action space \\(A^i\\), and when all players choose actions \\((a^1, ..., a^N) \\in A^1 \\times \u2026 \\times A^N := A\\), the state transitions according to a probability distribution \\(T : S \\times A \\rightarrow D(S)\\). The reward function for each player i is represented as \\(r^i: S \\times A \\rightarrow R\\), mapping the current state and joint actions to a real-valued reward."}, {"title": "3.1 Partially-observable Markov games", "content": "The Hypothetical Minds model consists of several cognitive modules that altogether form an embodied LLM agent (Figure 1). The egocentric observations are represented by a textual map/state representation, which is added to a memory system after every step. The memory system also logs rewards and other important state information like the inventories from previous interactions in Running With Scissors. Two cognitive modules depend on an LLM, a Theory of Mind module and a Subgoal module, which output high-level goals and action plans respectively. An action planner takes an action plan (i.e. \"move to coordinate (13, 5)\") and creates a sequence of actions that achieves that action plan with a pathfinding algorithm. Each cognitive module is explained in more detail in the Appendix. Below we describe the key novel contributions of our method that implement hierarchical planning."}, {"title": "3.2 Hypothetical Minds Model", "content": "In multi-agent environments, other agents' behavior can be influenced by various latent variables, such as their strategies, goals, competence levels, and locations in space. These latent variables are often not directly observable and must be inferred from the agent's observations. We represent these latent variables as a multidimensional space \\(\\Theta = \\theta_1, \\theta_2, ..., \\theta_m\\), where each dimension \\(\\theta_i\\) corresponds to a specific latent variable. The ToM module maintains a set of hypotheses \\(H = h_1, h_2, ..., h_n\\) in its working memory, where each hypothesis \\(h_t\\) is a natural language representation of a belief about the latent variables \\(h_i = p(\\Theta)\\) (Figure 2). A hypothesis at time t is generated by asking an LLM to infer another agent's strategy, conditioned on HM's memory M of important past observations O. Additionally, the LLM is shown the top k valued previously generated hypotheses, such that it can perform hypothesis refinement (see Appendix and code for more details and prompts):\n\\[h_t = LLM(M, h_{<t})\\qquad(1)\\]\nwhere M is a memory buffer storing information about an agent's actions, observations, and rewards. Each hypothesis \\(h_i\\) is scored based on how well it predicts the other agent's future behavior, noted here formally as a distribution of trajectories \\(p(T)\\). We formalize this scoring mechanism using a likelihood function \\(p(T|h_i)\\) representing the probability of an agent exhibiting trajectory \\(\\tau\\) given the hypothesis \\(h_i\\). The best hypothesis \\(h^*\\) is selected using the Maximum a Posteriori (MAP) estimate:\n\\[h^* = \\arg \\max_{h_i \\in H} p(h_i|t) = \\arg \\max_{h_i \\in H} \\frac{p(T|h_i)p(h_i)}{p(T)} \\qquad(2)\\]\nwhere \\(p(h_i)\\) is the prior probability of hypothesis \\(h_i\\) and \\(p(T)\\) is the marginal probability of the observed action a and has no effect on the argmax. The likelihood is approximated by a hypothesis evaluation mechanism described below. The LLM predicts the other agent's future behavior conditioned on each hypothesis separately. Hypotheses leading to correct predictions will have higher values reflecting higher likelihoods. The prior \\(p(h_i)\\) corresponds both to the background knowledge embedded in the weights of an LLM from pretraining and to the refinement mechanism that shows the top valued hypotheses to the LLM when the LLM generates a hypothesis. By continuously updating and selecting the best hypothesis based on observed information, the ToM module can effectively infer the latent variables governing the other agents' behavior and adapt its own strategies accordingly."}, {"title": "Theory of Mind Module", "content": "Drawing on cognitive modeling approaches [Rescorla, 1972, Daw and Tobler, 2014], multiple hypotheses are scored with a value system \\(V_{h_i} = E[r]\\) where r reflects intrinsic reward based on the accuracy of the predictions the hypothesis generates. We compute self-supervised intrinsic rewards bootstrapped from the LLM's own predictions. Let \\(\\phi(\\tau)\\) be a particular behavior, a feature from an observed trajectory, and \\(\\hat{\\phi}(\\tau)\\) be the predicted behavior by the LLM, such as the inventory played by an agent in Melting Pot environments. The intrinsic reward function \\(r_i\\) can then be defined as:\n\\[r_i = \\begin{cases}\nc & \\text{if } \\phi(t) = \\hat{\\phi}(\\tau) \\\\\n-c & \\text{if } \\phi(t) \\neq \\hat{\\phi}(\\tau)\n\\end{cases}\\]\nwhere c is a hyperparameter. \\(V_{h_i}\\) is then dynamically updated with a Rescorla Wagner update rule [Rescorla, 1972], expressed as:\n\\[\\delta = r_i - V_{h_i}\\]\n\\[V_{h_i} \\leftarrow V_{h_i} + \\alpha \\cdot \\delta\\]\nmodulated by learning rate \\(\\alpha\\) via a prediction error \\(\\delta\\). The learning rate dictates how much to weigh recent interactions, a useful property when playing against evaluation scenarios where agents change their strategy within an episode. When the value of a hypothesis meets a threshold \\(V_{thr}\\), the ToM module marks the hypothesis as validated and uses this hypothesis to condition high-level plans (using the highest-valued one if multiple pass the threshold). This hypothesis will then continue to be used for planning until it no longer makes good predictions and its value subsequently falls below the threshold. If no hypothesis meets the threshold, then the latest generated hypothesis (with the most updated information) is used for conditioning high-level plans."}, {"title": "Hypothesis Evaluation", "content": "The ToM module then conditions its high-level plans on the inferred latent variables represented by the hypotheses. A high-level plan z is a natural language description of HM's overall strategy, goal, or intention, conditioned on the best hypothesis \\(h^*\\) and memory of past events:\n\\[z = LLM(M, h^*) \\qquad(3)\\]\nBy conditioning the high-level plans on the hypotheses, HM can adapt its strategy based on its understanding of the other agents' latent states.\nFinally, the Subgoal module selects a sequence of subgoals. Let \\(g = g_1, g_2,..., g_k\\) be a sequence of subgoals, where each subgoal \\(g_i\\) is an action or short sequence of actions that the agent needs to take to achieve the high-level plan z:\n\\[g = LLM(O, M, z) \\qquad(4)\\]\nThe sequence is generated by conditioning the LLM on the high-level plan, observations, and memory, and prompt to achieve the high-level plans. The LLM outputs a sequence of specified subgoal function calls, which are then parsed and mapped to the corresponding actions in the environment by a hardcoded Action Planner (see Appendix)."}, {"title": "Conditioning High-Level Plans", "content": "Here we investigate the following to analyze the generalizability and scalability of our method:\nQ1. How does Hypothetical Minds perform compared to LLM agent and RL baselines in embodied competitive zero-sum environments?\nQ2. How does Hypothetical Minds perform compared to LLM agent and RL baselines in a collaborative domain that requires adaptation to a partner's role and competence?\nQ3. How does Hypothetical Minds perform compared to LLM agent and RL baselines in a mixed-motive setting?\nQ4. Does the Hypothetical Minds agent scale effectively to environments with larger populations of agents?\nQ5. How do the different components of the Hypothetical Minds agent and the Theory of Mind module contribute to its overall performance?\nWe directly test our LLM-based agent on the evaluation scenarios in four Melting Pot environments (Fig. 3). The key evaluation method is to test agents against different bots with various policies. Across environments, this consists of 30 distinct evaluation scenarios. Crucially, our agent has no knowledge about which strategies they may be playing in the prompts given. Strategies have to be ascertained online within an episode via in-context learning."}, {"title": "4 Experiments", "content": "synergizes reasoning and acting in language models, allowing them to generate both reasoning traces and task-specific actions in an interleaved manner [Yao et al., 2022]. includes three main components: an Actor module that generates actions and text, an Evaluator that scores these actions, and a Self-Reflection module that uses the evaluations to provide constructive feedback stored for subsequent use [Shinn et al., 2024]. To provide a hierarchical baseline to test against our hierarchical model, we include the PlanReAct architecture introduced in [Liu et al., 2023]. This structure allows the agent to plan before interacting with the environment. PPO is a model-free RL baseline [Schulman et al., 2017] and we train agents in a population of models with the same parameters. Prompts and architectures are shared across baselines to provide a fair comparison and natural ablations of our model. The Subgoal module provides a baseline actor shared between LLM baselines, and the only difference between PlanReAct and Hypothetical Minds is that high-level planning is mediated by the Theory of Mind module, including hypothesis generation, evaluation, and refinement."}, {"title": "4.1 Baselines", "content": "A zero-sum competitive environment with two players moving around a map and collecting yellow, purple, or blue resources that correspond to rock, paper, and scissors respectively. Zapping your opponent causes an interaction, with one agent getting positive reward and the other agent getting an opposite negative reward according to the inventories of resources picked up by each player, mirroring the rock, paper, scissors matrix game.\nRWS presents nine distinct evaluation scenarios, which range in complexity. These include three straightforward strategies where opponents consistently play rock, paper, or scissors. The remaining scenarios introduce more dynamic and adaptive strategies (see Appendix for details and full list). Therefore in order to succeed on the scenarios, an agent needs to correctly infer the strategy and exploit it. Against the simple policies, the agent should play the same type of inventory every round rather than playing randomly or anticipating a change in its opponent's policy. In contrast, success against the adaptive strategies demands not only the anticipation of the opponent's next move based on personal previous plays but also selecting the most advantageous counter."}, {"title": "4.2 Competitive Environments", "content": "Figure 3 and Appendix Table 1 demonstrate how Hypothetical Minds model consistently achieves large magnitude rewards and performs reliably better than the baselines on every single scenario. Hypothetical Minds performs the best on the static strategies, scenarios 0, 6, 7, 8 representing fixed policies that play for the same inventory on every interaction (6: rock, 7: paper, 8: scissors, 0: random sample from 6-8). Therefore it is able to exploit the static strategy reliably once it correctly infers it. The agent is also able to consistently return positive rewards against the difficult scenario 1, the adaptive bot that plays the best response to your last round. In contrast, baselines are failing to"}, {"title": "Running With Scissors in the Matrix Repeated (RWS).", "content": "An eight-player extension of RWS, where the agent controls one player against a population of 7 strategies. This adds additional complexity to the decision-making process and tests the scalability of models, as agents now must infer the strategies of separate agents and maintain this information in memory. Thus, Hypothetical Minds maintains separate hypothesis evaluation streams for every agent. Additionally, in order to maximize reward, agents should only interact with opponents that it knows it can beat with its current inventory. Models are therefore tasked with integrating uncertainty and seeking out opponents for which they have high confidence about their strategy.\nHypothetical Minds achieves higher rewards than the baselines in RWS Arena (Figure 3) and is the best model on 6/8 scenarios (Appendix Table 1). HM performs particularly well on homogeneous populations of rock, paper, or scissors (scenarios 5-7). Scenarios 2-4 consist of heterogeneous populations where 2/3rds of the population are one pure strategy and the remaining 1/3rd represents the pure strategy that would beat the best response to the majority strategy (e.g., scenario 2 is 2/3 rock and 1/3 scissors). HM performs the best on 2 out of 3 of these difficult scenarios, and performance for scenario 4 is dragged down by one highly negative episode in which the agent was exploited by the minority strategy. In contrast, all baseline models struggle to achieve rewards of more than 10 on average in nearly every scenario. This highlights the difficulty of the environment, for which proper coordination between high-level plans, the embodiment, and memory is crucial for success. These results also suggest that Hypothetical Minds scales well to population-based environments in which you need to either handle distinct agents differently or make population-level inferences."}, {"title": "Running With Scissors in the Matrix Arena (RWS Arena).", "content": "In the Collaborative Cooking: Asymmetric environment, two players on distinct sides of a divided kitchen must collaborate to efficiently cook tomato soup. The layout provides distinct advantages to each side-one side is closer to the goal delivery but farther from the tomato dispenser, and vice versa for the other side. To maximize rewards, the two players should specialize based on their proximity to resources: one handles tomato dispensing and delivery, and the other manages dish retrieval and soup delivery. Evaluation scenarios challenge the focal agent to demonstrate dual forms of adaptation: adjusting to the specialized role dictated by their side of the kitchen and to the varying competence of their partner, from skilled and specialized to entirely unresponsive."}, {"title": "4.3 Collaborative Cooking Asymmetric", "content": "Again, Hypothetical Minds achieves higher rewards than the baselines on every scenario (Figure 3). Interestingly, HM performs significantly better than the baselines on the scenarios where there is a functional partner (Appendix Table 1). This suggests that if there is value in a partner, HM can take advantage of this and adapt its behavior accordingly, highlighting the model's usefulness for complex, dynamic environments where cooperative interaction is crucial. The LLM baselines perform relatively well with the negligent partner, where success hinges on repeatedly executing"}, {"title": "4.4 Mixed Motive: Prisoner's Dilemma in the Matrix Repeated", "content": "In the Prisoner's Dilemma in the Matrix (PD) environment, agents navigate a similar grid world to RWS and collect resources corresponding to cooperation or defection in the iterated prisoner's dilemma game. The payoff matrix incentivizes mutual defection in a single interaction, but the highest total welfare is achieved through mutual cooperation across an episode.\nHypothetical Minds achieves the highest reward overall and in 5/10 scenarios (Figure 3, Appendix Table 1). This highlights its ability to perceive the background agent's strategy and adapt accordingly. HM outperforms the baselines relatively more with dynamic partners. With tit-for-tat for example (scenarios 5 and 6), Hypothetical Minds achieves the highest score, by engaging in more consistent cooperation while demonstrating some forgiveness to avoid cycles of alternating defection, a pattern that plagues baselines. In scenarios 8 and 9, Hypothetical Minds showcases a capacity for \"corrective punishment.\" By initially defecting against exploitative partners, it persuades them to switch to conditional cooperation. The agent then shifts to a forgiving cooperation strategy to maintain a mutually beneficial equilibrium.\nRelative to the other three environments tested, HM performs comparably to the baselines in PD, and all LLM baselines perform similarly in the majority of the scenarios. This similarity likely arises from their convergence on simple heuristic strategies, such as consistently defecting against static bots (the optimal approach) and employing a tit-for-tat strategy against adaptive bots. Again, this highlights the importance of focusing on the more dynamic strategies as previously discussed. All models struggle with the grim reciprocators, as testing defection is needed to perform well on the task overall, but is not tolerated by these bots."}, {"title": "4.5 Ablations", "content": "As previously discussed, we compared baseline LLM agents with our models to perform natural ablations, maintaining similar prompts but varying information flows. We also compare base LLMs between GPT4 (default), GPT 3.5, and Llama3 (Figure 5). GPT4 performs by far the best across environments, and GPT 3.5 performs poorly. Llama3's performance lies in between these two on RWS Repeated and Collaborative Cooking, and slightly below GPT 3.5 on Prisoner's Dilemma. Due to input context limitations, we could not run Llama3 on RWS Arena.\nWe conducted an additional detailed ablation analysis on the ToM module for the challenging environment, RWS Repeated. Vanilla Mind Prompting (VMP): Utilizes a single API call for all ToM module steps, excluding hypothesis evaluation and refinement (see Appendix and code for more"}, {"title": "5 Conclusion", "content": "Here we evaluate the Hypothetical Minds model on challenging multi-agent environments with varied dynamics. One limitation of our method is the human in the loop component necessary to set up the scaffolding and prompting for the agent. Additionally, knowledge of the game rules and mechanics are listed in the prompts. An avenue for future research is learning these concepts and the appropriate types of scaffolding within the cognitive modules autonomously from environmental feedback."}, {"title": "A Appendix", "content": "B Environments"}, {"title": "B.1 Running With Scissors Repeated", "content": "Specifically, here we evaluate our model on the Running With Scissors in the matrix: Repeated environment (RWS) in the Melting Pot multi-agent decision-making benchmark [Agapiou et al., 2022]. This is a zero-sum competitive environment with two players moving around a map and collecting yellow, purple, or blue resources that correspond to rock, paper, and scissors respectively. In addition to movement, the agents have an action to fire an \"interaction\" beam which initiates a duel with the other player when that player is within range of the beam. An interaction results in one agent getting positive reward and the other agent getting an opposite negative reward according to the inventories of resources picked up by each player. Specifically a player will collect an inventory, which is only observable by that player:\n\\[p = (p_{\\text{yellow}}, p_{\\text{purple}}, p_{\\text{blue}}).\\]\nReward is determined by matrix multiplication operations mirroring the rock, paper, scissors matrix game:\n\\[r_{\\text{row}} = v_{\\text{row}}^T A_{\\text{row}} v_{\\text{col}},\\qquad r_{\\text{col}} = -r_{\\text{row}}\\]\nwhere \\(v_i = \\frac{p_i}{\\sum_{j=1}^3 p_j}\\), and\n\\[A_{\\text{row}} = \\begin{bmatrix}\n 0 & -10 & +10 \\\\\n+10 & 0 & -10 \\\\\n-10 & +10 & 0\n\\end{bmatrix}\\]\nThe partially-observable input in Melting Pot consists of a 5x5 window around the agent such that it can see three grids in front of itself and one behind it, and two on each side."}, {"title": "B.1.1 Scenarios", "content": "Description of scenarios for each substrate are reproduced directly from [Agapiou et al., 2022]:\nSC0: Versus mixed strategy opponent. Here the focal agent must defeat an opponent that was trained to play a pure strategy: either rock, paper, or scissors. However, the specific opponent is sampled at test time so it could be any of those. All opponents commit strongly to their choice, aiming to collect at least three resources before interacting. To defeat them, the focal agent should scout out which pure strategy its opponent is playing and then collect the resources to implement its counter strategy. Since this is a one-shot interaction, success requires the focal agent to pay close attention to which resources are missing since they provide a clue to which strategy their opponent is implementing."}, {"title": "B.2 Running With Scissors Arena", "content": "We also evaluate our model on Running With Scissors in the matrix: Arena environment in the Melting Pot multi-agent decision-making benchmark [Agapiou et al., 2022]. This environment has the same dynamics as Running With Scissors in the matrix: Repeated with the main exception being that their are 8 players in this substrate playing on a larger 25 by 24 matrix with a 11 by 11 observability window, skewed towards viewing more in front of the agent. All scenarios under this agent represent one focal resident (agent) playing against 7 others with varying fixed and dynamic policies, attempting to maximize reward decided by the payoff matrix equivalent to the one in Running with Scissors Repeated."}, {"title": "B.2.1 Scenarios", "content": "SC0: Versus a background population containing bots implementing all three pure strategies. Here one focal player joins seven from the background population. The background population contains bots who implement all three pure strategies: rock, paper, and scissors. They may either commit to their strategy moderately (aiming to collect three resources before interacting) or more strongly (aiming to collect five). The task for the focal agent is to watch its opponents, see what strategy one of them is implementing, and act accordingly.\nSC1: Versus gullible bots. Here one focal player joins seven from the background population. The background population consists entirely of weak bots who were trained to best respond to agents playing pure strategies. They are weak opponents.\nSC2: Versus mixture of opponents who play rock and some who flip to scissors after two interactions Here one focal player joins seven from the background population. The focal player should pay attention to what each prospective partner has collected since 2/3 of them play rock while 1/3 play scissors after the first two interactions. Choosing paper to best respond to rock is a bad choice if accidentally paired with an opponent playing scissors.\nSC3: Versus mixture of opponents who play paper and some who flip to rock after two interactions. Like SC2 but with bots playing paper and bots switching from paper to rock.\nSC4: Versus mixture of opponents who play scissors and some who flip to paper after two in-teractions. Like SC 2 but with bots playing scissors and bots switching from scissors to paper.\nSC5: Visiting a population of pure paper bots. Here one focal player joins seven from the background population. All seven background bots play paper so the focal player can get a high score by playing scissors.\nSC6: Visiting a population of pure rock bots Here one focal player joins seven from the background population. All seven background bots play rock so the focal player can get a high score by playing paper.\nSC7: Visiting a population of pure scissors bots. Here one focal player joins seven from the background population. All seven background bots play scissors so the focal player can get a high score by playing rock."}, {"title": "B.3 Prisoners Dilemma Repeated", "content": "The Prisoners Dilemma in the Matrix Repeated environment is a mixed-motive one where two individuals collect resources that represent 'defect' (red) or 'cooperate' (green) and compare inven-"}, {"title": "B.3.1 Scenarios", "content": "SC0: Partner may play either cooperate or defect The optimal strategy is simply to unconditionally defect. However, given that the focal doesn't know the strategy of the background player, a good strategy is more subtle. A reasonable strategy is to be a grim reciprocator cooperator, which would cooperate with the cooperator, and defect to the defector. Alternatively the focal player might try to ascertain whether the background player is exploitable. Doing so, however, carries a risk, for if the background player were to be a Grim reciprocator (like in other scenarios), this would cause them to defect for the rest of the episode.\nSC1: Partner typically plays cooperate. The optimal strategy is simply to unconditionally defect. The same considerations about uncertainty of the background player's strategy from Scenario 0 apply here.\nSC2: Partner typically plays defect The optimal strategy is simply to unconditionally defect. However, because the focal player doesn't a priori know the strategy of the background player, they must first try to find out their strategy. This can be done by looking at which resources they collect or by paying attention to the results of the first few interactions. Once the focal has identified its background partner is defecting then it may have confidence that it should defect as well. The focal player should also consider the possibility that the background bot is corrigible, i.e. that it could be persuade to switch from defection to cooperation. This is not the case here but the background populations used in SC 8 and SC are corrigible.\nSC3: Partner is a hair-trigger grim reciprocator, i.e. one who initially cooperates but, if defected on once, will retaliate by defecting forever after. The optimal strategy is simply to cooperate. Grim reciprocator background players are non-exploitable, and there is no way to know how they will react to a defection ahead of time. Because of this uncertainty, testing for exploitability can lead to poor performance of the focal player. Conditional cooperators who cooperate first but retaliate if defected on should achieve a high score.\nSC4: Partner is a two-strikes grim reciprocator, i.e. one who initially cooperates, but if defected on twice, will retaliate by defecting forever after The optimal strategy is simply to cooperate. Grim reciprocator background players are non-exploitable, and there is no way to know how they will react to a defection ahead of time. Because of this uncertainty, testing for exploitability can lead to poor performance of the focal player. In principle, it would be possible to defect once against the background player leading to higher reward. But since it is not possible to know the background player is a two-strikes grim reciprocator, and testing it against a hairtrigger grim reciprocator leads to defection, in practice is better simply to cooperate. Conditional cooperators who cooperate first but retaliate if defected on should achieve a high score.\nSC5: Partner is a tit-for-tat conditional cooperator The optimal strategy is simply to cooperate. Defecting against a tit-for-tat agent, even occasionally, might lead to miscoordinated interactions where one player cooperates and the other defects, in an alternating way. Forgiveness is one way to break out of such cycles of recrimination. Conditional cooperators who cooperate first but retaliate if defected on should also be forgiving to ensure they do well in this scenario.\nSC6: Partner is a tit-for-tat conditional cooperator who occasionally plays defect instead of cooperate. Like the previous scenario, except the tit-for-tat background player occasionally will defect instead of cooperate. This is known as trembling hand in game theory. A strict tit-for-tat focal player would occasionally fall into miscoordinated interactions with the background player resulting in alternating cooperation and defection. As in SC5, focal"}, {"title": "B.4 Collaborative Cooking Asymmetric", "content": "In the Collaborative Cooking Asymmetric substrate, players need to collaborate to follow recipes. The environment described in [Agapiou et al., 2022] follows the regular pseudoreward scheme, which is turned off by default. The asymmetric environment is a version of the Collaborative Cooking with an asymmetric advantages map. This is to test whether players can choose high-level strategies that play to their strengths."}, {"title": "B.4.1 Scenarios", "content": "SC0: Collaborate with a skilled chef Here the background player implements a particular policy that can be very effective when its partner does its part. The two players are on two distinct and disconnected sides of the map. On one side the goal delivery location is close to cooking pots and the tomato dispenser is far away whereas on the other side the the goal delivery location is far from the cooking pots but the tomato dispenser is close. The players should collaborate, each specializing in the part of the task that it is most efficient for them to do on the side of the map where they spawned. The background player implements this kind of policy, which depends on the actions of its partner to complete the task. The background player was trained with the V-MPO algorithm.\nSC1: Collaborate with a semi-skilled apprentice chef This scenario is similar to SC 0 but the background player is not as well trained. In fact the background population used here is the same as in SC0 but from an earlier point in training. The importance of evaluating cooperation with bots of varying skill levels, and different points in training.\nSC2: Succeed despite an unhelpful partner In this scenario the background player never moves or helps in any way. On this map it is less efficient to implement all steps of the recipe alone versus to work together with a partner. But it is still possible for either player to perform all the steps on their own. The task is to realize that the background player won't do their part of the joint policy so the focal agent had better do everything itself."}, {"title": "C Methods", "content": "Pixel images of the global state are preprocessed into a text-based state representation. The images are divided into 8x8 patches, each corresponding to a cell within the Melting Pot grid that entities can occupy. Each patch may represent one of four types: an agent, a resource, a wall, or a blank space. The patches are labeled by comparing them to manually labeled reference patches of each type of entity, including the numerous possible body orientations and hat colors an agent can embody in Running With Scissors. The global state can then be fully represented in text by coordinates in a 23x15 grid and the entity label at each coordinate. Egocentric states are then created from this"}, {"title": "C.1 Textual Map", "content": "The memory system consists of two parts. The first data structure appends the observed states in the previous step to lists of each entity type in a tuple with the step it was observed. For example: 'yellow_box': [((13, 3), \u2018Step: 1087'), ((13, 4), 'Step: 1087'), ((7, 3), 'Step: 1091')]. The LLM is prompted that its memory can be outdated and therefore should take the step it was last observed into account.\nThe second data structure in the memory system contains a list of the agent's inventories and rewards from the interactions that occurred so far. This specific information, distinct from the previously observed states, is relayed to the ToM module."}, {"title": "C.2 Memory", "content": "The Theory of Mind Module is queried periodically after discrete events. For the * in the Matrix substrates, this occurred after an interaction. For collaborative cooking, this occurred after a dish was delivered.\nThe ToM module consisted of a 5 step process, as depicted in Figure 2 for the * in the Matrix substrates:\n1. Record the observed behavior from the other agent's trajectory (\\(\\tau\\)). Here this refers to whether they played rock, paper, or scissors, the argmax of the inventory (or cooperate/defect in Prisoner's Dilemma). Since the opponent's inventory is never observed, we have to estimate it given the inventory Hypothetical Minds played and the reward it received. Thus, we ask the LLM to estimate the opponent's inventory given this information, and note the output as the empirical opponent's inventory.\n2. Evaluate Hypotheses about opponent's strategy. In the previous interaction, the top k hypotheses are used to generate predictions about the opponent's next inventory \\(\\hat{\\phi}(\\tau)\\). These argmax of the inventory predictions are compared to the argmax of the empirical opponent's inventory (did they play rock, paper, or scissors). As described in the main text, Hypotheses that led to correct predictions get a positive intrinsic reward and negative otherwise. If a hypothesis is validated, meeting \\(V_{thr}\\), then step 3 is skipped and this hypothesis is used for step 4 until the hypothesis falls below the threshold (meaning its not longer making good predictions)\n3. Generate new and refine old hypotheses. The LLM is tasked with generating a hypothesis about the other agent's strategy given the entire interaction history (see prompt below for more details). The prompt also includes the top k hypotheses generated so far if the hypotheses have a value above 0 (meaning at least one correct prediction) such that the LLM can refine previously generated hypotheses.\n4. Guess opponent's next goal. The LLM is prompted to guess the opponent's next inventory given a hypothesis and the interaction history. If no hypothesis has yet surpassed \\(V_{thr}\\), then guesses are made for the top k hypotheses and the last generated hypothesis. The prediction from the last generated hypothesis would be used to select a counter inventory in the next step. Moreover, all the predictions will be used for step 2 (hypothesis evaluation) after the next interaction. If a hypothesis crosses \\(V_{thr}\\), then only it will be used in this step.\n5. Select goal to counter opponent. The LLM is prompted to select a counter inventory given the prediction about the opponent's next inventory. Since this step involves straightforward reasoning given the opponent's predicted inventory, it is done simultaneously in the"}, {"title": "C.3 Theory of Mind Module", "content": "API/LLM call with the previous step. Therefore, the LLM is specifically tasked with outputting both the predicted opponent's next inventory \\(\\hat{\\phi}(\\tau)\\) and its own goal/target inventory z in a single API/LLM call.\nIn RWS Arena, step 4 is done separately than step 5. The LLM is first prompted to guess the next inventory for the opponent they just played. Then in a subsequent step, it is asked again to select which opponents to seek out, and to guess what inventory they will play (this could be a different opponent than the one interacted with in the last round). Therefore, the ToM module can evaluate the hypotheses for each opponent based on the quality of the predictions they make for that particular opponent, and this process is separable from selecting the target inventory in the next interaction (see prompts).\nA similar process occurs for Collaborative Cooking Asymmetric. Step 1 is hardcoded and not LLM dependent; the other agent's actions/behavior \\(\\phi(\\tau)\\) are labeled by custom code given the observations (abstracting away the problem of action recognition from textual observations). These actions include \"Teammate picked up a dish\", \"Teammate put down a dish\", \"Teammate picked up cooked soup in dish\", \"Teammate delivered cooked soup\", \"Teammate picked up a tomato\", or nothing. In step 4, the LLM is prompted to guess the teammate's next behavior \\(\\hat{\\phi}(\\tau)\\) in natural language. Another LLM instance is used in step 2 to assess whether the prediction was correct or not, prompting the LLM to output True or False, and giving each hypothesis the appropriate intrinsic reward. Step 5 is completed in a separate API call for Collaborative Cooking, and the LLM is prompted: \"what strategy do you want to take next and why? Teammate's observed strategy: \". Think step by step about how to adapt to their behavior and maximize all resources and efficiency accordingly.\""}, {"title": "C.4 Subgoal Module", "content": "The subgoal module is responsible for generating efficient subgoal plans for the agent. Given the high-level strategy and the current state of the game, the module decomposes the strategy into a sequence of subgoals in the form of action function calls to efficiently implement the strategy. The subgoal module uses the LLM to generate these plans as a sequence of action function calls (usually 3-6). The prompt includes the current step of the game, the high-level strategy/target inventory previously decided upon, details about the current observations (including player position, orientation, inventory, observable resource locations, other agent locations), valid movement locations, memory, and instructions about the action functions.\nAction functions for the * in the Matrix substrates:\n\u2022 move_to(src_coord, target_coord): Efficiently move agent from source coordinate to target coordinate.\n\u2022 fire_at(target_coord): Stay around specified coordinate and fire interaction when opponent is spotted to initiate duel.\nAction functions for Collaborative Cooking Asymmetric:\n\u2022 move_to(src_coord, target_coord): Efficiently move agent from source coordinate to target coordinate. Only move to valid move_to locations where counters or objects are not present.\n\u2022 interact(target_coord): Move to and interact with the entity at the target coordinate, such as picking up ingredients or delivering dishes of cooked soup. To place an object on a counter to free your hands, use interact(counter_coord).\n\u2022 wait(target_coord): Wait for the pot at target_coord to finish cooking. Check the progress of the pots and only use valid locations where pots are present."}, {"title": "C.5 Action Planner", "content": "The action planner turns the sequence of subgoals specified by the subgoal module into a sequence of atomic actions compatible with the Melting Pot environment. These actions include step forward, backward, left, or right, turn left or right, fire zapping beam, and noop. For the move_to(source_coordinate, target_coordinate) function, the A* algorithm find the most efficient path given a set of obstacles. Walls are always considered obstacles, and other resources are"}, {"title": "C.6 Self-Reflection - Collaborative Cooking", "content": "For Collaborative Cooking Asymmetric, an evaluator and self-reflection mechanism was added as in the Reflexion baseline [Shinn et al., 2024]. This was added such that if the agent was making action plans that did not change the state of the world, for example trying to pick up a tomato while holding a dish, these action plans were not repeated with the same state information. By first reflecting on whether the previous action plan was successful, the agent was able to make less mistakes over the course of the episode. This additional cognitive module for self-reflection could in principle also be added for the other substrates, but was not necessary for good performance on the * in the matrix games because the state transitions were simpler for LLMs to understand."}, {"title": "D Baselines", "content": "The ReAct [Yao et al., 2022] agent combines reasoning traces with task-specific actions in an interleaved manner. This approach allows the agent to generate both reasoning steps and actions within the same language model framework. In our context, the reasoning consists of chain of thought reasoning preceding a subgoal plan in the specified format of utilizing action functions. The agent is prompted to think about the other agents' strategies and come up with a subgoal plan accordingly. Thus, ReAct is functionally an ablation of Hypothetical Minds such that there is only a subgoal module and not a theory of mind module. Three example responses are shown as few-shot prompts, consistent with the ReAct framework."}, {"title": "D.1 ReAct", "content": "Reflexion adds evaluation and self-reflection to the ReAct agent backbone serving as the Actor module. After a subgoal plan is completed, the LLM is queried to evaluate the outcomes of that plan. Outcomes are represented as the reward during the plan, and salient state information pre and post plan. This state information includes the position and the inventory of the agent for the * in the matrix games. For Collaborative Cooking Asymmetric, the given state information included position, what the agent is holding, and the state of the two pots."}, {"title": "D.2 Reflexion", "content": "We include the PlanReAct architecture introduced in [Liu et al., 2023] as a hierarchical baseline. This model first generates a high-level plan in language and then feeds this plan to a subgoal module that outputs a subgoal plan based on the high-level plan. Thus, the only difference between PlanReAct and Hypothetical Minds is that high-level planning is mediated by the multiple processing steps of the theory of mind module, including hypothesis generation, evaluation, and refinement."}, {"title": "D.3 PlanReAct", "content": "We train RL agents in a population of PPO agents [Schulman et al., 2017] on each substrate. The weights are randomly initialized for each agent in the population and weights are not shared. Therefore the agents are not playing against identical copies of themselves and see a greater diversity during training than traditional self-play. Models were trained in PyTorch using the Ray Rllib pipeline and this starter code https://github.com/rstrivedi/Melting-Pot-Contest-2023. Optimal parameters were searched over and the final models were trained for 1e8 steps."}, {"title": "D.4 PPO", "content": "The Hypothesis Evaluation + Hypothesis Refinement model had a different evaluation procedure than the other models. Rather than computing values based on predicting the opponent's inventory, here we use extrinsic reward and counterfactual reward. If a hypothesis is used online for goal selection, then the rewards received in the next interaction can be directly used for evaluating it. For the other considered hypotheses, we simulate counterfactual reward by 1. asking GPT to generate a target inventory given the hypothesis/strategy and the given situation and 2. after the next interaction we ask GPT again to reason about what the reward would have been if it played the inventory from 1. GPT is asked to output, positive, negative, or neutral, which we convert to reward with the c parameter."}, {"title": "E Ablation Details", "content": "All prompts can be seen in our public repository: https://github.com/locross93/Hypothetical-Minds/. The most representative prompts are also reproduced below."}, {"title": "F Hyperparameters", "content": "You are Agent 0 in the eight player 'running_with_scissors' Melting Pot multiagent reinforcement learning environment that is a 25x24 (x by y) grid with resources to collect and walls to navigate around. 8 Players can move around the map and collect resources of 3 discrete types corresponding to rock, paper, and scissors strategies - Yellow box = rock Purple box = paper Blue box = scissors. Rock/yellow beats scissors/blue, paper/purple beats rock/yellow, and scissors/blue beats paper/purple. In addition to movement, the agents have an action to fire an \"interaction\" beam which initiates a duel with one player getting positive reward and the other agent getting an opposite negative reward according to their inventories. All players carry an inventory with the count of resources picked up since last respawn and for each respawn start with an inventory of 1 resource each. This inventory is visible in the state with the key 'inventory'. To play a pure strategy strongly, pick up at least 5 resources or more of the color and then fire the interaction beam at another player. To commit less strongly to a strategy, pick up around 3 resources of the color and then fire the interaction beam at another player. Usually you will only want to pick up one type of resource before an interaction, in order to gain the most information about the other players' strategies and to not waste time collecting other resources. You also want to maximize the number of interactions so after you pick up 4-6 resources, you should seek out a duel to reset your inventory and gain more information about the other players' strategies. Your opponents will also almost always only pick up one type of resource before an interaction. For example, player0_inventory = [7, 1, 1) (Yellow, Purple, Blue) is a good inventory that will lead to an informative duel, whereas player0_inventory = [2, 2, 2] (Yellow, Purple, Blue) will not be informative. Your reward is the result of a matrix multiplication involving your inventory in a vector format, and your opponent's inventory vector, and a payoff matrix similar to rock paper scissors. r_t = transpose(your_inventory) * A_payoff * opponent_inventory where A_payoff = np.array([[0, -10, 10], [10, 0, -10], [-10, 10, 0]]) The reward usually ranges from (5, -5) depending on the inventories of both players (the min is -10 and max 10, but it is rare to get these magnitudes). Typically +/- 3-5 is a high magnitude, and a reward near O suggests both players played a similar inventory. State Description: This environment is partially-observable, you can observe an 11x11 grid around your agent depending on your position and orientation (you can see more in front of you than behind). Previously seen states will be represented in memory, but note that these states could potentially be outdated. For example, the other agent could collect a resource that you previously saw. Given the partially-observable nature of the environment, you will need to explore the environment appropriately and select goals based on the information you've gathered. Also pay attention to your opponents' positions when you see them in order to duel with them and gain information about their strategy. To find a specific player, you can first move towards the last known location of the player and then move randomly around the map. Hanging around the center of the map and waiting for a player to come to you is not a good strategy for this environment. After you gather information about your opponents' strategies, seek out opponents whose strategy you know and can exploit and play a counter-strategy."}, {"title": "G Prompts", "content": "You are Agent {self.agent_id} in the two player 'prisoner_dilemma_in_the_matrix' Melting Pot multiagent reinforcement learning environment that is an 23x15 (x by y) grid with resources to collect and walls to navigate around. Your goal is to maximize reward. Players can move around the map and collect resources of 2 discrete types corresponding to cooperate and defect strategies Green box = cooperate Red box = defect. You and another agent will collect resources and compare inventories in an encounter in repeated rounds. In addition to movement, the agents have an action to fire an \"interaction\" beam which initiates a duel to compare inventories and distribute rewards. All players carry an inventory with the count of resources picked up since last respawn and for each respawn start with an inventory of 1 resource each. This inventory is visible in the state with the key 'inventory'. Here is how the payoff matrix in the generic prisoner dilemma works: \u2022 If both players cooperate, they both receive a reward of around 3, \u2022 If one player cooperates and the other defects, the defector receives a reward of around 5 and the cooperator receives a reward of around 0, \u2022 If both players defect, they both receive a reward of around 1. The nuance of 'prisoner_dilemma_in_the_matrix__repeated' is that the rewards are using the payoff rules of the classic prisoner dilemma, but the interaction is repeated and the rewards are distributed based on the inventory of resources collected by each player. \u2022 If both agents cooperate, the one with more cooperate resources will receive a reward lower than the one with less cooperate resources, \u2022 If one agent cooperates and the other defects, the more defect resources the defector has, the higher the reward for the defector, \u2022 If both agents defect, the one with more defect resources will receive a higher reward than the one with less defect resources. Your goal before each interaction is to try and infer what the other player will play and how their strategy over time is affected by your plays. You will only want to pick up one type of resource before an interaction. For example, the inventories {'cooperate/green': 1, 'defect/red': 1} and {'cooperate/green': 3, 'defect/red': 3} will both result in the same reward, so don't waste time collecting more than you need. Player 1 will also always only pick up one type of resource before an interaction. To play a strategy strongly, pick up at least 6 resources or more of only one color and then fire the interaction beam at the other player. To commit less strongly to a strategy, pick up around 2 resources of only one color and then fire the interaction beam at the other player. State Description: This environment is partially-observable, you can observe a 5x5 grid around your agent depending on your position and orientation (you can see more in front of you than behind). Previously seen states will be represented in memory, but note that these states could potentially be outdated. For example, the other agent could collect a resource that you previously saw. Given the partially-observable nature of the environment, you will need to explore the environment appropriately and select goals based on the information you've gathered. Also pay attention to Player 1's position when you see it in order to duel with them and gain information about their strategy. Your goal is to maximize reward attained over an entire episode, so keep in mind the long-term consequences of your actions. Look at events in a gestalt manner."}, {"title": "G.1 Running With Scissors: Arena Prompts", "content": "You are Player {self.agent_id} in the Collaborative Cooking Asymmetric environment, the goal is to cook and deliver tomato soup dishes with a partner. The environment consists of a kitchen with a tomato dispenser, pots, delivery locations, and dish dispensers. Each agent (of 2) has access to specific parts of the kitchen and can perform actions like picking up ingredients, putting soup in a dish, and delivering cooked soup dishes. There is an impassable barrier in the middle of the kitchen that separates the agents' sides at x=4, where the pots are located. The goal is to work together with the other agent to efficiently cook and serve as many dishes of tomato soup as possible to maximize the collective reward. However, communication is not possible, so you must infer your partner's strategy from their actions and adapt accordingly to coordinate tasks. To cook tomato soup, 1. put 3 tomatoes in a pot, 2. pick up a dish when it is finished cooking, 3. put the cooked soup in a dish, and 4. deliver it to the delivery location. Your team receives a reward of 20 for each successfully delivered dish. Only interact with objects on your side of the kitchen. You can only hold one tomato at once. You cannot pick up a tomato from the tomato dispenser with another item like a dish in your hand. You need to pick up a dish before you pick up cooked soup from a pot. The environment is partially observable, and you can only see a 5x5 grid around your agent. You will be prompted at different points to provide high-level strategies and lower-level action plans to achieve them. Use these three functions for lower-level action plans: move_to(src_coord, target_coord): Efficiently move agent from source coordinate to target coordinate. Only move to valid move_to locations where counters or objects are not present. Use sparingly. interact(target_coord): Move to and interact with the entity at the target coordinate, such as picking up ingredients or delivering dishes of cooked soup. To place an object down on a counter to free your hands, use interact (counter_coord). Mostly use this function. wait(target_coord) Wait for the pot at target_coord to finish cooking. Check the progress of the pots and only use valid locations where pots are present. You probably only want to use this when both pots are full to maximize efficiency. Most of the time you will just want to use the interact function because it both moves to and interacts with objects, therefore all the cooking steps can be completed with the interact function. To put down an item to pick something else up, interact with a counter to free your hands. Do not put down items on the floor or the delivery location."}, {"title": "G.1.1 Hypothetical Minds", "content": "Strategy Request:\nYou are at step {step} of the game.\nProvide a strategy for agent {self.agent_id}.\nYour response should outline a high-level strategy what strategy do you want to take next and why?\nTeammate's observed strategy:\nThink step by step about how to adapt to their behavior and maximize all resources and efficiency accordingly.\nThis response will be shown to you in the future in order for you to select lower-level actions to implement this strategy.\nExample response:\nHigh-level strategy: I want to focus on cooking tomato soup dishes.\nYou will be prompted again shortly to select subgoals and action plans to execute this strategy, so do not include that in your response yet."}, {"title": "G.4 Collaborative Cooking Prompts", "content": "Strategy Request:\nYou are at step {step} of the game.\nYour task is to devise efficient action plans for agent {self.agent_id}, reason through what the next subgoals should be given the state information.\nYour previously specified high-level strategy is:\nYour response should be broken up into two parts:\n1. Subgoal Plan Based on the current state and the high-level strategy you previously specified, decompose this strategy into a sequence of subgoals and actions to efficiently implement this strategy. For every subgoal, think step by step about the best action function and parameter to use for that function. This could be fairly long.\n2. Action Plan Output this sequence of actions in the following Python dictionary format, parsable by ast.literal_eval() starting with:\n{{ 'action_plan': ['interact((5, 1))'] }}\nExample response 1:\nSubgoal Plan: Given the current state and my high-level strategy to focus on cooking tomato soup dishes, I should:\nMove to the tomato dispenser and pick up a tomato.\n{{ 'action_plan': ['interact((5, 1))'] }}\nExample response 2:\nSubgoal Plan: Given the current state and my high-level strategy to focus on delivering tomato soup dishes, I should:\nMove to the dish dispenser and pick up a dish, then plate the cooked soup.\n{{ 'action_plan': ['interact((3, 4))', 'interact((4, 2))'] }}\nExample response 3:\nSubgoal Plan: Next I should move to the delivery location and deliver the cooked soup.\n{{ 'action_plan': ['interact((3, 1))'] }}"}, {"title": "Subgoal Module Message", "content": "You are an action plan evaluator.\nThe last subgoal included an interact action that failed.\nYour task is to look at the subgoal the agent took, the state of the environment before and after the subgoal, and evaluate why the subgoal was unsuccessful and provide feedback about what the agent should do next time.\nWe will next plan an entire new action plan, so suggest specific action plans and action functions to use next when applicable.\nIf subgoal succeeded:\nYou are an action plan evaluator.\nYour task is to look at the action plan the agent took, the state of the environment before the plan and the state of the environment after the plan, and evaluate whether the action plan was successful, and if not, provide feedback about what failed and what the agent should do next time.\nTake into account that your teammate could have influenced the outcome of the subgoal in some circumstances.\nSuggest specific action plans and action functions to use next when applicable."}, {"title": "Evaluate Action Outcomes/Self-Reflection", "content": "Based on the observed actions of your teammate (player_1), what do you think their strategy is?\nAre they specializing in any specific activity or subtask?\nTeammate's observed actions:\n{self.teammate_actions}\nHere are your previous hypotheses about the strategy your partner is playing:\n{self.top_hypotheses}.\nThink step by step and provide an analysis of their strategy, any specialization you infer from their behavior, and their competence.\nThen analyze how you can adapt your strategy to maximize efficiency and coordination with your teammate.\nRemember communication is not allowed."}, {"title": "Infer Teammate Strategy Message", "content": "A dish has been delivered at step {step}.\nYou previously guessed that your teammate's (player_1) policy is: {possible_teammate_strategy}\nBased on the proposed hypothesis about your teammate (player_1), what do you think they will do next?\nOutput a concise label about your teammate's next behavior in the following Python dictionary format, parsable by ast.literal_eval() starting with: python\n{{ 'predicted_next_behavior': 'placing tomatoes into pot (4,2), }}"}, {"title": "Predict Teammate Behavior Message", "content": "A dish has been delivered at step {step}.\nYou previously guessed that your teammate's (player_1) would perform this behavior in this round: {predicted_next_behavior}\nHere is the observed behavior of your teammate (player_1) in this round: {latest_teammate_actions}\nDid your prediction match the observed behavior?\nConcisely output True or False in the below Python dictionary format, parsable by ast. literal_eval() starting with:\n{{ 'evaluate_predicted_behavior': True }}"}, {"title": "Evaluate Predicted Behavior", "content": "G.1 Running With Scissors: Arena Prompts\nRWS Arena System Message\nYou are Agent 0 in the eight player 'running_with_scissors' Melting Pot multiagentreinforcement learning environment that is a 25x24 (x by y) grid with resources tocollect and walls to navigate around. 8 Players can move around the map and collectresources of 3 discrete types corresponding to rock, paper, and scissors strategies- Yellow box = rock Purple box = paper Blue box = scissors. Rock/yellow beatsscissors/blue, paper/purple beats rock/yellow, and scissors/blue beats paper/purple.In addition to movement, the agents have an action to fire an \"interaction\" beam whichinitiates a duel with one player getting positive reward and the other agent gettingan opposite negative reward according to their inventories.All players carry an inventory with the count of resources picked up since lastrespawn and for each respawn start with an inventory of 1 resource each. Thisinventory is visible in the state with the key 'inventory'.To play a pure strategy strongly, pick up at least 5 resources or more of the colorand then fire the interaction beam at another player. To commit less strongly to astrategy, pick up around 3 resources of the color and then fire the interaction beamat another player.Usually you will only want to pick up one type of resource before an interaction, inorder to gain the most information about the other players' strategies and to notwaste time collecting other resources.You also want to maximize the number of interactions so after you pick up 4-6resources, you should seek out a duel to reset your inventory and gain moreinformation about the other players' strategies.Your opponents will also almost always only pick up one type of resource before aninteraction.For example, player0_inventory = [7, 1, 1) (Yellow, Purple, Blue) is a good inventorythat will lead to an informative duel, whereas player0_inventory = [2, 2, 2] (Yellow,Purple, Blue) will not be informative.Your reward is the result of a matrix multiplication involving your inventory in avector format, and your opponent's inventory vector, and a payoff matrix similar torock paper scissors.r_t = transpose(your_inventory) * A_payoff * opponent_inventory where A_payoff =np.array([[0, -10, 10], [10, 0, -10], [-10, 10, 0]])The reward usually ranges from (5, -5) depending on the inventories of both players(the min is -10 and max 10, but it is rare to get these magnitudes). Typically +/-3-5 is a high magnitude, and a reward near O suggests both players played a similarinventory.State Description: This environment is partially-observable, you can observe an 11x11grid around your agent depending on your position and orientation (you can see more infront of you than behind).Previously seen states will be represented in memory, but note that these states couldpotentially be outdated. For example, the other agent could collect a resource thatyou previously saw.Given the partially-observable nature of the environment, you will need to explore theenvironment appropriately and select goals based on the information you've gathered.Also pay attention to your opponents' positions when you see them in order to duelwith them and gain information about their strategy.To find a specific player, you can first move towards the last known location of theplayer and then move randomly around the map.Hanging around the center of the map and waiting for a player to come to you is not agood strategy for this environment.After you gather information about your opponents' strategies, seek out opponentswhose strategy you know and can exploit and play a counter-strategy."}, {"title": "G.2 Running with Scissors: Repeated", "content": "G.1.1 Hypothetical MindsSubgoal Module MessageCurrent State Description:Global Map Size: {map_size"}, "grid (Walls are located at the boundaries of the mapand in other places that are invalid for move_to).Valid Locations for move_to: {movable_locations}Player Position: {player_position}Player Orientation: {player_orientation}Player Inventory (yellow, purple, blue): {player_inventory}Egocentric Observations Size: 11x11 grid around your agent. You currently canobserve the following based on your position and orientation:Observable Yellow Box Locations (format: ((x,y), distance from current location)):{yellow_locations_with_distance}Observable Blue Box Locations: {blue_locations_with_distance}Observable Purple Box Locations: {purple_locations_with_distance}Observable Opponent Locations: {opponent_locations}Previously seen states from memory (format: ((x,y), step last observed, distancefrom current location)): {self.memory_states}Execution Outcomes: {execution_outcomes}Error for extracting and executing actions from the response:{get_action_from_response_errors}Rewards: {rewards_str}Strategy Request:You are at step (step) of the game.You have decided to execute a high-level strategy/target inventory in a previousresponse given what you predicted your opponent will do.Select subgoals in order to"]}