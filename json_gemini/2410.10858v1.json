{"title": "Reasoning Paths Optimization:\nLearning to Reason and Explore From Diverse Paths", "authors": ["Yew Ken Chia", "Guizhen Chen", "Weiwen Xu", "Luu Anh Tuan", "Soujanya Poria", "Lidong Bing"], "abstract": "Advanced models such as OpenAI ol exhibit impressive problem-solving capabilities\nthrough step-by-step reasoning. However, they\nmay still falter on more complex problems,\nmaking errors that disrupt their reasoning paths.\nWe attribute this to the expansive solution\nspace, where each step has the risk of diverging\ninto mistakes. To enhance language model\nreasoning, we introduce a specialized training\nframework called Reasoning Paths Optimiza-\ntion (RPO), which enables learning to reason\nand explore from diverse paths. Our approach\nencourages favorable branches at each reason-\ning step while penalizing unfavorable ones, en-\nhancing the model's overall problem-solving\nperformance. Reasoning Paths Optimization\ndoes not rely on large-scale human-annotated\nrationales or outputs from closed-source mod-\nels, making it scalable and data-efficient. We\nfocus on multi-step reasoning tasks, such as\nmath word problems and science-based exam\nquestions. The experiments demonstrate that\nour framework significantly enhances the rea-\nsoning performance of large language mod-\nels, with up to 3.1% and 4.3% improvement\non GSM8K and MMLU (STEM) respectively.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have shown re-markable proficiency in following instructions and\nreasoning (Brown et al., 2020; Ouyang et al., 2022;Touvron et al., 2023b; Jiang et al., 2023). Anal-ogous to human cognitive processes, chain-of-thought prompting guides models to reason step-by-step before producing the final answer (Wei\net al., 2022), significantly boosting their reasoning\ncapabilities and demonstrating exceptional perfor-mance across a wide array of tasks (Wang et al.,\n2023b; Chung et al., 2024). Despite these advance-ments, LLMs still exhibit limitations in scenarios\nthat require more complex reasoning (Zhong et al.,\n2024).\nAs shown in Figure 1, the step-by-step reason-ing path of the model is at risk of diverging to\nunfavorable branches that contain mistakes, thus\nreducing the chance of reaching the correct solu-tion. While such mistakes may not immediately\nlead to the wrong answer, they can compound and\nderail the reasoning process (Ling et al., 2023).\nFurthermore, this challenge is amplified for more\ncomplex problems such as competition-level math\nquestions (Hendrycks et al., 2021b) that require\nlong reasoning paths to solve. Hence, there is a\nneed to address this challenge by encouraging the\nmodels to generate the correct reasoning path while\navoiding the unfavorable branches.\nTo ensure a trustworthy answer derivation pro-cess, prior studies have explored a range of meth-ods, encompassing both prompting and fine-tuning\ntechniques. Prompting methods repeatedly sam-ple from LLMs for the same question and employ\na voting mechanism to select the most accurate\nreasoning step among several alternatives. Such\nvoting mechanisms can be applied at the final stage\nof the process, as demonstrated in Self-Consistency\n(Wang et al., 2023c), or at every intermediate step,\nas illustrated in Tree-of-Thought (Yao et al., 2023a).\nYao et al. (2023b) shows that leveraging external\nenvironmental feedback could remind LLMs of\nsome potential errors within their reasoning pro-cess, which potentially prevents these errors from\naffecting subsequent steps. However, the prompt-ing methods generally demand extensive token us-age to explore multiple reasoning paths from LLMs\nand integrate feedback from the environment. This\ncauses a significant computational cost and huge\nexecution latency."}, {"title": "Reasoning Paths Optimization", "content": "Alternatively, fine-tuning methods can directlyenhance the reasoning capability of LLMs without\nexhaustive prompting engineering. Among these\nmethods, reinforcement learning from human feed-back (RLHF) (Christiano et al., 2017a; Stiennon\net al., 2020; Ouyang et al., 2022), which involves\ntraining a reward model to optimize LLMs, has\nshown considerable effectiveness in aligning LLMs.\nThis approach further spurs the development of sub-sequent works focused on preference optimization,\nsuch as DPO (Rafailov et al., 2023) and SimPO\n(Meng et al., 2024), which has gained widespread\npractical adoption due to its simplicity and stability.\nHowever, it has been observed that these preference\noptimization algorithms may be less effective or\neven detrimental to tasks requiring in-depth reason-ing (Meng et al., 2024). We hypothesize that these\noptimization methods may indiscriminately target\nthe entire reasoning path as problematic, whereas,\nas indicated in Figure 2, errors in reasoning often\noccur at specific steps and affect only the subse-quent erroneous branches.\nTo address the challenge of LLMs committing\nmistakes that can derail their reasoning paths, we\nintroduce Reasoning Paths Optimization, a novel\nframework designed to explore and learn from var-ied reasoning paths. As illustrated in Figure 2, our\napproach initiates by generating a reference reason-ing path for each question that can reach the correct\nanswer via chain-of-thought prompting. Following\nthis, we explore various solution branches emanat-ing from each step in the reference path. With the\nreference reasoning paths and the potential solution\nbranches explored, we optimize the model from\ntwo critical angles: (1) The model should generate\nthe reference reasoning path with a high probability."}, {"title": "Task Formulation", "content": "(2) The model should favor all potential branchesleading to the correct answer over those that do not.\nTo achieve the optimization, we propose a reference\nloss that maximizes the likelihood of generating the\nreference reasoning path and an exploration loss\nthat provides contrastive feedback over each pair\nof favorable and unfavorable branches. As a result,\nwe can explore the diverse mistakes the model is\nliable to produce, and reduce their occurrence by\naligning the models to the correct reasoning path.\nExperimental results on math-based reasoning\ntasks such as GSM8K (Cobbe et al., 2021a) and\nMATH (Hendrycks et al., 2021b) demonstrate the\neffectiveness of our approach compared to strong\nbaselines. In addition, we show that Reasoning\nPaths Optimization can generalize beyond math\ntasks to improve reasoning performance on the sci-ence, technology, engineering, and math (STEM)\nsubset of the MMLU (Hendrycks et al., 2021a)\nexam question dataset. Notably, the experiments\nshow up to 3.1% and 4.3% improvement compared\nto the high-performing baseline on GSM8K and\nMMLU (STEM) datasets respectively.\nIn this work, we focus on problems that requiremultiple steps to arrive at the final answer or pro-duce the final result, such as math word problems\n(Cobbe et al., 2021b; Hendrycks et al., 2021b).\nThus, we provide a concrete task formulation in\nthis section. Given a question Q posed in natural\ntext, the goal is to produce the final answer A in\nnatural text. The model is assumed to go through\nseveral reasoning steps $S_1, S_2, . . ., S_n$ to arrive at"}, {"title": "Framework Overview", "content": "Large language models are capable of reasoningstep-by-step to enhance their problem-solving abili-ties. However, they often fall short when faced with\nmore challenging problems, committing mistakes\nthat derail their reasoning paths. We believe this is-\nsue arises from the large solution space, where mul-tiple reasoning paths can lead to the correct final\nanswer, but each step carries the risk of branching\ninto errors. To address this, we propose a spe-cialized training framework that jointly considers\ndiverse reasoning paths for a given problem. Our\napproach encourages favorable branches at each\nreasoning step while penalizing unfavorable ones.\nThis framework, which we call Reasoning Paths\nOptimization (RPO), consists of three main stages\nas shown in Figure 2:\n1. Generation: The generation stage aims toelicit correct reasoning steps from the base model\nto serve as reference reasoning paths. This elimi-nates the need for acquiring ground-truth reasoning\npath annotations.\n2. Exploration: To effectively explore the po-tential solution space to a given problem, this\nstage progressively creates branches from each step\nalong reference reasoning paths. As a result, we\ncan obtain multiple favorable and unfavorable rea-soning branches, which will be used to provide\ncontrastive feedback to the model.\n3. Optimization: This final stage aggregates\nand optimizes according to the reference reasoning\npaths and explored branches to enhance the innate\nreasoning ability of the base model. Thus, our\nframework aims to improve the overall reasoning\nability of large language models."}, {"title": "Reasoning Generation", "content": "While training with explanations or step-by-stepreasoning paths (Mukherjee et al., 2023) can im-"}, {"title": "Reasoning Exploration", "content": "prove the reasoning performance of language mod-els, it is labor-intensive and costly to annotate such\ndata. Hence, our framework begins with a reason-ing generation stage that automatically generates\nthe reference reasoning paths. Concretely, given\na problem question Q, we use chain-of-thought\nprompting (Wei et al., 2022) to generate reasoning\npaths. The chain-of-thought demonstration input\n$D_{COT}$ consists of m ground-truth examples, where\neach example is a pair consisting of a problem\nquestion and its corresponding reasoning path.\nLet M be the base model, and we sample the\nreference reasoning path P by prompting the model\nwith the chain-of-thought demonstration $D_{CoT}$ and\nthe given question Q. We use temperature sampling\n(Fan et al., 2018) with a fixed temperature T:\n$P \\sim M(D_{CoT}, Q|T)$   (2)\nWe consider the generated path as correct if it\nconcludes with a correct answer. Therefore, we\ndefine the following function F to verify if the last\nstep $S_n \\in P$ contains the ground-truth answer A:\n$F(P)=\\begin{cases}\n1 & \\text{if } A \\in S_n \\\\\n0 & \\text{otherwise}\n\\end{cases}$    (3)\nIf the outputs are incorrect, i.e., $F(P_i)$ = 0, we\nrepeat the sampling and verification process until\n$F(P_i)$ = 1 with a cap of 10 attempts, i.e., i < 10.\nIf no suitable path is obtained after multiple at-\ntempts, we deem that this problem is far beyond the\nability of the model and remove it from the training\nset. Thus, we obtain an initial dataset $D_{init}$ which\ncontains the original questions, the ground-truth\nanswers, and model-generated reference paths.\nTo consider potential mistakes that can occur fromeach reasoning step, the exploration stage of our\nframework explores multiple branches at each step.\nConcretely, given the problem Q, chain-of-thought\ndemonstration, and previous steps of the generated\nreasoning path $P_{1:i-1} = (S_1, S_2, ..., S_{i-1})$, we\nuse temperature sampling (Fan et al., 2018) to ob-\ntain diverse branches from the current point in the\nreasoning path:"}, {"title": "Reasoning Optimization", "content": "$B_i \\sim M(D_{CoT}, Q, P_{1:i-1}|T)$,   (4)\nwhere each branch $B_i = (S'_i, S'_{i+1},\u00b7\u00b7\u00b7, S'_n)$ should\ncontain the current step up to the final step. We aim\nto obtain a favorable branch $B^+$ and an unfavorable\nbranch $B^-$ where the favorable branch leads to the\ncorrect final answer, and the unfavorable branch\ndoes not:\n$F(B^+) = 1, F(B^-) = 0$   (5)\nTo achieve this, we iteratively sample multiple\nbranches starting at each step $S_i$ and verify each\none using the function F, until we obtain one favor-\nable branch and one unfavorable branch, thus form-\ning a reasoning branch pair $(B^+, B^-)$. However, if\nwe are unable to form a branch pair after sampling\nat most ten branches, the problem is removed from\nthe training set. This ensures that the training data\nonly includes problems where the model can poten-\ntially learn from contrasting between the favorable\nand unfavorable branches of the reasoning path.\nTo optimize the base model M, we consider both\nthe reference reasoning path P generated in Sec.\n2.3 and the reasoning branch pairs $(B^+, B^-)$ ex-\nplored in Sec 2.4. Concretely, we encourage the\nmodel to produce a higher likelihood over the refer-\nence reasoning path. This is achieved by applying\nstandard causal language modeling loss (Bengio\net al., 2000) on the reference reasoning path P,\nconditioned on the input question Q:\n$L_{ref} = -log P_r^M(P | Q)$   (6)\nRegarding the branch pair, the comparison be-\ntween them may reveal the proper direction that\nguides the model's optimization. Therefore, we\ndefine a branch pair loss that provides contrastive\nfeedback between the favorable and unfavorable\nbranches. To formulate the branch pair loss in\nour framework, we can leverage preference-based\nobjectives from existing work, such as the direct\npreference (Rafailov et al., 2023) or the odds-ratio\nobjective (Hong et al., 2024). In this work, we\nmainly focus on the objective proposed by Hong\net al. (2024) due to its simplicity and empirical ef-fectiveness. Concretely, the branch pair loss $L_{bp,i}$\nat the i-th step can be computed as the log odd-ratio\nbetween the favorable branch $B^+$ and unfavorable\nbranch $B^-$, conditioned on the input question Q\nand reference path P:\n$L_{bp,i} = log \\frac{odds_M (B^+ | Q, P)}{odds_M(B^- | Q, P)}$   (7)"}, {"title": "Experiments", "content": "The odds of generating a branch can be computedas the ratio between the probability of generatingthe branch and the probability of not generatingit, conditioned on the input question Q and the\nprevious steps $P_{1:i-1}$ of the reference path:\n$odds_M(B_i | Q, P) = \\frac{P_r^M(B_i | Q, P_{1:i-1})}{1 \u2013 P_r^M(B_i | Q, P_{1:i-1})}$   (8)\nThus, we can aggregate the loss over the previ-ously explored branch pairs corresponding to each\nstep in the reasoning path:\n$L_{exp} = \\frac{1}{n-1} \\sum_{i=1}^{n} - logo (L_{bp,i})$   (9)\nwhere there are n steps in the reasoning path. Wefollow Hong et al. (2024) to apply the log-sigmoid\nfunction logo on the log odds-ratio for optimiza-tion purposes. Finally, the overall loss $L_{RPO}$ in\nour framework is represented as the combination\nof the reference path loss $L_{ref}$ and the exploration\nloss $L_{exp}$ which provides contrastive feedback over\nthe explored branch pairs:\n$L_{RPO} = L_{ref} + \\lambda \\cdot L_{exp}$   (10)\nwhere $\\lambda$ is a hyperparameter weight, which intu-itively balances between optimizing on the refer-\nence reasoning path, and the explored branches.\nWe would like to clarify that we compute the loss\nonly on the output tokens. In this case, the output\ntokens only consist of the incorrect last part, while\nthe correct prefixes serve as the input tokens, which\nare excluded from the loss calculation as shown in\nFigure 2. Specifically, the reasoning exploration\nstage in our framework first collect branch pairs\nfrom each step along a reference path, then ag-gregates the branch pair losses conditioned on the\ninput question and the previous steps of the refer-ence path. Therefore, the common prefix between\nthe favorable and unfavorable branch is excluded\nin the loss calculation."}, {"title": "Datasets", "content": "As we focus on enhancing the step-by-step reason-ing ability of large language models, we evaluate"}, {"title": "Implementations", "content": "our approach on datasets of various difficulty lev-els, including GSM8K (Cobbe et al., 2021b) formath word problems and MATH (Hendrycks et al.,\n2021b) for competition-level mathematics. We use\nthe original training, validation, and testing data\nsplits for our training and evaluation setup. On the\nother hand, we also include the MMLU (Hendrycks\net al., 2021a) exam question dataset to evaluate the\neffectiveness of our approach in other domains.\nHowever, as many of the exam questions focus onworld-knowledge and do not require multi-step rea-soning, we extract a subset covering 3375 questions\nin the science, technology, engineering, and math\n(STEM) domains, and denote this as the MMLU-STEM dataset. The dataset details can be found inAppendix A.2.\nNote that our Reasoning Paths Optimization-framework does not necessitate large-scale anno-tated reasoning paths for training LLMs. On the\ncontrary, for each task, we only need a small num-ber of reasoning demonstrations for implementing\nCoT prompting, which is easy to obtain. Specifi-cally, we randomly select four questions from the\ntraining data and use their ground-truth reasoning\npath as CoT demonstrations during the reasoning\ngeneration stage. For the remaining procedure,\nReasoning Paths Optimizationonly involves theground-truth answer to verify the correctness of\nthe explored branch. We include the prompt exam-ples in Appendix A.3.\nTo evaluate our approach, we implement Mistral-7B and LLaMA-3-8B as our base models, whichare recent and popular foundation large language\nmodels in the Mistral (Jiang et al., 2023) and\nLLaMA (Touvron et al., 2023a) model families\nrespectively. To our knowledge, these are the lead-ing foundation models in this parameter size cat-egory at the time of writing. To investigate howour approach affects models of different trainingstages, we also include experiments show that ourframework also benefits the LLaMA-3-8B-Instructversion in Appendix A.4, which has undergonegeneral instruction-tuning (Touvron et al., 2023a)to enhance performance in many aspects. Due tocomputational resource constraints, we are unfor-tunately unable to train larger model versions suchas LLaMA-3-70B in this work. To avoid potentialconfounding factors, we do not evaluate on modelsthat already have extensive math-specific training,\nsuch as Llemma (Azerbayev et al., 2024). To train"}, {"title": "Comparison Methods", "content": "the models, we use LoRA fine-tuning (Hu et al.,2022) with a fixed batch size of 8 and a learningrate of 5e-5. More training details and hyperpa-rameters can be found in the Appendix A.1. Tosample multiple outputs from the models, we use afixed sampling temperature of 0.5. For evaluation,we use greedy decoding for generation, and theaccuracy metric for scoring.\nTo demonstrate the effectiveness of our approach,we compare against strong baselines includingreasoning-specific training methods and preference-based optimization methods:\n1. Supervised Fine-Tuning (SFT): As a super-vised baseline, we consider the case of notusing any reasoning paths for training, andonly training the model to directly generatethe ground-truth final answer.\n2. Rejection Sampling Fine-Tuning (RFT) (Yuanet al., 2024): We include RFT as a strong base-line for supervised training, which leveragesthe model to self-generate reasoning paths fortraining. We note that this approach is analo-gous to the reasoning generation stage in ourframework, which aims to overcome the datalimitation of not having ground-truth reason-ing paths.\n3. Direct Preference Optimization (DPO)(Rafailov et al., 2023): As our methodcontrasts the favorable and unfavorablereasoning branches, it is similar in motivationto DPO which provides the model withcontrastive feedback."}, {"title": "Effect of Exploration Weight", "content": "4. Odds-Ratio Preference Optimization (ORPO)(Hong et al., 2024): Lastly, we compareagainst ORPO which proposed the odds ratioobjective for preference-based optimization.The main difference between our approachand ORPO is that Reasoning Paths Optimiza-tion is a holistic framework specifically de-signed for reasoning-based tasks; We considerthat reasoning mistakes are liable to occurat any step in the reasoning path, and henceexplore the possible solution paths which arenecessary to provide contrastive feedback overdiverse reasoning branch pairs.\nTo ensure a fair comparison between differentmethods, we implement the data setting such thateach method uses all viable training samples. Forinstance, SFT uses all the training samples as thedata setting stipulates that all samples contain thequestion and ground-truth final answers. On theother hand, RFT uses only the samples for whichthe model can generate at least one correct reason-ing path, and the preference-based methods DPOand ORPO use only the samples for which themodel can generate at least one correct reasoningpath and one incorrect reasoning path. Similar toour approach, the baselines other than SFT usea fixed temperature for sampling reasoning pathswith chain-of-thought prompting. If the model isunable to generate a correct reasoning path aftersampling a maximum of ten times, the given ques-tion is removed from the training set."}, {"title": "Main Results", "content": "To demonstrate the effectiveness of ReasoningPaths Optimization, we compare with strong base-lines as shown in Figure 3. We observe that ourapproach shows consistent improvements in per-formance on different datasets and models. Par-ticularly when trained on top of Mistral-7B, ourapproach can achieve up to 3.1% and 4.3% im-provement compared to the highest-performingbaseline on GSM8K and MMLU-STEM respec-tively. Given that MATH is a relatively difficulttask, the base models may struggle to generate thecorrect paths, thereby limiting the effectiveness ofpath-based methods. Nevertheless, our approachcan still improve other baselines, which shows thatour approach can more effectively learn from theexplored reasoning paths. On the other hand, wefind that SFT performance is lower compared to theother methods trained on self-explored reasoningpaths. This indicates that while it is possible forthe model to directly generate the answer withoutany reasoning steps, it is less effective for morecomplex reasoning problems.\nWe further investigate the performance of ourmethod on commonsense and general reasoningtasks in Appendix A.5. These tasks typically con-sist of straightforward questions that do not requirelengthy reasoning steps, which may possibly con-tribute to the high SFT performance. Nevertheless,when the model is prompted to engage in step-by-step reasoning, our framework outperforms otherpreference optimization approaches, demonstratingits effectiveness in multi-step thinking.\nTo investigate the effect of reasoning explorationwithin our framework, we conduct an analysis on"}, {"title": "Analysis On Reasoning Path Length", "content": "the loss weight $\\lambda$. Specifically, a lowervalueof $\\lambda$ would place greater emphasis on the super-vised loss over the reference path which leads tothe correct answer. On the other hand, a higher\nvalue would place greater weight on the exploredbranches during training, which contrasts betweenthe favorable and unfavorable branches arisingfrom each reasoning step. As shown in Figure4, we find that having a very low $\\lambda$ value leads tosub-optimal results, as it does not place enoughemphasis on the reasoning exploration. On the otherhand, we also find that placing too much weight onexploration also does not benefit the training, as itis still necessary to ground the model sufficiently inthe reference reasoning path. Thus, this shows thatwe need to balance between the optimization ofthe reference reasoning path and the other possibleexploration branches.\nTo investigate the effectiveness of our approachwith respect to the reasoning complexity, we an-alyze the evaluation accuracy in Figure 5. Com-pared to ORPO (Hong et al., 2024) which is thehighest-performing baseline, we observe benefitsfrom Reasoning Paths Optimization for longer rea-soning paths. This may indicate that our approachcan effectively reduce the occurrence of mistakeswhen solving questions that require more complexreasoning. In future work, we believe that potentialdirections include more diverse applications andtasks, such as code generation (Chen et al., 2021;Austin et al., 2021), and code-augmented reasoning(Gao et al., 2023; Li et al., 2023a)."}, {"title": "Case Study", "content": "To examine the qualitative benefits, Table 1 showsan example of outputs produced by different meth-ods for the same question. While SFT provides\nan incorrect and over-simplified answer without\nshowing any working, RFT, DPO, and ORPO di-\nrectly calculate without explanation and thus make\nmistakes at the very first step. In contrast, the de-\ntailed breakdown of steps shows that RPO not only\narrives at the correct answer but does so through a\ncoherent process."}, {"title": "Code-Based Reasoning", "content": "Beyond reasoning in natural language works suchas PAL (Gao et al., 2023) have shown that large\nlanguage models can be prompted to solve reason-ing problems with code. To this end, we haveconducted an analysis to show that our frame-work can also generalize to code-based reasoning.\nConcretely, in our reasoning generation stage, in-stead of generating text-based reasoning paths, weprompt the model with code demonstrations to gen-erate a python program, which is executed to obtainthe output answer. As shown in Table 2, we findsimilar benefits for text-based reasoning and code-based reasoning compared to ORPO, which is ourstrongest baseline."}, {"title": "Effect of Contrastive Objectives", "content": "To demonstrate the robustness of our framework,we have conducted additional experiments using\ndifferent objectives to contrast between favorable"}, {"title": "Effect of Reference Paths", "content": "and unfavorable paths. Specifically, we show thatthe odds-ratio objective (Hong et al., 2024) in ourbranch pair loss can be easily replaced with thedirect preference objective (Rafailov et al., 2023)for the branch pair loss in Equation 7. As shown inTable 3, the consistent benefit across different ob-jectives demonstrates that our framework is robustand outperforms the respective baselines.\nIn our exploration stage, we use the first reasoningpath with the correct answer as the reference path.\nHowever, the correct answer can often be achievedvia different paths. To analyse the effect of differ-ent reference paths, we select a random path withthe correct answer after sampling 10 times. Resultsin Table 4 show that our method remains effectiveeven with this variation, demonstrating its robust-ness across different reference paths. In addition,we analyse the effect of using more reference paths,eg, three correct reference paths. The results showthat our approach can scale to multiple referencepaths to further enhance performance."}, {"title": "Related Work", "content": "Alignment and Preference-Based OptimizationReinforcement learning from human feedback(RLHF) (Christiano et al., 2017b; Ouyang et al.,2022; Xu et al., 2024) is a popular technique thataligns large language models with human pref-erences and to follow instructions (Ghosal et al.,2023; Chia et al., 2024a). During RLHF, a separatereward model is trained to provide scalar value feed-back, which is passed to fine-tune LLMs with PPOalgorithm (Schulman et al., 2017; Ziegler et al.,2019). However, PPO is known to be complex andunstable (Zheng et al., 2023), and the multi-stagetraining of a reward model and a policy model isalso challenging (Meng et al., 2024). Recently,several techniques, including DPO (Rafailov et al.,2023; Pang et al., 2024), IPO (Azar et al., 2023),SimPO (Meng et al., 2024), and ORPO (Hong et al.,2024), have been proposed to eliminate the needfor a reward model, which significantly stabilizeand simplify the training process. They make pair-wise comparisons between two responses gener-ated by the models and push the model to assigna higher likelihood to the favorable response overthe unfavorable one. However, these preference op-timization methods indiscriminately compare thetwo responses in their entirety, overlooking the factthat errors in multi-step reasoning tasks arise onlyat specific steps and their subsequent branches. Inthis work, we propose Reasoning Paths Optimiza-tion which considers each intermediate step."}, {"title": "Conclusion", "content": "Multi-step Reasoning in Language ModelsLarge language models are capable of solving rea-soning tasks by generating solutions in a step-by-step manner (Nye et al., 2022; Wei et al., 2022;Kojima et al., 2022; Fu et al., 2023; Chu et al.,2024). For example, Wei et al. (2022) and Kojimaet al. (2022) demonstrate that by guiding the modelto generate the reasoning steps before generatingthe final answer, the multi-step reasoning capabil-ities of LLMs could be effectively elicited, evenin multimodal settings (Chia et al., 2024b; Zhanget al., 2024). However, LLMs are prone to produc-ing errors during the reasoning process, especiallyfor complex multi-step reasoning tasks (Li et al.,2024; Chia et al., 2023). To mitigate mistakes in thereasoning steps, a straightforward way is to verifythe reasoning paths step-by-step. This encouragesfurther investigations on process supervision. Ue-sato et al. (2022) and Lightman et al. (2024) collecthuman feedback labels for step-level solutions toverify the intermediate steps generated by reason-ing models. Recent studies (Li et al., 2023b; Wanget al., 2024a,b) construct the step-wise labels au-tomatically to prevent costly human annotations.These methods focus on training the verifiers (i.e.,reward models). In contrast, we apply process su-pervision to preference optimization methods, with-out requiring a separate reward model.\nPath Exploration in Artificial Intelligence Theexploration of diverse paths has been widely usedto improve the performance of complex tasks inthe field of artificial intelligence. AlphaGo (Silveret al., 2016) uses Monte Carlo Tree Search (Kocsisand Szepesv\u00e1ri, 2006) to explore a large space ofpossible moves. Similarly, Yao et al. (2023a) lever-age Tree-of-Thought prompting to explore possiblesolution space from LLMs. Other works (Fenget al., 2023; Xie et al., 2023) also design tree-baseddecoding strategies to search for the optimal so-lution. In the area of reasoning tasks, previousworks have explored using self-sampled solutionsfor training (Ni et al., 2023) and tree search forpath generation (Golovneva et al., 2023). Inspiredby these works, we explore the diverse solutionspace generated by language models. Furthermore,we optimize the models with contrastive feedbackfrom both favorable and unfavorable branches dur-ing training. Inspired by these works, we explorethe diverse solution space generated by the models.Furthermore, we optimize LLMs with both favor-able and unfavorable branches during training.\nIn this paper, we introduced a novel training frame-work called Reasoning Paths Optimization (RPO)to enhance the step-by-step reasoning capabilities\nof LLMs. Our approach addresses the challenge\nof complex problem-solving tasks, where each rea-soning step carries the risk of diverging into errors.\nRPO considers diverse reasoning branch pairs andencourages favorable branches at each reasoningstep while penalizing unfavorable ones. Our frame-work is scalable, as it does not rely on large-scalehuman-annotated rationales. Instead, it leverages\nthe model's own generated reasoning paths, mak-ing it adaptable to multi-step reasoning tasks suchas math word problems. Through extensive experi-ments on datasets of varying difficulties, our frame-work provides an effective approach to enhancereasoning, paving the way for more reliable andaccurate problem-solving in complex scenarios."}]}