{"title": "Comparative Study of Neural Network Methods for Solving Topological Solitons", "authors": ["Koji Hashimoto", "Koshiro Matsuo", "Masaki Murata", "Gakuto Ogiwara"], "abstract": "Topological solitons, which are stable, localized solutions of nonlinear differential equations, are crucial in various fields of physics and mathematics, including particle physics and cosmology. However, solving these solitons presents significant challenges due to the complexity of the underlying equations and the computational resources required for accurate solutions. To address this, we have developed a novel method using neural network (NN) to efficiently solve solitons. A similar NN approach is Physics-Informed Neural Networks (PINN). In a comparative analysis between our method and PINN, we find that our method achieves shorter computation times while maintaining the same level of accuracy. This advancement in computational efficiency not only overcomes current limitations but also opens new avenues for studying topological solitons and their dynamical behavior.", "sections": [{"title": "1. Introduction", "content": "Topological solitons are fascinating mathematical constructs with profound implications across various domains of physics and mathematics. These stable, localized solutions of nonlinear differential equations have been pivotal in advancing our understanding of complex phenomena ranging from particle physics to cosmology [1]. The presence of solitons in these fields often provides comprehensive understanding of the dynamics of the system, enabling us to model and interpret physical events that perturbative approaches may fail to elucidate [2].\nHowever, the quest to accurately solve the equations governing topological solitons poses significant challenges. The challenge in solving solitons is that analytical solutions are often not available due to nonlinearity and complexity of the equations, so numerical methods has to be relied upon [3]. In addition, conventional numerical methods consume large amounts of computational resources and require enormous computation time to perform highly accurate analysis."}, {"title": "2. Method", "content": "In this section, we introduce our NN method, which we call Neural Network for Difference Equation (NNDE). The objective of this model is to solve a differential equation with the following form:\n$D(x, f(x), f'(x),\u2026) = 0,$\nwhere in D represents higher-order derivatives of $f(x)$. The function $f(x)$ is sufficiently differentiable with respect to the coordinate x to meet the conditions of the differential equation. While we focus on the problem to find a single function $f(x)$ with respect to one dimensional space coordinate x, the application to higher dimensions and multiple functions is straightforward. Additionally, $f(x)$ must satisfy the boundary conditions of the form:\n$B_{b}(x_{b}, f (x_{b}), f'(x),\u2026) = 0,$"}, {"title": "2.1. NNDE", "content": "We illustrate NNDE model for solving the differential equation (1) with the boundary conditions (2). The structure of the NNDE designed to solve second-order differential equations is illustrated in Figure 1 as an example. This framework can be extended to accommodate higher-order cases as well. The details of each layer in NNDE are as follows:\n(i) The first layer is the input layer, which has one unit that receives x as input.\n(ii) The second layer receives x from the first layer and outputs a vector whose components are x and the neighborhood points of x. For example, (x + dx, x, x - dx) is output when solving a second-order differential equation, where dx is treated as a hyperparameter.\n(iii) The following layers are made up of dense layers. Each of x and its neighboring points, as output from the second layer, is passed through separate dense layers. All dense layers share the same weights.\n(iv) In the output layer, the solution functions corresponding to x and its neighboring points are produced from each dense layer.\nIn practice, the dense layers in the figure comprise four layers with 32, 64, 32, and 1 units, respectively, using the tanh function as the activation function. The summary of this model is shown in Table 1. The right column of the table displays the output shapes for each layer, where \"NONE\" indicates the batch size."}, {"title": null, "content": "The loss function is defined as the sum of the square of the differential equation and the squares of the boundary conditions:\n$L = L_{DF} + L_{BC}$\n$L_{DF} = D(x, f(x), f'(x),\u2026)^{2},$\n$L_{BC} = \\sum_{b} B_{b}(x_{b}, f (x_{b}), f'(x_{b}), ...)^{2}.$\nIn $L_{DF}$ and $L_{BC}$, the derivatives $f'(x), f''(x),\u2026\u2026$ are discretized:\n$f'(x) = \\frac{f(x + dx) - f(x - dx)}{2dx},$\n$f''(x) = \\frac{f(x + dx) + f(x \u2013 dx) \u2013 2f(x)}{dx^{2}}$\nA key distinction between NNDE and PINN lies in the method used for calculating derivatives. NNDE employs a difference method for approximating derivatives, where finite differences are used to estimate the derivatives of the solution function. In contrast, PINN utilizes automatic differentiation (autodiff) to compute derivatives directly within the NN framework. Autodiff leverages the backpropagation and is capable of calculating exact gradients with respect to network parameters. This method is often more flexible in handling continuous functions, as it does not rely on spatial discretization. However, since autodiff involves backpropagation computation not only for the gradient of the loss function but also for the derivatives of the function, PINN may result in longer computation time. In the following sections, we shall compare the accuracy and the computation time of NNDE with those of PINN for some nonlinear field theories that possess soliton solutions.\nThe NN models are implemented in this study with the use of TensorFlow. The following hyperparameters are commonly used in this paper: optimizer = Adam, dx = $10^{-2}$, epochs = 10000."}, {"title": "2.2. Field equations", "content": "In this section, we describe the field equations to which we will apply NN models: the \u03c64 theory and the Sine-Gordon model. Both field theories are well-known for possessing"}, {"title": "2.2.1. $\u03c6^{4}$ theory", "content": "The differential equation for the \u03c64 theory in one-dimensional space is given by\n$f''(x) + m^{2} f(x) \u2212 \\lambda f(x)^{3} = 0.$\nHere, $m^{2}$ and \u03bb are mass sqauared and the coupling constant respectively, and we assume time-independence. In this theory, there are two distinct stationary points in the field space: $f_{0} = \u00b1\\sqrt{m^{2}/\\lambda}$. It is well-known that this theory admits a topological kink solution that interpolates between these stable states, expressed as\n$f_{kink}(x) = f_{+} \\tanh [\\frac{m}{\\sqrt{2}} (x-a)],$\nwhere x = a is the position of the kink center. In fact, this solution asymptotically approaches the stationary points such that $f(x = \u00b1\u221e) = f_{0}$. For numerical calculations, it is inefficient to consider the entire real axis of x, so we apply a coordinate transformation $\\tilde{x} = \\tanh(x)$, mapping R to [-1,1]. The equation of motion in terms of $\\tilde{x}$ is\n$(1-\\tilde{x}^{2})^{2} \\frac{d^{2} f}{d\\tilde{x}^{2}} - 2 \\tilde{x}(1-\\tilde{x}^{2}) \\frac{d f}{d \\tilde{x}} + m^{2} f \u2013 \\lambda f^{3} = 0,$\nIn addition, the asymptotic behavior $f(\u00b1\u221e) = f_{0}$ is reformulated as the boundary conditions:\n$f(\u00b11) = f_{\u00b1}, f(0) = 0.$\nHere, the condition f(0) = 0 is imposed to specify the center of mass of the kink. For simplicity, in the following we will denote $\\tilde{x}$ as x.\nTo summarize, D and $B_{b}$ in (3), which contributes to the loss function, are given by\n$D = (1 \u2212 x^{2})^{2} f''(x) - 2x(1 \u2212 x^{2}) f'(x) + m^{2} f(x) \u2013 \\lambda f(x)^{3},$\n$B_{b} = f(x_{b}) \u2013 f_{b},$\nwhere $(x_{b}, f_{b}) = (\u22121, f_{\u2212}), (0, 0), (1, f_{+})$. In our numerical computation, we set m = \u03bb = 1."}, {"title": "2.2.2. Sine-Gordon equation", "content": "The time-independent form of the Sine-Gordon equation is given by\n$f''(x) - \\sin f(x) = 0.$\nThere are series of stationary points expressed as f(x) = 2\u03c0n. Similar to the $\u03c6^{4}$ case, this equation admits a topological kink solution of the form\n$f(x) = 4 \\arctan e^{x-a},$"}, {"title": null, "content": "where x = a represents the position of the kink center. This solution interpolates f = 0 and f = 2\u03c0 such that the asymptotic behaviors are f(-x) = 0 and f(x) = 2\u03c0. Again, to perform numerical calculations efficiently, we use the coordinate transformation $\\tilde{x} = \\tanh(x)$, which maps the entire real line R to the interval [-1,1]. Rewriting the equation in terms of $\\tilde{x}$ yields:\n$(1-\\tilde{x}^{2})^{2} \\frac{d^{2} f}{d\\tilde{x}^{2}} - 2 \\tilde{x}(1-\\tilde{x}^{2}) \\frac{d f}{d \\tilde{x}} - \\sin f = 0.$\nThe boundary conditions for the kink are set as:\n$f(-1) = 0, f(0) = \u03c0, f(+1) = 2\u03c0.$\nHere, f(0) = \u03c0 specifies the center of the kink. Again, for simplicity, we replace $\\tilde{x}$ with x in the following equations. The contributions to the loss function for this problem are defined as:\n$D = (1 - x^{2})^{2} f''(x) \u2013 2x(1 - x^{2}) f'(x) \u2013 \\sin f(x),$\n$B_{b} = f(x_{b}) \u2013 f_{b},$\nwhere the boundary points are $(x_{b}, f_{b}) = (-1,0), (0, \u03c0), (1, 2\u03c0)."}, {"title": "2.3. Training condition", "content": "We use both NNDE and PINN methodologies for the $\u03c6^{4}$ theory and the Sine-Gordon equation, both of which have known exact analytical solutions. We compare the NNDE results with these exact solutions to assess the accuracy of our approach. Additionally, we perform a similar evaluation with PINN to determine the accuracy of its results. Lastly, we compare the computational efficiency of NNDE and PINN in terms of both computational time and accuracy.\nAs depicted in Figure 1, our model uses unsupervised learning such that the training data consists only of the input x, which is the coordinate value. The training and test data use values ranging from \u22121.0 to 1.0 for both the $\u03c6^{4}$ theory and the Sine-Gordon equation. For the test data, we use values ranging from -1.0 to 1.0 with a discretization step size of $10^{-2}$. On the other hand, we use various discretization step sizes for the training data and various batch sizes to ensure a comprehensive comparison between NNDE and PINN. The discretization step sizes for the training data are set to $10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}$, and $10^{-5}$, and the batch sizes are set to 10%, 20%, 25%, and 50% of the total data points.\nFor each combination of step size and batch size, we perform the training and measure two metrics: the Mean Squared Error (MSE) between the model's predictions and the exact solutions for the test data, and the computation time for training. We then average the MSE and computation time results across all combinations to provide a comprehensive comparison. We compare the predictions of each model with the exact solutions for this test data to evaluate their accuracy."}, {"title": "3. Results", "content": "Here we present a comprehensive comparison between NNDE and PINN for both $\u03c6^{4}$ theory and Sine-Gordon equation."}, {"title": "3.1. Kink soliton of $\u03c6^{4}$ theory", "content": "The exact kink solution for m = \u03bb = 1 to be compared to the NN predictions is\n$f_{true}(x) = \\tanh [\\frac{1}{\\sqrt{2}} \\tanh^{-1}(x)].$\nWe compare each model's predictions with this exact solution to verify their accuracy."}, {"title": "3.2. Kink soliton of Sine-Gordon equation", "content": "The exact form of the kink solution of the Sine-Gordon equation is\n$f_{true}(x) = 4 \\arctan(\\exp(\\tanh^{-1}(x))).$\nSimilar to the $\u03c6^{4}$ theory, we compare each model's predictions with the exact solution for the Sine-Gordon equation. Figure 2b presents a comparison between the exact solution and the predictions by NNDE and PINN for the Sine-Gordon equation. This plot, generated using a step size of $10^{-5}$ and a batch size of 10%, shows that both models closely match the exact solution. Similar results are seen with various selections of step size and batch size. As shown in Table 2, NNDE achieved an average MSE of 0.014344, which is close to the average MSE of 0.015208 obtained with PINN, resulting in a relative difference of only 5.68%. However, NNDE requires significantly less computation time, averaging 1702.768 seconds compared to 3144.233 seconds for PINN, representing an 45.84% reduction in time. In addition, Figure 4 shows the trends in accuracy and computation time for different step sizes and batch sizes, further emphasizing NNDE's advantage in computational efficiency for the Sine-Gordon equation."}, {"title": "4. Summary and discussions", "content": "In this study, we developed a novel neural network model, which we refer to as NNDE, aimed at solving differential equations. Specifically, we investigated the effectiveness of NNDE in finding topological soliton solutions to non-linear field equations, such as those in the $\u03c6^{4}$ theory and the Sine-Gordon equation. The performance of NNDE was assessed based on computational time and the accuracy of the solutions under various choices of discretization step sizes and batch sizes. Overall, the results show that NNDE can achieve accuracy comparable to that of PINN while significantly reducing computation time across both the $\u03c6^{4}$ theory and the Sine-Gordon equation.\nThe significant difference in computation time may stem from the method used to calculate derivatives. Although both of NNDE and PINN use backpropagation to compute the derivative of the loss function with respect to the model parameters, PINN necessitates additional backpropagation steps to perform the automatic differentiation to obtain the derivatives of the target functions. These backpropagation steps are computationally"}, {"title": null, "content": "intensive, as they involve calculating gradients with respect to each parameter in the network. Specifically, higher-order derivatives require more backpropagation steps, leading to longer computation times. In contrast, NNDE employs a difference method to approximate derivatives, which avoids the need for repeated backpropagation steps. By discretizing the differential equations directly, NNDE could compute the necessary gradients more efficiently, leading to faster training times.\nWhile higher-order derivatives in PINN require additional backpropagation steps, NNDE necessitates the use of more neighboring points to compute the differences corresponding to these higher-order derivatives. This implies that NNDE consumes more GPU memory resources.\nThe proposed method for solving topological solitons using NN holds significant potential for the broader scientific community. This approach offers a novel and efficient way to address nonlinear differential equations, which play a crucial role in various fields of physics and applied mathematics. By reducing computational time, our method enables researchers to analyze complicated soliton dynamics using limited computational resources. This opens up new opportunities for those working in fields such as nonlinear optics, condensed matter physics, and spintronics, where the study of solitons is critical for advancing both theoretical understanding and practical applications.\nBeyond its direct applications, our work on NNDE may inspire further advancements in the study of other nonlinear systems. Given the universality of nonlinear dynamics in nature, from biological systems to financial models, our approach could be adapted and applied to a wide range of problems. This would expand its impact beyond physics to other fields that require the efficient analysis of complicated dynamical systems."}]}