[{"title": "3.4 Categorization", "authors": ["Christopher J. MacLellan", "Erik Harpstead", "Vincent Aleven", "Kenneth R. Koedinger"], "abstract": "The literature on concept formation has demonstrated that humans are capable of learning concepts\nincrementally, with a variety of attribute types, and in both supervised and unsupervised settings.\nMany models of concept formation focus on a subset of these characteristics, but none account for\nall of them. In this paper, we present TRESTLE, an incremental account of probabilistic concept\nformation in structured domains that unifies prior concept learning models. TRESTLE works by\ncreating a hierarchical categorization tree that can be used to predict missing attribute values and\ncluster sets of examples into conceptually meaningful groups. It updates its knowledge by partially\nmatching novel structures and sorting them into its categorization tree. Finally, the system supports\nmixed-data representations, including nominal, numeric, relational, and component attributes. We\nevaluate TRESTLE's performance on a supervised learning task and an unsupervised clustering\ntask. For both tasks, we compare it to a nonincremental model and to human participants. We\nfind that this new categorization model is competitive with the nonincremental approach and more\nclosely approximates human behavior on both tasks. These results serve as an initial demonstration\nof TRESTLE's capabilities and show that, by taking key characteristics of human learning into\naccount, it can better model behavior than approaches that ignore them.", "sections": [{"title": "1. Introduction", "content": "Humans can improve their performance with experience. To better understand these capabilities,\nnumerous research efforts have constructed computational models of human learning (Vanlehn et al.,\n1992; Fisher & Langley, 1990; Li et al., 2012c; Laird et al., 1986; Langley et al., 2009b). Early work\non human learning embraced categorization as a primary mechanism for organizing experiences,\nrecalling them in new situations, and using them to make decisions (Feigenbaum & Simon, 1984;\nFisher & Langley, 1990). In contrast, most recent work in cognitive architectures emphasizes the\ngeneration of solutions to problems or the execution of actions (Langley et al., 2009b). However,\ncategorization and conceptual understanding remain crucial aspects of cognition. For example, there\nis evidence that humans spend more time on learning to recognize the conditions for an action than\non learning the steps needed to perform it (Zhu et al., 1996). Thus, we argue that more research\nshould be conducted on human categorization and the role it plays in learning and problem solving."}, {"title": "2. The RumbleBlocks Domain", "content": "To investigate human learning in a rich domain, we have explored data from RumbleBlocks (Christel\net al., 2012), an educational game designed to teach concepts of structural stability and balance to\nyoung children. What makes this domain interesting is the detailed structural quality of game tasks.\nPlayers build a tower out of blocks in a two-dimensional, continuous world with a realistic rigid\nbody physics simulation, designing their towers to cover a series of energy balls in an attempt to\npower an alien's spaceship. These energy balls function as both scaffolding and\nconstraints on players' designs. After designing their tower, players place the spaceship on the\ntower, which charges the ship and causes an earthquake. If the tower survives the earthquake with\nthe spaceship intact, then the player succeeds and goes on to the next level; otherwise he fails and\nmust try the level again.\nEach level of the game is designed to emphasize one of three primary concepts of structural\nstability and balance: objects that are symmetrical, that have wide bases, and that have lower centers\nof mass are more stable. While the drag-and-drop actions of tower construction are necessary to\nsucceed in the game, they are not relevant its educational goals. The central learning challenge for\nplayers of RumbleBlocks is to understand how to categorize the states of the game world as good or\nbad examples of the game's target concepts.\nRumbleBlocks provides several interesting hurdles when attempting to model players' concep-\ntual learning. The biggest challenge is in representing the richly structured space of the game. States\nexist in a continuous space but also possess nominal (e.g., block types) and structural information\n(e.g., high level patterns like arches). Additionally, the physics simulation that models the earth-\nquake is nondeterministic and susceptible to minor perturbations in rigid body physics. Therefore,\ntwo towers that appear similar have a small chance of being treated differently in terms of the suc-\ncess criteria, resulting in noisy evaluation feedback. This noisy feedback makes it more challenging\nto learn the correct concepts and makes ordering effects more pronounced; if a borderline tower\nfails early in a student's problem sequence, then he will be more likely to avoid similar structures\nthroughout his play than if the tower had succeeded. Finally, RumbleBlocks lets students partake in\nboth supervised and unsupervised learning. They receive supervised feedback regarding the success\nof their towers, but are left to learn the game's target concepts (i.e., wide base, low center of mass,\nand symmetry) in an unsupervised way.\nAn early attempt exploring how players learn in RumbleBlocks sought to model the acquisition\nof structural patterns as a grammar induction task through a process called CFE or Conceptual\nFeature Extraction (Harpstead et al., 2013). CFE, which is similar to the grammar learning approach\nused by SimStudent (Li et al., 2012a), takes a series of examples and discretizes them to a grid\nbefore exhaustively generating a two-dimensional context-free grammar that is capable of parsing\neach example in every possible way. The parse trees for each example are then converted into feature\nvectors using a bag of words approach, where each symbol in the grammar has its own feature. The\ngenerated feature vectors are then used by traditional learning methods for prediction or clustering.\nThe goal was to construct features that capture the maximal amount of structural information in the\nexamples so that this information could be used in learning.\nThe CFE approach was successful for clustering student solutions in order to understand how\nplayers' design patterns differed from those expected by the game's designers (Harpstead et al.,\n2013), but it has not been evaluated as part of a model of human categorization in RumbleBlocks.\nThere are several obstacles to using CFE to model human categorization. First, the algorithm runs\nin a batch fashion instead of incrementally and requires all the examples for each task to generate its\ngrammar. Thus, any examples with previously unseen features would require the learning of an en-\ntirely new grammar. Second, the grammar rule formalism used by CFE breaks down in continuous\nenvironments and when two-dimensional objects cannot be cleanly decomposed across axes. For\nexample, when given a set of blocks in a spiral pattern, it cannot decompose the structure along the\nhorizontal or vertical axes; subsequently, it cannot reduce the structure to the primitive grammar el-\nements and fails at parsing the block structure. Finally, CFE and other similar grammar approaches\n(Talton et al., 2012) cannot handle missing data in that they cannot partially match during parsing.\nThus, if an attribute is missing from the training set, then examples containing that attribute cannot\nbe parsed during run time. Similarly, if an attribute is always present in training but not at run time,\nthen some examples would not be parsable."}, {"title": "3. The TRESTLE Model", "content": "In order to better model human concept formation in the RumbleBlocks domain we developed\nTRESTLE,\u00b9 a system that incrementally constructs a categorization tree given the sequential pre-\nsentation of instances. This categorization tree can then be used to make predictions about the given\ninstances or to generate cluster labelings. At a high level, TRESTLE proceeds through three major\nsteps when given an instance:\n1. Partial matching, which structurally renames the instance to align it with existing concepts;\n2. Flattening, which converts a structured instance into an unstructured one while preserving\nits structural information with a specific naming scheme; and\n3. Categorization, which incorporates the example into existing knowledge and, if necessary,\nmakes predictions.\nIn this section we describe how TRESTLE represents both instances and concepts. We also clarify\nthe processing steps that it carries out to learn from each instance."}, {"title": "3.1 Instance and Concept Representations", "content": "TRESTLE uses two representations to support learning: one for instances (i.e., specific examples\nit encounters) and one for concepts (i.e., generalizations of examples in memory). Instances are\nencoded in TRESTLE as sets of attribute values, where each attribute can have one of four types:\nnominal (e.g., the type or color of a block); numeric (e.g., the position of a block in continuous\nspace); component (e.g., a block with type and dimension attributes is a component of a greater\ntower); and relational (e.g., that two blocks are vertically adjacent). In this\nexample, we include the \u201cOn\u201d relation to demonstrate how relations are encoded, but in practice\nthe component information is often sufficient for learning meaningful concepts. Our subsequent\nevaluation does not use these hand-generated relations; we only used the component and success\ninformation that was automatically provided by the game engine.\nIn response to being presented with instances, TRESTLE forms a hierarchy of concepts, where\neach concept is a probabilistic description of the collection of instances stored under it. This prob-\nabilistic description is stored in the form of a probability table that tracks how often each attribute\nvalue occurs in the underlying instances. These probability tables can be used\nto lookup the probability of different attribute values occurring in an instance given its concept la-\nbel. Additionally, the concept maintains a count of the number of instances it contains, so that the\nprobability of the concept given its parent concept can be computed."}, {"title": "3.2 Partial Matching", "content": "When presented with an instance, TRESTLE searches for the best partial match between the in-\nstance and the root concept, which contains the attribute-value counts of all previous instances."}, {"title": "3.3 Flattening", "content": "After matching the instance to the root concept, TRESTLE then flattens the instance using two pro-\ncedures. First, it converts all relational attributes directly into nominal attributes. Second, compo-\nnent attributes are eliminated by concatenating component and attribute names into a single attribute\nname. Throughout the paper we denote this conventionally using dot notation. For examples, the\ninstance in Figure 2 would be flattened as {Successful: \"False\", Component1.type: \u201cUFO\u201d, Com-\nponent1.angle: 0.0, Component1.left: 0.1, Component1.right: 2.8, \u2026\u2026\u2026, \u201c(On Component1 Compo-\nnent2)\u201d: \u201cTrue\u201d, \u201c(On Component2 Component3)\u201d: \u201cTrue"}, ".", "The flattening process effectively\neliminates the component and relational attributes while preserving their information in a form that\ncan be used during partial matching to rename later instances. Flat representations can be converted\nback into a form that contains component and relational attributes. Once converted into a flat repre-\nsentation the instances only contain nominal and numeric attributes that can be handled by existing\napproaches to incremental categorization."], "content": "To categorize flattened instances, TRESTLE employs the COBWEB algorithm (Fisher, 1987; McKu-\nsick & Thompson, 1990), which recursively sorts instances into a hierarchical categorization tree.\nAt each concept encountered during sorting, it considers four possible operations to incorporate the instance into its tree: adding the instance to the most similar child concept;\ncreating a new child concept to store the instance; merging the two most similar child concepts\nand then adding the instance to the resulting concept; and splitting the most similar child concept,\npromoting its children to be children of the current concept, and recursing. COBWEB determines\nwhich operation to perform by simulating each action and computing the category utility (Fisher,\n1987) of the resulting child concepts. Like the similarity value being maximized during partial\nmatching, this represents the increase in the average number of expected correct guesses achieved\nin the children compared to their parent concept; thus it is similar to the information gain heuristic\nused in decision-tree induction (Quinlan, 1986). Mathematically, the category utility of a set of\nchildren {$C_1, C_2,\uff65\uff65\uff65, C_n$} is\n$CU({C_1, C_2,\uff65\uff65\uff65,C_n}) = \\frac{\\sum_{k=1}^n P(C_k) [\\sum_i \\sum_j P(A_i = V_{ij}|C_k)^2 - \\sum_i \\sum_j P(A_i = V_{ij})^2]}{n}$,\nwhere $P(C_k)$ is the probability of a particular concept given its parent, $P(A_i = V_{ij}|C_k)$ is the\nprobability of attribute $A_i$ having value $V_{ij}$ in the child concept $C_k$, $P(A_i = V_{ij})$ is the probability\nof attribute $A_i$ having value $V_{ij}$ in the parent concept, and n is the number of child concepts. Each\nof these terms can be efficiently computed via a lookup of the probability tables stored in the parent\nand child concepts.\nFor numeric attributes, COBWEB uses a normal probability density function to encode the\nprobability of different values of a numeric attribute. Given this assumption, the sum of squared\nattribute-value probabilities is replaced with an integral of the squared probability density function,\nwhich in the case of a normal distribution is simply the square of the distribution's normalizing\nconstant. Thus, $\\sum_i \\sum_j P(A_i = V_{ij}|C_k)^2 \u2013 \\sum_i \\sum_j P(A_i = V_{ij})^2$ is replaced with $\\sum_i \\frac{1}{\\sigma_{ik}} - \\frac{1}{\\sigma_{i}}$ in\nthe case of numeric attributes, where $\\sigma_{ik}$ is the standard deviation of values for the attribute $A_i$ in\nchild concept $C_k$ and $\\sigma_{i}$ is the standard deviation of values for the attribute $A_i$ in the parent concept.\nFor more detailed justification of this modification to category utility, see Gennari et al. (1989)."}, {"title": "3.5 Prediction and Clustering", "content": "During learning, COBWEB uses this categorization technique to incorporate new instances into its\nconceptual hierarchy. The resulting tree can then be used to make predictions about novel instances\nor provide clusterings of the examples to varying depths of specificity.\nWhen TRESTLE makes predictions, it follows its normal partial matching and flattening pro-\ncedures but employs COBWEB's non-modifying categorization process. In this version of catego-\nrization, an instance is sorted down the tree, but concepts do not update their probability tables to\nreflect the example. Additionally, the system only considers the creating and adding operations at\neach concept. When categorization encounters a leaf or a situation where the creating operation has\nthe highest category utility, it returns the current concept. The resulting concept's probability table\nis then used to make predictions about attribute values of the instance, with each missing attribute\npredicted to have its most likely value according to the table.\nIn addition to prediction, TRESTLE can cluster data by using COBWEB to categorize instances\ninto its concept tree and assign labels based on the selected concepts. Using this process, TRESTLE\nproduces a hierarchical clustering of the data. Alternatively, TRESTLE can convert this hierarchical\nclustering into a flat clustering by labeling all instance by the most general unsplit label (initially\nthe root concept for all instances). For more specific cluster labels, TRESTLE progressively applies\nCOBWEB's splitting procedure to the most general unsplit concept labels until a desired level of\nspecificity is achieved."}, {"title": "4. Experimental Evaluation", "content": "We designed TRESTLE to model three key aspects of human concept learning: incremental process-\ning, use of multiple information types (nominal, numeric, component, and relational), and operation\nin both supervised and unsupervised settings. Further, implicit in this design is the claim that, by\ntaking these three aspects of human categorization into account, we can better model human cate-\ngorization. Thus, we had two goals for our experimental evaluation: to demonstrate TRESTLE's\nabilities to perform incremental learning with multiple types of information in both supervised and\nunsupervised capacities, and to show that, by taking these into account, it models human perfor-\nmance better than similar systems that do not.\nTo demonstrate TRESTLE's functionality, we assessed its behavior on two tasks: a supervised\nlearning task and an unsupervised clustering task. For each task we used player log data from the\nRumbleBlocks domain, which contains nominal, numeric, and component information. Addition-\nally, we wanted to test our claim that the system models human categorization better than alternative\nones. To achieve this goal, we compared TRESTLE to CFE, a nonincremental approach to learn-\ning in structured domains. To establish a baseline for the comparison, we had humans perform\nboth tasks. We then compared the behavior of both systems to the human behavior to assess which\napproach better models the latter's learning."}, {"title": "4.1 Task 1: Supervised Learning", "content": "For the supervised learning task, each learner (TRESTLE, CFE, and human) was sequentially pre-\nsented with RumbleBlocks towers and asked to predict if the tower successfully withstood the earth-\nquake. They were then given correctness feedback about their prediction (i.e., provided the success\nlabel). Instances for this task were taken randomly from all player solutions to the same level of\nRumbleBlocks. To avoid a naive strategy of base rate prediction, we chose a level whose overall\nsuccess ratio was roughly even (a symmetry level in which 48.9% of player towers stood). We\npresented each learner with 30 randomly selected and randomly ordered examples and averaged the\naccuracy for each approach across opportunities.\nTo determine human behavior for this task, we used Amazon Mechanical Turk to have 20 hu-\nmans perform sequential prediction using the interface shown in Figure 5. We asked participants\nto decide if a screenshot of an example tower fit into \u201cCategory 1\" (successful) or \u201cCategory 2\u201d\n(unsuccessful) before telling them the correct category. We presented labels abstractly to avoid hu-\nman raters using their intuitive physics knowledge, which would not be accessible to either of the\ncomputational models.\nCFE is a nonincremental approach for handling structural information that must be paired with\nanother learning algorithm in order to do prediction or clustering. To sequentially predict the success\nlabeling of solutions, we paired CFE with CART, a decision-tree learner implemented in the Scikit-"}, {"title": "4.2 Task 2: Unsupervised Clustering", "content": "The results of the supervised learning task show that TRESTLE performs similarly to humans,\nbut we are also interested in demonstrating its unsupervised learning capabilities and investigating\nwhether its underlying representation of knowledge is qualitatively similar to that in humans. A\nstrong similarity between their organizations of knowledge would strengthen our claim that it offers\na better model of human learning than CFE. In particular, if its organization of concepts aligns with\nthat of humans, it would suggest that the agreement with humans on the supervised learning task\nis the result of a similar organization of knowledge rather than both learners performing poorly\nby chance. To examine this aspect of TRESTLE, we used an unsupervised learning task in which\ntowers were clustered without any information about their success or other category labels. For this\nevaluation, we chose records from three levels of RumbleBlocks that were part of an in-game test.\nThese levels were attractive because they have many player solutions and because the energy ball\nmechanic was removed for the test setting, allowing for a wider variance of player solutions than\nother levels.\nTo establish a human baseline, we had two researchers from our laboratory independently hand\ncluster screenshots of all the player solutions to each level. These raters were told to group solutions\nthat looked similar and to ignore any blocks that did not appear to be part of the solution tower\n(e.g., blocks off to the side of the frame), but were not given any other guidance. To measure\nagreement between raters, we calculated the adjusted Rand index between their clusterings (Rand,\n1971). This measure is a generalization of Cohen's Kappa that allows for raters to use different\nnumbers of categories. Values range from -1 to 1, with random clusterings producing an average of\nzero. The average obtained across all three levels was 0.88, which is high. Given the reasonably high\nagreement between raters, we chose one of the human clusterings to be used in further comparisons\nwith the machine approaches. The labeling from this rater had an average of 33 clusters across the\nthree levels.\nAs a comparison, we applied the CFE process to generate feature vectors for the RumbleBlocks\nsolutions. For each level, we clustered the feature vectors using the G-means algorithm (Hamerly\n& Elkan, 2004), which functions like a wrapper around k-means using a heuristic that tries to form\nclusters with a Gaussian distribution among features to choose a good value for k. This process\nproduced a clustering of solutions for each level. We ran it ten times and we report the average and\nstandard deviation in adjusted Rand index between the CFE and human clusterings.\nTo examine TRESTLE's ability to cluster RumbleBlocks solutions, we had it create a catego-\nrization tree for each level, with instances categorized into the tree in a random order. After all\ninstances had been categorized once, we shuffled them and categorized them a second time, taking\nthe concepts into which they were sorted as their labeling. We could have clustered instances using\na single run, but we were interested in trying to maintain parity to the humans, who were given\nall the examples at once and allowed to recategorize them. To model this task we categorized the\ninstances twice to give TRESTLE the opportunity to form an initial categorization tree from all the\nexamples once before committing to labels.\nBoth the human labeling and the CFE clustering produced flat clusterings, so for comparison\npurposes we transformed TRESTLE's hierarchical clustering into a nonhierarchical one. As men-\ntioned earlier, a flat clustering can be produced by splitting concepts in the tree starting at the root\nand returning the most general unsplit concept as a label for each instance. This process can be done\nwith an arbitrary number of splits; for example, zero splits will return the root concept as the label\nfor every instance, one split will return the children of the root as concept labels, and two splits will\nreturn a more refined set of concept labels produced by further splitting one of the children. This\nprocess is similar to the one used to produce flat clusterings from agglomerative techniques. Given\nthat the number of splits is arbitrary, we report the results for the first three splits of the categoriza-\ntion tree to provide a sense for how the clusterings at different levels of the hierarchy agree with\nhuman clusterings. As with the CFE evaluation, we repeated this process ten times and we report\nthe average and standard deviation in adjusted Rand index between the human labelings and each\nsplit of the TRESTLE labelings."}, {"title": "5. Discussion", "content": "One interpretation of our results is that TRESTLE is a better model of human behavior while CFE\nis a better machine learning approach, in that it achieves greater performance. Both our prediction\nand clustering results support this interpretation. First, TRESTLE's predictive accuracy is closer\nto human predictive accuracy, while CFE has higher accuracy than both TRESTLE and humans.\nFurther, TRESTLE's organization of knowledge more closely agrees with humans than the organi-\nzation produced using CFE. The two approaches differ, in part, because of the different motivations\nunderlying their designs. CFE was originally created as a means of transforming instances in a\nstructured domain into feature vectors so that other traditional learning algorithms (e.g., CART or\nk-means) could function in the space. It was designed primarily for use in offline settings, such\nas post-hoc analysis of RumbleBlocks solutions, rather than for making online decisions. This is\nwhy the algorithm makes use of exhaustive grammar generation and computes features based on\nall possible parses of structures, in an attempt to maximize its coverage of the space despite the\nadditional training costs. TRESTLE, on the other hand, arises out of existing theory on human\ncategorization and takes a probabilistic approach to the problem of coverage. Rather than focus on\nmultiple potential interpretations of a tower it looks at probabilistic similarity with existing knowl-\nedge. Additionally, TRESTLE maintains more psychological plausibility by avoiding exhaustive\nprocesses; for example, it does not seem very plausible that new players of RumbleBlocks have a\npreexisting RumbleBlocks grammar, nor that they are visually parsing towers using many grammar\nrules (CFE generated about 6,000 for RumbleBlocks). In summary, our results support the claim that\nTRESTLE is a better model of human behavior than CFE because it is incremental and optimizes\nfor the prediction of any attribute.\nOur results also serve as an initial demonstration of TRESTLE's abilities to operate in domains\nwith nominal, numeric, component, and relational attributes. While machine learning approaches\nexist that support each of these characteristics individually, rarely are they integrated into a single\nsystem. Further, ones that do support numeric, nominal, and relational attributes, such as FOIL\n(Quinlan, 1990), do not typically operate incrementally. Thus, we argue that TRESTLE is one of\nthe few systems that supports all of these capabilities, making it a novel contribution and particu-\nlarly well suited for modeling human categorization in a wide range of settings. For example, there\nis an active debate about whether blocking or interleaving similar types of problems is better for\nstudent learning (Carvalho & Goldstone, 2013). One hypothesis is that blocking problems supports\nstudents' ability to map structure across similar problems and thus supports the acquisition of struc-\ntural knowledge. The contrasting hypothesis is that interleaving problems better supports students'\nability to identify discriminating features. Previous work with intelligent tutors has suggested that\ninterleaved instruction is preferable (Li et al., 2012b), but this work first induced a grammar in batch\nfrom all examples (similar to CFE), effectively pre-blocking learning of common structure. Given\nthis pretrained structural knowledge, it makes sense that the model would learn best from inter-\nleaved problems because it only needed to identify discriminating features. In contrast, a model like\nTRESTLE would be well qualified to explore the issue of blocked vs. interleaved instruction, as it\nmodels the incremental acquisition of both structural information and discriminating features.\nIn addition to supporting multiple attribute types, our analysis demonstrates that TRESTLE\nsupports both supervised and unsupervised learning. We argue that this makes it a more general\nmodel of learning than CFE, which employed separate machine learning algorithms (i.e., CART\nfor prediction and k-means for clustering) for the two tasks. In the current work, we evaluated the\nsystem on separate supervised and unsupervised tasks, but TRESTLE should also support hybrid\ntasks that involve partial supervision. We claim that this capability makes TRESTLE ideal for\nmodeling human learning in semi-supervised settings. In summary, our studies demonstrate the\nwide range of settings that TRESTLE supports and provide evidence that it is a better models human\nbehavior than CFE in the RumbleBlocks domain."}, {"title": "6. Related Work", "content": "We have mentioned previously that the TRESTLE model builds on earlier accounts of concept for-\nmation in the COBWEB family (Fisher, 1987; McKusick & Thompson, 1990; Gennari et al., 1989).\nHowever, we should also discuss how it relates to these previous systems and other similar mod-\nels of human categorization that attempt to accomplish similar goals. One early account of human\ncategorization was EPAM (Feigenbaum & Simon, 1984), which modeled human memorization of\nnonsense syllables. It learned incrementally from sequentially presented training examples and took\nstructural information (i.e., the organizations of letters) into account. Additionally, EPAM organized\nits experiences using a discrimination network, which has a tree structure similar to that used by\nTRESTLE. COBWEB (Fisher, 1987) extended this idea to take into account attribute-value proba-\nbilities during concept formation. Further, COBWEB/3 (McKusick & Thompson, 1990) added the\nability to handle numeric information. However, these newer approaches did not support the ability\nto handle structural information originally supported by EPA\u041c.\nCLASSIT (Gennari et al., 1989) was another variant of COBWEB that supported both numeric\ninformation and component information. Like TRESTLE, it used both partial matching and flat-\ntening steps to handle component information during categorization. However, CLASSIT lacked\nsupport for both nominal or relational information. LABYRINTH (Thompson & Langley, 1991)\nwas developed to support component, nominal, and relational information. Like TRESTLE, it\nutilized a partial matching step to rename component objects in order to maximize category util-\nity. During categorization, the system supported components by categorizing subcomponents in a\nbottom-up fashion and replacing component values with nominal concept labels. LABYRINTH\nalso generalized subcomponent labels during the categorization of higher-level components. This\nbottom-up categorization approach let it utilize subcomponent information to improve prediction of\nhigher-level attributes, but it could not use higher-level component information when categorizing\nsubcomponents. In contrast, TRESTLE's flattening mechanism takes into account the attribute-\nvalues of all components in a holistic way when making predictions. Finally, neither CLASSIT nor\nLABRYINTH have been compared to humans or other systems, so it is unclear how their perfor-\nmance would compare to that of TRESTLE.\nThere also exist other systems that have similar goals, but do not derive from the COBWEB\nfamily. For example, SimStudent (Li et al., 2014) shares TRESTLE's aim of modeling novice\nlearning via a combination of statistical and structural learning. As in TRESTLE, SimStudent com-\nbines both unsupervised (perceptual representation) and supervised (skill) learning. One key differ-\nence is that TRESTLE is more unified (with a single learning mechanism) and fully incremental,\nwhereas SimStudent has four learning mechanisms (one for representations and three for skills),\nwith representation learning running in batch prior to skill learning. For comparison purposes, CFE\nis analogous to SimStudent, in that they use similar grammar learners and classification/clustering\nalgorithms. To the extent that CFE is a good approximation of SimStudent, our results suggest that\nTRESTLE may model human learning better than SimStudent in that the latter, like CFE, is likely\nto learn faster than humans. More generally, our evidence supports the hypothesis that an incre-\nmental integration of representation and classification learning better models human behavior than\nseparate, nonincremental, representation and classification learners. Elements of skill learning that\ngo beyond classification (e.g., SimStudent's action-sequence learner) are targets for further research\non TRESTLE.\nSAGE (Kuehne et al., 2000; McLure et al., 2010) is another system that shares the goal of\nmodeling human concept formation in structured domains. It learns concepts by matching new\ninstances to existing structures and generalizing matching concepts to include the instances. If no\nmatch is found, then it creates a new concept to cover the instance. For comparison purposes, SAGE\nis analogous to a form of TRESTLE that is nonhierarchical; it maintains only the children of the\nroot and it cannot split merged concepts because it does not maintain more specific variants. This\ndifference makes it harder for SAGE to recover from early misconceptions and biases it towards\nforming overly abstract concepts (Kuehne et al., 2000). Thus, in many respects TRESTLE could\nbe viewed as an extension of SAGE that overcomes these limitations and adds support for nominal\nand numerical information. Another key difference is that SAGE partially matches each instance\nto every concept, whereas TRESTLE only partially matches instances to its root concept.\u00b2 This\ndifference makes its partial matching more efficient (one match vs. many matches), but it is unclear\nhow it affects performance. Future versions of TRESTLE should explore this tradeoff. One strength\nof SAGE is that it agrees with data on human behavior (Kuehne et al., 2000). It would be interesting\nto explore how the differences between the two models impact their agreement with observations.\nIn summary, the literature includes several incremental and nonincremental models of concept\nformation. However, each model tends to support only a subset of attribute types and rarely is their\nbehavior compared with that of other systems or with humans. TRESTLE is novel in that it unifies\nfacets of previous incremental approaches by supporting concept formation over nominal, numeric,\nstructural, and relational information. It also extends systems like SAGE to support concept revi-\nsion. Finally, its behavior on RumbleBlocks compares favorably to that found in humans."}, {"title": "7. Conclusion and Future Work", "content": "In this paper, we presented TRESTLE and demonstrated its ability to incrementally learn concepts\nfrom mixed-data environments in both a supervised and unsupervised fashion. However, our eval-\nuation of the system is preliminary, and more work is necessary to fully assess its capabilities. In\nparticular, we should evaluate TRESTLE's ability to perform more flexible prediction tasks (Fisher,\n1987), but first, we must determine how to frame these tasks in structured domains. For example,\nshould we evaluate a system's ability to predict specific attribute values, or should we assess its abil-\nity to predict entire missing components with multiple attribute values? This problem is complicated\nby structure mapping, which makes it difficult to align component predictions during evaluation.\nIn addition to assessing the current system's capabilities, we should integrate it with methods\nfor planning and executing action. For example, we could extend TRESTLE to support \u201cfunctional\"\nattributes that have actions as values. This would let it decide what action to take given an instance.\nUnlike systems that separate concept and skill knowledge (e.g., Langley et al., 2009a), this extension\nwould let TRESTLE use functional information to guide its formation of concepts and vice versa.\nWe should also explore the implications of TRESTLE for supporting designers. Prior work\nshows that concept formation systems can be used in this area. For example, Reich (1993) devel-\noped BRIDGER to support bridge design, but his approach did not support component or relational\ninformation. More recently, Talton et al. (2012) demonstrated how to generate novel designs using\ngrammar patterns induced from examples provided by designers, but their work did not support\nmixed data types or missing attributes. TRESTLE could extend both approaches to support novel\ntasks, such as completing partially specified designs that are described with mixed data types. We\nwould also like to explore how it could help game designers better understand the space of player\nsolutions by clustering solutions with common structural patterns. This application was part of the\noriginal motivation for CFE, but our clustering results suggest that TRESTLE is better suited to the\ntask and may even provide more information through its hierarchical organization of concepts.\nFinally, we should explore how we can use the system as a computational model of human\nlearning in educational settings. In particular, we would like it to stimulate student's learning in"}]