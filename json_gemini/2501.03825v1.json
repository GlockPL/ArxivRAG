{"title": "Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in Ultrasound Imaging", "authors": ["Simon W. Penninga", "Hans van Gorp", "Ruud J.G. van Sloun"], "abstract": "Ultrasound images are commonly formed by sequential acquisition of beam-steered scan-lines. Minimizing the number of required scan-lines can significantly enhance frame rate, field of view, energy efficiency, and data transfer speeds. Existing approaches typically use static subsampling schemes in combination with sparsity-based or, more recently, deep-learning-based recovery. In this work, we introduce an adaptive subsampling method that maximizes intrinsic information gain in-situ, employing a Sylvester Normalizing Flow encoder to infer an approximate Bayesian posterior under partial observation in real-time. Using the Bayesian posterior and a deep generative model for future observations, we determine the subsampling scheme that maximizes the mutual information between the subsampled observations, and the next frame of the video. We evaluate our approach using the EchoNet cardiac ultrasound video dataset and demonstrate that our active sampling method outperforms competitive baselines, including uniform and variable-density random sampling, as well as equidistantly spaced scan-lines, improving mean absolute reconstruction error by 15%. Moreover, posterior inference and the sampling scheme generation are performed in just 0.015 seconds (66Hz), making it fast enough for real-time 2D ultrasound imaging applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Ultrasound systems perform sequences of pulse-echo experiments, called transmit events, to form an image. Due to the physical speed of sound, the optimization of these transmit events constitutes a trade-off between frame rate, depth of view, and image quality, making acquisition time a major limiting resource. By reducing the amount of transmit events required to form an image, the effective budget one can spend on this trade-off improves greatly. In addition, subsampling can reduce data transfer and battery drain, enabling cheaper and more portable ultrasound systems.\nEfficient subsampling and signal recovery can be achieved with Compressed Sensing [1], in which sparsity in some signal domain is used for effective reconstruction from compressed measurements, such as an undersampled Fourier spectrum and or observations from sparse arrays. Contemporary recovery methods go beyond signal sparsity and use deep learning to exploit the signal structure learned from the training data. In particular, deep generative models explicitly learn signal priors that can subsequently be used for inference. Such approaches have also been used in the context of ultrasound imaging [2, 3]. Deep learning also enables the optimization of the subsampling schemes themselves [4]. For instance, Deep Probabilistic Subsampling (DPS) [5] uses an end-to-end deep learning training method that finds the optimal subsampling strategy for downstream recovery tasks. Additional examples include subsampling of RF data through deep learning [6] and randomized channel subsampling for increased ultrasound speeds [7]. However, the aforementioned methods have in common that the learned subsampling masks are fixed, and their optimization does therefore not benefit from any information gained across the sequential sampling process at inference time. Conversely, adaptive sensing methods exploit previously acquired data to optimize future sampling schemes across a sequence of observations to improve performance [8].\nIn this paper, we propose an active subsampling method for ultrasound imaging that: (1) exploits a deep generative latent variable model and combines it with a deep Bayesian posterior encoder that performs fast inference of the parameters of its approximate latent posterior from partial observations; (2) designs adaptive subsampling schemes that maximize information gain on the fly across a sequence of ultrasound image frames in a video. Specifically, we optimize the evidence lower bound and train a deep neural network to estimate the parameters of the intricate latent posterior state distribution under partial observations, which we parameterize using a Sylvester normalizing flow [9]. Based on samples from this posterior, we subsequently design a new sampling scheme that optimizes an estimate of the expected information gain, by maximizing the marginal entropy for future observations. Both steps of the approach are illustrated in Fig. 1.\nMost related to our approach, van de Camp et al. recently proposed the use of deep generative latent variable models for adaptive subsampling designs [10]. While effective, the method relied on Markov Chain Monte-Carlo methods for generating samples from the posterior, rendering inference prohibitively slow for time-sensitive applications such as ultrasound imaging. Moreover, the scene was considered static, and observations of this static scene were taken one at a time. In contrast, we here operate on sequences of ultrasound frames and design full subsampling masks for each next frame sequentially. Using the Sylvester normalizing flow-based posterior encoder (requiring only a single neural function evaluation), we reduce inference time by several orders of magnitude, enabling real-time processing rates, while retaining the ability to fit intricate posteriors.\nThe remainder of this paper is organized as follows; Section II describes the problem setup for ultrasound line-scanning, our approach to fast posterior inference, and the design of sampling schemes based on mutual information. In Section III the method is applied to sequences of ultrasound frames, and compared against non-adaptive baselines. Finally, in Section IV, we conclude and outline future work."}, {"title": "II. METHODS", "content": "Let a partial observation of a video frame at a given time-step $y_t$ be defined as:\n\n$Y_t = A_tx_t + n_t$,\n\nwhere $A_t \\in \\mathbb{R}^{M \\times N}$ is the binary subsampling matrix (with $M << N$), $x_t \\in \\mathbb{R}^{N}$ is the fully-sampled vectorized video frame that we will refer to as the image state, and $n_t$ is the added noise at the time of observation $t \\in [0,1,..., T]$. The goal is to (1) perform efficient estimation of the Bayesian posterior for image states $p(x_t | y_t, A_t)$, and (2) design an optimal future sampling matrix $A_{t+1}$ that maximizes expected information gain.\nUnfortunately, computing the true Bayesian posterior quickly turns intractable in high dimensions. To overcome this, we use a deep latent variable model that approximates the true distribution of signals $p(x) \\approx \\int p_{\\theta}(x|z)p(z)dz$ using a simplified and lower-dimensional latent distribution $p(z)$, with $z \\in \\mathbb{R}^{D_z}$ and $D_z << N$. Our goal then becomes to infer $p(z_t | y_t)$. When confronted with strongly subsampled image states and highly ambiguous observations, i.e. $M << N$, this posterior will nevertheless remain intricate and often multi-modal."}, {"title": "B. Deep Sylvester Posterior Inference", "content": "To model the complex distribution $p(z_t|y_t) \\approx q_{\\phi}(z_t|y_t)$, we use a Sylvester Normalizing Flow (Sylvester-NF). The model architecture is an extension of the Variational Auto Encoder (VAE) [11] and consists of a convolutional image encoder and decoder. The encoder $q_{\\phi}$ outputs the latent Gaussian distribution parameters $\\mu(y_t) \\in \\mathbb{R}^{D_z}$, $\\sigma(y_t) \\in \\mathbb{R}^{D_z}$ and Normalizing Flow [12] parameters $\\Lambda \\in \\mathbb{R}^{N_P}$ for subsequent transformations of the Gaussian distribution, with $N_P$ the parameters of the normalizing flow layers. To train the image encoder, we minimize its variational free energy. Given an observation $\\hat{y}_t$ we can formulate the variational free energy as:\n\n$- \\mathcal{F}(\\theta, \\phi; \\hat{y}_t) = \\mathbb{E}_{q_{\\phi}(z_t^K |\\hat{y}_t)} [\\log P_{\\theta}(Y_t = \\hat{y}_t | z_t^K) - \\log q_{\\phi} (z_t^K | \\hat{y}_t) - \\log p(z_t^K) + \\sum_{k=1}^{K} \\log \\det (J[z_t^k(\\Lambda_k)])]$,\n\nin which $z_t^0 \\in \\mathbb{R}^{D_z}$ is a sample drawn from $\\mathcal{N}(\\mu(\\hat{y}_t), \\sigma(\\hat{y}_t))$ and $z_t^k \\in \\mathbb{R}^{D_z}$ the same sample warped through $k \\in [1,..., K]$ flow layers. Here, $J$ denotes the Jacobian matrix and $\\Lambda_k$ the transform parameters of layer $k$. Note that we leave the dependency on the subsampling mask that generates the observations $\\hat{y}_t$ implicit throughout this paper.\nThe generative latent variable model is first pre-trained using a dataset of full observations $x_t \\in \\mathcal{X}$ to capture the signal prior $\\int p_{\\theta}(x_t|z_t)p(z_t)dz$, after which the weights $\\theta$ are frozen and the inference model $q_{\\phi}(z_t|y_t)$ can be trained for a dataset of partial observations."}, {"title": "C. Sampling Policy", "content": "Our sampling policy is to maximize the information gain of future observations, which is equivalent to minimizing the expected posterior entropy [13]. The action-conditional mutual information between future latent states $Z_{t+1}$ and observations $y_{t+1}$ for a greedy (one-step-ahead) policy is given by:\n\n$I(y_{t+1}; Z_{t+1}|A_{t+1},\\hat{y}_t) = H(y_{t+1}|A_{t+1}, \\hat{y}_t) - H(y_{t+1}|Z_{t+1}, A_{t+1})$.\n\nWe leave the exploration of a longer action horizon to future work. Since the entropy of expected observations $y_{t+1}$ given $Z_{t+1}$ does not depend on $A_{t+1}$ (it depends only on the noise $n_{t+1}$), our policy reduces to the maximization of the marginal entropy as:\n\n$A_{t+1} = \\arg \\max_{A_{t+1}} [H(y_{t+1}|A_{t+1}, \\hat{y}_t)]$.\n\nThe marginal entropy scales with the log determinant of the co-variance matrix $\\Sigma_{y_{t+1}|A_{t+1}}$, which we estimate using the generative model $p_{\\theta}(x_t|z_t)$ and a sample aggregate of the posterior $q_{\\phi}(z_t|\\hat{y}_t)$, assuming an identity transition $z_{t+1}|z_t$:\n\n$\\mathbb{E}_{y_{t+1} | A_{t+1},\\hat{y}_t} [(\\Sigma_{y_{t+1} | A_{t+1}})] = \\mathbb{E}_{q_{\\phi}(z_t|\\hat{y}_t)} [(y_{t+1} - \\mu_{y_{t+1}})(y_{t+1} - \\mu_{y_{t+1}})^T]$\n\n$= A_{t+1} \\mathbb{E}_{q_{\\theta}(z_{t+1}|\\hat{y}_t)} [(x_{t+1} - \\mu_{x_{t+1}})(x_{t+1} - \\mu_{x_{t+1}})^T] A_{t+1}^T$\n\n$= A_{t+1} \\frac{1}{N_s} \\sum_{i=1}^{N_s} (x_{t+1}^{(i)} - \\mu_{x_{t+1}})(x_{t+1}^{(i)} - \\mu_{x_{t+1}})^T A_{t+1}^T$,\n\nwhere $N_s$ denotes the number of drawn posterior samples $z_t$. Because the action space for $A_{t+1}$ scales with the binomial coefficient $\\binom{N}{M}$, the computation is generally intractable and we instead explore only a subset of randomly selected actions $\\mathcal{S}_A$. We will refer to this policy as covariance sampling:\n\n$A_{t+1} = \\arg \\max_{A_{t+1} \\in \\mathcal{S}_A} \\log \\det(\\Sigma_{y_{t+1}|A_{t+1}}).$"}, {"title": "III. EXPERIMENTS & RESULTS", "content": "We evaluate our method using the EchoNet dataset [14], which contains 10,030 4-chamber cardiac ultrasound videos. Each video includes 50-250 grayscale frames with a resolution of 112 \u00d7 112 pixels, captured at 50 Hz. To standardize the data, the pixel values are normalized to the range [0,1], and Gaussian noise $n \\sim \\mathcal{N}(0,0.02)$ is added for improved generalization. The dataset is divided into 6,986 training videos, 500 validation videos for model selection, and 500 test videos for final evaluation. The remaining 2,044 videos are excluded due to artifacts or missing data.\nWe convert Cartesian images into polar coordinates, with depth $r=\\sqrt{x^2 + y^2}$ and scan-line angle $\\psi = \\arctan(\\frac{y}{x})$. To subsample full scan-lines, $A_t$ becomes highly structured and selects $M_{\\psi} < N$ columns in the polar domain (i.e. $N = N_rN_{\\psi}, M = N_rM_{\\psi})$.\nOur model architecture consists of a variational encoder, orthogonal Sylvester flow layers, and decoder. The encoder comprises 10 gated convolutional layers [15] with stride 2, each using c = 64 channels, reducing the input to a 512-dimensional latent variable $z_0 = \\mu + \\sigma \\cdot \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, I)$, as per the reparameterization trick. The latent space is further refined using K = 8 normalizing flow steps, each parameterized by 16 orthogonal vectors ($N_P = 16 \\times 512$), to obtain the final latent representation $z_K$. The decoder uses 8 blocks of gated transpose convolutions with stride 2, each using c = 128 channels. These layers are followed by Batch Normalization [16] and GELU activations [17]. The final image is reconstructed through a head comprising 3 additional convolutional layers, each with c = 128 channels.\nWe train the inference model with the loss function defined in (2) and we set $\\beta = 1 \\times 10^{-4}$ for both the generative model and the inference model. We compensate for the polar coordinate transformation by using the density of the inverse transformation as a per-pixel weighing on the training loss. In an attempt to capture all modes for a given state of observation, the IWAE [18] algorithm is used, which tightens the ELBO.\nWe compare the two proposed sampling policies against three baseline methods: uniform random sampling, variable density random sampling, and equispaced sampling. In uniform random sampling, independent samples from a uniform distribution are used to select the l columns for each frame. Variable density sampling uses a similar approach, but samples from a polynomial distribution centered on the middle of the image with a decay factor of 6 are used. In the equispaced policy, the system deterministically uses evenly spaced lines and shifts the set of lines by one index for each subsequent video frame, maintaining uniform sampling density across all frames. For the trace and covariance-based sampling policies, we use $N_s = 3$ posterior samples from our generative model and generate S = 10,000 random sampling schemes to form the candidate set $\\mathcal{S}_A$ every t. Increasing $N_s$ and S beyond these values resulted in increased computational costs with minimal performance improvement.\nAlthough all subsampling methods share the same generative model, each has a distinct inference model. The training procedure is given in Algorithm 1. The computational cost of the active methods is determined by the summation of the costs of the inference, sampling, image generation, and action selection steps."}]}