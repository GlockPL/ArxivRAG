{"title": "UNIQ: OFFLINE Inverse Q-LEARNING FOR AVOIDING UNDESIRABLE DEMONSTRATIONS", "authors": ["Huy Hoang", "Tien Mai", "Pradeep Varakantham"], "abstract": "We address the problem of offline learning a policy that avoids undesirable demonstrations. Unlike conventional offline imitation learning approaches that aim to imitate expert or near-optimal demonstrations, our setting involves avoiding undesirable behavior (specified using undesirable demonstrations). To tackle this problem, unlike standard imitation learning where the aim is to minimize the distance between learning policy and expert demonstrations, we formulate the learning task as maximizing a statistical distance, in the space of state-action stationary distributions, between the learning policy and the undesirable policy. This significantly different approach results in a novel training objective that necessitates a new algorithm to address it. Our algorithm, UNIQ, tackles these challenges by building on the inverse Q-learning framework, framing the learning problem as a cooperative (non-adversarial) task. We then demonstrate how to efficiently leverage unlabeled data for practical training. Our method is evaluated on standard benchmark environments, where it consistently outperforms state-of-the-art baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) is a powerful framework for learning to maximize expected returns and has achieved remarkable success across various domains. However, applying reinforcement learning to real-world problems is challenging due to difficulties in designing reward functions and the requirement for extensive online interactions with the environment. While some approaches have addressed these challenges, they often rely on costly datasets, requiring either accurate labeling or clean, consistent data, which is often impractical. Imitation learning offers a more feasible alternative, enabling agents to learn directly from expert demonstrations without the need for explicit reward signals. It has proven effective in several tasks, even with limited expert data, and is particularly useful in capturing human preferences.\nMost existing imitation learning approaches prioritize maximizing task performance (i.e., expected return) by closely mimicking expert demonstrations. However, in practice, expert or near-expert demonstrations may be unavailable or insufficient. In many scenarios, instead of high-quality examples, there may be collections of undesirable (or suboptimal) demonstrations that should be avoided. For example, in a large dataset of user conversations used to train a chatbot, the system must learn to avoid inappropriate or sensitive content. Similarly, in the development of self-driving cars, while companies may collect user driving data to train their models, the system must ensure it does not replicate faulty behavior, such as traffic violations or unsafe driving practices. In the field of treatment optimization, data may include actions that led to bad patient outcomes and the system must learn to avoid such behaviors. In these cases, avoiding undesirable behaviors is essential to training a safe and effective model. These types of undesirable demonstrations are common in real-world applications and are crucial for shaping the desired policy.\nAlthough this is an important and interesting problem setting, there has been limited research addressing it effectively. Here, we formulate this challenge as an Offline Reverse Imitation Learning problem: given a dataset containing undesired demonstrations we wish to avoid, alongside a much"}, {"title": "2 RELATED WORK", "content": "Imitation learning. Imitation learning is a key technique for learning from demonstrations. Behavioral Cloning (BC) maximizes the likelihood of expert demonstrations but often struggles due to distributional shift. To improve, Generative Adversarial Imitation Learning aligns the learner's policy with the expert's using GANS , while SQIL assigns simple rewards to expert and non-expert demonstrations to learn a value function. PWIL uses the Wasserstein distance to compute rewards. While these methods show promise, they rely on online interaction, which can be impractical. For offline learning, AlgaeDICE and ValueDICE use Stationary Distribution Correction Estimation (DICE) but face stability issues. Inspired by ValueDICE, O-NAIL introduced an offline method without adversarial training. IQ-learn , a popular approach, supports both online and offline learning and offers a state-of-the-art framework with several variants developed based on it. Unlike the above mentioned works, our focus in this paper is on offline reverse imitation learning that aims to avoid undesirable trajectories, as opposed to imitating expert trajectories. As indicated earlier, this requires fundamentally different methods due to the nature of the problem.\nImitation Learning from Sub-optimal Demonstrations. There are two main research directions in this area. The first focuses on online and offline preference-based imitation learning methods. Online approaches, such as T-REX , PrefPPO , and PEBBLE, leverage ranked sub-optimal demonstrations to learn a preference-based reward function using the Bradley-Terry model. While these methods achieve strong performance, they rely on interaction with the environment. In contrast, offline methods, such as those proposed by , rely heavily on extensive pairwise trajectory comparisons. SPRINQL addressed this limitation by utilizing demonstrations categorized into different levels of expertise, resulting in better performance with fewer comparison demands. While these methods concentrate on imitating preferred (or expert) trajectories, our focus is on avoiding non-preferred (or undesired) trajectories. This important distinction necessitates significant changes in methodology.\nThe second direction focuses the use of additional unlabeled datasets to enhance learning from expert data. Beginning with DemoDICE , several DICE-based methods have been developed to utilize small sets of expert demonstrations, supplemented by larger unlabeled datasets. In addition to these DICE-based methods, DWBC propose a simple and efficient method based on training a classifier using positive-unlabeled learning. SafeDICE presents a DICE-based framework capable of learning from undesirable demonstrations. This method combines an undesirable policy (represented by an undesirable dataset) with a random policy (represented by a larger unlabeled dataset), assigns negative weights to the undesirable policy, and then applies a standard DICE-based approach to mimic the combined policy. In our paper, we advance this line of work by developing an new inverse Q-learning algorithm where the primary goal is to maximize the statistical distance between the learning policy and the undesirable policy. Our method offers several advantages over prior approaches, including minimal hyper-parameter tuning and reduced sensitivity to the quality of the unlabeled data."}, {"title": "3 BACKGROUND", "content": "Preliminaries. We consider a MDP defined by the following tuple $M = (S, A, r, P, \\gamma, \\delta_0)$, where $S$ denotes the set of states, $\\delta_0$ represents the initial state set, $A$ is the set of actions, $r : S \\times A \\rightarrow \\mathbb{R}$ defines the reward function for each state-action pair, and $P : S \\times A \\rightarrow S$ is the transition function, i.e., $P(s'|s, a)$ is the probability of reaching state $s' \\in S$ when action $a \\in A$ is made at state $s \\in S$, and $\\gamma$ is the discount factor. In reinforcement learning (RL), the aim is to find a policy that maximizes the expected long-term accumulated reward $\\max_\\pi {\\mathbb{E}_{(s,a)~\\rho^\\pi}[r(s, a)]}$, where $\\rho_\\pi$ is the occupancy measure of policy $\\pi$: $\\rho_\\pi(s, a) = (1 - \\gamma)\\pi(a|s) \\sum_{t=1}^{\\infty}P(s_t = s|\\pi)$.\nMaxEnt IRL The goal of MaxEnt IRL is to derive a reward function $r(s,a)$ based on a set of expert demonstrations, $D_E$. Let $\\rho_E$ denote the occupancy measure of the expert policy. The MaxEnt IRL framework, as introduced by , aims to recover the expert's reward function by optimizing the following expression:\n$\\max_r \\min_\\pi \\{\\mathbb{E}_{\\rho_E} [r(s, a)] - \\mathbb{E}_{\\rho_\\pi} [r(s, a)] - H(\\pi) - \\psi(r)\\}$, where $H(\\pi) = \\mathbb{E}_{\\rho_\\pi}[-\\log \\pi(s, a)]$ is the discounted causal entropy of the policy $\\pi$ and $\\psi(r) : \\mathbb{R}^{S\\times A} \\rightarrow \\mathbb{R}$ is a convex reward regularizer. In essence, the objective is to identify a reward function that maximizes the gap between the expected reward under the expert's policy and the maximum expected reward across all other policies (as determined by the inner minimization).\nInverse Q-learning (IQ-Learn) from expert demonstrations. Given a reward function $r$ and a policy $\\pi$, the soft Bellman equation is defined as $B^{\\pi}[Q](s,a) = r(s,a) + \\gamma\\mathbb{E}_{s'}[V^{\\pi}(s')]$, where $V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(a|s)}[Q(s, a) - \\log \\pi(a|s)]$. The Bellman equation $B^{\\pi}[Q] = Q$ is contractive and always yields a unique Q solution. In IQ-Learn, they further define an inverse soft-Q Bellman operator $T^{\\pi}[Q] = Q(s,a) - \\gamma\\mathbb{E}_{s'}[V^{\\pi}(s')]$. show that for any reward function $r(a, s)$, there is a unique $Q^*$ function such that $B^{\\pi}[Q^*] = Q^*$, and for a $Q^*$ function in the Q-space, there is a unique reward function $r$ such that $r = T^{\\pi}[Q^*]$. This result suggests that one can safely transform the objective function of the MaxEnt IRL from r-space to the Q-space as follows:\n$\\max_Q \\min_\\pi {\\Phi(\\pi, Q) = \\mathbb{E}_{\\rho_E} [T^{\\pi}[Q](s, a))] - \\mathbb{E}_{\\rho_{\\pi}}[T^{\\pi}[Q](s, a)] - H(\\pi) - \\psi(T^{\\pi}[Q](s,a)))}$                                                                                                                (1)\nwhich has several advantages, namely, the objective function $\\varphi(\\pi, Q)$ is concave in $\\pi$ and convex in Q. Moreover, the inner problem $\\min_\\pi \\varphi(\\pi, Q)$ has a closed form solution as $\\pi^*(a|s) = \\exp(Q(s, a))/\\sum_{a} \\exp(Q(s, a'))$. As a result, the maximin problem can be converted to a non-adversarial problem in the Q-space as:\n$\\max_Q {\\mathbb{E}_{\\rho_E} [T[Q](s, a))] - (1 - \\gamma)\\mathbb{E}_{s_0}[V^Q(s_0)] - \\psi(T[Q](s, a)))}$                                                                                          (2)\nwhere $T[Q](s, a)) = Q(s, a) - \\mathbb{E}_{s'~P(s'|s,a)} [V^Q(s')]$ and $V^Q(s) = \\log (\\sum_a \\exp(Q(s, a)))$, which is a softmax of the Q function. The reward function can then be recovered as $r^?(s,a) = T[Q](s, a)$. Thus, in 2, the objective can be interpreted as training a reward function (via a Q-function) that maximizes the expected reward under the expert policy while minimizing the overall expected reward."}, {"title": "4 UNIQ: INVERSE Q-LEARNING FROM UNDESIRED DEMONSTRATIONS", "content": "We now introduce UNIQ, our framework for inverting a Q function based on undesired demonstrations. This approach can be seen as a reverse version of the standard Inverse Q-learning algorithm , where the goal is not to imitate but rather to avoid undesired behaviors."}, {"title": "4.1 INVERSE Q-LEARNING FROM UNDESIRED DEMONSTRATIONS", "content": "In our setting, we have a set of undesired demonstrations, denoted as $D_{UN}$, along with a supplementary set of unlabeled demonstrations, denoted as $D_{MIx}$. The unlabeled dataset $D_{MIx}$ may contain a mix of random, undesired, and expert demonstrations, and it will be used to support offline learning. Let $\\rho_{UN}$ be the occupancy measure (or stationary distribution) of the undesired policy (represented by the undesired dataset). Adapting the MaxEnt RL framework, we consider the following learning objective:\n$\\min_\\pi \\min_r {\\mathcal{L}(\\pi, r) = \\mathbb{E}_{\\rho_{UN}} [r(s, a)] - \\mathbb{E}_{\\rho_{\\pi}} [r(s, a)] - H(\\pi) + \\psi(r)\\}$                                                                                                                                (3)\nThe objective can be interpreted as finding a reward function that assigns low rewards to the undesired demonstrations and high rewards to others, while also identifying a policy function that maximizes the expected long-term reward. It is important to note that, in our context, where the objective contrasts with the standard learning-from-demonstration scheme, the learning problem is no longer adversarial as in prior imitation learning approaches . Instead, it can be framed as a cooperative learning problem, where the objective is to jointly identify a policy and reward function that minimize the objective function $\\mathcal{L}(\\pi, r)$.\nTo gain a deeper understanding of the objective function in Eq. 3, the following proposition demonstrates that solving Eq. 3 is indeed equivalent to maximizing the statistical distance, parameterized by $\\psi$, between the undesired policy and the learning policy.\nProposition 4.1. For a non-restricted feasible set of the reward function:\n$\\min_\\pi \\min_r {\\mathcal{L}(\\pi, r)} = - \\max_{\\pi} {d_{\\psi}(\\rho^{\\pi}, \\rho^{UN}) - H(\\pi)}$                                                                                                                                   (4)\nwhere $d_{\\psi}(\\rho^{\\pi}, \\rho^{UN}) = \\psi^*(\\rho^{\\pi} - \\rho^{UN})$, and $\\psi^*(t) = \\sup_z \\{(t, z) - \\psi(z)\\}$ is the convex conjugate of the convex function $\\psi$, i.e.,\nIt can be observed that solving 3 directly encourages the learning policy to deviate as much as possible from the undesired policy, which is derived from undesirable demonstrations. This approach contrasts with that of SafeDICE , which minimizes the KL divergence between the learning policy and the mixed policy, enabling the use of standard imitation learning methods. The primary limitation of this approach is that, with the quality of the unlabeled dataset being unknown, imitating the mixed policy may not lead to the desired learning outcome. In contrast, our approach does not rely on such a combination. Instead, we focus on maximizing the statistical gap in 4, leveraging the unlabeled dataset to support the practical training of our primary objective in 3.\nEven though the minimization problem Eq. 3 can be directly solved using standard optimization algorithms to recover a reward and policy function, prior research indicates that transforming Eq. 3 into the Q-space can improve efficiency. As discussed in Section 3, there is a one-to-one mapping between any reward function r and a corresponding function Q in the Q-space. Therefore, the minimization problem in Eq. 3 can equivalently be transformed as:\n$\\min_Q \\min_\\pi {\\mathcal{L}(\\pi, Q) = \\mathbb{E}_{\\rho_{UN}} [T^{\\pi}[Q](s, a))] - \\mathbb{E}_{\\rho_{\\pi}}[T^{\\pi}[Q](s, a)] - H(\\pi) + \\psi(T^{\\pi}[Q](s,a))\\}$                                                                                          (5)\nwhere $T^{\\pi} [Q](s, a)) = Q(s, a) - \\gamma\\mathbb{E}_{s'}[V^{\\pi}(s')]$ and $V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(a|s)}[Q(s, a) - \\log \\pi(a|s)]$\nCompared to the primary objective of the standard IQ-learn algorithm in Eq. 1, our objective function in Eq. 5 is no longer adversarial with respect to Q and $\\pi$. As a result, there are questions regarding whether the key advantages of the IQ-learn algorithm\u2014such as the closed-form for the optimization over $\\pi$ and the concavity of the objective in the Q-space\u2014would still hold with the new objective. To address this, we introduce the following proposition, which states that if the regularizer function $\\psi(\\cdot)$ is non-decreasing, then the objective function $\\mathcal{L}(\\pi, Q)$ is convex in $\\pi$. Furthermore, the minimization problem $\\min_\\pi \\mathcal{L}(\\pi, Q)$ retains a closed-form solution, thereby simplifying the learning objective."}, {"title": "4.2 LEARNING WITH UNLABELED DATA", "content": "To solve Eq. 6, the expectation $\\mathbb{E}_{\\rho_{UN}} [r^Q(s, a)]$ can be empirically approximated using samples from the set of undesired demonstrations $D_{UN}$. However, in our setting, this set is limited. Additionally, since the learning process must be conducted offline, without interaction with the environment, directly using the limited samples in $D_{UN}$ is not effective. Therefore, we leverage the larger set of unlabeled data $D_{MIx}$ to enhance the offline training. To achieve this, we first let $\\rho_{MIx}$ be the occupancy measure (or stationary distribution) of the policy represented by unlabeled dataset. We rewrite the expectation over $\\rho_{UN}$ as:\n$\\mathbb{E}_{\\rho_{UN}} [r^Q(s, a)] = \\sum_{s,a} \\rho^{UN} (s, a)r^Q(s, a) = \\sum_{s,a} \\rho^{MIx} (s, a)\\tau(s, a)r^Q(s, a) = \\mathbb{E}_{\\rho_{MIx}} [\\tau(s, a)r^Q(s, a)]$\nWhere $\\tau(s, a) = \\frac{\\rho^{UN} (s,a)}{\\rho^{MIX} (s,a)}$ represents the occupancy ratio between $\\rho^{UN}$ and $\\rho^{MIx}$, we then rewrite the learning objective as follows:\n$\\min_Q {\\mathcal{F}(Q) = \\mathbb{E}_{\\rho_{MIx}} [\\tau(s, a)r^Q(s, a)] - (1 - \\gamma)\\mathbb{E}_{s_0} [V^Q(s_0)] + \\psi(r^Q)\\}$\nIn this approach, the expectation $\\mathbb{E}_{\\rho_{MIx}} [\\tau(s, a)r^Q(s, a)]$ can be empirically approximated using the unlabeled samples from $D_{MIx}$, where $\\tau(s, a)$ acts as an occupancy correction. This correction allows us to leverage samples from the unlabeled dataset to estimate the expectation over the undesirable policy. A key challenge here is that the occupancy ratio $\\tau(s, a)$ is unknown. To address this, we propose estimating the ratio by solving the following implicit maximization problem:\n$\\max_{\\mu_1, \\mu_2: \\mathbb{R}^{S\\times A}\\rightarrow[0,1]} {g(\\mu_1, \\mu_2) = \\mathbb{E}_{\\rho_{MIx}} [\\log(\\mu_2(s, a) - \\mu_1(s, a)\\mu_2(s, a))] + \\mathbb{E}_{D_{UN}} [\\log(\\mu_1 (s, a) - \\mu_1(s, a)\\mu_2(s, a))]}$                                                                                                                                                                                                                                                                       (7)\nThe above formulation is an extension of the discriminator formulation widely used in prior work . The following proposition theoretically shows that solving 7 will exactly return the occupancy ratio $\\tau$.\nProposition 4.3. $g(\\mu_1,\\mu_2)$ is strictly concave in $\\mu_1, \\mu_2$. Furthermore, let $\\mu_1^*$ and $\\mu_2^*$ be the unique optimal solutions to 7, then we have: $\\tau(s,a) = \\frac{\\mu_1(s,a)}{\\mu_2(s,a)}$.\nSo, our learning process can be broken down into two steps. In the first step, we learn the occupancy ratios by solving the maximization problems presented in 7. Following this, we optimize the following problem to learn a Q function.\n$\\min_Q {\\mathcal{F}(Q) = \\mathbb{E}_{MIx} [\\frac{\\mu_1(s,a)}{\\mu_2(s,a)} r^Q(s, a)] - (1 - \\gamma)\\mathbb{E}_{s_0} [V^Q(s_0)] + \\psi(r^Q)\\}$                                                                                                                                   (8)"}, {"title": "4.3 POLICY EXTRACTION", "content": "The Q function obtained from solving the minimization problem in 8 can be used to recover a soft policy $\\pi^?(s, a) = \\exp(Q(s, a))/\\sum_{a'} \\exp(Q(s, a'))$, as stated in Proposition 4.2 above. However, this approach may suffer from overestimation, a common issue in offline Q-learning caused by out-of-distribution actions and function approximation errors . To address this problem and improve policy extraction, we instead propose using weighted behavior cloning (WBC) with the objective: $\\max_\\pi {\\sum_{s,a} \\exp(A(s, a)) \\log (\\pi(a|s))}$, where $A(s, a)$ is the advantage function defined as $A(s, a) = Q(s, a) - V(s)$. It can be shown that solving the WBC problem yields the exact desired soft policy, i.e., $\\pi^?(a|s) = \\frac{\\exp(Q(s,a))}{\\sum_{a'} \\exp(Q(s,a'))}$, $a, s$, is optimal for the WBC."}, {"title": "5 PRACTICAL IMPLEMENTATION", "content": "Our algorithm consists of two main steps. The first step involves solving 7 to estimate the occupancy ratio $\\tau(s, a) = \\frac{\\mu^*_1 (s,a)}{\\mu^*_2(s,a)}$. In the second step, we use these ratios to train the Q-function and extract a policy by solving a weighted behavior cloning problem.\nIn the first step, we construct two networks, $\\mu_{\\phi_1}(s,a)$ and $\\mu_{\\phi_2}(s, a) \\in [0,1]$, where $\\phi_1$ and $\\phi_2$ are learnable parameters. We then use samples from $D^{UN}$ and $D^{MIx}$ to estimate the expectations, leading to the following practical objective:\n$\\max_{\\phi_1, \\phi_2} {g(\\phi_1, \\phi_2) = \\sum_{(s,a)~D_{MIx}} [\\log(\\mu_{\\phi_2} (s, a) - \\mu_{\\phi_1} (s, a)\\mu_{\\phi_2}(s, a))] + \\sum_{(s,a)~D_{UN}} [\\log(\\mu_{\\phi_1} (s, a) - \\mu_{\\phi_1} (s, a)\\mu_{\\phi_2}(s, a))]}$                                                                                       (9)\nIn the second step, after obtaining $\\phi_1^*$ and $\\phi_2^*$ from the first step, we utilize the following empirical training objective:\n$\\min_w {\\mathcal{F}(w) = \\sum_{(s, a, s')~D_{MIx}} {\\frac{\\mu_1(s, a)}{\\mu_2(s, a)} [r_w(s, a) + \\gamma V^Q_w (s')] - (1 - \\gamma) \\sum_{s_0~D_{MIX}} [VQ_w(s)]\\}}$                                                                                                                                                           (10)\nwhere $w$ are the learnable parameters for the Q-network, and the reward function is computed as $r_{Q_w} (s, a, s') = Q_w(s, a) - \\gamma V^{Q_w} (s')$, with $V^{Q_w}(s) = \\log (\\sum_{a~D_{Mix}} \\exp(Q_w(s, a)))$. Following , we choose the reward regularizer function $\\psi(t) = t - t^2$, i.e., the $\\chi^2$-divergence. To extract a policy, we simply solve the weighted BC problem: $\\max_\\theta \\sum_{(s,a) \\in D_{Mix}} \\exp(Q_w(s, a) - V^{Q_w} (s)) \\log \\pi_{\\theta}(a|s)$, where $\\theta$ are learnable parameter of the policy network. Our main algorithm, UNIQ, is summarized as follows, noting that the training of the Q-function $Q_w$ and the updating of the policy network $\\pi_{\\theta}$ are performed simultaneously to enhance efficiency."}, {"title": "6 EXPERIMENTS", "content": "We assess our algorithm in the context of safe RL (or constrained RL) problems, where each state-action pair is associated with a cost value, and the objective is to learn a policy that satisfies certain cost constraints. We define an undesirable trajectory as one where the accumulated cost exceeds a specified threshold. This setting is similar to the one used in SafeDICE , one of the SOTA algorithms in the context of learning from undesirable demonstrations."}, {"title": "6.1 EXPERIMENT SETUP", "content": "Baselines. We compare our algorithm against several baseline methods. First, we benchmark UNIQ against standard imitation learning algorithms that utilize the entire unlabelled demonstration dataset, specifically BC and IQ-learn, which we refer to as BC-mix and IQ-mix, respectively. Additionally, we benchmark our approach against the SOTA preference-based reinforcement learning algorithm, IPL . Lastly, we include comparisons with DWBC and SafeDICE , both of which are recognized for their ability to learn from undesired demonstrations. More details are provided in the Appendix B.3.\nEnvironments and Data Generation. We evaluate our method in four Safety-Gym and two Mujoco environments . Following the setup from , the undesired policy\u00b9 is trained using unconstrained PPO, while the safe policy is trained with SIM . The unlabeled dataset is constructed by combining trajectories from both safe and undesired policies in a specified ratio. The undesired data consists of trajectories that violate the constraint. Detailed information about the policies and datasets is provided in the Appendix B.2.\nMetrics. The main goal is to train a policy that is safe in the context of constrained RL. Therefore, an ideal outcome is achieving the lowest possible cost without significantly sacrificing return. We report the accumulated return and cost for the trained policies, computed based on the last 20 evaluations, with all results summarized across at least 5 training seeds.\nExperimental Concerns. Throughout the experiments, we aim to address several key questions: (Q1) How does UNIQ perform compared to other baseline methods? (Q2) How does the presence of undesired demonstrations impact the performance of UNIQ and other baselines? (Q3) What happens if the policy is directly extracted from the Q-function, rather than by solving the WBC? (Q4) How does the policy learned by UNIQ compare to a policy trained purely from expert demonstrations? (Q5) How do UNIQ and other baselines perform when being trained without access to the unlabeled dataset? While (Q1) and (Q2) are addressed in the main paper, the experiments for the remaining questions are provided in the appendix. The appendix also includes proofs of the theoretical claims made in the main paper, additional details about our experimental setup, and further results such as CVaR cost comparisons, the complete set of experiments for the MuJoCo tasks, comparisons using datasets from the SafeDICE paper, and detailed learning curves."}, {"title": "6.2 MAIN COMPARISON ON SAFETY-GYM TASKS", "content": "In this section, we aim to evaluate the performance of our method in comparison with the mentioned baselines. We test across three difficulty levels for each environment by varying the amount of safe data (100, 400, and 1600 trajectories) while keeping the number of undesired data fixed at 1600 trajectories. These settings are referred to as env-1, 2, 3, respectively. This allows us to examine how the proportion of the desired and undesired demonstrations in the unlabeled dataset impacts the performance of each baseline.\nOverall, as the difficulty increases, both the cost and return of all methods rise. This is primarily because the undesired data typically yields a much higher return than the safe data (except in the Point-Goal task, where the returns of safe and undesired data are close in return). BC-mix and IQ-mix struggle to distinguish between safe and undesired behaviors, leading to poor cost performance. IPL also fails to capture the correct preference, as most of its pairwise comparisons involve undesired-undesired pairs, making it unable to infer the correct preference. DWBC and SafeDICE manage to achieve relatively high returns but fail to match the cost performance of the Safe Policy. Our method consistently achieves the lowest cost across all experiments. However, in the Point-Button and Car-Button tasks, the return for our method is lower, as it avoids undesired actions, leaving no high-return options to pursue."}, {"title": "6.3 MUJOCO VELOCITY BENCHMARKS", "content": "We test our method with two MuJoCo velocity tasks: Cheetah and Ant. We keep the same size of the unlabelled dataset as difficulty level 2 of the Safety-Gym experiments (Section 6.2), which is composed of 400 trajectories from the desired data and 1600 trajectories from the undesired data. Moreover, due to the nature of the environment, we only require a smaller number of labeled undesired datasets, which is 5."}, {"title": "6.4 ABLATION STUDY - PERFORMANCE WITH DIFFERENT SIZES OF UNDESIRED DATASET", "content": "In this experiment, we evaluate our method using varying sizes of the undesired dataset (25, 50, 100, 200, 300, and 500 trajectories), while keeping the unlabelled dataset fixed (400 desired and 1600 undesired trajectories) across two Safety-Gym environments. The results, reported in Figure 3, include the return, cost, and CVaR 10% cost for each undesired dataset size, where CVaR 10% cost is the mean cost of the worst 10% runs in the evaluation. The training curves are shown in the Appendix D.2. Here, BC-safe refers to Behavioral Cloning with only the desired (or expert) demonstrations, which serves as the highest safety performance benchmark. In general, increasing the size of the undesired dataset tends to reduce the cost for all approaches, and UNIQ shows the greatest effect in utilizing the undesired data. Interestingly, UNIQ is able to achieve a lower cost than BC-safe, which can be explained by the fact that the main goal of UNIQ is to avoid the undesired (i.e., high-cost) demonstrations, while BC-safe lacks this avoidance capability."}, {"title": "7 CONCLUSION, FUTURE WORK, AND BROADER IMPACTS", "content": "Conclusion. We have developed UNIQ, a principled framework based on inverse Q-learning to facilitate learning from undesirable demonstrations. Our algorithm can be seen as a reverse version of standard imitation learning frameworks, where the goal is to maximize the statistical distance between the learning policy and the undesirable policy. UNIQ requires minimal hyper-parameter tuning, as it does not introduce any additional hyperparameters beyond those typically used in inverse Q-learning algorithms. Moreover, it demonstrates superior performance in producing safe policies in several safe reinforcement learning experiments, outperforming other baseline methods.\nLimitations and Future work. There are several aspects that have not been addressed in this paper, as they are too significant to be fully explored here. For instance, we generally assume the presence of only one set of undesirable demonstrations, whereas in practice, multiple datasets of varying quality could be leveraged to enhance the training. Additionally, each undesirable trajectory may not be undesirable in its entirety, as it could contain some good actions. Extracting the good parts from undesirable demonstrations could improve sample efficiency but introduces new challenges that warrant further investigation. Another open question is how to extend the framework to multi-agent settings, which would be both relevant and interesting to explore in future research.\nBroader Impact. Beyond the standard impacts of imitation learning, our work is particularly useful in scenarios where undesirable demonstrations are richer, clearer, or more reliable than desirable ones. This is especially valuable in critical applications like healthcare and autonomous driving, where avoiding harmful actions is essential. However, our approach also carries potential negative impacts. If not carefully designed, the system might unintentionally reinforce undesirable behaviors or learn harmful actions from poorly curated data. Misinterpreting bad demonstrations could result in unintended consequences, particularly in safety-critical contexts. Additionally, there is a risk that malicious actors could misuse this framework to deliberately train AI systems with harmful or unethical behaviors.\nETHICAL STATEMENT\nOur work on the UNIQ framework addresses learning from undesirable demonstrations. This research holds potential for impactful real-world applications, including areas such as autonomous systems, healthcare, and robotics, where avoiding harmful behaviors is crucial. However, it is important to acknowledge the ethical implications and risks associated with our work.\nWhile the UNIQ framework aims to learn safe policies by avoiding undesirable behaviors, there is a potential risk that the algorithm could be misused in ways that reinforce unintended or harmful actions if trained on poorly curated data. For example, in scenarios where undesirable demonstrations are not clearly defined, the model could inadvertently reinforce biased or harmful behaviors. Furthermore, there is the risk of deploying the framework in environments where safety-critical decisions are made without sufficient validation or human oversight, leading to unintended consequences.\nTo mitigate these risks, we emphasize the importance of using well-curated datasets that accurately represent undesirable behaviors and conducting thorough testing in controlled environments before applying the system to real-world scenarios. Moreover, we encourage transparency and collaboration with domain experts to ensure the framework is used responsibly, particularly in safety-critical applications such as healthcare and autonomous driving. Finally, we advocate for the inclusion of human oversight in the deployment of policies learned using the UNIQ framework to ensure ethical and safe outcomes.\nREPRODUCIBILITY STATEMENT\nTo ensure the reproducibility of our work, we have submitted the source code for the UNIQ framework along with the datasets used to generate the experimental results reported in this paper (this source code and datasets will be made publicly available if the paper gets accepted). We also provide comprehensive details of our algorithm in the appendix, including implementation details and key steps required to reproduce the experimental outcomes. Additionally, we have included the hyper-parameter configurations used in all experiments, ensuring that others can replicate the results under the same conditions. We encourage the research community to build on our work and test the UNIQ framework across different environments to validate and extend the findings presented in this paper."}, {"title": "A MISSING PROOFS", "content": "A.1 PROOF OF PROPOSITION 4.1\nProposition. For a non-restricted feasible set of the reward function:\n$\\min_\\pi \\min_r {\\mathcal{L}(\\pi, r)} = - \\max_{\\pi} {d_{\\psi}(\\rho^{\\pi}, \\rho^{UN}) - H(\\pi)}$\nwhere $d_{\\psi}(\\rho^{\\pi}, \\rho^{UN}) = \\psi^* (\\rho^{\\pi} - \\rho^{UN})$, and $\\psi^*$ is the convex conjugate of the convex function $\\psi$, i.e., $\\psi^*(t) = \\sup_z \\{(t, z) - \\psi(z)\\}$.\nProof. We write the objective function as\n$\\mathcal{L}(\\pi,r) = \\sum_{s,a}r(s, a) (\\rho^{UN} (s, a) - \\rho^{\\pi} (s, a)) + \\psi(r) - H(\\pi)$\nSo we can write the minimization of $\\mathcal{L}(\\pi, r)$ over r as"}]}