{"title": "Configurable Foundation Models: Building LLMs from a Modular Perspective", "authors": ["Chaojun Xiao", "Zhengyan Zhang", "Chenyang Song", "Dazhi Jiang", "Feng Yao", "Xu Han", "Xiaozhi Wang", "Shuo Wang", "Yufei Huang", "Guanyu Lin", "Yingfa Chen", "Weilin Zhao", "Yuge Tu", "Zexuan Zhong", "Ao Zhang", "Chenglei Si", "Khai Hao Moo", "Chenyang Zhao", "Huimin Chen", "Yankai Lin", "Zhiyuan Liu", "Jingbo Shang", "Maosong Sun"], "abstract": "Advancements in large language models (LLMs) have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on the instruction to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs, Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.3. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, as the domain of configurable LLMs remains nascent and evolving, we highlight several open issues and directions for future research, including the correlation between emergent and customized bricks, general brick development proto-cols, evaluation of configurable LLMs, efficient brick computing frameworks, and systems consisting of multiple model-level bricks. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.", "sections": [{"title": "1 Introduction", "content": "Large pre-trained models, especially large pre-trained language models (LLMs), have achieved remarkable success in a variety of tasks (Devlin et al., 2019; Brown et al., 2020; Han et al., 2021a; OpenAI, 2023; Touvron et al., 2023b; Zhao et al., 2023b). LLMs have become the foundation models of artificial intelligence applications by providing amounts of world knowledge (Petroni et al., 2019; Shin et al., 2020) and powerful reasoning capabilities (Bommasani et al., 2021). Current advanced LLMs, such as GPT-4 (OpenAI, 2023), are deployed on large-scale central servers with high-bandwidth memory and GPUs to address various user instructions. With the development of LLMs, the future applications of LLMs will inevitably face the following trends, which in turn present challenges for LLMs:\n(1) Deployment on end devices. With the capabilities of LLMs continuing to improve, the trend of deploying these models on devices with limited computing power, such as smartphones and personal computers, is attracting increasing attention, allowing LLMs to serve as personal assistants for millions of users (Apple, 2024; Xue et al., 2024; Hu et al., 2024). The use of monolithic LLMs that require substantial computational resources is gradually becoming infeasible, and improving the computational efficiency of LLMs is a significant challenge. (2) Widespread application across multiple domains. LLMs are widely applied in various fields and applications to enhance people's work efficiency (Kaddour et al., 2023). However, the knowledge and capabilities required by different domains, users, and even different instructions vary greatly. Storing all world knowledge in monolithic LLMs and serving all scenarios with full parameters often leads to redundant computations, and conflicts between different domain knowledge may even result in sub-optimal performance. (3) Rapid evolution in new scenarios. As application scenarios increase and time progresses, we usually need LLMs to efficiently adapt to new tasks and learn from environment feedbacks (Li et al., 2024; Tao et al., 2024). Meanwhile, the world knowledge stored in LLMs is constantly updating and expanding. This demands that LLMs are able to evolve efficiently and continuously, learning new knowledge and skills while avoiding forgetting existing knowledge.\nTo address these issues, studying and analyzing LLMs from a modular perspective has gradually become an important focus for current researchers (Pfeiffer et al., 2023; Fedus et al., 2022a; Ding et al., 2023; Zhang et al., 2023c). These works decompose LLMs into functional modules. In this way, for each computation step, we can only involve part of modules to save computation costs and achieve efficient continual training by constructing or updating modules. Modularity has long been an endogenous mechanism or a central principle in diverse fields, ranging from biomedical sciences to engineering fields (Simon, 1962). A module is conceived as an independent component with specialized functionality that can coordinate and interface with other modules, thereby giving rise to complex systemic behavior. Owing to modularity, many complex systems become more understandable and scalable. For instance, in cognitive neuroscience, the modularity of mind"}, {"title": "2 Configurable Foundation Models", "content": "In this section, we elaborate on the general framework for configurable foundation models, consisting of various bricks. These bricks encompass both the emergent bricks from the pre-training process and customized bricks from post-processing to enhance LLMs. Specifically, we first present that the pre-trained LLMs naturally possess the property of modularity and can be split into bricks with pre-defined structures or self-organized functional neuron clusters (\u00a7 2.1). Then, in the pursuit of advancing LLMs, it is promising to parameterize the external knowledge and capacities into neural bricks, which can be inserted into LLMs in a plug-and-play manner (\u00a7 2.2). Subsequently, we discuss how to select the granularity of bricks to trade off efficiency and effectiveness (\u00a7 2.3). Lastly, we present five benefits to constructing LLMs with configurable bricks, including high efficiency, reusability, traceability, sustainability, and distributed computation (\u00a7 2.4)."}, {"title": "2.1 Emergent Bricks", "content": "The emergent property of modularity has been observed in the pre-training process of language models (Wang et al., 2022a; Zhang et al., 2023c), which indicates that a subset of the parameters can function properly as the entire model for specific instructions. Such property makes it possible to break down the gigantic LLMs, including both dense models (Brown et al., 2020; Touvron et al., 2023a) and sparse models (Lepikhin et al., 2021; Fedus et al., 2022b), into tiny modules. With the breakdown, the aforementioned issues of efficiency and scalability can be tackled via module dropping (Fan et al., 2020; Zeng et al., 2023; Liu et al., 2023d), subnetwork extraction (Frankle & Carbin, 2019; Wang et al., 2020; Xia et al., 2022), and recombination (Gururangan et al., 2022; Zhang et al., 2022c). We term these modules directly broken down from the pre-trained models as emergent bricks, which acquire certain capabilities of the entire model from the pre-training process. In this subsection, we summarize the potential inspirations for emergent bricks and introduce two different categories of emergent bricks, including bricks with human-defined and self-organized structures. The discussion may boost our understanding of the working mechanism inside the LLMs and help us better configure the LLMs with various internal modules."}, {"title": "2.1.1 Observations on Parameter Differentiation", "content": "LLMs tend to be over-parameterized when performing some specific tasks (Houlsby et al., 2019; Ding et al., 2023), which indicates that there exists a sub-module functioning nearly the same as the entire model with the rest parameters being redundant. This over-parameterization phenomenon leads to two general questions: (1) \"Which part of the model is actually functioning?\u201d (2) \u201cWhat kind of ability does it have?\". In this subsection, we discuss existing observations about the functional specialization of internal parameters in LLMs, that is, each parameter is only responsible for a specific function.\nActivation Sparsity Inspired by the sparsity in human brains (Olshausen & Field, 1996; Kerr et al., 2005; Poo & Isaacson, 2009; Barth & Poulet, 2012) that only a small portion of the neurons activate at each time, special architectures such as sparsely-activated Mixture-of-Experts (MoE) are introduced into Transformers to enforce activation sparsity and thus improve model efficiency (Du et al., 2022; Rajbhandari et al., 2022; Jaszczur et al., 2021; Fedus et al., 2022b,a). Different from the sparsity of expert activation in MoE, researchers also explore the activation sparsity of model neurons, which is in finer granularity. Specifically, the neuron \u201cactivation\" refers to the intermediate output of the fully connected layer after the non-linear activation function, and \"sparsity\" indicates that only a few entries of the activation values are nonzero for each given input. Zhang et al. (2022c) inspect the computational pattern of pre-trained Transformers and find that the activation sparsity naturally exists in pre-trained dense Transformers. Specifically, they delve into the feed-forward networks (FFNs), which constitute two-thirds of the Transformer model parameters, and find the emergence of sparse activation (e.g., only around 5% of the neurons are with nonzero activation values for 90% of the input for a fine-tuned T5-Large model (Raffel et al., 2020)). Li et al. (2023c) comprehensively investigate sparse activation in Transformers and conclude that it is a ubiquitous phenomenon that emerges for both natural language and vision models, on both training and evaluation data, on datasets of varying scale, on Transformers of varying configurations, and across all layers of a Transformer. Although the above works focus on the sparsity within ReLU-based models, an increasing number of modern LLMs have been trained with non-ReLU activations, and it is more tricky to explore activation sparsity for them since there are typically many near-zero but nonzero"}, {"title": "Function Localization", "content": "In addition to activation sparsity, neurons in the human brain exhibit a modular characteristic: neurons with similar functions tend to cluster together to form specific functional partition (Bullmore & Sporns, 2009; Meunier et al., 2010). Similarly, it is widely reported that substantial functions are specifically localized in a small number of parameters within pre-trained models, i.e., \u201cneurons\u201d or \u201ccircuits\": (1) Knowledge Neuron. Dai et al. (2022) and Meng et al. (2022) find that factual knowledge tuplets are stored in neurons of FFNs, and manipulating the activations or weights of these \"knowledge neurons\" can effectively edit the knowledge-related predictions of LLMs. (2) Skill Neuron. Some researchers dive into finding skill neurons, of which the activations are highly predictive of the task labels. These skill neurons are task-specific and perturbations to skill neurons can drastically impair the performance of corresponding tasks, implying that the task skills are surely localized in these neurons (Wang et al., 2022a; Ackermann & Ohmer, 2023; Panigrahi et al., 2023). (3) Linguistic Neuron. Gurnee et al. (2023) and Voita et al. (2023) study the linguistic features encoded in neurons and observe that neuron activations have correlations with a wide range of features like n-grams and positions. Zhao et al. (2023a) and Tang et al. (2024) discover language regions of LLMs that are specialized in multilingual text processing. Inspired by these observations, Zhang et al. (2023c) expand the neuron analysis to include MoE (Mixture of Experts) experts, which represent clusters of neurons. It demonstrates that these sparsely-activated experts are specialized in different functions including knowledge storing, task skills, and semantic understanding."}, {"title": "2.1.2 Human-Defined Emergent Bricks", "content": "Neural networks are usually constructed by stacking multiple human-designed network modules at different granularity levels. For example, the original Transformer (Vaswani et al., 2017) model consists of multiple identical blocks, within which there are multi-head attention (MHA) layers and FFNs. Simultaneously, each of the MHA layers is a combination of multiple attention heads and each FFN can be viewed as a collection of single artificial neurons that can be further regrouped based on their activations. Here, the structures and connections of these stacked modules, including neurons, attention heads, and whole layers, are explicitly designed by humans. Thus, we term these bricks as human-defined emergent bricks.\nAfter defining the structures of neural models, the next pivotal step to empower each brick with problem-solving capabilities is training, to which there are generally two main approaches:\nEnd-to-end Training End-to-end training of the entire network, which is the most common practice in the deep learning community. In this case, the skills or abilities of each module are not explicitly defined. Existing intriguing observations find that these human-defined bricks can gradually become functionally specialized"}, {"title": "Modular Training", "content": "Modular training conducts training for different modules separately, with their functionalities predefined in advance. Andreas et al. (2016) first propose the neural module networks for visual question answering, where multiple question-specific modular networks are dynamically initiated according to different reusable functional components (e.g., for recognizing dogs, and classifying colors). In this case, each of the reusable components represents a human-defined emergent brick that possesses predefined functionality. Such training paradigm also appeals to Transformer-based models, characterized by the MoE models (Gururangan et al., 2022; Zhang et al., 2022a), where each expert brick is responsible for one domain or one task. Based on these human-defined emergent bricks with predefined functionalities, it is cost-efficient to construct a new task-specific model by intentionally combining part of the bricks within a general-purpose pre-trained model.\nGenerally, human-defined emergent bricks are commonly observed and hierarchically structured, which naturally expedites the research on various brick configurations. However, there still exist inescapable obstacles to configuring foundation models with human-defined bricks: (1) The functionalities of human-defined bricks acquired by end-to-end training are hard to interpret and localize, preventing them from being clearly and effectively utilized; (2) Modular training of human-defined bricks requires delicate design of each single brick, such as the structure and scale of the modules, to ensure that each brick can effectively attain the functionality predefined by humans in advance."}, {"title": "2.1.3 Self-Organized Emergent Bricks", "content": "Language models acquire various capabilities from the pre-training process, which are further stored as parametric knowledge within the model parameters. Observations from the sparse activation and function localization phenomenon imply that the internal parametric knowledge or capabilities are not universally distributed among the entire model, but instead are stored in a centralized manner. However, such centralized distribution of the parametric knowledge does not strictly follow the human-defined module structure, as most modern LLMs adopt the end-to-end training paradigm which intrinsically does not constrain the learning objective of each module explicitly. In this case, there must exist an implicit structure of parametric knowledge distribution that differs from the human-defined module structure and the universal distribution over the entire model. Such an implicit structure is naturally formed during the training process, where different parts of the model interact and collaborate without explicit instructions by humans. Therefore, though language models are built upon human-defined bricks, dependencies and connections between bricks also emerge during the training process, resulting in self-organized emergent bricks.\nDistinct from any individual human-defined brick, the concept of self-organized bricks emerges from the interaction between multiple human-defined bricks. There have already been preliminary explorations of self-organized bricks in the literature shedding light on their characteristics and implications. For example, Zhang et al. (2022c) find that some small proportion of the neurons within the Transformer feed-forward networks tend to activate together while the rest of the neurons are inactivated, indicating that such subset of neurons are self-organized to function properly after pre-training and thus give rise to a new form of emergent bricks that is different from those pre-defined by humans. Following this finding, Pi\u00f3rczynski et al. (2023) enforce the activation sparsity and only adopt the self-organized bricks for improvement in inference efficiency. Furthermore, Zhang et al. (2023c) demonstrate that these self-organized groups of neurons possess high productivity for specific functions and any perturbations to them can lead to drastic degradation of the performance in the corresponding function, implying that such self-organized neuron clusters can serve as emergent bricks and are functionally specialized. Liu et al. (2023d) dive deeper into the concept of self-organized bricks by exploring both Transformer feed-forward networks and muti-head attention layers. Specifically, based on the discovery that the residual connections (He et al., 2016) in LLMs make token embeddings barely change across different layers, they envision input-dependent subsets of neurons in feed-forward networks and attention heads to yield performance comparable to employing the entire model. Moreover, Mirzadeh et al. (2023) introduce ReLU non-linear activation functions into the layer normalization of Transformers, which further enhances the collaboration among human-defined bricks and leads to more"}, {"title": "2.2 Customized Bricks", "content": "A monolithic LLM can be split into several emergent bricks, even when LLM layers are trained end-to-end with fully connected neurons. As LLM parameters continue to scale, the number of emergent bricks acquired during pre-training also increases, which enables satisfactory downstream performance. However, as the world continually changes, the capacities and knowledge that models need to master are constantly being updated and expanded. For example, world knowledge contained in widely-used Wikipedia is edited and updated daily (Cao et al., 2021; Meng et al., 2022); new academic papers published on scholarly websites continuously advance domain knowledge (Gururangan et al., 2022; Jin et al., 2022); and novel tasks emerging in different application scenarios also demand ever-growing task capacities from LLMs (Wang et al., 2023b). For ease of introduction, we use the term, knowledge, to refer to both knowledge and capacities, which can be regarded as a type of abstract knowledge.\nTraining the whole model to incorporate new knowledge is computation-intensive and requires massive storage space. To address this issue, many efforts have been devoted into parameterizing the external knowledge as customized bricks, which can be injected into LLMs for performance promotion (Dathathri et al., 2020; Lauscher et al., 2021; Zhang et al., 2023b; Xiao et al., 2023b; Ding et al., 2023). Specifically, customized bricks are usually constructed after pre-training with the original parameters frozen. Customized bricks can serve as an external knowledge bank for LLMs. Given an instruction, we first retrieve bricks with relevant knowledge and then insert them into LLMs for better responses. Different from training LLMs to store knowledge in emergent bricks, customized bricks possess the plug-and-play characteristic for dynamic and reusable knowledge injection. Therefore, customized bricks are usually named \"plugins\". In this subsection, we will first summarize the underlying reasons for the feasibility of customized bricks and introduce several typical customized bricks."}, {"title": "2.2.1 Observations on Intrinsic Dimension of LLMs", "content": "Customized bricks aim to inject external knowledge or new task adaptations into foundation models with tiny neural modules with a few parameters. This raises a natural question: \u201cCan the external knowledge be represented in limited parameters?\"\nIntrinsic Dimensionality It has been widely recognized that LLMs are highly over-parameterized (Frankle & Carbin, 2019; Liang et al., 2021; Prasanna et al., 2020), which is also the case of almost all the modern"}, {"title": "2.2.2 Typical Customized Bricks", "content": "Customized bricks have emerged as a significant avenue for enhancing LLMs during their post-processing phase, which involves the insertion of tiny bricks after the pre-training or fine-tuning procedures. This approach aims at efficiently enhancing the LLM's customized capabilities. As shown in Figure 3, we categorize widely used bricks into three types based on their capabilities: task bricks, knowledge bricks, and modality bricks."}, {"title": "2.2.3 Task Bricks", "content": "Task bricks, also known as parameter-efficient tuning or delta tuning (He et al., 2022b; Ding et al., 2023), are widely explored as a substitute for full-parameter fine-tuning, which usually requires substantial computational and storage costs. Task bricks adopt task adaptation by tuning only a small portion of parameters. As the number of model parameters grows, the performance gap between task bricks and full-parameter fine-tuning narrows. Consequently, in the realm of LLMs, employing task bricks for adaptation has become a widely accepted paradigm. Following Ding et al. (2023), we divide task bricks into three types according to the operations on the tunable parameters. Besides, recent efforts demonstrate that the task bricks can also be obtained by extracting task vectors from the internal representations without tuning, and we term these efforts as training-free task bricks.\nAddition-based Bricks Addition-based bricks introduce extra parameters into the LLM for fine-tuning. Within this category, the most extensively studied methods are adapter tuning (Houlsby et al., 2019) and prompt tuning (Liu et al., 2023b). The fundamental structure of adapter tuning consists of two linear layers with"}, {"title": "2.2.4 Knowledge Bricks", "content": "Knowledge bricks aim to supplement LLMs with external knowledge. While it is well-documented that LLMs internalize amounts of world knowledge to facilitate robust language comprehension (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), their finite parameter space inevitably limits the capacity to encapsulate the nearly infinite spectrum of external knowledge. This limitation often manifests itself in the form of \"hallucinations\", where the model generates erroneous information in responses due to a lack of relevant knowledge (Maynez et al., 2020; Lin et al., 2022; Honovich et al., 2022; Ji et al., 2023). Moreover, the computational overhead of LLMs make them less agile in adapting to the ever-changing landscape of world knowledge (Wu et al., 2022; Wang et al., 2023b). In the following paragraphs, we will present methods for representing knowledge as compact neural bricks and discuss the advantages of knowledge bricks compared to traditional knowledge injection methods. Based on the knowledge type, we can divide the knowledge bricks into structured knowledge graph (KG) bricks and unstructured text bricks.\nStructured KG Bricks Leveraging structured KGs to enhance pre-trained language models has long been a pivotal direction in NLP (Zhang et al., 2019; Han et al., 2021b). Compared to traditional models that incorporate knowledge during the pre-training phase, the construction of tiny structured KG bricks is cost-efficient, with stronger scalability to build bricks across diverse knowledge types. The core of structured KG bricks lies in pluggable knowledge representations, which can be injected into LLMs to provide external knowledge. One main line of research attempts to concatenate informative entity representations with original input embedding sequences for knowledge fusion. To this end, Ye et al. (2022) average the output vectors"}, {"title": "2.2.5 Modality Bricks", "content": "Multimodal large language models (MLLM), which utilize LLMs as the brain for reasoning and are capable of processing various perceptual signals such as images and speech, have become pivotal in the pursuit of artificial general intelligence. With the continuous growth in LLM training data and parameter scale, LLMs have exhibited numerous surprising emergent abilities, including instruction-following, in-context learning, and chain-of-thoughts (Wei et al., 2022a,b). To leverage these remarkable abilities in multimodal tasks and scenarios, many researchers have shifted their focus towards the training of MLLM (Yin et al., 2023). However, building an MLLM from scratch necessitates substantial computation and multimodally aligned data pairs. As a result, much of the current work treats pre-trained models from other modalities as bricks for LLMs, effectively leveraging them to transform multi-modal signals into features that the LLMs can readily process and understand. Based on the types of interface features communicated between models, these models can be categorized as bricks with textual interface and continuous interface.\nBricks with Textual Interface This line of work initially converts multi-modal data into text, which is then combined with textual instructions and fed into the LLMs. For example, Li et al. (2023b) adopts open-source video caption and detection models to convert videos into textual descriptions, which are then fed into LLM to generate responses. Recent popular tool-augmented LLMs treat models for other modalities as APIs. When given the functional descriptions and input-output formats of models, LLMs decompose the input instruction into multiple sub-tasks, where various models are involved to solve one by one (Wu et al., 2023a; Shen et al., 2023c; Yang et al., 2023). These approaches require no additional training and do not necessitate access to the model's parameters, making them particularly suitable for API-based models such as ChatGPT and GPT-4 (OpenAI, 2023). However, converting other modalities into text often results in inevitable information loss, which can considerably impact the model's performance.\nBricks with Continuous Interface To eliminate the information loss, many researchers attempt to construct a learnable continuous interface between LLMs and other pre-trained models. As the models are trained separately on single-modality data, the representation spaces between these models are quite different, which poses challenges for the learnable interface to translate visual and audio inputs as LLM continuous prompts. As for the model architecture, a widely-used method is adopting attention mechanism to extract important information with several learnable query vectors (Alayrac et al., 2022; Li et al., 2023a; Chen et al., 2023a). Further investigation indicates that a simple multi-layer perceptron is powerful for modalities bridge (Liu et al., 2023a; Zhu et al., 2023; Luo et al., 2023; Wu et al., 2023b). Different from bricks with textual interface, learnable continuous interface relies heavily on multi-modality aligned data (Hu et al., 2023; Zhao et al., 2023c). Besides, due to the limited representation capacities of continuous prompts, the learnable interface sometimes suffers from fine-grained information loss."}, {"title": "2.3 Brick Granularity", "content": "As stated previously, the granularity of a brick is highly customizable, ranging from a solitary neuron to a whole pre-trained model. As the size of bricks increases, their capacity expands correspondingly and the computational resources required also increase. This presents a challenge in selecting the optimal brick size, necessitating a careful balance between efficiency and effectiveness. In this section, we will first review existing observations on the capability of four different granularity of bricks: the solitary neuron (\u00a7 2.3.1), the neuron group (\u00a7 2.3.2), the layer (\u00a7 2.3.3), and the full model (\u00a7 2.3.4). Furthermore, we discuss how to choose the brick granularity properly (\u00a7 2.3.5)."}, {"title": "2.3.1 Solitary Neuron Granularity", "content": "The neuron, defined as a row or a column in the weight matrix of the linear layer, is often considered the finest functional unit in Transformer-based foundation models (Suau et al., 2020; Zhang et al., 2023c). After being trained properly, they can carry certain skills or knowledge, laying a solid foundation for the complex behavior of the entire deep learning system.\nFrom the perspective of skills, neurons in well-trained neural networks are demonstrated to possess the ability to capture specific input patterns and predictivity for some basic NLP tasks. Some early works find that neurons can learn the position of words (Karpathy et al., 2015) or parts of speech (Dalvi et al., 2019) such as nouns, verb forms, articles, numbers, etc. Others prove the specialization of certain neurons in capturing groups of words with similar meanings (e.g., electronic items or legislative terms) (Li et al., 2016; K\u00e1d\u00e1r et al., 2017; Na et al., 2019). Further, recent studies demonstrate the potential of visual model neurons in learning meaningful perceptual concepts such as the tall structures in images (Mu & Andreas, 2020). The sensitivity of neurons to various input patterns constitutes their high predictivity for some fundamental NLP tasks, including sentiment analysis, natural language inference, topic classification, etc (Wang et al., 2022a).\nMoreover, factual knowledge is also an important aspect of information that can be obtained by neurons. An important work in this field is done by Dai et al. (2022), who demonstrates the potential storage of factual knowledge in specific neurons (i.e., knowledge neurons). The activation of these knowledge neurons is positively correlated to the expression of the corresponding knowledge triplet, which sheds light on a promising approach to training-free knowledge editing and model manipulation (Sajjad et al., 2022).\nAnother interesting fact found in previous work is that the skills or knowledge contained in a solitary neuron can even be non-singular. Polysemous neurons capturing multiple concepts or word senses widely exist in deep neural networks (Xin et al., 2019; Suau et al., 2020). The knowledge neurons were responsible for different factual knowledge are also proven to have intersections (Dai et al., 2022). This observation underscores the potential of exploring smaller units within LLMs for understanding the storage of knowledge and skills."}, {"title": "2.3.2 Neuron Group Granularity", "content": "Neuron groups, namely tiny sublayers involving a group of neurons, can often display more complex behaviors than solitary neurons. As demonstrated in Zhang et al. (2023c), neurons can be emergently clustered into different function groups during the pre-training. Besides, the customized bricks usually consist of tiny sublayers to store certain knowledge and abilities.\nOne of the most popular organization forms of neuron groups is Mixture-of-Expert (MoE), where each expert is a specialized neuron group and the MoE output is aggregated from the expert outputs through a routing function. As for Transformers, pre-defined MoE is usually implemented by replacing a single linear layer in the attention module (Zhang et al., 2022b) or the feedforward network (Fedus et al., 2022b) with multiple linear experts. In some previous works (Zhang et al., 2022b; Zoph et al., 2022; Zhang et al., 2023c), these"}, {"title": "2.3.3 Layer Granularity", "content": "The utilization of stacked layers within deep models has consistently showcased its superior performance across numerous scenarios (Wang et al., 2019c; Devlin et al., 2019). LLMs typically comprise multiple layers with the same architecture, each possessing unique parameters (Touvron et al., 2023a,b). Understanding and manipulating different layers are both crucial aspects to maximizing the potential of LLMs.\nNumerous studies have delved into the examination of the functions of distinct model layers. For instance, Lin et al. (2019) present that the word order information is mostly contained in lower layers, while Hewitt & Manning (2019) propose the structural probing framework and find that syntactic knowledge is most prominent in middle layers. Liu et al. (2019) also find that final layers are usually more related to specific tasks. Rogers et al. (2020) provide a relatively thorough survey about the function of each layer in BERT (Devlin et al., 2019). In addition to linguistic functions, Geva et al. (2021) demonstrate that the feed-forward layers in Transformer models (Vaswani et al., 2017) can be construed as key-value memories, which inspired a new way to addressing and editing the knowledge stored in language models. As models progress from lower to higher layers, the functional scope transitions from local, lexical aspects to global, semantic dimensions.\nBased on the observations, there exist several approaches for the manipulation of inner model layers to improve efficiency, among which a straightforward one is to swap out some layers to accelerate model training or inference. Zhang & He (2020) present a Switchable-Transformer block, which introduces a gate to determine whether the corresponding layer is disabled or not in the training stage, based on which they further proposed a progressive-layer-dropping approach that can effectively reduce the training cost. Regarding inference, Xin et al. (2020) introduces DeeBERT, which reduces inference time by bypassing certain upper layers rather than passing through the entire model. Another branch of layer manipulation is knowledge editing, which aims to change the existing knowledge within pre-trained models by modifying some specific layers. For example, Huang et al. (2023b) proposed to inject multilingual knowledge into the feed-forward layers. Recently, Wang et al. (2023d) present EasyEdit, which supports various cutting-edge knowledge editing methods and applies to representative large language models, such as Llama 2 (Touvron et al., 2023b)."}, {"title": "2.3.4 Full Model Granularity", "content": "Various types of models frequently exhibit distinct advantages and drawbacks. For instance, large models excel in performance, while small models offer higher speed and demand fewer computational resources. Combining existing models is an efficient strategy for harnessing the strengths of individual models. Li et al. (2021) propose to dynamically select models of different sizes for input samples with different difficulties. Inspired by the dual-process theory of human cognition, Lin et al. (2023) propose to employ a small model that performs fast and intuitive thinking and a large model for complex and slow thinking. In addition to amalgamating models of varying sizes, as discussed in \u00a72.2 the integration of models from different modalities offers a practical approach to constructing multimodal models (Li et al., 2023a)."}, {"title": "2.3.5 Discussion", "content": "Based on the above statements, we turn back to the issue of selecting the appropriate granularity according to the required ability. Intuitively, coarser-grained bricks with more parameters are better suited for addressing complex tasks. For instance, solitary neurons can discern specific patterns of low-level linguistic units (Karpathy et al., 2015; Dalvi et al., 2019; Li et al., 2016; K\u00e1d\u00e1r et al., 2017; Na et al., 2019). By contrast, full model bricks are typically associated with higher-level capabilities, encompassing the general understanding of specific modalities (Li et al., 2023a), language"}]}