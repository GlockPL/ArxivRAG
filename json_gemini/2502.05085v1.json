{"title": "Causality can systematically address\nthe monsters under the bench(marks)", "authors": ["Felix Leeb", "Zhijing Jin", "Bernhard Sch\u00f6lkopf"], "abstract": "Effective and reliable evaluation is essential for advanc-\ning empirical machine learning. However, the increas-\ning accessibility of generalist models and the progress\ntowards ever more complex, high-level tasks make sys-\ntematic evaluation more challenging. Benchmarks are\nplagued by various biases, artifacts, or leakage, while\nmodels may behave unreliably due to poorly explored\nfailure modes. Haphazard treatments and inconsistent\nformulations of such \"monsters\" can contribute to a\nduplication of efforts, a lack of trust in results, and un-\nsupported inferences. In this position paper, we argue\ncausality offers an ideal framework to systematically\naddress these challenges. By making causal assumptions\nin an approach explicit, we can faithfully model phe-\nnomena, formulate testable hypotheses with explanatory\npower, and leverage principled tools for analysis. To\nmake causal model design more accessible, we identify\nseveral useful Common Abstract Topologies (CATs) in\ncausal graphs which help gain insight into the reasoning\nabilities in large language models. Through a series of\ncase studies, we demonstrate how the precise yet prag-\nmatic language of causality clarifies the strengths and\nlimitations of a method and inspires new approaches for\nsystematic progress.", "sections": [{"title": "1 Introduction", "content": "Machine learning achievements continue to break records\nand grab headlines, drawing attention from both the\npublic and the research community. However, the rapid\nproliferation of powerful models and the increasing com-\nplexity of tasks continue to amplify existing challenges\nin reliable evaluation of these models [Mao et al., 2024].\nBetween inflated expectations [Bubeck et al., 2023, Ull-\nman, 2023, Grace et al., 2024], opaque or misleading\nassessments [Mart\u00ednez, 2024], and even the occasional\nmistake [Chowdhuri et al., 2023], the poor communica-\ntion [Bowman, 2022] and unreliable benchmarks [Raji\net al., 2021, Bowman and Dahl, 2021, Alzahrani et al.,\n2024] can significantly undermine our understanding of\nthe capabilities and limitations of these models [Nezhu-\nrina et al., 2024, Yan et al., 2024]. This risks a decline\nof public trust [Bender et al., 2021, Green and Hu, 2018,\nHu and Kohler-Hausmann, 2020a] and perhaps even an\nAI winter. A key issue is that many evaluations focus on"}, {"title": "2 A Bestiary of Monsters", "content": "Prompt: Listen here, I'm starvin' and it's already\nhalf past four. Gotta rake in 10 grand in protection\ndough before I eat. I'm hittin' up every joint in the\nhood with my bat, collecting an average of 800 bucks\na pop. Takes me 10 minutes to move from one mark\nto the next. Am I gonna get to chow down before 7?\nLLM Chatbot: If you're aiming to collect 10 grand\nby shaking down joints for an average of $800\neach and it takes you 10 minutes to move from one\ntarget to the next, it would take you approximately\n3 hours and 20 minutes (200 minutes) to reach\nyour goal. Starting at half-past four, you should be\nable to hit your target before 7 PM. So, yes, you\ncan have a meal before 7 if everything goes as\nplanned. Enjoy your dinner!\nExample 1: The final answer is correct, and yet\nthe reasoning is very wrong. Although this LLM's\nresponse is technically correct in a narrow sense, its\nanswer for this GSM8k-like multi-step reasoning ques-\ntion Cobbe et al. [2021] is unsatisfactory both the pur-\nposes of evaluating the model's arithmetic reasoning, as\nwell as commonsense or moral reasoning as the LLM\nappears not to pick up on the obvious unethical context\nimplied by the scenario.\nAs LLMs become an increasingly integral component\nof decision-making systems, reasoning failures (including\nseemingly toy problems like in Example 1) may have dire\nreal-world consequences Nezhurina et al. [2024], Bender\net al. [2021]. This necessitates more powerful tools to\nidentify, catalog, and address the bestiary of issues that\narise in the design as well as evaluation of large models.\nExample 1 is indicative of several common reason-\ning failures in LLMs, and, by implication, our evalua-\ntion procedures for addressing such issues. The input\nprompt is structurally similar to the questions found in\nGSM8k Cobbe et al. [2021], a benchmark used for evalu-\nating a model's multi-step arithmetic and commonsense\nunderstanding skills. However, we select the subject\nmatter and word choice to evoke a mobster discussing\nplans to extort money from local businesses. Although\nthe LLM's final answer is factually correct, there are\nseveral problems with the rationale: (1) it makes several\narithmetic mistakes which happen to cancel out, (2) it\nfails to pick up on the unethical situation implied by the\nscenario, and (3) by implicitly condoning the criminal\nbehavior, it does not consider the broader consequences\nof the response. Crucially, if we only check for correct-\nness, as is standard practice Huang and Chang [2023],\nwe would find no fault in the response.\nThe problem is that to demonstrate good reasoning\nabilities, a correct answer is insufficient. We need to\nshow that the model answers the question correctly\nfor the right reasons. In other words, our evaluation\nmust verify that the model's processing of the input\ninformation leads to the correct answer consistently and\nreliably. This criterion makes a causal claim about the\nmodel's reasoning process, and thus must be supported\nby a causal analysis."}, {"title": "2.1 \u201cHere be dragons\u201d 1", "content": "To get a qualitative sense of the myriad of issues, or\nmonsters, that plague our benchmarks and experiments,\nwe will briefly survey recent approaches, including broad\noverviews into the nature of reasoning tasks [Huang and\nChang, 2023, Yu et al., 2023] and the evaluation of\nLLMS [Mao et al., 2024, Chang et al., 2023, Hajikhani\nand Cole, 2023]. For investigations of more specific\nissues, we separate efforts into three clusters depending\non whether the problem originates with the (1) models,\n(2) datasets, or (3) evaluation procedures.\nModels This line of work focuses on characterizing the\nreasoning failures and biases of language models, which is\nnontrivial given their opaque behavior [Binz and Schulz,\n2023]. These failures range from well-defined formal\nerrors such as logical fallacies [Jin et al., 2022], red her-\nrings [Naeini et al., 2023], or invalid inferences [Saparov\nand He, 2023] to broader issues including sensitivity to\nsuperficial features [Hajikhani and Cole, 2023, Ullman,\n2023], overconfidence [Nezhurina et al., 2024], halluci-\nnations [Dziri et al., 2022, Cui et al., 2023], and lack of\nrobustness [Zheng et al., 2024, Wang et al., 2023, Jin\net al., 2020]. Some studies explore how models exhibit\n\"content effects\" [Poesia et al., 2023], absorbing and am-\nplifying human biases [Dasgupta et al., 2022, Ze\u010devi\u0107\net al., 2023] including social and cultural biases [Bender\net al., 2021, Messner et al., 2023, Hutchinson et al., 2020,\nVig et al., 2020, Cao et al., 2023a, AlKhamissi et al.,\n2024, Motoki et al., 2024], such as stereotyping [Kotek\net al., 2023].\nDatasets Meanwhile, subtle variations of popular\nbenchmarks, such as premise order in reasoning\ntasks[Chen et al., 2024] or minor changes in problem\nparameters [Mirzadeh et al., 2024, Wu et al., 2024],\ncan cause large performance drops [Nezhurina et al.,\n2024, Yan et al., 2024], raising concerns not just about\nwhether models genuinely reason [Zhou et al., 2024],\nbut also about exploitable issues in the training data\nand benchmarks [Rogers and Rumshisky, 2020, Bow-\nman and Dahl, 2021]. These are can be described as\nenabling cheating [Zhou et al., 2023], heuristics [Mc-\nCoy et al., 2019], or shortcuts [Branco et al., 2021, Li\net al., 2022, Marconato et al., 2023], possibly due to"}, {"title": "3 Common Abstract Topologies", "content": "Coming up with a causal graph that faithfully repre-\nsents the underlying structure of an experiment or data\ngenerating process can be very challenging. Especially,\nsince usually when we design an experiment, we think in\nterms of more vague concepts like independent, depen-\ndent, and controlled variables, and consequently only\nimplicitly make causal assumptions. However, explicit\ncausal graphs:"}, {"title": "3.1 Confounding", "content": "Confounding occurs when there is a common cause be-\ntween the independent and dependent variables. For\nour purposes, we further restrict the \"confounding\" CAT\nto the case where the confounder is known and can,\nin principle, be controlled for. This is in contrast to\nthe \"spurious correlation\u201d CAT, where the confounder\nis unknown or too complex to be modeled explicitly.\nConfounding makes evaluation difficult or unreliable\nbecause the observed statistical relationship between the\nstimulus and response is not representative of the un-\nderlying causal relationship, thus unbiased causal effect\nestimation necessitates controlling for the confounder."}, {"title": "3.2 Mediation", "content": "Another important type of causal topology is media-\ntion, where there are multiple causal paths between\nthe stimulus and response. For simplicity, we illustrate\nthis general structure with one direct causal link and\none that goes through a mediator variable. Mediation\nanalysis is often used to quantify the impact of subcom-\nponents or side-effects on the model's behavior. For\nexample, a common setting may be to study the impact\nof a specific prompting strategy or representation on the\nmodel's response, which can be modeled as mediation\nas in Figure 3."}, {"title": "3.3 Spurious Correlations", "content": "The final common pattern we discuss here is spurious\ncorrelations, which are closely related to confounding but\ndiffer in the interpretation and implications for analysis.\nSpurious correlations (depicted as a dashed curved edge)\nare statistical associations between variables that are not\ncausally related (neither is an ancestor of the other), but"}, {"title": "4 Case Studies", "content": "In this section, we discuss a variety of specific research\nprojects which either make use of one of the Common\nAbstract Topologies (CATs) or could benefit from a\nmore explicitly causal framing."}, {"title": "4.1 Confounding", "content": "One project that explicitly uses a causal framing to\nunderstand the biases in the text generation of large\nlanguage models (LLMs) is Xia et al. [2024]. To address\nconfounding due to biases in the training data or prompt,\nthey propose using a reward model as an instrumental\nvariable.\nZhang et al. [2024b] formulate a human-LM collab-\norative writing setting as a causal inference problem\nwhere the past human commands and LLM responses\nare confounders for the current command and the overall\ninteraction outcome. To identify strategies that improve\nthe collaboration, they introduce a new causal estimand,\nthe Incremental Stylistic Effect (ISE), which allows them\nto abstract away from specific interactions and focus\non how actions incrementally contribute to the desired\nstylistic outcome of the text.\nMeanwhile, a good example of an active area of re-\nsearch that largely revolves around the confounding\nCAT, despite \u201cconfounding\" rarely being mentioned ex-\nplicitly, is the study of how the mathematical reasoning\nabilities of LLMs are affected by various undesirable\nfactors [Zhou et al., 2024, Patel et al., 2021]. In par-\nticular, a variety of projects have focused on using the\ndataset GSM8K [Cobbe et al., 2021] to evaluate multi-\nstep arithmetic reasoning as well as common sense un-\nderstanding [Mirzadeh et al., 2024, Chen et al., 2024,\nZhang et al., 2024a]."}, {"title": "4.2 Mediation", "content": "Mediation analysis guides the approaches of mechanistic\ninterpretability [Stolfo et al., 2023, Gupta et al., 2023,\nMeng et al., 2023, Wang et al., 2022], but it is also\nuseful in augmentation of language models Mialon et al.\n[2023], embedding LLMs within larger programs Schlag\net al. [2023]. and the quantification of biases like, gender\nbias Vig et al. [2020].\nA common setup for mechanistic interpretability is to\nstudy the impact of a specific component, such as an\nattention head or even a single parameter on the model\nbehavior. Olsson et al. [2022] propose that transformers\ncan learn simple, interpretable algorithms called \"induc-\ntion heads,\" which they hypothesize significantly con-\ntribute to in-context learning abilities. While mediation\nanalysis is not explicitly used in their work, we can frame\ntheir approach as studying a mediation graph, where\nthe tendency for a given model architecture (stimulus)\nto exhibit in-context learning (response) is mediated by\ninduction heads. Their six supporting arguments can be\ninterpreted through this causal lens: arguments 1 and 2\nestablish links between stimulus, mediator, and response\nthrough co-occurrence and co-perturbation; argument\n3, an ablation study, resembles controlled direct effect\nestimation; and arguments 4-6 examine the causal influ-\nence of the mediator on the response. This framing also\nhighlights potential limitations, particularly regarding\nunmeasured confounders that could affect causal inter-\npretations, as the authors' \"pattern-preserving\" ablation\ndoes not fully isolate the induction heads' effect. By con-\nsidering mediation explicitly, we can better understand\nthe underlying assumptions in their analysis and identify\nareas for further investigation, such as quantifying the\nnatural indirect effect to understand the full impact of\nthe induction heads on in-context learning abilities.\nIn contrast, Stolfo et al. [2023] propose a method for\nmechanistic interpretability of arithmetic reasoning in\nLLMs by editing the model's parameters to characterize\nthe information flow in the network. Note that the level\nof abstraction for this approach is quite different from\nthe causal model we proposed for Olsson et al. [2022],\nas the focus is on how information flows between indi-\nvidual model subcomponents, rather than how specific\nsubcomponents affect the overall model's behavior."}, {"title": "4.3 Spurious Correlations", "content": "There are several recent projects that use causal models\nto characterize spurious correlations in, for example, fac-\ntual knowledge [Cao et al., 2023b], multi-modal models\nfor fake news detection Chen et al. [2023], or to avoid\nspurious features by designing strategies for finding use-\nful demonstrations in few-shot learning Zhang and Yu\n[2023] or control NLP classifiers Bansal and Sharma\n[2023].\nChen et al. [2023] develop a causal model to system-\natically quantify and remove two specific kinds of bias:\npsycholinguistic (use of emotional language) and image-\nonly (ignoring text features). Note that the assumptions\nof the causal model address very specific types of bias\nusing both interventional and counterfactual techniques.\nBansal and Sharma [2023] presents a particularly in-\nteresting case as it addresses the same issue as Gardner\net al. [2021], but from a causal perspective. They both"}, {"title": "5 Alternative Views", "content": "We are hardly the first to point out systematic shortcom-\nings of evaluation methodology, particularly in NLP. One\nexisting perspective focuses on improving the external\nvalidity of benchmarks to ensure that high performance\non a benchmark actually translates to improved capa-\nbilities in the real world, such as with common sense\nreasoning [Elazar et al., 2021], or more precisely defining\nLLMS [Rogers and Luccioni, 2024] and how tasks relate\nto specific cognitive capabilities [Schlangen, 2019]. Raji\net al. [2021] argue that the common practice for certain\n\"standard\" benchmarks to become proxies for testing\ncomplex, high-level abilities, such as natural language\nunderstanding (NLU) leads to vague or unreliable re-\nsults, while Rogers and Rumshisky [2020] connect this\nto a proliferation of low-quality datasets.\nPrecisely this issue, that \"benchmarking for NLU is\nbroken\" Bowman and Dahl [2021], can be addressed\nusing causality. Not only does a causal framing provide\na versatile way to define the underlying assumptions\nand design choices of a benchmark, but it also offers\nprincipled methods for evaluating the benchmark's ex-\nternal validity [Bareinboim and Pearl, 2012, Pearl and\nBareinboim, 2022].\nIn the context of evaluating the reasoning abilities\nof language models, a natural field to turn to is psy-\nchometrics, which has been studying the evaluation of\nhuman reasoning abilities for over a century [Wilhelm,\n2005]. This direction also coincides with an increasing\npractice in Natural Language Processing (NLP) to treat\nlanguage models as agents [Park et al., 2023, Liu et al.,\n2023] or subjects in the social sciences [Horton, 2023,\nLeng and Yuan, 2023, Pellert et al., 2024]. Specifically,\nitem response theory [Lord and Novick, 2008, Baker,\n2001] holds promise to develop tools to systematically\nquantify what information about the model's reasoning\nabilities can be extracted from a benchmark with re-\nspect to some population candidate models, and there\nare some projects applying this framework in the con-\ntext of NLP [Rodriguez et al., 2021]. Within the field\nof NLP there are also notable calls for more holistic"}, {"title": "6 Conclusion", "content": "The burgeoning research on large models, and, in par-\nticular, high-level reasoning tasks, faces a variety of\nchallenges, or monsters, to reliably evaluate and im-\nprove models. Despite the wide variety of approaches\nand frameworks that have been developed to tackle\nthese challenges, this variety obscures their shared struc-\ntural features and recurring issues. By recognizing that\nmonsters can often be effectively formulated in terms of\ncausal assumptions underlying an experimental design or\ndata generation process, we can unify our understanding\nusing the language of causality.\nA causal framing aids along several steps of the re-\nsearch process by guiding experimental design, formulat-\ning testable hypotheses, and interpreting results. Causal\nmethods enable researchers to gain a clearer lens to un-\nderstand how variables of interest interact, rather than\nmerely optimizing for predictive performance on an arti-\nficial benchmark. We argue that causality offers a path\ntoward deeper scientific insights, more transparent com-\nmunication of assumptions, and stronger justifications\nfor the conclusions drawn.\nOne stumbling block to adopting causal methods is\nthat the restrictive assumptions and formalism may\nseem unapproachable at first. Additionally, researchers\nmay hesitate to commit modeling assumptions to paper\nwhere they can be scrutinized. However, data-driven\napproaches which rely on implicit or vague assump-\ntions along with results that may (inadvertently) be\ninterpreted as causal contribute to confusion and unsup-\nported claims, which hinder scientific progress. Causal\nmethods, by contrast, encourage explicit modeling and\ncritical thinking about the mechanisms that underlie\nempirical observations.\nTo make causality more accessible and practically\napplicable, we introduce Common Abstract Topologies\n(CATs) to faithfully describe the underlying structure of\nmany issues that arise in designing and evaluating ML\nmodels. In the case studies in Section 4, we have shown\nhow a causal framing can formalize a various common\nissues and help develop mitigate them. We envision\nCATs as a practical guide, helping researchers quickly\nidentify relevant causal models and choose appropriate"}]}