{"title": "UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations", "authors": ["Yuan Ren", "Guile Wu", "Runhao Li", "Zheyuan Yang", "Yibo Liu", "Xingxin Chen", "Tongtong Cao", "Bingbing Liu"], "abstract": "Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. Our contributions are two-fold. First, we propose a new differentiable rendering method that distorts 3D Gaussians using a series of affine transformations tailored to fisheye camera models. This addresses the compatibility issue of 3D Gaussian splatting with fisheye cameras, which is hindered by light ray distortion caused by lenses or mirrors. Besides, our method maintains real-time rendering while ensuring differentiability. Second, built on the differentiable rendering method, we design a new framework that learns a unified Gaussian representation from multiple camera models. By applying affine transformations to adapt different camera models and regularizing the shared Gaussians with supervision from different modalities, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As a result, our approach models multiple sensors (pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR point clouds). Our experiments show that our method achieves superior rendering quality and fast rendering speed for driving scene simulation.", "sections": [{"title": "1. Introduction", "content": "Urban scene reconstruction aims to reconstruct real-world driving scenes from images and achieve photorealistic rendering [3, 24, 31, 33]. It is crucial for realizing real-world closed-loop evaluation in end-to-end autonomous driving simulators [29, 31]. One of the most popular methods for driving scene reconstruction is the use of Neural Radiance Field (NeRF) [19], an implicit radiance field representation. Although NeRF-based methods [6, 21, 24, 27, 31] have achieved photorealistic rendering, they suffer from slow rendering speed and expensive training cost. Recently, some researchers resort to 3D Gaussian Splatting (3DGS) [9], an explicit radiance field representation, for urban scene reconstruction. 3DGS-based methods [3, 30, 33, 34] use explicit 3D Gaussian primitives to represent 3D scenes and employ a differentiable tile rasterizer for rendering. This enhances editability of driving scenes and achieves real-time photorealistic rendering.\nHowever, existing driving scene reconstruction methods mainly focus on rendering from pinhole cameras and largely neglect fisheye cameras. Compared with pinhole cameras, fisheye cameras provide a wider field of view (FOV) essential for navigation and perception tasks in autonomous driving, especially for near-field sensing and automatic parking [12, 13, 22]. Nevertheless, it is non-trivial to adapt 3DGS to fisheye cameras. For example, the direct application of 3DGS to fisheye cameras is impeded by distortions induced by camera optics, which disrupts the affine transformations of 3D Gaussians. Intuitively, an engineering strategy to tackle this issue is directly rectifying fisheye images and treating them as pinhole images for model training. However, this strategy yields suboptimal rendering quality, particularly in the regions with large field of view angle. Another strategy is rendering multiple cubic images and subsequently amalgamating them to construct fisheye images [11, 25]. Nevertheless, this strategy introduces noticeable artifacts indicative of the stitching process. In fact, how to effectively render driving scenes from fisheye images for simulation remains an open question.\nTo address the aforementioned challenges, this work presents UniGaussian, a novel 3DGS-based approach that learns a unified Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. First, we aim to address the compatibility issue of 3DGS with fisheye cameras. To this end, we propose a novel differentiable rendering method for 3DGS tailored to fisheye cameras. In our method, the distortion of light rays caused by fisheye cameras is translated into the deformation of the radiance field. This is realized through a series of affine transformations, incluiding translation, rotation and stretching, applied to 3D Gaussians. These operations maintain the parallelization strategy and real-time performance of 3DGS, while ensuring differentiability. Second, with the proposed differentiable rendering method, we design a new framework that jointly optimizes a unified 3D Gaussian representation from multiple camera models for driving scene reconstruction. We apply affine transformations on 3D Gaussians to adapt them for different camera models and regularize the shared Gaussians with supervision from different modalities. In this way, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As illustrated in Fig. 1, our approach models multiple sensors, including pinhole cameras and fisheye cameras, and multiple modalities, including depth, semantic, normal and optional LiDAR point clouds. Besides, since we use a unified rasterizer for 3D Gaussian rendering, our framework shows adaptability to various differentiable camera models and maintains real-time performance. The framework of the proposed approach is depicted in Fig. 2.\nIn summary, our contributions are two-fold:\n\u2022 We propose a novel differentiable rendering method for 3DGS tailored to driving scene reconstruction from fisheye cameras.\n\u2022 We propose a new framework to learn a unified 3D Gaussian representation from multiple camera models for driving scene reconstruction. Our approach achieves holistic driving scene understanding by modeling multiple sensors and modalities."}, {"title": "2. Related Work", "content": "3D Gaussian Splatting. 3DGS [9] is a real-time radiance field rendering approach. It represents a 3D scene with explicit 3D Gaussian primitives. With a differentiable tile-based rasterizer, 3DGS achieves highly effective training and photorealistic real-time rendering. We refer the readers to [9] for more details of 3DGS. With the incredible success achieved by 3DGS, many 3DGS-based methods have emerged with the aim for solving the problems of anti-aliasing [32], acceleration [4], relighting [5], sparse-view synthesis [28], 4D reconstruction [26], etc. Most of these works focus on reconstruction with pinhole camera models only. Recently, Liao et al. [15] adapt 3DGS to fisheye cameras by recalculating 3DGS projection and gradients, but as they point out, their method is only based on ideal camera models with equidistant projection and not for generic fisheye camera models and real large-FOV cameras. This hinders their use in driving scene reconstruction because fisheye cameras in driving scene are usually generic models and have large FOVs. In contrast, our work proposes a new differentiable rendering method for 3DGS tailored to driving scene reconstruction with fisheye cameras. Through a series of affine transformations, our method does not significantly increase GPU memory consumption and maintains real-time rendering. Furthermore, our method has adaptability to various differentiable camera models and can deal with driving scene reconstruction with large FOVs.\nDriving Scene Reconstruction. Typically, autonomous driving scene simulation involves driving scene reconstruction [24, 27, 31] and road asset reconstruction [16, 17]."}, {"title": "3. Methodology", "content": "This work aims at realizing real-time photorealistic rendering for driving scene reconstruction from multiple camera models, including pinhole and fisheye cameras. Before delving into driving scene reconstruction from multiple camera models, it is necessary to discuss differentiable rendering of 3DGS for fisheye cameras. Thus, in Sec. 3.1, we briefly introduce mathematical models of fisheye cameras; then, in Sec. 3.2, we present the proposed differentiable rendering of 3DGS for fisheye cameras; finally, in Sec. 3.3, we introduce the proposed UniGaussian framework for driving scene reconstruction."}, {"title": "3.1. Mathematical Models of Fisheye Cameras", "content": "The Kannala-Brandt [8] and MEI [18] mathematical models are two commonly used models for fisheye cameras. Each model consists of two main steps. The first step can be referred to as the \"mirror transformation\" that characterizes the twisting of light rays induced by the lens/mirrors or the transformation that translates 3D points to new positions by rotating the camera-point vectors towards the optical axis of the camera. The second step is projecting 3D points from new positions onto the image plane. This step utilizes either the camera intrinsic matrix or a pseudo intrinsic matrix.\nLet \u03b8 and $\u03b8_a$ be the included angles between camera-point vectors and the optical axis before and after applying the rotation, respectively. Then, the first step is defined as:\n$\u03b8_a = M(\u03b8)$,\nwhere M is the transformation specified by different models. For the Kannala-Brandt model, Eq. (1) is defined as:\n$\u03b8_a = arctan r_a = arctan \\left(\u03b8 \\left(1 + k_1\u03b8^2 + k_2\u03b8^4 + k_3\u03b8^6 + k_4\u03b8^8\\right)\\right)$,\nwhere $k_i$ is a distortion coefficient. For the MEI model, Eq. (1) is defined as:\n$\u03b8_a = arctan r_a = arctan \\left(\\frac{\\sin \u03b8}{\\xi \\cos \u03b8} \\left(x + k_1x^3 + k_2x^5\\right)\\right)$,\nwhere $x = \\frac{\\sin \u03b8}{\\xi \\cos \u03b8}$ and the twist of light rays is controlled by three parameters $k_1$, $k_2$ and $\u03be$. Next, these translated 3D points are projected onto the image plane with intrinsic parameters $f_x$, $f_y$, $u_o$ and $v_o$. In specific, the pseudo intrinsic matrix defined in Eq. (4) is used to project the translated 3D points onto the image plane. Since the focal length $f_x$, $f_y$ and the gain $\u03b7$ cannot be estimated independently, $\u03b3_1$ and $\u03b3_2$ are used as the pseudo focal length of the MEI model.\n$\\begin{bmatrix}\n\u03b3_1 & 0 & u_o \\\\\n0 & \u03b3_2 & v_o \\\\\n0 & 0 & 1\n\\end{bmatrix} = \\begin{bmatrix}\nf_x\u03b7 & 0 & u_o \\\\\n0 & f_y\u03b7 & v_o \\\\\n0 & 0 & 1\n\\end{bmatrix}$"}, {"title": "3.2. 3DGS Rendering with Fisheye Cameras", "content": "Overview. The architecture of the proposed differentiable rendering method is shown in Fig. 3a. Suppose a driving scene is represented by a set of 3D Gaussians. When rendering from a specific viewpoint, we firstly select all visible 3D Gaussians and compute their colors utilizing spherical harmonics in the world space. Subsequently, we adjust the positions of the selected 3D Gaussians via rotating the camera-Gaussian center vectors towards the optical axis of the camera. Also, the poses of the selected 3D Gaussian are rotated with the camera-Gaussian center vectors. Then, the adjusted 3D Gaussians are compressed in both polar and tangential directions. The scaling factor and the quaternion of the 3D Gaussians are updated via eigendecomposition. Finally, the updated 3D Gaussians are projected onto the image plane to generate the final rendering. Overall, our method is generic to various differentiable camera models and can be used for driving scene reconstruction with large FOVs. Besides, our method does not require large amounts of additional GPU memory so is suitable for processing millions of 3D Gaussians in driving scene reconstruction.\n3D Gaussian Position and Pose Adjustment. Let $r_g$ and $r_c$ be the position of 3D Gaussians and cameras in the world coordinate space respectively and $r_a$ be the optical axis of cameras. As shown in Fig. 3b, when performing transformation, the camera-Gaussian center vector needs to be rotated by $\u0394$ around the vector $r_{rot}$. Here, $\u0394 = \u03b8 - \u03b8_a$ and $r_{rot}$ is denoted by:\n$r_{rot} = \\frac{r_{gc}}{\\lVert r_{gc} \\rVert} \u00d7 r_a$,\nwhere $r_{gc} = r_g - r_c$, $\u03b8$ is the included angle between the camera-Gaussian center vector $r_{gc}$ and $r_a$, and $\u03b8_a$ is computed using Eq. (1). Then, the new position $r'_g$ of the 3D Gaussian is computed as:\n$r'_g = C(dq)r_{gc} + r_c$, where $dq = quec(r_{rot}, \u0394)$,\nand $quec$ denotes the transform from the axis-angle to the quaternion and $C(\u22c5)$ is the transform from the quaternion to the rotation matrix. Since the pose of the 3D Gaussian needs to be rotated along the camera-Gaussian center vector, the new quaternion $q'_g$ is defined as:\n$q'_g = \u03b4q\u2297q_g$,\nwhere $q_g$ is the original quaternion and $\u2297$ denotes the quaternion multiplication.\nCompression of 3D Gaussians. Corresponding to the compression of FOV, 3D Gaussians needs to be compressed in the polar and tangential directions; otherwise, the adjacent 3D Gaussians will have greater degree of overlap on the splatting plane. To compress 3D Gaussians in the given direction, we use a stretching matrix S which is defined as:\n$\\begin{bmatrix}\n1 + (k - 1)n_x^2 & (k - 1)n_xn_y & (k - 1)n_xn_z \\\\\n(k - 1)n_xn_y & 1 + (k - 1)n_y^2 & (k - 1)n_yn_z \\\\\n(k - 1)n_xn_z & (k - 1)n_yn_z & 1 + (k - 1)n_z^2\n\\end{bmatrix}$,\nwhere n is the stretching direction and k is the stretching ratio. In Fig. 3c, the blue and green ellipsoids denote 3D Gaussians before and after translation, rotation and stretching. These two ellipsoids have the same tangential field angle $\u0394$. The tangential stretching ratio is computed by $\\frac{\\sin \u03b8_a}{\\sin \u03b8}$ and the polar stretching ratio is defined as:\n$\u03ba_\u03b8 = \u0394\u03b8_a/\u0394\u03b8$,"}, {"title": "3.3. UniGaussian for Driving Scene Reconstruction", "content": "Although it is feasible to separately optimize different 3D scene representations for different camera models, this increases the difficulty for deployment in real-world simulators and prevents each camera model from learning complementary information. In this work, we proposes to learn a unified 3D Gaussian representation from multiple camera models for driving scene reconstruction. An overview of our framework is depicted in Fig. 2.\nDriving Scene Gaussians. With input images from different cameras, we represent a driving scene with composite 3D Gaussians. Following [33, 34], we decompose the scene into background Gaussians for static scenes, dynamic Gaussians for moving objects and sky Gaussians for distant regions. Besides, we employ LiDAR point clouds to initialize 3D Gaussians by accumulating all LiDAR frames and projecting points onto images for color extraction. This provides a better representation of the scene geometry and enables the optional LiDAR simulation in our framework.\nModeling Multiple Sensors and Modalities. To account for the distortion of fisheye images, we applied a series of affine transformations to further process 3D Gaussians for fisheye cameras as introduced in Sec. 3.2 while pinhole cameras skip these transformations. These processed 3D Gaussians are rendered with a unified tile-based rasterizer. Then, to resolve the exposure difference between pinhole and fisheye cameras, we applied camera-dependent scaling and biases factors on the rendering images as [33]. This models appearance difference between pinhole and fisheye cameras. Besides, we also render other modalities, including depth maps, semantic maps and normal maps. To render these maps, each 3D Gaussian is added with the corresponding 3D logits and then 2D maps are obtained via \u03b1-blending of these 3D logits in the rasterizer. Although LiDAR simulation may not be directly achieved by explicit 3D Gaussians, we can obtain point clouds from the rendering depth maps by extracting points based on real-world LiDAR parameters and scans. Similarly, intensities of point clouds can be obtained by generating intensity maps from the rasterizer and extracting the corresponding points. Note that, LiDAR simulation is optional and not the focus of this work though we can derive it from depth and intensity maps.\nAdaptive Density Control. Driving scene reconstruction usually requires millions of 3D Gaussians. To enhance the adaptive density control of these Gaussians, we employ the Markov Chain Monte Carlo (MCMC) sampling and relocation strategy following [10]. In specific, we consider 3D Gaussian densification and pruning as a deterministic state"}, {"title": "4. Experiments", "content": "Our UniGaussian relies on the proposed differentiable rendering method of 3DGS for fisheye cameras. Thus, in our experiments, we firstly verify the effectiveness of the proposed rendering method in Sec. 4.1 and Sec. 4.2 and then examine the efficacy of the unified framework in Sec. 4.3. Please refer to the supplementary material for more details."}, {"title": "4.1. Fisheye Rendering Geometric Error Analysis", "content": "Since the proposed method is a \u201cconvert-project\u201d method, the conversion part contains some approximations. After the light rays are twisted by fisheye cameras, the distorted 3D Gaussian no longer follows an exact normal distribution. Hence, the purpose of this experiment is to analyze the rendering geometric error introduced by the approximation.\nFlowchart for Analysis. We employ the method depicted in Fig. 4 to analyze the rendering geometric error introduced by these approximations. First, a 3D Gaussian model is trained by using the images of a pinhole camera. Then, images of a fisheye camera are rendered in two different ways: the \"convert-project\u201d method as introduced above and the \"project-convert\u201d method that renders a pinhole image and distorts it to a fisheye image. The \"project-convert\" method does not have any approximation in the conversion part, so its rendering result is considered as the reference image. We use the \"BICYCLE\u201d and \u201cGARDEN\" of the Mip-NeRF360 dataset [1] in this experiment because these data have enough multi-view images and accurate camera poses that enable rendering from any point of view.\nResults Discussion. The results are presented in Table 1. From these results, we can see that the stretching of 3D Gaussians is effective for improving the quality of rendering. This proves the correctness of our rendering algorithm's derivation as the approximations of our method does not degrade the image quality. Moreover, in our method, most of the rendering time is spent on affine transformations of 3D Gaussians, while rasterization only takes around 3 ms. Note that, the time consumption shown in Table 1 is for images with large resolutions, but for autonomous driving scenes, 3D Gaussians are projected within a frustum and the image resolution is less than 4358 \u00d7 2824 so the rendering can be done in real time. Besides, although the 2nd order approximation of polar stretching ratio gives improvement on rendering quality, it greatly increases the rendering time."}, {"title": "4.2. Driving Scene Fisheye Camera Simulation", "content": "In this experiment, we further verify the effectiveness of the proposed rendering method for fisheye camera simulation in driving scene novel view synthesis.\nDataset. We conduct experiments on the KITTI-360 dataset [14]. KITTI-360 contains images captured from fisheye cameras in real-world driving scenes and provides camera calibration and ego-vehicle pose. We select a segment of 221 images (frame ids 227-447 from \"2013_05_28_drive_0000_sync\") and evenly select the every 8th image as the test set while the others are used as the training set. In addition to comparing the global image quality, three local zones (A, B, C) are compared. As it is shown in Fig. 5, zone \"A\" (Blue) is the bottom area where objects appear usually very close to the camera, zone \u201cB\u201d (Green) is the sky with little texture where the geometry is not very accurate, and zone \"C\" (Red) is far away from the camera with the large distortion in this region.\nResults Analysis. We present the quantitative results in Table 2 and the qualitative results in Fig. 6. Overall, our approach achieves the best rendering quality both globally and locally (at zones A, B and C), while maintaining a real-time rendering speed of 39 FPS. In comparison, 3DGS+Undistort performs poorly on fisheye simulation, which indicates that direct distortion is not a suitable choice. Besides, from Table 2 and Fig. 7, we can see that our method without stretching yields slightly worse image quality, especially for texture details, but achieves significantly faster rendering speed of 138 FPS. Also, skipping stretching increases the number of Gaussians by 40%, which increases the risk of GPU memory overflow. The reason is that small Gaussians are insensitive to changes in the distribution caused by light rays distortion and the optimizer tends to approximate the scene with a large number of small 3D Gaussians in exchange for better rendering quality."}, {"title": "4.3. Multiple Camera Model Simulation", "content": "In this experiment, we evaluate our UniGaussian framework for driving scene simulation from multiple camera models. Note that, to the best of our knowledge, no existing driving scene reconstruction method simulates both pinhole and fisheye cameras in a unified framework. Thus, the compared state-of-the-art methods are used for the reference purpose only, while modifying these methods for fisheye camera simulation beyond the scope of this work.\nDataset. We conduct our experiments on KITTI-360 [14], because it is a real-world autonomous driving dataset providing both pinhole and fisheye images, while the other commonly used datasets provide only one type of images. For evaluation, we select four sequences as [24]. Each sequence contains 64 frames and we select the every 4th frame as the test set while the others are used as the training set.\nResults Analysis. We present the quantitative results in Table 3 and visualize some rendering results in Fig. 8. As shown in Table 3, for pinhole camera simulation, for fair comparison with AlignMiF and UniSim-SF, we also report the results of our approach with pinhole camera only. We can see that both ours and ours(pinhole) achieve better results than the state-of-the-art driving scene reconstruction methods, such as HUGS, AlignMiF and UniSim-SF. As for fisheye camera simulation, we add our fisheye rendering method to HUGS (named HUGS++) because HUGS does not support fisheye cameras. The results in Table 3 show that our approach outperforms the modified method. Besides, as shown in Fig. 8, our approach generates better rendering images with more fine-grained details. For example, our approach renders the license plate, the fence, the taillight, etc., while HUGS++ generates worse rendering images. Moreover, we also visualize some semantic and depth maps in Fig. 8. We can see that our multimodality outputs achieve slightly better holistic 3D scene understanding.\nAblation study. The ablation studies of the proposed rendering method's stretching and approximation operations are discussed in Sec. 4.1 and Sec. 4.2. In this experiment, we further evaluate other components of our approach. From Table 4, we can see that our approach with all components perform the best and the supervisions from different modalities, including depth, semantic and normal, are helpful for model optimization. These modalities also facilitate better holistic scene understanding as shown in Fig. 8. Besides, from the last row of Table 4, we can see that without the adaptive control and $L_{reg}$, the PSNRs are slightly worse, which indicates the importance of learning a compact representation in driving scene reconstruction."}, {"title": "5. Conclusion", "content": "In this work, we propose a new framework for learning unified 3D Gaussians of driving scenes from multiple camera models and present a new differentiable rendering method of 3D Gaussians tailored for fisheye cameras. Our method enables holistic driving scene understanding by modeling multiple sensors and modalities. Our experimental results on real-worl autonomous driving dataset verify the effectiveness of the proposed method.\nLimitation. Due to the nature of explicit 3D Gaussian representations, 3DGS may not provide sufficient details when the viewing distance is close to the observation areas. This issue is exacerbated when adapting 3DGS to fisheye cameras in driving scene reconstruction due to the capability of fisheye cameras to observe nearby vehicles and buildings along the road. Our future work aims to solve this limitation and develop a real-world autonomous driving simulator."}]}