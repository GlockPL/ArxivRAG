{"title": "GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion Prior and Parametric Light Source Optimization", "authors": ["Kang Du", "Zhihao Liang", "Zeyu Wang"], "abstract": "We present GS-ID, a novel framework for illumination decomposition on Gaussian Splatting, achieving photorealistic novel view synthesis and intuitive light editing. Illumination decomposition is an ill-posed problem facing three main challenges: 1) priors for geometry and material are often lacking; 2) complex illumination conditions involve multiple unknown light sources; and 3) calculating surface shading with numerous light sources is computationally expensive. To address these challenges, we first introduce intrinsic diffusion priors to estimate the attributes for physically based rendering. Then we divide the illumination into environmental and direct components for joint optimization. Last, we employ deferred rendering to reduce the computational load. Our framework uses a learnable environment map and Spherical Gaussians (SGs) to represent light sources parametrically, therefore enabling controllable and photorealistic relighting on Gaussian Splatting. Extensive experiments and applications demonstrate that GS-ID produces state-of-the-art illumination decomposition results while achieving better geometry reconstruction and rendering performance. The source code is available at https://github.com/dukang/GS-ID.", "sections": [{"title": "Introduction", "content": "Reconstructing physical attributes from multiple observations has long been challenging in computer vision and computer graphics. Illumination is a highly diverse and complicated factor that significantly influences observations. Illumination Decomposition (ID) aims to achieve controllable lighting editing and produce various visual effects. However, ID is an extremely ill-posed problem, as varying interactions between different lighting distributions and materials can produce identical light effects. This issue is compounded by the complexity of illumination (e.g., self-emission, direct, and indirect illumination). Without priors of geometry and materials, this task becomes exceedingly difficult. Furthermore, the need to accurately decompose materials and geometry, solve the complex rendering equation, and consider multiple light sources and ray bounce exacerbates the challenge.\nMany recent works focus on appearance reconstruction. Neural Radiance Field (NeRF) (Mildenhall et al. 2021) employs an MLP to represent a static scene as a 5D implicit light field, achieving photorealistic novel view synthesis results. 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) further incorporates explicit representation with efficient rasterization, enabling real-time rendering. However, their 3D appearance reconstruction mainly concentrates on view-dependent appearance without further decomposition.\nTo acquire a controllable light field, a well-posed geometry is necessary to support the subsequent decomposition task. Recently, 2D Gaussian Splatting (2DGS) (Huang et al. 2024) has emerged as a promising method for geometry reconstruction, utilizing surfel-based representation to ensure geometric multi-view consistency. 2DGS aims to deliver a well-posed geometry, providing significant support for ID. However, it primarily focuses on geometry reconstruction and falls short in modeling physical materials and light transport on the surface. This limitation impedes its effectiveness in more complex tasks like relighting and light editing. Another line of research (Jin et al. 2023; Jiang et al. 2024; Liang et al. 2023) on 3D inverse rendering can model both material and geometry simultaneously. However, these methods almost exclusively focus on formulating environmental illumination, struggling with user-friendly and intuitive lighting editing.\nIn this paper, we propose a novel framework called GS-ID, designed to decompose illumination by leveraging suitable geometry and materials from multiple observations. Our approach utilizes 2DGS for the reconstruction of geometry and materials. However, we observe that 2DGS falls short in accurately representing reflective and distant regions. To address this limitation, we incorporate a normal prior (Eftekhar et al. 2021) to obtain a well-posed geometry for subsequent decomposition tasks. For illumination decomposition, we employ a learnable panoramic map to encapsulate multiple ray-bouncing results as environmental illumination. Additionally, we use a set of Spherical Gaussians (SGs) to parametrically model direct light sources, efficiently depicting direct illumination in highlight regions. Furthermore, we adopt deferred rendering. This approach decouples the number of light sources from Gaussian points, facilitating efficient light editing and compositing.\nOur GS-ID framework facilitates explicit and user-friendly light editing, enabling the composition of scenes with varying lighting effects. Experimental results demonstrate that our framework surpasses contemporary methods, offering superior control over varying illumination effects. Our contributions can be summarized as follows:\n\u2022 We introduce GS-ID, a novel framework for illumination decomposition that integrates parametric direct and environmental illumination to approximate complex light fields for accurate editing and composition.\n\u2022 To the best of our knowledge, we are the first to introduce the diffusion model as a prior for improving geometry and material estimation under unknown illumination.\n\u2022 We employ deferred rendering, enhancing rendering performance with multiple light sources while achieving superior light editing and compositing results."}, {"title": "Related Work", "content": "Geometry Reconstruction Regarding surface reconstruction, some methods (Park et al. 2019; Niemeyer et al. 2020; Oechsle, Peng, and Geiger 2021; Yariv et al. 2020; Wang et al. 2021; Yariv et al. 2021) use an MLP to model an implicit field representing the target surface. After training, surfaces are extracted using isosurface extraction algorithms (Lorensen and Cline 1998; Ju et al. 2002). Recently, several methods (Huang et al. 2024; Yu, Sattler, and Geiger 2024) achieve fast and high-quality geometry reconstruction of complex scenes based on 3DGS (Kerbl et al. 2023). However, these methods often treat the appearance of a functional of view directions, neglecting the modeling of physical materials and light transport on the surface. This limitation hinders their ability to handle complex tasks such as light editing.\nIntrinsic Decomposition To decompose the intrinsics from observations, some monocular methods (Zhu et al. 2022b,a; Kocsis, Sitzmann, and Nie\u00dfner 2024; Zeng et al. 2024) learn from labeled datasets and estimate intrinsics directly from single images. However, these methods lack multi-view consistency and struggle to tackle out-of-distribution cases. In contrast, other methods (Bi et al. 2020; Srinivasan et al. 2021; Zhang et al. 2021, 2022; Munkberg et al. 2022; Jin et al. 2023; Jiang et al. 2024; Liang et al. 2023) construct a 3D consistent intrinsic field from multiple observations. Notably, GaussianShader and GS-IR build upon 3DGS and enable fast intrinsic decomposition on glossy areas and complex scenes. While obtaining impressive results, current methods almost only consider environmental illumination, making precise editing challenging."}, {"title": "Preliminaries", "content": "Gaussian Splatting 3D Gaussian Splatting (3DGS) represents a 3D scene as a collection of primitives. Each primitive models a 3D Gaussian associated with a mean vector \u00b5, covariance matrix \u03a3, opacity \u03b1, and view-dependent color c formulated by Spherical Harmonic (SH) coefficients. Given a viewing transformation as extrinsic and intrinsic matrices W, K, 3DGS efficiently projects 3D Gaussian into 2D Gaussian on the 2D screen space and performs \u03b1-blending volume rendering to obtain the shading color C(u) of pixel u:\n$$C(u) = \\sum_{i=1}^{N} T_{i} g_{i}^{2D}(u) \\alpha_{i} c_{i}, \\quad T_{i} = \\prod_{j=1}^{i-1} (1 - g_{j}^{2D}(u) \\alpha_{j}),$$\n$$\\mu' = KW[\\mu, 1], \\quad \\Sigma' = JWEW^{T}J^{T},$$\n$$g^{2D}(u) = exp(-(u - \\mu')^{T} (\\Sigma')^{-1} (u - \\mu')),$$ where J is the Jacobian matrix of the affine approximation of the perspective projection (Zwicker et al. 2001a,b, 2002), N denotes the number of Gaussians used to shade. In this work, we employ a modified 2DGS to ensure consistent geometry and output the necessary G-buffer pack, which includes materials and geometric structures, such as albedo map \u00c2, metallic map M, and roughness map R.\nPhysical-Based Rendering We use Physical-Based Rendering (PBR) to model the view-dependent appearance, enabling effective illumination decomposition. We follow the rendering equation (Kajiya 1986) to model the outgoing radiance of a surface point \u00e6 in direction : \n$$L_{o}(x, \\omega_{o}) = \\int_{\\Omega} L_{i}(x, \\omega_{i}) f_{r} (\\omega_{i}, \\omega_{o}) (\\omega_{i} \\cdot n) d\\omega_{i},,$$ where \u03a9 represents the upper hemisphere centered at point o with normal n, and \u03c9i and \u03c9o denote the incident and view directions, respectively. The term Li(x, \u03c9i) signifies the incident radiance. In particular, we use the Cook-Torrance model (Cook and Torrance 1982) to formulate the Bidirectional Reflectance Distribution Function (BRDF) fr:\n$$f_{r}(\\omega_{i}, \\omega_{o}) = (1 - M) \\frac{A}{\\pi} + \\frac{DFG}{4(\\pi \\cdot w_{i}) (\\pi \\cdot w_{o})},$$ where the Normal Distribution Function D, Fresnel Function F, and Geometry Function G are formulated by materials. \u00c2 and M represent the albedo and metallicity, respectively. Please refer to the supplemental material for more details."}, {"title": "Methodology", "content": "We introduce a novel three-stage framework for illumination decomposition called GS-ID. Unlike previous methods focusing solely on environmental illumination, GS-ID decomposes the light field using environmental illumination (represented by a learnable panoramic map) and parametric direct light sources (modeled by Spherical Gaussians).\nIn the initial stage, we combine the normal prior from the Omnidata model (Eftekhar et al. 2021) with 2DGS (Huang et al. 2024) to produce a reasonable normal estimation. As shown in Figure 3, this approach overcomes the challenges of reconstructing textureless and glossy surfaces, which 2DGS struggles with. In the second stage, following the methodology of (Liang et al. 2023), we utilize probes to cache occlusion information, aiding in the reconstruction of ambient occlusion. In the final stage, we employ a learnable panoramic map to represent environmental illumination and a set of parametric direct light sources to model direct illumination. We then use deferred shading to render outcomes using the G-buffer pack mentioned before. Figure 2 shows the full pipeline of GS-ID."}, {"title": "Stage 1: Reconstruction Using Normal Prior", "content": "In this stage, we adopt 2DGS to reconstruct geometry. However, we observe that 2DGS mistakenly interprets glossy regions as holes and reduces the expressiveness of distant areas. To address these issues, we incorporate priors from a monocular geometric estimator to enhance the output geometric structures. Specifically, we use a pre-trained Omnidata model (Eftekhar et al. 2021) to provide normal supervision. Additionally, we employ Intrinsic Image Diffusion (Kocsis, Sitzmann, and Nie\u00dfner 2024) to generate materials for supervising the subsequent decomposition stage. In summary, the supervision loss Ls1 in Stage 1 is defined as:\n$$L_{S1} = L_{c} + \\lambda_{1}L_{d} + \\lambda_{2}L_{n} + \\lambda_{3}L_{m},$$\n$$L_{m} = 1 - n^{T} n_{m},$$ where Lc is an RGB reconstruction loss that combines L1 loss with the D-SSIM from 3DGS, Ld and Ln are regularization terms from 2DGS. Simultaneously, we employ the Omnidata Model to render normal nm and supervise the rendered normal n. By incorporating geometric priors, we enhance normal estimation accuracy and address geometry reconstruction challenges in textureless and distant areas, as illustrated in Figure 3. Our experimental results in Table 2 show a significant improvement in PSNR and normal accuracy compared to state-of-the-art methods. In the subsequent illumination decomposition stage, we conduct volume rendering to output the necessary G-buffer, including physical materials and geometric structures."}, {"title": "Stage 2: Baking Ambient Occlusion", "content": "To model environmental light transport more accurately, we bake probes that precompute and store occlusion. This enhances albedo reconstruction and illumination decomposition by calculating the exposure of each point near obstructing surfaces (Figure 8). We use probes with spherical harmonics (SH) coefficients to store scene occlusion. After Stage 1, we fix the geometry (i.e., 2DGS) and regularly place occlusion probes within the bounded 3D space as P. Each probe pCP is designed to cache the current depth cube-map {D}=1. We label the depths below a certain threshold as occlusion, resulting in the occlusion cube-map {P}=1 Finally, we cache the occlusion in the form of SH coefficients fim. In the Light Optimization stage, we can recover the ambient occlusion O(3) relative to the direction w as:\n$$O(\\omega) = \\sum_{l=0}^{deg} \\sum_{m=-l}^{l} f_{lm} Y_{lm}(\\omega),$$\n$$f_{lm} = \\int_{S^{2}} \\hat{O}(\\omega) Y_{lm} (\\omega) d\\omega,$$ where deg denotes the degree of SH, S2 denotes the unit sphere, and direction w \u2208 S2 can also be written as (\u03b8, \u03c6). {Ylm(\u00b7)} is a set of real SH basis functions, and \u00d4(w) represents the occlusion query from the occlusion cube-map {P}=1. Practically, we perform six rendering passes and get the cube-map {D}=1 of occlusion probe p. Then we transform it into a binary occlusion cube-map {P}=1 using a manually set distance threshold. In the subsequent stage, we reconstruct the ambient occlusion (AO) for each surface point from these SH probes."}, {"title": "Stage 3: Light Optimization with Material Prior", "content": "Directly solving the render equation is challenging due to the nearly infinite ray bouncing in the real world. In Stage 3, we divide light into environmental and direct illumination. Direct illumination Ldir refers to light that directly hits the surface x with one bounce, while environmental illumination Lenv involves light reflected from media. we derive the the radiance Li from incidence wi in Eq. 2 as:\n$$L_{i}(x, \\omega_{i}) = L_{v}(x, \\omega_{i}) \\cdot \\tilde{O}(\\omega_{i}) + L_{dir}^{air}(x, \\omega_{i}),$$\n$$L_{o}(x, \\omega_{o}) \\approx L_{env} (x, \\omega_{o}) \\cdot O(x) + L_{dir}(x, \\omega_{o}).$$ In this context, \u00d5 represents the environmental light visibility. We approximate the integral of the occlusion term \u00d5(wi) using the ambient occlusion O(x) obtained from Stage 2. Additionally, we employ deferred rendering to compute Lenv and Ldir in Physically Based Rendering (PBR), utilizing the G-buffer pack. The final color, Lo, is used to calculate the loss function Lc, which is a component of Ls1."}, {"title": "Environment Illumination", "content": "According to Eq. 3, environment illumination $L_{env}$ can be reformulated into its diffuse ($L_{env}^{diff}$) and specular ($L_{env}^{spec}$) components. We adopt the image-based lighting (IBL) model and split-sum approximation (Karis and Games 2013) to handle the intractable integral. $L_{env}$ can be represented as:\n$$L_{env}(x) = L_{env}^{diff} + L_{env}^{spec}$$\n$$L_{env}^{diff} \\approx K_{env}^{diff} I_{cube}, I_{d} \\qquad K_{env}^{diff} = (1 - M) \\frac{A}{\\pi},$$\n$$L_{env}^{spec} \\approx \\int_{\\Omega} \\frac{DFG}{4(n \\cdot i)(n \\cdot l)} dL \\tilde{U}(i, l) dl.$$\nThe term $K_{env}^{diff}$ can be precomputed and stored in lookup tables, whereas $I_{env}^{any}$ and $I_{env}^{inv}$ can be embedded within a learnable environment image."}, {"title": "Direct Illumination", "content": "Although environmental light encompasses illumination from all directions, the absence of specific light source positions complicates lighting edits and diminishes the representation of highlights. To address this issue, we adopt the approach proposed by (Wang et al. 2009), utilizing a mixture model of scattered Spherical Gaussians (SG mixture) to simulate direct illumination with 3-channel weights representing RGB colors. Each SG mixture can be represented as a direct light source. This method enhances the expressiveness of highlights and increases the flexibility of lighting edits. Each SG mixture, consisting of Nsg Spherical Gaussians, is formulated as:\n$$SGM(\\omega_{o}; b, \\lambda, \\mu) = \\sum_{k} \\mu_{k} e^{\\lambda_{k} (w_{o} - b_{k} - 1)} \\cdot W_{k},$$ where Wk represents the weight of each SG in the light source. bk \u2208 S2, \u03bc\u03b5 \u2208 R3, and \u03bbk \u2208 R+ denote the lobe axis, amplitude, and sharpness of the k-th SG, respectively. As shown in Figure 4, each SG mixture efficiently simulates the direct light source.\nSince we model limited and infinitesimally small light sources, the integral of direct illumination can be solved via summation according to the Importance Sampling principle (Veach 1998). The direct illumination $L_{dir}(x, \\omega_{o})$ is given by:\n$$L_{dir} (x, \\omega_{o}) = \\int_{\\Omega} f_{r} (\\omega_{i}, \\omega_{o}) L_{it} (x, \\omega_{i}) (n \\cdot \\omega_{i})d\\omega_{i},$$\n$$\\approx \\frac{1}{N_{high}} \\sum_{j} \\frac{f_{r,j} \\cdot SGM(\\omega_{o,j}; \\gamma_{j}) (n \\cdot w_{i,j})}{P_{j}d}$$ where Nlight denotes the number of direct light sources (SG mixtures), and fr,j represents the j-th light source's coefficients that conform to the BRDF Cook-Torrance microfacet model (Walter et al. 2007) defined in Eq. 3. pj represents the probability density function (PDF) for sampling the direct light source, and di is the distance between the surface and the i-th light source. Specifically, Eq. 9 can be efficiently solved using deferred ray tracing techniques (Pharr 2005). By learning the parametric direct light, as shown in Figure 1 and Figure 6, we can accurately separate direct illumination, allowing us to explicitly edit the light within the scene."}, {"title": "Losses", "content": "In Stage 3, our objective is to develop a lighting representation that is both controllable and richly expressive, capturing the nuanced interplay of light within the scene. To achieve this, we adopt the regularization term proposed by (Kocsis, Sitzmann, and Nie\u00dfner 2024):\n$$L_{pos} = \\sum_{i}^{N_{light}} 1/d_{i}, L_{val} = \\sum_{i} \\sum_{j}^{N_{light} N_{sg}} ||W_{ij} ||^{2},$$\n$$L_{light} = \\lambda_{pos} L_{pos} + \\lambda_{val} L_{val},$$"}, {"title": "Experiments", "content": "We conduct experiments on real-world datasets, such as Mip-NeRF 360 (Barron et al. 2022), to evaluate our illumination decomposition approach. Our evaluation metrics include PSNR, SSIM, and LPIPS (Zhang et al. 2018). Additionally, we utilize the TensoIR (Jin et al. 2023) synthetic dataset for comparative analysis of illumination decomposition. This dataset features scenes rendered with Blender under various lighting conditions, along with ground truth normals and albedo."}, {"title": "Comparisons", "content": "Our work aims to improve normal accuracy in Stage 1 and emphasize illumination decomposition in Stage 3. To evaluate this, we conduct comparisons using public datasets. First, we use the Mip-NeRF 360 datasets to compare novel view synthesis results and normal estimation in Stage 1, contrasting our approach with 2DGS and other contemporary methods. Then we use Mip-NeRF 360 and TensoIR Synthetic datasets to compare novel view synthesis results and ID comparison outcomes in Stage 3.\nComparison in geometry reconstruction In Stage 1, as shown in Figure 3, incorporating the pretrained Omnidata model (Eftekhar et al. 2021) as geometric priors improves normal estimation in glossy and distant areas. We evaluate novel view synthesis results and Table 2 presents quantitative comparisons on the Mip-NeRF 360 dataset. Our results demonstrate improvements compared to the vanilla 2DGS method. Importantly, our approach excels in indoor scenes due to the use of diffusion models optimized for indoor environments.\nComparison in light optimization With the improved geometry structure from Stage 1, we achieve remarkable illumination decomposition results. In the TensoIR Synthetic dataset, our approach excels in novel view synthesis and albedo estimation, achieving the second-best normal estimation. Table 1 demonstrates our precise illumination decomposition capabilities, using the same metrics for evaluating albedo quality as for novel view synthesis.\nWe utilize the Mip-NeRF 360 dataset to evaluate real-world scenes with complex lighting. As depicted in Figure 6, we accurately identify regions of direct and environmental illumination. Table 2 shows that, compared to recent relighting works such as (Liang et al. 2023; Jiang et al. 2024), our method in Stage 3 achieves superior quality. Furthermore, we successfully decompose materials, including albedo, metallicity, and roughness. As illustrated in Figure 5, our real-world decomposition enhances albedo, roughness, and normal components compared to other concurrent works. Additionally, we can explicitly determine the location and intensity of the light source, enabling editable light editing and scene composition. For a more comprehensive understanding, more applications are showcased in the supplement."}, {"title": "Ablation Studies", "content": "We introduce 2DGS for illumination decomposition (ID) and propose supervision training using a diffusion model as priors. Our method is evaluated on TensorIR Synthetic and Mip-NeRF 360 datasets, demonstrating its effectiveness in ID tasks. The following sections present a detailed ablation study on geometry and material priors, direct illumination, environment illumination, and deferred rendering.\nAnalysis on Normal and Material Regularization Accurate estimation of normals and materials is crucial for illumination decomposition (ID). To achieve this, we introduce intrinsic diffusion priors to guide the 2DGS normal and material estimation. In this section, we examine the effects of various acquisition schemes on normal, color quality, and albedo quality. Table 3 shows that the introduced prior improves normal estimation and ID results. Moreover, the albedo outcome for the TensoIR dataset is significantly lower when compared to using a material prior. Notably, the absence of a material prior results in decreased quality of novel view synthesis. The delighting results for the Mip-NeRF 360 dataset are also inferior, as shown in Figure 8.\nAnalysis on Direct and Environmental Illumination In Stage 3, we assume that the illumination consists of direct and environmental components with occlusion. To test the impact of different illumination and occlusion on ID, we explore various combination schemes. As demonstrated in Table 3 and Figure 8, removing direct illumination or occlusion both leads to poorer albedo quality, resulting in deteriorated ID outcomes. Direct illumination can better identify Bloom regions and effectively remove the highlights from the albedo. Occlusion can recognize environmental shadows and effectively reduce shadow effects.\nAnalysis on Deferred Rendering During the decomposition process, we employ the deferred shading technique instead of the forward shading approach typically used in earlier methods. In this context, we investigate its impact on light editing. As shown in Table 3, deferred rendering yields superior ID results and enhanced quality in all metrics."}, {"title": "Applications", "content": "We conduct light editing applications using the recovered geometry, material, and illumination from our GS-ID. As shown in Figure 7, the results demonstrate that GS-ID effectively handles precise light editing, generating photorealistic renderings. Furthermore, our parametric light sources allows for illumination extraction from one decomposed scene and integration into another scene without additional training. More results can be found in the supplementary materials."}, {"title": "Conclusion", "content": "We introduce GS-ID, a novel three-stage framework for illumination decomposition that enables photorealistic novel view synthesis, intuitive light editing, and scene composition. GS-ID integrates 2D Gaussian Splatting with intrinsic diffusion priors to regularize normal and material estimation. We decompose the light field into environmental and direct illumination, employing parametric modeling and optimization schemes for enhanced illumination decomposition. Additionally, GS-ID utilizes deferred rendering in screen space to achieve high rendering performance. Our work has a few limitations. For example, GS-ID relies on the priors from intrinsic diffusion methods, which fails to generalize to out-of-distribution cases, resulting in degeneration decomposition. Our future work includes improving the applicability of GS-ID. We plan to explore various applications, such as simulating a wide range of parametric light sources (e.g., area light) to increase the diversity of lighting effects. Furthermore, we will consider the visibility between direct lights and surface points to integrate shadow effects into novel view synthesis, ensuring more realistic and harmonic results, especially for the scene composition."}, {"title": "Supplementary Material", "content": "In this supplementary material, we first present the implementation details of our method. This is followed by additional results from various datasets. We also provide further qualitative results on material prediction, including factors such as albedo, normals properties, and lighting conditions for both synthetic and realistic datasets. Finally, we offer more application results. Additional videos demonstrating our work can be found in the supplementary material HTML file (main.html)."}, {"title": "Implementation Details", "content": "We implement Gaussian ID using the PyTorch framework with CUDA extensions and modify 2DGS to output G-buffer properties, including roughness, metallic, normal, depth, and ambient occlusion."}, {"title": "Representation", "content": "In the vanilla 2DGS, each 2D Gaussian utilizes learnable parameters T = {p,s,q,n,d} and C = {\u03b1, fc} to describe its geometric properties and volumetric appearance, respectively. Here, p denotes the position vector, s denotes the scaling vector, q denotes the unit quaternion for rotation, n denotes the normal, d denotes the depth, \u03b1 denotes the opacity, and fo denotes the spherical harmonics (SH) coefficients for view-dependent color. In GS-ID, we extend C to {\u03b1, fc, A, R, M} to describe the material properties of the 2D Gaussian."}, {"title": "Training Details", "content": "We employ Physically-Based Rendering (PBR) to model the view-dependent appearance, facilitating effective illumination decomposition. Specifically, we utilize the Cook-Torrance model to formulate the Bidirectional Reflectance Distribution Function (BRDF) fr:\n$$f_{r} (\\omega_{i}, \\omega_{o}) = (1 - M) \\frac{A}{\\pi} + \\frac{DFG}{4(\\pi \\cdot w_{i}) (\\pi \\cdot w_{o})},$$\n$$h = normalize(\\omega_{o} + \\omega_{i}),$$\n$$F_{o} = (1 - M) * 0.04 + M * A,$$\n$$D(n,h) = \\frac{R^{4}}{(n \\cdot h (R^{4} \u2013 1) + 1)^{2}},$$\n$$F(\\omega_{i}, n) = F_{o} + (1 \u2212 F_{o}) (1 \u2212 n \\cdot w_{i})^{5},$$\n$$G(\\omega_{o}, \\omega_{i}, h) = G_{1}(w, h) G_{1}(w, h),$$\n$$G_{1}(n,h) = \\frac{1}{1 + n \\cdot h / \\sqrt{R^{4} + n \\cdot h-R \\cdot h - R^{4} *n \\cdot h}}$$\nwhere A, R, and M denote the albedo, roughness, and metallicity, respectively. The Normal Distribution Function"}, {"title": "Stage 1", "content": "As outlined in the Methodology section, we aim to minimize the color reconstruction loss (Lc) and the normal loss (Ln). The normal loss is composed of \u039b1Ld + \u039b2Ln + \u039b3Lm, where \u039b3Lm represents the loss supervised by the Omnidata Model. This supervision is crucial for maintaining geometric consistency, particularly in the glossy and distant regions of the reconstructed 2DGS geometry. We use \u03bb\u2081 = 100, \u03bb\u2082 = 0.25, and \u03bb\u2083 = 0.25. To optimize the parameters T and C from 2DGS, we perform this minimization for 30k iterations."}, {"title": "Stage 2", "content": "In Stage 2, we introduce spherical harmonics (SH) architectures and cache occlusion into occlusion volumes Voccl, as illustrated in Fig. 10. For each volume vocel cocci, we position six cameras with a field of view (FoV) of 90\u00b0, ensuring they do not overlap. We then perform six render passes to obtain the depth cubemap {Dp}j=1. By comparing the values of the depth map with a threshold, we convert {0}6=1 into the occlusion cubemap {0}=1 and store the principal components of occlusion in spherical harmonics (SH) coefficients fim within the volume."}, {"title": "Stage 3", "content": "We employ a mixture model of scattered Spherical Gaussians (SG mixture) to represent the direct illumination as Ldir (x, wo):\n$$L_{dir} (x, w) = \\frac{1}{N_{light}} \\sum_{i} \\frac{K_{it} (n \\cdot w_{i})}{d^{2}} WSG(j),$$ where di denotes the distance from the i-th light source cluster to the surface point x. The probability density function (PDF) is 1.0 for a point light source. K and SG denote the BRDF function and the Spherical Gaussian Function, respectively, as defined in the Methodology section. Utilizing pretrained models T and C, along with material properties generated by a pretrained diffusion model, we optimize M and L over 30,000 iterations. The optimization is guided by the following loss function:\n$$L_{reg} = \\lambda_{R}L_{2}(R, \\hat{R}) + \\lambda_{\\mu}L_{2}(M, \\hat{M}) + \\lambda_{A}L_{2}(A, \\hat{A}),$$ where the weights for the loss function are set as: \u03bbR = 0.1, \u03bb\u03bc = 0.1, and \u03bba = 0.5. The entire illumination optimization process can be visualized as shown in Fig. 9."}]}