{"title": "GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion Prior and Parametric Light Source Optimization", "authors": ["Kang Du", "Zhihao Liang", "Zeyu Wang"], "abstract": "We present GS-ID, a novel framework for illumination decomposition on Gaussian Splatting, achieving photorealistic novel view synthesis and intuitive light editing. Illumination decomposition is an ill-posed problem facing three main challenges: 1) priors for geometry and material are often lacking; 2) complex illumination conditions involve multiple unknown light sources; and 3) calculating surface shading with numerous light sources is computationally expensive. To address these challenges, we first introduce intrinsic diffusion priors to estimate the attributes for physically based rendering. Then we divide the illumination into environmental and direct components for joint optimization. Last, we employ deferred rendering to reduce the computational load. Our framework uses a learnable environment map and Spherical Gaussians (SGs) to represent light sources parametrically, therefore enabling controllable and photorealistic relighting on Gaussian Splatting. Extensive experiments and applications demonstrate that GS-ID produces state-of-the-art illumination decomposition results while achieving better geometry reconstruction and rendering performance. The source code is available at https://github.com/dukang/GS-ID.", "sections": [{"title": "Introduction", "content": "Reconstructing physical attributes from multiple observations has long been challenging in computer vision and computer graphics. Illumination is a highly diverse and complicated factor that significantly influences observations. Illumination Decomposition (ID) aims to achieve controllable lighting editing and produce various visual effects. However, ID is an extremely ill-posed problem, as varying interactions between different lighting distributions and materials can produce identical light effects. This issue is compounded by the complexity of illumination (e.g., self-emission, direct, and indirect illumination). Without priors of geometry and materials, this task becomes exceedingly difficult. Furthermore, the need to accurately decompose materials and geometry, solve the complex rendering equation, and consider multiple light sources and ray bounce exacerbates the challenge.\nMany recent works focus on appearance reconstruction. Neural Radiance Field (NeRF) (Mildenhall et al. 2021) employs an MLP to represent a static scene as a 5D implicit light field, achieving photorealistic novel view synthesis results. 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) further incorporates explicit representation with efficient rasterization, enabling real-time rendering. However, their 3D appearance reconstruction mainly concentrates on view-dependent appearance without further decomposition.\nTo acquire a controllable light field, a well-posed geometry is necessary to support the subsequent decomposition task. Recently, 2D Gaussian Splatting (2DGS) (Huang et al. 2024) has emerged as a promising method for geometry reconstruction, utilizing surfel-based representation to ensure geometric multi-view consistency. 2DGS aims to deliver a well-posed geometry, providing significant support for ID. However, it primarily focuses on geometry reconstruction and falls short in modeling physical materials and light transport on the surface. This limitation impedes its effectiveness in more complex tasks like relighting and light editing. Another line of research (Jin et al. 2023; Jiang et al. 2024; Liang et al. 2023) on 3D inverse rendering can model both material and geometry simultaneously. However, these methods almost exclusively focus on formulating environ-"}, {"title": "Related Work", "content": "Geometry Reconstruction Regarding surface reconstruction, some methods (Park et al. 2019; Niemeyer et al. 2020; Oechsle, Peng, and Geiger 2021; Yariv et al. 2020; Wang et al. 2021; Yariv et al. 2021) use an MLP to model an implicit field representing the target surface. After training, surfaces are extracted using isosurface extraction algorithms (Lorensen and Cline 1998; Ju et al. 2002). Recently, several methods (Huang et al. 2024; Yu, Sattler, and Geiger 2024) achieve fast and high-quality geometry reconstruction of complex scenes based on 3DGS (Kerbl et al. 2023). However, these methods often treat the appearance of a functional of view directions, neglecting the modeling of physical materials and light transport on the surface. This limitation hinders their ability to handle complex tasks such as light editing.\nIntrinsic Decomposition To decompose the intrinsics from observations, some monocular methods (Zhu et al. 2022b,a; Kocsis, Sitzmann, and Nie\u00dfner 2024; Zeng et al. 2024) learn from labeled datasets and estimate intrinsics directly from single images. However, these methods lack multi-view consistency and struggle to tackle out-of-distribution cases. In contrast, other methods (Bi et al. 2020; Srinivasan et al. 2021; Zhang et al. 2021, 2022; Munkberg et al. 2022; Jin et al. 2023; Jiang et al. 2024; Liang et al. 2023) construct a 3D consistent intrinsic field from multiple observations. Notably, GaussianShader and GS-IR build upon 3DGS and enable fast intrinsic decomposition on glossy areas and complex scenes. While obtaining impressive results, current methods almost only consider environmental illumination, making precise editing challenging."}, {"title": "Preliminaries", "content": "Gaussian Splatting 3D Gaussian Splatting (3DGS) represents a 3D scene as a collection of primitives. Each primitive models a 3D Gaussian associated with a mean vector \u00b5, covariance matrix \u03a3, opacity \u03b1, and view-dependent color c formulated by Spherical Harmonic (SH) coefficients. Given a viewing transformation as extrinsic and intrinsic matrices W, K, 3DGS efficiently projects 3D Gaussian into 2D Gaussian on the 2D screen space and performs \u03b1-blending volume rendering to obtain the shading color C(u) of pixel u:\n$\\begin{aligned}\n&C(u) = \\sum_{i=1}^{N} T_{i} g_{i}^{2 D}(u) \\alpha_{i} c_{i}, \\quad T_{i}=\\prod_{j=1}^{i-1}\\left(1-g_{j}^{2 D}(u) \\alpha_{j}\\right),\n\\\\\n&\\mu^{\\prime}=K W[\\mu, 1], \\quad \\Sigma^{\\prime}=J W \\Sigma W^{T} J^{T},\n\\\\\n&g^{2 D}(u)=\\exp \\left(-\\frac{1}{2}(u-\\mu)^{T} \\Sigma^{\\prime-1}(u-\\mu)\\right) ;\n\\end{aligned}$\nwhere J is the Jacobian matrix of the affine approximation of the perspective projection (Zwicker et al. 2001a,b, 2002), N denotes the number of Gaussians used to shade. In this work, we employ a modified 2DGS to ensure consistent geometry and output the necessary G-buffer pack, which includes materials and geometric structures, such as albedo map \u00c2, metallic map M, and roughness map R.\nPhysical-Based Rendering We use Physical-Based Rendering (PBR) to model the view-dependent appearance, enabling effective illumination decomposition. We follow the rendering equation (Kajiya 1986) to model the outgoing radiance of a surface point \u00e6 in direction :\n$\\begin{equation}\nL_{o}\\left(\\mathbf{x}, \\omega_{o}\\right)=\\int_{\\Omega} L_{i}\\left(\\mathbf{x}, \\omega_{i}\\right) f_{r}\\left(\\omega_{i}, \\omega_{o}\\right)\\left(\\mathbf{w}_{i} \\cdot \\mathbf{n}\\right) d \\mathbf{w}_{i},\n\\end{equation}$\nwhere \u03a9 represents the upper hemisphere centered at point \u00e6 with normal n, and \u03c9i and \u03c9o denote the incident and view directions, respectively. The term Li(x, w\u2081) signifies the incident radiance. In particular, we use the Cook-Torrance model (Cook and Torrance 1982) to formulate the Bidirectional Reflectance Distribution Function (BRDF) fr:\n$\\begin{equation}\nf_{r}\\left(\\omega_{i}, \\omega_{o}\\right)=(1-M) \\frac{A}{\\pi}+\\frac{D F G}{4(\\mathbf{n} \\cdot \\omega_{i})(\\mathbf{n} \\cdot \\omega_{o})},\n\\end{equation}$\nwhere the Normal Distribution Function D, Fresnel Function F, and Geometry Function G are formulated by materials. \u00c2 and M represent the albedo and metallicity, respectively. Please refer to the supplemental material for more details."}, {"title": "Methodology", "content": "We introduce a novel three-stage framework for illumination decomposition called GS-ID. Unlike previous methods focusing solely on environmental illumination, GS-ID decomposes the light field using environmental illumination (represented by a learnable panoramic map) and parametric direct light sources (modeled by Spherical Gaussians).\nIn the initial stage, we combine the normal prior from the Omnidata model (Eftekhar et al. 2021) with 2DGS (Huang et al. 2024) to produce a reasonable normal estimation. As shown in Figure 3, this approach overcomes the challenges of reconstructing textureless and glossy surfaces, which 2DGS struggles with. In the second stage, following the methodology of (Liang et al. 2023), we utilize probes to cache occlusion information, aiding in the reconstruction of ambient occlusion. In the final stage, we employ a learnable panoramic map to represent environmental illumination and a set of parametric direct light sources to model direct illumination. We then use deferred shading to render outcomes using the G-buffer pack mentioned before. Figure 2 shows the full pipeline of GS-ID.\nIn this stage, we adopt 2DGS to reconstruct geometry. However, we observe that 2DGS mistakenly interprets glossy re-gions as holes and reduces the expressiveness of distant areas. To address these issues, we incorporate priors from a monocular geometric estimator to enhance the output geometric structures. Specifically, we use a pre-trained Omnidata model (Eftekhar et al. 2021) to provide normal supervision. Additionally, we employ Intrinsic Image Diffusion (Kocsis, Sitzmann, and Nie\u00dfner 2024) to generate materials for supervising the subsequent decomposition stage. In summary, the supervision loss Ls1 in Stage 1 is defined as:\n$\\begin{aligned}\nL_{S 1} &=L_{c}+\\lambda_{1} L_{d}+\\lambda_{2} L_{n}+\\lambda_{3} L_{m},\n\\\\\nL_{m} &=1-\\mathbf{n}^{T} \\mathbf{n}_{m},\n\\end{aligned}$\nwhere Lc is an RGB reconstruction loss that combines L1 loss with the D-SSIM from 3DGS, Ld and Ln are regularization terms from 2DGS. Simultaneously, we employ the Omnidata Model to render normal nm and supervise the rendered normal n. By incorporating geometric priors, we enhance normal estimation accuracy and address geometry reconstruction challenges in textureless and distant areas, as illustrated in Figure 3. Our experimental results in Table 2 show a significant improvement in PSNR and normal accuracy compared to state-of-the-art methods. In the subsequent illumination decomposition stage, we conduct volume rendering to output the necessary G-buffer, including physical materials and geometric structures."}, {"title": "Stage 2: Baking Ambient Occlusion", "content": "To model environmental light transport more accurately, we bake probes that precompute and store occlusion. This enhances albedo reconstruction and illumination decomposition by calculating the exposure of each point near obstructing surfaces (Figure 8). We use probes with spherical harmonics (SH) coefficients to store scene occlusion. After Stage 1, we fix the geometry (i.e., 2DGS) and regularly place occlusion probes within the bounded 3D space as P. Each probe pCP is designed to cache the current depth cube-map {Di}i=1. We label the depths below a certain threshold as occlusion, resulting in the occlusion cube-map {Pi}i=1 Finally, we cache the occlusion in the form of SH coefficients fim. In the Light Optimization stage, we can recover the ambient occlusion O(w) relative to the direction w as:\n$\\begin{aligned}\n&O(\\omega)=\\sum_{l=0}^{d e g} \\sum_{m=-l}^{l} f_{l m} Y_{l m}(\\mathbf{w}),\n\\\\\n&f_{l m}=\\int_{S^{2}} \\hat{O}(\\mathbf{w}) Y_{l m}(\\mathbf{w}) d \\mathbf{w},\n\\end{aligned}$\nwhere deg denotes the degree of SH, S2 denotes the unit sphere, and direction w \u2208 S2 can also be written as (\u03b8, \u03c6). {Ylm(\u00b7)} is a set of real SH basis functions, and \u00d4(w) represents the occlusion query from the occlusion cube-map {Pi}i=1. Practically, we perform six rendering passes and get the cube-map {Di}i=1 of occlusion probe p. Then we transform it into a binary occlusion cube-map {Pi}i=1 using a manually set distance threshold. In the subsequent stage, we reconstruct the ambient occlusion (AO) for each surface point from these SH probes."}, {"title": "Stage 3: Light Optimization with Material Prior", "content": "Directly solving the render equation is challenging due to the nearly infinite ray bouncing in the real world. In Stage 3, we divide light into environmental and direct illumination. Direct illumination Ldir refers to light that directly hits the surface x with one bounce, while environmental illumination Lenv involves light reflected from media. we derive the the radiance Li from incidence wi in Eq. 2 as:\n$\\begin{equation}\nL_{i}\\left(\\mathbf{x}, \\omega_{i}\\right)=L_{e n v}\\left(\\mathbf{x}, \\omega_{i}\\right) \\cdot \\tilde{O}\\left(\\omega_{i}\\right)+L_{d i r}\\left(\\mathbf{x}, \\omega_{i}\\right),\n\\end{equation}$\n$\\begin{equation}\nL_{o}\\left(\\mathbf{x}, \\omega_{o}\\right) \\approx L_{e n v}\\left(\\mathbf{x}, \\omega_{o}\\right) \\cdot O(x)+L_{d i r}\\left(\\mathbf{x}, \\omega_{o}\\right).\n\\end{equation}$\nIn this context, \u00d5 represents the environmental light visibility. We approximate the integral of the occlusion term \u00d5(wi) using the ambient occlusion O(x) obtained from Stage 2. Additionally, we employ deferred rendering to compute Lenv and Ldir in Physically Based Rendering (PBR), utilizing the G-buffer pack. The final color, Lo, is used to calculate the loss function Lc, which is a component of Ls1.\nAccording to Eq. 3, environment illumination Lenv can be reformulated into its diffuse (Lenvdiff) and specular (Lenvspec) components. We adopt the image-based lighting (IBL) model and split-sum approximation (Karis and Games 2013) to handle the intractable integral. Lenv can be represented as:\n$\\begin{equation}\nL_{e n v}(x)=L_{e n v}^{d i f f}+L_{e n v}^{s p e c},\n\\end{equation}$\n$\\begin{equation}\nL_{e n v}^{d i f f} \\approx K_{e n v} I_{c u b e}, \\quad K_{e n v}=(1-M) \\frac{A}{\\pi},\n\\end{equation}$\n$\\begin{equation}\nL_{e n v}^{s p e c} \\approx \\int_{\\Omega} \\frac{D F G}{(\\mathbf{l} \\cdot \\mathbf{n})\\left(\\mathbf{n} \\cdot \\omega_{i}\\right)} d l \\cdot \\int_{\\Omega} \\tilde{I}_{e n v}(\\mathbf{l}) d l,\n\\end{equation}$\nThe term Kenv can be precomputed and stored in lookup tables, whereas \u00ceenv and \u0128env can be embedded within a learnable environment image.\nAlthough environmental light encompasses illumination from all directions, the absence of specific light source positions complicates lighting edits and diminishes the representation of highlights. To address this issue, we adopt the approach proposed by (Wang et al. 2009), utilizing a mixture model of scattered Spherical Gaussians (SG mixture) to simulate direct illumination with 3-channel weights representing RGB colors. Each SG mixture can be represented as a direct light source. This method enhances the expressiveness of highlights and increases the flexibility of lighting edits. Each SG mixture, consisting of Nsg Spherical Gaussians, is formulated as:\n$\\begin{equation}\nS G M\\left(\\omega_{o} ; \\mathbf{b}, \\lambda, \\mu\\right)=\\sum_{k}^{\\text { Nsg }} \\mu_{k} e^{-\\lambda_{k} \\left(\\mathbf{b}_{k} \\cdot \\omega_{o}-1\\right)}, w_{k},\n\\end{equation}$\nwhere Wk represents the weight of each SG in the light source. bk \u2208 S2, \u00b5 \u2208 R3, and \u03bbk \u2208 R+ denote the lobe axis, amplitude, and sharpness of the k-th SG, respectively. As shown in Figure 4, each SG mixture efficiently simulates the direct light source.\nSince we model limited and infinitesimally small light sources, the integral of direct illumination can be solved via summation according to the Importance Sampling principle (Veach 1998). The direct illumination Ldir(x, w\uff61) is given by:\n$\\begin{equation}\nL_{d i r}\\left(\\mathbf{x}, \\omega_{o}\\right)=\\int_{\\Omega} f_{r}\\left(\\omega_{i}, \\omega_{o}\\right) L_{i d}^{i r}\\left(\\mathbf{x}, \\omega_{i}\\right) \\left(\\mathbf{n} \\cdot \\omega_{i}\\right) d \\omega_{i},\n\\end{equation}$\n$\\begin{equation}\n\\approx \\frac{1}{N_{l i g h t}} \\sum_{j}^{N_{l i g h t}} f_{r, j} \\cdot S G M\\left(\\omega_{o, j} ; \\theta_{j}\\right)\\left(\\mathbf{n} \\cdot \\omega_{i, j}\\right)\n\\end{equation}$\nwhere Nlight denotes the number of direct light sources (SG mixtures), and fr,j represents the j-th light source's coefficients that conform to the BRDF Cook-Torrance microfacet model (Walter et al. 2007) defined in Eq. 3. pj represents the probability density function (PDF) for sampling the direct light source, and di is the distance between the surface and the i-th light source. Specifically, Eq. 9 can be efficiently solved using deferred ray tracing techniques (Pharr 2005). By learning the parametric direct light, as shown in Figure 1 and Figure 6, we can accurately separate direct illumination, allowing us to explicitly edit the light within the scene.\nTo enhance illumination decomposition, we employ a dense light source initialization strategy. Specifically, we divide the bounded 3D space into a regular grid and place an SG mixture representing direct light sources at each grid point, ensuring comprehensive scene coverage. Each SG mixture includes Nsg SGs with varying weights. During optimization, the parameters of each SG and the positions of the light sources are trainable. Given that most real-world scenarios feature few significant light sources, we introduce a progressive pruning scheme to eliminate weak light sources, thereby improving decomposition efficiency and editing friendliness. Our experiments demonstrate that the light sources gradually converge to plausible locations, producing accurate visual effects as shown in Figure 4. For more details, please refer to the supplementary materials.\nIn Stage 3, our objective is to develop a lighting representation that is both controllable and richly expressive, capturing the nuanced interplay of light within the scene. To achieve this, we adopt the regularization term proposed by (Kocsis, Sitzmann, and Nie\u00dfner 2024):\n$\\begin{equation}\nL_{p o s}=\\sum_{i}^{N_{l i g h t}} 1 / d_{i}, \\quad L_{v a l}=\\sum_{i}^{N_{l i g h t}} \\sum_{j}^{N_{s g}}|\\left|W_{i j}\\right||^{2},\n\\end{equation}$\n$\\begin{equation}\nL_{l i g h t}=\\lambda_{p o s} L_{p o s}+\\lambda_{v a l} L_{v a l},\n\\end{equation}$\nwhere di denotes the distance between the light source and the nearest surface point, and Wij denotes the weight of each SG. For decomposition, we introduce intrinsic diffusion priors to alleviate the ill-posedness:\n$\\begin{equation}\nL_{r e g}=\\lambda_{R} L_{2}(\\mathbf{R}, \\hat{\\mathbf{R}})+\\lambda_{M} L_{2}(\\mathbf{M}, \\hat{\\mathbf{M}})+\\lambda_{A} L_{2}(\\mathbf{A}, \\hat{\\mathbf{A}}),\n\\end{equation}$\nwhere R, M, \u00c2 denote the predicted roughness, metallicity and albedo, respectively. R, M, A are the pseudo material supervision produced by Intrinsic Image Diffusion. Primarily, we follow Eq. 4 to finely optimize the geometry and appearance jointly in Stage 3. Combining with Eqs. 4, 10, and 11, the summary loss Ls3 in Stage 3 is:\n$\\begin{equation}\nL_{S 3}=L_{S 1}+L_{l i g h t}+L_{r e g}.\n\\end{equation}$"}, {"title": "Experiments", "content": "We conduct experiments on real-world datasets, such as Mip-NeRF 360 (Barron et al. 2022), to evaluate our illumination decomposition approach. Our evaluation metrics include PSNR, SSIM, and LPIPS (Zhang et al. 2018). Additionally, we utilize the TensoIR (Jin et al. 2023) synthetic dataset for comparative analysis of illumination decomposition. This dataset features scenes rendered with Blender under various lighting conditions, along with ground truth normals and albedo.\nOur work aims to improve normal accuracy in Stage 1 and emphasize illumination decomposition in Stage 3. To evaluate this, we conduct comparisons using public datasets. First, we use the Mip-NeRF 360 datasets to compare novel view synthesis results and normal estimation in Stage 1, contrasting our approach with 2DGS and other contemporary methods. Then we use Mip-NeRF 360 and TensorIR Synthetic datasets to compare novel view synthesis results and ID comparison outcomes in Stage 3.\nIn Stage 1, as shown in Figure 3, incorporating the pretrained Omnidata model (Eftekhar et al. 2021) as geometric priors improves normal estimation in glossy and distant areas. We evaluate novel view synthesis results and Table 2 presents quantitative comparisons on the Mip-NeRF 360 dataset. Our results demonstrate improvements compared to the vanilla 2DGS method. Importantly, our approach excels in indoor scenes due to the use of diffusion models optimized for indoor environments.\nWith the improved geometry structure from Stage 1, we achieve remarkable illumination decomposition results. In the TensoIR Synthetic dataset, our approach excels in novel view synthesis and albedo estimation, achieving the second-best normal estimation. Table 1 demonstrates our precise illumination decomposition capabilities, using the same metrics for evaluating albedo quality as for novel view synthesis.\nWe utilize the Mip-NeRF 360 dataset to evaluate real-world scenes with complex lighting. As depicted in Figure 6, we accurately identify regions of direct and environmental illumination. Table 2 shows that, compared to recent relighting works such as (Liang et al. 2023; Jiang et al. 2024), our method in Stage 3 achieves superior quality. Furthermore, we successfully decompose materials, including albedo, metallicity, and roughness. As illustrated in Figure 5, our real-world decomposition enhances albedo, roughness, and normal components compared to other concurrent works. Additionally, we can explicitly determine the location and intensity of the light source, enabling editable light editing and scene composition. For a more comprehensive understanding, more applications are showcased in the supplement."}, {"title": "Ablation Studies", "content": "We introduce 2DGS for illumination decomposition (ID) and propose supervision training using a diffusion model as priors. Our method is evaluated on TensorIR Synthetic and Mip-NeRF 360 datasets, demonstrating its effectiveness in ID tasks. The following sections present a detailed ablation study on geometry and material priors, direct illumination, environment illumination, and deferred rendering.\nAccurate estimation of normals and materials is crucial for illumination decomposition (ID). To achieve this, we introduce intrinsic diffusion priors to guide the 2DGS normal and material estimation. In this section, we examine the effects of various acquisition schemes on normal, color quality, and albedo quality. Table 3 shows that the introduced prior improves normal estimation and ID results. Moreover, the albedo outcome for the TensoIR dataset is significantly lower when compared to using a material prior. Notably, the absence of a material prior results in decreased quality of novel view synthesis. The delighting results for the Mip-NeRF 360 dataset are also inferior, as shown in Figure 8.\nIn Stage 3, we assume that the illumination consists of direct and environmental components with occlusion. To test the impact of different illumination and occlusion on ID, we explore various combination schemes. As demonstrated in Table 3 and Figure 8, removing direct illumination or occlusion both leads to poorer albedo quality, resulting in deteriorated ID outcomes. Direct illumination can better identify Bloom regions and effectively remove the highlights from the albedo. Occlusion can recognize environmental shadows and effectively reduce shadow effects.\nDuring the decomposition process, we employ the deferred shading technique instead of the forward shading approach typically used in earlier methods. In this context, we investigate its impact on light editing. As shown in Table 3, deferred rendering yields superior ID results and enhanced quality in all metrics."}, {"title": "Applications", "content": "We conduct light editing applications using the recovered geometry, material, and illumination from our GS-ID. As shown in Figure 7, the results demonstrate that GS-ID effectively handles precise light editing, generating photorealistic renderings. Furthermore, our parametric light sources allows for illumination extraction from one decomposed scene and integration into another scene without additional training. More results can be found in the supplementary materials."}, {"title": "Conclusion", "content": "We introduce GS-ID, a novel three-stage framework for illumination decomposition that enables photorealistic novel view synthesis, intuitive light editing, and scene composition. GS-ID integrates 2D Gaussian Splatting with intrinsic diffusion priors to regularize normal and material estimation. We decompose the light field into environmental and direct illumination, employing parametric modeling and optimization schemes for enhanced illumination decomposition. Additionally, GS-ID utilizes deferred rendering in screen space to achieve high rendering performance. Our work has a few limitations. For example, GS-ID relies on the priors from intrinsic diffusion methods, which fails to generalize to out-of-distribution cases, resulting in degeneration decomposition. Our future work includes improving the applicability of GS-ID. We plan to explore various applications, such as simulating a wide range of parametric light sources (e.g., area light) to increase the diversity of lighting effects. Furthermore, we will consider the visibility between direct lights and surface points to integrate shadow effects into novel view synthesis, ensuring more realistic and harmonic results, especially for the scene composition."}]}