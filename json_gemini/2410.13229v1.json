{"title": "Quamba: A Post-Training Quantization Recipe for Selective State Space Models", "authors": ["Hung-Yueh Chiang", "Chi-Chih Chang", "Natalia Frumkin", "Kai-Chiang Wu", "Diana Marculescu"], "abstract": "State Space Models (SSMs) have emerged as an appealing alternative to Transformers for large language models, achieving state-of-the-art accuracy with constant memory complexity which allows for holding longer context lengths than attention-based networks. The superior computational efficiency of SSMs in long sequence modeling positions them favorably over Transformers in many scenarios. However, improving the efficiency of SSMs on request-intensive cloud-serving and resource-limited edge applications is still a formidable task. SSM quantization is a possible solution to this problem, making SSMs more suitable for wide deployment, while still maintaining their accuracy. Quantization is a common technique to reduce the model size and to utilize the low bit-width acceleration features on modern computing units, yet existing quantization techniques are poorly suited for SSMs. Most notably, SSMs have highly sensitive feature maps within the selective scan mechanism (i.e., linear recurrence) and massive outliers in the output activations which are not present in the output of token-mixing in the self-attention modules. To address this issue, we propose a static 8-bit per-tensor SSM quantization method which suppresses the maximum values of the input activations to the selective SSM for finer quantization precision and quantizes the output activations in an outlier-free space with Hadamard transform. Our 8-bit weight-activation quantized Mamba 2.8B SSM benefits from hardware acceleration and achieves a 1.72 \u00d7 lower generation latency on an Nvidia Orin Nano 8G, with only a 0.9% drop in average accuracy on zero-shot tasks. When quantizing Jamba, a 52B parameter SSM-style language model, we observe only a 1% drop in accuracy, demonstrating that our SSM quantization method is both effective and scalable for large language models, which require appropriate compression techniques for deployment. The experiments demonstrate the effectiveness and practical applicability of our approach for deploying SSM-based models of all sizes on both cloud and edge platforms. Code is released at https://github.com/enyac-group/Quamba.", "sections": [{"title": "1 Introduction", "content": "State Space Models (SSMs) (Gu and Dao 2023; Lieber et al. 2024b) have attracted notable attention due to their efficiency in long sequence modeling and comparable performance to Transformers (Brown et al. 2020; Zhang et al. 2022). Although Transformers have shown a strong ability to capture the causal relationships in long sequences, the self-attention module within Transformers incurs a quadratic computation complexity with respect to the context length in the prefilling stage as well as a linear memory complexity (e.g., the K-V cache) in the generation stage. In contrast, SSMs, an attractive substitute to Transformers, perform sequence modeling with a recurrent neural network-like (RNN-like) linear recurrence module (i.e., selective scan) which has linear computation complexity in the prefilling stage and constant memory complexity in the generation stage."}, {"title": "2 Related Work", "content": "Model quantization. Deploying large, over-parameterized neural networks on resource-constrained devices is challenging, and researchers have developed model quantization (Han, Mao, and Dally 2015; Jacob et al. 2018; Wang et al. 2019) as a solution to this problem. Quantization techniques reduce the data type precision (e.g. FP32 to INT4) to compress the model size and accelerate inference. The quantization techniques are generally divided into two categories: Quantization-aware training (QAT) and post-training quantization (PTQ) (Gholami et al. 2022; Zhou et al. 2024; Zhu et al. 2023). QAT (Dettmers et al. 2024; Liu et al. 2023) requires additional training efforts to adapt models to low bit-width, in exchange for better model performance. Our work falls under PTQ, which does not require training and can therefore be plug-and-play.\nLLM post-training quantization. Post-training quantization (PTQ) techniques are generally broken down into two categories: weight-only quantization (e.g., W4A16 and W2A16) and weight-activation quantization (e.g., W8A8 and W4A4) (Zhu et al. 2023). Weight-only quantization (Frantar et al. 2022; Lin et al. 2023) focuses on quantizing weight matrices (e.g., 4-bit or 2-bit) while keeping the activations in half-precision floating point. However, although weight-only quantization reduces memory usage, it still requires costly floating-point arithmetic for compute-intensive operations (e.g., linear layers). To utilize low bit-width operations, Ashkboos et al. (2024b), Dettmers et al. (2022), Xiao et al. (2023), and Zhao et al. (2023) study quantization for both weights and activations in Transformers. They address outliers in activations by using mixed-precision (Dettmers et al. 2022), rescaling quantization factors (Xiao et al. 2023), group quantization (Zhao et al. 2023), and quantizing activations in an outlier-free space (Ashkboos et al. 2024b). Unfortunately, these techniques target Transformers, do not generalize well to SSMs, and either fail to handle the sensitive tensors in SSMs resulting in poor performance (Dettmers et al. 2022; Xiao et al. 2023) or introduce additional computational overhead to the input of the selective scan (Ashkboos et al. 2024b; Zhao et al. 2023). Our research addresses this gap by examining SSM weight-activation quantization, aiming to concurrently reduce memory and compute costs by harnessing hardware acceleration for integer operations.\nState Space Models. In recent times, a new wave of RNN-like models (Beck et al. 2024; Gu and Dao 2023; Gu, Goel, and R\u00e9 2021; Peng et al. 2023; Smith, Warrington, and Linderman 2022) has emerged, noted for their efficacy in modeling long-range dependencies and achieving performance comparable to Transformers. State Space Models (SSMs) (Gu and Dao 2023; Gu, Goel, and R\u00e9 2021; Smith, Warrington, and Linderman 2022) are a promising class of architectures that have been successfully applied to various applications, such as text (Gu and Dao 2023; Wang et al. 2024), image (Nguyen et al. 2022; Zhu et al. 2024), video (Li et al. 2024; Nguyen et al. 2022), and audio (Goel et al. 2022; Saon, Gupta, and Cui 2023). Despite their successes, the challenge of deploying SSMs across resource-limited hardware platforms remains largely unresolved. Our work addresses this challenge by proposing a quantization method specifically tailored for SSMs."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Selective State Space Models", "content": "State Space Models. Inspired by continuous systems, discrete linear time-invariant (LTI) SSMs (Gu et al. 2020, 2022; Gu, Goel, and R\u00e9 2021) map input sequences x to output sequences y. Given a discrete input signal $x_t$ at time step t, the transformation $x_t \\rightarrow y_t$ through a hidden state h is defined as\n$h_t = Ah_{t-1} + Bx_t, y_t = Ch_t + Dx_t$  (1)\nwhere A and B are discrete parameters. The discretization function for A and B with a given A is defined as $\\bar{A} = exp(\\Delta A)$, $\\bar{B} = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I) \\cdot \\Delta B \\approx AB$. This system uses A as a state transition parameter and B and C as projection parameters. A is the time-scale parameter that is used to discretize the continuous parameters A and B. D is an optional residual parameter. (A, B, C, D, \u2206) are trainable parameters. A residual branch $z_l$ is applied to the SSM output such that $y_t$ SiLU($z_t$) before the output projection.\nSSMs with selection. Gu and Dao (2023) improve SSMs with selection by letting their parameters B, C, and A be input-dependent, allowing the model to selectively remember or ignore inputs based on their content. Specifically, the interaction with input $x_t$ is defined as $B_t = F_B(x_t)$, $C_t = F_C(x_t)$, $A_t = softplus(F_\\Delta(x_t))$ where $F_B$ and $F_C$ are linear layers that map $x_t \\rightarrow B_t$, $C_t$. $F_\\Delta$ use two consecutive projection layers, such that $F_\\Delta = Proj(Proj(x)) + bias$. With the selection mechanism, the model has changed from time-invariant to time-varying."}, {"title": "3.2 Quantization", "content": "We focus on symmetric uniform quantization to approximate floating-point weights and activations with discrete 8-bit signed integers (i.e., INT8) due to its hardware compatibility. The general symmetric uniform quantization function is defined as\n$\\widehat{X} = s \\cdot clamp(\\frac{X}{s}, -2^{N-1}, 2^{N-1} \u2013 1)$, $s = \\frac{max(|X|)}{2^{N-1}-1}$, (2)\nwhere $\\widehat{X}$ represents the quantized weights or activations in INT8, X is the input matrix in floating point, and s is the scaling factor (i.e., quantization step) that is determined by the target bit-width N (N = 8 in our setting). The static scaling factor s is pre-calibrated on a subset of data and is fixed during inference. We use the notation $\\widehat{X}$ to represent the floating-point matrices, and X to represent their quantized matrices with their floating-point scaling factors $s_X$. For operators, we use $\\widehat{f}(\u00b7)$ to represent the quantized version of the function f(\u00b7) (i.e., the weights are quantized in the function $\\widehat{f}$)."}, {"title": "3.3 Walsh-Hadamard Transform", "content": "Hadamard matrices. A Hadamard matrix is an n-dimensional square matrix whose entries are either +1 or -1, and the rows and columns are mutually orthogonal with the computational property $HH^T=nI_n$. Walsh-Hadamard matrix is a special category of Hadamard matrix, consisting of square matrices of size $2^k$ and can be constructed as follows:\n$H_2= \\begin{bmatrix} 1 & 1\\\\ 1 & -1 \\end{bmatrix} , H_{2^k} = \\begin{bmatrix} H_{2^{k-1}} & H_{2^{k-1}}\\\\ H_{2^{k-1}} & -H_{2^{k-1}} \\end{bmatrix} = H_2 \\otimes H_{2^{k-1}}$ where $H_2 = \\begin{bmatrix} 1 & 1\\\\ 1 & -1 \\end{bmatrix}$.\nWalsh-Hadamard transform. The Walsh-Hadamard transform (WHT), a generalized class of Fourier transforms, has been applied to many related areas, such as LLM quantization (Ashkboos et al. 2024b) and efficient transfer learning (Yang et al. 2024), due to its favorable computational properties. We perform WHT to remove outliers from the output of the selective SSM. WHT projects a discrete input sequence signal onto a set of square waves (i.e., Walsh functions). Its forward and inverse transform can be expressed in matrix form as $x = H_n \\widehat{x}$, and $\\widehat{x} = H_n x$. x is the input discrete sequence signal, and $\\widehat{x}$ denotes the WHT coefficients (i.e., sequence components) that describe the magnitude of the square waves. WHT is efficient since the transform matrices consist only of real numbers, +1 or -1, so no multiplications are needed. The fast Walsh-Hadamard transform (FWHT) can compute the transformation with a complexity of nlogn in a GPU-friendly (i.e., parallelizable) fashion (Dao 2024b). For input dimension $n \\neq 2^k$, we factorize $n = 2^p m$, where m is the size of a known Hadamard matrix (Sloane 1999)."}, {"title": "4 Quamba: Quantizing Mamba Blocks", "content": ""}, {"title": "4.1 Preliminary Study", "content": "Theoretical error bound for SSM quantization. For a comprehensive analysis of our proposed Mamba block quantization approach, we derive a theoretical error bound of a discrete 1D linear time-invariant (LTI) SSM. The details of the proof can be found in Section A in Appendix.\nTheorem 4.1. The quantization error of h[t] from each time step of the given discrete 1D linear time-invariant model is bounded by $b\\epsilon \\frac{e^{t-T}}{e-1}$ such that |h[t] \u2013 h[t]| \u2264 $b\\epsilon \\frac{e^{t-T}}{e-1}$.\nEmpirical analysis of SSM quantization. As shown in Figure 2 and Figure 3, the main challenge in quantizing Mamba blocks is the precision of the SSM input activation x, which is sensitive to quantization errors, and the output activation y. In Figure 1 (a), the naive 8-bit quantization introduces large quantization errors, resulting in model collapse for all model sizes. We delve into the causal relationship between (A, B, C, A, x) and y as modeled by the linear recurrent system in Equation 1. As shown in Figure 2 (b) and Figure 3 (a), we find that x is sensitive to quantization errors and it leads to the largest errors in the SSM output y, although the values in the x tensor are numerically small (ref: Figure 12). We conjecture the reason behind the phenomenon is that (B, C, \u2206) are input-dependent (i.e., x-dependent). We note that this finding is specific to SSMs. As shown in Figure 2 (a), self-attention layers are more resilient to quantization errors and do not experience the same issues.\nSSM outliers. Our study shows SSMs exhibit distinct outlier patterns compared to Transformers (Ashkboos et al. 2024b; Dettmers et al. 2022; Xiao et al. 2023; Zhao et al. 2023). We show that outliers appear in the SSMs output (i.e. the y tensor), which perform a similar token-mixing function to self-attention layers. In contrast, the input and output of self-attention layers are relatively smooth and do not exhibit outlier issues. More comparisons can be found in Section I. This highlights that different quantization methods are"}, {"title": "4.2 Quantization for Selective SSM", "content": "We aim to quantize the weight (A, D) to 8 bits, and the activations ($B_t, C_t, \\Delta_t, x_t$) to 8 bits for selective SSMs. The quantized selective SSM takes 8-bit weights and activations as input, as well as their scaling factors, and outputs half precision $y_t$ such that $y_t = SSM(\\widehat{A}, \\widehat{B_t}, \\widehat{C_t}, \\widehat{D}, \\widehat{\\Delta_t}, \\widehat{x_t}, s_{in})$. ($\\widehat{A}, \\widehat{B_t}, \\widehat{C_t}, \\widehat{D}, \\widehat{\\Delta_t}, \\widehat{x_t}$) are the quantized weights and activations, and their scaling factor $s_{in}$ in floating point. ($\\widehat{B_t}, \\widehat{C_t}, \\widehat{\\Delta_t}$) depend on the input $\\widehat{x_t}$ to perform selection mechanism as $\\widehat{B_t} = \\widehat{F_B}(x_t)$, $\\widehat{C_t} = \\widehat{F_C}(x_t)$, $\\widehat{\\Delta_t} = softplus(\\widehat{F_\\Delta}(x_t))$. The weights and biases in linear layers $\\widehat{F_B}, \\widehat{F_C}$, and $\\widehat{F_\\Delta}$ are also quantized to 8-bit integers. For simplicity, we omit the residual branch $z_l$ in the discussion.\nSSM inputs. Our findings show that x is highly sensitive to quantization errors, which results in the largest errors in the SSM output y. Specifically, we found the quantization error of x is dominated by outliers during the calibration. Although they are numerically small (\u2264 10), as shown in Figure 12, the small amounts of outliers (\u2264 0.1%) increase the quantization step (i.e., scaling factors s in Eq.2) and reduce the quantization precision for x. Clipping the values with a percentile max (Li et al. 2019; Zhao, Dong, and Keutzer 2022) is a simple solution to restrict the activation range and has no additional overhead during inference. For example, using the 99th percentile to clip the top 1% of the highest activation values prevents the activation range from being skewed by extreme outliers. We use percentile max to calculate the scaling factor for x: $s_x = (max^p (|x|))/(2^{N-1} \u2013 1)$, where p is a parameter for percentiles. In our experiments, we found p = 99.999 works well for Quamba.\nSSM outputs. We perform WHT to remove the outliers from the SSM output. The output y is transformed to an outlier-free space using a Hadamard matrix such that $\\widehat{y_H} = H_n\\widehat{y}$ where n is the token dimension of y and the dimension of the squared Hadamard matrix H. We fuse the inverse Hadamard matrix into the output linear projection $\\widehat{W_{H_o}} = H_n\\widehat{W_{out}}$ to avoid additional computation overhead and achieve compute-invariance (Ashkboos et al. 2024a,b) (i.e., the same output) by $Mamba_{output} = \\widehat{W_{out}}\\widehat{y} = \\widehat{W_{out}}(H^TH)y = (\\widehat{W_{H_o}})^T \\widehat{y_H}$. In the calibration stage, we collect the quantization scaling factor for $\\widehat{y_H}$ (i.e., transformed y). Therefore, the fused Hadamard quantization layer can be expressed as\n$\\widehat{y} = clamp(\\frac{\\widehat{y_H}}{s_y}, -2^{N-1},2^{N-1} \u2013 1) \\frac{max(|y_H^h|)}{s_y}$  (3)\nwhere N represents the target bit-width. We fuse the scaling factor $s_y$ in the forward Hadamard transform such that $\\frac{\\widehat{y}}{s_y} = H_n\\frac{\\widehat{y}}{s_y}$, so the quantization does not incur additional computational overhead. The fused Hadamard quantization layer is parallelizable and efficient on GPU with a complexity of nlogn (Dao 2024b)."}, {"title": "4.3 Other Operators", "content": "Projection layers. Projection layers, which perform dense matrix multiplications, are the most over-parameterized and the most compute-intensive operators in the models. With quantized activations and weights, projection layers benefit from hardware acceleration (e.g., Tensor Cores) for 8-bit integers as well as reducing the memory costs by half. We implement the 8-bit linear layers with commercial libraries, except for the output projection, which produces half-precision outputs for the subsequent normalization layer.\nFused causal convolution. Causal convolution applies a $w \\times c$ weight to perform the calculation in a depthwise convolution fashion only using tokens from previous time steps t \u2013 w to the current time step t, each of which is a c dimensional vector. The operator is memory-bound, as typical depthwise convolution (Lu, Zhang, and Wang 2021; Zhang, Lo, and Lu 2020), so applying quantization to the input and output activations and weights largely reduces memory pressure. We quantize the inputs and weights as 8-bit integers and fuse the quantization operator before writing the result to memory. The causal convolution operation is $\\widehat{X_{out}} = \\frac{1}{s_{out}} (Conv(\\widehat{X_{in}}, \\widehat{W}, s))$, $s = s_W s_{X_{in}}$. The SiLU (Elfwing, Uchibe, and Doya 2018) \u03c3 is fused into the convolution, as described by Gu and Dao (2023)."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Model and datasets. We evaluate Quamba on the open-sourced Mamba family of SSMs (Gu and Dao 2023) and on Jamba (Lieber et al. 2024a), a hybrid architecture composed of self-attention, SSMs, and Mixture of Experts (MoE). For zero-shot tasks, we use LM-EVAL (Gao et al. 2023), on which we evaluate baselines and Quamba on LAMBADA (Paperno et al. 2016), HellaSwag (Zellers et al. n.d.), PIQA (Bisk et al. 2020), ARC (Clark et al. 2018) and WinoGrande (Sakaguchi et al. 2020). Model accuracy on each dataset and the average accuracy are reported. We follow the evaluate protocol with Mamba Gu and Dao 2023, and report the accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for HellaSwag and ARC-challenge. For perplexity, we evaluate the models using the testing set of WikiText2 (Merity et al. 2016) and a randomly sampled subset from validation set of Pile dataset (Gao et al. 2021).\nQuantization setup. The calibration set is constructed by randomly sampling 512 sentences from the Pile dataset (Gao et al. 2021). We collect the static scaling factors for each operator based on the absolute maximum value observed from the calibration set and apply symmetric per-tensor quantization for weights and activations, except for the input to the SSM, where we use the 99.999th percentile (i.e., the p described in Section 4.2) to clip the maximum values. The same scaling factors are applied in all our experiments. We note that our method does not require extra training efforts and can be plug-and-play.\nImplementation. We implement the INT8 linear layer using CUTLASS library (Thakkar et al. 2023). Quantization is integrated and adapted into the CUDA kernels of both the fast Hadamard transform (Dao 2024b) and causal convolution (Dao 2024a). Additionally, the selective SSM CUDA kernel (Gu and Dao 2023) is modified to accommodate inputs with quantized weights and activations, and their scaling factors. We profile the latency and memory usage of baselines and Quamba on A5000 GPUs and Nvidia Orin Nano 8G. For the latency, we perform a few warm-up iterations and then report the average latency of the following 100 iterations.\nBaselines. In our W8A8 setting, we compare Quamba with static quantization, dynamic quantization, and Mamba-PTQ (Pierro and Abreu 2024). We re-implement the state-of-the-art W8A8 SmoothQuant (SmQ) (Xiao et al. 2023) and W4A4 QuaRot (Ashkboos et al. 2024b) in Transformers for 8-bit weight-activation SSM quantization as additional baselines. These are denoted by SmQ-SSM and QuaRot-SSM, respectively. Since QuaRot fails to quantize SSMs with W4A4 precision, we re-implement QuaRot to support W8A8 to fit our setting. We include the details of our QuaRot re-implementation in Section C. For the re-implemented SmoothQuant (Xiao et al. 2023), we apply a smoothing factor \u03b1 = 0.5 to all linear layers within Mamba SSM. We find the low bit-width quantization methods for Transformers do not generalize well to SSMs and quantizing SSMs with low-bit-width remains unexplored, we include some of the results in Section E."}, {"title": "5.2 Model size and Latency", "content": "Quamba reduces the size of the 2.8B model nearly by half (5.29 GB vs. 2.76 GB) by quantizing weights as 8-bit integers except for normalization layers, as shown in the Table 1. We profile the latency on an A5000 GPU and an Orin Nano 8G for cloud and edge deployment scenarios. Quamba enjoys the 8-bit acceleration and improves the latency by 1.27\u00d7 with a 512 input context (i.e., time-to-first-token, TTFT) on the A5000 GPU and 1.21\u00d7 in the generation stage (i.e., L=1, time-per-output-token, TPOT). On the Orin Nano, Quamba improves the latency by 1.2\u00d7 with a 512 input context and 1.72x in the generation stage. Figure 9 shows the snapshot of real-time generation on Nano 8G. Despite that having similar accuracy to the re-implemented QuaRot (Ashkboos et al. 2024b) dubbed as QuaRot-SSM, Quamba delivers a better speedup on both A5000 and Nano, since QuaRot-SSM requires extra transpose and Hadamard transforms to handle the SSM input"}, {"title": "5.3 Perplexity Evaluation", "content": "Table 2 presents the perplexity results of Quamba and the baseline methods on the Mamba family of SSMs. Static quantization fails to maintain the precision in SSM quantization, resulting in significant performance degradation (i.e., increased perplexity, where lower is better). Even with scaling factors calculated dynamically, which introduces significant computational overhead during inference, it still results in a considerable increase in perplexity (+7.5) on Mamba-2.8B. Although SmQ-SSM mitigates the issue of outliers in the output of the SSM, a performance gap remains when compared to the non-quantized Mamba because it fails to address the issue of quantizing sensitive x input tensors to SSMs. Quamba achieves similar perplexity to the re-implemented QuaRot-SSM (Ashkboos et al. 2024b) but delivers a better speedup on both A5000 and Nano, as shown in Table 1 and Figure 1 (a). Since QuaRot-SSM is not optimized for SSMs, it requires extra"}, {"title": "5.4 Zero-shot Evaluation", "content": "We evaluate Quamba and other methods on six common-sense tasks in a zero-shot fashion. The accuracy of each task and the average accuracy across the tasks are reported in Table 3. Quamba 2.8B has only a 0.9% accuracy drop compared to floating-point Mamba 2.8B and outperforms Mamba-PTQ (Pierro and Abreu 2024) and SmQ-SSM (Xiao et al. 2023) in accuracy. Quamba achieves similar accuracy to the re-implemented QuaRot-SSM (Ashkboos et al. 2024b) but achieves a better trade-off between accuracy and latency, as shown in Figure 1 (a)."}, {"title": "5.5 Quantizing Jamba: A Large-Scale Hybrid Mamba-Transformer LLM", "content": "Jamba (Lieber et al. 2024b) is a hybrid transformer-mamba language model with 52B parameters, built with self-attention, Mixture of Experts (MoEs), and Mamba blocks, making it the first large-scale Mamba-style model with a number of parameters comparable to Mixtral (Jiang et al. 2024). In Table 4, we compare the LAMBADA OpenAI accuracy of Jamba's FP16 inference by combining off-the-shelf quantization methods with different quantization strategies. Applying LLM.int8 (Dettmers et al. 2022) to self-attention and MoE preserves the model's accuracy, whereas jointly quantizing Mamba with LLM. int8 (Dettmers et al. 2022) degrades the model and fails to produce meaningful accuracy. In contrast, we combine Quamba with LLM. int8 (Dettmers et al. 2022) and as a result we achieve competitive accuracy (1.1% accuracy drop) with a lower model footprint than FP16."}, {"title": "6 Ablation study", "content": ""}, {"title": "6.1 Quamba Ablation", "content": "In Table 5, We conduct a performance analysis on each component in Quamba and report the average accuracy across six zero-shot datasets. Naive W8A8 quantization results in significant performance discrepancies across all sizes of models. We improve the performance of quantized models by constraining the quantization range of the SSM input x using percentile clipping (+ In Per.). While addressing the large outlier in the SSM output using the Hadamard transform improves performance (+ Out Had.), the results remain unsatisfactory. Quamba integrates two techniques, thereby closing the performance gaps across all model sizes."}, {"title": "6.2 Percentile-based Activation Clamping", "content": "In Table 6, we conduct a sensitivity analysis on the percentile maximum clipping for the input x to SSM. We test different percentiles (i.e., the p described in 4.2) and report the accuracy on LAMBADA dataset. The table shows more outliers in the larger models, while smaller amounts of outliers are clipped in the smaller models. Therefore, clipping 0.001% (i.e., p = 99.999) of outliers in the model with 130m parameters produces best performance. In contrast, for the model with 2.8b parameters, clipping 0.1% (i.e., p = 99.9) performs best on the LAMBADA dataset (Paperno et al. 2016)."}, {"title": "7 Conclusion", "content": "We investigate quantization methods for selective State Space Models and propose Quamba, a methodology for successfully quantizing the weight and activations as 8-bit signed integers tailored for the Mamba family of SSMs. Our experiments show that Quamba maintains the original FP16 when accuracy compared with state-of-the-art counterparts, including current techniques for Transformers. The profiling results on a wide variety of platforms show that the low bit-width representation of Quamba not only enables deployment to resource-constrained devices, such as edge GPUs, but also benefits from hardware acceleration with reduced latency. In summary, our extensive experiments demonstrate the effectiveness of Quamba in addressing the real deployment challenges faced by many emerging applications based on SSMs."}, {"title": "A Quantization Error Analysis for SSMs", "content": "We show the error bound of a discrete 1D linear time-invariant (LTI) SSM and the empirical experiments on the quantization errors of the discretized high-dimensional SSMs."}, {"title": "A.1 Theoretical Error Analysis", "content": "We consider a discrete 1D linear time-invariant (LTI) state space model such that h[t", "1": "bx[t", "dx[t": "such that x[t", "x[t": "dx[t"}, {"dx[t": "\u0454. The system is initialized to 0 such that h[0", "h[0": 0.0, "h[t": "from each time step of the given discrete 1D linear time-invariant model is bounded by $b\u03f5 \\frac{e^{t\u2212T}}{e\u22121}$ such that |h[t"}, {"h[t": "b\u03f5 \\frac{e^{t\u2212T}}{e\u22121}$.\nProof. Given the quantization error |dx[t", "x[t": "x[t", "dx[t": "of each step, we have an original system h[t"}, {"h[t": "nh[t", "1": "bx[t"}, {"1": "bx[t", "nh[t": "a(T, t)h[t \u2013 1", "bx[t": "e^{t\u2212T}$h[t \u2013 1"}, {"bx[t": "e^{t\u2212T}$h[t \u2013 1", "b(x[t": "dx[t", "h[t": "h[t", "e^{t\u2212T}$(h[t-1": "h[t-1", "bdx[t": "."}, {"h[t": "h[t", "t": "to simplify our notation, we have\nw[t", "1": "bdx[t", "n|@[t": "e^{t\u2212T}$w[t \u2212 1", "bdx[t": "n|w[t"}, {"1": "bdx[t", "inequality)\n|w[t": "e^{t\u2212T}$w[t \u2212 1", "be\n|w[t": "e^{t-T}$|w[t \u2212 1"}, {"1": "\u03a9[t \u2212 1", "\u03a9[t": "e^{t-1}$\u03a9[t \u2212 1", "\u03a9[0": "\u03c9[0", "h[0": "h[0", "\u03a9[1": "e^{\u00b9\u2212\u03a4}$\u03a9[0", "\u03a9[2": "e^{\u00b2\u2212\u03a4}$\u03a9[1"}]}