{"title": "CONTENT-STYLE LEARNING FROM UNALIGNED DOMAINS: IDENTIFIABILITY UNDER UNKNOWN LATENT DIMENSIONS", "authors": ["Sagar Shrestha", "Xiao Fu"], "abstract": "Understanding identifiability of latent content and style variables from unaligned multi-domain data is essential for tasks such as domain translation and data generation. Existing works on content-style identification were often developed under somewhat stringent conditions, e.g., that all latent components are mutually independent and that the dimensions of the content and style variables are known. We introduce a new analytical framework via cross-domain latent distribution matching (LDM), which establishes content-style identifiability under substantially more relaxed conditions. Specifically, we show that restrictive assumptions such as component-wise independence of the latent variables can be removed. Most notably, we prove that prior knowledge of the content and style dimensions is not necessary for ensuring identifiability, if sparsity constraints are properly imposed onto the learned latent representations. Bypassing the knowledge of the exact latent dimension has been a longstanding aspiration in unsupervised representation learning our analysis is the first to underpin its theoretical and practical viability. On the implementation side, we recast the LDM formulation into a regularized multi-domain GAN loss with coupled latent variables. We show that the reformulation is equivalent to LDM under mild conditions\u2014yet requiring considerably less computational resource. Experiments corroborate with our theoretical claims.", "sections": [{"title": "1 INTRODUCTION", "content": "In multi-domain learning, \u201cdomains\" are typically characterized by a distinct \"style\" that sets their data apart from others (Choi et al., 2020). Take handwritten digits as an example: writing styles of different persons can define different domains. Shared information across all domains, such as the identities of the digits in this case, is termed as \"content\". Learning content and style representations from multi-domain data facilitates many important applications, e.g., domain translation (Huang et al., 2018), image synthesis (Choi et al., 2020), and self-supervised representation learning (Von K\u00fcgelgen et al., 2021; Lyu et al., 2022; Daunhawer et al., 2023); see more in Huang et al. (2018); Lee et al. (2020); Choi et al. (2020); Wang et al. (2016); Yang et al. (2020); Wu et al. (2019).\nRecent advances showed that understanding the identfiability of the latent content and style components from multi-domain data allows to design more reliable, predicable, and trustworthy learning systems (Hyvarinen et al., 2019; Lyu et al., 2022; Xie et al., 2023; Kong et al., 2022; Shrestha & Fu, 2024; Gresele et al., 2020; Gulrajani & Hashimoto, 2022). A number of works studied content/style identifiability when the multi-domain data have sample-to-sample cross-domain alignment according to shared contents. Specifically, identifiability was established for sample-aligned multi-domain settings under the assumption that multi-domain data are linear and nonlinear mixtures of latent content and style components, in the context of canonical correlation analysis (CCA), multiview analysis and self-supervised learning (SSL); see Ibrahim et al. (2021); S\u00f8rensen et al. (2021); Wang & Isola (2020); Von K\u00fcgelgen et al. (2021); Lyu et al. (2022); Karakasis & Sidiropoulos (2023); Daunhawer et al. (2023)"}, {"title": "2 BACKGROUND", "content": "Content-Style Modeling in Multi-Domain Analysis. Consider the case where the data are acquired over N domains $X^{(n)} \\subseteq \\mathbb{R}^{d}$, where n = 1, . . ., N. We assume that any sample from domain n can be represented as a function (or, a nonlinear mixture) of content and style components, i.e.,\n$c\\sim P_c, s^{(n)} \\sim P_s^{(n)}, x^{(n)} = g(c, s^{(n)})$,                                                                                                      (1)\nwhere $P_{s^{(n)}}$ and $P_c$ are distributions of the style components in nth domain and the content compo-nents, respectively. Let $C \\subseteq \\mathbb{R}^{d_c}$ and $S^{(n)} \\subseteq \\mathbb{R}^{d_s}$ be the open set supports of $P_c$ and $P_s^{(n)}$. Then, we define $X^{(\\eta)} = \\{g(c, s^{(n)})|c, s^{(n)} \\in C \\times S^{(n)}\\} \\subseteq \\mathbb{R}^d$ as the support of $x^{(n)} \\sim P_x^{(n)}$. Let $X = \\cup_{n=1}^{N}X^{(n)} \\subseteq \\mathbb{R}^d$ and $S = \\cup_{n=1}^{N}S^{(n)} \\subseteq {\\mathbb{R}}^{d_s}$ represent the whole data space and the whole style space, respectively. We assume that the nonlinear function $g : C \\times S \\rightarrow X$ is a differentiable bijective function. Note that although $X \\subseteq \\mathbb{R}^d$ and d might be greater than $d_s + d_c$, the bijective property can hold as X resides within a low dimensional manifold (Von K\u00fcgelgen et al., 2021).\nThe model in (1) is widely adopted (explicitly or implicitly) in multi-domain analysis; see examples from Huang et al. (2018); Lee et al. (2020); Choi et al. (2020); Wang et al. (2016); Yang et al. (2020); Wu et al. (2019). This model makes sense when the \u201cdomains\" are participated using distinguishable semantic meaning; e.g., in Fig. 1, \"style\" includes the writing manners (handwritten/printed) and display background colors (black/gray). Under the model in (1), learning g (and its inverse f) as well as the latent components c and $s^{(n)}$ is the key to facilitate a number of important applications.\nApplication: Cross-Domain Translation. Learning content and style components from a sample in the source domain $(c_i, s_i^{(s)}) = f(x_i^{(s)})$ and a sample from the target domain $(c_i, s_i^{(t)}) = f(x_i^{(t)})$ can assist translate $x_i^{(s)}$ to its corresponding representation in the target domain. This can be realized by generating a new sample $x_i^{(st)} = g(c_i, s_i^{(t)})$; see Fig. 1 for illustration and Lyu et al. (2022); Huang et al. (2018); Wang et al. (2016).\nApplication: Data Generation. If c and $s^{(n)}$ can be learned from the samples, then one can also learn the distributions $P_c$ and $P_s^{(n)}$ using off-the-shelf distribution learning tools, e.g., GAN (Goodfellow et al., 2014). This way, one can draw samples from the distributions, i.e., $c_{new} \\sim P_c, s_{new}^{(n)} \\sim P_{S^{(n)}}$ and generate new samples $x_{new}^{(n)} = g(c_{new}, s_{new}^{(n)})$ with intended styles.\nOther Applications. We should mention that the content-style modeling is also a critical perspective for understanding representation learning paradigms, e.g., the SSL frameworks (Von K\u00fcgelgen et al., 2021; Lyu et al., 2022; Daunhawer et al., 2023; Wang & Isola, 2020).\nContent-Style Identifiability. In recent years, the identifiability of $f, c$ and $s^{(n)}$ started drawing attention, due to its usefulness in building more reliable/predictable systems.\nAligned Domains: Results from Self-Supervised Learning (SSL). The works (Von K\u00fcgelgen et al., 2021; Daunhawer et al., 2023; Lyu et al., 2022; Karakasis & Sidiropoulos, 2023) studied content identifiability in the context of representation learning, in particular, SSL and multiview learning. It was shown that when N = 2, if content-shared pairs $\\{x^{(1)}, x^{(2)}\\}$ are available, then enforcing $f(x^{(1)}) = f(x^{(2)}), \\forall$ content-shared pairs $(x^{(1)}, x^{(2)})$ can provably learn $c$, under reasonable conditions. The learning criterion can be realized by various loss functions, e.g., Euclidean fitting-based (Lyu et al., 2022; Karakasis & Sidiropoulos, 2023) and contrastive loss-based (Von K\u00fcgelgen et al., 2021; Daunhawer et al., 2023) criteria. The identifiability of the style components was also considered under similar aligned domain settings; see (Lyu et al., 2022; Eastwood et al., 2023).\nUnaligned Domains: Progresses and Challenges. Aligned samples are readily available in applications such as data-augmented SSL (Von K\u00fcgelgen et al., 2021; Daunhawer et al., 2023; Lyu et al., 2022). However, in other applications such as image style translation and image generation, aligned samples are hard to acquire (Zhu et al., 2017). For unaligned multi-domain data, the identfiiability"}, {"title": "3 MAIN RESULT", "content": "In this work, we revisit content-style learning problem from a latent distribution matching (LDM) viewpoint. Consider the following assumption:\nAssumption 3.1 (Block Independence). The block variables c and $\\{s^{(n)}\\}_{n=1}^{N}$ are statistically independent, i.e., $p(c, s^{(1)}, ..., s^{(N)}) = p_c(c) \\prod_{n=1}^{N} p_{s^{(n)}}(s^{(n)})$.\nThe assumption was used in various multi-domain models (Lyu et al., 2022; Eastwood et al., 2023; Wang et al., 2016; Choi et al., 2020; Timilsina et al., 2024). It makes sense when the styles can be combined with contents in an \u201carbitrary\u201d way without affecting the contents (e.g., the writing style of digits can change freely without affecting the identity of the digits). Next, we will use this assumption to build our learning criterion.\nWe propose the following learning criterion:\nfind $f : X \\rightarrow \\mathbb{R}^{d_c+d_s}$ injective\ns.t. $[f_c]#P_{x^{(i)}} = [f_c]#P_{x^{(j)}}, i \\neq j,\\forall i, j \\in [N]$, (distribution matching)                                  (2a)\n$[f_s]#P_{x^{(n)}} \\amalg [f_c]#P_{x^{(n)}}, \\forall n \\in [N]$, (block-indep. enforcing)                       (2b)\nwhere $f_c(x^{(n)}) \\in \\mathbb{R}^{d_c}$ represents the first $d_c$ outputs of f that are designated to represent the content components, $f_s(x^{(n)}) \\in \\mathbb{R}^{d_s}$ represents the learned style from domain n, Eq. (2a) matches the distributions of $f_c(x^{(i)})$ and $f_c(x^{(i)})$\u2014i.e., the learned contents from domains i and j, respectively\u2014and Eq. (2b) imposes a block independence constraint on the learned content $f_c(x^{(n)})$ and style $f_s(x^{(n)})$ from each domain following Assumption (3.1)."}, {"title": "3.1 WARM-UP: ENHANCED IDENTIFIABILITY WITH KNOWN LATENT DIMENSIONS", "content": "We first show that the content-style identifiability under (1) and known $d_c$ and $d_s$ can be substantially enhanced relative to existing works. We will remove the need for the dimension knowledge in the next subsection. To establish identifiability via solving (2), we make the following assumption:\nAssumption 3.2 (Domain Variability). Let $A \\subseteq Z := C \\times S$ be any measurable set that satisfies (i) $P_{Z^{(n)}}[A] > 0$ for any $n \\in [N]$ and (ii) A cannot be expressed as $B \\times S$ for any set $B \\subset C. Then, there exists a pair of $i_a, j_a \\in [N]$ such that the following holds:\n$P_{Z^{(i_a)}}[A] \\neq P_{Z^{(j_a)}}[A]$, .(3)\nNote that for any A, we only need one pair of $(i_a, j_a)$ to satisfy the condition, and the pair can change over different A's. Essentially, Eq. (3) requires that the styles have sufficiently diverse distributions. This assumption is a standard characterization for the distributional diversity of the domains in the literature; see Xie et al. (2023); Kong et al. (2022) and its variant Timilsina et al. (2024)."}, {"title": "3.2 MAIN RESULT: IDENTIFIABILITY WITHOUT DIMENSION KNOWLEDGE", "content": "Theorem 3.3 still uses the knowledge of $d_c$ and $d_s$. In this subsection, we propose a modifed learning criterion that does not use the exact dimension information. To proceed, let $\\bar{d}_c$ and $\\bar{d}_s$ denote the user-specified latent dimensions for f, i.e., $f : X \\rightarrow \\mathbb{R}^{\\bar{d}_c+\\bar{d}_s}, f_c : X \\rightarrow \\mathbb{R}^{\\bar{d}_c}$ and $f_s : X \\rightarrow \\mathbb{R}^{\\bar{d}_s}$. Note that these dimensions need not to be exact. We consider the following learning criterion:\n$\\underset{f: injective}{minimize} \\sum_{n=1}^{N} [|f_s (x^{(n)})||_o]$ (4a)\nsubject to $[f_c]#P_{x^{(i)}} = [f_c]#P_{x^{(j)}}, \\forall i, j \\in [N]$,(4b)\n$[f_s]#P_{x^{(n)}} \\amalg [f_c]#P_{x^{(n)}}, \\forall n \\in [N]$,(4c)\nProblem (4) minimizes the \u201ceffective dimension\" of the extracted style component, while satisfying the distribution matching and independence constraints. The idea is to use excessive $d_c$ and $d_s$ so that one has enough dimensions to represent the content and style information. Note that trivial solutions could occur when using over-estimated $d_c$ and $d_s$. For instance, when $f_c$ is a constant function, $f_s$ can still be an injective function of $x^{(n)}$ given large enough $\\bar{d}_s$. This pathological solution satisfies both constraints (4b) and (4c). We use the sparsity objective in (4a) to \u201csqueeze out\u201d the redundant dimensions in $f_s$. This prevents the content information from \u201cleaking\u201d into the learned $f_s$. We formalize this intuition in the following theorem:\nTheorem 3.4 (Identifiability without Dimension Knowledge). Assume that the conditions in Theorem 3.3 hold. Let f represent a solution of Problem (4) and f is differentiable. Assume the following conditions hold: (a) $d_c \\geq d_c$ and $d_s > d_s$. (b) $0 < P_z^{(n)} (z) < \\infty, \\forall z \\in Z = C \\times S,\\forall n \\in [N]$. Then, there exists injective functions $\\gamma : C \\rightarrow \\mathbb{R}^{\\bar{d}_c}$ and $\\delta : S \\rightarrow \\mathbb{R}^{\\bar{d}_s},\\forall n \\in [N]$ such that $f_c(x^{(n)}) = \\gamma(c)$ and $f_s(x^{(n)}) = \\delta(s^{(n)}), \\forall n \\in [N]$.\nTheorem 3.4 means that using (4), there is no need to know $d_s$ or $d_c$ in advance. The identifiability result has significant practical implications for content-style identification, where the latent dimension in practice is always hard to acquire."}, {"title": "4 IMPLEMENTATION: SPARSITY-REGULARIZED MULTI-DOMAIN GAN", "content": "At first glance, a conceptually straightforward realization of the learning criterion in (2) could take the following form:\n$\\underset{f: injective}{minimize} \\sum_{i=1}^{N}\\sum_{j>i}^{N} LDM(f_c(x^{(i)}), f_c(x^{(j)})) + \\sum_{i=1}^{N} L_{indep} (f_c(x^{(i)}), f_s(x^{(i)}))$, (5)\nwhere the first term and the second term promotes the distribution matching (DM) constraint (2a) and the independence constraint (2b), respectively. This type of implementation is potentially viable but can be costly: Both the LDM modules and the block independence regularization on the learned components often needs rather nontrivial optimization (see (Lyu et al., 2022)). Enforcing f to be injective also needs extra regularization, e.g., autoencoder type regularization (Lyu et al., 2022; Zhu et al., 2017) and entropy-type regularization (Von K\u00fcgelgen et al., 2021; Daunhawer et al., 2023).\nInstead of using the straightforward implementation as in (5), we propose a unified formulation for (2) and (4) using GAN learning:\n$\\underset{q, e_c, e_s}{min} \\, \\underset{d^{(n)}}{max}\\sum_{n=1}^{N} [log \\,\\, d^{(n)} (x^{(n)}) + log \\, \\,\\,(1 - d^{(n)}(q \\,\\,(e_c(r_c), e_s^{(n)}(r_s^{(n)}))))]$, (6a)\nsubject to $E[(e_s^{(n)}(r_s^{(n)}))$ has minimal $||e_s^{(n)}(r_s^{(n)}))||_o, \\, \\forall r_s^{(n)}]$. (6b)\nThe above approximates (2) and (4) when the constraint (6b) is absent and active, respectively. In practice, the sparsity constraint can be approximated using sparsity regularization terms (e.g., $\\ell_1$ norm) easily. Denote $d_c$ and $d_s$ are the estimates of $d_c$ and $d_s$, respectively. The idea is to learn invertible nonlinear mappings $e_c$ and $e_s^{(n)}$ that transform independent Gaussian variables (i.e., $r_c$ and $r_s^{(n)}$) to represent content c and style $s^{(n)}$, respectively. Generate $r_c \\sim \\mathcal{N}(0, I_{\\bar{d}_c})$ and construct an invertible $e_c$ such that $e_c(r_c) \\in \\mathbb{R}^{\\bar{d}_c}$. Similarly, construct invertible $e_s^{(n)}$ such that $e_s^{(n)}(r_s^{(n)}) \\in \\mathbb{R}^{\\bar{d}_s}$ with $r_s^{(n)} \\sim \\mathcal{N}(0, I_{\\bar{d}_s})$. Then, the content and style are mixed by q to match the distribution of $x^{(n)}$ using a logistic loss (i.e., GAN-type DM). In other words, the formulation looks for $e_c, e_s^{(n)}$ and q such that $P_{x^{(n)}} = P_{q^{(n)}}, P_{q^{(n)}} = q(e_c(r_c), e_s^{(n)}(r_s^{(n)})), \\, \\forall n \\in [N]$. This way, instead of directly learning f, we learn the generative process g using q. Our next theorem shows that q is indeed the inverse of f (up to some ambiguities).\nTo proceed, denote $\\hat{C}$ and $\\hat{S}^{(n)}$ as the sets representing the range of $\\hat{e}_c$ and $e_s^{(n)}$, respectively. Then, the effective domain of $\\hat{q}$is $\\hat{C} \\times \\hat{S}$ where $\\hat{S} = \\cup_n \\hat{S}^{(n)}$. We show that:\nTheorem 4.1. Let $(q, e_c, e_s^{(n)}, d^{(n)})$ be any differentiable optimal solution of (6). Let C and S be simply connected open sets. Let $0 < P_{Z^{(n)}}(z) < \\infty, \\forall z \\in Z = C \\times S$. Under the assumptions in Theorem 3.3, we have the following:\n(a) If $d_c = d_c$ and $d_s = d_s$ and (6b) is absent, then $\\hat{q} : \\hat{C} \\times \\hat{S} \\rightarrow X$ is bijective and $f = \\hat{q}^{-1}$ is also a solution of (2).\n(b) If $d_c > d_c$ and $d_s > d_s$ and $\\hat{q} : \\hat{C} \\times \\hat{S} \\rightarrow X$ is bijective, then $f = \\hat{q}^{-1}$ is also a solution of (4).\nThe formulation in (6) has a number of practical advantages over the direct implementation in (5). Particularly, it avoids complex operations in the latent domain. In LDM, performing DM on $f_c(x^{(i)})$ and $f_c(x^{(i)})$ poses quite a nontrivial optimization process. This is because both of the inputs to the DM modules (i.e., $f_c(x^{(i)})$ for all $i \\in [N]$) change from iteration to iteration\u2014yet the DM module (e.g., GAN and Wasserstein distance-based DM (Goodfellow et al., 2014; Arjovsky et al., 2017)) itself often involves complex optimization with its own parameters updated on the fly. The new formulation performs GAN-based DM in the data domain and keeps one input (the real data) to every GAN module fixed. This reduces a lot of agony in optimization parameter tuning. Eq. (6) also does not need any explicit constraint/regularization to enforce the block independence of $f_e$ and $f_s$ (which could be resource consuming (Lyu et al., 2022; Gretton et al., 2007)), as $e_c$ and $e_s^{(n)}$ are constructed to be block independent."}, {"title": "5 RELATED WORKS", "content": "Nonlinear ICA. Learning content and style components from a nonlinear mixture model is reminiscent of nonlinear independent analysis (nICA) (Hyv\u00e4rinen & Pajunen, 1999; Hyvarinen & Morioka, 2017; Hyvarinen et al., 2019). Most nICA works were developed under single domain settings, with some recent generalizations to multiple views/domains (Gresele et al., 2020; Hyvarinen et al., 2019). Nonetheless, nICA requires that all the latent variables are (conditionally) independent. This is considered a somewhat restrictive assumption in content-style learning.\nContent-Style Models in Aligned Multi-Domain Learning. Aligned multi-domain content-style learning is a key technique in data-augmented SSL and representation learning. There, it was shown that elementwise (conditional) independence is not needed, if the goal is to isolate content from style (Von K\u00fcgelgen et al., 2021; Lyu et al., 2022; Karakasis & Sidiropoulos, 2023). It was further shown that block independence (similar to Assumption 3.1) is the key to identify the style (Lyu et al., 2022; Daunhawer et al., 2023). However, all these works require cross-domain data alignment.\nContent-Style Identification in Unaligned Multi-Domain Learning. Identifiability of unaligned multi-domain learning was studied in the context of various applications, e.g., image translation (Shrestha & Fu, 2024), data synthesis (Xie et al., 2023), cross-domain information retrieval Timilsina et al. (2024), and domain adaptation (Kong et al., 2022; Gulrajani & Hashimoto, 2022; Timilsina et al., 2024). The work (Kong et al., 2022) postulated a similar content-style model as in (Xie et al., 2023) and came up with identifiability conditions similar to those in (Xie et al., 2023). The mostly related work to ours is (Xie et al., 2023), as both works are interested in content-style identification under (1). Our implementation in (6) partially recovers the marginal distribution matching criterion in (Xie et al., 2023), despite the fact that our learning criteria started with an LDM perspective. Nonetheless, our method enjoys much less restrictive model assumptions for content-style identifiability. Our multi-domain GAN also admits more relaxed neural architecture (see Appendix G).\nContent-Style Learning without Knowing Latent Dimensions. The SSL work (Von K\u00fcgelgen et al., 2021) presented a proof that essentially established that the content can be learned without knowing the exact dimension $d_c$. However, their result was under the assumption that the domains are aligned. In addition, the proof could not hold when style learning is also involved. Our proof solved these challenges. The work in (Xie et al., 2023) used a mask-based formulation to remove the requirement of knowing $d_c$ and $d_s$. The mask-based formulation has the flavor of sparsity promoting as in our proposed method. However, they still need to know $d_c + d_s$, which is unlikely available in practice. In addition, the mask-based method in (Xie et al., 2023) did not have theoretical supports."}, {"title": "6 NUMERICAL EXAMPLES", "content": "Multi-Domain Data Generation. For the data generation task, we validate our theoretical claims using three real world datasets: animal faces (AFHQ) (Choi et al., 2020), CelebA-HQ (Karras et al., 2018), and CelebA (Liu et al., 2015) with 3, 2, and 7 domains, respectively (see Appendix G.6).\nThe baselines here are I-GAN (Xie et al., 2023), StyleGAN-ADA (Karras et al., 2020) and Transitional-cGAN (Shahbazi et al., 2021).\nFollowing Xie et al. (2023), we use StyleGAN2-ADA (Karras et al., 2020) to represent our generative function q in (6a). We set $d_c = 384$ and $d_s = 128$ in all the experiments. We use a regularization term $||e_s^{(n)} (r_s^{(n)})||_1$ to approximate the sparsity constraint (6b). We find that the algorithm is not"}, {"title": "7 CONCLUSION", "content": "We revisited the problem of content-style identification from unaligned multi-domain data, which is a key step for provable domain translation and data generation. We offered a LDM perspective. This new viewpoint enabled us to prove identifiability results that enjoy considerably more relaxed conditions compared to those in previous research. Most importantly, we proved that content and style can be identified without knowing the exact dimension of the latent components. To our knowledge, this stands as the first dimension-agnostic identifiability result for content-style learning. We showed that the LDM formulation is equivalent to a latent domain-coupled multi-domain GAN loss, and the latter features a simpler implementation in practice. We validated our theorems using image translation and generation tasks.\nLimitations. Our work focused on sufficient conditions for content-style identifiability, yet the necessary conditions were not fully understood\u2014which is also of great interest. Additionally, our model considers that the domains are in the range of the same generating function. The applicability is limited to homogeneous multi-domain data, e.g., images with the same resolution. An interesting extension is to consider heterogeneous multi-domain models that can deal with very different types of data (e.g., text and audio)."}, {"title": "C PROOF OF THEOREM 3.3", "content": "We restate the theorem here:\nTheorem 3.3 Under Eq. (1), suppose that Assumptions 3.1 and 3.2 hold. Then, we have\n$f_c(x^{(n)}) = \\gamma(c)$ and $f_s(x^{(n)}) = \\delta(s^{(n)}), \\forall n \\in [N]$, where $\\gamma : C \\rightarrow \\mathbb{R}^{d_c}$ and $\\delta : S \\rightarrow \\mathbb{R}^{d_s}$ are injective functions.\nProof Outline. The proof pipeline of Theorem 3.3 can be divided into two parts: content identification and style identification. The content identification part includes two steps: (i) showing that $f_c$ does"}, {"title": "C.1 CONTENT IDENTIFICATION", "content": "Let $z^{(n)} = (c, s^{(n)})$ and $Z = C \\times S$. Let $\\hat{c}^{(n)} = f_c(x^{(n)}), \\forall n \\in [N]$, and $\\hat{s}^{(n)} = f_s(x^{(n)}), \\forall n \\in [N]$. The proof consists of the following two steps:\nStep 1. First, we show that under the assumptions in Theorem 3.3, $\\hat{c}^{(n)}$ does not depend upon $s^{(n)}$.\nStep 2. Next, we show that $\\hat{c}^{(n)}$ is transformed from the true shared component c via an injective function $\\gamma : C \\rightarrow \\mathbb{R}^{d_c}$ for all $n \\in [N]$.\nStep 1: We want to establish the following:\n$\\frac{\\partial \\hat{c}_i^{(n)}}{\\partial s_j^{(n)}} = 0, \\forall i \\in [d_c], j \\in [d_s]$. (10)\nThis can be shown using similar arguments in the domain adaptation work (Kong et al., 2022). To be specific, let $h : Z \\rightarrow \\mathbb{R}^{d_c+d_s}$ be defined as follows:\n$h := f \\circ g$.\nLet $C = h_c(Z)$. Due to the distribution matching constraint (2a), the following holds for any $A_c \\subset C$:\n$P_{x^{(i)}}[A_c] = P_{x^{(j)}}[A_c], \\forall i, j \\in [N]$\n$\\Downarrow (a)$\n$P_{Z^{(i)}}[h_c^{-1}(A_c)] = P_{Z^{(j)}}[h_c^{-1}(A_c)]$ (11)\nwhere $h_c^{-1}(A_c) := \\{z \\mid h_c(z) \\in A_c\\}$ is the preimage of $h_c(\\cdot) := [h(\\cdot)]_{1:d_c}$. The equivalence in (a) holds because $P_{\\hat{c}^{(n)}}[A_c] = P_{h_c(z^{(n)})}[A_c] = P_{z^{(n)}}[h_c^{-1}(A_c)], \\forall n \\in [N]$.\nSufficient Condition of (10): In order to show (10) holds almost surely, it suffices to show that\n$\\forall \\bar{c} \\in \\hat{C}, \\exists B_c \\neq \\emptyset$ and $B_c \\subseteq C$\ns.t. $h^{-1}(\\bar{c}) = B_c \\times S$.  (12)\nEq. (12) implies that no matter what the value of the private variable $s^{(n)}$ is, as long as the shared variable $c \\in B_C$, the extracted $\\hat{c} = \\bar{c}$. This implies that small changes in $s_j \\, \\forall j \\in [d_s]$, while keeping all other variables fixed, does not result in any change in $\\hat{c}_i, \\, \\forall i \\in [d_c]$\u2014which means that (10) holds.\nFurther, the following condition is sufficient to ensure that (12) holds:\n$\\forall \\bar{c} \\in \\hat{C}, \\forall \\epsilon > 0, \\exists G_c \\neq \\emptyset$ and $G_c \\subseteq C$,\nh^{-1}(N_{\\epsilon}(\\bar{c})) = G_c \\times S$, (13)\nwhere $N_{\\epsilon}(c) = \\{c' \\in \\hat{C} \\mid ||\\bar{c} \u2013 c'||_2 < \\epsilon\\}$ is an open set. To see how (13) implies (12), we use contradiction. Suppose that (13) does not imply (12). Then there exists an $\u00f1 \\in [N]$ and $z = (c, s^{(\u00f1)}) \\in Z$ with\n$\\bar{c} \\in B_c := \\{z_{1:d_c} : z \\in h^{-1}(\\bar{c})\\}$,"}, {"title": "C.2 STYLE IDENTIFICATION", "content": "Our goal is to show that $h_s(c, s^{(n)}) = \\delta(s^{(n)})$ for some injective function \u03b4. As before, we first show that $h_s(c, s^{(n)})$ is only a function of $s^{(n)}$, i.e., $h_s(c, s^{(n)}) = \\delta(s^{(n)})$ for some function \u03b4. Then, we show that \u03b4 has to be injective.\nTo proceed, note that the statistical independence constraint $\\hat{c}^{(n)} \\amalg \\hat{s}^{(n)}$ implies the following:\n$\\hat{c}^{(n)} \\amalg \\hat{s}^{(n)} \\Downarrow$\n$p(\\hat{c}^{(n)}, \\hat{s}^{(n)}) = p(\\hat{c}^{(n)})p(\\hat{s}^{(n)}) \\Downarrow$\n$I(\\hat{s}^{(n)}; \\hat{c}^{(n)}) = 0$,"}, {"title": "D PROOF OF THEOREM 3.4", "content": ""}]}