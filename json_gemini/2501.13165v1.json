{"title": "QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks", "authors": ["Naman Jain", "Amir Kalev"], "abstract": "We introduce Quantum Feature Extraction (QuFeX), a novel quantum machine learning module. The proposed module enables feature extraction in a reduced-dimensional space, significantly decreasing the number of parallel evaluations required in typical quantum convolutional neural network architectures. Its design allows seamless integration into deep classical neural networks, making it particularly suitable for hybrid quantum-classical models. As an application of QuFeX, we propose Qu-Net a hybrid architecture which integrates QuFeX at the bottleneck of a U-Net architecture. The latter is widely used for image segmentation tasks such as medical imaging and autonomous driving. Our numerical analysis indicates that the Qu-Net can achieve superior segmentation performance compared to a U-Net baseline. These results highlight the potential of QuFeX to enhance deep neural networks by leveraging hybrid computational paradigms, providing a path towards a robust framework for real-world applications requiring precise feature extraction.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing has emerged as a computational paradigm with the potential to solve problems intractable for classical computers. The rise of the machine learning (ML) methodology has spurred interest in quantum machine learning (QML) as a potential means to improve classification, regression, and clustering tasks [1, 2].\nEarly on in the development of QML, quantum neural networks (NNs) were proposed [3, 4] as an extension of the concept of classical NNs by utilizing quantum circuits as layers of the network. The design of the quantum layers corresponds to different ML architectures, with notable examples including quantum convolutional NN (QCNN) [3] and Quanvolutional NN (QuanNN) [4]. While convolutional NNs (CNNs) \u2013 the cornerstone of modern NNs - have achieved significant success across a wide range of ML applications [5, 6], their resource requirements, especially for training large models, increase drastically with the size and complexity of data [7, 8]. Quantum NN extensions, such as QCNN and QuanNN, were proposed under the hypothesis that, since quantum computers can process information in fundamentally different ways, quantum NNs may potentially offer greater efficiency compared to their classical counterparts [9, 10].\nIn parallel to developing quantum NN models, in recent years there has been a growing interest in creating a hybrid quantum-classical model where quantum and classical NNs work in tandem [11-14]. This division of computing resources allows leveraging the strengths of both classical and quantum computing while managing the limitations of near-term quantum devices. Hybrid models can be particularly promising in domains such as image processing and generative modeling, where quantum circuits may offer advantages in high-dimensional feature spaces, while classical systems handle more routine tasks. Thus, the hybrid quantum-classical ML approach provides a feasible pathway toward enhancing ML tasks under the current quantum technological constraints.\nA typical hybrid model includes classical layers for pre-processing data, followed by quantum processing layers (circuits), and concludes with classical post-processing steps. Nevertheless, various strategies have been proposed for integrating quantum layers into classical deep NN architectures. For example, in image classification tasks, quantum layers are often positioned near the end of the hybrid network to generate the final output label [15], while some approaches involve encoding data through quantum layers at the beginning of the network [16]. The design of the quantum layer itself varies widely as well, with notable examples including QCNN [3] and QuanNN [4]. These variations underscore the need for a comprehensive study of optimal quantum layer placement in hybrid networks, as well as a systematic approach to constructing effective quantum layers.\nIn this work, we take a step forward in these two directions. We introduce a novel quantum learning architecture, which we term quantum feature extraction, or QuFeX for short, which combines and leverages techniques from QCNN [3] and QuanNN [4]. At a high-level it integrates the data analysis structure"}, {"title": "II. THE QuFeX ARCHITECTURE", "content": "Setting the stage for the proposed quantum architecture, we first highlight a few key components of classical CNNS, QCNNs, and QuanNNs that are relevant to our work.\nA. Convolutional neural networks and their quantum counterparts\nAt its core, a CNN is composed of a hierarchal series of convolutional layers, which are the fundamental building blocks of modern classical NNs. As illustrated in Fig. 1 each convolutional layer computes new feature values from a weighted linear combination of nearby ones in the preceding layer. The set of weights (usually arranged in the form of a small matrix) is called a kernel or a filter. The kernel (or a collection thereof) \"slides\" across and convolve over the input data, generating a feature map. This translational invariance of the CNN makes them a powerful ML tool. The maps produced at different layers of the network highlight specific and often different patterns (features) of the input data, enabling its applicability across various learning tasks. Typically, pooling layers are interleaved between adjacent convolution layers to reduce the size of the feature maps, e.g. by taking the maximum or average value of a few contiguous feature values. A non-linear activation function, such as ReLU (rectified linear unit), usually follows the pooling layer. The weights of the kernels are optimized by training on a dataset. In contrast, variables like the number of kernels, the size of each kernel, and pooling layer specifics are called hyperparameters and are fixed for a given model.\nMotivated by these powerful information processing components researchers have developed quantum counterparts of CNN. QCNN, first introduced in [3], was designed to mimic the translational invariance property of CNN. Figure 2 provides an illustration of the QCNN architecture. In QCNNs, the (classical) convolutional layers are replaced with a set of parametrized unitary transformations (U; in the figure) and pooling layers are replaced with fixed unitary operations (V; in the figure) conditioned on the measurement results of few specific qubits. These blocks of unitary operations (circuits) are concatenated in a serial fashion, and as shown in Fig. 2, each unitary circuit is designed in a translational invariant way to maintain this CNN property. One of the advantages of this approach is that for an N qubit circuit, this network has only O(log N) trainable parameters [3]. While the structure of each circuit layer is fixed, the parametrized unitary operations (Uj) themselves are trainable via an optimization strategy, e.g., gradient descent to minimize a particular loss function, and therefore are often thought of as the quantum-equivalent of CNN kernels.\nWhile, operationally, the quantum convolutional circuits within QCNNs are considered efficient data processing modules that mimic the translational invariance of kernels in CNN, the data pipeline of QCNN generally does not possess some of the key features of CNNs. Specifically, since the convolutional and pooling layers in a typical QCNN circuit are ordered sequentially a circuit, the input to the circuit is often too large to be handled efficiently (this is known also as the input problem). On the other hand, classically manipulating the input to reduce it to a manageable size, may result in loss of information when when handling data with multiple (or non-local) features.\nTo address these challenges, QuanNN [4] has been proposed, which leverage quantum circuits to enable the simultaneous extraction of local features across input data. QuanNNs act as feature extractors, analogous to convolutional layers in classical networks, and can be integrated seamlessly with classical NN architectures for hybrid quantum-classical processing. As illustrated in Fig. 3, in QuanNN each quantum circuit is treated as a kernel and generates a feature map when applied to an input by transforming spatially-local subsections of it. These filters correspond to specific circuit designs and can be configured to potentially learn different features of the input. The QuanNN architecture thus offers greater flexibility and is considered a closer representation of classical CNNs in its way of handling the flow of data. Nonetheless, by mimicking the data flow of classical CNN, QuanNN fall short in efficiency. Since QuanNNs extract local quantum features independently for each patch, similar to CNNs, to process the input data we must design O(kN) quantum experiments, with thousands of learning iteration per filter as reported in [4], where N is the size of the input and k is the number of filters used.\nIn this work, we take inspiration from the data processing of QCNN and the data flow in QuanNN to create a novel quantum feature tool, QuFeX, that emulate the hierarchical and local feature extraction that is characteristic to CNNs while overcoming the limitations of each architectures separately. We describe in detail next.\nB. QuFeX: Quantum feature extraction architecture\nThe QuFeX architecture, shown schematically in Fig. 4, follows the QuanNN style to manage the data flow and uses the QCNN spatial-invariance circuit design for data processing. In addition, we introduce several critical modifications to efficiently handle the data, specifically for near-term devices, and for compatibility with deep NNs.\nIn this architecture, the QuFeX circuit is designed as a quantum filter that creates a feature map from its input data. To avoid limiting the application of the filter to a spatially local subsection of the input data, we design it to act on a mix of different input feature maps, as illustrated in Fig. 4. This enables us to generalize the quantum feature extraction to the important (and common) cases where features are not spatially local in the input data. The way we mix the input data can be considered a hyperparameter, similar to CNN. Notably, this mixing can be done in parallel on all feature maps input, and therefore, the QuFeX can handle k input feature maps with O(1) uses of the QuFeX circuit [QuFeX(0) in the figure]. This is one of the main advantages of the proposed architecture, compared to existing ones. By using QuFeX circuit on a small number of qubits (say, 4 qubits) we are able to handle k classical feature maps, where k can be very large, each of size N \u00d7 N, to produce an output feature map of size M \u00d7 M (typically M is chosen to be smaller than N). The cost of producing an output feature map using QuFeX depends only the size of the QuFeX circuit (which is logarithmically in the number of qubits) and the output feature map size M. The exact cost may vary from one implementation to another, depending on the choice of hyperparameters such as how we mix different input feature maps and the way we slide the convolution window. In addition, more filters, and hence more output feature maps, can be created simply by adding multiple QuFeX layers, which can run in parallel and with each being fully customizable.\nWithin each QuFeX layer we process the data in a translationally invariant way, similar QCNNs, as depicted in the Fig. 4, with one critical change. Here, we are not discarding the control qubits in the pooling layer. In QCNN the pooling layers are implemented by discarding the measured (control) qubits, effectively reducing the dimension of the network. In the QuFeX architecture, similar to classical CNNS and QuanNN, the reduction of dimension is done directly through the data flow. Therefore, instead of discarding the control qubits, and by doing so discarding information within a filter, we keep the control qubits and treat their output as a part of the (or even separate) feature map.\nOne key advantage of the QuFeX architecture is its capacity for parallelized execution of multiple quantum filters, allowing for multi-filter operations without increasing circuit complexity. Each filter operates independently but shares a uniform execution structure. Additionally, since the data flow of QuFeX architecture closely mirrors that of CNNs, its integration with deep networks in a hybrid quantum-classical ML model is straightforward, enabling seamless compatibility with established training paradigms and architectures while leveraging quantum circuits for enhanced feature extraction."}, {"title": "III. THE Qu-Net ARCHITECTURE WITH APPLICATION TO IMAGE SEGMENTATION", "content": "As a proof of concept, in the next section we propose a novel hybrid quantum-classical model, a Qu-Net, were the QuFeX is integrated within a classical U-Net model [20] \u2014 a CNN with a U-shape topology specifically developed for image segmentation. We demonstrate the potential power of the QuFeX though segmentation performance of the Qu-Net architecture. Our demonstrations represent one of the first applications of quantum-enhanced deep NNs to real-world image segmentation tasks.\nA. The U-Net architecture\nIllustrated in Fig. 5(a), the U-Net architecture is a deep CNN with a U-shape topology specifically developed for image segmentation [20]. It features an encoder-decoder structure with skip connections, which help preserve spatial information, and enable precise pixel-level predictions for more accurate segmentation results. The encoder progressively down-samples the input image, capturing essential features through convolutional and pooling layers. Subsequently, the decoder upsamples the compressed feature map to the original input size, reconstructing the segmentation map. At its bottleneck, i.e., at the most compressed layers, the U-Net transforms features into a latent representation that bridges the encoder and decoder. This design \u201cencourages\" the model to retain only the relevant information in the bottleneck layer, facilitating the construction of an effective segmentation map.\nThis design helps U-Net excel in segmenting objects with fine boundaries, making it highly effective, for example, in medical applications like tumor delineation, organ segmentation, and cell tracking [20, 21]. Beyond medical imaging, U-Net is used in various fields requiring pixel-level classification, e.g., for satellite image analysis [23] and autonomous driving [22]. Its ability to handle small datasets through data augmentation and its versatile performance in capturing detailed structural information has made U-Net a widely adopted tool in image segmentation across industries.\nB. The Hybrid Qu-Net architecture\nWe propose a hybrid quantum-classical U-Net architecture, termed Qu-Net, that integrates the QuFeX as a quantum module within the classical pipeline, see Fig. 5(b). Our tests and evaluation are meant to showcase the advantages of the QuFeX architecture and suggest that the hybrid model, where QuFeX is placed at the bottleneck of the U-Net, may deliver better image segmentation performance than compared to a baseline U-Net model.\nIn hybrid models, such as the one we are considering here, the positioning of the quantum module within the classical architecture is crucial. In deep NNs, the placement of the quantum layers influences both the kinds of features that the network learns and how effectively it processes information at various stages. Different placements may impact the network's ability to generalize, process complex correlations, and learn distinct features that might be challenging for purely classical models.\nIn deep CNNs early layers, closer to the input, typically capture low-level features such as edges, colors, and textures, while deeper layers identify more complex and abstract patterns. While placing a quantum layer early in the hybrid architecture might enable it to capture unique low-level properties in the raw data, potentially simplifying the task for subsequent classical layers, this approach may under-utilize quantum resources, as the context of the entire input is limited at this stage. Similarly, positioning a quantum layer closer to the output may under-utilize potential quantum advantages particularly in cases where the classical part of the network effectively and robustly learns abstract representations of the input data. In contrast, integrating a quantum layer in the middle of the network, especially between encoding and decoding parts of deep CNNs, may present significant potential benefits. First, and most importantly, at this position the quantum layer can function similarly to a bottleneck, compressing information while preserving essential details for the final stages of processing. This strategy creates a hybrid embedding space that leverages both quantum and classical learning representations. Our tests and evaluation of the Qu-Net architecture detailed in the next subsection, support this approach. Second, from a resource perspective, placing a quantum layer in the bottleneck of a hybrid CNN, gives us the required flexibility to tailor hyperparameters, such as the size of the input to the quantum layer, to match the available quantum resources.\nHence, in the proposed hybrid U-Net we replace the bottleneck layer of the U-Net with a QuFeX. The layers preceding and succeeding the QuFeX are (classical) U-Net layers. The level of compression (the input data to the QuFeX) is a hyperparameter chosen based on, e.g., the available quantum resources. We tested and evaluated the Qu-Net performance on image segmentation tasks and compared it to the performance of an all-classical U-Net (optimized) model.\nThe downscaling of the input all the way down to the bottleneck of the architecture, introduces limitations on the representability of the original image. In addition to QuFeX, inspired by the ResNet architecture [24], we include (classical) residual connections from one part of the network to the other that bypass the QuFeX module, see Fig. 5(b). The residual connection can be expressed as $y = Q(x) + x$ where $Q(x)$ represents the output of the quantum layer applied to the input x and the addition of x serves as the identity mapping. We argue that residual connections, typically used to stabilize training and convergence in deep NNs, are very relevant in hybrid quantum-classical setups. Adding identity mapping over the quantum layer enables the model to propagate features learned by the classical layers forward and backward through the network without being disrupted by the quantum transformation. This configuration allows the network to select an optimal path: either through a classical-only route or a path incorporating quantum processing, thus leveraging the advantages of both. Our tests below show that introducing residual connections to the Qu-Net architecture provides better segmentation performance compared to baseline U-Net models where the QuFeX is not included.\nC. Testing and evaluation\nWe have tested and evaluated the performance of the proposed QuFeX module within the novel Qu-Net architecture using the Fruit Seg30 Segmentation Dataset & Mask Annotations [25] which consists of high-resolution images of a variety of fruits and their corresponding segmentation masks. The complete dataset includes 30 fruit classes, totaling 1969 images, each paired with precise segmentation masks. For this research, a curated subset of 751 images was selected (along with their segmentation masks) and downscaled, using Bilinear interpolation, from the original 512 \u00d7 512 pixels to 64 \u00d7 64 pixels to align with the available computational resources [Intel Core i5-11300H CPU @ 3.10GHz (4 cores), 8 GB DDR4 RAM running Ubuntu 22.04 LTS]. The subset was selected to ensure diversity between fruit types while maintaining computational feasibility.\nTo provide a baseline for the performance of the Qu-Net architecture, a U-Net architecture with approximately the same number of trainable parameters was also trained and evaluated on the same dataset. The code for all the numerical experiments reported in this subsection are available in a public repository [26].\nWe tested three variations of the models, which we refer to as tiny, small, and medium, that consist of approximately 12,000, 26,000, and 40,000 trainable parameters, respectively. These variations in parameter count were achieved by adjusting the number of filters in the classical layers of the architectures. We refer the reader to the Appendix for more details about the specific U-Net and Qu-Net models design.\nTo ensure statistical reliability of the results, we generated 10 random partitions of the dataset into training and testing subsets, and run each model on these 10 pre-determined partitions. This approach accounts for potential variability in the results due to dataset splitting and ensures a robust evaluation. We use the Intersection over Union (IoU) as the performance metric for all the tested models, as it provides a balanced assessment of segmentation quality by measuring the overlap between the predicted and ground truth masks. This metric is particularly well-suited for binary segmentation tasks, ensuring accurate foreground-background separation.\nOur models were trained using an Adam optimizer with a learning rate of 0.001 and a binary cross-entropy loss function. Training was performed for 10 epochs with a batch size of 64. The Qu-Net architecture was executed on quantum simulator using PennyLane [27]. An ablation study was conducted to evaluate the influence of the QuFeX layer's design choices, see the appendix for more details. The performance of Qu-Net was analyzed with varying numbers of quantum filters and qubits per filter within the QuFeX module. Specifically, here we report the results obtained using a quantum filter with 8 qubits"}, {"title": "IV. CONCLUSION", "content": "In this work we introduce a new quantum feature extraction module, QuFeX, which integrates and borrows elements from leading convolutional quantum architectures, QCNN [3] and QuanNN [4]. The quantum data analysis of QuFeX follows ideas introduced in QCNN while the data flow is inspired by that introduced in QuanNN. In this way, QuFeX allows us to utilize \"best of both worlds\" positioning it as a candidate for integration with deep classical CNNs, and deployment in real-world application. To test the proposed module we have integrated it at the bottleneck of a U-Net architecture. In numerical tests we found that the resultant hybrid quantum-classical model, which we termed Qu-Net, showed promising results, exceeding the performance of an all-classical U-Net as measured by the IoU metric for segmentation tasks on fruit images.\nWe believe that these tests showcase the potential of the QuFeX as a basic quantum module for classification tasks. In addition, by its design, the QuFeX well-suited for integration in deeper levels of CNNs, and thus does not require large quantum circuits for its execution. This feature makes it ideal for running on quantum hardware with relatively small number of qubits. We thus believe that the the new quantum convolutional module we have developed opens the door to the deployment of hybrid quantum-classical ML models to real-world applications.\nIn future work we plan to explore the effectiveness of the QuFeX, and of the Qu-Net, in generalizing to other segmentation and classification tasks, such as for medical applications."}, {"title": "Appendix A: Details on the numerical tests", "content": "The U-Net architecture (see Fig. 5a) used in this study consists of an encoder-decoder structure with symmetric skip connections to preserve spatial information during upsampling. In our numerical tests, the input to the U-Net is 2D images of size 64 \u00d7 64 and its output is segmentation masks of the same dimension. The encoder comprises five convolutional blocks. Each block includes two convolutional layers with 3 \u00d7 3 filters, and ReLU activation. Max-pooling with a 2\u00d72 window and stride of 2 is applied after each block for downsampling. The decoder mirrors the encoder, consisting of five upsampling blocks. Each block includes an upsampling operation (transposed convolution with 2 \u00d7 2 filters), concatenation with the corresponding encoder feature map, followed by two convolutional layers with 3 \u00d7 3 filters, and ReLU activation. Skip connections are implemented by concatenating feature maps from the encoder to the corresponding decoder layers at the same resolution, ensuring preservation of spatial context. The final layer applies a 1 \u00d7 1 convolution with a single filter and a sigmoid activation function to produce the segmentation mask.\nUnlike standard U-Net implementations, the number of filters in the convolutional blocks does not double after each block. Instead, the filter configuration is tailored to balance model complexity and performance. In the tiny scale, the encoder consists of five convolutional blocks with the number of filters set to 4, 4, 8, 8, and 8, followed by 4 filters in the bottleneck layer. In the small scale, the encoder uses 4, 8, 8, 8, and 16 filters, with 8 filters in the bottleneck. For the medium scale, the encoder is configured with 8, 8, 8, 16, and 16 filters, and the bottleneck layer contains 16 filters. In all cases,"}, {"title": "QU-Net Qu-Net 8(1) Qu-Net 4(2)", "content": "the decoder mirrors the encoder, ensuring symmetry in feature extraction and reconstruction. This design provides a flexible framework to adapt the model's parameter count to different resource constraints and application needs.\nThe Qu-Net design follows a structure similar to the U-Net, with identical encoder and decoder layers. The distinction lies in the bottleneck layer, where the classical bottleneck is replaced by a QuFeX layer. In the simplest configuration with one QuFeX layer, an 8-qubit quantum circuit is employed. For configurations with two QuFeX layers, the model uses 4-qubit circuits for each layer. The process of data transformation from classical to quantum, the parameterized quantum circuits, and the measurement strategy are detailed later in the text.\nFor instance, in the tiny scale with one QuFeX layer, the encoder outputs 8 feature maps, each of size 2 x 2. These feature maps are grouped into 4 groups, with 2 feature maps per group. Each group is then processed by an 8-qubit QuFeX layer. For configurations with two QuFeX layers [Qu-Net 4(2)], no grouping is performed (i.e., in this case there is no feature map multiplexing); all 8 feature maps of size 2 \u00d7 2 are fed directly to both QuFeX layers. The same treatment is extended to the small and medium scales. The total number of parameters in all the model types are listed in Table I.\nIt is important to emphasize that these configurations are fully customizable, and the choices presented here are driven by constraints on computational resources. This design provides flexibility in adapting the quantum layers to different scales and hardware limitations.\nThe QuFeX layer is a parameterized quantum layer designed to replace the classical bottleneck layer in the Qu-Net architecture. For a single QuFeX layer using an 8-qubit circuit, there are 4 trainable parameters. The classical inputs are scaled by and encoded as rotation angles for RY gates (angle encoding). The parameterized gates in the circuit are represented in Fig. 9. The pooling gates (Vj's) are control-Z gates in all implementation. The outputs are obtained as the expectation values of the Pauli-Z observable on each qubit.\nIn the configuration with two QuFeX layers, the design for the second layer differs slightly. It also contains 4 trainable parameters, but the input data is encoded using RZ gates with angle encoding in the X basis. Additionally, the parameterized gates remain the same except that U\u2081 and U2 blocks are interchanged. The measurement strategy remains consistent, extracting the expectation values of the Pauli-Z observable.\nIt is important to note that these constructions are merely design choices and do not impose any inherent constraints. The configurations are fully modifiable and can significantly influence the model's performance. In Fig. 10 we provide the run-wise IoU scores (raw data) for of all performed tests."}, {"title": null, "content": "$U_1$\n$RX (\\theta_1)$\n$RZ(\\theta_2)$\n$RX (\\theta_3)$\n$U_2$\n$RY (\\theta_4)$"}]}