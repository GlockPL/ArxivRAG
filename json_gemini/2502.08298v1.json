{"title": "IMPROVING EXISTING OPTIMIZATION ALGORITHMS WITH LLMS", "authors": ["Camilo Chac\u00f3n Sartori", "Christian Blum"], "abstract": "The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve & Adapt (CMSA)\u2014a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs.", "sections": [{"title": "1 Introduction", "content": "There is a wide variety of optimization algorithms of all kinds and flavors. A simple search using the term 'optimization algorithm' in databases like Scopus or platforms like GitHub yields thousands of results, with that number growing every year. Additionally, optimization researchers often maintain private collections of their algorithms. All these algorithms-open-source or proprietary, and implemented in diverse programming languages-can potentially be improved. By refining their original code with modern techniques and technologies, we can achieve more efficient designs and implementations that go beyond what their creators originally envisioned.\nIn recent years, the development and growth of Large Language Models (LLMs)\u2014popularized by models such as OpenAI's GPT-4 [21], Anthropic's Claude [26], Google's Gemini [27], Meta's Llama 3 [28], and recently DeepSeek [9]\u2014has opened the door to a wealth of new possibilities. Among the most transformative advancements is code generation. Tools like GitHub Copilot\u00b2, which integrates seamlessly with code editors like Visual Studio Code, and Cursor\u00b3, an editor with built-in LLM capabilities, have become essential tools for many software developers, revolutionizing their daily workflows. For example, a Python developer could request an LLM to generate a template for invoking external APIs, and the LLM would automatically produce the required code. LLMs have become invaluable for streamlining coding tasks, especially those that are routine and highly repetitive [12]. Therefore, it is natural to wonder: if LLMs excel at simple programming tasks, could they also aid in improving sophisticated optimization algorithms?\nOne of the recent examples of leveraging LLMs in optimization algorithms is the development of frameworks for generating new black-box metaheuristics [29]. Finding an effective metaheuristic, for example, for a combinatorial optimization problem can be a significant challenge. However, using LLMs to build upon an existing metaheuristic as context and employing them as tools to discover new heuristics or more efficient implementations (e.g., reducing RAM usage or computation time) in a given programming language remains a largely unexplored area. Implementing metaheuristics, unlike other algorithms, requires a focus on computational efficiency, mathematical expressions in scientific computing, and efficient data structure selection. Thus, designing new algorithms in this field demands expert implementation skills.\nIn this paper, we demonstrate how LLMs like GPT-4o can be leveraged to enhance a sophisticated optimization algorithm, specifically the Construct, Merge, Solve & Adapt (CMSA) hybrid metaheuristic [1, 3, 2]. Starting with an expert-developed C++ implementation of CMSA for the Maximum Independent Set (MIS) problem (of approximately 400 code lines), we employed an in-context prompting strategy combined with interactive dialogue. Our results show that the LLM successfully comprehended the CMSA implementation for MIS and discovered novel heuristics while suggesting improvements to the C++ codebase. This successful example opens up new possibilities for enhancing existing complex optimization algorithms by using LLMs as assistants.\nThe paper unfolds as follows. In Section 2, we introduce code generation using LLMs, making it accessible for readers without prior experience in this field. We also explain the MIS problem and provide a brief overview of the CMSA algorithm. Next, in Section 3, we present our methodology for enhancing CMSA for the MIS problem using LLMs through a novel narrative that describes our process and provides detailed steps for result reproduction. Section 4 presents our experimental results and their interpretation. The limitations of our approach and future research directions are discussed in Section 5. The paper concludes by summarizing our key findings and emphasizing the potential impact of LLMs on existing metaheuristics."}, {"title": "2 Background", "content": null}, {"title": "2.1 Code Generation with LLMs", "content": "Code generation is one of the primary research areas in LLMs [12, 15, 5]. The concept is straightforward: given a prompt, such as \u201cI need an algorithm to sort a list of numbers in Python\", the LLM is expected to return a corresponding algorithm (e.g., Quicksort) implemented in Python. This capability arises because LLMs are trained on massive amounts of data, which include code from diverse programming languages available on the internet. These codes are sourced not only from GitHub repositories but also from StackOverflow, technical documentation, scientific articles, and publicly available books.\nA more explicit prompt can lead to more sophisticated responses. For example, the prompt \"I need an efficient sorting algorithm for a list of numbers in Python, which can be processed in parallel, utilizing modern optimization techniques\" in GPT-4o generates a ParallelMergeSort. While efficient, it can be further refined by interacting with the model. For instance, asking \u201cFind new ways to improve it\u201d results in an enhanced version using techniques like optimized Quicksort for small arrays, better memory management with numpy, and heap merge for combining sorted arrays. This approach leverages MergeSort's ease of parallelization. However, the model might have suggested a different algorithm if the focus was on memory optimization without parallelism. This emphasizes the importance of prompt design and iterative refinement of responses [36].\nLLM response quality depends on the data used for training, and with the refinement of datasets and the increase in size, their performance in code generation has improved [34]. Moreover, no LLM is flawless when it comes to generating error-free code. Continuing with the ParallelMergeSort example, a model might return code with bugs, so interaction with the model (e.g., copying and pasting error messages from the Python interpreter) is necessary to refine the responses. One could even ask the model to generate its own test suite to verify its algorithm. Different LLMs for these tasks are generally evaluated through benchmarks. One of the most widely used is HUMANEVAL [6], which features programming challenges that assess language comprehension, algorithmic competencies, and basic mathematics-some of which are comparable to straightforward software developer interview questions.\nFurthermore, code generation using LLMs is a broad area, as code may originate or be destined for very different domains, including the following: data science code for analyzing data and building predictive models [31]; systems code for managing hardware and low-level operations [14, 8]; frontend code development for web applications and UI [33]; and optimization code for solving complex computational problems. Each domain presents its own unique challenges and requirements, with our focus being on the latter, specifically in the field of metaheuristics.\nConcerning the automatic generation of metaheuristics with LLMs, given the famous \"No Free Lunch Theorem\" [32], researchers understand that no single metaheuristic algorithm can outperform all others across all optimization problems. This fundamental principle naturally leads to an interesting possibility: LLMs could serve as powerful automatic generators of black-box metaheuristics, significantly reducing the time needed to find the 'best' implementation of a metaheuristic for a specific problem. In this context, LLMs could be tasked with discovering novel variations and operators that create new metaheuristics-potentially even surpassing state-of-the-art algorithms for particular problems [29].\nThe first notable demonstration of heuristic discovery using LLMs was conducted by FunSearch [24], which showcased the potential of leveraging LLMs to generate novel heuristics for the Bin Packing (BP) Problem through a heuristic evolution process. However, a recent study [25] points out that the heuristics developed by FunSearch face challenges in generalizing across diverse BP problem instances. While FunSearch relies on an incomplete or suboptimal base program as a starting point, our approach starts with a complete and carefully designed implementation of an optimization algorithm. Other related works can be summarized as follows. van Stein and B\u00e4ck [29]'s LlaMEA is a framework that integrates evolutionary algorithms with LLMs to iteratively generate and refine novel black-box metaheuristics during runtime. Similarly, Hemberg et al. [11] proposed LLM GP, combining genetic programming with LLMs to evolve operators through LLM assistance. Meanwhile, Pluhacek et al. [23] demonstrated via prompt engineering that LLMs can generate innovative metaheuristics by identifying and decomposing six high-performing swarm algorithms for continuous optimization.\nWhile these studies already give a glimpse of possible use cases for LLMs in optimization, they primarily focus on creating new algorithms rather than improving existing ones\u2014that is, building upon pre-existing complex code. This distinction is significant given the vast landscape of published optimization algorithms\u2014a simple GitHub search for 'optimization algorithm' returns over 20,000 results, representing a wealth of algorithm implementations that could benefit from improvement. This is why we believe an opportunity exists to create a subfield focused on generating code from existing optimization algorithms. Building on this, our work explores using LLMs to enhance implemented algorithms, enabling the models to uncover new heuristics overlooked by experts in the original implementations. In this sense, our approach treats LLMs as assistants to researchers, not replacements. By doing so, we leverage LLMs to work with existing code (as context) and improve it using the current knowledge these black-box model models have gained during their pre-training phase.\""}, {"title": "2.2 Maximum Independent Set (MIS) Problem", "content": "To validate our hypothesis that LLMs can enhance existing optimization algorithms, we will employ an algorithm (detailed in the following subsection) to solve the Maximum Independent Set (MIS) problem, a well-studied and NP-hard combinatorial optimization problem with applications in network design, scheduling, and bioinformatics. Formally, the MIS problem is defined as follows: Given an undirected graph $G = (V, E)$, the objective is to find a largest subset $S \\subseteq V$ such that no two vertices in $S$ are adjacent in $G$, i.e., there is no edge $(u, v) \\in E$ for all pairs $u \\neq v \\in S$."}, {"title": "2.3 CMSA", "content": "Construct, Merge, Solve & Adapt (CMSA) is a hybrid metaheuristic, also known as a matheuristic, that combines elements of classical metaheuristics with exact solvers (such as Integer Linear Programming (ILP) solvers) for combinatorial optimization [3]. Each CMSA iteration is a sequence of four fundamental phases:\n1. Construct: Generates solutions probabilistically through a probabilistic greedy mechanism (remember this phase; it will be key in the next section).\n2. Merge: Combines solution components from the generated solutions to form a reduced subproblem.\n3. Solve: Applies an exact solver (in the case of the MIS: an ILP solver) to optimally solve the reduced subproblem.\n4. Adapt: Updates parameters and data structures based on the quality of the solution returned by the solver.\nThis hybrid architecture combines the efficiency of metaheuristics for search space exploration with the precision of exact methods in reduced spaces. The CMSA algorithm is controlled by some key parameters. First, and most importantly, each solution component has an associated age value, which is initialized to zero when a component is added to the subproblem. Moreover, a solution component in the subproblem is subject to an increase of its age value, in case it does not form part of the subproblem's solution generated by the exact solver. In this context, the parameter $age_{max}$ plays a crucial role, defining the maximum age of solution components before they are removed from the incumbent subproblem, thereby preventing stagnation and encouraging diversity. Other parameters include $n_a$, which sets the number of solution constructions per iteration, $t_{max}$, the total CPU time, $t_{limit}$, the time limit for the ILP solver per iteration, and $0 \\leq drate < 1$, which controls the determinism rate for solution construction. Note that higher values of $drate$ lead to more deterministic solution constructions, while lower values increase randomness. Well-chosen values for these parameters ensure a balanced exploration of the search space, effective exploitation of promising areas via exact subproblem solving, and efficient use of computational resources.\nWe selected CMSA for the purpose of this paper due to its relative complexity in implementation as compared to simple metaheuristics. For instance, implementing CMSA for the MIS problem in C++ presents significant technical challenges. The four phases must be precisely defined, properly integrated, and implemented efficiently to leverage the framework's full potential. This complexity would only increase when dealing with more sophisticated optimization problems. In essence, a CMSA implementation demands expertise not only in metaheuristics and exact methods but also proficiency in C++ (or whatever programming language is chosen for implementation).\nFor our study, we utilize the original C++ implementation to solve the MIS problem, which was provided by CMSA's inventor and can be downloaded from our website. This choice serves two crucial purposes: first, it ensures we are working with a well-implemented version of the algorithm, and second, it presents an interesting opportunity to test LLMs' capability to enhance expert-written code. This scenario allows us to evaluate whether LLMs can identify potential improvements even in code developed by domain experts. Our findings are presented in the following section."}, {"title": "3 LLM-Improved CMSA for MIS", "content": "In this section, we introduce a methodology for utilizing LLMs to improve existing optimization algorithms. To present our research and the technical approach in an accessible way, we use the narrative of Dr. Zoe, a fictional senior researcher in hybrid metaheuristics, whose work illustrates how we arrived at this methodology and demonstrates its practical application."}, {"title": "3.1 Beginning of the story", "content": "Zoe has published extensively on hybrid metaheuristics and has significant expertise in Large Neighborhood Search (LNS) and CMSA, focusing on combinatorial optimization problems such as the Minimum Dominating Set (MDS) and MIS problems. She codes primarily in C++, producing efficient, compact implementations, and only introduces abstractions when necessary, which gives her a competitive edge in developing high-performing algorithms.\nHer experience with LLMs is minimal, limited to using ChatGPT to improve her writing or fix package installation issues on Ubuntu. She has not explored LLM-driven code generation and is unfamiliar with the literature in this area. When she hears about LLMs' potential in optimization, she is initially skeptical, aware of their tendency to \u2018hallucinate' from blogs and conversations with colleagues."}, {"title": "3.2 First Steps with LLMs", "content": "Her perspective begins to shift when a friend suggests using LLMs to update her C++ code to modern standards (e.g., C++23). Intrigued by the possibility of improving her implementation, she decides to experiment with her CMSA code for MIS. \"It is only 400 lines long, nothing too complex,\u201d she thinks. She crafts a prompt: \"What improvements can I make to my CMSA implementation for solving the Maximum Independent Set (MIS)? {{ pastes her 400-line CMSA code here }}\". GPT-4o (which she uses through her free ChatGPT account) provides detailed feedback, with one particular suggestion catching her attention:\""}, {"title": "3.3 Discovering New Heuristics", "content": "The first thing Zoe does is recall what her heuristic does, as it has been a while since she implemented it. Therefore, she studies function generation_solution(), which represents the solution construction phase in the CMSA [1]:"}, {"title": "3.3.1 New heuristic from the LLM", "content": "To enhance her heuristic, as suggested by the LLM, by incorporating CMSA's age parameter, Zoe provides another prompt to the LLM. In response, the LLM improves the heuristic by introducing a weighted vertex selection mechanism and provides Zoe with the corresponding C++ code. For better context, please refer to case (a) of Figure 3 first. In particular, the updated heuristic now considers both node degrees and current age values. The vertex selection mechanism designed by the LLM can technically be described as follows:\n$V_i \\leftarrow \\begin{cases}\n  argmin{P_w(v_j) | v_j \\in V} & \\text{if } r < a \\\\\n  \\text{roulette-wheel selection w.r.t. } P_w(.) \\text{ values} & \\text{otherwise}\n\\end{cases}$\nwhere $P_w(v_j)$ is a weighted probability:\n$P_w(v_j) = \\frac{w(v_j)}{\\sum_{v_i \\in V} w(v_i)}, \\quad w(v) = \\frac{1}{2 + age(v)} + \\frac{1}{1 + degree(v)}$\nThe weight function $w(v)$ favors vertices with low age and degree values to promote diversity in the selection process. Thus, the LLM was able to change the original heuristic by incorporating the age values to diversify the node selection process, while balancing it with degree information through a composite weight function. The more Zoe thinks about it, the better she likes the idea of using the age values in this way. It makes a lot of sense to decrease the probability of solution components with high age values being incorporated in newly constructed solutions. Zoe recognizes that the LLM had to understand the context of CMSA to propose this change. However, Zoe identifies a minor implementation error in the LLM-generated code; see Figure 3 (a). She corrects it, noting that adding 1 to age might cause a division by zero, as age values in CMSA are set to -1 for those solution components that do not form part of the subproblem. This highlights that LLMs require feedback [4] and cannot be fully autonomous. Finally, we would like to highlight that this use of the age values never occurred to anyone working on CMSA algorithms.\nThis new CMSA variant is henceforth called LLM-CMSA-V1. It was obtained by replacing the generate_solution() of the original CMSA implementation with the new function provided by the LLM."}, {"title": "3.3.2 Improving LLM-CMSA-V1 with the LLM", "content": "After having obtained the new heuristic, Zoe begins to wonder if the LLM could further improve it. Therefore, she inputs the following prompt: \u201cAre there ways to enhance the dynamic selection heuristic to allow for a more diverse and advanced search?\u201d The LLM responds with several suggestions, one of which involves incorporating the concept of entropy. The C++ code provided by the LLM is characterized by a corresponding change, which involves replacing the definition of $P_w(.)$ (see Equation 1) with the following entropy-adjusted probabilities:\n$P_H(j) = \\frac{P_w(v_j) + H}{\\sum_{v_i \\in V} P_w(v_i) + H}$,\nwhere\n$H = - \\sum_{v_i \\in V} P_w(v_i) \\log(P_w(v_i))$.\nThe entropy adjustment increases selection diversity by adding the system's uncertainty to each probability.\nThis CMSA variant is henceforth called LLM-CMSA-V2. We directly replaced the generate_solution() function of the original CMSA with the LLM-generated code."}, {"title": "3.4 Code Optimization Strategies", "content": "After discovering the two new versions of CMSA outlined above with the help of the LLM, another idea occurs to Zoe: \"Could the LLM create an improvement at the C++ code level?\" In other words, is there a more efficient way of implementing the new CMSA variants without affecting their logic, by using, for example, more efficient data structures or even low-level techniques? Although Zoe is an expert in C++, she knows that C++ is a very broad language with many features she is unfamiliar with. Perhaps the LLM, with its vast knowledge base, can find a better way to implement the heuristics without altering their logic.\nZoe creates the following prompt and provides it to the LLM, together with the C++ code of LLM-CMSA-V1 and LLM-CMSA-v2:"}, {"title": "3.5 Reproducility", "content": "Although it is not possible to replicate the exact results of an LLM, due to their autoregressive nature that predicts the most probable token based on a probability distribution (with the next token being determined stochastically) [16], it is possible to reproduce similar responses by using the same prompts, the same LLM, and its parameters. For this reason, our repository includes a chatbot that implements the same prompts used in our research (as shown in Figure 3). In fact, each element in Figure 3 (textbox and checkbox) loads pre-built prompts, known as in-context prompts [10, 18], to eliminate the need for manual input. Thus, our chatbot features two types of in-context prompts: (1) external ones, related to the C++ CMSA code for the MIS, and (2) internal ones, focused on improving the heuristic, the C++ code, and specifying which function in the code requires enhancement. Next, we will assess the quality of the heuristics proposed by the LLM."}, {"title": "4 Empirical Evaluation", "content": "This section is divided into two parts: the preliminary phase (setup, benchmark, and CMSA parameter tuning) and the results."}, {"title": "4.1 Preliminary", "content": "First, we used Chatbot Arena [7] to test various LLMs without incurring costs. For our experiments, we finally selected GPT-4o (version: 2024-11-20) as it is one of the top-performing models to date. Experiments concerning algorithm variants CMSA, LLM-CMSA-V1, and LLM-CMSA-V2 were conducted on a cluster equipped with Intel\u00ae Xeon\u00ae CPU 5670 processors (12 cores at 2.933 GHz) and 32 GB of RAM.\nOur benchmark set consists of three types of graphs: Barab\u00e1si-Albert, Watts-Strogatz, and Erd\u0151s-R\u00e9nyi graphs, with four different sizes and four density levels (see Figure 4). For each combination of size and density level, the benchmark set contains 1 tuning instance and 30 testing instances. This makes a total of 48 tuning instances and 1440 testing instances. The parameters of all three CMSA variants (original, V1, V2) were tuned using irace, a tool for tuning algorithm parameters based on problem instances [20]. Due to the lack of space (and not being the important point of this paper), we do not report the final values here. We used 150, 300, 450, and 600 CPU seconds as computation time limits for graphs of the four different sizes."}, {"title": "4.2 Numerical Results", "content": "In addition to the three tuned algorithms, we also tested two variants obtained by using the efficiency-improved C++ codes described in Section 3.4. These two variants are henceforth called LLM-CMSA-V1-PERF and LLM-CMSA-V2-PERF."}, {"title": "5 Discussion", "content": "Our study has the following limitations: due to space constraints, we tested only one LLM (GPT-4o) for CMSA im- provements, and future work could benefit from comparing additional LLMs, especially open-weight ones. Additionally, we did not explore using GPT-4o to enhance other complex algorithms for additional optimization problems. These limitations could be addressed in an extended article. Despite these limitations, our comprehensive analysis of CMSA with new heuristics for solving MIS instances presents promising opportunities for new lines of research emerging from our work:\n1. Creation of specialized benchmarks. Our experiments focused on a single optimization algorithm (CMSA) applied to one problem (MIS). Just as general-purpose code generation has well-established benchmarks, there is a clear need for specialized benchmarks tailored to optimization. This would enable us to assess which models can effectively 'discover' better heuristics for existing algorithms.\n2. Integration of LLM-based agents. Figure 3 shows a manual interaction with an LLM. However, tasks such as executing code and correcting errors (feedback process) could be delegated to autonomous code agents capable of communicating with each other. Developing a platform where these agents can collaborate on existing optimization algorithms and propose better implementation strategies could lead to significant advancements in the field (see [13, 35]). Some progress has been made in creating a platform that unifies the design of optimization algorithms from scratch (e.g., LLM4AD [19]).\n3. Programming language translation for optimization algorithms. To enhance maintainability and perfor- mance, algorithms may need translation across languages. LLMs can efficiently optimize readability and performance [22, 17], but specialized LLMs (possibly fine-tuned) are required to ensure coherent translations tailored to optimization algorithms.\nAn essential question arises: should the LLM receive credit for discovering a superior heuristic compared to a human expert's best implementation? While prompt design certainly influences outcomes, it is worth thinking about questions concerning authorship, ownership, and AI's role in scientific discovery (see [30])."}, {"title": "6 Conclusions", "content": "In our research, we demonstrate that LLMs can be effectively applied to enhance existing optimization algorithms. We used the non-trivial Construct, Merge, Solve, and Adapt (CMSA) algorithm implemented in C++ to solve the classical Maximum Independent Set problem. By leveraging in-context prompts with GPT-4o, the model successfully captured the context of the CMSA implementation and proposed new heuristics for the probabilistic construction phase of CMSA. After conducting a thorough comparative analysis, the suggested heuristics outperformed those implemented by an expert in CMSA."}]}