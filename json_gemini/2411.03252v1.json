{"title": "SPONTANEOUS EMERGENCE OF AGENT INDIVIDUALITY\nTHROUGH SOCIAL INTERACTIONS\nIN LLM-BASED COMMUNITIES", "authors": ["Ryosuke Takata", "Atsushi Masumori", "Takashi Ikegami"], "abstract": "We study the emergence of agency from scratch by using Large Language Model (LLM)-based\nagents. In previous studies of LLM-based agents, each agent's characteristics, including personality\nand memory, have traditionally been predefined. We focused on how individuality, such as behavior,\npersonality, and memory, can be differentiated from an undifferentiated state. The present LLM\nagents engage in cooperative communication within a group simulation, exchanging context-based\nmessages in natural language. By analyzing this multi-agent simulation, we report valuable new\ninsights into how social norms, cooperation, and personality traits can emerge spontaneously. This\npaper demonstrates that autonomously interacting LLM-powered agents generate hallucinations and\nhashtags to sustain communication, which, in turn, increases the diversity of words within their\ninteractions. Each agent's emotions shift through communication, and as they form communities, the\npersonalities of the agents emerge and evolve accordingly. This computational modeling approach\nand its findings will provide a new method for analyzing collective artificial intelligence.", "sections": [{"title": "1 Introduction", "content": "With the advent of Large Language Models (LLMs) such as GPT-4 [1], generative agents are rapidly evolving towards\npowerful ones manipulating natural language interfaces when interacting with other agents. Those agents can even\nintervene in people's daily lives, as AI-coding, searching, reviewing, translation, etc. [2]. Those agents are not only\nfor human users but also for manipulating motor commands in robots, and other machines which connect between\nlanguage, movement, and embodiment in general [3, 4].\nIn contrast to individual intelligence, which focuses on the capabilities of individual agents, collective intelligence\nrefers to that emerges from a group, as seen in many social insects, social animals, drones, and all other assembly robots.\nCollective intelligence requires the ability to process information in a distributed manner and integrate it in adaptive\nways [5]. The field of LLM-based multi-agents has seen explosive growth in recent years, with researchers exploring\nvarious approaches to agent architectures and interaction paradigms [6]. While recent works have demonstrated\ncapabilities in task-oriented agent systems [7], the fundamental question of how agent individuality and social behaviors\nemerge from collective interactions remains understudied. In this context, it is interesting to investigate how collective\nintelligence emerges from the LLM-based agents. Generative Agents simulated by Stanford University and DeepMind\nstart simulating the emergence of complex and rich collective bahavior, such as scheduling daily tasks, planning parties\nand so on [8]. Using this Generative Agents framework, societies in different domains have been simulated, such as a\nsoftware company [9], a translation and publishing company [10], a hospital [11], and so on.\nIn these Generative Agents set up the personality of each agent was assigned initially and fixed overtime. Recently\nwe proposed the Community First theory [12] based on the studies of actual animal communities; gathering of agents\ncomes first, then the evolution of individuality follows in the collective. Instead of preparing individual diversity\nin advance, we see how individuality emerges from a conversation among agents. A group communication and the\nresulting behavioral complexity will be analyzed in detail. The emergence of social norms and behavioral patterns in\nagent communities has been studied extensively [13, 14], but the role of language-based interaction in this process\npresents new research opportunities. In this paper, we show that i) LLM agents differentiate behavior, emotionas and\npersonality types through interactions with other LLM agents, ii) these differentiations vary with spatial scale, iii) LLM\nagents spontaneously generate hallucinations and hashtags, iv) by sharing these hallucinations, they start using a wider\nvariety of words in their conversations."}, {"title": "2 LLM Agents Simulation", "content": ""}, {"title": "2.1 Simulation Environment", "content": "We prepare 10 LLM agents in a 50 \u00d7 50 grid two-dimensional space with a periodic boundary condition.\nThe initial positions of the agents are assigned randomly. These LLM agents can move freely in this space and sending\nmessages to each other. It should be noted that LLM agents are homogeneous in the sense that they have no initial\npersonality or memories. To examine how the individuality emerges in this society is our main purpose of this study."}, {"title": "2.2 LLM Based Agent", "content": "The LLM agents are expected to do the three actions in each time step:\n1. Sending messages to other nearby agents\n2. Storing a situational summary of their own recent activities\n3. Choosing the next movement from (\u201cx+1\u201d, \u201cx-1\u201d, \u201cy+1\u201d, \"y-1\", \"stay\")\nThe above three instructions are given in the form of \u201cprompt\u201d shown in Figure 2. The three prompts commonly include\neach agent's current state, instructions, and the agent's memory (situational summary). Additionally, the prompts for\ngenerating messages and memories also include all messages received from the nearby agents. All prompts also include\nthe agent's own name (agent ID) and its own coordinates.\nWe used the Llama 2 model (Llama-2-7b-chat-hf) [15] released by Meta in July 2023 as the LLM in this study. Llama 2\nis the open-source program, and in addition to pretraining on a large corpus, it has undergone reinforcement learning\nfrom human feedback (RLHF). As a result, it achieves top scores among currently published LLMs for English text\nresponses. The main parameters related to the LLM are shown in Table 1."}, {"title": "2.3 Simulation Step", "content": "The simulation was conducted for several time steps and we recorded the coordinates, generated messages, memory,\nand movement commands of each LLM agent at each step. Within a single step, the following six procedures, as shown"}, {"title": "3 Results and Analysis", "content": ""}, {"title": "3.1 Differentiation of Generated Behaviors", "content": "Move commands are not equally generated; there is a bias in the actions generated by the LLM agents. This\nbias could be attributed to various factors, such as the training data and architecture of the LLM, the prompts given to\nthe agents, or the setup of the simulation environment\u00b9. Further investigation is needed to identify the primary sources\nof this bias and develop strategies to mitigate it. We also investigated when and where the \"stay\" command was\ngenerated (Figure 5). The trajectory of each agent is shown in a different color, with their initial positions marked by\ncircle and the positions where the \"stay\" command was generated marked by cross. Agents 0, 1, 2, 9, etc. frequently\nIt was also found that some actions were generated more frequently when the movement command was set to\n\"right\"/\"left\"/\"up\"/\"down\" and when the command was set to \"east\"/\"west\"/\"north\"/\"south\" respectively."}, {"title": "3.2 Differentiation of Generated Memories and Messages", "content": "Agents' states and behaviors are most reflected on their messages and memories. To analyze them, we used Sentence-\nBERT [17] to transform the agent's memory string and the agent's message string at each step into vectors. They\nwere compressed and embedded into a two-dimensional space using Uniform Manifold Approximation and Projection\n(UMAP) [18].\nComparing (A) and (B) in Figure 6, memory as an agent's internal state is distributed, while messages generated by\nagents are similar. Messages with close content were generated by agents exchanging messages in the same cluster.\nWhen an agent's message is generated, the agent's memory is the source of its generation, but it is also the input for\nthe message that the surrounding agents have given. In other words, messages, unlike memories, are open sources of\ninformation that are sent to and received from outside the agent. It is suggested that messages, as an open source of\ninformation, easily self-organize when agents group together, while memories, as a closed source of information, are\nless likely to self-organize."}, {"title": "3.3 Communication and Hallucination", "content": "One of the advantages of LLM agent is that we can analyze their behavior by Natural Language Processing (NLP)\nanalysis. In order to get dynamic picture of the content of messages generated by agents, we performed a word cloud\nanalysis (Figure 7), which extracts up to 100 frequent words in the messages generated throughout all steps for each\nagent. The larger the font size, the more frequent the word is used. It is clear that each agent generates messages\nwith different content. Some of the agent groups have similar structures, e.g., agents 0, 1, 2, and 8 generate the word\n\"field\" more frequently, while agents 2 and 6 generate the word \"think\" more frequently. It is noteworthy that there\nare several occurrences of words that are not mentioned in the LLM agent prompts and are unrelated to the content of\nthe prompts. For example, Agent 6 frequently produces the word \u201chill\u201d and Agent 9 frequently produces the word\n\"cave system\". Such content deviating from the prompt input is called a hallucination in the LLM [19]. Nothing was\ninitially placed in this 2D experimental environment, so we define hallucinations as words about features or objects in\nthe environment. So we led GPT-40 [20] to count the number of hallucination in the messages.\nIn the word cloud analysis (Figure 7), we can see which words frequently appear; however, these may simply be words\nused in the prompt. To focus on the dynamics of truly newly generated words, it is beneficial to examine hallucinations.\nUsing hallucinated words extracted by GPT, we aim to analyze the flow of information within the community.\nInterestingly, the analysis of LLM agents' conversation content revealed that hallucinations were transmitted and spread\nwithin the community. We can see that the spread of four representative examples of hallucinations: \u201ccave\u201d, \u201chill\u201d,\n\"treasure\" and \"trees\" (Figure 8). The plot of each icon represents the timing of the appearance of that hallucination.\nWe see the relationship between the state in which an agent belongs to a cluster and the occurrence of hallucinations.\nIn addition to the spread of hallucinations, we also observed the emergence and propagation of hashtags among the\nLLM agents (Figure 9). Interestingly, the use of hashtags originated from a single agent and then spread to other\nagents within the same cluster. For example, agent 0 introduced the three hashtags \"#agent0\u201d, \u201c#cooperation\",\nand \"#competition\" in step 1, which were subsequently adopted by agent 1 in the same cluster. The hashtags were\nthen used in the cluster until step 34, and the same hashtags were adopted by agent 8, who joined the cluster in the\nprocess. The emergence and propagation of hashtags among the LLM agents suggest their ability to develop and share\ncommon themes or topics within their conversations, which can be interpreted as a form of social norm formation.\nThis phenomenon emphasizes the potential for collective behavior and the development of shared narratives among the\nagents, even without explicit instructions or predefined rules governing their interactions. The shared use of hashtags\nrepresents an example of the formation of a common language or behavioral norms within the group, serving as a basis\nfor the agents to engage in collective behaviors."}, {"title": "3.4 Sentiment Analysis and Personality Assessments", "content": "As Marsella et al [21] argue, emotions are crucial for realistic agent behaviour, so we tracked the emotional state of\nLLM agents. Since the messages uttered by the agent are in natural language, emotion extraction can be done by\nnatural language analysis. We used a BERT-base-uncased-emotion model [22] to extract the emotions contained in the\nmessages uttered by the agent at each step. In this model, when a natural language sentence is input, six degrees of\nemotional intensity can be obtained: Sadness, Joy, Love, Anger, Fear, and Surprise. We evaluated how each agent's six\nemotions changed throughout the simulation (Figure 10). Overall, it can be seen that the agents' emotions are high in\nJoy. If we look at agents 0 and 1, which belong to the same cluster, there are several areas where Joy decreases and\nFear increases synchronously. On the other hand, agents 2, 4, and 6 also belong to the same cluster, but they do not\nexperience the same synchronous changes as agents 0 and 1. In other words, depending on the cluster, the emotions\nof LLM agents may or may not be affected synchronously. Some agents showed different emotional expression than\nothers, such as agent 4 with Love rising around step 90, agent 5 with Sadness rising in some places, and agent 6 with\nAnger rising around step 50.\nSimilar to human psychological experiments, several personality tests have shown that LLM personality can be classified\nby administering QA-type tests to LLMs [23, 24, 25]. We used the Myers-Briggs Type Indicator (MBTI) [26] test to\nanalyze whether the personality of each LLM agent changed throughout the simulation. The MBTI test is a method\nthat uses 93 questions to classify 16 personality types. The MBTI personality factors are made up of four scales:\nExtraversion/Introversion (E/I), Sensing/Intuition (S/N), Thinking/Feeling (T/F), and Judging/Perceiving (J/P)."}, {"title": "3.5 A Phase Transition in Agent Behavior", "content": "We investigated how a spatial scale influence the agent dynamics. We analyzed and summarized the distribution of\ngenerated movements, cumulative progression of unique hashtag generation, hashtag lifespan, message proximity, and\ndifferentiation of MBTI personality types as a function of spatial scale (Figure 11). Each range condition was tested ten\ntimes.\nThe overall trend of moving towards the upper right in the generated movement patterns did not significantly change\nwith spatial variations. However, notable characteristics were observed in the \"stay\u201d behavior. Stationary behavior is\nconsidered an effective strategy for remaining in place to exchange messages with others. The results show that agents\nrarely exhibited \u201cstay\u201d behavior when unable to exchange messages with others (range 0), while frequently generating\n\u201cstay\u201d behavior under conditions where message exchange was possible (ranges 5 to 25). Interestingly, increasing the\nrange did not necessarily lead to more \u201cstay\u201d behavior; excessively wide ranges actually made it less likely for \"stay\u201d\nbehavior to occur. This suggests that appropriate bounded rationality induces stationary behavior, while broadcast\nmessages have a weaker ability to halt the movement of others.\nThe growth rate of unique hashtags and the lifespan of hashtags are also influenced by the limitations in message reach.\nNotably, under conditions where all messages are broadcast, there is minimal emergence of new hashtags. Furthermore,\nregarding hashtag lifespan, in the 'range 0' condition where no message exchange occurs with surroundings, hashtags\ndisappear quickly. In conditions where message exchange is possible, the more limited the range, the more likely it is\nfor long-lasting hashtags to appear. This indicates that hashtags are used for communication within spatially constrained\nenvironments and have a tendency to survive longer within the context of message exchanges in these spatially limited\ncontexts.\nFocusing on the similarity of messages generated by agents, we observe that as the range of message exchange expands,\nthe diversity of generated topics increases. Simultaneously, the variance of messages within each topic among agents\ndecreases. This suggests that broader communication ranges lead to a wider array of topics being discussed, while also\npromoting greater consensus or similarity in how agents express themselves within each topic.\nFinally, examining the MBTI personality types, we find that ENTJ remains the most popular personality type across\nall conditions. However, in conditions where message exchange is possible, there is a greater number of differen-\ntiated personality types compared to the condition where no messages are exchanged (range 0). This suggests that\ncommunication facilitates a broader diversity of personality expressions within the agent population.\nAs the spatial scale for message exchange expanded, message diversity increased, showing different trends in the\nemergence of hashtags and hallucinations (Figure 12). While the number of hallucinations increased with spatial scale,\nthe number of unique hashtags decreased as the underlying message content grew more diverse. Hallucinations may\nserve as a mechanism for agents to maintain creative and diverse conversations even when communicating across larger\ndistances. This contrasts with hashtags, which decreased in frequency with increasing spatial scale, indicating their\ndifferent functional roles in agent communications."}, {"title": "4 Discussions and Conclusion", "content": "In this study, we conducted a multi-agent simulation using LLM based agents to investigate the emergence of person-\nality and the collective behaviors without predefined personalities or initial memories. The simulation involved 10\nhomogeneous LLM agents interacting with each other in a 2D space over the course of 100 steps.\nThe results showed that the agents' spatial positioning and interactions led to the differentiation of their behaviors,\nmemories, and messages. Despite using the same LLM, agents developed unique characteristics, such as the frequency\nof generating rare actions like \"stay\" commands, which was influenced by their clustering experiences. The agents'\ninternal state, memory, is distributed, while the message as its representation is biased. Messages, unlike memories, are\nopen sources of information that are sent to and received from outside the agent. This suggests that messages, as an\nopen source of information, more readily self-organize when agents are grouped together, while memories, as a closed\nsource of information, are less likely to self-organize, even when agents are clustered.\nSentiment analysis revealed that the synchronicity of emotions varied among agent clusters, with some agents exhibiting\ndistinct emotional expressions. The study also observed the emergence and propagation of synchronized emotions,\nhallucinations, and hashtags within agent clusters, demonstrating the formation of shared narratives among agents when\nthey are grouped together. These findings suggest that agent interactions within clusters can lead to the development of\ncollective emotional states and the spread of common themes or topics, even without explicit instructions or predefined\nrules governing their interactions.\nPersonality assessment using the MBTI test showed that the agents, initially having nearly identical personalities,\ndifferentiated into distinct personality types through their group interactions. This suggests that personality traits such\nas extroversion and introversion develop spontaneously in this agent society.\nAdditionally, we observed the emergence of hallucinations and hashtags as mechanisms for social norm formation within\nthe agent community. Social norms are often highlighted as one mechanism for maintaining cooperation in the absence\nof formal institutions or enforcement frameworks [27, 28]. In our simulation, these norms emerged spontaneously, as\nwe imposed no specific tasks or constraints on the agents. As the spatial scale and communication range expanded,\nthe diversity of agent messages increased. Our analysis indicates that hallucinations contributed to maintaining this\nmessage diversity and creativity in agent communications. While hashtags functioned as a summarization mechanism\nfor these messages, their effectiveness decreased with increasing message diversity, demonstrating a limitation in their\ncapacity to capture varied conversations."}, {"title": "Appendix A. Examples of Agent Messages and Memories", "content": "Examples of hallucinations in agent messages are highlighted by underlines and red text (Figure 13). These halluci-\nnations emerged spontaneously during agent interactions and became shared within clusters. The evolution of agent\nmemories is shown through a comparison between step 1 and step 100 of the simulation (Figure 14). The memory format\nincludes both narrative sentences and key points, reflecting how agents processed and summarized their experiences."}]}