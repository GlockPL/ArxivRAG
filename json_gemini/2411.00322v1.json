{"title": "Constant Acceleration Flow", "authors": ["Dogyun Park", "Taehoon Lee", "Sojin Lee", "Youngjoon Hong", "Sihyeon Kim", "Hyunwoo J. Kim"], "abstract": "Rectified flow and reflow procedures have significantly advanced fast generation by progressively straightening ordinary differential equation (ODE) flows. They operate under the assumption that image and noise pairs, known as couplings, can be approximated by straight trajectories with constant velocity. However, we observe that modeling with constant velocity and using reflow procedures have limitations in accurately learning straight trajectories between pairs, resulting in suboptimal performance in few-step generation. To address these limitations, we introduce Constant Acceleration Flow (CAF), a novel framework based on a simple constant acceleration equation. CAF introduces acceleration as an additional learnable variable, allowing for more expressive and accurate estimation of the ODE flow. Moreover, we propose two techniques to further improve estimation accuracy: initial velocity conditioning for the acceleration model and a reflow process for the initial velocity. Our comprehensive studies on toy datasets, CIFAR-10, and ImageNet 64\u00d764 demonstrate that CAF outperforms state-of-the-art baselines for one-step generation. We also show that CAF dramatically improves few-step coupling preservation and inversion over Rectified flow.", "sections": [{"title": "Introduction", "content": "Diffusion models [1, 2] learn the probability flow between a target data distribution and a simple Gaussian distribution through an iterative process. Starting from Gaussian noise, they gradually denoise to approximate the target distribution via a series of learned local transformations. Due to their superior generative capabilities compared to other models such as GANs and VAEs, diffusion models have become the go-to choice for high-quality image generation. However, their multi-step generation process entails slow generation and imposes a significant computational burden. To address this issue, two main approaches have been proposed: distillation models [3, 4, 5, 6, 7, 8, 9] and methods that simplify the flow trajectories [10, 11, 12, 13, 14] to achieve fewer-step generation. An example of the latter is rectified flow [10, 11, 13], which focuses on straightening ordinary differential equation (ODE) trajectories. Through repeated applications of the rectification process, called reflow, the trajectories become progressively straighter by addressing the flow crossing problem. Straighter flows reduce discretization errors, enabling fewer steps in the numerical solution and, thus, faster generation.\nRectified flow [10, 13] defines the straight ODE flow over time t with a drift force v, where each sample $x_t$ transforms from $x_0 \\sim \\pi_0$ to $x_1 \\sim \\pi_1$ under a constant velocity $v = x_1 - x_0$. It"}, {"title": "Related work", "content": "Generative models. Learning generative models involves finding a nonlinear transformation be-tween two distributions, typically denoted as $\u03c0_0$ and $\u03c0_1$, where $\u03c0_0$ is a simple distribution like a Gaussian, and $\u03c0_1$ is the complex data distribution. Various approaches have been developed to achieve this transformation. For example, variational autoencoders (VAE) [16, 17] optimize the Evidence Lower Bound (ELBO) to learn a nonlinear mapping from the latent space distribution $\u03c0_0$ to the data distribution $\u03c0_1$. Normalizing flows [18, 19, 20] construct a series of invertible and differentiable map-pings to transform $\u03c0_0$ into $\u03c0_1$. Similarly, GANs [21, 22, 23, 24, 25] earn a generator that transforms $\u03c0_0$ into $\u03c0_1$ through an adversarial process involving a discriminator. These models typically perform a one-step generation from $\u03c0_0$ to $\u03c0_1$. In contrast, diffusion models [2, 26, 27, 28, 29, 30] propose learning the probability flow between the two distributions through an iterative process. This iterative process ensures stability and precision, as the model incrementally learns to reverse a diffusion process that adds noise to data. Diffusion models have demonstrated superior performance across various domains, including images [12, 31, 32, 33], 3D [34, 35, 36, 37], and video [38, 39, 40].\nFew-step diffusion models Addressing the slow generation speed of diffusion models has become a major focus in recent research: Distillation methods [3, 4, 5, 6, 7, 8, 9] seek to optimize the inference steps of pre-trained diffusion models by amortizing the integration of ODE flow. Consistency models [6, 7, 8] train a model to map any point on the pre-trained diffusion trajectory back to the data distribution, enabling fast generation. Rectified flow [10, 11, 13] is another direction, which focuses on straightening ODE trajectories under a constant velocity field. By straightening the flow and reducing path complexity, it allows for fast generation through efficient and accurate numerical solutions with fewer Euler steps. Recent methods such as AGM [41] also introduce acceleration modeling based on Stochastic Optimal Control (SOC) theory instead of relying solely on velocity. However, AGM predicts time-varying acceleration, which still requires multiple iterative steps to solve the differential equations. In contrast, our proposed CAF ODE assumes that the acceleration term is constant with respect to time. Therefore, there is no need to iteratively solve complex time-dependent differential equations. This simplification allows for a direct closed-form solution that supports efficient and accurate sampling in just a few steps."}, {"title": "Preliminary", "content": "Rectified flow [10, 13] is an ordinary differential equation-based framework for learning a mapping between two distributions $\u03c0_0$ and $\u03c0_1$. Typically, in image generation, $\u03c0_0$ is a simple tractable distribution, e.g., the standard normal distribution, defined in the latent space and $\u03c0_1$ is the image distribution. Given empirical observations of $x_0 \\sim \u03c0_0$ and $x_1 \\sim \u03c0_1$ over time $t \u2208 [0, 1]$, a flow is defined as\n$\\frac{dx_t}{dt} = v(x_t, t), \\qquad (1)$"}, {"title": "Method", "content": "We aim to develop a generative model based on the ODE framework that enables faster generation without compromising quality. To achieve this, we propose a novel approach called Constant Acceleration Flow (CAF). Specifically, CAF formulates an ODE trajectory that transports $x_t$ with a constant acceleration, offering a more expressive and precise estimation of the ODE flow compared to constant velocity models. Additionally, we propose two novel techniques that address the problem of flow crossing: 1) initial velocity conditioning and 2) reflow procedure for learning initial velocity. The overall training pipeline is presented in Alg. 1.\nWe propose a novel ODE framework based on the constant acceleration equation, which is driven by the empirical observations $x_0 \\sim \u03c0_0$ and $x_1 \\sim \u03c0_1$ over time $t \u2208 [0, 1]$ as:\n$\\frac{dx_t}{dt} = v(x_0, 0)dt + a(x_t, t)tdt, \\qquad (4)$\nwhere $v : \\mathbb{R}^d \u00d7 [0] \u2192 \\mathbb{R}^d$ is the initial velocity field and $a : \\mathbb{R}^d \u00d7 [0,1] \u2192 \\mathbb{R}^d$ is the acceleration field. We abbreviate time variable $t$ for notation simplicity, i.e., $v(x_0, 0) = v(x_0), a(x_t, t) = a(x_t)$. By integrating both sides of (4) with respect to t and assuming a constant acceleration field, i.e., $a(x_{t_1}) = a(x_{t_2}), \u2200t_1, t_2 \u2208 [0, 1]$, we derive the following equation:\n$x_t = x_0 + v(x_0)t + \\frac{1}{2}a(x_t)t^2. \\qquad (5)$\nGiven the initial velocity field v, the acceleration field a can be derived as\n$a(x_t) = 2(x_1 - x_0) \u2013 2v(x_0), \\qquad (6)$\nby setting t = 1 and the constant acceleration assumption. Then, we propose a time-differentiable interpolation I as:\n$x_t = I(x_0, x_1, t, v(x_0)) = (1 - t^2)x_0 + t^2x_1 + v(x_0)(t \u2212 t^2), \\qquad (7)$\nby substituting (6) to (5). Using this result, we can easily simulate an intermediate sample $x_t$ on our CAF ODE trajectory.\nLearning initial velocity field. Selecting an appropriate initial velocity field is crucial, as different initial velocities lead to distinct flow dynamics. Here, we define the initial velocity field as a scaled displacement vector between $x_1$ and $x_0$:\n$v(x_0) = h(x_1 \u2013 x_0), \\qquad (8)$\nwhere $h \u2208 \\mathbb{R}$ is a hyperparameter that adjusts the scale of the initial velocity. This configuration enables straight ODE trajectories between distributions $\u03c0_0$ and $\u03c0_1$, similar to those in Rectified flow. However, varying h changes the flow characteristics: 1) h = 1 simulates constant velocity flows, 2) h < 1 leads to a model with a positive acceleration, and 3) h > 1 results in a negative acceleration, as illustrated in Fig. 3. Empirically, we observe that the negative acceleration model is more effective for image sampling, possibly due to its ability to finely tune step sizes near data distribution.\nThe initial velocity field is learned using a neural network $v_\u03b8$, which is optimized by minimizing the distance metric $d(\u00b7, \u00b7)$ between the target and estimated velocities as\n$\\min_\\theta E_{x_0,x_1\u223c\u03b3,t\u223cp(t),x_t\u223cI} [d(v(x_0), v_\u03b8(x_t))], \\qquad (9)$\nwhere p(t) is a time distribution defined on [0, 1]. Note that our velocity model learns target initial velocity defined at t = 0. This differs from Rectified flow, which learns target velocity field defined over $t \u2208 [0, 1]$.\nLearning acceleration field. Under the assumption of constant acceleration, the acceleration field is derived from (6) as\n$a(x_t) = 2(x_1 - x_0) \u2013 2v(x_0). \\qquad (10)$\nWe learn the acceleration field using a neural network $a_\u03c6$ by minimizing the distance metric $d(\u00b7, \u00b7)$ as:\n$\\min_\u03a6 E_{x_0,x_1\u223c\u03b3,t\u223cp(t),x_t\u223cI} [d(a(x_t), a_\u03c6(x_t))]. \\qquad (11)$"}, {"title": "Addressing flow crossing", "content": "Rectified flow addresses the issue of flow crossing by a reflow procedure. However, even after the procedure, trajectories may still intersect each other. Such intersections hinder learning straight ODE trajectories, as demonstrated in Fig. 1a. Similarly, our acceleration model also encounters the flow crossing problem. This leads to inaccurate estimation, as the model struggles to predict estimation on these intersections correctly. To further address the flow crossing, we propose two techniques.\nWe propose conditioning the estimated initial velocity $v_\u03b8 = v(x_0)$ as the input of the acceleration model, i.e., $a_\u03c6(x_t, v_\u03b8)$. This approach provides the acceleration model with auxiliary information on the flow direction, enhancing its capability to distinguish correct estimations and mitigate ambiguity at the intersections of trajectories, as illustrated in Fig. 1. Our IVC circumvents the non-intersecting condition required in Rectified flow (see Theorem 3.6 in [10]), which is a key assumption for achieving a straight coupling $\u03b3$. By reducing the ambiguity arising from intersections, CAF can learn straight trajectories with less constrained couplings, which is quantitatively assessed in Tab. 4.\nTo incorporate IVC into learning the acceleration model, we reformulate (11) as:\n$\\min_\u03a6 E_{x_0,x_1\u223c\u03b3,t\u223cp(t),x_t\u223cI} [d(sg[a(x_t)], a_\u03c6(x_t, v_\u03b8))]. \\qquad (12)$\nwhere sg[] indicates stop-gradient operation. Since our velocity model learns to predict the initial velocity (see (9)), we ensure that the model can handle both forward and reverse CAF ODEs, which start from $x_0$ and $x_1$, respectively. Thus, our acceleration model can generalize across different flow directions, enabling inversion as demonstrated in Sec. B.2.\nIt is also important to improve the accuracy of the initial velocity model. Following [10], we address the inaccuracy caused by stochastic pairing of $x_0$ and $x_1$ by employing a pre-trained generative model $\u03c8$, which constructs a more deterministic coupling $\\tilde{\u03b3}$ of $x_0$ and $x_1$. We subsequently use this new coupling $\\tilde{\u03b3}$ to train the initial velocity and acceleration models."}, {"title": "Sampling", "content": "After training the initial velocity and acceleration models, we generate samples using the CAF ODE introduced in (4). The discrete sampling process is given by:\n$x_{t+\u0394t} = x_t + \u2206t \u00b7 v_\u03b8(x_0) + t' \u00b7 \u2206t \u00b7 a_\u03c6(x_t, t, v_\u03b8(x_0)), \\qquad (13)$\nwhere N is the total number of steps, $\u2206t = \\frac{1}{N}$, $t = i\u00b7\u2206t$, and $t' = \\frac{(2i+1)}{2N}$. at where $i \u2208 {0,..., N\u22121}$"}, {"title": "Experiment", "content": "We evaluate the proposed Constant Acceleration Flow (CAF) across various scenarios, including both synthetic and real-world datasets. In Sec. 5.1, our investigation begins with a simple two-dimensional synthetic dataset, where we compare the performance of Rectified flow and CAF to clearly demonstrate the effectiveness of our model. Next, we extend our experiments to real-world image datasets, specifically CIFAR-10 (32\u00d732) and ImageNet (64\u00d764), in Sec. 5.2. These experiments highlight CAF's ability to generate high-quality images with a single sampling step. Furthermore, we conduct an in-depth analysis of CAF through evaluations of coupling preservation, straightness, inversion tasks, and an ablation study in Sec. 5.3."}, {"title": "Synthetic experiments", "content": "We demonstrate the advantages of the Constant Acceleration Flow (CAF) over the constant velocity flow model, Rectified Flow [10], through synthetic experiments. For the neural networks, we use multilayer perceptrons (MLPs) with five hidden layers and 128 units per layer. Initially, we train 1-Rectified flow on 2D synthetic data to establish a deterministic coupling. We then train both CAF and 2-Rectified flow. For CAF, we incorporate the initial velocity into the acceleration model by concatenating it with the input, ensuring that the model capacities of both CAF and 2-Rectified flow remain comparable. We set d as l2 distance. Fig. 2 presents samples generated from CAF in one step and from 2-Rectified flow in two steps. Our CAF more accurately approximates the target distribution $\u03c0_1$ than 2-Rectified flow. In particular, CAF with h = 2 (negative acceleration) learns the most accurate distribution. In contrast, 2-Rectified flow frequently generates samples that significantly deviate from $\u03c0_1$, indicating its difficulty in accurately estimating straight ODE trajectories. This experiment shows that reflowing alone may not overcome the flow crossing problem, leading to poor estimations, whereas our proposed acceleration modeling and IVC effectively address this issue. Moreover, Fig. 3 shows sampling trajectories from CAF trained with different hyperparameters h. It clearly demonstrates that h controls the flow dynamics as we intended: h > 1 indicates negative acceleration, h = 1 represents constant velocity, and h < 1 corresponds to positive acceleration flows. Additional synthetic examples are provided in Fig. 6."}, {"title": "Real-data experiments", "content": "To further validate the effectiveness of our approach, we train CAF on real-world image datasets, specifically CIFAR-10 at 32\u00d732 resolution and ImageNet at 64\u00d764 resolution. To create a determin-istic coupling $\u03b3$, we utilize the pre-trained EDM models [29] and adopt the U-Net architecture of ADM [30] for the initial velocity and acceleration models. In the acceleration model, we double the input dimension of first layer to concatenate the initial velocity to the input $x_t$ of the accelera-tion model, which marginally increases the total number of parameters. We set h = 1.5 and d as LPIPS-Huber loss [43] for all real-data experiments."}, {"title": "Coupling preservation", "content": "We evaluate how accurately CAF and Rectified flow approximate the deterministic coupling obtained from pre-trained models via a reflow procedure. To analyze this, we first conduct synthetic experiments where the interpolant paths I are crossed, as illustrated in Fig. 5a. Due to the flow crossing, the sampling trajectory of Rectified flow fails to preserve the ground-truth coupling (interpolation path I), leading to a curved sampling trajectory. In contrast, our CAF learns the straight interpolation paths by incorporating acceleration, demonstrating superior coupling preservation ability.\nMoreover, we evaluate the coupling preservation ability on real data from CIFAR-10. We randomly sample 1K training pairs (x0,x1) from the deterministic coupling $\u03b3$ and measure the similarity between x1 and $x_1$, where $x_1$ is a generated sample from x0. In other words, we measure the distance between a ground truth image and a generated image corresponding to the same noise. If the coupling is well-preserved, the distance should be small. We use PSNR and LPIPS [52] as distance measures. The result in Tab. 3 demonstrates that CAF better preserves coupling. In terms of PSNR, CAF outperforms Rectified flow by 3.37. This is consistent with the qualitative result in Fig. 5b, where $x_1$ from CAF resembles more to x1 (ground truth) than $x_1$ from Rectified flow."}, {"title": "Flow straightness", "content": "To evaluate the straightness of learned trajectories, we introduce the Normalized Flow Straightness Score (NFSS). Similar to previous works [10, 11], we measure flow straightness $S$ by the $L_2$distance between the normalized displacement vector $(x_0 - x_1)$ and the normalized velocity vector $x_t$ as below:\n$S = E_{x_0,x_1,t} \\left| \\left| \\frac{x_1 - x_0}{||x_1 - x_0||_2} - \\frac{x_t}{||t||_2} \\right| \\right|_2^2 . \\qquad (14)$\nHere, a smaller value of $S$ indicates a straighter trajectory. We compare $S$ between CAF and Rectified flow using synthetic and real-world datasets, as presented in Tab. 4. For Rectified flow, we use $x_t = v_\u03b8(x_t)$, while for CAF, we use $x_t = v_\u03b8(x_0) + a_\u03c6(x_t)t$. The results show that CAF outperforms Rectified flow in flow straightness."}, {"title": "Inversion", "content": "We further demonstrate CAF's capability in real-world applications by conducting zero-shot tasks such as reconstruction and box inpainting using inversion. We provide implemenetation details and algorithms in Sec. B.2. As shown in the Tab. 6 and 7, our method achieves lower reconstruction errors (CAF: 46.68 PSNR vs. RF: 33.34 PSNR) and better zero-shot inpainting capabilities even with fewer steps compared to baselines. These improvements are attributed to CAF's superior coupling preservation capability. Moreover, we present qualitative comparisons between CAF and the baselines in Fig. 12 and 13, which further validates the quantitative results."}, {"title": "Ablation study", "content": "We conduct an ablation study to evaluate the effectiveness of components in our framework under the one-step generation setting (N = 1). We examine the improvements achieved by 1) constant acceleration modeling, 2) initial velocity (v\u03b8) conditioning, and 3) the reflow procedure for v\u03b8. The configurations and results are outlined in Tab. 5. Specifically, A and B correspond to 1-Rectified flow and 2-Rectified flow, respectively. Configurations C to F represent our CAF frameworks, with C being our CAF without IVC. By comparing A,B,C, and F, we demonstrate that all three components in our framework substantially improve the performance. In addition, we analyze the final model across various acceleration scales controlled by h. The performance difference between D and F is relatively small, indicating that our framework is robust to the choice of hyperparameters. Empirically, we observe that configuration F, i.e., CAF (h = 1.5) with negative acceleration, achieves the best FID of 2.68. Notably, our CAF without v\u03b8 conditioning, still outperforms rectified flow (configuration B) by 3.06 FID. This highlights the critical role of constant acceleration modeling in enhancing the quality of few-step generation. Also, we verify the significance of reflowing by comparing configurations A and B, which achieve 378 FID and 6.88 FID, respectively."}, {"title": "Conclusion", "content": "In this paper, we have introduced the Constant Acceleration Flow (CAF) framework, which enhances precise ODE trajectory estimation by incorporating a controllable acceleration variable into the ODE framework. To address the flow crossing problem, we proposed two strategies: initial velocity conditioning and a reflow procedure. Our experiments on toy datasets, real-world dataset demonstrate CAF's capabilities and scalability, achieving state-of-the-art FID scores. Furthermore, we conducted extensive ablation studies and analyses\u2014including assessments of flow straightness, coupling preser-vation, and real-world applications\u2014to validate and deepen our understanding of the effectiveness of our proposed components in learning accurate ODE trajectories. We believe that CAF offers a promising direction for efficient and accurate generative modeling, and we look forward to exploring its applications in more diverse settings such as 3D and video."}, {"title": "Implementation details", "content": "We utilize the pre-trained EDM model [29] to build the deterministic coupling \u03b3 for training our models. To construct deterministic couplings for CIFAR-10 and ImageNet, we select N = 18 and N = 40, respectively, using deterministic sampling following the protocol in [29]. For CIFAR-10 and ImageNet, we generate 1M and 3M pairs, respectively. We use the batch size of 2048 and train for 700K/700K iterations on ImageNet. For CIFAR-10, we use the batch size of 512 and train for 500K/500K iterations. For all experiments, we use AdamW [53] optimizer with a learning rate of 0.0001 and apply an Exponential Moving Average (EMA) with a 0.999 decay rate. For training acceleration model, we initialize it with initial velocity model for faster convergence.\nFor adversarial training, we employ adversarial loss $L_{gan}$ using real data $x_{1, real}$ from [24]:\n$L_{gan,\u03c6}(\u03c6) = E_{x_{1, real}} [log d_\u03b7(x_{1, real})] + E_{x_0} [log(1 - d_\u03b7(\u03b3(x_0)))], \\qquad (15)$\nwhere $d_\u03b7$ is a discriminator and $x_1 = x_0 + v_\u03b8(x_0) + a_\u03c6(x_0, v_\u03b8(x_0))$. In the end, we use the following combined loss to update the acceleration model:\n$L(\u03c6, \u03b7) = L_{acc}(\u03c6) + \u03bb_{gan}L_{gan}(\u03c6, \u03b7), \\qquad (16)$\nwhere $L_{acc}$ corresponds to (12) and $\u03bb$ are weight hyperparameters. Following [42, 54], we employ adaptive weighting as $\u03bb_{gan} = \\frac{||\u2207_\u03c6L_{acc}(\u03c6)||}{||\u2207_\u03c6L_{gan}(\u03c6,\u03b7)||} $, where $\u03c6_L$ is the last layer of the acceleration model. Without $L_{acc}$, we found the training unstable and frequently exhibit mode collapse issue, which is a common problem with adversarial training. We follow the training configuration from StyleGAN-XL [24]. We bilinearly upscale the image to 224\u00d7224 resolution and use EfficientNet [55] and DeiT-base [56] for extracting features. During the adversarial training, we only optimize the acceleration model and discriminator with the learning rate of 2e-5 and 1e-3, respectively. We keep the parameters of the initial velocity model fixed for stable training. The total training takes about 21 days with 8 NVIDIA A100 GPUs for ImageNet, and takes 10 days 8 NVIDIA RTX3090 GPUs for CIFAR-10."}, {"title": "Additional results", "content": "In Fig. 6, we provide additional generation results and sampling trajectories on various 2D synthetic datasets with N = 1, demonstrating the effectiveness of our approach for fast generation. Fig. 7 provides additional examples of coupling preservation on 2-RF and CAF.\nIn Fig. 8 and 9, we show additional generation results from our base CAF model on CIFAR-10 with N = 1, 10, and 50. In Fig. 10, we compare the generation result between 2-RF and CAF distilled versions. Fig. 11 shows sampling results from our base CAF models with different hyperparameters h. Lastly, Fig. 14 shows the generation results on ImageNet with N = 1."}, {"title": "Real-world applications", "content": "Inversion techniques are essential for real-world applications such as image and video editing [57, 58]. However, existing methods typically require 25-100 steps for accurate inversion, which can be computationally intensive. In contrast, our method significantly reduces the inference time by enabling inversion in just a few steps (e.g., N < 20). We demonstrate this efficiency in two tasks: reconstruction and box inpainting.\nTo reconstruct $x_1$, we first invert $x_1$ to obtain $x_0$, as described in Alg. 3. We then use the generation process (Alg. 2) with $x_0$ and same initial velocity $v_\u03b8(x_1)$ used in Alg. 3 to generate $x_1$. For box inpainting, we inject conditional information\u2014the non-masked image region\u2014into the iterative inversion and generation procedures, as detailed in Alg. 4. As demonstrated in Tab. 6 and 7, our method achieves better reconstruction quality (CAF: 46.68 PSNR vs. RF: 33.34 PSNR) and zero-shot inpainting capability even with fewer steps compared to baseline methods. Qualitative results are presented in Fig. 12 and 13, which further illustrate the effectiveness of our approach. This demonstrate that our method can be efficiently used for real-world applications, offering both speed and accuracy advantages over existing techniques."}, {"title": "Comparison with previous acceleration modeling literatures", "content": "Here, we elaborate on the crucial differences between AGM [41] and CAF. The main distinction is that CAF assumes constant acceleration, whereas AGM predicts time-dependent acceleration. Since the CAF ODE assumes that the acceleration term is constant with time, there is no need to solve time-dependent differential equations iteratively. This allows for a closed-form solution that supports efficient and accurate sampling, given that the learned velocity and acceleration models are accurate. Specifically, the solution for CAF ODE is given by:\n$x_1 = x_0 + \\int_0^1 v(x_0) + a(x_t) t dt = x_0 + v(x_0) \\int_0^1 dt + a(x_t) \\int_0^1 t dt \\qquad (17)$\n$= x_0 + v(x_0) + a(x_t) \\frac{1}{2} \\qquad (18)$\nThe integral simplifies thanks to the constant acceleration assumption, leading to one-step sampling. In contrast, AGM's acceleration is time-varying, meaning that the differential equation cannot be reduced in an analytic form. It requires multiple steps to approximate the true solution accurately. In Tab. 8, we systemically compare AGM with our CAF, where CAF consistently outperforms AGM. Moreover, we conducted additional experiments where AGM was trained with deterministic couplings as in our reflow setting. Incorporating reflow into AGM did not improve its performance in the few-step regime, which further highlights the distinct advantage of CAF over AGM."}, {"title": "Marginal preserving property of Constant Acceleration Flow", "content": "We demonstrate that the flow generated by our Constant Acceleration Flow (CAF) ordinary differential equation (ODE) maintains the marginal of the data distribution, as established by the definitions and theorem in [10].\nDefinition C.1. For a path-wise continuously differentiable process $x = {x_t : t \u2208 [0, 1]}$, we define its expected velocity $v^\u2217$ and acceleration $a^\u2217$ as follow:\n$v^\u2217(x, t) = E[\\frac{dx_t}{dt} | X_t = x], a^\u2217(x, t) = E[\\frac{d^2x_t}{dt^2} | X_t = x], \u2200x \u2208 supp(x_t). \\qquad (19)$\nFor $x \\notin supp(x_t)$, the conditional expectation is not defined and we set $v^\u2217$ and $a^\u2217$ arbitrarily, for example $v^\u2217(x,t) = 0$ and $a^\u2217(x, t) = 0$.\nDefinition C.2. [10] We denote that $x$ is rectifiable if $v^\u2217$ is locally bounded and the solution to the integral equation of the form\n$z_t = z_0 + \\int_0^t v^\u2217(z_t, t)dt, t \u2208 [0,1], z_0 = x_0, \\qquad (20)$\nexists, which is the rectified flow as defined in Definition C.2. In (4), we define the CAF ODE as\nexists and is unique. In this case, $z = {z_t : t \u2208 [0,1]}$ is called the rectified flow induced by $x$. Theorem 1. [10] Assume $x$ is rectifiable and $z$ is its rectified flow. Then $Law(z_t) = Law(x_t), \u2200t \u2208 [0, 1]$.\nRefer to [10] for the proof of Theorem 1.\nWe will now show that our CAF ODE satisfies Theorem 1 by proving that our proposed ODE (4) induces $z$, which is the rectified flow as defined in Definition C.2. In (4), we define the CAF ODE as\n$\\frac{dx_t}{dt} = v(x, 0) + a(x, t)t. \\qquad (21)$\nBy taking the conditional expectation on both sides, we obtain\n$v^\u2217(x, t) = v^\u2217(x, 0) + a^\u2217(x, t)t, \\qquad (22)$\nfrom Definition C.1. Then, the solution of the integral equation of CAF ODE is identical to the solution in Definition C.2 by (22):\n$z_t = z_0 + \\int_0^t v^\u2217(z_0, 0) + a^\u2217(z_t, t) t dt \\qquad (23)$\n$= z_0 + \\int_0^t v^\u2217(z_t, t)dt. \\qquad (24)$\nThis indicates that $z$ induced by CAF ODE is also a rectified flow. Therefore, the CAF ODE satisfies the marginal preserving property, i.e., $Law(z_t) = Law(x_t)$, as stated in Theorem 1."}, {"title": "Limitations", "content": "One limitation of our model is the increased number of function evaluations (NFE) required for N-step generation. While Rectified flow achieves an NFE of N by only computing the velocity at each step, our method necessitates an additional computation, resulting in a total NFE of N + 1. This is because we compute the initial velocity at the beginning and the acceleration at each step. Although this extra evaluation slightly increases the computational burden, it is relatively minor in terms of overall performance and still enables efficient few-step generation. Moreover, this additional step can be reduced by jointly predicting velocity and acceleration terms with a single model, which we leave for future work. Another limitation is the additional effort required to generate supplementary data. We utilize generated data to create a deterministic coupling of noise and data samples for training CAF. While generating more data enhances our model's performance, it can increase GPU usage, leading to higher carbon emissions."}, {"title": "Broader Impacts", "content": "Recent advancements in generative models hold significant potential for societal benefits across a wide array of applications, such as image and video generation and editing, medical imaging analysis, molecular design, and audio synthesis. Our CAF framework contributes to enhancing the efficiency and performance of existing diffusion models, offering promising directions for positive impacts across multiple domains. This suggests that in practical applications, users can utilize generative models more rapidly and accurately, enabling a broad spectrum of activities. However, it is crucial to acknowledge potential risks that must be carefully managed. The increased accessibility of generative models also broadens the potential for misuse. As these technologies become more widespread, the possibility of their exploitation for fraudulent activities, privacy breaches, and criminal behavior increases. It is vital to ensure their ethical and responsible use to prevent negative impacts. Establishing regulated ethical standards for developing and deploying generative AI technologies is necessary to prevent such misuse. Additionally, imposing restricted access protocols or verification systems to trace and authenticate generated contents will help ensure their responsible use."}]}