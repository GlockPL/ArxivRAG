{"title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations", "authors": ["Linke Ouyang", "Yuan Qu", "Hongbin Zhou", "Jiawei Zhu", "Rui Zhang", "Qunshu Lin", "Bin Wang", "Zhiyuan Zhao", "Man Jiang", "Xiaomeng Zhao", "Jin Shi", "Fan Wu", "Pei Chu", "Minghao Liu", "Zhenxiang Li", "Chao Xu", "Bo Zhang", "Botian Shi", "Zhongying Tu", "Conghui He"], "abstract": "Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies. However, current document parsing methods suffer from significant limitations in terms of diversity and comprehensive evaluation. To address these challenges, we introduce OmniDocBench, a novel multi-source benchmark designed to advance automated document content extraction. OmniDocBench includes a meticulously curated and annotated high-quality evaluation dataset comprising nine diverse document types, such as academic papers, textbooks, slides, among others. Our benchmark provides a flexible and comprehensive evaluation framework with 19 layout category labels and 14 attribute labels, enabling multi-level assessments across entire datasets, individual modules, or specific data types. Using OmniDocBench, we perform an exhaustive comparative analysis of existing modular pipelines and multimodal end-to-end methods, highlighting their limitations in handling document diversity and ensuring fair evaluation. OmniDocBench establishes a robust, diverse, and fair evaluation standard for the document content extraction field, offering crucial insights for future advancements and fostering the development of document parsing technologies.", "sections": [{"title": "1. Introduction", "content": "Document parsing is a foundational task in computer vision, focused on accurately extracting content from documents [18, 36, 39, 41, 45]. High-quality document content extraction typically involves the integration of multiple algorithmic modules. Layout detection algorithms identify different content areas on a page, OCR technology converts images of text regions into text, while formula and table recognition models identify specific regions and transform them into corresponding source code. These modules and reading order algorithms form a comprehensive process of converting documents into machine-readable formats.\nWith large models increasingly requiring high-quality data, the importance of document content extraction has become more pronounced. Although vast amounts of data are available online for training, knowledge-rich document data is relatively scarce. Documents such as academic papers and technical reports contain rich structured information that can significantly enhance the knowledge depth of large models. Moreover, the development of retrieval-augmented generation (RAG) [10, 21] technology relies on extracting accurate information from documents to improve the quality and relevance of generated content. Consequently, research in document content extraction has intensified, leading to a series of pipeline-based high-quality document extraction algorithms [36] and the emergence of end-to-end multimodal large model solutions [3, 5, 6, 27, 39, 40, 42]. These methods have significantly improved document content parsing quality, providing robust support for the needs of large models and RAG technology.\nIn analyzing current module-based pipeline and multimodal end-to-end methods, we identified several limitations. For instance, methods like Marker and MinerU, which are mainstream pipeline methods, primarily evaluate individual modules on academic paper data, lacking document diversity and comprehensive evaluation results. Although MinerU considers the generalization of diverse data, it only demonstrates this through a single model and visualization results, lacking overall end-to-end evaluation. Multimodal large model methods [3, 5, 27, 39, 40], while easier to use than pipeline methods, lack performance validation on diverse documents, and some evaluation metrics are in-"}, {"title": "2. Related Work", "content": "Document Content extraction remains a challenging task, and there is yet to emerge a unified benchmark tailored for"}, {"title": "2.1. Traditional Document Content Extraction", "content": "real-world scenarios. Traditional algorithms typically employ multiple expert modules to handle different extraction subtasks, such as document layout detection [11, 16, 32, 46], optical character recognition (OCR) [14, 22, 28, 33, 37], formula recognition [4, 25, 34, 44], and table recognition [15, 17, 22].\nWhile expert models of these subtasks are advancing rapidly, recent work such as Mineru [36] attempts to concatenate multiple expert modules into a pipeline and provides a high-precision open-source solution for document content extraction. READOC [26] also unifies heterogeneous evaluation methods from the perspective of Document Structure Extraction, breaking down texts, images, formulas, tables, and other dimensions for evaluation, thus offering a solution-oriented towards real-world scenarios for DSE tasks. However, due to the complexity of Document data sources and the intricacies of PDF document information, previous efforts still fall short in terms of data diversity, failing to cover the categories users encounter in practical applications. Similarly, there is an issue with the explainability of document parsing."}, {"title": "2.2. VLM-based Document Content Extraction", "content": "The emergence of Vision-Language Models (VLMs) [1, 6, 12, 38] has revolutionized the field of document content extraction. These models leverage multi-modality capability to achieve remarkable performance in document understanding tasks. Document extraction tools powered by VLMs excel at comprehending both visual layouts and textual content, effectively handling complex document structures while capturing rich contextual information. Representative works such as Nougat [5], Vary [40], Fox [27], and GOT [39], along with recent advances [13, 29], demonstrate significant progress in automated document parsing and comprehension. Despite these advances, the field lacks a standardized and unified benchmark for evaluating VLMbased document extraction task. This absence has hindered objective assessment of PDF document processing capabilities and impeded fair comparison across different approaches. To address this limitation, we present OmniDocBench, a comprehensive end-to-end benchmark designed specifically for evaluating VLM-based document parsing in real-world scenarios."}, {"title": "2.3. Benchmark for Document Content Extraction", "content": "An end-to-end benchmark for PDFs can intuitively reflect the effectiveness of PDF extraction tools, which is crucial for their iteration and selection. However, current benchmarks predominantly focus on module-level evaluations; we have listed related benchmarks in Table 1. Additionally, while there are existing end-to-end benchmarks, they lack detailed annotation rules and suffer from insufficient diversity, as well as unreasonable metrics for formula and table evaluations. For example, READOC [26] covers only two types of sources\u2014arXiv and GitHub\u2014and uses EDS [20] and TEDS [47] to compute metrics for formulas and tables, which may lead to inaccuracies CDM [35]. Therefore, there is a need for a more finely annotated, diverse, and reasonably evaluated end-to-end benchmark."}, {"title": "3. OmniDocBench Dataset", "content": "Constructing a diverse and comprehensive document parsing benchmark with precise annotations is a formidable challenge. As illustrated in Figure 2, we have designed a systematic and professional annotation framework for OmniDocBench, encompassing data acquisition, intelligent pre-annotation, and manual refinement. This ensures that OmniDocBench possesses the following key attributes:\n\u2022 Page Diversity. We sourced document pages from a variety of origins to ensure a wide range of document types.\n\u2022 Comprehensive Annotation. We meticulously anno-"}, {"title": "3.1. Data Acquisition", "content": "During the data acquisition phase, we sourced document pages from diverse origins and used clustering algorithms to initially select visually diverse pages, followed by manual annotation of page attributes to finalize the OmniDocBench pages. Specifically, we collected 200,000 initial PDF documents from Common Crawl, Google, Baidu search engines, and internal data. Subsequently, we extracted visual features from these document pages using ResNet-50 and performed clustering using Faiss\u00b9, sampling 6,000 visually diverse pages from 10 cluster centers. Finally, annotators provided page-level attribute annotations, including page type, layout type, and language type, and further balanced the selection to 981 samples for the final dataset. The OmniDocBench dataset includes pages with 9 types of pages, multiple layout categories, and various attribute annotations, covering a wide range of real-world scenarios."}, {"title": "3.2. Data Annotation", "content": "To ensure the comprehensiveness of OmniDocBench's annotations, we conducted detailed annotations for layout detection and content recognition."}, {"title": "3.2.1. Annotation Types", "content": "Layout Detection Annotations: Unlike typical layout detection tasks, OmniDocBench includes four comprehensive types of annotations: (1) Layout Bounding Box Annotations: Locating information for 19 types of regions such as titles, text paragraphs, tables, and images. (2) Layout Attribute Annotations: Detailed attribute annotations for detected boxes, including 3 text box attributes, 6 table at-"}, {"title": "3.2.2. Annotation Process", "content": "For these annotation tasks on diverse pages, we design a standardized process to ensure quality and efficiency, comprising intelligent pre-annotation, annotator correction, and expert quality inspection.\nIntelligent Pre-Annotation. Manually annotating entire documents is time-consuming and costly. To enhance efficiency, we employ state-of-the-art detection and recognition models for pre-annotation of layout detection and content recognition. Specifically, we use fine-tuned LayoutLMv3 [16] for layout detection annotations and PaddleOCR [22], UniMERNet [34], and GPT-40 [2] for text, formula, and table annotations, respectively.\nAnnotator Correction. After layout detection phase, annotators refine the detection boxes and enhance annotations with reading order and affiliation details. Each character is verified to ensure accuracy in content recognition. For complex annotations of tables and formulas, requiring LaTeX and HTML formats, annotators use tools like Tables Generator 2 and latexlive 3 for verification and correction.\nExpert Quality Inspection. Despite thorough annotator corrections, the complexity of formulas and tables may result in residual issues. To address these, we use CDM's rendering techniques to identify unrenderable elements. These are then reviewed and corrected by three researchers to ensure accuracy and fidelity in the final annotations."}, {"title": "3.3. Dataset Statistics", "content": "Page Diversity. OmniDocBench comprises a total of 981 PDF pages across 9 distinct types. Each page is annotated with global attributes, including text language, column layout type, and indicators for blurred scans, watermarks, and colored backgrounds.\nAnnotation Diversity: OmniDocBench contains over 10,0000 annotations for page detection and recognition: (1)"}, {"title": "4. OmniDocBench Evaluation Methodology", "content": "To provide a fair and comprehensive evaluation for various models, we proposed an end-to-end evaluation pipeline consisting of several modules, including extraction, matching algorithm, and metric calculation, as shown in Figure 3. It ensures that OmniDocBench automatically performs unified evaluation on end-to-end DCE tasks, thereby producing reliable and effective evaluation results."}, {"title": "4.1. Extraction", "content": "Preprocessing: The model-generated markdown text should be preprocessed, which includes removing images, eliminating markdown tags at the beginning of the document, and standardizing the number of repeated characters.\nSpecial Component Extraction: Extraction is primarily carried out using regular expression matching. To ensure that the extraction of content does not interfere with each other, it is necessary to follow a specific order. The extraction sequence is as follows: LaTeX tables, HTML ta-"}, {"title": "4.2. Matching Algorithm", "content": "To avoid the impact of paragraph splitting on the final results, we proposed a method, Adjacency Search Match, that merges and splits paragraphs in both GTs and Preds to achieve the best possible match. The specific strategy involves: i) Calculate a metrix of Normalized Edit Distance between GTs and Preds. If the similarity between a Pred and a GT exceeds a specific threshold, they are considered a successful match. ii) For the rest, we apply fuzzy matching to determine whether one string is a subset of another string. If so, we further apply the truncation and merging algorithm which would try to merge adjacent paragraph. This process would continue to merge more paragraph until the Normalized Edit Distance starts to decrease. After this process, the best match will be found for GTs and Preds."}, {"title": "4.3. Metric Calculation", "content": "Ignore Handling: We implement an ignore logic for certain components in PDF page content, meaning they participate in matching but are excluded from metric calculations. This is mainly because of inconsistent output standards among models, which should not affect the validation results. For fairness, we ignore: (1) Headers, footers, page numbers, and page footnotes, which are handled inconsistently by different models. (2) Captions for figures, tables, and footnotes often have uncertain placement, complicating reading order. Additionally, some models embed table captions in HTML or LaTeX tables, while others treat them as plain text."}, {"title": "5. Benchmarks", "content": "The OmniDocBench dataset features comprehensive and precise annotations, allowing for a fair and rigorous comparison of various document content extraction algorithms in real-world scenarios. Based on the distinct characteristics of these algorithms, we categorize document content"}, {"title": "5.1. Component-specific Evaluation Results", "content": "extraction methods into three main classes:\nPipeline Tools. These methods integrate layout detection and various content recognition tasks (such as OCR, table recognition, and formula recognition) into a document parsing pipeline for content extraction. Prominent examples include MinerU [36], Marker [30], and Mathpix 4.\nExpert VLMs. These are large multimodal models specifically trained for document parsing tasks. Representative models include GOT-OCR2.0 [39] and Nougat [5].\nGeneral VLMs. These are general-purpose large multimodal models inherently capable of document parsing. Leading models in this category include GPT-40 [2], Qwen2-VL [38], and InternVL2 [6]."}, {"title": "5.2. End-to-End Evaluation Results", "content": "Utilizing the OmniDocBench dataset and our evaluation framework, we conducted end-to-end assessments of mainstream document parsing methods, evaluating their performance from input PDF images to the resultant document parsing outputs.\nOverall Evaluation Results. As illustrated in Table 2, pipeline tools specifically designed for document parsing, demonstrate superior performance across the board. MinerU and Mathpix achieved the best results for English and Chinese pages, respectively. In contrast, even the best general-purpose Vision Language Models (VLMs), GPT40, exhibits a performance gap compared to these specialized models, especially in Chinese. This trend is evident across sub-tasks like text recognition, formula recognition, and table recognition, where methods tailored for document parsing consistently outperform others. This advantage is largely due to the fine-tuning of these models on large datasets specific to document parsing tasks."}, {"title": "6. Conclusion", "content": "This paper addresses the lack of diverse and realistic benchmarks in document parsing research by introducing OmniDocBench, a dataset featuring a variety of page types with comprehensive annotations, along with a flexible and reliable evaluation framework. OmniDocBench enables systematic and fair assessments of document parsing methods, providing crucial insights for advancing the field. Its task-specific and attribute-level evaluations facilitate targeted model optimization, promoting more robust and effective parsing solutions."}, {"title": "I. More End-to-End Evaluation Results", "content": "Table S1 presents the evaluation results of End2End Tables grouped by Table Attributes. As it shows, most of the models perform better in English Tables rather than Chinese ones. Most models perform relatively poorly with Full Frame and No Frame tables. The accuracy of most models is affected by special conditions. Merged cells and formulas mainly test the breadth of data the model can recognize, while colored backgrounds and table rotation test their robustness. The results show that table rotation significantly impacts the accuracy of all models. Pipeline Tools perform well on more challenging tables, but colored backgrounds can affect recognition accuracy. Several Vision Language Models (VLMs) tend to perform worse on tables with merged cells, but colored backgrounds do not significantly impact table recognition accuracy.\nTable S2 shows the evaluation results of End2End Text blocks grouped by Text Attributes. Almost all models have lower recognition accuracy in Chinese compared to English. Some models, such as MinerU and Marker, experience a further decrease in accuracy when recognizing mixed Chinese and English content. Complex background colors significantly affect the recognition accuracy of pipeline tools, but they have little impact on VLMs."}, {"title": "II. Dataset Statistics and Visualization", "content": "OmniDocBench contains 981 pages, including 9 types of PDF pages, 4 types of layouts, and 3 types of languages. Some pages also include special conditions, such as watermarks. Table S3 and Figure S1 show the number of pages with each page attribute. Figures S3 to S6 are examples of PDF pages with different PDF types, Layout Types, and Special Issues.\nTable S6 and Figure S2 show all annotation categories included in OmniDocBench. All of them are annotated by bounding boxes. There are 15 types of block-level annotations and 4 types of span-level annotations, with span-level annotations nested within the block-level ones. In addition, there are 3 types of annotations marked as page interference information (No.20-22), whose bounding boxes are used to mask the specific regions of the PDF pages to avoid affecting the evaluation results. The recognition annotations are also provided for each annotation category except for Figures. Formulas is written in LaTeX format and Table is annotated in both HTML and LaTeX formats. Others are annotated in plain text.\nFurthermore, the Text Attributes are also annotated for"}, {"title": "III. Model Results Visualization", "content": "Figures S9 to S17 show the examples of Good model outputs and Bad model outputs of Document Parsing among different PDF types. As it shown, different models exhibit varying performance across different PDF types. For example, MinerU detects all handwritten notes as figures, resulting in very low recognition accuracy in Notes. Marker and InternVL2 experience missed detections, leading to lower scores. InternVL2 and Qwen2-VL, in specific PDF types (such as slides or financial reports), tend to merge multicolumn text.\nFigures S18 to $20 show the examples of Good model outputs and Bad model outputs under special issues of the PDF pages. It shows that Marker tends to generate typos when the PDF pages are fuzzy scanned or with watermarks, while GOT-OCR fails to recognize content on pages with colored backgrounds. MinerU performs well under special situations, while Mathpix occasionally generates typos.\nFigures S21 to $24 show examples of Good model outputs and Bad model outputs for PDF pages with different layouts. MinerU has a low reading order score for single-column layouts primarily because most notes are single-column, and MinerU performs poorly in recognizing Notes, leading to a low reading order score accordingly. InternVL2 scores high in Single-Column layouts but scores poorly on Double-Column and Three-Column layouts. It is mainly due to frequent missed content recognition and errors in reading order judgment in multi-column layouts pages. MinerU's reading order and recognition accuracy decrease with complex layouts, primarily because it incorrectly merges multiple columns during recognition."}]}