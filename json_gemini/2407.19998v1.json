{"title": "Do LLMs Really Adapt to Domains? An Ontology Learning Perspective", "authors": ["Huu Tan Mai", "Cuong Xuan Chu", "Heiko Paulheim"], "abstract": "Large Language Models (LLMs) have demonstrated unprecedented prowess across various natural language processing tasks in various application domains. Recent studies show that LLMs can be leveraged to perform lexical semantic tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL). However, it has not effectively been verified whether their success is due to their ability to reason over unstructured or semi-structured data, or their effective learning of linguistic patterns and senses alone. This unresolved question is particularly crucial when dealing with domain-specific data, where the lexical senses and their meaning can completely differ from what a LLM has learned during its training stage. This paper investigates the following question: Do LLMs really adapt to domains and remain consistent in the extraction of structured knowledge, or do they only learn lexical senses instead of reasoning? To answer this question and, we devise a controlled experiment setup that uses WordNet to synthesize parallel corpora, with English and gibberish terms. We examine the differences in the outputs of LLMs for each corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical results show that, while adapting to the gibberish corpora, off-the-shelf LLMs do not consistently reason over semantic relationships between concepts, and instead leverage senses and their frame. However, fine-tuning improves the performance of LLMs on lexical semantic tasks even when the domain-specific terms are arbitrary and unseen during pre-training, hinting at the applicability of pre-trained LLMs for OL.", "sections": [{"title": "1 Introduction", "content": "Knowledge Bases (KB) and ontologies play a key role in structuring and organizing knowledge across domains, and offer powerful solutions to link data that would otherwise remain unstructured (such as text). As of now, many sources of data can be used as ontologies, of varying specificity. For instance, WordNet [17], ConceptNet [14] and WebIsA [23] contain common knowledge, whereas KBs such"}, {"title": "2 Related Work", "content": "Ontology Learning with LLMs. OL is the (semi)automatic acquisition of T-Box and/or A-Box data from various data sources. In the context of this work, we look at OL from unstructured text or semi-structured data such as Knowledge Graphs paired with textual descriptions. More generally, recent studies show that LLMs can be leveraged to perform ontology related tasks, such as OM (Ontology Matching) or OL. For instance, Norouzi et al. [20] use a naive approach that uses ChatGPT for ontology alignment, by providing the entire source and target ontologies. Mateiu et al. [15] use a fine-tuned GPT-3 to convert natural language into OWL Functional Syntax for ontology enrichment. Hertling et al. [9] use few-shot prompting to enhance the performance of open-source LLMs on OM tasks.\nIn LLMs4OL [7], the authors argue that with sufficient formulation, all tasks pertinent to OL fall within one of the three categories: A) Term Typing (determining a generalized type for a lexical term), B) Taxonomy Discovery (determin-"}, {"title": "3 Approach", "content": "Figure 1 illustrates an overview of the pipeline we employ to test the adaptability and generalizability of off-the-shelf LLMs (pretrained, and as is), which includes two main steps: corpus preparation and LLMs evaluation.\nIn particular, we use the English WordNet (2023 Edition) [16] to generate a parallel corpus of terms and definitions in the form of gibberish, which serve as our reference domain-specific setting. The process begins by choosing root concepts (for instance, sweets and desserts) in the WordNet taxonomy, then explores the graph through hyponymy, derivation and other (e.g. topic) relationships across concepts (with a set maximal exploration depth). The explored concepts form a domain in the real WordNet (e.g. sweets domain) that can be used to create a parallel corpus by propagating gibberish representations and definitions. More details about the algorithm can be found in Section 3.1."}, {"title": "3.1 Parallel Corpus Synthesis", "content": "To simulate a domain that is unseen for the LLM, we generate another KG where the domain concepts have gibberish representations and definitions that do not collide (e.g. if \"sugar\" is turned into \"arghl\" then any definition that contains the word \"sugar\" will see it turned into the word \"arghl\" instead). For this purpose, we devise a procedure which includes three steps: concept mining, concept linking and gibberish generation. The code can be found online. 3\nConcept Mining. The concept mining algorithm is a simple Breadth-First Search algorithm, starting from each of the root concepts Co and only going through user-selected relationships (hypernymy, sense derivation, and concept topic). We set a maximal exploration depth D, which is set to 5 during experiments. The explored concepts form a dataset D.\nConcept Linking. The next step of this process is to establish the dependencies between concept definitions and other concepts. For example, if \"sugar\" is mentioned in the definition of concept c, then we will link all the concepts Csugar which have \"sugar\" as a representation with c by introducing a blank node as follows, where {c_id} denotes the WordNet ID of c:\nGibberish Generation. We assume that we have an algorithm T that creates a gibberish representation T(c) from a concept c based on its initial representation, definition and part-of-speech. A concept is fully processed when it has a gibberish definition AND a gibberish representation. A concept is partially processed when it has a gibberish representation (fully processed implies partially processed).\nDenote Do the set of concepts in D that have no internal dependencies (i.e. for any c in Do, the definition of c does not refer to any concept in D), as shown in Figure 2a. We create an initial representation T(c) for any c in Do, and give them a gibberish definition identical to their original one. We will additionally"}, {"title": "3.2 Off-the-Shelf Evaluation Methodology", "content": "For each dataset D, we perform an evaluation by comparing the performance of off-the-shelf LLMs in two different lexical semantic tasks: relation extraction and taxonomy discovery; and compare the outputs coming from two cases: on the original dataset, and its gibberish counterpart.\nRelation Extraction. Given a query concept in D, its lexical senses (i.e. written forms) and its definition, the goal of this task is to extract all relations between the concepts mentioned in the definition, including the query concept. To remain in the frame of ontology learning, we focus only on hypernymy and holonymy relationships. For instance, in Example 1, the extracted relations should be as follows: macaron is a subclass of confection, egg white is a part of macaron, icing sugar is a part of macaron, etc. Likewise, given the gibberish definition in 2 instead, we expect the same relations to be extracted, only with the concept names replaced with their gibberish counterpart. In order to retrieve a prediction, the LLM is prompted to output triples in the form: (A, r, B) where r is either is a subclass of or is a part of.\nTaxonomy Discovery. Given two concepts A and B in D, with their lexical senses and definitions, the goal of this task is to determine whether or not A is a subclass of B. Likewise, we expect that turning the lexical senses and definitions of A and B into gibberish will not change the outcome. WordNet hypernymy relations are used as a ground-truth: the predictions on the real (resp. gibberish counterpart of the) dataset are compared with the real (resp. gibberish coun-terpart of the) ground-truth. Indirect hypernymy relations (obtained with the transitive closure of the relation gwn: hypernym) are also used. Negative examples are produced by corrupting the hypernym once or twice per query (hyponym, subclass of, ?). This classification task can be evaluated with F1-score: a drop in performance should indicate that a LLM relies on the lexical senses to infer taxonomical relationships rather than textual semantic information.\nPrompting. For each task/model configuration, we follow general guidelines for prompting a LLM. (1) The return is in JSON format, (2) Chain-of-Thought (CoT) [30] can be used if it improves the performance of the model, (3) One or few exemplar(s) can be used, with example(s) outside of the dataset, if it improves the performance of the model. In the relation extraction task, for a concept C, its written form Fc, its part-of-speech Pc, and its definition Dc, we construct a prompt p(C) as shown in Listing 1.2. In the taxonomy discovery task, for A, B two concepts, FA, FB their written forms, DA, DB their definition, we construct a prompt p(A, B) as shown in Listing 1.3. Prompt templates can be found here."}, {"title": "3.3 Fine-tuning Experiment", "content": "While it was previously verified that fine-tuning improves OL performance in existing domains [19], it is not clear if this statement still holds for arbitrary domains and unknown vocabularies where reasoning is required. To verify this question, after evaluating the zero/one/few-shot performance of LLMs in the gibberish domains, we propose to assess the effect of fine-tuning models on the inference performance for a specific task.\nData split. For each dataset D, we fine-tune a LLM for taxonomy discovery on a train split of hypernymy relations. Half of the concepts in the dataset, and their hypernymy relations, are used for training. The inverse relations are used as negatives (if A is a subclass of B, then B is not a subclass of A), alongside some randomly sampled negatives. The remaining relations are used for testing.\nPrompting. We train LLMs on instructive datasets with the prefix prompt in Listing 1.4, completed using each pair of concepts in the hypernymy dataset."}, {"title": "4 Experiments", "content": "In this section, we explore the off-the-shelf evaluation proposed in Subsection 3.2, and the fine-tuning experiments described in Subsection 3.3."}, {"title": "4.1 Experimental setup", "content": "Datasets. To assess the performance variation with domain specificity, we generate three synthetic domain-specific datasets as parallel corpora from the Open English WordNet [16], using the methodology described in Section 3.1. They are:\nSweets: A collection of concepts related to sweets, desserts, sweet food or sugar. In this dataset, hypernyms are frequent, and concepts are relatively well constructed from their hypernyms.\nFootball: A collection of concepts related to football. This dataset, created by browsing co-topic concepts, includes less taxonomic relationships, but has its own terminology and jargon.\nMusic: A collection of concepts related to musical instruments. It is the largest of the three."}, {"title": "5 Conclusion", "content": "We have explored and tested the limits of adaptability and generalizability of LLMs, and observed that LLMs do not adapt well to arbitrary domains. By creating gibberish datasets based on real data and real domains from WordNet, and using LLMs to perform ontology learning tasks on these data, it is realized that LLMs are unable to consistently retrieve the same taxonomic relationships between analogous concepts, which highlights their clear reliance on priorly learned semantics, lexical senses, and the frame of the tokens. However, we notice that after fine-tuning on gibberish data, LLMs improve at discovering hierarchies, both on the domain they were trained on and other arbitrary domains. We attribute this improvement to the emergence of reasoning with lexical semantics. Our work serves as cautionary advice for the community that LLMs do not adapt to arbitrary domains, and we hope that it can inspire future work to leverage reasoning with LLMs for Ontology Learning."}]}