{"title": "BAYESIAN SCALING LAWS FOR IN-CONTEXT LEARNING", "authors": ["Aryaman Arora", "Dan Jurafsky", "Christopher Potts", "Noah D. Goodman"], "abstract": "In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a family of novel Bayesian scaling laws for ICL. In experiments with GPT-2 models of different sizes, our scaling laws exceed or match existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then experiment on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause the suppressed behavior to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) can infer how to perform a task given only demonstrations and without additional training updates. This capability is known as in-context learning (ICL; Brown et al., 2020; Dong et al., 2022). Under ICL, task performance generally increases with the number of demonstrations, though the precise relationship between these two quantities is unclear. We call this relationship the ICL curve and seek to model it. Being able to predict the shape of the ICL curve would help us decide whether to do many-shot ICL (Agarwal et al., 2024) after testing only few-shot performance, predict potential alignment failures under many-shot jailbreaking (Anil et al., 2024), and decide how much fine-tuning we need in order to suppress ICL of undesirable behaviours.\n The learning algorithm underlying ICL has been characterised as Bayesian by Xie et al. (2022) and many later works (\u00a72). Drawing on this line of research, we use Bayes' theorem to derive a family of Bayesian scaling laws for ICL (\u00a73) which model the ICL curve of an ideal Bayesian learner.\n To evaluate the performance of our Bayesian laws, we model the ICL curve for gpt2 models trained on simple synthetic data following Xie et al. (2022) as well as real-world LLMs tested on standard benchmarks (\u00a74.1). Compared to the power laws proposed by Anil et al. (2024), our Bayesian laws achieve lower error rates on both interpolation and extrapolation of the ICL curve, while also providing interpretable parameters for the prior over tasks, the efficiency of ICL, and per-example probabilities under different tasks. In our second set of experiments (\u00a74.2), we present a case study using our Bayesian laws to model how post-training affects ICL of favoured and disfavoured behaviours. On toy models, we find that smaller amounts of post-training strongly change the prior over tasks but not the model's knowledge of each task, and the amount of post-training needed to suppress ICL of disfavoured tasks increases with scale.\n Finally, we present experiments on real-world LLMs ranging from 1B to 405B parameters (\u00a75). Our laws accurately predict the ICL behaviour of several models on both capabilities and safety"}, {"title": "RELATED WORK", "content": "Understanding in-context learning. LMs trained from scratch on controlled synthetic data have been variously claimed to approximate Bayesian learners (Xie et al., 2022; Hahn & Goyal, 2023; Zhang et al., 2023; Jiang, 2023; Wies et al., 2023), gradient descent (von Oswald et al., 2023; Ahn et al., 2023), or differing learning algorithms depending on the task, model scale, and training progress (Aky\u00fcrek et al., 2022; Garg et al., 2022; Bai et al., 2023; Shen et al., 2023; Falck et al., 2024). Neverthless, no work has attempted to directly model the ICL curve on the basis of claims about the learning algorithm underlying ICL. In this work, we test the claims that LMs are Bayesian learners by deriving an expression for the ICL curve under Bayesian assumptions and seeing how well it models actual ICL behaviour.\n Scaling laws. Researchers have sought to characterise how LM loss and performance relates to model architecture, model scale, data scale, and training hyperparameters in order to predict and optimise training runs (Kaplan et al., 2020; Hoffmann et al., 2022). LM scaling laws may also take into account data complexity (Pandey, 2024) or use more expressive formulations for better extrapolation (Alabdulmohsin et al., 2022; Caballero et al., 2023). Power laws seem ubiquitous in describing LM behaviour and have recently been adopted to model the ICL curve under different model and data settings (Anil et al., 2024; Liu et al., 2024); we use these power laws as baselines.\n The ineffectiveness of post-training. Much work has found that post-training, even when applied at scale, only changes LLM behaviour in ways that are superficial and easy to bypass (Qi et al., 2024; Zou et al., 2023; Shayegani et al., 2024; Carlini et al., 2023; Geiping et al., 2024; Jain et al., 2024; Prakash et al., 2024; Wei et al., 2024a; Lee et al., 2024; Wei et al., 2024a; Schwinn et al., 2024; Sheshadri et al., 2024).\n Concerningly, ICL enables re-learning of behaviours that were suppressed with fine-tuning (Wei et al., 2024b; Xhonneux et al., 2024; Anil et al., 2024; Anwar et al., 2024). Under a Bayesian view of post-training, it is possible that task priors are only reweighted while task knowledge is unchanged; our Bayesian scaling laws can test this hypothesis."}, {"title": "A BAYESIAN LAW FOR IN-CONTEXT LEARNING", "content": "As discussed in \u00a72, there are many competing hypotheses about how ICL is learned and implemented in LMs. When training LMs on a variety of simple algorithmic tasks (e.g. linear regression, HMM next-emission prediction), many works find that ICL approximates a Bayesian learner (Xie et al., 2022, inter alia).\n If ICL is indeed Bayesian, we should be able to use Bayesian assumptions to exactly predict how prediction accuracy relates to number of in-context examples. This observation leads us to state some key assumptions necessary to frame ICL as Bayesian. Next, we use repeated application of Bayes' theorem to model how ICL updates the task prior after encountering each new in-context example (\u00a73.1). Finally, we simplify our model to reduce parameter count and add an efficiency coefficient K to take into account the effect of example length and informativeness (\u00a73.2). This results in a family of Bayesian scaling laws. We close the section by setting up some baselines and metrics for our experiments (\u00a73.3)."}, {"title": "DERIVATION", "content": "Definition 1 (Bayesian model of ICL). We define a Bayesian model of ICL as a tuple M = (\u03a3, T, \u03c1, \u03b4), where\n \u2022 \u03a3 is a finite alphabet of symbols \u03c3.\n \u2022 T = {T\u2081, ..., TM} is a set of tasks of size M.\n \u2022 \u03c1: T \u2192 [0, 1] is the prior probability distribution over tasks, such that $\\sum_{m=1}^{M} \\rho(T_m) = 1$.\n \u2022 \u03b4: T\u00d7\u03a3 \u2192 [0, 1] is a likelihood function, mapping a task Tm \u2208 T and symbol \u03c3\u2208 \u03a3 to probability such that $\\sum_{\\sigma} \\delta(T_m,\\sigma) = 1$ for all Tm \u2208 T. This represents the conditional probability p(\u03c3 | Tm) = \u03b4(Tm, \u03c3).\n Now let D \u2208 \u03a3n be a string of n symbols, i.e. a document. When processing this document, our Bayesian model of ICL M computes a posterior over tasks in accordance with Bayes' theorem:\n p(Tm | D) = $\\frac{p(D | T_m)p(T_m)}{\\sum_{m=1}^{M}p(D | T_m)p(T_m)}$                                                             (1)\n We enforce the condition that the probability of future symbols under this model depends entirely on the task posterior, i.e. p(\u03c3 | D) = $\\sum_{m=1}^{M}p(\\sigma | T_m)p(T_m | D)$, and is thus independent of any other properties of the previously processed symbols.\n Theorem 1 (Bayesian law for ICL). Given the following:\n \u2022 M = (\u03a3, T, \u03b4), is a Bayesian model of ICL;\n \u2022 \u03bb: \u03c3 \u2192 R\u22650, such that $\\sum_{\\sigma \\epsilon \\Sigma} \\lambda(\\sigma) = 1$, is a sampling distribution over the alphabet \u03a3;\n \u2022 D \u2208 \u03a3 is a list of symbols sampled i.i.d. under \u03bb, i.e. a document.\nthe next-example probability under the Bayesian model M given a document D consisting of n in-context examples sampled from \u03bb is\n E\u03c3\u223c\u03bb [p(\u03c3 | D)] = $\\frac{\\sum_{m=1}^{M} E_{\\sigma \\sim \\lambda} [p(\\sigma | T_m)]^{n+1} p(T_m)}{\\sum_{m=1}^{M} E_{\\sigma \\sim \\lambda} [p(\\sigma | T_m)]^{n} p(T_m)}$  (2)\nwhere p(Tm) is the prior probability of task Tm, and the expectation E\u03c3\u223c\u03bb [p(\u03c3 | TM)] is computed over \u03bb, the distribution the documents are sampled from.\nProof. Consider a particular sequence D \u2208 \u03a3n. To compute the posterior probabilities of of the M distributions after the Bayesian learner has processed this sequence, we can use Bayes' theorem.\np(Tj | D) = $\\frac{p(D | T_j)p(T_j)}{p(D)}$                 (Bayes' theorem) (10)\n = $\\frac{p(D | T_j)p(T_j)}{\\sum_{m=1}^{M}p(D | T_m)p(T_m)}$                                                                        (expand denominator) (11)\n = $\\frac{p(T_j) \\prod_{i=1}^{n} p(D_i | T_j)}{\\sum_{m=1}^{M} \\prod_{i=1}^{n} p(T_m)p(D_i | T_m)}$                      (D is an i.i.d. sequence of symbols) (12)\nWe can now marginalise the probability of the next symbol o over these M distributions:\np(\u03c3 | D) = $\\sum_{m=1}^{M} p(\\sigma | T_m)p(T_m | D)$               (expand) (13)\n = $\\frac{\\sum_{m=1}^{M} p(\\sigma | T_m)p(T_m) \\prod_{i=1}^{n} p(D_i | T_m)}{\\sum_{m=1}^{M}  \\prod_{i=1}^{n} p(T_m)p(D_i | P_m)}$        (substitute eq. (12)) (14)\n (15)\nWhat we actually care about though is the expectation of p(\u03c3 | D) over the whole distribution of documents. Since our documents are sequences of symbols sampled i.i.d. from \u03bb, we can exploit the independence of the symbols to decompose the whole-document probability into a product of symbol probabilities."}, {"title": "MAKING THE BAYESIAN SCALING LAW PRACTICAL", "content": "We now describe some minor modifications to this law that simplify the model without harming empirical performance.\nReducing unobserved parameter count. The initial formulation of the Bayesian law has a much larger parameter count than e.g. a power law. Instead of scaling quadratically with the number of distributions, we want the parameter count to scale linearly to make the comparison fair.\nWhen fitting our Bayesian law to every task Tk, we must fit $M^2$ terms of the form $E_{\\sigma \\sim T_k} [p(\\sigma | T_m)]$. This represents the probability of a sample from The when scored under Tm. When processing a series of examples sampled from task Tk, under an ideal Bayesian learner the task posterior converges to\nMultiple updates. A key assumption in our law is that a Bayesian update only occurs after each in-context example is processed. In practice, LLMs process inputs token-by-token, and an in-context example may consist of multiple tokens. Examples may also vary in informativeness. To allow for flexibility in this regard, we multiply n (number of in-context examples) by a learned ICL efficiency coefficient K which modulates the strength of the Bayesian update.\nFinal scaling law. We finally obtain the following functional form for the Bayesian scaling law:\nE\u03c3\u223c\u03bb [p(\u03c3 | D)] = $\\frac{\\sum_{m=1}^{M} (P_{x,m})^{Kn+1} p_m}{\\sum_{m=1}^{M} (P_{x,m})^{Kn} p_m}$                (3)\nWhen fitting M distributions, the total parameter count is $M^2+M+1$ for the original parameterisation of P, and 3M + 1 for sampling- and scoring-wise parameterisations. The only difference between the three variants of the Bayesian scaling law is how we tie values in P."}, {"title": "BASELINES", "content": "We compare our Bayesian scaling law with a few other functional forms; our choice of baselines is further justified in appendix B. Anil et al. (2024) attempt to fit scaling laws to the curve relating number of in-context examples to negative log-likelihood. They use a power law and a bounded power law:\n- log Ppower(\u03c3 | D) = Cn-\u03b1 + K                                                            (4)\n- log Pbounded(\u03c3 | D) = C$\\left(\\frac{1}{(1+\\frac{1}{n})^\u03b1}\\right)$ + K              (5)\nAlong with these, we benchmark the logistic function with input in log space as a baseline.\n- log Plogistic (o | D) = $\\frac{C}{1+(\\frac{n}{x})^{-\u03b1}}$ + K                  (6)\nWe list all the laws we study in Table 1 and report our procedure for fitting all laws in appendix D."}, {"title": "EVALUATION METRICS", "content": "To evaluate how well a scaling law fits, we compute the normalised root mean-squared error (NRMSE). Given ground-truth values y = [y1,..., Yn] and predicted values \u0177 = [\u01771, ..., \u0177n],\nRMSE(y, \u0177) = $\\sqrt{\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{n}}$         NRMSE(y, \u0177) = $\\frac{RMSE(y, \\hat{y})}{\\frac{1}{n}\\sum_{i=1}^{n} y_i}$                    (7)\nNRMSE is comparable across different populations, so we can use it to compare how good fits are between different models and datasets. We compute this metric on raw probabilities, not NLL. Finally, to establish statistical significance between the NRMSE of pairs of scaling laws, we simply run a paired t-test and report a significant comparison if the p-value is below 0.05."}, {"title": "EXPERIMENTS ON SYNTHETIC DATA (GINC)", "content": "We conduct a series of experiments comparing how well different scaling laws fit the ICL behaviour of toy transformer models trained from scratch on synthetic data. We use Xie et al. (2022)'s GINC dataset as our testbed for studying ICL in a controlled manner, pretraining LMs at various scales from scratch and observing their ICL behaviour before and after post-training. We report a summary of the results from this section in Table 1."}, {"title": "EXPERIMENT 1: CAN BAYESIAN SCALING LAWS DESCRIBE ICL ON GINC?", "content": "Xie et al. (2022) introduce the GINC (Generative In-Context Learning) dataset as a synthetic testbed for studying ICL. GINC is created by sampling trajectories from a mixture of hidden Markov models that have sparse transition matrices. Not only does training on GINC lead to ICL behaviour, but we also have knowledge of the ground-truth prior over the HMMs which we can use to sanity-check the inferred prior of our Bayesian scaling laws. Thus, we start by evaluating our laws in this controlled setting.\nData. We create a GINC dataset with parameters specified in appendix D. The dataset consists of documents of length 10240 (including a prepended BOS token) sampled uniformly from 5 hidden Markov models. We also create a validation set of 50 documents of length 1024 sampled from the same GINC distribution.\nMethod. We pretrain gpt2-architecture autoregressive language models with varying numbers of layers on GINC. We replicate the architecture and training setup in Xie et al. (2022). We chunk documents into sequences of length 1024, the maximum size of our context window. Our training objective is the next-token prediction task, minimising cross-entropy loss with teacher-forcing over all tokens.\nmin {-E [log p\u03b8 (xi | X<i)]}                                                                          (8)\nWe provide additional details on model architecture and training hyperparameters in appendix D. For each of the model scales, we report pretraining losses on a training and validation set in Figure 2a."}, {"title": "BAYESIAN SCALING LAWS OUTPERFORM BASELINES", "content": "We now fit each of the scaling laws in Table 1 to the curve relating number of ICL examples to probability of the gold k-th token. Since only gpt models with at least 3 layers exhibit ICL on this task, we do not include scores for models with 1 or 2 layers when reporting averages. To compute statistical significance between pairs of models, we perform a paired t-test and report whether the p-value is below 0.05. We report detailed results in appendix F.\nInterpolation error. We fit each of the laws to all of the data and evaluate the fits, averaged over 5 random seeds. We plot average NRMSE for each law across model scales and trajectory lengths (k) in Figure 2b, and report average NRMSE in Table 1. We find that the Bayesian (original) scaling law handily achieves statistically-significantly lower NRMSE than every other law, except for a non-significant comparison with our strong logistic baseline.\nExtrapolation error. Following Caballero et al. (2023)'s qualitative evaluation of extrapolation behaviour for model scaling laws, we perform a quantitative evaluation of extrapolation error. We take the first 10% of the points in every ICL curve, fit each scaling law once, and report NRMSE on the remaining 90% of the curve (which the laws were not fit to) in Table 1. Under this evaluation, the scoring-wise Bayesian scaling law achieves the best performance."}, {"title": "BAYESIAN SCALING LAWS HAVE INTERPRETABLE PARAMETERS", "content": "Now that we have confirmed that the Bayesian law is an accurate model of ICL behaviour, we can interpret the learned parameters of the Bayesian fits. We plot some interesting parameters of the scoring-wise Bayesian law in Figure 3. We observe the following:\n \u2022 The prior (p) distributions are somewhat noisy but roughly uniform, agreeing with the uniform pretraining distribution over the HMMs.\n \u2022 ICL efficiency (K) roughly increases with model depth i.e. larger models have faster ICL, and with the length of each provided ICL example, i.e. more informative examples lead to faster ICL.\nIn general, we find that the scoring-wise Bayesian scaling law is the most in agreement with our knowledge about the pretraining distribution. On GINC, it seems that Bayesian scaling laws are interpretable and explain the shape of the ICL curve well, across a variety of model scales and ICL trajectory lengths."}, {"title": "EXPERIMENT 2: CAN BAYESIAN SCALING LAWS MODEL SFT ON GINC?", "content": "The brittleness of post-training (\u00a72) shown through e.g. many-shot jailbreaking (Anil et al., 2024) raises the question: does post-training merely update model priors over subdistributions, or does it fundamentally change the knowledge models have about those subdistributions? We can operationalise this hypothesis with our Bayesian scaling laws by post-training various models with SFT, fitting the laws to their ICL behaviour, and examining whether parameters other than the prior (p) shift under post-training.\nData. We fine-tune each model on samples taken only from HMM 0, on datasets equivalent in size to {1%, 2%, 5%, 10%, 20%, 50%, 100%} of the total number of pretraining examples.\nMethod. We use the same next-token cross-entropy loss as in eq. (8) to perform supervised finetuning only on this positive subdistribution; see appendix D for hyperparameters. We fit a separate instance of the Bayesian law for each combination of model depth, example length, and # of SFT examples."}, {"title": "SFT IS MORE SUPERFICIAL WITH SCALE", "content": "Table 1 shows that the original Bayesian scaling law achieves the lowest average NRMSE, while scoring-wise beats all but the bounded power law. We present plots of some of the priors and the in-distribution symbol probabilities (i.e. the probability the model will converge to given infinite examples from a particular distribution) for the scoring-wise Bayesian scaling law in Figure 4.\nIn Figure 4a, we can observe how the prior suddenly shifts to favour HMM 0 as SFT progresses with greater amounts of data. Notably, both the prior and the in-distribution scores (Figure 4b) change much more slowly for larger models, implying that SFT is less effective at larger scales at changing the knowledge the model possesses about subdistributions. Past a threshold, SFT seems to indeed change the model's knowledge of the subdistributions (and not just its priors), but this threshold is higher for larger models."}, {"title": "EXPERIMENT 3: DPO ON GINC", "content": "Data. We do the same as in the SFT experiment but with {0.1%, 0.2%, 0.5%, 1%, 2%, 5%, 10%} of the total number of pretraining examples. The prompt of each document is a single BOS token; the positive continuation is a sample from HMM 0 and the negative continuation is a sample from one of the other HMMs, taken uniformly.\nMethod. DPO is a preference-learning RLHF method capable of directly optimising a language model without training a separate reward model (Rafailov et al., 2023). Given a positive output yw and a negative output y\u2081, the training objective of DPO is\nmin E log \u03c3\u03b2log$\\left(\\frac{P_\u03b8(y_w | X)}{P_{ref}(y_w | X)}\\frac{P_{ref}(y_l | X)}{P_\u03b8(y_l | X)}\\right)$                                                 (9)"}, {"title": "DPO (EVENTUALLY) BREAKS THE ICL CURVE", "content": "We show some key results in Figure 5. Unlike SFT, DPO can use very little training data to successfully suppress the disfavoured HMMs beyond the ability of ICL to recover. However, with enough DPO training, the probability of the preferred output (HMM 0) also declines and the ICL curve eventually collapses. As a result, none of the scaling laws model the ICL curve well after some amount of DPO training. We do observe that larger models require slightly more DPO training to suppress the negative distribution, but not as starkly as for SFT.\nThe collapse of the positive distribution is a known failure mode of DPO, which occurs because it maximises the relative difference between the probabilities of the positive and negative distributions (Pal et al., 2024; Feng et al., 2024; D'Oosterlinck et al., 2024). Overall, DPO impacts more of the model's knowledge about tasks than SFT."}, {"title": "EXPERIMENTS ON REAL-WORLD LLMS AND DATASETS", "content": "We extensively studied the application of Bayesian scaling laws on a synthetic testbed (GINC) for pretrained and SFT/DPO models that we trained from scratch. Still, it is unclear to what extent"}, {"title": "EXPERIMENT 4: BAYESIAN SCALING LAWS ARE COMPETITIVE ON REAL-WORLD LLMS", "content": "Data. Our datasets include both capabilities and safety evaluations, including 2 multiple-choice reasoning benchmarks, 3 binary-choice personality evaluations from Perez et al. (2022), and a new many-shot jailbreaking dataset that we created using HarmBench (Mazeika et al., 2024). More details are in appendix E.2.\nMethod. We experiment on 7 instruction-tuned LLMs from the Gemma and Llama families, with parameter counts spanning from 1B to 405B parameters; see appendix E.1 for details. For each dataset and model pair, we construct 50 many-shot prompts adhering to each model's chat template. We use as many shots as possible, filling the context window. We run the LLM on each of these many-shot prompts and, for each shot, store the next-token prediction probability of the relevant portion of the response. We find that many LLMs suffer degradation near the end of their context window, so we only use the data from the starting 90% of the context window.\nResults. As before, we fit each of the scaling laws to the ICL curves and evaluate the quality of the fits by comparing the NRMSE of the predictions. We report overall results across all models in Table 2; we find that most comparisons between the scaling laws are not statistically significant, so again the Bayesian laws are not worse than alternatives."}, {"title": "EXPERIMENT 5: COMPARING LLAMA 3.1 8B BASE AND INSTRUCT", "content": "In our final experiment, we compare the parameters of the Bayesian (scoring-wise) law on Llama 3.1 8B Base and Instruct on all of the real-world tasks. The Base model was not used in the previous experiment. We report raw probabilities as well as the posterior probabilities for the task computed by the scaling law in Figure 6. We find that the instruction-tuning of this model does reduce the prior probability of unsafe behaviours (harmbench and the 3 persona evals) but fails to prevent many-shot jailbreaking.\nOur scaling law shows that the posterior eventually saturates even if instruction-tuning reduces the prior. Along with our synthetic experiments with SFT and DPO in a low-data setting, this is additional evidence for the claim that real-world instruction-tuning merely modifies the prior over tasks and not task knowledge. This may be because the compute allocated to instruction-tuning is is still too small compared to that for pretraining."}, {"title": "DISCUSSION", "content": "In-context learning, like most of the noteworthy properties of large language models, is something that we don't quite understand. This paper emerged from our attempt to reconcile the existing literature that attempts to ascribe a Bayesian basis for the emergence of ICL with the empirical science of scaling laws. We did find that Bayesian scaling laws are competitive with non-theoretical (and relatively unconstrained) scaling laws at modelling ICL behaviour in both toy and real settings.\nReal-world applications. The Bayesian approach seems to perform better at extrapolating model behaviour from a few shots. This can be useful for predicting multi-turn safety failures before they happen or whether additional inference-time computation will deliver worthwhile gains.\nInterpretability. An additional advantage of our approach is that the parameters of the scaling laws mean something and so can shed light on the internal workings of LLMs without needing to fully open the black box. E.g. studying both the prior over tasks and how ICL affects their posterior is valuable for interpreting the effects of alignment on real-world LLMs. Future work could also mechanistically interpret how Bayesian ICL is performed (e.g. localise the prior in activation space).\nAre LLMs Bayesian? In this work we attempt to elucidate model behaviour without reference to model internals. We believe that our results strengthen the claim that LLMs perform Bayesian inference, but do not necessarily prove it. We note that previous works claiming that LLMs are theoretically Bayesian prove their claims on toy models that vastly simplify the complexity of natural language and web-scale pretraining data; it's possible that actual web-scale Bayesian reasoning is beyond the capacity of current LLMs, but they still may behave approximately Bayesian, explaining the success of our scaling law."}, {"title": "CONCLUSION", "content": "In this paper, we combined two questions to make progress at understanding ICL: (1) what scaling law best describes ICL, and (2) is ICL Bayesian? We showed that Bayesian assumptions naturally lead to a scaling law for ICL, and that Bayesian scaling laws are a great fit for both ICL behaviour by small LMs trained on controlled synthetic data, as well as LLMs trained on natural language. Using a Bayesian formulation gave us interpretable parameters for the prior, learning efficiency, and task-conditional probabilities, which can help us understand how model behaviour changes under alignment. We use these to show how ICL ability varies at different model scales, understand how finetuning harms knowledge of disfavoured distributions, and compare base and instruction-tuned LLMs. We are confident that further progress on understanding ICL is possible through the empirical science of scaling laws."}, {"title": "DERIVING A LAW FOR IN-CONTEXT LEARNING", "content": "Every expectation below is computed over \u03c3 ~ \u03bb. For notational simplicity, we do not explicitly indicate this.\n\u0395 [p(\u03c3 | D)] = E$\\left[\\frac{\\sum_{m=1}^{M}p(\\sigma | T_m)p(T_m) \\prod_{i=1}^{n}p(D_i | T_m)}{\\sum_{m=1}^{M}p(T_m) \\prod_{i=1}^{n}p(D_i | T_m)}\\right]$                (16)\n= $\\frac{\\sum_{m=1}^{M}p(\\sigma | T_m)p(T_m)E \\left[\\prod_{i=1}^{n}p(D_i | T_m)\\right]}{\\sum_{m=1}^{M}p(T_m)E \\left[\\prod_{i=1}^{n}p(D_i | T_m)\\right]}$                     (linearity) (17)\n= $\\frac{\\sum_{m=1}^{M}p(\\sigma | T_m)p(T_m) \\prod_{i=1}^{n} E \\left[p(D_i | T_m)\\right]}{\\sum_{m=1}^{M}p(T_m) \\prod_{i=1}^{n} E \\left[p(D_i | T_m)\\right]}$                            (independence) (18)\n= $\\frac{\\sum_{m=1}^{M}E [p(\\sigma | T_m)]^{n+1} p(T_m)}{\\sum_{m=1}^{M}E [p(\\sigma | T_m)]^{n} p(T_m)}$                            (identical) (19)"}, {"title": "OUR CHOICES FOR BASELINES", "content": "Our inclusion of the power law and the bounded power law stem from their use in Anil et al. (2024). We note that their justification for fitting a power law to the ICL curve is predicated on (1) the ubiquity of power laws in describing language model behaviour in general, particularly during training; and (2) a few toy derivations which show how the attention mechanism could implement ICL in a way that results in a power law shape for the ICL curve.\nAs for the bounded power law, Anil et al. (2024) propose it in Appendix H.1 of the paper, but do not provide theoretical justification for it as they did for the power law. The key advantage of the bounded power law, they point out, is that \"it asymptotes to constant values for both limits n \u2192 0 and n\u2192\u221e\" (where n is the number of ICL examples).\nWhen reading this justification, we couldn't help but recall the canonical example of a function that asymptotes in both directions: the logistic function. If we apply a log transform to the input variable, the logistic asymptotes to constant values for n \u2192 0 and n \u2192 \u221e, just like the bounded power law.\nWe also note that since laws that asymptote towards both limits (such as the bounded power law, our log-logistic baseline, and our Bayesian scaling laws) are empirically better fits for ICL behaviour on real-world LLMs, the toy model of ICL that Anil et al. (2024) propose must not capture the real mechanism underlying ICL, since it only predicts power law fits (which assymptote only as n \u2192 \u221e)."}, {"title": "OUR FORMULATION OF THE LOGISTIC BASELINE", "content": "Interestingly, we found that if we define a function logistic(lnx), we get something almost identical to the bounded power law. Starting with the standard logistic function\nf(x) = $\\frac{L}{1+ e^{-k(x-x_0)}}$ + C                                                                                   (20)\nwe replace x := logn and x0 := log n0.\nf(x) = $\\frac{L}{1+e^{-k(logn-log n_0)}}$ +C= $\\frac{L}{1+e^{-klogn/n_0}}$ + C                               (21)\n = $\\frac{L}{1+ (\\frac{n_0}{n})^{-k}}$ + C                                                                                                (22)\nThe only difference from the bounded power law is that the 1 added in the denominator is outside the parentheses for the exponentiation."}, {"title": "IMPLEMENTATION OF THE SCALING LAWS", "content": "Oddly, not all scaling laws papers document how they fit their functional forms. We referred to Hoffmann et al. (2022); Besiroglu et al. (2024); Borgeaud (2024) to figure out how to fit ours, which we describe in this section.\nWe implement our scaling laws and their optimisation routines in PyTorch (Paszke et al., 2019)."}, {"title": "DERIVING NUMERICALLY STABLE EXPRESSIONS", "content": "Our first goal is to use parameterisations that maintain numerical stability. A major (and sometimes only) source of instability"}]}