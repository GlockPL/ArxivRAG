{"title": "What Would You Ask When You First Saw $a^2 + b^2 = c^2$? Evaluating LLM on Curiosity-Driven Questioning", "authors": ["Shashidhar Reddy Javaji", "Zining Zhu"], "abstract": "Large language models (LLMs) can store a massive amount of knowledge, yet their potential to acquire new knowledge remains unknown. We propose a novel evaluation framework that evaluates this capability. This framework prompts LLMs to generate questions about a statement introducing scientific knowledge, simulating a curious person when facing the statement for the first time. We score the qualities of the generated questions, thereby evaluating the knowledge acquisition potential of the LLM. We apply controlled ablation studies to validate our scoring procedures. Additionally, we created a synthetic dataset consisting of 1101 statements in physics, chemistry, and maths with distinct levels of difficulties, 300 general knowledge statements, and 567 incorrect statements. Human evaluations were conducted to validate our model assessments, achieving an approximate weighted Cohen's kappa of 0.7 on all three metrics considered. We find that while large models like GPT-4 and Mistral 8x7b are adept at generating coherent and relevant questions, the smaller Phi-2 model is equally or more effective. This indicates that size does not solely determine a model's knowledge acquisition potential. The proposed framework quantifies a critical model capability that was commonly overlooked and opens up research opportunities for developing more knowledgeable AI systems.", "sections": [{"title": "1 Introduction", "content": "Nowadays, large language models (LLMs) trained on internet-scale datasets are capable of storing massive amounts of knowledge. However, their potential for discovering novel knowledge is of more interest to us. Conventional evaluation methods that involve asking LLMs to answer questions and asking LLMs to follow instructions do not provide sufficient insights into this potential. In this paper, we take inspiration from how humans discover new knowledge: asking questions out of curiosity.\nQuestioning is a key cognitive skill that under-pins learning and knowledge acquisition. By asking questions, people seek to understand the world around them, explore how things work, and challenge existing beliefs. This act of inquiry not only helps humans learn new information but also sharpens their thinking, promotes critical analysis, and drives innovation. Effective questioning fuels intellectual growth by sparking curiosity, encouraging deeper exploration of subjects, and improving comprehension (Acar et al., 2023). In education, questioning is closely linked to higher-level thinking skills like analysis, synthesis, and evaluation (Kurdi et al., 2020). The complexity and depth of questions asked often reflect a person's grasp and understanding of a topic (Kotov and Zhai, 2010).\nQuestions also play a crucial role in reasoning (Zelikman et al., 2024; Hao et al., 2023) because asking insightful questions requires logical thinking, clarifying assumptions, identifying knowledge gaps, and exploring alternative viewpoints (Lucas et al., 2024). OpenAI's o1 model uses its own \"chain of thought\" approach, where it engages in structured reasoning and iterative inquiry/questioning to refine and enhance its outputs (OpenAI, 2024). Thoughtful questions are essential for thorough and logical reasoning (Ashok Kumar et al., 2023). Questioning is equally important for fact-checking. Good questions guide the verification process by identifying gaps, biases, and inconsistencies in the information (Li et al., 2017). Questions like \"Does this agree with other sources?\" or \"Is this consistent with historical data?\" lead to careful checking of facts and encourage cross-referencing across multiple sources. Effective fact-checking requires context and nuance, and good questions can help reveal false or misleading information. Besides reasoning and fact-checking, questioning plays a major role in many other areas (Masterman et al., 2024), like encouraging creativity (Wang et al., 2024), stimulating discussion, and"}, {"title": "2 Related Works", "content": "In evaluating text generation from LLMs, recent methodologies have expanded beyond traditional metrics to include multifaceted approaches that align more closely with human judgment. GPTScore (Fu et al., 2023) and UniEval (Leiter et al., 2023) utilize the natural language understanding capabilities of LLMs to tailor evaluations to specific criteria, with GPTScore focusing on customized fluency and UniEval using a Boolean question-answering format for multiple quality dimensions. Similarly, CheckEval (Lee et al., 2024) employs a structured checklist to enhance reliability, while X-Eval (Liu et al., 2024) dynamically selects evaluation aspects, enhancing adaptability and depth. Further enriching these approaches are frameworks like the zero-shot comparative methodology (Liusie et al., 2024), which performs direct quality judgments, and the Unified Framework (Zhong et al., 2022), which combines traditional and specialized models for the assessment. PlanBench (Valmeekam et al., 2023) explores LLMs' reasoning through various planning tasks, while TIGERSCORE (Jiang et al., 2023) emphasizes explainability in evaluations. These are complemented by strategies that assess LLMs' ability to follow complex instructions (He et al., 2024) and a composite metric system that aggregates individual scores for a holistic view (Verga et al., 2024), enhancing the development and refinement of LLMs across different applications. However, these methodologies primarily center on how LLMs answer questions and perform predefined tasks, with little exploration into how effectively these models can generate meaningful questions themselves. Our research aims to bridge this gap by focusing explicitly on the questioning abilities of LLMs, an aspect that has not been the central focus of prior evaluations, thus introducing a new dimension to the assessment of their cognitive processes and interactive potential."}, {"title": "2.1 Question Generation", "content": "Question generation has long been recognized as a critical task in education, with numerous studies underscoring its significance (Elkins et al., 2023; Kurdi et al., 2020). The evolution of this field has seen a progression from early rule-based question generation systems (Yao et al., 2022) to more sophisticated methods employing transformer-based"}, {"title": "2.2 Evaluation of Generative Models", "content": "In evaluating text generation from LLMs, recent methodologies have expanded beyond tradi-tional metrics to include multifaceted approaches that align more closely with human judgment.GPTScore (Fu et al., 2023) and UniEval (Leiteret al., 2023) utilize the natural language under-standing capabilities of LLMs to tailor evalu-ations to specific criteria, with GPTScore focus-ing on customized fluency and UniEval using aBoolean question-answering format for multiplequality dimensions. Similarly, CheckEval (Leeet al., 2024) employs a structured checklist to en-hance reliability, while X-Eval (Liu et al., 2024)dynamically selects evaluation aspects, enhancingadaptability and depth. Further enriching theseapproaches are frameworks like the zero-shot com-parative methodology (Liusie et al., 2024), whichperforms direct quality judgments, and the UnifiedFramework (Zhong et al., 2022), which combinestraditional and specialized models for the assess-ment. PlanBench (Valmeekam et al., 2023) ex-plores LLMs' reasoning through various planningtasks, while TIGERSCORE (Jiang et al., 2023)emphasizes explainability in evaluations. Theseare complemented by strategies that assess LLMs'ability to follow complex instructions (He et al.,2024) and a composite metric system that aggre-gates individual scores for a holistic view (Vergaet al., 2024), enhancing the development and refine-ment of LLMs across different applications. How-ever, these methodologies primarily center on howLLMs answer questions and perform predefinedtasks, with little exploration into how effectivelythese models can generate meaningful questionsthemselves. Our research aims to bridge this gap byfocusing explicitly on the questioning abilities ofLLMs, an aspect that has not been the central focusof prior evaluations, thus introducing a new dimen-sion to the assessment of their cognitive processesand interactive potential."}, {"title": "2.3 Prompt Engineering", "content": "Recent advancements in LLM evaluation have focused on optimizing prompting techniques to align more closely with human judgment. Studies show that LLM evaluations are more reproducible than human evaluations and effectively use a 5-point Likert scale (Chiang and Lee, 2023). The G-EVAL framework improves evaluation accuracy by leveraging GPT-4 with chain-of-thought prompting (Liu et al., 2023a). Emphasizing prompt engineering, research demonstrates that well-crafted instructions and score aggregation significantly enhance LLM performance (Baswani et al., 2023). Additionally, smaller LLMs, guided by effective prompts, can match larger models' evaluation performance, highlighting the importance of prompt design (Kotonya et al., 2023). Moreover, reference-free evaluation methods show that LLMs can assess text quality through comparative judgments and rationales (Chen et al., 2023). These advancements informed our prompt engineering strategies."}, {"title": "2.4 LLMs for Evaluation", "content": "Recent studies highlight LLMs' potential to achieve human-level assessment quality in various tasks (Gilardi et al., 2023) (Huang et al., 2024). The GEMBA framework, for instance, showcases the effectiveness of LLMs in reference-free machine translation evaluation (Kocmi and Federmann, 2023), while FrugalScore offers a streamlined approach by combining LLM-based metrics with lightweight models for efficient assessment (Kamal Eddine et al., 2022). Literature such as \"Is ChatGPT a Good NLG Evaluator?\" underscores ChatGPT's strong alignment with human judgments across NLG tasks (Wang et al., 2023). AUTOCALIBRATE enhances LLM-human alignment by iteratively refining evaluation criteria with human feedback (Liu et al., 2023b). Additionally, LLMs have proven effective in delivering relevance judgments with natural language explanations (Faggioli et al., 2023). Evaluations in machine translation and chatbot conversations show LLMs closely align with human ratings (Zheng et al., 2023). Instruction tuning has been shown to improve the correlation between LLM evaluations and human judgments (Xiong et al., 2024), while the development of explainable metrics emphasize the importance of transparency in LLM assessments (Leiter et al., 2024). While numerous methods have been introduced, many still have limitations and lack"}, {"title": "3 Methodology", "content": "The dataset, keystone of our research, was crafted with a keen focus on the CDQG task. Leveraging GPT-4's generative capabilities under human oversight, we assembled the dataset incrementally (Xu et al., 2023), selecting statements that span various topics and complexity levels. This dataset is segmented into sub-datasets encompassing statements from the realms of physics, mathematics, and chemistry, each categorized into basic, intermediate, and advanced levels. In addition to these subject-specific statements, our dataset includes a section of general statements reflecting everyday life scenarios, thereby extending beyond the confines of academic subjects. Specifically, the dataset includes 483 statements for Chemistry, 301 for Physics, and 317 for Maths, distributed equally among advanced, basic, and intermediate levels-161, 161, and 161 for Chemistry; 101, 100, and 100 for Physics; and 101, 108, and 108 for Maths, respectively. Additionally, the dataset contains 300 general knowledge statements and 567 statements explicitly designed to be incorrect across Physics (181), Chemistry (225), and Maths (161) to test the models' ability to question credibility. A particularly novel aspect of our dataset is the inclusion of intentionally erroneous statements, designed to probe the models' critical questioning capabilities. For instance, a math statement like \"The sum of 5 and 6 is 55\" deliberately distorts the correct answer to challenge the models. These \"wrong statements\" span physics, mathematics, and chemistry, crafted by subtly altering accurate statements. This subset aims to test whether models can discern and question the veracity and logical consistency of statements, especially given the directive that they are encountering the information for the first time. Our hypothesis posits that if a model behaves as though it has prior knowledge, it will prioritize questioning the statements' legitimacy."}, {"title": "3.1 Research Design Overview", "content": "As summarized by fig 1, this section describes our methodology to assess the performance of various models in the CDQG task. Unlike basic question generation that might address any relevant query, CDQG specifically cultivates questions prompted by intrinsic curiosity, closely mirroring the human questioning process. Our goal is to systematically evaluate these models across three critical performance metrics to discern which models best simulate this human-like inquiry. This approach allows us to propose that the ability to generate questions should be a key criterion in evaluating the capabilities of LLMs."}, {"title": "3.2 Curiosity-Driven Question Generation (CDQG)", "content": "The process began with selecting statements from our dataset, which served as the foundation for our prompt engineering phase. This phase involved tailoring prompts to accommodate the distinct instructional formats of eight different models, ensuring the prompts were universally applicable. Our approach was centered around the concept of personification, where we tasked each model to conceptualize itself as a human. This hypothetical human, encountering a statement for the first time and devoid of prior knowledge, was prompted to generate the top five questions that would instinctively arise. This methodology aimed to explore the models' inquisitive capabilities in a novel and controlled environment. Upon finalizing the prompts, we automated the generation process across the entire dataset for each model. This yielded eight unique sets of five questions for each data point, coming from the distinct models."}, {"title": "3.3 Evaluation", "content": "We evaluated the knowledge acquisition capability of the models via a multi-dimensional framework. This framework considers the following three scores:\n\u2022 Relevance: Relevance assesses how directly each question pertains to the specific details,"}, {"title": "3.4 Assessing the validity with noise-addition ablation", "content": "We propose a novel technique that validates the LLM evaluators, based on the idea of incremental noise addition. By incrementally incorporating noise into the questions, we can assess how well the evaluators scored the questions with varying levels of relevance, coherence, and diversity.\nFor every entry in the output dataset, where an initial set of five questions is generated, we construct two additional derivative entries through the deliberate insertion of disturbances: the first variant involves modifications to two of the questions (2 Altered), while the second sees alterations in four questions (4 Altered). This introduction of noise"}, {"title": "3.5 Assessing the validity with human evaluation", "content": "We conducted human evaluations on questions generated by LLMs, specifically focusing on a subset of the data to make the workload manageable. We selected the questions of the first 10 statements from the first three models (out of eight) for each of the two subject area (out of four). For each subject area, all three variations are included, to ensure the validation results generalize. From the total pool of 6,708 statements across all files in our sub-dataset, we selected 1,320 for human evaluation, representing approximately 19.67% of the data considered.\nA PhD student manually rated the questions. The student was provided with the statements and corresponding questions but was blinded to which model produced them. Based on the given statements and the quality of the questions, the student rated the outputs on three key metrics: relevance, coherence, and diversity. Upon completing the evaluations, we analyzed the agreement between human and LLM"}, {"title": "4 Preparing the dataset", "content": "The dataset, keystone of our research, was crafted with a keen focus on the CDQG task. Leveraging GPT-4's generative capabilities under human oversight, we assembled the dataset incrementally (Xu et al., 2023), selecting statements that span various topics and complexity levels. This dataset is segmented into sub-datasets encompassing statements from the realms of physics, mathematics, and chemistry, each categorized into basic, intermediate, and advanced levels. In addition to these subject-specific statements, our dataset includes a section of general statements reflecting everyday life scenarios, thereby extending beyond the confines of academic subjects. Specifically, the dataset includes 483 statements for Chemistry, 301 for Physics, and 317 for Maths, distributed equally among advanced, basic, and intermediate levels-161, 161, and 161 for Chemistry; 101, 100, and 100 for Physics; and 101, 108, and 108 for Maths, respectively. Additionally, the dataset contains 300 general knowledge statements and 567 statements explicitly designed to be incorrect across Physics (181), Chemistry (225), and Maths (161) to test the models' ability to question credibility. A particularly novel aspect of our dataset is the inclusion of intentionally erroneous statements, designed to probe the models' critical questioning capabilities. For instance, a math statement like \"The sum of 5 and 6 is 55\" deliberately distorts the correct answer to challenge the models. These \"wrong statements\" span physics, mathematics, and chemistry, crafted by subtly altering accurate statements. This subset aims to test whether models can discern and question the veracity and logical consistency of statements, especially given the directive that they are encountering the information for the first time. Our hypothesis posits that if a model behaves as though it has prior knowledge, it will prioritize questioning the statements' legitimacy."}, {"title": "5 Models and implementation", "content": "We examined models ranging from a wide array of sizes: Llama 7b, Llama 13b, Llama 70b (Touvron et al., 2023), Mistral 8x7b (Jiang et al., 2024) Microsoft Phi-2 2.7b, Gemini, GPT 3.5 Turbo (Brown et al., 2020), and GPT-4. Our selection was based on practical considerations such as open-source availability and ease of access through APIs. Mistral's architecture, designed for handling complex queries, and Phi-2.7b's specialization in Q&A, made them well-suited for CDQG. By choosing models with varying architectures and parameter sizes, we ensured a broad comparison of model capabilities while maintaining accessibility and relevance to the task. The models [Gemini, GPT-3.5 turbo, GPT-4] were accessed using available API's and the other models [Llama-2 7b, Llama-2 13b, Llama-2 70b, Mistral 8x7b, Microsoft Phi-2] were inferenced using the open-source weights and models, downloaded from Hugging face. These models are used based on the requirement using different prompt engineering techniques."}, {"title": "6 Results", "content": "Table 1, figs 2 and 3 illustrate our main results. The other figures are included in the Appendix."}, {"title": "6.1 Performance by Model", "content": "GPT-4: Dominates in almost all metrics and subjects, especially in advanced tasks. This superior performance can be attributed to its extensive training on a diverse dataset, which equips it with a broad knowledge base and sophisticated reasoning capabilities.\nMistral 8x7b: Frequently matches or exceeds GPT-4, showing exceptional strength in Chemistry and Maths. Its potential use of a Sparse Mixture of Experts architecture allows it to efficiently manage specific query types, demonstrating the importance of targeted architectural advancements.\nPhi-2: Phi-2's performance is particularly noteworthy. Despite its smaller scale of 2.7 billion parameters, Phi-2 consistently produces relevant and coherent questions in basic to intermediate task levels. This model benefits significantly from"}, {"title": "6.2 Insights and Implications", "content": "While larger models like GPT-4 generally offered robust overall performance, smaller or specialized models like Phi-2 and Mistral 8x7b performed exceptionally well. This challenges the conventional notion that bigger is inherently better (Hoffmann et al., 2022), suggesting a nuanced approach to model selection based on specific task requirements."}, {"title": "6.3 Wrong Statements Insights", "content": "We observed variability in model performances with incorrect statements, aiming to assess their questioning of information credibility. While models generally followed instructions by asking questions, their responses varied significantly. For example, some models questioned the credibility of"}, {"title": "6.4 Validating the Noise Addition Ablation", "content": "The added noise significantly impacted the metrics. We observed a consistent decline in them as the noise level increased from no alterations to 2 Altered and then 4 Altered. However, the declining"}, {"title": "7 Conclusion and Future Work", "content": "This work demonstrates the growing capacity of LLMs to emulate human curiosity through question generation, highlighting both their strengths and limitations in tackling complex, curiosity-driven tasks. Across various subject domains, LLMs exhibited a strong capability to formulate relevant and coherent questions, underscoring their potential to engage in meaningful inquiry. However, the diversity and depth of their questioning still reveal room for improvement, as models often favored surface-level exploration. While larger models like GPT-4 consistently led in performance, smaller models like Phi-2 showed that optimized architectures can rival larger models on simpler tasks, suggesting a path forward for more efficient designs.\nMoving forward, expanding this framework to include more nuanced evaluations such as reasoning depth, creativity, and factual accuracy-will allow for a richer understanding of LLMs' cognitive abilities. Additionally, future research should explore how models can be adapted to generate not only questions but also critical reflections on their own outputs, moving towards systems capable of self-driven knowledge acquisition. This deeper, more reflective capability will be key in advancing LLMs from passive generators to active participants in learning and discovery."}, {"title": "8 Limitations", "content": "While this study introduces an innovative framework for evaluating the questioning capabilities of LLMs, it primarily utilizes metrics of Relevance, Coherence, and Diversity which, though robust, may not capture the full depth of human-like questioning, such as emotional intelligence, knowledge acquisition, factual reasoning etc. Future research could explore the integration of metrics that assess these human-centric qualities to better mimic real-world applications. Additionally, the evaluations are performed within a controlled academic setting, which might not fully reflect the complexities of natural environments where LLMs typically operate. Extending the evaluation to more dynamic settings or incorporating unstructured, real-world data could enhance the applicability of the findings. Moreover, while our noise addition method strengthens the assessment of model robustness, exploring more varied and naturally occurring disturbances could provide a richer understanding of how LLMs perform under realistic and unpredictable conditions."}, {"title": "A.1 Model Configuration Details", "content": "Gemini Settings: The Gemini model was configured with a low temperature setting of 0.1 to ensure predictable and consistent outputs. The top_p and top_k parameters were both set to 1, constraining the model to the most likely outcomes. The maximum output tokens were limited to 400 to balance detail with computational efficiency. Safety settings were established to minimize the risk of generating harmful content, with no blocks applied across categories such as harassment, hate speech, sexually explicit content, and dangerous content.\nMistral Model Setup: The Mistral model utilized a tokenizer and model settings specifically tailored for instruction-based tasks. This setup included using the AutoTokenizer and AutoModelForCausalLM from a pretrained snapshot, equipped with BitsAndBytesConfig for efficient quantization. The configuration ensured operations were optimized for 4-bit quantization and the compute dtype set to float16, enhancing the model's performance while reducing memory usage. The text-generation pipeline was adjusted with a temperature of 0.1 and a repetition penalty of 1.1 to generate more coherent and less repetitive text, with a limit of 128 new tokens per generation instance.\nLlama Model Configurations: For the Llama models, including, Llama 7b, Llama 13b and Llama 70b, configurations were similarly tailored to enhance performance and efficiency. Both models used quantization settings conducive to low-memory consumption while maintaining computational precision. These settings were crucial for managing the large parameter size inherent to these models. Each model's generation pipeline was configured to produce full text outputs with controlled temperature settings and repetition penalties to ensure relevance and diversity in the generated text.\nPhi2 Model Configuration: The Phi2 model from Microsoft was set up with advanced quantization techniques to support efficient processing. The model and tokenizer were loaded from a specific snapshot with settings that enabled high-performance text generation. The generation settings included a controlled temperature for predictability, a sampling strategy to introduce variety, and a repetition penalty to avoid redundant content, making it well-suited for generating diverse and engaging text.\nCompute Resources: For models accessed via"}]}