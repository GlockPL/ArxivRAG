{"title": "Advancing Chart Question Answering with Robust Chart Component Recognition", "authors": ["Hanwen Zheng", "Sijia Wang", "Chris Thomas", "Lifu Huang"], "abstract": "Chart comprehension presents significant challenges for machine learning models due to the diverse and intricate shapes of charts. Existing multimodal methods often overlook these visual features or fail to integrate them effectively for chart question answering (ChartQA). To address this, we introduce CHARTFORMER, a unified framework that enhances chart component recognition by accurately identifying and classifying components such as bars, lines, pies, titles, legends, and axes. Additionally, we propose a novel Question-guided Deformable Co-Attention (QDCAt) mechanism, which fuses chart features encoded by CHARTFORMER with the given question, leveraging the question's guidance to ground the correct answer. Extensive experiments demonstrate that the proposed approaches significantly outperform baseline models in chart component recognition and ChartQA tasks, achieving improvements of 3.2% in mAP and 15.4% in accuracy, respectively. These results underscore the robustness of our solution for detailed visual data interpretation across various applications.", "sections": [{"title": "1. Introduction", "content": "Comprehending charts and correctly answering chart-related questions [4, 17, 25] is essential in today's data-driven world. Charts are powerful tools to distill complex data into visual formats, making it easier to identify trends, patterns, and insights at a glance. Despite the significant progress that researchers have made on Visual Question Answering [1, 9, 29, 31, 34, 37, 38, 40], Chart Question Answering (i.e., ChartQA) is particularly challenging as it requires seamless and fine-grained interpretation and analysis of both textual and visual elements in the charts to answer natural language questions [4, 15, 17, 18, 25\u201327, 45]. Consider the example shown in Figure 1. To correctly answer the question based on the given chart, models need to accurately locate textual elements such as \u201cWhite\u201d in the category axis and \u201cFemale presidents\u201d in the legend. In addition, they also need to identify the visual elements such as the blue bar for \"White\" category and correctly link it to the corresponding text label, i.e., \u201c33%\u201d.\nThough many efforts have been made on chart component recognition to correctly identify key components such as chart types (e.g., pie, bar, and line charts), visual elements in each plot (e.g., connected lines, colors), and textual elements (e.g., legends and value axes), they usually require a pipeline approach that first detects key points of lines or boxes and then classifies and groups the detected regions. Such methods struggle to comprehend complex graphics, such as stacked boxes, overlapped labels, and crossed lines. For ChartQA, it is essential to incorporate rich and accurate chart information and ensure proper attention is given to the relevant chart regions based on the specific questions asked. Existing approaches that leverage OCR tools or extracted tables often introduce noise and confusion due to inaccuracies. Additionally, existing models typically fuse the visual information with the question embedding at a later stage, relinquishing the ability to guide attention to the relevant parts of the graphics via the question.\nTo tackle these challenges, we propose a novel framework named QDCHART, which integrates an innovative unified solution, CHARTFORMER, to recognize all the various chart components from diverse types of charts. CHARTFORMER leverages deformable attention [36] to effectively capture the visuals of chart graphics, and incorporates a mask attention mechanism [3] with learnable query features to cover diverse chart components. To address ChartQA, we propose a novel Question-guided Deformable Co-Attention (QDCAt) mechanism that leverages the rich chart features encoded by CHARTFORMER. This mechanism fuses question information with chart features through a Question-guided Offset Network (QON) and integrates visual and chart-related features using a deformable co-attention module. The resulting question-guided features are then passed to a text decoder to generate the answer to the given question.\nWe first evaluate the effectiveness of CHARTFORMER on chart component recognition, on the public benchmark dataset ExcelChart400K [23]. CHARTFORMER significantly outperforms existing strong baselines, such as DAT [36] Mask2former [3], by 11.4% and 3.2% in mAP, respectively. It especially shows superior performance on stacked bars, overlapped or fluctuating lines, and narrow pie slices. We further evaluate QDCHART on ChartQA [25], a large dataset that includes human-written and machine-generated questions. QDCHART outperforms the previous strong baseline Pix2Struct [17] by 1.2% accuracy. Our model excels in handling visually related questions, particularly those involving color disambiguation or size comparison of chart components. Additionally, it demonstrates superior ability in effectively grounding key chart information.\nThe contribution of this work can be summarized as follows:\n\u2022 CHARTFORMER, a state-of-the-art end-to-end model for unified chart component recognition, provides accurate identification for diverse class objects with distinct visual structures and rich chart semantics.\n\u2022 QDCHART, a multimodal application leveraging CHARTFORMER's rich chart semantics with a novel Question-guided Deformable Co-Attention (QDCAt) fusion layer for ChartQA, enables the model to focus on components related to the question.\n\u2022 CHARTFORMER exceeds the baseline model by 3.2% in mAP on chart component recognition, while QDCHART surpasses the baseline model by 15.4% in accuracy on ChartQA.\n\u2022 An automatically annotated unified chart component recognition instance segmentation dataset featuring several prominent chart types."}, {"title": "2. Related Work", "content": "Chart Component Recognition Existing approaches for chart element recognition can be divided into three broad categories: detection via bounding boxes [6, 20, 24], key point detection and grouping [4, 23, 24, 39], and line graph detection [16, 28]. Among these, ChartOCR [23] uses an hourglass network to identify key points and a rule-based approach to associate them with chart elements. Lenovo [24] trains separate detectors for points and bars, followed by a deep neural network to measure feature similarities for data conversion. ChartDETR [39] employs a transformer-encoder-decoder model to detect and categorize key point groups within a unified framework. Lineformer [16] focuses on line charts, treating line detection as instance segmentation using the mask2former model [3]. Compared to previous studies, our proposed CHARTFORMER instead employs a comprehensive end-to-end instance segmentation framework to enhance chart component recognition.\nObject Detection and Instance Segmentation Object detection and instance segmentation [8,41,44] are core tasks in computer vision and have been widely explored by using Convolutional Neural Networks [12, 13, 32, 33, 35]. Recently, many new approaches have been developed based on Transformer [42, 43], inspired by its success in the field of Natural Language Processing. Among them, ViT [10] processes images as non-overlapping patch sequences, using global attention to capture long-range dependencies. Swin Transformer [22], on the other hand, employs partitioned window attention to focus on specific regions within images. Enhanced attention mechanisms like the Deformable Convolutional Network (DCN) [7] and Masked attention Mask Transformer (Mask2Former) [3] are further introduced to enhance these approaches. DAT (Deformable Attention Transformer) [36] improves DCN's capabilities by learning deformed key points through feature sampling and updating positional embeddings with relative position bias. Our work leverages the strengths of instance segmentation frameworks on small and varied objects to improve chart understanding.\nChart Question Answering Recent advancements in language and multimodal models have significantly enhanced their ability to tackle the complex reasoning required for ChartQA tasks. Donut [15] is a vision-encoder-text-decoder model that leverages Swin Transformer [22] and MBart [21] to answer questions with a visual context. VL-T5 [5] extends [30] by incorporating visual features from chart images, while VisionTaPas [25] extends TaPas [14] by integrating a vision transformer encoder to process chart images. ChartT5 [45] improves chart understanding by leveraging a visual and language pre-training framework on chart"}, {"title": "3. Method", "content": "In this section, we first introduce CHARTFORMER, the first unified and end-to-end solution for recognizing components from diverse types of charts with distinct visual structures (Section 3.1), and then illustrate QDCHART which integrates the chart components identified by CHARTFORMER and selectively incorporates them to answer the target question (Section 3.2).\n3.1. CHARTFORMER\nAs shown in Figure 2, given an input chart image \\(I\\) with dimensions \\(H \\times W\\), chart component recognition aims to identify and classify various components within the chart, including both visual elements such as \\(lines\\), \\(bars\\), and \\(pie slices\\), and textual elements such as \\(value axes\\), \\(category axes\\), \\(legends\\), and \\(chart titles\\). Let \\(C\\) be the set of all chart component types, e.g., \\(C = {\\text{\\\"Bar\\\"}, \\text{\\\"Line\\\"}, \\text{\\\"Pie\\\"}, . . .}\\). Let \\(M_c\\) represent the set of instance segmentation regions corresponding to a particular chart component type \\(c \\in C\\), and \\(M = {M_c}_{c\\in C}\\) be the set of all annotations for the image \\(I\\).\nCHARTFORMER is designed to learn to accurately identify and classify each chart component in chart images, and consists of three main modules: a vision encoder, a pixel decoder, and a mask transformer decoder, as shown in Figure 2. Formally, the input chart image \\(I \\in \\mathbb{R}^{H \\times W}\\) first undergoes processing by a CNN layer to generate an initial feature map \\(E \\in \\mathbb{R}^{K \\times H/4 \\times W/4}\\) with a channel size \\(K = 64\\) in our experimental setting. This feature map \\(E\\) is fed into the vision encoder for further processing and feature learning. The vision encoder features multiple blocks of neighborhood attention [11] and deformable attention [36]. Neighborhood attention expands each pixel's attention span to its nearest neighborhood\n\\[\\chi = NeigAttention(E) .\\]\nChart images have distinct borders and consecutive visual elements like lines and bars, thus applying deformable attention becomes intuitive to emphasize precise focus on the visual connections. In deformable attention, uniformly spaced reference points \\(p\\) are offset by \\(\\Delta p\\), obtained via an offset network \\( offset\\) applied to the query vectors \\(q\\). Features are then computed using bilinear sampling \\(\\phi\\) at the deformed points\n\\[\\Delta p = offset(q), \\quad \\chi = \\phi(\\chi; p + \\Delta p) .\\]\nThen the deformation attention is computed as\n\\[q = \\chi W_q, \\quad k = \\chi W_k, \\quad v = \\chi W_v, \\]\n\\[DefoAttention(\\chi) = softmax \\bigg(\\frac{qk}{\\sqrt{d}}\\bigg) v,\\]\nwhere key \\(k\\) and value \\(v\\) vectors are projected from sampled features \\(\\chi\\), and \\(d\\) is the dimension of the attention head.\nThese specialized attention mechanisms allow CHARTFORMER to effectively capture local and global contextual information, thereby enhancing the ability to extract meaningful features from chart images. Together, the CNN layer and the vision encoder extract features \\(\\chi \\in \\mathbb{R}^{8K \\times H/32 \\times W/32}\\) from the input image \\(I\\)\n\\[\\chi = E_{chartEncoder} (I) .\\]\nThe pixel decoder then upsamples the extracted features \\(\\chi\\) and outputs a 2D per-pixel embedding \\(P \\in \\mathbb{R}^{C_E \\times HW}\\), where \\(C_E = 256\\) is the number of channels\n\\[P = P_4, \\quad P_i = D_{pixel}(P_{i-1}), \\quad P_0 = \\chi .\\]\nSubsequently, a mask transformer decoder combines object queries \\(u \\in \\mathbb{R}^{C_Q \\times N}\\) and pixel decoder features to compute embeddings \\(Q \\in \\mathbb{R}^{C_Q \\times N}\\), where \\(C_Q = 256\\) denotes the number of channels and \\(N\\) denotes the number of object queries\n\\[Q = D_{mask}(u; P_{1:3}) .\\]\nThe embeddings \\(Q_N\\) are passed through dense layers to predict object classes \\(O_{class} \\in {\\mathbb{R}}^{\\mathcal{N}+1}\\) and binary per-pixel mask predictions \\(O_{mask} \\in {\\{0,1\\}}^{N \\times HW}\\)\n\\[O_{class} = softmax(QW),\\]\n\\[O_{mask} = \\sigma\\big(s(MTP) - t\\big),\\]\nwhere \\(\\sigma\\) is the element-wise sigmoid function, \\(t \\in [0,1]\\) is a threshold parameter, \\(s\\) is the step function, and \\(M = MLP(Q)\\). The bounding box \\(O_{bbox}\\) can be directly obtained by drawing the smallest box that encloses the segmentation mask. Following [3], we use a combination of focal loss and dice loss for \\(O_{mask}\\), and classification loss for \\(O_{class}\\)."}, {"title": "3.2. QDCHART", "content": "Given an input image \\(I\\) and a natural language question \\(Q\\), we further design QDCHART to provide an answer \\(A\\) by leveraging the chart components detected by CHARTFORMER. As shown in Figure 3, QDCHART consists of four main modules: a chart encoder, a vision encoder, a Question-guided Deformable Co-Attention (QDCAt) fusion block that fuses the output of two encoders, and a text decoder.\nChart Encoder We include the vision encoder of the ChartFormer model (\\(E_{ChartEncoder}\\)) as a primary image encoder, utilized to capture explicit chart segment information. During the fine-tuning stage on the ChartQA dataset, we freeze the weights \\(\\Theta\\) of the CHARTFORMER model to reduce training costs.\nVision Encoder The Vision Encoder module follows the previous work [15] and utilizes a Swin Transformer [22] architecture to provide complementary visual information, denoted as \\(E_{SWIN}(I)\\). The parameters are initialized based on the pre-trained model [15]. We pass the image through the Swin encoder to obtain the feature \\(y\\)\n\\[y = E_{SWIN}(I),\\]\nwhere \\(\\Theta\\) are trainable parameters of the Vision Encoder.\nQuestion-guided Deformable Co-Attention To fuse image features from complimentary encoders, and to incorporate information guided by the question, we propose Question-guided Deformable Co-Attention (QDCAt), which consists of a question-guided offset network and a deformable co-attention block."}, {"title": "4. Experimental Setup", "content": "We first evaluate CHARTFORMER on the chart component recognition task and then further assess the effectiveness of QDCHART on chart question answering.\n4.1. Chart Component Recognition\nBaselines We employ four advanced models as strong baselines for chart component recognition and compare them with CHARTFORMER: Mask R-CNN, which [12] extends Faster R-CNN by adding a branch for predicting segmentation masks; SOLOv2 [35] utilized a specialized segmentation branch with decoupled head for better mask feature learning; Mask2Former [3] incorporates masked attention mechanisms for unified segmentation tasks; DAT [36], characterized by its integration of deformable attention mechanisms and global image features, facilitates the comprehensive analysis of multiple chart components."}, {"title": "5. Results", "content": "5.1. Chart Component Recognition\nTable 2 illustrates the performance of all the models employed in chart element detection and classification. Among them, CHARTFORMER exhibits the most impressive performance across all mAP evaluation metrics. Notably, the results attained by CHARTFORMER remain consistently superior across all categories, attaining either the top or second best mAP score, as shown in Table 3. Note that the line component emerges as the most challenging category due to its distinctiveness from other categories and its unique structural characteristics.\nCompared to DAT, CHARTFORMER achieves better performance, particularly in categories that are challenging for DAT, such as lines. Experimental results indicate that the narrower the line width we annotate, the more difficult it is for DAT to predict the line segments accurately. However, CHARTFORMER consistently predicts the line segments accurately, regardless of how narrow the line annotations are. Additionally, compared to Mask2Former, CHARTFORMER increases the accuracy for bar segments by 10.0%. In complex scenarios such as stacked bar charts and overlapping line charts, CHARTFORMER more accurately detects the correct number of components. Detailed comparison examples are provided in Figures 8, 9, and 10 in Appendix B. These examples reveal the shortcomings of competing models, such as the inability to predict smooth pie edges, small bars, or steep lines. Though Mask2former performs relatively well, it occasionally misses stacked bar predictions and overcompensates on the width of the lines.\n5.2. Chart Question Answering\nThe results of the chart visual question answering task are summarized in Table 4. QDCHART significantly outperforms all non-pretraining baseline methods and some of the pertaining and LLM-based baselines. However, our model underperforms compared to extensively pre-trained models like MatCha and Unichart. It is important to note that MatCha is pre-trained on 25 million chart-related data points, while Unichart leverages 13 million high-resolution chart data entries. This extensive pre-training enables these models to learn a wider variety of patterns and nuances, leading to superior performance. In contrast, our model involves minimal pre-training on ExcelChart400K, which may not capture the same level of detail and complexity, resulting in relatively lower performance.\nWe further visualize deformed points sampled by the bilinear sampling step in QDCHART in Figure 5 to demonstrate the effectiveness of the proposed framework. We present three human-written ChartQA examples with different types of charts (bar, line, and pie) from the test dataset. These visualizations illustrate a significant correlation between deformed points and regions containing potential answers, particularly the correct answer (highlighted by the red box in Figure 5). A common observation is that the deformed points tend to cluster around text and chart data element regions while being sparse or uniformly spaced in the blank areas. We initialize our deformable points uniformly spaced, and they move towards nearby visual elements or stay still when nothing is around. The deformed points align with the question text and visual traits; for instance, in the line graph, points cluster around the answer \u201c2009\u201d region, locating the answer correctly; in the bar graph, where the question asks for the \u201cleftmost value\u201d, most points shift to the left, demonstrating the model's ability to understand and follow the question; in the pie graph, even there are two \u201c2%\u201d texts, the points are clustered on the one that matches the question description \u201crefused\u201d. More examples can be found in Figure 7 in Appendix B. This demonstrates that our proposed QDCAt enhances the model's reasoning ability through the movement of deformable points.\n5.3. Ablation Studies\nTo demonstrate the contributions of different components to the overall performance of QDCHART, we conduct ablation studies and experiment with two additional methods to combine the features from the two image encoders.\n\u2022 QDCHART - QON: We remove the question-guided offset network (QON) and bilinear sampling from QDCHART.\n\u2022 QDCHART Concat: Instead of using the deformable co-attention block, we test concatenation over the channel dimension, denoted by \\(x \\oplus y\\).\n\u2022 QDCHARTCNN: We further experiment with concatenation followed by a CNN layer to adaptively fuse the spatial features, denoted by \\(CNN(x + y)\\).\nThe experimental results shown in Table 5 demonstrate that without our proposed deformable co-attention, using CNN fusion (QDCHARTcnn) decreases performance by 5.52%. Similarly, removing the CNN layer and using simple concatenation (QDCHARTConcat) results in a further"}, {"title": "6. Conclusion", "content": "In this study, we address ChartQA by enhancing chart component recognition and proposing a novel question-aware attention fusion module. We introduce CHARTFORMER, a unified network designed to handle multiple chart comprehension tasks across various chart types in an end-to-end manner. This innovative architecture is carefully crafted to handle the intricate nuances associated with diverse chart components, offering a robust solution for accurate and reliable instance segmentation. We further propose QDCHART, which integrates a novel Question-guided Deformable Co-Attention (QDCAt) fusion block to align question-aware chart features extracted by CHARTFORMER with general-purpose multimodal encoder features. This approach explores new possibilities in multimodal fusion and enhances the guidance derived from the question. Through extensive experimentation and evaluation, our approach demonstrates exceptional performance, highlighting its effectiveness in addressing ChartQA challenges."}, {"title": "A. Segmentation Annotation", "content": "Pies: The coordinates for three vertices of the slices are already provided as keypoints. It remains to approximate the arc of the circle via insertion of intermediate points, roughly 5 per radian. To achieve this, we compute the angles and radii of the two edge vertices with respect to the center, then linearly interpolate these quantities for the intermediate points.\nLines: As the provided keypoints are placed at line centers, no information regarding the thickness of a particular line is available. To address this while accounting for differing image sizes, we set annotation line thickness to 1% of the image height. While this parameter choice produces fairly accurate annotations in most cases, it does not necessarily correspond to the ground-truth line thickness shown in images. The edges of the created polygon are defined by line segments parallel to the ones in the provided line, placed at the line width distance apart from each other. Their endpoints (which are polygon vertices) are the intersection points of two adjacent pairs of lines. To account for acute bends in the line producing elongated 'spikes' in its segmentation mask, the line vertices at such bends are duplicated and shifted a minute distance apart.\nBars and others: Remaining chart components have rectangular annotations, for which the four vertices can be computed directly from the given parameters."}, {"title": "B. Supplementary materials", "content": ""}]}