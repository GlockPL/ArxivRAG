{"title": "Label Convergence: Defining an Upper Performance Bound in Object Recognition through Contradictory Annotations", "authors": ["David Tschirschwitz", "Volker Rodehorst"], "abstract": "Annotation errors are a challenge not only during training of machine learning models, but also during their evaluation. Label variations and inaccuracies in datasets often manifest as contradictory examples that deviate from established labeling conventions. Such inconsistencies, when significant, prevent models from achieving optimal performance on metrics such as mean Average Precision (mAP). We introduce the notion of \u201clabel convergence\" to describe the highest achievable performance under the constraint of contradictory test annotations, essentially defining an upper bound on model accuracy.\nRecognizing that noise is an inherent characteristic of all data, our study analyzes five real-world datasets, including the LVIS dataset, to investigate the phenomenon of label convergence. We approximate that label convergence is between 62.63-67.52 mAP@[0.5:0.95:0.05] for LVIS with 95% confidence, attributing these bounds to the presence of real annotation errors. With current state-of-the-art (SOTA) models at the upper end of the label convergence interval for the well-studied LVIS dataset, we conclude that model capacity is sufficient to solve current object detection problems. Therefore, future efforts should focus on three key aspects: (1) updating the problem specification and adjusting evaluation practices to account for unavoidable label noise, (2) creating cleaner data, especially test data, and (3) including multi-annotated data to investigate annotation variation and make these issues visible from the outset.", "sections": [{"title": "1. Introduction", "content": "Machine learning systems can be categorized into three broad phases: (1) dataset creation, (2) model development, and (3) evaluation. Within the computer vision community, there is a significant focus on the second phase-the development of innovative methods to address new challenges or improve results for existing problems. In such cases, the training data, test data, and evaluation criteria remain constant, and the sole focus is on model modifications and adaptations to enhance system performance. In contrast to this model-centric paradigm, there exists an alternative approach known as data-centric AI. This concept was introduced by Andrew Ng [9] and primarily concentrates on the first phase. This strategy involves maintaining a constant model while systematically enhancing the quality of the data. Within this data-centric approach, improvements focus on aspects such as label consistency, the guidelines or conventions followed by annotators\u00b9, enhancing domain coverage, adjusting the dataset size, and identifying errors, among others. Both methods aim to boost performance, either through advancements in data quality or through model optimization.\nIn this study, we adopt an alternate perspective by including an additional \"zeroth\u201c phase \u2013 problem specification (0). Our focus is on both the problem specification (0) and the evaluation (3), and we frame our investigation around the following central question:\n\"How can we estimate the intrinsic performance threshold for models and data given annotation variation in human labels such as noise or uncertainty?\"\nThis study introduces the concept of label convergence, which hypothesizes that the performance limitations of a dataset arise from internal label inconsistencies. These inconsistencies may stem from variations in labeling conventions, annotator variability, or outright errors. Many of these challenges originate in the problem specification phase, where datasets typically assume the provided labels are \"gold standard\u201c, representing a singular ground truth. However, for complex data with unavoidable label noise, perfect ground truth is often unattainable. In such cases, label convergence serves as a measure of the inherent ambiguity in both problem specification and evaluation. Before delving into this topic, we first review the significance of labels in machine learning.\nThe issues with annotations are referred to by several terms: noisy labels, human label variation, annotation errors, or uncertain labels. We will refer to them as (annotation/label) variations. Despite the different terminologies, they all denote the same underlying concept where the mapping from a feature vector to a label is not uniquely defined. This phenomenon can be expressed for a set of images, denoted by {\ud835\udc65\ud835\udc56}\ud835\udc56=0\ud835\udc41, where an annotator has produced a set of\n\n\u00b9 We consider the guideline as the instructions given to the annotators and the annotation convention, the interpretation of annotators made, which in the best case exactly matches the guideline if the guideline is unambiguous."}, {"title": "2. Related Work", "content": "Previous studies have approached the concept of label convergence indirectly, though not explicitly named as such. These efforts, aimed at understanding the upper bounds of model performance, have generally relied on the analysis of learned models. However, assessing label convergence ideally requires a model-independent approach that focuses solely on label quality.\nBorji et al. [3] investigated an \"Empirical Upper Bound\" for object detection models, potentially touching on aspects of the three types of convergence we've discussed. They hypothesized an upper bound in mAP determined by the performance of an ideal detector using ground truth bounding boxes and the optimal object classifier. While intriguing, their methodology overlooks localization and relies on learned models, which deviates from a pure label convergence evaluation, which should be independent of any model.\nAgnew et al. [1] attempted to quantify annotation quality empirically by introducing synthetic variations into the annotations. Their methods include uniform noise for expanding bounding boxes and Gaussian radial noise for polygons. While the objective is to identify a performance ceiling due to contradictory examples, the reliance on synthetic noise and learned models limits the study. These noise patterns, being predictable, are significantly easier to learn by models [24], and the assessment is not fully detached from the original, potentially flawed, annotations."}, {"title": "3. Analyzed Dataset", "content": "In this section, we briefly introduce the datasets used in our study, which are summarized in Table 1. A crucial requirement for these datasets is that each image must be annotated by at least two independent raters. It is not mandatory for the raters to mark instances; if they conclude that no instance of the defined class set for the dataset is present in an image, it is acceptable to leave it unannotated. Based on these strict criteria, we selected segments from three well-known datasets in object detection: LVIS [11], COCO [19], and Open Images [16].\nFor LVIS, we use the doubly annotated subset of 5,000 images that was originally compiled to assess annotation consistency for v0.5 of the dataset. To ensure comparability with current SOTA models, we adjusted the data to align with v1.0 of the dataset, reducing the number of categories from 1723 to 1203 and excluding annotations of the removed categories. This subset serves as our primary benchmark due to its high quality, community recognition, consistent annotation guidelines, and complexity due to its wide class variety and long-tailed distribution. Unfortunately, this dataset lacks rater identification information, which divides the dataset into two generic subgroups.\nAdditionally, we examined two reannotated datasets introduced by Ma et al. [21], combined with the original COCO and Open Images datasets, to assess label convergence. However, these datasets have limitations that affect their validity and generalizability, detailed in Appendix A, which prevent us from determining label convergence for these datasets.\nLastly, we consider two smaller domain-specific datasets, VinDR-CXR [27] and TexBiG [39], which have the advantage of multiple raters per image and consistent labeling conventions throughout the annotation process, including rater identification metadata. While their smaller size and specific focus limit their impact compared to the results of the LVIS dataset, they technically provide the most suitable data for our analysis."}, {"title": "4. Determining Label Convergence", "content": "Determining label convergence is divided into two parts: (1) for cases where each image is annotated by exactly two independent raters, and (2) for cases where images are annotated by any number of raters. A key aspect in defining label convergence is to make the convergence threshold comparable to standard evaluation metrics, allowing for direct comparison with current model performance. To achieve this, we align label convergence with the mAP used in datasets such as COCO, LVIS, and PASCAL VOC. To do this, we modify the regular mAP to work with different ground truth approximations instead of model predictions and a single ground truth. Since this still only allows us to evaluate between two annotators, we use Krippendorff's Alpha version for object detection [39], which we also extend to work on instance segmentation, to assess the consistency of multiple annotators. By correlating these two values, we can easily but effectively determine the convergence for any number of annotators.\n4.1. Two Annotators per Image\nTo adapt the mean Average Precision (mAP) metric for evaluating label convergence between two annotators, we make specific modifications. Normally, mAP requires model confidence scores and a single ground truth, but for human annotations, we address these differences as follows:\nWe set the confidence score of all annotations to 0.99, as human annotations are unlikely to orange produce severely overlapping detections of the same class. Unlike models, which then need to remove these excess detections through post-processing methods like non-maximum suppression.\nWe randomly switch which annotator is considered ground truth and which is considered prediction, repeating the evaluation several times to account for variability.\nDespite these adjustments, the evaluation remains comparable to the original mAP method. Here are the steps to compute the modified mAP:\nDetections are sorted from highest to lowest confidence score.\nEach detection is matched to a ground truth instance by IoU overlap, marking it as positive if matched or negative if not.\nPositives and negatives are added to the Precision-Recall (PR) curve.\nThe area under the PR curve is calculated to obtain the AP for a category and a specific IoU threshold.\nThe AP values are averaged across all categories to obtain the mAP@[IoU threshold].\nTo statistically estimate the convergence threshold, we use bootstrapping, sampling 1,000 subsets, each with 10% of the available images of the respective dataset and evaluate the modified mAP on each of these subsets. By evaluating the 95% confidence interval of the sampling distribution, we capture the variability of the data and infer the convergence threshold for the entire dataset. The results show a convergence threshold interval between 62.64 and 67.52 mAP for the LVIS consistency subset, with Co-DETR's performance near the upper end at 66.8 mAP (see Table 3 and Figure 1). Due to issues with the COCO Reannotated and Open Images Reannotated datasets, these results are not included in the determination of the convergence threshold (see Appendix A).\nConsiderations for Extrapolation:\nLabel convergence was estimated for a subset of the dataset, as only some images contain repeated labels. Although similar variations are expected in the remaining data, this is an extrapolation.\nDomain specificity of different subsets may create a domain gap between training and testing data, potentially affecting the generalizability of the convergence threshold.\nLabeling conventions need to be aligned. If a dominant group of annotators follows a particular labeling convention, models may overfit to that convention. Ensuring that annotators have roughly equal contributions is crucial for accurate label convergence estimation (discussed further in Section 5). It is reasonable to assume that if such a dominant labeling convention exists within the dataset, then following that labeling convention would allow a model to exceed the convergence threshold.\n4.2. Multiple Annotators per Image\nSince modified mAP cannot be used for images annotated by three or more annotators, we use Krippendorff's Alpha for Object Detection [39] to assess annotation quality. This method, parameterized with different IoU thresholds such as mAP, provides a K-a value representing reliability between -1 and +1. A detailed explanation of this method is available in Appendix B.\nTo determine the convergence threshold for multiple annotators, we follow these steps:\nUse the samples from the previous bootstrapping (LVIS, COCO, Open Images) and obtain the K-a values for different thresholds [0.5, 0.95, 0.05] and their mean. Also evaluate the mAP for all missing IoU thresholds as well.\nPerform a linear least-squares regression (Figure 3) with K-a as the independent variable and mAP as the dependent variable. The regression shows a Pearson correlation of \ud835\udf0c = 0.92 and an R-squared value \ud835\udc452 = 0.85, indicating a good model fit. The resulting equation is:\n\ud835\udc5a\ud835\udc34\ud835\udc43 = 0.836 \u22c5 \ud835\udefc + 0.197  (1)"}, {"title": "5. Annotation Variation Type Analysis", "content": "After establishing the convergence threshold, we further analyze the distribution of variation types (Figure 4) and provide qualitative examples (Figure 5) of real label variations. To facilitate this, we use the FiftyOne [25] framework to visualize variation across the analyzed datasets. We provide an extension [link omitted for review] that identifies different types of label variations. The algorithm is designed to match as many examples as possible, although it tends to be strict in evaluating cases with three or more annotators. Detailed algorithmic descriptions, more qualitative examples, and an extended quantitative results section on the other datasets and instance segmentation can be found in Appendix C."}, {"title": "6. Conclusion", "content": "In our study, we address our central research question by enhancing the understanding of how annotation quality impacts model performance, introducing a straightforward and effective method for determining label convergence, which establishes a theoretical upper bound on model performance.\nOur analysis shows that while state-of-the-art (SOTA) results approach this upper bound for the well-studied LVIS dataset, the primary constraint is not model or data convergence but label quality. This suggests that models have sufficient capacity to handle the complexity of current object recognition problems. However, the current reliance on \"gold standard\" labels, despite inherent annotation variations, requires improvement in how problems are specified. We propose a combination of three key aspects to address this issue:\nImproving Annotation Quality: Implement better guidelines and training for annotators to reduce variability and errors. The issue of label convergence should be addressed as early as possible, shifting the burden from the annotations themselves to clearer problem specification. This is particularly crucial for test data, where consistency is key to accurate evaluation.\nIncluding Multi-Annotated Data: Since labeled data will likely always include some degree of variation, having a portion of the data annotated multiple times allows for an analysis of the extent of these variations. This enables models to become more robust to real-world scenarios by recognizing and accounting for annotation inconsistencies [2].\nUpdating Evaluation Methods: Reevaluate how strictly we distinguish between correct and incorrect annotations. Rather than aiming to eliminate all test set noise, we propose using the concept of label convergence as a measure of ambiguity for unavoidable annotation inconsistencies. This approach recognizes that some level of variability in labels is inevitable and should be incorporated into the evaluation process, leading to a more flexible and realistic assessment of model performance.\nIn summary, our study emphasizes the critical role of label quality in achieving optimal model performance, and we propose a more nuanced approach to problem specification and evaluation that takes into account unavoidable annotation variability."}, {"title": "7. Outlook", "content": "Our study is limited by focusing solely on computer vision, without comparisons to fields like natural language processing (NLP). We believe these domains differ significantly, as language inherently involves ambiguities, while computer vision often aims for a single, but sometimes unattainable, ground truth."}, {"title": "Appendix A: Dataset exclusion criteria", "content": "While the two reannotated datasets introduced by Ma et al. [21] initially provided a valuable resource for determining convergence thresholds, we encountered several issues that prevented accurate threshold determination:\nDifferent Annotation Guidelines: The datasets did not adhere to the same guidelines. Since they were annotated by different groups with varying annotation pipelines and guidelines, the annotation variations cannot be attributed to regular issues shown in Figure 2. These are not ambiguities within a single guideline but rather differences between distinct guidelines, resulting in label conventions that deviate significantly from the guideline.\nSampling Bias: The reannotation process exhibits a sampling bias. Images were selected for reannotation based on the presence of at least one of the five chosen classes. This selection process focused on false positives while potentially overlooking false negatives, thereby skewing the dataset.\nAnnotation Inconsistencies: There were inconsistencies in annotation formatting, with some annotations being untraceable to their corresponding images and vice versa. This suggests that some annotation files were incomplete.\nSuspicious IoU Matches: Anomalously high instances of perfect IoU (Intersection over Union) matches (1.0) were noted, indicating possible annotation duplication from the original datasets, although this was not explicitly confirmed in their documentation. LVIS, TexBiG, and VinDr-CXR did not contain a single instance with a 1.0 IoU overlap.\nLimited Class Coverage: Only five classes were selected for reannotation, reducing the Open Images dataset to approximately 5,000 images due to resource constraints. Extrapolating the convergence threshold from these five classes to the entire dataset decreases the validity of the estimated convergence threshold.\nDue to these points, the reannotated datasets present limited validity and generalizability. Consequently, we decided not to determine label convergence using these reannotated versions, as we do not see results on these datasets as reflective of the remaining commonly used COCO dataset. However, we still use the data to fit the linear regression, as they reflect real annotation variations, which we prefer over synthetic data."}, {"title": "Appendix B: Recap of Krippendorff's Alpha for Object Detection", "content": "To evaluate annotation consistency, we use the method introduced by Tschirschwitz et al. [38], which adapts Krippendorff's Alpha (K-a) for object detection. This method calculates a single a value to measure inter-annotator agreement, where a = 1 indicates perfect agreement, a = 0 indicates no agreement, and a < 0 indicates disagreement. The general form of K-a is \ud835\udefc = 1 \u2212  \ud835\udc37\ud835\udc5c/ \ud835\udc37\ud835\udc52 , where Do is the observed disagreement and De is the expected disagreement.\nCalculation Procedure\nUsing our prior definition of annotations from Section 1 where a single annotation is described as \u1ef9; which refers to annotation j for image i annotated by annotator r, the following steps are executed for a single image i:\nLocalization Overlap Calculation: The intersection over union (IoU) is calculated between different annotators r for each of their respective instances. For example take annotator A and annotator B.\n\ud835\udc3c\ud835\udc5c\ud835\udc48(\u1ef9\ud835\udc34\ud835\udc57, \u1ef9\ud835\udc35\ud835\udc58) =  \u1ef9\ud835\udc34\ud835\udc57 \u2229 \u1ef9\ud835\udc35\ud835\udc58/ \u1ef9\ud835\udc34\ud835\udc57 \u222a \u1ef9\ud835\udc35\ud835\udc58   (2)\nCost Matrix and Matching: A cost matrix is created using the function:\n\ud835\udc36(\ud835\udc57, \ud835\udc58) = 1 \u2212 \ud835\udc3c\ud835\udc5c\ud835\udc48(\u1ef9\ud835\udc34\ud835\udc57, \u1ef9\ud835\udc35\ud835\udc58)   (3)\nAssume that annotator A has MA annotations and annotator B has MB annotations for image i. The sets are matched using the Hungarian algorithm, ensuring MA = MB by padding the smaller set with \u00d8. For multiple annotators (R > 2), a greedy matching is algorithm is applied between the matched sets.\nReliability Data and Coincidence Matrix: After matching, reliability data is organized into a coincidence matrix with values Ock representing the number of c-k pairs (referring here to a pair of categories assigned to the same unit by different annotators) for each instance (unit) u, calculated as:\n\ud835\udc42\ud835\udc50\ud835\udc58 =  \u03a3\ud835\udc62  \ud835\udc41\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc50\u2212\ud835\udc58 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f\ud835\udc60 \ud835\udc56\ud835\udc5b \ud835\udc62 /\ud835\udc40\ud835\udc62  (4)\nwhere mu is the number of annotators (observers) for unit u, so how many annotators found the same instance u. From this, we calculate:\nn\ud835\udc50 =  \u03a3\ud835\udc58  \ud835\udc42\ud835\udc50\ud835\udc58 and  n = \u03a3\ud835\udc50  \ud835\udc5b\ud835\udc50  (5)\nHere, ne represents the total number of times category c was assigned across all units, and n is the total number of paired observations across all categories."}, {"title": "Appendix C: Additional Material - Annotation Variation Type Analysis", "content": "For counting the variations, we employ an algorithm designed to match as many instances as possible. The algorithm requires three elements for each image: 1) the annotations, 2) an IoU threshold, and 3) a list of annotators assigned to this image. For each possible pair of annotators, their respective instance IoU is calculated. Using this localization information:\nMatching of Correct Instances: Instances of the same class are matched starting with the highest overlapping pair of instances until the last pair with an IoU value greater than or equal to the IoU threshold. These instances are then excluded from further matching.\nMatching of Merged/Unmerged Instances with Correct Classes: In the next step, all remaining instances from each annotator are merged within their own class. These merged instances are then included in the IoU evaluation, and the same matching procedure is executed again, excluding possible matches.\nMatching of Wrong-Class Instances: Instances with correct localization but mismatching classes are matched next, following the same procedure, this excludes the previously merged instances.\nMatching of Merged/Unmerged Instances with Incorrect Classes: Similar to step 2, merged instances"}]}