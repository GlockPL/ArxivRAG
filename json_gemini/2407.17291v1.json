{"title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?", "authors": ["Leo Yu-Ho Lo", "Huamin Qu"], "abstract": "In this study, we address the growing issue of misleading charts, a prevalent problem that undermines the integrity of information dissemination. Misleading charts can distort the viewer's perception of data, leading to misinterpretations and decisions based on false information. The development of effective automatic detection methods for misleading charts is an urgent field of research. The recent advancement of multimodal Large Language Models (LLMs) has introduced a promising direction for addressing this challenge. We explored the capabilities of these models in analyzing complex charts and assessing the impact of different prompting strategies on the models' analyses. We utilized a dataset of misleading charts collected from the internet by prior research and crafted nine distinct prompts, ranging from simple to complex, to test the ability of four different multimodal LLMs in detecting over 21 different chart issues. Through three experiments-from initial exploration to detailed analysis-we progressively gained insights into how to effectively prompt LLMs to identify misleading charts and developed strategies to address the scalability challenges encountered as we expanded our detection range from the initial five issues to 21 issues in the final experiment. Our findings reveal that multimodal LLMs possess a strong capability for chart comprehension and critical thinking in data interpretation. There is significant potential in employing multimodal LLMs to counter misleading information by supporting critical thinking and enhancing visualization literacy. This study demonstrates the applicability of LLMs in addressing the pressing concern of misleading charts.", "sections": [{"title": "1 INTRODUCTION", "content": "Misleading visualizations have long been identified and included in public discourse. In the 1950s, Huff published How to Lie with Statistics [15], a book that surfaced the deceptive nature of poorly constructed charts with examples from newspapers. These charts manipulated the data's visual representation to appear supportive of their intended claims. Recognizing these discrepancies is crucial when harnessing the power of data visualizations to inform rather than mislead. Education and critical scrutiny remain the most effective tools for identifying mis-leading visualizations [3,8]. Conversely, developing automated tools to detect misleading charts represents a promising area of research.\nRecent advancements in the automatic detection of misleading visu-alizations have largely centered around visualization linters [7, 19,27]. These tools scrutinize the structural programming of charts, identifying violations of established visualization guidelines. Designed to integrate with visualization authoring tools, they alert creators to the potential for misleading content in their charts before publication. Essentially, these innovations serve as a safeguard on the producer side. However, the challenge becomes more complex for data consumers, who frequently encounter visualizations in the form of unstructured bitmap images, of-ten embellished with diverse styles and annotations. This variety poses significant challenges for automated systems, leading to a critical gap: the lack of tools to assist or safeguard the everyday consumers of data visualizations, mainly the general public. Bridging this gap is crucial, and we urgently need tools to enable data consumers to navigate and interpret data visualizations accurately.\nThe recent development of Large Language Models (LLMs) [1] has opened up massive new opportunities, making it possible to tackle complex problems that were once impossible for computer algorithms to solve. Previous studies have already showcased the remarkable abili-ties of LLMs to reason logically and interpret data [11]. While these models were originally designed for processing and generating text, the introduction of multimodal LLMs has been a pivotal advancement [30] for visualization research. These multimodal models can understand different types of input, including images, significantly broadening the ways they can be used. This advancement in LLMs offers a promising approach to a crucial issue: detecting misleading charts from a con-sumer's point of view. The capabilities of multimodal LLMs open up the possibility of creating tools that help people who use data visualiza-tions to better navigate and comprehend visual information, filling a vital need in our digital world.\nDo multimodal LLMs possess the nuanced understanding re-quired to identify and flag misleading elements in data visualiza-tions? To explore this question, our study undertook a thorough assess-ment of three proprietary [29, 30, 35] and one open-source multimodal LLM [22]. Given that the effectiveness of LLMs is significantly shaped by the textual prompts fed them, our initial step was to conduct an exploratory experiment. This involved crafting three sets of prompts designed to guide the LLMs in recognizing five specific issues within charts. Pushing the boundaries further, our investigation expanded to assess the LLMs' capabilities in handling an increasing complexity of problems, conducting further experiments that presented the models with charts containing 10 and then 21 different issues.\nOne of the challenges we faced was the growing difficulty in scaling the number of issues for detection by the LLMs as we broadened the range of issues to be detected. As we expanded the scope, we inevitably had to increase the level of detail of descriptions and instructions, leading to longer prompts and responses. This escalation resulted in an increase in the length of both the prompts and the generated responses, presenting a significant challenge. Despite advancements that have expanded the context length LLMs can handle, the processing and generation of lengthy texts remains a computationally intensive and memory-demanding task. Utilizing the insights gained from our exploratory experiments, our ninth and final prompt was designed to guide the LLMs in detecting 21 different issues. Instead of designing a single prompt, our final attempt was to generate prompts dynamically, facilitated within a multi-round conversation setup.\nOur evaluation revealed the exceptional ability of multimodal LLMs to interpret charts presented as bitmap images. Following our instruc-tions, these models demonstrated the capabilities of recognizing differ-ent chart elements, exercising critical thinking in data interpretation, and detecting a wide range of issues in misleading charts. Interestingly, LLMs consistently sought additional context for the charts, showcasing an innate caution that proved instrumental in uncovering issues like dubious data sources and concealed information. Their proficiency in detecting charts with fabricated data was particularly impressive-a"}, {"title": "2 RELATED WORK", "content": "This work intersects four areas of research: (1) Misleading Visual-izations, (2) Visualization Linters, (3) Chart Analysis with Computer Vision, and (4) Chart Question Answering with LLMs.\n2.1 Misleading Visualizations\nThe discourse on misleading visualizations dates back to the pre-digital era, notably beginning with Darrell Huff's seminal work, How to Lie with Statistics [15], in the 1950s. In this book, Huff exposed the ma-nipulative potential of data in journalism. Subsequently, in the book The Visual Display of Quantitative Information [36], Tufte introduced essential concepts like Graphical Integrity and Lie Factors, advancing the discussion on how visual data representations can distort the truth. More recently, Cairo's How Charts Lie [6] offers a contemporary view-point on the issue, exploring the intricacies of recognizing dubious data sources and the malicious intentions behind visual designs, particularly in the digital media landscape. These publications provide a compre-hensive foundation that informs our understanding of the complexities and ethical issues in data visualization.\nRecent academic research has significantly deepened our understand-ing of misleading visualizations, especially in today's digital era, where misinformation can quickly proliferate online. Researchers like Pandey et al. [32] and Correll et al. [9] have revealed the subtle ways visualiza-tions can be manipulated-for example, by truncating axes-to drastically alter data interpretation. This expanding body of research emphasizes the urgent need for vigilance in data presentation and perception. Fur-ther studies by Lee et al. [18] and Lisnic et al. [20] investigate how visualizations contribute to the spread of misinformation on social me-dia and other platforms, underlining the extensive consequences of these deceptive techniques. In an effort to construct a taxonomy of these deceptions, Lo et al. [24] compiled a wide range of misleading visualizations from the internet, creating a detailed taxonomy with 74 unique issues. These academic contributions build upon the discussions initiated by early literature and highlight the continuous challenge and significance of devising methods to counter misinformation in data visualization. These works offer crucial insights into the mechanics of deceit and the essential role of integrity in data's visual representation.\nData visualization literacy and critical data thinking is an important topic in education. Chevalier et al. have highlighted the significance of embedding critical thinking within visualization literacy education, advocating for its introduction as early as elementary education [8]. This sentiment is echoed and expanded upon at the university level by Bergstrom and West. In their semester-long course, they instructed stu-dents to critically assess the data they encountered, especially through data visualizations. Their work has culminated in a comprehensive book on the subject [4]. In today's data-driven world, the ability to critically evaluate and interpret information become an essential skill. Data plays a crucial role in decision-making, influencing policies, and shaping public opinion, highlighting the critical need for individuals to possess the skills to scrutinize data with skepticism and insight. It's vital to embed critical data thinking within educational curricula and to develop and sharpen tools that enable the public to effectively engage with data visualizations. Such tools are essential in cultivating an in-formed society that approaches data visualizations with a critical eye, promoting a more nuanced and discerning consumption of information in our data-intensive reality.\n2.2 Visualization Linters\nIn the field of data visualization, significant progress has been made in creating automated systems that are designed to help chart creators produce visualizations that are clear and not misleading. Inspired by the concept of linters in computer programming-which detect potential errors in code to alert programmers about possible issues leading to incorrect or inefficient outcomes-these visualization linters are a step forward in ensuring the integrity of visual data representations. McNutt and Kindlmann introduced a linter for matplotlib, a popular charting library, implementing rules derived from the Algebraic Visualization Design (AVD) framework to improve the clarity and effectiveness of charts [27]. Similarly, VizLinter [7] uses Answer Set Programming (ASP) to analyze charts created with Vega-Lite, another popular chart-ing library, by scrutinizing chart specifications with established best practices. Expanding the scope, GeoLinter [19] applies cartographic principles to map visualizations in Vega-Lite, ensuring that they are not misleading from a cartographic perspective. These tools highlight the research focus on the production side of data visualization, emphasizing tools that integrate seamlessly with visualization libraries to support chart creators.\nWhile existing research on automated detection systems has largely focused on aiding chart creators in the production process, there is a growing emphasis on empowering consumers to critically evaluate the accuracy and reliability of visual data presentations. Toward this end, Fan et al. have pioneered a system specifically for analyzing line charts in bitmap format [12]. This system begins with reverse engineering [33] the chart to extract visualization specifications from its bitmap image. It then compares these specifications against established best practices, alerting users to any discrepancies or violations directly on the chart im-age with overlays highlighting misleading aspects alongside corrected versions or annotations, facilitating a more informed interpretation of the data. Similarly, Hopkins et al. have introduced a visual interface that flags potential issues within charts, directing users' attention to areas that might lead to misinterpretation [13]. Lo et al. analyzed and evaluated six different explanation strategies in explaining common charting issues to the general public [23]. These innovations are crucial for improving data literacy among visual data consumers, yet their success heavily relies on the accuracy of chart specification extraction. Even slight errors at this stage can compromise the system's effective-ness, emphasizing the importance of precision in chart analysis and visualization reverse engineering.\n2.3 Chart Analysis with Computer Vision\nChart analysis is a fast-developing area within computer vision research focused on extracting data and facilitating question-answering through visual representations. An initial effort in this field is the creation of the FigureQA dataset, which includes over 100,000 chart images and more than one million binary yes-or-no question-answer pairs, serving as a critical benchmark for evaluating progress in chart analysis [17]. This initiative has encouraged further research, with projects expanding the dataset through synthesis or compilation of real-world charts, enriching the resources for development. Highlighting the community's dedication to chart analysis, the annual Competition on Harvesting Raw Tables from Infographics [10], initiated by Davila et al., invites participants to complete seven complex chart analysis tasks. Together, these tasks form a pipeline that starts with identifying chart types from the chart image, then detecting text, classifying text roles, analyzing axes and legends, identifying plot elements, and finally extracting data. The competition, with its annotated dataset of over"}, {"title": "3 METHODOLOGY", "content": "Our experiments aim to explore the capabilities of multimodal LLMs in identifying misleading visualizations and distinguishing them from more specialized machine learning models. LLMs are known for their impressive ability to generalize and adapt to a wide range of tasks beyond their initial programming through methods like zero-shot, one-shot, or few-shot learning [2]. The ability of multimodal LLMs in chart understanding, critically assessing the deceptiveness of these visual-izations, and the potential of applying multimodal LLMs in detecting misleading charts are unexplored. This investigation seeks to deter-mine if such capacities exist and, if so, how crafting prompts alongside the chart images can effectively direct LLMs to produce the intended analytical outcomes.\nMultimodal LLMs: Our experiments covered four distinct multi-modal LLMs: (1) ChatGPT by OpenAI (gpt-4-1106-vision-preview) [30], (2) Copilot by Microsoft (model name not specified) [29], (3) Gemini by Google (gemini-pro-vision) [35], and (4) LLaVA-NeXT (llava-v1.6-mistral-7b-hf) [22]. ChatGPT, Copilot, and Gemini are developed by proprietary companies, whereas LLaVA-NeXT is an open-source multimodal LLM. Notably, at the time of our experiments, LLaVA-NeXT has achieved state-of-the-art results across various mul-timodal LLM benchmarks among open-source LLMs. Due to hardware constraints, our experiments were conducted with the 7B parameter vari-ant of LLaVA-NeXT, despite the more advanced state-of-the-art 34B parameter model. Nevertheless, including LLaVA-NeXT in our evalua-tion covers an important landscape of LLM research. The LLaVA-NeXT model was run on a server with an Nvidia Titan RTX 24GB graphical processing unit. On the other hand, the proprietary models were eval-uated via their Application Programming Interfaces (APIs). We have made the program code of the experiments publicly available on the OSF1.\nDataset: Our study compiled an evaluation dataset from previous research on misleading charts circulated on the internet [24]. The dataset was collected through search engines and social media. The chart images are annotated with the issues identified from the original web page or social media post. A total of 74 unique chart issues were identified in the study, providing a diverse sample of misleading charts that the general public may encounter.\nIn the initial phase of our exploratory study, we focused on the five most frequently identified issues: (1) Truncated Axis, (2) 3D Chart, (3) Missing Title, (4) Dual Axis, and (5) Misrepresentation. We subse-quently expanded to include the ten most common issues in Experiment Two by including (6) Missing Axis Title, (7) Missing Legend, (8) In-consistent Tick Intervals, (9) Not Data, and (10) Selective Data. In the third experiment, the study was further broadened to cover up to 21 issues due to a tie for the 20th spot, with both the 20th and 21st issues appearing an equal number of 25 times in the dataset. These ad-ditional issues included (11) Dubious Data, (12) Missing Value Labels, (13) Area Encoding, (14) Overusing Colors, (15) Inappropriate Axis Range, (16) Indistinguishable Colors, (17) Ineffective Color Scheme, (18) Discretized Continuous Variable, (19) Missing Normalization, (20) Missing Axis, and (21) Inconsistent Binning Size. For each issue, we randomly sampled six images, dividing them equally into development and test sets, resulting in sets for evaluation across five (N=30), ten (N=60), and 21 issues (N=126).\nMoreover, we included a set of valid charts to evaluate if LLMs can distinguish between misleading and accurate representations. This valid set, randomly sampled from a collection by Brokin et al. [5], consisted of chart images from various sources, including news and governmental organizations. Through stratified sampling from this pool of 2,000 chart images-excluding infographics and charts with issues like missing titles or 3D effects-we assembled a valid chart set of 24 images. This set was evenly sampled from four sources, each contributing six images, and covered six different chart types, each represented by four images. This sampling method ensured wide coverage of sources and chart types, minimizing bias toward particular charting styles, data types, or chart types. Unlike the misleading chart set, which was expanded throughout the progression of our experiments,\nWe began the investigation with an exploratory experiment, col-lecting various visualizations, including misleading and valid charts. The objective of the exploration phase is to establish a baseline for evaluating LLMs' ability to distinguish between these two types of charts. Building upon the insights gained in the exploratory experiment, the second phase of the experiments sharpens the focus on enhancing and broadening the prompts to capture a wider range of chart issues. However, this expansion poses scalability challenges, particularly in managing the length of the prompts and also the length of the out-puts from the LLMs, leading to degraded performance. To address these issues, the third phase investigates strategies to overcome these limitations, facilitating a more exhaustive examination of the 21 most common issues in charts. Each stage progressively deepens our un-derstanding of multimodal LLMs' analytical capabilities in detecting visual deceptions.\nthe valid chart set remained the same throughout the study. The valid set is divided evenly into development and test sets while maintaining its stratified property.\nFor each of our experiments, we organized the datasets as follows: (1) for Experiment One, we had a total of 54 images (N = 54), with 27 in the development set (Ndev = 27) and 27 in the test set (Ntest = 27); (2) for Experiment Two, the total was 84 images (N = 84), split equally into 42 for development (Ndev = 42) and 42 for testing (Ntest = 42); and (3) for Experiment Three, we worked with 150 images (N = 150), divided into 75 for development (Ndev = 75) and 75 for testing (Ntest = 75). Unlike traditional machine learning models, our study lacked a training phase, hence the development set was used for creating prompts, and the test set was strictly for evaluation purposes.\nEvaluation Metrics: Throughout the various stages of our experi-"}, {"title": "4 EXPERIMENT ONE: EXPLORATION PHASE", "content": "Our understanding of how to prompt multimodal LLMs to identify misleading charts-a task they have not previously encountered-is limited. Thus, we began our exploration with a straightforward prompt: \"Does this chart depict data truthfully or misleadingly?\". Through this approach, we aim to incrementally gain insights into effectively interacting with LLMs for the purpose of developing an automated system capable of detecting misleading visualizations.\nTo initiate this exploration, we started with a small set of misleading (Nmisleading = 30) and valid (Nvalid = 24) charts divided equally into a development set and a test set. To analyze the results objectively, with-out relying on our interpretation of the text generated by the LLMs, we re-input the dialogues generated in response to each prompt back into the LLMs, requesting the LLMs to summarize the dialogue in JSON format. This method allows for a structured and unbiased analysis of the LLMs' responses.\nIn this preliminary experiment, we tested three types of prompts: (1) direct inquiry, (2) Likert scale assessment, and (3) Chain of Thought (CoT) reasoning [38]. We evaluated the outcomes based on accuracy, F1-score, and relevance of the responses generated by the models.\nPrompt #1 (Direct Ask): The initial prompt we experimented with was directly asking: \"Does this chart depict data truthfully or mislead-ingly?\" When applied through web user interfaces, the responses often elaborated on the characteristics of misleading visualizations and pro-vided a checklist rather than offering a direct \"truthful\" or \"misleading\" answer. To guide the responses more effectively, we appended the instruction with \"Answer 'truthful' or 'misleading'. Name the issues and explain your answer.\" following the initial question. After receiving each response, we formatted the question and response into a dialogue and requested that the LLMs summarize this dialogue in JSON format. An illustration of various prompts and responses from different LLMS is presented in Fig. 1.\nResults of Prompt #1: In the test set comprising 27 responses, most LLMs labeled the charts as \"misleading,\" resulting in high recalls and low precisions. ChatGPT showed a marginally better capability by correctly identifying three out of 12 valid charts. The issue of misrepre-sentation was the most commonly cited problem, with responses noting concerns such as \"lack of scale consistency\" and \"disproportional rep-resentation.\" Tab. 1 shows the results of Experiment One.\nDespite the tendency of LLMs to criticize charts excessively, the results were encouraging in terms of their ability to identify certain issues, such as the inappropriate use of 3D effects, dual axes, and truncated axes. On the other hand, the absence of titles on charts did not seem to be flagged as a problem by any of the LLM responses. While not directly misleading, the omission of a title results in a loss of context, rendering the chart less comprehensible. The concept of \"lacking context\u201d was, however, frequently mentioned in the feedback. Fig. 4 shows the summary of the issues recognized by the LLMs across different prompts.\nPrompt #2 (Likert Scale): Based on the findings of Prompt #1, we considered introducing more nuance to the responses using a 5-point Likert scale, where the midpoint represents insufficient information to make a definitive judgment. This adjustment allows the LLMs to provide answers that aren't strictly \u201ctruthful\u201d or \u201cmisleading.\" We grouped their responses into three categories: scores of 1 or 2 as \"truthful,\" a score of 3 as \"neither,\" and scores of 4 or 5 as \"misleading.\" In addition to accuracy and the F1-score, it became necessary to track the proportion of responses categorized as \u201cneither.\u201d\nResults of Prompt #2: When utilizing this more nuanced prompt, all LLMs exhibited a decline in performance compared to the initial prompt, with the exception of Copilot, which achieved a perfect F1-score. Copilot's responses were particularly notable, with 21 out of 27 categorized as \"neither,\" alongside three correctly identified as misleading and three as truthful. Despite this, the relevance of the responses to the specific issues present in the charts was minimal. Overall, using a Likert scale prompt did not effectively guide LLMs in pinpointing issues within charts.\nPrompt #3 (Chain of Thought): In the third prompt, we imple-mented the CoT prompting strategy, which encourages LLMs to un-dergo a reasoning process before delivering a definitive answer. This approach is inspired by the methodology that guides LLMs through a step-by-step analytical process, culminating in a reasoned conclu-sion [38].\nThe prompt was designed to inquire about critical aspects of potential charting issues, such as the presence of a title, the use of 3D effects, and whether the axis begins at zero, among others. Subsequently, we presented a set of guidelines and questioned whether the chart adhered to all these criteria.\nResults of Prompt #3: Despite the F1-scorefor this prompt being similar to those of the first prompt, there was a notable enhancement in the relevance of the responses. By providing clear instructions, the"}, {"title": "5 EXPERIMENT TWO: CONSOLIDATION AND EXPANSION PHASE", "content": "In the first experiment, we derived three key insights regarding the efficacy of prompting LLMs to perform the task of detecting mislead-ing charts: (1) posing factual questions and providing a checklist are beneficial, (2) LLMs tend to avoid giving definitive answers, and (3) LLMs followed our naming of the issues.\nIn Experiment One, we found that the third prompt, which utilized the CoT strategy, was particularly effective in guiding LLMs to pinpoint issues in charts. While the CoT strategy has proven effective in previous studies [38], our construction of Prompt #3 included reasoning steps and a checklist of potential chart issues. We are interested in distinguishing between the impacts of the checklist and the CoT strategy. To explore this, Prompt #5 replicates the approach of Prompt #3, and Prompt #4 employs only a checklist, omitting the CoT reasoning steps.\nAs the experiment progressed to include more issues, the length of prompts, incorporating extensive issue definitions and related fac-tual questions, hinted at potential scalability challenges. Particularly, Prompt #5 revealed that posing numerous questions within a single prompt might lead LLMs to overlook earlier questions, focusing instead on the final checklist question only. To address this, Prompt #6 adopts a multi-pass conversation approach, dividing the extensive Prompt #5 into smaller prompts within a continuous dialogue format. This forces the LLMs to answer all the factual questions and reasoning steps before jumping to the checklist question to give the final answer.\nIn the second experiment, we expanded the scope to include the ten most common issues, thereby enlarging the dataset of misleading charts. The dataset for this phase comprised 60 misleading charts with the same set of 24 valid charts in the first experiment. Both sets are evenly divided into development and testing sets.\nGiven the dataset's imbalance between misleading and valid charts, we determined that accuracy was inadequate and opted to rely on the F1-score instead. In Prompts #5 and #6, the LLMs were prompted to include the final judgment of chart issues and the answers to the factual questions. Therefore, for Prompts #5 and #6, we also evaluated the accuracy of responses to factual questions. Fig. 2 shows the prompts evaluated in Experiment Two.\nPrompt #4 (Checklist Only): We instructed LLMs to go through a checklist and categorize identified issues into three levels of concern: major, minor, and either potential or inconclusive pitfalls.\nPrompt #5 (Chain of Thought): The prompt was structured on top of the checklist in Prompt #4 in a tiered manner, divided into three segments. The first segment posed basic inquiries regarding the chart's structure, including the type of chart, presence of 3D effects, axis, legends, and data source. The second tier involved more complex questions necessitating interpretation, such as determining the data type represented on the axis, the range of the axis, and whether the axis ticks were evenly spaced. Given the observed ability of LLMs to contextualize their analysis based on chart content, we investigated whether the information presented in the chart alone was sufficient for understanding, or if additional details such as a chart title or axis titles were necessary. The final set of questions, addressing the data directly, was the most intricate, probing whether the data might have been selectively chosen or fabricated.\nPrompt #6 (Split Chain of Thought): During the execution of Prompt #5, we noted instances where the LLMs' generated responses either reached the output limit or the models bypassed the initial two sets of questions, opting to respond directly to the final checklist. To address this, we segmented the prompt into three parts, presenting them to the LLMs in successive rounds while incorporating the preceding dialogue. This method allowed for a more manageable and structured approach to eliciting detailed analyses from the LLMs.\nExperiment Two Results and Discussion In categorizing the re-sponses, we classified charts as \"misleading\" if they presented at least one major issue; otherwise, they were deemed valid. The outcomes of Experiment Two are summarized in Tab. 2. While the three prompts yielded a similar F1-score, the quantity and relevance of the issues identified varied across different prompts. Specifically, LLMs reported a greater number of irrelevant issues when responding to the checklist in Prompt #4. In contrast, when engaging with the factual questions in Prompts #5 and #6, the incidence of reported issues decreased, yet the mention of relevant issues remained fairly consistent. Regarding these factual questions, the LLMs demonstrated a surprising ability to understand the charts, accurately answering questions about recogniz-ing different chart components, and correctly interpreting scales and encodings. This was achieved without specific training or one-shot or few-shot demonstrations, relying solely on the instructions within the prompts, showcasing the LLMs' impressive capacity for visual inter-pretation. Fig. 5 shows the results of the factual questions answered by the LLMs.\nAn interesting discovery from this experiment was the LLMs' ability to identify charts intended as parodies created with fictitious data rather than genuine information. Fig. 2 shows an example chart that was flagged as \"Not Data,\" with advisories against considering it as factual. During this experiment, it was noted that LLaVA-NeXT experienced issues with repetitive outputs, echoing certain sentences until they reached the maximum token count and terminated, leading to outputs that contained considerable repetitions. This significantly impacted its performance. Similarly, Gemini indicated errors suggesting that it faced issues with repetitions, too. To address these erratic responses,"}, {"title": "6 EXPERIMENT THREE: TACKLING SCALABILITY ISSUES", "content": "In Experiment Two, expanding the scope to cover ten issues revealed challenges associated with lengthy prompts or responses, leading to pre-mature termination and omission of reasoning questions. Additionally, despite LLMs correctly answering the factual questions in Experiment Two, they occasionally made contradictory decisions in their final as-sessments, such as noting a missing title but not recognizing it as an issue in their conclusive response. We incorporated more interpretive questions within the prompts to refine this aspect in Experiment Three.\nPrompt #7 (Split Chain of Thought): The initial layer consisted of basic factual inquiries, followed by a second layer examining data types, encoding, and scales, akin to the approach in Prompt #6. The third layer required LLMs to evaluate the chart more critically, considering the necessity of explicit titles and axes or the suitability of the axis range and color scheme. The strategy of segmenting prompts into multiple conversational passes proved effective in Experiment Two. Yet, as we sought to identify a broader range of 21 chart issues in Experiment Three, the length of each segmented prompt also increased considerably. The resulting full prompt comprises approximately 6,500 characters and totaling 1,550 tokens. Fig. 3 shows an example of Prompt #7.\nPrompt #8 (Direct JSON Output): Directly instructing LLMs to go through a checklist was deemed ineffective based on findings from Experiment Two. In Experiment Three, we tried to condense responses while maintaining the reasoning steps by instructing LLMs to produce a direct issue list in JSON format.\nPrompt #9 (Dynamic Chain of Thought): Another approach involved dynamically constructing split prompts based on LLM re-sponses, starting with basic questions about chart properties and tailor-ing follow-up queries based on these initial answers. For instance, if an LLM identified a chart as a pie chart, subsequent questions about axis range or tick consistency were deemed irrelevant and omitted from the follow-up prompt.\nExperiment Three Results and Discussion: Prompt #8 encoun-tered issues similar to those observed with Prompt #4, with a tendency to over-report issues in the charts. Specifically, when using Prompt #8, despite a high F1-score, Copilot identified major issues in 74 charts, deeming only one chart as valid, which was inaccurately classified as a false negative. The F1-score, generally appropriate for datasets with"}, {"title": "7 DISCUSSION", "content": "LLMs demonstrated capabilities in chart comprehension. Given that the LLMs we assessed were not specifically trained to identify issues in charts, their performance exceeded our initial expectations. This was particularly notable in their ability to analyze images (a task generally reserved for computer vision systems) and their comprehen-sion of textual content within those images. As shown in Fig. 5, LLMs accurately answered most questions regarding chart properties such as chart type, data type, data encoding, titles, and the use of 3D effects. However, they encountered difficulties with questions related to color counting and tick consistency checking, which is a known weakness in LLMs [39]. Consequently, as illustrated in Fig. 4, LLMs performed less effectively on chart issues related to colors or inconsistencies in the taxonomy [24], including overusing colors, indistinguishable col-ors, ineffective color schemes, and inconsistent tick intervals. On the other hand, it was surprising to see that these models are capable of differentiating between fictional and factual data, especially since such discernment is challenging for rule-based algorithmic detection sys-tems. This capability was demonstrated in the identification of \"not data\" charts, where LLMs had shown a robust understanding of charts and accurately reported the clues of fictional data in the images.\nChain of Thought is the most effective prompting strategy in our experiments. Over the course of the three experiments, we ex-plored various prompting strategies to optimize LLM performance. The intermediate output aided the model in building an understanding of complex misleading elements from simple chart properties. The con-cept of misrepresentation is challenging due to the logical dissonance of graphical integrity-the chart's geometry not reflecting the textual information on the chart.\nThe CoT strategy emerged as the most effective, yet as we expanded the range of issues for detection, the prompts' length increased sig-nificantly. To address this, we tested three alternative strategies: (1) dividing the prompt into a conversational format, (2) incorporating reasoning steps without explicitly including them in the output, and (3) dynamically generating prompts based on previous responses. Our findings indicated that Prompts #4 and #8, which did not include rea-soning steps in the output, led to over-reporting issues. Based on our experiments, Prompt #7 was identified as the most effective. However, expanding the scope to encompass more issues poses a challenge, as"}, {"title": "8 CONCLUSION", "content": "In this research, we explored the application of multimodal LLMs to the challenge of automatically detecting misleading charts. Through a series of three experiments, which progressed from an initial set of five types of issues to an eventual examination of 21 issues, we identified effective strategies for prompting multimodal LLMs. This progression allowed us to refine our approach and understand the nuances of uti-lizing LLMs for this specific task. Each experiment contributed to a deeper understanding of how LLMs interpret and analyze charts and data, revealing both the capabilities and limitations of these models in the context of misleading visualizations. Our findings unveil the potential and capabilities of LLMs in augmenting traditional methods for chart analysis and identifying misleading elements in charts. The significant development of LLMs provides a promising direction for further research in developing tools to support critical thinking on data interpretation and visualizations."}]}