{"title": "LuxVeri at GenAI Detection Task 3: Cross-Domain Detection of AI-Generated Text Using Inverse Perplexity-Weighted Ensemble of Fine-Tuned Transformer Models", "authors": ["Md Kamrujjaman Mobin", "Md Saiful Islam"], "abstract": "This paper presents our approach for Task 3 of the GenAI content detection workshop at COLING-2025, focusing on Cross-Domain Machine-Generated Text (MGT) Detection. We propose an ensemble of fine-tuned transformer models, enhanced by inverse perplexity weighting, to improve classification accuracy across diverse text domains. For Subtask A (Non-Adversarial MGT Detection), we combined a fine-tuned ROBERTa-base model with an OpenAI detector-integrated RoBERTa-base model, achieving an aggregate TPR score of 0.826, ranking 10th out of 23 detectors. In Subtask B (Adversarial MGT Detection), our fine-tuned RoBERTa-base model achieved a TPR score of 0.801, securing 8th out of 22 detectors. Our results demonstrate the effectiveness of inverse perplexity-based weighting for enhancing generalization and performance in both non-adversarial and adversarial MGT detection, highlighting the potential for transformer models in cross-domain AI-generated content detection.", "sections": [{"title": "Introduction", "content": "The proliferation of advanced language models such as GPT (Radford et al., 2019) and ROBERTa (Liu et al., 2019), machine-generated content has become prevalent across social media, journalism, and academia, raising concerns about authenticity and misinformation. Detecting AI-generated text is especially challenging across diverse domains, where variations in language and style can hinder detection efforts.\nIn Task 3 of the COLING 2025 Workshop on Detecting AI-Generated Content (Dugan et al., 2025), we tackle cross-domain Machine-Generated Text (MGT) detection using an ensemble approach that combines fine-tuned ROBERTa-base models (Liu et al., 2019) and OpenAI detection tools (Solaiman et al., 2019). Our method leverages inverse perplexity weighting to enhance the contributions of high-confidence models, yielding a robust detection system.\nOur approach achieved an aggregate score of 0.826 in Non-Adversarial Cross-Domain MGT detection (Subtask A), ranking 10th, and 0.801 in Adversarial Cross-Domain MGT detection (Subtask B), ranking 8th. This paper outlines our ensemble-based methodology, dataset considerations, and insights for effective cross-domain AI-generated text detection."}, {"title": "Background", "content": ""}, {"title": "2.1 Dataset", "content": "The RAID dataset (Dugan et al., 2024), provided for the competition, is designed for evaluating machine-generated text detectors. It contains over 10 million documents across 11 language models, 11 genres, 4 decoding strategies, and 12 adversarial attacks, including both human-written and machine-generated content from 8 different domains like books, news, poetry, and recipes. For training and validation, we used the RAID-train subset (802 million words, 11.8GB) and RAID-test subset (81 million words, 1.22GB). We also utilized the RAID-extra subset, which includes languages like Czech and German (275 million words, 3.71GB). This dataset provides a comprehensive resource for AI-generated text detection.\nFor the fine-tuning of our model, we reduced the dataset by using about 10% of the publicly available data. This reduction was carried out in a balanced manner across all genres, decoding strategies, attacks, and domains to ensure that each subset was proportionally represented. Specifically, we reduced the data across the following domains: abstracts, books, news, poetry, recipes, reddit, reviews, and wiki. The distribution of this reduced data across models is shown in Table 1, with domain-specific sample sizes for each model. For example, the number of samples for \"Chat-"}, {"title": "2.2 Related Work", "content": "The detection of machine-generated text has gained attention with the rise of large language models (LLMs) like GPT (Radford et al., 2019) and BERT (Devlin et al., 2019). Fine-tuned Transformer models have succeeded in binary classification tasks, but challenges remain in cross-domain and multi-lingual contexts due to data biases (Liu et al., 2019; Solaiman et al., 2019). Ensemble methods combining models like BERT, ROBERTa, GPT variants, and perplexity-based weighting have been explored to improve domain robustness (Schick and Sch\u00fctze, 2020; Clark et al., 2019).\nRecent work in cross-domain detection shows that RoBERTa-based detectors for GPT-2 generated technical text can be transferred with few labeled examples, such as from physics to biomedicine (Rodriguez et al., 2022). Paragraph-level detection is also being explored to address document tampering in mixed-domain texts.\nFor multilingual detection, models like XLM-ROBERTa (Conneau et al., 2019) and RemBERT (Chung et al., 2021) improve cross-lingual detection, though challenges remain for low-resource languages (Hu et al., 2020). Recent SemEval tasks (Fetahu et al., 2023; Wang et al., 2024) have refined these approaches with task-specific fine-tuning. Our work builds on these methods by using inverse perplexity-weighted ensembles to enhance detection across domains and languages."}, {"title": "3 System Overview", "content": "We developed an ensemble approach for AI-generated text detection across multiple domains, using Transformer models with inverse perplexity-based weighted voting for improved accuracy. The system overview is shown in Figure 1."}, {"title": "3.1 Ensemble Model Selection and Justification", "content": "For the ensemble model, we selected two Transformer-based models tailored for Non-Adversarial and Adversarial cross-domain text detection, leveraging their strengths in capturing linguistic, syntactic, and semantic patterns essen-"}, {"title": "3.2 Data Pre-processing", "content": "For text classification, the data was preprocessed using model-specific tokenizers, incorporating truncation and padding as required. To enhance memory efficiency and training performance, texts were sorted by word count, reducing unnecessary padding. A fixed random seed was maintained to ensure reproducibility."}, {"title": "3.3 Training Procedure", "content": "The models were fine-tuned using the Hugging Face Transformers library \u00b9 for English and multilingual text classification. Tokenization was performed with 'AutoTokenizer', and the architectures were adapted for classification tasks with appropriate label mappings.\nTraining was conducted for 3 epochs with a learning rate of 2 \u00d7 10\u22125, batch sizes of 4 for training and 16 for validation, and weight decay of 0.01. Early stopping was applied with a patience of 5 evaluations and a 0.001 improvement threshold. Evaluation checkpoints were saved after each epoch, and the best-performing model was used for testing.\nThis procedure ensured robust generalization across subtasks. Further training details are provided in Table 2."}, {"title": "3.4 Ensemble Voting Strategy", "content": "Our ensemble employs a weighted soft-voting strategy, combining predictions from all fine-tuned models for each subtask. The weights are determined based on inverse perplexity, with lower perplexity values reflecting higher confidence."}, {"title": "3.4.1 Perplexity Calculation", "content": "For each model, we compute the perplexity based on its predictions. The perplexity P is computed using the Negative Log Likelihood formula:\n$P = \\exp(-\\frac{1}{N} \\sum_{i=1}^{N} \\log(p(y_i|x_i)))$ \nwhere $p(y_i | x_i)$ is the predicted probability for the true label $y_i$, and N is the number of test samples. Lower perplexity values indicate higher confidence.\nTo compute perplexity, we use each model's logits, apply softmax to obtain probabilities, and then calculate perplexity based on the true labels and these probabilities."}, {"title": "3.4.2 Perplexity-Based Weighting Adjustment", "content": "To calculate model weights, each model's perplexity is adjusted by subtracting 1, creating an effective weighting scale. The weight $w_i$ for model i is then computed as the inverse of this adjusted perplexity and normalized across models, giving higher confidence models greater influence.\n$w_i = \\frac{1/(P_i - 1)}{\\sum_{j=1}^{M}(1/(P_j - 1))}$\nwhere M represents the total number of models, and Pi is the original perplexity of model i."}, {"title": "3.4.3 Weighted Soft-Voting", "content": "Each model's predicted probabilities are scaled by its weight and summed to form the final ensemble prediction. This weighted voting prioritizes models with higher confidence (lower perplexity), giving them greater influence on the final decision. The ensemble's final prediction for each class c is:"}, {"title": "4 Results", "content": "Table 3 shows cross-domain MGT detection performance for non-adversarial and adversarial testing, with detectors ranked based on aggregate True Positive Rate (TPR)."}, {"title": "4.1 Performance", "content": "In the non-adversarial setting, the fine-tuned ROBERTa + ROBERTa OpenAI model which was fine-tuned on RAID dataset (Dugan et al., 2024) achieved the highest performance, with an aggregate (AGG) score of 0.826, ranking 10th out of 23 detectors (see Table 3). This model effectively combined fine-tuned RoBERTa Base and RoBERTa Base OpenAI models, with perplexity-based weighting to give more influence to lower-"}, {"title": "4.2 Model Comparison", "content": "The performance of various detectors was evaluated under both non-adversarial and adversarial conditions, revealing key insights into their strengths and limitations.\nIn the non-adversarial setting, FT ROBERTa + ROBERTa OpenAI emerged as the top performer, achieving an AGG TPR of 0.826 and ranking 10th overall. It demonstrated exceptional performance with models such as ChatGPT (TPR: 0.960) and GPT-4 (TPR: 0.861), outperforming FT ROBERTa (AGG TPR: 0.813, ranked 12th) and the ensemble model FT ROBERTa + ROBERTa OpenAI + BERT (AGG TPR: 0.825, ranked 11th). Interestingly, Binoculars (Hans et al., 2024) showed strong"}, {"title": "5 Limitations", "content": "Our approach, while effective, has several limitations. Focusing on RoBERTa models for fine-tuning and ensemble weighting excluded alternatives like RemBERT (Chung et al., 2021) and XLM-ROBERTa (Conneau et al., 2019), which might better handle longer sequences, noisy data, and multi-label tasks.\nDue to computational constraints, we trained on a subset of the RAID dataset, limiting the model's ability to capture its full diversity. Training on the full dataset could greatly improve detection performance, especially for underrepresented domains.\nPerformance variability across generator models (e.g., GPT-4 vs. Mistral) and limited multilingual capabilities highlight the need for better cross-domain generalization and robust multilingual detection. While the ensemble approach enhanced generalization, it increased computational overhead, warranting exploration of more efficient strategies in future work."}, {"title": "6 Discussion and Conclusion", "content": "In this paper, we proposed an ensemble-based approach for cross-domain MGT detection, combining fine-tuned RoBERTa Base and RoBERTa Base OpenAI detectors with inverse perplexity weighting. Our method achieved competitive results, ranking 10th and 8th in non-adversarial and adversarial tasks, respectively, in Task 3 of the GenAI content detection workshop at COLING-2025. Inverse perplexity weighting improved generalization by prioritizing more confident models across diverse domains. For non-adversarial tasks, we explored an inverse perplexity-based ensemble approach. However, the detectors in this ensemble underperformed compared to the fine-tuned RoBERTa model, highlighting the value of fine-tuning on task-specific data and suggesting avenues for refining ensemble techniques.\nOur results show that transformer-based models, particularly RoBERTa, are effective for non-adversarial and adversarial MGT detection. For non-adversarial detection (Subtask A), we achieved a score of 0.826, and for adversarial detection (Subtask B), we scored 0.801. However, cross-domain detection remains challenging, especially with varied generator models and multilingual data. Our system performed well with generators like ChatGPT and GPT-4 but struggled with others like Cohere and Mistral, indicating the difficulty of detecting diverse machine-generated content.\nDue to limited computational resources, we trained on a subset of the available data. Despite this, our models performed well, demonstrating the potential of our approach even with partial data. This work lays the foundation for further progress in MGT detection, especially in adversarial and cross-lingual settings. Future research can focus on enhancing multilingual capabilities, incorporating more diverse language models, and exploring dynamic ensemble strategies to improve performance across domains and attack scenarios."}, {"title": "A Appendix", "content": ""}]}