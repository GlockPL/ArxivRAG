{"title": "Towards Understanding the Robustness of LLM-based Evaluations under Perturbations", "authors": ["Manav Chaudhary", "Harshit Gupta", "Savita Bhat", "Vasudeva Varma"], "abstract": "Traditional evaluation metrics like BLEU and ROUGE fall short when capturing the nuanced qualities of generated text, particularly when there is no single ground truth. In this paper, we explore the potential of Large Language Models (LLMs), specifically Google Gemini 1, to serve as automatic evaluators for non-standardized metrics in summarization and dialog-based tasks. We conduct experiments across multiple prompting strategies to examine how LLMs fare as quality evaluators when compared with human judgments on the SummEval and USR datasets, asking the model to generate both a score as well as a justification for the score. Furthermore, we explore the robustness of the LLM evaluator by using perturbed inputs. Our findings suggest that while LLMs show promise, their alignment with human evaluators is limited, they are not robust against perturbations and significant improvements are required for their standalone use as reliable evaluators for subjective metrics.", "sections": [{"title": "1 Introduction", "content": "Natural Language Generation (NLG) tasks such as abstractive summarization and dialog completion are essential for advancing human-computer interaction and automating content generation. However, the evaluation of such tasks poses unique challenges, especially when traditional metrics like BLEU and ROUGE, which rely on token overlap with reference texts, fail to account for the inherent subjectivity and flexibility in the human language. This limitation has sparked substantial research efforts to explore more effective automated evaluation methods (Bhandari et al., 2020) (Yeh et al., 2021).\nIn tasks where there is no definitive ground truth, nuanced metrics such as coherence and fluency are critical for evaluating text quality. Although human evaluation has long been the gold standard in such contexts, it presents several limitations. Human evaluators are susceptible to errors, and large-scale assessments demand significant time and resources. Furthermore, relying exclusively on human evaluations can result in inconsistencies in evaluation quality, and evaluators may struggle to accurately assess content outside their area of expertise. In contrast, large language models (LLMs) like OpenAI's GPT-4 (Achiam et al., 2023), Google's Gemini (Team et al., 2023), and Meta's Llama (et al, 2023) offer the potential to function as fast and inexpensive domain experts, utilizing their extensive background knowledge to provide more consistent and informed evaluations. As a result, it is important to investigate automated evaluation methods to reduce the reliance on human evaluators, thereby enhancing the efficiency and scalability of the evaluation process.\nIn this paper, we aim to:\n1. Investigate the ability of Google Gemini to serve as a \"quality-evaluator\" for subjective metrics by measuring the proximity of LLM evaluations to human experts.\n2. Evaluate the impact of different prompting strategies on the performance of Gemini in the context of summarization and dialog eval-"}, {"title": "2 Related Work", "content": "Evaluation metrics in NLG have traditionally focused on BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), which compare generated text with reference ground truths. While effective for tasks like machine translation, they struggle with abstractive summarization and dialog evaluation, where multiple valid outputs exist, and coherence is critical.\nRecent research has introduced reference-free metrics like BERTScore (Zhang et al., 2020) and QAEval (Deutsch et al., 2021), but these approaches require human intervention and lack flexibility (Deutsch et al., 2021) across diverse generation tasks. More recent efforts explore using LLMs for evaluation (Gilardi et al., 2023), which significantly reduces costs but still faces challenges in robustness, especially in adversarial settings.\nStudies like Raina et al. (2024) and Shuyuan et al. (2024) show vulnerabilities in LLM-based assessments, revealing their susceptibility to adversarial attacks that manipulate evaluation scores. These findings emphasize the need for more robust frameworks as LLM-generated labels become more prevalent."}, {"title": "3 Methodology", "content": "3.1 Datasets Used\n1. SummEval (Fabbri et al., 2021): This dataset includes human evaluations of model-generated summaries for CNN/Daily Mail, with annotations on coherence, consistency, fluency, and relevance. It contains 1600 article-summary pairs, annotated by 3 experts and 5 turkers. For consistency, we use only expert annotations and exclude 80 model-rejected pairs, leaving 1,520 for our experiments.\n2. USR (Mehri and Eskenazi, 2020): This dataset provides human evaluations of dialog responses for Amazon Topical-Chat and Persona Chat, across metrics like understandabil-\nWe make the dataset public at the following link."}, {"title": "3.2 Experimenting with Prompting Strategies", "content": "For the task of evaluating summaries and dialog responses, we instruct the model to generate a score for each metric along with a justification for the score using the following four prompting strategies:\n3.3\n\u2022 Zero-Shot (ZS) (Wei et al., 2022) We instruct the model to perform the task of scoring the pair based on it's own understanding of the metric.\n\u2022 Knowledge-Prompt (KP) (Liu et al., 2022): We paraphrase and use the definitions of the metrics provided in the respective dataset papers.\n\u2022 Few-Shot (FS) (et al, 2020): We provide two examples, one with a very high score and one with a very low score, to provide more context to the model on how to rate.\n\u2022 Chain-of-Thought (CoT) (Wei et al., 2023): We ask the model to think step-by-step to provide the score."}, {"title": "Introducing Perturbations", "content": "We select the \"Knowledge Prompt\" as our base for its ability to provide a direct, contextually informed evaluation that minimizes ambiguity and aligns with expert judgment, without requiring additional examples or complex reasoning. The model is informed that the response has been assigned a \"Perturbed Rating\" (PR) by the human expert, calculated by first determining the mode of the expert ratings. In cases where all three annotators differ, we take the highest rating as the mode. Here is the formal definition of PR:\n$$PR(r_1, r_2, r_3) = \\begin{cases} \\frac{r_1 + r_2}{2}, & \\text{if } mode(r_1, r_2, r_3) \\leq \\frac{max(scale)}{2} \\\\  \\frac{max(scale) + min(scale)}{2}, & \\text{if } mode(r_1, r_2, r_3) > \\frac{max(scale)}{2} \\\\  \\frac{max(scale) + min(scale)}{2}, & \\text{if } r_1 \\neq r_2 \\neq r_3 \\text{ and } max(r_1, r_2, r_3) \\leq \\frac{max(scale)}{2} \\\\   \\frac{max(scale) + min(scale)}{2}, & \\text{if } r_1 \\neq r_2 \\neq r_3 \\text{ and } max(r_1, r_2, r_3) > \\frac{max(scale)}{2}  \\end{cases}$$\nPR is not simply the mode; we introduce a perturbation that inverts the rating scale based on the mode's value to emphasize edge cases and challenge the evaluation process. This approach is applied uniformly across various rating scales to ensure consistency in evaluation, regardless of the rating metric used.\nThis perturbation inverts low and high ratings, creating a challenging evaluation scenario."}, {"title": "4 Results", "content": "To assess the reliability of LLM-based evaluations and their alignment with human raters, we used Krippendorff's alpha (Krippendorff, 2011) as the primary metric. We chose Krippendorff's alpha because it is well-suited for tasks requiring multiple raters and is capable of handling ordinal data, such as Likert scale ratings, which are used in both the SummEval and USR datasets.\nTo calculate Krippendorff's alpha, we extracted ratings for each metric from the SummEval and USR datasets, treating them as ordinal data. We used the Python krippendorff package, structuring the data as a matrix with rows as items and columns as ratings, handling missing values via the library's default settings. This enabled a direct comparison of LLM and human reliability."}, {"title": "4.1 LLM Consistency VS Human Consistency", "content": "In Table 2, we present the consistency of LLM evaluations compared to human evaluations across various metrics from the SummEval and USR datasets. Consistency was calculated separately for LLM and human raters.\n\u2022 LLM Consistency: We computed Krippendorff's alpha across the scores generated by the model across prompting strategies. This approach treated the LLM's ratings under different prompting strategies as independent raters. The resulting alpha values reflect how stable the model's evaluations are, regardless of variations in prompt design.\n\u2022 Human Consistency: For human ratings, Krippendorff's alpha was computed using the scores provided by three expert raters for each metric. This measures the level of agreement among human evaluators in their assessment of the same items.\nThe relatively high alpha values for LLM consistency across metrics suggest that the model demonstrates robustness in its evaluations and is less influenced by prompt variations. This stability contrasts with the slightly lower alpha values observed for human raters, which might reflect differences in subjective judgment or interpretation."}, {"title": "4.2 LLMs are not robust against Perturbed Prompts", "content": "The alpha values reported in table 1 show how closely the LLM ratings align with human judgments for different quality metrics. When Krip-"}, {"title": "4.3 Justification Analysis", "content": "We perform sentiment analysis of the LLM-generated justifications to explore the impact of perturbations on the quality of evaluations. For this analysis, we use the TextBlob library (Loria et al., 2018), analyzing each justification to compute a sentiment polarity score, ranging from -1 (negative sentiment) to +1 (positive sentiment). The scores were averaged across justifications for each combination of evaluation metric (e.g., Coherence, Consistency) and prompting strategy (e.g., Zero-shot, Few-shot). To investigate the effects of input perturbations, we compared sentiment scores for justifications generated under both unperturbed and perturbed prompting strategies. Looking at table 4, we observe that for most metrics, introducing perturbations consistently leads to lower sentiment score. This trend indicates that the perturbed prompts lead to more negative justifications overall, suggesting that the LLM becomes more critical of its evaluations when faced with misleading or false information. The significant drop in sentiment scores in the presence of perturbations aligns with the notion that the model becomes misaligned when presented with conflicting information. The sentiment scores serve as a quantitative measure of this misalignment, where lower values indicate confusion or hesitance in the LLM's reasoning process. The results imply that while LLMs may function well as evaluators under normal conditions, their reliability is significantly compromised when faced with misleading inputs. This emphasizes the need for careful consideration of input integrity when employing LLMs for subjective evaluation tasks."}, {"title": "5 Conclusion and Future Work", "content": "Google's Gemini-1 shows consistency across prompting strategies but is vulnerable to adversarial perturbations, emphasizing the need for stronger training to improve LLM reliability in subjective evaluation tasks. Our experiments revealed significant shifts in scores and justifications under adversarial conditions, underscoring the importance of robustness in LLM evaluators."}, {"title": "6 Ethics Statement", "content": "The datasets used in this paper are publicly available and obtained under permissible licenses, ensuring compliance with their intended usage. We utilized ChatGPT-4 for language assistance, including paraphrasing and spell-checking, without generating new content or influencing research findings. All research activities adhere to ethical standards and respect for intellectual property rights."}, {"title": "A Prompts Used", "content": "A.1 SummEval Dataset\nA.1.1 Base Knowledge Prompt\n\u2022 Coherence: Given a news article and its corresponding summary, assess the coherence of the summary on a scale of 1 to 5. Coherence refers to the overall quality of how well-structured and organized the summary is, ensuring that the sentences build upon each other to form a coherent body of information about the topic. Rate the coherence of the provided summary, where 5 represents the highest level of coherence and 1 indicates the lowest.\n\u2022 Consistency: Given a news article and its corresponding summary, assess the consistency of the summary on a scale of 1 to 5. Consistency refers to the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Summaries that contain hallucinated facts should be penalized. Rate the consistency of the provided summary, where 5 represents the highest level of consistency and 1 indicates the lowest.\n\u2022 Fluency: Given a news article and its corresponding summary, assess the fluency of the summary on a scale of 1 to 5. Fluency is the quality of individual sentences. Sentences in the summary should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read. Rate the fluency of the provided summary, where 5 represents the highest level of fluency and 1 indicates the lowest.\n\u2022 Relevance: Given a news article and its corresponding summary, assess the relevance of the summary on a scale of 1 to 5. Relevance refers to the selection of important content from the source. The summary should include only important information from the source document. Summaries which contain redundancies and excess information should be penalized. Rate the relevance of the provided summary, where 5 represents the highest level of relevance and 1 indicates the lowest."}, {"title": "A.1.2 Zero-Shot Prompt", "content": "We remove the definition of the metric from the prompt and ask the model to rate the article-summary pair based on its own understanding of the metric. An example for coherence:\n\u2022 Given a news article and its corresponding summary, assess the coherence of the summary on a scale of 1 to 5. Please use your own understanding of coherence to rate the provided summary, with 5 indicating the highest level of coherence/consistency/fluency/relevance and 1 representing the lowest."}, {"title": "A.1.3 Few-Shot Prompt", "content": "Along with the Base Knowledge prompt, we provide precisely two examples, an article-summary pair with the highest score, and one with the lowest score."}, {"title": "A.1.4 Chain-of-thought Prompt", "content": "We take our base knowledge prompt, and then add the following line to it, \"Let's think step-by-step\"."}, {"title": "A.2 USR Dataset", "content": "A.2.1 Base Knowledge Prompt\n\u2022 Understandable: Given the provided context and its corresponding response, evaluate the"}, {"title": "A.2.2 Zero-Shot Prompt", "content": "We remove the definition of the metric from the prompt and ask the model to rate the dialog-"}, {"title": "B Gemini Model Settings", "content": "B.1 Safety Settings\nSafety settings are crucial for controlling the content generated by the model, ensuring it adheres to ethical guidelines and does not produce harmful or inappropriate content. The following HARM_CATEGORY settings were used:\n\u2022 HARASSMENT: This setting determines how the model handles content related to harassment. Setting it to \"BLOCK_NONE\" means that there is no automatic blocking or filtering applied for harassment-related content.\n\u2022 HATE_SPEECH: Similar to harassment, this setting controls the handling of hate speech. \"BLOCK_NONE\" indicates no filtering is applied for this category.\n\u2022 SEXUALLY_EXPLICIT: This setting deals with sexually explicit content. By setting it to \"BLOCK_NONE\", the model will not automatically block such content.\n\u2022 DANGEROUS_CONTENT: This category covers dangerous content such as content that could incite violence or self-harm. \"BLOCK_NONE\" means no automatic blocking is applied.\nWhile we set all the settings to BLOCK_NONE, we still observed that Gemini refused to generate for 80 of the samples in SummEval."}, {"title": "B.2 Generation Configuration", "content": "The generation configuration specifies parameters that control the quality and creativity of the model's output:\n\u2022 temperature: Set to 0.1, this parameter controls the randomness of the model's responses. A low temperature like 0.1 makes the output more deterministic and focused.\n\u2022 top_p: Set to 1, this parameter is related to nucleus sampling. A value of 1 indicates that all potential words are considered in the probability distribution, making it a more deterministic generation.\n\u2022 top_k: Set to 1, this parameter limits the number of most likely words considered. A value of 1 means the model selects from only the highest probability word, making the output more focused and less varied.\n\u2022 max_output_tokens: Set to 2048, this parameter limits the length of the generated text. A higher value allows for longer responses."}, {"title": "C The Metrics", "content": "C.1 SummEval\n\u2022 Coherence (1 - 5) is the collective quality of all sentences. The summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.\n\u2022 Consistency (1 - 5) refers to the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Summaries that contain hallucinated facts should be penalized.\n\u2022 Fluency (1 - 5) is the quality of individual sentences. Sentences in the summary should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read.\n\u2022 Relevance (1 - 5) refers to the selection of important content from the source. The summary should include only important information from the source document. Summaries which contain redundancies and excess information should be penalized."}, {"title": "C.2 USR", "content": "\u2022 Understandable (0 - 1): Is the response understandable given the previous context?\n\u2022 Natural (1 - 3): Does the response seem to be something that a person would naturally say?\n\u2022 Maintains Context (1 - 3): Does the response serve as a valid continuation of the preceding conversation?\n\u2022 Interesting (1 - 3): Is the response dull or interesting?\n\u2022 Uses Knowledge (0 - 1): Given the fact that the response is conditioned on, how well does the response use that fact?\n\u2022 Overall Quality (1 - 5): Given your answers above, what is your overall impression of the quality of this utterance?"}, {"title": "C.3 Example Prompt-Response Pair", "content": "C.3.1 Prompt\nGiven a news article and its corresponding summary, assess the relevance of the summary ... where 5 represents the highest level of relevance and 1 indicates the lowest.\nArticle: \"Andros Townsend an 83rd minute sub in Tottenham's draw ...\",\nSummary: \"Paul Merson was brought on with only seven minutes remaining ...\"\nThe output should be in the following format:\nScore: (A score from 1 to 5)\nJustification: (Give reasoning as to why the score was awarded)"}, {"title": "C.3.2 Response", "content": "\u2022 Score: 1\n\u2022 Justification: The summary is not relevant to the article. It incorrectly mentions ... neither statement aligns with the content of the article.\n(Full contents of the prompt and output have been omitted for brevity; only key excerpts are shown here.)"}]}