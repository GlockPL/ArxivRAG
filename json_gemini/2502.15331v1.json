{"title": "Lightweight yet Efficient: An External Attentive Graph Convolutional Network with Positional Prompts for Sequential Recommendation", "authors": ["Jinyu Zhang", "Chao Li", "Zhongying Zhao"], "abstract": "Graph-based Sequential Recommender systems (GSRs) have gained significant research attention due to their ability to simultaneously handle user-item interactions and sequential relationships between items. Current GSRs often utilize composite or in-depth structures for graph encoding (e.g., the Graph Transformer). Nevertheless, they have high computational complexity, hindering the deployment on resource-constrained edge devices. Moreover, the relative position encoding in Graph Transformer has difficulty in considering the complicated positional dependencies within sequence. To this end, we propose an External Attentive Graph convolution network with Positional prompts for Sequential recommendation, namely EA-GPS. Specifically, we first introduce an external attentive graph convolutional network that linearly measures the global associations among nodes via two external memory units. Then, we present a positional prompt-based decoder that explicitly treats the absolute item positions as external prompts. By introducing length-adaptive sequential masking and a soft attention network, such a decoder facilitates the model to capture the long-term positional dependencies and contextual relationships within sequences. Extensive experimental results on five real-world datasets demonstrate that the proposed EA-GPS outperforms the state-of-the-art methods. Remarkably, it achieves the superior performance while maintaining a smaller parameter size and lower training overhead. The implementation of this work is publicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.", "sections": [{"title": "1 INTRODUCTION", "content": "With the widespread application of online shopping [55], information retrieval [44], and mobile services [56], a vast amount of data about user behaviors is now routinely recorded by platforms and servers. As we all know, user behaviors are typically not fragmented or independent; instead, they manifest as historical sequences via temporal accumulation [15]. In light of this, scholars develop Sequential Recommender systems (SRs) to forecast user consumption based on their sequential preferences [27]. By modeling the evolution of user behaviors, SRs have the capability to provide more accurate and relevant recommendations. The rationale behind SR is that user preferences and behaviors are not static but evolving [36]. Therefore, by analyzing the temporal patterns and trends in user behaviors, SR can offer recommendations that are more in line with the user's current interests and needs. It has been validated by extensive research, demonstrating the significance of simultaneously considering collaborative filtering signals and sequential preferences. [5, 39, 54, 58].\nIn the early exploration of Sequential Recommendation (SR) tasks, Recurrent Neural Network (RNN)-based models are frequently employed to measure the sequential dependencies in user historical behaviors [36]. However, RNNs have difficulties in considering the long-term associations among user interactions. Then, researchers sought to enhance Sequential Recommender systems (SRs) by employing Generative Adversarial Networks (GANs) [32] and Transformers [41, 58]. These methods are designed to capture the evolution of users' sequential patterns more effectively. Despite these advancements, their performance in extracting structural information from sequential transitions is still limited. To address the above issues, some researchers attempt to study SR via graph-based techniques [16, 51], a.k.a. Graph-based Sequential Recommender systems (GSRs). GSRs leverage the power of graph structures to aggregate complex associations between users and items, encompassing user-item interactions and sequential relationships. Nevertheless, GSRs are still limited when considering the global correlations among items. Hence, scholars attempt to combine the Transformer with Graph Neural Networks (GNNs) to create the Graph Transformer [26, 50]. The self-attention mechanism in the Transformer enables global weighting for capturing long-range dependencies in sequential graphs. Although Transformer-based GSRs have achieved improved performance on SR tasks, they still face the following challenges:\n(1) The global weighting strategy of self-attention mechanism requires high computational complexity. Most Transformer-based GSRs uniformly employ graph encoders with self-attention mechanism to measure the global associations among nodes. The core idea of such methods is to consider the overall contextual associations by calculating the attention weight between pairwise nodes in sequential graphs [26]. Therefore, the GSRs are indeed good at handling long-distance dependencies and capturing global information. However, they typically demand quadratic computational complexities (i.e., \\(O(N^2 \\times d)\\)), posing a significant challenge for model deployment on resource-constrained edge devices [28].\n(2) The relative position encoding has difficulty in considering the complicated positional dependencies. Transformer-based GSRs with relative position encoding have been proven effective in modeling sequences with varying lengths. The core idea of relative position encoding is to calculate the relative positions between adjacent items in the sequence [18]. This strategy is based on the \"locality of position\" assumption, i.e., an item has a direct dependency relationship with its adjacent items [38]. However, the expression of user preferences within a sequence is sometimes discontinuous or periodic. Relative positional encoding merely focuses on the continuous positional correlations, overlooking the long-term or intermittent positional dependencies of items within a sequence [48]. Such an overlooking makes it challenging for GSRs to comprehend the positional relationships of items throughout the entire sequence.\nTo this end, we propose an External Attentive Graph convolutional network with Positional prompts for Sequential recommendation, namely EA-GPS. Specifically, we first introduce an external attentive graph convolutional network. It contains an External Attention (EA)-based external encoder, which operates in parallel with the primary graph encoder. Such an external encoder enables a linear measurement of global associations among nodes via two external memory units. The independent yet reusable external memory units also facilitate sharing global attention weights across convolution layers. Then, we propose a positional prompt-based decoder to consider the complicated positional dependencies of interactions while learning sequence representations. This decoder first treats the absolute item positions as prompts. Subsequently, it operates a vertical concatenation between sequence and prompt representations, which directly tells the model about the exact position of items within sequences. By employing a length-adaptive sequential masking and a soft attention network, the decoder could better understand the long-term positional dependencies and contextual relationships of items.\nThe main contributions of this work can be briefly described as:\n\u2022 We investigate SR in an emerging yet challenging scenario (i.e., the lightweight graph-based sequential recommendation scenario). After highlighting the challenges faced by existing GSRs, we propose a graph-based solution called EA-GPS.\n\u2022 We devise a lightweight external encoder to linearly measure the global associations among nodes. This design relieves the primary graph encoder from high computational complexity.\n\u2022 We present an efficient prompt-based sequence decoder that treats the exact position of items as prompts to facilitate capturing the complicated positional dependencies and contextual relationships of items.\n\u2022 We conduct extensive experiments on five real-world datasets. The experimental results fully demonstrate that EA-GPS requires a smaller scale of parameters and lower training consumption, but outperforms the state-of-the-art GSRs."}, {"title": "2 RELATED WORK", "content": "This section considers four types of recommendation methods, i.e., Sequential Recommendation, Graph-based Sequential Recommendation, Prompting-based Recommendation, and Lightweight Recommendation."}, {"title": "2.1 Sequential Recommendation", "content": "Sequential Recommender systems (SRs) aim to capture the evolution of users' behavioral preferences by modeling their interaction sequences [27]. Early explorations on Sequential Recommendation (SR) mainly focus on Recurrent Neural Network (RNN)-based [15, 36, 37] structures. For example, Hidasi et al. [15] are the first group that exploits RNN to measure user preferences in dynamic sequences. Quadrana et al. [37] propose a hierarchical structure to relay and evolve the hidden states, which achieves better performance on SR tasks. However, these works meet gradient vanishing problems while considering the item associations from long sequences [17]. A practical solution is to leverage Generative Adversarial Networks (GANs) [32] to maintain long-term memory and overcome the gradient block problem. GANs are good at learning complex data distributions, enabling the creation of high-quality synthetic data for sequential recommendation tasks [35]. However, a significant drawback of GANs is their training instability, which can lead to mode collapse and difficulty in convergence. Then, some scholars consider utilizing contrastive learning [44, 54] to improve the quality of long-term sequential preferences. By contrasting similar and dissimilar instances, this strategy encourages the model to capture the underlying structure and relationships within sequences [42]. Another popular solution is the Transformer [25, 58], which significantly improves the accuracy and robustness of sequential recommendation via the self-attention mechanism. Such a component allows the Transformer to capture long-range dependencies in sequences, enabling better performance on tasks requiring context understanding [13]. These deep learning-based networks achieve great success in SR tasks but are limited in excavating structural information inside the sequential transitions."}, {"title": "2.2 Graph-based Sequential Recommendation", "content": "As one of the most popular solutions for Sequential Recommendation (SR) tasks, Graph-based Sequential Recommender systems (GSRs) have demonstrated efficacy in extracting structural information from complicated user-item interactions by constructing sequential graphs [2, 57]. For example, PTGCN [18] captures sequential patterns and temporal dynamics of user-item interactions by incorporating a position-enhanced and time-aware graph convolutional operator. It simultaneously learns the dynamic representations of users and items on a bipartite graph, utilizing a self-attention aggregator. RetaGNN [16] is a relational attentive GNN that operates on local sub-graphs derived from user-item pairs. It distinguishes itself by allocating learnable weight matrices to the various relationships between users, items, and attributes, instead of applying them to nodes or edges. However, these models are limited when handling large-scale graphs. To address the above limitation, researchers start using more complicated structures (such as Hypergraphs [51] or Graph Contrastive Learning [47]). For example, Wu et al. [47] propose a contrastive learning-based method that supplements the traditional sequential recommendation tasks with an auxiliary self-supervised task, reinforcing node representation learning via self-discrimination. Xia et al. [51] integrate hypergraph and contrastive learning to capture the beyond-pairwise relations on sequential graphs. By exploiting a dual-channel hypergraph convolution network, it successfully models the complex high-order information among items. These GSRs achieve great success in SR tasks but have difficulty capturing global correlations among nodes from sequential graphs. To this end, scholars have started incorporating Transformer with GCNs [26, 50]. For example, Xia et"}, {"title": "2.3 Lightweight Recommendation", "content": "Lightweight recommendation algorithms aim to provide efficient and accurate recommendations by leveraging various techniques to reduce computational complexity and storage requirements, making them suitable for real-world applications with constrained resources [23, 52, 60]. There are some common strategies, including matrix factorization techniques like Singular Value Decomposition (SVD) [19] and neighborhood-based algorithms like k-nearest neighbors (KNN). These shallow traditional machine learning methods have been proven effective in the early exploration of dimensionality reduction. However, with the advancement of deep neural networks and the increasing complexity of problems, traditional lightweight methods have become less adaptive. Nonetheless, these concepts continue to guide current explorations [39]. Existing studies on Lightweight Recommendation (LR) can be classified into two categories: 1) The one aims to reduce computational complexity. By refining algorithms or simplifying model structures, this strategy directly alleviates the scale of parameters [56, 60]. For example, Zhou et al. [60] proposes a novel lightweight matrix factorization for recommendations that deploys shared gradients training on local networks, serving as a two-phase solution to protect the security of users' data and reduce the dimension of the items. Lian et al. [29] provide a novel solution to refine the backbone network, which employs an indirect coding approach. It reduces the computational cost of representation learning by maintaining a coding dictionary. 2) The other aims to remove unnecessary components [33]. This strategy can also effectively release the parameter scale but requires verifying the importance of components [23, 34, 53]. For example, Yan et al. [53] exploit bidirectional bijection relation-type modeling to enable scalability for large graphs. This method removes the constraints of negative sampling on knowledge graphs, simplifying the computational complexity. Miao et al. [34] remove the transmission structure between social and interactive graphs in traditional social recommendation tasks. They fuse the social relationships and interactions into a unified heterogeneous graph to encode high-order collaborative signals explicitly, significantly reducing the computational complexity. Although lightweight concepts become popular in traditional recommendation tasks, research on lightweight sequential recommender systems (especially on GSRs) remains largely unexplored [28]."}, {"title": "2.4 Prompt-based Recommendation", "content": "Prompt-tuning paradigm is initially proposed in Natural Language Processing (NLP), which adapts the Pre-trained Language Models (PLMs) to the specific downstream tasks (especially in few-shot scenarios) [1, 30]. One research direction is to use rigid prompting (token-level) templates, which involve manually designing prompts and splicing them into token sequences [9]. In the domain of recommendation systems, numerous studies have employed rigid templates for the application of prompt-based learning. The methods based on rigid prompting templates usually convert the input features to natural language sequences [7, 40]. Then, they construct the rigid templates by concatenating the input features with language prompts (e.g., descriptions of the downstream tasks). By recasting recommendations as cloze-style mask-prediction tasks, these methods could stimulate the potentials of large pre-trained models, thereby achieving better performance on recommendation tasks [59]. In contrast, soft prompting (embedding-level) templates consist of randomly initialized learnable continuous embeddings, frequently adopted in recent studies [1, 49, 55]. For instance, Wu et al. [49] build personalized soft prompts by mapping user profiles to embeddings and enabling sufficient masked training on prompting templates via prompt-oriented contrastive learning. As we know, the core idea of prompt-tuning involves creating appropriate prompts that can guide the pretrained model to generate desired predictions [7]. Inspired by this idea, scholars are currently delving into the methodology of leveraging \u201cprompts\" to augment the representation learning capabilities of traditional deep learning models [4, 31]. For example, Luo et al. [31] treat the embeddings of timestamp as external prompts to unearth the potentials of a Transformer-based model. Note that, the purpose of using external prompts is to guide the model to generate outputs that meet specific requirements without altering the model itself [31] while the objective of prompt-tuning paradigm is to enhance the performance of pre-trained models by optimizing the input prompts, which involves indirect adjustments to the model's parameters [4]. In this work, we extract the positional information of items within each sequence as external prompts to enhance the sequence representation learning, thereby capturing the complicated positional dependencies of items within sequences."}, {"title": "3 METHODOLOGIES", "content": ""}, {"title": "3.1 Preliminaries", "content": "In this section, we present the notations mainly used in EA-GPS and then define the SR tasks. Suppose that \\(U = \\{U_1, U_2, . . ., U_k, . . ., U_n \\}\\) is the set of users, where \\(u_k\\) denotes k-th individual user (k = 1, 2, 3, . . ., n). Similarly, we define \\(I = \\{I_1, I_2, . . ., I_i, . . ., I_m\\}\\) as the set of all items, where \\(I_i\\) represents the i-th item (i = 1, 2, 3, . . ., m). Let \\(S = \\{S_1, S_2, . . ., S_k, . . ., S_n\\}\\) be the interactive sequences of each user, where \\(S_k = \\{o_1, o_2, . . ., o_{t_k} \\}\\) denotes the historical behavioral sequence of \\(u_k\\) that contains several interactive items \\(v\\) ordered by their timestamps, \\(t_k \\in T\\) denotes the length of \\(S_k\\), \\(T = \\{t_1, t_2, . . ., t_k, . . ., t_n \\}\\) denotes the lengths of each sequence. And we also define \\(P_k = \\{P_1, P_2, ..., P_{t_k} \\}\\) as the exact position of each interaction in \\(S_k\\), which has been recorded during the data preprocessing. In addition to the above definitions, the utilized embedded vectors or matrices are presented in Table 1.\nThe purpose of Sequential Recommendation (SR) task is to predict the next item \\(I_{i+1}\\) that user \\(u_k\\) is most likely to click, based on her/his historical interaction sequences \\(S_k\\) [27]. The probabilities of all candidate items can be denoted as:\n\\(P(I_{i+1}|S_k, u_k) \\sim f(S_k, u_k),\\) (1)\nwhere \\(P(I_{i+1}|S_k, u_k)\\) denotes the probability of recommending \\(I_{i+1}\\) as the next interested item to user \\(u_k\\) based on her/his historical interaction sequence \\(S_k\\), \\(f(S_k, u_k)\\) is the function exploited to estimate the probability."}, {"title": "3.2 Framework of EA-GPS", "content": "In this paper, we propose a simplified graph-based solution for SR tasks, with a dual aim of enhancing performance while remaining lightweight in its design. As shown in Fig. 2, the EA-GPS contains three key components, i.e., the graph construction, the external attentive graph convolutional network and the positional prompt-based decoder."}, {"title": "3.2.1 Graph Construction", "content": "We first construct the sequential graphs by considering two types of associations, i.e., user-item interactions and item-item sequential relationships. The resulting sequential graphs can be denoted as \\(G = \\{G_1, G_2, . . ., G_r \\}\\), where r represents the number of training batches. In detail, the input Laplace matrix \\(M \\in R^{(m+n)\\times(m+n)}\\) contains both the two-way user-item interactive relationships and the one-way item-item sequential dependency. The definitions of M can be formulated as:\n\\(M = \\begin{pmatrix}\nA_{I \\rightarrow I} & A_{U\\rightarrow I}\\\\\nA_{I \\rightarrow U} & 0\n\\end{pmatrix},\\) (2)"}, {"title": "3.2.2 External Attentive Graph Convolutional Network", "content": "We propose an external attentive graph convolutional network, including a primary graph encoder and an external encoder. In the primary graph encoder, we merely retain the most necessary components (i.e., the neighborhood aggregating and the layer-wise sum component [12]) to propagate messages on sequential graphs. In the external encoder, we utilize the External Attention (EA) mechanism to capture the global associations among nodes [11]. Compared to Self-attention (SA), EA has linear computational complexity. Moreover, the calculation of EA occurs solely between the nodes and two low-rank external memory units. Hence, we operate the external encoder in parallel, alleviating the burden on the primary graph encoder. Besides, it also allows for sharing global attention weights across different convolution layers."}, {"title": "3.2.3 Positional Prompt-based Decoder", "content": "We present a positional prompt-based decoder to consider the complicated positional dependencies of interactions during the sequence-level representation learning. Inspired by the prompt-based methods [31], an intuitive idea is to directly tell the decoder about the item positions within each sequence. Hence, we treat the absolute item positions as prompts and project them into embedding-form. Then, we construct the positional prompting templates by vertically concatenating the position embeddings with sequence embeddings. Moreover, we devise a length-adaptive sequential masking strategy to mask the prompting templates. Finally, we utilize a soft attention network to refine the sequence representations. By forcing the model to predict the masked elements based on the context, such a component is able to capture the complicated positional dependencies of items and the sequential behavioral patterns of users.\nFinally, by concatenating the embeddings of users and sequences, EA-GPS results in user-specific sequential representations for final predictions."}, {"title": "3.3 External Attentive Graph Convolutional Network", "content": ""}, {"title": "3.3.1 Primary Graph Encoder", "content": "In the primary graph encoder, we merely retain the neighborhood aggregation component to realize lightweight node representation learning. It learns the node representations by propagating messages on sequential graphs. Both the sequential relationships and the collaborative filtering signals are taken into account via the layer-wise aggregating protocol [12]. We conduct a n layers graph convolution on sequential graphs, where \u03b7 is a hyper-parameter that controls the layer numbers. The node representation learning on l-th (0 < l < \u03b7) layer can be formulated as:\n\\(\\hat{E}^{(l)} = H(\\hat{E}^{(l-1)}, G),\\) (3)\nwhere \\(\\hat{E}^{(l)} \\in R^{(m+n)\\times d}\\) denotes the node representations on l-th layer, \\(\\hat{E}^{(l-1)}\\) is that of the previous layer, \\(H(\\cdot)\\) represents the message aggregating function that calculates the average value according to neighboring nodes. \\(H(\\cdot)\\) can be further denoted as:\n\\(H(\\hat{E}^{(l-1)}, G) = (D^{-\\frac{1}{2}}(M + I)D^{-\\frac{1}{2}})\\hat{E}^{(l-1)},\\) (4)\nwhere \\(I \\in R^{(m+n)\\times (m+n)}\\) denotes the characteristics of nodes themselves, and \\(D \\in R^{(m+n)\\times (m+n)}\\) is the normalized matrix. As the number of neighbors of each node is inconsistent, we introduce the normalized matrix D to alleviate this impact."}, {"title": "3.3.2 External Encoder", "content": "In this section, we first revisit self-attention and linear attention mechanism before introducing the external encoder (We take \\(S_k\\) as an example, \\(S_k\\) is one of the sequences that composes the input graphs G).\nSelf-Attention. As a common practice in global weighting, Graph Transformer [26] has achieved state-of-the-art performance on modeling nodes' sequential dependencies. The most essential component in Graph Transformer is the Self-Attention (SA) mechanism (as shown in Fig. 3 (a)). Given the input feature map \\(E_{S_k} \\in R^{t_k\\times d}\\) of the sequence \\(S_k\\), where \\(t_k\\) denotes the number of interacted items in \\(S_k\\) (i.e., the length of \\(S_k\\)) and d is the embedding size. Self-attention mechanism maps the input features to a query matrix \\(Q \\in R^{t_k\\times d}\\), a key matrix \\(K \\in R^{t_k\\times d}\\), and a value matrix \\(V \\in R^{t_k\\times d}\\). The calculation can be formulated as:\n\\(A_S = a_{i,j}^{S_k} = softmax(\\frac{QK^T}{\\sqrt{d}}),\\) (7)\n\\(Z_{S_k} = A_SV,\\) (8)\nwhere \\(A_S \\in R^{t_k \\times t_k}\\) is the attention map, \\(a_{i,j}^{S_k}\\) denotes the pairwise affinity between i-th and j-th items in sequence \\(S_k\\), and \\(Z_{S_k} \\in R^{t_k\\times d}\\) is the refined feature representations.\nLinear Attention. To address this limitation, Katharopoulos et al. [21] propose Linear Attention (LA) mechanism. As shown in Fig. 3 (b), LA first calculates the outer product of the key matrix and value matrix (i.e., the \\(K^TV\\)), and then calculates the correlation by the query matrix. As LA alters the calculation order, it successfully reduces the computational complexity to the scale of \\(O(N \\times d^2)\\). The calculation of LA can be formulated as:\n\\(A_L = a_{i,j}^{S_k} = softmax(\\frac{Q \\frac{E_{S_k} E_{S_k}^T}{\\sqrt{d}}}{\\sqrt{d}}),\\) (11)\n\\(Z_{S_k} = E_{S_k}A_L,\\) (12)\nwhere \\(A_L \\in R^{d\\times d}\\) denotes the attention map of linear attention mechanism.\nHowever, such changes also lead to the loss of the fine-grained information from each query, resulting in a lossy attention map \\(A_L\\) for LA. Besides, both SA and LA require generating key or value vectors for each input feature while calculating global correlations, leading to a hign memory consumption [56].\nExternal Attention in External Encoder. After revisiting SA and LA, we continue to introduce the key component of EA-GPS (i.e. the external encoder), which utilizes External Attention (EA) mechanism (as shown in Fig. 3 (c)) to measure the global correlations between item nodes. This external encoder consumes sequence embeddings from each convolutional layer as its inputs and produces output embeddings with weighted scores. We also take \\(S_k \\in S\\) as an example to detail the calculation of external encoder, and the input features learned by primary graph encoder can be denoted as \\(E_{S_k}\nIn contrast to self-attention and linear attention mechanism, the External Attention (EA) focuses on computing global weights between the input features \\(E_{S_k}\\) and two external memory units, i.e., \\(M_k \\in R^{a\\times d}\\) and \\(M_v \\in R^{a\\times d}\\) (a is a hyper-parameter that controls the dimension of the external memory units). The computation can be elaborated as Eqn. (13 - 14):\n\\(A_E = a_{i,j}^{S_k} = Norm(\\frac{E_{S_k} M_k^T}{\\sqrt{d}}),\\) (13)\n\\(Z_{S_k} = A_E M_v,\\) (14)\nwhere \\(A_E \\in R^{t_k\\times a}\\) is the attention matrix, \\(a_{i,j}^{S_k}\\) in Eqn. (13) is the similarity between the i-th items in \\(S_k\\) and the j-th row of \\(M_k\\). Here, \\(M_k\\) and \\(M_v\\) are two learnable matrices that respectively play the role as the Query and Value matrices in external attention."}, {"title": "3.3.3 Message aggregation between encoders", "content": "To aggregate the messages from encoders on l-th layer, EA-GPS appends the global correlations from EA to the item representations \\(\\hat{E}_{S}\\) via residual links, the updated representations are formulated as:\n\\(E_S^{(l)} = LayerNorm(\\hat{E}_S^{(l)} + \\delta Z_{S}^{(l)}),\\) (17)\nwhere \u03b4 is the hyper-parameter controls the participation of the correlations generated by the external encoder. After extensive experiments, we found that the optimal performance is always achieved when the \u03b4 = 1. Hence, we did not report the impact of the \u03b4 in this paper, but set it directly to 1.\nIn the primary graph encoder, the node embeddings at 0-th layer (\\(\\hat{E}_{U}^{(0)}\\) and \\(\\hat{E}_{S}^{(0)}\\)) are optimized individually, with which the node representations at higher layers are calculated by layer-wise message passing strategies. Then, the final node representations are learned via layer combination, which are formulated as:\n\\(E_U = \\frac{1}{1 + \\eta} \\sum_{l=0}^{\\eta} \\hat{E}_U^{(l)};\\quad E_S = \\frac{1}{1 + \\eta} \\sum_{l=0}^{\\eta} \\hat{E}_S^{(l)},\\) (18)\nwhere \u03b7 is a hyper-parameter that controls the number of graph convolution layers, \\(E_U\\) and \\(E_S\\) denote the resulting node representation for users and items, respectively.\nIn this way, the EA-GPS facilitates the detection of the global correlations among items with a lower computational complexity. Then we present a positional prompt-based decoder for sequence-level representation learning."}, {"title": "3.4 Positional Prompt-based Decoder", "content": "At the stage of sequence representation learning, the traditional relative position encoding in the Graph Transformer [50] fails to consider the complicated positional dependencies of items (e.g., items' long-term or discontinuous relationships) [48]. The primary reason is that the relative positional relationships between adjacent items are unable to assist models for perceiving the position of an item within the entire sequence. Inspired by the prompt-based methods [31], we have an intuitive idea that directly tells the model about the item positions. Specifically, we treat the absolute item position as prompts and then propose a positional prompt-based decoder to assist the sequence representation learning. The input of this decoder is the node representations (i.e., \\(E_S\\) and \\(E_U\\)) and the item positions of each sequence (T). The positional prompt-based decoder contains three key parts, i.e., prompting templates, sequential masking, and soft attention network."}, {"title": "3.4.1 The Construction of Prompting Template", "content": "We also take \\(S_k\\) as an example, its sequence embedding is \\(E_{S_k} \\in R^{t_k\\times d}\\). As shown in Fig. 4, we treat the the exact position of each item in sequences \\(P_k = \\{P_1, P_2, ..., P_{t_k} \\}\\) as the positional prompt. We initilaize it to embedding-form as \\(\\hat{E}_P \\in R^{t_k\\times d_1}\\). Then, we linearly project \\(\\hat{E}_P\\) to the same dimension as \\(E_{S_k}\\) via point-wise Conv1D. For linear dimensional transformation, both the kernel size and stride of the Conv1D are set to 1 [46]. The calculation is denoted as Eqn. (19).\n\\(E_{p_k} = \\hat{E}_{p_k} * W_c + b_c,\\) (19)\nwhere * denotes the 1D convolutional operation, \\(W_c \\in R^{d_1\\times d}\\) represents the convolutional kernel, \\(b_c \\in R^{1\\times d}\\) is the bias term.\nAfter obtaining the embedding-form positional prompts, we construct the prompting template by vertically concatenating the positional prompts \\(E_{p_k}\\) with the sequence embeddings \\(E_{S_k}\\) (As shown in Fig. 4). The main purpose of vertical concatenation is to integrate positional information into the sequence representation, enabling the model to better capture long-term dependencies and contextual relationships within the sequence. By incorporating absolute positional prompts, the model is able to understand the temporal evolution of user preferences more effectively, which is crucial for sequential recommendation tasks. Besides, compared to the dimension of the conventional horizontal concatenation \\(R^{(t_k+t_k)\\times d}\\), the prompting template by vertical concatenation"}, {"title": "3.4.2 Sequential Masking", "content": "Based on the initialized representation of prompting templates, EA-GPS employs a sequential masking strategy to adaptively foster the model to better understand the sequential context. Specifically, rather than randomly determining whether to mask each element in the prompting template, we randomly mask \\(t_k \\times \\gamma\\) elements on the positional prompting template, where \u03b3 is a hyper-parameter that controls the proportion of items to be masked in the prompting templates. We define the masking vector as \\([m_1, m_2, . . ., m_i, . . ., m_{|t_k|}]\\), where \\(m_i\\) denotes a binary value indicating whether the corresponding element in the template should be masked. Then, the masking operation is denoted as:\n\\(m_i = \\begin{cases}\n1 & \\text{if } e_i^` \\text{ is to be masked, and } g_k \\le t_k \\times \\gamma;}\\\\\n0 & \\text{otherwise,}\n\\end{cases}\\) (21)\nwhere \\(e_i^`\\) is an element of the prompting templates, \\(g_k\\) denotes the number of elements that has already been masked in the prompting templates \\(E_{X_k}`\\). Intuitively, the masked embedding \\(\\hat{E}_{X_k} = \\{\\hat{e}_1^`, \\hat{e}_2^`, ..., \\hat{e}_i^`, ..., \\hat{e}_{t_k}^` \\}\\) of prompting template is denoted as:\n\\(\\hat{e}_i^` = m_i e_i^` + (1 - m_i) \\cdot [mask].\\) (22)\nIn conventional masking strategies, the quantity of masked elements within each prompting template is unequal. Short sequences may inadvertently mask critical information when the masking is too extensive, compromising their representations. Conversely, long sequences may not leverage the full potential of prompts if the masking is too limited. By carefully considering sequence length during masking, sequential masking in EA-GPS ensures equitable treatment of sequences with varying lengths."}, {"title": "3.4.3 Soft Attention Network", "content": "After obtaining the masked templates \\(\\hat{E}_{X_k}\\), we devise a soft attention network to activate the masked prompting template. Here, we leverage the last element \\(\\hat{e}_t^`\\) within template as additional supervision signal, guiding the decoder to refine the sequence representation learning. For the i-th element \\(\\hat{e}_i^`\\) in \\(\\hat{E}_{X_k}\\), we compute its relevance to \\(\\hat{e}_t^`\\) by Eqn. (23).\n\\(\\alpha_{i,t} = \\frac{exp (f(\\hat{e}_i^`, \\hat{e}_t^`))}{\\sum_i^{t_k} exp (f(\\hat{e}_i^`, \\hat{e}_t^`))},\\) (23)\nwhere \\(f(\\cdot)\\) is the cosine similarity function that calculates the correlations between elements, \\(\\alpha_{i,t}\\) denotes the resulted correlations.\nThen, the refined sequence representation \\(E_{X_k}\\) is obtained by Eqn. (24) as:\n\\(E_{X_k} = \\sum_{i=1}^{t_k} \\alpha_{i,t} \\hat{e}_i^`\\)(24)\nBy employing such soft attention mechanism, the model is forced to infer the masked element based on the contextual information and the positional prompts. It facilitates the model to better understand the long-term positional dependencies and contextual relationships within sequences.\nFinally, the user-specific sequence-level representations \\(H_{S_k}\\) for user \\(u_k\\) are learned by concatenating refined sequence embeddings \\(E_{X_k}\\) with the user embeddings \\(E_{u_k}\\) as:\n\\(H_{S_k} = Concat[E_{X_k}, E_{u_k}].\\) (25)\nThe representation learning process for the single user \\(u_k\\) can also be easily extended to all users. Hence, the final representation output by the positional prompt-based decoder can be denoted as:\n\\(H_S = Concat[H_{S_1}, H_{S_2}, ..., H_{S_k}, ..., H_{S_n}].\\) (26)"}, {"title": "3.5 Final Prediction", "content": "After the sequence representation learning, EA-GPS generates the user-specific sequence-level representations \\(H_S\\). We feed it into the prediction layer to get the final probabilities.\n\\(P(I_{i+1}|S, U) = softmax(W_1 \\cdot H_S + b_1),\\) (27)\nwhere \\(W_"}]}