{"title": "The Potential of Answer Classes in Large-scale Written Computer-Science Exams \u2013 Vol. 2", "authors": ["Dominic Lohr", "Marc Berges", "Michael Kohlhase", "Florian Rabe"], "abstract": "Students' answers to tasks provide a valuable source of information in teaching as they result from applying cognitive processes to a learning content addressed in the task. Due to steadily increasing course sizes, analyzing student answers is frequently the only means of obtaining evidence about student performance. However, in many cases, resources are limited, and when evaluating exams, the focus is solely on identifying correct or incorrect answers. This overlooks the value of analyzing incorrect answers, which can help improve teaching strategies or identify misconceptions to be addressed in the next cohort. In teacher training for secondary education, assessment guidelines are mandatory for every exam, including anticipated errors and misconceptions. We applied this concept to a university exam with 462 students and 41 tasks. For each task, the instructors developed answer classes \u2013 classes of expected responses, to which student answers were mapped during the exam correction process. The experiment resulted in a shift in mindset among the tutors and instructors responsible for the course: after initially having great reservations about whether the significant additional effort would yield an appropriate benefit, the procedure was subsequently found to be extremely valuable. The concept presented, and the experience gained from the experiment were cast into a system with which it is possible to correct paper-based exams on the basis of answer classes. This updated version of the paper provides an overview and new potential in the course of using the digital version of the approach.", "sections": [{"title": "1 Introduction", "content": "A reliable and fair assessment of exam assignments is a fundamental requirement for professional teaching. Especially in large-scale written exams, resources are limited, and high-quality assessment of submissions is challenging for educators. Inconsistencies in correction are possible when multiple people correct the same assignment \u2013 especially if no high-quality assessment guidelines are available and where the correcting educator has a high degree of decision-making power. In addition, the feedback provided to students is very basic at best and is often only provided in scoring (points).\nTo support correction processes to make it fairer and more objective, in secondary and higher education, marking schemes respectively rubrics [AP11] are mandatory when creating assignments \u2013 especially exams. The advantage is that it reduces required corrections and provides more objective task evaluation options. In addition to the model solution, rubrics contain alternative solutions and a list of possible incorrect answers. This led us to the concept of answer classes - groups of answers that share the same underlying idea. The intelligent clustering of possible solutions into answer classes holds several great potentials for modern teaching \u2013 especially when dealing with so many learners that individual face-to-face support is impossible.\nTo investigate the potential of answer classes in large-scale written computer science exams, we developed a process model that we evaluated in the first iteration of an exam for an Artificial Intelligence major program that 462 students took. Following the process model, 192 answer classes were developed for 41 tasks. In the sequel, we present a detailed analysis of five tasks aimed at identifying the specific (possible) causes of incorrect answers and feeding those back into next year's teaching process. The results demonstrate the efficacy of answer classes in creating rubrics for large courses and highlight their significant potential for enhancing instructional materials, improving the fairness of correction, and developing adaptive feedback.\nA completely unexpected consequence was that the experiment resulted in a paradigm shift among the tutors and instructors responsible for the course, who in a university setting \u2013 were not accustomed to rubric practices in secondary education. Despite initial reservations regarding the additional effort required, the procedure was ultimately deemed valuable and worthy of undertaking in every future exam.\nThis rethink ultimately led to efforts to digitize this process. Based on the"}, {"title": "2 Related Work", "content": "Classifying answers in examinations or tasks, in general, is an essential but tedious part of correcting and rating written exams. It is no surprise that the increasing power of computers has led to efforts to automate that.\nAuto-grading systems and feedback generation often refer to a set of rules defined by the author of the task. For instance, this is often done in programming using static and dynamic code analysis, as seen in the system JACK, which uses a query language in XML to specify test cases and code structures relevant for grading [St16]. JACK handles diverse tasks like UML modeling and fill-in-the-gap assignments. Many e-assessment systems detect errors for complex programming tasks by executing test cases and analyzing source code [Ih10; SS22]. Automated grading matches human performance [GDG14], with fine-grained feedback being effective [Fa14]. In contrast, Course Master by Higgins et al. integrates marking schemes, separating exercises and grading logic as Java classes, enabling parameterized tasks, marks (grades), and feedback [HST02]. Lawrence et al. explore performance-based grading, crucial for e-learning and remote exams [LFU23].\nAnswer Clustering/Classification A different approach to classifying student answers is conducted by Zehetmeier et al. [Ze15]. They analyzed student submissions to identify and cluster errors and quantified significant groups to uncover the causes of errors. The researchers also observed the students while programming, interviewed them about their errors at crucial stages, and used the results to create teaching materials and tasks to prevent similar misconceptions.\nIn their system EvalSeer, Nabil et al. use machine learning for classifying"}, {"title": "3 Theoretical Background", "content": "The concept of answer classes (AC) is one of four components of the Y-Model [Lo23a] \u2013 a model for formalizing tasks in Computer Science (see Figure 1). The model forms a basis for a competence-oriented integration of tasks in adaptive learning systems.\nThe central component of the Y-Model is the task itself, represented by its description (and representation). The upper part illustrates the interaction of knowledge elements with cognitive processes. Knowledge elements and their interdependencies are modeled using knowledge/learning object graphs [KK08]. A learning taxonomy like the one by Fuller et al.[Fu07] or Bloom's revised [AK09] can be used to model cognitive processes. For a detailed overview of the task model's components and underlying theories, see [Lo23a].\nAnswer classes (ACs) (see lower part of the Y-model) can be understood as a set-theoretic propositional form. Each answer class includes an identifier $AC_x$ \u2013 unique to the task \u2013 and a detailed answer class description ACD, outlining the criteria for assigning a given answer to this class. ACs are notably not free of intersections so that answers can be assigned to several different ACs. For the AC to be reliably annotated by any educator, the ACD must be (1) unambiguous (free of interpretation) and (2) objectively observable. Therefore, criteria that refer to presumed causes of errors, such as \u201csloppy work\u201d or \u201clack of"}, {"title": "4 The Potential of answer classes", "content": "Developing and implementing answer classes involves additional effort for educators, but we claim that the potential benefits are significant. To evaluate the potential of ACs, we begin by establishing criteria derived from the advantages of rubrics in the literature (e.g., [WS07], [RA10]):\nST Save time Conventional correction process, involving marginal notes and closing comments, can be time-consuming. Answer classes reduce the need for these annotations, potentially making the correction process more efficient.\nBF Better feedback In most cases, the lack of time during corrections leads to the omission of valuable feedback. However, developing feedback for each answer class must only be done once (or for their combinations) and can be effortlessly provided as a table.\nMOC More objective correction Multiple educators correct the same task in large courses, leading to disparate results due to varying personal interpretations. Answer classes offer a set of precisely defined, objectively observable criteria, enhancing the reliability of the correction process.\nBPE Better pre-estimation Addressing potential student errors in advance helps to ensure that exam tasks are appropriate and error-free.\nIIT Incentives to improve teaching The annotation of answers with ACs allows for a higher quality evaluation of exam results, providing valuable insights for teaching improvement. Prevalent error classes can indicate particular areas in the teaching material that must be revised to avoid misconceptions."}, {"title": "5 Methodology", "content": "To investigate the potential categories stated in Section 4, we devised an iterative process model to develop and apply the concept of answer classes systematically (see Fig. 2).\nUpon creation of the exam, the instructors were introduced to the concept of answer classes as detailed in Section 3. To begin with, a preliminary set of answer classes was created for each task based on prior experience with similar tasks (STEP 1).\nSubsequently, we discussed the initial answer classes for each task in a plenary session (STEP 2). For this purpose, the instructors presented the tasks and the first draft of the set of answer classes to the group. During this session, all stakeholders checked that the answer classes were objectively observable and free of subjectivity or ambiguity.\nConsidering that the exam was administered as a paper-based test, we developed an AC mapping form to facilitate the mapping process of students' submissions to corresponding answer classes (STEP 3). The list contained besides the students' IDs a table with all the identifiers of the ACs for each task. (see Figure 3).\nTo ensure the correct mapping of answer classes by the correctors, we added a brief explanation of each answer class to the model solution. The AC mapping form also contained placeholders for (potential) new answer classes"}, {"title": "6 Results", "content": "The procedures outlined in Section 5 were initially tested in an exam for the course Artificial Intelligence 1. Of the 502 registered students, 462 participated in the written exam (90 minutes, conducted in presence). The majority of the students were pursuing their master's degree in Data Science (225), Artificial Intelligence (118), and Computer Science (69). The exam consisted of 12 problems, encompassing 41 distinct tasks. STEP 1 to 3 were done for all 41 tasks. A total of four common and 192 initial answer classes were developed. Since steps 4 and 5 were more time-intensive, we selected one problem (containing"}, {"title": "7 Discussion", "content": "Applying the steps described in Section 5 \u2013 especially developing initial ACS required additional time. Whether the use of ACs saves time (see ST in section 4) cannot be answered based on the results in this paper. In the long term, however, we assume that time savings can be substantial, especially in shared tasks and settings. Even in task variants \u2013 e.g., by \u201cchanging numbers\u201d \u2013 ACs can often be transferred (at least in spirit). In particular, if predominant error patterns are documented and suitable feedback has already been developed, the correction effort (and providing feedback) is thus reduced to the annotation with ACs.\nIn our example of problem 2.1, many of the ACs found are general for tasks involving the application of search algorithms to graphs.\nWhile it is impossible to tell without further knowledge of the learner whether an answer ended up in a particular AC due to a misunderstanding or just sloppy work, it is possible to determine possible causes of errors and address them specifically in subsequent cohorts. Referring to $AC_{30}$ in ST4, we discussed in plenary what the reason could be that so many students incorrectly applied the greedy search algorithm. The instructor assumes that one prominent example"}, {"title": "8 Beyond the Pilot", "content": "The results of the pilot show that a paper-based assignment of answer classes to learners' answers is possible and holds much potential to improve teaching processes. However, it is not particularly convenient to do it paper-based, and the potential of digitizing the process is obvious. As a result of the successful pilot, a system was developed with which answer classes can be assigned via a web interface to answers on paper-based exams the VoLl-KOrN system:\nVOL1-KOrN uses QR codes on the exam sheets containing relevant identifiers so learners' written answers can be automatically mapped to the corresponding task and person after scanning the exams. The advantage of this approach is that the sheets can be digitized in any order, and the risk of mixing up sheets after they have been stapled apart is no longer relevant. Using annotations in the source files of the exams, QR codes can be created automatically via a plugin."}, {"title": "9 Conclusion and Future Work", "content": "We have picked up the concept of answer classes from the Y-Model framework [Lo23a] and evaluated it in practice in a large-scale written exam. We have fleshed out a practical procedure for implementing answer classes in a written presence exam with manual paper-based correction. The results of this experiment are very encouraging for the quality of education by implementing answer classes. To quote the instructor of the course (who is a co-author of this paper):\nI was very skeptical whether this concept from secondary education could carry over to the university setting and whether the effort of AC annotation would be sustainable in a correction process that keeps my entire research group busy full-time for a week. Frankly, I only agreed to this to make my didactics colleague happy. [. . . ] But the result of the experiment has utterly convinced me of the approach, even in a paper-based setting. Seeing the discussions among the graders induced by developing and annotating ACs and the content awareness in the grading process made the added effort worthwhile. [. . . ], and that is before we have taken a closer look at the data that this experiment generated.\nEncouraged by this, we digitized the process presented in Section 5 in a browser-based application that presents scanned exam papers and supports AC assignment, AC-based grading, AC-based feedback, and exam review also to reap the scalability benefits conjectured in Section 4 and discussed in Section 7.\nDeveloping and annotating ACs is time-consuming, and creating them individually in all situations may not be feasible. Therefore, future efforts"}]}