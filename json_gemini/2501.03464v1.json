{"title": "LHGNN: Local-Higher Order Graph Neural Networks For Audio Classification and Tagging", "authors": ["Shubhr Singh", "Emmanouil Benetos", "Huy Phan", "Dan Stowell"], "abstract": "Transformers have set new benchmarks in audio processing tasks, leveraging self-attention mechanisms to capture complex patterns and dependencies within audio data. However, their focus on pairwise interactions limits their ability to process the higher-order relations essential for identifying distinct audio objects. To address this limitation, this work introduces the Local-Higher Order Graph Neural Network (LHGNN), a graph based model that enhances feature understanding by integrating local neighbourhood information with higher-order data from Fuzzy C-Means clusters, thereby capturing a broader spectrum of audio relationships. Evaluation of the model on three publicly available audio datasets shows that it outperforms Transformer-based models across all benchmarks while operating with substantially fewer parameters. Moreover, LHGNN demonstrates a distinct advantage in scenarios lacking ImageNet pretraining, establishing its effectiveness and efficiency in environments where extensive pretraining data is unavailable.", "sections": [{"title": "I. INTRODUCTION", "content": "The realm of audio classification and tagging has evolved rapidly with the adoption of deep learning technologies. Spanning sound event detection [1] to advanced applications like music recommendation [2] and keyword spotting [3], the impact of these technologies is profound. Historically, CNNS were the preferred architecture for audio classification [4] until Transformers [5] demonstrated their superiority in handling complex interactions and larger datasets. While convolutional layers use learnable kernels that reduce overfitting and enhance generalization (especially beneficial with smaller datasets due to their strong inductive bias), Transformers, with their adaptive attention mechanism, excel in modeling more intricate patterns by mapping a global receptive field from the first layer itself.\nAnother compelling line of research in deep learning architectures explores the integration of clustering methods with Transformers for tasks such as image classification and object detection [6]. The process involves projecting features into a set of cluster centers and subsequently redistributing these cluster centers back into the original feature space using similarity metrics. This approach conceptually mirrors the operations of a specialized form of Graph Neural Network (GNNs) known as Hypergraph Neural Networks (HGNNs) [7], [8]. In HGNNs, node features are first projected onto hyper-edges, and then updated node features are obtained by projecting back from these hyperedges. Although in deep learning literature, parallels have been drawn between transformers and GNNs [9], positioning transformers as a specialized iteration of the latter, only recently have graph neural networks been employed in vision [10] and audio [11].\nIn this work, we introduce Local-Higher Order Graph Neural Networks (LHGNN), a model which integrates the robust capabilities of GNNs with clustering techniques. LHGNN utilizes local relationships through the k-nearest neighbor (k-NN) algorithm and higher-order relationships via Fuzzy C-Means clustering, enhancing the model by transcending the pairwise interactions typical in standard Transformers and graph-based methods. Fuzzy C-Means [8] extends traditional k-means by allowing probabilistic cluster assignments, enabling data points to belong to multiple clusters with varying degrees of membership. Integrating local neighborhood information with higher-order clustering in our LHGNN model offers two benefits: (i) it enables the modeling of higher-order semantic relationships by leveraging clustering techniques, and (ii) it facilitates the modeling of multi-scale relationships in audio by integrating local k-NN and higher-order clustering information.\nThe key contributions of this paper are: (i) the introduction of a novel graph kernel for graph neural networks that integrates local and higher-order interactions for robust representations, and (ii) demonstration of the model's robust performance without the need for extensive ImageNet pretraining, enhancing its versatility in both data-rich and data-scarce environments."}, {"title": "II. METHOD", "content": "A high level overview of the model architecture is illus- trated in Fig 1. The input mel-spectrogram is first processed through a stem block that consists of four 3 \u00d7 3 convolutional layers with strides of 2, 1, 2, and 1 respectively. In contrast to the traditional non-overlapping tokenization approach, the convolution backbone is capable of extracting superior local representations and has become widely adopted in modern Vision Transformers (ViTs) [12]. The resulting feature map is fed into four stages of the stacked Local-Higher Order Graph (LHG) blocks.\nBetween the stages of the network, downsampling blocks that include 3\u00d73 convolutions with a stride of 2 are employed to decrease the number of tokens. The output from the final downsampling block undergoes global average pooling, followed by a 1 \u00d7 1 convolution and a fully connected layer to produce the final predictions."}, {"title": "B. LHG Block", "content": "The LHG block consists of two main components: Local- Higher Order Graph Convolution and Convolutional Feed Forward Network (ConvFFN).\n1) Local-Higher Order Graph Convolution: The output from the convolutional backbone is denoted as X, which is a feature map with dimensions $R^{H \\times W \\times C}$. Here, H, W, and C represent the height, width, and number of channels, respectively. To prepare this data for subsequent analysis, we initially flatten the feature map to obtain a set of nodes $X = \\{x_1,x_2,...,x_N\\} \\in R^{N \\times C}$, (where N = H x W). For each node $x_i$, we perform the following simultaneous operations:\n(i) k-NN - Identify the k nearest neighbors of $x_i$, forming a local subset $S_i \\subset X$. This can be expressed as:\n$S_i = k-NN(x_i, X, k)$\n(ii) Fuzzy C-Means Clustering - Apply Fuzzy C-Means clustering to obtain membership scores for $x_i$ relative to P centroids. The membership score $U_{ip}$ of a data point $x_i$ to the p-th centroid, $C_p$, is defined as:\n$U_{ip} = \\frac{1}{\\sum_{j=1}^{P} (\\frac{d(x_i,C_p)}{d(x_i,C_j)})^{\\frac{2}{m-1}}}$\nwhere d(xi, cj) represents the Euclidean distance between $x_i$ and centroid $c_j$, and m is the fuzziness parameter that controls the degree of fuzziness in the clustering. The parameter m is commonly set to 2 in Fuzzy C-Means clustering, as this is a widely accepted standard. Accordingly, we adhere to this typical value for m in all our experiments.\nOnce the membership scores are computed, the centroids are updated in the following manner:\n$C_p = \\frac{\\sum_{i=1}^{N} U_{ip}^{m} x_i}{\\sum_{i=1}^{N} U_{ip}^{m}}$\nThe entire process of calculating membership scores and centroid updates repeats for v iterations. Although higher value of v results in more robust centroids, it consumes significant amount of time even for small number of centroids, hence we restrict v = 1.\nThe set of K centroids with highest $u_{ip}$ are then selected to form the set $L_i$ for the data point $X_i$.\nGiven $S_i$ and $L_i$, we update node $x_i$ through the proposed graph convolution in the following manner:\n$x'_i = (x_i \\oplus max(S_i - x_i) \\oplus max(L_i - X_i))$\nwhere $ \\oplus$ denotes a non-linear operation implemented by an MLP network with GELU [13] non-linearity and $ \\bigoplus$ denotes concatenation operation. The proposed graph convolution, a variant of the max-relative graph convolution [10], is specifi- cally designed to capture hierarchical and multiscale relation- ships. The operation max(Si-xi) involves first subtracting the central node xi from each node in the set Si on an element- wise basis. Then, the max operation is applied across the resulting differences to capture the maximum deviation of the neighboring nodes from the central node along each feature dimension. Similarly, the operation max($L_i \u2013 x_i$) follows the same methodology but on a broader scale. $x'_i \\in R^{1 \\times 3C}$ is mapped back to the original dimensionality of $x_i$ using a linear projection function h(\u00b7), and then added to $x_i$ to produce the final updated node $y_i$:\n$y_i = x_i + h(x'_i)$"}, {"title": "2) ConvFFN:", "content": "ConvFFN is applied to each updated node embedding that emerges from the local-higher order graph convolution. A ConvFFN block, as proposed by [14], consists of two 1\u00d71 convolutions, one 3\u00d73 depth-wise convolution and one non-linear function, i.e., GELU. While Feed-Forward Networks (FFNs) were originally introduced within the context of Transformers, characterized by two linear layers separated by a non-linear activation, the incorporation of depthwise convolution serves to preserve local information across layer depth.\nNotably, prior research indicates that self-attention acts like a low-pass filter [15] and ConvFFN counteracts this effect by preserving high-frequency information [16], hence we employ this block to retain local correlation information throughout the layers."}, {"title": "3) Downsample Block:", "content": "The ConvFFN block output is re- shaped to $R^{\\frac{H}{r} \\times \\frac{W}{r} \\times C}$ and then processed by a downsampling block, reducing dimensions by a factor of r to $R^{\\frac{H}{r} \\times \\frac{W}{r} \\times C_t}$ where r is the downsampling ratio and $C_t$ is the new channel count at stage t. Downsampling is achieved by applying a Conv2d layer with a 3 \u00d7 3 kernel, stride 2, and padding 1. The downsampled feature map serves as input for the next stage, repeating processes from Sections II-B1 and II-B2."}, {"title": "C. Implementation & Pretraining Details", "content": "We follow a pyramid architecture similar to [10], where the channel dimensions progressively increase within each block, following the sequence [80,160,320,640]. The LHG blocks are iteratively applied, repeated in the sequence of [2,2,6,2] for stages 1, 2, 3, and 4, respectively. Our best results are obtained with k = 25 for k-NN and K = 10 for selecting the top K centroids based on membership scores. The number of centroids P remains constant at 50 across all stages and for ImageNet pretraining, we adapted the training protocol from [10], modifying the batch size to 512 and reducing the learning rate to le \u2013 3. Also, due to input size mismatch, the best results are obtained with k = 9 for k-NN and K = 5 for ImageNet pretraining."}, {"title": "III. EXPERIMENTS", "content": "We assess the model's performance across two tasks: tag- ging and classification. Audio tagging evaluation is conducted on Audioset [17] and FSD50K [18]. For audio classification, the model is evaluated using the ESC50 dataset [19]."}, {"title": "A. Audioset Experiments", "content": "1) Dataset and Experimental Procedure: AudioSet [17] comprises over 2 million 10-second audio clips extracted from YouTube videos, categorized into 527 sound event classes. It is a weakly labeled and multi-labeled dataset, where each clip can have various tags, but specific timestamps for the onset and offset of these labels are not provided.\nWe trained our model on the full-train set (2M samples) and evaluated it on the evaluation set (22K samples). All audio samples were converted to mono with a sampling rate of 16kHz. We computed the Short-time Fourier transform (STFT) using a window size of 25 ms and a hop size of 10 ms. A 128-dimensional mel filter bank was applied, followed by a logarithmic transformation to extract the log-mel spectrogram. To ensure uniformity, we standardized the temporal length of the mel-spectrogram to 1024 frames, resulting in a consistent shape of (1024, 128). Shorter clips were zero-padded, and longer ones cropped.\nFollowing the training pipeline suggested in [20], we used mixup [21] data augmentation with a = 0.5, spectrogram masking [22] with a time-mask of 192 frames and frequency mask of 48 bins. The LHGNN was implemented in PyTorch and trained using the AdamW optimizer with parameters \u03b2\u2081 = 0.9, \u03b22 = 0.999, \u20ac = 10\u22128, and a decay rate of 0.05. Training was conducted with a batch size of 128, distributed across four NVIDIA Tesla A100 GPUs."}, {"title": "2) Results on Audioset:", "content": "In Table I, we compare our model with different benchmark models. DeepRes [23], PANN [24] and PSLA [20] are CNN based models and AST [5] is a transformer based model. The reported scores for AST, PSLA, and LHGNN were calculated using weighted average of different model checkpoints as mentioned in [20]. Notably, the LHGNN model surpasses AST in performance while utilizing a significantly smaller number of parameters.\nA key observation is the distinct performance gap between AST and LHGNN when neither model is pretrained with ImageNet. This underscores the significant influence of ImageNet pretraining on supervised audio based tasks. The impact of such pretraining is further highlighted by comparing the performance outcomes of models like DeepRes, which lacks ImageNet training, to those that include it, such as PSLA, and AST. ImageNet pretraining is resource-intensive and time-consuming. However, LHGNN performs exceptionally well without pretraining, demonstrating the model's robustness and efficiency."}, {"title": "B. FSD50K Experiments", "content": "1) Dataset and Experimental Procedure: FSD50K [18] is a public dataset of weakly labeled sound event audio clips, classified into 200 categories using the AudioSet ontology. It consists of 37,134 training samples, 4,170 validation samples, and 10,231 evaluation samples. Like AudioSet, FSD50K is multi-labeled. We applied the same feature extraction and data augmentation pipeline as in the AudioSet experiments."}, {"title": "2) Results on FSD50K:", "content": "In Table II, we compare our model with different benchmark models. FSD50K baseline is a CNN based model, whereas Wav2CLIP [25] employs distillation from contrastive language-image pre-training (CLIP) [27]. Audio Transformer, like AST, is a self-attention model but uses a learnable MLP frontend to extract representations directly from raw audio. As shown in Table II, LHGNN with ImageNet pretraining achieves the best score compared to the benchmark models. Additionally, when trained from scratch, it delivers results comparable to AST with ImageNet pretraining, demonstrating its effectiveness even without relying on large- scale pretraining."}, {"title": "C. ESC50 Experiments", "content": "1) Dataset and Experimental Procedure: ESC50 [29] is a multi-class audio dataset consisting of 2000 audio clips, each with 5-sec duration. It is labelled with 50 environmental sound classes across 5 folds. Our model was trained for 5 times by selecting 4-folds (1600 samples) as training and 1-fold (400 samples) as test set. The entire experiment was repeated for 5 times with different random seeds to get the mean score along with its deviation. Accuracy is used as the evaluation metric for all experiments."}, {"title": "2) Results on ESC50:", "content": "We evaluate the ImageNet trained LHGNN on ESC50 dataset and observe that the model performs well on multi-class scenario as well. However, as shown in Table III, the ERANN [28] model performs equally well without pretraining."}, {"title": "IV. ABLATION STUDY", "content": "We conducted ablation studies on the FSD50K dataset with- out pretraining to optimize our model's parameters. FSD50K was chosen for its balance between size and scalability."}, {"title": "1) Graph Kernel:", "content": "As shown in Table IV, combining local feature information with cluster centroids produced the best results, likely due to the loss of local information when solely employing cluster information in ($x_i \\oplus max(L_i - x_i)$)."}, {"title": "2) Clustering Method:", "content": "Table V compares k-means, Fuzzy C-Means, and density-based clustering. While density-based clustering slightly outperformed Fuzzy C-Means, the latter was chosen for its computational efficiency."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "This paper presents LHGNN, a new model that combines graph neural networks with clustering techniques to improve audio classification and tagging. Our experiments showed that LHGNN outperforms AST models across multiple datasets, including Audioset, FSD50K, and ESC-50, performing notably well even without pretrained weights.\nIts key innovation in the proposed model is the combination of k-nearest neighbor graphs and Fuzzy C-Means clustering to capture complex audio patterns. Despite strong performance, LHGNN takes longer to converge, requiring 30 epochs on Audioset compared to 5 for AST with ImageNet pretraining. Furthermore, a more efficient method for integrating cluster centroids and local information needs to be devised in order to reduce the overall computation time. Ultimately, evaluating the model's performance across a spectrum of audio tasks, such as music tagging and speech recognition, becomes imperative to affirm its efficacy and versatility. This approach is particularly essential given the demonstrated success of Transformers across a diverse range of audio applications. We intend to address these limitations in our future work.\nIn conclusion, LHGNN stands as a significant step forward in the field of audio classification and tagging, providing a robust framework that leverages graph-based and clustering methodologies to achieve high performance."}]}