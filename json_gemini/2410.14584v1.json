{"title": "MCSFF: Multi-modal Consistency and Specificity Fusion Framework for Entity Alignment", "authors": ["Wei Ai", "Wen Deng", "Hongyi Chen", "Jiayi Du", "Tao Meng", "Yuntao Shou"], "abstract": "Multi-modal entity alignment (MMEA) is essential for enhancing knowledge graphs and improving information retrieval and question-answering systems. Existing methods often focus on integrating modalities through their complementarity but overlook the specificity of each modality, which can obscure crucial features and reduce alignment accuracy. To solve this, we propose the Multi-modal Consistency and Specificity Fusion Framework (MCSFF), which innovatively integrates both complementary and specific aspects of modalities. We utilize Scale Computing's hyper-converged infrastructure to optimize IT management and resource allocation in large-scale data processing. Our framework first computes similarity matrices for each modality using modality embeddings to preserve their unique characteristics. Then, an iterative update method denoises and enhances modality features to fully express critical information. Finally, we integrate the updated information from all modalities to create enriched and precise entity representations. Experiments show our method outperforms current state-of-the-art MMEA baselines on the MMKG dataset, demonstrating its effectiveness and practical potential.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge Graphs (KGs) represent a structured semantic knowledge base, typically organized using RDF or property graph models. With the proliferation of social media, the volume of image and textual data has surged, leading to the emergence of Multi-modal Knowledge Graphs (MMKGs), which are extensively applied in semantic search, recommendation systems, and artificial intelligence [1]\u2013[9]. However, a single MMKG often needs more complete data and isolated information, constraining the breadth and accuracy of knowledge coverage. To address this issue, multi-modal entity alignment (MMEA) methods have been introduced, aiming to align identical or related entities across different MMKGs and thereby integrate information to construct a more comprehensive knowledge system.\nWhen performing MMEA tasks, effectively leveraging the visual and attribute knowledge within MMKGs presents a significant challenge. Traditional approaches frequently rely on shared features to learn entity representations, but this often neglects modality-specific characteristics, resulting in data loss [10]\u2013[15]. Convolutional neural network(CNN) and Graph Neural Networks(GNN) has advantages in many fields, such as [16]-[20] and [21]-[24], so I will choose one of these networks to design the model. This paper introduces a novel approach, the Multi-modal Consistency and Specificity Fusion Framework (MCSFF), designed to enhance the precision and robustness of entity alignment. MCSFF not only captures consistent information across modalities but also preserves the specificity of each modality, leading to a more comprehensive and accurate entity representation. Moreover, integrating scale computing ensures that this framework can efficiently handle large-scale data in extensive computational environments.\nThe principal contributions of this paper are as follows:\n\u2022 We propose the MCSFF framework, which captures consistency and specificity information across multiple modalities to facilitate entity alignment.\n\u2022 We design the Cross-Modal Consistency Integration (CMCI) method, which removes noise through training and fuses information to achieve more robust entity representations.\n\u2022 We introduce a single-modal similarity matrix computation module that retains the unique information of each modality.\n\u2022 Experimental results demonstrate that MCSFF outperforms existing methods on several public datasets, showcasing its effectiveness in multi-modal entity alignment tasks."}, {"title": "II. RELATED WORK", "content": "Traditional Entity Alignment (EA) techniques typically employ data mining or database methods to identify similar entities, but these approaches often need help to achieve high accuracy and generalizability. In recent years, EA methods have increasingly leveraged deep learning to derive entity embeddings, significantly improving detection precision. Based on the type of entity embeddings used, EA methods can be broadly categorized into two types: (1) translation-based EA methods and (2) GNN-based EA methods. Translation-based methods aim to map entity embeddings from one knowledge graph into the embedding space of another, aligning entities by calculating their similarity. Prominent methods include MTransE [17], [25], [26]. GNN-based methods capture complex structural information through Graph Neural Networks (GNNs), updating entity representations by integrating information from neighbouring entities. Notable models include GCN-Align [27] and HGCN [28].\nEarly entity alignment techniques, such as string matching, rule-based alignment, and knowledge base alignment, laid the groundwork for the development of the EA field. However, due to their limited semantic understanding and lack of efficient automation, these methods faced significant challenges when handling complex and large-scale datasets. With the increasing availability of image, video, and audio data, entities can now be represented through a combination of textual, visual, and auditory information. Chen et al. [29] proposed the Multi-Modal Entity Alignment (MMEA) framework, which extracts relational, visual, and numerical embeddings using TransE [30], VGG16 [31], and the radial basis function (RBF) [32], respectively, and then fuses them to obtain comprehensive entity embeddings. In another work, Chen et al. [33] introduced the Multi-modal Siamese Network for Entity Alignment (MSNEA), which integrates visual features through an enhancement mechanism and adaptively assigns weights to attribute information to capture valuable insights. Zhu et al. [34] proposed PathFusion, which represents multiple modalities by constructing paths connecting entities and modality nodes, thereby simplifying the alignment process. They also introduced an Iterative Refinement Fusion (IRF) method that effectively combines different modality information by using paths as information carriers [12]."}, {"title": "III. PROPOSED METHOD", "content": "We propose the method of MCSFF, the framework of which is shown in Fig. 1. In KG Preprocessing, we design a new relation-based information transfer method, and use existing methods to obtain the textual semantic information of entity nodes and their attributes as feature vectors. Then, a variety of information graphs are generated based on the graph attention mechanism to learn the latent information between entities. A dense fully-connected layer then follows each attention for message passing, which captures deeper structural information and further improves information flow between layers. Afterwards, we add a linearly connected layer to aggregate the output of all dense connection layers to get the final entity node representation. Finally, we get the aligned entity through the entity alignment module."}, {"title": "A. Specific Information Processing Module", "content": "Upon examining the utilized dataset, it became evident that the attribute information consists of numerical data, with all attribute values being numeric. Consequently, we process attribute names and values separately to compute our attribute similarity matrix. For textual information, we derive the similarity matrix based on attribute names through matrix multiplication and other methods, using the embedding matrix. For attribute values, we calculate the inverse of the differences between values to determine their correlation, and by multiplying and combining this attribute information, we obtain a matrix, denoted as $S^A$, representing the similarity between two knowledge graph entities:\n$S^A = A_s \\times tanh \\left(W_K \\left(K_S K_T^T\\right)\\right) + \\frac{1}{\\left|V_S - V_T\\right| + \\epsilon} \\times A_V^T$\nHere, $W_K$ and $W_V$ represent weighted matrices for attribute name and attribute value similarity, respectively, designed to modulate the similarity weights between different attributes. $K_S$ and $K_T$ are the embedding matrices of attribute names for the source and target datasets, capturing the representation of attribute names in the embedding space. $V_S$ and $V_T$ are the attribute value matrices for the source and target datasets. $\\epsilon$ is a small positive constant introduced to prevent division by zero errors. The function $\\sigma(\\cdot)$ is a non-linear activation function employed to enhance the non-linear distribution of the similarity matrix.\nAn entity encompasses zero or more visual attributes. We perform a dot product on the obtained visual embedding matrices to ascertain the correlations between various visual embeddings. Subsequently, the outcome derived from the most significant correlation between two visual embeddings is denoted as the degree of visual modality relevance between the two entities. The matrix composed of these values constitutes the visual similarity matrix $S^V$.\n$S^{V}=\\max \\left(\\left(I_{i}^{k}\\right)^{T} \\times \\left(I_{j}^{k}\\right)\\right)$"}, {"title": "B. Consistency Information Processing Module", "content": "For the embedded entities, we perform a weighted summation to obtain the visual and attribute representations of the entities:\n$S(i d)=\\left\\{I D_{1}, I D_{2}, I D_{3}, \\ldots, I D_{z}\\right\\}$\nwhere $z$ represents the total number of entities, and $S(i d)$ denotes the set of entities with ID id.\n$\\overrightarrow{E_{i}}=\\frac{\\sum_{j=1}^{m_{i}} \\overrightarrow{w_{i j} X_{i j}}}{\\sum_{j=1}^{m_{i}} w_{i j}}$\nHere, $X_{ij}$ denotes the j-th information embedding of entity i, which could be the visual embedding $I_{ij}$ or the attribute information embedding $K_{ij}$ or $V_{ij}$. The weight $w_{ij}$ is the weight of the j-th piece of information. $m_i$ represents the total number of information items the entity possesses (whether visual or attribute). The embedding $E_{i}$ is the composite embedding of entity i, which can represent either the visual information or the attribute information.\nFor entities lacking attribute information or visual information, we aggregate the information from neighboring entities to represent the corresponding modality embedding $E^{(0)}$:\n$E_{i}^{(0)}=\\sigma\\left(\\frac{1}{c_{i j}} \\sum_{j \\in N(i)} w_{i j}^{(0)} E_{j}^{(0)}\\right)$"}, {"title": "C. Information Integration", "content": "First, the entity embeddings are sampled five times, from which we derive the corresponding entity embeddings (since we only need to validate the embedding scores in the validation set, we use the entity embeddings in the validation set, denoted as Lvec and Rvec). Based on this information, we calculate similarity scores to construct the similarity matrix $S^E$ based on the entity embeddings:\n$S^{E}=L_{v e c} \\cdot R_{v e c}^{T}$\nNext, we combine the visual, attribute, and entity similarity matrices, ultimately obtaining the similarity matrix for evaluation:\n$S=S^{E}+S^{A}+S^{V}"}, {"title": "IV. EXPERIMENTS", "content": "In this paper, we utilize two multimodal datasets, FB15K-DB15K and FB15K-YAGO15K, to evaluate the performance of our network. Both datasets contain approximately 30,000 entities, each of which is associated with zero or more images, numerical information, and extensive relational data. FB15K-DB15K comprises 12,846 alignment seeds, while FB15K-YAGO15K includes 11,199 alignment seeds. To enhance the model's performance, we preprocessed the data. We downloaded the image data based on the entities' corresponding image URLs and obtained the image embeddings using relevant networks, storing them in files for downstream tasks. The numerical information for each entity was aggregated to facilitate subsequent tasks.\nWe compare our method with the following baselines, divided into two categories, traditional methods: MtransE [25], GCN-Align [27], BootEA [35] and multimodal methods: MMEA [29], MEAformer [36], PathFusion [34].\nWe utilized 20% of the alignment seeds for training the network, with the remaining 80% reserved for testing. The network's performance was evaluated using the MRR, Hits@1, Hits@5, and Hits@10 metrics, focusing on the alignment seeds."}, {"title": "V. CONCLUSION", "content": "In this study, we propose a novel multimodal entity alignment framework, the Multimodal Consistency and Specificity Fusion Framework, designed to capture consistency and specificity information across modalities. Within this framework, we introduce the Cross-Modal Consistency Integration network, which refines and integrates information from various modalities to obtain more accurate entity embeddings. Our network will contribute significantly to future research in entity alignment. However, based on our current understanding, relationship information could serve as an independent module to generate a corresponding similarity matrix, thereby enhancing the utilization of multimodal information. Nevertheless, designing an effective method to calculate similarity scores based on the characteristics of relationship information remains a challenge. In our future work, we plan to address this issue, aiming to improve the accuracy and robustness of our entity alignment network."}]}