{"title": "A Survey of Recent Advances and Challenges in Deep Audio-Visual Correlation Learning", "authors": ["Lu\u00eds Vila\u00e7a", "Yi Yu", "Paula Viana"], "abstract": "Audio-visual correlation learning aims to capture and understand natural phenomena between audio and visual data. The rapid growth of Deep Learning propelled the development of proposals that process audio-visual data and can be observed in the number of proposals in the past years. Thus encouraging the development of a comprehensive survey. Besides analyzing the models used in this context, we also discuss some tasks of definition and paradigm applied in Al multimedia. In addition, we investigate objective functions frequently used and discuss how audio-visual data is exploited in the optimization process, i.e., the different methodologies for representing knowledge in the audio-visual domain. In fact, we focus on how human-understandable mechanisms, i.e., structured knowledge that reflects comprehensible knowledge, can guide the learning process. Most importantly, we provide a summarization of the recent progress of Audio-Visual Correlation Learning (AVCL) and discuss the future research directions.", "sections": [{"title": "1 INTRODUCTION", "content": "A wide variety of multimedia information and data such as image, text, visual data, and audio is aggregated on the Internet over time, bringing opportunities to build knowledge from the structure hidden in such heterogeneous data through Deep Learning (DL) methods. Understanding the knowledge and structure hidden in natural Audio-Visual (AV) phenomena requires the ability to process multiple signals that compose the concept of what we are experiencing."}, {"title": "1.1 Problem Definition", "content": "The emergence of Deep Learning (DL) and available audio-visual datasets has facilitated the development of Audio-Visual Correlation Learning (AVCL). AVCL seeks to relate both audio and visual data through Deep Neural Networks (DNN) into their feature sub-spaces where the joint space composed by their aggregation preserves the shared information contained in each modality and emphasize their commonalities. In a nutshell, we are interested in data consisting of tuples $x^{(n)} = (x_1, x_2)_{n=1}^{N}$, where $x_1$ and $x_2$ are features extracted for audio and video respectively and $d_1$ and $d_2$ can be of arbitrary sizes and dimensions.\n\n$x^{(n)} = (x_1, x_2)_{n=1}^{N}  \\ x_1 \\in R^{d_1}, x_2 \\in R^{d_2}$\n\nRepresentations are bridged into a common space S, which can be shared or not:\n\n(1) Shared: $S_{12}$ is obtained through h($S_1, S_2$), which establishes the common and shared representation space between modalities.\n\n$S_{12}= h(f_{1\\rightarrow12}(x_1), g_{2\\rightarrow12}(x_2))$\n(2) Separate: $S_1$ and $S_2$ are obtained through f($x_1; W_1$) and g($x_2; W_2$).\n\n$S_1 = f_{1\\rightarrow12}(x_1) \\ S_2 = g_{2\\rightarrow12}(x_2)$\nThe weights of f, g, and h are adjusted through back-propagation to maximize the correlation between modalities.\nThe \"joint\u201d space can be obtained through feature fusion or coordination, where the final output consists of single or parallel representations. As illustrated in Figure 1, all application scenarios propose to learn better representations for their specific goals. Usually, these proposals attempt to achieve their goals by constraining the outputs to have desired statistical properties or linking relevant features/subcomponents between them to synchronize semantic contexts [12, 77]. Both feature fusion and coordination can leverage these methodologies, as described in this paper.\nThe learning problem can be established as a supervised, self-supervised, or unsupervised task. The selection of the most suitable learning framework depends on how the modalities, e.g. audio and video, are used and if auxiliary information is available or not. For instance, exploring relations within tuples $(x_1, x_2)_{n_1=n_2}^{N}$ and assuming each pair as having specific semantics, i.e. pseudo-labels, allows extracting correlations in a self-supervised way. In contrast, it can also be desirable to relate modalities as a whole $(x_1, x_2)_{n_1=n_2\\forall n_1\\neq n_2}^{N}$ and take into account supervised information (i.e., labels) regarding each pair of representations (audio and visual data). In the unsupervised setting, we observed many examples focusing on reconstruction between modalities (and modified versions of each modality), where the latent spaces can also be used for AVCL. Additionally, we often use two terms relative to the learning process that help discriminate the type of relations extracted from both modalities: intermodal and intramodal. Intermodal refers to information shared between modalities, i.e., commonalities, while intramodal refers to specific information contained in each modality.\n\nWithin our research, we have observed that adversarial learning, reconstruction or weighted pooling processes through models such as Generative Adversarial Network (GAN) [124, 187], Auto-Encoder (AE) [113, 198] and models that explore multiple attention mechanisms [91, 188] have been investigated in recent years to improve the capability of AVCL methods. However, the current panorama of the SOTA is saturated with attention-based methods (i.e., transformers) due to its excellent performances in large-scale scenarios. Therefore, we find it necessary to review not only recent progress on large-scale attention-based models but also provide a systematic view of the field of AVCL."}, {"title": "1.2 Contribution and Relevance to existing surveys", "content": "Our research analyzed numerous published surveys focusing on AV processing and multimodal learning. The following sections provide a brief analysis of them and summarize this article's contributions."}, {"title": "1.2.1 Analysis of existing surveys", "content": "Existing surveys that cover AV processing introduce generic applications, core ideas, and theoretical concepts of Multimodal Machine Learning (MMML) [12, 77, 79]. [131] reviews the MMML field from the application level and lacks descriptive content for each method. More recently, a survey by Liang et. al. [79] proposed a taxonomy for the main research challenges within MMML with evidences from AVCL but without focusing entirely on audio-visual data.\n\nDedicated to AV learning we analyzed the following works: [97, 127, 159, 192]. [97] provides a benchmark study of various approaches for AV speech recognition. Specifically, the authors emphasize the impact of isolated modalities in the shared representation scenario, i.e., what happens when one modality is unavailable. Nevertheless, they only cover one application use case, and the paper is rapidly getting outdated (the field made advances since then). More recently, [192] and [159] provide new taxonomies and extensively cover the field, but their analyses only include overviews at an application level without detailing how they are solved. In some cases, works can fit the descriptions of multiple categories in the proposed taxonomies [159], which fails at grouping different methodologies. We believe this promotes the need for a study that compares and categorizes the different methodologies used in AV learning over all its application use-cases. This is relevant because methods that target different applications have similar properties. Thus, their similarities and differences should be highlighted and discussed. While existing surveys are valuable, the emergence of a large volume of audio-visual content learning methods makes it necessary to conduct a comprehensive comparison of these works, highlighting their similarities and differences. Our survey fills this gap by offering a comprehensive study of deep correlation learning between audio and visual contents. This is the main contribution of our survey paper compared with others available.\n\nWe also reviewed survey papers that focus on different methodologies for representing knowledge in multimodal domains [79, 171]. The combination of multiple modalities regarding depth of concept relationships and raw signal perception is discussed. This is relevant for our study since we aim to cover how knowledge is represented and correlated in the AV domain. This is related to the research challenges introduced by [79].\n\n[171] presents a taxonomy for the field, dividing it into deep and canonical knowledge representation. The first covers deep models for encoding raw signals into abstract representations. The latter assumes highly abstract concepts as inputs and concentrates on methods for extracting relationships from and among them. The authors also discuss the lack of explainability of deep models used for learning representations. This is relevant for current methodologies of the SOTA due to the number of proposals that rely on self-supervised learning. The authors believe that explainability can be relieved by introducing comprehensible knowledge in the learning process (i.e., similar to structured knowledge used in self-supervised learning settings). Moreover, it can also provide the means to evaluate the proposed models thoroughly."}, {"title": "1.2.2 Contributions", "content": "We address the natural complementary relationship between vision and audio that makes them suitable for exploiting underlying semantics. By analyzing the latest works in the field, we identified that the proliferation of attention-based models and objective functions based on self-supervised learning have provided a new bloom for the field, as can be observed by the high performances demonstrated in Table 1. Therefore, this survey paper focuses on AVCL methodologies regarding feature extraction, objective functions, used datasets, and evaluation metrics. We propose a structure of the field that systematically groups proposals according to the methods used to correlate audio-visual representations. We aim to study how knowledge is represented and leveraged between audio and visual data on all application use cases illustrated in Figure 1. Specifically, we review the models, objective functions/learning settings, and datasets. The main contributions of our article are listed below:\n\n(1) Systematic categorization of recent proposals according to the models and learning frameworks used. We believe this could support new researchers in quickly studying how audio-visual data and their respective semantics can be exploited and extended for their research projects.\n\n(2) The different techniques for AVCL are categorized using brief details of their methodologies (and corresponding references). This can help new researchers to AVCL to have a general view of the field.\n\n(3) We provide discussions of benchmark datasets and a global list of all datasets used for AV processing.\n\n(4) We cover how structured knowledge (e.g., proxy tasks for self-supervised frameworks) is injected in AVCL models and discuss future alternatives and challenges for the field."}, {"title": "2 FEATURE EXTRACTION", "content": "This section provides a brief analysis and discussion of the most used methods for extracting features from raw AV data, i.e., $x_a$ and $x_v$. Since AVCL uses these descriptions/features to extract knowledge from paired data, we believe it is essential to understand how AV information is encoded."}, {"title": "2.1 Audio Encoding", "content": "From the taxonomy of [104], audio signals can be divided into three categories due to their very distinct characteristics (i.e., frequency range and envelope):\n\n(1) Speech: characterized by a smooth envelope, i.e., transition between audio events, and is mostly present in an audible range from 100Hz to 7kHz;\n\n(2) Music: exhibits dependencies between its simultaneous sound sources in time and frequency (their occurrence follows constraints in time and frequency) and mostly provides repetitive patterns in an audible range that can go from 40Hz up to 19.5kHz;\n\n(3) Ambient Sounds: characterized by multiple independent sound sources, with or without periodicity. Some sounds have representations sparse in time and frequency due to the lack of periodicity. In contrast with the other two scenarios, ambient sounds are the most variable and have a broad audible range covering all others.\n\nAs such, audio representations used for AVCL must capture structures and dependencies in time and frequency. SOTA models for deep audio analysis use intermediate representations as input [69, 104]. This pre-processing step provides better descriptions of characteristics in both dimensions and matches the input requirements, i.e., input shape, of these models. The most used representations are:\n\n\u2022 Spectrograms: compressed representations in the time-frequency domain (i.e., heterogeneous dimensions). They have the downside of losing phase, creating challenges for reconstruction;\n\n\u2022 Mel Frequency Cepstral Coeficients (MFCC): Magnitude/power projected to a reduced set of frequency bands, converted to a log scale (perceptually relevant audio scale), and compressed with the Discrete Cosine Transform (DCT);\n\n\u2022 Log-Mel Spectrograms (LMS): Same processing steps as MFCC, but without compression from the DCT;\n\n\u2022 Constant-Q: High and low frequencies have lower and higher resolution (bandwidth), respectively; Reduces calculations in higher frequencies due to the reduction in frequency resolution;\n\n\u2022 Musical Instrument Digital Interface (MIDI): standard used to encode symbolic music.\n\nSOTA models can use raw waveforms (AW), increasing computation and data needs. These models need to extract features from long data sequences without sampling, leading to spectrogram-based representations. These compress temporal information but lose some details. LMS is favored over noise-sensitive MFCCs within these representations [139]. MFCCs are obtained via DCT and a filtering process, which selects cepstral coefficients relevant for speech audio but its dependent on the spectral form [139].\n\nWe selected AudioSet [43] as our baseline dataset for comparing audio encoding models, due to its popularity. In Section A we present its test performances for AudioSet. We categorized feature extraction methods by learning type and intermediate representations used, and detailed the spectrogram's computation parameters, audio embedding size, and kernel size and stride of the first layer for the audio waveform. This information aids in understanding the problem complexity through time-frequency representation dimensions."}, {"title": "2.2 Visual Data Encoding", "content": "Apart from Youtube-8M [1], there are no commonly used benchmarks for general video classification. Therefore, in this analysis, we will consider the task of Action Recognition (AR) due to its popularity and because it is one of the most common classification tasks where video frame sequences are analyzed. Nevertheless, AR datasets tend to be human-centric, and SOTA methods tend to infer actions by typically leveraging motion-related features [164]. Therefore, our analysis of visual feature extraction models is based on the following video classification architectures from which AR models commonly derive. We gathered performances for the top-2 benchmarking datasets for action recognition, HMDB-51, and UCF-101, as illustrated in Table 3 (Section B). We present results with the two most common evaluation settings, fine-tuning and linear evaluation, and order them using the mean of both results. In Figure 3, we summarize the identified approaches for feature extraction on visual data (video).\n\nImage/frame-based methodologies (Figure 3d) treat video clips as a collection of frames, using models for image classification to gain feature representation. This is then averaged or dimensionally reduced over the temporal axis.\n\nTwo-stream models decompose videos into spatial and temporal components (Figure 3c). They use two branches: RGB (Visual component) and Optical Flow (Temporal component) [55, 107]. These models require optical flow extraction, which can be costly, but some approaches estimate this from the RGB frames [102]. Another limitation is that the temporal stream is solely responsible for capturing features from motion [164]. Temporal Segment Networks (TSN) [150] extend two-stream models by dividing longer videos into segments, each classified separately, with the final classification merging the segment scores [48, 180].\n\n3D convolutional networks exploit spatial and temporal information with 3-dim kernels (Figure 3e). These are complex to train due to the increased number of parameters with an expanded temporal dimension. To address this, some proposals replace 3D convolutions with separate spatial and temporal convolutions, reducing parameters and computational cost [16, 138].\n\nModels focused on long temporal dynamics use Recurrent Neural Networks (RNN), Long Short Term Memory Networks (LSTM) or Transformers for the temporal axis [164, 174]. While sequence models are commonly placed on top of features from image classifiers, attention-based models reduce temporal complexity and favor parallel computation, leading to a rise in Transformers replacing RNNs/LSTMs (Figure 3b).\n\nTop models capture temporal correlations between frames [27, 86, 105, 121, 133] and their relation with other modalities [67, 108, 161], with Transformers excelling in this aspect. Initially applied to images, this architecture divided images into patches, transformed them into embeddings, and used them in a standard transformer model [33]. This approach drives the best-performing unimodal and multimodal proposals [37, 108, 161].\n\nVideo Masked Autoencoder (VideoMAE) and Video Vision Transformer (ViViT) extend the ViT architecture for video data, using visual embeddings from different video segments and applying masking strategies temporally (Figure 4) [10, 136]. Extensions propose new masking schemes, motion trajectory reconstruction, and knowledge distillation from larger models or modalities [122, 132, 144, 152]. VideoMAE2 is currently the best performer on average [144].\n\nText can also be exploited as supervision [67, 81, 108, 161]. For instance, Contrastive Language Image Pretraining (CLIP) proposed to learn text/image representations from large-scale datasets [108]. The approach can be applied to videos by selecting the most relevant frames [67, 161]. Both approaches predict which caption matches the audiovisual input. CLIP and VideoMAE show promising results for HMDB-51 and UCF-101. Despite not topping the UCF-101, CLIP is a top performer using only one frame (+ ViT) [108, 161]. These results are due to self-supervised learning frameworks leveraging large data amounts without supervision.\n\nLimited supervision learning needs inductive biases for relevant representations. Temporal cohesion and correlations, learned via pairs of relevant and non-relevant embeddings, are crucial in videos [105, 106, 121, 133, 135]. This is the starting point to encourage features to be distinct across the temporal dimension [27]. The model should also learn semantic relations between video frame sequences, exploitable through additional semantic information [28, 37, 106, 135].\n\nLate fusion models (5) combine multiple decisions for a final prediction (Figure 3a). Inputs can include predictions from video segments [150] or high-level information [146].\n\nVideo processing is computationally heavy due to high frame rates and extensive data per frame. Methods to reduce complexity include video sampling strategies [48, 156, 189]. Dynamic sampling rates can be achieved by leveraging motion [80, 189], high-level information like objects and people [157], using statistical methods [189], and relevance predictions [48, 70, 156] using features extracted within the temporal domain. In the UCF-101 and HMDB-51 context, SMART is the only dynamic video sampling example, selecting the top n frames with high discriminative scores [48]. It achieves top-5 scores on both datasets using ten frames, improving performance by 15% and 4% on HMDB-51 and UCF-101, respectively. SMART's potential with attention-based models remains unexplored. These results shed some light on the potential impact of frame sampling on AR and video processing.\n\nThe presented models are all viable options for obtaining visual embeddings for videos, and we believe that the choice of feature extractor for AVCL applications must be made according to the ratio between performance and input size (i.e., temporal length, number of frames)."}, {"title": "3 LEARNING AUDIO-VISUAL CORRELATION", "content": "In Audio-Visual Correlation Learning, audio and video representations can be obtained directly from existing pre-trained models [177, 183]. However, this results in representation spaces with different dimensions, thus hindering the extraction of relations between modalities. Due to inconsistent distributions and representations between them, we have to learn a common space to bridge this gap and further measure and maximize their correlation. In other words, since our input data ($x_1$ and $x_2$) belong to sub-spaces with different sizes, cross-correlating audio and visual data is most commonly done by using separate projections (f($x_1; W_1$) and g($x_2; W_2$)) [176, 186]. Nevertheless, this is not the only scenario that we should consider. Often, feature maps from different modalities can be correlated using attention-based approaches without requiring any projection. However, these sub-spaces should have similar dimensions.\n\nGenerally, Deep AVCL contains mainly two steps:\n\n(1) Extract temporal information from frame-level audio and visual features, where $x_a$ and $x_v$ are fed into two separate encoders. To better learn sequential dependencies between audio and visual data, recent works, such as the attention mechanism, focus on synchronizing semantic information between modalities and minimizing the heterogeneity gap simultaneously. We called them multimodal encoding models;\n\n(2) Objective functions optimize these two \"sub-networks\" to maximize the correlation between modalities in the \"joint\" space through back-propagation. According to different kinds of audio-visual data exploited in the deep learning stage, various learning frameworks (i.e., objective functions) are used in existing proposals.\n\nIn this section, we address the recent advances for step (1) and (2) and discuss some tasks and paradigms in Deep AVCL. In Table 1, we provide a brief summary of these SOTA AVCL methods."}, {"title": "3.1 Multimodal Encoding Models", "content": "Various encoding models are proposed to capture and synchronize semantic temporal information between audio and visual data, which facilitates the correlation of audio-visual sub-spaces."}, {"title": "3.1.1 Attention", "content": "In neural networks, the effect of attention aims to enhance some parts of the input data while attenuating other parts. It can be utilized for strengthening semantic information between modalities to align audio and visual sequences through a dynamic pooling process that allows learning relations between sub-elements (i.e., segments of audio or visual locations). It consists of a dynamic weighted sum of vectors with scalar values (i.e., probabilities) obtained through an interaction/score function (e.g., addition, dot-product, scaled dot-product). Therefore, it allows obtaining context from different sources (within or between modalities) to encode a given input (e.g., using $x_a$ to encode $x_v$). Attention is often computed using three kinds of vectors: Queries (Q), Keys (K), and Values (V). We calculate attention for the Qs, using Ks, which subsequently try to filter the results in Vs. This is analogous to a retrieval system, where we use the Q to filter the K and want to obtain the best results in V. We identified the following methodologies, which are illustrated in Figure 5:\n\ni. Co/Cross-Attention. (Figure 5a and 5b) enables the learning of pairwise attention, which can be used to explore inter-modal correlations by encoding a feature vector with context from another. Typically, a residual connection is added in the attended feature vector, which can be seen as adding missing relevant information from one modality to another. Co-attention allows learning correlations between multiple pairs of data [3, 34, 73, 73, 134, 170, 188]. Nevertheless, the context used to encode one representation can also be drawn from features extracted at multiple scales (i.e., different framerate) [160], different timesteps [89, 163] or between different sub-elements (e.g., image regions or patches) [40, 83]. In addition, memory banks can leverage context from previous training steps, which can further improve the quality of the context used [96]. Therefore, we can easily infer that relations between different sources of information can be obtained through attention. For AVCL, it is used to identify the most important patterns that characterize an audio-visual event (alignment). Furthermore, it can also be used to coordinate/align the weights of models used to extract features for each modality [91]. In other words, co/cross-attention can be used to coordinate/align representation spaces for different modalities.\n\nii. Self-Attention. (Figure 5c) relates different positions of a sequence to calculate a representation of this sequence, which can be used to explore the sequential structure of each modality (inter-modal), and it can be combined with co-attention to weight intra and inter-modal relations [23]. Moreover, this kind of attention can be used to extract relations from joint (multimodal) representations, which allows to weigh inter-modal relations. On the other hand, when applied to single modalities, it allows to weigh intra-modal features. It is the base of the Transformer (TF) architecture, which is applied in two different ways: multi-head self-attention (encoder) and masked self-attention (decoder). They both consist of the same process of adding several parallel self-attention projections. However, masked self-attention limits the context of each one to avoid attention bias (i.e., any query in the decoder only attends to all positions up to the current one).\n\niii. Transformer. is an encoder-decoder model with a self-attention mechanism as well as positional encoding, which can be exploited to capture semantic information between audio and visual data within the sequential structure of its input. TFs are formed by an encoder (i.e., multi-head) and a decoder (i.e., masked), as illustrated in Figure 5e, and an encoder-decoder module. The TF encodes the output from the decoder using the encoder context. Similar to co-attention, the TF encoder can be replaced by different feature extractors or features from sub-elements of each modality (e.g., past and future frames or different patches within the same frame) and leverage the implicit co-attention mechanism between modalities [39, 82, 94]. Furthermore, TFs can also be used to relate cross-translations between modalities of the same media asset to align/correlate them in sequence-to-sequence generation [94]. In Figure 6, we illustrate the proposed combinations of interactions between modalities that using Transformers.\n\nDiscussion: Attention mechanisms allow fusing and coordinating data from different sub-spaces by selectively weighting the importance of each element in each one, where redundant and relevant information is respectively removed or added [17, 194, 195]. The context for weighting the representations can be obtained from different combinations or features of $x_a$ and $x_v$ or their sub-elements. Thus, establishing where (and when) to draw context depends on the available data and the target application but opens a pathway for many possible extensions based on attention to audio-visual alignment. Nevertheless, attention requires an interaction/score function that fits the constraint length of each subspace and expresses the intended correlation between its inputs. For instance, addition requires inputs with equal lengths and accumulates the growth of both vectors, while the dot product can accept mixed sizes but capture the directional growth of one into the other. Therefore, defining the interaction function is crucial for many applications when attention is exploited. Specifically, for AV applications, attention can be used to provide global [94], local [91] and context between modalities [23, 124] to the extracted embeddings."}, {"title": "3.1.2 Auto-Encoders", "content": "AEs are unsupervised encoder-decoder models that create latent representations through reconstruction. The encoder $f_{\\theta_1}(x)$ computes the latent space $h = f_{\\theta_1}(x)$ from the input $x$ and the decoder computes its reverse mapping, $x = g_{\\theta_2}(h)$. Therefore, the latent representation reflects the structural distribution of the original data (similar to summarizing/reducing dimensionality). We identified the following methodologies (Figure 7) using AEs for AV applications:\n\ni. Using a shared latent representations obtained:\n\n(a) through concatenation of uni-modal latent spaces [153];\n\n(b) through attention between uni-modal latent spaces [184, 185].\n\nii. Injecting supervised semantic context using the latent space for classification [109];\n\niii. Imposing similarity constraints between separate latent spaces [14];\n\niv. Injecting additional information through attention in the latent spaces to condition the reconstruction [42, 59, 83];\n\nv. Leveraging similarities between reconstructions and cross-reconstructions [109, 113, 153];\n\nvi. Variational Auto-Encoder (VAE) reconstruction conditioned by multiple inputs [118, 181, 198];\n\nvii. Leveraging subsequent reconstructions [101].\n\nDiscussion: Through reconstruction, AEs are an efficient way to reduce the dimensionality of input features while preserving the same distribution of the original data, which can be applied for creating individual latent spaces of the same size (can be used as projectors) and balancing the information between coordinated sub-spaces (Figure 7a). For instance, the discrepancy between modalities can be reduced by AEs when the input features are obtained with different feature extractors (different information compression levels). This helps to balance the performance scores in applications where only one modality is given as input (e.g., cross-modal retrieval, retrieval in one modality using the other).\n\nInstead of strictly defined relations between modalities, using a shared latent space in reconstruction has the advantage of implicitly allowing each modality to leverage insight from itself and others (Figure 7b). This results in a lower heterogeneity gap than an approach with separate latent spaces. We have seen consistently better results than traditional methods based on Canonical Correlation Analysis (CCA) and Deep CCA [153]. Moreover, shared latent spaces in AE models make the input share the same context and apply constraints that affect all modalities simultaneously."}, {"title": "3.1.3 Generative Adversarial Networks", "content": "Generative Adversarial Network (GAN) are composed by two components: a generator G and a discriminator D. The generator G learns to produce target samples as similar to the real ones as possible to confuse the discriminator D, which attempts to distinguish generated samples from the real ones, keeping itself from being confused. Instead of directly mapping a latent representation from the data as AEs, GANs learn to implicitly map latent random representations to \u201csamples belonging to the training set\u201d, which try to narrow the difference between distributions of real (i.e., training set) and generated data.\n\nFor multimodal data, GANs can be used to translate different modalities. Moreover, their data generation process can also be conditioned using additional information, allowing them to learn a multimodal model. Therefore, as generative models, GANs can be used to learn representations, allowing to encode (multimodal) data. We identified the following methodologies based of GANs, which are also illustrated in Figure 9:\n\ni. Heterogeneity gap minimization through adversarial learning [124, 187];\n\nii. Using GANs and AEs for generation between modalities [11, 36, 92];\n\niii. Condition the discriminator by injecting context semantics in the form of:\n\ni. Class-labels [11];\n\nii. Latent representations of other modalities [36, 197];\n\niii. Classifiers' prediction on generated samples [11].\n\niv. Joint optimization between adversarial and other loss functions [92, 124].\n\nDiscussion: For retrieval between modalities with audio-visual data, the standard GANs adversarial scheme can be used to minimize the heterogeneity gap and obtain modality-independent representations [187] or to transfer knowledge between modalities that can be used to learn coordinated representation spaces [124]. These methods are often complemented with deep metric learning frameworks [187] or classification [124] to include semantic context. In contrast, from a generational point of view, we can also leverage reconstruction. The random vector is replaced by latent representations obtained from a model, typically an encoder-decoder, used for generating one modality with the other [11, 36, 92]. In this case, the main differences lie in exploring complementary information. The discriminator receives an input and tries to classify it as real or not, but its decision can be conditioned by adding information in the last layer before the classification. In addition, these models also allow to consider several sources of conditional information: latent representation (from AEs) [36] or class-label information.\n\nGANs perform generation of new samples in an unsupervised way, which reduces the dependency on training data and allows coping with scenarios of missing data. However, the randomness of the latent vector makes it hard to obtain the context semantics associated with it. This is an active research topic, and several works have been proposed to mitigate this through regularization or by adding auxiliary information. In addition, GANs suffer from training instability, requiring a careful tune of its hyper-parameters due to the adversarial nature of the process that makes the models' parameters oscillate. We believe that leveraging the generative nature of GANs (i.e., artificial data) will allow the development of more complex self-supervised frameworks, where the proxy tasks depend on samples generated with the learned data distribution. Thus, we expect to see their presence increase in future proposals for Deep AVCL.\n\nMore recently, diffusion models have been applied to the audio-visual domain. Diffusion Probabilistic Models are generative models that learn to convert a simple Gaussian distribution into the original data distribution [129]. It consists of forward diffusion and reverse denoising, where the input is corrupted with Gaussian noise (forward diffusion), and the model learns to remove the noise. Thus learning the data distribution of the input data.\n\nDiffusion can be implemented to perform cross-generation [50, 111, 199]. However, proposals typically focus on unimodal generation while incorporating additional information in the denoising process. The process can be formulated as follows:\n\n$P_{\\theta}(x_{t-1}|x_t, c) = N(x_{t-1}; \\mu_{\\theta}(x_t, t, c), \\sigma_{\\theta}^2(x_t, t)),$, where $\\sigma_{\\theta}(x_t, t) = \\beta_tI$\n$P_{\\theta}(x_{t-1}|x_t, x_0, c)$ is the posterior distribution between \"denoising\" steps also conditioned using additional information. Examples include class labels [31], text prompts [50], and information from additional modalities (e.g., audio if video generation is being performed) [8, 25, 72, 193]. Several works apply the two diffusion steps over pre-trained latent spaces (latent diffusion models), where the latent space can be encoded from multiple modalities [8, 115]. In these models, unimodal denoising is performed over the conditioned latent representations. In contrast, the joint distribution between AV can also be learned by denoising (reverse diffusion) samples from multiple modalities [117]. However, it increases the computational complexity of the training loop.\n\nDiffusion is at the forefront of current generative models. Despite their performances exceeding GAN-based models in text-to-image synthesis [31], there is little work on learning multimodal AV representations using diffusion models. Specifically, there is little exploration on the utilization of latent representations from forward diffusion for downstream tasks. We assume this is mainly due to the computational complexity associated with their training process as it requires multiple forward and backward passes through the model. The effectiveness of diffusion models and normalizing flows heavily relies on their iterative processes, which gradually construct a sample from random noise. This incremental refinement, involving sequential steps of evaluating large neural networks, leads to slower sampling speeds compared to single-step methods such as GANs and VAEs [117]. This inefficiency presents a challenge for real-time applications."}, {"title": "3.2 Objective Functions", "content": "Objective functions are used to guide the solution of optimization problems. In Deep AVCL, it helps to adjust audio-visual embedding models through back-propagation to maximize the correlation between modalities in the joint sub-space. This can help to maintain and capture specific audio-visual semantics. Typically, several regularization terms constrain the embeddings to mitigate the heterogeneity gap and further increase correlation.\n\ni. Cross-Entropy (CE) and other distance-based extensions can be used [54, 18"}]}