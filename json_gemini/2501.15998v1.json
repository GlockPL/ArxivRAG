{"title": "Controllable Forgetting Mechanism for Few-Shot Class-Incremental Learning", "authors": ["Kirill Paramonov", "Mete Ozay", "Eunju Yang", "Jijoong Moon", "Umberto Michieli"], "abstract": "Class-incremental learning in the context of limited personal labeled samples (few-shot) is critical for numerous real-world applications, such as smart home devices. A key challenge in these scenarios is balancing the trade-off between adapting to new, personalized classes and maintaining the performance of the model on the original, base classes. Fine-tuning the model on novel classes often leads to the phenomenon of catastrophic forgetting, where the accuracy of base classes declines unpredictably and significantly. In this paper, we propose a simple yet effective mechanism to address this challenge by controlling the trade-off between novel and base class accuracy. We specifically target the ultra-low-shot scenario, where only a single example is available per novel class. Our approach introduces a Novel Class Detection (NCD) rule, which adjusts the degree of forgetting a priori while simultaneously enhancing performance on novel classes. We demonstrate the versatility of our solution by applying it to state-of-the-art Few-Shot Class-Incremental Learning (FSCIL) methods, showing consistent improvements across different settings. To better quantify the trade-off between novel and base class performance, we introduce new metrics: NCR@2FOR and NCR@5FOR. Our approach achieves up to a 30% improvement in novel class accuracy on the CIFAR100 dataset (1-shot, 1 novel class) while maintaining a controlled base class forgetting rate of 2%.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, deep learning models have become integral to many mobile devices and home appliances for computer vision tasks [1], [2]. For instance, a food recognition application on a mobile device can be pre-trained on a fixed set of dishes (e.g., Western cuisine) and deployed on the device [3], [4]. Such an application can accurately recognize a dish if it belongs to one of the pre-trained classes. However, when a user requests recognition of an unseen dish (e.g., an Asian dish that was not part of the pre-training), the model will likely fail. To address this, it is essential to incorporate continual learning capabilities, allowing the system to learn from a few user-provided images of the novel dish and recognize future instances of that dish [3], [5]\u2013[7].\nThis setup, known as Few-Shot Class-Incremental Learning (FS-CIL) [8]\u2013[11], involves two key stages: a base training session and several incremental training sessions (see Fig 1). In the base session, the model is trained on a large number of samples from base classes (e.g., Western dishes). Once deployed, the model encounters a few annotated examples from novel classes (e.g., Asian dishes) and must adapt to improve its accuracy on these new classes while still retaining knowledge of the base classes.\nMost FSCIL solutions consider incremental sessions that introduce 5-10 novel classes, each with 5 labeled samples (or shots) [12]\u2013[15]. However, this setting is often unrealistic in real-world applications,"}, {"title": "II. METHODOLOGY", "content": "In this section, we outline our setup (see Fig. 1) and introduce our proposed Novel Class Detection (NCD) method for OSCIL with controllable forgetting (see Fig. 2).\nFSCIL typically begins with an initial backbone model Minit, which is often pre-trained on larger datasets (e.g., ImageNet-1k) using either a cross-entropy loss (e.g., ResNet) or a self-supervised contrastive loss (e.g., DINO Transformer).\nNext, the model undergoes a base training phase on domain-specific data, focusing on a fixed set of base classes with abundant samples (> 100 per class). The output of this phase is the domain-specific model MBT, trained on the base classes. We denote the base class train split as $X^{(0)}_{train}$, test split as $X^{(0)}_{test}$, and the number of base classes as $N_0$.\nOnce trained, MBT is deployed on a personal device, where it encounters a few annotated samples from previously unseen novel classes (e.g., Asian food dishes). This leads to continual stream of incremental training sessions, where the model adapts to novel class data and produces MIT. For an incremental training session $s > 0$, we denote annotated (or support) samples from novel classes as $X^{(s)}_{support}$, test (or query) samples from novel classes as $X^{(s)}_{query}$, the number of novel classes as $N_s$, and the number of support samples (or shots) per class as $K$. Without loss of generality, we focus on a case with a single incremental session $s = 1$, since we can combine all samples we've seen so far into a single support set $X^{(1)}_{support}$, and perform incremental training session on it. To account for that, we provide results for varying number of classes in the first session $N_1$.\nThe main challenge in FSCIL is to balance between adapting to novel classes and retaining knowledge about the base classes during the incremental training stage. Overly fitting the model to the novel classes leads to forgetting of the base classes. Specifically, we focus on three key metrics:"}, {"title": "A. Few-Shot Class-Incremental Recognition", "content": "\nBase class recognition: accuracy on the test split of base classes after the base training session.\n$BCR := ACC(\u041c\u0432\u0442; \u0425^{(0)}_{test}).$\nNovel class recognition: accuracy on the novel class query samples after incremental training.\n$NCR := ACC(MIT; X^{(1)}_{query}).$\nBase class forgetting rate: decline in base class accuracy due to learning new classes.\n$FOR := BCR \u2013 ACC(MIT; X^{(0)}_{test}).$\nIn this paper, we target the challenging one-shot setting, where each novel class has only a single annotated sample-a scenario closer to real-world applications but less explored in literature that currently focuses on 5 or 10 shots."}, {"title": "B. Base Training Session", "content": "Our method is focused on the inference stage (see Fig. 2) and is agnostic to the choice of base training procedure. To evaluate effectiveness of our inference method, we apply it on top of two base training procedures: the popular ProtoNet training [16] and the state-of-the-art SAVC [13], OrCo [17] and FACT [14]. Both methods, as well as most current FSCIL methods, rely on the notion of prototypes, which denotes a centroid of the class-wise feature vectors. For a given class c, the prototype is defined as\n$proto_c := Avg(M(x_i)),$\nwhere M is a backbone model, and $x_i \\in X_{train}$ is the i-th annotated sample of base class c.\nProtoNet employs prototypical loss which is more effective and robust for few-shot learning applications. SAVC and FACT use contrastive learning and augmented base classes in the base training session to effectively partition the feature space. OrCo promotes orthogonality between features.\nAfter the base training session, we store the set of base class prototypes (denoted as Bp) in memory and use it during inference."}, {"title": "C. Incremental Training Session", "content": "Following prior FSCIL approaches [13]\u2013[15], we freeze the back-bone during incremental training to prevent uncontrollable forgetting. During this phase, we compute and store the prototypes from Eq. 4 for novel classes (N\u2082) from their support samples in $X^{(1)}_{support}$."}, {"title": "D. Decision Rule for Inference Stage", "content": "During inference in standard (vanilla) FSCIL methods, a query sample $x^q$ is assigned to the class whose prototype is closest in feature space [24]:\n$C_{pred,van} = argmin_c (dist(f^q, proto_c)),$"}, {"title": "E. Controllable Forgetting", "content": "Denoting MIT,ned the model with our NCD rule and distance threshold a, note that we can calculate the accuracy of the model on base class samples without knowledge of novel class samples, since the NCD rule does not take the personal samples into account.\nTherefore, we can calculate the accuracy ACC(MIT.ned; X(0) a-priori before deploying the model on device. So, we can control the forgetting rate FOR from Eq. (3) by setting appropriate distance threshold a based on the pre-defined forgetting budget for the base classes. We call this feature of our method controllable forgetting, in which the base class recognition accuracy will always be within the forgetting budget, regardless of the encountered novel samples. This feature is crucial for on-device applications, where maintaining a predictable level of base class accuracy is essential for ensuring the quality of service."}, {"title": "III. EXPERIMENTS", "content": "Evaluation datasets. We choose CUB200 [29], a common FS-CIL fine-grained dataset, CIFAR100 [30] and CORe50 [31], picked specifically to evaluate DINOv2 model on a dataset not seen during self-supervised training\u00b9. In each dataset, we fix No base classes for base training and use the remaining classes as novel classes during incremental sessions. IN CUB200 we set No = 100, in CIFAR100 No = 50, and in CORe50 No = 40. We conduct 25 evaluation episodes, with each episode involving random subsampling of N1 novel classes, followed by selecting one support sample per novel class. We then use the chosen novel classes and support samples in few-shot evaluation.\nWe report results for two ultra-low-data scenarios: N\u2081 = 1 (one novel class) and N1 = 5 (five novel classes), focusing on the challenging one-shot setting (K = 1), where only one support sample is provided for each novel class."}, {"title": "A. Backbone Models and Datasets", "content": "Backbone Models. We evaluate the effectiveness of our Novel Class Detection (NCD) rule using three different backbone architectures with varying complexity to account for different resource con-straints during deployment: MobileNetV2 [25], ResNet18 [26], and DINOv2s [27]. During the base training session, we initialize these models from pre-trained checkpoints. MobileNetV2 and ResNet18 are pre-trained on the ImageNet-1k [28], while DINOv2 is pre-trained on a collection of multiple datasets. For base training, we apply either (i) ProtoNet loss [16], yielding MobileNetv2-PN, ResNet18-PN, and DINOv2s-PN pre-trained models, or (ii) state-of-the-art methods (SAVC, OrCo, and FACT). We select the checkpoint with the best validation accuracy on the base classes after fine-tuning with a slow learning rate. We also include a non-adapted DINOv2s model, with checkpoint taken from initial contrastive learning pretraining on large vision dataset. The corresponding backbone is denoted as DINOv2s-init."}, {"title": "B. Evaluation Metrics", "content": "Our evaluation metrics are BCR, NCR and FOR from Eqs. (1), (2), (3). We compare accuracy of our NCR (Eq. 7) to the baseline vanilla inference method (Eq. 5).\nFor base class recognition accuracy, we include BCR scores using simple nearest centroid method for base classes. The BCR metric is the same for both inference methods.\nFor vanilla inference method, we include NCR metric (denoted by V-NCR in the table). We don't include FOR metric, since it is neg-ligible yet uncontrolled for the frozen backbone during incremental training stage.\nFor inference with our NCD, we can select the distance threshold a depending on the bearable forgetting budget for the target appli-cation. For example, for a = 0, all incoming samples are detected as novel, resulting in high NCR, but complete forgetting of the base classes (FOR = BCR). On the other hand, big a level would result in 0% FOR but also 0% NCR.\nTo mimic a practical application where we are willing to trade some BCR for increased NCR, we choose two levels of forgetting budget: FOR = 2% and FOR = 5%. We find a values corresponding to those two levels of forgetting, and report two NCR metrics, denoted in the table as NCR@2FOR and NCR@5FOR, respectively."}, {"title": "C. Main Results and Discussion", "content": "Table I shows NCR comparison between vanilla inference method (V-NCR) and inference based on our NCD rule (NCR@2FOR and NCR@5FOR). The effectiveness of NCD rule shows some insight into organization and evolution of the feature space for different base training methods.\nProtoNet supervised base training. As we see from the table, NCR accuracy improves greatly when applied on top of simple ProtoNet base training. For MobileNetv2-PN and ResNet18-PN back-bones with one novel class on CUB200 dataset, NCR is improved by 9.3-10.8% for the price of 2% FOR, and by 27-31.3% for the price of 5% FOR. With five novel classes on CUB200 dataset with the same backbones, NCR is improved by 10-16.1% for the price of 5% FOR. Similar gains are also achieved by RssNet18-PN on CIFAR100 dataset.\nIntuitively, ProtoNet training on base classes with slow learning rate gradually deforms the feature space to cluster the base class samples together. In the process, the feature space corresponding to novel classes becomes more deformed, so the one-shot prototypes from those classes are more separated from the actual novel class centroid."}, {"title": "D. Ablations", "content": "We measure the effectiveness of our NCD rule for varying number of novel classes N\u2081 and number of shots K. As we see from Fig. 3, we can improve NCR accuracy considerably for up to 50 novel classes for one-shot recognition.\nHowever, NCD with 5% forgetting performs same or even worse than vanilla inference when we increase the number of shots. In other words, our method targets ultra-low shot regimes. Vanilla inference mode yields better results when 3 or more shots are available for novel classes.\nFinally, we remark that in a practical application: (i) the implemen-tation of the final solution could switch between NCD and vanilla inference modes, depending on the number of samples collected for the novel class; and (ii) the controllable forgetting rate in NCD inference can also be adjusted on device depending on the forgetting strategy."}, {"title": "IV. CONCLUSION", "content": "In this paper, we explored a novel approach to one-shot class-incremental learning based on novel class detection-based decision rule during inference. Our method can be applied on top of existing training methods for few-shot recognition, and can give quality-of-service guarantees when applied to on-device personalized applica-tions thanks to its controllable forgetting property.\nWe evaluated our method against the standard inference method and showed its effectiveness across various backbones and datasets on one-shot recognition task. Overall, we presented a robust and accurate method for one-shot continual class-incremental learning that can be seamlessly combined with any existing pre-training method."}]}