{"title": "Is your LLM trapped in a Mental Set?", "authors": ["Saiful Haq", "Niyati Chhaya", "Piyush Pandey", "Pushpak Bhattacharya"], "abstract": "In this paper, we present an investigative study on how Mental Sets influence the reasoning capabilities of LLMs. LLMs have excelled in diverse natural language processing (NLP) tasks, driven by advancements in parameter-efficient fine-tuning (PEFT) and emergent capabilities like in-context learning (ICL). For complex reasoning tasks, selecting the right model for PEFT or ICL is critical, often relying on scores on benchmarks such as MMLU, MATH, and GSM8K. However, current evaluation methods, based on metrics like F1 Score or reasoning chain assessments by larger models, overlook a key dimension: adaptability to unfamiliar situations and overcoming entrenched thinking patterns. In cognitive psychology, Mental Set refers to the tendency to persist with previously successful strategies, even when they become inefficient a challenge for problem solving and reasoning. We compare the performance of LLM models like Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct and GPT-40 in the presence of mental sets. To the best of our knowledge, this is the first study to integrate cognitive psychology concepts into the evaluation of LLMs for complex reasoning tasks, providing deeper insights into their adaptability and problem-solving efficacy.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in transformer architectures (Vaswani, 2017) have profoundly reshaped NLP and vision-language tasks. Models such as LLaMA-3 (Dubey et al., 2024), Phi-4 (Abdin et al., 2024), Mixtral (Jiang et al., 2024), Deepseek-v3 (Liu et al., 2024), InternVL-2.5 (Chen et al., 2024), and Pixtral (Agrawal et al., 2024) have set new benchmarks in reasoning and generalization capabilities. These open-source large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on challenging benchmarks like MMLU (Hendrycks et al., 2020), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021), solidifying their role in solving reasoning-intensive tasks.\nOne of the most significant developments driving these advancements is in-context learning (ICL) (Dong et al., 2022). Unlike traditional training paradigms that require fine-tuning on task-specific data, ICL enables models to adapt to new tasks through examples embedded directly in the input prompts. This paradigm not only enhances task adaptability but also reduces computational overhead. The flexibility and scalability of ICL have propelled its adoption in solving problems across diverse domains, from mathematical reasoning to commonsense understanding.\nDespite their success, traditional evaluation metrics such as accuracy, F1 score, and reasoning chain assessments primarily capture task performance in well-defined or familiar scenarios. These benchmarks, however, fail to account for an essential aspect of reasoning: adaptability in the face of novelty and the ability to overcome entrenched problem-solving patterns. This dimension aligns with the concept of a Mental Set (\u00d6llinger et al., 2008) in cognitive psychology\u2014a phenomenon where reliance on previously successful strategies inhibits the adoption of more efficient solutions in novel situations. Mental set, often described as cognitive rigidity, emerges when repeated success with specific approaches reinforces their use, even in cases where such approaches are suboptimal or ineffective. While this tendency streamlines problem-solving in routine contexts, it presents a critical bottleneck when models encounter unfamiliar challenges that demand innovative solutions.\nIn contrast to mental set, Insight represents the ability to overcome these entrenched patterns through a reorganization of mental representations, enabling novel solutions to emerge. Insightful problem-solving is characterized by sudden breakthroughs, often accompanied by the well-"}, {"title": "2 Related work", "content": "Mental set (\u00d6llinger et al., 2008) occurs when individuals continue to use a familiar strategy, even when it is no longer the most efficient approach. This is evident in problem-solving contexts like the Water-Jug problem (Luchins and Luchins, 1959), where participants persist in applying a complex method despite the availability of a simpler solution. Such entrenched thinking patterns can limit problem-solving effectiveness. For example, students might repeatedly use the \"add-subtract\" method in solving problems like 5 + 2 + 3 = ? + 3, even when the \"grouping\" method would be faster and less cognitively demanding. This rigidity in thinking is a clear example of mental set interfering with problem-solving flexibility. On the other hand, procedural flexibility (DeCaro, 2016) refers to the ability to shift between various strategies or methods when solving problems, particularly in mathematics, where students often learn multiple approaches to tackle different types of problems. This flexibility enables them to select the most efficient method depending on the problem's context. Research has shown that procedural flexibility is linked to a deeper conceptual understanding. For instance, when students are exposed to different problem-solving strategies, they become better equipped to adapt their approach to novel or unfamiliar challenges. Educational psychology studies suggest that students with greater procedural flexibility tend to experience reduced cognitive load and improved accuracy in their problem-solving. For example, in mathematics, students may be taught multiple strategies, such as the \"add-subtract\" method and the \"grouping\" method. The latter, which simplifies cognitive demands, may be more efficient in some cases. Flexible students are more likely to switch between methods depending on the problem, enhancing their understanding of key concepts like mathematical equivalence. While procedural flexibility is a valuable skill, mental set can hinder its development."}, {"title": "3 Dataset", "content": "The dataset used in this study is based on the problems outlined in (DeCaro, 2016), as summarized in Table 3. It consists of two types of mathematical equivalence problems: complex problems and shortcut problems. Each type includes six problems.\n\u2022 Complex problems require multi-step strategies to solve. These problems involve various addition operations where multiple numbers must be added before solving for the missing value.\n\u2022 Shortcut problems are designed for quicker resolution by leveraging repeated addends on both sides of the equation, enabling simpler and more efficient solutions.\nThe numbers used in both problem sets are consistent to ensure that any differences in problem-solving difficulty arise solely from the required strategy, rather than the numerical values themselves. The dataset is structured to evaluate how the order of problem presentation influences participants' (or LLMs') use of efficient problem-solving strategies."}, {"title": "4 Experiment Setup", "content": "The experimental setup is designed to evaluate the ability of large language models (LLMs) to solve mathematical equivalence problems, with a focus on the efficiency of strategy use (complex vs. shortcut strategies) based on the order of problem presentation. The experiment is adapted from the methodology proposed by (DeCaro, 2016) and consists of two conditions:\n\u2022 Complex-first condition: LLMs solve six complex problems first, followed by six shortcut problems.\n\u2022 Shortcut-first condition: LLMs solve six shortcut problems first, followed by six complex problems.\nEach LLM is presented with the same set of problems, encompassing both complex and shortcut problems. For each problem, the LLMs are tasked with solving the equation by filling in the missing number, demonstrating their reasoning, and applying the most efficient strategy. The LLMs evaluated in this experiment include: Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct and GPT-40. We employ both few-shot prompting and few-shot prompting with zero-shot chain-of-thought reasoning (Wei et al., 2022).\nThe evaluation metrics used are as follows: Exact Match, which measures whether the model produces the correct final answer and Steps, which tracks the number of steps taken to arrive at the correct solution."}, {"title": "5 Results", "content": "Performance, as measured by EM score, improves for all LLM models when using Few-shot+Zero-shot Chain-of-Thought prompting (FS+CoT), while they perform poorly with standard few-shot prompting (FS) alone. Incorporating in-context examples, whether under the CPF or SPG conditions, boosts EM scores, except for the SP+CPF condition with the Llama-3.1-8b-instruct model using FS prompting. However, these in-context examples do not influence the number of steps needed to reach the final solution. For successful cases, FS prompting generally requires fewer steps than FS+CoT. Notably, under FS+CoT, Llama-3.1-70b-instruct and GPT-40 show better performance on complex problems (CP) compared to shortcut problems (SP) in terms of EM score.\nHumans typically require just one step to solve problems in SP and SP+CPF, and LLMs demonstrate this capability in a limited number of successful cases, as shown in Table 1. However, when using CoT prompting to improve performance, LLMs tend to take more than one step to solve problems, as illustrated in Table 2, reflecting a mental set shift induced by CoT reasoning."}, {"title": "6 Summary, Conclusion and Future Work", "content": "We present an investigative study on how mental sets influence the reasoning capabilities of LLMs. Inspired by cognitive psychology, we set up an experiment to compare the problem-solving abilities of three LLM models-Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, and GPT-40\u2014on a mathematical equivalence dataset. Our findings show that in-context examples, whether under the CPF or SPF conditions, enhance performance but do not affect the number of steps taken to reach the final solution. Interestingly, while Few-shot (FS) prompting typically requires fewer steps, Few-shot+Zero-shot Chain-of-Thought (FS+CoT) prompting leads to more complex reasoning, as indicated by the increase in the number of steps required to solve problems. These results suggest that while humans can solve simpler problems in a single step, LLMs require more than one step to achieve higher accuracy, reflecting presence of mental sets. Future work includes expanding this dataset to test problem solving in VLMs."}]}