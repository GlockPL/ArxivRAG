{"title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented Dialog Systems", "authors": ["Giorgio Robino"], "abstract": "This study introduces Conversation Routines (CR), a structured prompt engineering framework for developing task-oriented dialog systems using Large Language Models (LLMs). While LLMs demonstrate remarkable natural language understanding capabilities, engineering them to reliably execute complex business workflows remains challenging. The proposed CR framework enables the development of Conversation Agentic Systems (CAS) through natural language specifications, embedding task-oriented logic within LLM prompts. This approach provides a systematic methodology for designing and implementing complex conversational workflows while maintaining behavioral consistency.\nWe demonstrate the framework's effectiveness through two proof-of-concept implementations: a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These case studies validate CR's capability to encode sophisticated behavioral patterns and decision logic while preserving natural conversational flexibility.\nResults show that CR enables domain experts to design conversational workflows in natural language while leveraging custom enterprise functionalities (tools) developed by software engineers, creating an efficient division of responsibilities where developers focus on core API implementation and domain experts handle conversation design.\nWhile the framework shows promise in accessibility and adaptability, we identify key challenges including computational overhead, non-deterministic behavior, and domain-specific logic optimization. Future research directions include enhancing system robustness, improving scalability for complex multi-agent interactions, and addressing the identified limitations across diverse business applications.", "sections": [{"title": "Conversational Agentic Systems (CAS)", "content": "In the LangChain blog article What is an AI agent? [1], Harrison Chase states:\nAn AI agent is a system that uses an LLM to decide the control flow of an application.\nThis definition resonates with the proposed approach, where business logic is embedded directly within LLM prompts to guide agent behavior. In this paradigm, the prompt itself acts as the primary mechanism for defining control flow.\nTherefore, in line with Chase's definition, the conversational agent examples presented here are AI agents: they utilize an LLM to determine the application's control flow the train ticket booking or troubleshooting process based on the embedded CR. This approach contrasts sharply with traditional hard-coded dialog managers, offering greater flexibility and adaptability by defining control flow declaratively through human-readable instructions rather than complex programming code."}, {"title": "Combining Chat Completion and Function Calling", "content": "A Conversational Agentic System (CAS) can be defined as an AI agent specialized in chat-based interactions, combining the conversational capabilities of LLMs with structured function calls to execute specific tasks and workflows.\nUnlike basic prompt text-completion, which characterizes foundational large language models, a CAS requires an LLM that has undergone specialized fine-tuning on dialogue data. This chat-specific training, exemplified by OpenAI GPT-3.5 models behind applications like ChatGPT, transforms the base LLM's capabilities into sophisticated dialogue management [2, 3]. A chat-completion model can effectively process the complete sequence of messages exchanged between the user and assistant, along with the system prompt, which is the focus of this study. This dialogue-oriented fine-tuning enables the LLM to maintain conversation coherence, track context across multiple turns, and generate contextually appropriate responses.\nUsing chat-completion models, dialogue applications operate through a dynamic feedback loop where user inputs and system responses are appended to a shared conversation history. This history, encompassing the system prompt and prior message exchanges, serves as the model's short-term memory, enabling in-context learning for coherent and contextually aware responses.\nEffective management of this conversation history is vital due to the token limitations of LLMs. While strategies like truncating older messages or summarizing interactions are commonly used to maintain dialogue relevance, these techniques fall outside the scope of this study.\nThe second foundational prerequisite for enabling LLM-based agents is the use of an LLM pretrained for function calling [4]. The LLM is supplied with a list of available functions (also referred to as tools) that can be implemented in any programming language. This capability to invoke external tools is essential for developing task-oriented applications within specific, closed domains."}, {"title": "Embedding Business Logic in A Conversational Agentic System", "content": "Traditional deterministic dialogue systems, which rely on rigid, rule-based state-machine approaches, often produce inflexible and cumbersome interactions. In contrast, LLM-driven workflows offer greater flexibility and adaptability by embedding business logic directly within the model's context using structured natural language prompts.\nConversational Agentic Systems manage conditional logic, iterative processes, and complex procedural sequences, facilitating the execution of moderately complex business tasks. The study explores how embedding business logic within LLM prompts can enhance functionality and efficiency in closed- domain applications. A key research question guiding this work is:\nHow can natural language instruction prompts be designed to manage procedural logic, conditional flows, and data dependencies, enabling LLMs to execute multi-step workflows while adhering to specific business constraints?"}, {"title": "System Prompts as Conversation Routines", "content": "While CAS excel at natural language interactions and task execution through backend systems, controlling their behavior to enforce business rules remains challenging. This study leverages prompt engineering by embedding procedural logic within Conversation Routines (CR), focusing on system prompts that encode business logic through natural language (low-code/pseudo-code) specifications. This enables LLMs to function as workflow interpreters.\nThe meaning of routine was introduced by OpenAI in the context of SWARM, an experimental open- source framework for conversational agents, as described in the cookbook article Orchestrating Agents: Routines and Handoffs [6][8]. In this article, Ilan Bigio defines a routine as:\nThe notion of a \u201croutine\u201d is not strictly defined, and instead meant to capture the idea of a set of steps. Concretely, let's define a routine to be a list of instructions in natural language (which we'll represent with a system prompt), along with the tools necessary to complete them.[...]\nThe main power of routines is their simplicity and robustness. Notice that these instructions contain conditionals much like a state machine or branching in code. LLMs can actually handle these cases quite robustly for small and medium sized routine, with the added benefit of having \u201csoft\u201d adherence the LLM can naturally steer the conversation without getting stuck in dead-ends.\nThe Conversation Routines approach outlined in this paper aligns with the OpenAI SWARM definition of routines, leveraging structured natural language directives to specify precise agent behaviors."}, {"title": "Structuring Applications with Conversation Routines and Tool Libraries", "content": "Application development using CR is structured into two primary components with distinct ownership and responsibilities. The first component, managed by conversation designers, consists of writing the system prompt. This prompt incorporates the CR technique and includes function declarations that clearly define the required tools with names, descriptions, parameters, and return values.\nThe second component, maintained by software developers, is the Tools Library a collection of custom-implemented functions. Each function is thoroughly documented with appropriate docstrings, providing the actual functionality referenced in the function declarations.\nThis clear separation of responsibilities allows conversation designers to concentrate on crafting effective conversational flows and specifying the required capabilities, while developers focus on implementing and maintaining the underlying functional tools. The function declarations act as the interface, ensuring seamless collaboration between the two domains.\nThe following sections outline the common components of a CR prompt."}, {"title": "Identities and Purpose", "content": "The foundation of any prompt begins with a clear definition of the agent's identity (bot-persona), the user's identity (user-persona) and purpose. This goes beyond simple goal- setting it establishes the context for all subsequent interactions. For example:\nYou are a specialized assistant focused on helping customers book train tickets.\nPrimary objectives:\nGuide customers through the booking process\nEnsure accurate information collection\nMaintain natural, helpful interactions\nHandle problems with patience and clarity"}, {"title": "Functions Integration Protocol", "content": "A key section focuses on defining how the agent should interact with available tools (external functions). By embedding function specifications also into the system prompt, the agent gains an understanding of both technical capabilities and contextual appropriateness. Example:\nCORE FUNCTIONS:\nStation Search: Use 'search_railway_station() to search for departure and arrival stations.\nSupport pagination (next, previous, specific page selection)\nand allow users to refine search queries if needed.\nUsers must explicitly select a station from the available options.\nBooking: Use 'book_train_ticket()' to finalize the booking.\nThis function requires essential parameters like stations, dates,\ntimes, passengers, and class.\nDate/Time Handling: Use 'get_date_time() to ensure valid and future dates.\nIf issues arise, provide context-aware suggestions for alternative dates.\nIn the above example, when using search_railway_station() to find stations, the agent manages paginated results and allows query refinement, integrating responses seamlessly into the conversation.\nThe protocol also emphasizes error management, equipping the agent to recover from common failures"}, {"title": "Workflow Control Patterns", "content": "The foundation of agent behavior lies in structured workflow patterns that guide decision-making and action sequencing. These patterns serve as essential frameworks for organizing complex interactions, ensuring consistent and reliable agent responses across various scenarios. The following sections explore key workflow patterns - sequential steps with conditionals, iteration logic, and user confirmation protocols - that form the building blocks of robust agent behavior."}, {"title": "Sequential Steps including conditionals", "content": "I use systematic indentation patterns to create hierarchical structures for agent behavior specifications. This formatting syntax, similar to Markdown or YAML, enables clear visualization of workflow relationships, with each level of indentation representing a deeper section of behavioral logic. Parent-child relationships between workflow components are expressed through consistent spacing, typically using 2-3 spaces per level, allowing both humans and LLMs to easily parse the control flow and scope of different behavioral instructions.\nTRAIN TICKET:\nStation Search\nGet departure station\n* Validate via search\n* Confirm selection\nGet arrival station\n* Validate via search\n* Confirm selection\nJourney Details\nCollect date/time\nGet passenger count\nSet travel class\nConfirmation\nIF all valid:\nShow summary\nProcess booking\nELSE:\nFlag issues\nReturn to input"}, {"title": "Iteration Logic", "content": "A crucial aspect is its ability to specify iteration patterns within workflows. These patterns enable agents to handle repetitive tasks and multi-step processes while maintaining context and progress. When defining iteration logic, designers can embed both explicit loop structures and conditional repetition patterns using natural language constructs. For instance, in moderate complex workflows like train booking, iteration specifications might include both user-driven and system-driven loops:\nITERATION CONTROL PATTERNS:\nUser-Driven Iteration\nPresent search results in paginated format\nWhile user has not made final selection:\n* Allow navigation between pages\n* Accept selection or refinement request\n* Track current page position\nExit loop only on explicit user confirmation\nSystem-Driven Validation\nFor each required booking detail:\n* Collect and validate input\n* Maximum 3 validation attempts\n* On failure, offer guided correction\nContinue until all fields validated or user requests process restart\nThe power of natural language allows for nuanced iteration control, seamlessly integrating loop management with user interaction patterns. This approach to iteration specification proves particularly valuable in scenarios requiring persistent state management and progressive refinement of user inputs, while maintaining the flexibility to break loops based on user satisfaction or system constraints."}, {"title": "User Confirmation Protocols", "content": "This section establishes critical checkpoints where user confirmation must be obtained before proceeding with the workflow. Rather than relying on assumed understanding,\nwe structure explicit verification points to ensure alignment between user intent and system actions.\nThese verification points can be implemented through natural conversational flows or through deterministic checkpoint functions requiring explicit binary responses. Here's how we might explain it:\nRemember to confirm important details naturally, like checking in with\nI've got [this information] so far is that correct? before moving forward.\nFor critical workflow stages requiring deterministic confirmation:\nTo proceed with the booking, please review these details: [booking details]\nYou must confirm with Yes or No.\nThere are no alter alternative responses at this stage.\nAfter user response always call user_confirmed_booking (confirmation_flag)"}, {"title": "Output Format and Tone", "content": "This section is all about consistency - making sure our agent communicates in a way that's both professional and engaging. This isn't about rigid templates, but rather about maintaining a consistent voice and structure throughout the conversation. For example:\nKeep your responses friendly but focused.\nWhen presenting options, structure them clearly:\nHere's what I found for your journey:\nMorning option: 9:15 AM departure\nAfternoon option: 2:30 PM departure\nWhich time works better for you?"}, {"title": "Conversation Examples", "content": "To further refine agent behavior, interaction examples can be incorporated using one-shot or few-shot prompting techniques. These examples serve as practical demonstrations of desired conversational flows, tones, and responses, helping to anchor the LLM's behavior within specific interaction patterns. By including one or more illustrative dialogues, designers can guide the system toward consistent responses in similar contexts.\nFor optimal effectiveness, examples can be tailored to each specific flow or decision point within the workflow. For instance, an example might demonstrate how the agent should confirm customer details during the data collection phase or how it should handle edge cases during the validation phase. These examples act as reference points, ensuring the LLM adheres to predefined interaction standards while maintaining natural and contextually appropriate communication.\nCONVERSATION EXAMPLE\nUser: I need a train ticket to London\nAgent: I'll help you book a ticket to London. First, I need some details:\n1. What's your departure city?\n2. What date would you like to travel?\n3. Do you prefer morning or evening travel?\nLet's start with your departure city.\nThis structured approach creates a natural flow from understanding the basic purpose through to handling specific interactions, all while maintaining a conversational tone that makes the agent feel more approachable and helpful. Each section builds upon the previous ones, creating a comprehensive but flexible framework for our conversational agent."}, {"title": "Applying Conversation Routines in Real-World Case Studies", "content": "In my previous white paper, \u201cA Conversational Agent with a Single Prompt?"}, {"title": "Virtual Caregiver for Telemedicine", "content": "This case study involved a virtual caregiver designed for remote patient monitoring. This agent conducted daily check-ins, collecting vital health metrics such as temperature and blood oxygenation while maintaining empathetic, natural conversations. The system demonstrated a key strength of prompt-engineered LLMs: the ability to accommodate natural conversational digressions while reliably returning to structured data collection tasks."}, {"title": "Customer Care Assistant", "content": "This case study featured a customer service agent that bridged the gap between free-form user descriptions and structured ticketing systems. This implementation highlighted dual advantages of natural language programming: simplifying development complexity while enabling natural human-like interactions. Users could express their issues conversationally, while the agent systematically gathered the structured data required for backend processing."}, {"title": "Virtual Job Interviewer", "content": "This prototype showcased a virtual recruiter that dynamically interviewed candidates by analyzing their CVs against job descriptions embedded in the prompt. This system demonstrated sophisticated cognitive abilities, probing candidates' strengths, weaknesses, and potential mismatches while maintaining a structured yet adaptive conversational flow. The implementation illustrated how LLMs can perform complex workflows within defined conversational settings.\nDuring the development of these prototypes, I utilized the OpenAI GPT-3 (text-davinci-003) text- completion model, preceding the release of the OpenAI GPT-3.5 chat-completion model.\nThese prototypes demonstrated the LLM's remarkable capacity for fluid dialogue while staying aligned with assigned objectives. The agents exhibited an extraordinary ability to identify the conclusion of conversations when the programmed goal was achieved, along with maintaining an internal state of completed steps and tracking state variables effectively."}, {"title": "Train Ticket Booking System", "content": "This use case demonstrates how embedded business logic, expressed through CR, enables a Conversational AI System (CAS) to guide users through the complexities of purchasing train tickets."}, {"title": "Interactive Troubleshooting Copilot", "content": "In this example, a CAS assists technicians in diagnosing and troubleshooting machinery issues. The agent guides users through structured diagnostic procedures and provides access to relevant information, thereby streamlining factory-floor operations."}, {"title": "Use Case 1: Train Ticket Booking System", "content": "The first use case presents a demonstrative scenario in which a CAS facilitates the purchase of a train ticket. The agent is integrated with backend functions, enabling it to retrieve the current time, query a railway station names database, identify available train connections between specified origin and destination stations, and generate a ticket in a predefined format. A key challenge lies in constructing a prompt that encapsulates the complete business logic necessary for an efficient booking workflow. This process generally involves acquiring and validating user-provided information, followed by invoking a backend function to finalize the booking and payment.\nTraditional implementations of such workflows rely on hard-coded scripts developed using dialogue management frameworks [16, 17, 18, 19]. This approach necessitates anticipating all potential user actions, including digressions, out-of-scope requests, and error handling. As a result, script complexity increases rapidly, leading to substantial development overhead, increased costs, and a rigid, unnatural user experience. In contrast, the approach explored in this article utilizes a CAS guided by a comprehensive prompt encoding business logic via CR. Rather than hard-coding every possible interaction scenario, the LLM autonomously manages user interactions while adhering to the guidelines and rules specified within the instructions.\nThe CR approach redefines the role of the conversation designer, expanding their responsibilities to include not only traditional interaction design specifications but also the encoding of business logic. As prompt designers, they now craft system behavior, tone of voice, user and assistant personas, and best practices for conversational interaction design - all articulated through natural language specifications within the prompt.\nA condensed and illustrative summarized representation of the prompt employed in this use case is provided here below:\nOBJECTIVE:\nAssist users in booking train tickets through clear, user-friendly interactions.\nCORE FUNCTIONS:\nStation Search: Use 'search_railway_station()'\nto search for departure and arrival stations.\nSupport pagination (next, previous, specific page selection)\nand allow users to refine search queries if needed.\nUsers must explicitly select a station from the available options.\nBooking: Use 'book_train_ticket()' to finalize the booking.\nThis function requires essential parameters like stations, dates,\ntimes, passengers, and class.\nDate/Time Handling: Use 'get_date_time() to ensure valid and future dates.\nIf issues arise, provide context-aware suggestions for alternative dates.\nINFORMATION COLLECTION:\nDeparture and Destination: Stations are selected through the 'search_railway_station()'\nfunction.\nIf multiple pages of results exist, navigate using pagination\nand present results clearly to the user.\nDate and Time: Collect the departure and return dates in 'YYYY-MM-DD' format.\nTimes can be flexible inputs like \"early morning\" or exact times like \"08:30\".\nPassenger Details: Collect the number of passengers (positive integer)\nand the travel class (strictly '1st' or '2nd').\nINTERACTION WORKFLOW:\nSearch Phase:\nUse 'search_railway_station() for departure and arrival stations.\nHandle paginated results (e.g., \"Page 1 of 3\")\nand offer options to navigate, refine, or select a station.\nValidation Phase:\nOnce all required information is collected (stations, dates, times, passengers, class),\npresent a full summary of the booking details.\nExplicitly ask the user to confirm with \"YES\" or \"NO\".\nBooking Phase:\nERROR HANDLING:\nIf the user confirms, call 'book_train_ticket() with the gathered details.\nIf the function returns an error, inform the user, suggest retry options,\nor guide them to contact support.\nIf successful, display all the booking receipt details clearly to the user.\nIf the booking fails, provide the user with a clear, polite message."}, {"title": "Allow the user to revise incorrect information at any point during the process", "content": "Allow the user to revise incorrect information at any point during the process.\nIf multiple failures occur, offer the user a chance to restart the process\nor save partial data for later use.\nUSER GUIDANCE AND BEST PRACTICES:\nPrioritize clear, explicit communication.\nProvide simple navigation options (next previous, refine) when browsing station results.\nEnsure flexibility for date/time input (accepts \"morning\", \"08:30\", etc.)\nand strict validation for critical fields (dates, class, and passenger count).\nAsk for explicit confirmation before making any booking to prevent user errors.\nGracefully handle booking errors and always offer corrective options to users.\nThe complete prompt is more detailed and structured; however, for brevity, certain specifics have been omitted, including details of the workflow description and the Python function docstrings (which specify detailed behavior, required variables, and the precise data format for function arguments provided by the LLM).\nAlthough the prompt instruction set is expressed in natural language, it adheres to a structured format to facilitate LLM comprehension. This format explicitly defines the logic and principles governing LLM behavior, such as the step-by-step workflow outlined in the INTERACTION WORKFLOW section, the data validation rules specified in the INFORMATION COLLECTION section, and the error handling procedures detailed in the ERROR HANDLING section.\nThe prompt also incorporates a catalog of available back-end functions (APIs) that the agent is authorized to invoke. These functions, such as search_railway_station(), book_train_ticket(), and get_date_time(), are referenced within the prompt to execute specific tasks.\nThese external functions are typically developed by a separate technical team, establishing a clear demarcation of responsibilities between prompt designers (~no-coders) and function developers (~coders)."}, {"title": "Train Ticket Booking system dialog session excerpt", "content": "The Train Ticket Booking system dialog session excerpt is in Appendix 6.1. This dialog excerpt interaction illustrates the agent's adherence to the prompt's instructions for managing a complex conversation, particularly in handling station searches and confirming travel details.\nInitially, the agent requests essential information for the train search such as the departure station, number of passengers, and travel class demonstrating adherence to a predefined logical flow. When the user indicates they cannot find the desired station, the agent does not halt but instead proposes refining the search by offering additional pages of results, effectively handling the ambiguity of the user's input (\u201ce' una staziuone di Genova ma no nmi ricordo il nome. . . \u201d) and correctly calling the search function with the appropriate page argument value. After the user identifies the station (\u201cNervi\u201d), the agent resumes the flow, confirming the details and requesting the preferred departure time.\nThe agent uses external function search_railway_station() to retrieve information and manage the search for Rome stations, demonstrating the ability to interact with external systems as specified in the prompt. Finally, the agent summarizes all travel details, requesting explicit confirmation before proceeding with the booking using the book_train_ticket() function. This demonstrates the application of the behavior rules defined in the prompt, such as requiring confirmation before taking decisive actions."}, {"title": "A Digression on Best Practices for Tools Design", "content": "Although the primary focus of this paper is on Conversation Routines as a prompt engineering technique for crafting system prompts, a significant portion of the development effort is dedicated to defining effective (e.g., Python) functions to provide backend services (e.g., booking services). It is important to highlight that these backend functions are integral to the problem domain and would be required regardless of whether the workflow was implemented using a deterministic, hard-coded software tool.\nA critical consideration is the design of functions that can be seamlessly invoked by an LLM. A crucial aspect of this design is the management of function state persistence. Ideally, functions should be stateless, adhering to the principles of functional programming, with the LLM maintaining state by providing the appropriate arguments for each function invocation.\nConversely, when implementing functions with stateful logic, additional precautions are necessary. In such cases, the state must be tracked and made accessible to the LLM. This can be achieved through shared memory mechanisms or by embedding the relevant state information within the LLM's prompt context, ensuring continuity across multiple function invocations.\nTo illustrate this point, challenges were encountered when implementing a function to search for railway stations within a dataset of thousands of entries, enabling users to navigate the results using pagination and standard navigation commands (e.g., next page, previous page, first page, last page). Initially, the implementation employed a Python class to manage the state of the instantiated class, necessitating the maintenance and explicit sharing of a transaction ID, thereby complicating the LLM-function interface.\nA more straightforward solution, however, involved designing a stateless function where the page number is passed as an argument. With this approach, the LLM maintains the current page number internally and provides the appropriate page number based on user navigation requests expressed in natural language with implicit references (e.g., \"I don't see the station I'm looking for\", when on page 1 of 10).\nIn this scenario, the LLM correctly requested page 2 via the function call:\nsearch_railway_station(query: \"...\", page=2)\nThis demonstrates the LLM's ability to handle state transitions without requiring explict state management, such as storing the page variable in the LLM's context window. The function's docstring is provided below:\ndef search_railway_station(query: str, page: int = 1) -> str\nPerform a search for railway station names based on a query and display results in a\npaginated format.\nArgs:\nquery (str): The search query, consisting of one or more space-separated words.\npage (int, optional): The current page number to display (1-based indexing). Defaults to\nReturns:\n1.\nstr: A formatted string displaying the search results for the specified page, including\npagination details.\nBehavior:\nMatches are case-insensitive.\nIf no matches are found, a message will indicate zero results.\nAutomatically handles pagination and displays the total number of pages and results.\nExample:\n results = search_railway_station (\"Genova\", page=1)\n print(results)\nFound 2 total results (Page 1 of 1):\nGenova Principe\nGenova Brignole"}, {"title": "Use Case 2: Interactive Troubleshooting Copilot", "content": "As a proof of concept demonstrating the capability of a LLM to facilitate real-time, task-oriented dialogues, the Interactive Troubleshooting Copilot was developed as a more complex application. This conversational assistant is designed to support technicians in troubleshooting and repairing industrial systems through continuous interactive guidance while adhering to predefined business logic embedded within the system prompt [12].\nThe proposed system is particularly beneficial for technicians with limited troubleshooting expertise and unfamiliarity with the equipment involved. It provides a conversational interface through which the user can describe equipment status observations and detail the actions performed during the troubleshooting process. Furthermore, the assistant can address ancillary inquiries regarding equipment specifications and details while maintaining focus on the repair task, guiding the operator back to the appropriate step in the procedure as necessary.\nThe assistant interacts with the user via voice and provides step-by-step guidance through the troubleshooting procedure. The process commences with an initial issue description and proceeds by retrieving relevant troubleshooting documentation, employing a Retrieval-Augmented Generation (RAG) approach. This methodology enables the assistant to offer continuous, contextually relevant guidance throughout the repair process, ensuring operator adherence to the documented procedure with precision."}, {"title": "The Troubleshooting Procedure Manual as Workflow Guideline", "content": "A fundamental challenge in leveraging a LLM for task-oriented dialogues is conveying the specific business logic or processes required to complete a task, such as a repair procedure. The central concept is to utilize a human-readable document outlining the procedure, ideally one already available to technicians.\nFor this study, a Conveyor Belt Troubleshooting Procedure Manual was developed a comprehensive document designed to guide technicians through the diagnosis and repair of industrial conveyor belt systems. This document provides detailed, step-by-step instructions, each associated with specific component codes (e.g., QDE-9900-PRO, RLT-8450-V2), ensuring accurate identification of the components involved.\nThe procedure incorporates conditional logic and decision points, where the subsequent step is contingent upon the technician's observations (e.g., detecting misalignment, damaged parts, or sensor malfunctions). Additionally, the procedure includes iterative loops, requiring the technician to revisit previous steps if anomalies persist following an initial resolution attempt. This structured approach ensures a systematic and repeatable troubleshooting process.\nA condensed and summarized representation of the original reference document is presented solely to illustrate the type of information employed in this study (see Appendix 6.2)."}, {"title": "Embedding Troubleshooting Manual Business Logic in the Prompt", "content": "The prompt developed for the use case is designed to enable a LLM to effectively assist technicians in diagnosing and repairing conveyor systems. It defines the assistant's role as an expert, with core functions including retrieving troubleshooting steps, providing component details, and generating reports.\nThe system maintains an internal state, tracking the current step, historical progress, safety status, and component information to ensure both procedural continuity and adherence to safety protocols.\nThe troubleshooting process is structured into four distinct stages: The Initial Assessment stage commences by retrieving the relevant instructions, employing a Retrieval-Augmented Generation (RAG) approach, and confirming the necessary safety protocols. During the Step-by-Step Execution phase, the assistant guides the technician through each sequential repair step, ensuring verification of and adherence to all safety measures. If additional information, such as component specifications, is required, the Part Inquiries stage is invoked, retrieving the relevant details before resuming the repair procedure. Finally, the Completion stage ensures proper execution of all steps and generates a report summarizing the troubleshooting process."}, {"title": "INDUSTRIAL MACHINERY TROUBLESHOOTING ASSISTANT", "content": "ROLE AND CONTEXT:\nYou are an expert industrial machinery repair assistant specializing in conveyor troubleshooting\nprocedures.\nYour primary goal is to ensure safe and effective repairs while guiding users through documented\nprocedures.\nCORE FUNCTIONS:\nretrieve_troubleshooting_instructions (query) -> Returns procedure details\nretrieve_part_details(device_code) -> Returns part specifications\nhandoff_report() -> Transfer control to reporting system\nCONVERSATION FLOW:\nINITIAL ASSESSMENT:\nAcknowledge user's reported issue\nCall retrieve_troubleshooting_instructions (issue)\nAnalyze the returned step-by-step instructions\nBased on the user issue and the available sep instructions, decide the correct step where\nto start\nPresent procedure ID and title\nShare a one-sentence summary of the possible diagnosis hypotesis\nRecap in one-sentence the step-by-step procedure\nSTEP-BY-STEP TROUBLESHOOTING PROCEDURE EXECUTION:\nFor each interaction:\nPresent current step instructions\nRequest specific confirmations\nOnly advance when explicit confirmation received\nHandle interruptions (part queries, clarifications) without losing state\nPART INQUIRIES HANDLING:\nWhen user requests part information:\nPause current procedure state\nCall retrieve_part_details()\nPresent relevant information\nResume previous procedure state\nCOMPLETION PROCEDURE:\nVerify all steps completed\nTransfer to Troubleshooting Report Agent\nBEHAVIOR RULES:\nRetrieve repair instructions by calling retrieve_troubleshooting_instructions (query)\nSummarize the troubleshooting procedure with its ID and title\nStart from the most relevant step based on the user's reported issue\nPresent instructions one step at a time\nKeep step IDs internal unless explicitly requested\nInclude safety warnings before any potentially hazardous steps\nTrack conditional logic internally without exposing it to users\nWait for explicit user confirmation before advancing to next step\nStay within the current repair sequence until completion\nHandle part inquiries by calling retrieve_part_details (device_code), then resume current step\nCall the transfer function to the Troubleshooting Report Agent upon completion\nSAFETY PROTOCOLS:\nMandatory safety warnings before hazardous steps\nRequired confirmations for critical operations\nExplicit voltage/power state verification\nNever skip safety prerequisites\nRESPONSE STRUCTURE:\n [Safety Warning if applicable]\n [Current Step Instructions]\n [Required Confirmation Items]\nLANGUAGE AND FORMATTING:\nCommunicate in Italian\nUse second person form for instructions\nMaintain plain text format without markdown\nKeep responses concise\nERROR HANDLING:\nIf user response unclear: Request specific clarification\nIf procedure interrupted: Save state and handle inquiry\nIf safety condition unmet: Block procedure until resolved"}, {"title": "In this kind of application, we encounter a dual layer of complexity", "content": "In this kind of application, we encounter a dual layer of complexity: First, integrating the business logic embedded in the step-by-step troubleshooting procedure. Second, guiding the CAS to adhere to an interaction workflow.\nThis approach requires the LLM to manage two distinct levels of internal state: a lower-level state that tracks the detailed progression of the troubleshooting steps, and a higher-level state that orchestrates the interaction workflow. The higher-level workflow encompasses tasks such as retrieving document data, evaluating the diagnosis, executing troubleshooting steps, and ultimately generating a repair report.\nThe prompt acts as a comprehensive instruction set, dictating the LLM's behavior during user interactions. This is exemplified in the prompt's structure, which defines distinct sections for role and context, core functions, conversation flow, behavior rules, and safety protocols.\nThe CONVERSATION FLOW section, for instance, outlines the high-level interaction stages, specifying the actions the LLM should take at each step. In the INITIAL ASSESSMENT stage, the prompt instructs the LLM to retrieve troubleshooting document, analyze the repair steps, present a diagnosis hypothesis, and recap the procedure. This directly embeds the initial steps of the troubleshooting workflow into the LLM's behavior.\nCrucially, the prompt also embeds rules for managing the step-by-step interaction, a key aspect of achieving autonomy. The BEHAVIOR RULES section dictates that the LLM must present instructions one step at a time, wait for explicit user confirmation before advancing, and handle interruptions (like part inquiries) without losing state.\nThis ensures that the LLM autonomously manages the conversation flow, adhering to the pre-defined workflow and business logic contained in the manual and embedded within the prompt itself. For example, when a user asks for part details during a procedure, the prompt specifies that the LLM should pause the current procedure, call the retrieve_part_details() function, and then resume the previous step, thus maintaining procedural continuity.\nThe application shows how the prompt enables the LLM to navigate troubleshooting workflows based on user input while maintaining consistent interaction patterns. For a full conversation session excerpt, refer to Appendix 6.3."}, {"title": "Interactive Troubleshooting Copilot Dialog Excerpt", "content": "Interactive Troubleshooting Copilot Dialog Excerpt"}, {"title": "Summary", "content": "This article presented a novel approach to developing Conversational Agentic Systems (CAS) by embedding business logic directly within LLM prompts using Conversation Routines (CR). The methodology introduced empowers domain experts", "methodology": "a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These examples illustrated the LLM's ability to autonomously manage complex conversations, handle ambiguous user input, maintain state across multiple interactions, and adhere to predefined business logic. By embedding the workflow directly within the prompt using CR, I demonstrated how LLMs can be guided to follow the intended procedural flow while interacting with backend systems and users in a context-aware manner. This low-code approach significantly reduces development overhead, enabling rapid prototyping and iteration on conversational flows.\nNotable limitations exist in CAS implementation. Primary concerns include computational costs that scale with workflow complexity, context window size, and function call frequency. These factors can significantly impact both LLM usage costs and response latency.\nThe non-deterministic nature of LLMs introduces risks of confabulations or deviations from prompt instructions during unexpected user interactions. While guardrail techniques can mitigate these risks, complete elimination remains challenging. The primary trade-off of utilizing an LLM-driven application lies in the significant flexibility it offers for managing unforeseen user interactions. However, this flexibility often comes at the cost of increased latencies and the potential for unpredictable behavior. In contrast, hard-coded deterministic solutions likely offer shorter latencies and fixed flow paths.\nThus, the trade-off is inherently tied to the complexity of the business logic. The"}]}