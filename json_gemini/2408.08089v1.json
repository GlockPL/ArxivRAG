{"title": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents", "authors": ["Guhong Chen", "Liyang Fan", "Zihan Gong", "Nan Xie", "Zixuan Li", "Ziqiang Liu", "Chengming Li", "Qiang Qu", "Shiwen Ni", "Min Yang"], "abstract": "In this paper, we present a simulation system called AgentCourt that simulates the entire courtroom process. The judge, plaintiff's lawyer, defense lawyer, and other participants are autonomous agents driven by large language models (LLMs). Our core goal is to enable lawyer agents to learn how to argue a case, as well as improving their overall legal skills, through courtroom process simulation. To achieve this goal, we propose an adversarial evolutionary approach for the lawyer-agent. Since AgentCourt can simulate the occurrence and development of court hearings based on a knowledge base and LLM, the lawyer agents can continuously learn and accumulate experience from real court cases. The simulation experiments show that after two lawyer-agents have engaged in a thousand adversarial legal cases in AgentCourt (which can take a decade for real-world lawyers), compared to their pre-evolutionary state, the evolved lawyer agents exhibit consistent improvement in their ability to handle legal tasks. To enhance the credibility of our experimental results, we enlisted a panel of professional lawyers to evaluate our simulations. The evaluation indicates that the evolved lawyer agents exhibit notable advancements in responsiveness, as well as expertise and logical rigor. This work paves the way for advancing LLM-driven agent technology in legal scenarios. Code is available at https://github.com/relic-yuexi/AgentCourt.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) technologies, particularly large language models (LLMs), are rapidly transforming the traditional legal industry. From automated text generation to interactive legal consulting, AI applications in the legal domain are becoming increasingly widespread (Lai et al. 2023). However, despite LLMs' vast potential, significant challenges remain in handling complex legal queries and simulating real court environments. Existing legal AI systems are often confined to specific tasks and struggle to comprehensively simulate the legal reasoning process and multi-party interactions (Hamilton 2023).\nTo address these limitations, we present AgentCourt, an innovative LLM-based system designed for the simulation of civil courts. AgentCourt stands out by involving multiple roles, including judges, attorneys, plaintiffs, and defendants, thus providing a more authentic and comprehensive legal scenario simulation. This system excels not only in handling standard legal queries but also in analyzing complex real-world cases, thereby bridging a significant gap in current legal AI research. Figure 1 showcases the AgentCourt environment, with the left panel depicting a simulated courtroom and the right panel showing the courtroom dialogue.\nAt the core of AgentCourt is a meticulously designed multi-agent system, with each agent representing a role in the courtroom, all driven by specially trained LLMs. Drawing inspiration from the success of Agent Hospital (Li et al. 2024) in the medical field, we have built a simulated environment for civil courts, allowing AI agents to strengthen their legal reasoning skills through continuous practice. Significantly, we achieve notable enhancements in LLMs' cognitive agility, professional expertise, and logical coherence via adversarial engagements among lawyer agents.\nOur approach leverages an adversarial evolutionary strategy that is free from preset parameters and independent of manually annotated datasets. This strategy dramatically reduces annotation costs while enhancing the model's ability to generalize across diverse and complicated court scenarios, enabling AI to autonomously devise potent defensive tactics. Experimental results show that an evolved legal agent has demonstrated significant improvement in multiple legal tasks, indicating the potential for ordinary models to surpass GPT-4 after evolutionary refinement in complex legal tasks. Human evaluation, conducted by a panel of legal experts using real-world criteria to assess lawyer performance, further substantiates the effectiveness of the evolution process through three key metrics: cognitive agility, domain-specific knowledge, and logical rigor."}, {"title": "Related Work", "content": ""}, {"title": "Large Language Models in the Legal Domain", "content": "AI applications in the legal domain have made significant strides, particularly with the advent of large language models (LLMs). These models have shown potential in various legal tasks, including case prediction, legal research, and document analysis (Lai et al. 2023; Hamilton 2023). For instance, DISC-LawLLM demonstrated the potential of fine-tuned LLMs in providing intelligent legal services (Yue et al. 2023), while the PLJP framework combines LLMs and domain models to enhance the accuracy of case judgment predictions (Wu et al. 2023). DeliLaw exemplifies the utility of LLMs in legal advice, effectively handling legal inquiries through its dialog-based system (Xie et al. 2024). Despite these advancements, current AI legal systems still face limi-"}, {"title": "Large Language Models for Real World Simulation", "content": "LLM-based multi-agent systems represent a new direction in Al research, leveraging collaborative agents to address complex problems. These systems excel in utilizing cognitive synergy and knowledge sharing, thereby enhancing overall decision quality and interaction capabilities (Talebirad and Nadiri 2023; H\u00e4ndler 2023). LLM-based multi-agent systems have shown potential in multiple domains. In natural language processing, they have improved language understanding and generation tasks (Tan and Motani 2024). In robotics, these systems have enhanced robots' understanding and decision-making capabilities in human-robot interaction (Kim, Lee, and Mutlu 2024). In task planning and execution, multi-agent systems can decompose and collaboratively complete complex tasks (Yang et al. 2024). Additionally, in education, they have created personalized learning experiences and intelligent tutoring systems (Yin et al. 2024). The financial sector has also benefited from these systems, applying them to market analysis, risk assessment, and investment decisions (Nascimento, Alencar, and Lucena 2023). A notable application of LLM-based multi-agent systems is Agent Hospital, which simulates a hospital environment with LLM-driven autonomous agents representing patients, nurses, and doctors (Li et al. 2024). Key features of Agent Hospital include comprehensive simulation of disease treatment processes, autonomous learning without manual data annotation, knowledge transfer from simulation to real-world medical benchmarks, efficient learning through accelerated simulation experience, and state-of-the-art performance on medical datasets. Agent Hospital demonstrates the potential of multi-agent systems in complex and specialized domains, paving the way for new approaches in professional training and decision support.\nOur AgentCourt builds on these advancements to address the limitations of current legal AI systems and extends the multi-agent approach to the legal domain. By simulating a civil court environment, AgentCourt aims to provide more comprehensive and realistic legal scenario simulations, potentially driving advancements in legal AI research."}, {"title": "Court Simulacrum", "content": ""}, {"title": "Visualization Settings", "content": "Inspired by previous studies (Li et al. 2024; Park et al. 2023), we design a court sandbox simulation environment using Pygame to clearly demonstrate the entire process within the court. Ultimately, we design two distinct scenarios to manage different functions. The first scenario is the law firm, where plaintiffs and defendants interact with their lawyers. The second scenario is the court, where the trial proceedings take place. Due to the limited interactions in the first scenario, our simulation environment only demonstrated the interactions in the court."}, {"title": "Agent Settings", "content": "To accurately recreate real litigation scenarios, we design six distinct roles: plaintiff, defendant, plaintiff's lawyer, defendant's lawyer, judge, and court clerk. An example of agents in a particular case can be referenced in Figure 2. Information about these roles was generated using LLM (ERNIE-Speed-128K) and can be easily extended. The following section will provide a detailed introduction to the functions of each role."}, {"title": "(1) Plaintiff and Defendant Agents:", "content": "Our simulation begins before a case has occurred, necessitating two agents powered by ERNIE-Speed-128K to respectively play the roles of a potential plaintiff and defendant. We programmed the plaintiff and defendant agents to be randomly involved in a case. Once involved, both agents autonomously seek legal assistance from a law firm, which is a typical behavior in legal disputes. To streamline the interaction process, we configured the system so that the plaintiff or defendant can obtain a complaint or an answer during their interactions with the lawyer, without needing to draft these documents from scratch. (2) Lawyer Agents: We design two lawyer agents powered by ERNIE-Speed-128K. When the plaintiff and defendant seek legal assistance at the law firm, these lawyer agents are randomly assigned as the plaintiff's lawyer and the defendant's lawyer. They communicate with their respective clients to gather relevant information about the case. Finally, drawing upon the existing and continuously enriched repositories of legal experience, case precedents, and statutory codes, they engage in court debates in accordance with the prescribed procedures, championing the interests of their respective clients. (3) Judge Agent: In the court, the judge is responsible for overseeing the entire process, listening to the arguments from both lawyers, and asking questions when appropriate. Finally, the judge summarizes and evaluates each round of the lawyers' arguments before delivering the final judgment. (4) Court Clerk Agent: To create a more realistic court environment and to facilitate the evolution of the agent, we design a court clerk agent to announce the commencement of the trial and document the entire process of the trial."}, {"title": "Data Settings and Processing", "content": "Our data settings and processing methodology, as illustrated in Figure 3, encompasses regularized filtering, BERT embedding, and privacy masking."}, {"title": "Accessing Confidential Pleadings", "content": "Pleadings, integral to legal proceedings, are often confidential and proprietary, restricting access to these key documents. Traditional open-source data access is insufficient, with primary documents typically secured within court filing systems and private records."}, {"title": "Dataset Construction and Preprocessing", "content": "Leveraging the China Judgement Website\u00b9, we compiled a dataset of 10,000 civil judgements. Preprocessing focused on enhancing dataset quality: we meticulously cleaned and selected 1,389 high-value cases featuring both plaintiff claims and defendant defenses. To address potential duplications, we employed BERT (Cui et al. 2021) for semantic vectorization of 'Case Introduction' sections and K-Means clustering (Kodinariya, Makwana et al. 2013) to group similar documents. This yielded 1,000 representative documents for our moot court training and testing."}, {"title": "Data Generation and Anonymization", "content": "We utilized the ERNIE-Speed-128K API (Baidu Intelligent Cloud Documentation 2024) to generate and anonymize high-fidelity simulated texts, tailored to civil judgements. This resulted in a curated dataset of 1,000 training and 50 test samples, designed to support robust legal argumentation and judgement prediction within our simulated moot court environment, thereby advancing legal analytics."}, {"title": "Interactions Settings", "content": "In this subsection, we will provide a detailed explanation of each step in our simulation."}, {"title": "Case Generation", "content": "We design two resident agents who randomly encounter cases. When these agents become involved in a case, they seek legal assistance from law firms, thereby"}, {"title": "Court proceedings", "content": "The full process of our simulation is illustrated in Figure 4. To complement the visual representation, we've provided Algorithm 1, which outlines the key steps of the court proceedings in pseudocode format.\nThe DEBATE_ROUNDS parameter in 1 is a hyperparameter that we've manually set to accelerate the iteration process. This can be adjusted based on specific requirements or to more closely simulate real-world proceedings."}, {"title": "Adversarial Evolutionary Approach", "content": "In the pursuit of advancing lawyer proficiency within court simulations driven by large language models, we introduce an adversarial evolutionary strategy. This strategy, devoid of preset parameters and independent of manually annotated datasets, dramatically cuts annotation expenses while augmenting the model's ability to generalize across diverse and complicated court scenarios, enabling AI to autonomously devise potent defensive tactics. The strategy's effectiveness hinges upon three fundamental modules, namely the experience database, case database, and legal code database.\nPrevious studies, such as AI-town (Park et al. 2023) and MedAgent-Zero (Li et al. 2024), have primarily focused on the communication between agents, leveraging information exchange and supplementation from large models to enable agents to accumulate databases, thereby facilitating their evolution. In contrast, our simulated court scenario features agents, with direct conflicting interests, where interactions during adversarial proceedings are more targeted, thereby enhancing their self-evolutionary capacity.\nAt the conclusion of each court proceeding, both lawyers are mandated to undertake a comprehensive reflection and"}, {"title": "Database Design", "content": ""}, {"title": "Construction of Experience Repository", "content": "Learning from past litigation experiences is essential for the development of an Al agent in legal contexts, and we posit that agent-driven by large language models have the capacity to engage in self-reflection upon these cases, distilling higher-order lessons and principles that ensure sound judgment in future, analogous scenarios. The repository of experiences maintained by such systems leans towards the macroscopic, focusing on broad strategies and overarching directions, thereby equipping the legal agent with a comprehensive perspective in case handling. As this experiential repository accrues, the responsiveness of the legal agent sharpens, enabling it to swiftly identify leverage points for argumentation, fostering a more agile and effective legal practice."}, {"title": "Development of Case Repository", "content": "In contrast to an experience repository, a case library emphasizes the retention of individual cases themselves. Within the context of actual court proceedings, referencing prior cases and their judgments is imperative. The case library enables legal agents to swiftly amass a wealth of cases for future citation, thereby enhancing their ability to draw upon relevant precedents. The content preserved in a case library is more granular, offering micro-level guidance and specifics. As the library grows, the responses provided by legal agents become increasingly persuasive and logically coherent, bolstered by a rich foundation of precedent knowledge."}, {"title": "Establishment of Legal Code Repository", "content": "In the court, possessing comprehensive and professional legal knowledge"}, {"title": "Experiment", "content": ""}, {"title": "Experimental Setup", "content": "Task Descriptions Our simulated court is based on 1,000 real-world civil cases. As the database of cases for the legal agents expands, their capabilities improve. This evolutionary process allows us to compare the performance of evolved and un-evolved agents, providing insights into the effectiveness of our learning approach. Our experiment includes two parts: automatic evaluation and manual evaluation."}, {"title": "Automatic Evaluation Tasks", "content": "To comprehensively evaluate the performance of the evolved legal agent, we applied the LawBench assessment metrics (Fei et al. 2023) to conduct an automatic evaluation of our model. LawBench has been meticulously crafted to provide a precise assessment of the LLMs' legal capabilities from three cognitive levels: legal knowledge memorization, legal knowledge comprehension, and the application of legal knowledge. In alignment with these cognitive levels, we chose appropriate tasks to critically assess the model's competencies.\nThe tasks include Article Recitation, Dispute Focus Identification, Issue Topic Identification, and Consultation. These tasks are designed to evaluate the agent's ability to memorize legal content, identify key points of contention, and provide appropriate responses to legal inquiries."}, {"title": "Manual Evaluation Tasks", "content": "Human experts evaluated the performance of the AI agents in simulated court debates based on three key dimensions: Cognitive Agility, Professional Knowledge, and Logical Rigor. We established clear criteria for each dimension."}, {"title": "Evaluation Criteria", "content": "We assessed the AI agents based on three key dimensions: (1) Cognitive Agility: The ability to quickly understand and react to new information, challenges, or rebuttals. Criteria include rapid comprehension of opposing viewpoints, swift identification of weaknesses in arguments, and quick integration of information for strong arguments. (2) Professional Knowledge: The depth and breadth of knowledge in the legal field, including theory, practice, and latest developments. Criteria include accurate citation of relevant laws and cases, deep understanding of legal principles, and clear articulation of legal arguments. (3) Logical Rigor: The logical consistency, rationality, and close connection between arguments. Criteria include clear structure of argumentation, explicit logical relationships in arguments, and smooth transitions without logical gaps."}, {"title": "Evaluation Methods", "content": ""}, {"title": "Automatic Evaluation", "content": "The experimental baseline was adopted from ERNIE-Speed-128K, and our model consists of the two legal agents that have undergone evolution. We applied the LawBench metrics to assess their performance across the specified tasks."}, {"title": "Manual Evaluation", "content": "We designed a comprehensive human evaluation experiment with a double-blind controlled design. It involved 62 real civil cases, each evaluated twice with different role configurations, resulting in 124 debate records. Five legal experts, each with an average of 7 years of experience in civil cases, independently reviewed all records using a binary choice scoring system for each dimension. Final results were determined through a majority vote system."}, {"title": "Experimental Results", "content": ""}, {"title": "Automatic Evaluation Results", "content": "Table 1 presents the results of automatic evaluation. The two evolved agents show noticeable improvements across all four tasks, attributed to our court simulation process and evolution strategy."}, {"title": "Manual Evaluation Results", "content": "Table 2 shows the performance of evolved and un-evolved agents as plaintiff and defendant. The evolved version of CourseAgent demonstrated"}, {"title": "Case Analysis", "content": "To further analyze the performance of legal agents before and after evolution, we conducted a detailed case study. This analysis focused on both automatic and manual evaluation tasks, providing insights into the improvements achieved through our evolutionary approach."}, {"title": "Automatic Evaluation: Article Recitation Task", "content": "We examined the article recitation task as an example of our automatic evaluation. The agents were prompted to provide the content of article 43 of the Labor Law. The case is shown in Figure 5, where the first row contains the input prompt, the second row is the response from the evolved agent, the third row is the response from the unevolved agent, and the fourth row is the standard answer. This comparison demonstrates the evolved agent's improved ability to accurately recall and cite legal content, a crucial skill in legal practice."}, {"title": "Manual Evaluation: Expert Analysis", "content": "For the manual evaluation, we present a case study where our evolved agent"}, {"title": "Conclusion", "content": "In this paper, we present AgentCourt, a novel simulation system for courtroom scenarios based on Large Language Models (LLMs) and agent technology. As the first to comprehensively simulate a civil court environment, AgentCourt facilitates multi-party interactions and intricate legal reasoning through a parameter-free approach. Our evaluations show significant enhancements in knowledge acquisition, response adaptability, and argumentation"}]}