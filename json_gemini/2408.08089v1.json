{"title": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents", "authors": ["Guhong Chen", "Liyang Fan", "Zihan Gong", "Nan Xie", "Zixuan Li", "Ziqiang Liu", "Chengming Li", "Qiang Qu", "Shiwen Ni", "Min Yang"], "abstract": "In this paper, we present a simulation system called AgentCourt that simulates the entire courtroom process. The judge, plaintiff's lawyer, defense lawyer, and other participants are autonomous agents driven by large language models (LLMs). Our core goal is to enable lawyer agents to learn how to argue a case, as well as improving their overall legal skills, through courtroom process simulation. To achieve this goal, we propose an adversarial evolutionary approach for the lawyer-agent. Since AgentCourt can simulate the occurrence and development of court hearings based on a knowledge base and LLM, the lawyer agents can continuously learn and accumulate experience from real court cases. The simulation experiments show that after two lawyer-agents have engaged in a thousand adversarial legal cases in AgentCourt (which can take a decade for real-world lawyers), compared to their pre-evolutionary state, the evolved lawyer agents exhibit consistent improvement in their ability to handle legal tasks. To enhance the credibility of our experimental results, we enlisted a panel of professional lawyers to evaluate our simulations. The evaluation indicates that the evolved lawyer agents exhibit notable advancements in responsiveness, as well as expertise and logical rigor. This work paves the way for advancing LLM-driven agent technology in legal scenarios. Code is available at https://github.com/relic-yuexi/AgentCourt.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) technologies, particularly large language models (LLMs), are rapidly transforming the traditional legal industry. From automated text generation to interactive legal consulting, AI applications in the legal domain are becoming increasingly widespread (Lai et al. 2023). However, despite LLMs' vast potential, significant challenges remain in handling complex legal queries and simulating real court environments. Existing legal AI systems are often confined to specific tasks and struggle to comprehensively simulate the legal reasoning process and multi-party interactions (Hamilton 2023).\nTo address these limitations, we present AgentCourt, an innovative LLM-based system designed for the simulation of civil courts. AgentCourt stands out by involving multiple roles, including judges, attorneys, plaintiffs, and defendants, thus providing a more authentic and comprehensive legal scenario simulation. This system excels not only in handling standard legal queries but also in analyzing complex real-world cases, thereby bridging a significant gap in current legal AI research. Figure 1 showcases the AgentCourt environment, with the left panel depicting a simulated courtroom and the right panel showing the courtroom dialogue.\nAt the core of AgentCourt is a meticulously designed multi-agent system, with each agent representing a role in the courtroom, all driven by specially trained LLMs. Drawing inspiration from the success of Agent Hospital (Li et al. 2024) in the medical field, we have built a simulated environment for civil courts, allowing AI agents to strengthen their legal reasoning skills through continuous practice. Significantly, we achieve notable enhancements in LLMs' cognitive agility, professional expertise, and logical coherence via adversarial engagements among lawyer agents.\nOur approach leverages an adversarial evolutionary strategy that is free from preset parameters and independent of manually annotated datasets. This strategy dramatically reduces annotation costs while enhancing the model's ability to generalize across diverse and complicated court scenarios, enabling AI to autonomously devise potent defensive tactics. Experimental results show that an evolved legal agent has demonstrated significant improvement in multiple legal tasks, indicating the potential for ordinary models to surpass GPT-4 after evolutionary refinement in complex legal tasks. Human evaluation, conducted by a panel of legal experts using real-world criteria to assess lawyer performance, further substantiates the effectiveness of the evolution process through three key metrics: cognitive agility, domain-specific knowledge, and logical rigor.\nOur research makes significant contributions to the field of legal AI. AgentCourt not only provides a powerful interactive tool for legal education but also opens new possibilities for case analysis and legal research. By open-sourcing our model and dataset, we aim to drive the development of the entire legal AI community, paving the way for a more intelligent and just legal system in the future. Through this work, we address the limitations of current AI legal systems and extend the multi-agent approach to the legal domain. By simulating a civil court environment, AgentCourt aims to provide more comprehensive and realistic legal scenario simulations, potentially driving advancements in legal AI research. The main contributions of our work are summarized as follows:\n\u2022 To the best of our knowledge, this is the first work to simulate a court environment comprehensively, mimicking a civil court setting, enabling multi-party interactions, and complex legal reasoning.\n\u2022 During the simulation of the court proceedings, the legal reasoning capabilities of LLMs are enhanced through adversarial interactions among agents. This approach, utilizing adversarial multi-agent interactions rather than simple information transmission, provides new insights for future related work.\n\u2022 We simulated 1,000 real civil cases that occurred over a span of ten days (which can take a decade for real-world lawyers). The effectiveness of agent evolution was validated through automatic and manual evaluation methods. Moreover, our data is sourced from professional law firms and has undergone privacy de-identification processing. We have open-sourced the dataset and complete code to advance the field of agent simulation worlds."}, {"title": "Related Work", "content": "Large Language Models in the Legal Domain AI applications in the legal domain have made significant strides, particularly with the advent of large language models (LLMs). These models have shown potential in various legal tasks, including case prediction, legal research, and document analysis (Lai et al. 2023; Hamilton 2023). For instance, DISC-LawLLM demonstrated the potential of fine-tuned LLMs in providing intelligent legal services (Yue et al. 2023), while the PLJP framework combines LLMs and domain models to enhance the accuracy of case judgment predictions (Wu et al. 2023). DeliLaw exemplifies the utility of LLMs in legal advice, effectively handling legal inquiries through its dialog-based system (Xie et al. 2024). Despite these advancements, current AI legal systems still face limitations in handling complex legal queries and simulating real court environments. Many existing systems are confined to specific tasks and struggle to comprehensively simulate the legal reasoning process and multi-party interactions (Janatian et al. 2023; Jin and Wang 2023).\nLarge Language Models for Real World Simulation LLM-based multi-agent systems represent a new direction in Al research, leveraging collaborative agents to address complex problems. These systems excel in utilizing cognitive synergy and knowledge sharing, thereby enhancing overall decision quality and interaction capabilities (Talebirad and Nadiri 2023; H\u00e4ndler 2023). LLM-based multi-agent systems have shown potential in multiple domains. In natural language processing, they have improved language understanding and generation tasks (Tan and Motani 2024). In robotics, these systems have enhanced robots' understanding and decision-making capabilities in human-robot interaction (Kim, Lee, and Mutlu 2024). In task planning and execution, multi-agent systems can decompose and collaboratively complete complex tasks (Yang et al. 2024). Additionally, in education, they have created personalized learning experiences and intelligent tutoring systems (Yin et al. 2024). The financial sector has also benefited from these systems, applying them to market analysis, risk assessment, and investment decisions (Nascimento, Alencar, and Lucena 2023). A notable application of LLM-based multi-agent systems is Agent Hospital, which simulates a hospital environment with LLM-driven autonomous agents representing patients, nurses, and doctors (Li et al. 2024). Key features of Agent Hospital include comprehensive simulation of disease treatment processes, autonomous learning without manual data annotation, knowledge transfer from simulation to real-world medical benchmarks, efficient learning through accelerated simulation experience, and state-of-the-art performance on medical datasets. Agent Hospital demonstrates the potential of multi-agent systems in complex and specialized domains, paving the way for new approaches in professional training and decision support.\nOur AgentCourt builds on these advancements to address the limitations of current legal AI systems and extends the multi-agent approach to the legal domain. By simulating a civil court environment, AgentCourt aims to provide more comprehensive and realistic legal scenario simulations, potentially driving advancements in legal AI research."}, {"title": "Court Simulacrum", "content": "Visualization Settings Inspired by previous studies (Li et al. 2024; Park et al. 2023), we design a court sandbox simulation environment using Pygame to clearly demonstrate the entire process within the court. Ultimately, we design two distinct scenarios to manage different functions. The first scenario is the law firm, where plaintiffs and defendants interact with their lawyers. The second scenario is the court, where the trial proceedings take place. Due to the limited interactions in the first scenario, our simulation environment only demonstrated the interactions in the court."}, {"title": "Agent Settings", "content": "To accurately recreate real litigation scenarios, we design six distinct roles: plaintiff, defendant, plaintiff's lawyer, defendant's lawyer, judge, and court clerk. Information about these roles was generated using LLM (ERNIE-Speed-128K) and can be easily extended. The following section will provide a detailed introduction to the functions of each role.\n(1) Plaintiff and Defendant Agents: Our simulation begins before a case has occurred, necessitating two agents powered by ERNIE-Speed-128K to respectively play the roles of a potential plaintiff and defendant. We programmed the plaintiff and defendant agents to be randomly involved in a case. Once involved, both agents autonomously seek legal assistance from a law firm, which is a typical behavior in legal disputes. To streamline the interaction process, we configured the system so that the plaintiff or defendant can obtain a complaint or an answer during their interactions with the lawyer, without needing to draft these documents from scratch. (2) Lawyer Agents: We design two lawyer agents powered by ERNIE-Speed-128K. When the plaintiff and defendant seek legal assistance at the law firm, these lawyer agents are randomly assigned as the plaintiff's lawyer and the defendant's lawyer. They communicate with their respective clients to gather relevant information about the case. Finally, drawing upon the existing and continuously enriched repositories of legal experience, case precedents, and statutory codes, they engage in court debates in accordance with the prescribed procedures, championing the interests of their respective clients. (3) Judge Agent: In the court, the judge is responsible for overseeing the entire process, listening to the arguments from both lawyers, and asking questions when appropriate. Finally, the judge summarizes and evaluates each round of the lawyers' arguments before delivering the final judgment. (4) Court Clerk Agent: To create a more realistic court environment and to facilitate the evolution of the agent, we design a court clerk agent to announce the commencement of the trial and document the entire process of the trial."}, {"title": "Data Settings and Processing", "content": "Our data settings and processing methodology, encompasses regularized filtering, BERT embedding, and privacy masking.\nAccessing Confidential Pleadings Pleadings, integral to legal proceedings, are often confidential and proprietary, restricting access to these key documents. Traditional open-source data access is insufficient, with primary documents typically secured within court filing systems and private records.\nDataset Construction and Preprocessing Leveraging the China Judgement Website, we compiled a dataset of 10,000 civil judgements. Preprocessing focused on enhancing dataset quality: we meticulously cleaned and selected 1,389 high-value cases featuring both plaintiff claims and defendant defenses. To address potential duplications, we employed BERT (Cui et al. 2021) for semantic vectorization of 'Case Introduction' sections and K-Means clustering (Kodinariya, Makwana et al. 2013) to group similar documents. This yielded 1,000 representative documents for our moot court training and testing.\nData Generation and Anonymization We utilized the ERNIE-Speed-128K API (Baidu Intelligent Cloud Documentation 2024) to generate and anonymize high-fidelity simulated texts, tailored to civil judgements. This resulted in a curated dataset of 1,000 training and 50 test samples, designed to support robust legal argumentation and judgement prediction within our simulated moot court environment, thereby advancing legal analytics.\nInteractions Settings In this subsection, we will provide a detailed explanation of each step in our simulation.\nCase Generation We design two resident agents who randomly encounter cases. When these agents become involved in a case, they seek legal assistance from law firms, thereby transforming into Plaintiff and Defendant Agents. Subsequently, the law firm randomly assigns lawyer agents to communicate with them and submit the complaint and defense documents to the court. To streamline the process, both the plaintiff and the defendant provide all relevant information to their respective lawyers in detail during their communications. Subsequently, the plaintiff and the defendant do not attend the court proceedings in person, a practice that is common and reasonable in actual cases.\nCourt proceedings The full process of our simulation is illustrated in Figure 4. To complement the visual representation, we've provided Algorithm 1, which outlines the key steps of the court proceedings in pseudocode format. The DEBATE_ROUNDS parameter in 1 is a hyperparameter that we've manually set to accelerate the iteration process. This can be adjusted based on specific requirements or to more closely simulate real-world proceedings.\nAdversarial Evolutionary Approach In the pursuit of advancing lawyer proficiency within court simulations driven by large language models, we introduce an adversarial evolutionary strategy. This strategy, devoid of preset parameters and independent of manually annotated datasets, dramatically cuts annotation expenses while augmenting the model's ability to generalize across diverse and complicated court scenarios, enabling AI to autonomously devise potent defensive tactics. The strategy's effectiveness hinges upon three fundamental modules, namely the experience database, case database, and legal code database.\nPrevious studies, such as AI-town (Park et al. 2023) and MedAgent-Zero (Li et al. 2024), have primarily focused on the communication between agents, leveraging information exchange and supplementation from large models to enable agents to accumulate databases, thereby facilitating their evolution. In contrast, our simulated court scenario features agents, with direct conflicting interests, where interactions during adversarial proceedings are more targeted, thereby enhancing their self-evolutionary capacity.\nAt the conclusion of each court proceeding, both lawyers are mandated to undertake a comprehensive reflection and summary of the proceedings. Valuable insights and lessons gained are extracted and preserved within an experiential knowledge database. The fundamental case specifics, including its title, contextual backdrop, categories, and other related aspects, are compiled and recorded in a case database. Furthermore, subsequent to the trial, lawyers are assigned to study the legal statutes invoked during the hearing, which are then incorporated into a legal code database. The precise mechanisms and organization of these three databases will be expounded in the following text.\nIn the court, prior to each lawyer's response, queries are dynamically generated based on the ongoing dialogue history, aimed at retrieving relevant knowledge from the experience repository, case database, and legal code database. This process enhances the coherence and logical consistency of their responses, accelerates reaction times, and broadens their legal knowledge base. As experiences, cases, and legal codes accumulate, the capabilities of the virtual lawyers undergo enhancement. Furthermore, we employ APIs from leading domestic legal institutions to ensure the currency and accuracy of our legal code database, thus maintaining its relevancy. For specifics regarding prompt designs and query formats, reference can be made to our source code.\nDatabase Design Construction of Experience Repository Learning from past litigation experiences is essential for the development of an Al agent in legal contexts, and we posit that agent-driven by large language models have the capacity to engage in self-reflection upon these cases, distilling higher-order lessons and principles that ensure sound judgment in future, analogous scenarios. The repository of experiences maintained by such systems leans towards the macroscopic, focusing on broad strategies and overarching directions, thereby equipping the legal agent with a comprehensive perspective in case handling. As this experiential repository accrues, the responsiveness of the legal agent sharpens, enabling it to swiftly identify leverage points for argumentation, fostering a more agile and effective legal practice.\nDevelopment of Case Repository In contrast to an experience repository, a case library emphasizes the retention of individual cases themselves. Within the context of actual court proceedings, referencing prior cases and their judgments is imperative. The case library enables legal agents to swiftly amass a wealth of cases for future citation, thereby enhancing their ability to draw upon relevant precedents. The content preserved in a case library is more granular, offering micro-level guidance and specifics. As the library grows, the responses provided by legal agents become increasingly persuasive and logically coherent, bolstered by a rich foundation of precedent knowledge.\nEstablishment of Legal Code Repository In the court, possessing comprehensive and professional legal knowledge is of utmost importance. Following the conclusion of a lawsuit, legal agents review the fundamental aspects of the case and past exchanges during court proceedings, reflecting on which legal provisions could have improved their responses. To this end, we provide an interface to a legal code database, allowing agents to study and subsequently incorporate relevant legal codes into their personal repositories. As their legal code database accumulates, the agents' responses become increasingly sophisticated and comprehensive, underpinned by a robust foundation of codified law.\nTo mitigate noise and optimize utilization of the three repositories, we introduce an additional evaluative stage prior to database utilization. The lawyer agent assesses, based on semantic similarity, the top-K retrieved experiences from the databases for their relevance to the ongoing trial proceedings. Only beneficial experiences are incorporated into prompts, while those deemed unhelpful are excluded, thereby enhancing the precision and efficacy of the agent's guidance."}, {"title": "Experiment", "content": "Experimental Setup Task Descriptions Our simulated court is based on 1,000 real-world civil cases. As the database of cases for the legal agents expands, their capabilities improve. This evolutionary process allows us to compare the performance of evolved and un-evolved agents, providing insights into the effectiveness of our learning approach. Our experiment includes two parts: automatic evaluation and manual evaluation.\nAutomatic Evaluation Tasks To comprehensively evaluate the performance of the evolved legal agent, we applied the LawBench assessment metrics (Fei et al. 2023) to conduct an automatic evaluation of our model. LawBench has been meticulously crafted to provide a precise assessment of the LLMs' legal capabilities from three cognitive levels: legal knowledge memorization, legal knowledge comprehension, and the application of legal knowledge. In alignment with these cognitive levels, we chose appropriate tasks to critically assess the model's competencies.\nThe tasks include Article Recitation, Dispute Focus Identification, Issue Topic Identification, and Consultation. These tasks are designed to evaluate the agent's ability to memorize legal content, identify key points of contention, and provide appropriate responses to legal inquiries.\nManual Evaluation Tasks Human experts evaluated the performance of the AI agents in simulated court debates based on three key dimensions: Cognitive Agility, Professional Knowledge, and Logical Rigor. We established clear criteria for each dimension.\nEvaluation Criteria We assessed the AI agents based on three key dimensions: (1) Cognitive Agility: The ability to quickly understand and react to new information, challenges, or rebuttals. Criteria include rapid comprehension of opposing viewpoints, swift identification of weaknesses in arguments, and quick integration of information for strong arguments. (2) Professional Knowledge: The depth and breadth of knowledge in the legal field, including theory, practice, and latest developments. Criteria include accurate citation of relevant laws and cases, deep understanding of legal principles, and clear articulation of legal arguments. (3) Logical Rigor: The logical consistency, rationality, and close connection between arguments. Criteria include clear structure of argumentation, explicit logical relationships in arguments, and smooth transitions without logical gaps.\nEvaluation Methods Automatic Evaluation The experimental baseline was adopted from ERNIE-Speed-128K, and our model consists of the two legal agents that have undergone evolution. We applied the LawBench metrics to assess their performance across the specified tasks.\nManual Evaluation We designed a comprehensive human evaluation experiment with a double-blind controlled design. It involved 62 real civil cases, each evaluated twice with different role configurations, resulting in 124 debate records. Five legal experts, each with an average of 7 years of experience in civil cases, independently reviewed all records using a binary choice scoring system for each dimension. Final results were determined through a majority vote system.\nExperimental Results Automatic Evaluation Results Table 1 presents the results of automatic evaluation. The two evolved agents show noticeable improvements across all four tasks, attributed to our court simulation process and evolution strategy.\nManual Evaluation Results Table 2 shows the performance of evolved and un-evolved agents as plaintiff and defendant. The evolved version of CourseAgent demonstrated significant improvement across all three evaluation dimensions, particularly in Professional Knowledge and Logical Rigor. In the same cases, the evolved agents, whether acting as plaintiff or defendant, were able to outperform their un-evolved counterparts, further illustrating the effectiveness of our evolution strategy.\nNotably, the human evaluation results showed high consistency with the automatic evaluation metrics. cognitive agility correlated positively with the dispute focus identification task, Professional Knowledge with the article recitation task, and logical rigor with the consultation task.\nCase Analysis To further analyze the performance of legal agents before and after evolution, we conducted a detailed case study. This analysis focused on both automatic and manual evaluation tasks, providing insights into the improvements achieved through our evolutionary approach.\nAutomatic Evaluation: Article Recitation Task We examined the article recitation task as an example of our automatic evaluation. The agents were prompted to provide the content of article 43 of the Labor Law. The case is shown in Figure 5, where the first row contains the input prompt, the second row is the response from the evolved agent, the third row is the response from the unevolved agent, and the fourth row is the standard answer. This comparison demonstrates the evolved agent's improved ability to accurately recall and cite legal content, a crucial skill in legal practice.\nManual Evaluation: Expert Analysis For the manual evaluation, we present a case study where our evolved agent acted as the plaintiff's lawyer. The specific details of the case will be given in the supplementary material. Legal experts evaluated the performance across three dimensions: cognitive agility, professional knowledge, and logical rigor. The plaintiff's lawyer (evolved agent) demonstrated superior agility, quickly understanding and responding to the defendant's arguments, such as immediately citing article 94 of the contract Law when the defendant argued against contract termination. The evolved agent exhibited deeper legal knowledge, accurately citing multiple laws and regulations (e.g., Contract Law, Property Rights Law, Guarantee Law) and integrating them with case specifics, while the un-evolved agent merely mentioned legal concepts without providing specific provisions. Moreover, the evolved agent presented a more structured and rigorous argument, organizing points logically around contract termination, loan calculations, mortgage rights, and liability determination, with each point supported by relevant legal provisions and evidence. This case analysis highlights the significant improvements achieved through our evolutionary approach, particularly in knowledge application, response adaptability, and argumentation quality."}, {"title": "Conclusion", "content": "In this paper, we present AgentCourt, a novel simulation system for courtroom scenarios based on Large Language Models (LLMs) and agent technology. As the first to comprehensively simulate a civil court environment, AgentCourt facilitates multi-party interactions and intricate legal reasoning through a parameter-free approach. Our evaluations show significant enhancements in knowledge acquisition, response adaptability, and argumentation quality as lawyer agents evolve via adversarial interactions. Although further research areas exist, such as role adaptability and performance in complex cases, AgentCourt holds promise as a robust legal auxiliary tool. By open-sourcing our privacy-anonymized dataset and code, we aim to drive the Legal AI field and potentially revolutionize legal practice through continuous improvement and optimization."}]}