{"title": "ENHANCING TEXT GENERATION IN JOINT NLG/NLU LEARNING THROUGH CURRICULUM LEARNING, SEMI-SUPERVISED TRAINING, AND ADVANCED \u039f\u03a1\u03a4\u0399\u039c\u0399\u0396\u0391\u03a4\u0399ON TECHNIQUES", "authors": ["Rahimanuddin Shaik", "Katikela Sreeharsha Kishore"], "abstract": "Text generation is the automated process of producing written or spoken language using computational methods. It involves generating coherent and contextually relevant text based on predefined rules or learned patterns. However, challenges in text generation arise from maintaining coherence, ensuring diversity and creativity, and avoiding biases or inappropriate content. This research paper developed a novel approach to improve text generation in the context of joint Natural Language Generation (NLG) and Natural Language Understanding (NLU) learning. The data is prepared by gathering and preprocessing annotated datasets, including cleaning, tokenization, stemming, and stop-word removal. Feature extraction techniques such as POS tagging, Bag of words, and Term Frequency-Inverse Document Frequency (TF-IDF) are applied. Transformer-based encoders and decoders, capturing long range dependencies and improving source-target sequence modelling. Pre-trained language models like Optimized BERT are incorporated, along with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA). Reinforcement learning with policy gradient techniques, semi-supervised training, improved attention mechanisms, and differentiable approximations like straight-through Gumbel SoftMax estimator are employed to fine-tune the models and handle complex linguistic tasks effectively. The proposed model is implemented using Python.", "sections": [{"title": "Introduction", "content": "The field of natural language processing (NLP) has made remarkable progress in recent years, thanks to advancements in deep learning and the availability of large-scale language models. Among the many subtasks in NLP, natural language generation (NLG) and natural language understanding (NLU) play vital roles in enabling machines to effectively communicate with humans [1] [2]. While NLG focuses on generating human-like text based on structured data or prompts, NLU aims to comprehend and extract meaning from human language inputs. Traditionally, NLG and NLU have been treated as separate processes, but there is growing recognition that the joint learning of these two components can lead to significant improvements in text generation [3] [4]. This paper delves into the concept of joint NLG/NLU learning and explore how it enhances text generation capabilities. Examine the challenges associated with NLG and NLU in isolation and highlight the benefits of integrating these two components. By bridging the gap between NLG and NLU, achieve more coherent, context-aware, and human-like text generation, ultimately advancing the state of the art in natural language processing [5]. NLG involves transforming structured data or prompts into coherent and fluent human-like text. It finds applications in various domains, such as chatbots, virtual assistants, and automated report generation. Traditional NLG approaches often rely on rule-based or template-based methods, which can be limiting in terms of flexibility and adaptability. However, the emergence of deep learning models, particularly transformer-based architectures like GPT-3, have revolutionized NLG by enabling data-driven and context-aware text generation [6].\nDespite these advancements, NLG models often struggle to generate text that truly understands and responds to user input, leading to generic or irrelevant responses. On the other hand, NLU focuses on understanding the semantics and intent of human language inputs [7]. It involves tasks like named entity recognition, sentiment analysis, intent classification, and slot filling. NLU has witnessed significant progress with the rise of deep learning techniques, especially with the advent of pre-trained language models like BERT and RoBERTa [8]. These models have demonstrated impressive performance in a wide range of NLU tasks. However, NLU models typically operate on individual sentences or short texts, and they lack the ability to generate coherent and contextually appropriate responses [9] [10]. Joint learning of NLG and NLU addresses these limitations by integrating the two components and leveraging their synergies. By jointly training NLG and NLU models, create a bidirectional flow of information between them [11]. The NLG component can benefit from the contextual understanding and semantic knowledge extracted by the NLU component, leading to more accurate, relevant, and context-aware text generation [12].\nThe NLU component can leverage the generated text from the NLG component to improve its understanding of user input and perform better intent classification or slot filling [13] [14]. There are several ways in which NLG and NLU can be jointly learned. One approach is to use reinforcement learning, where the NLU component provides feedback to the NLG component, guiding it towards generating more relevant and contextually appropriate responses. Another approach is to use a shared representation learning framework, where both components share intermediate representations, enabling them to exchange information and align their learning objectives [15]. Additionally, adversarial learning techniques can be employed to train NLG and NLU models in a competitive setting, where the NLG component tries to deceive the NLU component and vice versa, leading to more robust and accurate models.\nThis study's major contribution is exemplified below:\n\u2022\nTo enhance the model's capabilities, an encoder transformation is introduced by replacing the LSTM-based encoder with a Transformer encoder. This enables the model to capture long-range dependencies and effectively encode the input sequence using self-attention mechanisms and position-wise feed-forward networks. Similarly, a decoder transformation is applied by replacing the LSTM-based decoder with a Transformer decoder, allowing for better modelling of dependencies between the source and target sequences.\n\u2022\nTo enhance natural language understanding and generation, the NLG and NLU models utilize pre-trained language models such as Optimized BERT (Bidirectional Encoder Representations from Transformers).\n\u2022\nA hybrid optimization model HRAHA, combining the standard RFO with AHA, is employed to optimize the batch size of BERT.\nThe remainder of this research paper is organized as follows: Section II discusses the review of literature on text generation, and Section III presents the proposed mechanism used in the work. Section IV describes the experimental results. Section V brings this research to a conclusion."}, {"title": "Literature Review", "content": "In 2020, Cao [13] developed NLDT, a neural generative architecture for generating natural language descriptions from structured tables. NLDT leverages table semantics, adopts a two-level neural model, and introduces a word-conversion method for handling out-of-vocabulary words. We also incorporate the concept of theme and enhance the beam search algorithm. Experimental results on multiple datasets demonstrate significant improvements in BLEU-4 scores compared to state-of-the-art approaches.\nIn 2021, Chen et al. [14] introduced an intelligent approach for generating SPARQL queries in natural language processing systems. By leveraging machine learning techniques, a two-stage maximum-entropy Markov model is proposed to identify entity types and RDF types. This approach, implemented in the QAWizard prototype system, outperforms other systems in question answering evaluations based on QALD-8 metrics.\nIn 2022, Seifossadat and Sameti [15] presented a stochastic corpus-based model for data-to-text generation, leveraging syntactic dependency information to construct fluent sentences with correct grammatical structures. Our approach incorporates dependency relations and meaning labels to generate tree-form structures, ensuring semantic relevance and avoiding redundancy. By employing beam search, our model achieves high diversity in sentence generation.\nIn 2020, Yang et al. [16] developed FGGAN, a text generation model that improves upon traditional GAN approaches. FGGAN utilizes a feature guidance module to enhance the feedback from the discriminator network, resulting in better guidance for the generator. It also incorporates text semantic rules to enhance the quality of generated text. Experimental results demonstrate the effectiveness and superiority of FGGAN across different datasets.\nIn 2022, Yang et al. [17] propose a model with a dynamic planner to transcribe structural data into readable text. Our approach involves record planning and text realization as separate procedures, allowing for plan revision. We introduce a likelihood-driven training strategy that selects input records based on sentence likelihood, eliminating the need for annotated plans. Experimental results on E2E and EPW datasets demonstrate the superiority of our model in terms of text and plan metrics.\nIn 2019, Chen et al. [18] used a novel approach to enhance neural text generation. Our model combines RNN and CNN to capture global and local contextual features for improved text representation. We introduce a modified diverse beam search technique to encourage sentence diversity during decoding and rank the generated sentences based on key phrase co-occurrence, promoting semantic relevance. Experimental results on document summarization and headline generation tasks demonstrate significant performance improvement compared to state-of-the-art baselines.\nIn 2021, Steur and Schwenker [19] developed the potential of Capsule Networks (CapsNets) with routing-by-agreement for text classification. By conducting experiments on six datasets, the study addresses research questions, providing insights and best practices for CapsNet theory in the text domain. The results demonstrate the robustness of CapsNets across various network architectures, datasets, and text classification tasks, establishing them as a promising next-generation technology for text classification and urging further research.\nIn 2019, Zhang et al. [20] used a GAN-based cross-domain text sentiment transfer model for emotional text generation, addressing the challenge of limited annotated data. Our approach leverages annotated data from other domains to enhance training and combines adversarial reinforcement learning with supervised learning. Experimental results demonstrate that our model surpasses state-of-the-art methods, generating high-quality emotional text while preserving domain information and content semantics."}, {"title": "Problem Statement", "content": "The problem at hand is the disjointed nature of NLG and NLU processes in the field of NLP. NLG focuses on generating human-like text from structured data or prompts, while NLU aims to comprehend and extract meaning from human language inputs. However, the lack of integration between NLG and NLU hinders the development of coherent and contextually appropriate text generation [1]. The precise problem is the need to bridge the gap between NLG and NLU and enable joint learning to enhance text generation capabilities. Existing NLG approaches often produce generic or irrelevant responses that lack contextual understanding, while NLU models typically operate on isolated sentences and struggle to generate coherent and contextually appropriate text [10]. By integrating NLG and NLU, it is aimed to create a bidirectional flow of information, allowing the NLG component to benefit from the contextual understanding and semantic knowledge extracted by the NLU component. This integration would result in more accurate, relevant, and context-aware text generation, advancing the state of the art in NLP and enabling the development of more intelligent and human-like conversational agents in various applications."}, {"title": "Proposed Methodology", "content": "Text generation involves using computational models to generate human-like textual content from structured data or prompts. The challenge lies in producing contextually relevant and fluent text that accurately captures the user's intent. This requires addressing issues such as coherence, consistency, ambiguity, and variability in language usage. Overcoming these challenges is vital for advancing text generation and enabling more effective and natural human-machine interactions. This paper developed an enhanced architecture for improving text generation in joint NLG/NLU learning by incorporating advanced techniques such as curriculum learning and semi-supervised training. The focus is on enhancing the performance and data efficiency of the previous approach. Building upon the seq2seq model with attention, the proposed modifications are introduced to optimize the text generation process."}, {"title": "Data Preparation", "content": "To prepare the dataset for NLG and NLU tasks, a series of steps are undertaken, including gathering and pre-processing the data. The first step involves collecting the relevant dataset, which can include annotated data for NLG (such as paired input-output sequences) and NLU (such as labelled intent or entity recognition data). Once the dataset is gathered, the pre-processing phase begins."}, {"title": "Pre-Processing", "content": "In this research work, pre-processing is done using text cleaning, tokenization, stop word removal, and stemming/lemmatization."}, {"title": "Text Cleaning", "content": "Text cleaning is a fundamental pre-processing step in NLP that plays a crucial role in preparing textual data for accurate analysis. The goal of text cleaning is to remove irrelevant or inconsistent content and standardize the text to a consistent format. The first step is to remove punctuation marks, special characters, and stop words, which helps eliminate noise and irrelevant information from the text data. This process enhances the quality of subsequent analysis. Additionally, it involves correcting spelling errors, converting all text to lowercase, and expanding contractions to ensure consistency and standardization. Another vital aspect of text cleaning is removing duplicate content. Duplicate text can distort the analysis results, so it is essential to identify and eliminate such instances to maintain accuracy. Text cleaning is especially critical when dealing with large volumes of text data from diverse sources, including social media, news articles, and academic publications. The accuracy of the analysis heavily relies on the quality of the text data, underscoring the importance of thorough it. It is a crucial step in NLP that standardizes the text data, removes irrelevant content, and improves analysis accuracy."}, {"title": "Tokenization", "content": "Tokenization is a fundamental process in NLP that breaks down text into smaller units called tokens, typically words. Word-based tokenization removes punctuation and special characters, treating words as separate tokens. It enables the analysis of textual data for various applications such as machine translation, text classification, sentiment analysis, and information retrieval. Despite challenges like complex compound words or word ambiguity, word-based tokenization allows for structured and manageable analysis of text data. Other types of tokenization include white space tokenization, dictionary-based tokenization using pre-existing dictionaries or lexicons, and subword tokenization that breaks down text into smaller subword units for language modelling. Tokenization techniques play a critical role in unlocking the potential of natural language processing by enabling efficient and meaningful analysis of textual information."}, {"title": "Stop Word Removal", "content": "Stop word removal is a prevalent pre-processing technique in NLP that involves eliminating commonly occurring words, such as articles, pronouns, prepositions, and conjunctions, from a text corpus. These words often carry little semantic meaning and can introduce noise to the data. By removing stop words, the dataset size and training time can be reduced, and the accuracy of NLP models can be improved. Popular libraries like NLTK (Natural Language Toolkit) and SpaCy provide predefined lists of stop words for various languages, which can be customized as per project requirements. The removal process involves comparing each word in the text with the stop word list and excluding those that match. It is important to exercise caution when applying stop word removal, as in certain contexts, stop words can convey crucial information. For instance, in sentiment analysis, words like \"not\" and \"but\" can significantly impact the sentiment of a sentence. Therefore, the decision to remove stop words should be made based on the specific task and the particularities of the dataset."}, {"title": "Stemming/Lemmatization", "content": "Stemming and lemmatization are techniques employed in NLP to reduce words to their base or root forms, thereby simplifying text and enhancing the accuracy of text analysis algorithms. Stemming involves removing the suffixes from words to derive their stems. For instance, the stem of \"running\" is \"run.\" Popular stemming algorithms include the Porter stemming algorithm and the Snowball stemming algorithm. Lemmatization, on the other hand, transforms words into their base forms, known as lemmas. It takes into account the context of the word and its part of speech. The lemma of \"running\" is \"run,\" while the lemma of \"am\" is \"be.\" Lemmatization tends to yield more accurate results compared to stemming due to its consideration of word context. During the feature extraction step in NLP, meaningful and relevant features are derived from pre-processed text data. These features can be individual words or other linguistic units and serve to represent the underlying meaning and structure of the text. Feature extraction plays a crucial role in enabling subsequent analysis and modelling tasks by capturing important aspects of the text for further processing. By applying stemming or lemmatization techniques, text analysis algorithms can operate on a simplified representation of the text, reducing the complexity and increasing the accuracy of NLP tasks such as information retrieval, sentiment analysis, and text classification."}, {"title": "Feature Extraction", "content": "In this research work, the features are extracted using BOW, TF-IDF, and POS Tagging."}, {"title": "Bag of Words (BOW)", "content": "The Bag of Words (BOW) model is a popular approach in natural language processing (NLP) that represents text data by counting the occurrence of words, regardless of their order or context. In the BOW model, a text document is viewed as a \"bag\" of individual words, ignoring grammar and word relationships. The process begins with tokenization, where the text is split into words. A vocabulary is then constructed by collecting all unique words from the corpus. Each document is transformed into a numerical vector, where the dimensions correspond to the vocabulary size, and the values represent the frequency of words in the document. This vector representation enables quantitative analysis and machine learning algorithms on text data. Although BOW discards syntactic and semantic information, it has proven useful in various NLP tasks, including text classification, sentiment analysis, and information retrieval. By capturing word frequency, BOW provides a simple and effective way to represent and process textual information in a structured and quantitative manner."}, {"title": "TF-IDF", "content": "Term Frequency-Inverse Document Frequency (TF-IDF) is a popular method for extracting relevant features from pre-processed text data. TF-IDF measures the importance of a word in a document by computing a score that takes into account the frequency of the word in the document and the frequency of the word in the corpus of documents. The TF-IDF score of a word w in a document d can be calculated as per Eq. (1).\n$TF \u2013 IDF(w,d) = TF(w, d) * IDF(w)$ (1)\nwhere TF(w, d) is the term frequency of the word w in the document d, which measures how often the word appears in the document. IDF(w)is the inverse document frequency of the word w, which measures how rare the word is in the corpus of documents. The IDF score of a word w can be calculated as per Eq. (2).\n$IDF(w) = log(\\frac{N}{n_w})$ (2)"}, {"title": "Part-of-Speech (POS) Tagging", "content": "Part-of-Speech (POS) tagging is a fundamental task in natural language processing (NLP) that involves assigning grammatical tags to each word in a sentence, indicating its syntactic category and function within the sentence. POS tags provide valuable linguistic information about the words, allowing for deeper analysis and understanding of the text. The POS tagging process typically involves using pre-trained models or rule-based algorithms to assign tags to words based on their context and surrounding words. The tags represent various parts of speech, such as nouns, verbs, adjectives, adverbs, pronouns, determiners, conjunctions, and more. POS tagging has numerous applications in NLP, including grammar checking, word sense disambiguation, information extraction, text-to-speech synthesis, and machine translation. It helps in capturing the grammatical structure of sentences, identifying syntactic patterns, and facilitating higher-level language understanding. Accurate POS tagging can be challenging due to language ambiguities, words with multiple possible parts of speech, and context-dependent variations. However, with the advancements in machine learning and the availability of large annotated datasets, state-of-the-art POS tagging models have achieved high accuracy across multiple languages."}, {"title": "Encoder Transformation", "content": "A Transformer encoder with transfer learning is a neural network architecture that incorporates pre-trained models to enhance the encoding process. The Transformer encoder is specifically designed to capture long-range dependencies and effectively encode input sequences in natural language processing tasks. The Transformer encoder is composed of multiple layers, each containing self-attention mechanisms and position-wise feed-forward networks. Self-attention mechanisms enable the model to attend to all positions within the input sequence simultaneously, rather than processing the sequence sequentially like an LSTM-based encoder. This parallel processing capability allows the model to capture relationships and dependencies between distant words, improving its understanding of the overall context and meaning of the text. In the self-attention mechanism, each word in the input sequence is transformed into query, key, and value representations. The model then computes attention scores between all pairs of words, capturing the importance or relevance of each word with respect to others. These attention scores are used to weight the values, which are then combined to produce a contextualized representation for each word. Position-wise feed-forward networks apply non-linear transformations to these contextualized representations, further refining the encoded information. The multiple layers of self-attention and position-wise feed-forward networks enable the Transformer encoder to effectively capture and encode the input sequence, allowing downstream tasks such as text generation or understanding to benefit from the learned representations. Transfer learning is leveraged in the Transformer encoder by utilizing pre-trained models. These models are trained on large-scale datasets and capture general language patterns and knowledge. By incorporating the pre-trained parameters into the Transformer encoder, the model can benefit from the learned representations and effectively encode the input sequence for a specific task, even with limited task-specific training data. This transfer of knowledge helps improve the performance and efficiency of the encoding process."}, {"title": "Decoder Transformation", "content": "In the decoder transformation, it is made up of multiple layers of self-attention mechanisms and position-wise feed-forward networks. During the decoding process, the Transformer decoder generates the target words by attending to two key sources of information: the encoded representations from the Transformer encoder and the previously generated words. By attending to the encoder's encoded representations, the decoder can access the rich contextual information captured during the encoding phase, enabling it to understand the input sequence more effectively. Additionally, the decoder attends to the previously generated words to consider the context and dependencies between the generated and upcoming words. This allows the decoder to capture long-range dependencies and model the sequential nature of the target sequence generation. The use of Transformers in the decoder enhances the model's ability to model and capture complex dependencies between the source and target sequences. The self-attention mechanisms enable the decoder to focus on relevant parts of the input and generated context, enabling more accurate and contextually-aware generation of target words. Overall, the inclusion of Transformers in the decoder facilitates better modelling of the dependencies between the source and target sequences, resulting in improved performance in natural language generation tasks."}, {"title": "Pre-trained Language Models", "content": "The NLG and NLU models utilize pre-trained language models like Optimized BERT, trained on extensive text data, to enhance natural language understanding and generation. The hybrid optimization model HRAHA, incorporating elements of the standard RFO and AHA is employed to optimize the batch size of BERT for improved performance."}, {"title": "Optimized BERT", "content": "BERT is a bidirectional transformer. BERT (Bidirectional Encoder Representations from Transformers) is primarily used for Natural Language Processing (NLP) tasks, but can also be used for image classification tasks by incorporating image features into its inputs. In this case, BERT is used to encode the information in the image, combined with textual information to perform the classification task."}, {"title": "Multiheaded Self-Attention (MHSA)", "content": "A mapping between a query and a set of key-value pairs and an output is known as an attention function. The query, keys, values, and output are all vectors. The result is calculated as a weighted sum of the values, with each value's weight determined by the query's compatibility function with its corresponding key. The popular scaled dot-product attention is the mechanism as per Eq. (3).\n$att(qu, ke, va) = softmax(\\frac{quke}{\\sqrt{d}}) va$ (3)\nwhere d is the dimension of the input data and qu, ke, Ava stand for the query, key, and value, respectively. MHSA is written using the p head (h1, h2, ..., hp), $w^o$ is the learned metrices, as shown in Eq. (4).\n$mhsa(x) = concat (h_1, h_2, ..., h_p) w^o$ (4)\n$h_i = att (xW_q^i,xW_k^i,xW_v^i)$ (5)\nusing the learned parameter matrices with $W_{que}^i \\in \\mathbb{R}^{d \\times d} , W_{ke}^i\\in \\mathbb{R}^{d \\times d}, W_{Ua}^i\\in \\mathbb{R}^{d \\times d}$ affine projections as shown in Eq. (5). The MHSA mechanism's various heads each learn a different attention. Each head operates independently and concurrently. The scaled dot product attention is used to compute all attention distributions."}, {"title": "Gated Recurred Unit", "content": "A gated recurrent unit (GRU) is a type of recurrent neural network (RNN) that is used in natural language processing and other applications. A GRU is able to process sequential data, such as text or time series data, and make use of information from previous time steps to improve its predictions. GRUs are similar to long short-term memory (LSTM) networks, which are another type of RNN. However, GRUs have a simpler structure and fewer parameters, making them easier to train and potentially more efficient to run.\nA GRU consists of a \"gate\" that controls the flow of information into and out of the unit. The gate is a neural network layer that takes as input the current input and the previous hidden state, and produces a scalar value between 0 and 1 for each element in the hidden state. If the gate is close to 0, it means that the hidden state should be reset and the current input should be ignored. If the gate is close to 1, it means that the hidden state should be updated based on the current input and the previous hidden state. GRUs have been used in a variety of natural language processing tasks, such as language translation and text classification, and have achieved good results.\n$r_t = \\sigma (G_r x_t + W_r h_{t-1})$\n$z_t = \\sigma (G_z x_t + W_z h_{t-1})$\n$\\hat{h}_t = tanh (G_h x_t + W (r_t \\odot h_{t-1}))$\n$h_t = (1 - z_t) h_{t-1} + z_t \\hat{h}_t$\n(6)\nHere, x is the input vector, h is the output vector, $\\hat{h}$ is the candidate output, r is the reset gate, z is the update gate, G and Ware weight matrices and bias vectors. The sigma ($\\sigma$) and tanh functions are element-wise nonlinear activation functions. The reset gater and update gate z are both obtained using a sigmoid activation function, which produces scalar values between 0 and 1 for each element in the input. The output vector h is then computed using a combination of the previous hidden state, h, and the candidate output, $\\hat{h}$. A GRU has fewer gates and fewer parameters than an LSTM. GRU has only two gates: a reset gate and an update gate. The reset gate controls the extent to which the previous hidden state should be \"reset\" and ignored in the update process, while the update gate controls the extent to which the candidate output should be used to update the hidden state. This simplicity can also make it more efficient to run and potentially lead to better performance and faster convergence."}, {"title": "GeLU", "content": "The GELU (Gaussian Error Linear Unit) is a type of activation function that is used in neural networks. It is similar to the rectified linear unit (ReLU) in that it maps negative input values to zero, but it also modifies positive input values to produce a nonlinear output. The gradient vanishing problem affects the sigmoid function, and the ReLU function is statistically less motivated. Stochastic regularization, such as dropout, is frequently introduced to enhance the training of DNNs in order to address the issue of ReLU's lack of probabilistic interpretation. It is suggested to use GeLU to combine probabilistic regularization and an activation function. It is a typical Gaussian cumulative distribution function that, as opposed to using the input sign as in ReLU, introduces non-linearity onto the output of a DNN neuron based on their values as per Eq. (7) - Eq. (9),\n$fGeLU(a) = az\\Phi(A \\leq a)$ (7)\n$\\phi(a)$ (8)\n$0.5a (1+ref(\\frac{a}{\\sqrt{2}}))$ (9)\nwhere a and \u00d8(a) are the input to the activation function and cumulative distribution function N(0, 1), respectively."}, {"title": "Bi-LSTM", "content": "A bi-directional long short-term memory (LSTM) network is a type of recurrent neural network (RNN) that is trained to process sequential data in both forward and backward directions. A bi-directional LSTM consists of two separate LSTM networks, one that processes the input sequence in the forward direction and another that processes the input sequence in the backward direction. The outputs of the two networks are then concatenated and used to make predictions about the input sequence. Bi-LSTM and bidirectional recurrent neural network (Bi-RNN) both of which can process time series data in both directions. The network has LSTM hidden layers and outputs that are same in opposite directions. As per Eq. (10) \u2013 Eq. (15),\n$f_t = \\sigma(W_{Exf}x_t + W_{E_hf}h_{t-1}+ W_{Eg_f}m_{t-1} + d_{ef})$ (10)\n$i_t = \\sigma (W_{Exi}x_t + W_{E_hi}h_{t-1} + W_{Eg_i}m_{t-1} + d_{ei})$ (11)\n$O_t = \\sigma (W_{Exo}x_t + W_{E_ho}h_{t-1} + W_{Ego}m_{t-1} + d_{eo})$ (12)\n$\\hat{S}_t = tanh (W_{Exm}x_t + W_{E_x}h_{t} + d_{eg})$ (13)\n$S_t = f_{o_1} \\cdot S_{t-1} + i_t \\cdot \\hat{S}_t$ (14)\n$h_t = out tanh (S_t)$ (15)\nWhere, WE* denotes weight matrix, b*is defined as three gates deviation and the input transformer, tanh defines activation function, also known as the hyperbolic tangent function $f_t$ explains forgetting gate, act on $S_{t-1}$ to find transformer will be forgotten or not, $\\hat{S}_t$ defines new data $S_t$ is obtained from x\u1e6dand $h_{t-1}$, it represents input gate decides which data will be combined into the system memory and odefines output gate decides which data will be output after filtering the data in the memory."}, {"title": "ReLU", "content": "Bi-LSTM uses ReLU function, the rectified linear unit (ReLU) is a commonly used activation function in neural networks. It maps any input value less than zero to zero and any input value greater than or equal to zero to itself. ReLU is an activation function that is piece-wise linear and defined as per Eq. (16).\n$fReLU(a) = max(0, a) = \\begin{cases}a, if a \\geq0\\\\0, if a < 0\\end{cases}$ (16)\nwhere a serves as the activation function's input. ReLU maintains the input's dynamic range in the output when the input value is greater than zero. As a result, unlike the sigmoid function, it is not affected by the gradient vanishing issue. Additionally, compared to the sigmoid function, it provides better and faster convergence, which is why modern DNN systems with a variety of applications are very fond of it."}, {"title": "Graph Neural Network", "content": "An input for Graph Neural Networks (GNNs) is a graph, making them a unique subset of Neural Networks. Graphs can actually take on a variety of shapes, including those that are time-evolving, spatial, directed, undirected, labelled, and unlabelled. There are many GNN variants that have been developed to handle the high graph structure heterogeneity. However, a feature shared by the majority of GNNs is that the input graph occurs at multiple layers, defining the connectivity of the network itself, as opposed to being located at the first layer. Most often, several interaction blocks are stacked to create graph neural networks. Each block x = 1 .....X computes a graph representation $D_x \\in \\mathbb{R}^{d*e_x}$ where d is the number of nodes in the input graph and ex is the number of dimensions used to represent each node. The representation is created within a block by applying an aggregate step, in which each node receives information from the neighboring nodes, and a combine step, in which each node extracts new features. These actions link the representations of the subsequent blocks' Dx-1 and Dr numbers as shown in Eq. (17) and (18).\n$aggregate: G_x = AD_{x-1}$ (17)\n$combine : D_x (E_x (G_x,A)) \\ A$ (18)\nwhere A is the input graph given as a matrix of size d* d, e.g., the adjacency matrix to which add self-connections. Then denote by $G_x, A$ the row of $G_r$ associated to node A, and Ex is a 'combine' function, typically a neural network with one or more layers that creates a new representation for each node in the graph. The GNN's implementation of output can then be expressed as a function.\n$f (A; D_0) = h (D_x (A, D_{x-1} (A, . . . . . . ..D\u2081 (A, D_0))))$ (19)\nwhich is a recursive application starting from some initial state $D_0 \\in \\mathbb{R}^{d*e_0}$ followed by a readout function h. If no intrinsic information about the nodes is present, the initial state can either be set to constant values or can typically include intrinsic information. The readout function typically serves as a classifier for the entire graph, but it can also be configured to apply to specific subsets of nodes, in node classification or link prediction tasks. GNN is using the activation function named Leaky ReLU."}, {"title": "Leaky ReLU", "content": "Leaky ReLU is an alternative activation function to the standard ReLU function used in deep learning neural networks. ReLU is a popular activation function because it is computationally efficient and helps to reduce the vanishing gradient problem in deep networks. However, the standard ReLU function can result in neurons becoming inactive and no longer able to be updated. This is referred to as the \"dying ReLU\" problem. Leaky ReLU addresses this issue by allowing small negative values to pass through the activation function. The function is defined as f(x) = max(ax, x), where a is a small positive constant, typically set to a value between 0.01 and 0.1. The leaky part of the function refers to this small positive slope for negative input values. By allowing small negative values to pass through, the network is able to learn a wider range of features, improving its ability to generalize to new data. This can help the network to converge faster and produce better results."}, {"title": "HRAHA", "content": "A hybrid algorithm combining RFO and AHA would incorporate elements from both approaches to enhance the search capabilities and performance of the optimization process. RFO Algorithm brings the exploration and exploitation strategies inspired by the hunting behaviour of red foxes. This algorithm utilizes a population-based approach", "1": "The initial populations can be generated separately for each component and then combined to form the hybrid population. The size of the population and the parameter values for each individual can vary depending on the problem being solved. It is important to ensure sufficient diversity in the initial population to explore a wide range of solutions effectively.\nStep 2: The fitness values reflect the performance of each individual from both components in solving the optimization problem. The fitness values can be combined, weighted, or compared to determine the overall fitness of each individual in the hybrid population. The fitness computation process is crucial as it provides a quantitative measure of how well each individual performs in solving the optimization problem. It serves as a basis for the subsequent stages of the algorithm, such as selection, crossover"}]}