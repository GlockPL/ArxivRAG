{"title": "LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback", "authors": ["Tanushree Banerjee", "Richard Zhu", "Runzhe Yang", "Karthik Narasimhan"], "abstract": "Large Language Models (LLMs) excel at generating human-like dialogues and comprehending text. However, understanding the subtleties of complex exchanges in language remains a challenge. We propose a bootstrapping framework that leverages self-generated feedback to enhance LLM reasoning capabilities for lie detection. The framework consists of three stages: *suggestion*, *feedback collection*, and *modification*. In the *suggestion* stage, a cost-effective language model generates initial predictions based on game state and dialogue. The *feedback-collection* stage involves a language model providing feedback on these predictions. In the *modification* stage, a more advanced language model refines the initial predictions using the feedback. We investigate the application of the proposed framework for detecting betrayal and deception in Diplomacy games, and compare it with feedback from professional human players. The LLM-generated feedback exhibits superior quality and significantly enhances the performance of the model. Our approach achieves a 39% improvement over the zero-shot baseline in lying-F1 without the need for any training data, rivaling state-of-the-art supervised learning results.", "sections": [{"title": "1 Introduction", "content": "While Large Language Models (LLMs) excel at text generation and dialogue, it is unclear how much they can understand subtle nuances in human communication, such as lying or deception. LMs that can flag deception can be potentially useful in various applications like chat bots and virtual assistants, and even assist humans in challenging circumstances. In our paper, we take a first step towards evaluating and enhancing the ability of LLMs to detect deception using real player conversations within the game of Diplomacy. We first design prompts that make use of the game state, order information and conversation history, and ask 175B GPT-3 (Brown et al., 2020) and 1.7T GPT-4 (OpenAI, 2023a) models to predict statements that are lies. We find that the zero-shot performance of both models are similar, and much worse than a state-of-the-art LSTM-based model trained with supervised learning from previous work (Peskov et al., 2020) on macro and lying-F1 scores.\nTo improve performance, we design a bootstrapping reasoning method that utilizes LLMs to self-generate feedback on initial predictions, which can then be used to generate modified predictions. Our framework (Figure 1) consists of a *suggestion* stage where a cost-effective base LLM makes initial predictions, a *feedback collection* stage, where an LLM provides feedback on the predictions, and finally a *modification* stage, where a more advanced LM refines the initial predictions using the feedback. The feedback provided is in natural language and contains information on 1) systematic errors made by the LLM in the suggestion stage, and 2) opinions or suggestions for minimizing false negatives. Importantly, in contrast to other works that use LMs for self-evaluation (Zelikman et al., 2022; Shinn et al., 2023), the feedback LLM in our setup has no access to the ground truth answers.\nWe evaluate our approach on the Diplomacy conversations dataset collected by Peskov et al. (2020) and compare with several baselines as well as using expert human players for providing feedback in the second stage. Our bootstrapping method helps increase lying-F1 scores by 39% over the base LLM while also enabling zero-shot GPT-4 to rival the previous state-of-the-art supervised LSTM model of Peskov et al. (2020) on this task. Perhaps surprisingly, self-generated LLM feedback significantly outperforms even the best human feedback by 29% and seems to be most useful in cases where humans are unsure or disagree in their assessment. We also"}, {"title": "2 Related Work", "content": "Prior work on developing bootstrapping techniques to enhance LLM's capabilities without extensive supervision has involved leveraging existing models to generate data or provide guidance for further refinement. Ulmer et al. (2024) generate training data through \u201cself-talk\u201d of LLM agents, that is, LLM agents generate data in conversation with themselves, which can be refined and used for supervised fine-tuning (Ulmer et al., 2024). Zhang et al. (2023) automatically learn to solve new long-horizon, complex, and meaningful tasks through \"skill bootstrapping\", where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set (Zhang et al., 2023). This \u201cbootstrapping\" phase is guided by LLMs that inform the agent of meaningful skills to link together (Zhang et al., 2023). Weng et al. (2023) show that LLMs have \u201cself-verification\" abilities similar to humans, and propose backward verification of LLM-deduced answers to select a candidate of the highest score as a method to improve performance on several arithmetic, commonsense, and logical reasoning datasets (Weng et al., 2023). Compared to prior works, our proposed method is novel in that it uses a cheaper model to provide feedback. Our analysis shows that this effectively improves the system's performance on the lie detection task in the game of Diplomacy."}, {"title": "3 The Bootstrapping Framework for LLM-based Reasoning", "content": "We investigate the potential of large language models (LLMs) to enhance the reliability of their pre-dictions through the utilization of self-generated feedback. The bootstrapping framework, depicted in Figure 1, comprises three stages: 1) *suggestion*, 2) *feedback generation*, and 3) *modification*.\nIn the *suggestion* stage, we employ a pre-trained LLM, denoted as \\(M\\), along with a dataset \\(D := (x_i, y_i)_i\\), where \\(y_i\\) represents the target output for a textual input \\(x_i\\). Initially, we generate initial predictions as a sequence completion task using the LLM, i.e., \\(\\hat{y_i} \\sim P_M(x_i)\\). While these initial predictions, \\(\\hat{y_i}\\), may serve as a reasonable starting point, they can potentially reveal biases or systematic errors (Zhao et al., 2021; Wei et al., 2022).\nFor our specific lie-detection task in the Diplomacy game, the textual inputs \\(x_i\\) consist of a concise introduction to the game rules, board information, a conversation between two players during a season, and an instruction prompt, \"Which message numbers are lies? Provide reasons.\". The target output \\(y_i\\) includes the message numbers annotated as lies by the message sender.\nIn the *feedback collection* stage, we solicit additional feedback \\(f_i\\) from an LLM regarding the initial predictions \\(\\hat{y_i}\\), i.e., \\(f_i \\sim P_M(\\cdot|x_i, \\hat{y_i}, p_f)\\), where \\(p_f\\) represents the instruction prompt for generating feedback. The complete instruction prompt for this stage is provided in Appendix A.4.\nTo facilitate comparison, we also request human experts to provide feedback on the same initial predictions, i.e., \\(f_h \\sim H(\\cdot|x_i, \\hat{y_i}, p_f)\\). Typically, the feedback \\(f_i\\) contains two types of information: 1) observations on systematic errors made by the LLM in the *suggestion* stage, aiding in reducing false positive predictions, and 2) opinions regarding the appropriate response to \\(x_i\\), helping to minimize false negative predictions. Neither the LLM nor the human feedback providers have access to the ground truth \\(Y_i\\).\nIn the *modification* stage, we prompt the LLM to revise its initial predictions \\(\\hat{y_i}\\) based on the generated feedback \\(f_i\\), i.e., \\(y_e \\sim M(\\cdot|x_i, \\hat{y_i}, f_i, p_m)\\), where \\(p_m\\) represents the instruction prompt starting with \"Given this feedback on the initial predictions, which messages do you still consider as lies? ..\". We provide the complete modification stage prompt in Appendix A.5.\nWe utilize a rule-based extractor to obtain the message numbers predicted as lies from the suggestion and modification stage predictions, \\(\\hat{y_i}\\) and \\(y_e\\), respectively, and compare their performance. Results and analysis are presented in Section 6."}, {"title": "4 Feedback Collection", "content": "To compare the quality of LLM-generated feedback with feedback written by human experts, we collect human feedback on *suggestion* stage predictions. The feedback dataset is released with our paper at this link."}, {"title": "4.1 Human feedback collection", "content": "We recruit three expert Diplomacy players (denoted as {Human1, Human2, Human3}) who are active members of the Diplomacy community that compete in online and in-person tournaments to provide feedback on the LLM *suggestion* stage output. We provide each of our recruited human expert players with a conversation between a pair of players in a turn of the Diplomacy game, the territories under control of each country in the game at the start of that turn, and orders submitted at the end of the turn. We then provide each human expert with the LLM's output from the suggestion stage and ask the human expert to provide feedback on the LLM's suggestion stage output in natural language. We also provide some sample responses, and include the exact instructions and samples provided in Appendix B. However, we do not stress following the examples heavily as we wanted to collect natural language feedback rather than feedback that followed any specific template.\nEach human expert took around 4 hours to provide the feedback annotations for 102 conversations (915 messages) in our test set described in section 5. We paid the human experts US$20 per hour and a US$20 bonus upon completion. Thus, we spent US$100 on each expert for a US$300 total expenditure on feedback collection.\nThe feedback lengths differ among three human experts. As depicted in Figure 2, Human1 tends to provide longer feedback compared to the other two experts, with an average feedback length exceeding 50 words. Notably, Human3 submits a single feedback instance exceeding 250 words. Additionally, Figure S1 in the Appendix reveals that the feedback from Human1 and Human3 more effectively enhances the predictions in the modification stage compared to the feedback from Human2."}, {"title": "4.2 LLM feedback generation", "content": "We utilize OpenAI's GPT-3.5 (gpt-3.5-turbo) (OpenAI, 2023b) and GPT-4 (gpt-4) (OpenAI, 2023a) models to generate feedback. We only perform inference once for each input and collect"}, {"title": "5 Experiments", "content": "The suggestion stage outputs from GPT-3 were obtained in January 2023, while the remaining experiments were run in June 2023. Below, we provide descriptions for each step of our framework.\nSuggestion stage. For the initial predictions, we employ OpenAI's GPT-3 (text-davinci-003) model. The input for each example in our test set includes: 1) Diplomacy game information, 2) Board state information during the turn, and 3) A conversation between two players during the turn (see Appendix A.1 -A.3). This is followed by the prompt *\"Which message numbers are lies? Provide reasons why.\".* To quantify the performance, we repeat the zero-shot prediction for 5 independent trials, and only one fixed trial of model outputs is used for"}, {"title": "6 Results & Analysis", "content": "Our key findings are summarized in Figure 3, where the error bars indicate the 95% confidence interval derived from 5 independent runs."}, {"title": "6.1 Main Results", "content": "The feedback obtained from both GPT-3.5 and GPT-4 demonstrates a significant improvement in both macro and lying F1 scores compared to the zero-shot predictions of GPT-4. Specifically, the macro F1 score improves by 4.07% and 7.96% respectively, while the lying F1 score improves by 30.4% and 38.7% respectively (refer to Figure 3). This highlights the effectiveness of our framework, which involves utilizing a cost-effective model like GPT-3 for initial predictions in the suggestion stage and subsequently refining the output using feedback from a potentially more expensive model such as GPT-4 in the modification stage. Our approach outperforms the zero-shot performance of GPT-4 in detecting lies. Furthermore, even when employing GPT-3.5 in the modification stage, we observe a significantly better performance than the zero-shot performance of GPT-4, which is a more expensive model. Therefore, our proposed method of suggestion-feedback collection-modification leads to significantly enhanced performance compared to the zero-shot performance of GPT-4, even when utilizing cheaper models for the suggestion and modification stages."}, {"title": "6.2 Analysis", "content": "We analyze the relationship between model performance and feedback quality by examining the feedback consistency in Section 6.2.1. In addition, we conduct an ablation study to further investigate the impact of feedback in Section 6.2.2."}, {"title": "7 Conclusion", "content": "In this work, we introduce a novel bootstrapping framework that utilizes feedback generated by LLMs to enhance the reasoning capabilities of base LLMs for nuanced natual language tasks. We specifically explore the application of this framework in detecting betrayal and deception in Diplomacy games and compare the effectiveness of LLM-generated feedback with feedback provided by professional Diplomacy players.\nOur findings demonstrate that LLM-generated feedback exhibits superior quality and significantly improves the model's ability to detect lies. By incorporating LLM-generated feedback, our proposed approach achieves a remarkable 39% improvement in lying-F1 score without requiring any additional training data, effectively competing with state-of-the-art supervised learning-based approaches. Furthermore, when compared to feedback generated by human experts, LLM-generated feedback tends to be longer and provides more informative insights about potential missing predictions. Notably, LLM-generated feedback outperforms human feedback by 29% in lying-F1 score, while also being a more cost-effective solution. These results highlight the potential of leveraging LLM-generated feedback to enhance model performance, offering a more economical alternative."}, {"title": "Limitations", "content": "OpenAI's GPT-4 model is not yet open-source, and inference cannot on GPT-4 be run freely by everyone. This lack of free and public access limits the degree to which our work can be freely reproduced. Moreover, only three human experts are involved in our study, which is a relatively small sample size. However, our annotation task requires high-level domain knowledge, i.e. detailed understanding of the strategy and dynamics of the game Diplomacy, so we compromise on the sample size in order to preserve the high quality of human feedback. Our human feedback givers are skilled Diplomacy players who are active members of the Diplomacy playing community, so the feedback we have collected"}, {"title": "Ethics", "content": "Studying deception can unintentionally have a double-edged effect of improving deception. However, since the language here is structured around the game of Diplomacy, any deception involved should not have major-real world consequences. Moreover, we empirically observe the feedback collected from both humans and LLMs in our study in order to check for toxic language, offensive content or text that uniquely identifies any individuals. Based on our empirical observations, our collected feedback does not uniquely identify individuals. The collected feedback did not contain any toxic or offensive language, thereby reducing the likelihood of misuse."}, {"title": "A Additional Experimental Details", "content": "Temperature during inference is the only hyperparameter we use. For consistency, we use a sampling temperature of 0.7, and run inference 5 times for each input at each stage in order to compute the mean macro and lying F1 scores with a 95% confidence interval bound. Setting temperature at 0.7 reduces the variance of the outputs while remaining creative. We did not tune temperature systematically. The suggestion stage outputs from GPT-3 (text-davinci-003) were obtained in January 2023, while the remaining experiments were run in June, 2023. Below, we give details for each step of out proposed framework. We use OpenAI's application program interface to access GPT series models, usage of which is governed by an OpenAI license granting all rights to any input and output. We abide by all OpenAI usage policies (OpenAI, 2023c).\nSuggestion stage We use OpenAI's GPT-3 (text-davinci-003) model to obtain initial prediction. We only run inference once for each input in this stage, and collect the corresponding raw model output to be used in the subsequent stages. For each example in our test set, the input consists of 1) information about the Diplomacy game (see Appendix A.1), 2) information about the state of the board during the turn (see Appendix A.2) and 3) a conversation between a pair of players during the turn (for a sample conversation and the exact structure, see Appendix A.3). This is followed by the prompt \"Which message numbers are lies? Give reasons why.\".\nFeedback collection stage We collect and evaluate the performance of feedback generated by both OpenAI's GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4) model. We only run inference once for each input in this stage, and collect the corresponding raw model output to be used in the subsequent stages, and call this raw model output the model's \"feedback\". For each example in our test set, the input consists of 1) a system message describing the goal of the model, i.e. to provide feedback on GPT-3's output from the suggestion stage (see Appendix A.4 for the exact system message used) and 2) a user message consisting of the input given to GPT-3 during the suggestion stage, along with the output from the suggestion stage. This is followed by the prompt \"Given the information given to GPT-3 about the game Diplomacy and the state"}, {"title": "A.1 Game information", "content": "The Diplomacy game is a strategy game played between 7 countries forming and breaking alliances by lying tactfully to gain more territories. Given the board information below and a conversation between France and Germany in Winter 1902 in the game Diplomacy, your goal is to determine which messages, if any, in their conversation are lies. Each territory is represented by a three-letter acronym (e.g. Paris \u2013> Par). The game score delta represents the difference in the number of territories occupied by France and Germany.\nAt each season, every country has a chance to place 4 possible types of orders:\n1. Move: move a unit from one territory to another\n2. Hold: keep a unit in the same territory\n3. Support: support a move or hold by another country\n4. Build: build a unit in a territory\nThe Move, Hold and Support orders can only be placed in the Fall and Spring seasons. The Build order can only be placed in the Winter season.\nThe board information given below includes the territories under control of each country at the start of this season, the orders submitted by each country at the end of this season, the territories adjacent to each country at the start of this season, in this order."}, {"title": "A.2 Board information", "content": "The board information includes the following details.\n1. Territories occupied by each country at the start of the turn\n2. Orders submitted by each country at the end of the turn\n3. Territories adjacent to the territories occupied by each country at the start of the turn\nAn example from Winter 1902:"}, {"title": "A.3 Conversation structure", "content": "All messages in our test set are split up into *\"conversations\"*. Each conversation consists of messages sent between a pair of countries in the same turn. In the case where there were too many messages sent between a pair of countries in the same turn, such that the messages were too long to fit into the context window size of GPT-3, we split the messages into two separate *\"conversations\"*. These messages are 0-indexed and concatenated together into a single string. Our test set of 915 messages contains 102 conversations.\nA sample conversation:\nConversation between France and Germany in Spring 1902 in the game, Diplomacy:"}, {"title": "A.4 System message for feedback generation stage", "content": "You are an expert in the game of Diplomacy, and your goal is to give feedback on GPT-3's prediction and reasoning for which messages are lies in"}, {"title": "A.5 Modification stage prompt", "content": "Based on the above feedback, which messages do you still think are lies? Note that you may reject the given feedback if you disagree with it. Moreover, if the feedback says that your logic is wrong, that does not mean that your initial prediction is also wrong; you can still make the same prediction and simply correct your logic based on the given feedback."}, {"title": "B Instructions to human annotators for feedback collection", "content": "We emailed a few members from the Diplomacy community about an opportunity to help annotate AI generated predictions for lies in conversation excerpts from a single game of Diplomacy. In our email, we advertised a hourly pay of USD 20 per hour in exchange for help in annotating our dataset in order to advance NLP research. Of the Diplomacy players we emailed, three of them volunteered to help us annotate our dataset in exchange"}, {"title": "CLLM Common Errors", "content": "We asked one of our three expert human feedback annotators to summarize common mistakes they noticed that GPT-3 made in its output from the suggestion stage. The comments made by our expert annotator are listed below."}, {"title": "D Libraries Used", "content": "We use the following libraries in our code: huggingface {hub, tokenizers, tranformers} (Wolf et al., 2020), numpy (Harris et al., 2020), diplomacy (Paquette et al., 2019b), pandas (Wes McKinney, 2010), openai, seaborn (Waskom, 2021), sklearn (Pedregosa et al., 2011), scipy (Virtanen et al., 2020)"}, {"title": "E Feedback Consistency Analysis", "content": "To provide the labels for consistency between pairs of feedback both effectively and economically, we follow a two step process. We pass the following messages to GPT-4 in a chat completion setting, in order to compare whether two messages are consistent with each other:"}, {"title": "F Additional Details of Successive Round of Feedback", "content": "To explore the potential for further improvement in model performance, we introduce an experiment involving a second round of feedback collection and modification. In this experiment, we incorporate an additional feedback-collection stage, where GPT-4 generates feedback on its output from the first modification stage. Subsequently, a second modification stage is conducted, wherein GPT-4 refines its output from the first modification stage based on the feedback obtained from the second feedback collection stage.\nThis addition to our proposed framework includes the following two additional stages."}, {"title": "G Additional Results", "content": null}]}