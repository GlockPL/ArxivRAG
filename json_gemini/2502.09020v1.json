{"title": "EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition", "authors": ["Xiao Wang", "Jingtao Jiang", "Dong Li", "Futian Wang", "Lin Zhu", "Yaowei Wang", "Yongyong Tian", "Jin Tang"], "abstract": "Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB cameras which are sensitive to challenging factors such as low illumination, motion blur, and cluttered backgrounds. In this paper, we propose to recognize the scene text using bio-inspired event cameras by collecting and annotating a large-scale benchmark dataset, termed EventSTR. It contains 9,928 high-definition (1280 \u00d7 720) event samples and involves both Chinese and English characters. We also benchmark multiple STR algorithms as the baselines for future works to compare. In addition, we propose a new event-based scene text recognition framework, termed SimC-ESTR. It first extracts the event features using a visual encoder and projects them into tokens using a Q-former module. More importantly, we propose to augment the vision tokens based on a memory mechanism before feeding into the large language models. A similarity-based error correction mechanism is embedded within the large language model to correct potential minor errors fundamentally based on contextual information. Extensive experiments on the newly proposed EventSTR dataset and two simulation STR datasets fully demonstrate the effectiveness of our proposed model. We believe that the dataset and algorithmic model can innovatively propose an event-based STR task and are expected to accelerate the application of event cameras in various industries. The source code and pre-trained models will be released on https://github.com/Event-AHU/EventSTR.", "sections": [{"title": "I. INTRODUCTION", "content": "Scene Text Recognition (STR), often referred to as Optical Character Recognition (OCR) in the context of images or videos, is the process of detecting and recognizing text that appears in real-world photographs taken from arbitrary viewpoints and conditions. This technology enables machines to \"read\" text within scenes, which can be valuable for various applications such as data entry automation, translating documents, and enhancing the accessibility of digital content. Usually, the STR model is developed for RGB cameras which suffer from low illumination, cluttered backgrounds, fast motion, etc, as shown in Fig. 1 (a-c).\nThe performance of scene text recognition has been boosted significantly in the deep learning era with the release of large-scale RGB images or synthetic data. For example, methods like PARSeq [1] and MGP-STR [2] utilize attention mechanisms to model character sequences effectively, achieving high accuracy on benchmark datasets. Inspired by the success of the large models in natural language processing, some researchers also exploit to recognize the scene text using large vision-language models. Specifically, methods like mPLUG-DocOwl [3], TextMonkey [4], and DocPedia [5] leverage large vision-language models for scene text recognition, enhancing text-image interaction and document understanding. However, the inference in the practical scenarios is still unsatisfied due to the aforementioned issues.\nRecently, event cameras draws more and more attention in the computer vision community. Many researchers attempt to introduce event cameras to help or even replace the RGB cameras for their tasks, such as object detection and tracking [6] [7] [8], pattern recognition [9] [10], semantic segmentation [11], etc. Many works demonstrate the effectiveness and advantages of event cameras on low power consumption, low latency, high temporal resolution, and high dynamic range. The fundamental reason lies in that the event cameras emit a spike/event point (x,y,t,p) only when the variation of corresponding pixels is beyond the certainty threshold. Here, (x, y) denotes the spatial coordinates, t is the timestamp and p denotes polarity (i.e., positive or negative event). The scene text recorded using an event camera is visualized in Fig. 1 (d).\nConsidering the features and advantages of event cameras for perception, in this paper, we formally propose event stream based scene text recognition by providing a large-scale benchmark dataset and a large language model based event STR framework. The event-based STR dataset EventSTR contains 9,928 samples that fully reflect the key features of event cameras. More in detail, these event samples are collected under different lighting conditions, motions, occlusions, scene categories, and text orientations. Also, the collected data has a resolution of 1280 \u00d7 720, which can effectively support research on processing high-resolution neural networks. In addition, we also provide multiple baselines for this dataset which will be useful for future works to compare. Some representative samples of the EventSTR are visualized in Fig. 3.\nOn the basis of the newly proposed dataset, we further propose a new baseline approach for event-based scene text recognition, termed SimC-ESTR. Specifically, given the event stream, we first obtain its feature embeddings using a vision backbone network, meanwhile, we also adopt the Q-former module to transform the vision features to better adapt to the large language model. The vision features obtained from simple projection and Q-former modules are fed into the pre-trained large language model (LLM) together with generation prompts. We also introduce the memory mechanism to augment the visual features based on context samples. Additionally, we find that text recognition in scenarios involving Chinese characters is often susceptible to interference from visually similar characters. For the character\u201c\u67ab\u201d,its visually similar characters include \u201c\u677e\u201d, \u201c\u67cf\u201d, \u201c\u67f3\u201d, and \u201c\u6768\u201d. Therefore, we design a new similar word database to help the LLM refine the generated text due to the homophonic Chinese characters. Extensive experiments on three benchmark datasets fully validated the effectiveness of our proposed modules for the large language model based event scene text recognition.\nTo sum up, we draw the contributions of this paper as the following three aspects:\n1). We propose for the first time the task of scene text recognition based on event cameras, aiming to address challenging factors such as low illumination, complex backgrounds, and motion blur. To support this, we have constructed a large-scale high-definition event stream scene text recognition database, termed EventSTR.\n2). We have developed a framework for event stream scene text recognition based on large language models, termed SimC-ESTR. It simultaneously incorporates a memory mechanism for contextual sample augmentation and visually similar word error correction, achieving superior recognition accuracy.\n3). We provide an extensive benchmark involving multiple state-of-the-art scene text recognition algorithms. Additionally, we conducted detailed experimental analyses on three datasets, fully verifying the effectiveness of the proposed method.\nThe rest of this paper is organized as follows: In Section II, we give a review of the related works including scene text recognition, large language model based OCR, and event-based vision. Then, we propose the key frameworks"}, {"title": "II. RELATED WORKS", "content": "In this section, we review the related works on Scene Text Recognition, LLM-based OCR, and Event-based Vision. More related works can be found in the following surveys [12]\u2013[14] and paper list 1.\nA. Scene Text Recognition\nScene text recognition [15]-[18] naturally involves both vision and language processing. E2STR [19] enhances adaptability to diverse scenarios by introducing context-rich text sequences and applying a context training strategy, enabling flexibility in recognizing texts across different environments. Guan et al. propose CCD [20], a self-supervised character-to-character distillation method, which learns robust text feature representations via a self-supervised segmentation module and flexible augmentation techniques. SIGA [21] optimizes self-supervised segmentation and implicit attention alignment to improve attention accuracy, though it is constrained when character-level annotations are insufficient. CDistNet [22] incorporates visual and semantic positional embeddings into its transformer-based architecture, offering improvements but still facing difficulties with irregular or dense text layouts and complex backgrounds, limiting its generalization capacity. In contrast, another group of approaches focuses on using language models for iterative error correction in scene text recognition. These methods refine recognition results by correcting errors during inference, resulting in more robust and interpretable systems. Recent models, such as VOLTER [23], BUSNet [24], MATRNet [25], LevOCR [26], and ABINet [27], integrate language models for this iterative correction. Inspired by these models, in this paper, we also exploit large language model based scene text recognition using an event camera."}, {"title": "III. OUR PROPOSED APPROACH", "content": "In this section, we will first give an overview of our framework. Then, we focus on the detailed network architectures, including the Visual Encoder, Memory Mechanism, Glyph Error Correction Module, and Pre-trained Large Language Model. After that, we describe the loss function used for the optimization of our framework.\nA. Overview\nConsidering that existing scene text recognition algorithms are mostly based on RGB frames, in order to better adapt to these models, we also adopt the approach of stacking event streams into event frames for experimentation in this paper. Specifically, we first stack them into a single event frame by following the method used in EventVOT [7]. Then, we adopt a visual encoder to embed the input into feature representations. The features are fed into the Q-former to align the vision tokens and large language model and the output tokens are fed into a pre-trained large language model for text generation. Meanwhile, we propose to utilize the memory mechanism to augment the features further. This is mainly because text symbols have similarities, and the text symbols in the contextual samples can also provide a reference for the representation of the current symbol. We have observed that large language models sometimes output incorrect Chinese characters, but these characters are indeed very similar to the correct ones, i.e., they are visually similar characters. Therefore, we have designed a set of visually similar character correction modules to help large language models produce more accurate text recognition results. More details will be introduced in the subsequent subsections.\nB. Input Representation\nThe event stream $E = \\{e_1,e_2,..., e_n\\}$ can be seen as a spatial-temporal flow similar to the point cloud [34], each event point $e_i$ can be denoted as $[x, y, t,p]$. Here, N is the number of points in a single event stream. (x, y) is the spatial coordinates, t is the timestamp, $p \\in \\{1,-1\\}$ denotes the polarity (positive/negative) of the event point. As mentioned in the previous section, we stack the event streams into event frames $I \\in \\mathbb{R}^{T\\times C\\times H\\times W} \\{I_1, I_2, ..., I_T\\}$ to better adapt existing STR models for benchmark comparison, where T is the number of stacked event frames. Stacking event streams into frames akin to RGB frames offers key advantages such as compatibility with existing algorithms and toolchains, explicit modeling of temporal information, improved spatial feature extraction efficiency, mitigation of data sparsity and noise issues, and support for intuitive visualization, making it a practical and efficient solution well-suited for scenarios requiring rapid development and deployment. Due to the utilization of a large language model, in this work, we also take a generation prompt $P$ as the input, i.e., \"What is the text in the image?\". The LlamaTokenizer [35] is used to get the text embeddings $F_i$ for further processing."}, {"title": "C. Network Architecture", "content": "The key modules of our proposed SimC-ESTR framework are the Visual Encoder, Memory Module, Pre-trained LLM, and Glyph Error Correction Module, as shown in Fig. 2.\n\u2022 Visual and Prompt Encoder. Given the event frames I, we adopt the pre-trained EVA-CLIP [36] model (ViT-G/14) as the visual encoder for feature extraction. Specifically, it processes input images by dividing them into fixed-size patches (14\u00d714 pixels), which are flattened into tokens for long-range spatial representation. The key operator is the multi-head self-attention mechanism which focuses on discriminative features and the output visual feature can be denoted as $F_V$. It also outputs a global token [CLS] for representation of the whole input frame. These visual features are further refined by the Q-Former and projected into the LLM for final text recognition. \nFor the generation prompt P, we propose the text encoder to guide the model in understanding and recognizing the visual content from the event-based image. The textual prompt is tokenized and transformed into a text embedding. Here, the prompt is tokenized into token IDs, which are then passed through different tokenizers depending on the part of the model. The first tokenizer processes the prompt for use by the Q-Former, which integrates visual and textual information. The second tokenizer prepares the prompt for the LLM, which generates the final text predictions. The tokenized prompt $F_i$ is used by both the Q-Former and the LLM, where the embedding representations may differ due to optimization for their respective components. The tokenization process ensures that the prompt is properly formatted for integration with the model components and their attention mechanisms, enabling smooth interaction between textual and visual features. By using the tokenized prompt, we ensure the efficient integration of textual context with event-based visual features, optimizing the model's performance in generating accurate scene text recognition results.\n\u2022 Memory Module. It is designed to enhance the model's ability to capture long-term dependencies by leveraging a pattern-based memory mechanism. It consists of a set of learnable memory patterns that are used to improve the input features through mapping, similarity matching, and feature enhancement. More in detail, the input visual features, which have dimensions $B \\times L \\times D$ (where B represents the batch size, L is the sequence length, and D denotes the feature dimension), are reshaped and passed through a linear layer. This linear layer projects the features into a lower-dimensional space, specifically into a 128-dimensional space, which corresponds to the predefined pattern dimension. After that, the module computes the cosine similarity between the projected input features and a set of stored memory patterns. These patterns, which are initialized randomly and are learnable, capture key visual representations learned during training. The module selects the top-K most similar patterns from the memory, and these patterns are transformed back into the original feature space using another linear layer. This process generates a set of enhanced features.\nThe final output is obtained by adding the weighted average"}, {"title": "IV. EVENTSTR BENCHMARK DATASET", "content": "In this paper, we introduce a new event-based scene text recognition dataset, termed EventSTR. The following paragraphs provide a detailed description of the data collection and annotation process, statistical analysis, and the benchmark protocols for visual trackers.\nA. Protocols\nWe aim to provide a new direction for scene text recognition using event-based data. The EventSTR benchmark dataset was constructed adhering to the following protocols: 1). Lighting Conditions: The dataset was captured under challenging low-light conditions, where traditional image-based methods would struggle. However, thanks to the high sensitivity of the event camera, text remains clearly visible even in dark scenes with low light intensity. 2). Motion Variability: The dataset includes images captured at varying motion speeds, resulting in scenarios where text may appear blurred or distorted due to motion, adding complexity to text recognition tasks. 3). Occlusion: The dataset features images with varying levels of occlusion, where portions of the text may be obstructed, making the recognition task more difficult. 4). Scene Categories: A wide range of scene categories is included, such as posters, books, commodities, billboards, and license plates."}, {"title": "V. EXPERIMENTS", "content": "A. Dataset and Evaluation Metric\nFor the datasets, we evaluate our model alongside other state-of-the-art methods on three main datasets: WordArt* 3, IC15* 4, and our newly introduced EventSTR. Below is a brief introduction to each of these datasets. Details of the other datasets are provided in Table I.\n\u2022 WordArt* Dataset: As shown in Fig. 6, this dataset is derived from the original RGB-format WordArt [53] dataset and simulated into event-based images using event camera simulator (ESIM) [57]. It is split into a training set of 4,805 images and a validation set of 1,511 images. The dataset contains artistic text images, including posters, greeting cards, covers, billboards, handwritten texts, and more. These images feature a variety of artistic text styles.\n\u2022 IC15* Dataset: This dataset is created by transforming the original RGB-format IC15 [44] dataset into event-based images. IC15* is a natural scene text dataset, consisting of 4,468 training images and 2,077 testing images.\nFor the evaluation metrics, we use BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores to assess performance on the EventSTR dataset, which involves multi-text scenarios. BLEU scores are calculated by segmenting characters for Chinese text and by words (case-insensitive) for English text. For the WordArt* and IC15* datasets, we employ the word-level recognition accuracy.\nB. Implementation Details\nWe use the pre-trained weights from BLIVA [58], followed by fine-tuning on our dataset. The AdamW [59] optimizer was employed, with \u03b2\u2081 = 0.9, \u03b22 = 0.999, and a weight decay of 0.05. Additionally, we apply a linear warm-up for the learning rate over the first 1,000 steps, gradually increasing it from 10-8 to 10-5, followed by a cosine decay to a minimum"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose a novel event stream based scene text recognition task. A large-scale benchmark dataset is proposed for this research problem, termed EventSTR, which targets achieving high-performance and robust scene text recognition. The videos are collected using a high-definition Prophesee event camera and involve both Chinese and English text recognition. We also provide multiple baselines for this benchmark dataset and believe it will pave a new road for the event-based STR. In addition, we also propose a large language model based text recognition framework equipped with an error correction module and memory mechanism. Extensive experiments on multiple benchmark datasets fully validated the effectiveness of our proposed STR framework.\nIn future works, we will exploit new knowledge distillation strategies based on the SimC-ESTR framework to make it more lightweight and hardware-friendly. Also, the different efficient and low-latency event representations will also be an interesting research direction for the high-definition event-based STR task."}]}