{"title": "TRACE: Transformer-based Risk Assessment for Clinical Evaluation", "authors": ["Dionysis Christopoulos", "Sotiris Spanos", "Valsamis Ntouskos", "Konstantinos Karantzalos"], "abstract": "We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation), a novel method for clinical risk assessment based on clinical data, leveraging the self-attention mechanism for enhanced feature interaction and result interpretation. Our approach is able to handle different data modalities, including continuous, categorical and multiple-choice (checkbox) attributes. The proposed architecture features a shared representation of the clinical data obtained by integrating specialized embeddings of each data modality, enabling the detection of high-risk individuals using Transformer encoder layers. To assess the effectiveness of the proposed method, a strong baseline based on non-negative multi-layer perceptrons (MLPs) is introduced. The proposed method outperforms various baselines widely used in the domain of clinical risk assessment, while effectively handling missing values. In terms of explainability, our Transformer-based method offers easily interpretable results via attention weights, further enhancing the clinicians' decision-making process.", "sections": [{"title": "Introduction", "content": "Healthcare is an industry that can gain significantly by utilizing modern artificial intelligence (AI) and machine learning (ML) methods by assisting clinicians in diagnosis, risk assessment and pathology of diseases. By incorporating AI-driven risk assessment algorithms ([1, 2, 3]), clinicians can offer risk stratification of the patients for screening recommendations, which can help in early detection of diseases, enabling better informed decision-making from the clinicians, on the one hand, and timely intervention for patients that are considered high-risk, on the other.\nHealthcare data, typically collected using questionnaire-based surveys, exhibit a large diversity both in the nature, the quantity and the completeness of the attributes that are recorded (or reported) for each patient. Importantly, clinical data are multi-modal, comprising a combination of numerical (i.e., age, height, weight etc.) and categorical features (i.e., eye color, hair color, etc.) or even \"checkboxes\", where multiple values within the same feature are valid simultaneously (i.e., ancestry, doctors visited, etc.). On the other hand, missing values and other issues affecting clinical, and tabular data in general, also pose a significant challenge for maximizing data utilization. This is crucial in the case of healthcare data, as they are generally scarce, and their collection is laborious and with high cost, while AI, and deep-learning in particular, typically assume a large quantity of available data for training representative models. Finally, another characteristic of clinical data is the notable imbalance between case and control groups."}, {"title": "Related Work", "content": "Risk assessment models have been developed in the healthcare domain, providing risk scores for health condition diagnoses, based on personal clinical data. Logistic regression is a widely used method for such tasks (i.e., primary melanoma classification) [6, 7]. Another study [8] employs various machine learning classifiers like Decision Trees, Polynomial/Rbf/Linear SVM, Naive Bayes, Random Forest and Logistic Regression, as well as a neural network model, aiming to predict type 2 diabetes risk and identifying associated risk factors. Regarding the predictive accuracy, the neural network model, as expected, showcased the best results, suggesting that deep learning models can potentially develop critical decision-making abilities in medical tasks, to enhance prevention on various healthcare chronic conditions. Similarly, [9] employs a single hidden layer MLP network for cardiovascular disease risk prediction, although stating its weak interpretability.\nRecent works such as [5, 10] utilize the properties of \u201cnon-negative\u201d neural networks, demonstrating their effectiveness on exploring various combinations of potential causes on health outcomes, based on clinical data, or increase the interpretability of medical image analysis, respectively. The architecture proposed in [5] constraints all learnable weights to non-negative values in order to ensure that the existence of an exposure can only increase the risk of the final outcome. In contrast with the weights, biases are constrained to negative values, acting as a threshold, that only allows large weights to pass the activation function and subsequently affect (positively) the final outcome. If a person ends up having no risk contribution to any of the exposures, meaning that the outputs across every node were zeros, then it is assumed that this person has a risk equal to a predefined baseline risk. In our work, we design an improved non-negative neural network as a strong baseline for the task at hand.\nExtending the task in other domains, many works shifted towards leveraging Transformer architectures [4], originally designed for Natural Language Processing tasks, to handle tabular data. This shift is motivated through the self-attention mechanism, which provides the Transformer model with the ability to capture complex feature interactions and dependencies."}, {"title": "Methods", "content": "Inspired by [5], we develop an architecture that features a three-layer non-negative MLP (nnMLP), intended for baseline comparisons. This design ensures that, during training, weights are constrained to be non-negative. In this context, exposures could either contribute positively to the risk outcome (when $w > 0$) or have no effect (when $w = 0$). Following the baseline, the biases of the two hidden layers are also constrained to be negative, allowing only sufficiently large weights to pass through the activation function, thereby positively affecting the final outcome. However, the bias term of the output layer is left unconstrained, providing additional flexibility in the model. Given the non-negativity of the network's weights and the utilization of the sigmoid function as the output layer's activation function, the unconstrained bias provides a baseline risk calculation. Finally, the weight parameters connecting the second hidden layer and the output layer are made learnable. Consequently, the output logit vector is characterized as a weighted sum of the second hidden layer's activation outputs, rather than simply aggregating them.\nLet $i, j, k$ denote the indices of the nodes in the input layer, the first hidden layer, and the second hidden layer, respectively, $X^{\\{1...i\\}}$ denote the input data features, $Z^{\\{1...j\\}}_1, Z^{\\{1...k\\}}_2$ the activation outputs of the first and second hidden layer, respectively. Letting $W, b$ denote the learnable weight parameters and biases of each layer and superscripts (+),(-) the corresponding non-negativity and non-positivity constraints, respectively, equations (1) - (3) describe the operations behind the proposed baseline architecture:\n$Z^{1}_{j} = ReLU\\left(\\sum_{i} \\left(W^{(+)}_{i,j} \\cdot X_{i}\\right) + b^{(-)}_{j} \\right);$ (1)\n$Z^{2}_{k} = ReLU\\left(\\sum_{j} \\left(W^{(+)}_{j,k} \\cdot Z^{1}_{j}\\right) + b^{(-)}_{k} \\right);$ (2)\n$Y = \\sum_{k} \\left(W^{(+)}_{k,1} \\cdot Z^{2}_{k}\\right) + b_{1}.$ (3)\nIt is worth mentioning that ReLU is employed as the non-linear activation function on both hidden layers. Additionally, the output logits Y, can be interpreted as probabilities of the positive class outcome, with values in range [0, 1], through the application of the sigmoid activation function."}, {"title": "TRACE model", "content": "The architecture of the proposed Transformer-based Risk Assessment for Clinical Evaluation (TRACE) method is illustrated in Figure 1. It handles various data types encountered in clinical datasets, namely: numerical (continuous), categorical and \u201ccheckbox\" data. For continuous data, we employ a Multi-Layer Perceptron (MLP), consisting of two linear layers and a SELU activation function between them, to introduce non-linearity. The dimensions of the MLP output are (B, Nnum, E) where B is the batch size, Nnum the number of numerical features available on the dataset and E the selected embedding size. Regarding categorical data, we deploy standard categorical embeddings for each feature, ensuring their discrete nature is maintained, while creating the embedded tensor (B, Ncat, E) where Ncat represents the number of categorical features available in the dataset.\nAdditionally, we define \u201ccheckbox\" data, as vectors receiving values 0 or 1, with ones representing the existence of the corresponding category and zeros its absence, but with the possibility of multiple positive categories on a single feature, thereby making it more complex to handle than standard one-hot encoded data. This type of data are processed with a custom embedding layer, where each checkbox feature is represented as a binary vector of size Ccheck corresponding to the total number of possible categories within the feature for i \u2208 {1, ..., Ncheck}. Each category within the i-th checkbox feature, passes through a categorical embedding, resulting in a tensor of size (B,C'check, E). Next, an element-wise multiplication is applied between the embedded tensor and the initial binary vector, serving as a mask, zeroing out all the embedded vectors of non-active categories. Subsequently, the active embeddings are aggregated allowing all valid categories to interact with each other, creating a single vector of size E, representing the combined information within the checkbox feature. This process is repeated Ncheck times, resulting in the final embedding tensor (B, Ncheck, E).\nThe concatenated embedded tensor (B, Nnum + Ncheck + Ncat, E) integrates all the diverse data representations into a unified feature space, serving as the input to the Transformer Encoder module. This module utilizes the multi-head self-attention mechanism, allowing the model to capture complex relationships and interactions across the entirety of the features. Finally, a linear head is employed for estimating the risk score.\nFinally, the proposed architecture explicitly handles missing values as follows. For continuous data, an element-wise weight masking is applied on the MLP outputs, masking out the weight vectors that correspond to missing numerical values in the input data. For categorical and checkbox data, a special embedding token, is defined for signaling missing values, effectively ignoring these entries."}, {"title": "Implementation", "content": "The first dataset considered in this work is dedicated for first primary melanoma classification and consists of a total of 415 patient records, with a ratio of 68.7% positive melanoma diagnoses, to 31.3% negative ones. The dataset consists of 29 features (Table 1), four of which contain numerical values, two checkbox-type data, while the rest of them are categorical.\nThis study utilizes data from Behavioral Risk Factor Surveillance System survey conducted in 2014 and 2022 [15]. Following [8], we utilize the 2014 dataset for type 2 diabetes classification, with 139,266 respondents retained, of whom 21,587 have been diagnosed with type 2 diabetes. During preprocessing, 26 features related to this task were selected, while samples containing at least one missing value were ignored. To fully exploit the capabilities of the proposed architecture, we created another version of the same dataset, that retains respondents with missing values and numerical features are maintained as continuous values rather than being converted into categorical variables with arbitrary bins. This version includes 408,599 respondents in total, with 60,538 positive diagnoses. In the same manner, we utilize the 2022 dataset for heart attack/disease classification, with 48 features (Table 2) selected and 440,111 respondents retained in total (39,751 diagnosed with heart attack/disease at least once). A version of this dataset that excludes instances with missing values, indicates a significant drop in the available samples, with just 1,745 respondents remaining."}, {"title": "Model Training", "content": "During training both our proposed networks, we employ Focal loss[16], which can be particularly effective for healthcare clinical datasets, where the case group (disease presence) is significantly outnumbered by the control group (disease absence). Additionally, it emphasizes on hard examples, which in our context, are the false negatives (instances where the model failed to detect a disease), through the Focal loss a hyperparameter. These examples are usually harder to identify, during clinical risk assessment, compared to false positives (instances where the model incorrectly identified a disease) due to class imbalance. The a value selection, relates to the degree of class imbalance in the dataset.\nWe train our models in a workstation equipped with an NVIDIA RTX A5000 with 24GB of VRAM. Unless stated otherwise, each TRACE model consists of a single transformer encoder layer, with a global model size of 128. The models were trained for 100 epochs using ADAM optimization algorithm with a learning rate equal to 2e-4. During training, we ensure that each batch maintains the positive-negative target label ratio of the corresponding dataset. Appendix A provides a more detailed analysis of the training hyperparameters."}, {"title": "Experimental Evaluation", "content": "In this study we consider five key metrics to evaluate the performance of our trained models: Accuracy, F1-Score, Sensitivity and Specificity and Balanced Accuracy (BA). Accuracy is a straightforward metric, defined as the ratio of correctly predicted examples, both true negatives and true positives, to the total number of instances evaluated. F1-Score is the harmonic mean of Precision and Recall of the positive class to assess the model's performance on imbalanced datasets. Sensitivity (or Recall) and Specificity are widely used in clinical tasks, illustrating the model's ability to correctly identify positive and negative instances, respectively, while BA represents their average score.\nTables 3 and 4 compare our proposed method with various baselines on both the melanoma and the BRFSS22 datasets. In particular, two ML methods (Logistic Regression, XGBoost) were considered as baselines, since they are widely used in clinical risk assessment, as well as our proposed non-negative neural network (nnMLP) and FT-Transformer[12] as the current state-of-the-art method for tabular data.\nTable 3 represents the results from five runs on the melanoma dataset, each with different random splits, that are consistent across all methods, utilizing a stratified k-fold cross validation. Our model outperforms the baselines in terms of Specificity and Balanced Accuracy (BA) metrics, and achieves the second-best results in Accuracy and F1-Score with smaller standard deviations and significantly fewer trainable parameters compared to FT-Transformer.\nTable 4 represents the results on the BRFSS22 dataset, which introduces a more challenging task due to its highly imbalanced class distribution. In such scenarios, higher accuracy alone may not reflect higher generalization, thus metrics like accuracy and specificity can be misleading, as observed with both ML algorithms. Instead, F1-Score and BA metrics offer a more representative overall assessment of the model's performance. Our proposed model significantly outperforms the baselines regarding the F1-Score and achieves the second-best BA, just behind the proposed nnMLP. These results demonstrate that our method provides robust and balanced performance across all scenarios.\nTable 5 compares the performance of the proposed TRACE model, with the Neural Network from [8] that outperformed all the baselines for the task of type 2 diabetes risk prediction on BRFSS 2014 dataset. Our model was evaluated using different values of the Focal Loss hyperparameter a and as shown, it consistently outperforms the neural network by [8] in several key metrics. More specifically, for higher a values, our model achieves the best performance in terms of Sensitivity and Balanced Accuracy metrics, reflecting its robustness in identifying positive cases while maintaining a balanced performance across both classes.\nSince the TRACE model employs the self-attention mechanism, it offers great insights and interpretability into the decision-making process and results, as visualized by the attention maps in Figures 2 and 3. More specifically, Figure 2 represents the attention weight visualizations for the melanoma (left) and heart disease/attack (right) classification tasks. Its aim is to identify the features that predominantly influence the final positive diagnosis. Rows represent participants randomly selected from the validation set and columns the key features considered during training. Each cell is calculated by averaging the attention weights of each key feature across all input queries. For instance, the frequency of skin checks a patient has, consistently provides high attention weights across the majority of the participants within the melanoma dataset. Similarly, for the heart attack/disease classification task, the participant's history of CT/CAT scan, C.O.P.D (chronic obstructive pulmonary disease), emphysema, chronic bronchitis, stroke and age category are examples of predominant features. In Figure 3 each cell represents how much focus to place on each key feature when processing a single query feature, revealing underlying relationships between pairs of features. Diagonal elements typically represent self-attention and high diagonal values indicate that the model is giving significant importance to individual features."}, {"title": "Ablation", "content": "Table 6 explores the impact of our proposed checkbox embedding mechanism, on the melanoma dataset, which includes \u201ccheckbox\" features. The first row, represents the scenario where checkbox embeddings are not utilized. In this case, each category within the multiple choice features is considered as an independent binary feature and categorical embeddings are applied normally, without any interactions among the categories included in a single feature. The second row, indicates that incorporating checkbox embeddings, improves the model's performance by capturing interactions within each checkbox feature.\nTable 7 compares the performance of our model on the BRFSS 2022 dataset, under two different training scenarios. At first, only samples with complete data are utilized during training (first five rows), resulting in a much smaller dataset with 1,745 samples. On the contrary, the second scenario (last five rows), includes all available samples, by effectively handling missing features, which increases the total number of samples to 440,111. Both scenarios were evaluated with the same validation set. The results suggest that incorporating additional entries, even affected by missing data, enhances the model's ability to generalize, improving performance across all metrics.\nFigure 4 illustrates the performance of our proposed method (shown on the y axis), across various ratios of randomly simulated missing values (shown on the x axis). Balanced Accuracy captures the combined performance of both Specificity and Sensitivity metrics, while F1-Score reflects the overall performance of the trained model. As expected, higher percentages of missing values (up to 50%) lead to a decline in performance. F1-Score is not significantly affected when up to 30% of the data is masked out. Balanced Accuracy, however, is more sensitive, exhibiting a descending trend, when the percentage of missing values exceeds 5% of the data."}, {"title": "Conclusions", "content": "In this paper, we proposed a novel clinical risk assessment method that utilizes the self-attention mechanism within a Transformer-based architecture. The proposed method, by effectively handling various data modalities and incomplete data, achieves a competitive performance when compared to state-of-the-art methods while requiring significantly less trainable parameters thus minimizing the computational cost. Additionally, a strong baseline for clinical risk estimation is introduced based on non-negative neural networks (nnMLP), which constrains the network weights to be non-negative, ensuring that the exposure to risk factors only increases the risk of an adverse outcome."}, {"title": "Focal Loss", "content": "Mathematically, Focal loss is defined as:\n$FocalLoss(p_t) = -a_t \\cdot (1 - p_t)^\\gamma \\cdot log(p_t),$ (4)\nwhere $p_t$ represents the probability corresponding to the true class (disease existence).\nIt incorporates a modulating factor $(1 \u2013 p_t)^\\gamma$ on top of the cross entropy loss criterion. This factor forces the model to focus on hard examples, during training. For $y = 0$ Focal loss is equivalent to Cross Entropy, but for $y > 0$ it raises the confidence and subsequently down-weights the contribution of easy examples. In practice, factor a is used as a weighting factor to balance the contribution from both classes, where $0 < a < 1$. Setting a near 0 increases the influence of the negative class, while setting it near 1 the positive class will contribute more to the final result, despite being less represented. The latter scenario aligns with the objective of improving classification of rarely observed instances."}, {"title": "Training Hyperparameters", "content": "For the proposed non-negative MLP network, the default hyperparameters utilized during training are listed in Table 8. We employ a 5-fold stratified k-fold cross validation approach, to ensure that each split maintains the class distribution of the dataset.\nRegarding the proposed TRACE model the default hyperparameters used during training, are summarized in Table 9. More specifically we follow a custom sampling pipeline, ensuring that, each batch preserves the target class distribution found in each dataset, in order to mitigate the large contribution of the majority class that leads to overfitting. Each experiment in this study, follows an 80%-20% training-validation split on the melanoma dataset and a 90%-10% ratio on all BRFSS dataset versions.\nAdditionally, categorical data are transformed into one-hot encoded vectors, to meet the input requirements of the non-negative MLP network. In contrast, for the TRACE model, categorical data remain in their original form, with distinct indices assigned on each category and missing values represented with 0 across all features. Finally, during validation and inference steps, we convert the model's output logits to probabilities using the sigmoid function. This transformation yields the probabilities of the positive class for each instance. We then apply a threshold of 0.5 to perform binary classification, determining whether each instance belongs to the positive or negative class. We employ \"ReduceLROnPlateau\" which reduces the learning rate when the balanced accuracy metric (average score of sensitivity and specificity metric) stops improving, with 10 epochs of patience. For both architectures, the best model is determined by the highest F1-Score."}, {"title": "Extended ablation", "content": "Regarding the proposed methodology, exhaustive hyper-parameter optimization has been performed regarding the model size (Table 10) and the encoder layers (Table 11). Increasing the model size and thus the number of trainable parameters did not lead to significant improvements in performance metrics, leading us to not consider models with sizes greater than 128 from further evaluation. In Table 11, a similar trend was observed when varying the number of transformer encoder layers; increasing the layers did not result in significant performance improvements. Therefore, the study concluded that focusing on models with 64 and 128 sizes, and utilizing fewer encoder layers, is more efficient without sacrificing accuracy.\nFigure 5 illustrates the quantitative comparison between our best performing model performance and the FT-Transformer [12] which is the state-of-the-art approach for tabular data. The figure focuses on the trade-off between the Fl-score and the number of trainable parameters of the FT-Transformer. In both plots, our method is positioned in the top left, achieving the highest F1-score with the least amount of trainable parameters. Although the F1-scores differ between the two datasets (melanoma at the top and BRFSS22 at the bottom), the overall trend remains consistent. Fl-score in both datasets starts with a decent in Fl-score having a high fluctuation around 1.5-2 million parameters and achieves the best score around 3 million. After this point, the performance tends to decline. Our model, consistently outperforms the FT-Transformer in both dataset while maintaining minimal trainable computational overhead. The consistency of these results indicates the effectiveness and robustness of the proposed method."}]}