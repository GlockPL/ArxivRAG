{"title": "ONLY-IF: REVEALING THE DECISIVE EFFECT OF INSTRUCTION DIVERSITY ON GENERALIZATION", "authors": ["Dylan Zhang", "Justin Wang", "Francois Charton"], "abstract": "Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization only emerges when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of specialist and generalist models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancements in large language models (LLMs) have revolutionized a wide range of tasks, including language comprehension (Wang et al., 2020), generation (Brown et al., 2020), knowledge-based question answering (Hendrycks et al., 2021a), and solving complex reasoning problems in fields like mathematics (Cobbe et al., 2021; Hendrycks et al., 2021b) and programming (Chen et al., 2021a; Austin et al., 2021; Chen et al., 2021b; Li et al., 2022). These successes hinge on the foundational capabilities of LLMs, such as knowledge retrieval, reasoning, planning, and notably, instruction following-enabled through instruction-tuning (Ouyang et al., 2022; Taori et al., 2023; Wei et al., 2022; Sanh et al., 2022a). Instruction-tuning trains LLMs on a wide variety of instruction-output pairs, allowing them to handle diverse prompts and better generalize to new tasks.\nWhile knowledge retrieval focuses on accessing stored information and reasoning involves multi-step problem-solving, instruction following concerns the accurate interpretation and execution of diverse natural language prompts (Zhou et al., 2023b). This capability is vital for user interaction, as it involves understanding the intent behind instructions and performing tasks without relying on complex logic (Liu et al., 2024a). Despite its importance, the mechanisms underlying instruction following remain less explored compared to other capabilities like reasoning.\nThe current research landscape on instruction tuning has produced varied and sometimes contradictory findings, ranging from the impact of dataset selection (Zhou et al., 2023a) to the effects of scaling up data (Zeng et al., 2024; Zhang et al., 2024a). These disparate findings suggest that instruction"}, {"title": "2 EXPERIMENTS WITH STRING REPLACEMENTS", "content": "To start a systematic investigation of instruction-following, we model instruction-following tasks as string-replacement operations, which are fundamental in theoretical computer science. String replacement forms the basis of Markov Algorithms (Markov, 1954), a Turing-complete model where sequences are iteratively transformed using ordered rewrite rules. These algorithms represent a structured, rule-driven process in which the first applicable rule is used to replace the leftmost matching substring, continuing until no further matches exist. This structured transformation mirrors the sequential, instruction-driven behavior we aim to explore. Appendix A provides illustrative examples of Markov algorithms in action.\nIn our study, we focus on a simplified form of this process. The replacement rule $R$\u2014a pair like $aa \\rightarrow bac$ is applied to an input string $\\xi$ (e.g., caaba), yielding an output string $\\tau$ (e.g., cbacba). The rule is applied to the leftmost occurrence of a match, and if no match exists, the original input remains unchanged. For instance, applying $\\iota : iss \\rightarrow art$ to $\\xi = mississippi$ produces $\\tau = martissippi$, while applying $\\iota$ to $\\xi = canada$ leaves the string unaltered.\nThis task, though simple, serves as a powerful proxy for instruction tuning by teaching models to handle structured, rule-based transformations. Training on these string rewrites allows models to internalize core aspects of instruction-following tasks, equipping them with the capability to generalize across a wide range of rule-based tasks without the overhead of more complex operations."}, {"title": "2.2 EXPERIMENT SET-UP", "content": "We designed two string replacement tasks to evaluate the model's ability to follow simple transforma-tion rules:\nBasic Replacement: Apply the rule $\\iota : x \\rightarrow y$ to any input string containing $x$. The model replaces the first occurrence of $x$ with $y$ and returns the modified string; and\nConditional Replacement: Apply the rule $\\iota : x \\rightarrow y$ if $x$ is present in the input string; otherwise, return the input unchanged.\nThe model takes in $\\xi$ and $\\iota$, outputs either the transformed string or the original input if no match is found.\nTask 1 focuses on fundamental rewrite operations, while Task 2 introduces a conditional decision-making aspect. This experimental setup allows us to assess the model's capacity for both direct rule application and handling cases where no action is required.\nIn later sections, we also generalize the task from regular to context-sensitive, where $x$ and $y$ represent abstract structures e.g. $x = a^2$ will represent all squared terms.\nWe train GPT-2 models (256 dimensions, 6 layers, 4 attention heads) on synthetic instruction/outcome pairs. The dataset contains $S \\times I$ sequences, with $S$ input strings and $I$ replacement rules. Models are tested on $10^5$ unseen examples to assess generalization across rules. Full training details are in Appendix B."}, {"title": "2.3 RESULTS", "content": "Figure 2a presents the generalization accuracy for models trained on a fixed budget of $I \\times S = 10^6$ examples, as a function of the number $I$ of different instructions in the training set. The number of examples per instruction ($S$) decreases as $I$ increases. In these experiments, all input sequences feature at least one instance of the replacement rule.\nIn Figure 2a,a sharp step-shaped transition is observed: models trained on fewer than $I_{min}$ (where $I_{min} = 300$) unique instructions consistently fail to generalize, regardless of example count per instruction. Conversely, models exposed to over $I_{max}$ (where $I_{max} = 1,000$) distinct instructions generalize effectively to unseen instructions, even with minimal examples per instruction. This phase transition underscores our hypothesis that the sheer number of distinct instructions ($I$) is"}, {"title": "2.3.2 DIVERSIFICATION ALLOWS GENERALIZATION IN CASE-BASED REASONING SET-UP", "content": "In earlier experiments, the model replaced a sub-string always present in the input. Now, we introduce a more complex task where some rules may not apply, requiring the model to decide whether a rule is applicable and either perform the operation or leave the input unchanged. This two-step process challenges the model to determine rule applicability and execute the correct action simultaneously.\nTo explore this, we introduce a third parameter in the training set: the frequency of \"No-Ops\" (instructions that cannot be satisfied), which we vary between 10% and 50%. The size of the training and test sets remains the same as in previous experiments,keeping the data size constant.\nFigure 2b presents the generalization accuracies of trained models, as a function of the number of instructions and the frequency of No-Ops. Interestingly, despite No-Ops dominating the dataset\u00b9, the model generalizes well to unseen instructions after training on around 400 distinct cases. The proportion of No-Ops does not significantly affect generalization beyond that point, demonstrating that training on diverse instructions effectively teach the model to assess rule relevance and apply them accurately.\nOverall, our conclusions remain consistent with previous experiments, albeit with a slightly lower number of in-structions needed for generalization (400 vs. 500). This demonstrates the effectiveness of diversification in more complex scenarios involving case-based reasoning and rule relevance assessment."}, {"title": "2.3.3 IMBALANCED DISTRIBUTION IS STILL EFFECTIVE IN DRIVING GENERALIZATION", "content": "In previous experiments, instructions were evenly distributed between examples in the training set: in a training set of 1 million examples, with 500 different instructions, every instruction would be featured 2000 times.\nSuch a situation is unlikely to happen in real-world settings. In real-world training sets, some tasks will be much more common than others (due to the availability of fine-tuning data and the nature of the tasks)."}, {"title": "2.3.4 SEMANTIC DIVERSIFICATION BOOSTS TASK PERFORMANCE", "content": "In real-world instruction-tuning, it is impractical to sample instructions uniformly from all se-mantic spaces due to their vastness. Instead, focusing on constrained sub-domains is crucial.\nWe emulate this scenario by training models on instructions with semantic constraints, such as repeated characters (aaabbbccc for $k = 3$ - each character repeated 3 times), periodic pat-terns (abcabc for $k = 2$ a sub-string repeated 2 times), and mirrored structures (abccbaabc for $k = 3$ mirroring the substring for 3 times), and measure their generalization across dif-ferent levels of $k$, where $k$ is a parameter controlling the constrained-ness of rule semantics.\nOur findings show that models trained on a single constrained sub-domain with high constraints (large k) fail to generalize to less constrained tasks (low k), indicating overfitting to spe-cific patterns. Training on mixtures of constrained sets improves general-ization, but only when the subspace is sufficiently rich. Larger instruc-tion sets (e.g., 5000 examples) boost generalization, while highly restricted semantic domains (larger k) make it harder.\nThese results underscore that instruc-tion diversity must span a range of semantically rich sub-spaces, not just rely on larger datasets within highly restricted domains, to foster robust generalization across tasks."}, {"title": "3 REWRITING WITH ABSTRACTION", "content": "In real-world applications, instruction-following often extends beyond simple \"search-and-replace\" tasks, requiring a model to abstract high-level concepts and ground them in specific contexts. To simulate more complex real-world instruction-following tasks, we extend the string-rewriting experi-ment by introducing two layers: abstract rules and their specific applications (groundings). Abstract rules represent high-level, task-independent patterns, while groundings refer to how these rules are instantiated in concrete input expressions. For example, in translation tasks, abstract rules might capture syntactic transformations, while their groundings apply these rules to specific phrases based on context."}, {"title": "3.1 DATA GENERATION", "content": "We collate a set of distinct equational algebraic deduction rules of the form $LHS = RHS$.\nRandom Tree Generation We randomly construct mathematical expression trees of a specified depth d. Non-leaf nodes were systematically assigned operators (e.g., +, -, *, /), while leaf nodes were populated with variables, constants, or unary operations.\nProducing Pattern-Carrying Sub-Expressions To generate expressions, a pattern-carrying sub-tree was generated with a depth of $d_p$, denoting the depth of expression tree for each entry in the rule. e.g. we may replace $a$ with $(y + 2x + 5)$ with $d_p = 2$. We then randomly choose a leaf node and swap it with the concrete sub-expression."}, {"title": "3.2 SIMULATING SPECIALIST AND GENERALIST TRAINING", "content": "We examine two common settings for instruction-tuning: generalist, and specialist training via the set of simulated experiments. We fine-tune all models from a pre-trained Mistral-7B-v0.3 (Jiang et al., 2023) checkpoint."}, {"title": "3.2.1 DIVERSIFYING INSTRUCTION SEMANTICS EMPOWERS BETTER GENERALIST LMS", "content": "We control the number of training instances (50K), and vary the number of abstract deduction rules & the instantiated rules per abstract rule. We test the models on unseen sequences and unseen abstract rules. We train the model on triples of ($\\xi, t_{abs}, \\tau$) pairs where $\\tau$ is the result of applying $t_{abs}$ to $\\xi$. The result is shown in Figure 5a. Our findings demonstrate a clear advantage of increased rule diversity, consistent with previous string rewriting experiments. Holding the number of training instances constant, we observed that expanding the diversity of rules (1) in the training data significantly enhances the model's ability to generalize to unseen abstract rules during testing. This improvement is achieved despite the model encountering fewer groundings per rule and expression. These results not only validate the hypothesis from our string rewriting experiments but also underscore that increased diversity in training data significantly enhances the model's ability to concretize abstract instructions, improving generalization even with limited exposure to specific instances. This underscores the importance of diversity in developing models that can unseen tasks."}, {"title": "3.2.2 SWEET POINT BETWEEN SPECIALIZATION AND DIVERSIFICATION FOR OPTIMAL SPECIALIST PERFORMANCE", "content": "In this experiment, we simulate a scenario where a model is trained as a specialist and tested on out-of-distribution queries that still belong to the same overall instruction type. To achieve this, we divide the set of rules $R$ into two categories: specialized rules $R_{spec}$ and diversification rules $R_{diver} = R \\setminus R_{spec}$. The training data is constructed as a mixture of instances generated from these two sets of rules. Specifically, the instances based on the specialized rules use patterns with depth $d_{train} < d_{test}$ to simulate out-of-distribution test instances, while the instances from the diversification rules add variety to the training. This setup allows us to examine the trade-off between specialization and diversification for better instruction following in this specialized task.\nFigure 5c, the results exhibit a clearly peaked structure as we incorporate more out-of-domain data for diversification. We demonstrated within a fixed budget, such trade-off indeed exists between specialization and enhanced instruction-following via training on a more diverse set of instructions. Figure 5b shows the trend when we diversify across an increasingly rich semantics. We notice the benefit of a more diverse $R_{diver}$, which suggests that even when diversifying for specialists, one should be mindful to curate a dataset that spans over wider domains."}, {"title": "4 FINE-TUNING A SEPCIALIST INSTRUCTION-FOLLOWER: CASE OF CODE GENERATION", "content": "Building on our foundational insights from controlled string-rewriting tasks, we extend our analysis to training a real-world specialist instruction-follower.\nHere, we investigate how cross-domain instruction diversity impacts the model's ability to handle real-world tasks that require a nuanced balance between instruction-following and domain-specific expertise by the example of code generation.\nCode generation is primarily an instruction-following task, where models translate explicit prompts into executable code. Pre-trained code LLMs, having encountered common coding patterns during pre-training on large code corpora, are fine-tuned to utilize these structures effectively. Unlike reasoning tasks that demand creative problem-solving, code generation focuses on adhering to established patterns and passing test cases, emphasizing precise procedural execution over complex reasoning.\nIn this experiment, we aim to demonstrate that a diverse set of instructions can significantly enhance a model's adaptability to new coding instructions. This analysis provides crucial evidence for"}, {"title": "4.1 EXPERIMENTS", "content": "To rigorously evaluate the impact of cross-domain instruction diversity, we employ two widely-used code generation benchmarks: HumanEval (Chen et al., 2021a) and MBPP (Austin et al., 2021), alongside the augmented EvalPlus (Liu et al., 2023). These benchmarks present a diverse array of coding challenges that test the model's ability to interpret and execute novel instructions. As our training dataset, we use 20,000 instances sampled from the OSS-Instruct (Wei et al., 2023), a synthetic coding dataset, which has been sanitized to avoid data contamination. We also incorporate general-domain instruction data from Alpaca Taori et al. (2023), a well-known dataset that covers a wide semantic domain. By gradually replacing code-specific instruction data with general-domain instructions, we assess how this cross-domain diversity influences the model's performance in code generation. We utilize two state-of-the-art pre-trained code language models as base models: DeepSeek-Coder-6.7B-Base (Guo et al., 2024) and CodeQwen-7B-Base (Bai et al., 2023)."}, {"title": "4.2 STRIKING THE RIGHT BALANCE BETWEEN CODING AND DIVERSE DATA", "content": "The Role of Semantic Diversity in Generalization Our results in Tables 2 and 1 highlight a crucial insight: while increasing the size of coding datasets may seem like the obvious solution for improving performance, this strategy is not always optimal. In fact, diversifying instruction domains leads to greater improvements. For example, adding data from general-purpose QA (Alpaca) and reasoning tasks (CoT) significantly outperforms incorporating an equivalent amount of coding data. Notably, the Plum-colored configuration in Table 1, which uses only 20,000 data points, surpasses the performance of models trained on 75,000 coding data points.\nThis pattern is consistent with our synthetic experiments in Section 3.2.2, where diverse instructions enabled the model to generalize better to out-of-distribution task instances for seen abstract instruc-tions. Even in the basic string-replacement experiments (Section 2.3.4), we observed that introducing varied instructions\u2014such as those in the Alpaca dataset-expanded the semantic range and boosted code generation performance, even when the quantity of each instruction is low compared to the main task (the long-tail distribution scenario in Section 2.3.3).\nThe Power of Cross-Domain Diversification Figure 1a demonstrates how instruction-tuning datasets cluster within specific semantic sub-spaces. By incorporating datasets like Alpaca, which is designed for human language interaction, and the CoT-Collection (Kim et al., 2023), which challenges the model with complex reasoning tasks, we extend the model's exposure to diverse semantic spaces. This further improves the model's generalization capabilities across domains."}, {"title": "5 FINE-TUNING GENERALIST LLMS", "content": "In this section, we evaluate the impact of cross-domain instruction diversification on general reasoning tasks and investigate the optimal high-level strategy to improve the quality of a dataset."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "In this study, we evaluate the impact of cross-domain instruction diversification on large language models (LLMs) by comparing our approach with a baseline model trained exclusively on UltraInteract-SFT dataset (Yuan et al., 2024). UltraInteract-SFT is a collection of complex, multi-step reasoning problems emphasizing on math problem-solving, code generation, and logical reasoning, promoting robust reasoning and planning capabilities in LLMs.\nWhile UltraInteract-SFT primarily focuses on math and coding problems and contains a rich collection of those problems, its scope is limited to these domains. OpenOrca (Lian et al., 2023) and Alpaca, though sparse and varied, introduce broader instruction-following tasks.\nTo assess the effectiveness of cross-domain instruction diversity, we constructed a training set that includes a mixture of UltraInteract-SFT, OpenOrca, and Alpaca datasets. While UltraInteract-SFT is rich in math, coding and complex QA problems, it remains limited to primarily these domains despite its challenging and diverse nature within them. OpenOrca and Alpaca, on the other hand,"}, {"title": "5.2 DATA DIVERSITY MATTERS MORE THAN QUANTITY FOR GENERALISTS", "content": "Our findings emphasize that expanding the model's exposure to varied domains leads to superior overall performance, underscoring the importance of data variety over sheer volume in enhancing model robustness and adaptability. Table 3 demonstrates the clear advantage of training models with diversified data across different domains, a phenomenon observed across various data budgets. By simulating the process of composing data up to a predefined budget, we show that the model benefits from composition that includes data diversifying beyond UI's domain. Additionally, when increasing the overall data quantity, comparisons across different Total budgets reveal that incorporating diverse data improves performance more than simply increasing the size of the UI dataset.\nThe key takeaway is that models exposed to a broad range of domains consistently achieve better performance than those trained solely on domain-specific reasoning data. Apart from the benefits from significantly enhanced instruction-following, this holds true even when tested on tasks that demand strong reasoning skills, which reinforces the results discussed in Section 3.2.1.\nThe results suggest that when aiming to improve a model's overall capability through dataset expan-sion, it's more effective to prioritize diverse datasets rather than simply increasing the volume of data from a specific domain. Our findings highlight that exposing models to varied domains enhances"}, {"title": "6 RELATED WORKS", "content": "Datasets for instruction-tuning. Many datasets for instruction-tuning have been proposed. The best quality is achieved for sets collated by human annotators (Khashabi et al., 2020; Ye et al., 2021; Sanh et al., 2022b; Wang et al., 2022; Longpre et al., 2023; Conover et al., 2023; K\u00f6pf et al., 2023), but their size is constrained by the cost of annotation. Alternative methods, which use large language models to generate instruction sets, have been proposed (Wang et al., 2023b; Honovich et al., 2022; Taori et al., 2023; Peng et al., 2023; Chiang et al., 2023; Xu et al., 2023a; K\u00f6ksal et al., 2023; Kim et al., 2023). They provide larger instruction sets, at the cost of reduced annotation quality.\nData curation for instruction-tuning. It is widely recognized that the quality of instruction-tuning datasets has a massive impact on the performance of fine-tuned models. Previous works acknowledged the contributions of several key factors. Most research on the subject insist on the importance of the size and quality of the instruction sets (Chung et al., 2022; Iyer et al., 2022; Wang et al., 2023a). Liang et al. (Liang et al., 2024) point out the importance of consistent formats. Several recent works (Zhou et al., 2023a; Cao et al., 2023) suggest that models fine-tuned on carefully selected examples can achieve high performance with small datasets. Various strategies for data curation have been proposed, focusing on instruction diversity, and the quality of answers (Zhou et al., 2023a; Cao et al., 2023; Xu et al., 2023b; Li et al., 2024; Liu et al., 2024b). Several authors discuss the benefit of mixing tasks from different categories (Longpre et al., 2023; Iyer et al., 2022; Bukharin & Zhao, 2024). Closest to our work, Dong et al. (Dong et al., 2024) discuss the impact of mixing general and domain-specific instructions, in order to achieve the best results with the smallest dataset."}, {"title": "7 CONCLUSION", "content": "In this work, we systematically studied instruction-following of language models via a suite of carefully designed controlled experiments. By introducing a symbolic string rewrite framework, we provide a focused model to isolate and study models' instruction-following abilities, offering a new perspective on this fundamental capability. Our experiments further demonstrate that instruction diversity, even within a fixed data budget, plays a critical role in improving model generalization. This finding underscores the value of diverse instruction semantics over large dataset size, in enhancing performance across both specialized and generalized LLM applications. Finally, we offer practical insights into dataset collation strategies, highlighting that proper diversification can significantly outperform dataset expansion."}, {"title": "A COMPLEMENT ON MARKOV ALGORITHMS", "content": "Markov algorithms Markov (1954) are ordered sets of rewrite rules, operating on sequences of symbols in a fixed alphabet $U$. A sequence $S$ is processed by applying the first rewrite applicable to $S$, at the leftmost position if several exist: i.e. the rewrite rule $ss \\rightarrow tr$ transforms the sequence $S = mississipi$ into $S' = mitrissipi$. The algorithm is then applied to $S'$, and the process is repeated until either no rules apply, and the algorithm is said to be blocked, or a special rule, called a stop rule is invoked, and the algorithm terminates and returns the final rewritten sequence.\nSpecifically, the algorithm uses and alphabet $A$, which includes the alphabet $U$ used buy the sequences to be processed (henceforth, small case latin letters), a set of additional symbols (henceforth, the small case greek letters {$\\alpha, \\beta . . . $}, and a special symbol \u00b7 indicating a stop rule.\nFor instance, we could define the following algorithm, with $U = {a,b}$, and $A = {a, b, \\alpha, \\beta, \u00b7}$, and the rules\n$\\alpha x \\rightarrow x\\beta \\alpha$ \\\\$\\beta xy \\rightarrow y\\beta x$ \\\\$\\alpha \\beta a \\rightarrow x\\alpha$\\\\\n$\\alpha \\rightarrow $\u00b7\\\\  where x and y stand for any letter a or b. This will transform any sequence of a and b into a concatenation of the sequence and its reverse. Applied on abb, the algorithm will perform the following rewrites:\n abb $\\rightarrow$ aabb \n aabb $\\rightarrow$ aa$\\beta$abb \n aa$\\beta$abb $\\rightarrow$ aab$\\beta$ab \n aab$\\beta$ab $\\rightarrow$ aba$\\beta$abb \n abab$\\beta$bab $\\rightarrow$ ababb$\\beta$a\n aba$\\beta$bb$\\beta$a $\\rightarrow$ abab$\\beta$ba\n abab$\\beta$b$\\beta$a $\\rightarrow$ abba$\\beta$b$\\beta$a\n abba$\\beta$\\beta$\\beta$\\beta$a $\\rightarrow$ abbbba$\\beta$a \n abbbbaba $\\rightarrow$ abbbba$\\beta$a\n abbbba$\\beta$a $\\rightarrow$ abbbbaa \n abbbbaa $\\rightarrow$ abbbba\nSince rule 4 is a stop rule, the algorithm terminates and returns abbbba.\nJudicious introduction of additional (greek) letters allows one to compose Markov algorithms, effectively writing complex programs. Any effective process (i.e. finite computation) can be represented as a Markov algorithm (this is Markov's thesis)."}, {"title": "B EXPERIMENTAL SET-UP", "content": "In rewrite experiments, we train GPT-2 models Radford et al. (2019), a decoder-only transformer-based architecture, with 6 layers, 256 dimensions and 4 attention heads from scratch, on a generated instruction-tuning dataset using standard supervised fine-tuning approach. We use the AdamW optimizer, a learning rate of 10-3, and linear scheduling. All models are trained for 50 epochs. For the encrypted-rewriting task, we LoRA fine-tuned Llama-2 models with a learning rate of 1e-5, batch size 64, gradient accumulation step 1, and 8-bit quantization. The model takes about 2000 steps to"}, {"title": "B.2 DATA GENERATION", "content": "Synthetic Experiment Except for the diversity of semantics experiment, the results we reported in the main paper are obtained from an input length of 50 and a pattern length of 20. To validate the generality of our findings, we conducted experiments on various input sizes {50, 100, 200} and, correspondingly, pattern lengths {20,40,50}.\nIn the diversity of semantics experiment, we used an input length of 500 and a pattern length of 60. We strictly restricted the sub-strings to look for and to replace them with both to be unseen during testing.\nReal World Data We downloaded the datasets (OSS-INSTRUCT, ALPACA, COT, ULTRAINTER-ACT, OPENORCA) from the official Huggingface Datasets Repos.\nDemonstration of dataset sizes for long-tail rule distribution experiments. We included the plot of percentage against rank index with different $\\alpha$'s."}, {"title": "C MORE DETAILS ON EVALUATION", "content": "For coding task, we evaluated the performance following the standard settings in EvalPlus (Liu et al., 2023).\nIn Section 5, we evaluated a on variety of tasks. We used the evaluation suite (prompt, score computation script) provided by Yuan et al. (Yuan et al., 2024)."}]}