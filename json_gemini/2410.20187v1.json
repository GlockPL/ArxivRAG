{"title": "UNCERTAINTY-PENALIZED DIRECT PREFERENCE OPTIMIZATION", "authors": ["Sam Houliston", "Aliz\u00e9e Pace", "Alexander Immer", "Gunnar R\u00e4tsch"], "abstract": "Aligning Large Language Models (LLMs) to human preferences in content, style, and presentation is challenging, in part because preferences are varied, context-dependent, and sometimes inherently ambiguous. While successful, Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are prone to the issue of proxy reward overoptimization. Analysis of the DPO loss reveals a critical need for regularization for mislabeled or ambiguous preference pairs to avoid reward hacking. In this work, we develop a pessimistic framework for DPO by introducing preference uncertainty penalization schemes, inspired by offline reinforcement learning. The penalization serves as a correction to the loss which attenuates the loss gradient for uncertain samples. Evaluation of the methods is performed with GPT2 Medium on the Anthropic-HH dataset using a model ensemble to obtain uncertainty estimates, and shows improved overall performance compared to vanilla DPO, as well as better completions on prompts from high-uncertainty chosen/rejected responses.", "sections": [{"title": "INTRODUCTION", "content": "Aligning LLMs to human preferences in content, style, and presentation has become a central challenge in improving and deploying LLMs, leading to the advent of Reinforcement Learning with Human Feedback (RLHF), now a prominent technique to fine-tune state-of-the-art LLMs (Casper et al., 2023). The standard RLHF pipeline involves human feedback collection, reward model training, and LLM policy optimization via reinforcement learning (RL). Despite its success, each stage presents challenges, from feedback interpretation and policy generalization to challenging RL implementation (Casper et al., 2023). Direct Preference Optimisation (DPO) (Rafailov et al., 2023) effectively bypasses the reward model by fine-tuning the policy to maximize the likelihood of the preference data under the Bradley-Terry model (A. & Terry, 1952). DPO is easier to implement than RL algorithms, and benefits from computational efficiency and stability by avoiding potential inaccuracies and biases of a reward model (Xu et al., 2024; Casper et al., 2023).\nA predominant issue in RLHF techniques is proxy reward overoptimization, arising from the assumption that preferences are effectively captured as pointwise rewards (or binary comparisons), or from an imperfect coverage of the full preference distribution by the training data. In particular, DPO easily overfits on the preference dataset (Azar et al., 2023), and considers all binary preference pairs in the dataset equally, while some may be stronger than others (Amini et al., 2024). We study the DPO loss to understand this overfitting regime in Section 3 and reveal that DPO is particularly sensitive to erroneous or ambiguous preference pairs.\nInspiration can be taken from Offline RL which learns a policy from a fixed dataset but suffers from the distributional shift problem, where reward or value overestimation errors occur when the policy encounters OOD states that are underrepresented in the dataset, leading to poor generalization. To address this, pessimism towards OOD states is induced by penalizing high uncertainty rewards, which encourages reliable policy learning (Jin et al., 2020; Li et al., 2021). RLHF shares similar"}, {"title": "RELATED WORK", "content": null}, {"title": "UNCERTAINTY PENALIZATION IN STANDARD RLHF", "content": "Zhai et al. (2023a) show empirically that Kullback-Leibler (KL) regularization in the standard RLHF pipeline may be insufficient to avoid reward overoptimization. Their method Uncertainty Penalized-RLHF (UP-RLHF) trains an ensemble of diverse Low-Rank-Adaptation (LoRA) reward models to obtain a mean reward and ensemble epistemic uncertainty. A Lower Confidence Bound (LCB) on the reward is taken by subtracting a factor of the uncertainty from the mean. Similarly, Yang et al. (2024a) employ a bayesian reward model to use conservative reward estimates in the best-of-n sampling framework."}, {"title": "DPO AND VARIANTS", "content": "DPO (Rafailov et al., 2023) is an effective approach to finetuning for binary preferences without a reward model or reinforcement learning, while still optimizing for the original RLHF objective. The works below study its limitations and extend the method to more involved frameworks.\nAzar et al. (2023) notice DPO easily overfits on training preferences, especially for inputs where the policy's implicit reward are nearly deterministic (close to 1 or 0). They introduce Identity Preference Optimisation (IPO) which adds a regularisation term to DPO, enabling one to train models to convergence without requiring tricks like early stopping. In addition, they unify RLHF, DPO, and IPO under a common mathematical formulation \u03a8\u03a1\u039f.\nPal et al. (2024) frame LLM generation as a Markov Decision Process (MDP) at the token level (instead of a contextual bandit at the entire completion level) to show a failure mode of DPO on low-edit-distance preference pairs. In this case, DPO increases the relative probability between the chosen and rejected text, however is a reduction in both absolute likelihoods. DPO-Positive (DPO-P) adds a clipping term to the loss to ensure positive log-likelihood of the chosen text.\nAmini et al. (2024) introduce Offstet DPO (ODPO) which adds a margin between the implicit cho-sen/rejected rewards in the DPO loss. The margin is based on reward scores given by an external reward model, to help DPO distinguish between strong or weak preference pairs. Our work is similar to ODPO. For the standard Lower Confidence Bound uncertainty penalization, we recover a similar loss formulation with a margin (7); our margin equals the difference in chosen-rejected uncertainties, whereas ODPO uses the difference in reward scores. For our main method Energy Factor Penalization, the resulting penalized loss does not have an additive margin, instead, the individual chosen-rejected implicit rewards are multiplied by an energy function of their uncertainty (10) which prevents uncertainties from canceling out, leading to a more precise penalization and improved results."}, {"title": "ANALYSIS OF THE DPO LOSS", "content": "Analysis of the DPO loss shows larger policy gradient update steps are applied on preference pairs with low chosen/rejected likelihood ratios compared to reference ratios 3. This behavior can be harmful for mislabeled or similar pairs. Additionally, the overoptimization phenomenon is shown to happen for rejected samples with low probability (\u03c0\u03b8(y\u03b9|x) < 1), where DPO does not regularize and decreases \u03c0\u03b8(Y\u03b9|x) further, potentially increasing the relative probability of other completions. These observations, tied to the overoptimization problem addressed by IPO (Azar et al., 2023), motivate the incorporation of pessimism as a gradient update attenuation mechanism for erroneous, or similar (uncertain) samples.\nProblem Setting. The aim is to align the parametrized LLM policy \u03c0\u03bf to the preference dataset D = {xi, Yi,w, Yi,l}N=1 composed of prompts x, chosen completions yw, and rejected completions y\u0131, while keeping close to a given reference model policy ref. The DPO loss (Rafailov et al., 2023) is formulated as:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = E_{(x,y_w,y_l)~D}[-log\\sigma(\\beta log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})]$\n$ = E_{(x,y_w,y_l)~D}[-log\\sigma(\\beta log \\frac{\\pi_\\theta(y_w|x)\\pi_{ref}(y_l|x)}{\\pi_\\theta(y_l|x)\\pi_{ref}(y_w|x)})]$\n$= E_{(x,y_w,y_l)~D}[-log\\sigma(\\beta A_\\theta)]$\nBy the monotonicity of the log and sigmoid functions, minimizing the DPO loss in Equation (1) corresponds to maximizing A\u03b8. A\u03b8 is pro-portional to the policy's chosen-rejected likelihood ratio and provides a measure of how well the policy distinguishes between the completions compared to the reference. Figure 1 shows the loss value and its gradient both increase as Ao decreases; thus, the DPO loss severely penalizes (and strongly updates on) inputs where Ae approaches zero.\nThe strong update regime is described in Equation (3) and confirms DPO performs stronger updates when the target policy performs worse relative to the reference policy.\n$A_\\theta = \\frac{\\pi_\\theta(y_w|x) \\pi_{ref}(y_l|x)}{\\pi_\\theta(y_l|x) \\pi_{ref}(y_w|x)} < 1 \\Leftrightarrow \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_l|x)} < \\frac{\\pi_{ref}(y_w|x)}{\\pi_{ref}(y_l|x)}$\nImpact of Ill-Labeled Preference Pairs. Suppose yw, \u1ef9\u0131 represent a corrupted or ill-labeled pref-erence pair i.e. p*(\u1ef9w > \u1ef9\u0131) \u2264 0.5 as per the Bradley-Terry model p*. Assuming a decent reference"}, {"title": "ANALYSIS OF THE GRADIENT", "content": "The DPO loss gradient w.r.t. policy parameters @ is derived to have a finer understanding of the mechanics of DPO updates that follow gradient-based optimization. The full derivation is provided in Appendix A. For shorthand, we denote the variable pe := \u03b2 log(Yw|x) \u2013 ro(y\u0131|x) = \u03b2 log A\u0473, as the difference between implicit rewards, which increases with A\u03b8.\n$V_\\theta L_{DPO}(x, y_w, \\psi_l; \\theta) = E_{(x,y_w,y_l)~D} [\\frac{-\\beta\\sigma(-\\rho_\\theta)}{\\sigma(\\rho_\\theta)} (\\frac{\u00b4\\nabla_\\theta\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_w|x)} - \\frac{\\nabla_\\theta\\pi_\\theta(y_lx)}{\\pi_\\theta(y_lx)})]$\nWe observe the following:\n\u2022 The term \u2460 shows the magnitude of the loss is proportional to \u03c3(\u2212\u03c1\u03b8). Thus gradient-based optimization of the DPO loss will perform stronger updates for data samples that have a low pe, i.e. a low value Ae, which corresponds to \"poor\" policy performance.\n\u2022 The term \u2461 shows the policy gradient for the chosen/rejected outputs is divided by their respective policy output probability: \u2207\u04e9\u03c0\u04e9(Yi|x)/\u03c0\u03bf(Yix). Thus, gradient updates for an output are enhanced if the probabilities of this output are already low. This may be a problem for low-probability rejected completions (\u03c0\u03bf(\u03b3\u03b9|x) < 1) that experience continual decrease throughout training, potentially raising the relative probability of other comple-tions. This finding ties with the empirical observation from Azar et al. (2023) that DPO performs poorly for near-deterministic preference pairs (\u03c0\u03b8 \u2208 {0,1}) and requires further regularization."}, {"title": "CONCLUSION: AN INVITATION FOR PESSIMISM", "content": "The analysis of the loss and its gradient in terms of the quantity Ao shows DPO performs strong up-dates when \u2460 the target policy exudes low Ae i.e. a low chosen/rejected likelihood ratio compared to the reference policy; or \u2461 when the probabilities \u03c0\u03bf(Yw|x), \u03c0\u03bf(yi|x) are both low. This is bene-ficial for well-labeled and abundant datasets, however, ill-labeled preference pairs likely correspond to i), and low-edit-distance and similar pairs may correspond to both i) and ii).\nThis sensitivity of DPO to \u2460, and the overfitting regime \u2461 call for a mechanism to attenuate gradient updates on known weak or wrong preference pairs. If preference uncertainty scores are available, they could be leveraged as a proxy. In addition, attenuated gradient updates with a valid proxy also address an issue in DPO that all pairwise preferences are of equal weight in a dataset, despite some preference pairs being much stronger than others. Without additional attenuation, the only safety net is the quality of the reference model which regulates \u0391\u03b8."}, {"title": "METHOD", "content": "The central contribution of this paper is to propose a penalization scheme appropriate for DPO, which leverages known uncertainty estimates on preferences. Our approach applies a reward uncer-tainty penalization to the overarching RLHF objective, from which we derive a new penalized DPO loss. For starters, we introduce the standard Lower Common Bound penalisation (Jin et al., 2020) to DPO. Our main method termed \"Energy Factor Penalization\" is a multiplicative penalization which brings considerable benefits to the binary chosen-rejected nature of DPO. The framework setting and notation build on that of DPO (Rafailov et al., 2023). In addition, we assume access to a reward model r(x, y) equipped with uncertainty quantification u(y|x)."}, {"title": "IMPORTING STANDARD UNCERTAINTY PENALIZATION TO DPO", "content": "Pessimistic RL subtracts a factor of the reward uncertainty u(y|x) from the reward score r(x, y) to obtain a conservative estimate of the reward function as a Lower Confidence Bound. Applying this to the general RLHF objective results in Equation (5).\n$\\max_{\\pi_\\theta} E_{x~D, y~\\pi_\\theta(y|x)} [r(x, y) \u2013 u(y|x)] \u2013 \\beta DKL (\\pi_\\theta(y|x) || \\pi_{ref}(y|x)) $\nFollowing prior work (Peters & Schaal, 2007; Go et al., 2022; Rafailov et al., 2023) the unique solution Equation (5) is derived. The optimal policy \u3160 corresponds to the reference policy being modulated by the conservative reward estimate, with Zu(x) as the appropriate partition function. Indeed, a high reward uncertainty u(y|x) for a given completion y will induce a lower policy probability:\n$\\pi^*(y|x) = \\frac{1}{Z_u(x)} \\pi_{ref}(y|x) e^{\\frac{(r(x,y)-u(y|x))}{\\beta}}$\nFollowing the original DPO derivation (Appendix A), Equation (6) is injected in the Bradley-Terry model, the optimal policy is replaced by the parameterized policy \u03c0\u03b8 which is then optimized by maximum likelihood under the Bradley-Terry model, giving the following loss:\n$L_{UPO}(\\pi_\\theta; \\pi_{ref}) = E_{(x,y_w,y_l)~D} [- log \\sigma (\\frac{\\beta}{\\pi_\\theta(x, y_w) - \\pi_\\theta(x, y_l) + u(y_w|x) - u(y_l|x))}]$\n$L_{UPO} = E_{(x,y_w,y_l)~D} [-\\beta\\sigma (- \\rho_\\theta \u2013 \\Delta_u) V_e log \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_l|x)}]$ \nEffect of the penalization. The pessimistic correction amounts to adding the margin Au = u(Yw|x) \u2013 u(yi|x) between implicit rewards. Following the analysis of the loss, Equation (8) suggests a higher positive margin will decrease the gradient magnitude. This modification is pessimistic: i) A high chosen reward uncertainty u(yw|x) will reduce the gradient, attenuating the gradient up-date, thus \u03c0\u03b8(Yw|x) will not increase much. ii) A high rejected reward uncertainty u(y\u0131|x) will enhance the update: \u03c0\u03bf(y1|x) will additionally decrease. The chosen and rejected uncertainties in this scheme individually exhibit desirable effects, however they are not independent: they may cancel out or interfere.\nConnection to Offset-DPO. The derivation above recovers a DPO loss Equation (8) with an ad-ditional margin Au. This loss has the same form as Offset-DPO Amini et al. (2024) which con-siders the offset Au as an increasing function f of the difference between reward model scores \u2206 = f(r(x,yw) \u2013 r(x,y\u0131)). They link this penalization to the Softmax-Margin loss, which we study in Appendix B.3 and derive a fully additive scheme \u2206u = u(yw|x)+u(y\u0131|x) in Equation (39) which sums uncertainties, preventing their cancellation."}, {"title": "MAIN METHOD: ENERGY FACTOR PENALIZATION", "content": "The previous section motivates a multiplicative penalization scheme (instead of subtraction) to en-sure the penalization effect of either chosen or rejected uncertainties carries to the respective chosen or rejected policy gradient update terms in Equation (8). Our proposed scheme multiplies the pref-erence value or reward by an energy-like function of the uncertainty. Such penalization can be modulated by a temperature parameter T > 0:\n$\\max_{\\pi_\\theta} E_{x~D, y~\\pi_\\theta(y|x)} [r(x,y)e^{-u(y|x)/\\tau}] \u2013 \\beta DKL. (\\pi_\\rho(Y|x) || \\pi_{ref}(y|x))$ \nThis objective is derived into a DPO loss following the same steps as above, to obtain the expression (10). The full derivation is found in Appendix B.2.\n$L_{DPO} = E_{(x,y_w,y_l)~D} [-\\sigma(\\frac{e^{u(Y_w/x)/\\tau}r_o(x, Y_w) - e^{u(Y_l/x)/\\tau}r_o(X, Y)}{\\beta})]$\n$\\bigtriangledown_{\\theta} L_{UPO} = E_{(x,y_w,y_l)~D} [-\\beta\\sigma^{'}(\\rho_\\theta) (\\frac{e^{u(Y_w|x)/\\tau}\\bigtriangledown_{\\theta}\\pi_\\theta (Y_w/X)}{\\pi_{\\theta}(Y|x)} - \\frac{e^{u(Y_l|x)/\\tau}\\bigtriangledown_{\\theta}\\pi_\\theta (Y_l/X)}{\\pi_{\\theta}(Y|x)})]$"}, {"title": "PRACTICAL IMPLICATION: SCALING OF THE PENALTY", "content": "The uncertainty penalties are obtained from an external reward model, preference dataset statistics or even additional user labels. These will most likely not be to the scale of DPO's implicit rewards re. Hence we apply a scalar multiplier a to the penalty: Au \u2190 \u03b1\u0394\u03b9\nThe scaling parameter az% is computed such that the penalty Au is approximately to z% of the mean implicit reward (z is the hyperparameter; a one standard deviation penalty of the reward roughly corresponds to z = 30%). The same principle is applied to the temperature parameter 72% for multiplication penalty. Denote the mean implicit reward as re, and the mean uncertainty value \u016b, the scaling parameter is computed as follows:\n$\\alpha_z \\Delta_u = (1 \u2013 z) \\T_{\\theta} \\Leftrightarrow \\alpha_z = (1 - z) \\T_{\\theta} / \\Delta_u$\n$e^{u/z} = (1 + z) \\Rightarrow \\tau_z = \\overline{u} / log(1 + z)$\nNaturally, implicit reward values evolve throughout training which motivates the use of an expo-nential moving average estimate of the mean reward. In practice, we compute this every batch of training: ft) = (t-1) + (1 \u03bb) rt), where ft) and r(t-1) denote the moving average estimates at batch t and t 1 respectively, r (t) denotes the mean implicit reward of batch t, and \u03bb \u2208 [0,1) is the decay factor which controls the influence of previous estimates, balancing between smooth-ness and responsiveness of the moving average. The same is applied to estimate u in the case of multiplication penalty."}, {"title": "GENERALIZATION \u03a4\u039f \u03a8\u03a1\u039f AND IPO", "content": "The IPO framework by Azar et al. (2023) is a general objective that encompasses many RLHF methods, including the standard RLHF objective (14), DPO, and Identity Preference Optimization (IPO). We generalize our penalization schemes to \u03a8\u03a1\u039f in Appendix B.4, and import our schemes to IPO. Our schemes for IPO are evaluated empirically."}, {"title": "SUMMARY OF PROPOSED PENALIZATIONS.", "content": "We present the initial LCB addition penalization, our main method (multiplication) and other uncer-tainty penalization schemes. The Cost-Margin-motivated penalizations are derived in Appendix B.3 and presented as ablations over different ways to include uncertainty in DPO. The reward model free penalizations are not empirically evaluated and are dedicated to future work."}, {"title": "EXPERIMENTS", "content": null}, {"title": "EXPERIMENTAL SETUP", "content": "An ensemble of reward models is trained to compute uncertainty estimates for the Anthropic-HH dataset. All LLMs first undergo supervised fine-tuning (SFT) on the chosen completions of the dataset, and then preference fine-tuning is performed. The evaluation is done by scoring model completions for prompts from the Anthropic-HH test set. The experiments are performed on GPT2 Medium (355M weights, pretrained); complete implementation details and hyperparameter search ranges are found in Appendix C.\nDataset. The Anthropic-HH dataset (Bai et al., 2022) consists of 160'800 train and 8552 test records of chosen and a rejected human-assistant conversations.\nReward Model Ensemble. 5 individual reward models are trained on shuffled 90% splits of the Anthropic-HH training dataset. GPT2 is used with a regression head, and trained via the Hugging-face TRL library's RewardTrainer with default arguments (1 epoch). The ensemble obtains a mean classification accuracy of 67% on the test dataset.\nSFT Reference. SFT training is performed in completion-only mode on chosen completions using the TRL SFT Trainer with a linearly decreasing learning rate of 1.45e-5, 8 batch size, 8 gradient accumulation steps, 10% warmup for 1 epoch, and no LoRA.\nDPO Models and Baseline. Fine-tuned models were trained on top of the SFT reference using LoRA, with an initial hyperparameter search. For the DPO baseline with GPT2 Medium, the optimal parameters were \u03b2 = 0.6, 1e-7 learning rate with linear decrease schedule, 32 batch size, no gra-dient accumulation, LoRA parameters (r, a) = (16, 16), and 10% warmup for 1 epoch. Pessimistic DPO ablations used the same parameters."}, {"title": "PERFORMANCE EVALUATION", "content": "The penalized DPO models perform on par or better than vanilla DPO. Figure 2b shows the addition scheme performs similarly to DPO whereas our multiplication scheme outclasses the baseline for all penalization strengths. For each of the schemes, the middle penalization strength of 30% performs best. Scores and uncertainties across all models and chosen/rejected baselines, are presented in table 3, showing our schemes outperform the SFT and DPO baselines."}, {"title": "ROBUSTNESS EVALUATION", "content": "Performance on Uncertain Samples. Our method should improve training on chosen and rejected pairs exhibiting high reward uncertainty. To evaluate this, we isolate the 50 test records whose"}, {"title": "CONCLUSION", "content": "This work proposes a new framework for DPO inspired by pessimistic offline RL that integrates pes-simism into DPO by leveraging preference uncertainty estimates. The derived penalization schemes are tailored to the binary nature of DPO, with our Energy Factor scheme performing best overall and robustness-wise in our illustrative experiments. The empirical findings invite further evaluation with more powerful models on various tasks (summarization, dialogue, completion...). Finally, a generalization to \u03a8\u03a1\u039f, IPO, and a reward-model-free scheme are proposed; further work is invited to develop and implement these."}, {"title": "DPO BACKGROUND AND DERIVATION", "content": "Problem setup. We have a static dataset of comparisons denoted as D = {x, Yw, Y\u0131}N=1, which is usually obtained by prompting an SFT (supervised-fine-tuned) model with prompts x to produce pairs of answers (yw, y\u0131) (these comparisons do not have scores, they are absolute). Human labellers identify the preferred output yw over the undesired y\u0131. We define a reference (SFT) model policy as Tref and our parameterized policy as \u03c0\u03bf, which we aim to fit as to make the preferred outputs yw more likely, while staying close to the reference policy."}, {"title": "DPO DERIVATION FROM RLHF.", "content": "The RLHF objective is:\n$\\max_{\\pi_\\theta} E_{x~D, y~\\pi_\\theta(y|x)} [r(x, y)] \u2013 \\beta KL (\\pi_\\theta(y|x) || \\pi_{ref}(y|x)) $\nBy factoring the terms under the expectation, we obtain an KL divergence-like expression, we in-troduce the partition function\u00b2 Z(x) = \u03a3y ref(y|x) exp((r(x,y))/\u03b2) to normalize the denominator, and the optimal value annuls this KL-divergence, giving us the unique optimal solution:\n$\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) e^{\\frac{r(x,y)}{\\beta}}$\nIntuitively, our optimal policy aligns with the reference policy, modulated by high or low rewards of certain outputs. Now, equation 15 can be re-arranged to express the ground-truth reward model in function of the induced optimal and reference policies: r(x, y) = Blog (()) + Blog Z(x).\nThe authors, define the re(x, y) = Blog (()) + Blog Z(x) \u2248 Blog ((2)), as the reward implicitly defined by the language model \u03c0\u03b8.\nEnter, the Bradley-Terry model: p(Y1 > Y2|x) = \u03c3(r(x,y1) \u2212 r(x, y2)), interpreted as the proba-bility of answer y\u2081 being favoured over y2 as a function of their human-labelled rewards. In RLHF the reward model is trained to maximize p(Yw, Y\u0131) over a dataset. For DPO, we do the same, by substituting the parameterised reward expressions:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = E_{(x,y_w,y_l)~D} [log p(y_w > y_l)]$\n$ = E_{(x,y_w,y_l)~D}[log\\sigma (\\frac{\\beta}{\\pi_o(x, Y_w) - \\pi_o(X,Y_l)})]$\n$ = E_{(x,y_w,y_l)~D} [log\\sigma (\\beta log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})]$"}, {"title": "DPO AS A BINARY CLASSIFICATION PROBLEM.", "content": "Another view interprets the DPO loss 1 as akin to binary classification, where the given y1 is preferable to y2, and we aim to train the parametrized policy \u03c0\u03b8 such that the preference model Po(Y1 > Y2x) = \u03c3(re(x,y1) \u2013 re(x, y2)) predicts 1, meaning we aim to maximize the quan-tity pe (see GPO paper for proper derivation), which effectively increases the margin between the probability of the preferred and unpreferred sample. Note: sometimes in DPO optimization, both probabilities are decreased, but the unpreferred sample probability is decreased more strongly; while this improves performance on the training preference dataset this might have the adverse effect of increasing the probability of other output text sequences."}, {"title": "DPO LOSS GRADIENT DERIVATION", "content": "We provide the full derivation for the loss gradient w.r.t. policy parameters. Recall the properties of the sigmoid \u03c3'(x) = \u03c3(x)(1 \u2212 \u03c3(x)), \u03c3(\u2212x) = 1 \u2212 \u03c3(x), with \u2207 log(x) = x.\n$V_\\theta L_{DPO}(x, y_w, y_\\iota; \\theta) = \u2212\\nabla_e log \u03c3 (\\rho_\\theta)$\n$ = \\frac{\\nabla_{\\pi}\\sigma (\\rho_\\theta)}{\\sigma(\\rho_\\theta)}$\n$ = \\frac{\\sigma(\\rho_\\theta) (1 \u2013 \\sigma(\\rho_\\theta)) \\nabla_{\\theta}\\rho_{\\theta}}{\\sigma(\\rho_\\theta)}$\n$ = \u2212\\sigma(\u2212\\rho_{\\theta}) \\nabla_{\\theta}\\rho_{\\theta}$\n$ = -\\beta\\sigma(-\\rho_{\\theta}) (\\frac{\\bigtriangledown_{\\theta}\\pi_{\\theta} (Y_wX)}{\\pi_{\\theta}(Y_w|X)} - \\frac{\\bigtriangledown_{\\theta}\\pi_{\\theta} (Y_lX)}{\\pi_{\\theta}(Y_l|X)}) \\qquad (23)$\n$ = -\\beta\\sigma(-\\rho_{\\theta}) ((\\frac{\u00b4\\nabla_{\\theta}\\pi_{\\theta} (Y_w|x)}{\\pi_{\\theta}(Y_w|x)}) - (\\frac{\\nabla_{\\theta}\\pi_{\\theta} (Y_l|x)}{\\pi_{\\theta}(Y_l|x)}))$"}, {"title": "EXTENDED RELATED WORKS FOR DPO", "content": "Filtered DPO (FDPO) (Morimura et al., 2024) uses a trained reward model to add a data refinement step to the DPO workflow: for a prompt and preference pair, the policy completion to the prompt is scored by the reward model, if that score is higher than the chosen completion's, this sample pair is discarded for its low quality. The authors assert that DPO is particularly prone to low text quality compared to reward-based RLHF methods.\nMuldrew et al. (2024) develop an active learning strategy for DPO to perform tuning on the pol-icy's own completions in an online setting, while assuming access to a preference oracle (reward model). Their iterative workflow begins by sampling prompts, generating two policy completions per prompt, scoring them via an acquisition function, shortlisting highest-scoring pairs, labeling these pairs with a reward model, and finally performing DPO on this subset. They propose a practi-cal acquisition function for prompt/completion pairs based on the predictive entropy of the language model, shown to be well-calibrated measure of uncertainty in LLMs (Kadavath et al., 2022a). The fine-tuning process is therefore biased towards prompts the model is more uncertain about (in terms of generation).\nXiong et al. (2024) coin \"the limited exploration of the environment\" as the main challenge of align-ing generative models through PPO and DPO. The authors analyze the reverse-KL regularized con-textual bandit formulation which hasn't rigorously been done before. They examine its behavior in offline, online, and hybrid contexts, and propose efficient algorithms with finite-sample guarantees. Notably, they introduce an iterative version of the Direct Preference Optimization (DPO) algorithm for online scenarios and a multi-step rejection sampling strategy for offline applications, showing significant improvements over standard DPO and Rejection Sampling Optimization (RSO)."}, {"title": "DERIVATIONS OF UNCERTAINTY PENALIZATION SCHEMES", "content": null}, {"title": "STANDARD UNCERTAINTY PENALIZATION", "content": "We assume reward uncertainty is known and denote u(y|x) as the standard deviation of the reward score r(x, y).\n$\\max_{\\pi_\\theta} E_{x~D, y~\\pi_\\theta(y|x)} [r(x, y) \u2013 u(y|x)] \u2013 \\beta DKL (\\pi_\\theta(y|x) || \\pi_{ref}(y|x)) $\nThe optimal policy of the problem is:\n$\\pi^*(yx) = \\frac{1}{Z_u(x)} \\pi_{ref}(y|x) e^{\\frac{(r(x,y)-u(y|x))}{\\beta}}$\nwhere Zu(x) = \u03a3y ref(y|x) exp ((r(x, y) \u2013 u(y|x))/\u03b2) is the appropriate partition function (very likely close to 1 (Rafailov et al., 2023)). Rearranging for the reward function that induces this optimal policy, results in:\nr(x,y) = Blog(()+Blog Zu(x) + u(y|x).\nNext, the optimal policy is replaced by the parameterized target policy to express the so-called \"implicit\" reward of the language model:\n$r_u(x, y) = Blog(\\frac{\\pi_\\alpha (yx)}{\\pi_{ref}(y|x)}) + Blog Z_u (x) + u(y|x).$\nFinally, the expression of the implicit reward induced by the pessimistic policy is substituted into the Bradley-Terry model, giving the DPO-like loss\n$L_{UPO}(\\pi_\\theta; \\pi_{ref}) = E_{(x,y_w,y_l)~D} [log\\sigma(\\frac{e^{u(Y_w|x)} - r_0(x, Y_w) - e^{u(Y_l|x)}r_0(x, Y_l) \\text{new}}{\\beta})]$"}, {"title": "ENERGY FACTOR PENALIZATION", "content": "We induce pessimism dividing the reward by some factor of its uncertainty to obtain a LCB equiv-alent. The derivation shows this brings both welcome and unwelcome characteristics. We begin with the RLHF objective, denoting the reward uncertainty as u(yx), and a temperature parameter as T > 0.\n$\\max_{\\pi_\\theta} E_{x~D, y~\\pi_\\theta(y|x)} [r$(x,y)e^{-u(y\\x)/\\tau}"}]}