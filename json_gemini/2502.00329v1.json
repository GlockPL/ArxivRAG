{"title": "CODDLLM: Empowering Large Language Models for Data Analytics", "authors": ["Jiani Zhang", "Hengrui Zhang", "Rishav Chakravarti", "Yiqun Hu", "Patrick Ng", "Asterios Katsifodimos", "Huzefa Rangwala", "George Karypis", "Alon Halevy"], "abstract": "Large Language Models (LLMs) have the potential to revolutionize data analytics by simplifying tasks such as data discovery and SQL query synthesis through natural language interactions. This work serves as a pivotal first step toward the development of foundation models explicitly designed for data analytics applications. To propel this vision forward, we unveil a new data recipe for post-training LLMs, enhancing their comprehension of data management and empowering them to tackle complex real-world analytics tasks. Specifically, our innovative approach includes a scalable synthetic data generation method that enables the creation of a broad spectrum of topics centered on data representation and manipulation. Furthermore, we introduce two new tasks that seamlessly bridge tables and text. We show that such tasks can enhance models' understanding of schema creation and the nuanced translation between natural language and tabular data. Leveraging this data recipe, we post-train a new foundation model, named CODDLLM, based on Mistral-NeMo-12B. To assess the language understanding and reasoning capabilities of LLMs in the realm of data analytics, we contribute AnalyticsMMLU, a benchmark containing thousands of multiple-choice questions on databases, data analysis, and machine learning. Our focus on data discovery, has resulted in the contribution of three comprehensive benchmarks that address both database and data lake scenarios. CODDLLM not only excels in performance but also sets a new standard, achieving the highest average accuracy across eight datasets. It outperforms GPT-3.5-Turbo on AnalyticsMMLU, exceeding GPT-40 by 12.1% in table selection and showing an average improvement of 24.9% in Text-to-SQL compared to the base model.", "sections": [{"title": "1 Introduction", "content": "Large language models promise to usher in a new wave of innovation in data analytics [4, 13, 40, 77]. With LLMs, users will be spared the time-consuming tasks of discovering relevant data in messy data lakes, integrating diverse sources, and preparing the data for further use. Once the data is identified and prepared, users should be able to solve problems simply by asking questions in natural language, without having to navigate complex database schemas or domain-specific query languages. One method to realize these goals is by improving LLM performance on data-related tasks through prompt engineering [23, 40]. However, this approach requires careful selection of optimal instructions and few-shot demonstrations for specific tasks, and it has not yet proven to be sufficient, often yielding erroneous answers [78]. In a related line of work, models such as Table-GPT [33], TableLlama [72], TableLLM [61], and TableGPT2 [49] have been finetuned with specific instructions for various table understanding and data wrangling tasks. Despite this progress, these works do not adequately address tasks that require a deep understanding of business concepts and their mapping to database schema and datasets. Existing works largely focus on tasks based on a single or a pair of tables and do not explore the relationships across various tables, which necessitates strong data modeling and integration capabilities.\nTo realize the vision mentioned above, we argue that LLMs must also be able to deal with messy collections of data, as often witnessed in data lakes. To do that, LLMs need to grasp a broad set of data management concepts. This includes understanding basic tabular representations, how business concepts are represented as complex database schemas or collections of interlinked datasets within a data lake, how tables can be created from other tables using views or other forms of computation, and the principles of data wrangling and integration [11]. One key challenge is that models like GPT-4 [1] and Llama [53], are primarily trained on general knowledge derived from web data, which limits their exposure to the specific training data from which they can learn fundamental data management concepts. Consequently, current LLMs perform poorly on tasks such as searching for relevant data within a large data lake that contains multiple interconnected datasets or evaluating hypotheses that require integrating data from diverse sources.\nThis work takes a first step toward developing foundation models that perform well on a broader set of data analytics tasks. We introduce a new data recipe designed for post-training any LLM, enhancing their ability to understand the \"messy\" reality of data management. More specifically, we post-train CODDLLM, a 12-billion-parameter foundation model based on Mistral-NeMo-Instruct, using our well-curated training corpus. Initially, we fine-tune CODDLLM on a large set of synthetically generated instruction-response pairs to enhance domain-specific knowledge. Next, we improve its data comprehension and problem-solving abilities by contributing two new table-text alignment tasks, followed by instruction fine-tuning on a smaller set of task-specific examples that focus on data discovery and real-world SQL code generation. Our CODDLLM demonstrates significant improvement over the base model and performs competitively with other state-of-the-art models, including GPT-40, across various tasks, including three newly curated AnalyticsMMLU datasets, three table discovery datasets, and two public TEXT-TO-SQL tasks."}, {"title": "2 CODDLLM Overview", "content": "In this section, we give an overview and motivation for how we curate data to train and evaluate CODDLLM for analytics tasks.\nScalable Data Curation Methods. Developing an expert-level LLM for data analytics is challenging due to the lack of high-quality, diverse, and supervised datasets. Human-annotated instruction datasets are limited in scale and can be expensive to obtain, while purely synthetic data often contains factual errors and lack diversity. To address these issues, we adopt an extraction-and-synthesis strategy that leverages a large-scale web corpus rich in analytics-relevant knowledge and use cases. This approach involves identifying naturally occurring instruction data, such as question-answer pairs from Stack Overflow, and augmenting documents with instruction-response pairs grounded in their content. After content distillation, we remove the plain text documents to form the final dataset. It is worth noting that by grounding responses in reference documents during the generation process, we can reduce hallucinations and increase the diversity of the dataset. Empirical studies show that post-training with instruction-aware data enhances model performance more than plain text, as it aligns the model better with domain-specific queries and responses [9, 10].\nNew Training Tasks. To improve the model's understanding of the relationship between natural language and tabular data, we design two table and text alignment tasks. The first task, TEXT-TO-SCHEMA, is to generate a table schema from textual scenario description, which allows the to model understand how different pieces of information relate to, and are expressed with, various business entities. The second task, ROW-TO-TEXT, aims to generate a text description for every row in a table. Mastering this task is essential for enhancing the model's ability to translate structured information into human-readable formats and vice versa, which is crucial for tasks such as generating reports, summaries, and data motivations."}, {"title": "3 Preliminaries", "content": "Autoregressive Language Modeling. Language provides a versatile way to represent tasks, data, inputs, and outputs, all as a sequence of tokens. Autoregressive language modeling is the basis for LLMs like GPT [6, 46]. This approach predicts the probability of a sequence of words or tokens, with each prediction conditioned on the previous elements in the sequence.\nFormally, given a language token sequence x = (x1, x2,...,xn), autoregressive language modeling decomposes the joint distribution of the sequence as the product of a series of conditional probabilities: p(x) = \u041f=1 P(xi|x1, ..., Xi\u22121), where p(x1|xo) = p(x1) is the marginal probability. With the factorized distribution and a parameterized model (e.g., Transformers [54]), the parameterized distribution po(x) can be optimized via minimizing the negative log-likelihood loss:\n$L(\\theta) = -log p(x) = - \\sum_{i=1}^{n} log p(x_i | x_1, ..., x_{i-1}).$\nQuery-based Data Analytic Tasks. This paper focuses on query-based data analytic tasks, represented as {task, data, query, answer}. Given a task description, the data to be analyzed, and a natural language query, the goal is to predict the answer, i.e, p(answer|task, data, query). For example, we can format the input text for a table selection task as: \"find tables, table schema, who was the only athlete...\". Then, the model is expected to return the table name(s) that can answer the question, e.g., \"<tables>Final_1, Athletics_1</tables>\".\nSupervised Instruction Tuning is a critical fine-tuning process employed to enhance the performance of LLMs on specific tasks by leveraging labeled datasets. Supervised instruction tuning focuses on adapting the model to follow explicit instructions and produce task-specific outputs. This process involves training the model on input-output pairs, where the inputs are typically natural language instructions or prompts, and the outputs are the desired responses or completions. The loss function of supervised instruction tuning is computed only on the \"output\" tokens to optimize the ability to execute specific tasks and understand instructions."}, {"title": "4 Training Corpus Creation", "content": "We curate a large and diverse collection of analytics-specific training corpus with three specific objectives: (1) ensuring broad coverage of various data analytics knowledge concepts, (2) improving the model's comprehension of database schema and table content, and (3) incorporating task-specific input-output examples to enhance the model's capabilities in solving real-world analytics problems.\nThe training corpus is formulated as akin to a textbook, presenting each data component as a distinct chapter. Chapter 1 contains vast knowledge filtered from web-crawled data using a purposely trained classifier. These knowledge elements are formatted into the input-output pairs to enhance the model's learning efficiency. Chapter 2 expands on this foundational stack by introducing two new tasks: 1) TEXT-TO-SCHEMA, which involves designing a schema based on a provided scenario description, and 2) ROW-TO-TEXT, which generates a text description for each row in a given table. These tasks enhance the model's ability to understand and translate between the modalities of natural language text and tabular data. Chapter 3 includes TABLE SELECTION and TEXT-TO-SQL as representative downstream analytics tasks of the real world."}, {"title": "4.1 Chapter 1 - Analytics-specific Knowledge", "content": "Training a domain-specific LLM requires a vast corpus of relevant knowledge. However, well-organized datasets on data analytics are still scarce. Recent studies have explored the use of seed data to prompt LLMs in order to expand domain-specific datasets [3, 63, 77]. While this approach shows promise, synthesized data that lacks proper references often exhibits significant biases, lacks diversity, and is prone to hallucinations [66]. To address these challenges, we use a three-step pipeline to curate large-scale analytics-related instruction-response pairs:\n(1) Filtering: Identifying and extracting data analytics-related documents - such as finance analysis, sales prediction, and code-related data - from large-scale web sources;\n(2) Instruction Creation: Converting plain text documents into question-answer pairs via extraction and synthesis;\n(3) Assessment: Evaluating the extracted QA pairs to eliminate low-scored examples and enhance dataset quality.\n4.1.1 Step 1: Filtering Analytics-specific Documents. We use the FineWeb-Edu dataset [36] as our source, which contains 1.3 trillion tokens of educational-quality web pages. Due to the dataset's multi-domain content, manually identifying data analytics-related documents presents significant challenges. To address this, we develop a model-based filtering approach to extract relevant documents. This process consists of three stages: a) LLM as a document rater, by leveraging LLMs to assess document relevance in order to obtain labels; b) Training a content classifier, by developing a supervised model based on rated samples; c) Deploying the trained filter by applying the classifier to extract analytics-related documents. We now detail these stages.\nLLM as a Document Rater. Training the document classifier requires both positive samples (high-quality documents relevant to data analytics) and negative samples (low-quality or minimally relevant documents). Inspired by the effectiveness of LLM-as-a-judge in automatic text evaluation [16], we sample approximately"}, {"title": "4.1.2 Step 2: Instruction Extraction/Synthesis", "content": "Recent studies have demonstrated the effectiveness of instruction data in post-training LLMs, showing that LLMs perform better when trained on instruction-response formatted data rather than plain text with the same semantic meaning [9]. Motivated by the extraction and refinement strategy for extracting mathematical contents from raw documents [66], we propose an extraction and synthesis strategy. For each filtered document, we either extract native question-answer pairs using predefined rules or generate synthetic question-answer pairs by grounding them in the documents and ensuring explicit references to the content. The goal is to construct a synthetic instructed version through content distillation and then remove the plain text documents to form the final dataset.\nExtraction. We first use regular expressions to identify potential indicators of QA pairs - \"[Qq]uestion:\" and \"[Aa]nswer:\" \u2013 to classify the 5.8M collected analytics-related documents. Among these, about 2.8K documents contained both question and answer keywords. We then leverage Claude-3.5-Sonnet to extract question-answer pairs from these documents, yielding a total of 46K QA pairs, as a single document might contain multiple QA pairs. After conducting a pilot study with two human annotators on 300 samples, we observed that 97% of the extracted pairs were deemed valid and, therefore, can be used as our training corpus.\nSynthesis. For the remaining documents where explicit question-answer pairs cannot be extracted, we prompt Claude-3.5-Sonnet to synthesize question-answer pairs of varying difficulty levels, ensuring the generation of relevant QA pairs of high quality. Specifically, we require that the generated QA pairs meet the following criteria: 1) Varying difficulty: Include questions ranging from basic common sense to advanced data-related topics. 2) Context-dependency: Ensure simple questions rely on common knowledge, while complex ones require provided context for answers. 3) Diverse format: Use varied question types beyond \"How\" and \"What\", and encourage longer, detailed questions and answers where possible. Following this QA generation pipeline, we obtain about 8.8M QA pairs. Below is an example of the synthesized question-answer pair and the original document content, where the original document is a passage about the two outputs from a forecasting operation in Excel."}, {"title": "4.1.3 Step 3: Assessment", "content": "After obtaining all QA pairs, we employ a widely-used LLM-as-a-Judge, Prometheus-eval [25], to conduct two rounds of filtering for each QA pair, further improving the quality of Chapter 1 data. In the first round of screening, we focus on the completeness of Questions and Answers, ensuring that all information necessary to arrive at the answer is contained within the Question text. In the second round of screening, we further evaluate the accuracy of the answers, ensuring that only correct question-answer pairs are retained in the training dataset. Eventually, we curate a dataset of 8.8 million instruction-response pairs, comprising a total of 0.9 billion training tokens."}, {"title": "4.2 Chapter 2 - Table and Text Alignment", "content": "In this chapter of the textbook, we design two tasks aimed at enhancing the LLM's ability to understand and process both tabular and textual data. These tasks are designed to facilitate the model's ability to translate between textual and structured data modalities.\n4.2.1 TEXT-TO-SCHEMA. The objective of this task is to generate a table schema from an input description of a database or information system (e.g., a credit card transaction system). Training on this task enables the model to understand how different pieces of information relate to each other within a structured representation. The underlying hypothesis here is that by learning to design schemas, the model will also develop the ability to interpret schemas when it receives them as inputs [37].\nThe main challenge in acquiring scenario descriptions and schema pairs stems from their infrequent co-occurrence within a single database or document. To tackle this issue, we generate synthetic pairs by translating well-structured schemas into multiple scenario descriptions using LLMs. The data generation steps begin with filtering 4,399 high-quality schemas from the overall 221,171 schemas in SchemaPile [12]. We use Claude-3.5-Sonnet to assess the quality of the data. After classification, three different scenario descriptions are generated for each schema. These generated descriptions are then evaluated using Prometheus-eval [25] based on the criterion, \"Is the scenario description concise and relevant?\". Only descriptions that receive a score higher than four are retained. As a result, we generate 4.8 thousand examples (refer to Table 1).\n4.2.2 Row-to-Text. Given a table and contextual data, the goal is to generate a detailed text description for a given row in the table. Training a model on a large number of Row-TO-TEXT examples improves its ability to convert structured tabular data into meaningful, fluent, and accurate text. Below, we present an input-output example from the sports domain. The input includes a task instruction and a single row from a table, along with the page title, section title, and caption. The output is a detailed text description."}, {"title": "4.3 Chapter 3 - Data Analytics Tasks", "content": "Chapter 3 presents carefully curated training examples designed to enhance model performance on downstream analytics tasks, with a focus on real-world applications. We prioritize two critical tasks: TABLE SELECTION and TEXT-TO-SQL conversion. These foundational tasks are selected to provide our model with a comprehensive understanding of datasets, enabling and augmenting their capacity to execute more complex analytics tasks.\n4.3.1 Table SELECTION. Relevant data selection from large collections of datasets is a long-standing challenge that organizations struggle with to this day [21]. In this task, we focus on question-based search, where the goal is to identify one or more data pieces that contain the necessary information to answer a user-specified question. The data can be structured or unstructured, often requiring integration from multiple sources. In this context, the ability to recognize relationships between datasets can significantly enhance both the accuracy and efficiency of task completion.\nTo create training examples, we use the training set of BIRD and Open-WikiTable and convert these datasets to serve the needs of the TABLE SELECTION task. Section 5.2 gives details on task examples.\nLeveraging BIRD for TABLE SELECTION. The BIRD dataset [32] was originally designed for text-to-SQL tasks, where each example includes a question, multiple tables from a database, and a corresponding ground truth SQL query. We select 8,954 questions from its training set and include all tables from the same database as potential candidates. We use only the schemas to highlight table"}, {"title": "4.3.2 TEXT-TO-SQL", "content": "TEXT-TO-SQL is a well-established data analytical task that aims to convert natural language questions into SQL queries, extracting desired information with proper computation and transformation performed on relational databases. The challenge lies in accurately interpreting the semantics of the input text, mapping it to the appropriate database schema, and generating a syntactically correct SQL query that fulfills the user's intent. Unlike traditional methods that rely on predefined templates or rules[31], LLMs have become the predominant tools for this task, as highlighted in recent studies [15, 45, 51]. Common approaches with LLMs include prompt engineering and task-specific fine-tuning. Prompt engineering employs techniques such as few-shot learning and multi-step reasoning without updating the model weight, but sometimes producing incorrect results for complex SQL queries. On the other hand, fine-tuning enhances performance by training the LLM on task-specific data, but it compromises the model's general instruction-following ability. In contrast, CODDLLM takes a different approach by offering a domain expert model that excels not only in TEXT-TO-SQL but also across other analytical tasks.\nWe extract a sample of approximately 9K examples from the BIRD [32] dataset's training set and 7K examples from the Spider [64] dataset's training set. Additionally, we incorporate 105.8K examples from synthetic text-to-SQL [39]. For each example, we construct the input text prompt by concatenating three components: <SCHEMA> <TASK INSTRUCTION> <QUESTION>. To represent the schema, we use CREATE TABLE SQL statements for the tables. The output is the SQL query enclosed within the <SQL> tags. For more data processing details, see Section 5.3."}, {"title": "4.4 Model Architecture", "content": "CODDLLM is built on top of Mistral-NeMo-Instruct, a decoder-only Transformer of 12B parameters, with the following specifications:\n\u2022 128k vocabulary size\n\u2022 Rotary Positional Embedding (RoPE) [50]\n\u2022 40 Transformer layers [54]\n\u2022 5,120 token dimension and 14,436 hidden dimensions\n\u2022 128 head dimensions\n\u2022 32 attention heads and 8 grouped query attention [2]\n\u2022 SwiGAU activation [48]"}, {"title": "4.5 CODDLLM Training", "content": "We train CODDLLM using the Nvidia NeMo framework, leveraging its capabilities for efficient distributed training with data and model parallelism techniques.\nWe begin the supervised instruction-tuning process using input-output pairs from Chapter 1 (see Section 4.1). By focusing on the basic knowledge of data analytics, the model can establish a strong understanding of key concepts, terminology, and reasoning patterns specific to data analytics. This foundational knowledge is crucial before the model addresses more complex, task-specific scenarios. We train the Chapter 1 data for one epoch. As noted by Hernandez et al. [18], repeated tokens can negatively impact performance.\nNext, we continue with supervised fine-tuning using data from Chapters 2 and 3 to improve the model's generalization capabilities, enabling it to apply its foundational knowledge to diverse downstream tasks more effectively. To ensure consistency across datasets, we standardize the format of these chapters in our analytic data corpus, aligning them with a unified data structure. This standardization ensures that the fine-tuned models can process data uniformly, regardless of the original chapter formats, thereby streamlining the training process and enhancing its efficiency.\nTraining Configurations. We use the AdamW [35] optimizer with \u03b2\u2081 = 0.9, \u03b22 = 0.98, \u20ac = 10\u22128, and a weight decay of 0.01. We set the maximum learning rate to Irmax = 1e-6, the minimum learning rate to Irmin = 0, with a Cosine learning rate scheduler to allow the model to make fine-grained adjustments with the labeled data.\nHow to choose the CODDLLM checkpoint?. We train the Chapter 1 data for one epoch, followed by training on Chapter 2 and Chapter 3 data for two epochs while mixing 10% uniformly selected Chapter 1 data. The final endpoint is selected as CODDLLM for evaluation in all the following experiments."}, {"title": "5 Evaluation Tasks and Experiments", "content": "We evaluate CODDLLM's reasoning capabilities across various analytics tasks, mainly focusing on domain-knowledge testing, table selection, and text-to-SQL tasks.\nLLM Baselines. We compare CODDLLM with five open-source LLMs, all obtained from Hugging Face: Mistral-7B-Instruct-v0.3, Codestral-22B-v0.1, Mistral-Small-Instruct-2409, Mixtral-8x7B-Instruct-v0.1, as well as the base model Mistral-Nemo-Instruct-2407. We use the instruction-tuned versions for all baseline models that have been fine-tuned on extensive general-purpose tasks. Meanwhile, we include three closed-source OpenAI models: GPT-3.5-Turbo, GPT-40-mini, and GPT-40, establishing strong baselines. Additionally, we included some task-specific baselines where appropriate. Unless explicitly specified, we report results on CODDLLM and all the baselines in a zero-shot setting, with no demonstration examples provided during inference.\nEvaluation Datasets. Table 2 provides an overview of evaluation benchmarks categorized into three main tasks: AnalyticsMMLU, TABLE SELECTION, and TEXT-TO-SQL. It lists datasets used for evaluation, the number of examples in each, the metric applied, their data sources, and whether there are training sets included in the training corpus (i.e., in or out of distribution). We provide a detailed explanation of the data generation procedure in the next sections."}, {"title": "Inference Sampling Hyperparameters", "content": "We experiment with different sampling hyperparameters: temperatures of 0.0, 0.7, and 1.0, and top_p values of 0.99 and 0.95. Lower temperatures yield more predictable responses, while higher temperatures encourage creativity. The top_p value used in the nucleus sampling defines the range of tokens considered during generation, with higher values expanding the range. After tuning, we fix the hyperparameters for all subsequent experiments. For AnalyticsMMLU, TABLE SELECTION and TEXT-TO-SQL, we set the temperature to be 0.0, 0.7, 1.0 and top_p as 0.99, 0.95, 1.0, respectively.\nInference and Model Serving. We use the vLLM model-serving framework [28] for inference. First, we convert the saved model checkpoints from Nemo format to Huggingface format using a conversion toolkit. We then deploy the Huggingface-formatted model using vLLM.\nResults Overview. Table 3 presents the accuracy results across eight datasets, along with the average accuracy scores for each task. Notably, CODDLLM achieves the highest overall score of 0.697, surpassing the base model by 39.3% and outperforming the best model (GPT-40) by 4.0%.\nThe evaluation consists of three tasks across eight datasets. Among these, the three AnalyticsMMLU datasets are completely new. We do not explicitly include any multi-choice questions in the training corpus; however, the synthesized QA pairs in Chapter 1 may contain such question types. For the TABLE SELECTION task, we add the training sets of BIRD-TS and Open-WikiTable-TS into the training corpus as in Chapter 3 and evaluate on the test sets. Notably, we have not included any examples from WikiPage-TS in the training corpus, make it a total new dataset for the post-trained model. Therefore, we mark MCQ-DB, MCQ-DA, MCQ-ML, and WikiPage-TS as \"unseen\" datasets and others as \"seen\" datasets, where a subset of examples are included in the training corpus. From Table 3, we find that CODDLLM surpasses GPT-40 on all seen"}, {"title": "5.1 Analytics-specific Knowledge Testing", "content": "First, we examine how well CODDLLM absorbs knowledge in the analytics field. Inspired by the commonly-used benchmark, Massive Multitask Language Understanding (MMLU) [17, 55], we curate a new dataset, AnalyticsMMLU, to measure the model's capabilities in language understanding and reasoning in the analytics domain.\nNew Datasets. The AnalyticsMMLU dataset consists of thousands of multiple-choice questions (MCQs) across three critical areas in analytics: database management (DB), data analysis (DA), and machine learning (ML). This results in three distinct datasets: MCQ-DB, MCQ-DA, and MCQ-ML. The questions feature complex queries that require models to exhibit deep expertise and advanced problem-solving abilities to achieve high accuracy. We source some of the questions from textbooks and generated additional questions and answers using Claude-3.5-Sonnet. All answers are manually reviewed and revised by three annotators to ensure quality. Table 2 summarizes the data statistics. We use the accuracy score to evaluate performance.\nTask Prompts. The adopted prompt consists of a question, followed by four answer choices, and the required answer format. The task is to select the correct answer from the given choices. Here, we show an input-output example from MCQ-DA."}, {"title": "5.2 Table Selection", "content": "Table selection aims to identify the most relevant subset of tables from a pool of candidate tables to answer a specific natural language question. Understanding how datasets complement or contradict each other helps users better determine which datasets provide the most relevant information. For example, related datasets may share common attributes or originate from similar domains, resulting in richer insights when combined. Conversely, recognizing discrepancies or redundancies among datasets can help avoid errors and misinterpretations in analysis.\nDatasets. We have created three evaluation benchmarks to evaluate various scenarios for table selection (TS). The first two datasets are derived from BIRD [32] and Open-WikiTable [27]. The third dataset is our newly annotated benchmark, which includes text"}, {"title": "5.3 TEXT-TO-SQL", "content": "Datasets. We evaluate model performance on the development sets of two public benchmarks, BIRD [32] and Spider [64], consisting of 1,534 and 1,034 examples, respectively.\nEvaluation Metrics. We use execution accuracy [32] as our primary evaluation metric. This measure evaluates whether the execution results of the predicted SQL query exact match those of the gold standard SQL query. To compute execution accuracy, we adopt the evaluation script from the BIRD codebase.\nTask Prompts. In this task, the model, acting as an SQL expert, receives the database schema and a question and outputs an appropriate SQL query to retrieve the information that answers the question. To ensure a fair comparison of the model's core capability in SQL generation, we adopt a standard zero-shot TEXT-TO-SQL prompt rather than optimizing for maximum accuracy with demonstrations, chain-of-thought reasoning, or data pre-processing techniques. Our zero-shot prompt consists of three main components: (1) Task instruction: an initial prompt introducing the TEXT-TO-SQL task; (2) Data: The table schema, including table names, column names, data types, as well as domain-specific knowledge when applicable (for the BIRD dataset only); and (3) Question: the question for which a SQL statement needs to be generated. Note that we do not include any sample cell values in this prompt."}, {"title": "5.4 Ablation Studies", "content": "In this section, we conduct ablation studies to assess the impact of different training data formats and chapter selections. The accuracy scores of different model variants are presented in Table 6. Note that CODDLLM was first trained on the instruction-response pairs of Chapter 1 data, followed by training on Chapter 2 and 3 datasets.\n5.4.1 Effects of the training on plain text documents v.s. Synthetic instruction-response pairs. To examine the impact of different data formats, we applied domain-adaptive continual pre-training [24] on plain text data and supervised instruction tuning on the instructed data version. Both models were built on an instructed model (Mistral-Nemo-Instruct) and trained using the cross-entropy loss for the next token prediction. However, during the continual pretraining process, the loss is calculated across all tokens to learn general language features and structures. For supervised instruction tuning, the loss is calculated only on the output (response) tokens, focusing on specific task objectives.\nBy comparing the accuracy scores of Chapter1-PlainText and Chapter1-Instructed, both trained exclusively on Chapter 1 data but with different data formats, we note that continual pre-training on plain text often degrades performance when applied to an instructed base model. On three AnalyticsMMLU datasets, Chapter1-Instructed achieves scores of 0.681, 0.732, and 0.714, whereas Chapter1-PlainText"}, {"title": "5.4.2 Effects of the Chapter 1 Analytics-specific Knowledge Corpus", "content": "Comparing the accuracy scores of the base model and Chapter1-Instructed, we observe that introducing Chapter 1 data leads to notable gains on domain-specific tasks while maintaining comparable performance on AnalyticsMMLU. It improves accuracy on TABLE SELECTION and TEXT-TO-SQL, even without task-specific examples, demonstrating the effectiveness of instruction-aware domain adaptation. Comparing Chapter2+3 and CODDLLM, we conclude that incorporating large-scale instructed instruction-response pairs from diverse tasks (Chapter 1) enhances overall model performance across a broad range of tasks, as evidenced by improvements in AnalyticsMMLU. Furthermore, the instructed examples do not degrade the model's performance on TABLE SELECTION and TEXT-TO-SQL, demonstrating its ability to generalize effectively without compromising domain-specific capabilities."}, {"title": "5.4.3 Effects of the Chapter 2 Table and Text Alignment Data", "content": "From Table 6, by comparing Chapter1-Instructed+3 and CODDLLM that includes all the three chapters data, chapter 2's table and text alignment data is crucial for tasks involving structured information. The highest surges in performance on table-centric discovery and TEXT-TO-SQL tasks occur when the model is supplemented with this alignment data. For instance, accuracy on WikiPage-TS increases notably from 0.462 to 0.558 when Chapter 2 is added, showing how specialized alignment data about interpreting tables and text is instrumental for the model's ability to handle more complex data discovery and retrieval tasks."}, {"title": "6 Related Work", "content": "6.1 LLMs for Data Analytics\nLLMs are revolutionizing data management [56] and analysis [77], and enabling new capabilities [13] across various tasks like data discovery [23], metadata enrichment [14, 70], SQL query synthesis [67], and entity matching [43, 74, 75]. Several studies have employed \"prompt engineering\" techniques, carefully optimizing task instructions and few-shot examples to tackle specific tasks [23, 40, 43]. This approach is further enhanced by strategies like retrieval-augmented generation (RAG) [76], which reduces hallucinations by incorporating domain-specific knowledge, and vector databases that enhance efficiency through semantic search capabilities [42]. LLM-based agents go beyond simple automation by LLMs to planning, writing code, executing it in an external sandbox, and interpreting results to solve complex data science [29, 73] and analysis challenges [7, 20]. In addition to prompting LLMs without updating model weights, task-specific fine-tuning techniques have been widely adopted to"}, {"title": "6.2 Domain-specific Foundation Models", "content": "Unlike training from scratch [60], post-training is a commonly-used approach to build a domain-specific model across diverse fields such as mathematics [65], science [52], code [16], finance [60], and medicine [59]. Two main approaches are continual domain-adaptive pretraining [38, 62] and instruction tuning [71]. Continual pretraining involves training LLMs on large-scale, domain-specific text corpora. While this approach enriches the model with domain-specific knowledge, it may also degrade its ability to follow instructions effectively [24]. Instruction tuning applies the loss to well-prepared answers, necessitating a large number of input-output instruction pairs. MAmmoTH2 [66] extracts instruction-response pairs from large-scale web corpus to enhance the model's reasoning capabilities. SciInstruct [68] employs a self-reflective instruction annotation approach to generate step-by-step reasoning for unlabeled scientific questions. Magicoder [57] utilizes open-source code snippets to generate a diverse set of instructional data for coding tasks. CODDLLM also applies instruction tuning to our well-curated, large analytics-specific training corpus. Unlike existing methods for creating instructional data, we leverage reference documents to synthesize large-scale, high-quality instruction-response pairs."}, {"title": "7 Conclusion and and Future Work", "content": "This work takes an initial step toward developing an expert analytics model. We have taken the approach of curating a \"textbook\""}]}