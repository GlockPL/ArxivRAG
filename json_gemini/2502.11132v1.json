{"title": "UNITE-FND: Reframing Multimodal Fake News Detection through Unimodal Scene Translation", "authors": ["Arka Mukherjee", "Shreya Ghosh"], "abstract": "Multimodal fake news detection typically demands complex architectures and substantial computational resources, posing deployment challenges in real-world settings. We introduce UNITE-FND\u00b9, a novel framework that reframes multimodal fake news detection as a unimodal text classification task. We propose six specialized prompting strategies with Gemini 1.5 Pro, converting visual content into structured textual descriptions, and enabling efficient text-only models to preserve critical visual information. To benchmark our approach, we introduce Uni-Fakeddit-55k, a curated dataset family of 55,000 samples each, each processed through our multimodal-to-unimodal translation framework. Experimental results demonstrate that UNITE-FND achieves 92.52% accuracy in binary classification, surpassing prior multimodal models while reducing computational costs by over 10x (TinyBERT variant: 14.5M parameters vs. 250M+ in SOTA models). Additionally, we propose a comprehensive suite of five novel metrics to evaluate image-to-text conversion quality, ensuring optimal information preservation. Our results demonstrate that structured text-based representations can replace direct multimodal processing with minimal loss of accuracy, making UNITE-FND a practical and scalable alternative for resource-constrained environments.", "sections": [{"title": "1 Introduction", "content": "The rapid proliferation of multimodal fake news (misleading text combined with manipulated images) has emerged as a major threat to information integrity. Social media platforms such as Instagram, Twitter, and Threads accelerate the spread of deceptive content, making automated detection systems critical for mitigating misinformation (A\u00efmeur et al., 2023). However, existing multimodal fake news detection (FND) methods (Jun- hao and Xu, 2024; Shen et al., 2024) often require complex architectures and extensive computational resources, posing significant challenges for real-world deployment. For instance, GAMED (Shen et al., 2024) and MAGIC (Jun-hao and Xu, 2024) require over 250 million parameters, while self-learning models (Chen et al., 2024) scale beyond 7 billion parameters. These resource-intensive approaches create a substantial barrier for researchers and organizations lacking access to large-scale infrastructure. Moreover, the complexity of these systems often necessitates expertise in multiple deep learning frameworks and sophisticated deployment strategies.\nWe argue the prevailing assumption that multimodal fake news detection must rely on heavy multimodal transformers. Instead, we reframe the problem as a unimodal task by leveraging the vision-language capabilities of large multimodal models (VLMs) such as Gemini 1.5 Pro\u00b2, GPT-40\u00b3, or Claude 3.5 Sonnet\u2074. To the best of our knowledge, this is the first attempt to solve multimodal fake news detection without a dedicated multimodal classifier.\nWe introduce UNITE-FND (UNImodal Translation Enhanced Fake News Detection), a novel framework that eliminates the need for multimodal deep learning architectures. Instead of directly processing multimodal data, UNITE-FND translates images into structured textual representations using six specialized prompting strategies. This enables the use of lightweight text-only classifiers, preserving critical visual information while dramatically reducing computational overhead. Our framework achieves 92.52% accuracy on 3-way classification using an RTX 4060 laptop GPU, with fine-tuning costs as low as $1.9, a significant improvement over prior multimodal models requiring cloud-scale resources.\nThe key contributions of our work include:\n1. UNITE-FND, a framework that transforms multimodal fake news detection into an efficient unimodal classification task, eliminating the need for heavy multimodal models.\n2. We develop Uni-Fakeddit-55k, a dataset containing 55,000 samples derived from the Fakeddit (Nakamura et al., 2020) corpus, processed through six structured prompting techniques to maximize visual information extraction.\n3. A comprehensive suite of five novel metrics for evaluating image-to-text conversion quality, ensuring effective preservation of critical visual cues.\u2075\n4. We conduct extensive empirical validation, demonstrating that structured text-based representations can replace multimodal models with minimal accuracy loss, enabling efficient inference on consumer-grade hardware (0.2-8.7GB VRAM).\nOur experimental results demonstrate that UNITE-FND achieves comparable or superior performance to existing approaches while dramatically"}, {"title": "2 Related work", "content": "2.1 Early Fusion-based Approaches\nEarly fusion-based approaches established the foundation of multimodal fake news detection, with SpotFake (Singhal et al., 2019) pioneering BERT and VGG-19 integration, while EANN (Wang et al., 2018) introduced event discriminators with VGG and Text-CNN architectures. Basic CNN architectures (Segura-Bedmar and Alonso-Bartolome, 2022) achieved 88% accuracy on Fakeddit, followed by improvements through SpotFake+ (Singhal et al., 2020) (85.6% on GossipCop) and MVAE (Khattar et al., 2019) (82.4% on Weibo). SAFE (Zhou et al., 2020) and CAFE (Jin et al., 2021) advanced fusion mechanisms, while HMCAN (Qian et al., 2021) and VERITE (Papadopoulos et al., 2024) introduced attention-based architectures. These early approaches, while groundbreaking, suffered from rigid fusion mechanisms and limited interaction between modalities.\n2.2 Cross-Modal Interaction Frameworks\nSophisticated approaches focusing on modality interactions aim to solve issues with fusion-based techniques. MIMOE-FND (Liu et al., 2025)"}, {"title": "2.3 Large Model Integration", "content": "In recent years, the release of large vision-language models introduced new possibilities, with FND-CLIP (Zhou et al., 2023) achieving 94.2% on PolitiFact. IMFND (Jiang and Wang, 2024) explores GPT4V and CogVLM, though with limited success (80.1% on PolitiFact). A self-learning approach (Chen et al., 2024) leverages LLMs for feature extraction without labeled data, achieving 88.88% on Fakeddit. CMA (Jiang et al., 2025) investigated few-shot learning, though with modest results (79.77% on PolitiFact). These methods primarily use large models as feature extractors or direct classifiers, whereas UNITE-FND innovatively employs Gemini 1.5 Pro as a modality translator, enabling more effective use of specialized text classification models."}, {"title": "2.4 Knowledge-Enhanced Detection", "content": "Knowledge-enhanced methods emerged as another direction, with AKA-Fake (Zhang et al., 2024) employing reinforcement learning and GAMED (Shen et al., 2024) introducing multi-expert decoupling (achieving impressive 98.46% results on the specialized Yang dataset). RaCMC (Yu et al., 2024) incorporates multi-granularity constraints with a residual-aware compensation network. These approaches achieve strong performance but require extensive knowledge bases and complex integration mechanisms. Our approach achieves comparable results through Gemini's inherent knowledge, eliminating the need for external knowledge bases."}, {"title": "2.5 Specialized Approaches", "content": "Specialized approaches have also emerged, including AMPLE (Xu et al., 2025) with emotion awareness (90% accuracy on PolitiFact), MMCFND (Bansal et al., 2024) addressing multilingual challenges across multiple Indic languages (99.6% on MMIFND), and MAGIC framework (Jun-hao and Xu, 2024) using geometric deep learning (98.8% accuracy on a curated subset of Fakeddit with 3,127 samples). While these methods excel in specific scenarios, UNITE-FND provides a more generalizable solution through its modality translation."}, {"title": "3 UNITE-FND", "content": "UNITE-FND redefines multimodal fake news detection by transforming complex multimodal analysis into a streamlined unimodal task. The key innovation lies in leveraging state-of-the-art Large Language Models (LLMs) to convert visual content into structured textual descriptions, enabling efficient text-only classification without sacrificing critical visual information."}, {"title": "3.1 Image-to-Text Conversion Framework", "content": "At the core of our approach is a robust image-to-text conversion framework powered by Gemini 1.5 Pro, a state-of-the-art multimodal large language model (LLM). We design a structured prompting strategy consisting of six distinct methods, each tailored to extract different aspects of visual information."}, {"title": "3.1.1 Basic Object Identification", "content": "The first method employs a List of Objects approach, utilizing a carefully crafted prompt to generate a comprehensive CSV-formatted inventory of distinct objects within the image. This method ensures the capture of fundamental visual elements while maintaining a structured, machine-processable output format."}, {"title": "3.1.2 Contextual Description Generation", "content": "To enhance semantic richness, we incorporate two complementary descriptive approaches: Simple Image Description and Structured Image Description. Both methods generate dual-sentence descriptions, with the first sentence capturing observable facts and the second providing contextual interpretation. This dual-layer approach ensures both concrete visual information and implied contextual cues are preserved"}, {"title": "3.1.3 Spatial Analysis", "content": "The Relational Mapping framework systematically catalogs spatial relationships through a structured JSON schema. Each object is assigned a unique identifier and location descriptor, while relationships between objects are documented with specific interaction types and confidence scores. This approach transforms complex visual-spatial information into a machine-readable format, capturing both direct physical relationships (like \"above\" or \"next to\") and interactive associations (such as \"facing\" or \"holding\"). The framework prioritizes high-confidence relationships, ensuring reliability in downstream processing while maintaining a clear representation of the image's spatial hierarchy. This method proves particularly valuable for fact-checking claims about spatial arrangements in news photos, such as verifying the authenticity of crowd sizes or the relative positioning of people in event coverage."}, {"title": "3.1.4 Manipulation Detection Components", "content": "Two specialized components focus on identifying potential image manipulations:\n\u2022 Inconsistency Detection: Performs a comprehensive analysis of visual coherence, examining lighting, perspective, boundaries, and resolution patterns. This component generates structured JSON output detailing potential manipulation indicators with associated confidence scores.\n\u2022 Scene Graph Analysis: Creates a detailed representation of the image scene, capturing object relationships, visual quality metrics, and potential manipulation artifacts. This component provides a holistic view of the image's structural integrity."}, {"title": "3.2 Uni-Fakeddit-55k Dataset Creation and Classification Pipeline", "content": "Our work introduces a family of six unimodal datasets, collectively called Uni-Fakeddit-55k, derived from the multimodal Fakeddit-700k dataset. Each dataset variant corresponds to one of our six prompting techniques, offering different perspectives on the same underlying content. Figure 3 illustrates the careful sampling strategy employed to maintain class distribution consistency with the original dataset. More details about the exact prompts and settings used to build the datasets are reported in Appendix F.\nThe textual descriptions generated by our six-component framework undergo several processing steps:\n$T_{final} = f_{merge}(T_{clean}, T_{desc})$\nwhere $T_{clean}$ represents the cleaned news title, $T_{desc}$ represents the concatenated image descrip"}, {"title": "3.3 UNITE-FND Model Architecture: Optimized for Resource-Constrained Environments", "content": "While recent approaches have achieved impressive accuracy through increasingly complex architectures, UNITE-FND prioritizes efficiency without significant performance trade-offs. As shown in Table 5, current state-of-the-art approaches like MAGIC (Jun-hao and Xu, 2024) and GAMED (Shen et al., 2024) require over 250 million parameters, while self-learning approaches exceed 7 billion parameters. In contrast, our most efficient implementation using TinyBERT requires only 14.5 million parameters while achieving 87.4% accuracy on Uni-Fakeddit-55k. Even our largest model variant using DeBERTa-large (400M parameters) requires significantly fewer parameters than certain contemporary approaches while achieving a competitive accuracy of 92.5%.\nThis efficiency translates directly into practical advantages: (a) Our models require as little as 0.2GB VRAM (TinyBERT) and scale up to just 8.7GB (DeBERTa), enabling deployment on consumer-grade hardware without the need for specialized infrastructure. (b) The cost-effectiveness of our approach is equally notable, with full model fine-tuning achievable for as little as $2 using cloud-based compute resources. (c) Additionally, the flexibility of UNITE-FND allows organizations to select model variants that best align with their accuracy-efficiency trade-offs, making it adaptable to diverse operational constraints. These characteristics position UNITE-FND as a highly scalable solution for resource-constrained environments while maintaining performance competitive with significantly more complex architectures."}, {"title": "4 Evaluation Metrics", "content": "To comprehensively evaluate the quality and effectiveness of our image-to-text conversion framework, we propose a novel suite of five complementary metrics, each designed to capture different aspects of information preservation and transfer quality. These metrics are then combined into a unified Composite Information Quality Score (CIQS)."}, {"title": "4.1 Image Preservation Rate (IPR)", "content": "The Image Preservation Rate quantifies how effectively the textual description retains the essential information present in the original image. We improve upon traditional correlation-based approaches by introducing a non-linear transformation that better captures the relationship between feature spaces. Let I be the image features and T be the text embeddings:\n$IPR(I,T) = 1 \u2013 e^{-5s}$\nwhere s represents the scaled cosine similarity between normalized projections $P_{I}$ and $P_{T}$ in a common space of dimension $d\\leq min(dim(I), dim(T))$:\n$s = \\frac{cos(P_{I}, P_{T}) +1}{2}$\nThe projections are obtained through Xavier-initialized linear transformations to ensure stable feature mapping across modalities."}, {"title": "4.2 Semantic Coverage Score (SCS)", "content": "SCS evaluates the comprehensiveness of the generated description, focusing on object-centric content. For object list techniques, we employ a multi-criteria evaluation:\n$SCS(T) = \\omega_{l} \\cdot L + \\omega_{s} \\cdot S + \\omega_{c} \\cdot C$\nwhere $L = min(|O|/10, 1.0)$ represents the normalized object count, S quantifies specificity by penalizing generic terms, and C measures completeness based on the presence of multi-word descriptions. The weighting factors are set as $\\omega_{l} = 0.3, \\omega_{s} = 0.4$, and $\\omega_{c} = 0.3$."}, {"title": "4.3 Information Specificity Score (ISS)", "content": "ISS measures the semantic depth and specificity of the generated description:\n$ISS(T) = \\frac{1}{|W|} \\sum_{w\\in W} \\frac{D(w)}{D_{max}}$\nwhere W is the set of content words, D(w) represents the WordNet depth of word w, and $D_{max}$ is the maximum WordNet depth (approximately 20)."}, {"title": "4.4 Structural Information Retention (SIR)", "content": "SIR evaluates the preservation of structural relationships, with distinct formulations for different technique types:\n(1) For graph-based techniques:\n$SIR(G) = \\frac{1}{4}(N_{s} + E_{d} + R_{d} + C_{s})$\nwhere: $N_{s} = min(|V|/10, 1.0)$ represents the node score, $E_{d}$ is the edge density, $R_{d} = min(|R|/5, 1)$ measures relationship diversity and $C_{s} = \\frac{conf_{v}+conf_{e}}{2}$ indicates confidence score.\n(2) For text-based techniques:\n$SIR(T) = \\frac{|S|}{5}$\nwhere S is the set of sentences."}, {"title": "4.5 Modality Transfer Efficiency (MTE)", "content": "MTE assesses both the efficiency of information transfer and preservation of feature complexity:\n$MTE(I, T) = 0.7 \\cdot S + 0.3 \\cdot C_{r}$\nwhere S represents cosine similarity between normalized projections, $C_{r} = \\frac{min(\\sigma_{I}, \\sigma_{T})}{max(\\sigma_{I}, \\sigma_{T})}$ measures complexity ratio, $\\sigma_{I}, \\sigma_{T}$ are standard deviations of projected features."}, {"title": "4.6 Composite Information Quality Score (CIQS)", "content": "Finally, we combine all metrics into a single comprehensive score:\n$CIQS = (IPR \\cdot SCS \\cdot ISS \\cdot SIR \\cdot MTE)^{1/5}$\nThis geometric mean ensures a balanced contribution from all components while penalizing poor performance in any single metric. The CIQS provides a holistic assessment of the image-to-text conversion quality, considering preservation, coverage, specificity, structure, and transfer efficiency."}, {"title": "5 Results", "content": "5.1 Quality of Image-to-Text Conversion\nWe first evaluate the quality of our image-to-text conversion strategies using our proposed metrics suite (Table 1), where each metric is normalized to a [0,1] range except for SIR which can exceed 1 due to its structural complexity measure. All strategies demonstrate strong Information Preservation Rate (IPR > 0.91), indicating effective retention of core visual information. Scene Graph Analysis and Inconsistency Detection achieve particularly high Semantic Coverage Scores (SCS > 0.96), suggesting comprehensive capture of image content. While List of Objects shows the highest Information Specificity Score (ISS = 0.4635), Scene Graph Analysis excels in Structural Information Retention (SIR = 2.1231), reflecting its superior ability to preserve relational information. Modality Transfer Efficiency (MTE) remains consistent across all strategies (0.65), indicating stable information transfer between modalities."}, {"title": "5.2 Classification Performance", "content": "5.2.1 Comparison with Vision-Language Models\nUNITE-FND framework achieves state-of-the-art performance on the Uni-Fakeddit-55k dataset, significantly outperforming existing vision-language models (Table 2). The framework achieves 92.52% accuracy in binary classification, substantially surpassing recent models like Llama-3.2-11B-Vision (63.92%) and Video-LLaVA-7B (59.34%). This marked improvement demonstrates the effectiveness of our modality translation approach over direct multimodal processing.\n5.2.2 Comparison with Existing Fake News Detection Model\nWhen compared to existing multimodal fake news detection systems (Table 3), UNITE-FND demonstrates substantial improvements across all metrics. Our framework achieves gains of:\n\u2022 Accuracy: +3.64%(\u2191over Self-learning (Chen et al., 2024))\n\u2022 Precision: +6.16% (\u2191 over SpotFake+ (Singhal et al., 2020))\n\u2022 Recall: +7.23% (\u2191 over CAFE (Jin et al., 2021))\n\u2022 F1-score: +6.63% (\u2191 over Self-learning (Chen et al., 2024))\nThese results demonstrate that our approach of converting multimodal fake news detection into a unimodal problem through sophisticated image-to-text conversion not only simplifies the architecture but also leads to superior performance. The consistent improvements across all metrics suggest that our framework better captures the nuanced relationships between visual and textual content in fake news detection.\nWhile our results demonstrate strong performance, it is essential to consider variations in"}, {"title": "5.3 Observations on Proposed Evaluation Metrics", "content": "Analysis of our evaluation metrics (Table 1) reveals distinct patterns that offer key insights into the effectiveness of different prompting strategies. All techniques exhibit consistently high Information Preservation Rates (IPR \u2248 0.917), which explains the substantial improvements observed over text-only baselines.\nObservation 1. The Semantic Coverage Score (SCS) highlights a clear distinction between simpler and more sophisticated approaches. JSON-based methods such as Scene Graph (0.9668) and Inconsistency Detection (0.9780) achieve significantly higher coverage than simpler strategies like List of Objects (0.7417). However, this increased coverage does not translate into proportional classification gains, suggesting that transformer models may not fully exploit the additional structured information.\nObservation 2. A notable inverse relationship emerges between technique sophistication and Information Specificity Scores (ISS). Simpler methods, such as List of Objects (0.4635), outperform more complex strategies like Scene Graph (0.3677). This pattern suggests that while structured representations capture extensive visual information, they may introduce noise that hinders model interpretability.\nObservation 3. The Structural Information Retention (SIR) metric shows considerable variation, with Scene Graph Analysis achieving the highest score (2.1231) due to its detailed encoding of spatial relationships. Interestingly, Relational Mapping performs worse (0.2055) than even basic descriptions, likely because it emphasizes object interactions rather than spatial structure.\nObservation 4. The Composite Information Quality Score (CIQS) presents an unexpected discrepancy: Scene Graph Analysis achieves the highest CIQS (0.8484) but ranks second in classification performance, whereas Structured Image Description, which ranks fourth in CIQS (0.5827), consistently outperforms other techniques across transformer architectures. This suggests that maximizing information capture alone is insufficient; rather, effective fake news detection requires balancing information complexity with model interpretability and processing efficiency.\nThese findings highlight the importance of our proposed evaluation metrics that not only capture rich multimodal information but also align with model interpretability and processing capabilities. By providing deeper insights into the relationship between structured textual representations and classification performance, our metrics offer a valuable framework for optimizing multimodal-to-unimodal transformations in misinformation detection."}, {"title": "6 Conclusion and Future Works", "content": "We present UNITE-FND, a novel framework that reframes multimodal fake news detection as a unimodal task through structured image-to-text conversion. Our approach significantly reduces computational overhead while maintaining competitive accuracy, enabling deployment on consumer-grade hardware. Experimental results demonstrate that structured textual representations effectively replace direct multimodal processing with minimal performance loss, highlighting the potential of language models in preserving critical visual information. Future work can explore adaptive prompt optimization, multilingual adaptation, and temporal modeling to enhance robustness. Additionally, improving explainability and mitigating adversarial vulnerabilities will be crucial for responsible deployment. By balancing efficiency, accessibility, and interpretability, UNITE-FND lays the foundation for scalable, real-world misinformation detection."}, {"title": "Limitations", "content": "While UNITE-FND shows promising results, there are few limitations. First, our approach's reliance on VLMs (specifically Gemini 1.5 Pro) for image-to-text conversion introduces a potential bottleneck in processing speed. Second, the quality of text generation can vary based on the VLM's understanding of complex visual scenarios. Third, while our method significantly reduces computational requirements for deployment, it still requires access"}, {"title": "Ethical Considerations", "content": "Our work utilizes the Fakeddit dataset, which contains Reddit posts that have been carefully curated and pre-processed by its creators to address privacy and ethical concerns. Given the dataset's thorough preparation, which includes removal of personally identifiable information and offensive content, we maintained its existing privacy safeguards without additional processing to preserve data integrity and statistical properties. Our image-to-text conversion strategies are designed to extract only objective visual elements, ensuring that generated descriptions exclude references to specific individuals or private information. While our approach enhances accessibility and efficiency in fake news detection, we recognize the potential risks associated with its misuse, including the possibility of censorship or content manipulation. We advocate for responsible and transparent use, respecting individual privacy and freedom of expression, with clear communication about its deployment and the option for users to opt-out. We acknowledge the possibility for potential false positives and false negatives, and we suggest continuous research, development, and stakeholder feedback for system refinement. We declare no competing interests. The research was conducted independently, and the framework was developed for academic and public benefit aiming to better understand and fight misinformation online."}, {"title": "A Literature Review", "content": "This appendix provides a comprehensive review of multimodal fake news detection approaches, organized by their methodological focus and chronological development."}, {"title": "A.1 Early Fusion-based Approaches (2019-2022)", "content": "Early work in multimodal fake news detection established foundational architectures for combining visual and textual information:\n\u2022 SpotFake (Singhal et al., 2019) pioneered multimodal fusion using BERT and VGG-19, achieving 89.23% accuracy on Weibo\n\u2022 MVAE (Khattar et al., 2019) introduced a bimodal variational autoencoder approach, reaching 82.4% accuracy on Weibo\n\u2022 SpotFake+ (Singhal et al., 2020) enhanced the architecture with XLNet, achieving 85.6% on GossipCop\n\u2022 SAFE (Zhou et al., 2020) introduced similarity-aware fusion, reaching 87.4\n\u2022 MM-CNN (Segura-Bedmar and Alonso-Bartolome, 2022) demonstrated that basic CNN architectures could achieve 87.0% accuracy on Fakeddit"}, {"title": "A.2 Advanced Architectures (2023-2024)", "content": "Recent approaches have introduced more sophisticated architectural innovations:\n\u2022 MPFN (Wang et al., 2023) explored progressive fusion to capture both shallow and deep features (83.8% on Weibo)\n\u2022 FND-CLIP (Zhou et al., 2023) leveraged CLIP's pre-trained representations (94.2% on PolitiFact)\n\u2022 MAGIC (Jun-hao and Xu, 2024) introduced graph neural networks, achieving 98.8% on a curated Fakeddit subset\n\u2022 GAMED (Shen et al., 2024) employed modal decoupling with expert networks (93.93% on Fakeddit)\n\u2022 RaCMC (Yu et al., 2024) introduced residual-aware compensation networks (92.2% on Weibo-21)"}, {"title": "A.3 Knowledge-Enhanced Methods", "content": "Several approaches have incorporated external knowledge:\n\u2022 AKA-Fake (Zhang et al., 2024) used reinforcement learning for knowledge graph construction (91.9% on PolitiFact)\n\u2022 AMPLE (Xu et al., 2025) integrated emotion-aware analysis (90% on PolitiFact)\n\u2022 DAAD (Su et al., 2024) introduced dynamic analysis mechanisms (94.2% on Weibo-21)"}, {"title": "A.4 Large Model Integration", "content": "Recent work has explored the potential of large vision-language models:\n\u2022 IMFND (Jiang and Wang, 2024) investigated zero-shot capabilities of GPT4V (80.1% on PolitiFact)\n\u2022 CroMe (Choi et al., 2025) combined BLIP2 with tri-transformers (97.4% on Weibo)\n\u2022 MIMOE-FND (Liu et al., 2025) introduced mixture-of-experts architecture (95.6% on Weibo-21)"}, {"title": "A.5 Specialized Applications", "content": "Several approaches have focused on specific challenges:\n\u2022 MMCFND (Bansal et al., 2024) addressed multilingual detection, achieving 99.6% on MMIFND\n\u2022 Self-Learning FND (Chen et al., 2024) explored unlabeled data utilization (88.88% on Fakeddit)\n\u2022 MGCA (Guo et al., 2024) introduced multi-granularity clue alignment (91.3% on Weibo-21)"}, {"title": "A.6 Dataset Usage Patterns", "content": "Analysis of the literature reveals several commonly used datasets:\n\u2022 Weibo and Weibo-21: Popular for Chinese content evaluation\n\u2022 PolitiFact and GossipCop: Standard benchmarks for English news\n\u2022 Fakeddit: Largest dataset, often used for a comprehensive evaluation\n\u2022 Specialized datasets: MMIFND (multilingual), Yang (domain-specific)\nThis comprehensive review demonstrates the field's evolution from simple fusion approaches to sophisticated architectures incorporating large models and specialized techniques. Our UNITE-FND framework builds upon these advances while introducing a novel approach to modality translation that achieves competitive performance with reduced computational requirements."}, {"title": "B Model Architecture and Performance Studies", "content": "While the main paper focuses on UNITE-FND's key contributions and primary results, this appendix provides a detailed comparison of model architectures and their respective performance metrics. Table 5 presents a comprehensive overview of recent approaches in multimodal fake news detection, highlighting the relationship between model complexity and detection accuracy.\nCurrent state-of-the-art approaches demonstrate a clear trend toward increasingly complex architectures. MAGIC (Jun-hao and Xu, 2024) and GAMED (Shen et al., 2024) employ sophisticated neural networks with over 250 million parameters each, while self-learning approaches (Chen et al., 2024) utilize large language models ranging from 7.2 to 13.2 billion parameters. These approaches achieve impressive accuracy but at substantial computational cost.\nUNITE-FND, in contrast, demonstrates that efficient architectures can achieve competitive performance through effective modality translation. Our implementation spans a range of model sizes:\n\u2022 TinyBERT (14.5M parameters) achieves 87.4% accuracy, matching the performance of more complex multimodal architectures while using fewer parameters than even basic unimodal CNNs from earlier approaches (Segura-Bedmar and Alonso-Bartolome, 2022)\n\u2022 DistilBERT (66M parameters) and BERT-base (110M parameters) demonstrate that moderate-sized models can achieve strong performance (88.8% and 89.2% respectively)\n\u2022 Our larger models, RoBERTa-large (355M parameters) and DeBERTa-large (400M parameters), achieve competitive accuracy (91.5% and 92.5%) while still maintaining signifi"}, {"title": "C Ablation Studies", "content": "Detailed analysis of model performance across different architectures (Table 6) reveals several significant patterns and insights. RoBERTa-large demonstrates superior performance across all classification settings, particularly with the Structured Image Description technique, achieving 87.95%, 91.45%, and 91.88% accuracy on 6-way, 3-way, and 2-way classification tasks respectively. This represents substantial improvements over its text-only baseline (78.80%, 86.99%, 87.23%).\nThe performance scaling across model sizes is particularly noteworthy. TinyBERT, despite its compact architecture, shows remarkable improvements with visual information integration, achieving up to 87.36% accuracy in binary classification compared to its 83.24% text-only baseline. DistilBERT demonstrates even stronger gains, with its Structured Image Description performance reaching 88.75% in binary classification, a 3.76 percentage point improvement over its text-only counterpart. BERT-base follows a similar pattern, achieving 89.21% with Structured Image Description versus 85.09% with text alone.\nAcross all models, certain patterns emerge in prompting strategy effectiveness:\n\u2022 Structured Image Description consistently outperforms other techniques, suggesting that well-organized, natural language descriptions are most effective for transformer models\n\u2022 Simple Image Description and List of Objects show strong performance despite their simplicity, often achieving within 1-2 percentage points of the best results\n\u2022 Scene Graph Analysis performs particularly well with larger models (87.91% with RoBERTa on 6-way classification) but shows diminished returns with smaller architectures\n\u2022 Relational Mapping and Inconsistency Detection, while sophisticated, generally underperform simpler techniques, possibly due to the challenge of effectively encoding complex spatial and visual relationships in text"}, {"title": "E.1 Technical Advantages", "content": "\u2022 Context Window: With a 2-million token context window, Gemini 1.5 Pro can handle images of varying complexity and generate detailed descriptions without truncation\n\u2022 Response Quality: Produces more consistent and detailed outputs compared to open-source alternatives\n\u2022 Flexibility: Supports multiple prompting strategies without triggering content restrictions"}, {"title": "E.2 Limitations of Alternatives", "content": "E.2.1 Llama 3.2 Vision\n\u2022 Built-in guardrails in the mlx-community/Llama-3.2-11B-Vision-Instruct-8bit model frequently block outputs for our prompting tasks\n\u2022 Lower performance in zero-shot classification (63.92% vs. base accuracy)\n\u2022 More restrictive in handling complex prompting strategies\nE.2.2 GPT-40\n\u2022 Limited to 128K token context window\n\u2022 Significantly higher operational costs\n\u2022 Less suitable for large-scale deployment\nE.3 Accessibility Considerations\n\u2022 $300 free trial credit enables initial deployment without significant investment\n\u2022 Cost-effective for both research and small-scale applications\n\u2022 Lower per-token costs compared to GPT-40\nThe combination of superior technical capabilities, fewer restrictions, and better accessibility makes Gemini 1.5 Pro the optimal choice for our framework's vision-to-text translation component. Its performance characteristics and cost structure align well with our goal of democratizing fake news detection capabilities."}, {"title": "F.1 List of Objects Strategy", "content": "F.1.1 Prompt Template\nAnalyze the image and list all clearly visible objects and elements. Return a comma-separated list of distinct, identifiable objects. Focus on physical objects, not interpretations or actions. Be specific but concise in naming objects.\nF.1.2 Example Outputs\nThis strategy focuses on creating an objective inventory of visible objects without interpretation or context. The comma-separated format ensures consistent parsing and processing in downstream tasks."}, {"title": "F.2 Simple Image Description Strategy", "content": "F.2.1 Prompt Template\nDescribe this image in exactly two sentences. Return only the description, with no additional text or explanation.\nF.2.2 Example Outputs\nThis strategy focuses on generating concise, factual descriptions of image content. The two-sentence constraint ensures consistent length while allowing for both primary subject description and contextual details."}, {"title": "F.3 Structured Image Description Strategy", "content": "F.3.1 Prompt Template\nProvide exactly two sentences about this image: First sentence: State only the observable facts who and what is in the image, and where it takes place. Focus solely on what can be directly seen. Second sentence: Interpret the context - explain the likely purpose, situation, or story behind what's shown, including any relevant how or why elements. Return only these two sentences, with no additional text."}, {"title": "F.4 Relational Mapping Strategy", "content": "F"}]}