{"title": "A Comprehensive Survey on Root Cause Analysis in (Micro) Services: Methodologies, Challenges, and Trends", "authors": ["TINGTING WANG", "GUILIN QI"], "abstract": "The complex dependencies and propagative faults inherent in microservices, characterized by a dense network of interconnected services, pose significant challenges in identifying the underlying causes of issues. Prompt identification and resolution of disruptive problems are crucial to ensure rapid recovery and maintain system stability. Numerous methodologies have emerged to address this challenge, primarily focusing on diagnosing failures through symptomatic data. This survey aims to provide a comprehensive, structured review of root cause analysis (RCA) techniques within microservices, exploring methodologies that include metrics, traces, logs, and multi-model data. It delves deeper into the methodologies, challenges, and future trends within microservices architectures. Positioned at the forefront of AI and automation advancements, it offers guidance for future research directions.", "sections": [{"title": "1 INTRODUCTION", "content": "The evolution of IT operations has undergone three significant phases, manual operations, DevOps, and AIOps. Initially, IT operations were predominantly manual, relying heavily on human intervention for system monitoring, troubleshooting, and problem resolution. However, with the escalating scale and complexity of systems, the efficacy and precision of manual operations have been increasingly challenged. Subsequently, DevOps was introduced, building upon manual operations and fostering a synergistic collaboration between development and operations. Through automated deployment and continuous integration, DevOps has the capability to expedite the release of new features and rectify issues with greater speed and reliability. Nonetheless, DevOps still necessitates manual involvement in certain complex decision-making processes and tasks. To further mitigate this challenge and enhance cost-effectiveness and efficiency, AIOps leverages machine learning and data analysis to automatically collect and scrutinize vast amounts of IT operation data, enabling real-time monitoring, anomaly detection, fault localization, and automated processing of IT systems. AIOps not only augments the efficiency and accuracy of IT operations but also equips IT operations with the capacity to adapt more effectively to complex and dynamic IT environments, utilizing artificial intelligence and big data technologies."}, {"title": "2 PRELIMINARIES", "content": "In the realm of cloud services, an incident signifies any event that disrupts regular service operations or degrades SLA. Upon such incidents arise, a root cause analysis is undertaken to identify the underlying issue responsible for the disturbance. RCA, in the realm of cloud services, represents a sophisticated endeavor involving several stages:\n\u2022 Data Collection: The initial phase entails gathering relevant data about the incident from various sources, such as logs, metrics (or KPIs), traces, as well as events stemming from publications or configuration changes.\n\u2022 Data Analysis: Subsequently, the amassed data undergoes thorough analysis to discern patterns, deviations, or connections that may offer insights into the underlying cause of the incident.\n\u2022 Hypothesis Validation: Following the data analysis, potential root causes are hypothesized and subsequently validated by SREs.\n2.1 Data for RCA\nData for root cause analysis can originate from various sources, including metrics, traces, or logs. When selecting datasets for such analysis, the following types are commonly considered.  demonstrates the scope, intersection, and volume of metric data, trace data, and log data. Metrics data encompasses system metrics, such as CPU utilization, memory consumption, disk I/O activity, and network traffic, as well as transaction metrics like Queries Per Second (QPS) and Top Percent 99 (TP99). By scrutinizing fluctuations in these performance indicators, deviations from normal system behavior can be identified. Metrics often harbor a substantial amount of anomalous data, providing a direct window into potentially faulty or impacted modules, frequently leveraged for precise fault localization. Nevertheless, the intricacies of call relationships among metric data, graph construction methodologies, and the sequential triggering of metric alerts all play pivotal roles in conducting an accurate root cause analysis. Log data, encompassing application, system, and network logs, constitute the predominantly utilized form of data. These logs harbor an abundance of operational status information, encompassing error messages, warning notifications, and status alterations, among others. This diverse array of data serves as a valuable tool for identifying the underlying causes of emerging issues. However, logs often come with significant challenges, given their vast volume, unstructured nature, and diverse formats. Additionally, the rate at which these logs are gathered can be influenced by the sampling rate, which must strike a careful balance between cost-effectiveness and the adequacy of data sampling. Trace data serves as a valuable tool for documenting and analyzing request paths within distributed systems. In intricate microservice architectures, where a single request often traverses multiple services to reach completion, trace data offers critical insights. By meticulously logging the commencement and termination times of each service involved, as well as their interconnected relationships, a comprehensive map of the request's journey can be assembled. Trace data reveals a detailed link topology and call relationships, providing fine-grained information invaluable for graph-based RCA. However, the potentially extensive trace links, resulting from complex call relationships, may undergo truncation, thereby complicating the root cause analysis. demonstrates a distributed service request trace with request timeout anomaly. The trace illustrates the path taken by the request through multiple services, specifically from service A to C, then to D, and finally to the database (DB). Notably, the total time taken along the path A->C->D->DB was 1000ms, with a significant delay of 990ms observed between D and DB, indicating a possible performance degradation in DB access.\nIn addition to harnessing real-world datasets from both public and private sources, the creation of datasets can also be aided by employing diverse benchmarks. Among the numerous benchmarks available for microservices evaluation, several stand out, including TrainTicket , Sockshop , OnlineBoutique , and SocialNetwork. These benchmarks play a pivotal role in the evaluation and advancement of microservice architectures by offering standardized environments crucial for conducting RCA.\nWhen conducting root cause analysis, ensuring data reliability is absolutely essential, whether the data comes from metrics, traces or logs. For example, anomalies must be monitorable, which means they should be detectable through metrics. Moreover, these anomalies need to be identifiable through logs, requiring that they are logged with corresponding log levels. Similarly, anomalies should be reflected in traces, indicating that the impacted business processes have undergone comprehensive tracking and instrumentation. In situations where monitoring is not feasible, and anomalies cannot be identified through metrics, traces or logs, even the most advanced root cause analysis methods would be rendered useless. Therefore, ensuring the reliability of data is fundamental to the success of any root cause analysis.\n2.2 RCA Methods\nThe Dependency (Topology) Graph illustrates the intricate interdependencies among various components, modules, services, or systems, which are prevalent entities in root cause analysis. Dependency graph based approaches involve constructing a graph utilizing Key Performance Indicators (KPIs) and domain expertise. Subsequently, anomalous subgraphs or paths are identified and extracted in response to observed anomalies. The utilization of dependency graphs in root cause analysis aligns seamlessly with actual manual judgment when determining the influence among modules, components, and services. Within dependency graphs, the status of given node can directly impact other nodes that are dependent on it. Dependency graphs serve as a valuable tool for tracing potential problem propagation paths in RCA. For example, in the event of a malfunction in a foundational service, the dependency graph can reveal all upper-tier applications reliant on this service. This, in turn, facilitates the determination of the disruption's scope and potential underlying causes. In contrast to the dependency graph, topology places greater emphasis on the structure of the network, emphasizing the connections between elements rather than their dependency or causal relationships. This approach provides a comprehensive visualization of the network's layout, enabling a more holistic understanding of its components and their interactions.  illustrates a framework of graph-based RCA.\nTypically, in Bayesian Network graphical model, variables are represented as nodes, and the conditional dependencies among them are depicted by directed edges. The model adheres to the local Markov property, which asserts that any given node is conditionally independent of all its non-descendants when conditioned on its parent nodes. This property ensures a structured and efficient representation of the conditional relationships within the model.\nOperation and Maintenance Knowledge Graph (O&M Knowledge Graph) offers a way to consolidate and represent the intricate relationships existing between the various components and entities that constitute complex systems. This is accomplished by seamlessly integrating both structured and unstructured data culled from a wide array of sources. Multi-modal integration significantly aids in the comprehension and analysis of the intricate web of interactions and dependencies among system components. Construct a knowledge graph (KG) based on pre-defined entities, relationships, and their associated attributes. Utilize semantic querying and reasoning to retrieve relevant results. This can depict the knowledge graph of either a flawed application architecture or the knowledge graph representing various fault types. The constructed KG, housed in a graph database, allows for swift access and exploration. Furthermore, the semantic nodes and relationships within the KG can be leveraged for causal inference and associative analysis, enhancing the depth and breadth of insights derived from the data.  presents a knowledge graph of high CPU usage case.\nRule-Based RCA relies on predefined rules and patterns to identify the root cause of a problem. These methods work well in clear, rule-defined scenarios, but may require significant human involvement and maintenance when dealing with complex, dynamically changing IT environment issues.\nStatistical-Based RCA employs statistical techniques to assess the correlation between two Key Performance Indicators (KPIs) by analyzing the relatedness of metrics from historical data, such as the Pearson correlation coefficient or the Spearman rank correlation coefficient, to identify and predict the root cause. This method can handle large amounts of data but may require professional statistical knowledge. The effectiveness may be affected when the data quality is poor or the data volume is insufficient.\nTime Series Analysis-Based RCA. In AIOps, operational monitoring metrics and business monitoring metrics exhibit daily, weekly, and other time-series periodicities. Time series analysis can be used to identify abnormal patterns in these data, thereby finding out the root cause of the problem.\nGraph Theory-Based RCA. In the microservices environment, complex interrelations exist among services, between services and components, as well as between components and the underlying infrastructure. These interactions are reflected in various graphical representations, including dependency graphs, topological graphs, and causal graphs, which illustrate call dependencies and event causality. By examining the nodes (representing elements) and edges (representing relationships) within these graphs, researchers can conduct root cause analysis. For example, analyzing graph paths and connectivity reveals the patterns of problem propagation, pinpointing the routes through which failures spread. Additionally, centrality measures, such as degree centrality, closeness centrality, and betweenness centrality, enable us to identify the most critical nodes in the network, which are potential root causes of issues. Furthermore, community detection techniques aid in recognizing tightly-knit clusters of nodes, possibly indicating groups of entities sharing similar functionalities or problem areas. By employing graph traversal methods like breadth-first search, we can efficiently locate potential root causes of problems within the microservices architecture.\nMachine Learning/Deep Learning-Based RCA. Machine learning and deep learning techniques can be employed to process intricate datasets for the purpose of identifying and predicting root causes, based on the constructed graph structures. As an example, graph neural networks (GNNs) can model the intricate relationships and interdependencies among system components by harnessing the inherent graph-based structure of the data. These networks encapsulate dependencies within graphs through mechanisms like message passing or neighborhood aggregation, thereby facilitating accurate root cause analysis."}, {"title": "3 METHODOLOGIES", "content": "3.1 Metric-based Root Cause Analysis Techniques\nMetric-based Root Cause Analysis can be categorized into graph-based and non-graph methods. illustrates the investigated root cause analysis approaches of metrics. Among the non-graph methods, PAL and FChain firstly conduct direct KPI correlation analysis. In contrast, PAL derives the propagation pattern within components, whereas FChain takes into consideration both fault propagation patterns and intercomponent dependencies. DLA analyzes the metrics (CPU, Memory, or Network usage) associated with each level (container, node, micro-service) to pinpoint the source of the issue by training a Hierarchical Hidden Markov Model (HHMM) [82]. [86] aims to identify the e correlation in time by measuring the similarity of time series. iSQUAD diagnoses cause of intermittent slow Queries (iSQs) by clustering iSQs in databases. leverages random control trials (RCT) to obtain less ambiguous data in data-driven root cause analysis.\nOn the other hand, graph-based methods are widely used in RCA. Methods based on Dependency Graphs typically begin by employing a graph construction approach, initially creating causal graphs or topological graphs. Methods perform root cause analysis on the constructed graph by building topological or dependency graphs. During the dependency graph construction process, Granger Causality and PC algorithm are used to construct completed partially directed acyclic graph. There are also more through the trace request to construct the dependency graph , there are also through the law of large numbers and the Markov properties , deployment relationships to construct call graph or dependency graph . Quite many approaches use metric data to construct call graphs, however, ambiguous correspondences between upstream and downstream calls may exist and result in exploring unexpected edges in the constructed call graph, while advocate conducting RCA on this graph may lead to misjudgments . CMDiagnostor leverages the law of large numbers and the Markov properties of network traffic to investigate ambiguity problem and construct an ambiguity-free call graph.\nOnce the dependency graph has been constructed, subsequent RCA can be performed through various methodologies, including graph theory, machine learning, and deep learning. These approaches leverage the established graph to analyze data for fault diagnosis and problem resolution. uses correlation of a correct version and faulty version to realize RCA. FacGraph and RCA-graph utilize subgraph mining (FSM) algorithm graph or graph similarity-based pattern matching through traversal techniques, such as breadth-first search (BFS) ordered strings and depth-first search (DFS) on dependency graphs, to identify the root cause. TON18 , MicroRCA and employ random walks on graphs for root cause analysis. Concurrently, adopts subgraph similarity pattern matching on topological structures for fault diagnosis.\nThe advancement of deep learning methods on dependency graphs has led to significant progress in fault diagnosis. applies an autoencoder to identify abnormal service metrics, utilizing a ranked list of reconstruction errors to pinpoint potential culprit services. On the other hand, D\u00e9j\u00e0Vu methodology harnesses Gated Recurrent Units (GRU) and Convolutional Neural Networks (CNN) to encode the metrics of a fault unit into a fixed-length vector. It then employs a binary classification scheme to output a root cause score ranging from 0 to 1 for each fault unit, representing the likelihood of the unit being the root cause of the failure, based on the aggregated features. Furthermore, D\u00e9j\u00e0Vu posits that recurrent events are of particular interest RCA, as they provide valuable insights into recurring patterns of failure.\nAs a classic example of dependency graphs, Bayesian Networks have garnered attention from both researchers and industry practitioners for it excel in probabilistic deduction and causal inference, particularly when dealing with uncertain and incomplete information. CIRCA takes full advantage of the Causal Bayesian Network (CBN), bridging between observational data and interventional knowledge, the change of probability distribution conditioned on the parents in the CBN. exploits the explainable AI (XAI) Bayesian Linear Attribution (BALANCE) for RCA. BALANCE uses a Bayesian multi collinear feature selection (BMFS) model to predict the target KPIs given the candidate root causes in a forward manner while promoting sparsity and concurrently paying attention to the correlation between the candidate root causes. And it introduces attribution analysis to compute the attribution score for each candidate in a backward manner.\nUnlike dependency or topology aimed at understanding the structure and connectivity of a system, while causal graph is concerned with the cause-and-effect relationships within the system. In particular, causal graph are instrumental in identifying and interpreting the underlying reasons for RCA. Causality Graph-based RCA also comprises two stages: the initial stage involves generating a directed acyclic the causality graph with spatiotemporal restriction, followed by performing RCA on the causality graph. In the realm of causal relationship discovery, considerable number of researchers have also constructed component or service causality graphs using the PC algorithm and its variants and they put more emphasis on RCA methods. employs a data mining method to extract an initial causality graph, which is then refined using the random forest algorithm to construct a more precise causality graph. The correctness and completeness of causality discovery influents second stage RCA. Concerning this, CAR discovers almost a complete graph based on XG-Boost active learning in case of miss a large proportion of relations and fluctuation propagation among time series. REASON employs novel hierarchical graph neural networks to model both intra-level and inter-level non-linear causal relations, thereby constructing interdependent causal networks. CORAL , an incremental disentangled causal graph learning approach, aims at decoupling state-invariant and state-dependent information. Completeness and complexity often present a paradox in causality graph analysis, where intricate causal relationships can lead to reduced efficiency and accuracy in processing. CausIL provides a list of general rules in the form of prohibited edges, along with ways to incorporate them, aiming to improve computation time and accuracy.\nFollowing the construction of the causality graph, in the second phase of causality graph analysis, correlation of causality graph analysis is employed for conducting root cause analysis under the hypothesis that the anomalous KPIs related to a fault are highly correlated and form a connected subgraph of the propagation graph. traverse the causal graph in DFS (Depth First Search) and reverse DFS to inspect the neighboring nodes of each node, introducing correlation coefficient to measure the relationship between the service level objective of the anomalous front service and that of the potential root cause candidates. Numerous researchers tend to utilizes random walks on the constructed causality graph, and place greater emphasis on the construction of the causality graph itself. CloudRanger , MS-Rank , AutoMAP  MicroCause , CAR , CORAL , and REASON apply random walk to perform RCA leveraging the learned interdependent causal networks. MicroCause captures the sequential relationships in time series data when performing temporal cause oriented random walk.\nKnowledge graph within the domain of expertise is referred to as O&M Knowledge Graph, a completely undirected graph. In O&M Knowledge Graph, the entity and relation demonstrate a faulty architecture.  and OpsKG construct O&M knowledge graph by extracting a joint entity-relation extraction according to alarm query, root-cause"}, {"title": "3.2 Trace-based Root Cause Analysis Techniques", "content": "Trace-based RCA employs traces to pinpoint the fundamental reasons behind issues by scrutinizing the sequence of events that preceded the occurrence of the problem. Different with metric-based RCA, trace-based RCA centers on traces instead of metrics or logs. Since traces naturally exhibit invocation relationships, trace-based techniques are inherently associated with dependency or topology graphs. However, these dependency graphs, which may encompass temporal or regular trace dependencies, may not possess the completeness of a comprehensive dependency graph. Among dependency graph-based methods, statistical analysis and classification on traces are primarily employed. illustrates investigated root cause analysis approaches of traces.\n3.2.1 Correlation Analysis. Non-graphical methods are not frequently used in trace RCA. leverages alarm clustering techniques to support the analysis process by identifying and eliminating the most predominant and persistent root causes of alarms. X-ray employs dynamic flow analysis to associate events with potential root causes, prioritized by probability. The aggregate cost for each root cause is calculated by summing the product of individual event costs and their likelihood of being caused by a specific root cause.\nDependency graph constructed from monitoring data may exhibit incompleteness, as not all potential interactions among architectural elements are necessarily represented by edges in the graph . Drawing from reconstruction of architecture models from monitoring data and the subsequent evaluation of anomaly detectors, the caller-callee relationships, along with their respective anomaly scores, can be discerned. introduces the anomaly correlator, RanCorr, a set of rules designed to scrutinize calling dependencies among software components within the constructed graph. The correlation analysis is facilitated by implementing a rule set founded on general assumptions about propagation. Ultimately, the visualization of potentially correlated anomalies serves to bolster decision-making. utilize a heuristic approach to compare suspicious traces with predefined normative patterns, which relies on graph properties of sequential patterns, flagging anomalies when deviations exceed the 95% statistical confidence limits. They employ a regression-based relative importance analysis, which assesses the explained variance of the total response time in relation to the latency of individual transitions within a trace. TraceRCA employs frequent pattern mining techniques to narrow down the search space, and ranks microservices with less normal traces passing through. performs anomaly entity node analysis and propagation chain analysis by traversing the graph along anomalous service call edges. Candidate root causes are then ranked based on the absolute value of the Pearson correlation coefficient. trACE pinpoints the root cause within a microservices architecture by exploiting its hierarchical deployment structure. It analyzes dependencies correlation between anomalous Pods and services to infer the causes of service anomalies. While this method excels at identifying root causes at both the node and service levels, it does not directly reveal the triggers behind these failures.\nREPTRACE employs categorizing trace similarity for RCA. They construct a dependency graph based on read/write and parent/child process dependencies, tracing back from inconsistent artifacts to identify the responsible processes. To address the challenges posed by noisy dependencies and uncertain parent/child relationships, REPTRACE focuses on system calls that produce differing outputs across builds and computes the similarity of runtime values to establish relevant dependencies. GMTA employs a graph representation to visualize trace data in microservice systems, which allow operators to manually determine the possible root causes. GMTA supports a wide range of analytical applications, including visualizing service dependencies, guiding architectural decisions, analyzing service behavior changes, detecting performance issues, and enhancing the efficiency of problem identification.\nDependencies path analysis microservices is another correlation analysis method . computes potential dependencies among anomaly elements using a correlation model, identifying the critical path leading to the detected fault through this correlation model. TraceAnomaly employs a variational autoencoder (VAE) to identify anomalies within traces, utilizes the three-sigma rule to determine anomaly intervals, and defines the root cause as the longest continuous path encompassing these anomalous intervals. Similar to TraceAnomaly, CRISP uses an anomaly detection framework but encodes only the critical paths into vectors generated through Critical Path Analysis (CPA). CRISP's tracing facility captures the sequence of operations and interactions among system components, using this data to construct a dependency graph. This graph illustrates the relationships between microservices or system elements, allowing CRISP to pinpoint the most influential critical paths for system performance. Through the analysis of these paths, CRISP detects anomalies and bottlenecks, aiding in the diagnosis and resolution of issues in intricate software systems.\nResearchers utilize random walks for path analysis in dependency graph identifying the underlying causes of issues. MonitorRank advocates that a basic call graph might not accurately represent the true dependency relationships. By harnessing both historical and real-time data from various sensors, the system utilizes a specially designed random walk algorithm alongside a sensor dependency graph. TraceRank also gathers service invocation data in a microservice setting to assemble a service dependency graph. A PageRank-inspired random walk algorithm is then employed to accurately identify the primary culprits behind any issues.\n3.2.2 Trace Classification. Another category for trace RCA is trace classification. Pinpoint adopts data mining techniques to cluster client requests based on their successes and failures, correlating them with the components that served them. This correlation aids in pinpointing which components are most probably malfunctioning. MEPFL, similarly harnesses machine learning to predict potential errors, fault locations, and types. This method selects relevant features, preprocesses data, and trains models using Random Forests, K-Nearest Neighbors, and Multi-Layer Perceptron for within a microservices architecture . proposes the use of a supervised learning model, specifically Deep Neural Networks (DNNs), to implement a classifier. This classifier is trained on both normal and faulty datasets to learn patterns associated with both normal and abnormal behaviors, thereby aiding in fault localization. On the other hand, categorizes requests based on call sequences, identify abnormal requests through principal component analysis, and then single out anomalous methods using the Mann-Whitney hypothesis test. They asses the behavioral similarities of all replicated instances of the anomalous methods with the Jensen-Shannon divergence and select those with differing behaviors as the ultimate culprits of performance anomalies.\nSpectrum-based analysis is also utilized for trace classification. MicroRank utilizes clues extracted from both normal and abnormal traces to pinpoint the root causes of latency issues. Its PageRank Scorer module employs abnormal and normal trace data, distinguishing the significance of various traces through advanced spectrum techniques. Spectrum method computes a ranking list, leveraging the weighted spectrum data from the PageRank Scorer to identify the root causes. Utilizing weighted spectrum analysis, TraceStream generates a vector representation for each trace by extracting key structural and temporal features. Leveraging these representations, traces exhibiting similar behaviors are continuously grouped into normal and anomalous clusters by calculating the centrality of each candidate node and ranking service nodes based on centrality-derived spectrum scores. Sleuth leverages a graph neural network (GNN) to capture the causal impact of each span in a trace, and trace clustering using a trace distance metric to reduce the amount of traces required for root cause localization.\nAutoencoders are also extensively employed in trace root cause analysis, where they are used to learn efficient coding of the input data, enabling the identification of underlying patterns and anomalies that contribute to the occurrence of errors or faults within a system. LSTM-AD extracts operation sequences and latency time series from traces, modeling the temporal features using a multi-modal Long Short-term Memory (LSTM) VAE. Uniquely, this method maximizes node anomaly scores instead of summing them up. TraceAnomaly employs a variational autoencoder (VAE) to identify anomalies within traces. It utilizes the three-sigma rule to determine anomaly intervals, and defines the root cause as the longest continuous path encompassing these anomalous intervals. Sage constructs a visual representation of these complex dependencies utilizing Causal Bayesian Networks, presenting a clear and structured method to decipher the intricate relationships within the microservice architecture. Sage employs counterfactuals via a graphical Variational Autoencoder to simulate hypothetical scenarios and evaluate their potential impact on the cloud service's QoS. Sage not only identifies the underlying issues but also takes proactive steps to restore and maintain optimal QoS levels in cloud services. TraceVAE introduces an innovative dual-variable graph variational autoencoder, targeting two types of anomalies commonly found in traces: structural and time-consumption anomalies. TraceVAE encodes the trace structure and utilizes the negative log-likelihood (NLL) as a quantifiable measure of the anomaly score. NLL signifies how improbable it is for that trace to align with the model's learned distribution."}, {"title": "3.3 Log-based Root Cause Analysis Techniques", "content": "Log based RCA begins with the acquisition of log data from the system, followed by preprocessing steps such as log parsing, formatting, and noise reduction. Log analysis techniques are applied to detect anomalies within the logs, common techniques including anomaly detection, correlation analysis, pattern mining, and machine learning methods. These techniques specifically involve identifying abnormal patterns in logs, analyzing correlations and causal relationships between log events, mining frequent patterns or behaviors in logs, and classifying and clustering logs. With the assistance of system context information (such as system topology, configuration, metadata, etc.) and knowledge graphs, contextual relationships are integrated. Subsequently, RCA methods are employed to locate the root cause of issues within the logs. Here, RCA methods are categorized into the following types: rule-based RCA, statistical RCA, graph-based RCA, and machine learning-assisted RCA. demonstrates the investigated root cause analysis approaches of logs.\n3.3.1 Correlation Analysis. Rule-based RCA relies on predefined rules and patterns to identify the root cause of a problem. These methods work well in clear, rule-defined scenarios, but may require significant human involvement and maintenance when dealing with complex, dynamically changing IT environment issues. propose a rule-based approach for failures through event logs analysis. clusters frequent event sequences into event groups and extract failure rules based on events of the same types. both adopt association rule mining (ARM) to automate RCA. Lin2020fast incorporate a new rule refinement algorithm to reduce the number of redundant explanations in the result set. They use the Apriori and FP-Growth, association rule learning, to find item-sets in structured logs that are strongly linked to association failures. Built upon this work, LogRule introduces new compression and rule selection techniques, by removing the need for manual tuning of metric thresholds, and by introducing the use of operators designed specifically for each data types, efficiently preserving semantic information.\nCorrelation analysis-based methods leverage statistical correlation to identify and analyze relationships between log entries, events in system logs and faults metrics to pinpoint the root causes of failures. Pinpointing the underlying causes of a given failure from extensive logs, reconstructs event sequences and establish connections among events and identifies log entries, both of them employs statistical directly correlation analysis. involves selecting features from structured logs related to execution time, data locality of each task, memory usage, and garbage collection of each node. By analyzing weighted features, they determine the probability of various root causes. To identify root causes, analyzes process on event logs, variations in trace durations, event timings, and path frequency. leverages Istio's default log count computes an indicator, representing the structure of the HTTP response body and logged as an additional field, to identify application errors. It tallies items like maps or lists in JSON format extracted from the HTTP response body, serving as the indicator for error identification. In methods of correlation analysis, applies correlation metrics to analyze log data following log anomaly detection with DeepLog algorithm.\nIn the realm of correlation analysis methods, path-based analyses also exist. These approaches are particularly useful when examining the causal pathways that link various factors and outcomes in a complex system . SherLog reports pathways connecting the longest subsequence of logging messages involving the error message. In cases where multiple paths link the same sequence of Logging Points, SherLog compares these paths and initially reports the common records along these paths as the Must-path before presenting the rest . LogSed mines a time-weighted control flow graph (TCFG) to represent healthy execution flows in cloud components. By analyzing deviations from this TCFG, LogSed automatically raises anomaly alerts. introduces a distance-based approach calculating distance-based score from call trees.\nIn the chronological sequence of events, some scholars posit that the antecedent event is the prospective root cause. perform the short-term time series analysis and convert logs into multivariate time series data by counting error logs within specific time bins for affected services. They employ the personalized PageRank to provide a ranked order of potentially faulty nodes. In contrast to approaches that correlate the performance of various service instances or logged events to prioritize root causes based on their likelihood of causing observed failures, without elucidating how these failures propagate to other service instances, offers a fresh perspective. They present a declarative root cause analysis technique capable of determining the potential cascading failures that may have precipitated an observed failure, specifically, identifying the service instances that could have failed initially.\n3.3.2 Log Classification. Log classification represents a principal method for root cause analysis in the domain of system log analysis, distinct from correlation-based approaches. It involves categorizing log entries into specific classes or types, often with a focus on differentiating normal system behavior from anomalous events that may indicate issues or failures. This classification can be based on various techniques, SAT-solver derives from Artificial Intelligence theories of action and diagnosis to identify the most relevant log entries for the given query . introduce machine learning approaches in log classification RCA. Pariket converts the event log into a sequential dataset with window-based and Markovian techniques and employ decision tree classifiers to extract rules elucidating the root causes . DeCaf utilizes random forest to mine predicates from the logs that correlate with regressions . Logistic regression, na\u00efve Bayes, decision trees, and random forests are introduced for comprehensive issue categorization RCA Bot . The extraction of relevant features or patterns and the application of clustering techniques are integral to the functionalities LogDC, LogLens, and HWLogAnalysis . These methods enable the systematic organization of log data by identifying and grouping similar entries, which facilitates the detection of anomalies and the subsequent root cause analysis. LADRA employs the General Regression Neural Network (GRNN) to pinpoint"}, {"title": "3.4 multi-Modal based Root Cause Analysis Techniques", "content": "In the realm of complex software systems, diagnosing performance issues and identifying their underlying causes can be an arduous task. Methods relying on a single data source, such as metrics, traces, or logs, are insufficient, which may provide a narrow perspective and hinder a comprehensive understanding of the system's behavior . This insufficiency is generated by each microservice individually at a local level, which violates the close dependency among microservices. To overcome this limitation, different with previous data-driven RCA methods relying on data from a single modality, researchers have explored multi-model root cause analysis. This approach integrates heterogeneous data sources, including metrics, traces, and logs, to offer a holistic and multi-faceted view of the system's performance. In heterogeneous data sources, perform RCA based on the time series of events. Pdiagnose captures suspicious microservices and corresponding logs for logs and traces, and determines root causes through voting abnormal time series . MULAN leverages a log-tailored language model to facilitate log representation learning, converting raw log sequences into time-series data uses a contrastive learning-based approach to extract modality- invariant and modality-specific representations within a shared latent space, and introduces a novel key performance indicator-aware attention mechanism for assessing modality reliability and co-learning a final causal graph. MULAN employs random walk with restart to simulate system fault propagation and identify potential root causes.\nMulti-model data source increases the complexity of RCA. By constructing O&M knowledge graph or Causal Knowledge Graph (CKG), knowledge from multiple data sources will be integrated for further RCA . Optimized PC Algorithm based on the Knowledge Graph is used to construct a directed acyclic graph (DAG) Causality Graph, and breadth-first search (BFS) is employed to search for the root cause . From a knowledge base MicroCBR constructs spatial-temporal knowledge graph, and troubleshoots through case-based hierarchical reasoning . SOTA neural NLP techniques are used to extract targeted knowledge from past incidents and they build a CKG, which is used to collectively infer the root cause through link prediction and graphical neural model methods ."}, {"title": "3.5 LLMs' Role in Enhancing RCA", "content": "The recent emergence and remarkable performance of large language models (LLMs) in handling intricate tasks indicate a potential pathway for improving root cause analysis. Cloud providers and researchers have employed advanced AI-based solutions LLMs to aid SREs in identifying the root causes of cloud incidents. suggests fine-tuning large language models (LLMs) using domain-specific datasets, aiming to generate potential root causes of an incident solely based on the title and summary information available during the incident's creation. RCACopilot expands upon this work and add retrieval augmentation and diagnostic collection tools to the LLM-based root cause analysis pipeline. They design custom workflows for different types of incidents that trigger data collection procedures, which are then aggregated to predict a root cause category for the incident and help SREs with root cause analysis. RCACopilot relies on predefined handlers that must be engineered by hand. Considering the black-box nature of many LLM based methods, advocates fine-tuning or temperature-scaling-based approaches are inapplicable. introduces an innovative confidence estimation framework leveraging retrieval-augmented large language models. It assesses confidence in predicted root causes through two scoring phases, evaluating the model's \"grounded-ness\" in reference data and rating predictions based on historical references, followed by an optimization step to combine these scores. Besides, ADARMA employs LLM based RCA for probabilistic reasoning, historical data, and correlation patterns to identify the likely root cause. uses semantic, lexical, and correctness metrics in zero-shot and few-shot scenarios. In-context examples can serve as an alternative to fine-tuning for domain adaptation, but for agent-based RCA, crafting entire reasoning trajectories can be challenging . This is because agents require sophisticated prompting and typically also require fine-tuning . explore LLM agents for root cause analysis. They all solve the problem of dynamic data collection based on ReAct agent to overcome the limitations of dynamically collect additional diagnostic information such as incident related metrics, logs or traces."}, {"title": "4 EVALUATION OF RCA", "content": "4.1 Effectiveness and Efficiency\nWhen evaluating root cause analysis methods", "utilized": "nTop n Accuracy ($A@K$) denotes the likelihood that the correct outcome is included within the top K (K=1", "follows": "T_{train}$ = Tend time of training \u2013 Tstart time of training, Ttrain represents the training time."}]}