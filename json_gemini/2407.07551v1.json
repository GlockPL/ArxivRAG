{"title": "Arabic Automatic Story Generation with Large Language Models", "authors": ["Ahmed Oumar El-Shangiti", "Fakhraddin Alwajih", "Muhammad Abdul-Mageed", "Mohamed bin Zayed"], "abstract": "Large language models (LLMs) have recently emerged as a powerful tool for a wide range of language generation tasks. Nevertheless, this progress has been slower in Arabic. In this work, we focus on the task of generating stories from LLMs. For our training, we use stories acquired through machine translation (MT) as well as GPT-4. For the MT data, we develop a careful pipeline that ensures we acquire high-quality stories. For our GPT-4\u00b9 data, we introduce crafted prompts that allow us to generate data well-suited to the Arabic context in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian and Moroccan). For example, we generate stories tailored to various Arab countries on a wide host of topics. Our manual evaluation shows that our model fine-tuned on these training datasets can generate coherent stories that adhere to our instructions. We also conduct an extensive automatic and human evaluation comparing our models against state-of-the-art proprietary and open-source models. Our datasets and models will be made publicly available at https://github.com/UBC-NLP/arastories.", "sections": [{"title": "1 Introduction", "content": "Storytelling is an essential human skill that serves to transmit knowledge, impart values, and connect individuals through tales of daily experiences. It is utilized in education, where teachers harness children's natural affinity for stories to foster cognitive and literacy development. Additionally, stories and legends, viewed as cultural heritage, are passed down through generations by parents, enriching the culture and preserving traditions.\nThe role of storytelling also extends beyond its traditional roots; it acts as a vital connection between the primitive oral language skills in early childhood and the advanced language abilities associated with literacy. As such, the task of automatic story generation presents numerous benefits across different fields. In entertainment, it allows for the efficient creation of diverse narratives (Xie and Riedl, 2024). In education, tailored stories can be crafted to address the unique needs of learners. In gaming, interactive storytelling significantly enhances user engagement and enjoyment (Patel et al., 2024). And these are only a few application domains.\nProgress in natural language processing (NLP) technologies, particularly with large language models (LLMs) such as GPT-4 and Gemini, has made automatic story generation both viable and effective, producing stories with notable fluency and coherence. While substantial efforts have been made to advance automatic story generation in English using generative models, the development of such technologies for Arabic has been limited due to a scarcity of Arabic short story data and minimal focus from the research community.\nIn this study, we present a novel approach to automatic story generation utilizing the powerful Arabic LLM, AraLLaMA (Alwajih et al., 2024). We enhance AraLLaMA through fine-tuning with both translated and synthetic datasets to optimize its story-generating capabilities. We explore two fine-tuning strategies: one involving direct application of a synthetic dataset produced by GPT-4, and another beginning with an analogous synthetic dataset translated from English. Additionally, we extended the model's utility by fine-tuning it with data from two Arabic dialects, enabling the generation of stories in both Modern Standard Arabic (MSA) and these two dialects. The efficacy of our model is assessed through human evaluation, which confirmed its ability to produce coherent and fluent narratives as per specified instructions.\nOur contributions are manifold, summarized as follows:\n1. We introduce powerful models capable of generating coherent and fluent stories in MSA and two Arabic dialects.\n2. We offer a newly created framework for Arabic automatic story evaluation based on LLMs.\n3. We develop two novel datasets for automatic story generation: one consisting of translated narratives from the TinyStories (Eldan and Li, 2023) dataset, which was meticulously curated, and another comprising a synthetic dataset created using GPT-4, featuring narratives in MSA and two dialects.\n4. We compare two distinct fine-tuning methods on AraLLaMA against AceGPT-7B (Huang et al., 2024), GPT-3.5, and Command-R2, powerful open source and proprietary models using extensive automatic and human evaluations.\nThe remainder of this paper is organized as follows: Section 2 provides a review of prior studies focusing on the task of automatic story generation. Section 3 details the creation of our datasets. In Section 4, we outline our prompt design. In section 5 we detail our different experiments. Results and key insights from our comparative analysis of our fine-tuned models against various commercial and open-source models are discussed in Section 6. The paper concludes with Section 7."}, {"title": "2 Related Work", "content": "2.1 Early Work on Story Generation.\nJain et al. (2017) is an early work on generating coherent stories, experimenting with two paradigms: Statistical Machine Translation (SMT) and Deep Learning. SMT treats story generation as a translation task, while Deep Learning uses Recurrent Neural Networks (RNNs) to encode sequences of input descriptions into hidden representations, which are then transformed into detailed summaries. They evaluate their models using BLEU, ROUGE-L, and human evaluation. Fan et al. (2018) propose a hierarchical model that first generates a story premise using a convolutional language model (Dauphin et al., 2017) and then a seq2seq model to create a story that follows the premise. They incorporate gated multi-scale attention and model fusion to improve prompt adherence.\nAkoury et al. (2020) introduce the STORIUM dataset\u00b3 and fine-tune GPT-2-medium (Radford et al., 2019) for generating short story scene entries, motivated by GPT-2's 1024-token context window. Plug-and-Blend (Lin and Riedl, 2021) consists of a Blending Generative Model (BGM) and Planner for controllable story generation. BGM facilitates controlled continuations, while Planner specifies control parameters based on topic descriptions and story sections. The authors fine-tune GPT-2-large (Radford et al., 2019) on ROCStories (Mostafazadeh et al., 2016) and use pre-trained GeDi (Krause et al., 2020) as the guiding model, evaluating fluency and fidelity through human evaluation.\n2.2 LLM Story Generation.\nMirowski et al. (2022) propose using a 70B Chinchilla LLM called Dramatron for generating long narratives, such as full scripts and screenplays, through prompting, prompt chaining, and hierarchical generation. Dramatron supports collaborative writing and was qualitatively assessed via co-writing sessions and interviews with 15 industry professionals.\nYang et al. (2022) propose the Recursive Reprompting and Revision (Re3) framework automatically generates longer stories without human intervention, distinguishing it from previous approaches. Re3 comprises four modules: Plan, Draft, Rewrite, and Edit. The Plan module creates a story plan using GPT-3 (Brown et al., 2020) to add details to a given premise. The Draft module generates story continuations by recursively prompting GPT-3, dynamically updating the prompt with information from the plan and story. The Rewrite module reranks alternate continuations to select the best ones, and the Edit module ensures factual consistency with earlier parts of the story. Re3 operates in a zero-shot manner, allowing it to generate longer stories without domain constraints.\nPatel et al. (2024) propose a creative storytelling framework with two components: the story generation model and the Action Discriminator model (AD LLM). They train these models in a feedback loop called SWAG. Initially, a prompt is used to generate the first paragraph, which is then fed into the AD LLM with actions (e.g., \"add suspense\") to produce the best continuation. This process is repeated until the story reaches the desired length."}, {"title": "2.3 Evaluation of Story Generation in Literature", "content": "There are basically two types of evaluations for story generation in the literature: human evaluation and automatic evaluation. We explore these evaluation methods in the following subsections:\n2.3.1 Human Evaluation\nAkoury et al. (2020) integrate their fine-tuned model into the STORIUM collaborative storytelling platform, where real authors can query the model to generate suggested story continuations. The authors could edit the generated text by adding or deleting content. The edited stories were collected along with ratings from the authors' on properties such as relevance, fluency, coherence, and likability. They also propose a new automatic metric called User Story Edit Ratings (USER), inspired by the longest common subsequence (LCS) of the ROUGE metric (Lin, 2004), which measures how much of the generated text is preserved in the edited version.\nThe authors of Re3 (Yang et al., 2022) ask workers from Amazon Mechanical Turk to rate Re3-generated stories against GPT-3 and GPT-3 fine-tune on stories from the WritingPrompts dataset. The evaluation criteria include interestingness, coherence, fluency, human-likeness, and relevance. Workers also identify shortcomings in the generated stories, such as disfluency, repetitiveness, confusing inconsistencies, and narration problems. Re3 outperforms all baselines on almost all criteria.\nXie and Riedl (2024) rely on a pool of three human studies to evaluate their framework for suspenseful story generation. In the first study, human judges compare stories generated by their approach against those generated by a strong baseline (ChatGPT) based on suspense, novelty, enjoyment, logical sense, and naturalness. The second study involved ablations on their system compared against the full system. In the third phase, participants reviewed the story's structure to verify internal processes. Ninety participants assessed 30 story pairs, with each pair reviewed by 30 participants. Their approach outperforms all baselines on all criteria except for a 56% tie with ChatGPT on naturalness.\nThe authors of (Patel et al., 2024) evaluate their story generation method using human judges and GPT-4. Surge AI employees assessed 50 stories generated by the Llama-2-7B and Mistral-7B models, enhanced by the SWAG technique, against four baselines: the end-to-end approach, a random selection method, GPT-3.5-turbo, and GPT-4-turbo. Evaluations focused on interestingness, surprise, and coherence. The findings show a preference for SWAG-enhanced stories over conventional methods by both human judges and GPT-4, with SWAG models winning 61.5\nWang et al. (2024) compare Weaver's variations against other open-source and proprietary LMs, including GPT-4, GLM-4, ERNIE-Bot-4.0, and Gemini-pro. Evaluations by human professionals and GPT-4 were based on creativity, style, relevance, and fluency. Weaver-Ultra was preferred 1576 and 1657 times out of 3540 samples by humans and GPT-4, respectively.\n2.3.2 Automatic Evaluation\nEvaluating creative writing such as story generation is a challenging task. Jain et al. (2017), one of the earlier works on neural-based story generation, uses machine translation metrics (BLEU-4, METEOR, TER, and ROUGE-L) to evaluate story generation. The overall results were low, with SMT-based methods scoring better on BLEU-4 than seq2seq models, despite being less coherent. The scores were 3.5 and 1.98 for SMT and seq2seq models, respectively, indicating that n-gram-based metrics are not suitable for creative writing judgment. GPT-Eval, an evaluation framework based"}, {"title": "2.4 Common Datasets for Story Generation", "content": "To provide an overview of the resources used in story generation research, we summarize the most common datasets used to build automatic systems for generating stories in Table 1. These datasets vary in size, nature, availability, and average length of the stories they contain. The datasets include human-generated stories as well as those created by advanced language models like GPT-3.5 and GPT-4. They are valuable resources for training and evaluating story-generation models."}, {"title": "2.5 Arabic Story Generation", "content": "The task of automatic story generation is uninvestigated in the Arabic NLP community. However, (Alhussain and Azmi, 2024) utilize cross-lingual transfer learning to address the scarcity of Arabic data in Story Ending Generation (SEG) task by leveraging English story corpora."}, {"title": "3 Data Collection", "content": "We compile data from different resources. We first translate 1.13M English stories generated by GPT-4 alongside their prompts from the TinyStories dataset (Eldan and Li, 2023) using Google translate API.\u2074 To ensure that we have only high-quality translation, we apply a filtering strategy based on multilingual sentence embeddings (Feng et al., 2022) and remove the story pairs whose embedding similarity is less than 92%.\n3.1 Filtering Strategy\nWith the aim to train only on high-quality data, we apply Algorithm 1 to our dataset. The final threshold was 92%. And we were able to maintain 545K samples which represents 48.3% of the translated data.\n3.2 Generated Data\nWe also generate our own stories from GPT-4-Turbo API using a carefully designed set of prompts and features (see Section 4). The dataset generated with our prompt template is in three Arabic varieties, namely MSA, Moroccan, and Egyptian. We tested the ability of GPT-4-Turbo to generate other Arabic dialects, but the generated content"}, {"title": "Algorithm 1 Filtering Stories Based on Similarity Score", "content": "Require: Stories dataset D, Similarity threshold t, Minimum word count m = 50\n1: Remove stories shorter than m words\n2: Sort stories based on similarity score\n3: Filter out stories whose similarity with the original story is less than the threshold t\n4: Get a human in the loop to manually check some random samples\n5: Set a new threshold t'\n6: Repeat steps 2-5 until satisfactory translation quality is achieved"}, {"title": "4 Prompt Design", "content": "Prompting is an approach employed by users to interface with LLMs (White et al., 2023). It functions as the primary mode of communication with these models, effectively serving as the input language that LLMs are designed to interpret and respond to. The efficacy of the generated output is significantly correlated with the quality and structure of the input prompt. This relationship underscores the critical role that prompt engineering plays in optimizing LLM performance and output relevance. In our context, the prompt can be conceptualized as a set of instructions or parameters that guide the LLM's for the Arabic story generation process. The prompt's composition, including its specificity, clarity, features, and relevance to the desired output, directly influences the model's ability to generate appropriate and accurate stories that adhere to our instructions. This causal relationship between prompt quality and output quality highlights the importance of developing sophisticated prompting strategies to fully leverage the capabilities of LLMs for Arabic story generation.\n4.1 Initial Investigation\nWhen we started this study, we had three prompting choices. We either prompt in English, Arabic, or"}, {"title": "4.2 Prompt Template", "content": "We design our prompt template with two goals in mind. These are (i) to ensure high quality of the generated output and (ii) make the generated output as diverse as possible. To ensure the variety of the generated stories, we carefully design a set of 12 features: {age, place, end of story, dialogue, number of characters, moral of the story, topic, country, season, activity, emotion, plot twist}. Our template is designed in such a way that each feature has a probability p of appearance in a particular prompt. Meaning some features might be present in a prompt while others are not. Except for the following features where they appear in each prompt: age, number of characters, and country. Based on our preliminary observations, GPT-4 is able to generate coherent stories from dialectal prompts (i.e., Egyptian and Moroccan). Hence, we ask two native speakers to translate our prompt template, originally written in MSA to Egyptian and Moroccan dialects. This prompt template is used to generate MSA and dialectal stories from GPT-4 and later to fine-tune our models."}, {"title": "5 Experiments", "content": "We conduct two sets of experiments:\n1. Directly fine-tuning on the generated data from GPT-4 Turbo-preview.\n2. Fine-tuning on translated data, followed by further fine-tuning on data generated with the GPT-4-Turbo-preview model.\nThe details of each experiment are described next.\n5.1 Supervised Fine-Tuning (SFT)\nWe instruct fine-tune AraLLaMa-2-base (Alwajih et al., 2024) using a diverse Arabic instruction tuning dataset generated with our custom prompt template. AraLLaMa-2-base is a 7B parameter model based on Llama-2 (Touvron et al., 2023), continually pre-trained on Arabic data. AraLLaMa-2 has shown superior performance compared to other Arabic LLMs such as AceGPT-7B (Huang et al., 2024) and Jais-7B (Sengupta et al., 2023), hence we adopt it for our experiments. For computational efficiency, all our models are trained with Hugging-face PEFT library (Mangrulkar et al., 2022). In each experiment, our base model, AraLLaMa-2 is quantized in 4-bit precision and then a new QLoRA layer (Dettmers et al., 2023) is added. During our experiments, we keep the base model frozen and update the QLoRA layer only. The instruction fine-tuning is performed by updating a newly added QLoRA layer, with a set to 16, r set to 64, QLoRA layer dimension set to 64, gradient accumulation at 10, batch size equal 1. We use as optimizer AdamW (Loshchilov and Hutter, 2019), a dropout is equal to 10%, a learning rate set to 4 * 10e \u2013 5. We train for 20 epochs. This training took approximately 5.5 hours on a single Nvidia A100 GPU. We call the model artifact resulting from this training Model A.\n5.2 Two-Step Fine-Tuning\nThis experiment is divided into two steps. First, we instruct fine-tune AraLLaMa-2-base on large-scale translated data from the TinyStories dataset (Eldan and Li, 2023) for 15,000 steps. The second step is taking the trained model from the previous step and further instruct fine-tuning it on a smaller dataset from experiment 1 (Section 5.1). The hyperparameters are the same as in the previous experiment. The overall training took about 17.5 hours on the Nvidia A100 GPU. We provide examples of stories generated with our models across all three Arabic varieties in Figure 1. We call the model artifact resulting from this training Model B."}, {"title": "6 Evaluation", "content": "Evaluating generative tasks remains an open problem in the AI community. However, in our study, we follow previous works such as (Eldan and Li, 2023) in adapting GPT-4 as an evaluator for model performance. We also conduct an extensive human evaluation trying to understand different model capabilities and how the evaluation of GPT-4 compares to that of human judges. We next describe our two evaluation strategies."}, {"title": "6.1 GPT-4 As a Judge", "content": "We evaluate our models on five criteria scored by GPT-4. We design a comprehensive prompt that works as follows: given the original story prompt plus the corresponding generated story, we ask GPT-4 to act as an Arabic language expert and assign a score out of five on the following criteria:\n\u2022 Fluency: The degree to which the text reads smoothly and naturally, with appropriate grammar, vocabulary, and sentence structure.\n\u2022 Coherence: The logical connection and flow between sentences and ideas, making the text easy to understand.\n\u2022 Instruction Following: The extent to which the text meets the given instructions or task requirements.\n\u2022 Consistency: The degree to which the information and style within the text remain uniform and accurate throughout.\n\u2022 Variety: How well the model generates a story in the required Arabic variety.\nWe find that GPT-4 tends to score the MSA content higher than dialectal ones, even if the task is to generate a dialectal story. To mitigate this issue, we added this last feature where we explicitly ask GPT-4 if the generated content follows the required Arabic variety specified in the prompt or not, and to which degree."}, {"title": "6.2 Human Evaluation", "content": "Pilot evaluation. We carry out a pilot investigation where a native speaker of Arabic with knowledge of different dialects inspects stories generated by different models and comes up with observations. The expert finds that Model A tends to generate longer stories compared to Model B. Both models A and B are less perfomant in the Moroccan dialect compared to the Egyptian dialect. AceGPT-7B-Chat (Huang et al., 2024) and GPT-3.5 fail to generate dialectal stories, even when explicitly prompted to do so.\nExtensive human comparison of different models. We ask four Arabic native speakers to rank ten stories generated by different models based on the following criteria: Instruction Following, Fluency, and Variety Adherence. However, since AceGPT-7B-Chat and GPT-3.5 failed to generate dialectal content, we include these models only in the MSA part of the human evaluation task (i.e., we exclude them from the dialectal evaluation). We thus compare our two models, model A and model B, to Command-R. We ask an Arabic native speaker to compare different models to each other, finding our Model A to surprisingly be almost always better than GPT-3.5(90%). Our model A also outperforms Command-R(35B) 40% of times, always outperforms AceGPT-7B-Chat, and Model B 70%. More details are in Figure 4.\nFor the human evaluation of dialects, we compare our models against Command-R, which was the only model other than ours able to generate dialectal content. Our experts find that Model A to be able to outperform Command-R 20% of the time for both dialects and outperforming Model B 50% and 60% on Moroccan and Egyptian dialectal stories, respectively (Figure 5 for visualization)."}, {"title": "7 Conclusion", "content": "In this paper, we present the first LLM-based study on automatic Arabic story generation. Our study takes as its target MSA and two Arabic dialects (Egyptian and Moroccan). For our purpose, we translate and generate datasets based on a custom prompt template. We fine-tune our models on these datasets, comparing against both equally- and bigger-sized models. Through extensive automatic and human evaluation, we empirically show our models' superiority to strong baselines. In the future, we plan to fine-tune bigger models on larger datasets. We also plan to include more dialects in our training, for wider coverage."}, {"title": "Limitations", "content": "This study has the following limitations:\n\u2022 Compute constraints. Due to computational limitations, we restricted ourselves to models with a maximum size of seven billion parameters or those with an available API.\n\u2022 Limited data. Our training dataset consisted of only 3,000 samples of high-quality data generated by GPT-4. In the future, we are planning to generate more data with the newly released GPT-40.\n\u2022 Lack of error analysis: We believe carrying out an error analysis would benefit our work. In particular, we observe that GPT-4 does not fully adhere to our instructions 100% of the time during the generation of training data. This could lead to issues in the data generated using this model and an error analysis could uncover any such limitations. Future work should take this into account."}, {"title": "Ethical Considerations", "content": "Similar to other generative models, our model can reflect the bias in its data. Any use of the model should take this into account."}]}