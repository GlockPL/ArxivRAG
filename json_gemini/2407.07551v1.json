{"title": "Arabic Automatic Story Generation with Large Language Models", "authors": ["Ahmed Oumar El-Shangiti", "Fakhraddin Alwajih", "Muhammad Abdul-Mageed", "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)", "Invertible AI"], "abstract": "Large language models (LLMs) have recently emerged as a powerful tool for a wide range of language generation tasks. Nevertheless, this progress has been slower in Arabic. In this work, we focus on the task of generating stories from LLMs. For our training, we use stories acquired through machine translation (MT) as well as GPT-4. For the MT data, we develop a careful pipeline that ensures we acquire high-quality stories. For our GPT-4\u00b9 data, we introduce crafted prompts that allow us to generate data well-suited to the Arabic context in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian and Moroccan). For example, we generate stories tailored to various Arab countries on a wide host of topics. Our manual evaluation shows that our model fine-tuned on these training datasets can generate coherent stories that adhere to our instructions. We also conduct an extensive automatic and human evaluation comparing our models against state-of-the-art proprietary and open-source models. Our datasets and models will be made publicly available at https://github.com/UBC-NLP/arastories.", "sections": [{"title": "Introduction", "content": "Storytelling is an essential human skill that serves to transmit knowledge, impart values, and connect individuals through tales of daily experiences. It is utilized in education, where teachers harness children's natural affinity for stories to foster cognitive and literacy development. Additionally, stories and legends, viewed as cultural heritage, are passed down through generations by parents, enriching the culture and preserving traditions.\nThe role of storytelling also extends beyond its traditional roots; it acts as a vital connection between the primitive oral language skills in early childhood and the advanced language abilities associated with literacy. As such, the task of automatic story generation presents numerous benefits across different fields. In entertainment, it allows for the efficient creation of diverse narratives (Xie and Riedl, 2024). In education, tailored stories can be crafted to address the unique needs of learners. In gaming, interactive storytelling significantly enhances user engagement and enjoyment (Patel et al., 2024). And these are only a few application domains.\nProgress in natural language processing (NLP) technologies, particularly with large language models (LLMs) such as GPT-4 and Gemini, has made automatic story generation both viable and effective, producing stories with notable fluency and coherence. While substantial efforts have been made to advance automatic story generation in English using generative models, the development of such technologies for Arabic has been limited due to a scarcity of Arabic short story data and minimal focus from the research community.\nIn this study, we present a novel approach to automatic story generation utilizing the powerful Arabic LLM, AraLLaMA (Alwajih et al., 2024). We enhance AraLLaMA through fine-tuning with both translated and synthetic datasets to optimize its story-generating capabilities. We explore two fine-tuning strategies: one involving direct application of a synthetic dataset produced by GPT-4, and another beginning with an analogous synthetic dataset translated from English. Additionally, we extended the model's utility by fine-tuning it with data from two Arabic dialects, enabling the generation of stories in both Modern Standard Arabic (MSA) and these two dialects. The efficacy of our model is assessed through human evaluation, which confirmed its ability to produce coherent and fluent narratives as per specified instructions.\nOur contributions are manifold, summarized as follows:"}, {"title": null, "content": "We introduce powerful models capable of generating coherent and fluent stories in MSA and two Arabic dialects.\nWe offer a newly created framework for Arabic automatic story evaluation based on LLMs.\nWe develop two novel datasets for automatic story generation: one consisting of translated narratives from the TinyStories (Eldan and Li, 2023) dataset, which was meticulously curated, and another comprising a synthetic dataset created using GPT-4, featuring narratives in MSA and two dialects.\nWe compare two distinct fine-tuning methods on AraLLaMA against AceGPT-7B (Huang et al., 2024), GPT-3.5, and Command-R, powerful open source and proprietary models using extensive automatic and human evaluations.\nThe remainder of this paper is organized as follows: Section 2 provides a review of prior studies focusing on the task of automatic story generation. Section 3 details the creation of our datasets. In Section 4, we outline our prompt design. In section 5 we detail our different experiments. Results and key insights from our comparative analysis of our fine-tuned models against various commercial and open-source models are discussed in Section 6. The paper concludes with Section 7."}, {"title": "Related Work", "content": ""}, {"title": "Early Work on Story Generation.", "content": "Jain et al. (2017) is an early work on generating coherent stories, experimenting with two paradigms: Statistical Machine Translation (SMT) and Deep Learning. SMT treats story generation as a translation task, while Deep Learning uses Recurrent Neural Networks (RNNs) to encode sequences of input descriptions into hidden representations, which are then transformed into detailed summaries. They evaluate their models using BLEU, ROUGE-L, and human evaluation. Fan et al. (2018) propose a hierarchical model that first generates a story premise using a convolutional language model (Dauphin et al., 2017) and then a seq2seq model to create a story that follows the premise. They incorporate gated multi-scale attention and model fusion to improve prompt adherence."}, {"title": null, "content": "Akoury et al. (2020) introduce the STORIUM dataset\u00b3 and fine-tune GPT-2-medium (Radford et al., 2019) for generating short story scene entries, motivated by GPT-2's 1024-token context window. Plug-and-Blend (Lin and Riedl, 2021) consists of a Blending Generative Model (BGM) and Planner for controllable story generation. BGM facilitates controlled continuations, while Planner specifies control parameters based on topic descriptions and story sections. The authors fine-tune GPT-2-large (Radford et al., 2019) on ROCStories (Mostafazadeh et al., 2016) and use pre-trained GeDi (Krause et al., 2020) as the guiding model, evaluating fluency and fidelity through human evaluation."}, {"title": "LLM Story Generation.", "content": "Mirowski et al. (2022) propose using a 70B Chinchilla LLM called Dramatron for generating long narratives, such as full scripts and screenplays, through prompting, prompt chaining, and hierarchical generation. Dramatron supports collaborative writing and was qualitatively assessed via co-writing sessions and interviews with 15 industry professionals.\nYang et al. (2022) propose the Recursive Reprompting and Revision (Re3) framework automatically generates longer stories without human intervention, distinguishing it from previous approaches. Re3 comprises four modules: Plan, Draft, Rewrite, and Edit. The Plan module creates a story plan using GPT-3 (Brown et al., 2020) to add details to a given premise. The Draft module generates story continuations by recursively prompting GPT-3, dynamically updating the prompt with information from the plan and story. The Rewrite module reranks alternate continuations to select the best ones, and the Edit module ensures factual consistency with earlier parts of the story. Re3 operates in a zero-shot manner, allowing it to generate longer stories without domain constraints.\nPatel et al. (2024) propose a creative storytelling framework with two components: the story generation model and the Action Discriminator model (AD LLM). They train these models in a feedback loop called SWAG. Initially, a prompt is used to generate the first paragraph, which is then fed into the AD LLM with actions (e.g., \"add suspense\") to produce the best continuation. This process is repeated until the story reaches the desired length."}, {"title": null, "content": "The model is trained using Direct Preference Optimization (DPO) (Rafailov et al., 2023), with preference data generated by GPT-4 (OpenAI et al., 2024) and Mixtral-8\u00d77B (Jiang et al., 2024). GPT-4 samples are chosen, while Mixtral-8\u00d77B samples are rejected. Evaluation is conducted using both human and GPT-4 assessments.\nXie and Riedl (2024) introduces a method for generating suspenseful stories with LLMs using iterative prompting based on psychological and narratological theories of suspense. This zero-shot approach does not require pre-existing story corpora. Human evaluations demonstrate the effectiveness of this technique in crafting engaging suspenseful stories, and controlled studies explore factors influencing readers' perception of suspense.\nRadwan et al. (2024) introduces SARD, a tool with a visual drag-and-drop interface for creating multi-chapter stories using advanced large language models. Wordcraft (Yuan et al., 2022) is a web application for story writing that combines a text editor with controls for prompting an LLM to perform various story-generation tasks."}, {"title": "Evaluation of Story Generation in Literature", "content": "There are basically two types of evaluations for story generation in the literature: human evaluation and automatic evaluation. We explore these evaluation methods in the following subsections:"}, {"title": "Human Evaluation", "content": "Akoury et al. (2020) integrate their fine-tuned model into the STORIUM collaborative storytelling platform, where real authors can query the model to generate suggested story continuations. The authors could edit the generated text by adding or deleting content. The edited stories were collected along with ratings from the authors' on properties such as relevance, fluency, coherence, and likability. They also propose a new automatic metric called User Story Edit Ratings (USER), inspired by the longest common subsequence (LCS) of the ROUGE metric (Lin, 2004), which measures how much of the generated text is preserved in the edited version.\nThe authors of Re3 (Yang et al., 2022) ask workers from Amazon Mechanical Turk to rate Re3-generated stories against GPT-3 and GPT-3 fine-tune on stories from the WritingPrompts dataset. The evaluation criteria include interestingness, coherence, fluency, human-likeness, and relevance."}, {"title": null, "content": "Workers also identify shortcomings in the generated stories, such as disfluency, repetitiveness, confusing inconsistencies, and narration problems. Re3 outperforms all baselines on almost all criteria.\nXie and Riedl (2024) rely on a pool of three human studies to evaluate their framework for suspenseful story generation. In the first study, human judges compare stories generated by their approach against those generated by a strong baseline (ChatGPT) based on suspense, novelty, enjoyment, logical sense, and naturalness. The second study involved ablations on their system compared against the full system. In the third phase, participants reviewed the story's structure to verify internal processes. Ninety participants assessed 30 story pairs, with each pair reviewed by 30 participants. Their approach outperforms all baselines on all criteria except for a 56% tie with ChatGPT on naturalness.\nThe authors of (Patel et al., 2024) evaluate their story generation method using human judges and GPT-4. Surge AI employees assessed 50 stories generated by the Llama-2-7B and Mistral-7B models, enhanced by the SWAG technique, against four baselines: the end-to-end approach, a random selection method, GPT-3.5-turbo, and GPT-4-turbo. Evaluations focused on interestingness, surprise, and coherence. The findings show a preference for SWAG-enhanced stories over conventional methods by both human judges and GPT-4, with SWAG models winning 61.5\nWang et al. (2024) compare Weaver's variations against other open-source and proprietary LMs, including GPT-4, GLM-4, ERNIE-Bot-4.0, and Gemini-pro. Evaluations by human professionals and GPT-4 were based on creativity, style, relevance, and fluency. Weaver-Ultra was preferred 1576 and 1657 times out of 3540 samples by humans and GPT-4, respectively."}, {"title": "Automatic Evaluation", "content": "Evaluating creative writing such as story generation is a challenging task. Jain et al. (2017), one of the earlier works on neural-based story generation, uses machine translation metrics (BLEU-4, METEOR, TER, and ROUGE-L) to evaluate story generation. The overall results were low, with SMT-based methods scoring better on BLEU-4 than seq2seq models, despite being less coherent. The scores were 3.5 and 1.98 for SMT and seq2seq models, respectively, indicating that n-gram-based metrics are not suitable for creative writing judgment. GPT-Eval, an evaluation framework based"}, {"title": null, "content": "on GPT-4 (Eldan and Li, 2023), takes in a story and provides a general assessment and a score out of 10 in four criteria: grammar, creativity, consistency, and age group."}, {"title": "Common Datasets for Story Generation", "content": "To provide an overview of the resources used in story generation research, we summarize the most common datasets used to build automatic systems for generating stories in Table 1. These datasets vary in size, nature, availability, and average length of the stories they contain. The datasets include human-generated stories as well as those created by advanced language models like GPT-3.5 and GPT-4. They are valuable resources for training and evaluating story-generation models."}, {"title": "Arabic Story Generation", "content": "The task of automatic story generation is uninvestigated in the Arabic NLP community. However, (Alhussain and Azmi, 2024) utilize cross-lingual transfer learning to address the scarcity of Arabic data in Story Ending Generation (SEG) task by leveraging English story corpora."}, {"title": "Data Collection", "content": "We compile data from different resources. We first translate 1.13M English stories generated by GPT-4 alongside their prompts from the TinyStories dataset (Eldan and Li, 2023) using Google translate API.4 To ensure that we have only high-quality translation, we apply a filtering strategy based on multilingual sentence embeddings (Feng et al., 2022) and remove the story pairs whose embedding similarity is less than 92%."}, {"title": "Filtering Strategy", "content": "With the aim to train only on high-quality data, we apply Algorithm 1 to our dataset. The final threshold was 92%. And we were able to maintain 545K samples which represents 48.3% of the translated data."}, {"title": "Generated Data", "content": "We also generate our own stories from GPT-4-Turbo API using a carefully designed set of prompts and features (see Section 4). The dataset generated with our prompt template is in three Arabic varieties, namely MSA, Moroccan, and Egyptian. We tested the ability of GPT-4-Turbo to generate other Arabic dialects, but the generated content"}, {"title": "Prompt Design", "content": "Prompting is an approach employed by users to interface with LLMs (White et al., 2023). It functions as the primary mode of communication with these models, effectively serving as the input language that LLMs are designed to interpret and respond to. The efficacy of the generated output is significantly correlated with the quality and structure of the input prompt. This relationship underscores the critical role that prompt engineering plays in optimizing LLM performance and output relevance. In our context, the prompt can be conceptualized as a set of instructions or parameters that guide the LLM's for the Arabic story generation process. The prompt's composition, including its specificity, clarity, features, and relevance to the desired output, directly influences the model's ability to generate appropriate and accurate stories that adhere to our instructions. This causal relationship between prompt quality and output quality highlights the importance of developing sophisticated prompting strategies to fully leverage the capabilities of LLMs for Arabic story generation."}, {"title": "Initial Investigation", "content": "When we started this study, we had three prompting choices. We either prompt in English, Arabic, or"}, {"title": "Prompt Template", "content": "We design our prompt template with two goals in mind. These are (i) to ensure high quality of the generated output and (ii) make the generated output as diverse as possible. To ensure the variety of the generated stories, we carefully design a set of 12 features: {age, place, end of story, dialogue, number of characters, moral of the story, topic, country, season, activity, emotion, plot twist}. Our template is designed in such a way that each feature has a probability p of appearance in a particular prompt. Meaning some features might be present in a prompt while others are not. Except for the following features where they appear in each prompt: age, number of characters, and country. Based on our preliminary observations, GPT-4 is able to generate coherent stories from dialectal prompts (i.e., Egyptian and Moroccan). Hence, we ask two native speakers to translate our prompt template, originally written in MSA to Egyptian and Moroccan dialects. This prompt template is used to generate MSA and dialectal stories from GPT-4 and later to fine-tune our models."}, {"title": "Experiments", "content": "We conduct two sets of experiments:\nDirectly fine-tuning on the generated data from GPT-4 Turbo-preview.\nFine-tuning on translated data, followed by further fine-tuning on data generated with the GPT-4-Turbo-preview model.\nThe details of each experiment are described next."}, {"title": "Supervised Fine-Tuning (SFT)", "content": "We instruct fine-tune AraLLaMa-2-base (Alwajih et al., 2024) using a diverse Arabic instruction tuning dataset generated with our custom prompt template. AraLLaMa-2-base is a 7B parameter model based on Llama-2 (Touvron et al., 2023), continually pre-trained on Arabic data. AraLLaMa-2 has shown superior performance compared to other Arabic LLMs such as AceGPT-7B (Huang et al., 2024) and Jais-7B (Sengupta et al., 2023), hence we adopt it for our experiments. For computational"}, {"title": "Evaluation", "content": "Evaluating generative tasks remains an open problem in the AI community. However, in our study, we follow previous works such as (Eldan and Li, 2023) in adapting GPT-4 as an evaluator for model performance. We also conduct an extensive human evaluation trying to understand different model capabilities and how the evaluation of GPT-4 compares to that of human judges. We next describe our two evaluation strategies."}, {"title": "GPT-4 As a Judge", "content": "We evaluate our models on five criteria scored by GPT-4. We design a comprehensive prompt that works as follows: given the original story prompt plus the corresponding generated story, we ask GPT-4 to act as an Arabic language expert and assign a score out of five on the following criteria:\nFluency: The degree to which the text reads smoothly and naturally, with appropriate grammar, vocabulary, and sentence structure.\nCoherence: The logical connection and flow between sentences and ideas, making the text easy to understand.\nInstruction Following: The extent to which the text meets the given instructions or task requirements.\nConsistency: The degree to which the information and style within the text remain uniform and accurate throughout.\nVariety: How well the model generates a story in the required Arabic variety.\nWe find that GPT-4 tends to score the MSA content higher than dialectal ones, even if the task is to generate a dialectal story. To mitigate this issue, we added this last feature where we explicitly ask GPT-4 if the generated content follows the required Arabic variety specified in the prompt or not, and to which degree. Figure 2 demonstrates the prompt"}, {"title": null, "content": "we pass to GPT-4 for evaluation.\nWe evaluate 20 new prompts by asking the models to generate 20 corresponding stories and then pass the prompt plus story to GPT-4 for evaluation. We compare our two models against three other open and proprietary models. Namely, we compare against GPT-3.5, Command-R, and AceGPT-7B-Chat (Huang et al., 2024).\nIt is pertinent to note that we experimented with other strong open-source models, such as LLaMA-3-70B-Chat (Touvron et al., 2023) and Mixtral-8x7B (Jiang et al., 2024) (accessed through an API), but these models failed to adhere to our instructions. This failure highlights the superiority of our models over these strong baselines. Furthermore, we could not compare our models against larger Arabic LLMs, such as Jais-30B (Jain et al., 2017) and AceGPT-13B (Huang et al., 2024), due to computational constraints. In other words, we limited our comparisons to 7B models unless an API was available.\nTable 2 depicts the results of each tested model according to GPT-4. As we clearly see from Table 2, the overall results gap between the models is very narrow. In addition, both our model A and model B are very competitive with larger models even though they are an order of magnitude smaller. Model A performs well in Instruction Following across all three varieties and shows strong Consistency and Fluency in MSA. Model B exhibits better Consistency in MSA and Moroccan, shows strong Fluency and Coherence in Egyptian, and relatively lower Variety scores.\nModel B which was exposed to additional training steps on translated data, performs better than Model A on almost all metrics across dialects, which proves indeed the intuition behind training on more data does help. Comparative results suggest that there might be opportunities for further fine-tuning or learning from stronger models such as Command-R since this model shows strong performance across multiple metrics. Our results demonstrate that Command-R is the strongest baseline, and the most consistent model across metrics and varieties. Even in the Variety feature where the performance of other models falls, Command-R achieves a score as high as 3.27. Command-R outperforms even GPT-3.5 while being only a fraction of its size, suggesting that the model size is not everything and the quality of data does help. We can see from Table 2 that almost all metrics drop for dialectal varieties compared to MSA. This can be directly linked to the lack of Arabic dialectal data that LLMs have been exposed to during the pre-training stage. We included bar charts in Figure 3 for more details."}, {"title": "Human Evaluation", "content": "Pilot evaluation. We carry out a pilot investigation where a native speaker of Arabic with knowledge of different dialects inspects stories generated by different models and comes up with observations. The expert finds that Model A tends to generate longer stories compared to Model B. Both models A and B are less perfomant in the Moroccan dialect compared to the Egyptian dialect. AceGPT-7B-Chat (Huang et al., 2024) and GPT-3.5 fail to generate dialectal stories, even when explicitly prompted to do so.\nExtensive human comparison of different models. We ask four Arabic native speakers to rank ten stories generated by different models based on the following criteria: Instruction Following, Fluency, and Variety Adherence. However, since AceGPT-7B-Chat and GPT-3.5 failed to generate dialectal content, we include these models only in"}, {"title": null, "content": "the MSA part of the human evaluation task (i.e., we exclude them from the dialectal evaluation). We thus compare our two models, model A and model B, to Command-R. We ask an Arabic native speaker to compare different models to each other, finding our Model A to surprisingly be almost always better than GPT-3.5(90%). Our model A also outperforms Command-R(35B) 40% of times, always outperforms AceGPT-7B-Chat, and Model B 70%. More details are in Figure 4.\nFor the human evaluation of dialects, we compare our models against Command-R, which was the only model other than ours able to generate dialectal content. Our experts find that Model A to be able to outperform Command-R 20% of the time for both dialects and outperforming Model B 50% and 60% on Moroccan and Egyptian dialectal stories, respectively (Figure 5 for visualization)."}, {"title": "Conclusion", "content": "In this paper, we present the first LLM-based study on automatic Arabic story generation. Our study takes as its target MSA and two Arabic dialects (Egyptian and Moroccan). For our purpose, we translate and generate datasets based on a custom prompt template. We fine-tune our models on these datasets, comparing against both equally- and bigger-sized models. Through extensive automatic and human evaluation, we empirically show our models' superiority to strong baselines. In the future, we plan to fine-tune bigger models on larger datasets. We also plan to include more dialects in our training, for wider coverage."}, {"title": "Limitations", "content": "This study has the following limitations:\nCompute constraints. Due to computational limitations, we restricted ourselves to models with a maximum size of seven billion parameters or those with an available API.\nLimited data. Our training dataset consisted of only 3,000 samples of high-quality data generated by GPT-4. In the future, we are planning to generate more data with the newly released GPT-4O.\nLack of error analysis: We believe carrying out an error analysis would benefit our work. In particular, we observe that GPT-4 does not fully adhere to our instructions 100% of the time during the generation of training data. This could lead to issues in the data generated using this model and an error analysis could uncover any such limitations. Future work should take this into account."}, {"title": "Ethical Considerations", "content": "Similar to other generative models, our model can reflect the bias in its data. Any use of the model should take this into account."}]}