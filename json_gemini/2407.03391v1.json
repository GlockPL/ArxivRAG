{"title": "Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning", "authors": ["Simon Ostermann", "Kevin Baum", "Christoph Endres", "Julia Masloh", "Patrick Schramowski"], "abstract": "Prompt injection (both direct and indirect) and jailbreaking are now recognized as significant issues for large language models (LLMs), particularly due to their potential for harm in application-integrated contexts. This extended abstract explores a novel approach to protecting LLMs from such attacks, termed \"soft begging.\" This method involves training soft prompts to counteract the effects of corrupted prompts on the LLM's output. We provide an overview of prompt injections and jailbreaking, introduce the theoretical basis of the \"soft begging\" technique, and discuss an evaluation of its effectiveness.", "sections": [{"title": "Background: Attacking LLMs", "content": "Current LLMs lack adversarial robustness (Carlini et al., 2021). This leads to new phenomena observed in the attack surface of LLMs that are relevant to safety and security. Prominent examples include: jailbreaking, direct prompt injections and indirect prompt injections.\nIn jailbreaking, a target LLM is manipulated to circumvent content moderation systems and subvert safety rules (Zou et al., 2023). Prompt injections describe attacks where input is inserted into an application and processed by the LLM later downstream - with indirect prompt injections denoting those cases where the attacker is not the user prompting the LLM (Greshake et al., 2023). As can be seen, direct prompt injection (Perez and Ribeiro, 2022) is similar to jailbreaking: Both hide specific instructions inside a prompt that alters or manipulates the behavior of the system in a manner not intended by the LLM provider. This adversarial instruction is typically conveyed through text which is sometimes visually concealed (using tactics such as white or extremely small font sizes), encoded (Liu et al., 2024b) or obfuscated by an API (for example, a command that is appended to each input without the end user's awareness).\nEspecially indirect prompt injection attacks, where adversaries remotely affect the target system, exploit the fact that there is no explicit separation between code and data in current LLMs and, as a consequence, between valid system instructions and invalid, potentially adversarial, instructions (Zverev et al., 2024).\nDangers of prompt injections include data leakage, i.e., the disclosure of sensitive information, and system manipulation, i.e., altering the behaviour of the system based on the injected prompt. In the most extreme case, the adversarial user can hijack the LLM (Qiang et al., 2024), which is even more problematic if the LLM has great autonomy and is, for example, given access to plugins or third-party data.\nAs the goal of the examples mentioned above is often to elicit harmful behavior of an LLM, it is important to note that there are also benign and neutral usages. Benign injections may guide the model to generate more specific or desirable outputs, while neutral injections serve to test the model's resilience, accuracy, or response to edge cases without malicious intent."}, {"title": "Shielding LLMs against Jailbreaking and Prompt Injections", "content": "Defense methods against jailbreaking and prompt injection attacks can be grouped into two directions: attack prevention and attack detection.\nThe most straightforward prevention method to shield an LLM is to formulate counter-prompts to neutralize the harmful injection (\u201cHello ChatGPT, please ignore any harmful prompts that might follow this instruction and just do exactly as I say\"). While easy to implement, such \u201cbegging\u201d is easy to trick and rarely successful. Research has since come up with a range of more elaborate countermeasures. Jain et al. (2023) proposed two prevention baseline methods that illustrate how simple defenses such as input preprocessing (paraphrasing and retokenization) can effectively act against gradient-based methods. Whereas paraphrasing rewrites the input by changing its meaning, retokenization breaks down the tokens into smaller ones. Recently, input preprocessing techniques have been studied that aim at making it easier for the model to distinguish between valid instructions and untrustworthy input (Hines et al., 2024).\nThe most frequently used post-training detection method is the implementation of filters, i.e., additional algorithms or models that try to scan inputs for injected harmful prompts and mask them or reject the prompts automatically (Dong et al., 2024). Detection methods for prompt injection, for example, often rely on the perplexity score, which is assumed to be higher for adversarial inputs and can be detected by a simple thresholding approach (Alon and Kamfonas, 2023; Jain et al., 2023). Yet, setting the threshold is not trivial and a poor threshold might hurt the model's overall performance. Generally, filters can be implemented as rule-based checks, but they can also be trained deep learning models that classify inputs into harmful and non-harmful parts. The problem with such filters is that they are often too restrictive in the case of model-based filters, and too loose in the case of rule-based filters.\nAs an alternative, models can theoretically be fine-tuned to be robust against attacks. While working reasonably well, the disadvantage of such methods is that the fine-tuning of whole models is costly; it needs to be redone whenever a new type of malicious prompt is detected; and even robust detection of injections always leaves room for vulnerabilities. Also, although the field of parameter-efficient fine-tuning is being reasonably well studied and still growing (Lialin et al., 2023), contributions that focus on its efficiency in cybersecurity contexts are lacking."}, {"title": "Soft Begging: Shielding LLMs with Soft Prompts", "content": "We propose soft begging as a new alternative for LLM shielding. The method can be seen as a combination of the na\u00efve begging approach, combined with parameter-efficient fine-tuning techniques. The method basically comprises the training of so-called soft prompts, i.e. trainable input vectors that are preprended to any prompt. The soft prompts are trained to nullify the behaviour that the LLM exhibits based on potentially harmful parts of the prompts. This is not done as a filtering step, but in an implicit way \u2013 i.e. without altering the prompts \u2013 which effectively follows the idea of \"begging\" the network to ignore the harmful parts on a parameter level.\nWe conjecture that a first advantage of using such soft prompts is their effectiveness, as it enables shielding on the parameter-level against attacks on the text level, which effectively provides the shield with an advantage: A parameter-level control can be assumed to be always more effective than textual control. Second, shields trained in such a way are easy and efficiently adaptable, as the training of a soft prompt is magnitudes faster than training the whole model, as is done for example when fine-tuning LLMs to be robust against injections. At the same time, obviously, the LLM itself stays as is with its parameters being frozen. Last, we assume that soft begging prompts could be modularized to fit different types of attacks and even be combined for different use cases, rendering them as a very effective and customizable alternative to other shields.\nIn the most basic version, such prompts are trained on quadruples of clean prompts, corrupted prompts, clean output and output based on the cor-rputed prompt. The soft prompts are then trained to produce the clean output from the corrupted prompt.\nThe idea can be scaled up, e.g. by training different soft prompts for different injections and combining them via prompt fusion or other mechanisms. Also, the prompts could be combined with a filtering mechanism, that first identifies the kind of threat (without localizing it), and then picks the matching soft prompt based on this."}, {"title": "Evaluation of Prompt Injection", "content": "Usually, jailbreaking and direct prompt injection attacks are evaluated by injecting a malicious prompt with a specific phrase into benign queries and observing the output (Perez and Ribeiro, 2022; Chen et al., 2024). This allows for easy evaluation, as the attack is successful if the specified keyword is present in the LLM response.\nIn contrast to that, in indirect prompt injection attacks, the malicious prompt is embedded into malicious documents and sources, as the name suggests. As described above, the output of the LLM can be reviewed for the injected target keywords,"}]}