{"title": "Spatio-Temporal Road Traffic Prediction using Real-time Regional Knowledge", "authors": ["Sumin Han", "Jisun An", "Dongman Lee"], "abstract": "For traffic prediction in transportation services such as car-sharing and ride-hailing, mid-term road traffic prediction (within a few hours) is considered essential. However, the existing road-level traffic prediction has mainly studied how significantly micro traffic events propagate to the adjacent roads in terms of short-term prediction. On the other hand, recent attempts have been made to incorporate regional knowledge such as POIs, road characteristics, and real-time social events to help traffic prediction. However, these studies lack in understandings of different modalities of road-level and region-level spatio-temporal correlations and how to combine such knowledge. This paper proposes a novel method that embeds real-time region-level knowledge using POIs, satellite images, and real-time LTE access traces via a regional spatio-temporal module that consists of dynamic convolution and temporal attention, and conducts bipartite spatial transform attention to convert into road-level knowledge. Then the model ingests this embedded knowledge into a road-level attention-based prediction model. Experimental results on real-world road traffic prediction show that our model outperforms the baselines.", "sections": [{"title": "Introduction", "content": "Traffic prediction refers to predicting taffic-related values such as traffic speed or volume (Yuan and Li 2021). From the perspective of urban planning, traffic prediction is essential as it helps to determine traffic signal operation or plan public transportation. From an individual's point of view, it becomes necessary to plan a trip such as when to leave or which transportation to use given the traffic condition. Traffic prediction is broadly classified into three categories depending on the time period: short-term (a few minutes to an hour), mid-term (within a few hours), and long-term (more than one day) prediction (Hou and Li 2016). A typical application of short-term prediction is a real-time navigation system, and mid/long-term predictions are used for transportation planning.\nWhile most of the previous deep learning approaches have tackled short-term traffic prediction problems (Jiang and Luo 2021), the mid/long-term traffic prediction has recently become increasingly important for advanced traffic management systems (ATMS), especially with the rise of autonomous cars and car-sharing services (Lana et al. 2018; Hoermann, Stumper, and Dietmayer 2017). Unlike short-term prediction, which mainly utilizes graph information of road network, recent studies have shown that mid/long-term traffic prediction benefits from regional knowledge (e.g., Point of Interests (POIs)) (Zhang et al. 2017; Hu et al. 2018; Gonz\u00e1lez, Loukaitou-Sideris, and Chapple 2019). However, regional knowledge is not static but dynamic \u2013 it may change as a social event occurs or travel demand changes by unexpected causes (Huang et al. 2019). This would be particularly true for those cities with a large population and high density. Thus, existing models that incorporate static regional knowledge for traffic prediction may fail in a complex urban environment where traffic is affected by real-time regional exploitation.\nAs live regional datasets are becoming more available such as real-time regional population or ridership data as Figure 1, there is increasing interest to utilize this regional knowledge to produce better traffic prediction. In incorporating real-time regional knowledge for traffic prediction, three main challenges remain to be solved as follows:\n1) Incorporating region-level and road-level data equally: Previous work (Zhang et al. 2020; Lv et al. 2020; Yuan, Zhou, and Yang 2018) did not handle regional-level and road-level data separately, which may cause performance degradation. For example, it would produce a better result if regional data is analyzed using convolutional neural network (CNN) while road data is incorporated using graph convolutional neural network (GCN).\n2) Dynamic regional knowledge learning: Although a model is given real-time regional traffic data and finds certain patterns, it can not solely explain why such patterns occur without background information about the region. The model can have limited learning performance if there is no reasoning from the built-in environment to analyze the regional data.\n3) Transformation of regional knowledge into a road: For modality transformation, in determining how much regional information to use for each road, transportation studies typically bound 500m nearby region, which is a walking distance to transit (van Soest, Tight, and Rogers 2020). However, this bound should be adapted differently depending on the road characteristics.\nIn this study, we propose a novel method to embed both real-time and static regional knowledge for traffic prediction. In particular, we use fine-grained, hourly population estimated from LTE access trace counts together with POIS and satellite images to capture the built-in environment. Our model is composed of a regional spatio-temporal module that consists of dynamic convolution and temporal attention, and conducts bipartite spatial transform attention to convert into road-level knowledge. Then the model ingests this embedded knowledge into a road-level attention-based prediction model. Experimental results show that our model outperforms the baselines. We summarize our contributions as follows:\n\u2022 To the best of our knowledge, this is the first research that incorporates real-time regional population data for road-level traffic prediction with each modality training. For this, we propose a novel method that learns real-time region-level knowledge for road-level prediction.\n\u2022 We propose a dynamic convolution based on regional correlation and distance, and a bipartite masked transform attention which adds a gaussian mask to train attention scores from the nearby region for each road.\n\u2022 We construct real-world dataset in Seoul, Korea, and the evaluation shows that our model outperforms by utilizing real-time regional knowledge. We make our dataset publicly available to contribute research community."}, {"title": "Related Work", "content": "Road Traffic Prediction\nThe biggest challenge of the road traffic prediction model is to use the road connectivity network for training, as the impact of connection in a road graph is higher than the Euclidean distance between traffic sensors. DCRNN (Li et al. 2017) captures spatio information of road graph through diffusion convolution and combines with RNN module to learn spatio-temporal information. ST-MetaNet (Pan et al. 2019) learns the meta knowledge of nodes and edges, and extracts the weights for an RNN cell from meta information to give illusion that each traffic sensor has its own RNN cell that conducts temporal prediction. GMAN (Zheng et al. 2020) leverages spatio-temporal embeddings by Node2Vec (Grover and Leskovec 2016) and timestamps, and applies a spatial attention and a temporal attention combined with a gated fusion. ST-GRAT (Park et al. 2020) proposes sentinel mechanism that complement a spatial attention when there are nodes have less spatial correlation.\nRegional Traffic Prediction\nFor regional traffic prediction models, it is important to learn features for each adjacent cell in a 2D grid. Therefore, a model that passes spatial information through regional filters like convolutional neural network (CNN) and learns temporal information through RNN/temporal attention is mostly proposed. ConvLSTM (Xingjian et al. 2015) is an improved structure that can be used for next video frame prediction by combining a convolution layer instead of fully Connected layer inside LSTM. HeteroConvLSTM (Yuan, Zhou, and Yang 2018) applies this ConvLSTM structure for regional spatio-temporal traffic accident prediction. MC-STGCN (Tang et al. 2021) incorporates spatial correlations among regions using regional community graphs and leverages GRU for temporal correlation learning for better taxi ridership demand prediction.\nMulti-modal Traffic Prediction\nThere are a handful of research that use both real-time region-level and road-level data for prediction. Curb-GAN (Zhang et al. 2020) proposes generative adversarial network (GAN) that estimates regional traffic speed from travel demand inferred from taxi ridership. HeteroConvLSTM (Yuan, Zhou, and Yang 2018) transforms the road graph onto 2D-grid region to embed road information for regional traffic prediction. On the other hand, there are several trials to use regional features for traffic prediction. Lv et al. (2020) utilizes POIs with gaussian kernel from each road to capture road-level geographical information to enhance road traffic prediction. Lin et al. (2020) captures land-use proportion within 500m (walking distance) of a subway station for station-level traffic prediction."}, {"title": "Preliminaries", "content": "We define our problem as a road traffic prediction using both road and regional traffic history. We denote road traffic data (e.g. traffic speed, traffic flow volume) as X, and regional traffic data (e.g. population density, travel demand) as Z. For a timestamp t, $Z^{(t)} \\in \\mathbb{R}^{N_z}$, where $N_z = N_h \\times N_w$ is the number of grid cells of a $N_h$-height and $N_w$-width rectangular region, and $X^{(t)} \\in \\mathbb{R}^{N_x}$ where $N_x$ is the number of traffic sensors of the roads. The problem is formulated as finding an optimal function $h(\\cdot)$ that inputs P-temporal history of road and regional traffic data, to output Q-sequence road traffic prediction of a timestamp t, as Equation 1.\n$[X^{(t-P+1)}, ..., X^{(t)}, Z^{(t-P+1)}, ..., Z^{(t)}] \\xrightarrow{h()}, [\\hat{X}^{(t+1)}, ..., \\hat{X}^{(t+Q)}]$ (1)"}, {"title": "Multi-head Attention Mechanism", "content": "We leverage dot-product attention (Vaswani et al. 2017) in our method, which has the following form:\n$H = Att(Q, K, V) = S(Q, K)V = softmax(\\frac{Q K^T}{\\sqrt{d_h}})V$ (2)\nwith Att() is the attention function, and Q (query), K (key), V (value) is formulated by\n$Q = f_1(X_q), K = f_2(X_k), V = f_3(X_v)$ (3)\nwhere $X_q \\in \\mathbb{R}^{N_Q \\times d_q}, X_k \\in \\mathbb{R}^{N_p \\times d_k}, X_v \\in \\mathbb{R}^{N_p \\times d_v}$ are inputs for Q, K, V and $f_1, f_2, f_3$ are activation functions. In this paper, we denote a non-linear activation function as $f(x) = ReLU(xW + b)$ where W, b are learnable parameters. $N_Q$ is the number of queries and $N_p$ is the number of keys and values. If we use the module as self-attention, $N_Q$ and $N_p$ become equal as we input $X_q, X_p, X_k$ the same value. In this work, we employ $f_1, f_2, f_3$ to produce the embedded dimensions of Q, K, V to be equally $d_h$. The attention score $S(Q, K)$ calculates $N_Q \\times N_p$ values of how $V\\in \\mathbb{R}^{N_p \\times d_h}$ will be summed. In multi-headed attention, we concatenate (denoted as ||) the output of K-attention heads, and apply an activation $f_o$. In this study, we set $K \\times d_h$ to be D for all attention mechanism.\n$MHAtt(Q, K, V) = f_o(||\\{H_k\\})$ (4)\nWe also denote self attention as $SelfAtt(X) = MHAtt(f_1(X), f_2(X), f_3(X))$ for an illustration purpose."}, {"title": "Masked Attention Networks", "content": "We leverage masked attention mechanism used in Sperber et al. (2018). The idea is to add mask M to attention scores before softmax is applied.\n$Att_M(Q, K, V, M) = S_M(Q, K, M)V \\ where$\n$S_M (Q, K, M) = softmax(\\frac{Q K^T}{\\sqrt{d_h}} + M)$ (5)\nIn general, we can apply strict masking $M \\in \\{0, -\\infty\\}$ to take attention on specific values. We can also apply different masks for multi-head attention heads. We denote multi-head masked attention networks as $MHAtt^M(\\cdot)$."}, {"title": "Methodology", "content": "Figure 2 shows overall architecture of our proposed model. Our approach is intuitioned by GMAN (Zheng et al. 2020) that leverages spatio-temporal embeddings for self-attention and transform attention. The biggest difference of our model to GMAN is that we give regional knowledge as query and key to the temporal transform attention of the GMAN block.\nTo begin with, there are spatio-temporal embeddings for road-level (STEx) and region-level (STEz) which mark the spatial information and timestamps. On the left side, region-level encoder and decoder compute representations of P- and Q-sequence regional data. On the right side, the model leverages the bipartite spatial transform attention to convert this region-level knowledge into road-level knowledge and feed the road-level temporal transform attention. A L-stacked ST-Att block simply concatenates a spatio-temporal embedding with the previous hidden output as its input and conducts self-attention, while making the residual addition. A transform attention block (temporal and bipartite spatial) uses different attention query and key depends on how it will transform the input whether in temporal dimension ($P \\rightarrow Q$) or in spatial dimension ($N_z \\rightarrow N_x$). In our implementation, all modules produce D-dimensional outputs for residual computation."}, {"title": "Spatio-Temporal Embedding", "content": "In order to train spatio-temporal knowledge based on the road-network and geographic regional environment, we give different spatio-temporal embeddings (STE) for region-level and road-level. Figure 3 shows how we create spatio-temporal embedding to be used in an attention module.\nThe STE for regional data (STEz) is produced by adding regional spatial embedding with temporal embedding. For a region spatial embedding, we use location (latitude, longitude), POI counts, and satellite image features. We concatenate these geographical features for each cell and apply a 2-layer fully connected network (FCN) to create D-dimensional output. For a temporal embedding, we concatenate one-hot encodings of hour-of-day and day-of-week ($R^{24+7}$) and apply a 2-layer FCN to create an output. Finally, we add these values in combination of $N_z \\times (P+Q)$ to create $STE_z \\in \\mathbb{R}^{N_z \\times (P+Q) \\times D}$.\nThe STE for road data (STEx) is produced as similar to STEz. But, for the road spatial embedding, we use road embedding feature trained from Node2Vec (Grover and Leskovec 2016) using road network, and conduct a 2-layer FCN to create D-dimensional output. For the temporal embedding, we share the same one as STEx. Then we add these values in combination of $N_x \\times (P + Q)$ to create $STEx \\in \\mathbb{R}^{N_x \\times (P+Q) \\times D}$."}, {"title": "Spatio Temporal Attention Block", "content": "Figure 4 shows the how we implement regional and graph spatio-temporal attention (ST-Att) block. A ST-Att block learns spatio-temporal patterns of the input data by self-attention mechanism. The regional ST-Att (R-ST-Att) block captures spatial pattern by dynamic convolution and temporal pattern by temporal attention. The road graph ST-Att block (G-ST-Att) captures spatial and temporal pattern by respective attention mechanism. In both modules, the gated fusion is applied to sum spatial output and temporal output by trainable ratio that produces optimal output.\nDynamic Convolution A pair of cell is correlated if their regional characteristics are similar, while there is a less correlation between distant cells. To give regional correlation degraded by distance, we apply a dynamic graph convolution (Zhang et al. 2020) with different edge weights based on Pearson correlation of real-time population on total timespan and distance. We formulate it as $H^{(1)} = f(A_R H^{(l-1)}) = ReLU(\\tilde{A_R} H^{(l-1)} W)$ with $\\tilde{}$ is a row-normalization, $W \\in \\mathbb{R}^{D \\times D}$ is a learnable parameter, and $A_R \\in \\mathbb{R}^{N_z \\times N_z}$ is an edge weight matrix calculated as\n$A^{R}_{i,j} = \\begin{cases} r_{i,j} exp(-(d_{i,j}/\\sigma_{dist})) & \\text{if } r_{i,j} > \\lambda \\\\ 0 & \\text{otherwise} \\end{cases}$ (6)\nwhere $r_{i,j}$ is a pearson correlation between regional traffic data of total timespan, $d_{i,j}$ is an Euclidean distance, and $\\sigma_{dist}$ is a standard deviation of distances.\nSpatial Attention Basically, a spatial attention and a temporal attention are in the same form, but in different attentional dimension: whether in spatial or temporal. A spatial attention is used for road-level spatial feature extraction by calculating $R^{N_x \\times N_x}$ attention score to find correlation between the roads. We first concat STEx to the previous hidden output, and apply a self attention as $H_{XS}^s = SelfAtt((H_X^{(l-1)} || STE_X)^s)$.\nTemporal Attention The temporal attention is used for both region-level and road-level temporal feature extraction. The module calculates $R^{P \\times P}$ or $R^{Q \\times Q}$ attention scores depending on the module's objective is an encoder or decoder. Similar to spatial attention, we first concatenate STE to the input before we apply temporal attention. For each modality, we concatenate corresponding STE with previous hidden output and apply a self attention as $H_{MT}^s = SelfAtt((H^{(l-1)} || STE_M)^T)$, where M \u2208 {\u2018X\u2019, \u2018Z\u2019}.\nGated Fusion In order to mix spatial and temporal hidden outputs depending on their importance, we leverage gated fusion proposed by Zheng et al..\n$H^{(l)} = P * H^{(l-1)}_S + (1 - P) * H^{(l-1)}_T$, with\n$P = ReLU(H_S^{(l-1)} W_{P,1} + H_T^{(l-1)} W_{P,2} + b_p)$ (7)\nwhere * is an element-wise multiplication and $W_{P,1} \\in \\mathbb{R}^{D \\times D}, W_{P,2} \\in \\mathbb{R}^{D \\times D}, b_p \\in \\mathbb{R}^D$ are trainable parameters. A gated fusion is used both for R-STAtt and G-STAtt blocks."}, {"title": "Bipartite Spatial Transform Attention", "content": "To give real-time regional knowledge for road-level traffic prediction, we propose a bipartite transform attention that converts the spatial modality of regional representations from $N_z$ to $N_x$. Since a road is affected by its nearby built-in environment (Cervero and Kockelman 1997), we incorporate proximity information on a masked attention network. We give soft gaussian mask $M \\in \\mathbb{R}^{N_x \\times N_z}$ intuitioned by Sperber et al. as follows:\n$M_{r,c} = -d_{rc}/2\\sigma_e^2$ (8)\nHere, $\\sigma_e \\in \\mathbb{R}$ is a trainable standard deviation of distances to regional cells from r-th road. This enables individual road to train how much proximity information to take. Furthermore, we extend this approach into multi-head masked attention by applying different attention masks for K-heads that trains $\\sigma \\in \\mathbb{R}^{N_x \\times K}$. We formulate this road-level real-time regional knowledge representation $H^{KT}$ as follows:\n$H^{KT} = MHAtt^M (f_1(STE^T_{Z(l-1)}), f_2(H^{(l-1)}_X), f_3(H^{(l-1)}_X), M_{1,...,K}), where T \\in \\{P,Q\\}.$ (9)\nTemporal Transform Attention\nIn order to transform the temporal sequence from P to Q for an actual next sequence prediction, we leverage temporal transform attention. As described in Figure 2, we use P and Q temporal sequences of STE or HK as query and key to calculate $R^{Q \\times P}$ attention scores for each modality, and use previous hidden output as value, as follows:\n$H^{ZQ}_T = MHAtt(f_1(STE^{T(l-1)}_Z), f_2(STE^{P(l-1)}_Z), f_3(H^{(l-1)}_P))$\n$H^{XQ}_T = MHAtt(f_1(H^{KT(l-1)}), f_2(H^{KP(l-1)}), f_3(H^{(l-1)}_P))$ (10)"}, {"title": "Objective function", "content": "We insert 2-layer FCNs at the beginning when we expand dimension from 1 to D, and at the end when we shrink to produce the final output from D to 1 as described in Figure 2. Using this final output prediction $\\hat{Y}$, we train our model using mean absolute error with the ground truth $Y \\in \\mathbb{R}^{Q \\times N_x}$ for the objective loss function defined as\n$\\mathcal{L}(\\theta) = \\frac{1}{N_xQ} \\sum_{t=t_{p+1}}^{t_{p+Q}} |Y_t - \\hat{Y_t}|$"}, {"title": "Dataset", "content": "Seoul provides road traffic speed\u00b9 and regional living population data2 for public data use policy as Figure 1. We construct our real-world urban traffic dataset in three representative downtown regions in Seoul, Korea: Gangnam, Hongik, and Jamsil. The dataset description is listed in Table 1.\nRoad traffic speed data Road traffic speed is collected by measuring average hourly traffic speed from GPS signals of around 70,000 taxis in Seoul. We construct a road network using geographic information system (GIS). Each node represents a road while each edge does a link between given two roads. We assume that a traffic sensor is located at the center of a road, and calculate a distance to an adjacent road.\nReal-time regional living population (LTE) We use regional living population estimated hourly from an LTE cell tower. The original data is estimated in zip-code based region unit, but we spatially normalize into 150-meter grid cells. We normalize cell data using training data by the Z-score method, and use this trend data for our model."}, {"title": "Experimental Settings", "content": "Data Processing We split the dataset successively in 70% of the data for training, 10% of the data for validation, and 20% of the data for testing. We find that the missing values in training data affect the results of all baseline models poorly, so we fill them by next or previous valid values.\nHyperparamters The model inputs $P = 12$ historical time steps (12 hours) to predict the next $Q = 3$ steps (3 hours) of road traffic values. The hyperparameters for our model are set as $L_x = 3, L_z = 2, K = 8, d_h = 8$ (D = 64), and $\\lambda = 0.6$. The initial learning rate is set to 0.001, and we use Adam optimizer (Kingma and Ba 2014) to train our model. Our model is trained on one GPU environment of NVIDIA GeForce GTX 1080 Ti. We test 5 times and record the average and standard deviation of results.\nMetrics We use three major traffic evaluation metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\nBaselines We set our baselines as (1) Historical Average based on the hour-of-day and day-of-week (HA), (2) Support Vector Regression (SVR), (3) Random Forest Regression (RFR), (4) DCRNN (Li et al. 2017), (5) ST-MetaNet (Pan et al. 2019), (6) GMAN (Zheng et al. 2020). Unlike GMAN, other models does not have an extra module to embed a temporal timestamp. For SVR and RFR, we give the hour-of-day and the day-of-week of the first timestamp of P input sequences as extra features. For DCRNN, ST-MetaNet, we give these two timestamp information as an additional input, total 3 input channels. For each baseline models, we also test with road-level regional population data by averaging cells within 500 meters (walking distance) from each road, and give as an extra input channel. In this case, the input channel of road traffic data becomes 4 for DCRNN and ST-MetaNet, and 2 for GMAN."}, {"title": "Experimental results", "content": "Forecasting Performance Comparison Table 3 shows experimental results on our dataset. We denote +LTE as the model with extra LTE input. First of all, our model outperforms the other baselines on average RMSE, and MAPE. Compared to the RNN-based models (DCRNN and ST-MetaNet), the attention based models (GMAN and our model) are better at longer step prediction (2h, 3h). Considering that GMAN is also an attention-based model, the result shows that the improvement of our model is driven by the real-time regional knowledge. In ST-MetaNet and GMAN, the LTE data gives slight more information to improve prediction. However, in DCRNN, it leads to worse prediction in Gangnam and Hongik. Since DCRNN depends more on graph computation, it cannot fully utilize the regional LTE data due to different original modality. On the other hand, ST-MetaNet trains individual parameters for an RNN cell of each traffic sensor and GMAN applies spatial attention, but without an strict attention mask for edge connection, thus they can slightly benefit from LTE data.\nWeekly Analysis on Different Road POI Characteristics To show how regional knowledge benefits our model, we compare two groups of roads from each region-top 30% of high POI density (POI-H) and bottom 30% of low POI density (POI-L)\u2014and conduct a weekly comparison in Figure 5. We compare our model with DCRNN, ST-MetaNet+LTE, GMAN+LTE, considering whether they can benefit from LTE data. In general, our model outperforms more in POI-H than in POI-L compare to other baselines. Here, we also measure the weekly maximal performance improvement compare to GMAN+LTE to show how much our model benefits from regional knowledge. In Gangnam, our model is improved up to 3.0% in POI-H and 3.8% in POI-L (Figure 5 (a,b)). When we review Table 2, the POI density is highest in Gangnam among the test regions, so there are still many POIs in POI-L group. This allows our model to perform well on all roads in Gangnam. In Hongik, our model is improved up to 6.5% in POI-H and 3.0% in POI-L (Figure 5 (c,d)). Hongik is visited by people who come for dining and socializing and travelers who come for entertainment. Thus our model primarily benefits on the roads nearby POIs. In Jamsil, our model is improved up to 5.2% in POI-H and 3.4% in POI-L (Figure 5 (e,f)). Since there is a big stadium and an amusement park where dynamic social events occur, our model can capture such live regional information to improve our model. On the other hand, the low-POI roads in Hongik and Jamsil are located in residences or freeways, so there is less dynamic regional knowledge that our model can have benefit.\nAblation Study Table 5 shows effectiveness of each modules of our method. We first empirically measure $\\lambda$-value is suitable around 0.6 (#a, #b, #c). When we train our model only with geographic location without POI or satellite image information, the performance gets much worse as LTE data misleads the prediction (#d). However, if we have either POI or satellite image, it gives the POI or land use information that is useful to analyze regional LTE pattern (#e, #f). When we do not consider LTE data, we simply conduct $L_x$ stacks of 5 \u00d7 5-kernel 2D-CNN using geographic regional features, and produce static regional knowledge to feed bipartite spatial attention (#g). However, this case does not improve the performance than GMAN since the regional knowledge is not dynamic and limited to provide real-time knowledge. When we replace dynamic convolution into 2D-CNN (#h), the performance gets marginally better than baselines in Table 3, but not the best. When we do not use gaussian mask on bipartite transform attention, the model takes attention even for distant regions of a road, so it lowers performance (#i)."}, {"title": "Conclusion", "content": "We propose a novel method that learns spatio-temporal regional knowledge via dynamic convolution with temporal attention and transforms it into the road-level feature via the bipartite transform attention to feed a graph multi-attention for the final output. We construct three real-world datasets in Seoul consisting of traffic speed and regional population, and evaluation results show that our model outperforms other baseline models in all regions. Especially, our model performs significantly better on the roads with more POI as more dynamic regional information is available.\nWe plan to investigate how to enable our model to adaptively learn based on the social sense of place by leveraging temporal POI exploitation and land use. We believe our study provides insights for urban planners and researchers who investigate the correlation between the social use of a region and its corresponding traffic."}]}