{"title": "Towards Generalized Offensive Language Identification", "authors": ["Alphaeus Dmonte", "Tejas Arya", "Tharindu Ranasinghe", "Marcos Zampieri"], "abstract": "The prevalence of offensive content on the internet, encompassing hate speech and cyberbullying, is a pervasive issue worldwide. Consequently, it has garnered significant attention from the machine learning (ML) and natural language processing (NLP) communities. As a result, numerous systems have been developed to automatically identify potentially harmful content and to mitigate its impact. These systems can follow two approaches; (i) Use publicly available models and application endpoints, including prompting large language models (LLMs) (ii) Annotate datasets and train ML models on them. However, both approaches lack an understanding of how generalizable they are. Furthermore, the applicability of these systems is often questioned in off-domain and practical environments. This paper empirically evaluates the generalizability of offensive language detection models and datasets across a novel generalized benchmark: GenOffense. We answer three research questions on generalizability. Our findings will be useful in creating robust real-world offensive language detection systems.", "sections": [{"title": "1 Introduction", "content": "The presence of offensive posts on social media platforms leads to various negative consequences for users. Offensive posts have been linked to harmful outcomes such as increased suicide attempts [19,27] and mental health issues such as depression [3,8]. To address these serious repercussions, content moderation is typically employed on online platforms. Given the overwhelming volume of posts, however, human moderators alone cannot handle the task effectively, necessitating the development of automatic systems to assist them [41,51,48].\nA highly effective method for constructing systems that can detect offensive language involves using publicly accessible application endpoints and models in an unsupervised fashion. Notably, the development of openly accessible services such as perspective API [24] and models such as toxicBERT have greatly facilitated this approach. Furthermore, a more recent development involves the use of LLMs"}, {"title": "2 Related Work", "content": "Offensive Language Detection The problem of offensive language on social media has gained a lot of attention within the ML/NLP community. Researchers and organizations have developed systems to identify multiple types of offensive content such as aggression, cyberbullying, and hate speech [12,34]. Perspective API [24] is one such free API that was trained on the Toxic Comment Classification dataset [7]. More recently, with the rise of LLMs such as GPT, researchers have used LLMs to detect and identify various forms of offensive language [50]. [20] utilized"}, {"title": "3 GenOffense: A Generalized Offensive Language Detection Benchmark", "content": "The root cause for the lack of generalization research on offensive language detection is that no standard benchmark exists for the domain. While there are several popular datasets for offensive language identification, each of them has been annotated using different annotation guidelines and taxonomies. This, in theory, limits the possibility of combining existing datasets when training and evaluating robust offensive language identification models. To address this"}, {"title": "3.1 GenOffense Construction", "content": "We use eight popular publicly available datasets containing English data summarized in Table 1 to construct GenOffense. As the datasets were annotated using different guidelines and labels, following the methodology described in [33], we map all labels to OLID level A [46], which is offensive (OFF) and not offensive (NOT). We choose OLID due to the flexibility provided by its general three-level hierarchical taxonomy below, where the OFF class contains all types of offensive content, from general profanity to hate speech, while the NOT class contains non-offensive examples.\nLevel A: Offensive (OFF) vs. Non-offensive (NOT).\nLevel B: Classification of the type of offensive (OFF) tweet - Targeted (TIN) vs. Untargeted (UNT).\nLevel C: Classification of the target of a targeted (TIN) tweet - Individual (IND) vs. Group (GRP) vs. Other (OTH).\nIn the OLID taxonomy, offensive (OFF) posts targeted (TIN) at an individual are often cyberbullying, whereas offensive (OFF) posts targeted (TIN) at a group are often hate speech.\nAHSD is one of the most popular hate speech datasets available. The dataset contains data retrieved from Twitter, which was annotated using crowdsourcing. The annotation taxonomy contains three classes: Offensive, Hate, and Neither. We conflate Offensive and Hate under a class OFF while neither class corresponds to OLID's NOT class."}, {"title": "3.2 GenOffense Properties", "content": "We highlight the following generalization types that GenOffense benchmark tests. These are shown as crucial generalization types by [21].\nPlatform Shift GenOffense benchmarks contains datasets from six different social media platforms. While most of the datasets are based on Twitter, GenOffense has datasets that are based on other social media platforms such as Facebook and Reddit. Therefore, GenOffense benchmark evaluates how the models can handle different platforms."}, {"title": "4 Unsupervised Offensive Language Detection Models", "content": "The following public models and APIs are evaluated in the test sets of GenOffense without any training or fine-tuning."}, {"title": "4.1 Methods", "content": "Public APIs/Models We evaluate Perspective API [24] and ToxicBERT [11]5. Perspective API is a free API developed by Google Jigsaw, that leverages machine learning to identify toxic comments. This API was first trained using a BERT [13] model, which is then distilled into monolingual CNN based models. The model was mainly trained on the TCC dataset, which we also included in GenOffense. The model has six attributes, toxicity, severe toxicity, identity attack, insult, profanity, and threat. The model generates a score between 0 and 1 for each of these attributes. For each test dataset, we get all the attribute scores for each instance. If any of the attributes have a value greater than 0.5, we classify that instance as OFF, else it is classified as NOT.\nWe also evaluate ToxicBERT on GenOffense. ToxicBERT is a BERT model trained primarily on the TCC dataset. The model is a multi-label classification model with six labels similar to Perspective API. We follow a similar approach to Perspective API to convert the ToxicBERT outputs into OFF and NOT classes.\nAdapting Transformers We evaluate different general-purpose transformer models; BERT, and two domain-specific transformer models; fBERT [37] and HateBERT [9] on offensive language identification using an unsupervised approach. We classify a test sentence as positive or negative, where the positive label represents the NOT class and the negative represents the OFF class. We concatenate the last four hidden states returned by the model as the representative embeddings for the test sentence and the labels. We then find the cosine similarity between the representative embeddings of the labels and that of the test sentence. Finally, the sentence is assigned the label with the highest cosine similarity score.\nPrompting LLMs Finally, we evaluate how LLMs perform in GenOffense benchmark, a recent trend as we discussed before. We use the following prompt to get a response from LLMs.\nComments containing any form of non-acceptable language (profanity) or a targeted offense, which can be veiled or direct, are offensive comments. This includes insults, threats, and posts containing profane language or swear words. Comments that do not contain offense or profanity are not offensive. Is this comment offensive or not? Comment:\nWe use several LLMs for prompting. We first use Davinci-003 through OpenAI API. Additionally, we use MPT-7B-Instruct, Falcon-7B-Instruct and T0-3B [36]. All of these models are available in HuggingFace6 [45], and we use the LangChain implementation."}, {"title": "4.2 Results", "content": "The results of the aforementioned models are shown in Table 2. Public APIs/models generally performed well on GenOffense compared to the other two methods. However, LLMs also provide competitive results. From the LLMs, Davinci-003 performs best, closely followed by Falcon-7B. It is clear that recent LLMs produce better results on GenOffense. Overall, Perspective API performed best on the GenOffense benchmark. It provided the best results for six datasets out of eight and had the highest overall average.\nMost of the models show inconsistent results on the datasets. Particularly, all the models do not perform well on HatE and OHS datasets which indicates that these models do not generalize well across different tasks and platforms."}, {"title": "5 Training Offensive Language Detection Models", "content": "In this section, we evaluate the supervised ML models on GenOffense benchmarks. We train the following ML models under different settings on the training sets in GenOffense benchmark and evaluate on the test sets.\nLSTM We experiment with a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the baseline in OffensEval 2019 [47]. The model consists of (i) an input embedding layer with fasttext embedding [6], (ii) a bidirectional LSTM layer, and (iii) an average pooling layer of input features. The concatenation of the LSTM layer and the average pooling layer is further passed through a dense layer, whose output is ultimately passed through a softmax to produce the final prediction. We used updatable embeddings learned by the model during training as the input.\nTransformers We also use transformers as a classification model, which have achieved state-of-the-art on a variety of offensive language identification tasks. From an input sentence, transformers compute a feature vector $h \\in R^d$, upon which we build a classifier for the task. For this task, we implemented a softmax layer, i.e., the predicted probabilities are $y(B) = softmax(Wh)$, where $W \\in R^{k\\times d}$ is the softmax weight matrix and k is the number of labels. For the experiments, we use the bert-large-cased and domain-specific fBERT [37] and HateBERT [9] available in HuggingFace [45]."}, {"title": "5.1 Model Configuration", "content": "For LSTM, we used a Nvidia Tesla k80 to train the models. We divided the dataset into a training set and a validation set using 0.8:0.2 split. We performed early stopping if the validation loss did not improve over 10 evaluation steps. For the LSTM model we used the same set of configurations mentioned in Table 3 in all the experiments. All the experiments were conducted for three times and the mean value is taken as the final reported result."}, {"title": "5.2 Results", "content": "We use multiple strategies to answer the three RQs considering generalizability with respect to training and testing data.\nWe address training set variation by training the three models in the following settings:\n1 to 1 We train a separate machine learning model on each of the eight training sets. We then evaluate the trained model on each of the eight test sets in isolation.\nAll -1 We concatenate all training sets except one and train a single machine learning model. We then evaluate the model on the test set of that particular dataset that was left out."}, {"title": "5.3 Test Set Combination", "content": "We also look at the performance of the models on a single test set combining all individual test sets in GenOffense. We use a separate BERT model on each of the eight training sets and tested them on the concatenated test set. We present the results obtained on the consolidated test set in terms of Macro and Weighted F1 Table 6."}, {"title": "6 Conclusion", "content": "This paper introduced the first generalization benchmark for offensive language detection; GenOffense. We also presented a comprehensive evaluation of the generalizability of different computational models, including recently released LLMs. We hope that our findings motivate the community to further explore the question of generalizability as argued by other recent studies [15,16].\nWe revisit the research questions posed in the introduction:\nRQ1 - Generalizability: Despite being popular, LLMs did not perform well in the GenOffense benchmark. APIs, such as Perspective, showed better generalizability. In the supervised setting, models trained on OLID, AHSD,"}]}