{"title": "Position: Towards a Responsible LLM-empowered Multi-Agent Systems", "authors": ["Jinwei Hu", "Yi Dong", "Shuang Ao", "Zhuoyun Li", "Boxuan Wang", "Lokesh Singh", "Guangliang Cheng", "Sarvapali D. Ramchurn", "Xiaowei Huang"], "abstract": "The rise of Agent AI and Large Language Model-powered Multi-Agent Systems (LLM-MAS) has underscored the need for responsible and dependable system operation. Tools like LangChain and Retrieval-Augmented Generation have expanded LLM capabilities, enabling deeper integration into MAS through enhanced knowledge retrieval and reasoning. However, these advancements introduce critical challenges: LLM agents exhibit inherent unpredictability, and uncertainties in their outputs can compound across interactions, threatening system stability. To address these risks, a human-centered design approach with active dynamic moderation is essential. Such an approach enhances traditional passive oversight by facilitating coherent inter-agent communication and effective system governance, allowing MAS to achieve desired outcomes more efficiently.", "sections": [{"title": "1. Introduction", "content": "Multi-Agent Systems (MAS) represent a critical area of research in decision-making, where multiple autonomous agents\u00b9 interact within a defined environment to achieve individual or collective goals. In the rapidly evolving landscape of Large Language Models (LLM), tools like LangChain (Topsakal & Akinci, 2023) have begun to revolutionize the way we interact with LLM, enabling a programming-like interface for sculpting application-specific interactions. Furthermore, technologies such as Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) enhance LLM capabilities by allowing them to access external databases and even other tools, and therefore broadening their operational horizon. The integration of LLM into MAS has further extended the decision-making capabilities, providing a huge knowledge base and advanced reasoning abilities that significantly enhance efficiency beyond what is achievable by human efforts alone. However, this integration introduces new challenges that are absent in traditional MAS setups.\nA core challenge in LLM-MAS intrinsically is achieving enhanced mutual understanding among agents. Unlike traditional MAS with predefined protocols ensuring deterministic behaviours, LLM-based agents, trained on diverse datasets, exhibit emergent and unpredictable behaviours. This unpredictability create a need for quantifiable mechanisms, such as trust metrics, to facilitate and verify effective agreement among agents. Without such mechanisms, agents may struggle to interpret or align with one another's actions.\nBeyond the internal challenges of agent interaction, LLM-MAS face external challenges related to uncertainty propagation. As these systems grow in complexity, the inherent uncertainties of individual LLM agents can accumulate and cascade through the network (Gu et al., 2024), potentially compromising system correctness and stability. This challenge becomes particularly salient when considering the lifecycle of LLM-MAS, where uncertainties must be quantified and managed at both individual agent-level and the system level.\nTo address these challenges while harnessing the powerful knowledge representation and reasoning capabilities of LLM, A human-centered design approach is essential. This approach incorporates active dynamic moderation as a core component of LLM-MAS, moving beyond traditional passive oversight. The moderator plays a critical role in system governance, engaging in collaborative decision-making, providing high-level perspectives to LLM agents, implementing real-time intervention protocols, and steering the system toward desired outcomes.\nIn this paper, we posit that:\n1. Agents must \"understand\" one another, necessitating quantifiable metrics with probabilistic guarantees to assess inter-agent agreement\u00b2 under uncertainty."}, {"title": "2. Challenges in Existing LLM-MAS", "content": "In this section, we first conduct a comprehensive examination of the intrinsic challenges and systemic vulnerabilities in LLM-MAS, followed by our perspectives and potential solutions to address these issues."}, {"title": "2.1. Knowledge Drift & Misinformation Propagation", "content": "Unlike traditional MAS with explicitly programmed goals, LLM-MAS faces unique challenges such as \"knowledge drift\" and \"misinformed perspective propagation\", stemming from the inherent variability and probabilistic nature in natural language processing (Fastowski & Kasneci, 2024; Xu et al., 2024c; Wang et al., 2024c). These challenges are particularly pronounced in collaborative reasoning tasks, where phenomena like the conformity effect and authoritative bias lead agents to align with wrong consensus or defer to perceived authority, amplifying reasoning errors and distorting knowledge bases-even some agents initially hold correct viewpoints (Zhang et al., 2024b). For instance, in multi-agent debates, an agent with a partially flawed understanding may generate persuasive yet erroneous rationales, potentially impacting others and collectively diverting the reasoning path from accurate solutions (Breum et al., 2024).\nAdditionally, LLM agents exhibit a tendency for \"cognitive bias expansion,\" wherein, unlike humans who compress and filter information, they amplify and propagate errors, further exacerbating knowledge drift and collective reasoning inaccuracies (Liu et al., 2024c). Existing approaches, such as prompt engineering (Fernando et al., 2024), the use of LLM agents as judge to arbitrate and refine reasoning (Zheng et al., 2023; Chan et al., 2024), and \u201chuman-in-the-loop\" intervention (Triem & Ding, 2024), attempt to address these issues. However, prompt engineering often lacks scalability and struggles with context-specific biases, while human intervention is labour-intensive and impractical for large-scale systems. Moreover, judge agents, being LLM-based themselves, are susceptible to similar biases and can unintentionally reinforce reasoning errors, leaving knowledge drift a persistent challenge (Wang et al., 2024c). In contrast, methods integrating uncertainty have shown improved performance; however, their reliance on open-source LLMs, sensitivity to decision-making strategies, and lack of theoretical assurances limit their applicability to proprietary models and complex multi-agent real-world scenarios (Yoffe et al., 2024; Yang et al., 2024a; Zeng et al., 2024). These limitations underscore the need for a paradigm shift in LLM-empowered MAS design, demanding a framework that leverages quantifiable uncertainty to mitigate knowledge drift and misinformation propagation while providing robust theoretical guarantees for the whole system.\nOur Perspective: Addressing aforementioned issues in LLM-MAS requires a transition from current heuristic solutions to principled system architectures with provable guarantees, particularly to ensure reliable knowledge agreement (Bensalem et al., 2023). Different from existing approaches based on heuristic mechanisms, we advocate a probabilistic-centric system architecture that fundamentally integrates uncertainty quantification and propagation mechanisms into its core operational principles to ensure consistent knowledge alignment across whole agent network instead of focusing on individual agents. Specifically, we propose that future LLM-MAS should: (1) implement rigorous probabilistic frameworks for quantifying and propagating uncertainty in inter-agent communications to maintain agreement consistency, (2) establish formal verification mechanisms that provide certified bounds (either statistical or deterministic bounds) on the probabilities of knowledge corruption and drift (Zhang et al., 2024c), and (3) develop scalable certification procedures with automated assurance cases for efficient agreement verification (Wang et al., 2023a). For instance, conformal prediction-style guarantees have been used to ensure collective decisions align with a specified confidence level while quantifying individual agent uncertainties (Wang et al., 2024b; Vishwakarma et al., 2024)."}, {"title": "2.2. Conflicting Agreement", "content": "Conflicts in LLM-MAS normally arise from objective misalignment and knowledge asymmetry (Phelps & Ranson, 2023). At the objective level, conflicts stem from differing task criteria or requirement interpretations. For example, in collaborative task planning, agents may adopt competing interpretations of the same high-level goal (typically performance vs. safety), resulting in divergent execution strategies, particularly in scenarios requiring complex trade-offs (Tessier et al., 2005). Knowledge-based conflicts emerge from different reasoning paths and knowledge sources, where agents may construct different mental models or reach contradictory conclusions despite identical initial information (Wang et al., 2024a). This is evident in RAG-enhanced systems where variations in chain-of-thought reasoning and retrieved knowledge lead to inconsistent understanding across temporal and domain-specific contexts (Ju et al., 2024). The probabilistic nature of LLMs, coupled with inherent semantic ambiguities in natural language, amplifies the effect of knowledge misalignment. For instance, in an autonomous driving scenario, when one agent issues an alert such as \"slow down due to road conditions,\" different agents might interpret this message differently, leading to varying implementations of the slowdown (Yang et al., 2024b). While LLMs as agents offer significant advantages, how do we address the unique conflicts they introduce, posing a new dilemma? That is, we must determine whether integrating LLMs into MAS can prevent conflicts from inherent knowledge ambiguities in LLM and produce outcomes aligned with our expectations.\nOur Perspective: Current approaches rely mainly on ad-hoc solutions (Bhatia et al., 2020; Liu et al., 2024b; Din et al., 2024), which lack robust mechanisms to quantify and validate uncertainties in decision-making within LLM-MAS, potentially masking conflicts when agents operate with imperfectly alignment levels, easy to allow over-confident yet unreliable decisions (Rodriguez et al., 2023). In contrast, we advocate for a principled, theory-driven framework that extends the classical Belief-Desire-Intention (BDI) architecture with guaranteed hierarchical mechanisms for conflict resolution (Fischer et al., 1995). Specifically, the belief layer uses formal verification to standardize interpretation of ambiguous instructions. The knowledge layer, extending desire, utilizes probabilistic belief updating (e.g. Conformal Bayesian Inference (Fong & Holmes, 2021)) to weight conflicting information based on source reliability and contextual relevance. The objective layer as intention layer, leverages uncertainty-aware multi-criteria decision theory to explicitly modelling objective priorities and constraints for adaptive trade-offs in complex scenarios. This hierarchical design can be augmented by causal reasoning frameworks for preemptive conflict identification (Zeng et al., 2022). We view conflicts not as anomalies to be eliminated, but as inherent system features requiring dedicated management mechanisms with theoretical foundations."}, {"title": "2.3. Inherent Behaviours & Potential Threats", "content": null}, {"title": "2.3.1. HALLUCINATION", "content": "Hallucination, defined as the generation of fluent yet factually incorrect information, poses more severe systemic risks in multi-agent settings (Ji et al., 2023). The inherent uncertainty in LLM outputs, driven by their tendency toward overconfident responses, is especially problematic in multi-agent coordination (Huang et al., 2023b). In such scenarios, hallucinated information from one agent can be treated as valid input by others, creating a propagation cycle as mentioned in section 2.1 where false content is not only transmitted but also reinforced through subsequent agent interactions. This vulnerability becomes especially concerning when adversaries can exploit it for persuasive manipulation or collusive behaviours, transforming an individual agent's uncertainty into a system-wide vulnerability."}, {"title": "2.3.2. COLLUSION", "content": "Collusion is another potential risk, arising both from inter-agent communication and emergent behaviour within individual agents' internal mechanisms (Huang et al., 2024). For instance, research has demonstrated that LLM agents in Cournot competition can engage in implicit collusion, such as covert market division without explicit coordination, thereby evading detection (Wu et al., 2024b; Lin et al., 2024a). Furthermore, semantic cues or steganographic techniques further support collusive behaviours, making them hard to identify and easily exploitable by adversaries (Motwani et al., 2024). LLM's opaqueness further exacerbates the issue, as their outputs are often contextually plausible, effectively obscuring the underlying collusive dynamics."}, {"title": "2.3.3. DATA POISONING & JAILBREAKING ATTACK", "content": "Data poisoning and jailbreaking attacks introduce significant vulnerabilities in LLM-MAS by exploiting communication channels, contaminated knowledge retrieval, and manipulated context windows (Das et al., 2025). Unlike conventional MAS, where poisoning typically targets the training phase, LLM-MAS faces expanded attack vectors due to its reliance on dynamic interactions and external knowledge (Das et al., 2025). For instance, RAG introduces additional risks as it may unguardedly allow poisoned external knowledge bases to infiltrate the originally intact system (Chen et al., 2024d). Furthermore, natural language communication between agents further amplifies the attack surface, allowing adversaries to exploit LLMs' context sensitivity through subtle linguistic manipulations and safety-bypassing prompts. Jailbreaking, normally aimed at bypassing safety constraints in individual LLMs, becomes more dangerous in LLM-MAS (Liu et al., 2024a; Peng et al., 2024). The property of misinformation propagation leads to both poisoned and jailbroken information being enhanced through collaborative reasoning, creating cascading security breaches across the system. These adversarial settings highlight the necessity for utilizing a dedicated run-time mechanisms that can continuously detect and filter potentially compromised data throughout the system's operation, ensuring information consistency and agreement information across agents during task execution."}, {"title": "2.3.4. CYBER THREATS", "content": "Cyber threats also become a significant challenge to LLM-MAS due to their distributed architecture and complex interaction patterns (Zeeshan et al., 2025). Network-level attacks, such as wormhole (Ren et al., 2024) and denial-of-service (Wen et al., 2023), can disrupt temporal consistency and degrade operational performance. The frequent API interactions required for LLM services and inter-agent communication not only expose vulnerabilities in network protocols and authentication mechanisms, but also create performance bottlenecks (Wang et al., 2024d). Furthermore, the integration of external knowledge sources introduces more attack targets (Gummadi et al., 2024), highlighting the need for robust cybersecurity measures that balance protection with system responsiveness, while quantifying the timeliness and completeness of information exchange.\nOur perspective: Current mitigation strategies for these risks, while proven effective for individual LLMs, face limitations when extended to LLM-MAS. Traditional hallucination mitigation techniques like retrieval augmentation (Shuster et al., 2021) and static guardrail (Dong et al., 2024) is insufficient when hallucinated content can be reinforced and propagated through inter-agent interactions, as false information can gain credibility through repeated validation (Xu et al., 2024c). For collusive behaviours, existing detection mechanisms rely heavily on post-hoc analysis of interaction logs, which fails to meet the real-time intervention requirements of dynamic LLM-MAS applications (Bonjour et al., 2022; Motwani et al., 2024). Similarly, data poisoning and jailbreaking defences primarily focus on robust training and input sanitization at model initialization, becoming inadequate in multi-agent scenarios where compromised information can be injected and propagate through various interaction channels during runtime (Wang et al., 2022). Traditional cybersecurity measures, such as rule-based firewalls, struggle to address both the uncertainties from dynamic reasoning and the increased communication channels in LLM-MAS (Applebaum et al., 2016). Moreover, network-level detection mechanisms have proven less effective against LLM-generated misinformation, as these contents often have more deceptive impact despite semantic equivalence to human-designed attacks (Chen & Shu, 2024). These approaches, originally designed for static protection, cannot effectively handle the dynamic protection of knowledge exchange and accumulation in interactive MAS.\nWe suggest a runtime monitoring and AI provenance framework, enhanced by uncertainty-based governance rules (Souza et al., 2022; Werder et al., 2022; Xu et al., 2022). This approach emphasizes continuous surveillance of system behaviours, tracking information flow and decision origins. It should integrate provenance chains and uncertainty quantification, then the system can trace and validate information propagation with probabilistic guarantees (Shorinwa et al., 2024). Besides, the framework should enable adaptive monitoring that dynamically adjusts scrutiny based on risk, trust, and reputation, maintaining reliable records of information and decisions (Hu et al., 2024). Also, runtime machine unlearning can remediate contaminated representations (Pawelczyk et al., 2024), while neural-symbolic methods combine explicit symbolic reasoning (e.g. abductive inference) with neural flexibility for safety enhancement (Tsamoura et al., 2021). By embedding these capabilities within the core architecture, such LLM-MAS should achieve both security and transparency in their operations, providing evidence of system behaviours and their origins during runtime while ensuring robust operation under uncertainty."}, {"title": "2.4. Evaluation in LLM-MAS", "content": "Evaluating agreement in LLM-MAS shows more difficulties in comparison to a single LLM assessment. The temporal dynamics of LLM agent interactions introduce fundamental evaluation complexities. Capturing the temporal evolution of multi-dimension agreement states, especially under feedback loops and historical dependencies that drive cumulative effects for continuous agreement, remains an open challenge in agent collaboration networks (Shen et al., 2023a). For instance, an LLM agent's learning from past interactions may asymmetrically alter its belief alignment and become apparent over extended operational periods (Schubert et al., 2024). Additionally, the probabilistic nature of LLM reasoning means that different sequences of agent interactions can lead to divergent outcomes - for example, in a collaborative planning scenario, having Agent A propose a solution before Agent B might result in a different final strategy compared to when B initiates the planning process, even with identical initial conditions and objectives (Yoffe et al., 2024).\nMoreover, system-level quantification of agreement faces challenges mostly due to the lack of unified frameworks for aggregating individual agent metrics (Guo et al., 2024a). While individual agents might achieve high scores in standard trustworthy dimensions such as toxicity filtering and bias detection, these metrics become insufficient in multi-agent scenarios where agents can reinforce biases through their interactions. Even performance metrics like response efficiency and task completion rates fail to reflect emergent behaviours in collaborative scenarios, where individually optimal responses might lead to collectively suboptimal outcomes, particularly when LLMs inherently have selfish strategies such as maintaining conversational dominance (Tennant et al., 2024). Notably, (Wang et al., 2024c) demonstrate that interaction dynamics can lead to worse performance compared to single-agent's solutions, indicating that the participation of more individually well-performing agents does not necessarily lead to better outcomes."}, {"title": "3. Agreement in LLM-MAS", "content": "As LLMs become increasingly embedded in agents, LLM-MAS has demonstrated unprecedented capabilities in complex task solving (Bubeck et al., 2023). This integration necessitates a reconceptualization of system-wide safety and efficiency beyond traditional protocol-based approaches. From a internal perspective of LLM-MAS, the primary objective is to achieve global agreement (Xu et al., 2023; Zhao et al., 2024) across heterogeneous agents, ensuring both ethical and operational consistency through mutual understanding among all components. Recent advances have reviewed some methods in establishing agreement between agents and human intentions, as well as inter-agent coordination. However, existing studies (Kirchner et al., 2022; Shen et al., 2023b; Cao et al., 2024; Pan et al., 2023; Fernandes et al., 2023) mainly focus on the local agreement for single-agent rather than facilitating global agreement for LLM-MAS."}, {"title": "3.1. Agent to Human Agreement", "content": "For establishing agreement with humans, agents must accurately interpret natural language, grasp assigned tasks or goals, and understand societal constraints. Recent advancements broadly classify these agreement-building methods into three categories: reinforcement learning, supervised fine-tuning, and self-improvement."}, {"title": "Reinforcement Learning", "content": "A most commonly used method to achieve human value agreement is reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022a; Stiennon et al., 2020; Ziegler et al., 2019) and includes two steps: train reward models according to collected human feedback data, finetune language models through reinforcement learning (such as a prevalent method Proximal Policy Optimisation (PPO) using policy update (Schulman et al., 2017)) to achieve agreement. Therefore, in (Bai et al., 2022b; Lee et al., 2023), human feedback is replaced and compared by off-the-shelf LLMs to save human work on high-quality preference labels. Then RLHF are further enhanced in (Glaese et al., 2022; Bai et al., 2022a; Tan et al., 2023; Kirk et al., 2023; Zhu et al., 2023)."}, {"title": "Supervised Fine-tuning", "content": "Another way to promote human-agent agreement is Supervised Fine-tuning (SFT) (Dong et al., 2023; Taori et al., 2023a), which compares the loss between LLMs' outputs and labelled datasets to update the model. These manual-annotated preference data mainly encompass human-written instruction-response pairs (Taori et al., 2023b; Ding et al., 2023) and query-form preferences (Guo et al., 2024b). For example, Instruction-finetuning (IFT), a form of instruction-driven SFT, is primarily used for static tasks. In contrast, preference labelling is usually adopted to capture users' personalised subtle preferences, and is mostly used in dynamic interactions. Examples of SFT include Stanford Alpaca (Taori et al., 2023a) and Alpagaus (Chen et al., 2024b), demonstrating how their IFT fine-tuning leads to better instruction-following abilities. InstructGPT (Ouyang et al., 2022b) combines IFT with preference learning. Furthermore, frameworks like LIMA (Zhou et al., 2023) and PRELUDE (Gao et al., 2024b) introduce new angles to agreement fine-tuning by aligning user preferences through high-quality prompt-response pairs, learning users' latent preferences from dialogues and edit losses, rather than directly fine-tuning the pre-trained model. Also, (Yuan et al., 2024a) introduces the Preference Tree, based on the ULTRAINTERACT dataset, enabling offline fine-tuning of LLMs via SFT by learning preferred reasoning paths."}, {"title": "Self-improvement", "content": "Inductive biases are used to refine agreement iteratively by self-improvement, as the framework illustration shown in Figure 3. Self-consistency (Wang et al., 2023b) uses Chain-of-Thought (COT) (Wei et al., 2022) and Tree-of-thought (TOT) (Yao et al., 2024) to generate multiple reasoning paths and marginalise the response with the highest consistency when decoding to improve output quality. Based on this, Self-improve (Huang et al., 2023a) chooses high-confidence inference paths as training samples to fine-tune more consistent models. SAIL (Ding et al., 2024) utilize bi-level optimization, combining SFT and online RLHF to reduce the reliance on human annotated preferences. Self-rewarding (Yuan et al., 2024b) shows LLMs can improve preferences by judging their own answers. Based on this, Meta-Judge (Wu et al., 2024a) add a meta-judging stage to optimist its judgement skills unsupervisedly."}, {"title": "3.2. Agent to Agent Agreement", "content": "In a multi-agent system, agreement manifests as an agent's capability to accurately process other agents' intent, information, and output for informed collective decision-making (Zhou et al., 2024). This section examines existing agreement mechanisms across heterogeneous agents."}, {"title": "Cross-Model Agreement", "content": "There are two directions: One is Strong-to-weak. An aligned stronger teacher model generates training data for a weak model to learn behaviours, including response pairs (Xu et al., 2024a; Taori et al., 2023b; Peng et al., 2023) and preferences (Cui et al., 2024). For example, Zephyr (Tunstall et al., 2023) fine-tunes smaller LLMs through distilled SFT (dSFT). Before the last step DPO, the teacher LLM judge the smaller models' output as labellers instead of humans. Another is Weak-to-strong. SAMI (Fr\u00e4nken et al., 2024) writes constitutions using weak institution-fintuned models to avoid over-reliance on strong models. In (Burns et al., 2024), weak teacher models are trained on ground truth by fine-tuning pre-trained models, which generate labels for strong student models. Considering the correlation of agents' behaviours in collaboration, mutual information (MI) is also used to optimise cross-model agreement. A multi-agent reinforcement learning (MARL) method, Progressive Mutual Information Collaboration (PMIC) (Li et al., 2023b), set the criterion that the MI of superior behaviours should be maximised and the MI of inferior ones should be minimised."}, {"title": "Debate and Adversarial Self-Play", "content": "Debate normally exploits adversarial dynamics to refine agreement in a MAS, especially for an interdisciplinary MAS. There are two types: Generator-Discriminator and Debate. In the Generator-Discriminator framework, the generator generates the response, and the discriminator judges the quality. CONSENSUS GAME (Jacob et al., 2023) enhances agreement between a Generator and a Discriminator by iteratively refining their policies to minimize regret and reach a regularized Nash equilibrium. With the Debate Framework, a debate process is simulated to improve the models' reasoning and agreement from strong opponents' perspectives. During the (Irving et al., 2018), Supervised pre-trained models play as debaters to generate arguments withstanding scrutiny, and RLHF is used to achieve a Nash equilibrium, enhancing agents' agreement with human expectations."}, {"title": "Environment Feedback", "content": "To achieve interdisciplinary agreement, a large amount of multimodal background knowledge is needed to build a World Model (LeCun, 2022) for independent tasks and different roles, constituting a basis for common sense. The agents' states and actions are the input, and the World Model provides multiple possible state predictions, such as state transition probabilities and relative rewards (Hu & Shu, 2023). The agents will find the strategy with the lowest estimated cost in the World over the long run under the common sense. Environment-driven tasks can also incorporate external tools and social simulations instead of purely manual annotation to expand agreement beyond language-based interactions to multimodal and task-specific applications. Study like MoralDial (Sun et al., 2023) simulates social discussions between agents and the environment, improving the model's performance in moral answering, explanation, and revision."}, {"title": "3.3. Agreement Evaluation", "content": "To effectively achieve and evaluate global agreement in a multi-agent system, dedicated evaluation methods to measure whether the extent of the agreement is acceptable for a MAS are essential. The MAgIC (Xu et al., 2024b) brings forward metrics to evaluate capabilities within a MAS, where the Cooperation and the Coordination calculate the proportion of successful cases that achieve common goals compared with benchmarks. (Li et al., 2023a) uses differences in opinions between individuals or groups to describe consistency, and uses the time for the difference in opinions between individuals decreasing to a threshold and standard deviation of group opinions to represent convergence. (Fung et al., 2024; de Cerqueira et al., 2024) introduce Trust Scores to evaluate how much an agent trusts others, which affects consensus in discussions. Each agent maintains a binary trust score for its Neighbours and updates the score based on others' behaviours in interactions. Consensus is also measured by the degree of agreement of agents' final states after multiple rounds of negotiation. (Chen et al., 2025) believe the ultimate output represents a systematic consensus, so the consensus can be quantified by measuring the deviation by variance. Semantic Similarity (Xu et al., 2024e; Aynetdinov & Akbik, 2024) is also used to assess the level of agreement among agents during their optimization process."}, {"title": "4. Uncertainty in LLM-MAS", "content": "With the shift from single-agent planning to multi-agent collaboration, uncertainty management becomes a crucial external challenge for ensuring a responsible LLM-MAS. This requires effective traceability, probabilistic guarantees, and strategic utilization of uncertainty across all system components. This section explores how uncertainty quantification techniques enhance AI agents and evaluation metrics, facilitating the transition to multi-agent setups and fostering more robust, reliable MAS for responsible decision-making."}, {"title": "4.1. Uncertainty in AI Agents System", "content": "Despite the widespread deployment of LLM across various domains, the explicit consideration of uncertainty in LLM-empowered agents remains relatively unexplored. When we analyse an AI-agent system by breaking it down into individual components, it transforms into a multi-component system. Therefore, we firstly focus on the core components that influence the AI agent's uncertainty, e.g. memory management, and strategic planning."}, {"title": "Memory Retrieval-Augmented Generation (RAG)", "content": "enhances LLMs by integrating external, up-to-date, domain-specific knowledge, improving factual accuracy and reducing hallucinations without extensive retraining. However, not all retrieved sources equally influence decision-making. To address this, an attention-based uncertainty quantification (Duan et al., 2024) analyzes variance in attention weights across retrieved sources to estimate uncertainty. Similarly, LUQ (Zhang et al., 2024a) uses an ensemble-based approach to re-rank documents and adjust verbosity based on confidence. Xu et al. (Xu et al., 2024d) introduce a self-consistency mechanism, comparing retrieved evidence with generated outputs to refine both retrieval and generation, ultimately improving the model's knowledge representation and reducing hallucinations."}, {"title": "Planning", "content": "Planning is another essential component for LLM-based agents as it enables structured decision-making by decomposing complex tasks into manageable steps. However, planning remains the most uncertain aspect in a stochastic environment. To address uncertainty in stochastic environments; studies focus on improving efficiency and reliability. Tsai et al. (Tsai et al., 2024) fine-tunes Mistral-7B to predict prompt-action compatibility, using conformal prediction to identify the most probable actions. To assess the need for human evaluation, Ren et al. (Ren et al., 2023) introduce KnowNo, a method that evaluates token probabilities for next actions. Building on this, IntroPlan (Liang et al., 2024a) incorporates introspective planning, refining prediction sets with tighter confidence bounds, reducing human intervention and enhancing autonomy."}, {"title": "4.2. Uncertainty in Agents Interaction", "content": "While uncertainty quantification in LLM-MAS has been explored, existing methods typically assess uncertainty at individual instances, overlooking prior interaction history. Real-world applications, like autonomous medical assistants (Li et al., 2024; Savage et al., 2024), often involve multi-instance interactions, where responses depend on accumulated information from previous exchanges (Chen et al., 2024a; Pan et al., 2024). In multi-agent settings, methods like DiverseAgentEntropy (Feng et al., 2024) assess uncertainty by evaluating factual parametric knowledge in a black-box setting, providing a more accurate prediction and helping detect hallucinations. It further reveals that existing models often fail to consistently retrieve correct answers across diverse question formulations, even when the correct answer is known. Moreover, failure to express uncertainty explicitly can misguidance other agents (Liang et al., 2024b; Burton et al., 2024). DebUnc (Yoffe et al., 2024) tackles this issue by incorporating confidence metrics throughout the entire interaction, improving the clarity and reliability of agent communication. It adapts the LLM attention mechanism to adjust token weights based on confidence levels and uses textual prompts to convey confidence more effectively."}, {"title": "4.3. Uncertainty Evaluation", "content": "From an agent-monitoring perspective, the performance of LLM-MAS can be assessed using statistical metrics, through human-in-the-loop verification, or a combination of both. Ideally, to minimize human intervention and enhance the efficiency of responsible agent systems, only outputs identified as uncertain should be deferred to an auxiliary system or human experts for further evaluation."}, {"title": "Statistical Analysis", "content": "Uncertainty estimation in LLMs can be broadly categorized into single-inference and multi-inference approaches. Single-inference estimation use token log probabilities with logit values partially capture inherent uncertainty (Yang et al., 2023), while conformal prediction (Ren et al., 2023) further quantifies confidence for predefined success probabilities (Ren et al., 2023). In contrast, multi-inference estimation evaluates uncertainty across multiple outputs, bypassing token-level details. Intuitively, if a model has effectively learned a concept, its generated samples should exhibit semantic equivalence. Methods like Semantic entropy (Farquhar et al., 2024) detects confabulations (arbitrary and incorrect generations) by measuring uncertainty at the semantic level, and spectral clustering (Lin et al., 2024b) quantifies uncertainty by analyzing semantic dispersion in multiple responses, providing a robust estimate without accessing internal parameters."}, {"title": "Human-in-the-loop Setting", "content": "an uncertainty threshold helps identify potential errors and delegate high-risk cases to external systems or human experts, with outcomes exceeding the threshold flagged for reassessment. For example, KnowLoop framework (Zheng et al., 2024) uses entropy-based measures for failure detection and human intervention in LLM-based task planning. Similarly, UALA (Han et al., 2024) integrates uncertainty quantification into its workflow, using metrics like maximum or mean uncertainty to identify knowledge gaps, prompting the agent to seek clarification. These mechanisms enhance the robustness and adaptability of LLM-based systems, reducing risks from erroneous outputs and improving reliability across diverse applications. Despite recent progress in uncertainty quantification, LLM-MAS still lacks rigorous uncertainty measures that both incorporate traceable agent interaction histories and establish verifiable statistical bounds, which is a critical requirement for developing responsible LLM-MAS frameworks."}, {"title": "5. Responsible LLM-MAS Framework", "content": "Building a responsible LLM-MAS inherently requires interdisciplinary perspectives", "to": "consensus among agent decisions, policy alignment, goal consistency, etc. Additionally, system-wide considerations such as communication protocol compliance, privacy information propagation, and temporal synchronization constraints must be carefully evaluated across the multiagent network (He et al., 2025). Meanwhile, uncertainty quantification operates at both system and agent levels, addressing various aspects such as knowledge confidence assessment, decision reliability estimation, and environmental state prediction, among others. These metrics, with probabilistic bounds, ensure operational risks stay within acceptable margins (Nikolaidis et al., 2004; Hsu et al., 2023). These quantifiable guarantee metrics not only enable objective evaluation of system trustworthiness and performance but also serve as the foundation for building robust monitoring mechanisms.\nA moderator, integrating symbolic rules with formal verification, can manage the system rigorously"}]}