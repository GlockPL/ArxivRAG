{"title": "THINK WHILE YOU GENERATE: DISCRETE DIFFUSION\nWITH PLANNED DENOISING", "authors": ["Sulin Liu", "Juno Nam", "Andrew Campbell", "Hannes St\u00e4rk", "Yilun Xu", "Tommi Jaakkola", "Rafael G\u00f3mez-Bombarelli"], "abstract": "Discrete diffusion has achieved state-of-the-art performance, outperforming or\napproaching autoregressive models on standard benchmarks. In this work, we\nintroduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework\nthat separates the generation process into two models: a planner and a denoiser. At\ninference time, the planner selects which positions to denoise next by identifying\nthe most corrupted positions in need of denoising, including both initially cor-\nrupted and those requiring additional refinement. This plan-and-denoise approach\nenables more efficient reconstruction during generation by iteratively identifying\nand denoising corruptions in the optimal order. DDPD outperforms traditional\ndenoiser-only mask diffusion methods, achieving superior results on language\nmodeling benchmarks such as text8, OpenWebText, and token-based generation\non ImageNet 256 \u00d7 256. Notably, in language modeling, DDPD significantly\nreduces the performance gap between diffusion-based and autoregressive methods\nin terms of generative perplexity. Code is available at github.com/liusulin/DDPD.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative modeling of discrete data has recently seen significant advances across various appli-\ncations, including text generation [5], biological sequence modeling [25, 4], and image synthesis\n[8, 9]. Autoregressive transformer models have excelled in language modeling but are limited to\nsequential sampling, with performance degrading without annealing techniques such as nucleus\n(top-p) sampling [20]. In contrast, diffusion models offer more flexible and controllable generation,\nproving to be more effective for tasks that lack natural sequential orderings, such as biological\nsequence modeling [25, 1] and image token generation [8]. In language modeling, the performance\ngap between discrete diffusion and autoregressive models has further narrowed recently, thanks to\nimproved training strategies [29, 36, 34, 13], however, a gap still remains on some tasks [36, 29].\nState-of-the-art discrete diffusion methods train a denoiser (or score) model that determines the\ntransition rate (or velocity) from the current state to predicted values. During inference, the generative\nprocess is discretized into a finite number of steps. At each step, the state values are updated based on\nthe transition probability, which is obtained by integrating the transition rate over the timestep period.\nIn order to further close the performance gap with autoregressive models, we advocate for a rethinking\nof the standard discrete diffusion design methodology. We propose a new framework Discrete\nDiffusion with Planned Denoising (DDPD), that divides the generative process into two key\ncomponents: a planner and a denoiser, facilitating a more adaptive and efficient sampling procedure.\nThe process starts with a sequence of tokens initialized with random values. At each timestep, the\nplanner model examines the sequence to identify the position most likely to be corrupted and in need\nof denoising. The denoiser then predicts the value for the selected position, based on the current noisy\nsequence. The key insight behind our plan-and-denoise approach is that the generative probability at\neach position can be factorized into two components: 1) the probability that a position is corrupted,\nand 2) the probability of denoising it according to the data distribution.\nThe advantages of our approach are two-fold, providing both a simplified learning process and a more\neffective sampling algorithm. First, by decomposing the originally complex task into two distinct"}, {"title": "2 PRELIMINARIES", "content": "We begin by introducing the problem setup and notations. Following [7, 13], we then explain the\nContinuous Time Markov Chain (CTMC) framework [31], which is used to define the forward and\nreverse processes of discrete diffusion, and how the discrete timestep version is derived from it.\nSetup and Notations. We aim to model discrete data where a sequence x \u2208 {1,\u2026, S}D is\nD-dimensional and each element \u00e6d takes S possible states. x\\d denotes all dimensions except d.\nFor clarity in presentation, we assume D = 1 and all results hold for D > 1 (see Section 3.1). We"}, {"title": "Continuous Time Markov Chains and Discretization.", "content": "We adopt the CTMC framework to define\nthe discrete diffusion process. A realization of the CTMC dynamics is defined by a trajectory xt over\ntime t \u2208 [0, 1] that makes jumps to another state after a random waiting period known as the holding\ntime. The transition rates and the next states are governed by the rate matrix Rt \u2208 RS\u00d7S (analogous\nto the velocity field vt in continuous state spaces), where the off-diagonal elements, representing\njumps to different states, are non-negative. For an infinitesimal timestep dt, the probability of\ntransitioning from xt to a different state j is given by Rt(xt, j)dt.\nIn practice, the trajectory is simulated using finite time intervals \u2206t. As a result, the transition\nprobability follows a categorical distribution with the PMF:\nPt+\u2206t\\t(j|xt) = \u03b4{xt, j} + Rt(xt, j)\u2206t,\nwhere we denote this as Cat(\u03b4{xt, j} + Rt(xt, j)\u2206t), and Rt(xt,xt) := - \u03a3s\u2260xt Rt(xt, s) to\nensure that the transition probabilities sum to 1."}, {"title": "Forward Corruption Process.", "content": "Following [7, 34, 13], which were inspired by flow matching in\ncontinuous state space [26, 27, 2], we construct the forward process by interpolating from noise\nPo(xo) = Pnoise(x0) to clean data p\u2081(x1) = Pdata(x1). Common choices for the noise distribution\ninclude: (i) punise (xt) = 1/s, a uniform distribution over ver {1,\u2026, S'}; and (ii) pmask (xt) = \u03b4[\u039c, xt},\na delta PMF concentrated on an artificially introduced mask state M. Let at be the noise schedule\nthat introduces noise to the data over time t. For example, a linear schedule is given by at = t. The\nconditional marginal Pt|1(xt|x1) is given in closed form:\nPunif (xt|x1) = Cat(at\u03b4{x1,xt} + (1 \u2212 at)/S),\npmask (xt|x1) = Cat(at\u03b4{x1, xt} + (1 \u2212 at)\u03b4{M, xt}).\nAt t = 1, the conditional marginal converges to the datapoint 21, i.e. \u03b4{X1,Xt}. At t = 0, the\nconditional marginal converges to the noise distribution Pnoise (xt)."}, {"title": "Reverse Generation Process.", "content": "Sampling from Pdata is achieved by learning a generative rate\nmatrix Rt(xt, j) to reverse simulate the process from t = 0 to t = 1 using Eq. (1), such that we\nbegin with samples of Pnoise and end with samples of Pdata. The datapoint conditional reverse rate\nRt(xt, j|x1) for j \u2260 xt\u00b9 under the uniform or mask noise distributions is given by [7, 13]:\nRunif (xt, j|x1) = at \u03b4{x1, j} (1\u2212\u03b4{x1,xt}), Rmask (xt, j|x1) = at \u03b4{x1, j} \u03b4{xt, M}.\n1-at\n1-at\n[7, 13] show that the rate we aim to learn for generating Pdata is the expectation of the data-conditional\nrate, taken over the denoising distribution, i.e., Rt(xt, j) := Ep1|t(x1|xt) [Rt(xt, j|x1)]:\nRunif (xt, j) = at P1|t (X1=j|xt), Rmask (xt,j) = at \u03b4{xt, M}P1|t (x1 = j|xt).\n1-at\n1-at\nThe goal is to approximate this rate using a neural network. During inference, the generative process\nis simulated with the learned rate by taking finite timesteps, as described in Eq. (1)."}, {"title": "3 \u041c\u0415\u0422\u041dOD", "content": ""}, {"title": "3.1 DECOMPOSING GENERATION INTO PLANNING AND DENOISING", "content": "Recent state-of-the-art discrete diffusion methods [39, 29, 7, 36, 34, 13] have converged on parameter-\nizing the generative rate using a denoising neural network and deriving cross-entropy-based training\nobjectives. This enables simplified and effective training, leading to better and SOTA performance in\ndiscrete generative modeling compared to earlier approaches [3, 6]. In Table 1, we summarize the\ncommonalities and differences in the design choices across various methods."}, {"title": "Mask Diffusion.", "content": "For mask diffusion, the planning part is assigning probability of\nat \u03b4{xt, M}\u2206t for the data to be denoised with an actual value. \u03b4{xt, M} tells if the data\n1-at\nis noisy (M) or clean. 1 is the rate of denoising, which is determined by the remaining time\naccording to the noise schedule. The denoiser P1|t assigns probabilities to the possible values to be\nfilled in if this transition happens.\nRmask (xt,j)\u2206t = at \u03b4{xt, M}\u2206t p1|t (X1 = j|xt)\n1-at\nrate of making correction\nprob. of denoising"}, {"title": "Uniform Diffusion.", "content": "Similarly, we would want to decompose the transition probability into two\nparts: the planning probability based on if the data is corrupted and the denoising probability that\ndetermines which value to change to. But in contrast to the mask diffusion case, the noise/clean state\nof the data is not given to us during generation. We use za \u2208 {N, D} as a latent variable to denote if\na dimension is corrupted, with N denoting noise and D denoting data.\nFrom Bayes rule, for j \u2260 xt, since P1|t (X1 = j|Xt, zt = D) = p(xt|x1=j,zt=D)p(x1=j) = 0\np(xt|zt=D)\n\u03a3 P(Zt|xt)P1|t(X1 = j|xt, zt) = P(zt = N|xt) P1|t(x1 = j|xt, zt = N)\nP1|t (x1 = j|xt) =\nZt\u2208{N,D}\nprob. of being corrupted\nprob. of denoising\nThe first part of the decomposition is the posterior probability of xt being corrupted, and the second\npart gives the denoising probability to recover the value of xt if xt is corrupted. Plugging Eq. (6) into\nEq. (4), we arrive at:\nRunif (xt, j)\u2206t = at p(zt = N|xt) \u2206t P1|t(x1 = j|xt, Zt = N)\n1-at\nrate of making correction\nprob. of denoising\nBy comparing Eq. (7) and Eq. (5), we find that they share the same constant part at which\n1-at\nrepresents the rate determined by the current time left according to the noise schedule. The main\ndifference is in the middle part that represents the probability of xt being corrupted. In mask diffusion\ncase, this can be readily read out from the M token. But in the uniform diffusion case, we need to\ncompute/approximate this probability instead. The last part is the denoising probability conditioned\non xt being corrupted, which again is shared by both and needs to be computed/approximated."}, {"title": "Generative Rate for Multi-Dimensions.", "content": "The above mentioned decomposition extends to D > 1.\nWe have the following reverse generative rate [7, 13] for mask diffusion:\nRmask (xt, jd) \u2206t = at \u2206t \u03b4{x, M}P1|t (xq = jd|xt), Vjd \u2260 xq,\n1-at\nand we derive the following decomposition result for uniform diffusion (proof in Appendix A.1):\nProposition 3.1. The reverse generative rate at d-th dimension can be decomposed into the product\nof recovery rate, probability of corruption and probability of denoising:\nRunif (xt, jd) \u2206t = 1 P1|t (xq = jd|xt) \u2206t\n1-at\n1 P(z = N|xt) P1|t (xq = jd|xt, zq = N) \u2206t, vjd \u2260 xq\nnoise removal rate prob. of corruption\nprob. of denoising\nwhere p (z\u0142 = N|xt) = 1 \u2212 P1|t (xq = xq|xt) a++(1-a)/S\nP1|t (xq = jd|xt, zq = N) = Put (xx)\nP1\t (xq = jdxt)\np(z=Nxt)"}, {"title": "Previous Parameterization.", "content": "In the case of mask diffusion, as studied in recent works [29, 7, 36,\n34, 13], the most effective parameterization for learning is to directly model the denoising probability\nwith a neural network, as this is the only component that needs to be approximated.\nIn the case of uniform diffusion, the conventional approach uses a single model to approximate the\ngenerative rate as a whole, by modeling the posterior p1|t (xq = jd|xt) as shown in Eq. (9). However,\ndespite its theoretically greater flexibility \u2013 allowing token values to be corrected throughout sampling,\nakin to the original diffusion process in the continuous domain \u2013 its performance has not always\noutperformed mask diffusion, particularly in tasks like image or language modeling."}, {"title": "Plan-and-Denoise Parameterization.", "content": "Based on the observation made in Proposition 3.1, we take\nthe view that generation should consist of two models: a planner model for deciding which position\nto denoise and a denoiser model for making the denoising prediction for a selected position.\nRunpimp (xt, jd) \u2206t = at \u2206t po (zd = N|xt) P\u2081|t (xq = jd|xt, zq = N), \u2200xt \u2260 jd\n1-at\nnoise removal rate\nplanner\ndenoiser\nThis allows us to utilize the planner's output to design an improved sampling algorithm that optimally\nidentifies and corrects errors in the sequence in the most effective denoising order. Additionally, the\ntask decomposition enables separate training of the planner and denoiser, simplifying the learning\nprocess for each neural network. Often, a pretrained denoiser is already available, allowing for\ncomputational savings by only training the planner, which is generally faster and easier to train.\nRemark 3.2. Under this perspective, masked diffusion (Eq. (8)) can be interpreted as a denoiser-only\nmodeling paradigm with a fixed planner, i.e., po (z\u0105\u0142 = N|xt) = \u03b4{xq, M}, which assumes that\nmask tokens represent noise while actual tokens represent clean data. This planner is optimal under\nthe assumption of a perfect denoiser, which rarely holds in practice. When the denoiser makes errors,\nthis approach does not provide a mechanism for correcting those mistakes.\nNext, we demonstrate how our plan-and-denoise framework enables an improved sampling algorithm\nthat effectively leverages the planner's predictions. From this point forward, we assume uniform\ndiffusion by default and use Rt,jump to denote the reverse jump rate, unless explicitly stated otherwise."}, {"title": "3.2 SAMPLING", "content": "The reverse generative process is a CTMC that consists\nof a sequence of jumps from t = 0 to t = 1. The most common way [6, 39, 29, 36, 34, 13] is to\ndiscretize it into equal timesteps and simulate each step following the reverse generative rate using"}, {"title": "Generative Rate for Multi-Dimensions.", "content": "The above mentioned decomposition extends to D > 1.\nWe have the following reverse generative rate [7, 13] for mask diffusion:\nRmask (xt, jd) \u2206t = at \u2206t \u03b4{x, M}P1|t (xq = jd|xt), Vjd \u2260 xq,\n1-at\nand we derive the following decomposition result for uniform diffusion (proof in Appendix A.1):\nProposition 3.1. The reverse generative rate at d-th dimension can be decomposed into the product\nof recovery rate, probability of corruption and probability of denoising:\nRunif (xt, jd) \u2206t = 1 P1|t (xq = jd|xt) \u2206t\n1-at\n1 P(z = N|xt) P1|t (xq = jd|xt, zq = N) \u2206t, vjd \u2260 xq\nnoise removal rate prob. of corruption\nprob. of denoising\nwhere p (z\u0142 = N|xt) = 1 \u2212 P1|t (xq = xq|xt) a++(1-a)/S\nP1|t (xq = jd|xt, zq = N) = Put (xx)\nP1\t (xq = jdxt)\np(z=Nxt)"}, {"title": "Previous Parameterization.", "content": "In the case of mask diffusion, as studied in recent works [29, 7, 36,\n34, 13], the most effective parameterization for learning is to directly model the denoising probability\nwith a neural network, as this is the only component that needs to be approximated.\nIn the case of uniform diffusion, the conventional approach uses a single model to approximate the\ngenerative rate as a whole, by modeling the posterior p1|t (xq = jd|xt) as shown in Eq. (9). However,\ndespite its theoretically greater flexibility \u2013 allowing token values to be corrected throughout sampling,\nakin to the original diffusion process in the continuous domain \u2013 its performance has not always\noutperformed mask diffusion, particularly in tasks like image or language modeling."}, {"title": "Plan-and-Denoise Parameterization.", "content": "Based on the observation made in Proposition 3.1, we take\nthe view that generation should consist of two models: a planner model for deciding which position\nto denoise and a denoiser model for making the denoising prediction for a selected position.\nRunpimp (xt, jd) \u2206t = at \u2206t po (zd = N|xt) P\u2081|t (xq = jd|xt, zq = N), \u2200xt \u2260 jd\n1-at\nnoise removal rate\nplanner\ndenoiser\nThis allows us to utilize the planner's output to design an improved sampling algorithm that optimally\nidentifies and corrects errors in the sequence in the most effective denoising order. Additionally, the\ntask decomposition enables separate training of the planner and denoiser, simplifying the learning\nprocess for each neural network. Often, a pretrained denoiser is already available, allowing for\ncomputational savings by only training the planner, which is generally faster and easier to train.\nRemark 3.2. Under this perspective, masked diffusion (Eq. (8)) can be interpreted as a denoiser-only\nmodeling paradigm with a fixed planner, i.e., po (za\u0142 = N|xt) = \u03b4{xq, M}, which assumes that\nmask tokens represent noise while actual tokens represent clean data. This planner is optimal under\nthe assumption of a perfect denoiser, which rarely holds in practice. When the denoiser makes errors,\nthis approach does not provide a mechanism for correcting those mistakes.\nNext, we demonstrate how our plan-and-denoise framework enables an improved sampling algorithm\nthat effectively leverages the planner's predictions. From this point forward, we assume uniform\ndiffusion by default and use Rt,jump to denote the reverse jump rate, unless explicitly stated otherwise."}, {"title": "3.2 SAMPLING", "content": "The reverse generative process is a CTMC that consists\nof a sequence of jumps from t = 0 to t = 1. The most common way [6, 39, 29, 36, 34, 13] is to\ndiscretize it into equal timesteps and simulate each step following the reverse generative rate using"}, {"title": "Generative Rate for Multi-Dimensions.", "content": "The above mentioned decomposition extends to D > 1.\nWe have the following reverse generative rate [7, 13] for mask diffusion:\nRmask (xt, jd) \u2206t = at \u2206t \u03b4{x, M}P1|t (xq = jd|xt), Vjd \u2260 xq,\n1-at\nand we derive the following decomposition result for uniform diffusion (proof in Appendix A.1):\nProposition 3.1. The reverse generative rate at d-th dimension can be decomposed into the product\nof recovery rate, probability of corruption and probability of denoising:\nRunif (xt, jd) \u2206t = 1 P1|t (xq = jd|xt) \u2206t\n1-at\n1 P(z = N|xt) P1|t (xq = jd|xt, zq = N) \u2206t, vjd \u2260 xq\nnoise removal rate prob. of corruption\nprob. of denoising\nwhere p (z\u0142 = N|xt) = 1 \u2212 P1|t (xq = xq|xt) a++(1-a)/S\nP1|t (xq = jd|xt, zq = N) = Put (xx)\nP1\t (xq = jdxt)\np(z=Nxt)"}, {"title": "Previous Parameterization.", "content": "In the case of mask diffusion, as studied in recent works [29, 7, 36,\n34, 13], the most effective parameterization for learning is to directly model the denoising probability\nwith a neural network, as this is the only component that needs to be approximated.\nIn the case of uniform diffusion, the conventional approach uses a single model to approximate the\ngenerative rate as a whole, by modeling the posterior p1|t (xq = jd|xt) as shown in Eq. (9). However,\ndespite its theoretically greater flexibility \u2013 allowing token values to be corrected throughout sampling,\nakin to the original diffusion process in the continuous domain \u2013 its performance has not always\noutperformed mask diffusion, particularly in tasks like image or language modeling."}, {"title": "Plan-and-Denoise Parameterization.", "content": "Based on the observation made in Proposition 3.1, we take\nthe view that generation should consist of two models: a planner model for deciding which position\nto denoise and a denoiser model for making the denoising prediction for a selected position.\nRunpimp (xt, jd) \u2206t = at \u2206t po (zd = N|xt) P\u2081|t (xq = jd|xt, zq = N), \u2200xt \u2260 jd\n1-at\nnoise removal rate\nplanner\ndenoiser\nThis allows us to utilize the planner's output to design an improved sampling algorithm that optimally\nidentifies and corrects errors in the sequence in the most effective denoising order. Additionally, the\ntask decomposition enables separate training of the planner and denoiser, simplifying the learning\nprocess for each neural network. Often, a pretrained denoiser is already available, allowing for\ncomputational savings by only training the planner, which is generally faster and easier to train.\nRemark 3.2. Under this perspective, masked diffusion (Eq. (8)) can be interpreted as a denoiser-only\nmodeling paradigm with a fixed planner, i.e., po (za\u0142 = N|xt) = \u03b4{xq, M}, which assumes that\nmask tokens represent noise while actual tokens represent clean data. This planner is optimal under\nthe assumption of a perfect denoiser, which rarely holds in practice. When the denoiser makes errors,\nthis approach does not provide a mechanism for correcting those mistakes.\nNext, we demonstrate how our plan-and-denoise framework enables an improved sampling algorithm\nthat effectively leverages the planner's predictions. From this point forward, we assume uniform\ndiffusion by default and use Rt,jump to denote the reverse jump rate, unless explicitly stated otherwise."}, {"title": "3.2 SAMPLING", "content": "The reverse generative process is a CTMC that consists\nof a sequence of jumps from t = 0 to t = 1. The most common way [6, 39, 29, 36, 34, 13] is to\ndiscretize it into equal timesteps and simulate each step following the reverse generative rate using"}, {"title": "4 TRAINING", "content": "Our plan-and-denoise parameterization in Eq. (12) enables us to use two\nseparate neural networks for modeling the planner and the denoiser. Alternatively, both the planner\nand denoiser outputs can be derived from a single uniform diffusion model, p\u2081\u2081t (xq|x+), as described\nin Proposition 3.1. This approach may offer an advantage on simpler tasks, where minimal approx-\nimation errors for neural network training can be achieved, avoiding the sampling approximation"}, {"title": "5 RELATED WORK", "content": "Previous discrete diffusion/flow methods, whether in discrete\ntime [3, 21, 34] or continuous time [6, 39, 29, 7, 36, 13], adopt the denoiser-only or score-modeling\nperspective. In contrast, we introduce a theoretically grounded decomposition of the generative\nprocess into planning and denoising. DFM [7] and the Reparameterized Diffusion Model (RDM)\nintroduce stochasticity into the reverse flow/diffusion process, allowing for adjustable random\njumps between states. This has been shown to improve the generation quality by providing denoiser\nmore opportunities to correct previous errors. Additionally, RDM uses the denoiser's prediction\nconfidence as a heuristic [14, 35, 8] for determining which tokens to denoise first. Lee et al. [24]\nintroduces another heuristic in image generation that aims at correcting previous denoising errors by\nfirst generating all tokens and then randomly regenerating them in batches.\nPredictor-corrector sampling methods are proposed for both continu-\nous and discrete diffusion [37, 6] that employ MCMC steps for correction after each predictor step.\nHowever, for continuous diffusion, this approach has been found to be less effective compared to the\nnoise-injection stochasticity scheme [22, 42]. In the case of discrete diffusion, an excessively large\nnumber of corrector steps is required, which limits the method's overall effectiveness."}, {"title": "6 EXPERIMENT", "content": "Before going into details, we note that DDPD incurs 2 NFE v.s. 1 NFE per step in denoiser-only\napproaches, an extra cost we pay for planning. To ensure a fair comparison, we also evaluate denoiser-\nonly methods that are either 2\u00d7 large or use 2\u00d7 steps. Our findings show that spending compute on\nplanning is more effective than doubling compute on denoising when cost is a factor.\nWe first evaluate DDPD on the small-scale character-level text modeling benchmark, text8\n[30], which consists of 100 million characters extracted from Wikipedia, segmented into chunks\nof 256 letters. Our experimental setup follows that of [7]. Methods for comparison include 1)\nautoregressive model 2) DFM: discrete flow model (and 2\u00d7 param. version) [7], the best available\ndiscrete diffusion/flow model for this task, 3) DFM-Uni: original DFM uniform diffusion using\ntau-leaping, 4) DDPD-DFM-Uni: DDPD using uniform diffusion model as planner and denoiser,\n5) DDPD-UniD: DDPD with separately trained planner and uniform denoiser, 6) DDPD-MaskD:\nDDPD with separately planner and mask denoiser. Details in the sampling differences are summarized"}, {"title": "7 CONCLUSION", "content": "We introduced Discrete Diffusion with Planned Denoising (DDPD), a novel framework that de-\ncomposes the discrete generation process into planning and denoising. We propose a new adaptive\nsampler that leverages the planner for more effective and robust generation by adjusting time step\nsizes and prioritizing the denoising of the most corrupted positions. Additionally, it simplifies the\nlearning process by allowing each model to focus specifically on either planning or denoising. The\nincorporation of planning makes the generative process more robust to errors made by the denoiser\nduring generation. On GPT-2 scale language modeling and ImageNet 256 \u00d7 256 token generation,\nDDPD enables a significant performance boost compared to denoiser-only discrete diffusion models."}, {"title": "A PROOFS", "content": ""}, {"title": "A.1 PROOF OF PROPOSITION 3.1", "content": "Part 1: Calculate p (zd = N|xt) in Eq. (10).\nWe first derive how to calculate p (z\u0142 = N|xt) in Eq. (10) using P1|t\nFirst, from law of total probability,\np (z = N|xt) = \u2211 P1|t (xq = ja|xt) p (zd = N|xq = jd, xt)\nNext we derive closed form of p (z\u0142 = N|xq = jd,xt).\nAccording to the noise schedule,\np(zd, xxx1) =if z\u0142 = D, xq = xqat\u03b4t0if z\u0142 = D, xq \u2260 xq(1-at)/sif z = N, x = x((1-at)/s_if z\u0142 = N,xq \u2260 xqUsing Bayes rule p (zd|xq, x1) = p (z\u0142, xd|x1) /p (xd|x1), we havep (zax, x1) =at+(1-at)/Sif z\u0142 = D,xq = xq0if zq = D, xq \u2260 xq(1-at)/Sif zq = N, xq = xq1if zq = N,xq \u2260 xqPlugging Eq. (18) into Eq. (16), we havep (z = Nxt) = \u2211 p (xq = jdxt) p (zd = N|xq = jd, xt)ja= \u2211 p (x1 = jdxt) \u00b7 1 + p (xq = x+\\xt).=\u03a3p(x \u03a3 ja\u2260xqja\u2260xd(1-at)/Sat + (1 - at)/S(1-at)/Sat + (1 - at)/S= 1 \u2212 p (x1 = x/xt)at+(1-at)/S,which gives us the form in Eq. (10).\nPart 2: Calculate P1|t (xq = jd|xt, z\u0142 = N) in Eq. (11).\nFrom Bayes rule, we haveP1|t (xq = jd/xt, z = N) =p (xq = jd, zq = N\\xt)p (zd = N\\xt)=p(xq = jd\\xt) p (zd = N\\xt, xq = jd)p(z\u0142 = N\\xt)=p (xq = jd/xt)Vxq \u2260 jap(zd = N\\xt),'Eq. (25) is by plugging in the following from Eq. (18) that p (z\u0142 = N\\xt, xq = jd) = 1 if xq \u2260 jd.\nPart 3: Verify equivalence to the original optimal rate in Eq. (9)."}, {"title": "By plugging in Eq. (10) and Eq. (11) into Eq. (9), we have", "content": "Runif (xt, jd) =at1-atrecovery ratep (za = Nxt) P1|t (xq = jaxt, za = N) =at1-atrecovery rateprob. of corruptionprob. of denoisingP1|t (x = jdxt)composed denoising prob"}, {"title": "A.2 PROOF OF THEOREM 4.1: DERIVING THE ELBO FOR TRAINING", "content": "This derivation follows the Continuous Time Markov Chain framework. We refer the readers to\nAppendix C.1 of Campbell et al. [7", "al.\n[7": ".", "7": "Appendix C.1", "logpo(x1)": "is given by introducing the corruption process as the variational\ndistribution:\nEpdata (x1) [log po (x1)", "It": "R(W"}, {"It": "W+w+ R(W", "by": "the starting state from the prior distribution po (Wo)", "po(x1)": "can be defined as:Epdata (21) [log po (x1)"}]}