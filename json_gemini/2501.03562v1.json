{"title": "Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution Perspective", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Zheng Lin", "Yue Gao", "Ling Xiong", "Yong Cui", "Hongbin Liang", "Xianhao Chen", "Heming Cui", "Dong Huang"], "abstract": "Deep Reinforcement Learning (DRL) suffers from uncertainties and inaccuracies in the observation signal in real-world applications. Adversarial attack is an effective method for evaluating the robustness of DRL agents. However, existing attack methods targeting individual sampled actions have limited impacts on the overall policy distribution, particularly in continuous action spaces. To address these limitations, we propose the Distribution-Aware Projected Gradient Descent attack (DAPGD). DAPGD uses distribution similarity as the gradient perturbation input to attack the policy network, which leverages the entire policy distribution rather than relying on individual samples. We utilize the Bhattacharyya distance in DAPGD to measure policy similarity, enabling sensitive detection of subtle but critical differences between probability distributions. Our experiment results demonstrate that DAPGD achieves SOTA results compared to the baselines in three robot navigation tasks, achieving an average 22.03% higher reward drop compared to the best baseline.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Reinforcement Learning (DRL) has exhibited exceptional performance across a spectrum of complex robotic control tasks [1]\u2013[3], successfully solving challenges such as robot navigation [4]\u2013[6], obstacle avoidance [7], [8], and robotic manipulation [9], [10]. However, in real-world applications, observation signals are often subject to noise, latency, and inaccuracies [11], [12]. Moreover, subtle variations in environmental dynamics, such as changes in surface friction or lighting conditions, can substantially degrade DRL performance [13].\nAdversarial attacks have emerged as an effective method for evaluating the robustness of DRL models [14], [15]. By introducing carefully designed perturbations to input data, these attacks effectively reveal the vulnerabilities of DRL models under uncertainties and environmental changes [16]. Training DRL agents in the presence of such adversarial scenarios has been shown to improve their robustness [17]. While current research on adversarial attacks primarily focuses on supervised learning tasks, such as image classification [18]\u2013[20], the application in the domain of DRL remains relatively unexplored. Existing efforts have concentrated on developing defense mechanisms compatible with DRL frameworks [21]\u2013[23], while little attention is paid to devising attack methods against DRL, particularly in high-dimensional continuous state and action spaces.\nUnlike supervised learning [24]\u2013[27], which involves deterministic input-output mappings [28]\u2013[30], DRL learns a policy function that maps state space to action space, typically modeled as a conditional probability distribution over possible actions [31]. As illustrated in the top of Figure 1, existing adversarial attack methods primarily target the output, which in DRL manifests as calculating adversarial perturbations based on sampled actions from the policy (or the action with the"}, {"title": "II. RELATED WORK", "content": "Adversarial attacks are predominantly applied to supervised learning tasks such as image recognition [35]\u2013[37]. FGSM [35] is a basic attack method that generates adversarial samples by perturbing inputs based on the sign of gradients in a single step, evaluating the robustness of deep neural networks (DNNs) [38], [39]. PGD [36] extends FGSM with an iterative process and projection step, generating stronger adversarial samples through multiple updates within constrained perturbations. Most current attack methods are built on PGD [40]\u2013[42]. Momentum Iterative MI-FGSM [40] incorporates momentum and gradient iteration techniques to stabilize update directions. DI2-FGSM [41] applies random transformations (e.g., scaling, translation, and rotation) to the input image each iteration before calculating gradients. NI-FGSM [42] introduces the Nesterov gradient acceleration technique to generate more effective adversarial samples and improve their transferability.\nIn DRL, Huang et al. [32] first use FGSM to study its ability to interfere with agent policies. Lin et al. [33] propose an enchanting attack to maliciously manipulate DRL agents through strategic attacks on agents over a sequence of time steps. Weng et al. [43] present a model-based DRL attack framework designed to enhance the efficacy of adversarial attacks in DRL. Other works focus on adversarial defense, primarily using adversarial examples to train agents to improve policy robustness. Fischer et al. [44] devise Robust Student-DQN (RS-DQN), a method that enables simultaneous robust online training of Q-networks and policy networks. Zhang et al. [45] proposed SA-MDP to theoretically unify the adversarial defense process in DRL. Oikarinen et al. [23] develop RADIAL-RL framework to enhance the effectiveness of adver-sarial defense. However, these methods compute perturbations for attack or defense based on actions sampled from the policy (or actions with the highest weights), thus failing to capture broader vulnerability across the policy distribution."}, {"title": "III. METHODOLOGY", "content": "DRL is formalized as a Markov decision process [46], which is formalized as a tuple $(S, A, P, R, \\gamma)$, where $S$ and $A$ denote the action and state spaces. Policy $\\pi : S \\rightarrow P(A)$ represents a probability distribution mapping from the state space to the action space. At each time step $t$, the agent chooses an action $a_t \\sim \\pi (\\cdot | s_t)$ based on the current state $s_t$ and interacts with the environment. The environment transitions to a new state $s_{t+1}$ following the state-transition probability function $P(s_t \\rightarrow s_{t+1}, a_{t+1})$ and provides a reward $R(s_t, a_t)$ to the agent. The agent's goal is to learn a policy $\\pi$ to maximize the expected discounted return $E [\\Sigma_{i=0}^{\\infty} \\gamma^i R (S_{t+i}, a_{t+i})]$, where $\\gamma$ denotes the discount factor. In an adversarial attack to policy, assuming $\\pi_\\theta (\\cdot | s)$ represents a parameterized policy that has converged in the environment, where $\\theta$ denotes the network parameters of the policy. For notational simplicity, we use $\\pi(\\cdot|s)$ as $\\pi[s]$. Given a state-action pair $(s, a)$, the goal of the adversarial attack is to find an adversarial example $s^*$ that maximizes the loss function while satisfying a paradigm constraint [14]:\n$\\arg \\max_{s^*} J (s^*, a), \\text{ s.t. } ||s^* - s||_p \\leq \\epsilon,$ (1)\nwhere $s^* = s + \\eta$ and $\\eta$ denote the adversarial perturbation, $J(s^*, a)$ represents the loss function, and $|\\cdot||_p$ is the $L_p$ norm. In cases where the state and action space are discrete, the loss function is the cross-entropy loss, while for continuous cases, it is the mean square error loss. $L_p$ norm is usually the $L_1$ norm, $L_2$ norm or $L_\\infty$ norm.\nThe core idea of gradient-based attack methods is to calculate the gradient of the input data and then apply small perturbations to the input data along the gradient direction to induce incorrect output from the model [35], [36]. Numerous studies have demonstrated that this method can also significantly degrade the performance of DRL. For applying gradient-based attack methods to DRL, existing approaches typically rely on policy sampling to generate adversarial examples. To motivate this, we use the PGD [36] as an example to illustrate this"}, {"title": "IV. EVALUATION", "content": "We conduct extensive experiments on three continuous control navigation tasks from the Safety Gymnasium framework [47]: SafetyRacecarButton1-v0 (Button), SafetyRacecarCircle1-v0 (Circle), and SafetyRacecarGoal1-v0 (Goal). These tasks simulate scenarios relevant to autonomous vehicle navigation, such as activating buttons, maintaining circular trajectories, and reaching targets, while adhering to safety constraints like avoiding hazards and boundaries. The agent operates with realistic car dynamics in a 2-dimensional action space (velocity and steering angle), and the observation space includes multiple sensor data from lidar, accelerometer, speedometer, gyroscope, and magnetometer.\nWe evaluate the effectiveness of various adversarial attack methods on DRL agents trained for continuous control navigation tasks. In each environment, agents are trained using the Trust Region Policy Optimization (TRPO) algorithm [48]. The agents are configured with 20 conjugate gradient iterations, 15 search steps, 10 learning iterations, a discount factor $\\gamma$ of 0.99, a batch size of 128, and a maximum gradient norm of 50, trained for $1 \\times 10^7$ steps. All networks consist of two hidden layers with 64 nodes.\nWe implement and evaluate a range of attack methods, including FGSM [35], DI\u00b2-FGSM [41], MI-FGSM [40], NI-FGSM [42], PGD [36], TPGD [49], EOTPGD [50], and our proposed DAPGD method. For iterative methods, we conduct experiments with the number of iteration"}, {"title": "V. CONCLUSION", "content": "We propose DAPGD, a novel adversarial attack method for DRL agents. DAPGD utilizes distribution similarity and targets the entire policy distribution addressing limitations of existing attacks, especially in continuous action spaces. It use Bhattacharyya distance to measure policy similarity, enabling sensitive detection of subtle but critical differences between probability distributions. Experimental results show that DAPGD outperforms the all baselines in three navigation tasks, achieving average 22.03% higher reward drop compared to the best baseline. As a potential future direction, we are looking forward to extending our DAPGD to improve the performance of various applications such as large language models [52]\u2013[54], multi-modal training [55]\u2013[58], and au-tonomous driving [55], [59]\u2013[62]."}], "equations": ["arg max, J (s*, a), s.t. ||s* \u2013 8||p \u2264 \u20ac", "Sk+1 = s + a sgn (\u2207s* J (a, a))", "so = s + \u03b5\u00b7 \u039d (0, 1)", "\u03b1\u03ba ~ \u03c0\u03bf [5], \u03b1 ~ \u03c0\u03bf [\u03b4],", "arg max\uff61* J (\u03c0\u03b8 [s*], \u03c0e [s]), s.t. ||s* - 8||p \u2264 6", "J (\u03c0\u03bf [8*], \u03c0\u03bf [5])\n= - ln\n=-", "J (\u03c0\u03bf [8*], \u03c0\u03b8 [as)da", "\u03c0\u03c1 (\u03b1 | s*) \u03c0\u03bf (\u03b1|s)da", "Sk+1 = s + a sgn (\u2207s* J (\u03c0\u03b8 [s*], \u03c0\u03bf [5]))", "s = s + \u03b5\u00b7 \u039d (0, 1)"]}