{"title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training", "authors": ["Yuancheng Wang", "Jiachen Zheng", "Junan Zhang", "Xueyao Zhang", "Huan Liao", "Zhizheng Wu"], "abstract": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data.", "sections": [{"title": "1. Introduction", "content": "Advancing a unified framework capable of addressing diverse tasks is a central research objective within the domain of artificial intelligence. In natural language processing and computer vision , foundation models leveraging large-scale self-supervised pre-training have demonstrated remarkable adaptability across a wide spectrum of downstream tasks. However, in the domain of speech generation, this potential remains underexplored. A unified speech can integrate various speech generation technologies, such as text-to-speech , voice conversion , and speech enhancement . This integration reduces redundant development, facilitates broader applications across diverse domains, and enhances the efficiency of human-machine interaction.\nPrevious speech generation models are mostly expert models that require extensive task-specific designs . UniAudio and SpeechX are pioneering works that attempt to use autoregressive language models for multiple speech generation tasks. However, they require a large amount of paired training data for each task and face challenges in extending to new tasks based on pre-trained models. In addition, the autoregressive approach leads to suboptimal results in certain tasks and is relatively inefficient.\nIn this paper, we address the research question of how to design a unified speech generation framework that leverages large-scale unlabeled speech data for pre-training and efficiently adapts to diverse speech generation tasks through fine-tuning. Inspired by previous two-stage speech generation models, we revisit the speech generation process and observe a common structure underlying most tasks: generating intermediate representations from task-specific conditions and synthesizing acoustic representations from these intermediates. The intermediate representations are typically discrete units quantized from self-supervised speech features that encode semantic and prosodic information, while acoustic representations are"}, {"title": "2. Related Work", "content": "Masked Generative Models for Speech Masked generative models (MGMs) are a family of generative models that typically employ non-autoregressive transformers . These models have achieved significant success, demonstrating performance comparable to or even surpassing autoregressive and diffusion models in image and video generation, while offering a better balance between quality and speed. In the speech domain, SoundStorm uses the semantic tokens from AudioLM and employs MGMs to"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Background: Masked Generative Models", "content": "In this section, we provide a brief introduction to masked generative models. Consider a discrete sequence x = [y\u2081, y\u2082,..., y\u2099], where n denotes the length of the sequence. We define x\u209c = x \u2299 m\u209c as the operation of masking a subset of tokens in x using the corresponding binary mask m\u209c = [m\u209c,\u2081, m\u209c,\u2082,..., m\u209c,\u2099]. Specifically, this operation in-"}, {"title": "3.2. Overview of Metis", "content": "Figure 1 provides an overview of our system, which adopts a two-stage speech generation process for unified speech generation tasks. We first briefly introduce the distinct speech discrete representations used in the two stages in Section 3.3. Then, we present the pre-training (Section 3.4) and fine-tuning (Section 3.5) strategies for the first stage, which form the core of our work. Finally, we provide a brief overview of a unified acoustic decoder for all speech generation tasks in Section 3.6."}, {"title": "3.3. Discrete Representations for Two-Stage Generation", "content": "Metis employs two discrete speech representations for the two-stage generation, as illustrated in Figure 2. 1) SSL tokens: Derived from SSL features of large-scale speech"}, {"title": "3.4. Masked Generative Pre-training with SSL Tokens", "content": "Based on the previous discussion, most speech generation tasks can be generalized into two stages: conditions to SSL tokens and SSL tokens to acoustic tokens. The primary distinction among tasks lies in the nature of the conditions. To address this, we propose a unified pre-trained model for the first stage, which can be adapted to various tasks.\nWe use an unconditional masked generative model on SSL tokens for pre-training. Specifically, we randomly mask tokens in the SSL token sequence x\u02e2\u02e2\u02e1 using the strategy outlined in Section 3.1 and predict the masked tokens. We introduce a prompt sequence with the probability p to further enhance the in-context learning ability of the model. With this probability, a prefix sequence x\u02e2\u02e2\u02e1\u209a\u1d63\u2092\u2098\u209a\u209c from the SSL token sequence is used as a prompt and remains unmasked. This mechanism enables the model to leverage prompt information, thereby improving its adaptability to downstream tasks requiring prompts, such as zero-shot TTS and target speaker extraction. The pre-training objective is to model p\u2080(x\u2080|x\u209c, x\u02e2\u02e2\u02e1\u209a\u1d63\u2092\u2098\u209a\u209c).\nOur design is motivated by the observation that models trained on extensive data can recover masked SSL tokens from prompts and unmasked tokens, even without task-specific conditions, as shown in other domains. Empirically, the pre-trained model can generate speech that mimics the prosodic style and timbre of a prompt. However, the generated speech often lacks intelligibility, producing random word concatenations due to the absence of semantic guidance. This highlights the need for task-specific conditions, which can be efficiently incorporated through fine-tuning to adapt the model to various speech generation tasks."}, {"title": "3.5. Efficient Adaptation to Various Generation Tasks", "content": "Now, we describe how to efficiently adapt the pre-trained model to various speech generation tasks. We first categorize the conditions for different speech generation tasks into two types: non-frame-level conditions and frame-level conditions. For the former, such as TTS, the condition is a phoneme sequence or a text token sequence. In this case, the model needs to implicitly learn the alignment between the condition and the SSL token sequence. For the latter, such as voice conversion or speech enhancement, the conditions (e.g., the source speech voice conversion or the noisy speech for speech enhancement) can be aligned with the target SSL token sequence at the frame level. Based on these distinctions, during fine-tuning, for non-frame-level conditions, we simply concatenate the condition with the input sequence along the time dimension. For frame-level conditions, we apply a simple interpolation to align the condition with the input sequence in the time dimension and then pass it through an MLP-based adapter before adding it to the input. Then, the fine-tuned model is trained to learn p\u03b8(x\u2080|x\u209c, x\u02e2\u02e2\u02e1\u209a\u1d63\u2092\u2098\u209a\u209c, c), where c is the task-specific condition. Our experiments demonstrate that the pre-trained model can efficiently adapt to various tasks and achieve remarkable results with minimal data. Additionally, we explored the use of Low-Rank Adaptation (LoRA) , which allows fine-tuning with a small number of trainable parameters. More details are provided in Section 4."}, {"title": "3.6. Masked Generative Acoustic Decoder", "content": "We train an SSL-to-acoustic model based on masked generative modeling, which serves as a unified acoustic decoder for all speech generation tasks. The model is trained to recover masked tokens from a masked acoustic token sequence x\u1d57\u1d43, conditioned on SSL tokens x\u02e2\u02e2\u02e1 and prompt acoustic tokens x\u1d56\u02b3\u1d52\u1d50\u1d56\u1d57. This can be formulated as p\u03b8(x\u1d57\u1d43| x\u02e2\u02e2\u02e1, x\u1d56\u02b3\u1d52\u1d50\u1d56\u1d57, x\u1d57\u1d43). During training, we randomly select a layer for masking in the multi-layered acoustic tokens, while the lower-layer tokens remain unmasked and serve as conditional inputs to the model, following . During inference, we generate acoustic tokens layer by layer."}, {"title": "4. Experiments and Results", "content": ""}, {"title": "4.1. Setup", "content": "Model Architecture We adopt the same model architecture as described in Wang et al. (2024c), except removing the text embedding during the pre-training phase. The model follows the standard Llama-style architecture , but replaces causal attention with bidirectional attention.\nDataset We use a dataset consisting of 300K hours of speech for pre-training, including 100K hours from the Emilia dataset and an additional 200K hours self-collected through the Emilia pipeline, to train our pre-trained model. The dataset contains a diverse range of in-the-wild multilingual speech data, including 87K hours in Chinese, 185K hours in English, 7K hours in German, 8K hours in French, 2.5K hours in Japanese, and 7.5K hours in Korean. The dataset used for fine-tuning is sampled from the pre-trained dataset.\nTraining We pre-train our model on 8 GPUs for a total of 1200K steps. We use the AdamW optimizer with a learning rate of 1e-4 and 32K warmup steps. We employ a dynamic batch size, where each batch contains 10K tokens (200 seconds) per GPU. During training, we randomly select a prefix of the sequence as a prompt that is not masked with a probability p = 0.8. The length of the prompt is uniformly sampled from the range [0%, 40%] of the total sequence length.\nInference We follow the inference strategy outlined in Wang et al. (2024c), with task-specific step adjustments for the fine-tuned model to generate SSL tokens. Additionally, we use classifier-free guidance for tasks that involve prompts.\nEvaluation Metrics We use multiple evaluation metrics to assess different aspects of the generated speech, including similarity (SIM), intelligibility (WER), and audio quality (DNSMOS, SIG, BAK, OVRL, NISQA). For the subjective metrics, quality mean option score (CMOS) and similarity mean option score (SMOS) are used to evaluate speech quality and similarity. The results of the subjective evaluations are presented in Appendix D.3. Details about these metrics are provided in Appendix C."}, {"title": "4.2. Results on Different Speech Generation Tasks", "content": "We present the fine-tuning results of Metis on various speech generation tasks. These tasks include zero-shot TTS (with-"}, {"title": "4.2.1. ZERO-SHOT TTS", "content": "Implementation Details For zero-shot TTS, we fine-tune the pre-trained model on three different datasets: 1K hours and 10K hours randomly sampled from the Emilia dataset, and the LibriTTS dataset, which contains 0.58K hours of English speech data. We use two fine-tuning methods: full-scale fine-tuning and LoRA fine-tuning. For LORA fine-tuning, we set the rank r = 32, resulting in only 32M trainable parameters, including the newly added text embedding module. All fine-tuned models are trained using 4 GPUs for 200K steps.\nEvaluation and Baseline We evaluate our zero-shot TTS models using three test sets: 1) SeedTTS test-en, a test set introduced in Seed-TTS comprising 1,000 samples extracted from English public corpora, including the Common Voice dataset . 2) SeedTTS test-zh, a test set introduced in Seed-TTS consisting of 2,000 samples extracted from Chinese public corpora, including the DiDiSpeech dataset . 3) LibriSpeech test-clean , a widely used test set for TTS. We compare our models with several recent zero-shot TTS models, including AR-based models: VALL-E , VoiceCraft , XTTS-v2 , and Cosy Voice ; NAR-based models: VoiceBox , and MaskGCT . Specifically, MaskGCT can be seen as Metis without pre-training. For VoiceBox and Natural Speech 3, we only compare with them on LibriSpeech test-clean since they are not open-source. See more details about these baselines in Appendix B.1.\nResult The results are presented in Table 1, with additional results for LibriSpeech test-clean set provided in Ap-pendix D.1. The table shows that Metis-TTS LORA 32, fine-tuned with only 1K hours of data and 32M trainable parameters, achieves performance on metrics such as WER, SIM, and DNSMOS that is comparable to or exceeds that of some baselines trained on datasets 10 to 100 times larger. Furthermore, when scaled with full fine-tuning on 10K hours of data, Metis demonstrates an improved WER on SeedTTS test-en, outperforming MaskGCT, which was trained on 100K hours of data, while maintaining comparable WER performance on SeedTTS test-zh. When fine-tuned with only 580 hours of LibriTTS data, Metis achieves competitive WER and SIM performance, surpassing all baselines except MaskGCT on SeedTTS test-en. We also compare with models that are not pre-trained but trained for the same number of steps. The results show that the fine-tuned models"}, {"title": "4.2.2. VOICE CONVERSION", "content": "Implementation Details Previous voice conversion systems typically extract timbre-independent features from input signals using methods such as information bottlenecks , timbre perturbation , or specialized loss functions . However, these approaches often introduce complexity, risk the loss of semantic information, and may still inadvertently retain timbre-related details. In this work, we use a simpler yet effective solution inspired by Anastassiou et al. (2024a); Neekhara et al. (2023). Specifically, we leverage a lightweight voice conversion model to perform real-time voice conversion on the target speech using a randomly sampled prompt speech, thereby achieving timbre perturbation. The perturbed speech features are then used as input to predict the target speech based on the prompt. Specifically, we employ the w2v-bert-2.0 features of the perturbed speech as conditioning inputs and randomly extract a prefix of the target speech as the prompt. Our experiments demonstrate that with minimal data and training steps, our pre-trained model can effectively and rapidly adapt to the voice conversion task. We use both full-scale fine-tuning and LoRA"}, {"title": "4.2.3. TARGET SPEAKER EXTRACTION", "content": "Implementation Details For target speaker extraction, we randomly sample 10K hours of data from the pre-training dataset to create the fine-tuning training set without any filtering. During training, samples are dynamically mixed to simulate multi-speaker speech data. Specifically, a random prefix from a sample is extracted as the prompt, and the remaining portion is mixed with another randomly selected sample by directly adding their waveforms. The w2v-bert-2.0 features of the mixed speech are then used as conditions. We use two fine-tuning methods: full-"}, {"title": "4.2.4. SPEECH ENHANCEMENT", "content": "Implementation Details For speech enhancement, we simulate noisy speech following previous works . We utilize noise datasets such as WHAM! and DEMAND , along with room impulse response (RIR) datasets , OpenSLR26 and OpenSLR28, in accordance with the 2020 DNS-Challenge. For clean speech data, we randomly sample 10K hours from our pre-trained dataset without any filtering. Inspired by prior works , we simulate speech degradation by probabilistically applying various distortions to the audio signals. These include adding noise within an SNR range of -5 to 20 dB with probability p = 0.9, introducing reverberation with p = 0.35, and limiting the speech signal bandwidth (randomly selected from 2 kHz, 4 kHz, or 8 kHz) with a probability of with p = 0.25. The w2v-bert-2.0 features extracted from the degraded speech serve as input conditions for our models. We fine-tune the models using both full-scale fine-tuning and LoRA fine-tuning. For LoRA fine-tuning, we also experiment with r = 4, 16, 32, corresponding to LoRA modules with trainable parameter counts of 2M, 9M, and 18M, respectively.\nEvaluation and Baseline We evaluate our models using the 2020 DNS-Challenge test sets, which consist of three categories: 1) synthetic data with reverb, 2) synthetic data without reverb, and 3) real recordings. For comparison, we downsample the output of our models to 16 kHz. We compare our models with several recent speech enhancement models, including TF-GridNet , VoiceFixer , SELM and MaskSR . Notably, MaskSR is a model that also uses masked generative modeling but directly generates acoustic tokens for speech enhancement. See more details about these baselines in Appendix B.4.\nResult The results are presented in Table 4. The table shows that our models achieve state-of-the-art performance across all benchmarks. 1) Metis-SE achieves state-of-the-art results across all three test sets with the highest SIG, BAK, OVRL, and NISQA scores, showing significant improve-ments over previous baselines. 2) Results among different fine-tuned versions are comparable, even for Metis-SE LORA 4. 3) Notably, Metis-SE performs exceptionally well on the real recording dataset, further validating its practical applicability."}, {"title": "4.2.5. LIP-TO-SPEECH", "content": "Implementation Details We use a combined data set comprising the training and sets of LRW\u00b3, LRS24, and LRS35"}, {"title": "4.3. Multi-Task Fine-Tuning", "content": "In addition to fine-tuning the pre-trained model separately for different tasks, we explore the potential of jointly fine-tuning it on multiple tasks, resulting in a multi-task model, which we refer to as Metis-Omni. For this study, we select four tasks: zero-shot TTS, voice conversion, target speaker extraction, and speech enhancement. Further details and experimental results are provided in Appendix E and Table 11.\nWe take target speaker extraction as an example to illustrate the effectiveness of Metis-Omni in enabling novel applications through task combinations. As shown in Table 6, Metis-Omni outperforms baseline methods in terms of WER. Furthermore, by integrating text-to-speech and target speaker extraction tasks, the text-guided version of Metis-Omni achieves a remarkable WER reduction to 2.70, demonstrating a substantial improvement over all other models. Notably, despite the model not being explicitly trained on text-guided target speaker extraction, it generalizes well to this novel setting, highlighting the advantage of our system in leveraging multimodal conditional inputs to enable flexible and efficient task adaptation."}, {"title": "5. Conclusion", "content": "In this work, we propose Metis, a foundation model for unified speech generation that leverages large-scale unlabeled speech data for pre-training and can be effectively adapted to diverse speech generation tasks through fine-tuning. Our experiments demonstrate that Metis outperforms state-of-the-art task-specific and multi-task systems on zero-shot TTS, voice conversion, target speaker enhancement, speech enhancement, and lip-to-speech after fine-tuning, even with fewer than 20M trainable parameters or up to 300 times less training data for certain tasks while supporting multimodal conditional inputs. In addition, we propose Metis-Omni, a version of our pre-trained model fine-tuned on multiple tasks, which demonstrates further improvements."}, {"title": "Impact Statement", "content": "Given that Metis is a powerful foundation model capable of generating high-quality speech across multiple tasks, it also presents potential risks of misuse, such as voice spoofing, speaker impersonation, and unauthorized content generation. To mitigate these risks, it is essential to develop robust detection mechanisms for synthetic speech, establish safeguards to prevent malicious use, and implement a responsible reporting system for identifying and addressing misuse cases."}, {"title": "A. Details of Two Types of Speech Discrete Representations", "content": "Metis employs two discrete speech representations for the two-stage generation, following the approach of Wang et al. (2024c).\n1) SSL tokens: Derived from SSL features of large-scale speech self-supervised learning models , SSL tokens encapsulate both semantic and prosodic information, making them well-suited for conditional generation. To minimize information loss, we employ a vector quantization (VQ) model to quantize SSL features into discrete tokens, in contrast to the k-means approach used in previous works . SSL features are extracted from the 17th layer of w2v-bert-2.06 . The VQ model has a codebook of 8,192 and a codebook dimension of 8. The model is trained using only reconstruction loss and VQ loss.\n2) Acoustic tokens: Directly obtained from the waveform via vector quantization. The goal is to preserve all the information from the speech to reconstruct a high-quality waveform. We follow the recipe of DAC codec for model architecture, discriminators, and training losses, with modifications to adopt the Vocos decoder for more efficient training and inference. The 24 kHz speech waveform is compressed into discrete tokens using residual vector quantization (RVQ) across 12 layers, each with a codebook size of 1,024 and a codebook dimension of 8."}, {"title": "B. Baselines", "content": ""}, {"title": "B.1. Zero-Shot TTS", "content": "VALL-E It uses an AR transformer to predict codes from the first layer of EnCodec and a NAR transformer to predict codes from the remaining layers of EnCodec. We use the released checkpoint in Amphion which is pre-trained on 45K hours of the MLS English set.\nVoiceCraft A token-filling neural codec language model for text editing and text-to-speech. It predicts multi-layer tokens in a delay pattern. We use the official code and checkpoint which is pre-trained on 9K hours of GigaSpeech dataset.\nCosy Voice A two-stage large-scale TTS system. The first stage is an autoregressive model and the second stage is a diffusion model. It is trained on 171K hours of multilingual speech data. We use the official code and checkpoint.\nXTTS-v2 An open-source multilingual TTS model that supports 16 languages. It is also based on an autoregressive model. We use the official code and checkpoint10.\nMaskGCT An open-source large-scale NAR TTS system that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. It employs masked generative models for two-stage modeling and is trained on 100K hours of multilingual speech data. We use the official code and checkpoint11.\nNaturalSpeech 3 A large-scale NAR TTS system featuring a factorized speech codec for speech decoupling representation and factorized diffusion models for speech generation. It achieves human-level naturalness on the LibriSpeech test set. We report the scores of LibriSpeech test-clean obtained from the original paper.\nVoiceBox A large-scale NAR multi-task speech generation model based on flow matching . We report the scores of LibriSpeech test-clean obtained from the original paper."}, {"title": "B.2. Voice Conversion", "content": "HireSpeech++ A speech generation system designed based on the VITS architecture . It is trained on 2.8K hours sourced from Libri-light and LibriTTS . We use the official code and checkpoint12.\nLM-VC It uses an AR hierarchical transformer to predict SoundStream codes from soft units similar to HuBERT k-means tokens, trained on the Libri-light dataset . It is not open-source, we report the scores obtained from . \nUniAudio An AR model system can perform multiple audio generation tasks. It uses 500-cluster k-means tokens from HuBERT to predict their proposed acoustic codes for voice conversion. We use the official code and checkpoint13. It uses 60K hours of Libri-light for training voice conversion.\nVevo It is a versatile zero-shot voice imitation model which can control both timbre and style. It is trained on 60K hours of Libri-heavy . We obtain the generated samples from the authors."}, {"title": "B.3. Target Speaker Extraction", "content": "UniAudio An AR model system can perform multiple audio generation tasks including target speaker extraction. We use the official code and checkpoint.\nVoiceFilter A system that isolates a target speaker's voice from multi-speaker audio using a reference signal and neural networks for speaker embedding and spectrogram masking. We use the checkpoint 14 provided from Nguyen & Waibel (2024).\nWeSep A target speaker extraction model trained on LibriMix and VoxCeleb . We use the official code and checkpoint15.\nTSELM A target speaker extraction model that leverages a NAR transformer to predict discrete speech tokens driven from WavLM . We use the official code and checkpoint16."}, {"title": "B.4. Speech Enhancement", "content": "TF-GridNet A deep neural network for speech separation integrating full- and sub-band modeling in the time-frequency (T-F) domain. It can also used for speech enhancement. We use the checkpoint\u00b9\u2077 provided from Interspeech URGENT 2025 Challenge18.\nVoiceFixer A generative framework to address the speech enhancement task. It consists of an analysis stage and a synthesis stage and employs a ResUNet to model the analysis stage and a neural vocoder to model the synthesis stage. We use the official code and checkpoint19.\nSELM A speech enhancement model that leverages an AR transformer to predict discrete speech tokens driven from WavLM . We report the scores obtained from .\nMaskSR It uses masked generative models to predict acoustic tokens from DAC codec for speech enhancement. We report the scores obtained from the original paper."}, {"title": "B.5. Lip-to-Speech", "content": "Lip2Speech-Unit A lip-to-speech model trained to generate multiple targets, mel-spectrograms and quantized self-supervised speech representations. We use the official code and checkpoint20."}, {"title": "C. Evaluation Metrics", "content": "SIM We evaluate speaker similarity between the prompt speech and the generated speech by computing the cosine similarity of the WavLM TDNN21(Chen et al., 2022) speaker embeddings between the generated sample and the prompt. SIM is reported for tasks involving prompt speech, including zero-shot TTS, voice conversion, target speaker extraction, and lip-to-speech. For speech enhancement, we compute SIM between the generated speech and ground truth using a separate checkpoint22.\nWER Word Error Rate measures the intelligibility of the generated speech. We use whisper-large-v323 for all languages except Chinese, where we use paraformer-zh24 (Gao et al., 2022; 2023) as the ASR model to calculate WER. WER is reported for zero-shot TTS, voice conversion, target speaker extraction, and lip-to-speech.\nDNSMOS DNSMOS is a neural network-based mean opinion score estimator that correlates strongly with human quality ratings. It comprises three components: 1) speech quality (SIG), 2) background noise quality (BAK), and 3) overall quality (OVRL). We report DNSMOS scores for all tasks while providing the average scores for zero-shot TTS and VC tasks, and all three metrics for the remaining tasks.\nNISQA NISQA is a deep learning framework for speech quality prediction. We use the public checkpoint25. We report NISQA for voice conversion, speech enhancement, target speaker extraction, and lip-to-speech.\nQMOS We use the Quality Mean Opinion Score (QMOS) for subjective listening tests to evaluate speech quality. QMOS is rated on a 5-point scale: 5 (Excellent), 4 (Good), 3 (Fair), 2 (Poor), and 1 (Bad).\nSMOS We use the Speaker Mean Opinion Score (SMOS) for subjective listening tests to assess speaker similarity in speech. SMOS is rated on a 5-point scale: 5 (Excellent), 4 (Good), 3 (Fair), 2 (Poor), and 1 (Bad). SMOS is reported for zero-shot TTS."}, {"title": "D. Additional Experimental Results", "content": ""}, {"title": "D.1. Zero-Shot TTS Results on LibriSpeech", "content": "Table 7 shows zero-shot TTS results on LibriSpeech test-clean. Metis-TTS achieves the highest SIM despite using significantly less training data, while maintaining a competitive WER."}, {"title": "D.2. Voice Conversion Results on SeedVC", "content": "We also provide voice conversion results on SeedVC in Table 8. Despite being fine-tuned on significantly less training data (0.4K hours compared to 2.8K hours used by HierSpeech++), Metis-VC achieves higher SIM scores on both SeedVC-en (0.76 vs. 0.56) and SeedVC-zh (0.63 vs. 0.39), demonstrating its strong capability in preserving speaker identity. Additionally, Metis-VC achieves a higher NISQA score than the baseline model, indicating improved speech quality."}, {"title": "D.3. Results of Subjective Evaluations", "content": "We conduct subjective listening tests for two representative tasks: zero-shot TTS and speech enhancement. For zero-shot TTS, we randomly sampled 20 test samples from the results of SeedTTS test-en. Both QMOS and SMOS are reported for zero-shot TTS. The results are shown in Table 9. For speech enhancement, we randomly sampled 20 test samples from the results of DNS2020 with reverb. QMOS is reported for speech enhancement. The results are shown in Table 10."}, {"title": "E. Metis-Omni: Multi-Task Fine-Tuning", "content": "In addition to fine-tuning the pre-trained model separately for different tasks, we explore the potential of jointly fine-tuning a pre-trained model on multiple tasks resulting a multi-task model, which we refer to as Metis-Omni. For this study, we select four tasks: zero-shot TTS, voice conversion, target speaker extraction, and speech enhancement. We show more details in Appendix E. A subset of 10K hours of data is randomly sampled from the Emilia dataset as the shared training data for these tasks. During training, the task proportions are set as p = {0.5, 0.1, 0.2, 0.2} for zero-shot TTS, voice conversion, target speaker extraction, and speech enhancement, respectively. As shown in Table 11, Metis-Omni achieves performance that is either on par with or surpasses task-specific fine-tuned models across all tasks. The only exception is the WER metric in zero-shot TTS, where performance is slightly lower. A possible reason for this is that, under the same number of training steps, multi-task fine-tuning allocates fewer steps to zero-shot TTS, leading to less task-specific optimization."}, {"title": "F. Speech Discrete Representation", "content": "Speech representation is a crucial aspect of speech generation. Recently, some speech generation systems have transitioned to using discrete speech representations, which can be broadly categorized into two types: 1) SSL tokens: typically derived by quantizing speech self-supervised learning features . Unlike acoustic tokens, SSL tokens are designed not for directly reconstructing waveform but for encoding essential semantic and prosody information in speech, making them more suitable for prediction in conditional generation models. 2) Acoustic tokens: typically obtained by training a VQGAN model for waveform reconstruction, as used in speech codecs . Acoustic tokens are effective for reconstructing high-quality waveforms. Currently, some speech generation systems utilize both types of representations for speech generation. In this work, we also adopt this two-stage paradigm, employing an MGM to generate SSL tokens from any condition and another MGM to generate acoustic tokens from SSL tokens."}, {"title": "G. Limitation and Future Work", "content": "There are still some limitations that can be studied in the future."}, {"title": "Unified Audio Representation", "content": "In this work, we utilize two distinct discrete speech representations, SSL tokens and acoustic tokens, for two-stage modeling, with SSL tokens specifically designed for the speech domain. Developing a unified discrete representation for all audio types, such as speech, music, and sound effects, that can seamlessly integrate with conditional generation models is an important direction for future research. Additionally, unifying the characteristics of SSL tokens and acoustic tokens to enable both high-quality waveform reconstruction and effective conditional modeling is equally significant."}, {"title": "Few-Shot Task Learning", "content": "In this work, we adapt our pre-trained model to different characters through fine-tuning. In the field of NLP, large language models exhibit the remarkable ability to learn new tasks in a zero-shot manner without requiring additional training. This capability merits further investigation in future research."}]}