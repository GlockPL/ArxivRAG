{"title": "RL for Mitigating Cascading Failures: Targeted Exploration via Sensitivity Factors", "authors": ["Anmol Dwivedi", "Santiago Paternain", "Ali Tajer", "Nurali Virani"], "abstract": "Electricity grid's resiliency and climate change strongly impact one another due to an array of technical and policy-related decisions that impact both. This paper introduces a physics-informed machine learning-based framework to enhance grid's resiliency. Specifically, when encountering disruptive events, this paper designs remedial control actions to prevent blackouts. The proposed Physics-Guided Reinforcement Learning (PG-RL) framework determines effective real-time remedial line-switching actions, considering their impact on power balance, system security, and grid reliability. To identify an effective blackout mitigation policy, PG-RL leverages power-flow sensitivity factors to guide the RL exploration during agent training. Comprehensive evaluations using the Grid2Op platform demonstrate that incorporating physical signals into RL significantly improves resource utilization within electric grids and achieves better blackout mitigation policies - both of which are critical in addressing climate change.", "sections": [{"title": "Introduction", "content": "Power grid resiliency and climate change are symbiotically interconnected. Climate change is increasing the frequency and intensity of extreme weather events, such as hurricanes, floods, wildfires, and heatwaves, requiring improved grid resiliency to maintain power and reduce economic and societal impacts. Mitigating climate change needs reduction in the energy system's carbon footprint, which critically hinges on integrating renewable resources at scale. However, grid resilience enhancement is needed to provide robustness against equipment failures and manage stability impact of variability from renewable generation. Thus, mitigating and adapting to climate change necessitates enhancing grid resilience. This paper provides a physics-informed machine learning (ML) approach to enhance grid resiliency, defined as the grid's ability to withstand, adapt, and recover from disruptions.\nOne major source of disruption impacting grid resiliency are transmission line and equipment failures, often caused due to aging infrastructure stressed by extreme weather and congestion due to growing electricity demand. These gradual stresses can lead to system anomalies that can escalate if left unaddressed [1]. To mitigate these risks, system operators implement real-time remedial actions like network topology changes [2, 3, 4, 5]. Selecting these remedial actions must balance two opposing impacts: greedy actions render quick impact to protect specific components but may have inadvertent consequences, while look-ahead strategies enhance network robustness but have delayed impact. Striking this balance is crucial for maintaining reliable operation and maximizing grid utilization.\nThere are two main approaches for the sequential design of real-time remedial decisions: model-based and data-driven. Model-based methods, like model predictive control (MPC), approximate the system model and use multi-horizon optimization to predict future states and make decisions [6, 7, 8, 9]. While these methods offer precise control by adhering to system constraints, they require an accurate analytical model, which can be difficult for T-grids. Moreover, coordinating discrete actions like line-switching over extended planning horizons is computationally intensive and time-consuming."}, {"title": "Problem Formulation", "content": "Transmission grids are vulnerable to stress by adverse internal and external conditions, e.g., line thermal limit violations due to excessive heat and line loading. Without timely remedial actions, this stress can lead to cascading failures resulting in blackouts. To mitigate these risks, our objective is to maximize the system's survival time over a horizon T, denoted by ST(T), defined as the time until a blackout occurs [19]. In this paper, we focus on line-switching actions $W_{line}[n] \\triangleq [W_1[n], ..., W_L[n]]^T$ to reduce system stress by controlling line flows, where the binary decision variable $W_l[n] \\in \\{0, 1\\}$ indicates whether line l is removed (0) or reconnected (1) at time $n \\in [T]$. We also define $c_{line}$ as the cost of line-switching for line l. Hence, the system-wide cost incurred due to line-switching over a horizon T is $C_{line}(T) = \\sum_{n=1}^T\\sum_{l=1}^L c_{line} \\cdot W_l[n]$.\nOperational Constraints: Line-switching decisions are constrained by operational requirements to maintain system security. Once a line is switched, it must remain offline for a mandated downtime period $T_D$ before being eligible for another switch. For naturally failed lines (e.g., due to prolonged overload), a longer downtime period $T_F$ is required before reconnection, where $T_F \\gg T_D$.\nMaximizing Survival Time: Our objective is to constantly monitor the system and, upon detect-ing mounting stress (e.g., imminent overflows), initiate flow control decisions (line-switching) to maximize the system's $S_T(T)$. Such decisions are highly constrained with decision costs $C_{line}(T)$ and operational constraints due to downtime periods $T_N$ and $T_F$. To quantify $S_T(T)$, we use a proxy, the risk margin for each transmission line l at time n, defined as $p_l[n] \\triangleq \\frac{A_l[n]}{A_l^{max}}$, where $A_l[n]$ and $A_l^{max}$ denotes the present and maximum line current flows, respectively. Based on $p_l[n]$, a line l is considered overloaded, if $p_l[n] \\geq 1$. Minimizing these risk margins reduces the likelihood of overloads, thereby extending $S_T(T)$. We also use risk margins to identify critical states, which are states that necessitates remedial interventions, defined by the rule $\\max_{l \\in [L]} p_l[n] \\geq \\eta$. To maximize $S_T(T)$, our goal is to sequentially form the decisions $W_{line} \\triangleq \\{W_{line}[n] : n \\in \\mathbb{N}\\}$ all while adhering"}, {"title": null, "content": "to operational constraints and controlled decision costs $\\beta_{line}$, formulated as:\n\n\n\n\n\n\n\n\n$\\mathcal{P}:\\begin{aligned}\\underset{\\{W_{line}\\}}{\\text{min}}  & \\sum_{n=1}^T\\sum_{l=1}^L p_l[n]\\\\\\text{s.t.} \\\\& C_{line}(T) < \\beta_{line}\\\\& \\text{Operational Constraints}\\end{aligned}$\n\n(1)\n\nCascading Failure Mitigation as an MDP: The complexity of identifying optimal line-switching (discrete) decisions grows exponentially with the number of lines L and the target horizon T, and is further compounded by the need to meet operational constraints. To address the challenges of solving $\\mathcal{P}$ in (1), we design an agent-based approach. At any instance $n \\in [T]$, the agent has access to the system's states $\\{X[m] : m \\in [n]\\}$ and uses this information to determine the line-switching actions. These actions lead to outcomes that are partly deterministic, reflecting the direct impact on the system state, and partly stochastic, representing the randomness of future electricity demands. To effectively model these stochastic interactions, we employ a Markov decision process (MDP) characterized by the tuple ($\\mathcal{S}, \\mathcal{A}_{line}, \\mathcal{P}, \\mathcal{R}, \\gamma$). Detailed information about the MDP modeling techniques employed is provided in Appendix A.1. Finding an optimal decision policy $\\pi^*$ can be found by solving [26]\n\n$\\mathcal{P}_2: \\pi^*(\\mathcal{S}) \\triangleq \\underset{\\pi}{\\arg\\max} Q(\\mathcal{S}, \\pi(\\mathcal{S})),$ (2)\nwhere $Q_\\pi(\\mathcal{S}, a)$ characterizes the state-action value function."}, {"title": "Physics-Guided RL Framework", "content": "Motivation: Model-free off-policy RL algorithms [27, 28] with function approximation [29] are effective in finding good policies without requiring access to the transition probability kernel $\\mathcal{P}$ for high-dimensional MDP state spaces $\\mathcal{S}$. However, the successful design of these algorithms hinges on a comprehensive exploration of the state space to accurately learn the expected decision utilities, such as Q-value estimates. Common approaches entail dynamically updating a behavior policy $\\pi$, informed by a separate exploratory policy like $\\epsilon$-greedy [28], illustrated in Algorithm 1. While Q-learning with random $\\epsilon$-greedy exploration is effective in many domains [29], it faces challenges in power-grid overload management. Random network topology exploration actions $a[n] \\in \\mathcal{A}_{line}$ can quickly induce severe overloads and, thus, blackouts. This is because topological actions force an abrupt change in the system state $X[n]$ by redistributing transmission line power-flows after a network topological change, compromising risk margins $p_l$ and exposing the system to potential cascading failures, preventing a comprehensive exploration of $\\mathcal{S}$. This results in inaccurate Q-value predictions for the unexplored MDP states, rendering a highly sub-optimal remedial control policy."}, {"title": "Conclusion and Future Work", "content": "We introduced a physics-guided RL framework for determining effective sequences of real-time remedial control actions to mitigate cascading failures. The approach, focused on transmission line-switches, utilizes linear sensitivity factors to enhance RL exploration during agent training. By improving sample efficiency and yielding superior remedial control policies within a constrained computational budget, our framework ensures better utilization of grid resources, which is critical in the context of climate change adaptation and mitigation. Comparative analyses on the Grid2Op 36-bus and the IEEE 118-bus networks highlight the superior performance of our framework against relevant baselines. Future work will involve using bus-split sensitivity factors [34] to computationally efficiently prune and identify effective bus-split actions for remedial control policy design. Another direction is to leverage the linearity of sensitivity factors to implement simultaneous remedial actions, expediting line flow control along desired trajectories."}, {"title": "Appendix", "content": ""}, {"title": "MDP Modeling", "content": "State Space $\\mathcal{S}$: Based on the system's state $X[n]$, which captures the line and bus features, we denote the MDP state at time n by $S[n]$, defined as a moving window of the states of length $\\kappa$, i.e.,\n\n$S[n] = [X[n-(\\kappa - 1)], ..., X[n]]^T$, (4)\nwhere the state space is $\\mathcal{S} = \\mathbb{R}^{5 \\cdot (L \\cdot N + F \\cdot H)}$. Leveraging the temporal correlation of demands, decisions based on the MDP state $S[n]$ help predict future load demands.\n\nAction Space $\\mathcal{A}$: We denote the action space by $\\mathcal{A} \\equiv \\mathcal{A}_{line}$, where $\\mathcal{A}_{line}$ is the space of line-switching. Action space $\\mathcal{A}_{line}$ includes two actions for each line $l \\in [L]$ associated with reconnecting and removing it. Besides these 2L actions, we also include a do-nothing action to accommodate the instances at which (i) the mandated downtime period $T_D$ makes all line-switch actions operationally infeasible; or (ii) the system's risk $\\max_{l \\in [L]} p_l[n]$ is sufficiently low. This action allows the agent to determine the MDP state at time n + 1 solely based on the system dynamics driven by changes in load demand D[n+1]."}, {"title": "Algorithmic Details", "content": "Stochastic Transition Kernel $\\mathcal{P}$: After an action a[n] $\\in$ $\\mathcal{A}$ is taken at time n, the MDP state S[n] transitions to the next state S[n + 1] according to an unknown transition probability kernel $\\mathcal{P}$ $S[n+1] \\sim \\mathcal{P}(S | S[n], a[n])$ where $\\mathcal{P}$ captures the system dynamics influenced by both the random future load demand and the implemented action a[n] $\\in$ $\\mathcal{A}$.\n\nReward Dynamics $\\mathcal{R}$: To capture the immediate effectiveness of taking an action a[n] $\\in$ $\\mathcal{A}$ in any given MDP state S[n], we define an instant reward function\n\n$r[n] = \\sum_{l=1}^L(1 - p_l[n]) - \\mu_{line}(\\sum_{l=1}^L c_{line} \\cdot W_l[n])$ (5)\nwhich is the decision reward associated with transitioning from MDP state S[n] to S[n+1], where the constant $\\mu_{line}$ is associated with the cost constraint $\\beta_{line}$ introduced in (1), respectively. The inclusion of parameter $\\mu_{line}$ allows us to flexibly model different cost constraints, reflecting diverse economic considerations in power systems. Greater values for the parameter $\\mu_{line}$ in (5) promote solutions that satisfy stricter cost requirements."}, {"title": "DQN Architecture and Training", "content": "Our DQN architecture features a feed-forward NN with two hidden layers, each having O units and adopting tanh nonlinearities. The input layer, with a shape of |S[n]| = \u039f\u00b7\u03ba, feeds into the first hidden layer of O units, followed by another hidden layer of O units. The network then splits into two streams: an advantage-stream $A_\\Theta(S[n], \\cdot) \\in \\mathbb{R}^{|\\mathcal{A}|}$ with a layer of |$\\mathcal{A}$| action-size units and tanh non-linearity, and a value-stream $V_\\Theta(S[n]) \\in \\mathbb{R}$ predicting the value function for the current MDP state S[n]. $Q_\\Theta(S[n], \\cdot)$ are obtained by adding the value and advantage streams. We penalize the reward function r[n] in (5) in the event of failures attributed to overloading cascades and premature scenario termination (n < T). Additionally, we normalize the reward constraining its values to the interval [-1, 1]. For the Grid2Op 36-bus system, we use a learning rate $\\alpha_n$ = 5\u00b710-4 decayed every 210 training iterations, a mini-batch size of B = 64, an initial $\\epsilon$ = 0.99 exponentially decayed to $\\epsilon$ = 0.05 over 26\u00b7103 agent-MDP training interaction steps and choose $\\gamma$ = 0.99. Likewise, for the IEEE 118-bus system we use similar parameters with a mini-batch size of B = 32. Likewise, for the IEEE 118-bus system we set $\\alpha_n$ = 9\u00b710-4 with a mini-batch size of B = 32 and 21\u00b7103 agent MDP training interaction steps."}]}