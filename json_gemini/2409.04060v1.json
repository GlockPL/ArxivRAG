{"title": "D4: Text-guided diffusion model-based domain adaptive data augmentation for vineyard shoot detection", "authors": ["Kentaro Hirahara", "Chikahito Nakane", "Hajime Ebisawa", "Tsuyoshi Kuroda", "Yohei Iwaki", "Tomoyoshi Utsumi", "Yuichiro Nomura", "Makoto Koike", "Hiroshi Mineno"], "abstract": "In an agricultural field, plant phenotyping using object detection models is gaining attention. Plant phenotyping is a technology that accurately measures the quality and condition of cultivated crops from images, contributing to the improvement of crop yield and quality, as well as reducing environmental impact. However, collecting the training data necessary to create generic and high-precision models is extremely challenging due to the difficulty of annotation and the diversity of domains. This difficulty arises from the unique shapes and backgrounds of plants, as well as the significant changes in appearance due to environmental conditions and growth stages. Furthermore, it is difficult to transfer training data across different crops, and although machine learning models effective for specific environments, conditions, or crops have been developed, they cannot be widely applied in actual fields. We faced such challenges in the shoot detection task in vineyard. Therefore, in this study, we propose a generative artificial intelligence data augmentation method (D4). D4 uses a pre-trained text-guided diffusion model based on a large number of original images culled from video data collected by unmanned ground vehicles or other means, and a small number of annotated datasets. The proposed method generates new annotated images with background information adapted to the target domain while retaining annotation information necessary for object detection. In addition, D4 overcomes the lack of training data in agriculture, including the difficulty of annotation and diversity of domains. We confirmed that this generative data augmentation method improved the mean average precision by up to 28.65% for the BBox detection task and the average precision by up to 13.73% for the keypoint detection task for vineyard shoot detection. D4 generative data augmentation is expected to simultaneously solve the cost and domain diversity issues of training data generation in agriculture and improve the generalization performance of detection models.", "sections": [{"title": "1 Introduction", "content": "Precision agriculture (PA) plays a crucial role in sustainable food production systems, and it involves a detailed recording of the conditions of farmland and crops and the precise control of the cultivation environment, which contributes to improving crop yield and quality and reducing environmental impact [1][2][3]. Thus, PA enables us to optimize land use and reduce pesticide and fertilizer usage, thereby enhancing productivity while reducing the environmental load. Plant phenotyping technology accurately measures the growth status of crops, and therefore, realizing PA is essential for this technology.\nObject detection models based on deep learning have recently attracted considerable research attention [4]. When object detection models are used for plant phenotyping, the quality and condition of crops can be easily quantified by detecting fruits and other parts from the acquired images and by performing fruit counting [5] [6] and growth analysis [7] for yield prediction. In addition, accumulating the growth process as time-series data and providing cultivation feedback based on comparisons with past data can help optimize environmental control [8].\nDespite the advantages in plant phenotyping, object detection models cannot be used for obtaining object-annotated images for model training. Securing such object-annotated images requires images with annotations, such as bounding box (BBox) and keypoint, making this data collection process time consuming and costly. For example, the COCO dataset [9] used for general object recognition tasks such as people and cars contains approximately 328,000 images and 1.5 million BBox annotations. In contrast, the GWHD dataset [10] for wheat head detection in an agricultural field provides 6,422 images and 275,167 BBox annotations. This comparison shows that the number of images and annotations is limited compared to those of other typical datasets. The lack of training data in agriculture is related to the \"difficulty of annotation\" and the \"diversity of domains.\"\nThe diversity of the agricultural domains makes it difficult to secure object-annotated images for model training. Thus, Plant phenotyping using object detection models is a promising technology in PA; however, in actual operation, it faces several challenges in securing object-annotated images. Therefore, the generalization performance of object detection models is a concern caused by overfitting, and there are limitations in improving the generalization performance and accuracy. Efficiently acquiring training data and developing an operational method that allows the training data to be applied to other fields and crops is necessary to address these challenges.\nThis study proposes a new data-augmentation method (D4) that uses a text-guided diffusion model for object detection. The technology for the periodic and long-term capture images of crop growth has improved with the development of automatic shooting equipment in the agricultural sector, thereby making it easier to collect large amounts of video data [11]. We propose a data-augmentation method that utilizes a pre-trained multimodal image generation model (text-guided diffusion model) with many original images extracted from video data and a small number of object-annotated images. A text-guided diffusion model was used for generating new object-annotated images with background information adapted to the target domain, while retaining the annotation information necessary for object detection. D4 is expected to contribute to addressing the challenges of plant phenotyping in PA and the realization of a sustainable food production system.\nThe remainder of this paper is organized as follows: Section 2 discusses related work. Section 3 describes D4. Section 4 evaluates D4 through experiments. Section 5 presents an analysis of the experimental results. Section 6 discusses the effectiveness and challenges of D4 through an ablation study. Finally, Section 7 concludes the paper."}, {"title": "2 Related Work", "content": "2.1 Generative Data Augmentation for Image Recognition\nIn training deep learning models, data augmentation techniques are commonly applied when there is a small amount of training data or a need for more diversity in the dataset. Data augmentation increases the diversity of the training data by adding new data, making it possible to improve the generalization ability of deep learning models and effectively and efficiently enhancing their accuracy for unknown data [12] [13]. In areas where it is difficult to obtain training data, such as medical image analysis [14] and agricultural image processing [15], data augmentation contributes to the construction of high-precision models from limited training data. For image recognition tasks, data augmentation can be classified broadly into the \"data transformation approach\" and \"data synthesis approach\" [16]. The data transformation approach involves data augmentation, which creates new images by transforming existing images. In particular, rotating or flipping images can improve the robustness of the model by enabling it to recognize objects from various positions and angles. These methods are relatively simple; however, they are known to contribute significantly to improving model accuracy. The application scope of the data transformation approach is limited because the generated images are restricted to variations in the original dataset. Their inability to handle diverse domain changes is specified as a disadvantage.\nIn contrast, the data synthesis approach involves generative data augmentation, which generates new images. The accuracy of object recognition under unknown environments and conditions beyond the range of representations within a limited dataset can be improved by using image generation models such as 3DCG, GAN [17], and VAE [18] for data augmentation. Therefore, generative data augmentation methods of the data synthesis approach have attracted considerable attention for addressing the issue of domain diversity.\n2.2 Generative Data Augmentation for Domain Adaptation\nA domain shift occurs when there is a discrepancy between the distributions of the training and test data, thereby leading to a decrease in the generalization performance of the model. One approach to address this issue is the study of generative data augmentation techniques through data synthesis using image generation models. The object recognition accuracy in unknown environments or under different conditions can be improved beyond the range of representations within a limited dataset using image generation models. Several image generation models have been proposed, for example, ARIGAN proposed by Valerio et al. [19], and EasyDAM proposed by Zhang et al. [20][21][22][23]. These studies demonstrated the effectiveness of generative data augmentation using images generated in the agricultural field using image generation via GANs. EasyDAM constructs an image transformation model using an Across-Cycle GAN, which is an improved version of the CycleGAN [24] for achieving domain adaptation to different varieties. Moreover, Trabucco et al. proposed DA-Fusion [25], which uses a diffusion model of Text2img to generate images from text, improving the generalization performance of models in classification tasks. Thus, generative data augmentation methods based on the data-synthesis approach achieve domain adaptation using image-generation models, thereby significantly improving the adaptability of models to different domains. Several methods achieved domain adaptation in object detection tasks using image generation models by transforming the labeled data. However, to the best of the authors' knowledge, there is no generative data augmentation method for object detection tasks based on a data synthesis approach because the generated images require coordinate information indicating the location of the objects to be detected, and it is difficult to generate image-coordinate information pairs. Therefore, it is necessary to explore a new data-augmentation method based on a data-synthesis approach that considers domain shifts in object detection.\n2.3 IQA Metrics for Image Generation\nGenerative data augmentation using image-generation models requires high-quality images. Therefore, the image quality assessment (IQA) [26] metric is used to evaluate the quality of the generated images. The IQA metric is an indicator for quantitatively evaluating the quality of the generated images, and its calculation methods can be classified into three approaches: full-reference (FR), no-reference (NR), and distribution-based (DB) methods. The FR methods of IQA metrics evaluate the quality by directly comparing the reference and generated images. Examples of FR-IQA metrics include classical methods such as the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) [27]. Recently, IQA metrics that use deep learning have been proposed, such as LPIPS [28] and DreamSim [29]. FR-IQA metrics can be used to evaluate the quality and structural features of the generated image by comparing them with the reference image. The NR methods of IQA metrics evaluate the quality based on the generated image alone. Examples of NR-IQA metrics include total variation (TV) and blind/referenceless image spatial quality evaluator (BRISQUE) [30]. The NR-IQA metrics can evaluate quality even without a reference image because they assess quality based solely on the generated image. Distribution-based (DB) methods of the IQA metrics evaluate the quality by comparing the distribution of image features in a dataset. Examples of DB-IQA metrics include Fr\u00e9chet inception distance (FID) [31] and kernel inception distance (KID) [32]. The DB-IQA metrics can simultaneously evaluate the image quality and diversity by comparing the distribution of image features across the entire dataset. The IQA metrics make it possible to quantify the quality of the generated images. Therefore, we ensure the quality of the generated images during generative data augmentation by objectively evaluating and utilizing IQA metrics in this study."}, {"title": "3 Materials and methods", "content": "3.1 Dataset Overview\nWe prepared a custom dataset for estimating the internodal distance of shoots (young branches in viticulture) in vineyards. This dataset was created to estimate the growth of grapevines at the pre-blooming BBCH stage [33] of 57-59 by capturing videos using an unmanned ground vehicle (UGV). We aimed to quantify crop growth by extracting images from video frames and calculating the internodal distance through image recognition.\nFor object detection, annotations were defined for the BBox and keypoints. The BBox annotation consisted of only one class, \"Shoot.\" The Shoot class contains a keypoint label indicating \"node.\" The node labels were defined from node_1 to node_10 and arranged from the top to the bottom of the shoot. If there were less than ten node\n3.2 Requirements\nWe considered generative data augmentation to simultaneously address the challenges of \"insufficient training data\" and \"diversity of domains\" for employing plant phenotyping in PA using object detection models. We set the following four requirements to propose the new generative data augmentation method. The D4 method aims to address the challenge of securing training data for learning object detection models and realizing plant phenotyping in PA by meeting the following requirements.\nUtilization of video data\nIn modern agriculture, task automation in vast cultivation fields using an UGV has helped address labor shortages. UGVs that move freely throughout the cultivation fields are gaining attention as data collection equipment for PA and used for video recording and sensor data accumulation [34]. Therefore, although securing training data for image recognition in the agricultural sector is challenging, the environment for collecting cultivation data is established steadily, thereby making it easier to collect periodic video data. Therefore, we consider utilizing the vast amount of video data UGV has captured and accumulated in the cultivation fields.\nUtilizing a small number of object-annotated datasets\nIn the agricultural field, creating object-annotated datasets requires considerable effort; therefore, we cannot expect perfect and consistent object-annotated datasets. We consider learning a high-precision detection model from a minimal number of perfectly created object-annotated datasets with minimal effort.\nDomain adaptation\nIn agriculture, it is extremely difficult to create an object-annotated dataset that covers various domains. Further, transferring a training dataset between different crops with the same learning task is challenging, and therefore, we consider generating object-annotated images of different domains prepared in advance without annotating the target domain.\nPreservation of annotation information for detection models\nTraining detection models requires object coordinate information indicating the positions of the objects in addition to image data. Therefore, exploring methods for generative data augmentation that accurately preserve the relationship between the detection targets and their coordinate information is necessary. The goal is developing methods that apply not only to BBox detection but also to keypoint detection and segmentation tasks."}, {"title": "3.3 Proposed method: D4", "content": "We propose a new generative data augmentation method for object recognition tasks, called \"D4,\" based on a text-guided diffusion model. D4 is defined as text-guided diffusion model-based domain-adaptive data augmentation for object detection. The text-guided diffusion model is a multimodal model that can transform an input image into different images under arbitrary conditions based on textual information (prompts) while preserving its structure, which is an improvement over the image-to-image model. We developed a method for training data expansion and domain adaptation through a \"data synthesis\" approach in data expansion Using such an image generation model.\nFigure 4 presents the basic framework and key components of D4. D4 uses a large dataset of real images from different domains with prompts and a small annotated dataset. The prompt contains textual information indicating each image domain, and it can either be predefined fixed text or automatically generated text obtained using methods such as CLIP [35] and GPT-4V[36]. In D4, a small annotation dataset does not need to include the target domain; further, a large number of high-quality synthetic images of the target domain can be generated based only on the annotated images of other domains. This can be achieved by the two-stage fine-tuning of the text-guided diffusion model.\nIn stage 1 fine-tuning, the text-guided diffusion model is trained using unlabeled data to learn broad features for the unknown dataset that is yet to be acquired by the underlying model. Conventional domain adaptation using image-generation models corresponds to style transfer using a pretrained model from stage 1 [37] [38].\nIn stage 2 fine-tuning, an image plotted with annotation information (annotation-plotted image) is used as input to learn the local features from a small annotated dataset.\nIn the trained model in stage 2, fine-tuning the model trained in stage 1 enables generating many high-quality images for the target domain without preparing annotated images for the target domain.\nIn addition, the input is an annotation-lotted image, and therefore, adding geometric changes can increase variation, thereby enabling diverse image generation that cannot be achieved only using style transfer. Subsequently, in D4, validation based on IQA metrics is performed during both the training and generation phases of the ctext-guided diffusion model for maximizing the quality of the generated images. The images generated by generative data augmentation using D4 are of high quality, equivalent to real images, and closely resemble the target domains. This enables the generative data augmentation of D4 to achieve a high-accuracy improvement and generalization performance for training the object-recognition models.\nSubsequently, the details of pretraining the text-guided diffusion model are described in Section 3.4, details of the generative data augmentation are provided in Section 3.5, and improvement in the quality of the generated images discussed in Section 3.6."}, {"title": "3.4 Pre-training of the text-guided diffusion model", "content": "3.4.1 Overview\nD4 is realized by pre-training the foundation model of the text-guided diffusion model with a custom dataset that includes images of the target domain for which generative data augmentation is desired. The foundation model undergoes two-stage pretraining to learn both broad and local features in the proprietary dataset. Further, during pre-training, images generated by the text-guided diffusion model are verified based on the IQA metrics. The transition of the IQA metrics during the training process is monitored to select the best text-guided diffusion model that produces high-quality-generated images.\n3.4.2 Stage 1:Learning broad features\nThe first stage of pre-training (Stage 1) involves teaching the foundation model of a general-purpose text-guided diffusion model with broad image features from a custom dataset. In stage 1, the model learns to generate images that closely resemble real images from inputs comprising pairs of Canny edge images and prompts. For Figure 5(a), the text-guided diffusion model learns the domain \"vineyard,\" which was not learned by the foundation model. Consequently, the text-guided diffusion model can generate images of the target crops and cultivation fields from the custom dataset.\nA distinctive feature of stage 1 is that it does not require annotated training data with object annotations. Therefore, learning from many images extracted from video frames is possible, thereby enabling the text-guided diffusion model to acquire image features, even in domains without training data. The only requirement in stage 1 is a Canny-edge image labeled with textual information (prompts) describing each region.\nHowever, Canny edge images can be created automatically. Further, text labeling can be as simple as setting a predetermined sentence for each domain. Moreover, text labeling can be performed using CLIP [35] or GPT-4V[36] to create prompts, which are significantly less expensive than object annotation.\n3.4.3 Stage 2: Learning of local features\nThe second pre-training stage (stage 2) focuses on learning local features in the training dataset using the text-guided diffusion model trained in stage 1. In stage 2, unlike the training in stage 1, learning is conducted for generating realistic images from inputs consisting of plain images with annotated data drawn on them (annotation-lotted images) and prompts. For Figure 5(b), the text-guided diffusion model learns the coordinate information of BBox and keypoint from the annotation-plotted images derived from the training dataset, thereby enabling the text-guided diffusion model to generate objects of detection based solely on coordinate information. In addition, the learning conducted in stage 1 allows generation images from domains that were not present in the training dataset learned in stage 2."}, {"title": "3.5 Generative data augmentation by text-guided diffusion model", "content": "Figure 6 illustrates generative data augmentation via image generation using a pre-trained text-guided diffusion model. Further, pre-training the text-guided diffusion model makes it possible to shift to any domain from an annotation-plotted image based on the prompt. In the figure, an annotation-plotted image is created from nighttime image training data, and daytime images are generated through prompt \"Daytime shooting.\" Thus, employing D4 makes it possible to expand the dataset significantly under various scenarios and conditions without relying solely on limited training data or data under specific conditions. Indeed, Figure 6(a) shows the results of generating nine daytime images from a single nighttime image using D4.\nA significant feature of this technique is that an annotation-plotted image can be created, and images of any domain can be generated from a pre-trained text-guided diffusion model. Indeed, coordinating the information that matches the task of the object to be detected is necessary. However, this coordinate information can be generated automatically or created from the teaching data of different crops. In addition, although limited to this research task, it is possible to generate images of any domain using human skeletal information. As images from entirely different domains can be used to generate images of any domain, various teaching datasets can be repurposed, thereby realizing domain adaptation."}, {"title": "3.6 Automatic selection mechanism of generated images", "content": "In D4, the quality of images generated by the text-guided diffusion model is paramount. Therefore, a mechanism for automatically selecting the generated images was developed, and this automatic selection mechanism filters only high-quality images for use in generative data augmentation.\nA challenge arises when selecting the generated images. This challenge lies in the discrepancy between the domains of the original and generated images. For example, when converting a dataset of nighttime images into daytime images, comparing the quality of the images directly between different domains is problematic. DreamSim [29] is utilized to compare the distance in the feature space of the target domain dataset with the generated images to circumvent this problem, thereby facilitating the selection process through a quality comparison of the generated images. DreamSim, leveraging embedding representations created by CLIP [35] and DINO [39], is a FR-IQA metric that can capture not only foreground objects and semantic content with an emphasis, but also texture and structural features. Further, using embedded representations simplifies the identification of similar images, and therefore, DreamSim was employed as an automatic selection mechanism for the generated images because it can easily extract the most similar images from the target dataset.\nConditions for determining the generated images are given in Equation (1). Using this selection mechanism, the DreamSim score between any pair of images within the dataset is calculated, and the median is determined. The generated image is considered to be of high quality if the DreamSim score between a generated image and most similar image in the dataset is higher than the median. Only images deemed high-quality by this selection mechanism were used as augmented data, thereby ensuring a certain quality in the generated images augmented in D4.\n$Quality(D_t, I_g) = \\begin{cases} 1 & \\text{if } \\text{median}\\{DreamSim(I_i, I_j)|i, j \\in D_t, i \\neq j\\} > \\text{min}\\{DreamSim(I_g, I_k)|k \\in D_t\\} \\\\ 0 & \\text{otherwise} \\end{cases}$    (1)"}, {"title": "4 Experiments and Results", "content": "4.1 Overview of experiment\n4.1.1 Purpose\nIn our experiments, we tested the effectiveness of domain adaptation in object detection tasks using the proposed D4. Our goal was quantitatively evaluating the effectiveness of D4 in both the BBox and keypoint detection models. In these experiments, generative data augmentation was performed solely on a small dataset of nighttime images, and its effectiveness was evaluated against a daytime image test dataset. Through this experiment, we provided quantitative results demonstrating the effect of limited training generative data augmentation and domain adaptation in D4.\n4.1.2 Base datasets\nThis experiment uses a dataset for estimating the internodal distances in grapevine cultivation, as shown in Section 3.1. The dataset for training the detection model comprises 100 nighttime images, whereas the test dataset comprises 50 daytime images. We aimed to improve the accuracy of the detection model and quantitatively evaluate the effectiveness of D4 based on the accuracy of the model by implementing generative data augmentation and domain adaptation using D4.\nAn original image dataset, which did not require annotations, was prepared to implement D4. This dataset includes daytime and night-time image domains, which were adjusted to contain 10,352 images each, totaling 20,704 images. All images were resized to a uniform resolution of 512 pixels (height and width).\n4.1.3 Text-guided diffusion model\nControlNet[40], which is a multimodal image generation model based on a diffusion model founded on StableDiffusion [41], was employed as the text-guided diffusion model in the experiments. This model enhances the controllability and flexibility of image generation significantly by inputting image and text information simultaneously. In the experiments, the image was generated from coordinate information during the daytime by performing two-step pretraining with ControlNet; further, the effectiveness of D4 in the object detection models was evaluated.\n4.1.4 Prompts\nThe prompts used in the experiment combined a common prompt for all images with two different texts for the daytime and nighttime images."}, {"title": "4.2 Pre-Training of ControlNet", "content": "4.2.1 Stage 1 Pre-Training of ControlNet\nIn stage 1, we fine tune the publicly available ControlNet foundation model and train it to generate realistic images from the inputs of Canny edge images linked with prompts. We prepared four parameters with varying edge densities to create the Canny edge images, thereby allowing ControlNet to demonstrate high generalization capabilities across images with different edge densities, ultimately enabling the generation of high-quality images. In addition, the training dataset was augmented by a factor of two via horizontal flipping data augmentation. Therefore, we prepared 165,632 pairs of inputs and the corresponding images associated with prompts to train ControlNet. The training of ControlNet was conducted with a batch size of 8 and a learning rate of 3e-4. The training continued for up to 30,000 steps, with validation based on the IQA metric conducted every 100 steps. According to the validation, the model at step 18,500, which showed the best value of LPIPS [28], was used for training in stage 2.\n4.2.2 Stage 2 Pre-Training of ControlNet\nIn stage 2, the ControlNet model pre-trained in stage 1 was fine tuned, and the prompts associated with the annotation-plotted images created from object-annotated images were input to train the model for generating realistic images. As the training dataset, a set of 100 input-answer pairs was prepared by applying horizontal flipping data augmentation to 50 object-annotated nighttime images, thereby doubling the dataset. These pairs were used to train the ControlNet model with the associated prompts.\nThe ControlNet model was trained with a batch size of one and a learning rate of 5e-5. Setting the batch size and learning rate to values lower than those used for stage 1 training helped addressed the early convergence of training attributed to the extremely small amount of data in stage 2. Model validation was performed by generating images from the remaining 50 object-annotated nighttime images that were not used in the training dataset and performing validation based on IQA metrics every 50 steps. The model with 1200 steps, which showed the best value in LPIPS, was selected as the image generation model for D4."}, {"title": "4.3 Generative Data Augmentation by D4", "content": "A dataset was created for use in the experiments via D4. The dataset used for the training the detection model was augmented, and the experiments were conducted to test D4's effectiveness based on the accuracy of the detection model. A total of 100 object-annotated images captured at night were prepared as the initial dataset This dataset was divided into 50 images for training (Normal training dataset(a)) and validation (Normal validation dataset) purposes. Six training datasets were prepared for comparison.\nFirst, we prepared a dataset that was style-transferred using ControlNet trained in stage 1 (Transferred Train Dataset (b)). This dataset is a Canny edge image created from a real image and style transferred into a daytime image while preserving the composition using the model learned in stage 1. Subsequently, five training datasets were prepared by generating images through D4 and by adding them to the \"transferred training dataset(b).\" These training datasets were prepared with 50, 100, 250, 500, and 1000 images augmented through D4, and they were denoted as datasets c, d, e, f, and g, respectively. Performances of the models trained using each dataset was compared to evaluate the effects of D4.\nThe normal validation dataset was used to learn the normal training dataset (a). A transferred validation dataset, which ControlNet converted and learned in stage 1, was used to learn with the other datasets."}, {"title": "4.4 Experiments with BBox Detection Models", "content": "4.4.1 Overview\nIn the object-detection tasks, we validated the effectiveness of D4 for the BBox detection models that estimate the rectangular information of the objects to be detected. In our experiments, we evaluated three BBox detection models: YOLOv8, which specializes in high-precision, real-time BBox detection; EfficientDet, a CNN-based model capable of high-precision detection [42]; and DETR, which adopts a transformer-based architecture [43]. We confirmed the effectiveness of D4 by testing models with different architectural designs. Training was conducted with YOLOV8 for 500 epochs and the other models for 300 epochs to evaluate and compare the model that showed the highest accuracy on the validation dataset at each epoch.\nFor YOLOv8 training, a default-implemented data augmentation pipeline was applied. The existing data augmentation pipeline was applied, which included distortion, scale, color space, crop, flip, rotate, random erases [44], cutout [45], hideandseek [46], Grid Mask [47], Mixup [48], CutMix [49], and mosaics [50]. Thirteen types of existing data augmentation techniques were applied automatically during training. EfficientDet/DETR was trained without applying existing data augmentation methods."}, {"title": "4.4.2 Results", "content": "In our experiments, we applied three types of BBox detection models across seven different datasets and conducted training under 21 different conditions. For each condition, we ensured the reliability of the results by conducting training at least five times with various seed values and by calculating the average and standard deviation of the evaluation metrics. We used the mean average precision (mAP) based on loU thresholds as an evaluation metric to assess the effectiveness of D4 quantitatively. The experimental results of the BBox detection models are listed in Table Table 1. Three main trends are evident from the results.\nFirst, the effectiveness of D4 is discussed. Datasets c, d, e, f, and g augmented using D4 demonstrated an improvement in the detection accuracy across all BBox detection models compared with that of the the regular training dataset (a). This implies the success of domain adaptation from nighttime to daytime images through D4.\nSecond, the improvement in accuracy is associated with an increase in the number of data augmentation images because of D4. In every BBox detection model, with an increase in the number of images in the dataset, the highest detection accuracy was observed for the dataset of 1050 images (g) in terms of mAP50.\nIn the YOLOv8 results, the 1050 image dataset (g) achieved an improvement in accuracy of 13.2% in mAP and 27.69% in mAP50 compared to the regular training dataset (a). In contrast, for the mAP values of EfficientDet, no improvement in accuracy was observed with an increase in the number of augmented images. In all BBox detection models, no significant improvement in accuracy was observed in the datasets with more than 300 images (e, f, g). This suggests that the effect of D4 may saturate beyond a certain number of augmented images. Therefore, selecting high-quality images and enhancing their diversity are considered more important than indiscriminately adding data.\nThird, improvement in accuracy when the existing data augmentation methods of the data transformation approach are applied simultaneously. A difference in the range of accuracy improvement was observed between YOLOV8, where the existing data augmentation methods of the data transformation approach were applied simultaneously, and EfficientDet and DETR were not applied. When comparing the difference in the mAP50 accuracy between the regular training dataset (a) and dataset comprising 1050 images (g), EfficientDet achieved a 22.54% improvement in accuracy, DETR achieved a 15.99% improvement, and YOLOv8 achieved a 27.69% improvement. Therefore, the combination of D4 and the existing data augmentation methods of the data transformation approach suggests the possibility of further accuracy improvement."}, {"title": "4.5 Experiments with Keypoint Detection Models", "content": "4.5.1 Overview\nIn the field of object detection tasks, we validated the effectiveness of D4 for keypoint detection models that estimate the coordinate information of the points of the detected objects. In our experiments, we evaluated three types of keypoint detection models based on a top-down approach. The top-down approach was adopted because the nodes of the plants have similar features, making detection difficult using the bottom-up approach. The models compared were YOLOv8_Pose, which is CNN-based and specializes in real-time detection; HR-Net[51], which is CNN-based and capable of high-accuracy detection; and ViT-Pose[52], which adopts a Transformer-based architecture. We verified the effectiveness of D4 by testing models with different architectural designs. Training was conducted for 500 epochs with YOLOV8_Pose and 300 epochs with the other models to evaluate and compare the best model that showed the highest accuracy in each epoch against the validation dataset.\nThe implemented default data augmentation pipeline was applied to train YOLOv8_Pose. In contrast, for HR-Net and VIT-Pose, training was conducted without applying the existing data augmentation methods.\n4.5.2 Results\nThree types of keypoint detection models were applied to the seven different datasets. The training was conducted under 21 different conditions using these combinations. For each condition, we ensured the reliability of the results by conducting training at least five times with various seed values and calculating the average and standard deviation of the evaluation metrics.\nThe evaluation metric utilizes the mean average precision (mAP) based on the object keypoint similarity (OKS) threshold to assess the efficacy of D4 quantitatively. The formula for calculating OKS is given in Equation (2). In this formula, N represents the number of keypoint classes, and $d_i$ indicates the Euclidean distance between the predicted and the actual keypoints. The scale of the object is denoted by s, and $k_i$ represents a unique constant that indicates the significance of each keypoint class. Furthermore, $\\delta(v_i > 0)$ represents an indicator function that equals 1 if keypoint i is visible and O otherwise.\n$OKS = \\frac{\\sum_i exp(-d_i^2 / 2s^2k_i^2)\\delta(v_i > 0)}{\\sum_i \\delta(v_i > 0)}$       (2)\nThe experimental results for the keypoint detection models are presented in Table 2. Three main trends are evident from the results.\nFirst, the effectiveness of the D4. Similar to the experimental results of the BBox detection model, datasets c, d, e, f, and g augmented using D4 showed an improvement in detection accuracy across all keypoint detection models. Therefore, the effectiveness of D4 is suggested for both the BBox and keypoint detection models.\nSecond, the improvement in accuracy associated with an increase in the number of data augmentation images owing to D4. Similar to the experimental results of the BBox detection model, we observed an improvement in the accuracy of the object detection model with an increase in the number of images in the dataset. However, for HR-Net, we confirmed that the evaluation scores of mAP and mAP50 were reversed between datasets with a total of 550 (f) and 1050 images (g). This result suggests that, similar to the experimental outcomes of the BBox detection model, there is a limit to the number of images that can be augmented using D4. Thus, it is necessary to consider the quality of the generated images.\nThird, the results for the ViT-Pose. In the evaluation of ViT-Pose, a significant improvement in accuracy was observed for a dataset with 1050 images (g) based on the mAP50 metric. However, this was presumed to be caused by the inherent characteristics of the ViT model because ViT models exhibit significantly lower accuracy with extremely small amounts of data [53]. Therefore, D4, by increasing the amount of data, could have contributed to the performance improvement of the ViT model."}, {"title": "5 Discussion", "content": "5.1 Qualitative evaluation of generated images\nThe validity of images generated using D4 was assessed qualitatively based on human perceptual judgment. In these poor-quality images, there are instances where the detected object and annotation information do not match. In addition, we identified the generated images with unnatural backgrounds and images in which the"}]}