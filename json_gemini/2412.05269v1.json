{"title": "Chimera: Accurate retrosynthesis prediction\nby ensembling models with diverse inductive biases", "authors": ["Krzysztof Maziarz", "Guoqing Liu (\u5218\u56fd\u5e86)", "Hubert Misztela", "Aleksei Kornev", "Piotr Gai\u0144ski", "Holger Hoefling", "Mike Fortunato", "Rishi Gupta", "Marwin Segler"], "abstract": "Planning and conducting chemical syntheses remains\na major bottleneck in the discovery of functional\nsmall molecules, and prevents fully leveraging genera-\ntive AI for molecular inverse design. While early work\nhas shown that ML-based retrosynthesis models can\npredict reasonable routes, their low accuracy for less\nfrequent, yet important reactions has been pointed\nout. As multi-step search algorithms are limited to\nreactions suggested by the underlying model, the ap-\nplicability of those tools is inherently constrained by\nthe accuracy of retrosynthesis prediction. Inspired by\nhow chemists use different strategies to ideate reac-\ntions, we propose Chimera: a framework for building\nhighly accurate reaction models that combine predic-\ntions from diverse sources with complementary induc-\ntive biases using a learning-based ensembling strat-\negy. We instantiate the framework with two newly de-\nveloped models, which already by themselves achieve\nstate of the art in their categories. Through experi-\nments across several orders of magnitude in data scale\nand time-splits, we show Chimera outperforms all ma-\njor models by a large margin, owing both to the good\nindividual performance of its constituents, but also\nto the scalability of our ensembling strategy. More-\nover, we find that PhD-level organic chemists prefer\npredictions from Chimera over baselines in terms of\nquality. Finally, we transfer the largest-scale check-\npoint to an internal dataset from a major pharmaceu-\ntical company, showing robust generalization under\ndistribution shift. With the new dimension that our\nframework unlocks, we anticipate further acceleration\nin the development of even more accurate models.", "sections": [{"title": "Introduction", "content": "Chemical Synthesis is central to the discovery and\nsupply of small molecule-based medicines, materi-\nals, and fine chemicals. However, as syntheses often\nfail, and thus constitute a critical bottleneck, using\ncomputational methods to propose better synthesis\nroutes is highly desirable. Computer-aided synthesis\nplanning has a long research tradition, going back to\nthe vision of Vleduts and Corey in the 1950ies, with\ntools usually implemented via rule-based expert sys-\ntems.1,2 However, over several decades progress had\nbeen limited. Only recently, by reframing synthesis\nplanning as a machine learning (ML) problem, where\ndeep neural networks are trained on large reaction\ndatasets to predict synthetic disconnections and re-\naction outcomes, which are then coupled with neural-\nguided search, a paradigm shift has been achieved.4,5\nSince then, several new ML models6-14 and search\nalgorithms15-18 have been introduced. Incorporated\ninto tools for retrosynthetic search, which are increas-\ningly used in computational workflows and as a source\nof inspiration for route planning, ML-based synthesis\nplanning has been experimentally validated.19\u201321\nConcurrently, Corey's expert system approach of\nmanually coding reaction rules has been reimple-\nmented22, 23 by Szymkuc and coworkers, and exper-\nimentally validated on hand-selected test cases.24,25\nWhile conceptually ML-based synthesis planning\npromises viable scaling with the ever-growing body\nof knowledge of organic chemistry in the literature,\npatents, and electronic laboratory notebooks, so far,\ncompared to hand-coded expert systems, ML-based\nplanning suffered from limited accuracy in particular\nfor rare reaction classes. In addition, chemists of-\nten combine multiple strategies, from direct pattern\nmatching to envisioning new transformations, which\ncontemporary computational approaches currently do\nnot reflect.\nIn this work, we introduce Chimera, a framework\nfor retrosynthesis prediction that uses an ensemble of\nmodels with diverse inductive biases and a learning-\nto-rank strategy. Instantiated with two newly in-\ntroduced state-of-the-art models \u2013 one based on edit\nrules and one on de-novo generation using a modern\nTransformer \u2013 we achieve high accuracy on common"}, {"title": "Computer-Aided Synthesis Planning", "content": "Systems for Computer-Aided Synthesis Planning usu-\nally perform retrosynthesis, i.e. predicting transfor-\nmations which correspond to reverse chemical reac-\ntions starting with the target molecule, and have\nfour components: (1) a single-step model or algo-\nrithm to propose transformations that correspond\nto feasible reactions in the forward direction, (2) a\nsearch algorithm that chains together transformations\ninto multi-step routes, (3) ranking criteria for the\nroutes, and (4) admissible building block molecules\ninto which the target has to be deconstructed.26, 27\nThus, an accurate single-step model is crucial as it de-\nfines the search space of possible reactions to explore.\nAs the model is called recursively during search, the\nrequirements for accuracy are very strict, as errors\ncompound with multiple steps, and a single error will\ninvalidate the entire route.\nCurrent single-step models can be classified into\nediting models, which change only the parts of the\nmolecule involved in the reaction, e.g. make or break\nbonds and add leaving groups, or de-novo models,\nwhich generate the reactant structures from scratch,\nincluding regeneration of the unchanged parts. While\nin recent years several models have been proposed,\nhigh accuracy still poses a significant challenge, espe-\ncially for reaction types of lower precedence. 6\u201313, 28\u201334\nHowever, rarer reactions are often highly specific and\nstrategically useful."}, {"title": "Ensembling", "content": "Model ensembling is an ML technique where several\nmodels trained to perform the same task are com-\nbined to obtain better performance than any of the\nensembled models would in isolation.35 It is believed\nto work best when the models being combined are\ndiverse.36 While ensembling for reaction prediction\nand retrosynthesis has been attempted, results have\nbeen limited so far. Schwaller et al.7 ensemble up to\n20 forward reaction models, but report only minimal\ngains at the cost of a significant increase in inference\ntime. However, they employ the default method in\nOpenNMT, 37 which averages next token probability\ndistributions predicted by the different models, and\nis limited to models sharing the same output space.\nCombinations of models have been reported with\nspecialized models for ring-forming reactions38 or en-\nzymatic catalysis. 39,40 Lin et al.41 combine outputs\nfrom different models, but determining the final or-\nder relies on a separately trained ranking model, ef-\nfectively discarding the rich information present in\nthe order predicted by the original models. Torren-\nPeraire observed differences in the solutions different\nsingle-step models find.42 In a recent paper by Sai-\ngiridharan et al., it was explicitly pointed out that\nwhile different models have been combined ad-hoc,42\nno principled ensembling approach is available.43\nIn retrosynthesis prediction, instead of directly en-\nsembling in token probability space, we can also per-\nform count-based ensembling in the molecule space\nby aggregating outputs shared by ensembled models,\nwhich we hypothesize to be more expressive. More-\nover, count-based ensembling is much more versatile,\nas it can ensemble any set of models, as well as other\nnon-model sources of reactions; for example, it would\nallow to mix in proposals coming from lookups in re-\naction databases, or manual input from chemists."}, {"title": "Proposed ensembling strategy", "content": "We propose a\nstrategy to merge several output ranking lists based\non overlaps between them, which for the first time\nleads to substantial gains over the ensembled models.\nGiven ranked outputs ri,k from m retrosynthesis\nmodels, where ri,k denotes the k-th top prediction\nfrom the i-th model, we rank unique reactant sets r\nby decreasing score(r) defined as\n$$score(r) = \\Sigma_{i=1}^{m}\\Sigma_{k=1}^{k_{max}} \\mathbb{1} [r = r_{i,k}] \\cdot \\theta_{i,k},$$\nwhere kmax is the maximum number of predictions\nconsidered per model and \u03b8 \u2208 Rm\u00d7kmax; we omit the\ndependence of score and losses defined below on \u03b8\nfor clarity. In other words, a reactant set occurring\non rank k in the output of model i is assigned score\n\u03b8i,k, with the scores aggregated over the models. In-\ntuitively, reactant sets that rank high across several\nmodels simultaneously will be assigned a larger score\nthan those suggested only by a single model.\nTo determine ensembling weights \u03b8 we propose to\nlearn them from model predictions on the validation\nset Dual. Inspired by work on learning to rank, 44 we\nlearn \u03b8 by minimizing the following ranking loss\n$$L_{rank} = E_{(r^{+},r^{-})\\sim Dual} \\Sigma_{r^{-} \\in R^{-}} L_{rank} (r^{+},r^{-}),$$\nwhere R\u00af = {ri,k : ri,k \u2260 r+} is the set of model\npredictions differing from the ground-truth r\u207a. Loss\nstemming from a particular (r+,r\u00af) pair is given by\n$$L_{rank}(r^{+},r^{-}) = \\sigma\\left(\\frac{score(r^{-}) - score(r^{+})}{\\tau} + \\epsilon \\right),$$\nwhere \u03c3 is a constant. For \u03f5, \u03c4 \u2192 0,\nLrank(r+,r\u00af) \u2192 1[score(r\u00af) > score(r+)], i.e. in-\ndicator of whether r+ and r\u00af are ordered incorrectly.\nIn the limit Lrank has zero gradients almost every-\nwhere, thus in practice we start with \u03c4 > 0 and lin-\nearly anneal it to 0 over the course of optimization.\nOne could minimize Lrank directly, but small val-\nidation set size and poor coverage of cases where r+\nappears at higher ranks lead to overfitting and poor\ngeneralization. To fix this, we constrain \u03b8 to be de-\ncreasing and convex (\u03b8i,k > \u03b8i,k+1 and \u03b8i,k \u2212 \u03b8i,k+1 >\n\u03b8i,k+1 \u2212 \u03b8i,k+2), expressing the intuition that later\nranks are less likely to be correct, and differences be-\ntween ranks are more pronounced closer to the top.\nIn the experiments we optimize \u03b8 on the valida-\ntion set of the considered dataset and then evaluate\non the test set; see Methods for more details and hy-\nperparameters of this procedure. We find that our\nstrategy consistently outperforms other approaches,\nand learns non-trivial schemes where relative model\nimportance depends on k"}, {"title": "Ensembling public models on USPTO-50K", "content": "To test our ensembling strategy, we consider 7 ret-\nrosynthesis models available as part of syntheseus:45\nChemformer, 46 GLN,8 Graph2Edits,47 LocalRetro,\nMEGAN, 29 RetroKNN12 and R-SMILES.10 We ad-\nditionally retrain R-SMILES \u2013 the best performing\nmodel in this set \u2013 to study the effect of ensem-\nbling two instances of the same model; we refer to\nit as R-SMILES'. We also include our reimplemen-\ntation of a template classification model, which has\nin later literature been referred to as NeuralSym. 4\nWe focus on m = 2 and study the results of all\npairwise model combinations (Extended Data Fig-\nure 7). Remarkably, ensembling any model pair re-\nsults in performance better than attained by either\nmodel in isolation. This is true even when combining\na strong model with a much weaker one: for exam-\nple, top-5 accuracy of R-SMILES can be improved by\n0.9% by ensembling with MEGAN, and 1.5% by en-\nsembling with GLN, despite both being significantly\nweaker than R-SMILES. We also note the emergence\nof model clusters which show only a small benefit from\nbeing combined: NeuralSym and GLN (both based\non global prediction of standard reaction templates),\nLocalRetro and RetroKNN (based on local prediction\nof minimal templates), and the two checkpoints of R-\nSMILES. This suggests diversity is key to strong per-\nformance of the ensemble; consequently, best result\nfor a particular top-k is not obtained by ensembling\nthe two top performers on that metric, but rather the\nbest model with one belonging to a different cluster,\ne.g. the de-novo R-SMILES model with an editing\nmodel such as LocalRetro.\nThis motivates us to propose two new models \u2013\none based on molecule editing and one on de-novo\ngeneration \u2013 and investigate the performance of their\nensemble at scale. Prior work often considers model\nensembles as incomparable to individual models due\nto higher computational cost, but we challenge this\nassumption by noting that ensembling a fast editing\nmodel with a de-novo Transformer can lead to only a\nnegligible increase in inference cost over the latter. In\nthe following sections, we introduce our models and\nbenchmark them at increasing data scales.\nEnsembles discussed above already set a new\nstate of the art on USPTO-50K, even outperforming\nmodel-reranker combinations.41 However, in the fol-\nlowing sections we show even better performance by\nutilizing our proposed models and larger ensembles."}, {"title": "Model architecture", "content": "We instantiate Chimera as an ensemble of two sepa-\nrately trained models \u2013 one based on molecule editing\nand one on de-novo generation \u2013 each designed to ad-\ndress specific limitations in their respective modeling\nclasses. As the edit-based model can be implemented\nvery efficiently, Chimera delivers inference cost com-\nparable to a single de-novo model such as R-SMILES,"}, {"title": "Editing Model (NeuralLoc)", "content": "Molecule-editing models are gen-\nerally believed to stay closer to the data distribution\ndue to reliance on symbolic transformations that have\nsupport in the training data, especially when the ed-\nits are limited to stricter reaction rules or templates.\nEven though they were the first ML-based retrosyn-\nthesis model, template classification continues to be\na default choice in commonly used workflows. How-\never, two limitations hinder these models at scale:\n(1) weights responsible for choosing the template are\nusually treated as free parameters, precluding repre-\nsentational transfer between templates; and (2) ap-\nplying a selected template can produce more than\none prediction due to multiple locations in the input\nmolecule matching its left-hand side, and these alter-\nnatives are often not differentiated. Prior work has\nexplored partial solutions to these challenges: (1) by\nusing a template encoder to handle uncommon tem-\nplates through transfer from similar ones;48 and (2)\nby separately predicting the reaction centre to con-\nstrain template match 8,9,49 or by introducing a sepa-\nrate module to rank the final reactant sets. However,\nnarrowing the template application to a chosen reac-\ntion centre may not be enough to uniquely specify the\nreactants in case of symmetries (Figure 2c).\nInspired by these works we design a new template"}, {"title": "Classification objective", "content": "Apart from an encoder for the input product\nas in NeuralSym, NeuralLoc also contains a separate\ntemplate encoder; unlike MHNreact, 48 this encoder\ndirectly processes the template as a graph using a\nspecialized featurization method for graph rewrites\n(Methods). Our model produces template probabili-\nties by multiplying template and product representa-\ntions, which are input to a standard cross-entropy\nclassification loss. To predict where the template\nneeds to be applied, we utilize node-level represen-\ntations of the atoms in the product pattern of the\ntemplate, and perform an all-pairs dot product with\natom-level representations for the input molecule; in-\ntuitively, these products predict the likelihood that\na particular atom in the product pattern should be\nmatched to a particular atom in the input. We de-\nnote the result of normalizing these values via softmax\nacross the input atoms as localization scores; these\nscores are input to a localization loss which sums ap-\npropriate cross-entropy losses over the product pat-\ntern atoms. NeuralLoc is trained end-to-end to mini-\nmize the sum of classification and localization losses;\nduring inference we first call the classi-\nfication branch to select a number of top templates,\nand then compute the localization scores for those\nto globally rank the resulting reactants based on a\nweighted sum of both objectives; see Methods for ar-\nchitectural details, hyperparameters, and description\nof model training and inference."}, {"title": "De-Novo Model", "content": "To develop our new de-novo\nmodel, we build upon the Seq2Seq framework pio-\nneered by Liu et al,6 and the recently successful R-\nSMILES model, 7,10 which utilizes an aligned SMILES\nformat to represent input products and ground-truth\nreactants. This involves training an encoder-decoder\nmodel based on the Transformer architecture. 50\u201352\nUnlike previous work relying on the OpenNMT li-\nbrary, 37 we employ an improved architecture with\nthree modifications. First, we use Group-Query At-\ntention (GQA)53 instead of standard multi-head at-\ntention, which reduces computational complexity by\nsharing query vectors within groups. Second, we ap-\nply pre-normalization using RMSNorm,54 which is\nsimpler and more efficient than LayerNorm. Third,\nin the feedforward layers, we incorporate the SwiGLU\nactivation55 in place of ReLU, enhancing expressivity\nand improving gradient flow. These modifications in-\ncrease the model's accuracy and inference speed. The\nmodel is trained by minimizing cross-entropy loss on\nreactant sequences. Finally, we also refined the beam\nsearch algorithm. Unlike OpenNMT, which keeps\ncompleted sequences until two conditions are met \u2013\nthe pool size equals the beam size and the top-rated\nsequence in the beam is lower in quality than all in\nthe pool \u2013 our method maintains finished sequences in\nthe beam. The process ends only when each sequence\nin the beam finishes with the EOS token. We refer to\nour updated model as R-SMILES 2; see Methods for more details."}, {"title": "Results on reaction prediction", "content": "To investigate the performance of our framework and\nmodels, we perform initial ensembling experiments on\nUSPTO-50K \u2013 a commonly used small benchmark\ndataset \u2013 and then scale up to the largest available\npublic dataset and better curated in-house datasets."}, {"title": "USPTO", "content": "For an initial comparison on public data\nwe use established USPTO-50K and USPTO-FULL\ndatasets preprocessed by prior work. We follow\nbest evaluation practices 45 and use syntheseus to\nbenchmark our models as well as those baselines\nthat are integrated into the library. We selected the\nbaselines to include best performing methods while\navoiding juxtaposing results obtained on different\ndataset versions which was often done in prior work;\nsee Methods for further discussion.\nWe find that NeuralLoc and R-SMILES 2 generally\nmatch or surpass the state of the art within their own\nmodel classes, while Chimera performs better than\neither and sets new state of the art fork > 1 on both\nUSPTO-50K and USPTO-FULL, pushing the top-10\naccuracy by 1.7% and 1.6%, respectively (Extended\nData Figure 9a, Extended Data Tables 1 and 2). To\ntest the scaling of our ensembling strategy, we also\nevaluated an ensemble containing both our proposed\nmodels and most of the baselines, and found it pushes\nthe state of the art even further, although it may\nnot be practical due to excessive resources required\nfor inference. Nevertheless, these results may inspire\nfuture work to distill the knowledge present in a large\nensemble back into a single model.\nTo obtain a good trade-off between resource re-\nquirements and accuracy, we limit the following ex-\nperiments to ensembling pairs of models, and scale\nChimera to larger and more diverse datasets."}, {"title": "Pistachio", "content": "We scale our models to the proprietary\nPistachio dataset, which is better curated and repre-\nsents more than a 3.5x increase in number of samples\ncompared to USPTO-FULL. We use the data pre-\npared by Maziarz et al,45 where reactions present in\nthe database as of June 2023 were grouped by prod-\nuct and randomly split into three folds. We reuse the\ntraining and validation sets, and build a new test set\nrelying on time-split. Specifically, we use reactions\nthat were added to Pistachio in 2024, were marked\nas high quality by the database curator, and whose\nthe product had fingerprint similarity to a training prod-\nuct below 0.95 (see Methods for details). This gave\nrise to a high quality test set of 146 393 reactions both\ntemporally and structurally separate from the data\nseen by the models during training and validation;\nwe use it as our default test set and defer the results\non the original test set to Extended Data Figure 9. As\nthere are no published results on this version of Pis-\ntachio, we also train and evaluate selected baselines\n(LocalRetro, R-SMILES, NeuralSym).\nSimilarly to the results on USPTO, our models es-\ntablish state-of-the-art performance within their re-\nspective model classes . Chimera matches\nthe robust performance of R-SMILES 2 for small k\nwhile delivering much stronger results for larger k\ndue to the pooling of diverse inductive biases from\nits constituents. With only 10 of its top results,\nChimera reaches the accuracy of considering 50 re-\nsults from the best baseline R-SMILES.\nTo further understand the strengths of the individ-\nual models, we analysed top-50 recall as a function\nof fingerprint similarity to training data, as well as\nfrequency of the ground-truth template (Figure 3b,\nsee Methods for details). All models perform better\non reactions more similar to training data, or those\nutilizing more common templates. When moving far-\nther from training data, de-novo models degrade less"}, {"title": "Reaction quality", "content": "Accuracy only tests how well a\ngiven model can recall the ground-truth reactions, but\nnot whether its non-ground-truth predictions are rea-\nsonable, which is arguably more important for multi-\nstep search.45 To assess how feasible model outputs\nare overall, it is common to either feed the predicted\nreactants to a forward model to measure round-trip\naccuracy, 9,57 or feed entire reactions to a feasibility\nmodel.14 In general, feasibility models are preferred\nas those are typically trained with both positive and\nnegative reactions, and can handle cases where the re-\nactants would not react.45 For our analysis we explore\nboth routes: we trained a forward model based on\nthe R-SMILES 2 architecture and a feasibility model\nbased on the approach of Gai\u0144ski et al.14 Both mod-\nels were trained on Pistachio; see Methods for details.\nAnalysing quality of k top predictions can be con-\nfounded by some models having higher top-k accu-\nracy, while others returning less than k outputs alto-\ngether. To study the quality of non-ground-truth pre-\ndictions directly, we filter the test products to those\nwhere all compared models return at least k outputs\nand recover the ground-truth answer within that; af-\nter removing the ground-truths from the output lists,\nwe obtain k-1 non-ground-truth predictions for each\ninput, which are fed into subsequent analysis.\nWe set k = 10 and filter the Pistachio test set\ndown to 113135 products (\u2248 66.7%) according to\nthe aforementioned criteria, with 9 non-ground-truth\npredictions associated with each. We then run both\nquality assessment models on the ground-truth reac-\ntions for those products, and calibrate so that each\naccepts around 95% of ground-truths; for the forward\nmodel this translates to accepting a reaction if its\nproduct is within top 2 predicted products given the\nreactants, while for the feasibility model if the pre-\ndicted feasibility is above 0.1. With these thresholds\nwe use the models as binary filters, and compute av-\nerage acceptance rate for each reaction model and\nrank. Interestingly, we see the two scor-\ning models partially disagree in their ranking of the\ndifferent reaction models: while both the forward and\nfeasibility model consider Chimera of higher quality\nthan R-SMILES 2, the forward model judges Neural-\nLoc much more highly. This highlights that while the\ntwo approaches to scoring correctly distinguish gen-\nerated predictions from ground-truths, they do so by\nleveraging disparate heuristics."}, {"title": "Results on multi-step search", "content": "To benchmark Chimera in multi-step\nsearch we integrate our models into syntheseus, and\nstart with an initial exploration of success rate on a\ndataset collected by Li et al.58 We reuse the exper-\nimental setup from SimpRetro, including the choice\nof the search algorithm, building blocks (23.1M com-\nmercially available molecules from eMolecules), GPU\ntype, and time limit. We consistently see higher suc-\ncess rates than SimpRetro, with Chimera also outper-\nforming its constituent models, and obtaining close\nto 100% solve rate under the largest time limit (Fig-\nure 3d). However, the creation of the SimpRetro test\nset did not control for similarity to Pistachio training\ndata. To supplement this initial analysis, we move to\na harder dataset of targets based on Pistachio."}, {"title": "Pistachio", "content": "To collect a challenging search dataset\nsufficiently distinct from training data, we started\nwith Pistachio test products and performed a se-\nquence of filtering and clustering steps. Specifically,\nwe kept molecules with high SAScore, 59 filtered out\noutliers, selected a diverse subset based on fingerprint\nsimilarity, and finally removed molecules for which a\nshort search using NeuralSym could find any routes\n(see Methods for details). This procedure left us with\n951 hard targets which we split into 151 for validation\nand 800 for testing."}, {"title": "Qualitative analysis", "content": "In order to understand the complementary strengths\nof our proposed models, as well as how ensembling\nmanages to improve upon their results, we run quali-\ntative analyses using the models trained on Pistachio."}, {"title": "Quality assessment by experts", "content": "To measure the\nquality of model predictions, we conducted double-\nblind AB-tests between models. Here, predictions for\nthe same target molecule from two models were pre-\nsented to PhD-level organic chemists, who were asked\nto express preference for one of the options.\nWe determine model scores using a Bradley-Terry\nmodel and scale them to form ELO scores .\nInterestingly, NeuralLoc and R-SMILES 2 exhibit\nsimilar quality, while Chimera is clearly preferred. As\na control, we employed a baseline which naively ap-\nplies only uncommon reaction templates without any\nML ranking; we normalized the ELO ratings so that\nthis model has rating 0 see Methods for details and\nExtended Data Figure 10 for all pairwise win rates)."}, {"title": "Ensembling visualization", "content": "To visualize what er-\nrors are being made by our models and how ensem-\nbling helps to mitigate them, we used an early ver-\nsion of the feasibility model with a low acceptance\nthreshold to mine unlikely predictions on Pistachio\ntest data. In a selected example (Figure 5), we see\nthat while all models correctly predict the ground-\ntruth as their first prediction, they diverge further\ndown the list. In particular, the 5th output from R-\nSMILES 2 is an erroneous version of the ground truth,\nwhere one of the rings has been turned aromatic. As\nthis is chemically implausible and not covered by tem-\nplate library, it is not predicted by NeuralLoc, and\nthus downweighed in Chimera's outputs in favour of\npredictions shared by both submodels. Note that the\nunlikely prediction still appears in Chimera's output;\nwhile in this case it may seem undesirable, many pre-\ndictions made only by R-SMILES 2 turn out to be\ncorrect, which is reflected in the ensembling weights\n\u03b8. Our ensembling formalism permits a solution in\nwhich outputs predicted by both models are ranked\nabove those predicted by only one of the models, but\nthis is not selected by the optimization procedure due\nto being empirically inferior on the validation set.\nAfter further analysis, we found more instances of\nmistakes made by R-SMILES 2: copy errors , implausible bond-breaking\nreactions, and duplicat-\ning one of the reactants . These cases are ranked lower by Chimera. However,\nnote that these examples were purposefully cherry-\npicked to be erroneous; our models generally make\nqualitatively reasonable predictions for most inputs."}, {"title": "Conclusion", "content": "In this work, we introduced a meta-framework for\nbuilding more powerful retrosynthesis models by en-\nsembling. Instantiated with two new models exhibit-\ning favorable performance in their own categories, we\ndemonstrated Chimera's efficacy on commonly used\ndatasets. Through time-split experiments, we pro-\nvided key insight into the strengths of different model\nclasses. For the first time, we have demonstrated\nclose to optimal retrieval for rare reaction classes, and\nshown that the ensemble is preferred over single mod-\nels. In search experiments on both existing and new\nbenchmarks, we validated that Chimera's strong per-\nformance carries over to multi-step search.\nWe hope that our results open up ensembling\nstrategies as a new dimension of optimization, and\nalso inspire new developments using the latest\nprogress in Transformers, as well as more powerful\nrepresentation learning for chemical transformations,\nto achieve the goal of even more accurate retrosyn-\nthesis models in the future."}, {"title": "Code Availability", "content": "Source code and models are currently being prepared\nfor release."}]}