{"title": "Attribution for Enhanced Explanation with Transferable Adversarial eXploration", "authors": ["Zhiyu Zhu", "Jiayu Zhang", "Zhibo Jin", "Huaming Chen", "Jianlong Zhou", "Fang Chen"], "abstract": "The interpretability of deep neural networks is crucial for understanding model decisions in various applications, including computer vision. AttEXplore++, an advanced framework built upon AttEXplore, enhances attribution by incorporating transferable adversarial attack methods such as MIG and GRA, significantly improving the accuracy and robustness of model explanations. We conduct extensive experiments on five models, including CNNs (Inception-v3, ResNet-50, VGG16) and vision transformers (MaxViT-T, ViT-B/16), using the ImageNet dataset. Our method achieves an average performance improvement of 7.57% over AttEXplore and 32.62% compared to other state-of-the-art interpretability algorithms. Using insertion and deletion scores as evaluation metrics, we show that adversarial transferability plays a vital role in enhancing attribution results. Furthermore, we explore the impact of randomness, perturbation rate, noise amplitude, and diversity probability on attribution performance, demonstrating that AttEXplore++ provides more stable and reliable explanations across various models. We release our code at: https://anonymous.4open.science/r/ATTEXPLOREP-8435/", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the widespread application of Deep Neural Networks (DNNs) in critical fields such as medical diagnostics, autonomous driving, and financial forecasting, the interpretability of their decision-making processes has become an essential research direction [1], [2], [3]. Although DNN models demonstrate excellent performance across various complex tasks, their black-box nature limits our understanding of their internal workings [4], [5], [6]. This lack of transparency not only hinders users' trust in model decisions but also complicates the evaluation and correction of models in real-world applications [7], particularly in domains with high security and fairness requirements [8].\nThe goal of interpretability methods is to enhance the transparency of DNNs by revealing how the models derive decisions from input features [9]. Among these, attribution methods have garnered significant attention for their ability to provide fine-grained explanations on a per-feature basis [10]. Attribution methods calculate the contribution of each input feature to the model's output, generating visual heatmaps that clearly show which features are most important for the model's decisions [11], [12]. These methods not only help understand the model's decision process but also identify potential biases or vulnerabilities [13].\nAt the same time, adversarial attacks play an important role in interpretability research. By applying small perturbations to model inputs, adversarial attacks can approximate and explore the model's decision boundaries [14], [15]. Adversarial examples often reveal which features are critical in the model's decision-making process because adversarial attacks aim to identify and exploit weaknesses in the model [16], [17]. These vulnerabilities frequently correspond to the instability of the model in certain feature dimensions, making adversarial examples a valuable tool for identifying the most sensitive features in the model's decisions [18]. This capability positions adversarial attacks as an effective means of exploring and enhancing DNN interpretability, providing a unique perspective into the internal mechanisms of models [19].\nCurrent research has proposed the AttEXplore [20] method, which focuses on enhancing the interpretability of attribution methods through transferability. AttEXplore leverages a mechanism of transferability across models and tasks, enabling it to not only identify important features in the source model but also maintain the interpretability and stability of these features in the target model. By combining attribution analysis with transfer learning, AttEXplore aims to reveal feature importance across different environments, expanding the applicability of interpretability methods. AttEXplore explores both the frequency and spatial domains of input features to generate transferable examples that help in understanding the features. These examples not only adjust model parameters but also maintain the validity and stability of features when transferred to new models or tasks. However, AttEXplore mainly relies on a limited number of adversarial attack methods and does not fully utilize more extensive and newer methods of adversarial transferability. Furthermore, AttEXplore does not deeply consider the impact of randomness in the attribution process, which could lead to issues in the stability and reliability of the results.\nWe further extend AttEXplore and propose AttEXplore+, which explores various types of transferable adversarial attack methods, addressing the limitations of AttEXplore in terms of attack diversity and stability. AttEXplore+ integrates 10 methods from three major categories of transferable adversarial attacks, significantly broadening and deepening the exploration of model decision boundaries. These methods include Gradient Editing, Semantic Similarity, Target Modification, and other attack types, generating a broader range of adversarial examples to provide more detailed information for attribution analysis. Through comparative experiments, we observe that different attack types vary in their effectiveness in explaining feature importance in models, thus demonstrating the value of diverse attacks in improving interpretability.\nMoreover, AttEXplore+ expands not only the types of attacks but also specifically addresses the impact of randomness on attribution results. Through more in-depth ablation studies, we investigate the role of different levels of randomness in generating adversarial examples and performing attribution analysis. We explore how to leverage and control randomness to enhance the robustness and reliability of results. These experiments show that introducing an appropriate level of randomness can effectively improve the transferability and stability of attribution methods across different models and tasks.\nThe main contributions and research questions of this study include:\n\u2022 How do different adversarial transfer methods affect the performance of AttEXplore+? We explore the effectiveness of various adversarial methods in revealing model decisions and compare their performance in attribution results through analysis and experiments.\n\u2022 What role do the parameters of large-scale adversarial attack methods play in the stability and reliability of adversarial attribution results? We examine the impact of parameters such as perturbation strength, optimization initialization, and randomness control on the attribution performance of AttEXplore+.\n\u2022 We propose an optimized version of the AttEXplore+ framework, named AttEXplore++, which achieves the best attribution performance.\n\u2022 We present a new attribution framework named AttEX- plore+ that can be optimized using other adversarial attack methods capable of computing gradient information. The code will be open-sourced to facilitate usage and continued research by relevant researchers and developers."}, {"title": "II. RELATED WORKS", "content": "A. Local Approximation Methods\nLocal approximation methods aim to simplify the model's complexity around individual data points, providing local explanations for globally complex models. These methods are widely applicable as they do not rely on the internal structure of the target model. For example, LIME (Local Interpretable Model-Agnostic Explanations) [21] uses local linear approximations to explain models, making it particularly suitable for handling nonlinear or non-differentiable models. However, the accuracy of LIME depends on the local linearity assumption and the selection of perturbations, which may not adequately capture the global behavior of the model. While submodular optimization can be used to select more representative samples to improve global explanations, LIME is more appropriate for local interpretations. Additionally, LIME requires training a local linear model for each explanation instance, which can be challenging when handling large datasets or complex models. In contrast, SHAP (Shapley Additive Explanations) [22] addresses the lack of global consistency in LIME by allocating fair contributions to each feature based on Shapley values. However, SHAP suffers from computational inefficiency due to the need to calculate the marginal contribution of each feature, which becomes computationally expensive as the input dimension increases exponentially. Another method, DeepLIFT [23], attributes the change in model output to input features by propagating neuron activation differences, effectively avoiding gradient saturation problems, but it does not satisfy the implementation invariance axiom [24].\nB. Gradient-Based Attribution Methods\nGradient-based attribution methods extend the idea of local approximation methods by calculating the gradients of model outputs with respect to inputs, providing finer-grained explanations. Saliency Maps (SM) were initially introduced as a visualization tool for deep convolutional neural networks to highlight which input features (such as pixels) most influence the model's predictions [25]. However, SM has limitations in handling gradient noise and gradient saturation issues, and it does not fully satisfy two important attribution axioms: Sensitivity and Implementation Invariance.\nTo address these issues, Integrated Gradients (IG) was introduced to satisfy the aforementioned two attribution axioms, greatly advancing the field of attribution methods. The Sensitivity axiom requires that if a change in an input feature leads to a change in the model output, that feature should receive a non-zero attribution. The Implementation Invariance axiom mandates that attribution results should depend solely on the input-output relationship of the model and not on the specific implementation of the model. IG integrates gradients along a path from a baseline to the actual input, solving the gradient saturation issue and adhering to these two axioms, thus laying a strong foundation for subsequent attribution methods [24].\nSmoothGrad (SG) further extends IG by adding noise to the input and calculating gradients multiple times to reduce noise, generating clearer explanations [26]. However, while SG improves interpretability, it increases computational costs and may miss subtle features critical to the decision process. Guided-IG [27] and ExpectedGrad [28] further reduce noise and improve the stability of explanations by guiding gradient integration paths and sampling multiple times. In addition, Boundary Integrated Gradients (BIG) focuses on gradient analysis in specific decision boundary regions, making it particularly effective in classification tasks [29]. Fast-IG accelerates the attribution process by reducing the number of gradient computations, dynamically adjusting step sizes, and leveraging parallel computing [30].\nC. Adversarial Example-Based Attribution Methods\nAdversarial example-based attribution methods build upon gradient-based methods by revealing model vulnerabilities through subtle input perturbations. These methods provide deeper explanations by analyzing how perturbations in input features impact model predictions. For instance, Adversarial Gradient Integrations (AGI) have been used to explain the effects of adversarial attacks on model decisions, helping to understand model robustness [31]. However, AGI may focus excessively on the model's sensitivity to small perturbations, neglecting critical features of normal inputs.\nTo further improve explanation accuracy and efficiency, more faithful boundary-based attribution methods, such as MFABA, utilize higher-order derivatives and explore decision boundaries to provide more fine-grained and accurate explanations, though at a higher computational cost [32]. AttEXplore explores model parameters and decision boundary transitions to explain attribution but faces high computational demands, limiting its application on large-scale datasets [20]. Iterative Search Attribution (ISA) introduces scaling parameters during the iterative process to ensure that features gradually gain higher importance, enhancing the hierarchical nature of attributions [33]. Local Attribution (LA) combines adversarial attacks with local spatial exploration, effectively optimizing the issue of ineffective intermediate states and providing clearer explanations for model predictions [34].\nD. Adversarial Attacks\nAdversarial attacks are based on fine adjustments to input data that cause the model to make incorrect predictions and can be classified into white-box and black-box attacks. White-box attacks assume that the attacker has access to internal model information such as gradients and weights, while black-box attacks assume that the attacker can only query the model's input-output relations. Gradient-based attacks are one of the classical methods, with FGSM being a representative technique that computes the gradient of the input with respect to the loss function and adjusts the input along the gradient direction to cause significant changes in model outputs [7]. Although FGSM is computationally simple and easy to implement, the adversarial examples it generates are relatively coarse and easy to detect. PGD attacks extend FGSM by performing multi-step iterations to optimize adversarial perturbations, generating more concealed and effective adversarial examples that can bypass most defense mechanisms, though at the cost of higher computational complexity and longer attack times [35]. Optimization-based attacks treat adversarial example generation as an optimization problem, aiming to minimize perturbations while maximizing the loss function. C&W attacks represent this type of method, which generates adversarial examples by optimizing perturbation size and classification loss, making them harder to detect and capable of bypassing most defense mechanisms, albeit at a high computational cost and requiring adjustment for different models [36].\nBlack-box attacks do not rely on internal model information. Transfer attacks are a common black-box attack method, based on the principle that adversarial examples generated on one model can also be effective on another, thus achieving cross-model attacks. Such attacks do not rely on the target model's internal information but instead infer potential adversarial perturbations through multiple queries of the model's input-output relationships, making them particularly useful when internal model information is unavailable. Many methods aim to improve the transferability of adversarial examples, as highly transferable adversarial examples can help attribution methods verify their interpretability and robustness across different models, revealing model vulnerabilities in similar features, thereby improving the reliability and generalizability of attribution analysis [37]. In this paper, we utilize the following transferability methods to optimize the performance of the AttEXplore+ framework: Momentum Iterative Method (MIM) enhances the transferability of adversarial examples across models by introducing a momentum mechanism [38]; Momentum Integrated Gradients (MIG) combines integrated gradient attribution and momentum strategies to further improve the cross-model generalizability of adversarial examples [39]; Diverse Input Method (DIM) performs multi-scale random scaling on input images, ensuring that adversarial examples exhibit good transferability across different network architectures [40]; Scale-Invariant Nesterov Iterative Method (SI-NIM) introduces scale-invariance, allowing adversarial examples to maintain strong transferability across images of different sizes and resolutions [41]; Translation-Invariant Method (TIM) adopts gradient-based convolution operations to optimize perturbation directions, significantly improving the cross-model transferability of adversarial examples [42]; Neuron Attribution-based Attack (NAA) analyzes neuron attribution information to attack important neurons, achieving more efficient transfer attacks across models [43]; Structure Invariant Attack (SIA) generates adversarial examples with higher transferability while maintaining image structure through structure-invariant random image transformations [44]; Gradient Relevance Attack (GRA) is a classical gradient-based method; however, its transferability heavily relies on hyperparameters for selecting neighborhood information [45]; Frequency-based Stationary Point Search (FSPS) combines frequency domain analysis with stationary point search to significantly improve the cross-model transferability of adversarial examples by optimizing frequency information [46]. These methods optimize the transferability of adversarial examples from different angles, enhancing their cross-model attack capabilities."}, {"title": "III. PRELIMINARIES", "content": "A. Problem Definition\nGiven a deep neural network model $f(\\cdot)$ and an input sample $x \\in R^n$, where $R^n$ denotes an n-dimensional sample space, the goal of attribution is to compute the contribution A of each feature in the input sample x to the model's decision $f(x; y)$. In the context of image classification, y represents the predicted class of the model.\nB. Integrated Gradients (IG)\nIn attribution methods, gradient information plays a crucial role in measuring feature importance. According to the concept of Saliency Maps [25], if $f(\\cdot)$ is continuously differentiable, the gradients of can be used to compute $A_i$, which is the importance of the i-th dimension in the input x. Integrated Gradients (IG) [24] computes the attribution by integrating the gradients along a straight-line path from a baseline sample x' to the input sample x. The gradients accumulated along this path are referred to as integrated gradients. For the i-th feature in the input sample, IG provides the contribution $A_i$ as shown in Equation 1:\n$A_i := (x_i - x'_i) \\int_{\\alpha=0}^{1} \\frac{\\partial f(x' + \\alpha \\times (x - x'))}{\\partial x_i} d\\alpha$ (1)\nHere, $\\frac{\\partial f(\\cdot)}{\\partial x_i}$ represents the gradient of the deep neural network model $f(\\cdot)$ along the i-th dimension. It is important to note that the baseline sample x' should be selected based on the specific model being explained. For instance, for image models, x' can be a black image, while for text models, it can be a zero embedding vector. The term $x' + \\alpha \\times (x - x')$ represents the linear path of IG. Since directly integrating the gradients from a to x' is difficult, IG approximates the integral by dividing the path into sufficiently small intervals and summing the gradients at each point along the path. The approximate computation of IG is given by Equation 2:\n$A_i := (x_i - x'_i) \\sum_{k=1}^{m} \\frac{\\partial f(x' + \\frac{k}{m}(x - x'))}{\\partial x_i} \\times \\frac{1}{m}$ (2)\nC. The Relationship between Adversarial Attacks and Attribution\nFrom the IG attribution process, we understand that IG requires a baseline sample as the starting point of integration. However, the choice of baseline sample is arbitrary and lacks strict justification. Although choosing a black image as the baseline works for simple tasks (such as MNIST classification), selecting a suitable baseline for more complex tasks significantly impacts the interpretability of the deep neural network model [31]. If we assume that the selected baseline sample always alters the model's output, the attribution problem becomes equivalent to finding the key features that change the model's decision. Since adversarial examples can be generated on different deep neural network models and easily mislead them, the IG integration process can be transformed into an integration process along the gradient path from adversarial examples to input samples.\nIt is noteworthy that adversarial examples can be defined as baseline samples given a specific deep neural network model, eliminating the ambiguity and inconsistency caused by selecting a baseline sample for a particular task. Next, we provide a detailed definition of adversarial attacks.\nDefinition of Adversarial Attack: From Sec. III-A, we know that given an input sample x with a true label y, the prediction output of a deep neural network model with parameters $\\theta$ is defined as $f (x;y; \\theta)$. Suppose we have an adversarial sample $x_{adv}$, then the output becomes $f(x_{adv}; \\theta) = y'$, where $y \\neq y'$. Clearly, the features that cause the shift from y to y' play a crucial role in the model's decision. The goal of adversarial attacks is to construct an adversarial sample $x_{adv}$ that belongs to the $\\epsilon$-ball $\\Phi_{\\epsilon}(x)$ surrounding the input sample x, where $\\Phi_{\\epsilon}(x) = \\{x_{ball} : ||x_{ball} - x ||_p \\leq \\epsilon \\}$. Here, $\\epsilon$ is the perturbation upper bound that ensures the fidelity of the adversarial sample. $||\\cdot||_p$ denotes the Lp norm constraint, such as the L2 norm. Let $L(x; y)$ be the loss function of the deep neural network model, which measures the difference between the true label y and the predicted output $f (x; y; \\theta)$. Adversarial attacks can be viewed as solving the following optimization problem:\n$\\underset{x_{adv} \\in \\Phi_{\\epsilon}(x)}{\\max} L(x_{adv}; y)$ (3)\nAs shown in Equation 4, to mislead the model more stably, the Basic Iterative Method (BIM) [47] iterates over the adversarial sample under a given perturbation limit:\n$g(x) = \\nabla_{x'} L(x_{adv}; Y)$ \n$x_{adv}^{t+1} = \\Pi_{\\Phi_{\\epsilon}(x)} \\{x_{adv}^t + \\alpha \\cdot sign(g(x))\\}$ (4)"}, {"title": "IV. EXPLORING THE IMPACT OF ADVERSARIAL TRANSFERABILITY ON ATTRIBUTION PERFORMANCE", "content": "This section explores the influence of adversarial transferability on attribution performance. It first reviews the AttEXplore method, focusing on how gradient information is obtained from adversarial attacks. Then, different adversarial attack methods are compared in terms of their gradient update strategies and their impact on transferability. Lastly, the role of randomness in adversarial attacks is analyzed, highlighting its effects on robustness, reproducibility, and attribution outcomes.\nA. AttEXplore Review\nThe attribution result for adversarial sample $x^t$ is defined as:\n$g(x^t) = \\nabla_{x'} L(x; y)$\n$A_i : = \\int_{\\Delta a}^{t} g(x^t) dt$ (5)\nIn Equation 5, $x^t$ is the adversarial baseline sample, while $x^0$ is the starting point of the attribution process, and $g(x^t)$ represents the gradient information, which can be obtained using different adversarial attack methods. This paper focuses on exploring the feasibility of using adversarial attacks with transferability to obtain gradient information, as discussed in Sec. III-C. The gradient iterates along a non-linear path $x^t = x + \\sum_{k=1}^{t} \\Delta x^k$, where $\\Delta x^k$ represents the change in the sample along the adversarial attack path. For each iteration, the attribution method can employ the BIM method [47] to update the adversarial sample as the baseline point. Thus, as shown in Figure 2, we can derive $\\Delta x^t = \\alpha \\cdot sign(\\nabla_{x^t} L(x; y))$.\nB. Different Ways to Obtain Gradient Information\nTransfer-based attacks do not directly train adversarial examples on the target deep neural network model but rather on a substitute model. This requires the adversarial examples trained on the substitute model to reliably cross the decision boundary of the target model and mislead its decision. Furthermore, in training adversarial examples, the proportion of training data in the total sample space is small, leading to issues with the decision boundary (as most of the data is in-distribution), and the boundaries are often rough (since the training samples are far from the decision boundary, adversarial training can make the decision boundary rough). Therefore, the minimal perturbations we obtain are not robust. Our goal is to train and cross more general decision boundaries, not just cross them.\nIn essence, the nature of transfer-based attacks is to explore the possibility of adversarial examples crossing decision boundaries. Although AttEXplore [20] explores more general decision boundaries by incorporating frequency domain information, the contribution of different adversarial example construction methods to attribution performance has not been fully explored. Different adversarial attack methods generate adversarial examples using various strategies and handle gradient updates $g(x^{(t+1)})$ differently. In Table I, for the first time, we integrate transfer-based attack principles such as gradient editing, semantic similarity, and target modification into the attribution process to explore the impact of adversarial example transferability on attribution performance. Specifically, the PGD method directly updates perturbations through gradient descent and clips the result after each step. In contrast, MIM introduces a momentum term to smooth gradient updates and normalizes the gradient using the L1 norm to prevent the direction from being dominated by the gradient magnitude. MIG combines integrated gradients with momentum, smoothing the gradients and enhancing stability through normalization.\nDIM enhances perturbation diversity by applying random scaling and cropping operations without affecting semantics. Similarly, SI-NIM incorporates noise at each iteration, further enhancing the robustness of adversarial examples. TIM improves the attack efficacy of adversarial examples at different locations through translation transformations.\nC. The Impact of Randomness\nIn the adversarial attack methods mentioned earlier, the setting of parameters plays a significant role in introducing randomness during the perturbation generation process. First, parameters related to diversity probabilit, DP, introduce noise during gradient computation in methods such as DIM, TIM, and AttEXplore to enhance privacy protection. In each gradient update, the original gradient g(x) is perturbed by adding noise $\\eta$, with the update formula as:\n$g'(x) = g(x) + \\eta$\nwhere $\\eta \\sim N(0, \\sigma^2)$ is Gaussian noise. The introduction of this noise causes uncertainty in the perturbation direction, making the generated adversarial examples behave differently across different experimental runs. Therefore, differential privacy-related parameters not only affect the magnitude of perturbations but also introduce randomness to the method, impacting the stability and reproducibility of adversarial example generation.\nSecond, the parameter $\\beta$ in GRA is used to control the strength of regularization or the step size for gradient updates. The update formula for gradient descent is:\n$x_{t+1} = x_t - \\beta \\nabla f(x_t)$\nWhen randomness is introduced into the setting of $\\beta$, the magnitude and direction of perturbations will vary, leading to uncertainty in adversarial example generation under different experimental conditions. This may affect the model's robustness and the reproducibility of attribution results.\nMoreover, the parameter $\\epsilon$ in methods such as PGD and SIA is a standard parameter used to control the perturbation upper bound, restricting the maximum difference between the adversarial and original samples. The perturbation constraint is given by:\n$x' = x + \\delta \\text{ where } ||\\delta||_{\\infty} \\leq \\epsilon$\nAlthough $\\epsilon$ itself is not a random parameter, the initial perturbation $\\delta_0$ and step size may introduce randomness during each iteration, making the adversarial example generation process stochastic.\nLastly, the parameters $\\rho$ and $\\sigma$ used in FSPS and AttEXplore control the magnitude and variance of noise. Specifically, the added noise $\\eta$ follows:\n$\\eta \\sim N(0, \\sigma^2)$\nAdjusting $\\rho$ and $\\sigma$ can change the strength and range of noise, thereby affecting the randomness of the generated adversarial examples. Under different experimental conditions, the configuration of this noise will lead to significant differences in adversarial example behavior. Therefore, these noise control parameters largely determine the randomness and robustness of the attack methods.\nThe impact of this randomness is important in attribution analysis because the core goal of attribution analysis is to explain the decision-making process of the model. Randomness may lead to different perturbation paths for the same input in different experiments, affecting the stability of attribution results. Moreover, randomness may impact the reproducibility of experiments, making it difficult to replicate results, thereby affecting the credibility of scientific research. Finally, excessive randomness may make attribution results overly sensitive to input perturbations, weakening the robustness of the explanations. Therefore, studying the impact of randomness on attribution methods helps us identify and control uncertainty factors, improving the robustness and interpretability of attribution analysis.\nFurthermore, when investigating the impact of adversarial attacks on attribution analysis, it is important to explore how many iterations of white-box attacks succeed, as well as the few additional iterations performed after the attack succeeds. After a white-box attack succeeds, attackers may continue to perform additional attack operations to enhance the transferability of adversarial examples, ensuring that they retain high misclassification effectiveness even on black-box models.\nSpecifically, in the early stages of adversarial attacks (e.g., methods like PGD, MIM), the main goal of white-box attacks is to quickly find a perturbation $\\delta$ that effectively misleads the model such that $f(x + \\delta) \\neq y$. Once the white-box model is successfully misled, the subsequent attack steps are not aimed at further improving the attack effectiveness on the white-box model but rather at increasing the generalization ability of the adversarial examples so that they can also successfully mislead the target model with different architectures or parameter settings. To achieve this, strategies such as random scaling (DIM), momentum updates (MIM), and noise injection (SI-NIM) are introduced. These strategies aim to enhance the transferability of adversarial examples by expanding the search space for perturbations and stabilizing gradient updates.\nWhen studying these attack methods, analyzing the few attack steps executed after the white-box attack succeeds is crucial to understanding the mechanism by which adversarial example transferability is improved. For example, in DIM (multi-scale attacks), after the attack succeeds, random scaling of input images at different scales allows the attacker to obtain gradient information across multiple input scales, generating adversarial examples with better generalization ability. Similarly, MIM smooths gradient updates by introducing momentum terms, preventing transferability from being impaired due to excessive gradient fluctuations during white-box attacks.\nThus, these post-success attack steps are not merely optimizations of existing perturbations but rather strategies to ensure that the generated adversarial examples maintain high transferability across models. These extended attack strategies enhance the robustness of adversarial examples, giving them greater cross-model attack capabilities. The significance of these additional steps lies in their ability to allow attribution analysis results to extend beyond the decision-making process of a single model. They verify the robustness and generalizability of attribution methods by observing the cross-model performance of adversarial examples."}, {"title": "V. EXPERIMENTS", "content": "A. Datasets and Models\nIn this study, we conduct experiments on the ImageNet dataset [49]. Specifically, we extract 1000 samples from the ImageNet dataset and evaluate them using various existing interpretability methods and adversarial attack methods [43], [48], [31], [32], [20], [33]. We select three classical Convolutional Neural Networks (CNN) models: Inception-v3 [50], ResNet-50 [51], and VGG16 [52]. Additionally, we include the MaxViT-T and ViT-B/16 [53] models to explore the interpretability of the proposed methods on vision transformer-based models.\nB. Baseline Methods\nWe select 11 classical and state-of-the-art interpretability algorithms as baseline methods to compare against our proposed AttEXplore++ method. These algorithms include BIG, DeepLIFT [23], EG [28], FIG [30], GIG [27], IG [24], MFABA, SG [26], SM [25], AGI [31], and AttEXplore. Among them, AttEXplore serves as the primary comparison for our proposed method."}, {"title": "F. Impact of Transferability on Interpretability", "content": "Table III presents the impact of different transferable adversarial attack methods on the attribution performance within the AttEXplore+ framework. It is evident that with the enhancement of attack method transferability, the attribution performance of AttEXplore+ significantly improves. Specifically, traditional white-box attack methods like BIM and PGD exhibit weaker interpretability, while more recent adversarial attack methods like MIG and GRA demonstrate the best performance, achieving the best or second-best results across nearly all models. For instance, the MIG method achieves the best insertion scores of 0.5335 and 0.4348 on the Inception-v3 and ResNet-50 models, respectively, while the GRA method achieves the best insertion score of 0.4977 on the ViT-B/16 model. These results indicate that stronger transferability plays a key role in enhancing the performance of attribution algorithms."}, {"title": "G. Impact of Randomness on Interpretability", "content": "As shown in Figure 3, we evaluate the impact of randomness on different attribution methods. In the experiments, we fix the diversity probability (DP) to 0.5, the noise amplitude $\\beta$ to 4.0, and the perturbation rate $\\epsilon$ to 16. We tested the impact of randomness on attribution performance using three different random seeds. The results show that randomness has minimal impact on the insertion and deletion scores of most methods. In almost all methods, the variation in attribution performance due to changes in random seeds is very small, with the insertion and deletion scores remaining stable. This demonstrates that the AttEXplore+ framework is robust to randomness, ensuring the stability and reliability of the attribution performance."}, {"title": "H. Impact of Transferability Method Parameters on Attribution Performance", "content": "This section examines the impact of the diversity probability (DP) on the attribution performance of AttEXplore and its two variants (AttEXplore+DIM and AttEXplore+TIM). We fix $\\rho$ at 0.5, the perturbation rate $\\epsilon$ at 16.0, and the random seed at 0, and test DP values of 0.2, 0.5, and 0.8. As shown in Figure 4, different DP values have varying impacts on different models. On ResNet-50, VGG16, and ViT-B/16, both the insertion and deletion scores increase synchronously with increasing DP. Given that insertion scores are significantly higher than deletion scores in magnitude and more critical for attribution evaluation, higher DP values are recommended for these models.\nIn contrast, on Inception-v3 and MaxViT-T models, the insertion scores of the original AttEXplore decrease as DP increases, while the deletion scores increase, indicating that larger DP values lead to performance degradation in these models. However, when AttEXplore's attribution process is optimized using DIM and TIM, the insertion scores generally increase with increasing DP, while the deletion scores show an initial increase followed by a decrease. Overall, after optimization with DIM and TIM, the attribution performance of AttEXplore improves on these models."}, {"title": "I. Impact of Perturbation Rate $\\epsilon$ on AttEXplore+ Performance", "content": "This section examines the impact of the perturbation rate $\\epsilon$ on the performance of three methods (AttEXplore, AttEX-plore+PGD, and AttEXplore+SIA). In the experiments, we fix $\\rho$ at 0.5, the random seed at 0, and set DP to 0.5, testing $\\epsilon$ values of 8, 16, and 24. As shown in Figure 5, in most cases, the insertion scores increase with increasing $\\epsilon$, while the deletion scores decrease, indicating that interpretability improves with higher perturbation rates.\nIt is worth noting that the interpretability of the original AttEXplore decreases on traditional CNN models as $\\epsilon$ increases. However, after optimizing AttEXplore with adversarial attack methods, the insertion scores significantly improve, and the deletion scores decrease with increasing $\\epsilon$, indicating a marked improvement in interpretability."}, {"title": "J. Impact of Noise Amplitude $\\beta$ on AttEXplore+ Performance", "content": "In this section, we evaluate the impact of noise amplitude $\\beta$ on the performance of AttEXplore+ when optimized with GRA. We conduct experiments with $\\beta$ values of 3.0, 3.5, and 4.0. As shown in Figure 6, the insertion scores are positively correlated with noise amplitude $\\beta$, as insertion scores increase with $\\beta$ across all models. For deletion scores, the scores also increase with increasing $\\beta$ in traditional CNN models. However, in transformer-based models, the interpretability improves optimally as $\\beta$ increases, as shown by an increase in insertion scores and a gradual decrease in deletion scores. This indicates that larger noise amplitudes help improve attribution performance in these models."}, {"title": "VI. CONCLUSION", "content": "In this work, we proposed the AttEXplore+ framework, which integrates multiple adversarial attack methods to explore the relationship between adversarial attack transferability and attribution interpretability. By selecting the most effective adversarial strategies, we introduced AttEXplore++, an optimized method that significantly improves interpretability compared to prior approaches. Our experiments demonstrate that AttEXplore++ enhances the performance of attribution methods across various models, including CNNs and vision transformers, with substantial improvements in insertion and deletion scores. Furthermore, we examined key parameters such as perturbation rate, noise amplitude, and diversity probability, showing that AttEXplore++ is robust against randomness, delivering stable and reliable explanations. These results underscore the critical role of adversarial transferability in improving attribution methods, offering deeper insights into neural network decision-making processes and paving the way for further advancements in model interpretability."}]}