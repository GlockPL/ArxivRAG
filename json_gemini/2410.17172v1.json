{"title": "KANICE: Kolmogorov-Arnold Networks with Interactive Convolutional Elements", "authors": ["Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Elias Ioup", "David Dobson", "Kendall N. Niles", "Ken Pathak", "Steven Sloan"], "abstract": "We introduce KANICE (Kolmogorov-Arnold Networks with Interactive Convolutional Elements), a novel neural architecture that combines Convolutional Neural Networks (CNNs) with Kolmogorov-Arnold Network (KAN) principles. KANICE integrates Interactive Convolutional Blocks (ICBs) and KAN linear layers into a CNN framework. This leverages KANs' universal approximation capabilities and ICBs' adaptive feature learning. KANICE captures complex, non-linear data relationships while enabling dynamic, context-dependent feature extraction based on the Kolmogorov-Arnold representation theorem. We evaluated KANICE on four datasets: MNIST, Fashion-MNIST, EMNIST, and SVHN, comparing it against standard CNNs, CNN-KAN hybrids, and ICB variants. KANICE consistently outperformed baseline models, achieving 99.35% accuracy on MNIST and 90.05% on the SVHN dataset.\nFurthermore, we introduce KANICE-mini, a compact variant designed for efficiency. A comprehensive ablation study demonstrates that KANICE-mini achieves comparable performance to KANICE with significantly fewer parameters. KANICE-mini reached 90.00% accuracy on SVHN with 2,337,828 parameters, compared to \u039a\u0391\u039dICE's 25,432,000. This study highlights the potential of KAN-based architectures in balancing performance and computational efficiency in image classification tasks. Our work contributes to research in adaptive neural networks, integrates mathematical theorems into deep learning architectures, and explores the trade-offs between model complexity and performance, advancing computer vision and pattern recognition. The source code for this paper is publicly accessible through our GitHub repository (https://github.com/m-ferdaus/kanice).", "sections": [{"title": "1 Introduction", "content": "Deep learning has transformed computer vision and pattern recognition, with Convolutional Neural Networks (CNNs) being fundamental to image classification. CNNs have proven successful in various applications [16]. As visual recognition tasks grow more complex, the need for advanced, adaptive, and theoretically sound architectures that can capture complex patterns and relationships increases [3, 19].\nNeural network architectures have evolved significantly to improve their capabilities. Traditional CNNs are effective but often fail to capture long-range dependencies and adapt to diverse input distributions. This shortcoming has led to research into more flexible models. For example, the introduction of attention mechanisms in vision transformers shows promise in overcoming these challenges [7, 12, 18].\nThere is a growing interest in using mathematical principles to improve neural network design. The Kolmogorov-Arnold representation theorem is a powerful result in approximation theory, and it states that any multivariate continuous function can be represented as a composition of univariate functions and addition operations [6, 14, 15]. This theorem has inspired the development of Kolmogorov-Arnold Networks (KANs) [9], aiming to exploit this representation for improved neural network performance. KANS replace linear weights with learnable univariate functions, offering improved accuracy and interpretability compared to Multi-Layer Perceptrons (MLPs) [20]. While some argue that Kolmogorov's theorem is irrelevant for neural networks [5], others demonstrate its significance in network design [8]. Recent research has explored KAN applications in time series forecasting [2] and nonlinear function approximation using Chebyshev polynomials [17]. Error bounds for deep ReLU networks have been derived using the Kolmogorov-Arnold theorem [11]. Smooth KANs with structural knowledge representation show potential for improved convergence and reliability in computational biomedicine [10], addressing limitations in representing generic smooth functions [14].\nKANs have shown great promise across various fields, outperforming traditional MLPs in accuracy and interpretability for tasks like classification, forecasting, and anomaly detection. This success suggests potential benefits in image classification as well. Concurrently, convolutional architectures have advanced, with Interactive Convolutional Blocks (ICBs) enhancing CNNs' feature extraction capabilities through dynamic, context-dependent processing [4]. This improves model adaptability to diverse inputs, crucial for complex image classification. Recent research has focused on hybrid architectures, such as ConvKAN [1], which combine convolutional layers with KAN principles to improve image classification performance.\nTo address the limitations of existing architectures, we propose KANICE (Kolmogorov-Arnold Networks with Interactive Convolutional Elements), a novel architecture that combines KANs, ICBS, and CNNs. KANICE aims to overcome standard CNN limitations by incorporating KANs' universal approximation capabilities and ICBs' adaptive feature learning. The key innovation of KANICE is its integration of KAN linear layers and ICBs into the CNN architecture. This combination allows for more effective adaptation to the complexities of image data, potentially capturing patterns missed by traditional CNNs.\nKANICE shows impressive performance, but full KANLinear layers may be too demanding in some applications. To address this, we introduce KANICE-mini, a compact variant aiming to maintain performance while reducing parameters. KANICE-mini uses a modified KANLinear layer balancing expressiveness with efficiency.\nIn this study, we evaluate KANICE across multiple image classification datasets, comparing it to standard CNNs and other hybrid architectures. We conduct an ablation study to assess the impact of different components in the KANICE architecture, and to compare the performance and efficiency of KANICE-mini against its full-scale counterpart. This study provides insights into the trade-offs between model complexity and performance in KAN-based architectures. Our work contributes to research in adaptive neural networks and integrates mathematical theorems into deep learning architectures, advancing computer vision and pattern recognition. We also contribute to the ongoing discussion of model efficiency in deep learning by demonstrating how principles from the Kolmogorov-Arnold theorem can create more compact yet highly effective models, as exemplified by KANICE-mini.\nOur key contributions and insights include:\n\u2022 Combining KANs and ICBs enhances feature processing and model adaptability beyond their individual capabilities.\n\u2022 Development of KANICE-mini, an surprisingly efficient variant that achieves comparable performance to KANICE with significantly fewer parameters, challenging assumptions about model size and performance.\n\u2022 KANICE's architecture, which blends local and global feature processing, unexpectedly shows improved resistance to adversarial attacks, enhancing model security.\nThe paper is structured as follows: Section 2 details the KANICE architecture, including its components and theoretical foundations. Section 3 presents our experimental setup and main results across various datasets. Section 4 provides an ablation study comparing KANICE, KANICE-mini, and baseline models. We conclude in Section 5 with a discussion of our findings and potential future research directions."}, {"title": "1.1 Kolmogorov-Arnold Networks and KANLinear Layers", "content": "KANs are an innovative neural network architecture based on the Kolmogorov-Arnold representation theorem. This theorem states that any continuous multivariate function can be composed of continuous univariate functions and addition operations [13]. KANS introduce KANLinear layers, which differ from traditional linear layers by using learnable univariate functions, typically spline-based, to approximate complex multivariate functions. This approach offers several theoretical advantages when replacing standard linear layers in CNNs:\n\u2022 Enhanced Function Approximation: KANLinear layers offer improved function approximation capabilities compared to standard linear layers. Based on the Kolmogorov-Arnold representation theorem, these layers can approximate any continuous multivariate function using combinations of single-variable functions. This property enables KANLinear layers to capture more complex relationships in data, particularly in image classification tasks. By modeling intricate dependencies between high-level features extracted by convolutional layers, KANLinear layers can potentially enhance classification accuracy.\n\u2022 Enhanced Expressiveness: KANLinear layers, particularly in the final stages of CNNs, significantly increase the network's ability to model complex relationships. Unlike traditional CNNs that often use simple linear transformations for final feature mapping, KANLinear layers introduce non-linear univariate functions. This addition allows for more sophisticated decision boundaries, potentially improving the model's ability to distinguish between classes.\n\u2022 Spatial Information Retention: We preserve the CNN's spatial awareness by replacing only linear layers with KANLinear, keeping convolutional layers intact. This hybrid approach combines the CNN's spatial feature extraction with KANLinear's enhanced processing capabilities. The result is a network that effectively utilizes both local patterns and global context, crucial for image classification tasks.\n\u2022 Adaptive Complexity: KANLinear layers offer adjustable complexity through variable control points in their spline representations. This feature allows researchers to fine-tune model capacity without changing the overall architecture. By matching layer complexity to the task's intricacy, a better balance between capacity and generalization can be achieved.\n\u2022 Improved Generalization: KANLinear layers' efficiency and adaptability may enhance the model's ability to capture generalizable features. This could boost performance on unseen data, crucial for real-world image classification where test and training distributions often differ.\n\u2022 Mitigation of the Vanishing Gradient Problem: KANLinear layers may alleviate the vanishing gradient problem in deep networks. Their learnable univariate functions create additional gradient paths, facilitating more effective training of deeper architectures. This feature is especially beneficial for complex image classification tasks requiring deep networks.\n\u2022 Flexibility in Function Space: KANLinear layers offer greater functional flexibility than CNNs, which are limited by their convolutional structure. This adaptability allows KANLinear models to better match data distributions, potentially improving performance across diverse image classification tasks and visual patterns.\nIntegrating KANLinear layers into CNN architectures advances neural networks for image classification. It combines CNNs' feature extraction with KANLinear layers' expressiveness and efficiency. The resulting models promise improved accuracy, interpretability, and adaptability across image classification tasks. This forms the basis of our proposed KANICE architecture, which incorporates KANLinear layers and ICBs into a CNN framework to enhance pattern recognition in images."}, {"title": "2 Proposed Method: KANICE", "content": "KANICE is an advanced neural network for image classification. It combines: 1. ICBs: Initial feature extractors capturing spatial relationships. 2. Traditional Convolutional Layers: Further refine and abstract visual information. 3. Batch Normalization and Pooling Layers: Stabilize learning and reduce spatial dimensions. 4. KANLinear Layers: Replace fully connected layers, offering enhanced function approximation. KANICE processes images through these components, transforming raw pixel data into abstract representations for classification. The architecture's design aims to leverage the strengths of each element, resulting in a robust and adaptable model for complex image classification tasks.\nFigure 1 provides a schematic overview of KANICE, illustrating the flow of information and interaction between components. The network processes input images using advanced components. ICBs utilize parallel 3 \u00d7 3 and 5 \u00d7 5 convolutional paths with GELU activations, combining outputs through element-wise multiplication. The core consists of two stages, each containing an ICB, a 3 \u00d7 3 convolutional layer, batch normalization (scaling from 64 to 128 channels), and 2 \u00d7 2 max pooling. After convolution, feature maps are flattened before entering the Kolmogorov-Arnold Network (KAN) component. The KAN, implemented as KANLinear layers, replaces standard fully connected layers with learnable univariate functions, typically spline-based. This design, based on the Kolmogorov-Arnold representation theorem, enhances the network's ability to approximate functions. Each KANLinear layer applies these univariate functions to its inputs, followed by summation operations, allowing the network to capture complex, non-linear relationships in the data. KANICE's architecture combines the spatial feature extraction capabilities of convolutional neural networks with the advanced function approximation abilities of KANs, resulting in a powerful and versatile framework for image classification tasks. We'll examine KANICE's innovation by understanding its major components. We start with the ICB, the foundation of KANICE's feature extraction."}, {"title": "2.1 Interactive Convolution Block (ICB)", "content": "The ICB is a key element of the KANICE architecture. It improves the model's capacity to detect intricate spatial patterns in input data. ICBs differ from standard convolutional layers by incorporating an interaction mechanism between various convolutional operations. This approach enables more flexible and context-sensitive feature extraction. Each ICB comprises two parallel convolutional paths followed by an interaction step. The paths use 3 \u00d7 3 and 5 \u00d7 5 convolutional layers, respectively. The interaction step combines their outputs through element-wise multiplication.\nLet $X \\in \\mathbb{R}^{C_{in}\\times H \\times W}$ represent the input tensor, where $C_{in}$ is the number of input channels, $H$ is the feature map height, and $W$ is the feature map width. The ICB can be expressed as:\n$Y = f(W_1 * X) \\odot f(W_2 * X)$ (1)\nHere, $W_1 \\in \\mathbb{R}^{C_{out}\\times C_{in}\\times 3\\times 3}$ and $W_2 \\in \\mathbb{R}^{C_{out}\\times C_{in}\\times 5\\times 5}$ are the weight tensors for the 3x3 and 5x5 convolutions respectively. The symbol $*$ denotes the convolution operation, $\\odot$ represents element-wise multiplication, and $f()$ is the GELU activation function.\n$GELU(x) = x \\cdot \\Phi(x)$ (2)\nwhere $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution. GELU provides a smooth, non-linear activation that has shown good performance in various deep learning tasks.\nThe forward pass through the ICB involves four steps: First, the input goes through 3 \u00d7 3 convolution ($X_1 = W_1 * X$) and 5\u00d75 convolution ($X_2 = W_2 * X$) in parallel. Next, GELU activation is applied to both outputs ($X'_1 = GELU(X_1), X'_2 = GELU(X_2)$). Finally, these activated outputs experience element-wise multiplication ($Y = X'_1 \\odot X'_2$) to produce the block's output.\nICB design provides key advantages: 1. Multi-scale feature extraction: It combines 3 \u00d7 3 and 5 \u00d7 5 convolutions to capture features at different scales. 2. Adaptive feature emphasis: It uses element-wise multiplication of features from different paths, acting as a feature-wise attention mechanism. 3. Enhanced non-linearity: It employs GELU activation and element-wise multiplication, enabling complex feature representations. These elements improve the network's ability to learn and process diverse visual information efficiently.\nThe block's multi-path structure enhances its expressive power compared to standard convolutional layers, allowing for more complex feature learning with fewer parameters. The multiplicative interaction between paths acts as implicit regularization, requiring agreement for strong activation and potentially improving feature robustness. ICBs adjust their field-of-view based on input, focusing on fine or coarse features as needed. This adaptive approach introduces stronger non-linearity than traditional Conv-ReLU patterns, combined with GELU activations and multiplicative interactions.\nIn KANICE, using ICBs in the early network layers allows adaptive and context-aware feature extraction from raw input images. This enables subsequent layers to work with rich, multi-scale feature representations, potentially improving classification performance. The ICB's ability to capture complex spatial relationships and adapt to input enhances the network's feature extraction capabilities, particularly in image classification tasks. The adaptive nature of ICBs makes them more robust to input data variations, as the block can adjust its focus based on the input, better handling different scales, orientations, or styles of features within images. This adaptability could lead to improved generalization across diverse datasets or in transfer learning."}, {"title": "2.2 Traditional Convolutional Layers", "content": "After each ICB, KANICE uses traditional convolutional layers to process and extract features. These layers use shared weights and local receptive fields, making them effective for processing grid-like data like images. The operation of a convolutional layer is:\n$Y = f(W * X + b)$ (3)\nwhere W is the weight tensor (kernel), X is the input tensor, b is the bias, and f() is an activation function. Using convolutional layers with increasing channel depths (32-> 64- > 128) helps the model capture abstract features. CNNs' translation invariance makes them ideal for image classification, recognizing patterns regardless of their position in the image.\nKANICE incorporates batch normalization layers after each convolutional operation to normalize the inputs to each layer:\n$x_i = \\frac{x_i - \\mu_{\\beta}}{\\sqrt{\\sigma_{\\beta}^2 + \\epsilon}}$ (4)\nwhere $x_i$ is the i-th input, $\\mu_{\\beta}$ and $\\sigma_{\\beta}^2$ are the mean and variance of the mini-batch, and $e$ is a small constant for numerical stability.\nMax pooling layers are used to reduce the spatial dimensions of the feature maps:\n$y_{i,j} = \\underset{(p,q) \\in R_{i,j}}{max} \\ x_{p,q}$ (5)\nwhere $R_{i,j}$ is a local region in the input tensor. Batch normalization stabilizes learning, enabling higher rates and faster convergence. Max pooling achieves translation invariance and reduces computational load."}, {"title": "2.3 KANLinear Layers", "content": "The final element of the KANICE architecture is the KANLinear layer, an advanced replacement for standard fully connected layers. These layers are based on the Kolmogorov-Arnold representation theorem, a key concept in approximation theory.\nThe Kolmogorov-Arnold representation theorem states that any continuous multivariate function can be expressed as a composition of continuous univariate functions and addition operations. The theorem guarantees the existence of continuous univariate functions $\\varphi_{q}$ and $\\varphi_{q,p}$ for a continuous function $f : [0, 1]^n \\rightarrow \\mathbb{R}$, mathematically:\n$f(x_1, x_2,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q \\bigg( \\sum_{p=1}^n \\varphi_{q,p}(x_p) \\bigg)$ (6)\nKANLinear layers generalize this concept to create a flexible and powerful neural network layer in KANICE:\n$y = \\sum_{q=1}^{Q} \\Phi_q \\bigg( \\sum_{p=1}^P \\varphi_{q,p}(x_p) \\bigg)$ (7)\nHere, $x_p$ represents the p-th input feature, P is the total number of input features, Q is the number of output features, and $\\varphi_{q,p}$ and $\\Phi_q$ are univariate functions. These univariate functions are implemented as splines, balancing expressiveness and computational efficiency.\nKANLinear layers, introduced by [9], are built on several key concepts that form their theoretical foundation and contribute to their effectiveness in the KANICE architecture. These concepts are crucial for both the layers' operation and practical implementation.\nFirstly, the KANLinear layers utilize a spline representation for the univariate functions $\\varphi_{q,p}$ and $\\Phi_q$. This representation is based on B-splines, which offer a flexible and computationally efficient method for approximating complex functions. For a given function $\\varphi(x)$, the spline representation can be expressed as:\n$\\varphi(x) = \\sum_i C_iB_i(x)$ (8)\nwhere $c_i$ are trainable coefficients and $B_i(x)$ are B-spline basis functions. This formulation allows the network to learn a wide range of function shapes through the optimization of the coefficients $c_i$.\nSecondly, to enhance the stability and trainability of the network, KANLinear layers use a residual activation function. This function is defined as:\n$\\varphi(x) = w(b(x) + spline(x))$ (9)\nIn this equation, w represents a trainable weight, b(x) is a base function (typically chosen as the sigmoid linear unit, SiLU), and spline(x) is the B-spline representation. This residual structure allows the layer to learn complex functions while maintaining a direct path for gradient flow, which can significantly improve training dynamics.\nLastly, to address the challenge of handling inputs that fall outside the initial spline range, KANLinear layers incorporate a grid extension technique. This method involves optimizing new spline coefficients c' to extend the function's domain. The optimization problem can be formulated as:\n$c'j = arg \\underset{c'_j}{min} \\ \\sum_{x \\sim p(x)} | \\varphi(x) - ( \\sum_{i=G_1+k-1}^{G_2+k-1} c_i B_i(x) + \\sum_{j=0}^{G_1+k-1} c'_j B_j(x) ) |^2$ (10)\nwhere G1 and G2 denote the original and new grid sizes, respectively, and k represents the B-spline degree. This grid extension technique ensures that the KANLinear layers can effectively process inputs that may lie outside the initial range of the spline representation, thereby enhancing the robustness and generalization capabilities of the network.\nThese concepts contribute to the power and flexibility of KANLinear layers. They enable the layers to efficiently approximate complex functions and adapt to diverse input distributions. By incorporating these elements, KANICE leverages the full potential of KANLinear layers to enhance its image classification performance."}, {"title": "3 Results", "content": "We evaluated KANICE's performance against several baseline models across four image classification datasets: MNIST, Fashion-MNIST, EMNIST, and SVHN. The baseline models included a standard CNN, CNN with KANLinear layers (CNN_KAN), a model using only Interactive Convolutional Blocks (ICB), an ICB model with KANLinear layers (ICB_KAN), and a hybrid ICB-CNN model (ICB_CNN). The configurations of all these models are illustrated in Figure 2. All models were trained for 25 epochs with identical hyperparameters for a fair comparison.\nFigure 2 displays the architectural configurations for all models evaluated in this study. The CNN and CNN_KAN models have a similar structure with two Conv2D 3x3 layers followed by MaxPool2D 2x2. They differ in their final layers where CNN_KAN uses KANLinear layers instead of standard Linear layers. The ICB and ICB_KAN models replace convolutional layers with ICB2D, maintaining the same max pooling structure. ICB_KAN uses KANLinear layers in its final stages.\nThe ICB_CNN and KANICE models start with an ICB2D layer followed by a Conv2D layer, then use BatchNorm2D and MaxPool2D. This pattern is repeated twice, with channel depth increasing from 64 to 128. Their final layers differ: ICB_CNN uses standard Linear layers, while KANICE implements KANLinear layers. All models conclude with a Flatten operation to transform 2D feature maps into a 1D vector for final classification, with consistent output dimensions producing 10 outputs corresponding to the classification task's classes. This architectural comparison highlights KANICE's innovative approach. It combines elements from CNN and KAN methodologies to leverage their strengths in a unified model architecture."}, {"title": "3.1 Performance Metrics", "content": "We assessed model performance using four key metrics: accuracy, precision, recall, and F1 score. Accuracy measures correct classifications. Precision indicates the proportion of correct positive identifications. Recall measures the proportion of actual positives identified correctly. The F1 score is the harmonic mean of precision and recall, providing a balanced measure of performance."}, {"title": "3.2 Model Performance Across Datasets", "content": "On Table 1, KANICE achieved the highest accuracy of 99.35% on the MNIST dataset, marginally outperforming the next best model, ICB_KAN (99.33%). This represents a reduction in error rate from 0.67% to 0.65%. Figure 3 illustrates the learning dynamics. The test loss curve (Figure 3a) shows that KANICE converges faster and maintains a lower test loss throughout. The F1 score curve (Figure 3b) demonstrates KANICE's consistently higher performance, particularly in later epochs, indicating better generalization.\nKANICE achieved 93.63% accuracy for the Fashion-MNIST dataset, surpassing ICB_CNN at 92.94% as shown in Table 1. The error rate reduced from 7.06% to 6.37%, an improvement of approximately 9.77%. Figure 4 shows the learning curves. The test loss plot (Figure 4a) shows KANICE with consistently lower loss, suggesting better feature extraction and generalization. The F1 score progression (Figure 4b) supports KANICE's superior performance.\nOn the EMNIST dataset (Table 1), which presents a more complex task with a larger number of classes, KANICE achieved 87.43% accuracy, outperforming the next best model, ICB_KAN (87.16%). This improvement reduces the error rate from 12.84% to 12.57%, a relative improvement of about 2.10%. Figure 5 displays the learning dynamics for EMNIST. The test loss curve (Figure 5a) shows KANICE achieving and maintaining lower loss values more rapidly than other models. The F1 score plot (Figure 5b) demonstrates KANICE's consistently higher performance, especially in later epochs, indicating better adaptation to this complex dataset.\nKANICE showed the most substantial improvement for the SVHN dataset (Table 1), representing a challenging real-world scenario. It achieved 90.05% accuracy, outperforming the next best model, ICB_CNN (89.60%). This reduces the error rate from 10.40% to 9.95%, a relative improvement of approximately 4.33%. Figure 6 illustrates the learning curves for SVHN. The test loss plot (Figure 6a) shows KANICE maintaining a lower loss throughout training, suggesting better generalization to real-world data. The F1 score curve (Figure 6b) emphasizes KANICE's superior and consistent performance across all epochs.\nKANICE achieved the highest precision, recall, and F1 scores, indicating balanced performance and robust handling of multi-class problems. The learning curves (Figures 3-6) show faster convergence, lower test loss, and consistently higher F1 scores. These results suggest that KANICE's architecture, combining ICBs with KANLinear layers, is more effective for feature extraction and classification across image recognition tasks, from simple handwritten digits to complex real-world scenarios.\nTo further validate KANICE's performance improvements, we conducted a statistical analysis. This analysis was based on multiple model runs. We computed means and standard deviations of accuracy across five runs for each model on each dataset. We also performed paired t-tests to assess the statistical significance of KANICE's improvements over the next best performing model. The results consistently support KANICE's superior performance"}, {"title": "4 Ablation Study", "content": "This ablation study investigates the efficacy of the KANICE architecture. It focuses on KANICE-mini, a compact version of the KANLinear layer as described below. Three closely related architectures-ICB-CNN, KANICE, and KANICE-mini are compared across image classification datasets to assess the impact of KANLinear layers on model performance and efficiency. The study aims to validate the design principles of the KANLinear layer and demonstrate its potential for enhancing neural network capabilities while maintaining efficiency."}, {"title": "4.1 KANLinear Layer for KANICE-mini", "content": "The KANLinear layer in KANICE-mini implements the principles of Kolmogorov-Arnold Networks (KANs) with fewer parameters than the original KANLinear implementation. This layer combines a traditional linear transformation with a spline-based nonlinear component for improved parameter efficiency and complex function approximation.\nThe layer is defined by the transformation $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, where n is the input dimension and m is the output dimension. The forward pass of the KANLinear layer can be expressed as:\n$f(x) = W_{base}x + b + S(x)$ (11)\nwhere $W_{base} \\in \\mathbb{R}^{m \\times n}$ is the weight matrix of the base linear transformation, $b \\in \\mathbb{R}^m$ is the bias vector, and S(x) represents the spline component.\nThe spline component S(x) is computed using a set of learnable weights and a fixed set of spline basis functions. Let $t_i \\in [0, 1]$, i = 1,..., g + k, be a uniform grid of g + k points, where g is the grid size and k is the spline order. The spline component is then defined as:\n$S(x) = \\begin{cases} \\sum_{i=1}^{g+k} W_{spline,i} B_i (t) & \\text{if shared weights} \\\\ \\sum_{j=1}^{n} x_j \\sum_{i=1}^{g+k} W_{spline,j,i} \\cdot B_i(t) & \\text{otherwise} \\end{cases}$ (12)\nwhere $W_{spline} \\in \\mathbb{R}^{m \\times (g+k)}$ in the shared weights case, or $W_{spline} \\in \\mathbb{R}^{m \\times n \\times (g+k)}$ otherwise. $B_i(t)$ represents the i-th B-spline basis function evaluated at points t.\nThe layer also incorporates a grouped linear operation for additional efficiency:\n$Y_{grouped} = \\sum_{c=1}^{C} X_cW_c + b$ (13)\nwhere X is divided into C groups along the channel dimension, $W_c \\in \\mathbb{R}^{(m/C) \\times (n/C)}$ are group-specific weights, and $b \\in \\mathbb{R}^m$ is a shared bias.\nTo regularize the spline weights and encourage sparsity, a simple L1 regularization term is added to the loss function:\n$L_{reg} = \\lambda \\sum_{i,j,k} |W_{spline,i,j,k}|$ (14)\nwhere A is a regularization hyperparameter.\nThis KANLinear layer formulation allows KANICE-mini to approximate complex functions with fewer parameters than the original KAN implementation. The combination of linear and nonlinear components, along with weight sharing and grouped operations, provides flexibility in modeling different data relationships, making it suitable for various image classification tasks."}, {"title": "4.2 Ablation Study Design", "content": "In this section, we detail the experimental framework used to conduct our ablation study. We compare three architectures: ICB-CNN, KANICE, and KANICE-mini. These models were tested on four datasets: MNIST, Fashion-MNIST, EMNIST, and SVHN. This allows us to assess model performance across different complexity levels in image classification tasks. We maintain consistent hyperparameters and training procedures across all models to ensure a fair comparison. Each model is trained for the same number of epochs on each dataset."}, {"title": "4.3 Ablation Results: Performance Comparison Across Model Variants", "content": "We assess the impact of KANLinear layers in KANICE and KANICE-mini on multiple datasets, comparing them to the ICB-CNN baseline. We initially focus on the MNIST dataset, a simpler classification task, then analyze model performance on the Fashion-MNIST dataset. Special attention is given to KANICE-mini's efficiency and performance. For the EMNIST dataset, we evaluate how each model scales, focusing on KANICE-mini's trade-off between efficiency and performance. We also analyze model performance on the SVHN dataset, comparing KANICE-mini to the full KANICE model. On the MNIST dataset, ICB-CNN achieved 98.92% accuracy with 1,841,738 parameters. KANICE improved this to 99.35% using 19,531,584 parameters. KANICE-mini reached 99.13% accuracy with 1,843,866 parameters. For Fashion-MNIST, ICB-CNN attained 92.94% accuracy. KANICE led with 93.63% accuracy at the cost of 14,712,224 parameters. KANICE-mini balanced performance and efficiency, achieving 93.21% accuracy with 1,849,082 parameters. On the EMNIST dataset, ICB-CNN achieved 87.00% accuracy, while KANICE reached 87.43% with 19,645,248 parameters. KANICE-mini closely followed with 87.05% accuracy using only 1,853,974 parameters. For the complex SVHN dataset, ICB-CNN managed 89.60% accuracy. KANICE performed best at 90.05% but required 25,432,000 parameters. KANICE-mini achieved 90.00% accuracy with just 2,337,828 parameters, almost matching KANICE with a fraction of the parameters. Across all datasets, KANICE achieved the highest accuracy but with significantly more parameters. KANICE-mini consistently demonstrated remarkable efficiency, nearly matching KANICE's performance with parameter counts similar to ICB-CNN. This trend was particularly evident in more complex datasets like SVHN, where KANICE-mini nearly matched KANICE's performance despite using far fewer parameters. Regarding other metrics such as precision, recall, and F1 score, all models performed consistently with their accuracy scores. KANICE generally scored highest, followed closely by KANICE-mini, and then ICB-CNN. These results highlight KANICE-mini's ability to efficiently capture complex patterns, offering a compelling balance between model performance and computational resources across various image classification tasks.\nKANICE-mini achieves similar performance to KANICE with fewer parameters across all datasets, thanks to the compact implementation of the KANLinear layer. The spline component, grouped linear operation, and L1 regularization allow for complex function approximation while maintaining parameter efficiency, as described in section 4.1. KANICE-mini adapts well to varying task complexity, from simple digit recognition in MNIST to real-world image classification in SVHN, showing robustness and adaptability. The model's ability to generalize across diverse datasets is a result of its architecture and regularization techniques. The interaction of these components and their impact on performance metrics and computational requirements demonstrate how KANICE-mini balances efficiency with powerful function approximation capabilities, making it a promising choice for resource-constrained environments or large-scale image classification tasks."}, {"title": "5 Conclusion", "content": "KANICE is a novel neural network architecture that combines ICBs and KANLinear layers within a CNN framework. Our evaluation shows that KANICE consistently outperforms traditional CNNs and other hybrid architectures on four image classification datasets (MNIST, Fashion-MNIST, EMNIST, and SVHN). It exceeded baseline models across all metrics, with notable improvements on SVHN and Fashion-MNIST. The success of KANICE comes from its unique combination of adaptive feature extraction (ICBs) and enhanced function approximation (KANLinear layers), creating a more powerful and flexible architecture for image classification.\nOur introduction of KANICE-mini and the ablation study offer insights into the efficiency-performance trade-offs in KAN-based architectures. KANICE-mini shows that KANICE principles can be applied to create compact models without significant performance degradation. This is evident in the SVHN results, where KANICE-mini achieved comparable accuracy to KANICE with fewer parameters.\nThe KANICE models showed faster convergence and lower test losses than baselines, indicating improved generalization and efficiency. This could reduce computational requirements and training time in real-world applications. KANICE-mini's ability to maintain high performance with fewer parameters is promising for resource-constrained environments or large-scale deployments. KANICE and KANICE-mini advance neural network architecture for image classification by integrating Kolmogorov-Arnold representation theorem principles with advanced convolutional techniques. This approach enhances performance and offers a pathway to more efficient model designs, potentially expanding the applicability of deep learning in computer vision.\nFuture work includes evaluating scalability on larger datasets, optimizing architecture components, exploring transfer learning potential, and investigating applicability beyond image classification. These findings advance neural network architecture for image classification, offering pathways to more efficient model designs in computer vision applications."}, {"title": "B KANICE's Robustness to Adversarial Attacks", "content": "During our evaluation of KANICE", "property": "enhanced resilience to adversarial attacks. This emerged from a comparative study using the CIFAR-10 dataset, testing KANICE against a standard CNN and our ICB-CNN baseline model."}]}