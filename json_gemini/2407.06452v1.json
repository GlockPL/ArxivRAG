{"title": "Exploiting Heterogeneity in Timescales for Sparse Recurrent Spiking Neural Networks for Energy-Efficient Edge Computing", "authors": ["Biswadeep Chakraborty", "Saibal Mukhopadhyay"], "abstract": "Spiking Neural Networks (SNNs) represent the forefront of neuromorphic computing, promising energy-efficient and biologically plausible models for complex tasks. This paper weaves together three groundbreaking studies that revolutionize SNN performance through the introduction of heterogeneity in neuron and synapse dynamics. We explore the transformative impact of Heterogeneous Recurrent Spiking Neural Networks (HRSNNs), supported by rigorous analytical frameworks and novel pruning methods like Lyapunov Noise Pruning (LNP). Our findings reveal how heterogeneity not only enhances classification performance but also reduces spiking activity, leading to more efficient and robust networks. By bridging theoretical insights with practical applications, this comprehensive summary highlights the potential of SNNs to outperform traditional neural networks while maintaining lower computational costs. Join us on a journey through the cutting-edge advancements that pave the way for the future of intelligent, energy-efficient neural computing.", "sections": [{"title": "I. INTRODUCTION", "content": "Spiking Neural Networks (SNNs), often referred to as the third generation of neural networks, have garnered significant attention due to their potential for lower operating power when mapped to hardware. SNNs, particularly those using leaky integrate-and-fire (LIF) neurons, have demonstrated classification performance comparable to deep neural networks (DNNs). However, the majority of these models rely on supervised training algorithms like backpropagation-through-time (BPTT) [1], [2], [3], which are highly data-dependent and struggle with limited training data and generalization [4], [5]. Moreover, BPTT-trained models require complex architectures with a large number of neurons to achieve good performance. While unsupervised learning methods such as Spike-Timing Dependent Plasticity (STDP) have been introduced, they typically underperform compared to their supervised counterparts due to the high complexity of training [6].\nTo address these challenges, this paper introduces a series of studies focused on enhancing SNN performance through heterogeneity in network dynamics. The first paper presents a Heterogeneous Recurrent Spiking Neural Network (HRSNN) with variability in both LIF neuron parameters and STDP dynamics [7]. Previous works have suggested that heterogeneity in neuron time constants can improve classification performance [8], [9], [10], [11], but lacked theoretical explanations for these improvements. Our study not only explores how heterogeneity in both neuronal and synaptic parameters can enhance performance with less training data and fewer connections but also leverages a novel Bayesian Optimization (BO) method for hyperparameter tuning. This approach scales well for larger, more complex tasks, such as action recognition, that were previously infeasible.\nThe second paper builds on these insights by providing rigorous analytical results that explain the effects of heterogeneity in LIF and STDP parameters [12]. We demonstrate through mathematical modeling and empirical validation that heterogeneity improves the linear separation properties of unsupervised SNN models. This paper establishes a robust theoretical framework, linking the observed performance enhancements to fundamental properties of heterogeneous networks. We show that optimizing neuronal and synaptic heterogeneity can reduce spiking activity while improving memory capacity, defined as the network's ability to learn and retain information [13], [14].\nDespite the benefits of heterogeneity, the increased computational cost remains a significant concern. The third paper addresses this by introducing Lyapunov Noise Pruning (LNP), a novel method that exploits the heterogeneity to prune the network efficiently [15]. By identifying and removing redundant or noisy connections, LNP maintains high performance while significantly reducing computational demands. Grounded in Lyapunov stability theory, this task-agnostic pruning strategy ensures the pruned network remains stable and effective across various applications without the need for extensive retraining.\nThe key contributions of these three papers are as follows:\n\u2022 Heterogeneous Recurrent Spiking Neural Networks (HRSNNs): Introduction of heterogeneity in LIF and STDP dynamics, demonstrating enhanced robustness and adaptability of SNNs [7].\n\u2022 Analytical Insights into HRSNNs: Detailed mathematical modeling and empirical verification of the effects of"}, {"title": "II. RELATED WORKS", "content": "The field of Spiking Neural Networks (SNNs) is enriched by various foundational and contemporary studies. SNNs leverage the temporal dynamics of spiking neurons, which has been a significant focus in neuroscience and machine learning. Early work on SNNs by [16] established the fundamental concepts of spiking neurons and their bio-inspired learning mechanisms, such as Spike-Timing Dependent Plasticity (STDP) [17][18], [19].\nEmpirical results have shown the effectiveness of SNNs in tasks like spatiotemporal data classification [20], [21], sequence-to-sequence mapping [22], object detection [23], [24], and universal function approximation [25], [26]. The energy efficiency of SNNs, attributed to their sparse firing activity, is a critical advantage highlighted in studies by [27], [28], and [29]. However, optimizing spiking activity while maintaining performance is a key challenge addressed by recent empirical and analytical studies [30], [31].\nThe concept of heterogeneity in SNNs has been explored to improve performance and efficiency. Studies by [32], [33], and [34] have shown that biological systems naturally employ heterogeneous dynamics to enhance robustness and adaptability. In the context of SNNs, [8] and [10] demonstrated that heterogeneity in neuronal time constants could improve classification performance, though they lacked a theoretical foundation. Our work builds on these insights, providing both empirical evidence and theoretical explanations for the benefits of heterogeneity.\nRecent advancements in optimizing SNNs have been marked by the development of novel pruning methods. The Lyapunov Noise Pruning (LNP) method introduced by [15] represents a significant step forward. LNP leverages heterogeneity to identify and prune redundant connections, thereby reducing computational complexity while maintaining network stability and performance. This approach contrasts with traditional activity-based pruning methods and highlights the importance of theoretical grounding in developing efficient neural network models.\nIn addition to these foundational works, studies on the practical applications of SNNs in various domains underscore their versatility. [35] and [36] explored the use of SNNs in brain-computer interfacing and EEG electrode optimization, respectively, demonstrating the potential for SNNs in biomedical applications. [23] and [37] further highlighted the application of SNNs in object detection and edge intelligence.\nThe integration of heterogeneous dynamics in SNNs for improved performance and energy efficiency has also been explored by [7] and [38], providing a robust framework for future research in this area. The intersection of theoretical insights and practical applications positions SNNs as a promising technology for next-generation neural network models."}, {"title": "III. METHODS", "content": "SNN consists of spiking neurons connected with synapses. The spiking LIF is defined by the following equations:\n$\\Tm \\frac{dv}{dt} = a + R_mI - v; v = v_{reset}, if v > U_{threshold}$                                                                                                                                                (1)\nwhere Rm is membrane resistance, Tm = RmCm is time constant and Cm is membrane capacitance. a is the resting potential. I is the sum of current from all input synapses connected to the neuron. A spike is generated when membrane potential v crosses the threshold, and the neuron enters refractory period r, during which the neuron maintains its membrane potential at vreset. We construct the HRSNN from the baseline recurrent spiking network (RSNN) consisting of three layers: (1) an input encoding layer (1), (2) a recurrent spiking layer (R), and (3) an output decoding layer (O). The recurrent layer consists of excitatory and inhibitory neurons, distributed in a ratio of NE: NI = 4 : 1. The PSPs of post-synaptic neurons produced by the excitatory neurons are positive, while those produced by the inhibitory neurons are negative. We used a biologically plausible LIF neuron model and trained the model using STDP rules.\nFrom here on, we refer to connections between I and R neurons as SIR connections, inter-recurrent layer connections as SRR, and R to O as SRO. We created SRR connections using probabilities based on Euclidean distance, D(i,j), between any two neurons i, j:\n$P(i,j) = C \\cdot exp\\left(-\\frac{D(i,j)^2}{\\lambda^2}\\right)$                                                                                                                                                                         (2)\nwith closer neurons having higher connection probability. Parameters C and \u03bb set the amplitude and horizontal shift, respectively, of the probability distribution. I contains exci-tatory encoding neurons, which convert input data into spike trains. SIR only randomly chooses 30% of the excitatory and inhibitory neurons in R as the post-synaptic neuron. The connection probability between the encoding neurons and neurons in the R is defined by a uniform probability PIR, which, together with \u03bb, will be used to encode the architecture of the HRSNN and optimized using BO. In this work, each neuron received projections from some randomly selected neurons in R."}, {"title": "IV. ANALYTICAL RESULTS", "content": "We used unsupervised, local learning to the spiking recurrent model by letting STDP change each Srr and STR connection, modeled as:\n$\\frac{dW}{dt} = A_+ T_{pre} \\delta(t-t_{post}) - A_- T_{post} \\Sigma_i \\delta(t-t_{pre}^i)$                                                                                                                                                             (3)\nwhere A+, A\u2212 are the potentiation/depression learning rates and Tpre/Tpost are the pre/post-synaptic trace variables, modeled as,\n$\\frac{dT_{pre}}{T_*} = -T_{pre} + \\alpha_+ \\Sigma_i \\delta(t-t_{pre}^i)$                                                                                                                                                   (4)\n$dt$\n$\\frac{dT_{post}}{T_*} = -T_{post} + \\alpha_- \\Sigma_i \\delta(t-t_{post}^i)$                                                                                                                                                (5)\n$dt$\nwhere \u03b1+, \u03b1\u2212 are the discrete contributions of each spike to the trace variable, \u03c4\u2217, \u03c4\u2217 are the decay time constants, $t_{pre}^i$ and $t_{post}^i$ are the times of the pre-synaptic and post-synaptic spikes, respectively.\nHeterogeneous LIF Neurons The use of multiple timescales in spiking neural networks has several underlying benefits, like increasing the memory capacity of the network. In this paper, we propose the usage of heterogeneous LIF neurons with different membrane time constants and threshold voltages, thereby giving rise to multiple timescales. Due to differential effects of excitatory and inhibitory heterogeneity on the gain and asynchronous state of sparse cortical networks [39], [40], we use different gamma distributions for both the excitatory and inhibitory LIF neurons. This is also inspired by the brain's biological observations, where the time constants for excitatory neurons are larger than the time constants for the inhibitory neurons. Thus, we incorporate the heterogeneity in our Recurrent Spiking Neural Network by using different membrane time constants \u03c4 for each LIF neuron in R. This gives rise to a distribution for the time constants of the LIF neurons in R.\nHeterogeneous STDP Experiments on different brain re-gions and diverse neuronal types have revealed a wide variety of STDP forms that vary in plasticity direction, temporal dependence, and the involvement of signaling pathways [41],\n[42], [43]. As described by Pool et al. [44], one of the most striking aspects of this plasticity mechanism in synaptic efficacy is that the STDP windows display a great variety of forms in different parts of the nervous system. However, most STDP models used in Spiking Neural Networks are homogeneous with uniform timescale distribution. Thus, we explore the advantages of using heterogeneities in several hyperparameters discussed above. This paper considers heterogeneity in the scaling function constants (A+, A-) and the decay time constants (\u03c4+, \u03c4\u2212).\nWe theoretically compare the performance of the het-erogeneous spiking recurrent model with its homogeneous counterpart using a binary classification problem. The ability of HRSNN to distinguish between many inputs is studied through the lens of the edge-of-chaos dynamics of the spiking recurrent neural network, similar to the case in spiking reservoirs shown by Legenstein et al. [45]. Also, R possesses a fading memory due to its short-term synaptic plasticity and recurrent connectivity. For each stimulus, the final state of the R, i.e., the state at the end of each stimulus, carries the most information. The authors showed that the rank of the final state matrix F reflects the separation property of a kernel: F = [S(1) \u00b7\u00b7\u00b7 S(2) \u00b7\u00b7\u00b7 S(N)] where S(i) is the final state vector of R for the stimulus i. Each element of F represents one neuron's response to all the N stimuli. A higher rank in F indicates better kernel separation if all N inputs are from N distinct classes.\nThe effective rank is calculated using Singular Value Decom-position (SVD) on F, and then taking the number of singular values that contain 99% of the sum in the diagonal matrix as the rank. i.e. F = U\u03a3VT where U and V are unitary matrices, and \u03a3 is a diagonal matrix diag (\u03bb1, \u03bb2, \u03bb3, ..., \u03bbN) that contains non-negative singular values such that (\u03bb1 \u2265 \u03bb2\u2026 \u2265 \u03bbN).\nDefinition: Linear separation property of a neuronal circuit C for m different inputs u1,..., um(t) is defined as the rank of the nx m matrix M whose columns are the final circuit states xui (to) obtained at time to for the preceding input stream u\u017c. Following from the definition introduced by Legenstein et al. [45], if the rank of the matrix M = m, then for the inputs ui, any given assignment of target outputs yi \u2208 R at time to can be implemented by C.\nWe use the rank of the matrix as a measure for the linear separation of a circuit C for distinct inputs. This leverages the complexity and diversity of nonlinear operations carried out by C on its input to boost the classification performance of a subsequent linear decision-hyperplane.\nTheorem 1: Assuming Su is finite and contains s inputs, let rHom, rHet are the ranks of the nx s matrices consisting of the s vectors xu (to) for all inputs u in Su for each of Homogeneous and Heterogeneous RSNNs respectively. Then rHom\u2264 rHet."}, {"title": "C. Optimal Hyperparameter Selection using Bayesian Opti-mization", "content": "While BO is used in various settings, successful applications are often limited to low-dimensional problems, with fewer than twenty dimensions [46]. Thus, using BO for high-dimensional problems remains a significant challenge. In our case of optimizing HRSNN model parameters for 2000, we need to optimize a huge number of parameters, which is extremely difficult for BO. As discussed by Eriksson et al. [47], suitable function priors are especially important for good performance. Thus, we used a biologically inspired initialization of the hyperparameters derived from the human brain (see Supplementary for details).\nThis paper uses a modified BO to estimate parameter distributions for the LIF neurons and the STDP dynamics. To learn the probability distribution of the data, we modify the surrogate model and the acquisition function of the BO to treat the parameter distributions instead of individual variables. This makes our modified BO highly scalable over all the variables (dimensions) used. The loss for the surrogate model's update is calculated using the Wasserstein distance between the parameter distributions.\nBO uses a Gaussian process to model the distribution of an objective function and an acquisition function to decide points to evaluate. For data points in a target dataset x \u2208 X and the corresponding label y \u2208 Y, an SNN with network structure V and neuron parameters W acts as a function fv,w(x) that maps input data x to predicted label \u1ef9. The optimization problem in this work is defined as\n$\\min_{V,W} \\Sigma_{x \\in X, y \\in Y} L(y, f_{v,w}(x))$                                                                                                                                                              (7)\nwhere V is the set of hyperparameters of the neurons in R (Details of hyperparameters given in the Supplementary) and W is the multi-variate distribution constituting the distributions of (i) the membrane time constants Tm\u2212E, Tm\u2212I of the LIF neurons, (ii) the scaling function constants (A+, A\u2212) and (iii) the decay time constants \u03c4+, \u03c4\u2212 for the STDP learning rule in SRR.\nAgain, BO needs a prior distribution of the objective function f(x) on the given data D1:k = {x1:k, f (x1:k)}. In GP-based BO, it is assumed that the prior distribution of f (x1:k) follows the multivariate Gaussian distribution, which follows a Gaussian Process with mean \u00b5D1:k and covariance \u03a3D1:k. We estimate \u03a3D1:k using the modified Matern kernel function, which is given in Eq. 6. In this paper, we use d(x,x') as the Wasserstein distance between the multivariate distributions of the different parameters. It is to be noted here that for higher-dimensional metric spaces, we use the Sinkhorn distance as a"}, {"title": "A. Preliminaries and Definitions", "content": "We define heterogeneity as a measure of the variability of the hyperparameters in an RSNN that gives rise to an ensemble of neuronal dynamics.\nEntropy is used to measure population diversity. Assuming that the random variable for the hyperparameters X follows a multivariate Gaussian Distribution (X ~ N(\u03bc, \u03a3)), then the differential entropy of x on the multivariate Gaussian distribu-tion, is $H(x) = \\frac{n}{2}ln(2\\pi e) + \\frac{1}{2}ln \\left \\|\\Sigma\\right \\|$ . Now, if we take any density function q(x) that satisfies \u222bq(x)xxdx = \u03a3ij and p ~ N(0, \u03a3), then H(q) \u2264 H(p). The Gaussian distribution maximizes the entropy for a given covariance. Hence, the log-determinant of the covariance matrix bounds entropy. Thus, for the rest of the paper, we use the determinant of the covariance matrix to measure the heterogeneity of the network.\nGiven an input signal x(t), the memory capacity C of a trained RSNN model is defined as a measure for the ability of the model to store and recall previous inputs fed into the network [49], [50].\nIn this paper, we use C as a measure of the performance of the model, which is based on the network's ability to retrieve past information (for various delays) from the reservoir using the linear combinations of reservoir unit activations observed at the output. Intuitively, HRSNN can be interpreted as a set of coupled filters that extract features from the input signal. The final readout selects the right combination of those features for classification or prediction. First, the \u03c4-delay C measures the performance of the RC for the task of reconstructing the delayed version of model input x(t) at delay \u03c4 (i.e., x(t \u2212 \u03c4) ) and is defined as the squared correlation coefficient between the desired output (\u03c4-time-step delayed input signal, x(t-\u03c4)) and the observed network output y\u03c4(t), given as:\n$\\mathcal{C} = \\lim_{T_{max} \\to \\infty} \\Sigma_{\\tau=1}^{T_{max}} C(\\tau) = \\lim_{T_{max} \\to \\infty} \\Sigma_{\\tau} \\frac{max Cov^2(x(t), y_\\tau(t))}{Var(x(t)) Var (y_\\tau(t))}$                                                                                                                                           (9)\nwhere Cov() and Var() denote the covariance function and variance function, respectively. The y\u03c4(t) is the model output in this reconstruction task. C measures the ability of RSNN to reconstruct precisely the past information of the model input. Thus, increasing C indicates the network is capable of learning a greater number of past input patterns, which in turn, helps in increasing the performance of the model. For the simulations, we use Tmax = 100.\nGiven an input signal x(t), the spike-efficiency (E) of a trained RSNN model is defined as the ratio of the memory capacity C to the average total spike count per neuron S.\nE is an analytical measure used to compare how C and hence the model's performance is improved with per unit spike activity in the model. Ideally, we want to design a system with high C using fewer spikes. Hence we define E as the ratio of the memory capacity using NR neurons C(NR) to the average number of spike activations per neuron (S) and is given as:\n$E = \\frac{C(N_R)}{S}; S = \\frac{\\Sigma_{i=1}^{N_R} S_i}{N_R}; S_i = \\frac{\\int_0^T \\Phi_i(t) dt}{T} = \\frac{\\Sigma_{n_{post}} \\int_{t_{ref}}^{\\infty} t f_{i,n}(t) dt}{T} \\approx \\frac{\\Sigma_{n_{post}} \\frac{t_{id}}{t_{ref}}}{T}$                                                 (10)\nwhere Npost is the number of postsynaptic neurons, \u03a6i is the inter-spike interval spike frequency for neuron i, and T is the total time. It is to be noted here that the total spike count S is obtained by counting the total number of spikes in all the neurons in the recurrent layer until the emission of the first spike at the readout layer.\nWe present three main analytical findings. Firstly, neuronal dynamic heterogeneity increases memory capacity by capturing more principal components from the input space, leading to better performance and improved C. Secondly, STDP dynamic heterogeneity decreases spike activation without affecting C, providing better orthogonalization among the recurrent network states and a more efficient representation of the input space, lowering higher-order correlation in spike trains. This makes the model more spike-efficient since the higher-order correlation progressively decreases the information available through neural population [51], [52]. Finally, incorporating heterogeneity in both neuron and STDP dynamics boosts the C to spike activity ratio, i.e., E, which enhances performance while reducing spike counts.\nThe performance of an RSNN depends on its ability to retain the memory of previous inputs. To quantify the relationship between the recurrent layer dynamics and C, we note that extracting information from the recurrent layer is made using a combination of the neuronal states. Hence, more linearly independent neurons would offer more variable states and, thus, more extended memory.\nLemma 3.1.1: The state of the neuron can be written as\n$r_i(t) = \\Sigma_{k=0}^{N_R} \\Sigma_{n=1}^{N_R} (v_n, w_{in}) (v_n)^\\dag x(t - k)$, where"}, {"title": "LN Pruning (Ours)", "content": "Here x represents the firing rate of N neurons, with xi specifying the firing rate of neuron i. b(t) denotes external input, including biases, and L is the previously defined Lyapunov matrix between neurons. D is a diagonal matrix indicating neurons' intrinsic leak or excitability, and A is defined as A = \u2212D + L. The intrinsic leak/excitability, Dii, quantifies how the firing rate, xi, of neuron i alters without external input or interaction, impacting the neural network's overall dynamics along with \u00e6, L, and external inputs b(t). Positive Dii suggests increased excitability and firing rate, while negative values indicate reduced neuron activity over time. We aim to create a sparse network (Asparse) with fewer edges while maintaining dynamics similar to the original network. The sparse network is thus represented as:\n$\\dot x = A^{sparse} x + b(t)$\ns.t. xT(Asparse \u2212 A) x \u2264 \u03f5 xTAx \u2200x\u2208RN\n(15)\nfor some small \u03f5 > 0. When the network in Eq. 14 is driven by independent noise at each node, we define b(t) = b+\u03c3\u03ad(t), where b is a constant input vector, \u03ad is a vector of IID Gaussian white noise, and \u03c3 is the noise standard deviation. Let \u03a3 be the covariance matrix of the firing rates in response to this input. The probability pij for the synapse from neuron j to neuron i with the Lyapunov exponent lij is defined as:\npij = {|p||lij|(\u03a3ii + \u03a3jj \u2212 2\u03a3ij)for wij > 0 (excitatory)|p||lij|(\u03a3ii + \u03a3jj + 2\u03a3ij)for wij < 0 (inhibitory)\nHere, p determines the density of the pruned network. The pruning process independently preserves each edge with probability pij, yielding Asparse, where Asparse = Aij/pij, with probability pij and 0 otherwise. For the diagonal elements, denoted as A ii sparse, representing leak/excitability, we either retain the original diagonal, setting A ii sparse = Aii, or we introduce a perturbation, \u0394i, defined as the difference in total input to neuron i, and adjust the diagonal as A ii sparse = Aii \u2212 \u0394i. Specifically, \u0394i = \u03a3jji|Ajsparse] \u2212 \u03a3j\u2021i|Aij|. This perturbation, \u0394i, is typically minimal with a zero mean and is interpreted biologically as a modification in the excitability of neuron idue to alterations in total input, aligning with the known homeostatic regulation of excitability.\nIn addressing network optimization, we propose an algorithm specifically designed to prune nodes with the lowest between-ness centrality (Cb) in a given graph, thereby refining the graph to its most influential components. Cb quantifies the influence a node has on information flow within the network. Thus, we calculate Co for each node and prune the nodes with the least values below a given threshold, ensuring the retention of nodes integral to the network's structural and functional integrity.\nStep III: Delocalizing Eigenvectors: To preserve eigen-vector delocalization and enhance long-term prediction per-formance post-pruning, we introduce a predetermined number of additional edges to counteract eigenvalue localization due to network disconnection. Let G = (V, E) and G' = (V, E') represent the original and pruned graphs, respectively, where E' CE. We introduce additional edges, E", "E'": "L,\nwhere L is a predetermined limit. This is formalized as an"}, {"title": "VI. RESULTS", "content": "We use SAGE (Shapley Additive Global importancE) [55], a game-theoretic approach to understand black-box models to calculate the significance of adding heterogeneity to each parameter for improving C and S. SAGE summarizes the importance of each feature based on the predictive power it contributes and considers complex feature interactions using the principles of Shapley value, with a higher SAGE value signifying a more important feature. We tested the HRSNN model using SAGE on the Lorenz96 and the SHD datasets. The results are shown in Fig. 4. We see that Tm has the greatest SAGE values for C, signifying that it has the greatest impact on improving C when heterogeneity is added. Conversely, we see that heterogeneous STDP parameters (viz., \u03c4\u00b1,\u03c4+) play a more critical role in determining the average neuronal spike activation. Hence, we confirm the notions proved in Sec. 3 that heterogeneity in neuronal dynamics improves the C while heterogeneity in STDP dynamics improves the spike count. Thus, we need to optimize the heterogeneity of both to achieve maximum E.\nWe compare the performance of the HRSNN model with heterogeneity in the LIF and STDP dynamics (HeNHeS) to the ablation baseline recurrent spiking neural network models described above. We run five iterations for all the baseline cases and show the mean and standard deviation of the prediction accuracy of the network using 2000 neurons. The results are shown in Table I. We see that the heterogeneity in the LIF neurons and the LTP/LTD dynamics significantly improve the model's accuracy and error.\nIn deep learning, it is an important task to design models with a lesser number of neurons without undergoing degradation in performance. We empirically show that heterogeneity plays a critical role in designing spiking neuron models of smaller sizes. We compare models' performance and convergence rates with fewer neurons in R.\nPerformance Analysis: We analyze the network perfor-mance and error when the number of neurons is decreased from 2000 to just 100. We report the results obtained using the HONHOS and HeNHeS models for the KTH and DVS-Gesture datasets. The experiments are repeated five times, and the observed mean and standard deviation of the accuracies are shown in Figs. 5. The graphs show that as the number of neurons decreases, the difference in accuracy scores between the homogeneous and the heterogeneous networks increases rapidly.\nConvergence Analysis with lesser neurons: Since the complexity of BO increases exponentially on increasing the search space, optimizing the HRSNN becomes increasingly difficult as the number of neurons increases. Thus, we compare the convergence behavior of the HoNHOS and HeNHeS models with 100 and 2000 neurons each. The results are plotted in Fig. 6(a), (b). Despite the huge number of additional parameters, the convergence behavior of HeNHeS is similar to that of HONHOS. Also, it must be noted that once converged, the standard deviation of the accuracies for HeNHeS is much lesser than that of HoNHOS, indicating a much more stable model.\nSRR is generated using a probability dependent on the Euclidean distance between the two neurons, as described by Eq. 2, where A controls the density of the connection, and C is a constant depending on the type of the synapses [56].\nWe performed various simulations using a range of values for the connection parameter A and synaptic weight scale Wscale. Increasing A will increase the number of synapses. Second, the Wscale parameter determines the mean synaptic strength. Now, a greater Wscale produces larger weight variance. For a single input video, the number of active neurons was calculated and plotted against the parameter values for synaptic weight Wscale and network connectivity \u03bb. Active neurons are those that fire"}, {"title": "F. Lyapunov Noise Based Pruning (LNP) Method", "content": "The experimental process begins with a randomly initialized HRSNN and CHRSNN. Pruning algorithms are used to create a sparse network. Each iteration of pruning results in a sparse network; we experiment with 100 iterations of pruning. We characterize the neuron and synaptic distributions of the \"Sparse HRSNN\" obtained after each pruning iteration to track the reduction of the complexity of the models with pruning.\nWe train the sparse HRSNN model obtained after each prun-ing iteration to estimate performance on various tasks. Note, the pruning process does not consider the trained model or its performance during iterations. The sparse HRSNNS are trained for time-series prediction and image classification tasks. For the prediction task, the network is trained using 500 timesteps of the datasets and is subsequently used to predict"}, {"title": "VII. CONCLUSION", "content": "This paper consolidates and summarizes a series of research studies that collectively advance the field of Spiking Neural Networks (SNNs) through the introduction of heterogeneity in neuron and synapse dynamics, rigorous analytical modeling, and innovative pruning methodologies.\nThe first study introduced the concept of Heterogeneous Recurrent Spiking Neural Networks (HRSNNs) with variability in LIF neuron parameters and STDP dynamics, demonstrating significant improvements in performance, robustness, and efficiency in handling complex spatio-temporal datasets for action recognition tasks. By incorporating heterogeneity, this research highlighted the potential of smaller, more efficient models with sparse connections that require less training data.\nBuilding upon this, the second study provided a compre-hensive analytical framework that elucidates the benefits of heterogeneity in enhancing memory capacity and reducing spiking activity in SNNs. This work established critical mathe-matical properties that align with neurobiological observations, emphasizing the importance of variability in achieving efficient and robust neural computations.\nThe final study addressed the computational challenges associated with heterogeneity by introducing the Lyapunov Noise Pruning (LNP) method. This novel pruning strategy leverages the inherent heterogeneity to reduce the number of neurons and synapses, thereby decreasing computational complexity while maintaining high performance. The task-agnostic nature of LNP ensures its applicability across diverse scenarios, making it a versatile tool for optimizing SNNs.\nIn conclusion, these studies collectively demonstrate that heterogeneity in SNNs is not only beneficial but essential for developing high-performance, energy-efficient neural networks. By bridging theoretical insights with practical applications, this body of work paves the way for future innovations in neuromorphic computing and related fields. The advancements presented here underscore the transformative potential of SNNs, offering new avenues for research and application in machine learning and artificial intelligence."}]}