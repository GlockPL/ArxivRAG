{"title": "Exploiting Heterogeneity in Timescales for Sparse Recurrent Spiking Neural Networks for Energy-Efficient Edge Computing", "authors": ["Biswadeep Chakraborty", "Saibal Mukhopadhyay"], "abstract": "Spiking Neural Networks (SNNs) represent the forefront of neuromorphic computing, promising energy-efficient and biologically plausible models for complex tasks. This paper weaves together three groundbreaking studies that revolutionize SNN performance through the introduction of heterogeneity in neuron and synapse dynamics. We explore the transformative impact of Heterogeneous Recurrent Spiking Neural Networks (HRSNNs), supported by rigorous analytical frameworks and novel pruning methods like Lyapunov Noise Pruning (LNP). Our findings reveal how heterogeneity not only enhances classification performance but also reduces spiking activity, leading to more efficient and robust networks. By bridging theoretical insights with practical applications, this comprehensive summary highlights the potential of SNNs to outperform traditional neural networks while maintaining lower computational costs. Join us on a journey through the cutting-edge advancements that pave the way for the future of intelligent, energy-efficient neural computing.", "sections": [{"title": "I. INTRODUCTION", "content": "Spiking Neural Networks (SNNs), often referred to as the third generation of neural networks, have garnered significant attention due to their potential for lower operating power when mapped to hardware. SNNs, particularly those using leaky integrate-and-fire (LIF) neurons, have demonstrated classification performance comparable to deep neural networks (DNNs). However, the majority of these models rely on supervised training algorithms like backpropagation-through-time (BPTT) [1], [2], [3], which are highly data-dependent and struggle with limited training data and generalization [4], [5]. Moreover, BPTT-trained models require complex architectures with a large number of neurons to achieve good performance. While unsupervised learning methods such as Spike-Timing Dependent Plasticity (STDP) have been introduced, they typically underperform compared to their supervised counterparts due to the high complexity of training [6].\nTo address these challenges, this paper introduces a series of studies focused on enhancing SNN performance through heterogeneity in network dynamics. The first paper presents a Heterogeneous Recurrent Spiking Neural Network (HRSNN) with variability in both LIF neuron parameters and STDP dynamics [7]. Previous works have suggested that heterogeneity in neuron time constants can improve classification performance [8], [9], [10], [11], but lacked theoretical explanations for these improvements. Our study not only explores how heterogeneity in both neuronal and synaptic parameters can enhance per-formance with less training data and fewer connections but also leverages a novel Bayesian Optimization (BO) method for hyperparameter tuning. This approach scales well for larger, more complex tasks, such as action recognition, that were previously infeasible.\nThe second paper builds on these insights by providing rigor-ous analytical results that explain the effects of heterogeneity in LIF and STDP parameters [12]. We demonstrate through math-ematical modeling and empirical validation that heterogeneity improves the linear separation properties of unsupervised SNN models. This paper establishes a robust theoretical framework, linking the observed performance enhancements to fundamental properties of heterogeneous networks. We show that optimizing neuronal and synaptic heterogeneity can reduce spiking activity while improving memory capacity, defined as the network's ability to learn and retain information [13], [14].\nDespite the benefits of heterogeneity, the increased compu-tational cost remains a significant concern. The third paper addresses this by introducing Lyapunov Noise Pruning (LNP), a novel method that exploits the heterogeneity to prune the network efficiently [15]. By identifying and removing redundant or noisy connections, LNP maintains high performance while significantly reducing computational demands. Grounded in Lyapunov stability theory, this task-agnostic pruning strategy ensures the pruned network remains stable and effective across various applications without the need for extensive retraining.\nThe key contributions of these three papers are as follows:\n\u2022 Heterogeneous Recurrent Spiking Neural Networks (HRSNNs): Introduction of heterogeneity in LIF and STDP dynamics, demonstrating enhanced robustness and adaptability of SNNs [7].\n\u2022 Analytical Insights into HRSNNs: Detailed mathematical modeling and empirical verification of the effects of"}, {"title": "II. RELATED WORKS", "content": "The field of Spiking Neural Networks (SNNs) is enriched by various foundational and contemporary studies. SNNs leverage the temporal dynamics of spiking neurons, which has been a significant focus in neuroscience and machine learning. Early work on SNNs by [16] established the fundamental concepts of spiking neurons and their bio-inspired learning mechanisms, such as Spike-Timing Dependent Plasticity (STDP) [17][18], [19].\nEmpirical results have shown the effectiveness of SNNs in tasks like spatiotemporal data classification [20], [21], sequence-to-sequence mapping [22], object detection [23], [24], and universal function approximation [25], [26]. The energy efficiency of SNNs, attributed to their sparse firing activity, is a critical advantage highlighted in studies by [27], [28], and [29]. However, optimizing spiking activity while maintaining performance is a key challenge addressed by recent empirical and analytical studies [30], [31].\nThe concept of heterogeneity in SNNs has been explored to improve performance and efficiency. Studies by [32], [33], and [34] have shown that biological systems naturally employ heterogeneous dynamics to enhance robustness and adaptability. In the context of SNNs, [8] and [10] demonstrated that heterogeneity in neuronal time constants could improve classification performance, though they lacked a theoretical foundation. Our work builds on these insights, providing both empirical evidence and theoretical explanations for the benefits of heterogeneity.\nRecent advancements in optimizing SNNs have been marked by the development of novel pruning methods. The Lyapunov Noise Pruning (LNP) method introduced by [15] represents a significant step forward. LNP leverages heterogeneity to identify and prune redundant connections, thereby reducing computational complexity while maintaining network stability and performance. This approach contrasts with traditional activity-based pruning methods and highlights the importance of theoretical grounding in developing efficient neural network models.\nIn addition to these foundational works, studies on the practical applications of SNNs in various domains underscore their versatility. [35] and [36] explored the use of SNNs in brain-computer interfacing and EEG electrode optimization, re-spectively, demonstrating the potential for SNNs in biomedical applications. [23] and [37] further highlighted the application of SNNs in object detection and edge intelligence.\nThe integration of heterogeneous dynamics in SNNs for improved performance and energy efficiency has also been explored by [7] and [38], providing a robust framework for future research in this area. The intersection of theoretical in-sights and practical applications positions SNNs as a promising technology for next-generation neural network models."}, {"title": "III. METHODS", "content": "SNN consists of spiking neurons connected with synapses. The spiking LIF is defined by the following equations:\n$\\tau_m \\frac{dv}{dt} = \\alpha + R_mI - v; v = v_{reset}, \\text{ if } v > v_{threshold}$ (1)\nwhere $R_m$ is membrane resistance, $\\tau_m = R_mC_m$ is time constant and $C_m$ is membrane capacitance. $\\alpha$ is the resting potential. $I$ is the sum of current from all input synapses connected to the neuron. A spike is generated when membrane potential $v$ crosses the threshold, and the neuron enters refractory period $r$, during which the neuron maintains its membrane potential at $v_{reset}$. We construct the HRSNN from the baseline recurrent spiking network (RSNN) consisting of three layers: (1) an input encoding layer ($I$), (2) a recurrent spiking layer ($R$), and (3) an output decoding layer ($O$). The recurrent layer consists of excitatory and inhibitory neurons, distributed in a ratio of $N_E : N_I = 4:1$. The PSPs of post-synaptic neurons produced by the excitatory neurons are positive, while those produced by the inhibitory neurons are negative. We used a biologically plausible LIF neuron model and trained the model using STDP rules.\nFrom here on, we refer to connections between $I$ and $R$ neurons as $SIR$ connections, inter-recurrent layer connections as $SRR$, and $R$ to $O$ as $SRO$. We created $SRR$ connections using probabilities based on Euclidean distance, $D(i,j)$, between any two neurons $i, j$:\n$P(i,j) = C \\cdot exp\\left(-\\frac{D(i,j)^2}{\\lambda^2}\\right)$ (2)\nwith closer neurons having higher connection probability. Parameters $C$ and $\\lambda$ set the amplitude and horizontal shift, respectively, of the probability distribution. $I$ contains exci-tatory encoding neurons, which convert input data into spike trains. $SIR$ only randomly chooses 30% of the excitatory and inhibitory neurons in $R$ as the post-synaptic neuron. The connection probability between the encoding neurons and neurons in the $R$ is defined by a uniform probability $P_{IR}$, which, together with $\\lambda$, will be used to encode the architecture of the HRSNN and optimized using BO. In this work, each neuron received projections from some randomly selected neurons in $R$."}, {"title": "B. Classification Property of HRSNN", "content": "We theoretically compare the performance of the het-erogeneous spiking recurrent model with its homogeneous counterpart using a binary classification problem. The ability of HRSNN to distinguish between many inputs is studied through the lens of the edge-of-chaos dynamics of the spiking recurrent neural network, similar to the case in spiking reservoirs shown by Legenstein et al. [45]. Also, R possesses a fading memory due to its short-term synaptic plasticity and recurrent connectivity. For each stimulus, the final state of the R,\ni.e., the state at the end of each stimulus, carries the most information. The authors showed that the rank of the final state matrix F reflects the separation property of a kernel:\n$F = [S(1) S(2) ... S(N)]$ where $S(i)$ is the final state vector of R for the stimulus i. Each element of F represents one neuron's response to all the N stimuli. A higher rank in F indicates better kernel separation if all N inputs are from N distinct classes.\nThe effective rank is calculated using Singular Value Decom-position (SVD) on F, and then taking the number of singular values that contain 99% of the sum in the diagonal matrix as the rank. i.e. $F = U\\Sigma V^T$ where U and V are unitary matrices, and $\\Sigma$ is a diagonal matrix $diag (\\lambda_1, \\lambda_2, \\lambda_3, ..., \\lambda_N)$ that contains non-negative singular values such that $(\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_N)$.\nDefinition: Linear separation property of a neuronal circuit C for m different inputs $u_1,..., u_m(t)$ is defined as the rank of the n x m matrix M whose columns are the final circuit states $x_{u_i}(t_0)$ obtained at time $t_0$ for the preceding input stream $u_i$.\nFollowing from the definition introduced by Legenstein et al. [45], if the rank of the matrix M = m, then for the inputs $u_i$, any given assignment of target outputs $y_i \\in \\mathbb{R}$ at time $t_0$ can be implemented by C.\nWe use the rank of the matrix as a measure for the linear separation of a circuit C for distinct inputs. This leverages the complexity and diversity of nonlinear operations carried out by C on its input to boost the classification performance of a subsequent linear decision-hyperplane.\nTheorem 1: Assuming $S_u$ is finite and contains s inputs, let $\\xi_{Hom}, \\xi_{Het}$ are the ranks of the n x s matrices consisting of the s vectors $x_u(t_0)$ for all inputs u in $S_u$ for each of Homogeneous and Heterogeneous RSNNs respectively. Then $\\xi_{Hom} \\leq \\xi_{Het}$."}, {"title": "C. Optimal Hyperparameter Selection using Bayesian Opti-mization", "content": "While BO is used in various settings, successful applications are often limited to low-dimensional problems, with fewer than twenty dimensions [46]. Thus, using BO for high-dimensional problems remains a significant challenge. In our case of optimizing HRSNN model parameters for 2000, we need to optimize a huge number of parameters, which is extremely difficult for BO. As discussed by Eriksson et al. [47], suitable function priors are especially important for good performance. Thus, we used a biologically inspired initialization of the hyperparameters derived from the human brain (see Supplementary for details).\nThis paper uses a modified BO to estimate parameter distributions for the LIF neurons and the STDP dynamics. To learn the probability distribution of the data, we modify the surrogate model and the acquisition function of the BO to treat the parameter distributions instead of individual variables. This makes our modified BO highly scalable over all the variables (dimensions) used. The loss for the surrogate model's update is calculated using the Wasserstein distance between the parameter distributions.\nBO uses a Gaussian process to model the distribution of an objective function and an acquisition function to decide points to evaluate. For data points in a target dataset $x \\in X$ and the corresponding label $y \\in Y$, an SNN with network structure $V$ and neuron parameters $W$ acts as a function $f_{V,W}(x)$ that maps input data x to predicted label $\\tilde{y}$. The optimization problem in this work is defined as\n$\\min_{V,W} \\sum_{x \\in X,y \\in Y} L\\left(y, f_{V,W}(x)\\right)$ (7)\nwhere $V$ is the set of hyperparameters of the neurons in R (Details of hyperparameters given in the Supplementary) and $W$ is the multi-variate distribution constituting the distributions of (i) the membrane time constants $\\tau_{m-E}, \\tau_{m-I}$ of the LIF neurons, (ii) the scaling function constants $(A_+, A_-)$ and (iii) the decay time constants $\\tau_+, \\tau_-$ for the STDP learning rule in $SRR$.\nAgain, BO needs a prior distribution of the objective function $f(x)$ on the given data $D_{1:k} = \\{x_{1:k}, f(x_{1:k})\\}$. In GP-based BO, it is assumed that the prior distribution of $f(x_{1:k})$ follows the multivariate Gaussian distribution, which follows a Gaussian Process with mean $\\mu_{D_{1:k}}$ and covariance $\\Sigma_{D_{1:k}}$. We estimate $\\mu_{D_{1:k}}$ using the modified Matern kernel function, which is given in Eq. 6. In this paper, we use $d(x,x')$ as the Wasserstein distance between the multivariate distributions of the different parameters. It is to be noted here that for higher-dimensional metric spaces, we use the Sinkhorn distance as a"}, {"title": "IV. ANALYTICAL RESULTS", "content": "Heterogeneity:\nWe define heterogeneity as a measure of the variability of the hyperparameters in an RSNN that gives rise to an ensemble of neuronal dynamics.\nEntropy is used to measure population diversity. Assuming that the random variable for the hyperparameters X follows a multivariate Gaussian Distribution $(\\Chi \\sim N(\\mu, \\Sigma))$, then the differential entropy of x on the multivariate Gaussian distribu-tion, is $H(x) = \\frac{n}{2} ln(2\\pi) + \\frac{1}{2} ln \\|\\Sigma\\|$. Now, if we take any density function $q(x)$ that satisfies $\\int q(x)x_ix_jdx = E_{ij}$ and $p = N(0, \\Sigma)$, then $H(q) \\leq H(p)$. The Gaussian distribution maximizes the entropy for a given covariance. Hence, the log-determinant of the covariance matrix bounds entropy. Thus, for the rest of the paper, we use the determinant of the covariance matrix to measure the heterogeneity of the network.\nMemory Capacity: Given an input signal x(t), the memory capacity C of a trained RSNN model is defined as a measure for the ability of the model to store and recall previous inputs fed into the network [49], [50].\nIn this paper, we use C as a measure of the performance of the model, which is based on the network's ability to retrieve past information (for various delays) from the reservoir using the linear combinations of reservoir unit activations observed at the output. Intuitively, HRSNN can be interpreted as a set of coupled filters that extract features from the input signal. The final readout selects the right combination of those features for classification or prediction. First, the \\tau-delay C measures the performance of the RC for the task of reconstructing the delayed version of model input $x(t)$ at delay $\\tau$ (i.\u0435., $x(t - \\tau)$ ) and is defined as the squared correlation coefficient between the desired output ($\\tau$-time-step delayed input signal, $x(t-\\tau)$) and the observed network output $y_\\tau(t)$, given as:\n$C = \\lim_{T_{max} \\to \\infty} \\sum_{\\tau=1}^{T_{max}} C(\\tau) = \\lim_{T_{max} \\to \\infty} \\sum_{\\tau=1}^{T_{max}} \\frac{Cov^2 (x(t), y(t))}{Var(x(t)) \\cdot Var (y_\\tau(t))}$ (9)\nwhere Cov() and Var() denote the covariance function and variance function, respectively. The $y_\\tau(t)$ is the model output in this reconstruction task. C measures the ability of RSNN to reconstruct precisely the past information of the model input. Thus, increasing C indicates the network is capable of learning a greater number of past input patterns, which in turn, helps in increasing the performance of the model. For the simulations, we use $T_{max} = 100$.\nSpike-Efficiency: Given an input signal x(t), the spike-efficiency ($E$) of a trained RSNN model is defined as the ratio of the memory capacity C to the average total spike count per neuron S.\nE is an analytical measure used to compare how C and hence the model's performance is improved with per unit spike activity in the model. Ideally, we want to design a system with high C using fewer spikes. Hence we define $E$ as the ratio of the memory capacity using $N_R$ neurons $C(N_R)$ to the average number of spike activations per neuron (S) and is given as:\n$E = \\frac{C(N_R)}{S} = \\frac{\\int_0^T s_i(t)dt}{N_R} = \\frac{ \\frac{\\int_0^T \\Phi_i(t) \\cdot N_{post} \\infty t_{ref}}{N_R} dt}$ (10)\nwhere $N_{post}$ is the number of postsynaptic neurons, $\\Phi_i$ is the inter-spike interval spike frequency for neuron i, and T is the total time. It is to be noted here that the total spike count S is obtained by counting the total number of spikes in all the neurons in the recurrent layer until the emission of the first spike at the readout layer.\nWe present three main analytical findings. Firstly, neuronal dynamic heterogeneity increases memory capacity by capturing more principal components from the input space, leading to better performance and improved C. Secondly, STDP dynamic heterogeneity decreases spike activation without affecting C, providing better orthogonalization among the recurrent network states and a more efficient representation of the input space, lowering higher-order correlation in spike trains. This makes the model more spike-efficient since the higher-order correlation progressively decreases the information available through neural population [51], [52]. Finally, incorporating heterogeneity in both neuron and STDP dynamics boosts the C to spike activity ratio, i.e., E, which enhances performance while reducing spike counts.\nMemory Capacity: The performance of an RSNN depends on its ability to retain the memory of previous inputs. To quantify the relationship between the recurrent layer dynamics and C, we note that extracting information from the recurrent layer is made using a combination of the neuronal states. Hence, more linearly independent neurons would offer more variable states and, thus, more extended memory.\nLemma 3.1.1: The state of the neuron can be written as follows: $r_i(t) = \\sum_{n=1}^{N_R} \\sum_{k=0}^{N_R} \\left(v_n, w_{in}\\right) (v_n); x(t - k)$, where $\\lambda_n, v_n^{-1} \\in V$ are, respectively, the left and right eigenvectors of W, $w_{in}$ are the input weights, and $\\lambda \\in \\Lambda$ belongs to the diagonal matrix containing the eigenvalues of W;"}, {"title": "V. LYAPUNOV NOISE PRUNING (LNP) METHOD", "content": "The proposed research presents a pruning algorithm em-ploying spectral graph pruning and Lyapunov exponents in an unsupervised model. We calculate the Lyapunov matrix, optimizing for ratios and rates to handle extreme data values and incorporate all observations. After pruning, nodes with the lowest betweenness centrality are removed to improve network efficiency, and select new edges are added during the delocalization phase to maintain stability and integrity. This method balances structural integrity with computational\nefficacy, contributing to advancements in network optimization. Algorithm 1 shows a high-level algorithm for the entire process.\nStep I: Noise-Pruning of Synapses: First, we define the Lyapunov matrix of the network. To formalize the concept of the Lyapunov Matrix, let us consider a network represented by a graph $G(V, E)$ where V is the set of nodes and E is the set of edges. For each edge $e_{ij}$ connecting nodes i,j let N(z) be the set of neighbors of nodes z, z = {i, j}. Thus, the Lyapunov exponents corresponding to these neighbors are represented as A(N(z)). The element $L_{ij}$ of the Lyapunov Matrix (L) is then calculated using the harmonic mean of the Lyapunov exponents of the neighbors of nodes i, j as:\n$L_{ij} = \\frac{n \\cdot |\\Lambda(N(i)) \\cup \\Lambda(N(j))|}{\\sum_{\\lambda \\in \\Lambda(N(i))\\Lambda(N(j))}}$ (13)\nwhere $\\Lambda$ denotes individual Lyapunov exponents from the set of all such exponents of nodes i and j's neighbors. L, encapsulates the impact of neighboring nodes on each edge regarding their Lyapunov exponents, which helps us evaluate the network's stability and dynamical behavior. Through the harmonic mean, the matrix accommodates the influence of all neighbors, including those with extreme Lyapunov exponents, for a balanced depiction of local dynamics around each edge."}, {"title": "VI. RESULTS", "content": "Heterogeneity Parameter Importance: We use SAGE (Shapley Additive Global importancE) [55], a game-theoretic approach to understand black-box models to calculate the significance of adding heterogeneity to each parameter for improving C and S. SAGE summarizes the importance of each feature based on the predictive power it contributes and considers complex feature interactions using the principles of Shapley value, with a higher SAGE value signifying a more important feature. We tested the HRSNN model using SAGE on the Lorenz96 and the SHD datasets. The results are shown in Fig. 4. We see that $\\tau_m$ has the greatest SAGE values for C, signifying that it has the greatest impact on improving C when heterogeneity is added. Conversely, we see that heterogeneous STDP parameters (viz., $\\tau_\\pm, \\tau_+$) play a more critical role in determining the average neuronal spike activation. Hence, we confirm the notions proved in Sec. 3 that heterogeneity in neuronal dynamics improves the C while heterogeneity in STDP dynamics improves the spike count. Thus, we need to optimize the heterogeneity of both to achieve maximum $E$.\nWe compare the performance of the HRSNN model with heterogeneity in the LIF and STDP dynamics (HeNHeS) to the ablation baseline recurrent spiking neural network models described above. We run five iterations for all the baseline cases and show the mean and standard deviation of the prediction accuracy of the network using 2000 neurons. The results are shown in Table I. We see that the heterogeneity in the LIF neurons and the LTP/LTD dynamics significantly improve the model's accuracy and error.\nIn deep learning, it is an important task to design models with a lesser number of neurons without undergoing degradation in performance. We empirically show that heterogeneity plays a critical role in designing spiking neuron models of smaller sizes. We compare models' performance and convergence rates with fewer neurons in R.\nPerformance Analysis: We analyze the network perfor-mance and error when the number of neurons is decreased from 2000 to just 100. We report the results obtained using the HONHOS and HeNHeS models for the KTH and DVS-Gesture datasets. The experiments are repeated five times, and the observed mean and standard deviation of the accuracies are shown in Figs. 5. The graphs show that as the number of neurons decreases, the difference in accuracy scores between the homogeneous and the heterogeneous networks increases rapidly.\nSince the complexity of BO increases exponentially on increasing the search space, optimizing the HRSNN becomes increasingly difficult as the number of neurons increases. Thus, we compare the convergence behavior of the HoNHOS and HeNHeS models with 100 and 2000 neurons each. The results are plotted in Fig. 6(a), (b). Despite the huge number of additional parameters, the convergence behavior of HeNHeS is similar to that of HONHOS. Also, it must be noted that once converged, the standard deviation of the accuracies for HeNHeS is much lesser than that of HoNHOS, indicating a much more stable model.\n generated using a probability dependent on the Euclidean distance between the two neurons, as described by Eq. 2, where $\\lambda$ controls the density of the connection, and C is a constant depending on the type of the synapses [56].\nIncreasing $\\lambda$ will increase the number of synapses. Second, the $W_{scale}$ parameter determines the mean synaptic strength. Now, a greater $W_{scale}$ produces larger weight variance. For a single input video, the number of active neurons was calculated and plotted against the parameter values for synaptic weight $W_{scale}$ and network connectivity $\\lambda$. Active neurons are those that fire"}, {"title": "VII. CONCLUSION", "content": "This paper consolidates and summarizes a series of research studies that collectively advance the field of Spiking Neural Networks (SNNs) through the introduction of heterogeneity in neuron and synapse dynamics, rigorous analytical modeling, and innovative pruning methodologies.\nThe first study introduced the concept of Heterogeneous Recurrent Spiking Neural Networks (HRSNNs) with variability in LIF neuron parameters and STDP dynamics, demonstrating significant improvements in performance, robustness, and efficiency in handling complex spatio-temporal datasets for action recognition tasks. By incorporating heterogeneity, this research highlighted the potential of smaller, more efficient models with sparse connections that require less training data. Building upon this, the second study provided a compre-hensive analytical framework that elucidates the benefits of heterogeneity in enhancing memory capacity and reducing spiking activity in SNNs. This work established critical mathe-matical properties that align with neurobiological observations, emphasizing the importance of variability in achieving efficient and robust neural computations.\nThe final study addressed the computational challenges associated with heterogeneity by introducing the Lyapunov Noise Pruning (LNP) method. This novel pruning strategy leverages the inherent heterogeneity to reduce the number of neurons and synapses, thereby decreasing computational complexity while maintaining high performance. The task-agnostic nature of LNP ensures its applicability across diverse scenarios, making it a versatile tool for optimizing SNNs.\nIn conclusion, these studies collectively demonstrate that heterogeneity in SNNs is not only beneficial but essential for developing high-performance, energy-efficient neural networks. By bridging theoretical insights with practical applications, this body of work paves the way for future innovations in neuromorphic computing and related fields. The advancements presented here underscore the transformative potential of SNNs, offering new avenues for research and application in machine learning and artificial intelligence."}]}