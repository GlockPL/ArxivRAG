{"title": "MATHGAP: OUT-OF-DISTRIBUTION EVALUATION ON PROBLEMS WITH ARBITRARILY COMPLEX PROOFS", "authors": ["Andreas Opedal", "Haruki Shirakami", "Bernhard Sch\u00f6lkopf", "Abulhair Saparov", "Mrinmaya Sachan"], "abstract": "Large language models (LLMs) can solve arithmetic word problems with high ac- curacy, but little is known about how well they generalize to problems that are more complex than the ones on which they have been trained. Empirical investigations of such questions are impeded by two major flaws of current evaluations: (i) much of the evaluation data is contaminated, in the sense that it has already been seen during training, and (ii) benchmark datasets do not capture how problem proofs may be arbitrarily complex in various ways. As a step towards addressing these issues, we present a framework for evaluating LLMs on problems that have arbitrarily com- plex arithmetic proofs, called MathGAP. MathGAP generates problems that follow fixed proof specifications\u2014along with chain-of-thought reasoning annotations\u2014 enabling systematic studies on generalization with respect to arithmetic proof com- plexity. We apply MathGAP to analyze how in-context learning interacts with gener- alization to problems that have more complex proofs. We find that among the mod- els tested, most show a significant decrease in performance as proofs get deeper and wider. This effect is more pronounced in complex, nonlinear proof structures, which are challenging even for GPT-40. Surprisingly, providing in-context examples from the same distribution as the test set is not always beneficial for performance. In par- ticular, zero-shot prompting as well as demonstrating a diverse range of examples that are less complex than the test data sometimes yield similar or higher accuracies.", "sections": [{"title": "1 INTRODUCTION", "content": "High performance on reasoning benchmarks are often taken to be evidence that transformer-based large language models (LLMs) are able to \"reason\". However, many current evaluations are unreliable since it is likely that the problems they contain are present in the model's training data (Sainz et al., 2023; Deng et al., 2024; Zhang et al., 2024). Moreover, most evaluations fail to capture that reasoning prob- lems can be arbitrarily complex, through composition of subproofs and use of multiple rules of infer- ence. High accuracy on a specific set of problems in a benchmark dataset is therefore not sufficient to conclude that LLMs can generalize to more complex, unseen problems. To obtain a more appropriate empirical measure of reasoning ability, one must evaluate on data that is not present in any benchmark dataset, containing proofs that are more complex than those used in training and context window alike.\nMath word problems (MWPs) are one of the most frequently used testbeds for LLM reasoning. Yet there exist few, if any, systematic analyses on out-of-distribution (OOD) generalization in regards to proof complexity that avoid contamination in this domain. We suspect that this is in part due to a lack of formalism for characterizing proof complexity, which we argue is necessary for making precise claims about generalization behavior for reasoning problems.\nIn this paper, we present a framework for evaluating Mathematical Generalization on Arithmetic Proofs\u2014MathGAP. First, we discuss how to characterize solutions to MWPs in terms of proof trees, in which the nodes are labeled with logical statements about the problem under a world-model framework (Opedal et al., 2023) and the edges are induced by proof steps on such logical statements. Doing so reveals several ways of characterizing a problem's complexity based on its proof tree, e.g., (non)linearity, depth, width, and the ordering of nodes (all defined in \u00a73.2). With this formalism, we"}, {"title": "2 RELATED WORK", "content": "Data contamination. Machine learning, broadly, is the study of algorithms that can learn from data, and generalize to unseen data. Machine learning models must therefore be evaluated on unseen test sets. As it turns out, modern-day LLMs are not being properly evaluated in this regard: Much of the data they are evaluated on has already been used during model training, both labeled (Dodge et al., 2021; Deng et al., 2024) and unlabeled (Elazar et al., 2023). Researchers have raised serious concerns about these issues (Jacovi et al., 2023; Sainz et al., 2023). For arithmetic reasoning specifically, Zhang et al. (2024) recently found evidence that the widely used GSM8k benchmark (Cobbe et al., 2021) is contaminated, and presented evaluations on a new dataset which is not publicly released. MathGAP mitigates data contamination by creating new, more complex synthetic test sets.\nGeneralization on reasoning tasks. Generalization to data from a different distribution (i.e., OOD data) is a core problem in machine learning, and has been studied extensively also in the context of LLM reasoning (Schwarzschild et al., 2021; Razeghi et al., 2022; An et al., 2023; Kudo et al., 2023; Liu et al., 2023; Saparov & He, 2023; Zhang et al., 2023; Thomm et al., 2024). While LLMs\u2019 performance on OOD data can be improved by various techniques (Anil et al., 2022; Hu et al., 2024), generalization to problems of arbitrary complexity under a set of known inference rules is still an open problem. For instance, Dziri et al. (2023) demonstrate that the accuracy of transformer-based LLMs on compositional generalization rapidly approaches zero as the complexity of the problem increases\u2014 regardless of in-context guidance. Saparov et al. (2023) focus on theorem proving for logical reasoning, presenting a systematic analysis on generalization ability of LLMs with respect to proof size and diversity of inference rules. Our paper contributes to this literature by providing an evaluation framework to systematically study OOD generalization in regards to arithmetic proof complexity.\nEvaluation on MWP benchmarks. It is common to treat MWPs as a testbed for studying reasoning abilities of LLMs (Cobbe et al., 2021; Patel et al., 2021; Fu et al., 2023; Shakarian et al., 2023; Stolfo et al., 2023; Zong & Krishnamachari, 2023); such problems are useful because solving them requires several distinct skills, yet, they remain conceptually simple (Stern, 1993). However, we are not aware of many studies that investigate generalization in regards to problem complexity in this domain. Our study presented in \u00a75 relates to Hase et al. (2024), who fine-tune LLMs on easy problems and evaluate them on harder ones at inference. Their complexity metrics focus on problem length, but do not capture how a longer reasoning problem gets more complex (i.e., the shape of the proof tree). Moreover, they evaluate on GSM8k, which is contaminated. Mishra et al. (2024) proposed a method that uses GPT-4 to generate new synthetic problems from symbolic specifications. Their approach is scalable, but using an LLM to generate problems may (i) introduce bias (Boguraev et al., 2024; Opedal et al., 2024) and (ii) produce problems that are unfaithful to their specifications. Mirzadeh et al. (2024) recently performed evaluations on a version of GSM8k in which the variable names and numbers have been altered, and found that LLMs are surprisingly sensitive to such substitutions. MathGAP is more general; while it allows for such substitutions, our main goal is to vary the proof structures."}, {"title": "3 A FORMAL TREATMENT OF MATH WORD PROBLEMS", "content": "We aim to generate new problems of arbitrary complexity, so we require a formalism for characterizing complexity in a precise manner. First, we explain how the semantics expressed in math word problems can be written as sequences of logical forms (\u00a73.1). We then show how to apply inference rules on these logical forms to deduce new ones (\u00a73.2). This leads to a view of problem solutions as proof trees, the structure of which can be used to characterize the reasoning required to solve the problem."}, {"title": "3.1 MATH WORD PROBLEMS AS LOGICAL FORMS", "content": "Each math word problem is represented as a sequence of logical forms under the formalism from Opedal et al. (2023; 2024). A logical form is a statement about a truth in the world that is being described by the problem, representing some arithmetic relationship between the number of entities possessed by one or several agents. Such statements can refer to the number of entities an agent has, how many more entities an agent has relative to another, the action of an agent giving their entities to another agent, etc. Formally, a logical form represents the semantics of a single sentence, and consists of a predicate that takes a set of properties as arguments. Every logical form has an agent"}, {"title": "3.2 CHAIN-OF-THOUGHT SOLUTIONS AS PROOF TREES", "content": "Solving math word problems is a form of deductive reasoning, where the correct answer follows necessarily from what is stated in the text through a combination of rules of arithmetic and world knowledge. We use such derivations, called proofs, to characterize the structure of problems. The proofs in our setup are analogous to those found in other deductive systems, e.g., of first-order logic.\nWe want to formally reason over the logical forms introduced in \u00a73.1. To that end we introduce inference rules, which are used to prove new logical forms from previously-known ones. A proof step, written as\n$\\frac{\\begin{array}{c}L_{1} \\\\ L_{2} \\\\ ... \\\\ L_{N}\\end{array}}{L}$\nis an instance of an inference rule where the conclusion logical form L is deduced from premises $L_{1}, L_{2},..., L_{N}$. An axiom is a proof step without premises. The axioms of a math word problem are the logical forms in the body of its world model; that is, all logical forms excluding the question.\nProof complexity. We seek a systematic way to analyze whether LLMs can generalize from simple to more complex proofs. The definition of proof given above gives rise to several ways of characterizing reasoning, out of which we consider four: linearity, depth, width, and ordering. These are defined below.\nWe say that a proof tree is linear if every one of its inference rules takes at most one premise that is not an axiom, and nonlinear otherwise. In other words, every step in the solution to a linear problem only uses at most one fact not directly present in the problem. The depth of a proof tree is defined as its height, i.e., the maximum path length from the root node to any leaf node. Next, the width of a proof tree is the number of leaf nodes, or axioms, that it contains. Lastly, we define the canonical ordering to be the ordering of leaf nodes as visited left-to-right. Note that the sentences in the natural language annotation from Fig. 1 follows the canonical ordering of the proof tree. Informally, this is the easy way of ordering the sentences in the problem, since it matches the ordering of the proof steps. However, there exist multiple alternative orderings in general. In particular, given a proof tree that only contains commutative inference rules all orderings of the leaf nodes are valid. For instance, swapping the ordering of the first two leaf nodes in the tree in Fig. 1 yields a problem with the same proof."}, {"title": "4 EVALUATION FRAMEWORK", "content": "Equipped with the formalism explained in \u00a73, we propose an evaluation framework which tests LLMs on MWPs of arbitrary proof complexity, called MathGAP. In \u00a74.1, we explain the generation method behind MathGAP. We then outline how this method can be used for OOD evaluation in \u00a74.2."}, {"title": "4.1 GENERATION METHOD", "content": "We propose a method for synthetic generation of problems and their CoT solution explanations. It consists of three high-level steps: (i) sample a proof tree, (ii) map the logical forms at the leaf nodes into a natural language problem, and (iii) map the proof steps into a CoT solution. Fig. 1 gives an illustration of these steps; note that the agent properties are abbreviated for brevity.\nIn detail, the procedure is as follows: We first sample a logical form at the root of the proof tree. We then sample a specific inference rule as well as its premises. We recursively repeat this procedure for each premise until a predetermined stopping criterion has been reached, resulting in a proof tree. The stopping criterion is user-specified; it could be, e.g., when the tree has reached a certain depth or width. The categorical properties for the logical forms (i.e., agent, entity, attribute, and unit) are sampled uniformly at random from handwritten vocabularies. The quantities are sampled uniformly at random from a user-specified range; in our study presented in \u00a75 we use 2-20.\nWe then traverse the leaves of the proof tree in some order and convert each into a sentence via a natural language template, forming the text corresponding to the body of the problem. The default order follows the proof tree's canonical ordering, but other orders are possible as well. The sentences are sampled uniformly at random from a predefined set of templates; some example templates are given in Table 1. The question is converted from the logical form at the root of the proof tree, and is sampled from a set of interrogative sentences."}, {"title": "4.2 MATHGAP: MATHEMATICAL GENERALIZATION ON ARITHMETIC PROOFS", "content": "With the generation method just described, we can generate new synthetic problems where we control for the structure of the proof and its complexity. MathGAP can be used for various forms of systematic studies on arithmetic reasoning; here, we focus on OOD generalization. Specifically, we can select a family of problems of interest (e.g., linear problems under comp inference rules) and generate a train-test split of problems for which the problems in the test set are more complex than those in the training set (e.g., containing problems with deeper proofs).\nThe generation method can be flexibly adapted to fit specific studies. One can add new logical forms and inference rules, restrict the choice of existing ones during sampling, or create logical forms for irrelevant sentences (Shi et al., 2023). One can also vary the text templates for the logical forms and inference rules, the vocabularies for agents, entities, attributes and units, and the range of numbers from which the quantities are sampled. In addition, one may increase linguistic diversity by incorporating a last step in which a language model paraphrases the templated texts, or, alternatively, use a procedure like the one proposed by Mishra et al. (2024). In our study we do not perform such paraphrasing, since we want to guarantee that the sentences are faithful to the proof trees from which they are generated. However, future studies using MathGAP may choose to prioritize differently."}, {"title": "5 EXPERIMENTS", "content": "We apply MathGAP to study how LLMs generalize from proof demonstrations of simple problems in context to solve problems of higher complexity at inference. We test OOD generalization through several sets of experiments: depth generalization for linear problems (\u00a75.1), width generalization for linear problems (\u00a75.2), depth generalization for nonlinear problems (\u00a75.3), and generalization to permutations (\u00a75.4). We are also interested in how the choice of the distribution of problems provided in context affects performance, which is often not obvious (Min et al., 2022; Turpin et al., 2023).\nExperimental setup. For each set of experiments, the general experimental setup is as follows: We generate five test sets of different degrees of complexity with 400 problems in each. We then generate model predictions for the problems in these test sets under four different prompts: (i) A zero-shot baseline prompt, which contains only the test problem; (ii) A prompt of primitive examples, in which all problems contain only one proof step of the same inference rule that is used in the test problem; (iii) A prompt of examples within a range of complexities, all of which are simpler than the test problem; (iv) An in-distribution baseline prompt, containing examples that are of the same complexity as the test problem."}, {"title": "5.1 LINEAR DEPTH GENERALIZATION", "content": "Problem sets. We consider linear problems using both comp and transfer. The five test sets con- sist of problems with depths between 6-10. As such, the examples provided in the range of setting (iii) described above have depths between 1-5. We give an example of a problem with depth 6 in App. B.1."}, {"title": "5.2 LINEAR WIDTH GENERALIZATION", "content": "Problem sets. To test generalization to greater proof width, we use partwhole problems with a fixed depth of 1. The five test sets have widths between 6-10 and the width of the problems in setting (iii) described above have widths between 1-5.\nResults. The results are shown in the bottom row of Fig. 2. For GPT-3.5 and Mixtral-8x7B, we observe that providing in-context examples improves accuracy over zero-shot in most cases. Like for linear depth generalization, it seems that having a range of OOD examples of varying complexity is superior to only including primitive examples. Llama3-8B exhibits worse performance with in-distribution context here as well. In general for the Llama models, adding OOD in-context examples usually leads to lower performance over zero-shot. Apart from GPT-3.5-Turbo, the performance is generally lower as compared to a deep problem with the same number of sentences (\u00a75.1), often with a sharper decreasing trend. Such is the case even for Llama3 70B, which performed almost perfectly in the linear depth setting. This would suggest that it is more difficult to generalize in terms of width than in terms of depth. Again, GPT-40 demonstrates near perfect performance across all test sets."}, {"title": "5.3 NONLINEAR DEPTH GENERALIZATION", "content": "Problem sets. We generate test datasets containing nonlinear problems with comp and comp-eq, having the same form as the problem in Fig. 1. Another example is shown in App. B.2. We generate a test set for each depth between 3-6. Thus, the ranged OOD context setting contains examples of depths between 1-2.\nResults. The results are shown in Fig. 3. All models exhibit a performance trend that tends to zero as depth increases, albeit at different rates. For Mixtral-8x7B, Llama3-8B, and GPT-3.5 Turbo, the perfor- mance decreases rapidly across all context modes. These models benefit from in-distribution contexts for low depths, and in some cases, from a range of OOD examples. Llama3-70B and GPT-40 are more robust to this type of OOD distribution shift, but for the deepest problems, their performance tends to"}, {"title": "5.4 ORDER GENERALIZATION", "content": "Problem sets. LLMs are known to be sensitive to changes in the order of axioms in logical and mathematical reasoning problems (Chen et al., 2024; Eisape et al., 2024). Here, we present a systematic analysis of their sensitivity to a particular form of order permutation. We consider linear, left-leaning proof trees using comp that have depth 5, and generate problems that deviate from the canonical, left-to-right order of the leaf nodes. In particular, we pick one of the leaf nodes to visit first, and then visit the remaining leaf nodes in left-to-right order. This has the effect of moving one of the sentences in the canonically-ordered problem text to the beginning of the problem. We characterize complexity as the distance of the movement, with the hypothesis that a greater movement from a canonical ordering constitutes a more difficult problem. We create five test sets with movement distances between 1-5. We do not include a primitive OOD in-context distribution, the notion of a primitive problem is ill-defined in this case. The range prompt includes movements of all distances between 1-5 except the one present in the test set.\nResults. The results are shown in Fig. 4. First, note that all models show lower performance on perturbed problems as compared to non-perturbed ones (compare to Fig. 2). Second, there is a nonlinear relationship between accuracy and distance of the movement, in contrast to the monotonically decreasing relation we expected. More specifically, the performance is the highest for the problems where the sentence is moved from near the beginning or the end of the problem. In addition, including in-context examples gives a larger boost in performance for short and long movements, as compared to medium-length ones."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced MathGAP, a framework for evaluating LLMs on math word problems of arbitrary arithmetic proof complexity. MathGAP can flexibly generate synthetic arithmetic word problems with controllable proof tree characteristics. Importantly, this enables studies of OOD generalization that avoid issues of data contamination, since a train-test split where the distributions are different can be created programmatically. In particular, the test set can be made arbitrarily more complex than the training set. We apply MathGAP to study whether LLMs can learn from simple examples in context to solve more complex problems in a test set. All LLMs tested show a decrease in performance as proof complexity increases through depth and/or width. We also find that LLMs are sensitive to the order in which sentences are presented in a particular manner: A problem is harder if a sentence is moved from the middle of the problem, as compared to the beginning or end. We find no clear relationship between the distribution of in-context examples and performance; zero-shot or OOD contexts sometimes yield performance higher than or comparable to in-distribution contexts. Finally, we demonstrate that we can use MathGAP to construct problems that are challenging even for state-of-the-art reasoning models like OpenAI 01-see App. C."}, {"title": "LIMITATIONS", "content": "Although we make sure to sample from a wide range of agent names, entities, attributes and units, and include multiple templates for the predicates, future work may investigate how our findings generalize to distributions of problems with higher linguistic diversity. It is also beyond the scope of the present study to consider problem texts in languages other than English; however, it is straightforward to extend MathGAP to other languages. In addition, we cannot guarantee that the distributions of our generated problems are different from those of any potential internal data-generating mechanisms from Meta or OpenAI. However, we deem it to be unlikely that they use a similar formalism as we do here, and it gets increasingly unlikely that problems are similar as complexity increases.\nWhile the more complex nonlinear problems are indeed challenging even for the most capable models, we note that the main purpose of our study was not to create a challenging evaluation set for state-of-the-art LLMs. If that is the goal, one could create harder problems by combining several inference rules (e.g., including the rate concept; Opedal et al., 2023), designing a wider variety of proof tree topologies, and presenting the axioms in arbitrary orderings. It is also possible to create more syntactically complex templates and use numbers on which it has been shown to be more challenging for models to perform arithmetic (Razeghi et al., 2022).\nFinally, it is hard to make a direct comparison between the difficulty of linear and nonlinear problems, since they use different inference rules. In particular, nonlinear proofs contain comp-eq predicates, which are not present in linear proofs."}]}