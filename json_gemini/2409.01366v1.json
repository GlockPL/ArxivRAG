{"title": "CHESS : Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification", "authors": ["Junhui He", "Shangyu Wu", "Weidong Wen", "Chun Jason Xue", "Qingan Li"], "abstract": "Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via Channel-wise thresholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have prevailed in a wide range of applications across various fields, such as code generation tools, office assistants, input method editors, voice assistants, and assistive applications designed for individuals with disabilities. However, due to the substantial computation and memory requirements of LLM inferences, deploying LLMs on edge devices is still challenging. To mitigate these overheads, utilizing the inherent activation sparsity of LLM has emerged as a promising strategy (Liu et al., 2023; Song et al., 2023; Alizadeh et al., 2023). This approach has proven effective for models with the ReLU activation function (Li et al., 2023; Liu et al., 2023).\nContemporary LLMs demonstrate that SwiGLU or GeGLU activation functions can further boost the model performance, but they induce less activation sparsity. Consequently, several methods (Mirzadeh et al., 2024; Song et al., 2024) are proposed to explore more sparsity by regularizing the SwiGLU or GeGLU activation. However, those works require fine-tuning the LLMs, which entails significant training overhead. To avoid training overheads and improve activation sparsification in modern LLMs, Lee et al. (2024) propose a thresholding-based pruning method to actively sparsify the activation tensors during the inference stage. However, this thresholding technique focuses solely on the statistics of the activation tensors themselves, failing to model the impact of sparsification on overall model performance. This lack of modeling results in suboptimal performance degradation.\nTo address the above limitations, this paper proposes CHESS , a new activation sparsification optimization via CHannel-wise thresholding and Selective Sparsification. First, this paper introduces a new objective that models the impact of sparsification on model performance and optimizes the sparsification decisions. Then, this paper proposes channel-wise thresholding for FFN layers in LLMs, which determines the unique threshold for each activation channel. Furthermore, this paper proposes selective sparsification, which applies thresholding-based activation sparsification to the target submodules in the attention module. Finally, this paper presents the implementations of sparse kernels to accelerate the inference based on the sparse activations."}, {"title": "2 Background and Motivations", "content": ""}, {"title": "2.1 Activation Sparsification", "content": "Activation functions introduce non-linearity into neural networks, allowing networks to capture complex patterns in the data. ReLU (Glorot et al., 2011), as a popular activation function, has been widely applied in most neural networks for addressing gradient vanish issues (Zhang et al., 2022). Another benefit of ReLU is introducing the sparsity into the activation tensors. Recent studies (Li et al., 2023; Liu et al., 2023) have demonstrated this effect, showing that up to 95% of the intermediate FFN activations in OPT models are zero. Such sparsity can be used to accelerate the model inference while maintaining comparable model performance (Liu et al., 2023; Alizadeh et al., 2023; Song et al., 2023).\nRecent state-of-the-art LLMs replace the ReLU activation function with more advanced activation functions, such as GeLU (Hendrycks and Gimpel, 2016), SiLU (Ramachandran et al., 2018), or GLU-series functions (Shazeer, 2020). Although these activation functions can significantly boost the LLMs' performance (Touvron et al., 2023), they induce less activation sparsity. Previous optimizations based on activation sparsity may not be suitable for the LLMs with those activation functions."}, {"title": "2.2 Motivation", "content": "Following the observations in CATS (Lee et al., 2024), this paper also aims to apply activation sparsification in the Gated-MLP blocks of FFN modules. The formal expression of the gated-MLP block is defined as,\nFFN(x) = ((xWgate) (xWup)) Wdown (1)\nwhere Wup, Wgate, Wdown are parameters, and (\u06f0) is the activation function. Therefore, the activation values in Gated-MLP blocks are,\naup = xWup, agate = (xWgate) (2)\nSince the activation function introduces sparsity where the values of many elements in the output tensor are close to zero, we focus on pruning the output of the gate projection layer, i.e., agate. Then, the following computations, such as the matrix multiplication for aup, the element-wise multiplication between aup and agate, or the matrix multiplication with Wdown, can further be reduced due to the zero elements in the pruned agate.\nInspired by layer-wise weight pruning (Frantar and Alistarh, 2023; Sun et al., 2024), this paper reformulates the activation sparsification problem to find the optimal pruned activation tensor \u00e2gate that guarantees a specified sparsity level while minimizing the output difference of the succeeding layer before and after pruning. More formally, the problem is defined as,\narg min ||aup agate - aupagate ||2 (3)\nagate\nwhere aup, agate are different activation tensors in FFN layers, \u00e2gate is the pruned activation tensor."}, {"title": "3 CHESS : Activation Sparsification via Channel-Wise Thresholding and Selective Sparsification", "content": "In this section, this paper first introduces channel-wise thresholding for FFN layers. Then, this paper presents the selective sparsification for attention layers. Finally, this paper shows the efficient implementation of the proposed custom sparse kernels."}, {"title": "3.1 Channel-Wise Thresholding", "content": "As described in Equation 4, whether to prune an activation element is determined by both aup and agate. To further quantify the significance of each activation element in this decision-making process, we introduce the importance score based on Equation 4,\nscore = aupgate (5)\nTo obtain all importance scores of elements in Equation 5, we need to compute two matrix-multiplication operations for agate and aup. However, the computation for aup can be reduced by leveraging the sparsity of agate. Therefore, we need to calculate the score only with agate. As is shown in Figure 1, we observe that, for each channel i, the values of a remain relatively consistent across different inputs. However, these values can vary significantly between different channels. Based on this observation, this paper estimates the |ap| using the expectation of |ap| over sampled training data,\nap \u2248E [|a|] = 1 \u03a3a (6)\nwhere n is the number of sampled data. Therefore, the importance score is further estimated as,\nscore = E [|a|] |a| (7)\nFor the sorting overhead, this paper also adopts the CDF-based thresholding Method following Lee et al. (2024). Specifically, we first outline the cumulative distribution function F of the proposed importance score across all channels,\nF(t) = P(score \u2264 t) (8)\nThen, given a sparsity level k, we can obtain the threshold ti for sparsifying the activation elements on channel i,\nti = arg mint F(t) >= k (9)\nE [a]\nThis threshold indicates the maximal activation value that should be pruned as zero. Different from CATS, this is a Channel-Wise Thresholding (CWT) technique that relates the model performance degradation with the activation sparsity via introducing the importance score in Equation 5.\nFinally, based on the channel-wise thresholds, the activation values can be sparsified as,\nCWT(ai) = { 0, if ai < ti ai, if ai > ti (10)\nand the final output of the FFN layer is computed as,\nFFNCWT(x) = (CWT(agate) auP) Wout (11)"}, {"title": "3.2 Selective Sparsification", "content": "Although the activation sparsity in attention modules is much lower than that in FFN modules, applying activation sparsification to these modules can still effectively reduce memory access and computational overhead. The standard attention mechanism involves four linear projects: query, key, value, and output projection. Similar to that in FFN modules, the objective of activation sparsification in the attention module is to find the optimal pruned activation tensor that guarantees a specified sparsity level while minimizing the output difference of the succeeding layer before and after pruning. More formally, the problem is defined as,\narg min ||xW - W||2 (12)\nwhere W is the weight tensor of the projection layer.\nThe error E= ||xW \u2013 W||2 can be approximated using the Taylor series as follows (LeCun et al., 1989; Hassibi and Stork, 1992; Frantar and Alistarh, 2022):\nE = g(x\u2212x)T+ (-x)H(-x)+O(||x-x||3) (13)\nwhere g and H denote the first-order and second-order derivatives of the error E with respect to x, respectively.\n= \u2202E = 0 (14)\n= (\u22022E) = WWT (15)\nThen, we replace g and H with true values, discard the higher-order terms, and apply diagonal approximation to H. The Equation 13 can be simplified as:\n\u03a3||Wi:||2(xi - xi)2 (16)\nwhere ||Wi,:|| denotes the square of 12 norm of row i in weight matrix W. As described in Section 2.2, we can also decompose the input features into pruned features (zeros) and non-pruned features (original values) and then transform the objective as follows,\narg min ||Wi: ||x (17)\nTo further simplify Equation 17, this paper analyzes the statistics of the weight matrix in the attention mechanism. Figure 2 shows the distribution of ||Wi,:||2 of different rows in projection weights. From the results, all rows from the same weight exhibit similar ||Wi,:||2, therefore we can eliminate this coefficient from Equation 17 and derive the simplified final objective:\narg min x (18)"}, {"title": "3.3 Efficient Sparse Kernels", "content": "To achieve wall-clock speedup and reduce inference latency based on sparse activations, this paper developed two custom CPU kernels: spvmm (sparse vector-matrix multiplication) and vmmsp (vector-matrix multiplication with output sparsity). The spvmm kernel is optimized for cases where the input activation tensor is sparse, and it is employed in attention modules and FFN down projections. Conversely, the vmmsp kernel is designed for cases where the output activation tensor is multiplied with a sparse mask, and it is used in FFN up projections.\nAlgorithm 3.1 and Algorithm 3.2 show the detailed steps of spvmm and vmmsp, respectively. Algorithm 3.1 splits the input vector into blocks of size B and accumulates the vector-matrix multiplication results of each block when x[k] is not 0 (Lines 5-7). Algorithm 3.2 also performs block-level vector-matrix multiplications but computes the outputs at the specific position based on the sparsity mask (Lines 5-9). Both algorithms reduce the latency by bypassing unnecessary weight reads and computations.\nThe implementation of the vmmsp kernel is relatively straightforward; it computes Y = XWT, consistent with the definition of linear projection in PyTorch (Paszke et al., 2019). However, the spvmm kenrel requires a more complex approach to ensure efficient computation on multi-core CPUs while avoiding atomic operations. To this end, we employ two advanced optimizations. First, we employ loop tiling and loop reordering strategies to make sure that each threads compute independently without the need for synchronization or atomic operations. Furthermore, we transpose the linear projection weights in advance during the model preprocessing stage, to memory access continuity and enhance cache hit rates."}, {"title": "4 Experiments", "content": "In this section, this paper first introduces the dataset, comparisons, and implementation details. Then, this paper presents the main results over 8 downstream tasks in terms of the model performance and model efficiency. Besides, this paper also conducts an ablation study across different sparsification module and analysis on efficiency over different sparsity level."}, {"title": "4.1 Datasets and Experimental Setup", "content": "Datasets We utilize ARC Challenge (Arc-C), ARC Easy (Arc-E), BoolQ, HellaSwag (HS), OpenbookQA (QA), PIQA, SCI-Q, Winogrande (WG) as benchmarks for downstream tasks, employing the Evaluation Harness library from Eleuther AI to ensure consistency with and Lee et al. (2024). These tasks are designed to assess various aspects of the language model's performance, including comprehension, common sense, and reasoning abilities, which effectively illustrate the model's capability loss with activation sparsification.\nComparisons To validate the effectiveness of the proposed CHESS, we conducted experiments using several state-of-the-art LLMs, including Llama-2-7B, Llama-2-13B, Llama-2-70B, Llama-3-8B and Mistral-7B. These models feature different attention mechanisms, specifically MHA and GQA, and utilize SwiGLU as the activation function for the FFN modules. For our main evaluation, we tested four different configurations across all five LLMs:\n\u2022 Base Model: the LLM model without any activation sparsification.\n\u2022 CATS (Lee et al., 2024): the state-of-the-art activation sparsification method, which applies magnitude pruning to FFN activations.\n\u2022 CHESS w/o: the proposed method including channel-wise thresholding but without attention sparsification.\n\u2022 CHESS w/: the proposed method including channel-wise thresholding and selective sparsification.\nFor the ablation study, we evaluate the following three models:\n\u2022 Llama-3: the Llama-3 8B model without activation sparsification.\n\u2022 FS: No activation sparsification applied to the FFNs; full sparsification applied in the attention modules.\n\u2022 SS: No activation sparsification applied to the FFNs; selective sparsification applied in the attention modules.\nImplementation Details For all models involving activation sparsification, thresholds are sampled from a subset of the C4 dataset (Raffel et al., 2020). Following the settings in CATS (Lee et al., 2024), the sparsity level k is set to 0.5, where the accuracy drop is minimal while the inference latency significantly decreases. The proposed method was implemented using the PyTorch v2.2.2 and HuggingFace Transformers v4.39.3. End-to-end decoding speedups are measured on a randomly collected subset of C4 dataset. Kernel efficiency and end-to-end speedup experiments are conducted with FP32 precision on a personal computer equipped with an Intel Core I9-12900K CPU and 64GB of DDR4 memory. Since our work can be applied to quantized models as well, changing weight precision to FP16 or even lower bit-width quantizations does not materially affect our results (Lee et al., 2024)."}, {"title": "4.2 Main Results on Downstream Tasks", "content": "Table 1 compares the accuracy of different models across 8 downstream tasks and Figure 3 evaluates the end-to-end inference speedups. Experimental results draw the following conclusions.\nChannel-wise thresholding can reduce accuracy degradation while achieving comparable sparsity. The proposed CHESS w/o exhibits a smaller average performance drop of 1.07 across 5 base models and 8 downstream tasks, compared to the 1.70 degradation observed with CATS. Specifically, CHESS w/o consistently outperforms CATS on ARC Easy, BoolQ, and HellaSwag, while showing modest gains on the remaining benchmarks. Moreover, CHESS w/o achieves a comparable sparsity to CATS.\nSelective sparsification of attention modules further improves sparsity while maintaining model accuracy. Compared to CHESS w/o, the average performance of CHESS w/ degrades by 0.04 on Llama-2-7B and 0.61 on Llama-3-8B, respectively. Interestingly, for larger models such as Llama-2-13B, Llama-2-70B, and Mistral-7B, CHESS w/ demonstrates comparable or even slightly superior overall performances. More specifically, CHESS w/ outperforms on OpenbookQA, but underperforms on ARC Easy, HellaSwag and BoolQ, while showing similar results on ARC Challenge, PIQA, SciQ, and Winogrande. These results demonstrate that the additional selective sparsification on attention modules has minimal impact on performance. In comparison to CATS, CHESS w/ consistently delivers superior average performance with fewer activated parameters."}, {"title": "4.3 End-to-End Decoding Speedup", "content": "CHESS achieves end-to-end speedups of up to 1.27x compared to Transformers baselines. The proposed CHESS w/ achieves the highest speedup of 1.25x on Llama-2-7B and Llama-2-13B, and 1.27x on Llama-3-8B and Mistral-7B, respectively. When not employing attention sparsification, CHESS w/o achieves comparable speedups to CATS, which is 1.17x on Llama-2-7B and Llama-2-13B, 1.20x on Llama-3-8B, and 1.21x on Mistral-7B, respectively. This is because of the comparable parameters activated per decoding pass of these two methods. Due to the limited capacity of main memory of edge devices, we did not perform the end-to-end speedup test for the Llama-2-70B model. However, based on the activated parameter count per inference pass, its speedup is estimated to be similar to that of Mistral-7B."}, {"title": "4.4 Ablation Study", "content": "Table 2 presents the ablation study with different sparsification in attention modules. While selective sparsification achieves a comparable reduction in overhead relative to full sparsification, it significantly outperforms full sparsification across all eight benchmarks. Specifically, selective sparsification exhibits substantial improvements on the HellaSwag and Arc Challenge benchmarks, while demonstrating modest gains on the remaining benchmarks. These results underscore the advantages of selective sparsification."}, {"title": "4.5 Kernel Efficiency", "content": "As illustrated in Figure 4, this paper conducts a comparative analysis of the latency against sparsity level between the proposed custom sparse kernel and the dense kernel in PyTorch (Paszke et al., 2019). At a sparsity level of 0, the vmmsp kernel used for up projections demonstrates slightly lower latency compared to the PyTorch dense kernel. Conversely, the spvmm kernel, utilized by attention projections and down projections, exhibits slightly higher latencies than the dense kernel. This increased latency is primarily due to the advanced loop tiling and reordering strategies, which cause slight performance degradation at low sparsity levels.\nAs the sparsity level increases, the latency of the dense kernel remains relatively constant, whereas the latency of our custom sparse kernels decreases proportionally. Notably, at a sparsity level of 0.5, our custom sparse kernels achieve latency reductions of 30%, 28%, and 51% for attention projection, FFN up projection, and FFN down projection, respectively. These findings highlight the efficiency of our custom kernels."}, {"title": "4.6 Impact on Different Sparsity Levels", "content": "Figure 5 shows the model performance on downstream tasks and end-to-end decoding speedups at different sparsity levels. We selected Llama-3-8B as the base model since it incorporates the contemporary GQA module.\nExperimental results indicate that at lower sparsity levels (0.3 and 0.5), both CATS and CHESS maintain performance comparable to the base model, with CHESS exhibiting superior performance. At higher sparsity levels (0.7 and 0.9), these models experience noticeable performance degradation, and CHESS models, particularly CHESS w/o models, consistently outperform CATS. Specifically, at a sparsity level of 0.7, the CATS, CHESS w/o, and CHESS w/ models achieve average performances of 56.49, 61.18, and 60.21, respectively. At a sparsity level of 0.9, the corresponding performances are 34.83, 43.15, and 38.86, respectively.\nRegarding end-to-end speedup, CHESS w/ models exhibit the highest speedup at all sparsity levels above 0.3, attributed to the selective sparsification of attention modules. Specifically, CHESS w/ achieves speedups of 1.46x and 1.72x at sparsity levels of 0.7 and 0.9, respectively, compared to 1.33x and 1.52x for CATS. However, at a sparsity level of 0.3, the CHESS w/ model exhibits slightly reduced speedup, mainly due to the suboptimal efficiency of the spvmm kernel kernel at lower sparsity levels."}, {"title": "4.7 Extended Comparisons with State-of-the-Art Training-Free Pruning Methods", "content": "To further demonstrate the effectiveness of our proposed CHESS method, we extend our comparisons to include other state-of-the-art training-free pruning approaches, such as Relufication (Mirzadeh et al., 2024) and Wanda (Sun et al., 2024). Notably, although Relufication achieves competitive performance when fine-tuned, it struggles with performance degradation in training-free scenarios. Wanda, on the other hand, focuses on weight pruning, which belongs to a different branch of work. Weight pruning typically results in unstructured sparsity or semi-structured sparsity, which is only supported by high-end NVIDIA GPUs with Ampere or Hopper architectures. In contrast, our proposed CHESS does NOT rely on specialized GPU architecture, making it more suitable for deploying on edge devices.\nAs presented in Table 3, the proposed CHESS method achieves superior performance in most benchmarks while activating comparable or fewer parameters compared to both Relufication and Wanda. Specifically, CHESS with a sparsity level of 0.7 outperforms other methods on several benchmarks including Arc Challenge, Arc Easy, HellaSwag, OpenbookQA, PIQA, SciQ, and Winogrande. Despite using only 54.92% of the model's parameters per decoding pass, CHESS delivers an average performance (60.21) that surpasses Wanda (56.41) and Relufication (28.94). These results emphasize the advantage of CHESS over existing methods."}, {"title": "5 Related Work", "content": "Various methods have been proposed to address the challenges associated with deploying LLMs locally. Weight quantization (Xiao et al., 2023; Frantar et al., 2022; Lin et al., 2024) aims to represent LLM weights using lower bit-widths, thereby reducing memory usage and access overhead. Activation quantization focuses on minimizing the memory footprint of activation tensors and KV cache (Zhao et al., 2024; Liu et al., 2024; Hooper et al., 2024). These methods can be applied along with our proposed CHESS method.\nWeight pruning (Frantar and Alistarh, 2023; Sun et al., 2024) involves setting a portion of the LLM weights to zero to reduce computational overhead and memory requirement. However, this approach faces several challenges including noticeable degradation in performance and limited hardware support when applied on personal devices.\nNon-autoregressive decoding approaches, such as speculative decoding (Leviathan et al., 2023; Zhou et al., 2023) or Medusa (Cai et al., 2024), seek to convert autoregressive decoding process of LLMs into parallel decoding to mitigate memory access overhead. However, these methods simultaneously impose increased computational demands, which presents significant challenges for deployment on personal devices with limited processing capabilities."}, {"title": "6 Conclusion", "content": "This paper reformulates the activation sparsification problem and introduces the CHESS, a general activation sparsification via channel-wise thresholding and selective sparsification. Experiments show that the proposed CHESS can achieve a lower performance degradation and accelerate the LLM inference with sparse activations."}, {"title": "7 Limitations", "content": "The limitations of this work can be summarized in two main aspects. First, while CHESS achieves lower accuracy degradation compared to existing methods with fewer activated parameters, it still experiences a noticeable accuracy loss, particularly at higher sparsity levels. Future research could explore fine-tuning techniques to help mitigate this decline in performance.\nSecond, our method performs optimally with a batch size of 1, which is suitable for edge deployment scenarios where typically only a single user is involved. However, under larger batch sizes, the structured sparsity of activation tensors deteriorates into unstructured sparsity, thereby limiting the potential for end-to-end speedups. This limitation constrains its effectiveness in data center deployments, where larger batch sizes are common."}]}