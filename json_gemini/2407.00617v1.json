{"title": "Iterative Nash Policy Optimization:\nAligning LLMs with General Preferences via No-Regret Learning", "authors": ["Yuheng Zhang", "Dian Yu", "Baolin Peng", "Linfeng Song", "Ye Tian", "Mingyue Huo", "Nan Jiang", "Haitao Mi", "Dong Yu"], "abstract": "Reinforcement Learning with Human Feedback (RLHF) has achieved great success in aligning\nlarge language models (LLMs) with human preferences. Prevalent RLHF approaches are reward-based,\nfollowing the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of\nhuman preferences. In this paper, we explore RLHF under a general preference framework and approach\nit from a game-theoretic perspective. Specifically, we formulate the problem as a two-player game and\npropose a novel algorithm, iterative Nash policy optimization (INPO). The key idea is to let the policy play\nagainst itself via no-regret learning, thereby approximating the Nash policy. Unlike previous methods,\nINPO bypasses the need for estimating the expected win rate for individual responses, which typically\nincurs high computational or annotation costs. Instead, we introduce a new loss objective that is directly\nminimized over a preference dataset. We provide theoretical analysis for our approach and demonstrate\nits effectiveness through experiments on various representative benchmarks. With an LLaMA-3-8B-based\nSFT model, INPO achieves a 41.5% length-controlled win rate on AlpacaEval 2.0 and a 38.3% win rate\non Arena-Hard, showing substantial improvement over the state-of-the-art iterative algorithm [Dong\net al., 2024] under the BT model assumption. Additionally, our ablation study highlights the benefits of\nincorporating KL regularization for response length control.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) such as ChatGPT [Achiam et al., 2023], Claude [Anthropic, 2023], and\nBard [Google, 2023] have achieved tremendous success in various instruction-following tasks. A key factor\nin this success is the technique of reinforcement learning with human feedback (RLHF) [Christiano et al.,\n2017], which aligns LLMs with human preferences and values. The first standard RLHF framework for\nLLM alignment was proposed by Ouyang et al. [2022]. They first train a reward model (RM) on a dataset\ncontaining human preferences. Subsequently, a pretrained LLM is fine-tuned to maximize the reward from\nthis RM using the proximal policy optimization (PPO) algorithm [Schulman et al., 2017]. Models trained\nwith this pipeline can generate human-preferred outputs even with 100x fewer parameters. Nevertheless,\nfitting a high-quality RM requires a large amount of human-labeled data, and training with PPO is generally\nless stable [Peng et al., 2023]. To bypass the training of the RM, Rafailov et al. [2024] propose the direct"}, {"title": "2 Preliminaries", "content": "Notations. We use \\(x \\in \\mathcal{X}\\) to denote a prompt where \\(\\mathcal{X}\\) is the prompt space. We assume that \\(x\\) is sampled\nfrom a fixed but unknown distribution \\(d_0\\). An LLM is characterized by a policy \\(\\pi : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{Y})\\) which takes\na prompt as the input and output a distribution over the response space \\(\\mathcal{Y}\\). A response \\(y \\in \\mathcal{Y}\\) is then sampled\nfrom the distribution \\(\\pi(\\cdot|x)\\). We use \\(\\sigma\\) to denote the sigmoid function and \\(\\text{Ber}(\\cdot)\\) to denote the Bernoulli\ndistribution. We use \\(\\mathcal{O}(\\cdot)\\) to hide absolute constants and use \\(\\tilde{\\mathcal{O}}(\\cdot)\\) to hide logarithmic factors.\nGeneral Preference Oracle. We first introduce the definition of the general preference oracle as follows."}, {"title": "2.1 RLHF with BT Model Assumption", "content": "Bradley-Terry (BT) Model Assumption. Instead of directly considering the general preference, the\nprevalent RLHF framework makes the Bradley-Terry (BT) model assumption. It assumes that there exists a\nreward function \\(R^*\\) such that for any \\(x \\in \\mathcal{X}\\) and \\(y^1, y^2 \\in \\mathcal{Y}\\):\n\n\\[\nP(y^1 > y^2 | x) = \\frac{\\exp(R^*(x, y^1))}{\\exp(R^*(x, y^1)) + \\exp(R^*(x, y^2))} = \\sigma (R^* (x, y^1) - R^*(x, y^2)) .\n\\]\n\nAfter learning a reward function \\(R\\), previous RLHF algorithms aim to maximize the following KL-regularized\nobjective:\n\\[\n\\mathcal{J}(\\pi) = \\mathbb{E}_{x\\sim d_0} \\left[\\mathbb{E}_{y\\sim \\pi(\\cdot|x)} [R(x, y)] - \\tau \\text{KL}(\\pi(\\cdot|x)||\\pi_{\\text{ref}}(\\cdot|x))\\right] .\n\\]\n\nHere \\(\\pi_{\\text{ref}}\\) is the reference policy, which is usually a supervised fine-tuned LLM, and \\(\\tau > 0\\) is the regularization\nparameter. By maximizing the objective, the obtained policy simultaneously achieves a high reward and stays\nclose to \\(\\pi_{\\text{ref}}\\, which can mitigate reward hacking [Tien et al., 2022, Skalse et al., 2022] to some extent.\nDirect Preference Optimization (DPO). Rafailov et al. [2024] propose the direct preference optimization\n(DPO) algorithm, which directly optimizes a policy and bypasses the need to learn a reward function. The\nkey idea is that there is a closed-form solution to Eq. (2):\n\\[\n\\pi^*(y|x) \\propto \\pi_{\\text{ref}}(y|x) \\exp \\left(\\frac{1}{\\tau} R(x,y)\\right),\n\\]\nwhich shows that each policy \\(\\pi\\) implicitly parameterizes a reward function. We can directly formulate a\nmaximum likelihood objective to learn the optimal policy:\n\\[\n-\\mathbb{E}_{x, y_w, y_l \\sim \\mathcal{D}} \\left[\\log \\sigma \\left(\\frac{1}{\\tau} \\left(\\log \\frac{\\pi(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\log \\frac{\\pi(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right)\\right],\n\\]\nwhere \\(\\mathcal{D}\\) is a preference dataset, \\((y_w, y_l)\\) is a preference pair under prompt \\(x\\) and \\(y_w\\) is the preferred response."}, {"title": "2.2 RLHF with General Preference", "content": "The previously mentioned algorithms all rely on the BT model assumption, which may not hold in practice.\nRecently, a line of work [Munos et al., 2023, Ye et al., 2024, Calandriello et al., 2024] formulate the\npolicy optimization problem as a two-player game and aim to learn the Nash equilibrium (NE) of the game.\nSpecifically, given two policies \\(\\pi_1\\) and \\(\\pi_2\\), the game objective is written as:\n\n\\[\n\\mathcal{J}(\\pi_1, \\pi_2) = \\mathbb{E}_{x\\sim d_0} \\left[\\mathbb{E}_{y_1\\sim \\pi_1, y_2\\sim \\pi_2} [P(y_1 > y_2 | x)] - \\tau \\text{KL}(\\pi_1(\\cdot|x)||\\pi_{\\text{ref}}(\\cdot|x)) + \\tau \\text{KL}(\\pi_2(\\cdot|x)||\\pi_{\\text{ref}}(\\cdot|x))\\right],\n\\]\n\nwhere \\(\\pi_1\\), the max-player, aims to maximize the objective, and \\(\\pi_2\\), the min-player, aims to minimize the\nobjective. Without loss of generality, we restrict our attention to the policy class \\(\\Pi\\) containing the policies\nwith the same support set as \\(\\pi_{\\text{ref}}\\). The NE of the game is then defined as:\n\n\\[\n\\pi^*, \\pi^* := \\underset{\\pi_1\\in\\Pi}{\\text{argmax}} \\underset{\\pi_2\\in\\Pi}{\\text{argmin}} \\mathcal{J}(\\pi_1, \\pi_2).\n\\]\n\nSince the game is symmetric for the two players, as proven by Ye et al. [2024], the Nash policies of the two\nplayers are unique and coincide, meaning that \\(\\pi_1^* = \\pi_2^* = \\pi^*\\).\nProperty of Nash Policy. We remark that for any policy \\(\\pi \\in \\Pi\\), we always have \\(\\mathcal{J}(\\pi^*, \\pi) \\geq 0.5\\), because\n\\(\\mathcal{J}(\\pi^*, \\pi^*) = 0.5\\) and \\(\\pi^*\\) is the best response against itself. This indicates that the win rate of \\(\\pi^*\\) over any\npolicy \\(\\pi\\) is at least one half if the KL divergence terms are negligible. Motivated by this property, our goal is\nto learn the Nash policy \\(\\pi^*\\).\nOnline IPO. To learn the Nash policy, Calandriello et al. [2024] propose the following online IPO loss\u00b9:\n\n\\[\n\\mathbb{E}_{x\\sim d_0, y, y' \\sim SG[\\pi], y_w, y_l \\sim \\lambda_p(x, y, y')} \\left[\\log \\left(\\frac{\\pi(y_w|x)\\pi_{\\text{ref}}(y_l|x)}{\\pi(y_l|x)\\pi_{\\text{ref}}(y_w|x)}\\right)^2 + \\frac{1}{2\\tau}\\right]^2,\n\\]\n\nwhere \\(SG[\\pi]\\) stands for a stop-gradient around \\(\\pi\\) in the data distribution. They show that Nash policy is\nthe minimizer of this loss function. Additionally, they propose the IPO-MD loss, which samples from a\ngeometric mixture between the current policy and the reference policy:\n\n\\[\n\\mathbb{E}_{x\\sim d_0, y, y' \\sim SG[\\pi_{1-\\beta}\\pi_{\\text{ref}}], y_w, y_l \\sim \\lambda_p(x, y, y')} \\left[\\log \\left(\\frac{\\pi(y_w|x)\\pi_{\\text{ref}}(y_l|x)}{\\pi(y_l|x)\\pi_{\\text{ref}}(y_w|x)}\\right)^2 + \\frac{1}{2\\tau}\\right]^2,\n\\]\n\nwhere \\(\\beta \\in [0, 1]\\) is an interpolation parameter. However, it is still unclear how to effectively minimize these\nlosses in practice. In this paper, we study the problem from the no-regret learning perspective and propose an\niterative algorithm based on the classical online mirror descent algorithm."}, {"title": "3 Algorithm", "content": "In this section, we present our algorithm that learns the Nash policy via no-regret learning. For simplicity, we\nconsider the non-contextual case and ignore the prompt \\(x\\). The extension to contextual case is straightforward."}, {"title": "3.1 Online Mirror Descent for Solving Nash Policy", "content": "Given the preference oracle \\(\\mathcal{P}\\), we first consider the planning problem and introduce how to use the online\nmirror descent (OMD) algorithm to solve for the Nash policy. We initialize our policy \\(\\pi_1\\) as \\(\\pi_{\\text{ref}}\\). At iteration\n\\(t\\), our current policy is \\(\\pi_t\\) and we define the loss function for any \\(\\pi \\in \\Pi\\) as:\n\n\\[\nl_t(\\pi) := -\\mathbb{E}_{y\\sim \\pi, y' \\sim \\pi_t} [P(y > y')] + \\tau\\text{KL}(\\pi||\\pi_{\\text{ref}}).\n\\]\n\nThe loss consists of two parts: the negative win rate of \\(\\pi\\) over current policy \\(\\pi_t\\) and the KL penalty term, which\nkeeps \\(\\pi\\) close to the reference policy \\(\\pi_{\\text{ref}}\\). In OMD with entropy regularization, also known as Hedge [Freund\nand Schapire, 1997], we seek the policy that minimizes the following objective:\n\n\\[\n\\pi_{t+1} = \\underset{\\pi \\in \\Pi}{\\text{argmin}} \\langle \\nabla l_t(\\pi_t), \\pi \\rangle + \\eta\\text{KL}(\\pi||\\pi_t),\n\\]\n\nwhere \\(\\nabla l_t(\\pi_t) = -\\mathbb{E}_{y' \\sim \\pi_t} [P(y > y')] + \\frac{\\tau}{\\eta} \\left(\\log \\frac{\\pi(y)}{\\pi_{\\text{ref}}(y)} + 1\\right)\\), \\(\\eta > 0\\) and \\(\\eta\\) is the learning rate of OMD.\nHere we also have a KL divergence term between \\(\\pi\\) and \\(\\pi_t\\) in the objective. The spirit is that we want to have\na stable algorithm, requiring the next policy \\(\\pi_{t+1}\\) not only outperforms \\(\\pi_t\\) but also stays close to \\(\\pi_t\\). Before\npresenting the theoretical guarantee, we make the bounded log density assumption, which is also used in\nprevious RLHF analysis [Rosset et al., 2024, Xie et al., 2024].\nAssumption A (Bounded Log Density). For any response \\(y \\in \\text{Supp}(\\pi_{\\text{ref}})\\), we assume that\n\n\\[\n\\log \\frac{1}{\\pi_{\\text{ref}}(y)} < B.\n\\]\n\nNext, we present the regret bound. The proof directly follows from the classical analysis of OMD algo-\nrithm [Lattimore and Szepesv\u00e1ri, 2020] and is deferred to Appendix A.1.\nLemma 2 (Regret Bound for OMD). Under Assumption A, let \\(C_{\\text{ref}} = - \\sum_y \\pi_{\\text{ref}}(y) \\log \\pi_{\\text{ref}}(y)\\) be the entropy\nof \\(\\pi_{\\text{ref}}\\, OMD algorithm in Eq. (3) with \\(\\eta = \\frac{\\tau}{\\text{max}(B, 1) \\sqrt{T}}\\) has the following guarantee:\n\n\\[\n\\sum_{t=1}^T \\langle \\nabla l_t(\\pi_t), \\pi_t \\rangle - \\sum_{t=1}^T \\langle \\nabla l_t(\\pi_t), \\pi^* \\rangle \\leq \\mathcal{O} \\left(\\text{max}(B, 1) \\sqrt{TC_{\\text{ref}}}\\right) := \\text{Reg}_T.\n\\]\n\nWith the regret bound, we are ready to show that the duality gap for uniform mixture of \\(\\pi_t\\) is well bounded.\nTheorem 3 (Duality Gap Bound for Uniform Mixture Policy in OMD). Let \\(\\bar{\\pi} := \\frac{1}{T}\\sum_{t=1}^T \\pi_t\\). With Assump-\ntion A and \\(\\eta = \\frac{\\tau}{\\text{max}(B, 1) \\sqrt{T}}\\, we have\n\n\\[\n\\text{DualGap}(\\bar{\\pi}) := \\underset{\\pi_1}{\\text{max}} \\mathcal{J}(\\pi_1, \\bar{\\pi}) - \\underset{\\pi_2}{\\text{min}} \\mathcal{J}(\\pi, \\pi_2) \\leq \\mathcal{O} \\left(\\frac{\\text{max}(B, 1) \\sqrt{C_{\\text{ref}}}}{\\sqrt{T}}\\right).\n\\]\n\nThe proof mainly relies on the convexity of \\(l_t\\) and the regret bound, see Appendix A.2. We remark that for a\nsymmetric game, \\(\\bar{\\pi}\\) is an \\(\\epsilon\\)-approximate Nash policy if \\(\\text{DualGap}(\\bar{\\pi}) \\leq \\epsilon\\). According to Theorem 3, our \\(\\bar{\\pi}\\) can\napproximate \\(\\pi^*\\) with an iteration complexity \\(\\mathcal{O}\\left(\\frac{1}{\\epsilon^2}\\right)\\). However, despite the OMD algorithm already enjoying\na good theoretical guarantee, it assumes that we have access to \\(\\mathbb{E}_{y\\sim \\pi, y' \\sim \\pi_t} [P(y > y')]\\) for any \\(\\pi \\in \\Pi\\), which\nis difficult to compute in practice. Therefore, we still need to design a learning algorithm that only assumes\nquery access to the preference oracle."}, {"title": "3.2 Population Loss", "content": "In this subsection, we introduce how to obtain a population loss objective for Eq. (3). Similar to the derivation\nof DPO [Rafailov et al., 2024], we start with the closed-form solution to Eq. (3):\n\n\\[\n\\pi_{t+1}(y) \\propto \\pi_t(y) \\exp\\left(-\\frac{1}{\\eta}l_t(\\pi)\\right)\n\\propto \\exp\\left(\\frac{1}{\\eta} \\left(\\mathbb{E}_{y' \\sim \\pi_t} [P(y > y')] + \\frac{\\tau}{\\eta} \\left(\\log \\frac{\\pi(y)}{\\pi_{\\text{ref}}(y)} + \\log \\pi_t(y)\\right)\\right)\\right),\n\\]\n\nwhere \\(P(y > \\pi_t)\\) represents \\(\\mathbb{E}_{y' \\sim \\pi_t} [P(y > y')]\\). To avoid computing the normalization factor, for each\nresponse pair \\(y, y'\\) and policy \\(\\pi\\), we define function \\(h_t(\\pi, y, y')\\) as:\n\n\\[\nh_t(\\pi, y, y') = \\log \\frac{\\pi(y)}{\\pi(y')} - \\frac{\\tau}{\\eta} \\log \\frac{\\pi_{\\text{ref}}(y)}{\\pi_{\\text{ref}}(y')} - \\frac{\\tau}{\\eta} \\log \\frac{\\pi_t(y)}{\\pi_t(y')}.\n\\]\n\nDifferent from Azar et al. [2024], we use an iterative algorithm, hence our \\(h_t\\) contains both the log-likelihood\nof the reference policy \\(\\pi_{\\text{ref}}\\) and the last iteration policy \\(\\pi_t\\). From Eq. 4, we know that the following equality\nholds for any response pair \\(y, y' \\in \\text{Supp}(\\pi_{\\text{ref}})\\):\n\n\\[\nh_t(\\pi_{t+1}, y, y') = \\frac{P(y > \\pi_t) - P(y' > \\pi_t)}{\\eta}.\n\\]\n\nBased on this observation, we define the loss function \\(L_t(\\pi)\\) as:\n\n\\[\nL_t(\\pi) = \\mathbb{E}_{y, y' \\sim \\pi} \\left[h_t(\\pi, y, y') - \\frac{P(y > \\pi_t) - P(y' > \\pi_t)}{\\eta}\\right]^2.\n\\]\n\nIt is clear to see that \\(\\pi_{t+1}\\) is the minimizer of \\(L_t(\\pi)\\) since \\(L_t(\\pi_{t+1}) = 0\\). Furthermore, in the following\nlemma, we show that \\(\\pi_{t+1}\\) is the unique minimizer of \\(L_t\\) within the policy class \\(\\Pi\\). The proof is deferred to\nAppendix A.3.\nLemma 4. Let \\(\\Pi = {\\pi : \\text{Supp}(\\pi) = \\text{Supp}(\\pi_{\\text{ref}})}\\), \\(\\pi_{t+1}\\) is the unique minimizer of \\(L_t(\\pi)\\) in \\(\\Pi\\).\nTherefore, solving for \\(\\pi_{t+1}\\) is equivalent to finding a policy that minimizes \\(L_t(\\pi)\\). However, we still have the\nintractable term \\(P(y - \\pi_t)\\) in our loss. To bypass this term, we propose the following population loss:\n\n\\[\n\\mathbb{E}_{y, y' \\sim \\pi_t, y_w, y_l \\sim \\lambda_p(y, y')} \\left[h_t(\\pi, y_w, y_l) - \\frac{1}{2\\eta}\\right]^2,\n\\]\n\nRecall that \\(\\lambda_p(y, y')\\) is the preference distribution defined in Eq. (1) without context. Similar to the derivation\nin Azar et al. [2024], we show the equality between \\(L_t(\\pi)\\) and Eq. (7) in the following proposition.\nProposition 5. For any policy \\(\\pi \\in \\Pi\\) and any iteration \\(t\\), \\(L_t(\\pi)\\) in Eq. (6) and expression in Eq. (7) are\nequal up to an additive constant independent of \\(\\pi\\).\nSee the proof in Appendix A.4. With the population loss in hand, we can collect a preference dataset with \\(\\pi_t\\)\nin each iteration and directly minimize the loss on the dataset to solve for \\(\\pi_{t+1}\\)."}, {"title": "3.3 Iterative Nash Policy Optimization Algorithm", "content": "In this subsection, we summarize our algorithm INPO in Algorithm 1. In the beginning, we initialize our\npolicy \\(\\pi_1\\) as the reference policy \\(\\pi_{\\text{ref}}\\). For each iteration \\(t\\), we sample \\(n\\) response pairs from current policy \\(\\pi_t\\)\nand query the preference oracle \\(\\mathcal{P}\\) to obtain the preference dataset \\(\\mathcal{D}_t\\). With the preference dataset, we find\nthe policy \\(\\pi_{t+1}\\) that minimizes the sampled version of Eq. 7. Instead of uniformly mixing the policies, we\ndirectly select the last iteration policy \\(\\pi_{t+1}\\), which better aligns with the practice."}, {"title": "4 Experiments", "content": "In this section, we use empirical results to verify the effectiveness of our INPO algorithm."}, {"title": "4.1 Main Results", "content": "We compare our INPO with the state-of-the-art method iterative DPO [Dong et al.,\n2024], as shown in Table 1. We observe that INPO outperforms iterative DPO on all three benchmarks, with\nparticularly significant improvements on AlpacaEval 2.0 and Arena-Hard v0.1. Additionally, we compare\nINPO with other LLMs, including LLaMA-3-70B-it, GPT-4-0613, Claude-3-Opus, and GPT-4 Turbo. For\nAlpacaEval 2.0, our INPO is only surpassed by GPT-4 Turbo and outperforms all other models. According to\nthe results in [Dubois et al., 2024], LC AlpacaEval 2.0 has the highest correlation with Chatbot Arena [Zheng\net al., 2024], highlighting the superior performance achieved by INPO.\nMoreover, we note that methods utilizing the preference model as the oracle generally perform better than\nthose using the BT reward model as the oracle. This observation aligns with the results reported by previous\nstudies [Ye et al., 2024, Dong et al., 2024], where the preference model outperforms the BT reward model on\nRewardBench [Lambert et al., 2024], demonstrating the necessity of considering general preferences without\nthe BT model assumption."}, {"title": "4.2 Ablation Study and Length Control", "content": "In this subsection, we conduct an ablation study to examine the importance of including the KL regularization\nterm in the game objective. The results are shown in Table 2. We observe that INPO with KL regularization\n(INPO w/ KL) generally outperforms its counterpart without KL regularization (INPO w/o KL) by a clear\nmargin. More importantly, by regularizing our policy towards the reference policy, we consistently obtain"}, {"title": "4.3 Results on Academic Benchmarks", "content": "It is known that RLHF alignment may have a negative effective on a model's abilities in reasoning, calibration\nand generating accurate responses [Ouyang et al., 2022, Bai et al., 2022c, Dong et al., 2024]. Therefore, it is\nnecessary to evaluate the model performance on academic benchmarks. In this subsection, we present the\nresults on six academic benchmarks, evaluating various model abilities including explicit instruction follow-\ning [Zhou et al., 2023], general knowledge [Rein et al., 2023], multitask language understanding [Hendrycks\net al., 2020], commonsense reasoning [Zellers et al., 2019], human falsehoods mimicking [Lin et al., 2021]\nand math word problem-solving [Cobbe et al., 2021]. We compare our INPO with both the SFT baseline and\niterative DPO, and the results are shown in Table 3.\nInterestingly, compared to the SFT baseline, both INPO and iterative DPO exhibit performance im-\nprovements on these benchmarks. A potential reason for this is that during the alignment stage, we only\nuse 60k prompts in total, which is relatively small in magnitude. Additionally, both INPO and iterative\nDPO incorporate KL regularization, which prevents the learned policy from deviating significantly from the\nreference policy, thereby avoiding performance degradation."}, {"title": "5 Related Work", "content": "Reward-Based RLHF. Since RLHF has achieved great success in LLM alignment [Ouyang et al., 2022,\nTouvron et al., 2023, Achiam et al., 2023], it has been extensively studied, including using RL algorithms\nsuch as PPO [Schulman et al., 2017] to maximize a KL-regularized objective [Bai et al., 2022c, Korbak et al.,\n2022, Li et al., 2023b] and reward-ranked finetuning [Dong et al., 2023, Yuan et al., 2023, Gulcehre et al.,\n2023]. Recently, Rafailov et al. [2024] propose the DPO algorithm, which directly optimizes the policy on\na preference dataset, bypassing the need for reward model training. Further studies by Xiong et al. [2024],\nDong et al. [2024], Xie et al. [2024] investigate the online variant of DPO, proposing iterative algorithms"}, {"title": "6 Conclusion and Future Work", "content": "In this work, we consider RLHF under general preferences and formulate it as a two-player game. Building\non no-regret learning, we propose a new self-play algorithm, iterative Nash policy optimization (INPO), to\nlearn the Nash policy of the game. To bypass the estimation of the expected win rate, we design a new loss\nobjective, and our algorithm directly minimizes it over a preference dataset. In the future, we plan to study\nthe finite-sample analysis of our algorithm and extend it to the general reinforcement learning framework,\nsuch as Markov decision process."}, {"title": "A Proofs for Section 3", "content": "A.1 Proof for Lemma 2\nProof. According to the classical analysis of OMD algorithm [Lattimore and Szepesv\u00e1ri, 2020], for any\npolicy \\(\\pi\\), we have\n\n\\[\n\\sum_{t=1}^T \\langle \\nabla l_t(\\pi_t), \\pi_t \\rangle - \\sum_{t=1}^T \\langle \\nabla l_t(\\pi_t), \\pi \\rangle \\leq -\\eta \\sum_y \\pi_1(y) \\log \\pi_1(y) + \\frac{1}{2\\eta} \\sum_{t=1}^T \\left\\|l_t(\\pi_t)\\right\\|^2\n\\]\n\n\\[\n\\leq \\eta C_{\\text{ref}} + \\frac{(2\\tau^2B^2 + 1)T}{\\eta}\n\\]\nIn the second step, w.l.o.g., we assume \\(B > 1\\). Picking \\(\\eta = \\frac{\\tau}{\\text{max}(B, 1) \\sqrt{T}}\\) finishes the proof.\nA.2 Proof for Theorem 3\nProof. We first decompose DualGap(\\(\\bar{\\pi}\\)) as\n\n\\[\n\\text{DualGap}(\\bar{\\pi}) = \\underset{\\pi_1}{\\text{max}} \\mathcal{J}(\\pi_1, \\bar{\\pi}) - \\mathcal{J}(\\pi^*, \\pi^*) + \\mathcal{J}(\\pi^*, \\pi^*) - \\underset{\\pi_2}{\\text{min}} \\mathcal{J}(\\pi, \\pi_2) .\n\\]\n\nNext, we show how to bound Term A. Since \\(l_t\\) is convex for all \\(t\\), for any \\(\\pi\\), we have\n\n\\[\n\\sum_{t=1}^T l_t(\\pi_t) - \\sum_{t=1}^T l_t(\\pi) \\leq \\sum_{t=1}^T \\langle \\nabla l_t(\\pi_t), \\pi_t \\rangle - \\sum_{t=1}^T \\langle \\nabla l_t(\\pi_t), \\pi \\rangle \\leq \\text{Reg}_T.\n\\]\n\nAccording to the definition of \\(l_t\\), we also get that\n\n\\[\n\\frac{1}{T} \\sum_{t=1}^T (l_t(\\pi_t) - l_t(\\pi))\n\\]\n\n\\[\n= \\frac{1}{T} \\sum_{t=1}^T(-\\mathbb{E}_{y\\sim \\pi, y' \\sim \\pi_t} [P(y > y')] + \\tau \\text{KL}(\\pi_t||\\pi_{\\text{ref}}) + \\mathbb{E}_{y\\sim \\pi, y' \\sim \\pi_t} [P(y > y')] - \\tau \\text{KL}(\\pi||\\pi_{\\text{ref}}))\n\\]\n\n\\[\n= \\frac{1}{T} \\sum_{t=1}^T(\\mathbb{E}_{y\\sim \\pi, y' \\sim \\pi_t} [P(y > y')] + \\tau \\text{KL}(\\pi_t||\\pi_{\\text{ref}})) - \\tau \\text{KL}(\\pi||\\pi_{\\text{ref}}) - \\frac{1}{2}\n\\]\n\n\\[\n\\geq \\mathcal{J}(\\pi, \\bar{\\pi}) - \\frac{1}{2} = \\mathcal{J}(\\pi, \\bar{\\pi}) - \\mathcal{J}(\\pi^*, \\pi^*).\n\\]\n\nThe inequality is from Jensen's inequality and convexity of KL divergence. Combining Eq. (8) and Eq. (9),\nwe obtain that for any \\(\\pi\\)\n\n\\[\n\\mathcal{J}(\\pi, \\bar{\\pi}) - \\mathcal{J}(\\pi^*, \\pi^*) \\leq \\frac{\\text{Reg}_T}{T}\n\\]\nSince the game is symmetric, Term B can also be bounded similarly. Finally, we get\n\n\\[\n\\text{DualGap}(\\bar{\\pi}) \\leq \\frac{2\\text{Reg}_T}{T} \\leq \\mathcal{O} \\left(\\frac{\\text{max}(B, 1) \\sqrt{C_{\\text{ref}}}}{\\sqrt{T}}\\right).\n\\]\n\nThe proof is completed.\nA.3 Proof for Lemma 4\nProof. We use contradiction to prove the lemma. Let \\(\\tilde{\\pi} \\in \\Pi\\) be another policy such that \\(\\tilde{\\pi} \\neq \\pi_{t+1}\\) and\n\\(L_t(\\tilde{\\pi}) = 0\\). Let \\(y\\) be an arbitrary element from \\(\\mathcal{Y}\\). For any other \\(y' \\in \\text{Supp}(\\pi_{\\text{ref}})\\) and \\(y' \\neq y\\), we have\n\n\\[\n\\frac{\\tilde{\\pi}(y)}{\\tilde{\\pi}(y')} = \\frac{\\exp \\left(\\frac{P(y>\\pi_t)}{\\eta}\\right)}{\\exp \\left(\\frac{P(y'>\\pi_t)}{\\eta}\\right)} = \\frac{\\left(\\\\frac{\\pi_{\\text{ref}}(y)}{\\pi_t(y)}\\right)^{\\frac{\\tau}{\\eta}}}{\\left(\\frac{\\pi_{\\text{ref}}(y')}{\\pi_t(y')}\\right)^{\\frac{\\tau}{\\eta}}},\n\\]\nSince \\(\\text{Supp}(\\tilde{\\pi}) = \\text{Supp}(\\pi_{\\text{ref}})\\), we also have \\(\\sum_{y' \\in \\text{Supp}(\\pi_{\\text{ref}})} \\tilde{\\pi}(y') = 1\\). Hence, the value of \\(\\tilde{\\pi}(y)\\) is uniquely\ndetermined. Because \\(\\pi_{t+1}\\) also satisfies Eq. 10 and shares the same support set as \\(\\tilde{\\pi}\\), we have \\(\\tilde{\\pi}(y) = \\pi_{t+1}(y)\\)\nand hence \\(\\tilde{\\pi}(y') = \\pi_{t+1}(y')\\) for all \\(y' \\in \\mathcal{Y}\\), contradicting with \\(\\tilde{\\pi} \\neq \\pi_{t+1}\\). Therefore, the minimizer is unique\nand the proof is completed.\nA.4 Proof for Proposition 5\nProof. We first consider the following expression and show that it equals to \\(L_t(\\pi)\\) up to some constants:\n\n\\[\n\\mathbb{E}_{y, y' \\sim \\pi, I \\sim \\text{Ber}(P(y>y'))} \\left[h_t(\\pi, y, y') - \\frac{I}{2\\eta}\\right]^2.\n\\]\n\nIt suffices to show that\n\n\\[\n\\mathbb{E}_{y, y'} [h_t(\\pi, y, y') (P(y > \\pi_t) - P(y' > \\pi_t))] = \\mathbb{E}_{y, y',I} [h_t(\\pi, y, y')I] .\n\\]\nLet \\(p_y = P(y > \\pi_t)\\) and \\(\\pi_y = \\frac{\\tau}{\\eta} \\log \\pi(y)\\), \\(\\pi_{\\text{ref},y} = \\frac{\\tau}{\\eta} \\log \\pi_{\\text{ref}}(y)\\) and \\(\\pi_{t,y} = \\frac{\\tau}{\\eta} \\left(1-\\frac{\\tau}{\\eta}\\right) \\log \\pi_t(y)\\). For RHS, it can\nbe written as\n\n\\[\n\\mathbb{E}_{y, y',I} [h_t(\\pi, y, y')I]\n\\]\n\n\\[\n= \\mathbb{E}_{y, y',I} [(\\pi_y - \\pi_{y'} - \\pi_{\\text{ref}, y} + \\pi_{\\text{ref}, y'} - \\pi_{t,y} + \\pi_{t,y'}) I]\n\\]\n\n\\[\n= \\mathbb{E}_y [(\\pi_y - \\pi_{\\text{ref}, y} - \\pi_{t,y}) \\mathbb{E}_{y',I}[I]] + \\mathbb{E}_{y'} [(-\\pi_{y'} + \\pi_{\\text{ref}, y'} + \\pi_{t,y'}) \\mathbb{E}_{y,I}[I]]\n\\]\n\n\\[\n= \\mathbb{E}_{y,y'} [\\pi_y p_y - \\pi_{\\text{ref}, y} p_y - \\pi_{t,y} p_y - (1 - p_{y'}) \\pi_{y'} + (1 - p_{y'}) \\pi_{\\text{ref}, y'} + (1 - p_{y'}) \\pi_{t,y'}]\n\\]\n\n\\[\n= \\mathbb{E}_y [(2p_y - 1) \\pi_y - (2p_y - 1) \\pi_{\\text{ref}, y} - (2p_y - 1) \\pi_{t,y}].\n\\]\n\nIn the last step, we use the fact that \\(y\\) and \\(y'\\) are from the same distribution. The LHS can be written as\n\n\\[\n\\mathbb{E}_{y, y'} [h_t(\\pi, y, y') (P(y > \\pi_t) - P(y' > \\pi_t))]\n\\]\n\n\\[\n= \\mathbb{E}_{y, y'} [(\\pi_y - \\pi_{y'} - \\pi_{\\text{ref}, y} + \\pi_{\\text{ref}, y'} - \\pi_{t,y} + \\pi_{t,y'}) (p_y - p_{y'})]\n\\]\n\n\\[\n= \\mathbb{E}_{y, y'} [2p_y\\pi_y - p_y\\pi_{y'} - p_{y'} \\pi_y - 2p_y\\pi_{\\text{ref}, y} + p_{y'} \\pi_{\\text{ref}, y} + p_y \\pi_{\\text{ref}, y'} - 2p_y\\pi_{t,y} + p_{y'} \\pi_{t,y} + p_y \\pi_{t,y'}]\n\\]\n\n\\[\n= \\mathbb{E}_y [(2p_y - 1) \\pi_y - (2p_y - 1) \\pi_{\\text{ref}, y} - (2p_y - 1) \\pi_{t,y}].\n\\]\n\nThe second equality is from that \\(y\\) and \\(y'\\) are from the same distribution. The last equality is from that\n\\(\\mathbb{E}_y [p_y] = \\frac{1}{2}\\). Therefore, we show the equivalence between \\(L_t(\\pi)\\) and Eq. 11. Next, we show the equivalence\nbetween Eq. 7 and Eq. 11. We expand the expectation over \\(\\lambda_p(y, y')\\) and rewrite Eq. 7 as\n\n\\[\n\\mathbb{E}_{y, y'} [P(y> y') \\left(h_t(\\pi, y, y') - \\frac{1}{2\\eta}\\right)^2 + (1 - P(y > y')) \\left(h_t(\\pi, y', y) - \\frac{1}{2\\eta}\\right)^2]\n\\]"}, {"title": "B Additional Experiment Details", "content": "We implement iterative DPO according to Dong et al. [2024] and their GitHub repository https://\ngithub.com/RLHFlow/Online-RLHF. For the implementation of INPO, we follow the hyperpa-\nrameters in Dong et al. [2024], including the cosine learning rate scheduler with a peak learning rate\nof 5 \u00d7 10-7, a 0.03 warm-up ratio, and a global batch size of 128. We use a grid search for \\(\\eta\\) over\n[0.1, 0.01, 0.0075, 0.005, 0.002] and set \\(\\eta = 0.0075\\). \\(\\tau\\) is directly set to be one-third of \\(\\eta\\), which is 0.0025.\nFor the tournament strategy of the preference model, we first split 8 samples into 4 pairs and compare\neach pair. If the result is a tie, we select the first one as the winner. Then, the winners are compared against\neach other and the losers against each other until we get the final winning response \\(y_w\\) and losing response \\(y_l\\).\nWe finally compare \\(y_w\\) with \\(y_l\\) and only train the model with the pairs where \\(y_w\\) wins over \\(y_l\\). We need 11\ncomparisons in total for 8 responses."}]}