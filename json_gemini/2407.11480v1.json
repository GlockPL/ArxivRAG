{"title": "AIGC for Industrial Time Series: From Deep Generative Models to Large Generative Models", "authors": ["Lei Ren", "Haiteng Wang", "Yang Tang", "Chunhua Yang"], "abstract": "With the remarkable success of generative models like ChatGPT, Artificial Intelligence Generated Content (AIGC) is undergoing explosive development. Not limited to text and images, generative models can generate industrial time series data, addressing challenges such as the difficulty of data collection and data annotation. Due to their outstanding generation ability, they have been widely used in Internet of Things, metaverse, and cyber-physical-social systems to enhance the efficiency of industrial production. In this paper, we present a comprehensive overview of generative models for industrial time series from deep generative models (DGMs) to large generative models (LGMs). First, a DGM-based AIGC framework is proposed for industrial time series generation. Within this framework, we survey advanced industrial DGMs and present a multi-perspective categorization. Furthermore, we systematically analyze the critical technologies required to construct industrial LGMs from four aspects: large-scale industrial dataset, LGMs architecture for complex industrial characteristics, self-supervised training for industrial time series, and fine-tuning of industrial downstream tasks. Finally, we conclude the challenges and future directions to enable the development of generative models in industry.", "sections": [{"title": "I. INTRODUCTION", "content": "Industry 5.0 incorporates cyber-physical-social elements into manufacturing, emphasizing digital-physical interaction and human-machine collaboration, effectively connecting the Internet of devices, things, and people. Currently, with the development of Artificial Intelligence Generated Content (AIGC), metaverse and digital twins technologies, industrial big data can be used to create digital manufacturing and industrial processes [1]. It facilitates significant growth in productivity, efficiency, and effectiveness in Industry 5.0 and cyber-physical-social systems (CPSS) [2].\nIndustrial time series data, including equipment sensor data, production-line operation data, and system status data, is an important foundation for a broad range of industrial intelligence applications [3]. Currently, more and more industrial intelligence applications, such as intelligent control systems [4], [5], predictive maintenance [6], [7], and fault diagnosis [8], [9], rely on mining and analyzing industrial time series. In particular, the success of industrial temporal deep learning methods is also dependent on learning and extracting features from industrial time series data.\nThe diversity, quality, and quantity of industrial time series are critical for improving system performance and facilitating industrial intelligence applications. Unfortunately, in industrial scenarios, obtaining such data can be challenging due to the difficulty of industrial data collection, data annotation, and privacy concerns. For example, data collection for industrial engine equipment requires significant labor costs. Fatigue testing may even damage industrial equipment. Moreover, data silos and sensitive information about industrial data further hinder the creation of rich and diverse datasets. To address these challenges, data generation through generative models provides a promising solution.\nIn the past two years, large generative models, such as DALLE 2 [10] and ChatGPT [11] have ushered in a new era of AIGC. These models can generate new data with similar statistical characteristics by learning the distribution of existing data. This enable significant progress in the fields of image synthesis, text generation, and molecular design, among other tasks. Microsoft, OpenAI, and others have used synthetic data from generative models to train intelligent models. This shows the powerful capability of generative models to deal with challenges related to data annotation and collection.\nGenerative models can be classified into large generative models (LGMs) and deep generative models (DGMs). DGMS for industrial data generation, include generative adversarial networks (GAN) [12]\u2013[15], variational autoencoders (VAE) [16]\u2013[18], and diffusion models [19]\u2013[21]. Unlike traditional data extension methods such as time-shifting, downsampling, and panning, DGMs are able to learn and model complex distributions of complex data to generate diverse new samples.\nIn the industrial field, DGMs have become a research hotspot and have been widely applied. Its core tasks can be roughly categorized into five: limited sample augmentation [22]\u2013[24], imbalanced data synthesis [25], [26], sensor signal imputation [27], [28], sensor signal denoising [29], [30], privacy protection [31]\u2013[33]. DGMs help to alleviate issues such as data privacy and shortage. By training generative models, companies can synthesize data without disclosing sensitive information, thus protecting data privacy. In addition, data generation is able to expand limited real datasets, reduce the cost of data acquisition, and improve the generality of models. Generating samples with less variety solves the problem of data imbalance. Recent research has exploited the ability of DGMs to generate real data samples for industrial applications such as anomaly detection [34], [35], multi-classification fault instance generation (to address data imbalance issues) [36], [37], trust boundary protection [38], [39], and platform monitoring [40], [41].\nSince generative models have been applied in various fields and have shown promising results in industrial time series applications, there have been recent studies analyzing and surveying DGMs. Comprehensive reviews, such as the survey [42], provide profound insights into diffusion models across domains such as image synthesis, time series prediction, and natural language generation. Additionally, domain-specific reviews like [43] presented a review about the application of DGMs in the industry. Similarly, with the rapid development of diffusion models, there are some reviews on diffusion models in specific fields such as biological signals [44] or computer vision [45]. Due to the significance of time series in industry, literature [3] has surveyed time series data modeling issues.\nIn addition to DGMs with specific-domain, specific-task, and specified generation ability, LGMs with multi-domain, multi-task, and generalized generation ability have made significant progress. Some recent studies [46], [47] have investigated the LGMs in general applications.\nAlthough these existing works review deep generative mod-els, GAN-based methods, and deep learning-based industrial applications, we have identified a gap in the literature regard-ing the classification and application of generative models in industrial time series. As time series data stands as the most prevalent and essential data type in industry, its generation holds significant importance. Moreover, there is still a lack of sufficient research on how to construct LGMs in industry. Therefore, this survey focuses on a comprehensive review of DGMs in industrial time series and also analyzes the critical technologies required to construct industrial LGMs in the industrial field. The comparison between existing related surveys and this paper is explicitly shown. The contribution of the paper can be summarized as follows:\n1) This paper provides a comprehensive overview of the generative models in the industry, from the current state-of-the-art DGMs to the future promising LGMs.\n2) A DGM-based AIGC framework is proposed for in-dustrial time series generation. Within the framework, this paper surveys advanced industrial DGMs and presents a multi-perspective categorization.\n3) This paper proposes a systematic analysis of the critical technologies required to construct industrial LGMs that are suitable for industrial time series data.\nThe organization of the rest of the sections is as follows. In Section II, the commonly used DGMs are introduced. In Section III, the definition, characteristics, and requirements of industrial time series generation are presented. In Section IV, the proposed DGM-based AIGC framework and the current state-of-the-art DGMs are introduced. In Section V, we present the typical industrial applications of DGMs. In Section VI, we analyze how to construct LGMs for industrial time series. Finally, Section VII concludes the challenges and future di-rections of generative models."}, {"title": "II. DEEP GENERATIVE MODELS", "content": "In this section, we introduce general DGMs architectures and compare their characteristics and applications."}, {"title": "A. Autoregressive Model", "content": "An autoregressive model (AR) [50] is a particular regression model, wherein a value from the time series is subjected to a regression on previous values. It predicts future values in a sequence based on previous values. The autoregressive model of order p (often termed AR(p) model) is defined as:\n$X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2}+ ... + \\phi_p X_{t-p} + E_t$ (1)\nwhere $X_t$ is the value of the series at time t, c is a con-stant, $\\phi_1, \\phi_2,..., \\phi_p$ are the parameters of the model, $e_t$ is white noise. The likelihood for an AR(p) model given data {X1,X2,...,XT} is:\n$L(\\phi; x_1, x_2,...,x_T) = \\prod_{t=p+1}^T f(X_t|X_{t-1}, X_{t-2},...,X_{t-p})$ (2)\nwhere f is the conditional density of $x_t$ given past values. For many autoregressive models, especially when the noise $E_t$ is Gaussian, the negative log-likelihood serves as loss function:\n$L = - \\sum_{t=p+1}^T log f(x_t|x_{t-1}, X_{t-2},..., X_{t-p})$ (3)\nMinimizing this loss function gives the maximum likelihood estimates of the parameters.\nAutoregressive models are commonly used in industrial forecasting [51] [52]. For example, Wang et al. [53] proposed a model, combining non-linear autoregressive neural network"}, {"title": "B. Variational Autoencoder (VAE)", "content": "VAE is a generative model that extends the concept of the traditional auto-encoder (AE) [54] by introducing a probabilis-tic approach to describe the data generation process. It operates by first encoding an input into a distribution over a latent space and then sampling from this distribution to generate new data. Unlike a standard AE that simply compresses and reconstructs data, VAEs [16] are designed to produce novel samples. It is achieved by constructing a loss function that not only minimizes reconstruction error but also includes a term that regularizes the latent variable distribution, often encouraging it to approximate a standard normal distribution.\nGiven x as the observed data, z as the latent variable, and the following definitions:\n\u2022 p(z): Prior distribution of the latent variable, typi-cally chosen as a standard normal distribution p(z) =\nN(z; 0, I).\n\u2022 q(zx): Output of the encoder, i.e., the posterior distribu-tion of the latent variable given the data with the weight\n$ of the encoder E.\n\u2022 po(xz): Data generative model given the latent variable\nwith the weight @ of the decoder D.\nThe VAE loss is defined as the sum of the reconstruction loss and the regularization loss:\n$L_{VAE} = Reconstruction loss + Regularization loss$ (4)\nMore specifically, the VAE loss is given by:\n$L_{VAE}(\\phi, \\theta) = -E_{q_{\\phi}(z|x)} [log p_{\\theta}(x|z)] + D_{KL}(q_{\\phi}(z|x)||p(z))$ (5)\nVAE, like other deep learning techniques, has received a lot of attention from academia since its inception, but its application in industry may take some time as industrial applications usually require the maturity and stability of the technology. VAE has been successively applied to the industry for some anomaly detection [55] [56] [57], etc."}, {"title": "C. Generative Adversarial Network (GAN)", "content": "The Generative Adversarial Networks (GANs) were first proposed by Goodfellow et al. [15] in 2014. It is an unsu-pervised learning algorithm that involves the interaction of two independent neural networks against each other to achieve learning, which can be characterized as a zero-sum game. GANs do not require prior knowledge and do not require assumptions that the sample obeys a certain distribution.\nA typical generative adversarial network consists of two parts: a discriminator D and a generator G. The generator aims to generate synthetic data samples, whereas the discriminator strives to differentiate between actual and synthetic data. Hence, the objective function of GAN comprises maximizing the loss of the discriminator and minimizing the loss of the generator. Let x represent real data, random noise z is sampled from a prior distribution pz, G(z) is the synthetic sample generated by the generator from the noise z, D(x) is the probability given by the discriminator that data x is real. Then we have:\nGenerator Loss :\n$L_G = -E_{z~p_z(z)} [log D(G(z))]$ (6)\nDiscriminator Loss:\n$L_D = -E_{x~P_{data}(x)} [log D(x)] - E_{z~p_z(z)}[log(1 \u2013 D(G(z)))]$ (7)\nHence, the objective function can be expressed as:\n$\\min_G \\max_D V (D, G) = E_{x~P_{data}(x)} [log D(x)]$\n$+ E_{z~p_z(z)} [log(1 \u2013 D(G(z)))]$ (8)\nIn the evolution of GANs within industrial applications, key milestones are marked by their use in various domains over the years. In 2015, GANs were first leveraged for data augmentation purposes [58]. The scope of their application expanded in 2017 when they were utilized for fault detection [59]. A significant advancement came in 2018 when Zhao et al. [60] pioneered the application of GANs in defect detection, a concept further elaborated in [61]. This progression under-scores the increasing versatility and significance of GANs in enhancing industrial processes through innovative AI-driven solutions."}, {"title": "D. Diffusion Model", "content": "Diffusion models, a group of generative models inspired by the physical process of diffusion, have become increas-ingly prominent in the field of machine learning recently. The Denoising Diffusion Probabilistic Model (DDPM) [19] is beginning to receive widespread academic attention. It is noteworthy to mention that before this study, Song Yang et al. [62] proposed a score-based diffusion model, and these two models were subsequently harmonized in the SDE study [63]. The core idea of Diffusion Models is to gradually transform the distribution of the data into a known simple distribution (e.g., a Gaussian distribution), and then sample from the simple distribution and reverse transform it back into the distribution of the data. And we implement both forward and backward processes through two Markov chains [64].\nGiven a distribution of data p(x), a diffusion process can be defined at each time step t, and the data x is perturbed to xt. This process can be characterized as follows:\n$x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t}e_t$ (9)\nwhere $e_t ~ N(0, I)$ is a Gaussian noise and $\\beta_t$ is a predefined noise factor.\nA denoising function qe can be trained to reconstruct $x_{t-1}$ from $x_t$. Given a sample $x_t$ and its corresponding noise $e_t$, the output of the denoising function is a conditional distribution $q_{\\theta}(x_{t-1}|x_t)$.\nThe loss function of DDPM is based on the difference between the output of the denoising function and the true $x_{t-1}$. It can be expressed as:\n$L(\\theta) = E_{x_t~p(x_t), e_t~N(0,1)} [\u2212 log q_{\\theta}(x_{t\u22121}|x_t)]$ (10)\nwhere the expectation is taken overall time steps and all possible $x_t$.\nDiffusion models, initially emerging within computer vision [65], have expanded their utility to encompass sequential modeling [66]\u2013[68], ventured into the audio domain [69], and bridged disciplines in AI for science [70]. These models have been adeptly integrated into various industrial sectors, tackling complex challenges such as the imputation of sensor data [71] and the nuanced task of anomaly detection [72]."}, {"title": "III. THE DEFINITION, CHARACTERISTIC, AND DEMAND OF INDUSTRIAL TIME SERIES GENERATION", "content": "In this section, we will present three points: 1) what is industrial time series generation 2) what are the characteristics of industrial time series generation 3) what are the current challenges of industrial time series acquisition, that evoke the demand for industrial generative models."}, {"title": "A. The Definition of Industrial Time Series Generation", "content": "The given time series T with N (N > 1) individual series of length L is represented as a matrix, i.e., $T = (s_1,...,s_N)^T$, where each individual series $s_i$ can be expressed as a L-dimensional vector, i.e., $s_i = (x_{i,1},...,x_{i,L})$, and each $x_{ij}$ corresponds to a single time point $t_j$ of $s_i$. We denote $p(s_1,...,s_N)$ as the real distribution of a given time series T. In the context of industrial time series generation, L-dimensional vector variables represent Lindustrial sensors. The goal of time series Generation is to create a synthetic time series $T_{gen} = (s_{gen,1},..., s_{gen,N})$ such that its distribution $q(s_{gen,1},..., s_{gen, N})$ is similar to $p(s_1,...,s_N)$, and $T_{gen}$ and T exhibit consistent statistical properties and patterns."}, {"title": "B. The Characteristic of Industrial Time Series Generation", "content": "Generative tasks in the industrial domain differ significantly from those in the internet domain, primarily in terms of data types and application scenarios. In the internet domain, generative tasks typically involve multimedia data such as text, images, and videos, with diverse applications like text-to-image and image-to-text generation. In contrast, industrial data is more diverse, including sensor data, text, and images, with the most critical being sensor time series data that records detailed operational states of equipment. Industrial time series generation has distinctive industrial characteristics.\n1) Industrial time series have complex time series dependen-cies and patterns. As industrial processes become automated and industrial systems become more and more complex, it is no longer sufficient to rely on univariate time series data alone to provide a comprehensive and effective representation of industrial processes. As a result, multiple sensors are often utilized to monitor the entire industrial process. Generating multivariate time series is particularly challenging, as it re-quires dealing with correlations and temporal dependencies between patterns as well as variables.\n2) Dynamic variability inherent in industrial processes. Industrial processes typically exhibit a high degree of dynamic variability and are susceptible to conditions including pressure, temperature, and humidity fluctuations. These environmental variations lead to fluctuations in outputs, resulting in data offsets and domain offsets. Therefore, to maintain accurate industrial data generation, these dynamic patterns must be captured.\n3) Industrial scenarios require high reliability of time series. Sensor time series is the most common data type in the industry, so the subject of the generation task should be sensor time series data. In addition, industrial scenarios have strict requirements on the reliability of the data and the need to accu-rately present complex equipment operating states. Therefore, the generated time series data in the industrial scenarios must have authenticity and reliability to ensure accurate simulation of real industrial scenarios."}, {"title": "C. Challenges in Industrial Time Series Acquisition", "content": "There are a number of challenges in industrial time series acquisition that need to be addressed by generative models.\nLimited Labeled Time Series: Collecting and testing data from complex industrial equipment such as aircraft engines is extremely difficult. This lead to the relatively small size of industrial well-labeled datasets. However, deep learning relies heavily on extensive labeled data for effective supervised learning, making it challenging to train models when labeled examples of industrial data are scarce. The amount of data can be augmented using generative models.\nImbalanced Industrial Data Distribution: The problem of imbalanced industrial data distribution. In manufacturing systems, failures and anomalous events in industrial systems typically occur less frequently than in normal operating states, and there may be an insufficient number of instances of industrial equipment failures. This makes it difficult for deep learning models to accurately and efficiently capture and generalize patterns associated with these less representative states. Employing generative models enables the generation of a smaller number of samples, such as fault samples, resulting in a dataset with a balanced distribution.\nMissing Sensor Time Series Values: Missing data values are a common challenge in industrial time series applications. Unexpected interruptions in sensor failures and network delays can cause missing time series data. Managing and dealing with missing data is critical to maintaining the integrity of the time series model, as these missing values can interfere with the model learning process and affect the accuracy of the forecast. Generative models have the ability to fill in these missing values.\nIndustrial Sensor Noise: Industrial environments are sus-ceptible to sensor perturbations that introduce high-level noise into time series data. The time series disturbed by noise may not accurately reflect actual changes in equipment operation. Accurate differentiation between true features and noise is crucial for the modeling of deep learning models, hence specific strategies need to be implemented to mitigate the impact of sensor noise on time series applications in industrial. Generative models can generate noise-free sensor data through signal denoising methods.\nIndustrial Privacy Concerns: Industrial data usually faces privacy protection issues. time series data collected by sensors and devices may contain private information or sensitive details about industrial processes. How to balance the need for data-driven insights with the imperative of protecting the privacy and security of industrial data is a great challenge in the industry. Generative models can address such issues by generating data that does not contain sensitive information.\nIn summary, challenges associated with industrial time series generation include limited labeled time series, imbal-anced industrial distribution, missing sensor time series values, industrial sensor noise, and industrial privacy concerns issues. Addressing these challenges is crucial for intelligent industrial applications."}, {"title": "IV. THE PROPOSED DGM-BASED AIGC FRAMEWORK AND THE CURRENT STATE-OF-THE-ART DGMS", "content": "High-quality and adequate industrial time series are insuffi-cient. In industrial systems, changes in equipment, environ-mental factors, and operating conditions, can lead to fluc-tuations in monitoring data. This variability and complexity create challenges for the data collection of complex equipment, leading to a lack of labeled datasets and an imbalanced data distribution. In addition, manufacturing systems may experience unexpected interruptions in sensor failures, network delays, and data transmission, leading to problems of high-level noise and missing values. In decentralized industrial collaborations such as cloud manufacturing, industrial data from different users remain difficult to share due to privacy concerns. These challenges make it difficult for industrial data to meet the demands of deep learning methods for time series applications in the industry.\nThe DGM-baed AIGC framework is proposed for industrial data generation, as illustrated. First, generative models learns the latent representations of industrial time series by modeling dynamic processes and correlations be-tween variables, which have been introduced in Section II and Section III. Second, quality enhancement, sample aug-mentation, and privacy protection are performed to improve the quality and quantity of data, which will be discussed in Section IV. Finally, the generated samples will be applied to various industrial scenarios to address challenges such as sample scarcity in industrial scenarios, as presented in Section V. With the development of generative intelligence, there are many methods are proposed to tackle these issues. These research works can be mainly categorized into five: limited sample augmentation, imbalanced data synthesis, sensor signal imputation, sensor signal denoising, and privacy protection."}, {"title": "A. Limited Sample Augmentation", "content": "Limited sample augmentation involves using sophisticated generative methods to enrich datasets where sample size is insufficient for effective model training. This approach is particularly crucial in scenarios where acquiring additional real-world data is challenging or impractical. By artificially generating realistic and diverse samples, this method enhances the dataset's comprehensiveness, aiding in the development of more accurate and generalized machine learning models.\nIn recent industrial applications, the enhancement of model performance and data processing efficiency through limited sample augmentation has emerged as a significant trend. The bidirectional alignment network combined with VAE [73] effectively addresses the issue of scarce samples in fault diagnosis in thermal power plants, achieving generalization across rare and unseen fault categories. The DA-JITL frame-work [18] incorporates a causality-informed VAE, signifi-cantly enhancing the performance of JITL-based soft sensors. MSGAN [74] merges offline training with online real-time inference, employing an adaptive update strategy and gradi-ent penalty in Wasserstein distance to generate high-quality false anomaly samples to compensate for the issue of data scarcity, thereby improving the accuracy of anomaly detection in industrial robotic sensors. Optimized diffusion models [24] and TimeDDPM [21] play crucial roles respectively in precise defect identification in CFRP structures and dynamic process soft sensor modeling by data augmentation, showcasing the versatility of these DGMs in limited sample augmentation."}, {"title": "B. Imbalanced Data Synthesis", "content": "Imbalanced data synthesis refers to the process of artificially generating data to address the challenge of unequal distribution of classes within a dataset. This technique is pivotal in machine learning and data science, especially when dealing with scenarios where certain types of data (often representing minority classes) are scarce or underrepresented.\nIn the realm of industry, a myriad of innovative approaches are being explored to tackle the prevalent challenge of data im-balance in datasets. Among these, Fan et al. [35] introduces a groundbreaking Variational Autoencoder (VAE)-based imbal-anced data synthesis method, tailored specifically for semicon-ductor fault detection. This method adeptly addresses the crit-ical scarcity of defective wafer samples. Simultaneously, the DRL-GAN model [75] ingeniously amalgamates distributional reinforcement learning with GAN, thereby showcasing re-markable efficiency in generating balanced data distributions. Its application results in substantial improvements in various anomaly detection metrics, highlighting the model's profound impact on the landscape of industrial anomaly detection. Furthermore, a Wasserstein conditional GAN [25] is proposed, which is enhanced with hierarchical feature matching. This novel approach is adept at tackling the long-tail distribution problem, a common quandary in the realm of bearing fault diagnosis. The implementation of this model demonstrates an effective and innovative method for addressing the data imbalance problem."}, {"title": "C. Sensor Signal Imputation", "content": "Sensor signal imputation describes the process of estimating and filling in missing or incomplete data in datasets collected from sensors. The accuracy and completeness of sensor data are essential for reliable analysis and decision-making.\nThe Vector Autoregressive Imputation Method (VAR-IM) [76] utilizes an integrated expectation-minimization and pre-diction error minimization algorithm, significantly improving imputation in multivariate time series, especially in scenarios involving missing signal data. Velasco-Gallego et al. [28] introduce a VAE-based sophisticated imputation method for sensor data from marine machinery. This approach yields a high determination coefficient, indicating its potential to improve the internet of ships and condition-based maintenance domains. Then in the realm of GANs, the Federated Transfer Missing Data Imputation method (FedTMI) [77] utilizes edge-computed GANs and federated transfer learning for heteroge-neous missing data in industrial applications, while the Fine-Tuned Imputation GAN (FIGAN) [78] focuses on quality-related variables in soft sensors, interweaving improved data imputation with pseudo labeling. Other GAN models like the IM-GAN [79], SGAIN [80], Semi-GAN [81], ST-GAIN [82], and SGT-GAIN [83] also demonstrate superior capabil-ities in data imputation, each addressing specific challenges such as complex multivariate time series issues, structural health monitoring, semiconductor equipment fault detection, and advanced manufacturing data imputation. As for diffusion models, the DiffAD model [27] employs a denoising diffusion-based imputation method combined with a multi-scale state space model for time series anomaly detection. It effectively adapts to concentrated anomaly episodes and captures long-term dependencies. Similarly, the GD-GRU model [84] inte-grates a Gaussian diffusion process with a Gated Recurrent Unit for enhancing atmospheric environmental quality data repair."}, {"title": "D. Sensor Signal Denoising", "content": "To enhance the quality and reliability of the information captured by sensors, sensor signal denoising is used to remove noise from sensor data. The primary objective is to filter out these extraneous noises without distorting the actual signal.\nStarting with AR models, the studies [34] and [29] showcase diverse applications of AR in sensor signal denoising. Among these, Liu et al. [29] introduce an advanced approach for in-dustrial robots, integrating wavelet regional correlation thresh-old denoising with autoregressive moving average models. This method significantly enhances signal clarity for acoustic emission data. In the domain of GAN models, a Bayesian nonparametric estimation within a CycleGAN architecture [30] is constructed for online sensor signal denoising in milling processes. Furthermore, the Att-DCDN is presented for seismic data denoising, combining an encoder-decoder structure within a GAN. This approach integrates synthetic and field seismic data, leveraging attribute-based constraints for noise attenuation. While Min et al. introduce a novel GAN-based approach [85] for seismic data denoising, combining a pre-trained deep denoising autoencoder with transfer learning to enhance training on limited field data."}, {"title": "E. Privacy Protection", "content": "In the context of the industrial sector and generative models, privacy protection refers to safeguarding sensitive and propri-etary information during the deployment and application of these advanced computational models.\nBeginning with AR models, Palekar et al. [31] present a privacy-preserving authentication model for industrial devices. This model employs the Autoregressive Poor and Rich Opti-mization (APRO)-based secret key generation and Box-Cox transformation for encryption, achieving secure data exchange with minimal computation time and enhanced encryption qual-ity. The study [32] proposes a privacy-preserving framework for smart power networks, integrating enhanced-proof-of-work blockchain and VAE to safeguard data privacy and detect anomalies, while Almaiah et al. [105] introduces a blockchain-based deep learning framework for industrial, employing VAES with Bidirectional Long Short-Term Memory networks for robust privacy protection. A KingFisher framework [103] uses VAEs for scrutinizing network interactions and safeguard-ing privacy. Lastly, the privacy-enhanced federated learning mechanism for the Internet of Medical Things [110], utilizes VAEs with differential privacy noise. As for GAN models, Chen et al. [33] discuss a differentially private GAN model to preserve the privacy of machine operation data. Fed-LSGAN [106] integrates federated learning with the least squares GANs for privacy-aware renewable energy scenario generation. Furthermore, the study [104] proposes a solution for debugging data issues in privacy-sensitive and federated learning contexts using differentially private federated GANs. A deep differential privacy data protection algorithm for industrial networks [108] is constructed. Finally, ProcessGAN [107] innovatively generates confidential synthetic process data, especially beneficial for small medical datasets. Besides, the differentially private DDPM [109] is a novel approach integrating differential privacy into diffusion models to ensure data privacy while generating high-quality synthetic data."}, {"title": "V. TYPICAL INDUSTRIAL APPLICATIONS OF DGMS", "content": "After discussing the main tasks of DGMs in industry, in this section, we mainly introduce the applications in industrial scenarios."}, {"title": "A. Anomaly Detection", "content": "In the industrial field, anomaly detection refers to the identification of irregular or abnormal patterns within in-dustrial data, which could signify issues such as equipment failure, production inefficiencies, or security breaches. It is crucial for maintaining operational efficiency, ensuring safety, and minimizing downtime in industrial settings. Advanced analytical techniques, including machine learning and deep learning, are employed for detecting these anomalies by ana-lyzing sensor data, network traffic, or production metrics. For instance, the AE-NAR model [34] is specifically designed for wind turbine pitch-bearing anomaly detection. It leverages an autoencoder for global feature extraction and nonlinear au-toregression with augmented Lagrangian for noise reduction, thereby significantly enhancing detection accuracy. Similarly, the adaptive weighted loss VAE [97] is an unsupervised model presented in industrial production for anomaly detection. This model achieves superior accuracy over traditional VAE by adaptively adjusting loss function weights during training.\nThe conditional GAN [87] with its unique encoder-decoder structure adeptly processes high-frequency time series data for anomaly detection in smart manufacturing. These innovative approaches, along with other GANs and Diffusion models [75] [27] [74], demonstrate the evolving landscape of anomaly detection in industrial. They emphasize the significance of complex computational models in tackling the intricate chal-lenges of anomaly detection within industrial contexts."}, {"title": "B. Predictive Maintenance", "content": "Predictive maintenance is a strategic approach that employs data analytics, machine learning, and advanced computational models to predict and prevent equipment failures before they occur. This approach aims to optimize maintenance opera-tions by identifying potential issues early, thereby reducing unplanned downtime and extending equipment life. A GAN-based data augmentation method [36] is illustrated to address the scarcity of fault condition data in Industry 4.0. This method enhances the training and performance of machine learning models, which are critical for accurately predicting equipment failures. The study [96] further expands on this by introducing a novel approach using Gaussian process regression and GAN for handling missing data, which is vital for making accurate predictions in quality assurance and maintenance scheduling. Additionally, Xiong et al. [91] present the Controlled Physics-Informed GAN (CPI-GAN), a hybrid framework that synthe-sizes degradation trajectories for enhancing Remaining Useful Life (RUL) predictions. This approach ensures accurate pre-dictions that adhere to fundamental physics, thereby improving maintenance scheduling. In addition to the aforementioned approaches, a Variational Autoencoders (VAEs) method [28] further diversifies the computational strategies employed in the realm of predictive maintenance"}, {"title": "C. Fault Diagnosis", "content": "Fault diagnosis refers to the process of identifying, analyz-ing, and rectifying faults in industrial equipment and systems. It leverages advanced data analytics, machine learning algo-rithms, and sensor data to accurately and promptly diagnose equipment failures. Zhao et al. [17] introduces the NCVAE-AFL framework for the bearing-rotor system, addressing class imbalance and long-tail distribution issues in fault diagnosis. A data-augmentation approach using time-varying autoregressive models [90] is constructed specifically for fault diagnosis in non-stationary multivariate time series, demonstrating en-hanced performance in real-world scenarios. Additionally, the studies [94] [37] explore the use of Wasserstein Generative Adversarial Networks (WGAN) for data enhancement, target-ing the improvement of fault diagnosis accuracy by addressing the scarcity of mechanical fault samples. A GAN-GDA model [95], combining GAN with gaussian discriminant analysis for data augmentation, aims at enhancing the effectiveness of fault diagnosis in industrial settings. Together with a federated learning method [73], and an AR-based method [111], these studies exhibit the diversity of advanced computational tech-niques in fault diagnosis. They underscore the crucial role of machine learning and data analytics in effectively diagnosing faults in the industrial environment."}, {"title": "D. Supply Chain Optimization and Production Scheduling", "content": "Supply chain optimization and production scheduling are key concepts in operations management, focused on enhancing the efficiency and effectiveness of the entire supply chain through rational resource allocation, coordination of produc-tion activities, and process improvement. Recent advancements in this domain have seen the introduction of innovative deep-learning models, particularly leveraging GANs and VAEs. A Conditional Variational Autoencoder (CVAE) [113] based model has been proposed to integrate production scheduling in the automotive component manufacturing sector. This model generates operation-specific health indicators from time series data, aiding in quantifying machine degradation and enhancing production efficiency. The study [89] aims to optimize supply chain management by leveraging deep generative models and time series analysis. This approach is enhanced through the generation of creative alternatives using GANs, thereby aiming to increase the success rate of product introductions. A hy-brid model combining TimeGAN and CNN-LSTM networks [93] addresses short-term load forecasting in commercial buildings, improving forecasting accuracy and aiding energy management in supply chains. Finally, the Seq2Seq-WGAN model [92] employs a sequence-to-sequence-wasserstein GAN approach to better capture temporal dependencies in cement production processes. This model generates extensive f-CaO label data and Seq2Seq for managing unequal length input-output sequences, leading to more accurate predictions and enhancing cement industry production scheduling."}, {"title": "VI. LARGE GENERATIVE MODELS FOR INDUSTRIAL TIME SERIES", "content": "In the industrial domain, generative models have made significant progress but their usefulness is limited. In com-plex and open industrial environments, generative models may face several challenges, including limited generalization capabilities, multi-tasking limitations, and cognitive limita-tions. Specifically, although generative models perform well in known scenarios, they struggle with the complexity of real industrial scenarios due to insufficient generalization capabilities. In addition, current generative models are usually designed for a single task. While industrial equipment usually contains hundreds of core components, developing a corre-sponding generative model for each core component separately is difficult to achieve. Finally, generative models have limited cognitive ability to understand the nature of industrial data, which causes them to potentially generate incomprehensible and erroneous results.\nTo address these issues, researchers have turned to large generative models such as ChatGPT [11] and DALLE 2 [10]. These models exhibit superior data generation, zero-sample generalization, and multitasking capabilities. The gen-erative AI research is shifting from the traditional paradigm of specific-domain, specific-task, and specialized generation ability to the new paradigm of multi-domain, multi-task, and generalized generation ability. However, there is still no definitive answer on how to construct the large generative model in industrial time series. To promote the research and application of large generative models in industry, we explores how to build large generative models suitable for industrial applications, as illustrated."}, {"title": "A. Large-Scale Industrial Dataset", "content": "Large-scale datasets are the cornerstone to drive large generative model research. Unlike the NLP or the CV, the industrial time series are generally sensor readings of time series data, such as pressure, temperature, speed, vibration signals, and power signals. There are a number of open-source industrial datasets of various sizes and domains, such as bearing failure dataset [114], wind turbine dataset [115], three-phase motor failure dataset [116], turbofan engine degradation dataset [117], gearbox failure dataset [118], etc. Specifically, the CWRU [119] dataset is a classical fault diagnosis dataset that records bearing vibration data under different operating conditions, including parameters such as different rotational speeds, loads, and operating times. It consists of four dif-ferent failure modes, including inner ring failure, outer ring failure, and rolling element failure. The CMAPSS dataset [120] records the turbofan engine degradation monitoring data, which contains six operating conditions and 2 failure states. Further, the recently open-sourced N-CMAPSS dataset [117] contains millions of aircraft engine run-to-failure degradation trajectories, including 128 units and 7 different failure modes. It can be seen that the industry has rich data resources, which provide a solid foundation for building large generative models of the industrial time series.\nAlthough LGMs have accomplished impressive results in the NLP and CV, industrial data involves various types of multi-source devices and sensors with complex device infor-mation. Therefore, data processing methods, feature extraction methods, and self-supervised representation learning methods need to be designed according to the unique characteristics of industrial data in order to make the LGMs applicable in this field. There has been some research on such algorithms, for example, an adaptive sensor weighting method [121] based on time-varying Gaussian encoders has been proposed to construct representative features of industrial time series. The article [122] proposes an LSTM-DeepFM model for solving industrial soft sensor problems, which utilizes an LSTM-autoencoder in the pre-training phase to enhance the feature construction process."}, {"title": "B. LGMs Architecture Tailored for Industrial Characteristics", "content": "Currently, the most representative LGMs include GPT-4 [123] and DALLE 2 [10], of which the linguistic generative model GPT-4 adopts Transformer as its core architecture, while the multimodal generative model DALL\u00b7E uses the Diffusion Model. Transformer has powerful sequence model-ing capabilities, and its self-attention mechanism allows it to process time series data in parallel while efficiently capturing important feature information. Transformer has also made notable achievements in industrial time series, including ap-plications such as remaining life prediction of aircraft engines [112], fault diagnosis of bearings [124], and prediction of degradation processes in batteries [125]. The basic idea of the Diffusion Model [19] is a forward diffusion process to systematically perturb the data distribution and then recover the data distribution through a reverse diffusion process. It is powerful in handling probability distributions and generating samples and is gradually becoming a new paradigm for data generation in industrial time series.\nOne of the keys to deal with industrial time series data is efficient modeling of complex systems and extracting time series dependencies. For LGMs Architecture, future research could focus on optimizing attention mechanisms, and decom-posing and transforming time series data to improve represen-tation and modeling capabilities. These methods include Fast Fourier Transform (FFT) [126], [127], optimising attention mechanisms [121], [128], wavelet transform [129], [130], etc. The computational efficiency of the model also needs to be considered in the design of large generative architectures to accommodate the requirement for real-time performance and efficiency in industrial environment. GT-MRNet [128] is a lightweight time series reduction transformer that reduces the computational cost and the number of parameters by adaptively eliminating redundant time steps."}, {"title": "C. Self-supervised Training of Industrial Time Series", "content": "Self-supervised learning is an approach to unsupervised learning that allows a model to learn its own representation from input data without external labels. As mentioned earlier, when building industrial LGMs, a large amount of data is usually required to train the model. However, not all of the data is well-labeled. Self-supervised learning enables models to extract valuable intrinsic features from unlabeled data, thus making more efficient use of large-scale datasets. This approach has become one of the core algorithms for building large models. Currently, there are some self-supervised learn-ing methods that have been proven successful in processing industrial data. The article [122] proposes an LSTM-DeepFM model, which utilizes LSTM-autoencoder for self-supervised learning to enhance the feature construction process.\nThe industrial environment involves a variety of hetero-geneous devices that may provide different types of data, including temperature, pressure, flow rate, etc. One challenge of self-supervised learning is to process heterogeneous device information and integrate it into a consistent representation. This enables the model to understand data from different devices and achieve pattern generalization. Therefore, domain adaptation and transfer learning approaches can be employed to process heterogeneous information from multiple sources in order to learn domain-independent generalized feature rep-resentations. For example, Ren et al. [131] proposed cross-domain learning methods of multi-source black-box data. It achieved cross-domain common mechanism learning in privacy and security scenarios without touching the source domain task data. The paper [132] proposed a supervised gen-eralization method for industrial time series analysis models to extract information from a variety of industrial scenarios."}, {"title": "D. Fine-tuning for Industrial Downstream Tasks", "content": "As described in Sections III and IV, industrial generative models are usually applied to a variety of tasks, including time series imputation, generation, denoising, and so on. In addition, generative models are used in a variety of appli-cation areas, including fault diagnosis, health management, and anomaly detection. At present, most of the generative models target a single task or a single domain. In multi-task and multi-domain scenarios, designing generative models for each task or application domain and training them separately requires a lot of resources. Some approaches have explored the multi-task learning to enhance the model performance. Liu et al. [133] proposed a multi-task one-dimensional convolutional neural network, which combines the primary bearing fault diagnosis task (FDT) and auxiliary tasks to realize shared feature learning, thus improving the bearing fault diagnosis performance.\nIndustrial generative models still need to make efforts in multi-task adaptation, and the main fine-tuning methods for large models are as follows: (1) Task-oriented model fine-tuning [134]: this approach typically builds on a pre-trained model and uses task-specific data for further training. This can be achieved by tuning the model parameters, adding new lay-ers, or modifying the loss function. Through this fine-tuning, the model will learn task-specific fine-grained features and representations. (2) Model fine-tuning with prompt learning [135]: Designing templates that fit the upstream pre-training task taps the potential of the pre-trained model, allowing the model to perform the downstream task better with as little labeled data as possible. (3) Model fine-tuning based on adapter networks [136]: adapter networks are lightweight network structures that can be inserted into pre-trained models for learning task-specific representations. Adapters can add additional task-related information to the model without de-stroying the knowledge of the pre-trained model."}, {"title": "VII. CHALLENGES, FUTURE DIRECTIONS, AND CONCLUSION", "content": "A. Generative Models for Low-quality and Mixed-type Data\nIndustrial time series data involves a wide variety of sensors and devices, and therefore a wide variety of data types. It includes continuous data, such as temperature and pressure readings from sensors. It also includes discrete data, such as equipment status and switching information. This diversity makes the types of industrial time series data complex and variable, requiring generative models with the ability to handle different types of data. At the same time, industrial scenes often have quality issues, such as noise and outliers due to equipment malfunctions. This can affect the modeling of the data and lead to a reduction in the fidelity of the generated data.\nTo address data quality issues, the collected data can be pre-processed before being input into the model. In addition, more robust generative models can be developed to effectively deal with quality issues of noise and outliers in the industry. For mixed data types, the generation of mixed data can be facilitated by co-representation. For instance, Lee et al. [137] proposed a co-evolving contrastive generation framework to separately model continuous and discrete variables of tabular data. During training, the two diffusion models evolve together by exchanging conditions with each other. In addition, a general framework can be developed to include the generation, estimation, and prediction of mixed-type time series models.\nB. Generative Models with Long Temporal Expressive Power\nIndustrial time series generative models face critical chal-lenges in dealing with long-term temporal dependencies. in-dustrial time series data typically have strong temporal de-pendencies where current values are influenced by previous values, and models need to be able to effectively capture these long-term dependencies in order to generate data with a reasonable temporal structure.\nFuture research could focus on exploring more advanced sequence modeling methods to handle long-term temporal dependencies more effectively. Advanced recurrent neural net-work (RNN) structure [138] is one such avenue. In addition, the attention mechanism [121] and state space models [139] are also effective methods. By capturing long-term temporal dependencies, the model can generate time series data more consistent with temporal correlations.\nC. Interpretable/Reliable/Credible Generative Models\nDeep generative models face the challenges of lack of inter-pretability and unreliability of the generated data. Reliability requires that the data generated accurately reflects the true behavior of the industrial system and ensures that the model is producing trustworthy results. Interpretability requires that the generated data is not only accurate but also needs to be able to be interpreted and understood to meet the needs of domain experts in the modeling decision-making process.\nFuture research is directed towards generative models that incorporate physical information to meet the high demand for reliability and interpretability in industry. Xiong et al. [91] presents the Controlled Physics-Informed Generative Adversarial Network (CPI-GAN), a hybrid framework that synthesizes degradation trajectories for enhancing remaining useful life predictions. This integrated approach is expected to improve the reliability and interpretability of the generated data and provide a more credible database for industrial applications. By incorporating physical information into the generative models, it can be ensured that the generated data is not only based on the results of statistical learning but also takes into account the real mechanisms within the system.\nD. Industrial Multimodal Generative Models\nIndustrial systems often contain multiple sensors and data sources that may have different modalities such as text, numer-ical values, etc. The challenge is that models need to be able to effectively handle such multi-modal data. The challenge is that the model needs to be able to efficiently handle such multi-modal data, where the data structure and feature representation of each modality may differ significantly. The data generated by different sensors may be heterogeneous, increasing the complexity of model fusion and correlation.\nFuture research directions should focus on the development of generative models applicable to multimodal data. This may involve an in-depth study of representation learning methods for multimodal data to better capture the correlations among different sensors, which will help the models to more fully simulate the behavior and state of industrial systems. In addition, by joining modalities from different data not only can performance be enhanced but possible tasks can be explored. For instance, AnomalyGPT [140] integrates the information of Industrial Anomaly Detection (IAD) task to the Large Vision-Language Models (LVLMs) to support the multi-dialogues. Moreover, it doesn't need manual threshold adjustment, which can directly present the anomalies. Stable diffusion [141] gen-erates stunning images from text prompts alone, dramatically reducing the amount of manual labor required. Multimodal time series generation can be performed by combining text and signals.\nE. Industrial Generalized Generative Models\nIndustrial environments can have a diversity of processes, equipment configurations, and operating conditions, leading to the need for models that can adapt and maintain high per-formance in different industrial environments. Trained DGMS often lack generalization capabilities as they can only generate samples that are consistent with the data in the training dataset, but can not generate new data samples.\nTo enable generative models to learn richer, generic feature representations, training large generative models on large-scale industrial time series datasets is a promising approach. There are a number of time series base models that take advantage of the powerful representational capabilities of large language models to enhance their time series modeling capabilities, such as One fits all [142], LLM4TS [143]. For example, One fits all [142] uses frozen pre-trained language models to attain state-of-the-art performance across a variety of time series tasks. Integrating large generative models like ChatGPT [11] or specialized time series models further amplifies the adaptability of the generative model. This combination not only improves performance but also enables the generation of data across different domains.\nF. Generative Models for Smart Manufacturing\nGenerative modeling can generate a variety of industrial design prototypes to help engineers explore diverse device con-cepts and accelerate product design and innovation, including code generation [144]\u2013[146], production process generation [147]\u2013[149], and so on. The advantages of DGMs include: (1) Generative design models can produce diverse designs and stimulate the creativity of designers. (2) Automated code generation can reduce the cost of generation and design, especially in mass production. For instance, the paper [146] proposed a generative AI methodology that automatically translates nuclear power plant safety operation flowcharts into executable code to improve the efficiency and reduce the cognitive burden of operators using flowcharts.\nCurrently, LGMs and DGMs have received much attention due to their ability to understand, generate, and create data. For this reason, generative models have made significant progress and show great potential in industrial time series. In conclusion, this review provides a detailed overview of the generative models in industrial. First, we analyze the definition, characteristics, and data scarcity challenges within industrial time series generation. Then, we propose a DGM-based AIGC framework for industrial data generation and present a multi-perspective categorization of industrial DGMs. We further discuss the application of DGMs in industrial time series. In addition, we explore the construction of LGMs in the industrial domain. Finally, we conclude with challenges and future directions to enable the development of generative models in industrial applications."}]}