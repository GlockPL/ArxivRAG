{"title": "AIGC for Industrial Time Series: From Deep Generative Models to Large Generative Models", "authors": ["Lei Ren", "Haiteng Wang", "Yang Tang", "Chunhua Yang"], "abstract": "With the remarkable success of generative models like ChatGPT, Artificial Intelligence Generated Content (AIGC) is undergoing explosive development. Not limited to text and images, generative models can generate industrial time series data, addressing challenges such as the difficulty of data collection and data annotation. Due to their outstanding generation ability, they have been widely used in Internet of Things, metaverse, and cyber-physical-social systems to enhance the efficiency of industrial production. In this paper, we present a comprehensive overview of generative models for industrial time series from deep generative models (DGMs) to large generative models (LGMs). First, a DGM-based AIGC framework is proposed for industrial time series generation. Within this framework, we survey advanced industrial DGMs and present a multi-perspective categorization. Furthermore, we systematically analyze the critical technologies required to construct industrial LGMs from four aspects: large-scale industrial dataset, LGMs architecture for complex industrial characteristics, self-supervised training for industrial time series, and fine-tuning of industrial downstream tasks. Finally, we conclude the challenges and future directions to enable the development of generative models in industry.", "sections": [{"title": "I. INTRODUCTION", "content": "Industry 5.0 incorporates cyber-physical-social elements into manufacturing, emphasizing digital-physical interaction and human-machine collaboration, effectively connecting the Internet of devices, things, and people. Currently, with the development of Artificial Intelligence Generated Content (AIGC), metaverse and digital twins technologies, industrial big data can be used to create digital manufacturing and industrial processes [1]. It facilitates significant growth in productivity, efficiency, and effectiveness in Industry 5.0 and cyber-physical-social systems (CPSS) [2].\nIndustrial time series data, including equipment sensor data, production-line operation data, and system status data, is an important foundation for a broad range of industrial intelligence applications [3]. Currently, more and more industrial intelligence applications, such as intelligent control systems [4], [5], predictive maintenance [6], [7], and fault diagnosis [8], [9], rely on mining and analyzing industrial time series. In particular, the success of industrial temporal deep learning methods is also dependent on learning and extracting features from industrial time series data.\nThe diversity, quality, and quantity of industrial time series are critical for improving system performance and facilitating industrial intelligence applications. Unfortunately, in industrial scenarios, obtaining such data can be challenging due to the difficulty of industrial data collection, data annotation, and privacy concerns. For example, data collection for industrial engine equipment requires significant labor costs. Fatigue testing may even damage industrial equipment. Moreover, data silos and sensitive information about industrial data further hinder the creation of rich and diverse datasets. To address these challenges, data generation through generative models provides a promising solution.\nIn the past two years, large generative models, such as DALLE 2 [10] and ChatGPT [11] have ushered in a new era of AIGC. These models can generate new data with similar statistical characteristics by learning the distribution of existing data. This enable significant progress in the fields of image synthesis, text generation, and molecular design, among other tasks. Microsoft, OpenAI, and others have used synthetic data from generative models to train intelligent models. This shows the powerful capability of generative models to deal with challenges related to data annotation and collection.\nGenerative models can be classified into large generative models (LGMs) and deep generative models (DGMs). DGMS for industrial data generation, include generative adversarial networks (GAN) [12]\u2013[15], variational autoencoders (VAE) [16]\u2013[18], and diffusion models [19]\u2013[21]. Unlike traditional data extension methods such as time-shifting, downsampling, and panning, DGMs are able to learn and model complex distributions of complex data to generate diverse new samples.\nIn the industrial field, DGMs have become a research hotspot and have been widely applied. Its core tasks can be roughly categorized into five: limited sample augmentation [22]\u2013[24], imbalanced data synthesis [25], [26], sensor signal imputation [27], [28], sensor signal denoising [29], [30], privacy protection [31]\u2013[33]. DGMs help to alleviate issues such as data privacy and shortage. By training generative models, companies can synthesize data without disclosing sensitive information, thus protecting data privacy. In addition, data generation is able to expand limited real datasets, reduce the cost of data acquisition, and improve the generality of models. Generating samples with less variety solves the problem of data imbalance. Recent research has exploited the ability of DGMs to generate real data samples for industrial applications such as anomaly detection [34], [35], multi-classification fault instance generation (to address data imbalance issues) [36], [37], trust boundary protection [38], [39], and platform monitoring [40], [41].\nSince generative models have been applied in various fields and have shown promising results in industrial time series applications, there have been recent studies analyzing and surveying DGMs. Comprehensive reviews, such as the survey [42], provide profound insights into diffusion models across domains such as image synthesis, time series prediction, and natural language generation. Additionally, domain-specific reviews like [43] presented a review about the application of DGMs in the industry. Similarly, with the rapid development of diffusion models, there are some reviews on diffusion models in specific fields such as biological signals [44] or computer vision [45]. Due to the significance of time series in industry, literature [3] has surveyed time series data modeling issues.\nIn addition to DGMs with specific-domain, specific-task, and specified generation ability, LGMs with multi-domain, multi-task, and generalized generation ability have made significant progress, as shown in Fig. 1. Some recent studies [46], [47] have investigated the LGMs in general applications.\nAlthough these existing works review deep generative mod-els, GAN-based methods, and deep learning-based industrial applications, we have identified a gap in the literature regard-ing the classification and application of generative models in industrial time series. As time series data stands as the most prevalent and essential data type in industry, its generation holds significant importance. Moreover, there is still a lack of sufficient research on how to construct LGMs in industry. Therefore, this survey focuses on a comprehensive review of DGMs in industrial time series and also analyzes the critical technologies required to construct industrial LGMs in the industrial field. The contribution of the paper can be summarized as follows:\n1) This paper provides a comprehensive overview of the generative models in the industry, from the current state-of-the-art DGMs to the future promising LGMs.\n2) A DGM-based AIGC framework is proposed for in-dustrial time series generation. Within the framework, this paper surveys advanced industrial DGMs and presents a multi-perspective categorization.\n3) This paper proposes a systematic analysis of the critical technologies required to construct industrial LGMs that are suitable for industrial time series data.\nThe organization of the rest of the sections is as follows. In Section II, the commonly used DGMs are introduced. In Section III, the definition, characteristics, and requirements of industrial time series generation are presented. In Section IV, the proposed DGM-based AIGC framework and the current state-of-the-art DGMs are introduced. In Section V, we present the typical industrial applications of DGMs. In Section VI, we analyze how to construct LGMs for industrial time series. Finally, Section VII concludes the challenges and future di-rections of generative models."}, {"title": "II. DEEP GENERATIVE MODELS", "content": "In this section, we introduce general DGMs architectures and compare their characteristics and applications.\nAn autoregressive model (AR) [50] is a particular regression model, wherein a value from the time series is subjected to a regression on previous values. It predicts future values in a sequence based on previous values. The autoregressive model of order p (often termed AR(p) model) is defined as:\n$X_t = c + \\phi_1X_{t-1} + \\phi_2X_{t-2}+ ... + \\phi_pX_{t-p} + E_t$   (1)\nwhere $X_t$ is the value of the series at time t, c is a con-stant, $\\phi_1, \\phi_2,..., \\phi_p$ are the parameters of the model, $e_t$ is white noise. The likelihood for an AR(p) model given data {$X_1,X_2,...,X_T$} is:\n$L(\\phi; x_1, x_2,...,x_t) = \\prod_{t=p+1}^T f(X_t|X_{t-1}, X_{t-2},...,X_{t-p})$  (2)\nwhere f is the conditional density of $x_t$ given past values. For many autoregressive models, especially when the noise $E_t$ is Gaussian, the negative log-likelihood serves as loss function:\n$L = - \\sum_{t=p+1}^T log f(x_t|x_{t-1}, X_{t-2},..., X_{t-p})$  (3)\nMinimizing this loss function gives the maximum likelihood estimates of the parameters.\nAutoregressive models are commonly used in industrial forecasting [51] [52]. For example, Wang et al. [53] proposed a model, combining non-linear autoregressive neural network"}, {"title": "B. Variational Autoencoder (VAE)", "content": "VAE is a generative model that extends the concept of the traditional auto-encoder (AE) [54] by introducing a probabilistic approach to describe the data generation process. It operates by first encoding an input into a distribution over a latent space and then sampling from this distribution to generate new data. Unlike a standard AE that simply compresses and reconstructs data, VAEs [16] are designed to produce novel samples. It is achieved by constructing a loss function that not only minimizes reconstruction error but also includes a term that regularizes the latent variable distribution, often encouraging it to approximate a standard normal distribution.\nGiven x as the observed data, z as the latent variable, and the following definitions:\n\u2022 p(z): Prior distribution of the latent variable, typically chosen as a standard normal distribution $p(z) = N(z; 0, I)$.\n\u2022 $q_{\\phi}(z|x)$: Output of the encoder, i.e., the posterior distribution of the latent variable given the data with the weight $\\phi$ of the encoder E.\n\u2022 $p_{\\theta}(x|z)$: Data generative model given the latent variable with the weight $\\theta$ of the decoder D.\nThe VAE loss is defined as the sum of the reconstruction loss and the regularization loss:\n$L_{VAE} = Reconstruction loss + Regularization loss$ (4)\nMore specifically, the VAE loss is given by:\n$L_{VAE}(\\phi, \\theta) = -E_{qq(z|x)} [log p_{\\theta}(x|z)] + D_{KL}(q_{\\phi}(z|x)||p(z))$ (5)\nVAE, like other deep learning techniques, has received a lot of attention from academia since its inception, but its application in industry may take some time as industrial applications usually require the maturity and stability of the technology. VAE has been successively applied to the industry for some anomaly detection [55] [56] [57], etc."}, {"title": "C. Generative Adversarial Network (GAN)", "content": "The Generative Adversarial Networks (GANs) were first proposed by Goodfellow et al. [15] in 2014. It is an unsu-pervised learning algorithm that involves the interaction of two independent neural networks against each other to achieve learning, which can be characterized as a zero-sum game. GANs do not require prior knowledge and do not require assumptions that the sample obeys a certain distribution.\nA typical generative adversarial network consists of two parts: a discriminator D and a generator G. The generator aims to generate synthetic data samples, whereas the discriminator strives to differentiate between actual and synthetic data. Hence, the objective function of GAN comprises maximizing the loss of the discriminator and minimizing the loss of the generator. Let x represent real data, random noise z is sampled from a prior distribution $p_z$, G(z) is the synthetic sample generated by the generator from the noise z, D(x) is the probability given by the discriminator that data x is real. Then we have:\nGenerator Loss :\n$L_G = -E_{z~p_z (z)} [log D(G(z))]$   (6)\nDiscriminator Loss:\n$L_D = -E_{x~P_{data}(x)} [log D(x)] - E_{z~p_z(z)}[log(1 \u2013 D(G(z)))]$  (7)\nIn the evolution of GANs within industrial applications, key milestones are marked by their use in various domains over the years. In 2015, GANs were first leveraged for data augmentation purposes [58]. The scope of their application expanded in 2017 when they were utilized for fault detection [59]. A significant advancement came in 2018 when Zhao et al. [60] pioneered the application of GANs in defect detection, a concept further elaborated in [61]. This progression under-scores the increasing versatility and significance of GANs in enhancing industrial processes through innovative AI-driven solutions."}, {"title": "D. Diffusion Model", "content": "Diffusion models, a group of generative models inspired by the physical process of diffusion, have become increas-ingly prominent in the field of machine learning recently. The Denoising Diffusion Probabilistic Model (DDPM) [19] is beginning to receive widespread academic attention. It is noteworthy to mention that before this study, Song Yang et al. [62] proposed a score-based diffusion model, and these two models were subsequently harmonized in the SDE study [63].\nThe core idea of Diffusion Models is to gradually transform the distribution of the data into a known simple distribution (e.g., a Gaussian distribution), and then sample from the simple distribution and reverse transform it back into the distribution of the data. And we implement both forward and backward processes through two Markov chains [64].\nGiven a distribution of data p(x), a diffusion process can be defined at each time step t, and the data x is perturbed to $x_t$. This process can be characterized as follows:\n$x_t = \\sqrt{1 - \\beta_t}x_{t-1} + \\sqrt{\\beta_t}e_t$  (9)\nwhere $e_t ~ N(0, I)$ is a Gaussian noise and $\\beta_t$ is a predefined noise factor.\nA denoising function $q_e$ can be trained to reconstruct $x_{t-1}$ from $x_t$. Given a sample $x_t$ and its corresponding noise $e_t$, the output of the denoising function is a conditional distribution $q_{\\theta}(x_{t-1}|X_t)$.\nThe loss function of DDPM is based on the difference between the output of the denoising function and the true $x_{t-1}$. It can be expressed as:\n$L(\\theta) = E_{x_t~p(x_t),e_t~N(0,1)} [- log q_{\\theta}(x_{t-1}|X_t)]$  (10)\nwhere the expectation is taken overall time steps and all possible $x_t$.\nDiffusion models, initially emerging within computer vision [65], have expanded their utility to encompass sequential modeling [66]\u2013[68], ventured into the audio domain [69], and bridged disciplines in AI for science [70]. These models have been adeptly integrated into various industrial sectors, tackling complex challenges such as the imputation of sensor data [71] and the nuanced task of anomaly detection [72]."}, {"title": "III. THE DEFINITION, CHARACTERISTIC, AND DEMAND OF INDUSTRIAL TIME SERIES GENERATION", "content": "In this section, we will present three points: 1) what is industrial time series generation 2) what are the characteristics of industrial time series generation 3) what are the current challenges of industrial time series acquisition, that evoke the demand for industrial generative models.\nThe given time series T with N (N > 1) individual series of length L is represented as a matrix, i.e., $T = (s_1,...,s_N)^T$, where each individual series $s_i$ can be expressed as a L-dimensional vector, i.e., $s_i = (X_{i,1},...,x_{i,L})$, and each $X_{ij}$ corresponds to a single time point $t_j$ of $s_i$. We denote $p(s_1,...,s_N)$ as the real distribution of a given time series T. In the context of industrial time series generation, L-dimensional vector variables represent Lindustrial sensors. The goal of time series Generation is to create a synthetic time series $T_{gen} = (s_{gen,1},..., s_{gen,N})$ such that its distribution $q(s_{gen,1},..., s_{gen,N})$ is similar to $p(s_1,...,s_n)$, and $T_{gen}$ and T exhibit consistent statistical properties and patterns.\nGenerative tasks in the industrial domain differ significantly from those in the internet domain, primarily in terms of data types and application scenarios. In the internet domain, generative tasks typically involve multimedia data such as text, images, and videos, with diverse applications like text-to-image and image-to-text generation. In contrast, industrial data is more diverse, including sensor data, text, and images, with the most critical being sensor time series data that records detailed operational states of equipment. Industrial time series generation has distinctive industrial characteristics.\n1) Industrial time series have complex time series dependencies and patterns. As industrial processes become automated and industrial systems become more and more complex, it is no longer sufficient to rely on univariate time series data alone to provide a comprehensive and effective representation of industrial processes. As a result, multiple sensors are often utilized to monitor the entire industrial process. Generating multivariate time series is particularly challenging, as it requires dealing with correlations and temporal dependencies between patterns as well as variables.\n2) Dynamic variability inherent in industrial processes. Industrial processes typically exhibit a high degree of dynamic variability and are susceptible to conditions including pressure, temperature, and humidity fluctuations. These environmental variations lead to fluctuations in outputs, resulting in data offsets and domain offsets. Therefore, to maintain accurate industrial data generation, these dynamic patterns must be captured.\n3) Industrial scenarios require high reliability of time series. Sensor time series is the most common data type in the industry, so the subject of the generation task should be sensor time series data. In addition, industrial scenarios have strict requirements on the reliability of the data and the need to accurately present complex equipment operating states. Therefore, the generated time series data in the industrial scenarios must have authenticity and reliability to ensure accurate simulation of real industrial scenarios."}, {"title": "C. Challenges in Industrial Time Series Acquisition", "content": "There are a number of challenges in industrial time series acquisition that need to be addressed by generative models.\nLimited Labeled Time Series: Collecting and testing data from complex industrial equipment such as aircraft engines is extremely difficult. This lead to the relatively small size of industrial well-labeled datasets. However, deep learning relies heavily on extensive labeled data for effective supervised learning, making it challenging to train models when labeled examples of industrial data are scarce. The amount of data can be augmented using generative models.\nImbalanced Industrial Data Distribution: The problem of imbalanced industrial data distribution. In manufacturing systems, failures and anomalous events in industrial systems typically occur less frequently than in normal operating states, and there may be an insufficient number of instances of industrial equipment failures. This makes it difficult for deep learning models to accurately and efficiently capture and generalize patterns associated with these less representative states. Employing generative models enables the generation of a smaller number of samples, such as fault samples, resulting in a dataset with a balanced distribution.\nMissing Sensor Time Series Values: Missing data values are a common challenge in industrial time series applications. Unexpected interruptions in sensor failures and network delays can cause missing time series data. Managing and dealing with missing data is critical to maintaining the integrity of the time series model, as these missing values can interfere with the model learning process and affect the accuracy of the forecast. Generative models have the ability to fill in these missing values.\nIndustrial Sensor Noise: Industrial environments are sus-ceptible to sensor perturbations that introduce high-level noise into time series data. The time series disturbed by noise may not accurately reflect actual changes in equipment operation. Accurate differentiation between true features and noise is crucial for the modeling of deep learning models, hence specific strategies need to be implemented to mitigate the impact of sensor noise on time series applications in industrial. Generative models can generate noise-free sensor data through signal denoising methods.\nIndustrial Privacy Concerns: Industrial data usually faces privacy protection issues. time series data collected by sensors and devices may contain private information or sensitive details about industrial processes. How to balance the need for data-driven insights with the imperative of protecting the privacy and security of industrial data is a great challenge in the industry. Generative models can address such issues by generating data that does not contain sensitive information.\nIn summary, challenges associated with industrial time series generation include limited labeled time series, imbalanced industrial distribution, missing sensor time series values, industrial sensor noise, and industrial privacy concerns issues.\nAddressing these challenges is crucial for intelligent industrial applications."}, {"title": "IV. THE PROPOSED DGM-BASED AIGC FRAMEWORK AND THE CURRENT STATE-OF-THE-ART DGMS", "content": "High-quality and adequate industrial time series are insufficient. In industrial systems, changes in equipment, environmental factors, and operating conditions, can lead to fluctuations in monitoring data. This variability and complexity create challenges for the data collection of complex equipment, leading to a lack of labeled datasets and an imbalanced data distribution. In addition, manufacturing systems may experience unexpected interruptions in sensor failures, network delays, and data transmission, leading to problems of high-level noise and missing values. In decentralized industrial collaborations such as cloud manufacturing, industrial data from different users remain difficult to share due to privacy concerns. These challenges make it difficult for industrial data to meet the demands of deep learning methods for time series applications in the industry.\nThe DGM-baed AIGC framework is proposed for industrial data generation, as illustrated in Fig. 3. First, generative models learns the latent representations of industrial time series by modeling dynamic processes and correlations between variables, which have been introduced in Section II and Section III. Second, quality enhancement, sample augmentation, and privacy protection are performed to improve the quality and quantity of data, which will be discussed in Section IV. Finally, the generated samples will be applied to various industrial scenarios to address challenges such as sample scarcity in industrial scenarios, as presented in Section V. With the development of generative intelligence, there are many methods are proposed to tackle these issues. These research works can be mainly categorized into five: limited sample augmentation, imbalanced data synthesis, sensor signal imputation, sensor signal denoising, and privacy protection.\nLimited sample augmentation involves using sophisticated generative methods to enrich datasets where sample size is insufficient for effective model training. This approach is particularly crucial in scenarios where acquiring additional real-world data is challenging or impractical. By artificially generating realistic and diverse samples, this method enhances the dataset's comprehensiveness, aiding in the development of more accurate and generalized machine learning models.\nIn recent industrial applications, the enhancement of model performance and data processing efficiency through limited sample augmentation has emerged as a significant trend. The bidirectional alignment network combined with VAE [73] effectively addresses the issue of scarce samples in fault diagnosis in thermal power plants, achieving generalization across rare and unseen fault categories. The DA-JITL framework [18] incorporates a causality-informed VAE, significantly enhancing the performance of JITL-based soft sensors. MSGAN [74] merges offline training with online real-time inference, employing an adaptive update strategy and gradi-ent penalty in Wasserstein distance to generate high-quality false anomaly samples to compensate for the issue of data scarcity, thereby improving the accuracy of anomaly detection in industrial robotic sensors. Optimized diffusion models [24] and TimeDDPM [21] play crucial roles respectively in precise defect identification in CFRP structures and dynamic process soft sensor modeling by data augmentation, showcasing the versatility of these DGMs in limited sample augmentation."}, {"title": "B. Imbalanced Data Synthesis", "content": "Imbalanced data synthesis refers to the process of artificially generating data to address the challenge of unequal distribution of classes within a dataset. This technique is pivotal in machine learning and data science, especially when dealing with scenarios where certain types of data (often representing minority classes) are scarce or underrepresented.\nIn the realm of industry, a myriad of innovative approaches are being explored to tackle the prevalent challenge of data imbalance in datasets. Among these, Fan et al. [35] introduces a groundbreaking Variational Autoencoder (VAE)-based imbalanced data synthesis method, tailored specifically for semiconductor fault detection. This method adeptly addresses the critical scarcity of defective wafer samples. Simultaneously, the DRL-GAN model [75] ingeniously amalgamates distributional reinforcement learning with GAN, thereby showcasing remarkable efficiency in generating balanced data distributions. Its application results in substantial improvements in various anomaly detection metrics, highlighting the model's profound impact on the landscape of industrial anomaly detection. Furthermore, a Wasserstein conditional GAN [25] is proposed, which is enhanced with hierarchical feature matching. This novel approach is adept at tackling the long-tail distribution problem, a common quandary in the realm of bearing fault diagnosis. The implementation of this model demonstrates an effective and innovative method for addressing the data imbalance problem."}, {"title": "C. Sensor Signal Imputation", "content": "Sensor signal imputation describes the process of estimating and filling in missing or incomplete data in datasets collected from sensors. The accuracy and completeness of sensor data are essential for reliable analysis and decision-making.\nThe Vector Autoregressive Imputation Method (VAR-IM) [76] utilizes an integrated expectation-minimization and pre-diction error minimization algorithm, significantly improving imputation in multivariate time series, especially in scenarios involving missing signal data. Velasco-Gallego et al. [28] introduce a VAE-based sophisticated imputation method for sensor data from marine machinery. This approach yields a high determination coefficient, indicating its potential to improve the internet of ships and condition-based maintenance domains. Then in the realm of GANs, the Federated Transfer Missing Data Imputation method (FedTMI) [77] utilizes edge-computed GANs and federated transfer learning for heterogeneous missing data in industrial applications, while the Fine-Tuned Imputation GAN (FIGAN) [78] focuses on quality-related variables in soft sensors, interweaving improved data imputation with pseudo labeling. Other GAN models like the IM-GAN [79], SGAIN [80], Semi-GAN [81], ST-GAIN [82], and SGT-GAIN [83] also demonstrate superior capabilities in data imputation, each addressing specific challenges such as complex multivariate time series issues, structural health monitoring, semiconductor equipment fault detection, and advanced manufacturing data imputation. As for diffusion models, the DiffAD model [27] employs a denoising diffusion-based imputation method combined with a multi-scale state space model for time series anomaly detection. It effectively adapts to concentrated anomaly episodes and captures long-term dependencies. Similarly, the GD-GRU model [84] integrates a Gaussian diffusion process with a Gated Recurrent Unit for enhancing atmospheric environmental quality data repair."}, {"title": "D. Sensor Signal Denoising", "content": "To enhance the quality and reliability of the information captured by sensors, sensor signal denoising is used to remove noise from sensor data. The primary objective is to filter out these extraneous noises without distorting the actual signal.\nStarting with AR models, the studies [34] and [29] showcase diverse applications of AR in sensor signal denoising. Among these, Liu et al. [29] introduce an advanced approach for industrial robots, integrating wavelet regional correlation threshold denoising with autoregressive moving average models. This method significantly enhances signal clarity for acoustic emission data. In the domain of GAN models, a Bayesian nonparametric estimation within a CycleGAN architecture [30] is constructed for online sensor signal denoising in milling processes. Furthermore, the Att-DCDN is presented for seismic data denoising, combining an encoder-decoder structure within a GAN. This approach integrates synthetic and field seismic data, leveraging attribute-based constraints for noise attenuation. While Min et al. introduce a novel GAN-based approach [85] for seismic data denoising, combining a pre-trained deep denoising autoencoder with transfer learning to enhance training on limited field data."}, {"title": "E. Privacy Protection", "content": "In the context of the industrial sector and generative models, privacy protection refers to safeguarding sensitive and proprietary information during the deployment and application of these advanced computational models.\nBeginning with AR models, Palekar et al. [31] present a privacy-preserving authentication model for industrial devices. This model employs the Autoregressive Poor and Rich Optimization (APRO)-based secret key generation and Box-Cox transformation for encryption, achieving secure data exchange with minimal computation time and enhanced encryption quality. The study [32] proposes a privacy-preserving framework for smart power networks, integrating enhanced-proof-of-work blockchain and VAE to safeguard data privacy and detect anomalies, while Almaiah et al. [105] introduces a blockchain-based deep learning framework for industrial, employing VAES with Bidirectional Long Short-Term Memory networks for robust privacy protection. A KingFisher framework [103] uses VAEs for scrutinizing network interactions and safeguarding privacy. Lastly, the privacy-enhanced federated learning mechanism for the Internet of Medical Things [110], utilizes VAEs with differential privacy noise. As for GAN models, Chen et al. [33] discuss a differentially private GAN model to preserve the privacy of machine operation data. Fed-LSGAN [106] integrates federated learning with the least squares GANs for privacy-aware renewable energy scenario generation. Furthermore, the study [104] proposes a solution for debugging data issues in privacy-sensitive and federated learning contexts using differentially private federated GANs. A deep differential privacy data protection algorithm for industrial networks [108] is constructed. Finally, ProcessGAN [107] innovatively generates confidential synthetic process data, especially beneficial for small medical datasets. Besides, the differentially private DDPM [109] is a novel approach integrating differential privacy into diffusion models to ensure data privacy while generating high-quality synthetic data."}, {"title": "V. TYPICAL INDUSTRIAL APPLICATIONS OF DGMS", "content": "After discussing the main tasks of DGMs in industry, in this section, we mainly introduce the applications in industrial scenarios.\nIn the industrial field, anomaly detection refers to the identification of irregular or abnormal patterns within industrial data, which could signify issues such as equipment failure, production inefficiencies, or security breaches. It is crucial for maintaining operational efficiency, ensuring safety, and minimizing downtime in industrial settings. Advanced analytical techniques, including machine learning and deep learning, are employed for detecting these anomalies by analyzing sensor data, network traffic, or production metrics. For instance, the AE-NAR model [34] is specifically designed for wind turbine pitch-bearing anomaly detection. It leverages an autoencoder for global feature extraction and nonlinear au-toregression with augmented Lagrangian for noise reduction, thereby significantly enhancing detection accuracy. Similarly, the adaptive weighted loss VAE [97] is an unsupervised model presented in industrial production for anomaly detection. This model achieves superior accuracy over traditional VAE by adaptively adjusting loss function weights during training. The conditional GAN [87] with its unique encoder-decoder structure adeptly processes high-frequency time series data for anomaly detection in smart manufacturing. These innovative approaches, along with other GANs and Diffusion models [75] [27] [74], demonstrate the evolving landscape of anomaly detection in industrial. They emphasize the significance of complex computational models in tackling the intricate challenges of anomaly detection within industrial contexts."}, {"title": "B. Predictive Maintenance", "content": "Predictive maintenance is a strategic approach that employs data analytics, machine learning, and advanced computational models to predict and prevent equipment failures before they occur. This approach aims to optimize maintenance operations by identifying potential issues early, thereby reducing unplanned downtime and extending equipment life. A GAN-based data augmentation method [36] is illustrated to address the scarcity of fault condition data in Industry 4.0. This method enhances the training and performance of machine learning models, which are critical for accurately predicting equipment failures. The study [96] further expands on this by introducing a novel approach using Gaussian process regression and GAN for handling missing data, which is vital for making accurate predictions in quality assurance and maintenance scheduling. Additionally, Xiong et al. [91] present the Controlled Physics-Informed GAN (CPI-GAN), a hybrid framework that synthesizes degradation trajectories for enhancing Remaining Useful Life (RUL) predictions. This approach ensures accurate predictions that adhere to fundamental physics, thereby improving maintenance scheduling. In addition to the aforementioned approaches, a Variational Autoencoders (VAEs) method [28] further diversifies the computational strategies employed in the realm of predictive maintenance"}, {"title": "C. Fault Diagnosis", "content": "Fault diagnosis refers to the process of identifying, analyzing, and rectifying faults in industrial equipment and systems. It leverages advanced data analytics, machine learning algorithms, and sensor data to accurately and promptly diagnose equipment failures. Zhao et al. [17] introduces the NCVAE-AFL framework for the bearing-rotor system, addressing class imbalance and long-tail distribution issues in fault diagnosis. A data-augmentation approach using time-varying autoregressive models [90] is constructed specifically for fault diagnosis in non-stationary multivariate time series, demonstrating enhanced performance in real-world scenarios. Additionally, the studies [94] [37] explore the use of Wasserstein Generative Adversarial Networks (WGAN) for data enhancement, target-ing the improvement of fault diagnosis accuracy by addressing the scarcity of mechanical fault samples. A GAN-GDA model [95], combining GAN with gaussian discriminant analysis for data augmentation, aims at enhancing the effectiveness of fault diagnosis in industrial settings. Together with a federated learning method [73], and an AR-based method [111], these studies exhibit the diversity of advanced computational techniques in fault diagnosis. They underscore the crucial role of machine learning and data analytics in effectively diagnosing faults in the industrial environment."}, {"title": "D. Supply Chain Optimization and Production Scheduling", "content": "Supply chain optimization and production scheduling are key concepts in operations management, focused on enhancing the efficiency and effectiveness of the entire supply chain through rational resource allocation, coordination of production activities, and process improvement. Recent advancements in this domain have seen the introduction of innovative deep-learning models, particularly leveraging GANs and VAEs. A Conditional Variational Autoencoder (CVAE) [113] based model has been proposed to integrate production scheduling in the automotive component manufacturing sector. This model generates operation-specific health indicators from time series data, aiding in quantifying machine degradation and enhancing production efficiency. The study [89] aims to optimize supply chain management by leveraging deep generative models and time series analysis. This approach is enhanced through the generation of creative alternatives using GANs, thereby aiming to increase the success rate of product introductions. A hybrid model combining TimeGAN and CNN-LSTM networks [93] addresses short-term load forecasting in commercial buildings, improving forecasting accuracy and aiding energy management in supply chains. Finally, the Seq2Seq-WGAN model [92] employs a sequence-to-sequence-wasserstein GAN approach to better capture temporal dependencies in cement production processes. This model generates extensive f-CaO label data and Seq2Seq for managing unequal length input-output sequences, leading to more accurate predictions and enhancing cement industry production scheduling."}, {"title": "VI. LARGE GENERATIVE MODELS FOR INDUSTRIAL TIME SERIES", "content": "In the industrial domain, generative models have made significant progress but their usefulness is limited. In complex and open industrial environments, generative models may face several challenges, including limited generalization capabilities, multi-tasking limitations, and cognitive limitations. Specifically, although generative models perform well in known scenarios, they struggle with the complexity of real industrial scenarios due to insufficient generalization capabilities. In addition, current generative models are usually designed for a single task. While industrial equipment usually contains hundreds of core components, developing a corresponding generative model for each core component separately is difficult to achieve. Finally, generative models have limited cognitive ability to understand the nature of industrial data, which causes them to potentially generate incomprehensible and erroneous results.\nTo address these issues, researchers have turned to large generative models such as ChatGPT [11", "10": "."}]}