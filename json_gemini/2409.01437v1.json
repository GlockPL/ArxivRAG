{"title": "Kvasir-VQA: A Text-Image Pair GI Tract Dataset", "authors": ["Sushant Gautam", "Andrea Stor\u00e5s", "Cise Midoglu", "Steven A. Hicks", "Vajira Thambawita", "P\u00e5l Halvorsen", "Michael A. Riegler"], "abstract": "We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and Kvasir-Instrument datasets, augmented with question-and-answer annotations to facilitate advanced machine learning tasks in Gastrointestinal (GI) diagnostics. This dataset comprises 6,500 annotated images spanning various GI tract conditions and surgical instruments, and it supports multiple question types including yes/no, choice, location, and numerical count. The dataset is intended for applications such as image captioning, Visual Question Answering (VQA), text-based generation of synthetic medical images, object detection, and classification. Our experiments demonstrate the dataset's effectiveness in training models for three selected tasks, showcasing significant applications in medical image analysis and diagnostics. We also present evaluation metrics for each task, highlighting the usability and versatility of our dataset. The dataset and supporting artifacts are available at https://datasets.simula.no/kvasir-vqa.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancement of medical diagnostics increasingly relies on the integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques to analyze complex medical images. The human digestive system is categorized into the upper and lower Gastrointestinal (GI) tracts and includes the esophagus, stomach, and intestines. Diseases in the GI tract, being prevalent and often requiring intricate diagnostic procedures, present an ideal domain for deploying Al-driven solutions. Existing datasets such as HyperKvasir [18] and Kvasir-Instrument [27] have made substantial contributions to this field by offering a wide range of classification- and segmentation-labeled GI images.\nHowever, the lack of comprehensive textual annotations, particularly question-and-answer pairs, limits the potential for developing advanced Al models capable of nuanced understanding and decision-making. This limitation is critical because question-and-answer pairs for an input can help to simulate the reasoning process that we humans go through, thereby enabling AI models to better understand context, interpret complex scenarios, and provide more accurate diagnostics. Furthermore, these annotations can facilitate the training of Al systems to handle a wider variety of cases, improving their generalizability and reliability in real-world clinical settings."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "The human gastrointestinal (GI) tract is susceptible to a wide range of abnormal mucosal conditions, varying from minor irritations to highly lethal diseases [22, 43]. According to the International Agency for Research on Cancer, a specialized cancer agency of the World Health Organization (WHO), GI cancers account for approximately 4.8 million new cases annually worldwide [11]. These cancers often have a high mortality rate, contributing to around 3.4 million deaths each year [11].\nEndoscopy is the current gold-standard procedure for examining the GI tract, but its effectiveness is limited by the operator's performance, which results in a significant average miss rate of around 20% for polyps in the colon [4, 60]. Consequently, improving endoscopic performance, enhancing the quality of clinical examinations, and implementing systematic screening are crucial for reducing morbidity and mortality associated with GI diseases [31, 51].\nThe emergence of AI-enabled support systems offers promise in assisting healthcare professionals in providing high-quality care on a large scale [8]. These AI systems, particularly those using ML techniques, require extensive training on well-curated datasets containing human-verified annotations to be effective in real-world tasks, such as detecting precancerous lesions or cancers in medical images [34]. The performance of AI in medical image analysis has seen significant advancements, primarily driven by the quality of the datasets and the sophistication of the algorithms employed [9, 45]."}, {"title": "2.1 Gastrointestinal Image Datasets", "content": "Earlier GI datasets have focused on different findings such as polyps [10, 12-16, 29, 47, 54, 57], endoscopic artifacts [5], GI lesions [2, 3, 20, 38, 39], angiectasia, bleeding, inflammation, esophagitis, ulcerative colitis, Z/line, pylorus, cecum, dyed resection margins, and stool [10, 12, 14, 35, 46, 47]. However, a number of these datasets are not publicly available, serve more as educational databases rather than being suitable for algorithm training, and/or are not usable for ML. Several other datasets have also focused specifically on diagnostic and therapeutic tool segmentation in endoscopy [6, 7, 17, 27, 49].\nKvasir [48], HyperKvasir [18], Kvasir-SEG [29], Kvasir-Capsule [55] and Kvasir-Instrument [27] are prominent datasets that have catalyzed research in GI diagnostics, see for example [21, 28, 56, 58, 61], providing a large number of GI images of various types and some with bounding boxes and segmentation masks. Recent studies have leveraged these datasets to develop models for specific tasks, such as polyp detection using Convolutional Neural Networks (CNNs) and capsule endoscopy analysis. However, these models are limited by the scope of available annotations, which primarily focus on image classification, object detection, and segmentation."}, {"title": "2.2 Image Captioning", "content": "Image captioning in medical imaging has advanced significantly with the introduction of transformer-based models [53]. These models, particularly those leveraging architectures like the Vision Transformer (ViT) and multimodal transformers, have demonstrated superior performance in generating accurate and contextually rich descriptions of medical images [23]. The ability of transformers to capture long-range dependencies and context within images makes them ideal for medical applications where subtle differences can be diagnostically significant [42]. In the context of the Kvasir-VQA dataset, the integration of fine-tuning in captioning models can enhance the models' capability to provide detailed and accurate descriptions that can support clinical decision-making and automated reporting."}, {"title": "2.3 Visual Question Answering", "content": "VQA is an emerging research area that combines image understanding with Natural Language Processing (NLP) to answer questions about images [37]. While VQA has seen success in general domains, its application in medical imaging remains nascent due to a lack of specialized datasets [30]. The introduction of question-and-answer annotations in our extended dataset Kvasir-VQA addresses this gap, providing a rich resource for training VQA models tailored to medical diagnostics."}, {"title": "3 THE KVASIR-VQA DATASET", "content": "The Kvasir-VQA dataset we present in this work is an extension of the publicly available HyperKvasir [18] and Kvasir Instrument [27] datasets. This extended dataset incorporates question-and-answer ground truth data, developed in collaboration with medical experts. It covers the entire GI tract, including both normal and abnormal findings, as well as images of various surgical instruments used in GI procedures, such as colonoscopies and gastroscopies."}, {"title": "3.1 Dataset Sources", "content": "The visual components of the Kvasir-VQA dataset are sourced from the HyperKvasir and Kvasir-Instrument datasets."}, {"title": "3.2 Annotation Process", "content": "Additional question-and-answer ground truth data were collected with input from medical professionals experienced in GI disease diagnostics. Initial annotations were conducted by computer scientists using LabelBox [1], followed by verification by medical experts\u00b9.\nFor collecting the ground truth, six types of questions were answered for each image in the dataset, namely: Yes/No questions, single-choice questions, multiple-choice questions, color-related questions, location-related questions, and numerical count questions. The annotations cover various GI aspects, including findings (questions 1, 2, 5, 6, 8, 17, 18), abnormalities (questions 9, 12, 15), anatomical landmarks (questions 10, 13, 16), and instruments (questions 11, 14, 19), as well as multimedia aspects, including image artifacts (question 3) and text (question 4)."}, {"title": "3.3 Final Dataset", "content": "The Kvasir-VQA dataset comprises 6,500 images, each annotated with various question-and-answer pairs. This dataset is fully accessible on HuggingFace. A subset of the Kvasir-VQA dataset, comprising 2,000 images and 20,241 captions derived from the annotations for those images, was recently used in a multimedia retrieval challenge [24], which includes two specific tasks:\n(1) Image Synthesis (IS): This task involves participants using text-to-image generative models to create a diverse dataset of medical images based on textual prompts. For instance, participants might receive prompts such as \"An early-stage colorectal polyp\" and are expected to generate an image that accurately represents this description. The development dataset provided includes prompt and image pairs to aid in developing solutions. During the testing phase, participants will be given a list of prompts and must generate one image per prompt to submit to the organizers.\n(2) Optimal Prompt Generation (OPG): This task requires participants to generate images using self-created prompts within defined categories. Examples include generating images with a specific number of polyps, a polyp in a designated area of the image, or a polyp of a particular type and size. Other categories may involve creating images without findings in the esophagus or large bowel, or including specific instruments such as biopsy forceps, metal clips, or tubes. Additionally, participants might need to create images featuring anatomical landmarks like the Z-line, pylorus, or cecum. The evaluation will consider the quality of the synthetic images, the complexity of the models and prompts, and the hardware requirements.\nIn this work, we chose to use the same subset of 2,000 images across the experiments to showcase different applications of the dataset. It is also worth noting that the images in the subset, along with their annotations, were randomly selected in equal proportions"}, {"title": "4 EXPERIMENTS", "content": "In this section, we present three preliminary experiments to demonstrate the effectiveness and applicability of the Kvasir-VQA dataset: Image Captioning, Visual Question Answering (VQA), and Synthetic Medical Image Generation. These tasks were selected to highlight different aspects of the dataset and its potential use in various medical imaging and diagnostic contexts. Our implementation for all experiments is available open source.\nThe results from our experiments underscore the versatility and robustness of the Kvasir-VQA dataset, providing a foundation for future advancements in medical image analysis and diagnostics. The results indicate that models trained on Kvasir-VQA can effectively perform various vision-language tasks, showcasing the dataset's potential in real-world medical applications."}, {"title": "4.1 Image Captioning", "content": "Model and Setup: For the image captioning task, we used Florence-2 [62], an open-source, lightweight vision-language model, known for its strong zero-shot and fine-tuning capabilities across various tasks, including captioning, object detection, grounding, and segmentation. We fine-tuned Florence-2 with the prefix <DETAILED_CAPTION>. The model consists of 0.23 billion parameters, and we applied Low-Rank Adaptation (LoRA) [25] for efficient fine-tuning.\nTo optimize training efficiency and resource usage, we froze the image encoder during fine-tuning. This decision was based on the observation that while unfreezing the image encoder could potentially enhance performance, it would also significantly increase computational demands and resource consumption. By maintaining the image encoder in a frozen state, we focused the training process on the language model component, allowing us to achieve substantial improvements in caption generation with reduced resource requirements.\nDuring training, we used a learning rate of 1.8e-6 and employed a linear scheduler to manage the learning rate over the course of training. The model was trained for ten epochs with a batch size of 20 on NVIDIA A100 GPU, utilizing the AdamW optimizer to update model weights. We monitored training and validation loss to assess model performance and adjust training parameters as needed. The model and processor were saved at each epoch, facilitating incremental evaluation and potential model deployment.\nDataset and Training: A subset of Kvasir-VQA was employed for this task, where each medical image is paired with a descriptive caption. The whole of the data-subset with 2,000 images was used for the training. The model was trained for 10 epochs, ensuring the loss had stabilized. During inference, the model generates the top 5 candidate captions for each image, acknowledging that different valid aspects of the image might be highlighted."}, {"title": "4.2 Visual Question Answering", "content": "Synthetic VQA Dataset Generation We generated a synthetic VQA dataset by leveraging the capabilities of the LLAMA-3 (7B)\u00b3 language model. The primary objective was to create diverse and contextually relevant question-and-answer pairs from existing image captions, thereby producing a valuable resource for training and evaluating VQA systems.\nWe commenced with the curated set of image captions. These captions already provided descriptive information about the images, encompassing visible properties, procedural details, and notable findings.\n\u2022 Language Model Selection: The LLaMA-3 (7B) model, a state-of-the-art language model known for its adeptness in understanding and generating human-like text, was selected for this task. The model's large parameter size and advanced training regimen equipped it to handle the complex and domain-specific language inherent in medical captions.\n\u2022 Prompt Design: To guide the language model in generating question-and-answer pairs, we designed a comprehensive prompt, which included:\nA role specification for the model, indicating its function as an intelligent dataset generator.\nExplicit instructions to formulate questions based on the content of the captions, ensuring that the questions did not refer directly to the images. This was crucial for maintaining generalizability and focusing on the textual information.\nSpecific guidelines for generating questions related to aspects such as the presence of text, the type of procedure, polyp count, color, size, and location, when mentioned in the caption.\n\u2022 Data Generation Process: Utilizing the LLaMA-3 (7B) model, for each caption, a JSON object was generated containing the question (Q) and answer (A) pairs. This process involved:\nParsing the caption to identify key elements.\nCrafting a question that emphasizes these elements.\nFormulating an answer that accurately reflects the information conveyed in the caption.\n\u2022 Quality Control: To ensure the quality and relevance of the generated data, several quality control measures were implemented:\nRetry Mechanism: The data generation process included a retry mechanism to handle instances where the output was incomplete or non-informative, thereby ensuring the robustness of the dataset.\nManual Review: A subset of the generated question-and-answer pairs was subjected to manual review by domain experts to evaluate the appropriateness of the questions and the correctness of the answers.\n\u2022 Output and Storage: The resulting question-and-answer pairs were saved, organized according to the index of the original captions. This systematic organization facilitates easy access and integration into VQA systems for training and evaluation."}, {"title": "Model and Setup:", "content": "To address the limitations of image captioning in controlling specific aspects of image information, we leveraged the VQA task. We fine-tuned the same Florence-2 model with the prefix <MedVQA> and included both the input image and the corresponding question in the input. This task benefits from easier evaluation compared to captioning, as the model's output is constrained to answering specific questions about the image.\nDataset and Training: We used the synthetic VQA dataset generated by prompting the LLaMA-3 (7B) language model\u00b3 to create relevant question-and-answer pairs from the existing captions. Similar to the image captioning task, the subset of VQA dataset with 2,000 images was used in the training for synthetic VQA generation, and 20,241 question-and-answer pairs were synthesized from then captions. The model was fine-tuned for 10 epochs."}, {"title": "Evaluation Metrics:", "content": "We assessed the performance of the VQA model using metrics such as BLEU, ROUGE, METEOR, and CIDEr on all the question-and-answer pairs generated from the subset. The detailed results are shown in Table 3, highlighting the model's proficiency in answering specific medical questions based on image content."}, {"title": "4.3 Synthetic Medical Image Generation", "content": "Model and Setup: To generate high-quality synthetic medical images, we trained the Stable Diffusion 3 model [19], which utilizes an advanced Multimodal Diffusion Transformer (MMDiT) architecture. This model translates textual prompts into high-resolution images, thereby enhancing the dataset's utility for training and testing various models. The training employed the DreamBooth technique [50], which updates the entire diffusion model by training on just a few images of a subject or style, associating a special word in the prompt with the example images. A resolution of 512x512 pixels was used, and a large batch size of 48 was employed with gradient accumulation steps set to 1 on the NVIDIA A100 GPU. Mixed precision (fp16) training and the Prodigy optimizer were utilized [40], alongside a sigma-sqrt weighting scheme and a constant learning rate. The model underwent validation at every epoch to evaluate the generated output with set prompts, and gradient checkpointing was enabled to optimize memory usage.\nDataset and Training: The input prompts for the synthetic medical image generation task were derived from the captions used in the image captioning task. We focused on generating high-quality synthetic images that accurately represent the textual descriptions. The model's fine-tuning process included the application of a low-rank adaptation (LoRA) with a rank value of 128, allowing for efficient parameter updates without overfitting. The model, containing 2 billion parameters, was fine-tuned using LoRA for 80 epochs until the training loss had stabilized. Post-training evaluation demonstrated that the model could consistently produce anatomically plausible and diagnostically relevant images, making it a valuable tool for generating medical image datasets."}, {"title": "5 DISCUSSION", "content": "The Kvasir-VQA dataset aims to bridge the gap between medical image analysis and practical diagnostic applications, ultimately contributing to improved patient care and diagnostic accuracy. Below, we elaborate on dataset applications, as well as limitations and potential future work."}, {"title": "5.1 Dataset Applications", "content": "The Kvasir-VQA dataset extends the capabilities of existing GI datasets by introducing comprehensive question-and-answer annotations for medical images. This enhancement allows for the development of sophisticated ML models that can perform a variety of tasks essential for medical diagnostics.\n\u2022 Image Captioning: As demonstrated in Section 4, Kvasir-VQA can be used to train captioning models. By generating descriptive captions for medical images, it is possible to automate the creation of detailed medical reports, reducing the burden on healthcare professionals and minimizing the risk of human error. The Florence-2 model, fine-tuned for the image captioning task, has shown impressive performance in generating accurate and relevant captions, as evidenced by high BLEU, METEOR, and CIDEr scores.\n\u2022 Visual Question Answering (VQA): The incorporation of VQA tasks enables models to interpret medical images in a more interactive and detailed manner. Our experiments with the Florence-2 model, fine-tuned for VQA, demonstrate the model's ability to accurately answer specific medical questions based on visual input. This capability is crucial for developing diagnostic tools that can assist clinicians by providing immediate, context-specific information about GI conditions. The ability for the model to provide text as output might also enhance healthcare professionals' interaction with the model because the prediction includes more context than just the predicted class or diagnosis. This can make it easier to 'quality check' the model's reasoning behind the prediction and contribute to a more natural way of interacting with a machine.\n\u2022 Synthetic Medical Image Generation: The ability to generate high-quality synthetic medical images using models such as Stable Diffusion 3 opens new avenues for data augmentation and training. Synthetic images can help address the issue of class imbalance in medical datasets and provide additional training samples for rare conditions. Our experiments demonstrate the effectiveness of Kvasir-VQA for synthetic medical image generation, with generated images exhibiting high fidelity and diversity as measured by FID and IS metrics.\n\u2022 Object Detection and Localization: The question-and-answer annotations related to the location of abnormalities, instruments, and anatomical landmarks enable precise training of object detection and localization models. These models are essential for tasks such as polyp detection and surgical instrument recognition, which are critical for real-time diagnostic support during endoscopic procedures.\n\u2022 Classification: The yes/no and choice questions in the Kvasir-VQA dataset facilitate the development of classification models. These models can assist in diagnosing specific GI conditions, identifying procedural contexts, and recognizing the presence of surgical instruments. Kvasir-VQA supports both single-choice and multiple-choice questions, allowing for a comprehensive evaluation of classification model performance."}, {"title": "5.2 Synthetic VQA Dataset Generation", "content": "The synthetic VQA dataset generated with LLM demonstrated a broad vocabulary and diverse question structures, showcasing the model's ability to understand and interpret complex medical language. This linguistic variety is crucial for developing robust VQA systems that can handle a wide range of questions and scenarios in medical imaging.\nUsing the LLaMA-3 (7B) model to generate synthetic VQA datasets has proven effective, offering a scalable and efficient method for producing large datasets with varied linguistic patterns. This approach not only improves training data for VQA systems but also advances medical AI by fostering the development of more nuanced models. Future work could refine prompt engineering and integrate iterative feedback to further enhance dataset quality."}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "While the Kvasir-VQA dataset marks significant progress, it is crucial to acknowledge that several limitations remain. Addressing these limitations presents opportunities for meaningful improvements and further advancements in the dataset's utility and accuracy.\n\u2022 Expert Verification of Annotations: As outlined in Section 3, the initial annotations in the Kvasir-VQA dataset were performed by computer scientists, with subsequent verification by medical professionals. However, due to time constraints, not all annotations received expert validation. Future work will include the release of an enhanced version of the dataset, which will undergo complete expert verification to ensure the highest standards of annotation accuracy and reliability.\n\u2022 Scope: The current Kvasir-VQA dataset includes samples from a range of GI conditions and procedural contexts. However, it does not cover the full spectrum of GI conditions encountered in clinical practice. Future efforts will focus on expanding the dataset to encompass a wider array of GI conditions and procedural contexts, thereby increasing its comprehensiveness and applicability for diverse diagnostic applications.\n\u2022 Scale: The Kvasir-VQA dataset currently comprises 6,500 annotated images, a subset of the images available in the source datasets, such as HyperKvasir, which contains over 10,662 labeled images.\nWe aim to expand the dataset by integrating additional images and annotations from these source datasets, thereby providing a more extensive resource for training and evaluating machine learning models.\n\u2022 Validation: All experiments and results reported in this study are based on a subset of 2,000 images, including their corresponding annotations, derived captions, and question-and-answer pairs. For reproducibility, comparative analysis, and proper validation of the trained models, the full dataset should be utilized with appropriate training, validation, and test data splits. Addressing this will be considered in future work.\n\u2022 Development of Diagnostic Tools: While the Kvasir-VQA dataset is designed to support the development of AI-driven diagnostic tools, the current iteration may not fully address all necessary components for comprehensive diagnostic support. Future enhancements will aim to align the dataset with emerging clinical requirements and technological advancements, thereby more effectively contributing to the development and refinement of diagnostic tools and systems in healthcare.\nFuture work will prioritize the augmentation of the dataset through additional expert-verified annotations and an expanded scope that includes a broader range of GI conditions and procedural contexts. By continuously improving and broadening this resource, we aspire to facilitate the development of advanced AI-driven diagnostic tools that have the potential to transform medical imaging and diagnostics."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduce the Kvasir-VQA dataset, an extension of the existing HyperKvasir and Kvasir-Instrument datasets, enriched with question-and-answer annotations. This dataset is specifically designed to advance research in medical image analysis, particularly in the field of gastrointestinal diagnostics. The Kvasir-VQA dataset facilitates a variety of applications, including image captioning, VQA, and synthetic medical image generation, as demonstrated through our preliminary experiments.\nThe introduction of synthetic question-and-answer pairs provides a new dimension to the dataset, enabling the development of sophisticated AI models capable of nuanced understanding and interactive diagnostics. These capabilities are crucial for improving the accuracy and efficiency of medical diagnostics, thereby improving patient care. Despite its promising potential, the Kvasir-VQA dataset is still a work in progress. Future efforts will focus on extending the dataset's scope and scale, ensuring comprehensive expert validation of annotations, and aligning with clinical needs. These enhancements will further solidify the dataset's role as a valuable resource for the medical community, supporting the development of next-generation diagnostic tools."}]}