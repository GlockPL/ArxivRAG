{"title": "LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch", "authors": ["Caigao Jiang", "Xiang Shu", "Hong Qian", "Xingyu Lu", "Jun Zhou", "Aimin Zhou", "Yang Yu"], "abstract": "Optimization problems are prevalent across various scenarios. Formulating and then solving optimization problems described by natural language often requires highly specialized human expertise, which could block the widespread applica-tion of optimization-based decision making. To make problem formulating and solving automated, leveraging large language models (LLMs) has emerged as a potential way. However, this kind of way suffers from the issue of optimization generalization. Namely, the accuracy of most current LLM-based methods and the generality of optimization problem types that they can model are still limited. In this paper, we propose a unified learning-based framework called LLMOPT to boost optimization generalization. Starting from the natural language descriptions of optimization problems and a pre-trained LLM, LLMOPT constructs the introduced five-element formulation as a universal model for learning to define diverse optimization problem types. Then, LLMOPT employs the multi-instruction tuning to enhance both problem formalization and solver code generation accuracy and generality. After that, to prevent hallucinations in LLMs, such as sacrificing solving accuracy to avoid execution errors, model alignment and self-correction mechanism are adopted in LLMOPT. We evaluate the optimization generalization ability of LLMOPT and compared methods across six real-world datasets cov-ering roughly 20 fields such as health, environment, energy and manufacturing, etc. Extensive experiment results show that LLMOPT is able to model various optimization problem types such as linear/nonlinear programming, mixed integer programming and combinatorial optimization, and achieves a notable 11.08% av-erage solving accuracy improvement compared with the state-of-the-art methods. The code is available at https://github.com/caigaojiang/LLMOPT.", "sections": [{"title": "Introduction", "content": "Optimization problems are widespread across a various range of scenarios, such as job scheduling [3], path planing [10, 12], matching problem [28], and revenue management [25], etc. Although powerful solvers are available, formally defining domain-specific optimization problems from natural language descriptions and then developing the solver code are highly complex tasks. It requires specialized domain knowledge, expert involvement, and significant time investment. For example, in finance, the portfolio problems [6] often managing thousands of variables and complex constraints, such as full investment constraints, cardinality constraints and pre-assignment constraints, where expert intervention is crucial for achieving accurate (i.e., low simple regret) solutions due to the complexity and specialization of optimization problems.\nWith the rapid development of large language models (LLMs) like ChatGPT [4] and GPT-4 [18], using LLMs to automatically formulate and solve optimization problems described by natural language is becoming increasingly attractive. Existing work about it can be roughly divided into prompt-based and learning-based methods. Pioneering studies like Chain-of-Expert [29] and OptiMUS [1] explore prompt-based methods, leveraging LLM interfaces to extract key information and solve optimization problems. Although these methods show promise, recent advancements in learning-based methods, such as LLaMoCo [17] and ORLM [1], could demonstrate even greater potential. Notably, ORLM [1] reveals that a fine-tuned 7B open-source model can outperform larger pre-trained models like GPT-4, highlighting the value of learning-based methods for optimization tasks.\nAlthough previous research has established the use of LLMs for formulating and solving optimization problems described by natural language, their optimization generalization remains limited, restricting broader applicability and deployment across diverse problems. Optimization generalization, which involves both accuracy and generality in assessing whether LLMs can effectively solve general optimization problems, remains a significant challenge. Specifically, accuracy refers to solving problems correctly, and generality refers to the ability to model and solve diverse optimization problem types across different task scenarios.\nIn this paper, we propose a unified learning-based framework called LLMOPT to significantly boost the optimization generalization. Through focusing on improving both accuracy and generality to effectively define and solve a wide range of optimization problems, this work tries to narrow the gap between methods and practical applications. Specifically, we propose the five-element formulation for defining optimization problems to enhance solving accuracy as illustrated in Figure 1. To ensure the effectiveness of learning, we design a data augmentation process and conduct data labeling, resulting in high-quality datasets. Leveraging these datasets, we implement multi-instruction supervised fine-tuning to improve the LLM's accuracy in both defining and solving optimization problems. Additionally, we introduce model alignment to further enhance accuracy and reduce the"}, {"title": "Related Work", "content": "LLMs for Formulating and Solving Optimization Problems. The NL4Opt competition [22] encourages researchers to explore how LLMs can be leveraged to solve optimization problems efficiently [11, 24, 29, 1, 17]. Related work can be divided into prompt-based and learning-based methods. The prompt-based methods utilize existing LLM interfaces to automate the solving opti-mization problems. Chain-of-Expert (CoE) [29] introduces an agent-based workflow, where specific tasks are assigned to LLM agents at each stage of reasoning, while OptiMUS [1] employs tailored prompts designed specifically for solving linear programming and mixed-integer linear programming problems. The learning-based method remains relatively underexplored. LLaMoCo [17] introduces an instruction tuning framework in a code-to-code manner and demonstrates its efficacy through model fine-tuning. ORLM [24] proposed a semi-automated approach for generating synthetic training data during the instruction tuning phase, addressing the data-hungry nature of optimization modeling. Besides, Mamo [11] offers a benchmark for mathematical modeling with two optimization datasets, and introduces a standardized process for generating solver code using LLMs.\nLLMs as Components of Optimizers. Yang et al. [31] propose to use LLMs as optimizers, inspired by Bayesian optimization [8]. One LLM iteratively optimizes prompts, which are then used to guide another LLM in performing specific optimization tasks. LLMs can also serve as components within an optimizer [9, 32, 14, 13, 15]. Guo et al. [9] uses LLMs to as the crossover and mutation operators in genetic algorithms, thereby finding high-quality prompts. When solving the traveling salesman problem, Liu et al. [14] uses LLMs to generate new trajectories based on existing ones, iteratively optimizing them to produce higher-quality solutions. Liu et al. [13] and Yang and Li [32] apply LLMs to specific multi-objective optimization tasks. Liu et al. [15] employs LLMs to simulate both the acquisition function and the surrogate model in the Bayesian optimization process."}, {"title": "Methodology", "content": ""}, {"title": "An Overview of the Proposed LLMOPT Framework", "content": "This work aims to achieve a better optimization generalization of defining and solving the optimiza-tion problems through a learning-based approach. To this end, this paper proposes the LLMOPT framework, as illustrated in Figure 2, which comprises three key components: data, learning, and auto-testing. The following sections provide a detailed introduction: First, in Section 3.2, we in-troduce the five-element formulation to define general optimization problems well, and discuss the augmentation and labeling of training data. Next, in Section 3.3, we detail the multi-instruction supervised fine-tuning (SFT) process and model alignment to enhance solving accuracy. Finally, in Section 3.4, we present the auto-testing process with self-correction."}, {"title": "Data: Defining and Labeling General Optimization Problems", "content": ""}, {"title": "Universal Formulation to Define General Optimization Problems", "content": "To enable LLMs to formulate more general optimization problems described in natural language, we propose five-element, a universal formulation to define optimization problems. Firstly, in mathematics,"}, {"title": "Data Augmentation and Labeling", "content": "The five-element formulation enables the mapping of optimization problems described in natural language to a universal structure, which can then be used to generate solver code. However, the effectiveness of learning is significantly influenced by the quality, quantity, and distribution of training"}, {"title": "Learning: Multi-Instruction SFT and Model Alignment", "content": ""}, {"title": "Multi-Instruction SFT", "content": "Direct utilization of LLM to address optimization problems described in natural language often results in inaccuracies, primarily due to their inability to comprehensively capture implicit information. To address this issue, we enhance the capability to both define and solve the problems through multi-instruction supervised fine-tuning (SFT).\nThe multi-instruction dataset $D_{SFT} = D_{SFT}^f \\cup D_{SFT}^s$, labeled by experts, is designed to improve both the problem formulation and solving code generation abilities of LLM. The formulation dataset $D_{SFT}^f = {(u_i, v_i)}_1^{N_f}$ focuses on enhancing the LLM's ability to accurately define general optimization problems by providing data pairs $(u_i, v_i)$, where $u_i$ represents a problem $p$ after being applied to a template, and $v_i$ is its corresponding five-element formulation $f$. Meanwhile, the solving dataset $D_{SFT}^s = {(u_i, v_i)}_1^{N_s}$ focuses on improving the LLM's ability to generate solver code. It contains two types of data: in one, $u_i$ is the problem $p$ and $v_i$ is the solver code $s$; in the other, $u_i$ is the formulation $f$ and $v_i$ is the solver code $s$. Both are aligned with the appropriate templates and all the labels in $D_{SFT}$ are correct labeled by experts. The instruction templates used in the learning process are detailed in Appendix H, which provides a comprehensive explanation, including a more in-depth description of the five-element to facilitate better understanding by the LLM.\nThen, given the instruction $u$ and its label $v$, the objective of SFT is to maximize the conditional probability $p(v | u)$. Assuming that $p(v | u) = \\prod_{i=1}^n p(v_i | v_{0:i-1}, u)$, this leads to minimizing the negative log-likelihood:\n$L_{SFT}(\\theta) = -E_{(u,v)\\sim D_{SFT}} \\sum_{i=1}^n \\log \\pi(v_i | v_{0:i-1}, u; \\theta)$.\nwhere $D_{SFT}$ denotes the multi-instruction SFT dataset, which combines instructions from $D_{SFT}^f$ and $D_{SFT}^s$ to train the LLM in defining and solving optimization problems. $\\pi(\\cdot)$ represents the predicted probability distribution of the LLM, and $\\theta$ denotes its parameters."}, {"title": "Model Alignment", "content": "To mitigate hallucinations, we incorporate model alignment, which is not utilized by other learning-based methods. We use Kahneman-Tversky Optimization (KTO)[7] as our alignment method, as it"}, {"title": "Auto-Testing: Formulation, Solving and Self-Correction", "content": "In the auto-testing process, first, the optimization problem is formulated using the five-element framework based on its natural language description; second, the solver code is generated for the five-element formulation and executed; finally, the solver's running logs, including output results and errors, are analyzed to determine whether self-correction is necessary. The auto-testing process automates the entire workflow of problem definition and solver code generation, while also integrating the self-correction mechanism for continuous improvement.\nInspired by Chen et al. [5], to enhance optimization generalization, we implement self-correction to automatically analyze the output results and identify errors arising during the execution of the solver code. Specifically, the instruction is organized around the problem, the five-element formulation, the solver code, and the execution output results, following a template outlined in Appendix H. The self-correction LLM then determines whether further resolution is necessary or whether the optimal solution has been achieved. If resolution is needed, the model generates an analysis with suggestions and decides to return to whether the problem formulation step or the code generation step. The self-correction mechanism ensures a more robust and adaptive optimization process, improving optimization generalization especially in the accuracy of formulations and final solutions."}, {"title": "Experiment", "content": "To analyze the performance of LLMOPT, we conduct SFT and model alignment based on the open-source LLM Qwen1.5-14B [2] and compare it with various learning-based and prompt-based methods on extensive datasets. The experiments aim to answer four key questions below.\n(Q1) Learning-based vs. Prompting-based Methods: What advantages do learning-based methods, such as LLMOPT, have over LLMs that rely solely on prompt engineering?\n(Q2) Optimization Generalization (Accuracy and Generality): To what extent can LLMOPT improve optimization generalization ability compared with existing methods?\n(Q3) Importance of Problem Definition in LLMOPT: How does the proposed five-element formu-lation as an intermediate step contribute to boost accuracy in solving optimization tasks?\n(Q4) Effectiveness of Model Alignment in LLMOPT: How effective is model alignment in enhancing the solving accuracy of LLMs for optimization tasks?\nThe four questions are answered in order in the next sections which include a detailed introduction to the experiments, followed by the analysis of results."}, {"title": "Experimental Setup", "content": "The experiments are conducted on six real-world optimization and operation task datasets, namely NL4Opt [22], Mamo (EasyLP and ComplexLP) [11], IndustryOR [24], NLP4LP [1], Com-plexOR [29]. These datasets encompass about 20 scenario and 7 types of optimization problems. Detailed descriptions of these datasets are provided in Appendix A. Training and testing data are strictly separated. For the NL4Opt, Mamo (EasyLP and ComplexLP) datasets, we shuffle the original datasets and randomly extract 100 data from each dataset as the testing datasets, and the remaining data is used as seed data for data augmentation. The IndustryOR dataset retains its original partition-ing. Due to the limited data in NLP4LP and ComplexOR, all data from these datasets are used for testing and excluded from the training process.\nIn the experiment, we use three performance metrics to comprehensively evaluate the optimization generalization of the algorithm, namely, Execution Rate (ER), Solving Accuracy (SA), and Average Solving Times (AST). Specifically, ER refers to the proportion of solutions whose code can run without any errors and has running results output. SA refers to the proportion of solutions that correctly solve the optimization problem, i.e., find the optimal solution. AST refers to the average number of times the self-correction process is performed during the test. In our experiment, the maximum number of self-correction re-solves is set to 12 times."}, {"title": "Analysis of Optimization Generalization", "content": "In this section, we compare LLMOPT with prompt-based methods (Reflexion [23], Chain-of-Experts [29], OptiMUS [1]), learning-based methods (ORLM [24] built on Mistral-7B, Deepseek-Math-7B-Base, LLaMa3-8B), and GPT-4 [18], demonstrating the optimization generalization capa-bility of LLMOPT.\nLearning-based vs. Prompting-based Methods (Answer to Q1). To demonstrate the potential of the learning-based method, we perform SFT on Qwen1.5-14B [2] and compare it with GPT-4"}, {"title": "Ablation Study", "content": "In this section, we conduct comprehensive ablation experiments, including LLMOPT without five-element formulation, without KTO alignment, and without self-correction. The results of these experiments are shown in Figure 3, Figure 4, and Table 3.\nImportance of Problem Definition (Answer to Q3). In order to explore the importance of problem definition, we evaluate three metrics on the ablation experiments: Execution Rate (ER), Solving Accuracy (SA), and Average Solving Time (AST). As detailed in Table 3 and Figure 3(b), these results show that using the five-element formulation as the problem definition improves SA across all six datasets. However, this definition can sometimes lower the ER, as the LLM may oversimplify the problem without it, producing error-free but inaccurate code. In contrast, using the five-element approach ensures correct code generation, improving SA at the cost of slightly reduced ER.\nEffectiveness of Model Alignment (Answer to Q4). Alignment typically enhances the efficiency and effectiveness of LLMs on specific tasks. As shown in Figure 4(b) and 3(c), the ablation results shows that KTO alignment not only significantly boosts SA but also reduces AST across all six datasets. Furthermore, as shown in Figure 4(b), 3(d), and Table 3, the combination of the five-element formulation and KTO alignment improves performance in terms of SA and AST on complex datasets."}, {"title": "Discussion", "content": "Attempt on Larger Models. To explore the potential for further performance improvement of LLMOPT on larger models, we deploy it on Qwen2-72B [30]. The detailed results are provided in Appendix D. We conduct experiments on two complex task datasets, Mamo Complex and IndustryOR, and the results show that LLMOPT based on Qwen2-72B shows significant improvements in both SA"}, {"title": "Conclusion", "content": "This paper focuses on the challenge of optimization generalization in LLMs, including the accuracy in solving optimization problems and the generality of the problem types that LLMs can handle. We propose a learning-based framework called LLMOPT, which significantly improves LLMs\u2019 ability to define and solve general optimization problems through multi-instruction supervised fine-tuning and model alignment. LLMOPT introduces the five-element formulation as a universal definition for various types of optimization problems to enhance solving accuracy. LLMOPT is extensively evaluated on a wide range of optimization tasks. It achieves the state-of-the-art optimization generalization ability across all of them, i.e., a notable 11.08% average solving accuracy improvement."}, {"title": "Ethics and Reproducibility Statement", "content": "Ethics. This work does not involve any human subjects, personal data, or sensitive information. All the testing datasets used are publicly available, and no proprietary or confidential information is used. We follow recommendations to use the Azure OpenAI service when using GPT models.\nReproducibility. Experimental settings are described in Section 4 and Appendix B, and datasets included in Appendix A. The code is available at https://github.com/caigaojiang/LLMOPT."}]}