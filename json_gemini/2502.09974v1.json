{"title": "HAS MY SYSTEM PROMPT BEEN USED? LARGE LANGUAGE MODEL PROMPT MEMBERSHIP INFERENCE", "authors": ["Roman Levin", "Valeriia Cherepanova", "Abhimanyu Hans", "Avi Schwarzschild", "Tom Goldstein"], "abstract": "Prompt engineering has emerged as a powerful technique for optimizing large language models (LLMs) for specific applications, enabling faster prototyping and improved performance, and giving rise to the interest of the community in protecting proprietary system prompts. In this work, we explore a novel perspective on prompt privacy through the lens of membership inference. We develop Prompt Detective, a statistical method to reliably determine whether a given system prompt was used by a third-party language model. Our approach relies on a statistical test comparing the distributions of two groups of model outputs corresponding to different system prompts. Through extensive experiments with a variety of language models, we demonstrate the effectiveness of Prompt Detective for prompt membership inference. Our work reveals that even minor changes in system prompts manifest in distinct response distributions, enabling us to verify prompt usage with statistical significance.", "sections": [{"title": "1 INTRODUCTION", "content": "Prompt engineering offers a powerful, flexible, and fast way to optimize large language models (LLMs) for specific applications, enabling faster and cheaper customization than finetuning while delivering strong specialized performance. Large language model providers, such as Anthropic and OpenAI, release detailed prompt engineering guides on prompting strategies allowing their customers to reduce hallucination rates and optimize business performance (OpenAI, 2023; Anthropic, 2024b). The use of system prompts also provides specialized capabilities such as taking on a character which is often leveraged by startups in their products\u00b9. Developers put significant effort into prompt engineering and prompts optimized for specific use-cases are even sold at online marketplaces\u00b2.The importance and promise of prompt engineering gave rise to the interest of the community in protecting proprietary prompts and a growing body of academic literature explores prompt reconstruction attacks (Hui et al., 2024; Zhang et al.; Morris et al., 2023; Geiping et al., 2024) which attempt to recover a prompt used in a language model to produce particular generations. These methods achieve impressive results in approximate prompt reconstruction, however their reconstruction success rate is not high enough to be able to confidently verify the prompt reuse, they are computationally expensive usually relying on GCG-style optimization (Zou et al., 2023), and some of these methods require access to model gradients (Geiping et al., 2024). Additionally, while some reconstruction methods provide confidence scores (Zhang et al.), they do not offer statistical guarantees for prompt usage verification.In this work, we specifically focus on the problem of verifying if a particular system prompt was used in a large language model. This problem can be viewed through the lens of an adversarial setup: an attacker may have reused someone else's proprietary system prompt and deployed an LLM-based chat bot with it. In LLM-based chatbots, control over part of the input is given to the end user. Consider a customer service chatbot that employs a general LLM to help customers get the answers they need. These systems typically add the user input into a longer template that includes a system prompt with application-specific instructions to help ensure that the back end large general purpose language model returns useful content to the user. Note the value in an expertly written system prompt - it could be critical in getting quality responses from large back end LLMs. Assuming access to querying this chatbot, can we verify with statistical significance if the proprietary system prompt has not been used? In other words, we develop a method for system prompt membership inference.Our contributions are as follows:\u2022 We develop Prompt Detective, a training-free statistical method to reliably verify whether a given system prompt was used by a third-party language model, assuming query access to it.\u2022 We extensively evaluate the effectiveness of Prompt Detective across a variety of language models, including Llama, Mistral, Claude, and GPT families including challenging scenarios such as distinguishing similar system prompts and black-box settings.\u2022 Our work reveals that even minor changes in system prompts manifest in distinct response distributions of LLMs, enabling Prompt Detective to verify prompt usage with statistical significance. This highlights that LLMs take specific trajectories when generating responses based on the provided system prompt."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 PROMPT EXTRACTION ATTACKS", "content": "Prompt engineering has emerged as an accessible approach to adapt LLMs for specific user needs (Liu et al., 2023), with system prompts playing a crucial role in shaping LLM outputs and driving performance across application domains (Ng & Fulford, 2023). Prior work has proposed several prompt extraction attacks, which deduce the content of a proprietary system prompt by interacting with a model, both for language models (Morris et al., 2023; Zhang et al.; Sha & Zhang, 2024; Yang et al., 2024) and for image generation models (Wen et al., 2024). Morris et al. (2023) frame the problem as model inversion, where they deduce the prompt given next token probabilities. Similarly, Sha &"}, {"title": "2.2 DATA MEMBERSHIP INFERENCE AND EXTRACTION ATTACKS ON LANGUAGE MODELS", "content": "In the evolving discussion on data privacy, a significant topic is membership inference, which involves determining whether a particular data point is part of a model's training set (e.g. Yeom et al., 2018; Sablayrolles et al., 2019; Salem et al., 2018; Song & Mittal, 2021; Hu et al., 2022). Shokri et al. (2017) and Carlini et al. (2022) both propose methods to determine membership in the training data based on the idea that models tend to behave differently on their training data than on other data. Bertran et al. (2024) further propose a more effective method and alleviate the need to know the target model's architecture, while Wen et al. (2022) propose perturbing the query data to improve accuracy of their attack. Jagielski et al. (2023) consider the setting where the system includes an ensemble of models that may be updated over time. Other works explore training data membership inference in image generation models (Duan et al., 2023; Matsumoto et al., 2023). Additionally, dataset inference techniques explore settings where the whole training set is considered rather than single data points (Maini et al., 2021; 2024). Compared to the standard membership inference setting, our work addresses a related but distinct question: whether a given text is part of the LLM input context, thus exploring prompt membership inference."}, {"title": "3 PROMPT DETECTIVE", "content": ""}, {"title": "3.1 SETUP", "content": "Prompt Detective aims to verify whether a particular known system prompt is used by a third-party chat bot as shown in Figure 1. In our setup, we assume an API or online chat access to the model, that is, we can query the chat bot with different task prompts and we have control over choosing these task prompts. We also assume the knowledge about which model is employed by the service in most of our experiments, and we explore the black-box scenario in section 6.This setup can be applied when a user, who may have spent significant effort developing the system prompt for their product such as an LLM character or a domain-specific application, suspects that their proprietary system prompt has been utilized by a third-party chat service effectively replicating the behavior of their product, and wants to verify if that was in fact the case while only having online chat window access to that service. We note that prompt engineering is a much less resource-intensive task than developing or fine-tuning a custom language model, therefore, it is reasonable to assume that such chat bots which reuse system prompts are based on one of the publicly available language models such as API-based GPT models (Achiam et al., 2023), Claude models (Anthropic, 2024a), or open source models like Llama or Mistral (Touvron et al., 2023; Jiang et al., 2023).Moreover, this adversarial setup can be seen through the lens of membership inference attacks, where instead of verifying membership of a given data sample in the training data of a language model, we verify membership of a particular system prompt in the context window of a language model. We therefore refer to our adversarial setting as prompt membership inference."}, {"title": "3.2 HOW DOES IT WORK?", "content": "Let $f$ denote a language model, let $p$ be a system prompt, and let $q$ be a task prompt. Together, we denote the full output as $f_p(q)$. For example, a system prompt could look like \u201cYou are a helpful"}, {"title": "4 EXPERIMENTAL SETUP", "content": ""}, {"title": "4.1 SYSTEM PROMPT SOURCES", "content": "Awesome-ChatGPT-Prompts 3 is a curated collection of 153 system prompts that enable users to tailor LLMs for specific roles. This dataset includes prompts for creative writing, programming, productivity, etc. Prompts are designed for various functions, such as acting as a Startup Idea Generator, Python Interpreter, or Personal Chef. The accompanying task prompts were generated with Claude 3 Sonnet (see Appendix F). For the 153 system prompts in Awesome-ChatGPT, we generated overall 50 task prompts. In these experiments, while a given task prompt is not necessarily a good probe for every system prompt, these 50 task prompts include at least one good probe for each of the system prompts.Anthropic's Prompt Library 4 provides detailed prompts that guide models into specific characters and use cases. For our experiments, we select all of the personal prompts from the library that include system prompts giving us 20 examples. Personal prompts include roles such as Dream Interpreter or Emoji Encoder. As the accompanying task prompts, we used 20 of the corresponding user prompts provided in the library.Hard Examples: To evaluate the robustness of Prompt Detective in challenging scenarios, we create a set of hard examples by generating variations of prompts from Anthropic's Prompt Library. These variations are designed to have different levels of similarity to the original prompts, ranging from minimal rephrasing to significant conceptual changes, producing varying levels of difficulty for distinguishing them from the original prompts.For each system prompt from Anthropic's Prompt Library, we generate five variations with the following similarity levels (see Figure 2 for examples):1. Same Prompt, Minimal Rephrasing: The same prompt, slightly rephrased with minor changes in a few words.2. Same Prompt, Minor Rephrasing: Very similar in spirit, but somewhat rephrased.3. Same Prompt, Significant Rephrasing: Very similar in spirit, but significantly rephrased.4. Different Prompt, Remote Similarities: A different prompt for the same role with some remote similarities to the original prompt.5. Different Prompt, Significant Conceptual Changes: A completely different prompt for the same role with significant conceptual changes.This process results in a total of 120 system prompts for hard examples. The system prompt variations and the accompanying task prompts were generated with the Claude 3 Sonnet model. For the hard example experiments, we generated 10 specific probe task queries per each of the original system prompts (see Appendices A,F)."}, {"title": "4.2 MODELS", "content": "We conduct our experiments with a variety of open-source and API-based models, including Llama2 13B (Touvron et al., 2023), Llama3 70B 5, Mistral 7B (Jiang et al., 2023), Mixtral 8x7B (Jiang et al., 2024), Claude 3 Haiku (Anthropic, 2024a), and GPT-3.5 (Achiam et al., 2023)."}, {"title": "4.3 EVALUATION: STANDARD AND HARD EXAMPLES", "content": "In the standard setup, to evaluate Prompt Detective, we construct pairs of system prompts representing two scenarios: (1) where the known system prompt p is indeed used by the language model (positive case), and (2) where the known system prompt p differs from the system prompt p used by the model (negative case). The positive case simulates a situation where the proprietary prompt has been reused, while the negative case represents no prompt reuse."}, {"title": "5 RESULTS", "content": ""}, {"title": "5.1 PROMPT DETECTIVE CAN DISTINGUISH SYSTEM PROMPTS", "content": "Table 1 shows the effectiveness of Prompt Detective in distinguishing between system prompts in the standard setup across different models and prompt sources. We report the false positive rate (FPR) and false negative rate (FNR) at a standard p-value threshold of 0.05, along with the average p-value for both positive and negative prompt pairs. In all models except for Claude on AwesomeChatGPT dataset, Prompt Detective consistently achieves a zero false positive rate, and the false negative rate remains approximately 0.05. This rate corresponds to the selected significance level, indicating the probability of Type I error \u2013 rejecting the null hypothesis that system prompts are identical when they are indeed the same. Figure 3 shows how the average p-value changes in negative cases (where the prompts differ) as the number of task queries increases. As expected, the p-value decreases with more queries, providing stronger evidence for rejecting the null hypothesis of equal distributions. Consequently, increasing the number of queries further improves the statistical test's power, allowing for the use of lower significance levels and thus ensuring a reduced false negative rate, while maintaining a low false positive rate.While there are no existing prompt membership inference baselines, prompt reconstruction methods can be adapted to the prompt membership inference setting by comparing recovered system prompts to the reference system prompts. We compare PLeak (Hui et al., 2024) one of the most high performing of the existing prompt reconstruction approaches to Prompt Detective in the prompt membership setting. We find Prompt Detective to be significantly more effective in the prompt membership inference setting and report the results in Table 5 of Appendix B.1."}, {"title": "5.2 HARD EXAMPLES: SIMILAR SYSTEM PROMPTS", "content": "Table 2 presents the results for the challenging hard example setup, where we evaluate Prompt Detective's performance on system prompts with varying degrees of similarity to the proprietary prompt. We conduct this experiment with Claude 3 Haiku and GPT-3.5 models, testing Prompt Detective in two scenarios. First, we use 2 generations per task prompt, resulting in 20 generations for each system prompt, as in the standard setup Anthropic Library experiments. Second, we use 50generations for each task query, resulting in 500 generations per system prompt in total. We observe that when only 2 generations are used, the false positive rate is high reaching 65% for GPT 3.5 and Claude models in Similarity Level 1 setup, indicating the challenge of distinguishing the response distributions for two very similar system prompts. However, increasing the number of generations for each probe to 50 leads to Prompt Detective being able to almost perfectly separate between system prompts even in the highest similarity category.We further explore the effect of including more generations and more task prompts on Prompt Detective's performance. In Figure 4, we display the average p-value for Prompt Detective on Similarity Level 1 pairs versus the number of generations, the number of task prompts, and the number of tokens in the generations. We ask the following question: for a fixed budget in terms of the total number of tokens generated, is it more beneficial to include more different task prompts, more generations per task prompt, or longer responses from the model? Our observations suggest that while having more task prompts is comparable to having more generations per task prompt, it is important to have at least a few different task prompts for improved robustness of the method. However, having particularly long generations exceeding 64 tokens is not as useful, indicating that the optimal setup includes generating shorter responses to more task prompts and including more generations per task prompt."}, {"title": "6 BLACK BOX SETUP", "content": "So far we assumed the knowledge of the third-party model used to produce generations, and in this section we explore the black-box setup where the exact model is unknown. As mentioned previously, it is reasonable to assume that chat bots which reuse system prompts likely rely on one of the widely used language model families. To simulate such scenario, we now say that all the information Prompt Detective has is that the third party model fp is one of the six models used in our previous experiments. We then compare the generations of fp against each model {f}i=1 used as reference and take the maximum p-value. Because of the multiple-comparison problem in this setup, we apply the Bonferroni correction to the p-value threshold to maintain the overall significance level of 0.05. Table 3 displays the results for Prompt Detective in the black-box setup. We observe that, while"}, {"title": "7 DISCUSSION", "content": "We introduce Prompt Detective, a method for verifying with statistical significance whether a given system prompt was used by a language model and we demonstrate its effectiveness in experiments across various models and setups.The robustness of Prompt Detective is highlighted by its performance on hard examples of highly similar system prompts and even prompts that differ only by a typo. The number of task queries and their strategic selection play a crucial role in achieving statistical significance, and in practice we find that generally 300 responses are enough to separate prompts of the highest similarity. Interestingly, we find that for a fixed budget of generated tokens having a larger number of shorter responses is most useful for effective separation.A key finding of our work is that even minor changes in system prompts manifest in distinct response distributions, suggesting that large language models take distinct low-dimensional \u201crole trajectories\" even though the content may be similar and indistinguishable by eye when generating responses based on similar system prompts. This phenomenon is visualized in Appendix Figure 5, where generations from even quite similar prompts tend to cluster separately in a low-dimensional embedding space."}, {"title": "A ADDITIONAL DETAILS ON SYSTEM PROMPT SOURCES", "content": "AwesomeChatGPT Prompts is licensed under the CC0-1.0 license. The dataset contains 153 role system prompts, for which we constructed 50 universal task prompts used to produce generations. In the default experiments, we produce a single generation per system prompt - task prompt pair. Additionally, we conduct ablations by varying the number of task prompts used, as shown in Figure 3.Anthropic Prompt Library is available on Anthropic's website and follows Anthropic's Terms of Use. We experiment with 20 personal system prompts, for which we construct 20 universal task prompts used to produce generations. In the default experiments, we produce a single generation per system prompt - task prompt pair. Additionally, we conduct ablations by varying the number of task prompts used, as shown in Figure 3.Anthropic Prompt Library \u2013 Hard Examples are variations of Anthropic Prompt Library personal system prompts constructed using strategies described in Section 4.1. We craft 10 unique task prompts for each of the 20 original system prompts, as detailed in Table 6. In our experiments, we vary the number of generations per system-task prompt pair from 2 to 50."}, {"title": "B ADDITIONAL RESULTS", "content": "Figure 5 provides a visual representation of the generation distributions for one task prompt across five system prompts of varying similarity levels for Claude. Despite conceptual similarities, the generations from different prompts form distinct clusters in the low-dimensional UMAP projection, aligning with our finding that even minor changes in system prompts manifest in distinct response distributions.In Figure 6 we illustrate the ROC-curves for Prompt Detective computed by varying the sifnificance level a in the standard setup for both Awesome ChatGPT Prompts and Anthropic Library datasets across all models. We observe that Prompt Detective achieves ROC-AUC of 1.0 in all setups except for the Claude model on AwesomeChatGPT prompts.In Table 4 we report results for Prompt Detective on Awesome ChatGPT Prompts dataset in a standard setup with various encoding models used in place of BERT embeddings. In particular, we experimented with smaller models from the MTEB Leaderboard, such as gte-Qwen2-1.5B-instruct from Alibaba, jina-embeddings-v3 from Jina AI and mxbai-embed-large-v1 from Mixedbread. We observe no significant difference in the results compared to the BERT embeddings. Therefore, we opt for using the cheaper BERT encoding model in Prompt Detective for obtaining multi-dimensional presentations of the generations."}, {"title": "B.1 COMPARISON TO PROMPT EXTRACTION BASELINES", "content": "Prompt reconstruction methods can be adapted to the prompt membership inference setting by comparing recovered system prompts to the reference system prompts. We compared PLeak (Hui et al., 2024) \u2013 one of the most high performing of the existing prompt reconstruction approaches to Prompt Detective in the prompt membership setting. We used the optimal recommended setup for real-world chatbots from section 5.2 of the original PLeak paper (Hui et al., 2024)we computed 4 Adversarial Queries with PLeak and Llama2 13B as the shadow model as recommended, and we used ChatGPT-Roles as the shadow domain dataset to minimize domain shift for PLeak. We observed that PLeak sometimes recovers large parts of target prompts even when there is no exact substring match, and that using the edit distance below the threshold of 0.2 to find matches maximizes PLeak's performance in the prompt membership inference setting. To further maximize the performance of the PLeak method, we also aggregate the reconstructions across the 4 Adversarial Queries (AQs) by taking the best reconstruction match (this aggregation approach is infeasible in prompt reconstruction setting where the target prompt is unknown but can be used to obtain best results in prompt membership inference setting where we know the reference prompt). We then applied these adversarial prompt extraction queries to Llama2 13B as the target model with system prompts from Awesome-ChatGPT-Prompts and computed False Positive and False Negative rates for direct comparison with the results of Prompt Detective reported in Table 1 of our paper. We report the results in Table 5.We see that Prompt Detective significantly outperforms PLeak in the prompt membership inference setting which is expected since Prompt Detective is specifically tailored to work in the verification setup while PLeak is geared towards solving a different problem of prompt reconstruction."}, {"title": "C CASE STUDIES", "content": "We additionally explore two challenging scenarios for distinguishing between system prompts."}, {"title": "C.1 YOU ARE HELPFUL AND HARMLESS AI ASSISTANT", "content": "We explore the effect of the common system prompt first introduced in Llama models, which instructs the model to behave as a helpful, harmless, and honest assistant. Specifically, we consider two versions of this prompt: the standard prompt, \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", and a rephrased version, \"Your role is to provide helpful, respectful, and truthful responses to users. Prioritize being as assistive as possible while operating within ethical boundaries. Avoid generating any content that could be considered harmful, unethical, biased, dangerous, or illegal. Maintain an impartial and positive tone in your outputs. If a query is nonsensical or lacks factual coherence, explain why instead of attempting an incorrect answer. Admit if you lack the knowledge to properly address a question rather than speculating or providing misinformation.\"We then prompt the GPT-3.5 model with these system prompts and 50 task prompts from AwesomeChatGPT Prompts experiments. We generate 5 generations for each task prompt. We consider this a more challenging scenario because neither prompt installs a particular character on the model, and instead asks it to behave in a generically helpful way. Nevertheless, Prompt Detective can separate between these two system prompts with a p-value of 0.0001."}, {"title": "C.2 SYSTEM PROMPT WITH A T\u03a5\u03a1\u039f", "content": "Next, we investigate whether introducing a couple of typos in the prompt leads to a changed \"generation trajectory.\" For this experiment, we take one of the prompts from the Anthropic Library, namely the Dream Interpreter system prompt, and introduce two typos as follows: You are an AI assistant with a deep understanding of dream interpretaion and symbolism. Your task is to provide users with insightful and meaningful analyses of the symbols, emotions, and narratives present in their dreams. Offer potential interpretations while encouraging the user to reflect on their own experiencs and emotions.. We then use the GPT-3.5 model to generate responses to 20 task prompts used in experiments with Anthropic Library prompts. Prompt Detective can separate the system prompt with typos from the original system prompt with a p-value of 0.02 when using 50 generations for each task prompt. This experiment highlights that even minor changes, such as small typos, can alter the generation trajectory, making it detectable for a prompt membership inference attack."}, {"title": "D PROMPT DETECTIVE: DETAILED EXPLANATION OF THE ALGORITHM", "content": "Inputs and Notations\u2022 Third-party language model: fp, prompted with an unknown system prompt p.\u2022 Known proprietary system prompt: p, used with a reference model fp.\u2022 Task prompts: q1, q2, ..., qn, used to query both fp and fp.\u2022 Number of generations per task prompt: k, the number of responses sampled for each task prompt.\u2022 Significance level: a, threshold for hypothesis testing.\u2022 Number of permutations: Npermutations, the number of iterations for the permutation test.Algorithm DescriptionStep 1: Generation of Responses.For each task prompt qi (i \u2208 [1, n]), generate k responses:G\u2081 = {fp(q\u2081)\u00b9, ..., fp(q\u2081)*, ..., fp(qn)\u00b9,..., fp(qn)*},G2 = {fp(q\u2081)\u00b9, ..., fp(q\u2081)*,..., fp(qn)\u00b9,..., fp(qn)k}.Step 2: Encoding GenerationsConvert text responses into high-dimensional vectors using a BERT embedding function \u03c6(\u00b7):V\u2081 = {\u03c6(fp(q\u2081)\u00b9), ..., \u03c6(fp(q\u2081)*), ..., \u03c6(fp(qn)\u00b9), ..., \u03c6(fp(qn)k)},V2 = {\u03c6(fp(q\u2081)\u00b9), ..., \u03c6(fp(q\u2081)*), ..., \u03c6(fp(qn)\u00b9),..., \u03c6(fp(qn)k)}.Step 3: Mean Vector ComputationCompute the mean vectors for V\u2081 and V2:\u03bc1 =$\\frac{1}{|V_1|}$ \u2211v, \u03bc\u03b5 = $\\frac{1}{|V_2|}$ \u2211v,Step 4: Observed Cosine SimilarityCalculate the observed cosine similarity between \u00b5\u2081 and \u00b52:Sobs = cos(\u03bc1, \u03bc2).Step 5: Permutation TestThe goal of this step is to test whether the observed similarity Sobs is significantly different from what would be expected if V\u2081 and V2 were drawn from the same distribution.Procedure:1. Combine Responses: Merge all embeddings into a single set:Vcombined = V\u2081 U V2.2. Shuffle the Combined Embeddings: For each task prompt qi, shuffle the embeddings associated with that prompt:Vcombined[i] = {Vi,1, ..., Vi,k, Ui,1, ..., Ui,k},where vi,j \u2208 V\u2081 and Ui,j \u2208 V2. After shuffling, the embeddings are randomly reordered, eliminating any inherent grouping."}, {"title": "E HARDWARE", "content": "Our experiments were conducted using NVIDIA A10G 24GB GPUs. Although a single run of Prompt Detective for a given system prompt takes only minutes, even with a large number of generations, the total number of GPU hours required to produce the results presented in this paper amounted to approximately 150 GPU hours. These experiments involved three different system prompt sources, black-box experiments, and thorough ablation studies to evaluate the test's performance under varying numbers of task prompts, generations, and generation lengths. We also utilized the corresponding APIs for the commercial models."}, {"title": "F PROMPT TEMPLATES AND EXAMPLES", "content": "Table 6 presents the instructions used with Claude 3 Sonnet for generating task queries and hard examples. Table 7 presents an example of prompts used in experiments with hard examples."}]}