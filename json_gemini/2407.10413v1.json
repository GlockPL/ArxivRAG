{"title": "Melon Fruit Detection and Quality Assessment Using\nGenerative AI-Based Image Data Augmentation", "authors": ["Seungri Yoon", "Yunseong Cho", "Tae In Ahn"], "abstract": "Monitoring and managing the growth and quality of fruits are very important tasks. To effectively train deep\nlearning models like YOLO for real-time fruit detection, high-quality image datasets are essential. However, such\ndatasets are often lacking in agriculture. Generative AI models can help create high-quality images. In this study,\nwe used MidJourney and Firefly tools to generate images of melon greenhouses and post-harvest fruits through\ntext-to-image, pre-harvest image-to-image, and post-harvest image-to-image methods. We evaluated these AI-\ngenerated images using PSNR and SSIM metrics and tested the detection performance of the YOLOv9 model. We\nalso assessed the net quality of real and generated fruits. Our results showed that generative AI could produce\nimages very similar to real ones, especially for post-harvest fruits. The YOLOv9 model detected the generated\nimages well, and the net quality was also measurable. This shows that generative AI can create realistic images\nuseful for fruit detection and quality assessment, indicating its great potential in agriculture. This study highlights\nthe potential of AI-generated images for data augmentation in melon fruit detection and quality assessment and\nenvisions a positive future for generative AI applications in agriculture.", "sections": [{"title": "1 Introduction", "content": "In recent years, rapid advances in artificial intelligence\n(AI) and deep learning technologies have significantly im-\nproved the precision and efficiency of fruit detection and\nquality assessment [17, 2]. Specifically, the deep learning-\nbased object detection algorithm YOLO (You Only Look\nOnce) has demonstrated high performance in real-time de-\ntection, attracting significant attention from researchers\n[26, 25]. Training these deep learning models effectively re-\nquires large-scale, high-quality image datasets [39]. How-\never, in agriculture, there is a serious lack of such large-\nscale image datasets. This shortage is due to the signif-\nicant time and resources needed to create datasets that\ncover crops under various conditions and environments\n[20]. To address this issue, AI-generated images have\nrecently gained attention [23]. Generative AI models\ncan produce high-quality images through text-to-image,\nimage-to-image, and image-to-video transformations [15].\nThis technology has great potential to address the problem\nof limited large-scale image datasets through data aug-\nmentation [29].\nSynthetic images created by computer algorithms have\nbeen widely used in various fields, including healthcare,\nfashion, architecture, geospatial studies, and agriculture\n[5, 3, 8, 22, 27]. Traditional image generation methods,\nsuch as parametric techniques [6], ray tracing [24], and\nphysics-based rendering [13], have advanced synthetic im-\nage technology. However, each of these methods has lim-\nitations: they struggle to adapt to complex shapes [31],\nhave high computational demands and time issues, and\nlack flexibility [11, 10, 28]. In contrast, generative AI of-\nfers substantial benefits for research and development by\nallowing extensive variations and information to be added\nto datasets with significantly less time and cost compared\nto traditional data augmentation techniques [36, 29, 16].\nPrevious studies on synthetic data enhancement for tasks\nsuch as tomato leaf disease classification, pest synthetic\nimage generation, and weed classification [21, 1, 7] show\nthat generative AI is a valuable tool for improving the\nquality of agricultural data sets.\nMuskmelon, prized for its attractive appearance and\nflavor, has a high market value. Throughout its growth\nfrom fruit set stage to harvest, the fruit undergoes sig-"}, {"title": "2 Materials and Methods", "content": "2.1 Cultivation and data collection\nThe cultivation experiment was conducted in a Venlo-type\nglass greenhouse at the Protected Horticulture Research\nInstitute (NIHHS, Haman, Korea). The muskmelon vari-\nety 'Dalgona' (Cucumis melo L., cv. Dalgona) was sown\non May 9, 2023, using groundwater and subsequently\ntransplanted into coir substrates (100\u00d720\u00d710 cm, Duck-\nyang Agrotech, Nonsan, Korea) after three weeks. The\nYamazaki standard nutrient solution suitable for melons\nwas supplied at electrical conductivity levels of 1.8, 2.0,\nand 2.3 dSm-\u00b9 during the early, middle and late stages of\ngrowth, respectively. The drainage ratios for each stage\nwere 40-50%, 30-40%, and 5-10%. Daytime and nighttime\ntemperatures were controlled at 33\u00b0C and 19\u00b0C. Pollina-\ntion was carried out by bees on May 30, 2023, and fruit\nthinning was performed on June 5, 2023. After the fruit\nset between the 11th and 13th nodes, the fruit was thinned\nto leave one fruit per plant, and the plant was pruned at\nthe 23rd node to maintain a single stem.\nThe images of the melon fruits were obtained before\nand after harvest (Fig.1A). The pre-harvest images were\ntaken using a smartphone (Galaxy S22, Samsung Elec-\ntronics Inc., Suwon, Korea) with a resolution of 1920 \u00d7\n1080 pixels under natural light conditions. The camera\nwas positioned to capture the entire fruit from below to\nprovide a comprehensive view. Post-harvest images were\ncollected in a square-shaped studio (W80 \u00d7 L80 \u00d7 H80\ncm) with LED lighting at the top, using the same camera.\n2.2 Generative A\u0399\nTo create melon fruit images, we used two popular gen-\nerative AI tools (Fig.1B): MidJourney (MidJourney Basic\nPlan, MidJourney, San Francisco, CA, US) and Firefly (Fi"}, {"title": "2.2.1 Text-to-image generation", "content": "Text-to-image generation involves carefully capturing the\nvariations in the image through the topic, style, and pa-\nrameters of the prompt. In this study, we used the 'de-\nscribe' command in MidJourney to generate prompts from\nimages and then created images based on these prompts.\nThe images used for the prompts included greenhouse im-\nages of melon cultivation and individual fruit images after\nharvest."}, {"title": "2.2.2 Image-to-image generation (Pre-harvest)", "content": "Compared to text-to-image generation, the image-to-\nimage generation technique references the structure and\nstyle of the original image while enhancing clarity and\nquality, producing accurate and realistic images [28].\nGreenhouse images of melon cultivation were used, en-\ncompassing various stages of fruit growth, camera angles,\nlighting, and focus. These diverse images were used as\nreference images to generate similar structured images."}, {"title": "2.2.3 Image-to-image generation (Post-harvest)", "content": "Melon fruit images focused on individual fruits after har-\nvest were generated for the purpose of evaluating the ex-\nternal quality of the melons."}, {"title": "2.3 YOLOv9", "content": "YOLOv9 is the latest version of real-time object detec-\ntion technology, achieving significant advancements. Re-\nleased in February 2024, this version introduces innovative\ntechnologies such as PGI (Programmable Gradient Infor-\nmation) and GELAN (Generalized Efficient Layer Aggre-\ngation Network). PGI addresses the problem of infor-\nmation being diluted or lost during the forward pass of\nthe neural network, while GELAN emphasizes lightweight\ndesign, fast inference, and accuracy, directly addressing\ninformation bottlenecks [32]. YOLOv9 outperforms cur-\nrent state-of-the-art models in various metrics, maintain-\ning equal or higher accuracy with fewer parameters. In\nthis study, we trained YOLOv9 using a dataset of 700\nreal melon images and tested its detection performance\non AI-generated images representing various environmen-\ntal conditions (Fig.1D)."}, {"title": "2.4 Evaluation methods", "content": "2.4.1 Evaluation of AI-generated images\nPeak Signal to Noise Ratio (PSNR): PSNR is a metric\nused to evaluate image quality by measuring the ratio of\nthe maximum potential power of the signal (represented\nby the original image) to the power of disruptive noise\n(represented by disparities between original and generated\nimages). This ratio is calculated according to Equation 1,\nand the mean squared error (MSE) is estimated accord-\ning to Equation 2. Higher PSNR values indicate that the\ngenerated image is closer to the original image with mini-\nmal distortion [28]. Here, MAX\u2081 represents the maximum\npixel value, and MSE indicates the pixel-level difference\nbetween the generated image (A) and the original image\n(O), with lower MSE values implying smaller differences\nbetween the images.\nPSNR = 10 log\u2081\u2080 ( {MAX\u2081\u00b2 \\over MSE})\n(1)\nMSE = {1 \\over N} \u03a3 (O\u1d62 - A\u1d62)\u00b2\n(2)\nStructural Similarity Index Measure (SSIM): SSIM is\na metric for evaluating the quality of modified or gener-\nated images, similar to PSNR. It assesses the correlation\nbetween two images (x, y) based on three aspects: Lumi-\nnance, Contrast, and Structure [35]. Luminance measures\nthe brightness of light, contrast measures the difference in\nbrightness, and structure measures the correlation. These\nvalues are calculated using the mean, standard deviation,\nand covariance of the pixels in the images.\nSSIM(x, y) = [l(x, y)]\u1d45 \u00b7 [c(x, y)]\u1d5d \u00b7 [s(x, y)]\u1d5e\n(3)"}, {"title": "2.4.2 Accuracy of YOLOv9 for fruit detection", "content": "Intersection over Union (IoU): IoU is a metric that is used\nto evaluate the accuracy of object detection, to determine\nwhether the detection of individual objects is successful.\nIt has a value between 0 and 1. In computer vision and\nobject detection, bounding boxes are typically represented\nas rectangles in 2D images. Based on this representation,\nthe IoU between the actual bounding box (Bg, Ground\ntruth) and the predicted bounding box (Ba) is calculated\nas follows [40]:\nIoU = {Area of overlap between B\u2089 and B\u2090 \\over Area of union between B\u2089 and B\u2090 }\n(4)"}, {"title": "2.4.3 Evaluation of net quality in generated images", "content": "Net quality is a major evaluation metric in human visual\ninspection, evaluated based on the density and uniformity\nof the net pattern (Fig.1E). To visualize the differences in\nnet quality, 20 fruits with high net quality were selected,\nand 20 generated images were classified. In this study, to\ndistinguish it from the net, the non-netted exocarp region\nis referred to as skin. The masks containing only the fruit\nwere then extracted from each dataset. Using a computer\nvision algorithm, the color of the skin and net were distin-\nguished, and the average area (net density) and standard\ndeviation (net uniformity) of the skin fragments in pixels\nwere calculated [37]."}, {"title": "3 Results and Discussion", "content": "3.1 Evaluation of AI-generated images\ndatasets and the original images (Fig.5). The evalua-\ntion results showed that the average and standard devi-\nation of PSNR values for text-to-image, image-to-image\n(pre-harvest), and image-to-image (post-harvest) dataset\ngroups were 27.4\u00b10.6, 27.9\u00b10.05, and 28.8\u00b10.3, respec-\ntively (Table 1). The SSIM values, which indicate a higher\nsimilarity to the original image when closer to 1, were\n0.06\u00b10.02, 0.12\u00b10.03, and 0.42\u00b10.1. These results sug-\ngest that images generated through text and pre-harvest\nreferences showed a high similarity to the original images,\nbut also had significant changes in brightness, contrast,\nand structure. In contrast, data augmentation using post-\nharvest individual fruit images resulted in high similar-\nity to the original images, while also capturing structural\nfeatures accurately due to the limited background and fo-\ncused fruit content."}, {"title": "3.2 Fruit detection using AI-generated images", "content": "riety of colors, camera angles, and environmental condi-\ntions (e.g., sunny days, cloudy days). The key factors\ninfluencing the detection and classification performance\nof deep learning models include the quality of image data,\nthe image capture conditions, and the hyperparameter set-\ntings of each model [33]. However, agricultural environ-\nments pose challenges for collecting high-quality data due\nto dense planting, mutual shading by leaves and crops,\nintense lighting from sunlight, or shading by greenhouse\nstructures. Therefore, overcoming false positives caused\nby these structural features of agricultural settings is a\ncritical task for researchers. Large datasets that include\na wide range of object classes, lighting conditions, and\nbackgrounds help the model learn extensive features [38].\nFor example, common datasets like COCO, ImageNet, and\nPascal VOC contain millions of annotated images, cover-\ning various categories and conditions [34]. In this context,\ngenerative AI offers the advantage of recreating diverse\ncultivation environments without the constraints of time\nand place, maximizing the quantity and diversity of data\navailable for model development."}, {"title": "3.3 Quality assessment of fruits using AI- generated images", "content": "In this study, we applied a methodology to analyze the\npores of muskmelons using computer vision technology\nand quantified net density and uniformity to assess quality\n[37]. Fig. 7AB shows representative original images with\nvalid masks and overlapping Regions of Interest (ROI),\ncomparing the nets of high-quality real fruits with those\nof generated images. Through binarization techniques, we\nwere able to highlight areas of the skin that contrasted\nwith the net as white, although complete separation was\nchallenging (Fig.7C). Using the depth estimation tech-\nnique, we detected shallow and deep areas in 2D images,\nallowing us to distinguish between the net and the skin\n(Fig.7D). Small red-pixel patches of skin were considered\nindividual islands, with lower individual area values in-\ndicating higher net density, and lower standard devia-\ntion of the area values indicating higher net uniformity\n(Fig. 7E). This allowed us to quantify the net quality of\nboth real and generated fruits (Fig. 7F). The nets in the\nAI-generated images detailed specific features of real fruit,\nsuch as bumps, cracks, and colors. Since the fruits used\nin the experiment were selected for their high net qual-\nity, both net density and uniformity were high (Fig.7F).\nThe generated fruits showed lower net density and greater\nuniformity variation, resulting in lower quality evaluations\ncompared to real fruits. However, it is encouraging that\nthe AI-generated images were realistic enough to be used\nin actual net quality assessment methods."}, {"title": "3.4 Limitations and implications", "content": "detailing the melon hydroponic cultivation system and\nthe external quality of post-harvest fruits. Beyond melon\nfruits, generative AI can be used to collect images of var-\nious crops at different stages of vegetative and reproduc-\ntive growth, accurately depict surface conditions to cap-\nture physiological disorder images, and obtain images of\nfruits segmented by quality grades based on marketabil-\nity. This means that high-quality AI-generated data can\nimprove the performance of automated models related to\ngrowth monitoring, pest diagnosis, shipment, and distri-\nbution management, in addition to fruit detection and\nclassification.\nWith the rapid advancement of AI generation tools,\nthe results of this study are expected to contribute to the\napplication of generative AI in the fields of agriculture and\nplant science."}, {"title": "4 Conclusion", "content": "In this study, we used the tools Midjourney and Firefly to\ngenerate images of melon cultivation in greenhouses and\npost-harvest fruit through text-to-image, image-to-image\n(pre-harvest), and image-to-image (post-harvest) meth-\nods. The generated images were evaluated using metrics"}]}