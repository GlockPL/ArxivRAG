{"title": "Creating a Lens of Chinese Culture: A Multimodal Dataset for Chinese Pun Rebus Art Understanding", "authors": ["Tuo Zhang", "Tiantian Feng", "Yibin Ni", "Mengqin Cao", "Ruying Liu", "Katharine Butler", "Yanjun Weng", "Mi Zhang", "Shrikanth S. Narayanan", "Salman Avestimehr"], "abstract": "Large vision-language models (VLMs) have demonstrated remarkable abilities in understanding everyday content. However, their performance in the domain of art, particularly culturally rich art forms, remains less explored. As a pearl of human wisdom and creativity, art encapsulates complex cultural narratives and symbolism. In this paper, we offer the Pun Rebus Art Dataset, a multimodal dataset for art understanding deeply rooted in traditional Chinese culture. We focus on three primary tasks: identifying salient visual elements, matching elements with their symbolic meanings, and explanations for the conveyed messages. Our evaluation reveals that state-of-the-art VLMs struggle with these tasks, often providing biased and hallucinated explanations and showing limited improvement through in-context learning. By releasing the Pun Rebus Art Dataset, we aim to facilitate the development of VLMs that can better understand and interpret culturally specific content, promoting greater inclusiveness beyond English-based corpora.", "sections": [{"title": "Introduction", "content": "Each culture develops its unique symbolic systems of visual elements, which are conventionally understood within that culture to convey specific meanings. For example, to viewers unfamiliar with Chinese arts and linguistics, the combination of a monkey and a horse might seem nonsensical. However, in Chinese culture, \"a monkey lying on top of the horse\" is described as a pun on \"\u9a6c\u4e0a\u5c01\n\u4faf\" (m\u0103 sh\u00e0ng f\u0113ng h\u00f3u)2, representing the wish for promotion. This form of wordplay is prevalent in Chinese decorative arts, appearing in various art formats throughout Chinese history, from the emperor's court to the commoners' kitchen, transcending boundaries of power, wealth, education, and media. As an example, in Figure 1, we demonstrate a Chinese pun rebus painting with \"a monkey lying on top of the horse,\" which indicates the wish for promotion by connecting homophonically similar Chinese characters of \"horse-\u9a6c(m\u0103),\" \"on top of-\u4e0a(sh\u00e0ng),\" combined to form 'mashang' also meaning 'right away', and \"monkey-\u7334(h\u00f3u), sounding similar to \u4faf(h\u00f3u) for \u2018marquis.\"\nIn this work, we propose the Pun Rebus Art Dataset, which is rooted in traditional Chinese culture. We focus on Chinese Pun Rebus art for three major reasons: 1) creating a pun rebus artwork involves combining textual meanings with corresponding visual representations, making it naturally multimodal; 2) pun rebus is prevalent in Chinese art, rarely seen in other cultures such as western painting [19]; 3) pun rebus art remains widespread in contemporary Chinese culture, demonstrating its enduring impact and lasting value to preserve cultural identity while engaging new generations."}, {"title": "General Framework for Pun Rebus Understanding", "content": "A pun rebus in Chinese culture leverages visual elements to indicate an underlying expression, metaphor, or meaning that is seemingly unrelated to the given image [19, 20]. The fundamental mechanism of pun rebuses hinges on the interplay between the imagery composed, on the one hand, and the semantic and phonetic components of the Chinese logographs used to express a message, usually auspicious. Specifically, the interpretation of pun rebuses relies on homophonic associations between the names of the depicted images (or their interactions) and the Chinese characters (logographs) used to express the concepts that form the intended message, either partially or fully. The names of the objects in a pun rebus are often homophonically similar to, or even identical with, the cued expression, analogous to using the English string \u2018eye\u2014can\u2014sea\u2014ewe' to express 'I can see you'. A pun rebus design is intended to initiate a cognitive translation process of \"image-sound-sound-meaning,\" contrasting sharply with the more direct and straightforward 'text-meaning' decoding typically observed in pure verbal understanding. Because the process is not only culturally but also linguistically specific, it is extremely challenging for an uninformed viewer to perceive and decipher any underlying meanings of this art form. These artworks are composed for aesthetic or attention-attracting purposes.\nGenerally, the chain of thought on understanding the pun rebus is composed of three sequential steps: (1) spotting the salient visual elements within the artwork; (2) utilizing these identified elements to formulate the underlying pun; (3) understanding the intended message or wish conveyed by the pun rebus. We present a visualized example in Figure 1 as an illustration of pun rebus understanding."}, {"title": "Pun Rebus Art Dataset", "content": "The Pun Rebus Art dataset is designed as a comprehensive benchmark for exploring the intersection of image analysis, morphological variation, and phonological elements within the context of Chinese linguistics and cultural artifacts. This dataset is the result of extensive efforts to curate a diverse array of historical artwork documents. Initiated in 1987 by Dr. Ni Yibin, a co-author of this paper, the dataset's preparation involved meticulous collection, annotation, and verification processes that require expert knowledge of Chinese art, literature, history, and linguistics. The corpus comprises 1,011 captioned images sourced predominantly from globally-renowned Chinese-art-collecting institutions, including the Palace Museum, the Metropolitan Museum of Art, and the British Museum. The images in this dataset are subject to the Creative Commons Zero (CC0) license. Spanning over two millennia, from the Han Dynasty (206 BCE \u2013 220 CE) to the 20th century, the dataset encompasses a rich diversity of more than ten different media types, including paintings, ceramics, bronzes, sculptures, jade, Cloisonn\u00e9, lacquerware, and embroidery. The collection of the Pun Rebus Art dataset is ongoing as we continue to curate it with additional artworks to enhance its representational diversity."}, {"title": "Data Annotation", "content": "Each entry has been meticulously annotated by human experts with knowledge of Chinese linguistics, art, and history. Figure 2 exemplifies the structured content in the Pun Rebus Art dataset. Each entry comprises the following components: (1) the original artwork without its caption; (2) the articulated pun rebus, presented bilingually to encompass both the original Chinese script and its English counterpart; (3) the salient elements that constitute the pun's design; and (4) an analysis delineating the relationship between the visual representation and the intended pun rebus. To ensure high-quality annotations, we implement a strict three-round validation check after the initial annotating process."}, {"title": "Task Setups", "content": "Based on the characteristics of the Chinese pun rebus artwork, we present three primary and progressive tasks in this paper: Element Identification, Symbolic Matching, and Expression Understanding. We want to highlight that the researchers are highly encouraged to explore additional applications and analyses tailored to their specific interests and needs using this dataset. In the following, we describe the details of each task and the corresponding evaluation metrics."}, {"title": "Task Design", "content": "In the initial task, we aim to explore: What catches the model's attention most in the artwork? Artworks are complex composites of features such as texture, shape, color, and other painting elements. However, not all these features are essential for constructing the pun embedded within the artwork. This task seeks to determine which elements the model prioritizes from its perspective. For instance, consider the artwork shown in Figure 3: a ceramic jar made in the"}, {"title": "Evaluation Metrics", "content": "For the element identification, we report the absolute score and the similarity score. The absolute score represents the overlap between key elements in the model's output and those in the ground truth. Let G = {91, 92, ..., gn} represent the set of elements in the ground truth description, and P = {P1, P2, ..., pm} denote the set of elements identified by the language model. The absolute score for a single instance, S, is calculated as follows:\n$S_{Abs}(G, P) = \\frac{|G \\cap P|}{|G|}$ (1)\nIt quantifies the extent to which essential elements are captured in the model's output, normalized by the total number of elements in the ground truth. For the overall performance across the dataset, we report the average score, SAbs, computed as the mean of individual scores across all test instances.\nApart from the absolute score, we introduce the similarity score to account for semantic equivalence, which considers synonyms and semantically related terms that align with the ground truth. We map both ground truth answers G and generated answers P to word embedding using the pre-trained Sentence-BERT [13]. For each test instance, we measure word-wise cosine similarity between each element in G and all elements in P, recording the highest similarity score for each element in G. The similarity score for each instance is the average of these maximum scores for all elements in G:\n$S_{sim}(G, P) = \\frac{1}{|G|} \\Sigma_{g \\in G} max_{p \\in P} cos(emb(g), emb(p))$ (2)\nwhere emb(x) denotes the embedding of the element x, and cos denotes the cosine similarity function. We report the average score Ssim for the overall performance of the dataset.\nFor the symbolic matching, we evaluate using the accuracy. It is worth noting that certain artworks may convey multiple implied meanings among the options provided. An answer is considered correct if it includes at least one implied meaning specified in the ground truth.\nFinally, we conduct human evaluation to judge the expression understanding. The panel of human judges consists of five individuals: three authors of this paper and two independent experts with educational and professional backgrounds in the field of art history. We ask each judge to grade the model-generated explanations on a scale from 1 to 10. A score of 10 represents a perfect explanation, indicating that the human judge cannot distinguish whether the answer is from the machine or a human expert. A score of 1 signifies that the response is completely incorrect and irrelevant. We also list our findings and hypothesis from human evaluations in Section 5.3."}, {"title": "Experiments", "content": "We evaluate the performance of various widely used VLMs using the Pun Rebus Art dataset. Our evaluation is conducted under both zero-shot and five-shot settings to examine the inherent ability without fine-tuning specific to our dataset. Specifically, we aim to probe the ingrained knowledge and reasoning processes of these models, exploring their potential limitations or biases in interpreting objects and concepts related to Chinese culture. This is particularly pertinent to ensure the inclusiveness of VLMs given that most models are predominantly trained on English-based resources, which may affect their performance on culturally specific tasks [21, 8]. We use the unified prompt for each task across all models, which are listed in Appendix. We sample with default hyperparameters in all cases. All experiments are conducted with NVIDIA A100 GPUs."}, {"title": "Baselines", "content": "Our selection prioritizes the largest, most recent, and highest-performing VLMs currently available. Our selection comprises: (1) The GPT-4 model family [1]. We include both GPT-40 and GPT-4V in our benchmark. (2) The Gemini 1.0 Pro Vision [17] is the only multimodality LMM available for public usage among the Gemini model family. (3) The Claude 3 model family [2]. Our benchmark includes Claude 3 Opus, Sonnet, and Haiku. (4) The Qwen-VL family [3]. It is worth noting that its training incorporates the Chinese image-text data corpus, making it more relevant to benchmark our dataset. We incorporate both Qwen-VL-Plus and Qwen-VL-Max in our evaluation. For all models listed, we utilize the latest model checkpoint available at the time of writing this paper. Detailed checkpoint information and version specifics are provided in Appendix.\nFollowing the previous study [5], we include an evaluation of human performance to compare with the VLMs. Unlike the expert panel described in Section 4.2, we enlist crowd-workers who lack a specialized background in Chinese art, representing the general population's understanding. Specifically, the panel consists of 3 bilingual individuals, all native Chinese speakers who are also fluent in English. Each participant will review 50 artworks and respond to the questions related to symbolic matching and element identification. These artworks are randomly selected in the full dataset with the same label distribution. We report the average scores across participants as the human performance estimates. We want to note that the human performance in this paper should not be considered as an upper bound for VLMs. Instead, it is used to measure how well ordinary people raised in contemporary Chinese society understand traditional Chinese arts."}, {"title": "Main Results", "content": "In this section, we compare different VLMs through a zero-shot evaluation of the Pun Rebus Art benchmark, as detailed in Table 1. We make five key observations:\n(1) The challenging nature of the Pun Rebus Art dataset. We observe that the highest accuracy in symbolic matching achieved under the zero-shot setting is around 40% for all models. Notably, the human estimation also averages only around 55%, underscoring the difficulty of understanding the symbolic meaning in the art. As we stated in Section 3, the Pun Rebus Art dataset spans artwork ranging over 2000 years, where many visual representations or underlying narratives may have lost their prominence in contemporary Chinese culture. Moreover, to correctly understand an artwork, VLMs must first identify the key elements and then connect these elements into a coherent story.\n(2) The Pun Rebus dataset extends beyond the knowledge scope of VLMs. The relatively low scores observed in the element identification reveal that the tested VLMs fail to understand the Pun Rebus artworks, leading to 50% of the key elements being missed in the recognition. The even lower accuracy in the symbolic matching reflects VLMs' sparse knowledge of Pun Rebus-related content, demonstrating their lack of sufficient knowledge and reasoning ability to transfer the identified key elements into the conveyed meanings. The substantial historical span of the dataset, combined with the struggling performance observed in our evaluations, indicates that the cultural and linguistic content within these artworks extends beyond the training knowledge of the tested models.\n(3) Element recognition versus cultural interpretation limits VLMs. The VLM with high symbolic matching accuracy, such as GPT-40, also scores well in element identification. However, VLMs like Claude 3 Opus score high in element recognition but struggle with symbolic understanding. For example, as we showed in Table 1, Claude 3 Opus identifies bok choy in the artwork but fails to link it to moral integrity, a symbol in Chinese culture due to its similar pronunciation with 'incorruptible.' This highlights a critical aspect of VLM's performance: translating visual recognition into meaningful cultural interpretation. In Appendix, we detail 12 distinct mechanisms used in Chinese culture to derive symbolic meanings from visual elements, including puns, shapes, numerals, and aliases.\n(4) GPT-4o demonstrates superior performance compared to other models. Notably, GPT-40 largely outperforms other models in our evaluation, including GPT-4V. This improvement is partly due to enhanced visual recognition abilities, as evidenced by higher element identification scores achieved by GPT-40 compared to GPT-4v. Other factors, such as the integration of end-to-end multimodal learning techniques in GPT-40, may also lead to a more effective interpretation of complex visual and textual information. Despite these notable improvements, the precise factors contributing to the improved performance of GPT-40 remain unclear to us.\n(5) The impact of using Chinese image-text data corpus in the Pre-training of VLMs. Among the tested models, only the Qwen-VL family publicly announced substantial Chinese data in their training corpus. Our Pun Rebus dataset is naturally bilingual, with content rooted in Chinese culture and questions posed in English. The Qwen-VL-Max achieved the second-highest accuracy in symbolic matching, only below GPT-40. Examination of Qwen model responses in element identification showed that 18.99% of Qwen-VL-Max and 17.90% of Qwen-VL-Pro responses were in Chinese characters. This language mismatch contributed to their relatively low scores in element identification, as the ground truth answers were in English. Appendix includes examples of these responses with corresponding artwork images. Human inspection further found that Chinese responses predominantly occurred with images deeply embedded in Chinese culture, such as traditional ink paintings or fable stories. We speculate that the Qwen models were exposed to Chinese culture-related image-text pairs"}, {"title": "Evaluation under Few-shot Settings", "content": "We also evaluate the in-context learning ability of models using a 5-shot prompt on the Pun Rebus dataset. Specifically, we select the best-performing models from each model family: GPT-40, Claude 3 Opus, and Qwen-VL-Max. We do not include Gemini Pro because the currently publicly available API only supports interleaved images as a few shot prompts but not the multiple image input as the other model. The results are presented in Table 1. We make two key observations:\n(1) Marginal improvements with five-shot prompting. With five-shot prompting, we observed slight increases in the symbolic matching performance of GPT-40 and in the element identification performance of both Claude 3 Opus and GPT-40. The prompt directly illustrates what the elements look like and highlights which elements are important to the conveyed meaning, leading to improved performance in element identification. However, element identification is inherently simpler and requires less reasoning compared to symbolic matching. Symbolic matching is more complex, as the model must identify the mechanisms to integrate the spotted elements into coherent stories. The prompts provided answers but did not explain the underlying mechanisms, resulting in minimal improvement in symbolic matching. In some cases, the performance is even lower compared to zero-shot settings, as the model could not understand the reasoning behind the prompts.\n(2) Hallucination and Shortcuts Exploitation to the in-context examples. With Qwen-VL-Max, we observe the performance decreases in all tasks under the five-shot settings. With human inspection of the element identification responses, we found that the word \"Pheasant\" appeared 317 times, approximately 31.35% of all answers. In our provided prompt, we included an example labeled \"Quail.\" Biologically, quails belong to the pheasant family. We speculate that the behavior of the large VLMs is associated with the \"lazy learners\" phenomenon discussed in [16], where the VLMs frequently exploit shortcuts in in-context examples for downstream tasks. This leads the model to incorrectly identify various elements as \"Pheasant,\" regardless of whether the artwork depicted humans, flowers, or other animals. These observations suggest that the VLMs tend to exploit shortcuts to the in-context examples, resulting in the generation of hallucinated answers that are close to the few-shot examples, which could be the major reason for the decline in performance."}, {"title": "Human Evaluation and Error Analysis", "content": "Our expert judges reviewed the expression understanding generated by GPT-40 and Gemini Pro. We randomly selected 50 responses from each VLM, ensuring the samples maintained the same category distribution as the full dataset. Overall, GPT-40 received an average score of 3.47, while Gemini Pro received an average score of 3.01 from the expert judges. The expert judges make two key observations from the reviews:\n(1) Reasons of errors in expression understanding. The primary issue is incorrect recognition or missing salient elements. For example, both models failed to recognize a persimmon in one artwork, mistakenly identifying it as a peach, reflecting the challenges in element identification shown in Table 1. Secondly, even when VLMs correctly identified elements, they often misunderstood the conveyed meaning. As shown in Figure 4, Gemini recognized fish but completely missed its pun. Also, Gemini tends to fabricate things that do not appear in the pun rebus designs. Lastly, in some cases, the VLMs achieved an expert-level understanding but selected an incorrect option.\n(2) Potential bias in VLMs. Experts noted potential bias in the generated answers. When VLMs fail to recognize an element, they tend to link it to common symbols in Chinese culture, specifically bats, peaches, pine trees, and rocks, which are frequently used to represent longevity and good luck. They often defaulted to associating uncertain elements with these four elements based on shape similarity. For example, they might interpret long, tree-shaped elements as pine trees and round-shaped elements as peaches based on shape similarity. Additionally, VLMs frequently associate the artwork with positive themes such as happiness, longevity, or wealth. Consequently, both VLMs performed poorly when interpreting artworks intended to express themes related to moral integrity or societal harmony."}, {"title": "Error Analysis", "content": "In this section, we conduct a deeper analysis of the key observations made by the experts. Our discussion addresses the following three questions:\n(1) Is computer vision a bottleneck for understanding artworks? We evaluated the models on text-only questions, providing only the story name conveyed by each artwork for symbolic matching. Each model achieved over 80% accuracy. However, when images were included, accuracy dropped to below 45% for all models. These results suggest that while the models can understand the meaning of the story, they struggle to visualize what the story looks like or is composed of when interpreting the actual artwork.\n(2) What is the model's preference in understanding? We analyzed the label-wise performance and the confusion matrix for incorrect symbolic matching answers for GPT-40, GPT-40 achieves the lowest performance on options related to moral integrity and societal harmony, with accuracies around 20%, mirroring expert observations. The confusion matrix shows that the model tends to favor option D, which relates to fecundity, among the erroneous choices.\n(3) Would fine-tuning help? We create a custom GPT-4V to explore. We compiled 68 pieces of artwork with their annotations into a single document and uploaded it to the ChatGPT web page to build a customized Pun Rebus GPT-4V model. Since the model could only be accessed through the web page, we conducted a small-scale evaluation with 120 different artworks. The customized GPT-4V achieved a symbolic matching accuracy of 66%, demonstrating the potential benefits of fine-tuning."}, {"title": "Related Works", "content": "Recent advancements in VLMs have spurred interest in enabling models to interpret culturally rich content. Researchers have begun to evaluate cultural commonsense [14], culturally diverse facts [9, 7], and cultural moral norms [12] in LLMs. These works discover LLMs have limited culturally specific knowledge and frequently output culturally biased responses to human prompts. Some studies on multicultural visual recognition have explored improving recognition performances for food [11], heritage [4], and clothing [6] in culturally diverse contexts. However, these works primarily focus on enhancing cultural understanding within a single modality. A more relevant effort to our proposed dataset is the MaRVL dataset [10], aiming to evaluate multicultural reasoning abilities in VLMs."}, {"title": "Computational Pun and Pun Rebus Understanding", "content": "Computational pun understanding has been extensively studied in NLP in the last decade, with efforts made to design language models for pun detection and comprehension [22, 15]. More recently, researchers have investigated the abilities of LLMs in understanding puns [18], demonstrating their capability to recognize and explain puns, although generating humorous puns remains challenging. However, the understanding of pun rebus, which requires both visual recognition and language reasoning, has not been extensively studied in evaluating VLMs. To the best of our knowledge, the closest work related to our proposed dataset is the humor understanding from the image presented in [5], which shows that VLMs struggle to recognize the humorous elements of the visual content."}, {"title": "Limitations", "content": "While our step-by-step error analysis provides valuable insights into the performance of VLMs on pun rebus understanding, it lacks an in-depth examination regarding the nuanced mechanisms within pun rebuses that may influence model performance. For example, we have not analyzed how the attribution of the elements (e.g., quantities, positions, etc.) in the artwork affects the models' reasoning abilities. We plan to continue collaborating with art historians to annotate each sample in the dataset with mechanism details and address this analysis in future studies. Additionally, our database contains a substantial collection of ceramic arts, which are 3D objects. However, we have only used the front image for testing, thereby ignoring their 3D characteristics. Addressing this limitation is crucial for a comprehensive understanding of these artworks. We plan to incorporate the 3D aspects of these objects in our future studies. Moreover, the expression understanding results were primarily reviewed by expert judges. While this ensures a high level of expertise, it is worth incorporating more crowdsourcing efforts to evaluate VLM's explanations to understand how different groups perceive VLM answers. This would further help identify discrepancies in understandings between experts and non-experts, shedding light on potential biases in VLM outputs."}, {"title": "Conclusions", "content": "In this work, we offer the Pun Rebus Art dataset and evaluate whether state-of-the-art VLMs can interpret Chinese culture and artworks. Our findings reveal that: 1) Current VLMs struggle to spot the salient visual elements in the Chinese Pub Rebus Arts, though they outperform ordinary humans; 2) Due to the knowledge gap in cultural understanding, VLMs face challenges in transferring the spotted elements into their underlying auspicious meaning or matching the symbolic meanings; 3) We also observe substantial limitations in VLM's ability to provide coherent explanations for interpreting Chinese Pun Rebus Arts. The responses provided by these VLMs often exhibit biases towards fixed objects and include significant hallucinations; 4) In-context learning does not effectively guide VLMs to improve their performance in pun rebus art understanding.\nIn the future, a promising area of research will be developing effective data curation to incorporate more diverse and cross-cultural knowledge into the training and evaluation processes of VLMs. This approach holds promise for making VLMs more inclusive and universally beneficial, enhancing their ability to understand and interpret various cultures."}, {"title": "Appendix", "content": "The Pun Rebus Art dataset is designed as a comprehensive benchmark for exploring the intersection of image analysis, morphological variation, and phonological elements within the context of Chinese linguistics and cultural artifacts. This dataset is the result of extensive efforts to curate a diverse array of historical artwork documents.\nThe Pun Rebus Art Dataset was created and collected by Dr. Ni Yibin, a co-author of this paper.\nInitiated in 1987 by Dr. Ni Yibin, a co-author of this paper, the dataset's preparation involved meticulous collection, annotation, and verification processes that require expert knowledge of Chinese art, literature, history, and linguistics. The corpus comprises 1,011 captioned images sourced predominantly from globally-renowned Chinese-art-collecting institutions, including the Palace Museum, the Metropolitan Museum of Art, and the British Museum. Spanning over two millennia, from the Han Dynasty (206 BCE \u2013 220 CE) to the 20th century, the dataset encompasses a rich diversity of more than ten different media types, including paintings, ceramics, bronzes, sculptures, jade, Cloisonn\u00e9, lacquerware, and embroidery. The images of these artworks are stored in the dataset in the Joint Photographic Experts Group (JPEG) format.\nThe Pun Rebus Art dataset could be accessed. The code for reproducing the results of this paper is available. It is worth noting that the category information for each data sample is stored in the GitHub link.\nThe collection of the Pun Rebus Art dataset is ongoing as we continue to curate it with additional artworks to enhance its representational diversity. We welcome researchers and enthusiasts interested in this program to join us in expanding and improving this valuable resource.\nThe images and their annotation in this dataset are subject to the Creative Commons Zero (CC0) license."}, {"title": "Symbolic Imagery Mechanism", "content": "In this section, we briefly describe the mechanisms behind pun rebus in visual artworks. Through our investigation, we have identified and summarized 12 distinct mechanisms that form a symbolic imagery as followings:\nSymbolic. Using the images of people/objects in the artwork as symbols.\nPun. Using homophones of names of people/objects in the artwork.\nShape. Using the shape attributes of objects in the artwork.\nLength/Size. Using the length or size attributes of objects in the artwork.\nColor. Using the color attributes of objects in the artwork.\nFigure. Using the names of people/objects in the artwork.\nAlias. Using aliases and polyphonic characters for people/objects in the artwork.\nNumeral. Using the quantity of visual elements in the artwork.\nVerb. Using verbs triggered by specific actions in the artwork events.\nPreposition. Using prepositions triggered by spatial relationships in the artwork events.\nCharacter. Using pictographic Chinese characters appearing in the artwork.\nLoanword. Using borrowed Chinese characters or radicals from the names of people/objects appearing in the artwork (names of people/objects in the artwork sound the same and share characters with the intended meaning)"}, {"title": "Experiment Details", "content": "For all models listed in this work, we utilize the latest model checkpoint available at the time of writing this paper. Specifically, for GPT-40, we used gpt-4o-2024-05-13 model; for GPT-4V, we used gpt-4-vision-preview model. For Gemini model, we used Gemini 1.0 Pro Vision. For Claude 3 model family, we used claude-3-opus-20240229, claude-3-sonnet-20240229, and claude-3-haiku-20240307. For Qwen-VL model family, we used qwen-vl-plus and qwen-vl-max.\nAll experiments are performed on two computing servers with ten GPUs. The server is equipped with AMD EPYC 7502 32-Core Processor and 1024G memory. The GPU is NVIDIA RTX A100. For models with API access, we just run the inference with CPUs.\nFor symbolic matching task, we used the following prompt for all models:\nThis is a traditional Chinese artwork that likely conveys its ideas,\nthoughts, or wishes through symbolic, punning, shape, color, figure,\nnumeral, verb, preposition, character, loanword or alias through the\nartwork. \nCarefully analyze the visual elements present in the artwork and select\nthe option from the list below that best aligns with its conveyed\nmeaning: \n\nA. Longevity and Good Health \n \nB. Happiness, Joy, Good Luck \n \nC. Prestige, Promotion, and Good Exam Results \n \nD. Fecundity, Harmonious Relationship and Family \n \nE. Wealth or Prosperity \n \nF. Moral Integrity, Eremitism \n \nG. Peace and Protection from Evil, Societal Harmony \n \nYou must make a selection using the option above in your response. Your\nresponse should start with the chosen letter that best matches the\nword's meaning based on a precise and sound justification for your\nselection. Please do not include your justification in your response.\nFor element identification, we used the following prompt for all models:\nPlease analyze the provided image carefully to identify key visual\nelements. Focus on components that traditionally have symbolic meaning\nin the cultural context from which the artwork originates.\nLook for elements that might represent ideas, virtues, or wishes,\nespecially those commonly found in nature or historical motifs.\nFor instance, in Chinese culture, certain animals and plants are known\nto symbolize specific messages when depicted in art. \nBased on these principles, identify the primary visual elements in the\nimage that are likely used to convey a message or a wish.\nPlease list the discernible elements present in the image, excluding\nany assumptions about elements not clearly visible.\nPlease answer the question in one line with the following format\nstrictly: name of element A, name of element B, etc\nFor expression understanding, we used the following prompt for all models:\nThis is a traditional Chinese artwork that likely conveys its ideas,\nthoughts, or wishes through symbolic, punning, shape, color, figure,\nnumeral, verb, preposition, character, loanword or alias through the\nartwork.\nCarefully analyze the visual elements present in the artwork and select\nthe option from the list below that best aligns with its conveyed\nmeaning: \n\nA. Longevity and Good Health \n \nB. Happiness, Joy, Good Luck \n \nC. Prestige, Promotion, and Good Exam Results"}, {"title": "Further Analysis on Experiment Results", "content": "We evaluated the models on text-only questions, providing only the story name conveyed by each artwork for symbolic matching. We used the accuracy as the evaluation metrics, the same as we used for symbolic matching task with artwork images in the main paper.\nAs we mentioned in the Section 5.2, we observed the language mismatch in the response from Qwen-VL model family. We also observed the hallucination in the responses from Qwen-VL Max model under the 5-shot settings.\nWe analyzed the category-wise accuracy performance and the confusion matrix of incorrect symbolic matching answers for GPT-40, as shown in Figure 7 and Figure 8, respectively. The results indicates that GPT-4o has the most confidence on the option D, which is related to fecundity, when reading the pun rebus artwork, as it achieves the highest accuracy for this category and frequently mislabeling other answers as option D. Also, GPT-40 made its lowest accuracy on the option F and G, which are related to the moral integrity and societal harmony. The confusion matrix suggests that GPT-40 has very sparse knowledge regarding option F, as the error distribution for this category is nearly uniform compared to the errors for other options."}]}