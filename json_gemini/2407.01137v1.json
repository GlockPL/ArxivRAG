{"title": "An Empirical Comparison of Generative Approaches for Product Attribute-Value Identification", "authors": ["Kassem Sabeh", "Robert Litschko", "Mouna Kacimi", "Barbara Plank", "Johann Gamper"], "abstract": "Product attributes are crucial for e-commerce platforms, supporting applications like search, recommendation, and question answering. The task of Product Attribute and Value Identification (PAVI) involves identifying both attributes and their values from product information. In this paper, we formulate PAVI as a generation task and provide, to the best of our knowledge, the most comprehensive evaluation of PAVI so far. We compare three different attribute-value generation (AVG) strategies based on fine-tuning encoder-decoder models on three datasets. Experiments show that end-to-end AVG approach, which is computationally efficient, outperforms other strategies. However, there are differences depending on model sizes and the underlying language model. The code to reproduce all experiments is available at: https://github.com/kassemsabeh/pavi-avg", "sections": [{"title": "1 Introduction", "content": "Product attributes are a crucial component of e-commerce platforms, facilitating applications such as product search (Chen et al., 2023), product recommendation (Truong et al., 2022), and product-related question answering (Deng et al., 2023). They provide useful details about product features, enabling customers to compare products and make informed purchasing decisions. Product attribute and value identification (PAVI) refers to the task of identifying both the attributes and their corresponding values from an input context, such as a product title or description. For example, given the product title \"Fossil Men's Watch Analog Display Slim Case Design with Brown Leather Band\" (see Figure 1), a model should identify the attributes Brand, Band Color, and Band Material, with the corresponding values Fossil, Brown, and Leather.\nMost existing work focuses on product attribute-value extraction (PAVE) (Zheng et al., 2018; Xu"}, {"title": "2 Related Work", "content": "Most existing approaches for attribute-value extraction use sequence tagging (Huang et al., 2015; Xu et al., 2019; Yan et al., 2021; Zheng et al., 2018) or question answering (Wang et al., 2020; Yang et al., 2022; Ding et al., 2022; Hu et al., 2022; Sabeh et al., 2022; Yang et al., 2023) methods. However, such approaches carry closed-world assumption, as they require the set of attributes as inputs to extract the corresponding values. More recently, researchers have explored the capabilities of generative models to tackle the PAVI task, in an open-world setting.\nRoy et al. (2024) proposed a generative framework for joint attribute and value extraction. They conduct experiments on the AE-110k dataset and show that the generative approaches surpass question-answering based methods. Shinzato et al. (2023) fine-tune a pre-trained T5 generative model (Raffel et al., 2020) to decode a set of target attribute-value pairs from the input product text of the MAVE dataset (Yang et al., 2022). They show that the generative approach outperforms extraction and classification-based methods (Chen et al., 2022).\nHowever, all above studies utilize an end-to-end generative approach. They did not explore other generative strategies for attribute-value identification (i.e., pipeline and multi-task). In addition, these approaches are not comparable as they are different in terms of datasets, settings, and evaluation metrics. Finally, none of the above proposed models have been made publicly available. In this work, we propose three generative approaches for PAVI and empirically compare them on three real-world datasets. We summarize how our approach differs from prior work in Table 1. As can be seen, we evaluate in total all approaches across three"}, {"title": "3 Proposed Methods", "content": "Given an input product data (title or description) x = {x\u2081,x\u2082,...,x|x|}, attribute-value generation aims to generate attribute-value pairs Qx related to the information in x:\nQx = {(a\u00b9, v\u00b9), (a\u00b2, v\u00b2), (a\u00b3, v\u00b3), ...} (1)\nFor instance, if x=\"Fossil\",...,\"Band\", then Qx = (\"Brand\",\"Fossil\"), (\"Band Color\",\"Brown\"), (\"Band Material\", \"Leather\").\nWe formulate the attribute-value identification problem as an attribute-value generation (AVG) task and propose three approaches based on fine-tuning language models, as depicted in Figure 2."}, {"title": "3.1 Pipeline AVG", "content": "The AVG task can be decomposed into two simpler sub-tasks, value extraction (VE), and attribute generation (AG). The VE model Pve(v|x) first generates the value candidate \u1e7d from x. Then, the AG model Pag(a|x,v) generates an attribute a whose value is \u1e7d in the input x. The VE and AG models can be trained independently on a product dataset consisting of the triplet (x, a, v) by maximizing the conditional log likelihood of:\n\u1e7d = arg max Pve(v | x) (2)\nv\na = arg max Pag (a | x, \u1e7d) (3)\na\nIn practice, the VE model input is [x\u2081, x\u2082,... x|x|], where xi is the i-th token of the product input x and |x| represents the number of tokens in the sequence. The input to the AG model takes the value into account by highlighting it inside the input. Specifically, following previous work (Chan"}, {"title": "3.2 Multitask AVG", "content": "Instead of training two separate generative models for each sub-task, we can instead use a single shared model that is fine-tuned in a multitask learning setting. Namely, we mix the training instances for the VE and AG tasks together, and randomly sample a batch at each iteration of seq2seq fine-tuning. We distinguish each task by adding a prefix to the beginning of the input text. Namely, we add extract value for the VE task, and generate attribute for the AG task."}, {"title": "3.3 End2End AVG", "content": "Instead of breaking the AVG task into two sub-tasks, we can directly model it by transforming the target attribute-value pairs to a flattened sentence z, and fine-tune a seq2seq model to directly generate the z from x. We define a function T that maps the target Qx to a sentence as:\nT(Qx) = \"{t(a\u00b9, v\u00b9)|t(a\u00b2, v\u00b2)|...}\". (5)\nt(a, v) = \"attribute: {a},value : {v}\" (6)\nWe use the template t to textualize the attribute-value pairs and separate them using a separator |.\""}, {"title": "4 Experimental Settings", "content": "Datasets. We use three real-world datasets.\n\u2022 \u0391\u0395-110K (Xu et al., 2019): This dataset contains tuples of product titles, attributes, and values from AliExpress Sports & Entertainment category. Instances with NULL values are removed, resulting in 39,505 products with 2,045 unique attributes and 10,977 unique values.\n\u2022 MAVE (Yang et al., 2022): This is a large and diverse dataset complied from the Amazon Review Dataset (Ni et al., 2019). We remove negative examples from the MAVE dataset, where there are no values for the attributes. The final dataset contains around 2.9M attribute-value annotations from 2.2M cleaned Amazon products.\n\u2022 OA-Mine (Zhang et al., 2022): We use the human-annotated dataset, which contains 1,943 product data from 10 product categories. No further processing is applied to this dataset.\nWe randomly split all datasets in train:val:test = 8:1:1. The splits are stratified by product category. Appendix A shows statistics of the three datasets.\nBase Models. For all approaches (pipeline, multitask, and end2end), we experiment with the base language models T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). We also compare between the model weights t5-{small,base,large} and facebook/bart-{base,large} from Hugging-Face34.\nEvaluation Metrics. Following previous works (Yang et al., 2022; Shinzato et al., 2023), we use precision P, recall R, and F\u2081 score as evaluation metrics. The datasets may contain missing attribute-value pairs that the model might generate. To reduce the impact of such missing attribute-value pairs (Shinzato et al., 2023), we discard predicted attribute-value pairs if there are no ground truth labels for the generated attributes."}, {"title": "5 Results", "content": "Table 2 provides the main results. In addition to the three approaches (i.e., pipeline, multitask, and end2end), we also provide an ensemble model that combines the generated attribute-value pairs from these approaches. Overall, T5 large (end2end) achieves the best scores across the three datasets. Additionally, the multitask approach exhibits commendable performance, often ranking the second best. There are several interesting observations in Table 2. First, while the end2end approach generally excels, there are instances where the pipeline or multitask approach outperforms it, especially with smaller model sizes. For example, for T5 small on the OA-Mine dataset, the multitask approach outperforms end2end with an F\u2081 score of 76.48 compared to 56.29. By analyzing the errors, we found that the end2end approach makes more errors in detecting attributes, which the multitask approach mitigates. This improvement is mainly because the multitask approach has been specifically trained on the task of attribute generation. Second, the influence of model size on performance is evident, with larger models generally achieving better results across all approaches. For instance, T5 base and T5 large consistently outperform T5 small across all datasets and approaches. This trend is also seen with BART models. Third, among the AVG approaches, T5 consistently works better with the end2end AVG, while BART is not well-suited when used end2end. A possible explanation is that T5 has observed sentences with structured observation due to its multitask pre-training objective, while BART did not encounter such training instances as it was trained only on a denoising sequence-to-sequence objective. Finally, there are notable differences in performance across the datasets. For instance, the MAVE dataset sees higher overall F\u2081 scores compared to AE-110k and OA-Mine datasets. The higher results on the MAVE dataset can be attributed to its uniform annotation process using an ensemble of models, unlike the more varied human annotations in AE-110k and OA-Mine5.\nEnsemble models, which combine the generated attribute-value pairs across the three approaches, consistently improve results. For instance, in AE-110k, ensembling trades off a small amount of precision for substantial gains in recall, while in OA-Mine, precision remains stable with improved recall. In general, ensembling helps to identify more attributes and therefore enhances the F\u2081 score by increasing the recall. However, it slightly reduces precision due to challenges in extracting accurate values for these new attributes."}, {"title": "6 Conclusion", "content": "In this paper, we formalized PAVI as an attribute-value generation task and established three different AVG approaches. Using T5 and BART base models, we conducted experiments on three benchmark product datasets. Our evaluation demonstrates that end2end AVG, which generates attributes and values simultaneously, is generally more reliable. However, pipeline or multitask approach can offer advantages, particularly for smaller models and when using language models like BART."}, {"title": "Limitations", "content": "Our study has two main limitations. First, the datasets used in our experiments do not have standard splits. We randomly split the datasets as discussed in Section 4, but we have provided the exact data splits in our repository to ensure reproducibility and comparability. Second, the evaluation measures employed do not penalize over-generated attribute-value pairs. We assume that the datasets do not have all possible annotations, so the generative models might correctly identify new attribute-value pairs. However, in our evaluation, we discard these newly generated attribute-value pairs. As future work, we plan to develop methods for the automatic evaluation of newly generated attribute-value pairs."}]}