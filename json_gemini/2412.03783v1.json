{"title": "Expressivity of Representation Learning on Continuous-Time Dynamic Graphs: An Information-Flow Centric Review", "authors": ["Sofiane Ennadir", "Gabriela Zarzar Gandler", "Filip Cornell", "Lele Cao", "Oleg Smirnov", "Tianze Wang", "Levente Z\u00f3lyomi", "Bj\u00f6rn Brinne", "Sahar Asadi"], "abstract": "Graphs are ubiquitous in real-world applications, ranging from social networks to biological systems, and have inspired the development of Graph Neural Networks (GNNs) for learning expressive representations. While most research has centered on static graphs, many real-world scenarios involve dynamic, temporally evolving graphs, motivating the need for Continuous-Time Dynamic Graph (CTDG) models. This paper provides a comprehensive review of Graph Representation Learning (GRL) on CTDGs with a focus on Self-Supervised Representation Learning (SSRL). We introduce a novel theoretical framework that analyzes the expressivity of CTDG models through an Information-Flow (IF) lens, quantifying their ability to propagate and encode temporal and structural information. Leveraging this framework, we categorize existing CTDG methods based on their suitability for different graph types and application scenarios. Within the same scope, we examine the design of SSRL methods tailored to CTDGs, such as predictive and contrastive approaches, highlighting their potential to mitigate the reliance on labeled data. Empirical evaluations on synthetic and real-world datasets validate our theoretical insights, demonstrating the strengths and limitations of various methods across long-range, bi-partite and community-based graphs. This work offers both a theoretical foundation and practical guidance for selecting and developing CTDG models, advancing the understanding of GRL in dynamic settings.", "sections": [{"title": "1 Introduction", "content": "Graph-structured data is prevalent across various domains such as chemoinformatics, bioinformatics, and social network analysis. These domains often require sophisticated machine learning approaches, driving the development of Graph Representation Learning (GRL). At its core, GRL aims to embed graph structures into low-dimensional vector spaces, preserving essential structural and semantic features. This embedding facilitates tasks such as node classification (e.g., music genre prediction (Kumar et al., 2019b)), link prediction (e.g., product recommendation (Wu et al., 2019)), and graph classification (e.g., drug discovery (Kearnes et al., 2016)). A cornerstone of this field is the Message-Passing (MP) paradigm (Gilmer et al., 2017), where nodes iteratively exchange and aggregate information from their neighbors to learn representations.\nWhile much of the research has focused on static graphs (Kipf & Welling, 2017; Veli\u010dkovi\u0107 et al., 2018; Xu et al., 2019), many real-world applications involve evolving graph structures. Such graphs, termed Dynamic Graphs (DGs), undergo changes over time through node/edge addition/deletion or node feature updates collectively referred to as events (Seo et al., 2018; Bai et al., 2021). DGs can be categorized as Discrete-Time DGs (DTDGs) or Continuous-Time DGs (CTDGs), distinguished by their treatment of temporal granularity. While DTDGs aggregate events into fixed time intervals, CTDGs accommodate temporally irregular events, offering a more flexible and widely adopted representation for DGs. Learning on CTDGs presents unique challenges, such as capturing temporal dependencies and handling sparse, irregular event data."}, {"title": "2 Theoretical Expressivity Framework", "content": "A static graph is defined as G = (V, E), where V is the set of vertices and E the set of edges. Let n = |V| and m = |E| denote the number of vertices and edges, respectively, and \\mathcal{N}(u) = \\{v: (v, u) \\in E\\} the set of neighbors of a node u \\in V. The degree of a node u is |\\mathcal{N}(u)|. A graph is often represented by its adjacency matrix A \\in \\mathbb{R}^{n \\times n}, where the (i, j)-th entry denotes the weight of the edge between the i-th and j-th nodes, or 0 if no edge exists. Node features are represented by X \\in \\mathbb{R}^{n \\times D}, where D is the feature dimensionality, and the i-th row of X corresponds to the features of the i-th node.\nFor DGs, the graph at time t is represented as G_t = (V, E_t), where E_t is the set of edges at time t. Events at time t + 1 can alter the topology by adding or removing edges or nodes, resulting in G_{t+1} = (V, E_{t+1}). To maintain consistency, we treat the node set V as constant over time by assuming nodes with zero degree exist at the start and can transition to active participation as events unfold."}, {"title": "2.1 Key Components of CTDG Representation Learning", "content": "To address the dynamic nature of CTDGs and their inherent temporal aspects, several recent works, such as TGN (Rossi et al., 2020a), TGAT (Xu et al., 2020), and CTAN (Gravina et al., 2024), have proposed adapted MP mechanisms. They allow nodes to communicate and update their states based on both current events and historical interactions. Specifically, for each node u, its temporal neighborhood \\mathcal{N}(u,t) at time t is defined to include all nodes that have interacted with u within a specified time window (also referred to as the context window) leading up to t. This temporal neighborhood captures the temporal aspect of interactions, ensuring that node updates consider both recent and relevant historical information.\nWe intend to propose a general framework that encompasses the various approaches relying on temporal MP to facilitate learning on CTDGs. This framework is built on two essential components:\n(i) Node representation: At any given time t, each node u has a representation h_u(t) in the embedding space. This representation is used for various pretraining tasks or downstream applications.\n(ii) Node memory: Each node u maintains a memory state, denoted as s_u(t), which evolves over time to track the node's historical interactions. While some methods leverage this memory explicitly, others rely solely on the node's current representation.\nWhen an event \\mathcal{E} = (u,v,e_{u,v}) occurs at time t between nodes u and v, where e_{u,v} captures the event's features, updates are performed in two stages:\n(i) The nodes directly involved in the event (i.e., u and v) are updated to reflect the new interaction. The generated message directly updates their corresponding memory based on the event's features e_{u,v}, and also their previous memories in time t^{-}. This can be written as:\n$s_u(t) = \\text{MEMUPD}([s_u(t^{-}), s_v(t^{-}), t - t^{-}, e_{u,v}])$ (1)\n$s_v(t) = \\text{MEMUPD}([s_v(t^{-}), s_u(t^{-}), t - t^{-}, e_{u,v}])$ (2)\nwhere function MEMUPD varies across methods; some use GRUs or LSTMs (Kumar et al., 2019a), while others opt for a simple identity function (Rossi et al., 2020a).\n(ii) All other nodes are updated through a MP mechanism designed to propagate the newly introduced information across the graph. This ensures that the entire network incorporates the updated information, extending beyond the immediately affected nodes. We begin by defining the concept of temporal neighborhood of each node u at time t as:\n\\mathcal{N}(u,t) = \\{(v,e_{uv}, t') | \\exists (u, v, t') \\in G_t\\}, (3)\nwhere \\mathcal{N}(u,t) captures all nodes v that have interacted with u over time, along with the corresponding interaction features e_{uv} and timestamps t'. Based on this temporal neighborhood, the representation of node u is updated as follows:\n$h^{(l)}_u (t) = \\text{AGG}^{(l)}(\\{\\{h^{(l-1)}_v(t), t -t', e_{uv}) | (u, e_{uv}, t') \\in \\mathcal{N}(v, t)\\}\\})$ (4)\n$h^{(l)}_u (t) = \\text{UPDATE}^{(l)} (h^{(l-1)}_u(t), h^{(l)}_u (t))$, (5)"}, {"title": "2.2 Expressivity Framework Based on Information Flow (IF)", "content": "The aforementioned update scheme illustrates that a node's representation in DGs evolves temporally, with its trajectory heavily influenced by the composition and activity level of its local neighborhood. Nodes that are consistently involved in events or positioned within close proximity to evolving nodes demonstrate particular dynamic representations over time, highlighting the interdependent interplay between network structure and temporal dynamics in shaping node embeddings. In this study, we aim to extend our understanding from a rigorous theoretical perspective. Our primary objective is to elucidate the precise mechanisms by which individual components of the update scheme influence node representations. By decomposing the update process and analyzing its components, we seek to develop a comprehensive mathematical framework that captures the essence of temporal node embedding evolution. This theoretical framework serves multiple purposes. Firstly, it provides insights into how different methods for CTDG representation learning generate distinct node embeddings. By identifying the key factors that drive these differences, we can better understand the strengths and limitations of these methods. Secondly, the framework enables us to predict and explain the performance of different methods across various graph types (e.g., sparse, homogeneous) and applications (e.g., recommendation systems, social networks).\nThe traditional Weisfeiler-Lehman (WL) framework, which was originally built on ideas from the Weisfeiler-Lehman isomorphism test, and further generalized to higher order variants (Morris et al., 2017), has attracted a lot of attention to modelize graph classifier's capabilities Morris et al. (2019). This approach was recently extended to temporal graphs as temporal-WL (Souza et al., 2022), which iteratively refines node color using node/edge features and timestamps to distinguish non-isomorphic graphs. While effective in some scenarios, the temporal-WL struggles to capture the progression of information over time, particularly the differential impact and propagation of events across snapshots. Its reliance on specific node/edge features further limits its generalization in high-dimensional, real-world settings. This limitation renders the approach not adapted for our aim to understand the effect of each model's components on a node's evolving representation. In contrast, the IF view focuses on how events propagate through consecutive graph snapshots, modeling their influence on node/edge representations over time. By examining embedding changes, this view provides a more nuanced understanding of temporal dependencies and the evolution of graph structure, offering practical insights into the applicability of different techniques in dynamic, event-driven contexts.\nSpecifically, our goal is to examine the change in node embeddings between different consecutive time points to understand how events impact the node representations. Naturally and intuitively, the nodes directly involved in the events are expected to undergo the most significant changes in their embeddings, while the representations of other nodes are less affected. This analysis provides valuable insights into how the graph's structure evolves in response to events, offering a detailed view of how different parts of the graph react. By studying the evolution of node embeddings, we can also assess how various methods handle information propagation across the graph. This sheds light on the strengths and limitations of different techniques, offering guidance on their applicability to specific use cases where certain types of event-driven changes are more prevalent."}, {"title": "3 Method Categorization and Review", "content": "This section reviews GRL methods for CTDGs, categorizing them based on our expressivity framework and exploring how SSRL approaches align with this framework to leverage unlabeled data effectively."}, {"title": "3.1 Method Categorization Inspired by Theoretical Framework", "content": "We start with reviewing various GRL methods for CTDGs, categorized based on the theoretical insights outlined in Section 2.2. The expressivity framework provides a systematic basis for evaluating the strengths and limitations of each method across different graph types. At a high level (cf. Figure 1), the categorization distinguishes between methods that use explicit MP mechanisms and those that rely on direct representation updates driven by events. Additionally, we examine strategies for memory aggregation and updates, emphasizing their influence on model performance, expressivity, and computational complexity."}, {"title": "3.1.1 Non-message-passing (Non-MP) methods", "content": "One of the earliest approaches to address CTDG is JODIE (Kumar et al., 2019a). The proposed approach captures interaction dynamics between users and items by learning two types of embeddings: static embeddings for long-term characteristics and dynamic embeddings that evolve over time based on interactions."}, {"title": "3.1.2 Message-passing (MP) based methods", "content": "Alongside the Non-MP category, methods propagating information across the graph structure were proposed. One such approach is DyRep (Trivedi et al., 2019), which models both the topological evolution of the graph and the temporal dynamics of node interactions. DyRep updates a node's representation by considering its direct neighbors, its previous representation, and a temporal projection technique based on a temporal point process, which treats each incoming interaction event as an observation of a stochastic process. While this method checks many important boxes from our theoretical analysis, there is a notable limitation in its reliance on direct neighborhoods. Nodes beyond one-hop distance (i.e., two or more hops from the event) are updated solely through the temporal projection and non-updated neighbors, without direct MP from further nodes. This can lead to over-smoothing when events are concentrated within the same neighborhood, as the aggregation pulls in redundant information and dilutes useful features over time. As a result, while DyRep performs well in scenarios where one-hop interactions are crucial (such as recommender systems and bi-partite graphs), it is less effective in extracting information from longer-range interactions in larger, sparser graphs. These insights and limitations are also applicable to the similar approach DyGNN Ma et al. (2020), which consists of two main components: an update mechanism that adjusts the representation of nodes involved in events using two LSTMs and a propagation unit that disseminates information within the 1-hop neighborhood of the event nodes.\nXu et al. (2020) extended DyRep and DyGNN by proposing TGAT. They incorporate Functional Time Encoding to map temporal data into a high-dimensional space. They also introduce Temporal Graph Attention Layer to aggregate information from a node's temporal neighbors using a self-attention mechanism that accounts for both structural and temporal aspects of the neighborhood. TGAT does not necessarily employ a memory component, as described in Section 2.1, but instead uses the node features directly as the initial representation $h_u^0$. Therefore, as highlighted by Theorem 2.3, selecting an appropriate number of layers L, and thus defining the L-hop neighborhood, is crucial for ensuring effective information propagation.\nIn a similar perspective to TGAT, TGN (Rossi et al., 2020b) offers a general formalization of CTDG functions, akin to the framework we presented in Section 2.1. TGN incorporates both temporal projection and information propagation through attention-based aggregation. Theorem 2.3 is directly applicable to this method, emphasizing the need to adjust the number of layers based on the specific use case. For example, in recommender systems, setting L = 2 may suffice, while in social networks or molecular graphs, the"}, {"title": "3.1.3 Hybrid methods", "content": "Another category of methods in the literature falls outside of our proposed categorization, adopting a hybrid approach that aims to propagate information within DG without strictly adhering to classical MP schemes.\nOne primary focus of hybrid methods is addressing the complexity challenges associated with existing CTDG methods. Complexity has two main aspects: computational (time or operation) complexity and storage (space or memory) complexity. From the computational perspective, Cong et al. (2023) analyzed the impact of incorporating RNNs and self-attention mechanisms into MP on model accuracy and performance. While these components can enhance performance, the study revealed that they are not strictly necessary to achieve strong results. So, the authors introduced GraphMixer, a simpler yet highly effective architecture composed of a \u201clink-encoder\u201d based on MLPs, a \u201cnode-encoder\" employing neighborhood mean pooling, and an MLP-based \u201clink-classifier\". This design significantly reduces both computational complexity, as MLPs are considerably more time-efficient than attention-based models and RNNs. Similarly, SimpleDyG (Wu et al., 2024) leverages the Transformer's self-attention mechanism by modeling the dynamic graph as a sequence that captures temporal evolution patterns through an alignment technique. By transforming the graph into a sequence and applying an unmodified Transformer architecture, SimpleDyG demonstrate lower operational complexity, resulting in a sparser and more efficient design."}, {"title": "3.2 Relevance of Self-Supervised Representation Learning (SSRL) Methods for CTDG", "content": "While the methods reviewed in the above categories are capable of extracting meaningful node and graph representations for a variety of tasks, they typically require large volumes of labeled data for effective training. However, the exponential growth of CTDG data across diverse fields in domains such as social networks, recommender systems and biological interaction networks, has rendered manual labeling both labor-intensive and prohibitively costly. As a result, SSRL has become an increasingly popular approach to address the scarcity of labeled data. SSRL tackles this challenge by creating alternative, auxiliary tasks from unlabeled data to help models learn useful representations. These auxiliary tasks, often referred to as pretraining or pretext tasks, allow models to leverage the inherent data structure as a learning signal. By defining these pretext tasks carefully, SSRL enables the model to capture essential features and patterns in the data without needing explicit labels. SSRL has achieved considerable success in fields like CV (YM. et al., 2020; He et al., 2022) and NLP (Radford et al., 2018), forming the backbone of foundational models that underpin today's AI systems (Bommasani et al., 2021).\nWe would like to highlight that the theoretical analysis provided in Section 2.2, grounded in IF, does not consider the presence of labels. Instead, it focuses on examining how an event influences a node's representation, thereby evaluating the method's expressive power in modeling the CTDG at time t. While this analysis has been instrumental in categorizing various methods as in Section 3, it is also highly relevant in the context of SSRL. A method's expressive power is essential for designing effective pretext tasks; for instance, if a method has limitations in propagating information over long ranges, contrastive augmentations that depend on capturing long-range dependencies may fail to deliver robust pretraining outcomes.\nIn the context of CTDGs, SSRL has significant potential for tasks such as link prediction, anomaly detection, and temporal pattern recognition, where labels are often unavailable or costly to obtain. Additionally, another learning paradigm, semi-supervised learning, has also been explored within the DG context. This approach is particularly useful for node classification tasks where only a subset of nodes is labeled, and the objective is to propagate these labels to the remaining unlabeled nodes. Semi-supervised learning has been widely applied in static graph settings, leveraging known labels to guide the representation learning process for all nodes within the graph. In this work, we focus on SSRL approaches in the context of CTDG. The next section will therefore focus on reviewing various SSRL-based methods within the CTDG context, exploring different approaches for designing pretext tasks that harness the temporal and structural aspects of evolving graphs to learn robust representations."}, {"title": "3.2.1 Pretext tasks for pretraining on CTDG", "content": "The commonly adopted pretext tasks for pretraining SSRL model on CTDG can be largely divided into two main categories: predictive and contrastive pretraining tasks.\nPredictive pretraining often relies on auto-regressive modeling paradigms, leveraging the sequential nature of temporal data. This task is particularly valuable in domains where data naturally follows a temporal or ordered structure, such as NLP or time-series analysis. In NLP, for example, predicting the next word based on the context of preceding words has been shown to help models learn robust and generalizable representations of language (Kenton & Toutanova, 2019). These learned representations capture semantic and syntactic patterns that enhance performance on a range of downstream tasks. For CTDGs, where the temporal aspect of interactions between nodes is a key feature, predictive pretext tasks are equally relevant. By taking advantage of the temporal ordering of events, an intuitive auxiliary pretext task emerges: predicting the occurrence of the next event. This task involves, at any given time step t, forecasting the subsequent event at the following time step t+1, underpinning the majority of SSRL approaches for CTDG."}, {"title": "3.2.2 Downstream tasks", "content": "Pretraining the model with SSRL pretext tasks aims to learn a robust graph function that provides useful representations for a range of predictive tasks, commonly referred to as downstream tasks in the literature. In the context of CTDGs, the literature has focused on three primary downstream tasks:\nLink prediction task is the most studied one that involves predicting future links for a CTDG. That is to predict the next interaction, identifying both the source and target nodes. Link prediction is particularly relevant in recommender systems, where the goal is to anticipate the next item a user may be interested in. By accurately predicting these links, we can enhance recommendation quality and increase the likelihood of user engagement or sales. In the same perspective related to edge, a less explored task in the literature is the one related to edge classification, regression and clustering, the main goal of which is to assign each edge a class/value/cluster at a future time.\nNode-level prediction mainly refers to node classification/regression in CTDGs, which essentially extends the static node-level prediction tasks by incorporating both the temporal dynamics and topologies. For example, in social networks, node classification may be used to detect spammers or malicious actors by analyzing patterns in their evolving connections and activities. As networks evolve, so do the relationships between nodes, meaning that a node's classification needs to be updated dynamically based on new interactions.\nGraph-level prediction tasks aim to classify or regress entire graphs rather than individual nodes, making them especially relevant in domains like bioinformatics. For instance, in molecular graph analysis, each graph represents a molecular structure, and the task involves predicting graph properties such as toxicity or stability based on the configuration of the structure. When graphs evolve over time, the objective shifts to assessing whether the graph remains valid under specific criteria. For example, the addition of certain bonds in a molecular graph may result in a chemically viable or unstable structure."}, {"title": "4 Empirical Validation", "content": "This section presents an empirical study to validate the theoretical insights and categorization proposed earlier. Specifically, we evaluate the effectiveness of reviewed methods and identify the types of graphs where they perform best. We first outline the experimental setup, followed by a discussion of the empirical findings. Additionally, an analysis of the tightness of the proposed upper bounds is provided in Appendix E."}, {"title": "4.1 Experimental Setup", "content": "Our experimental focus is on the popular link prediction task, chosen as a representative challenge within the broader scope of CTDGs. We examine a diverse selection of models from those reviewed in Section 3, focusing on models that capture different facets of CTDG methodologies: (I) JODIE (Kumar et al., 2019a), which models evolving embeddings over time; (II) DyRep (Trivedi et al., 2019), designed for capturing dynamic node interactions; (III) TGN (Rossi et al., 2020b), i.e., Temporal Graph Networks for inductive learning; (IV) TGAT (Xu et al., 2020), known for its attention-based approach for handling temporal information; (V) CTAN (Gravina et al., 2024), focused on modeling long-range dependencies within graphs. Experimental details and hyper-parameters used to train/test the model are provided in Appendix F.2.\nIn alignment with the selected methods, we choose a set of both synthetic and real-world DG datasets which are representative to the main application areas and unique characteristics of CTDGs. Additional details on each dataset, including design considerations and properties, are provided in Appendix F.1.\nLong-range graphs are those requiring long-range reasoning to achieve strong performance, as distant nodes or graph regions can influence each other. To capture these interactions, specific information propagation mechanisms are necessary, as conventional MP often fails to bridge these long-range dependencies effectively. Evaluating performance on these graphs is crucial to assess a method's ability to transfer information across distant parts of the graph. For this setting, we use an approach similar to the one proposed by Gravina et al. (2024) where we consider a temporal version of the PascalVOC-SP graph (Dwivedi et al., 2022). Additional information about the generation is provided in Appendix F.1.\nBi-partite graphs play a crucial role in applications like recommender systems, a prominent use case for CTDGs. To evaluate how different methods handle interactions between disjoint node sets, which are critical for recommendation quality, we use the TGBL-Wiki dataset from the TGB benchmarks (Huang et al., 2024). This dataset represents a bi-partite network where nodes are editors and Wiki pages, with edges added when an editor modifies a page at a specific timestamp. Edges carry text features derived from the page edits. Dataset statistics can be found in Table 5 in Appendix F.1.\nCommunity-based graphs consist of nodes grouped into distinct clusters, with relatively few inter-community links compared to intra-community connections. A Barbell graph is a representative example of this structure, where inter-community links act as bottlenecks (Alon & Yahav, 2021), making these graphs challenging for temporal MP schemes due to limited information propagation between communities. To evaluate how well a benchmark captures intra-community links, we generate a synthetic dataset using the Stochastic Block Model (SBM) with the NetworkX (Hagberg et al., 2008) package. This involves creating B dense communities with \\(N_B\\) nodes in each community, resulting in N nodes in separate disconnected components with a connection density of \\(p_{i,i}\\). At each timestep t, we select \\(k_c=4\\) pairs of communities that we (pairwise) connect over a random number \\(t_{gen,kc} \\in [6,20)\\). Specifically, this selection follows a normalized density of \\(\\rho_{ij}=0.025\\), normalized over a sampled time horizon \\(t_{gen,kc}\\) over which we connect the pair of clusters. Naturally, as time progresses, the graph becomes progressively more connected, and its topology evolves accordingly. The used parameters are reported in Table 3. For the evaluation, we leverage the normalized sampling densities between cluster pairs, derived from the data generation procedure, to identify which communities are most likely to be connected. Based on this, we formulate the task of ranking nodes within the active clusters and evaluate the results using Normalized Discounted Cumulative Gain (NDCG) (J\u00e4rvelin & Kek\u00e4l\u00e4inen, 2002). We choose the NDCG here, as we know which communities should be more and less likely to be connected during that specific timestep and their true relevance score as they are directly derived from the sampling of stochastic block model used.\nFor the Bi-Partite datasets, we have focused on using the real world graph TGBL-Wiki dataset, which is part of the TGB benchmark datasets (Huang et al., 2024). The dataset is a bi-partite network where Wiki pages and editors are nodes, while an edge is added when a user edits a page at a specific timestamp. For both the real and synthetic datasets, characteristics and information about the graphs utilized in the experimental results of the study are presented in Table 5."}, {"title": "4.2 Results Analysis", "content": "Table 1 presents the average AUC scores along with their corresponding standard deviations. As anticipated and discussed in Section 3, since the graph structure and topology are important for the downstream classification task, MP based models significantly outperform non-MP approaches like JODIE. This specific use-case showcases the limitation of this latter family of models, which rather focus on updating the evolved nodes in each event. We additionally see that CTAN is outperforming all the other MP based methods, confirming therefore its ability to track long-range dependencies within the graph."}, {"title": "5 Conclusion", "content": "In this paper, we presented a comprehensive review of Graph Representation Learning (GRL) on Continuous-Time Dynamic Graphs (CTDGs), emphasizing the critical role of expressivity in capturing temporal and structural dynamics. We introduced an Information-Flow (IF) centric theoretical framework to quantify the expressivity of CTDG models, providing a structured analysis of their performance across graph types and application scenarios. Leveraging this framework, we categorized existing methods, highlighting their strengths and limitations in addressing key challenges such as long-range dependency modeling, temporal irregularity, and sparse data. Additionally, we examined the growing relevance of Self-Supervised Representation"}, {"title": "A Preliminaries", "content": "Notations For our theoretical analysis, we consider the following notations:\n\\(\\bullet\\) d(u, v): is the shortest path distance between node u and node v.\n\\(\\bullet\\) deg(u): denotes node u's degree, i.e | N(u) |.\n\\(\\bullet\\) \\(d_Y\\) is the distance within our output manifold Y.\n\\(\\bullet\\) The node involved in the event is denoted as i.\n\\(\\bullet\\) L denotes the number of layers used in the considered model.\nProblem Set-up For our theoretical analysis, we consider that all the models are using 1 - Lipschitz continuous activation functions. We note that this is the case for the majority of classically used functions (ReLU, LeakyReLU, tanh, and others) (Virmaux & Scaman, 2018). As our goal is to understand the effect of each component on the model's evolving dynamics, we consider the most general use-case of a CTDG function. Typically, we consider that our model f follows the proposed components in Section 2.1. Specifically, we consider the model to based on the described temporal message-passing (either through a GCN-like aggregation or attention-based). We also consider that it makes use of the memory state and not only the node's individual representation. We additionally consider that the model encompasses a Linear temporal projection function to take into account temporal inactivity."}, {"title": "B Proof of Theorem 2.2", "content": "Theorem (GCN-based aggregation). Let's consider a CTDG-based function f: (A,X) \\(\\rightarrow\\) Y based on L GCN-like layers. After an event between node i and another node, we have the following:\n\\(\\bullet\\) For any node u not involved in the event and for which L < d(u,i) we have u is y \u2013 flowing with:\n\\[d_Y(f_u(G_{t+1}), f_u(G_{t})) \\leq w_u ||W_t||\\prod_{l=1}^{L}||W^{(l)}||.\\]\n\\(\\bullet\\) For any node u not involved in the event and for which L \\(\\geq\\) d(u,i) we have u is y \u2013 flowing with:\n\\[d_Y(f_u(G_{t+1}), f_u(G_{t})) \\leq \\prod_{l=1}^{L}||W^{(l)}|| [w_u ||W_t||+w_{u,i}\\Delta_{t,t+1}(S_i)],\\]\nwith wu being the sum of temporal normalized walks of length (L \u2013 1) starting from node u and \\(w_{u,i}\\) is the normalized shortest path between u and i.\nProof. We consider the case in which the propagation is done using a GCN-like aggregation. Specifically, we recall that a GCN update at layer l for a node within a neighborhood can be written as:\n\\[h_u^{(l)} = \\sigma^{(l)} \\bigg(\\sum_{v \\in N(u) \\cup \\{u\\}} \\frac{W^{(l)}h_v^{(l-1)}}{\\sqrt{(1 + deg(u))(1 + deg(v))}} \\bigg)\\]"}, {"title": "C Proof of Theorem 2.3", "content": "Theorem (Attention-based aggregation). Let's consider a CTDG function f : (A,X) \\(\\rightarrow\\) Y based on L attention-based layers. After an event between node i and another node, we have the following:\n\\(\\bullet\\) For any node u not involved in the event and for which L < d(u, i) we have u is y flowing with:\n\\[d_Y(f_u(G_{t+1}), f_u(G_{t})) \\leq deg(u) [||W_t||+B||W_t||^2].\\]\n\\(\\bullet\\) For any node u not involved in the event and for which L \\(\\geq\\) d(u, i) we have u is y \u2013 flowing with:\n\\[d_Y(f_u(G_{t+1}), f_u(G_{t})) \\leq deg(u) [||W_t||+B||W_t||^2] + \\Delta(S_i),\\]\nwith deg(u) being the degree of node u and B an upper-bound of hidden representation space.\nProof. In this part we rather focus on attention-based aggregation within our temporal neighborhood, which can formulated as the following:\n\\[h_u^{(l+1)}(t+1) = \\sigma^{(l)} \\bigg(\\sum_{k \\in \\mathcal{N}_u(t+1)} \\alpha_{uk}(t) h_k^{(l)} \\bigg)\\]"}, {"title": "D Specific Cases", "content": "D.1 In the case of MLPs\nA typical Multi-Layer Perceptron (MLP) with L layers implements the following:\n$h_1 = \\phi(W_1x + b_1)$\n$h_2 = \\phi(W_2h_1 + b_2)$\n\n$h_L = \\phi(W_Lh_{L-1} + b_L)$\nwhere \\(\\phi\\) is an 1-Lipschitz activation function applied element-wise. Then:\n\\[L_{MLP} \\leq \\prod_{i=1}^{L} ||W_i||_2\\]\nHence, for methods using MLPs, we have the following Lipschitz bound:\n\\[||h_L(x_1) - h_L(x_2)|| \\leq L_{MLP}||x_1 - x_2||.\\]"}, {"title": "E On the Tightness of the Provided Upper-Bound", "content": "In Theorem 2.3, we derive an upper bound on the expected norm difference in a node's representation between two consecutive time steps. To evaluate the tightness of this theoretical bound, we analyze the TGN model (Rossi et al., 2020b) since it is the closest model to align out with our proposed theoretical framework that is adapted on the general framework of a CTDG function presented in Section 2.1.\nWe note that in our theoretical proof, we don't consider event features (which are related to the added edges). Consequently, in order to align with our desire to study the tightness of the bounds, we make an adaptation of our derivations to take into account a TGN.\nWe adapt the Equation equation 8 to take into account edge features as it is the case in the TGN model, we have the following:\n\\[ \\alpha_{ij}^{(t)} = \\frac{\\exp{(e_{ij}^{(t)})}}{\\sum_{k \\in \\mathcal{N}_i(t)} \\exp{(e_{ik}^{(t)})}}}.\\]"}, {"title": "F Datasets and Implementation Details", "content": "F.1 Synthetic Datasets\nTo perform further experimentation and to validate our theoretical insights, we considered evaluated our considered benchmarks on a number of graph types. Since finding accessible real datasets"}]}