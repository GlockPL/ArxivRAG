{"title": "Evaluating Large Language Models on Financial Report Summarization: An Empirical Study", "authors": ["Xinqi YANG", "Scott ZANG", "Yong REN", "Dingjie PENG", "Zheng WEN"], "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable versatility across various applications, including natural language understanding, domain-specific knowledge tasks, etc. However, applying LLMs to complex, high-stakes domains like finance requires rigorous evaluation to ensure reliability, accuracy, and compliance with industry standards. To address this need, we conduct a comprehensive and comparative study on three state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their effectiveness in generating automated financial reports. Our primary motivation is to explore how these models can be harnessed within finance, a field demanding precision, contextual relevance, and robustness against erroneous or misleading information. By examining each model's capabilities, we aim to provide an insightful assessment of their strengths and limitations. Our paper offers benchmark for financial report analysis, encompassing proposed metrics such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative evaluation framework that integrates both quantitative metrics (e.g., precision, recall) and qualitative analyses (e.g., contextual fit, consistency) to provide a holistic view of each model's output quality. Additionally, we make our financial dataset publicly available, inviting researchers and practitioners to leverage, scrutinize, and enhance our findings through broader community engagement and collaborative improvement. Our dataset is available on huggingface\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) [1]-[7] have emerged as transformative tools in Natural Language Processing (NLP), demonstrating unprecedented capabilities across tasks such as text generation [8], summarization [9], classification [10], translation [11], automatic dialogue [12], and also extend to the multi-modal area [13]. Built upon massive datasets and powered by advanced neural architectures, LLMs can understand and generate human-like text, making them adaptable to a wide range of domains. Recently, their application has extended into high-stakes sectors such as finance [14], healthcare [15], and legal industries [16], where accuracy, informativeness and coherence are essential. However, deploying LLMs in such fields poses unique challenges, requiring careful evaluation to ensure reliability and trustworthiness.\nThe financial sector, in particular, benefits from LLM-driven innovations in automation, such as generating reports, analyzing market sentiment, and summarizing extensive financial data [17]. Yet, financial text is often dense with specialized terminology and context-sensitive information, demanding high levels of precision and contextual understanding from LLMs. Consequently, assessing LLM performance in financial applications goes beyond conventional metrics, requiring a nuanced approach that includes accuracy, informativeness and coherence, as well as alignment with financial domain knowledge.\nThis paper presents a comprehensive evaluation of our"}, {"title": "II. RELATED WORKS", "content": "A. Large Language Models\nLarge Language Models (LLMs) have revolutionized natural language processing, enabling significant advancements in text generation, summarization, translation, and question answering. Models like OpenAI's GPT-3 [18] and its successor, GPT-4 [19], represent breakthroughs in generative modeling due to their ability to generate coherent, contextually relevant text with minimal input. These models have been trained on diverse, large-scale datasets, allowing them to generalize across various domains and tasks. However, their general-purpose nature often requires domain-specific fine-tuning to perform well in specialized areas such as finance, healthcare, and legal domains.\nSeveral research efforts have sought to tailor LLMs to specialized fields. For instance, BioBERT [20] and Clinical-BERT [21] are domain-adapted models for biomedical text, fine-tuned to handle the unique terminology and contextual demands of medical research. Research on smaller yet effective models, such as DistilBERT [22] and MiniLM [23], has demonstrated that knowledge distillation and parameter optimization techniques can enhance computational efficiency without significantly compromising performance. These smaller models, along with fine-tuning methods like LoRA [24], support adaptability and efficiency for resource-constrained applications, which is increasingly relevant as LLM applications expand into industries requiring both accuracy and speed.\nB. Financial Statement Analysis\nLLM applications in the finance sector are rapidly expanding, with a few pioneering models leading the way. BloombergGPT [25] introduced the first LLM explicitly designed for the finance industry, showcasing the potential for domain-specific language models. To address the need for open-access tools, FinGPT [17] was developed as an open-source, fully featured LLM system. FinGPT provides modules for data collection, model fine-tuning, and cloud-based deployment. PIXIU [26] offers another comprehensive framework for financial analysis, encompassing tasks such as headline classification, question answering, sentiment analysis, and text summarization. Utilizing fine-tuned models based on LLaMA\u00b3, PIXIU is designed to handle complex, multi-faceted tasks within finance. Kim et al. [27] explored LLM applications in financial statement analysis, focusing on GPT-4. Their research demonstrated GPT-4's capacity to analyze standardized and anonymized financial statements, accurately predicting potential future earnings even without narrative or sector-specific context. Remarkably, GPT-4's performance often surpassed that of traditional financial analysts, reinforcing the potential for LLMs in financial forecasting.\nC. Evaluation of LLMs' Summaries\nIn recent years, evaluating LLM performance has relied on established metrics like ROUGE [28] and BERTScore [29]. ROUGE captures n-gram overlap, which is useful for content recall and syntactic similarity, while BERTScore leverages contextual embeddings to evaluate semantic similarity, offering a more nuanced assessment of relevance and factual accuracy.\nPrevious studies have primarily explored LLM summarization performance in general contexts. For example, Basya et al. [9] investigated text summarization with LLMs but focused on general, non-domain-specific text and smaller model architectures, limiting the applicability of findings to high-stakes or specialized fields like finance. More recent research has expanded the scope of LLM evaluation to include domain-specific applications. Afzal et al. [30] evaluated eleven models across medical, scientific, and governmental datasets, prioritizing the domain adaptation capabilities of LLMs. Their findings indicate that smaller LLMs can achieve competitive performance in domain-shift tasks even with minimal training samples, underscoring the potential of compact models in resource-limited or highly specialized settings."}, {"title": "III. METHODOLOGY", "content": "A. Preliminaries\nFew-Shot Prompting [31] is a technique in natural language processing where a pre-trained language model is given only a few examples (typically 1 to 5) of a particular task within the input prompt to help the model understand the task it needs to perform. This approach leverages the model's ability to learn from context without requiring fine-tuning on extensive task-specific data. Few-shot prompting has become a popular method for adapting LLMs, especially when there are limited labeled data available for a task.\nIn few-shot prompting, the language model is presented with a series of example inputs and their corresponding outputs, followed by a new input (the task query) [32]. The model then generates an output based on the pattern established in the examples. This method is especially effective with large pre-trained models like GPT-4, which have shown strong generalization capabilities across diverse tasks by only observing a few examples.\nGLM-4 [33] is a prominent LLM that excels across benchmarks in semantics, mathematics, reasoning, coding, and knowledge understanding. As an open-source project under the Apache License, it is suitable for commercial use. GLM-4 utilizes an auto-regressive model to capture local context in input text. Additionally, the end-to-end voice model GLM-4-Voice4 was released in October 2024.\nMistral-NeMo is a small language model with a 128K context length, making it accessible for third-party developers. Its \"sliding window attention\" enhances local comprehension of input text. Recently, Mistral AI has begun transitioning its models to the Mamba architecture [6]. Notably, just one day after the release of LLaMA3.1, Mistral AI unveiled Mistral Large 2, a 123B-parameter model comparable to LLaMA3.1.\nLLAMA has significantly advanced the field of LLMs as an open-source model. LLaMA emphasized the critical role of high-quality data in LLM training, influencing many research teams to prioritize diverse data collection. Notably, the 13B-parameter LLaMA outperformed OpenAI's GPT-3 in multiple benchmarks, underscoring the importance of data over sheer parameter size. The LLaMA series has since expanded rapidly, with LLaMA3.1 reaching 405B parameters. The more recent LLaMA3.27 includes multilingual models (1B, 3B) and text-image models (11B, 90B), with quantized versions for efficiency. Our study utilizes LLaMA3.1 with 70B parameters and 4-bit quantization.\nB. Usage of Prompt Engineering\nPrompt engineering is a commonly utilized technique for extending the capabilities of large language models (LLMs) and has become a comprehensive research topic [34] with the development of LLMs. Without updating the core model parameters, this approach leverages task-specific instructions, known as prompts, to enhance model efficacy. Usually, specified prompts are provided together with input to get better results.\nWe adopt prompt engineering when we summarize financial reports here and compare the performance among the models we introduced in Section III. Although prompt engineering is commonly utilized in LLM applications, there are no official standards or rules to obey. The beginning of prompts we used to generate the financial reports summarization are shown below:\nYou will be provided with financial notes,\nand your task is to summarize the points\nas follows:\nIn this paper, we use few-shot prompting [31] to make an effective prompt. We specify Iruca-GLM-4, Iruca-Mistral-NeNo, and Iruca-LLaMa3.1 in this paper to the models when we utilize prompt engineering with the base models.\nC. Data Overview\nWe source financial data from TradingView and MSN to support our LLMs performance comparison efforts, ensuring a diverse and high-quality dataset for effective model training.\nWe collect articles covering 10 different financial products, accessing this content through publicly available links on both platforms. The samples of our open dataset are shown in Table I, which comprises of 1,000 news articles, providing English (EN), Chinese (CN) and Japanese (JA) languages content (denoted as \"lang\"). The \"title\" serves as a brief and clear description of subject matter of the article, the 'content' refers to the main body of the article. And \"symbol\" typically denotes unique identifier used to represent a specific commodity in financial market. Each article contains 17 fields, we primarily utilize the \"symbol\" and \"content\" fields to focus on content summarization and domain-specific language adaptation. This dataset serves as a foundational resource for"}, {"title": "IV. EVALUATION AND ANALYSIS", "content": "We first clear the effectiveness of our proposed summarization of one practical sample in Table II. Compared with the raw text, the summary generated becomes obviously simplified, which would be helpful for people such the finance analyst and investigators to reduce the scanning the related news time.\nTo assess the results of the summarization, we focus on three primary dimensions: accuracy, informativeness and coherence.\nAccuracy evaluates whether the generated summary faithfully captures essential information and factual content from the original text. To assess the accuracy, we consider the following criteria:\n\u2022\tAre the facts and statements within the summary consistent with those presented in the original text?\n\u2022\tDoes the summary emphasize the core points from the original text, especially those central to the topic?\n\u2022\tImportantly, the summary should avoid introducing information not present in the original text, ensuring that readers are not misled by extraneous or fabricated content.\nInformativeness measures the extent of useful information included in the summary, i.e., whether the generated summary covers all the important information from the original text. To assess the informativeness, we consider the following criteria:\n\u2022\tDoes the summary encompass all major points and key details from the original text?\n\u2022\tWhile simplifying content, does the summary retain sufficient depth and scope to provide a comprehensive understanding?\n\u2022\tIt should aim to capture as much relevant information as possible while minimizing redundancy.\nCoherence examines the logical consistency and fluidity of the information flow within the summary. To assess the coherence, we consider the following criteria:\n\u2022\tIs the content organized in a logical sequence, allowing readers to understand it with ease?\n\u2022\tDoes the summary exhibit a smooth, natural flow of sentences, free from abrupt or disjointed transitions?\n\u2022\tIt is essential that the summary maintains a consistent theme and narrative style, avoiding fragmented or scattered content.\nThese three dimensions collectively enable a holistic evaluation of the summarization quality, guiding improvements in LLM performance for generating summaries that are accurate, coherent, and informative.\nA. Evaluation Metrics\nBuilding on our considerations on accuracy, informativeness and coherence, we apply specific evaluation methods to quantitatively and qualitatively assess the quality of generated summaries:\n\u2022\tAutomatic Evaluation: We utilize established metrics such as ROUGE [28] and BERTScore [29] to calculate objective scores, providing insights into the generated summaries' quality in terms of similarity to reference texts. ROUGE measures n-gram overlap, which is indicative of content recall, while BERTScore leverages contextual embeddings to evaluate semantic similarity, offering a deeper assessment of relevance and accuracy.\n\u2022\tLLM Evaluation: We specifically employ Nistral-Nemo, as evaluators for simulating financial experts. By prompting the model with three distinct, manually crafted prompts, we aim to simulate expert-level comparison between the generated summaries and original texts, focusing on qualitative aspects such as depth of understanding, factual alignment, and overall relevance in a financial context."}, {"title": "B. Experimental Results", "content": "The evaluation results for our models demonstrate several key insights regarding their performance in text summarization tasks.\nAs shown in Table III for evaluation on accuracy, in terms of ROUGE-1 Precision, Iruca-LLaMA3.1 achieves the highest score (0.65), indicating strong token-level alignment with reference texts, followed by Iruca-GLM-4 (0.57) and Iruca-Mistral-NeMo (0.41). For BERT Precision, Iruca-LLaMA3.1 again leads with a score of 0.91, closely followed by Iruca-GLM-4 at 0.89, and Iruca-Mistral-NeMo at 0.85, highlighting strong semantic accuracy across all models. The LLM Score Precision shows slightly different results, with Iruca-GLM-4 achieving the highest score (0.80), marginally outperforming Iruca-Mistral-NeMo (0.79) and Iruca-LLaMA3.1 (0.73). This suggests that Iruca-GLM-4 may offer better holistic quality as judged by this specific metric. Regarding iteration time, Iruca-LLaMA3.1 is significantly slower (63.35s) compared to Iruca-GLM-4 (8.25s) and Iruca-Mistral-NeMo (3.02s), indicating a trade-off between accuracy and computational efficiency.\nIn terms of ROUGE-1 Recall shown in Table III for evaluation on informativeness, both Iruca-LLaMA3.1 and Iruca-GLM-4 achieve high scores (0.51 and 0.50, respectively), coupled with strong BERT Recall values (0.88), indicating comprehensive vocabulary coverage and robust semantic alignment. These results suggest that elevated ROUGE-1 Recall often correlates with higher BERT Recall, implying that these models not only capture a broad range of vocabulary in their summaries but also maintain substantial semantic congruence with reference texts. Interestingly, the LLM Score Recall remains comparable across all models, suggesting that this metric reflects overall summary quality rather than simple vocabulary overlap. Therefore, while models with higher ROUGE-1 Recall (such as Iruca-LLaMA3.1 and Iruca-GLM-4) exhibit broader coverage, this does not necessarily result in a higher LLM Score Recall, as the latter encompasses a more holistic quality assessment.\nIn Table III for evaluation on coherence, the ROUGE-L F1 Score for Iruca-LLaMA3.1 is 0.52, with a BERT F1 Score of 0.89, demonstrating strong coherence, sequential accuracy, and effective semantic matching. This relationship suggests that higher coherence and sequence alignment often correlate with improved semantic accuracy. In contrast, Iruca-GLM-4 achieves a slightly lower ROUGE-L F1 Score of 0.47, while maintaining a high BERT F1 Score of 0.89. This indicates that, although the model's sequential alignment may be diminished, it retains semantic accuracy, effectively capturing the meaning of reference summaries. Iruca-Mistral-NeMo, however, presents a ROUGE-L F1 Score of 0.31 and a BERT F1 Score of 0.85, suggesting that its lower sequence alignment may adversely impact its semantic matching capabilities.\nAcross models, we observe a positive correlation between ROUGE-L F1 and LLM F1 Scores, suggesting that higher coherence aligns with improved overall quality ratings. Similarly, the strong correlation between BERT F1 and LLM F1 Scores indicates that better semantic alignment leads to higher quality evaluations.\nIn summary, our findings reveal that both ROUGE and BERT scores are positively correlated, capturing vocabulary coverage and semantic matching. Coherence and sequence information, as indicated by ROUGE-L F1, contribute significantly to semantic alignment and overall quality. The LLM Scores, as comprehensive metrics, offer valuable insights into the factors impacting summarization performance, underscoring their importance as benchmarks for model evaluation."}, {"title": "V. CONCLUSION AND FUTURE WORKS", "content": "This paper presents an empirical evaluation of GLM-4, Mistral-NeMo and LLaMA3.1 for text summarization tasks in the financial domain. Through a comprehensive analysis, we have identified the strengths and trade-offs associated with each model in terms of accuracy, coherence, and informativeness.\nOur findings indicate the following for summarizing financial reports:\n\u2022\tIruca-GLM-4 strikes a balance between speed and accuracy, Iruca-GLM-4 proves to be a versatile option. It is well-suited for tasks that require high-quality text within a moderate time frame, making it a practical option for most financial summarization needs.\n\u2022\tIruca-Mistral-NeMo is with a significant advantage in speed, this model offers the fastest summarization but may sacrifice some text quality in specific dimensions. If quick response time is the primary criterion, Iruca-Mistral-NeMo serves as an efficient choice.\n\u2022\tIruca-LLaMA3.1 demonstrates exceptional accuracy and high-quality text generation, albeit with the longest processing time. For tasks where accuracy and nuanced text quality are paramount and time constraints are minimal, Iruca-LLaMA3.1 is the preferred choice.\nIn our ongoing efforts to enhance LLM performance, we are actively engaged in fine-tuning to meet the practical requirements of our business partners, aiming for more accurate, relevant, and efficient outputs in real-world financial applications. Therefore, for the future work, we plan to explore knowledge distillation techniques to develop a finance-specific Small Language Model (SML), which would enable streamlined, domain-focused capabilities with lower computational demands.\nBesides, we notice that explainability remains a crucial and enduring challenge in LLM research, especially for applications in high-stakes fields like finance where trust and interpretability are paramount. We are committed to contributing to explainability research, focusing on methods that provide clearer insights into model decisions and align generated outputs with financial reasoning.\nLastly, to further validate and refine our models, we will collaborate with financial experts who will review and compare the generated summaries against original texts. Their expert feedback will be invaluable in assessing content accuracy, contextual appropriateness, and overall value, ultimately guiding us toward more robust and reliable LLMs in financial summarization tasks."}]}