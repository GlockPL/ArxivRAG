{"title": "Scalable Ensemble Diversification\nfor OOD Generalization and Detection", "authors": ["Alexander Rubinstein", "Luca Scimeca", "Damien Teney", "Seong Joon Oh"], "abstract": "Training a diverse ensemble of models has several practical applications such as\nproviding candidates for model selection with better out-of-distribution (OOD)\ngeneralization, and enabling the detection of OOD samples via Bayesian princi-\nples. An existing approach to diverse ensemble training encourages the models to\ndisagree on provided OOD samples.\nHowever, the approach is computationally expensive and it requires well-separated\nID and OOD examples, such that it has only been demonstrated in small-scale\nsettings.\nMethod. This work presents a method for Scalable Ensemble Diversification\n(SED) applicable to large-scale settings (e.g. ImageNet) that does not require OOD\nsamples. Instead, SED identifies hard training samples on the fly and encourages\nthe ensemble members to disagree on these. To improve scaling, we show how\nto avoid the expensive computations in existing methods of exhaustive pairwise\ndisagreements across models.\nResults. We evaluate the benefits of diversification with experiments on ImageNet.\nFirst, for OOD generalization, we observe large benefits from the diversification in\nmultiple settings including output-space (classical) ensembles and weight-space\nensembles (model soups). Second, for OOD detection, we turn the diversity of\nensemble hypotheses into a novel uncertainty score estimator that surpasses a large\nnumber of OOD detection baselines.", "sections": [{"title": "Introduction", "content": "Training an ensemble of diverse models is useful in multiple applications. Diverse ensembles are\nused to enhance out-of-distribution (OOD) generalization, where strong spurious features learned\nfrom the in-domain (ID) training data hinder generalization [25, 33, 38, 39]. By learning multiple\nhypotheses, the ensemble is given a chance to learn more predictive features that may otherwise be\novershadowed by prominent non-robust and spurious features [4, 49]. In Bayesian machine learning,\ndiversification of the posterior samples has been studied as a means to improve the precision and\nefficiency of sample uncertainty estimates [5, 45].\nA common strategy to train a diverse ensemble is to introduce a diversification objective while\ntraining the models in the ensemble in parallel [5, 25, 33, 35, 37]. The main loss (e.g. cross-entropy\nfor classification) encourages the models to fit the labeled training, while the diversification loss"}, {"title": "Diverse Ensembles through Prediction Disagreement", "content": "Setting. We denote our training data \\(D := \\{x_n, y_n\\}_{n=1}^N\\) and refer to it as the in-domain (ID)\ndata. Prior diversification methods based on \"prediction disagreement\" [25, 33] require a separate"}, {"title": "Proposed Method", "content": "We now present the Scalable Ensemble Diversification (SED) method. It improves upon A2D [33] by\neliminating the need for OOD data and improving its computational efficiency. The two technical\nnovelties are the dynamic selection of hard samples within the training data (\u00a73.1) and the stochastic\napplication to random pairs of models during training (\u00a73.2)."}, {"title": "Dynamic Selection of Hard Examples", "content": "With no OOD data, it is difficult to apply disagreement methods since all models are being trained to\nfit all available training examples, i.e. to agree. Yet in practice, such OOD that that clearly differs\nfrom the ID data may not be readily available. It is not even clear how to define and obtain such OOD\ndata where the feature-label correlations clearly differ from e.g. ImageNet.\nTo address this challenge, we propose to replace the OOD \u201cdisagreement data\" with a set of hard\ntraining examples identified dynamically during training. The models are then encouraged to disagree\non these examples. The desiderata for these hard samples are twofold: (a) we wish to discriminate\nsamples where the ensemble members make mistakes and (b) we only trust the ensemble prediction\nfor the hard sample identification when the ensemble is sufficiently trained.\nWe assign a sample-wise weight \\(a_n\\) to each training sample \\((x_n, y_n) \\in D\\):\n\\[\na_n := \\frac{CE(f^1, ..., f^M; x_n, y_n)}{\\left(\\sum_{b \\in B} CE(f^1, \\cdots, f^M; x_b, y_b)\\right)^2}\n\\]\nwhere \\(CE(f^1, ..., f^M; x_n, y_n) := CE(\\frac{1}{M}\\sum_{m} f^m(x_n), y_n)\\) is the loss on the logit-averaged pre-\ndiction and \\(B\\) is a mini-batch that contains the sample \\((x_n, y_n)\\). \\(a_n\\) is a weight proportional to the"}, {"title": "Tricks to Improve Scalability", "content": "Many diversification algorithms are based on exhaustive pairwise\ncomparisons between all the models in the ensemble (see the second\nterm of Equation 6). This scales quadratically with the size \\(M\\) of the\nensemble.\nWe propose to use a stochastic sum. For every mini-batch \\(B\\), we use\na random subset of models \\(I \\in \\{1,......, M\\}\\) on which to compute\nthe diversification term in Equation 6 (see figure on the right). In\nour experiments, we randomly sample one pair of models per batch\n(\\|I\\| = 2). Interestingly, we noticed empirically that this stochastic\nsum sometimes induces diversity by itself (without a diversification\nterm) and leads to better performance.\nTo further speed up the training, we consider updating only a subset\nof the layers of the model with the SED objective, keeping others\nfrozen. More specifically, each ensemble member in the experiments\nof Section 4 is based on a frozen Deit3b model [40] of which we\ndiversify only the last two layers."}, {"title": "Predictive Diversity Score (PDS) for OOD Detection", "content": "We now describe how to use diverse ensembles for OOD detection [14]. This is based on evaluating\nthe epistemic uncertainty, which is the consequence of the lack of training data in a given regions of\nthe input space [31, 22]. In these OOD regions, the lack of supervision means that diverse models are\nlikely to disagree in their predictions [30, 25, 33]. We therefore propose to use the agreement rate\nacross models on given sample to estimate the epistemic uncertainty and its \u201cOODness\u201d.\nBMA Baseline. Given an ensemble of models, a simple baseline for OOD detection is to compute\nthe predictive uncertainty of the Bayesian Model Averaging (BMA) by treating the ensemble members\nas samples of the posterior \\(p(\\theta|D)\\) [24, 45]:\n\\[\n\\sigma_{BMA} := \\max_c \\frac{1}{M} \\sum_m p^m(x).\n\\]\nWhile being a strong baseline [31] for OOD detection this notion of uncertainty does not directly\nexploit the potential diversity in individual models of the ensemble because it averages out the\npredictions along the model index m. In addition to that, mimicking the true distribution makes"}, {"title": "Experiments", "content": "We present experiments that first evaluate the intrinsic diversification from SED (\u00a74.2) then evaluate\nseveral use cases of diverse ensembles for OOD generalization (\u00a74.3) and OOD detection (\u00a74.4)."}, {"title": "Experimental Setup", "content": "Implementation. For both tasks, we train an ensemble of models with SED based on A2D [33]\nusing the AdamW optimizer [29], a batch size varies from 16 to 256, learning rate from 10-4 to\n10-3, weight decay is fixed to 0.01, and number of epochs to 10. The diversity weight \\(\\lambda\\) varies from\n0 to 5 and the stochastic pairing is done for \\(|I| = 2\\) models for each mini-batch (see Table 10 for\nan evaluation of other values). All experiments use models based on the Deit3b architecture [40]\npretrained on ImageNet21k [8]. As suggested in \u00a73.2 we train only the last 2 layers. As in-domain\n(ID) data we use the training split of ImageNet ( \\(|D| = 1,281, 167\\). All experiments were run on\nRTX2080Ti GPUs with 12GB VRAM and 40GB RAM. Each experiment took between 2 to 12 hours.\nBaselines. As a simple ensemble we use a variant of deep ensembles [24], which uses models trained\nindependently with different random seeds.\nTo match the resource usage of our SED, we also train only the last 2 layers of the models (i.e. they\nare \"shallow ensembles\").\nWe also consider simple ensembles of models with diverse hyperparameters [44]. We reimplemented\nA2D [33] and DivDis [25], with which we use unlabeled samples from ImageNet-R as disagreement\ndata (the choice of dataset used for disagreement has little influence on the results, as seen in\nTable 9). For A2D, we use a frozen feature extractor and parallel training, i.e. all models are trained\nsimultaneously rather than sequentially.\nEvaluation of OOD generalization. We evaluate the classification accuracy of the ensembles trained\non ImageNet with the (ID) validation split of ImageNet (IN-Val, 50,000 samples) and multiple OOD\ndatasets: ImageNet-A (IN-A [18], 7.5k images & 200 classes), ImageNet-R (IN-R [17], 30k images,\n200 classes), ImageNet-C (IN-C-i or just C-i for corruption strength i [15], 50k images, 1k classes),\nOpenImages-O (OI [43], 17k images, unlabeled), and iNaturalist (iNat [21], 10k images, unlabeled).\nEvaluation of OOD detection. The task is to differentiate samples from the above OOD datasets\nagainst those from the ImageNet validation data (considered as ID). The evaluation includes both\n\"semantic\u201d and \u201ccovariate\u201d types of shifts [52, 15, 17, 34, 48]. Openimages-O and iNaturalist represent\nsemantic shifts because their label sets are disjoint from ImageNet's. And ImageNet-C represents\na covariate shift because its label set is the same as ImageNet's but the style of images differs. We\nmeasure the OOD detection performance with the area under the ROC curve, following [16]."}, {"title": "Diversification", "content": "We start with the question of whether SED truly diversifies the ensemble. To measure the diversity of\nthe ensemble, we compute the number of unique predictions for each sample for the committee of\nmodels (#unique)."}, {"title": "OOD Generalization", "content": "We examine the first application of diverse ensembles: OOD generalization. We hypothesize that\nthe superior diversification ability verified in \u00a74.2 leads to greater OOD generalization due to the\nconsideration of more robust hypotheses that do not rely on obvious spurious correlations.\nEnsemble aggregation for OOD generalization. As a means to exploit such robust hypotheses,\nwe consider 3 aggregation strategies. (1) Oracle selection: the best-performing individual model\nis chosen from an ensemble [33, 38]. The final prediction is given by \\(f(x;\\theta_{m^*})\\) where \\(m^* :=\n\\arg \\max_m Acc(f^m, D_{ood})\\). (2) Prediction ensemble is a vanilla prediction ensemble where the logit\nvalues are averaged: \\(\\frac{1}{M}\\sum_m f^m(x)\\) [46]. (3) Uniform soup [46] averages the weights themselves.\nThe final prediction is given by \\(f(x; \\sum_m \\theta_m)\\).\nSED improves OOD generalization for ensembles. We show the OOD generalization performance\nof ensembles in Table 2, for the three ensemble prediction aggregation strategies described above. We\nobserve that our SED framework (SED-A2D) results in superior OOD generalization performance\nfor the prediction ensemble and uniform soup while being on par with best baselines for the oracle\nselection. SED-A2D is particularly strong in the prediction ensemble (e.g. 48.7% for \\(M = 5\\) and\n53.8% for \\(M = 50\\) on ImageNet-R) and uniform soup (e.g. 46.1% for \\(M = 5\\) and 46.5% for \\(M = 50\\)\non ImageNet-R). We contend that the increased ensemble diversity contributes to the improvements in\nOOD generalization. We also remark that the SED framework (SED-A2D) envelops the performance\nof A2D in this ImageNet-scale experiment. Together with the superiority of computational efficiency\n(as discussed at the end of \u00a7 4.4) of SED-A2D over the A2D, this demonstrates that SED fulfills its\npurpose of scaling up ensemble diversification methods like A2D.\nDeep ensembles are a strong baseline. We also note that deep ensemble, particularly with diverse\nhyperparameters, provides a strong baseline, outperforming dedicated diversification methodologies\nunder the oracle selection strategy when \\(M = 5\\). It also provides a good balance between ID\n(ImageNet validation split) and OOD generalization."}, {"title": "OOD Detection", "content": "We study the impact of ensemble diversification on OOD detection capabilities of an ensemble. Once\nan ensemble is trained, we compute the epistemic uncertainty, or likelihood of the sample being OOD,\nfollowing two schemes, \\(\\sigma_{BMA}\\) and \\(\\sigma_{PDS}\\) introduced in \u00a73.3.\nSED and PDS together lead to superior OOD detection performance. We show the OOD detection\nresults in Table 3. We chose BMA because it is considered a standard baseline [31] for uncertainty\nquantification, comparison to other OOD detection methods can be seen in Table 6. For the B\u041c\u0410\nscores, deep ensemble remains a strong baseline. In particular, when the hyperparameters are varied\n(\"+Diverse HPs\u201d), the detection AUROC reaches the maximal performance among the ensembles\nusing the BMA scores. The quality of PDS is more sensitive to the ensemble diversity, as seen in\nthe jump from the deep ensemble (e.g. 0.589 for OpenImages) to the diverse-HP variant (0.889)."}, {"title": "Related Work", "content": "Ensembling is a well-known technique that aggregates the outputs of multiple models to make more\naccurate predictions [2, 3, 12, 19, 23]. It is well known that diversity in the outputs of the ensemble\nmembers leads to better gains in performance [23] because they make independent errors [11, 12].\nIn addition, it has been shown empirically and theoretically [50, 13] that diverse ensembles can also\nimprove OOD generalization.\nDiversity through regularizers. Various auxiliary training objectives have been proposed to encour-\nage diversity across models' weights [5, 6, 7, 42], features [4, 49, 50], input gradients [35, 38, 39, 41],\nor outputs [5, 25, 28, 33, 37]. D'Angelo and Fortuin [5] showed that a regularizer that repulses the\nensemble members' weights or outputs leads to ensembles with a better approximation of Bayesian\nmodel averaging. This idea was extended by repulsing features [49] and input gradients [41]. Since\nensemble are most useful when the errors of its members are uncorrelated [23], the closest of the\nabove objective is to diversify their outputs. This cannot be guaranteed with other objectives such\nas weight diversity for example, since two models could implement the exact same function with\ndifferent weights due to the many symmetries in the parameter space of neural networks. For this\nreason, this paper focuses on methods for output-space diversification [25, 33]. These were also\nhighlighted as state-of-the-art in a recent survey on diversification [1].\nDiversity without modifying the training objective. The most straightforward way to obtain diverse\nmodels is to independently train them with different seeds (Deep Ensembles [24] and Bayesian\nextensions [45]), hyperparameters [44], augmentations [26], or architectures [51]. A computationally\ncheaper approach is to use models saved at different points during the training [20] or models derived\nfrom the base model by applying dropout [10] or masking [9]. The \u201cmixture of experts\u201d paradigm [53]\ncan also be viewed as an ensemble where diversification happens by assigning different training\nsamples to different ensemble members. Our experiments use Deep Ensembles [24] and ensembles\nof models trained with different hyperparameters [44] as baselines since they are strong approaches\nto OOD detection [32] and OOD generalization especially when combined with \u201cmodel soups"}, {"title": "Conclusions", "content": "Ensemble diversification has many implications for treating one of the ultimate goals of machine\nlearning, handling out-of-distribution (OOD) samples. Training a large number of diverse hypotheses\non a dataset is a way to generate candidates that may have the desired OOD behaviour (i.e. better\nOOD generalization). And the diversity of hypotheses can help distinguish ID from OOD samples\nby measuring disagreements across ensemble members. Despite these benefits, diverse-ensemble\ntraining has previously remained a lab-bound concept for two reasons. Previous approaches were\ncomputationally expensive (scaling quadratically with ensemble size) and required a separate OOD\ndataset to nurture the diverse hypotheses.\nWe have addressed these challenges through the novel Scalable Ensemble Diversification (SED)\nmethod. SED identifies OOD-like samples from the training data, bypassing the need to prepare a\nseparate OOD data. SED also employs a stochastic pair selection to reduce the quadratic complexity\nof previous approaches to a constant one. We have demonstrated good performance of SED on OOD\ngeneralization and detection tasks, both at the ImageNet scale, a largely underexplored regime in the\nensemble diversification community. In particular, for OOD detection, our novel diversity measure of\nPredictive Diversity Score (PDS) amplifies the benefits of diverse ensembles for OOD detection.\nLimitations. This work has focused on solving the applicability of disagreement-based diversifica-\ntion on realistic datasets. The contributions are thus mostly in the implementation, and the results\nfocus on empirical benefits. Work is needed to examine theoretical justifications for the method and\ncharacterize the exact conditions under which it should provide benefits.\nSimilarly, the proposed PDS is a conceptually sound measure of epistemic uncertainty, but work is\nalso needed to characterize the exact conditions where it is practically superior to other baselines."}, {"title": "Appendices", "content": ""}, {"title": "Varying the Number of Trainable Layers", "content": "To perform an ablation study on the number of layers diversified for each ensemble member we\ntrained only one last layer of DeiT-3b and compared it to the ensemble from the main experiments\nwith the last two layers trained. Both ensembles have size 5 and were trained on the ImageNet\ntraining split. The results can be seen in Table 4. Generalization performance did not change much,\nwith the biggest change for ImageNet-C with the corruption strength 5 where ensemble accuracy\ndropped from 40.8% for one layer to 40.6% for two layers. However, OOD detection performance\nis better across the board for the case when two layers are diversified, for example, the detection\nAUROC scores for one layer diversified vs two layers diversified are 0.928 vs 0.941 for OpenImages\nand 0.964 vs 0.977 for iNaturalist. We believe that it can be explained by the fact that when one linear\nlayer is trained with cross-entropy loss the optimization problem becomes convex making it harder\nfor disagreement regularizer to promote diversity for different solutions, i.e. ensemble members tend\nto have similar weight matrices and disagree on OOD samples less."}, {"title": "Other Backbones", "content": "To check the applicability of our method to other architectures we trained an ensemble of 5 models\nwith the whole model but last layer frozen using ResNet18 as a feature extractor. We compared\nSED with A2D disagreement regularizer and stochastic sum size I = 2 vs deep ensemble in Table 5.\nBoth ensembles were trained on the ImageNet training split. Deep ensemble and SED-A2D have\nsimilar generalization performance, with the biggest difference for ImageNet-C with the corruption\nstrength 1 where ensemble accuracy dropped from 51.9% for deep ensemble to 51.8% for SED-A2D .\nNevertheless, SED-A2D shows better OOD detection performance across the board, for example, the\ndetection AUROC scores for one deep ensemble vs SED-A2D are 0.802 vs 0.812 for OpenImages and\n0.865 vs 0.973 for iNaturalist. Ensemble accuracy on ImageNet-A is less than 1% for both ensembles:\n0.5% and 0.6% because this dataset was created with a goal to minimize ResNet performance on it."}, {"title": "Other Uncertainty Scores", "content": "To perform an ablation study on the OOD detection methods we compared PDS to other uncertainty\nscores computed for the outputs of SED with A2D disagreement regularizer trained on the ImageNet\ntraining split. The ensemble size is 5, stochastic sum size I = 2. Results can be seen in the Table 6.\nIn addition to popular baselines [27, 47], we also used A2D disagreement regularizer as an uncertainty\nscore (A2D-score in the table). PDS performed on par or better than other methods, for example,\nthe biggest gap is achieved on OpenImages with detection AUROC score 0.941 vs 0.917 against\nA2D-score, and the smallest gap is achieved on iNaturalist with detection AUROC of 0.977 for both\nPDS and Average Energy."}, {"title": "Comparison to a Two-Stage Approach", "content": "To perform an ablation study on the way samples for disagreement are selected in Table 7 we\ncompared an ensemble trained with Equation 6 (called \"joint\" in the table) against a 2-stage approach.\nInstead of disagreeing on all samples with adaptive weight an as in Equation 6 we first computed\nthe confidence of the pre-trained DeiT-3B model on all samples in ImageNet training split and then\nselected samples with a confidence lower than 0.2 which resulted in 18002 samples (to approximately\nmatch the sizes of ImageNet-A and ImageNet-R). Then we trained an ensemble by minimizing A2D\ndisagreement regularizer on these samples while minimizing cross-entropy on all other samples.\nBoth ensembles had size 5 and stochastic sum size I = 2. While such an approach might sound\nsimpler, SED is more straightforward and efficient, since there is no need to train an initial model to\ndetermine samples for disagreement. Both ensembles have a similar generalization performance, with\nthe biggest difference for ImageNet-R where ensemble accuracy dropped from 48.5% for 2-stage\napproach to 48.1% for the joint. In contrast, OOD detection performance is significantly better across\nthe board for the joint approach, for example, the detection AUROC scores are 0.845 vs 0.896 for\nImageNet-C with corruption strength 5 and 0.911 vs 0.941 for OpenImages. We think that such a\ndrastic difference in OOD detection performance can be caused by the fact that the set of samples\nselected for disagreement may be suboptimal which makes the confidence threshold (set as 0.2 for this\nexperiment) an important hyperparameter and adds even more complexity to the 2-stage approach."}, {"title": "Small-Scale Experiments", "content": "To check the performance of our method on the small-scale datasets, we conducted additional\nexperiments on the Waterbirds dataset [36] (Table 8), since both A2D and DivDis also provided\nresults on it. We report the worst group (test) accuracy for ensembles of size 4. We trained A2D,\nDivDis, and an ensemble with SED and A2D disagreement regularizer on Waterbirds training split.\nWe did not use stochastic sum for SED-A2D to factor out its influence. A2D and DivDis used the\nvalidation set for disagreement. While DivDis discovers a better single model having best accuracy\nof 87.2% against 83.2% for the proposed SED-A2D method, the ensemble is clearly better with\nSED-A2D: 80.6% vs 78.3% for DivDis."}, {"title": "OOD Datasets for Disagreement", "content": "To analyze the influence of OOD data used for disagreement we performed additional experiments\nwith ensemble members disagreeing on ImageNet-R and ImageNet-A in Table 9. We compare\nA2D and Div [25] diversification regularizers. Usage of ImageNet-A or ImageNet-R resulted in"}, {"title": "Variations of the Stochastic Sum Size", "content": "We performed an additional evaluation (Table 10) that shows the benefit of controlling the stochastic\nsum size (1) on the speed of training an ensemble. For example, to train an ensemble of size 5, the\ntime required for 1 epoch grows from 53s for I = 2 to 585s for I = 5 (without stochastic sum). We\ncould not train an ensemble of 50 models without stochastic sum with our resources, but it already\nrequires 7244s for I = 10 vs 2189s for I = 2. Standard deviations of training epoch times are\ncomputed across 10 different epochs."}]}