{"title": "Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation", "authors": ["Minhua Lin", "Zhengzhang Chen", "Yanchi Liu", "Xujiang Zhao", "Zongyu Wu", "Junxiang Wang", "Xiang Zhang", "Suhang Wang", "Haifeng Chen"], "abstract": "Time series data is ubiquitous across various domains, including manufacturing, finance, and healthcare. High-quality annotations are essential for effectively understanding time series and facilitating downstream tasks; however, obtaining such annotations is challenging, particularly in mission-critical domains. In this paper, we propose TESSA, a multi-agent system designed to automatically generate both general and domain-specific annotations for time series data. TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent. The general agent captures common patterns and knowledge across multiple source domains, leveraging both time-series-wise and text-wise features to generate general annotations. Meanwhile, the domain-specific agent utilizes limited annotations from the target domain to learn domain-specific terminology and generate targeted annotations. Extensive experiments on multiple synthetic and real-world datasets demonstrate that TESSA effectively generates high-quality annotations, outperforming existing methods.", "sections": [{"title": "Introduction", "content": "Time series data is prevalent in fields such as manufacturing (Hsu and Liu, 2021), finance (Lee et al., 2024), and healthcare (Cascella et al., 2023), where it captures critical temporal patterns essential for informed decision-making. However, general users frequently encounter difficulties in interpreting this data due to its inherent complexity, particularly in multivariate contexts where multiple variables interact over time. Furthermore, effective interpretation typically requires domain-specific knowledge to properly contextualize these patterns, thereby posing significant challenges for individuals without specialized expertise.\nHigh-quality annotations are crucial for addressing these interpretive challenges. Annotations provide meaningful context or insights into the time series data, highlighting important patterns, events, or anomalies. They facilitate accurate analysis, forecasting, and decision-making, enhancing the performance of downstream tasks such as anomaly detection, trend prediction, and automated reporting. For instance, in predictive maintenance, understanding sensor data trends is vital for preventing equipment failure, while in finance, interpreting stock price movements is crucial for informed investment strategies. Despite their importance, high-quality annotations are often scarce in real-world applications. This scarcity stems primarily from the reliance on domain experts for manual annotation, which is resource-intensive, costly, and prone to inconsistencies. Moreover, the need for precise and domain-specific terminology further complicates the annotation process, as different fields require highly specialized knowledge for accurate and contextually relevant interpretation.\nTo alleviate the above issues, one straightforward approach is to leverage external resources to generate annotations (Liu et al., 2024a). For example, Time-MMD (Liu et al., 2024a) uses web searches to retrieve information as annotations, aiming to find similar patterns and descriptions from the internet. Others (Jin et al., 2024; Liu et al., 2024b) directly apply large language models (LLMs) for annotation, leveraging their vast language understanding capabilities. Prototype-based methods, such as prototype networks (Ni et al., 2021), have also been employed to identify representative examples for annotation. However, these methods often fall short of producing high-quality annotations. Web search-based methods may retrieve irrelevant or inconsistent information. LLMs, while powerful, tend to generate annotations that are generic, capture only basic patterns, or even hallucinate, failing to account for the complex nature of time series data. Prototype networks rely on large amounts of data to train the network and identify representative prototypes, but the scarcity of high-quality annotations limits the quality and representativeness of these prototypes, making it difficult to generalize effectively to new or unseen patterns.\nTo address these limitations, we propose to extract knowledge from existing annotations across multiple source domains and transfer this knowledge to target domains with limited annotations. Specifically, we aim to develop a system that can automatically interpret time series data across various fields using either common or domain-specific language. Formally, given abundant annotations from multiple source domains and limited annotations from a target domain, our goal is to leverage both time-series-wise and text-wise knowledge to generate accurate and contextually appropriate annotations for the target domain.\nThere are two major technical challenges in developing such a system: (i) How to extract common knowledge from source domains? (ii) How to learn domain-specific jargon from limited target-domain annotations? To tackle these challenges and overcome the limitations of existing methods, we propose TESSA, a multi-agent system designed for both general and domain-specific TimE Series Annotation. As illustrated in Figure 1, TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent. The general annotation agent focuses on capturing common patterns and knowledge across various domains to generate annotations understandable by general users. To learn common knowledge from multiple domains, the general agent employs a time series-wise feature extractor and a text-wise feature extractor to extract both time-series-wise and text-wise features from time series data and domain-specific annotations from multiple source domains. To ensure important features are included in the general annotations, two feature selection methods\u2014LLM-based and reinforcement learning-based selection\u2014are introduced to effectively and efficiently select both the top-k most important time-series-wise and text-wise features. The domain-specific agent leverages limited target-domain annotations to learn and generate annotations for specific domains using domain-specific terminologies (jargon). It incorporates a domain-specific term extractor to learn jargon from the limited target-domain annotations. Additionally, an annotation reviewer is proposed to maintain consistency between general annotations and domain-specific annotations.\nOur contributions are: (i) Problem. We explore a novel problem in cross-domain multi-modal time series annotation, bridging the gap between general understanding and domain-specific interpretation; (ii) Framework. We propose a novel multi-agent system, TESSA, designed for both general and domain-specific time series annotation by leveraging both time-series-wise and text-wise knowledge from multiple domains; (iii) Datasets. We collect a real-world dataset from finance domain to leverage cross-domain knowledge, along with a synthetic dataset to evaluate TESSA. These datasets are released to support future research and development in this field. (iv) Experiments. Extensive experiments on multiple synthetic and real-world datasets demonstrate the quality of the general and domain-specific annotations generated by TESSA."}, {"title": "Related Work", "content": "Time Series Annotation. Time series annotation aims to assign labels or descriptions to specific segments, events, or patterns within a time series dataset to highlight significant features for further analysis. Traditionally, this process has relied on manual annotation (Reining et al., 2020), which is often time-consuming, labor-intensive, and requires substantial domain expertise. To reduce the effort needed for creating large-scale, high-quality annotated datasets, several studies have proposed semi-automatic annotation approaches (Cruz-Sandoval et al., 2019; Nino et al., 2016) that require minimal manual input or post-annotation revisions. Despite these advancements, fully automated time series annotation remains underexplored due to the challenges of capturing semantic and contextual information from the data (Yordanova and Kr\u00fcger, 2018).\nLLMs for Time Series Analysis. Recent advancements in LLMs have showcased their strong capabilities in sequential modeling and pattern recognition, opening up promising new directions for time series analysis. Several studies (Xue and Salim, 2023; Yu et al., 2023; Gruver et al., 2024; Jin et al., 2024; Li et al., 2024) have explored how LLMs can be effectively leveraged in this context. For instance, PromptCast (Xue and Salim, 2023) is a pioneering work that applies LLMs to general time series forecasting using a sentence-to-sentence approach. Yu et al. (Yu et al., 2023) extend this by investigating the application of LLMs to domain-specific tasks, such as financial time series forecasting. LLMTime (Gruver et al., 2024) demonstrates the efficacy of LLMs as time series learners by employing text-wise tokenization to represent time series data. Time-LLM (Jin et al., 2024) reprograms time series data into textual prototypes for input into LLaMA-7B, enriched with natural language prompts that include domain expert knowledge and task-specific instructions. Additionally, Li et al. (Li et al., 2024) illustrate how a frozen language model can enhance zero-shot learning in ECG time series analysis, showing the potential of LLMs to extract valuable features from complex time series data.\nCross-modality Knowledge Transfer Learning through Pre-trained Models. There has been growing interest in leveraging pre-trained models for cross-modality knowledge transfer, particularly between the language, vision, and time series domains (Bao et al., 2022; Lu et al., 2022; Yang et al., 2021; Zhou et al., 2023). For instance, Bao et al. (2022) proposes a stagewise pre-training strategy that trains a language expert using frozen attention blocks pre-trained on image-only data. Similarly, Lu et al. (2022) examines the transferability of language models to other domains, while Zhou et al. (2023) applies pre-trained language and image models to time series analysis tasks. To the best of our knowledge, no previous work has specifically explored cross-modality knowledge transfer for time series annotation. Our work aims to fill this gap by investigating the application of cross-modality transfer learning in the context of automatic time series annotation."}, {"title": "Methodology", "content": "In this section, we define the problem and present the details of our proposed TESSA framework, which aims to generate both general and domain-specific annotations for time series data.\nCross-Domain Time Series Annotation Problem. Given several source domains ${D_{s_1}, D_{s_2}, ...}$ and a target domain $D_t$, let ${e_{s_1}^1, e_{s_1}^2,...}$ denote the domain-specific annotations from the source domain $D_{s_i}$, and ${e_{t}^1, e_{t}^2, ...}$ represent the limited domain-specific annotations from the target domain $D_t$. Suppose $X = (x_1,\u00b7\u00b7\u00b7, x_L)$ is a time series in $D_t$, where $L$ is the number of past timestamps and $x_i = (x_{1i},\u2026\u2026, x_{ci})^T \u2208 R^C$ represents the data from $C$ different channels at timestamp $i$. The objective of cross-domain time series annotation is to generate the general annotation $e_g$ and the domain-specific annotation $e_s$ for $X$ based on the annotations from both the source and target domains. More notations are provided in Appendix B.\nOverview of TESSA. As illustrated in Fig. 1, the proposed TESSA comprises two key components: a general annotation agent and a domain-specific annotation agent. The general annotation agent is responsible for generating domain-independent annotations and consists of several modules: a time series feature extraction module to capture time-series-specific features, a domain decontextualization module to convert domain-specific text into common language, a text feature extraction module to retrieve textual features from the decontextualized text, two policy networks for selecting the top-k most salient time-series and textual features, and a general annotator to produce general annotations"}, {"title": "Multi-modal Feature Extraction", "content": "To address the challenge of extracting common knowledge from source domains, we introduce two feature extraction modules: a time-series feature extractor and a text-wise feature extractor, which extract features from time series data and source-domain annotations. We also propose a domain decontextualizer to enhance the extraction of common knowledge from multi-source annotations.\nTime Series Feature Extraction. We develop a time series extraction toolbox $f_t$ to extract various features from given time series data. Formally, for each channel $c\u2208 C$, the set of text-series features $F_t$ is denoted as:\n$F_t = {f_t^1,\u2026, f_t^{n_t}} = M_t(X)$ (1)\nwhere $f_i$ is the i-th extracted feature of $X$, and $n_t$ is the number of extracted features. For multivariate time series data, inter-variable features (e.g., Pearson correlation) are also included. More details on feature extraction can be found in Appendix C.\nDomain Decontextualization. In addition to time-series-wise features, textual annotations from source domains often contain valuable information (such as support or resilience in finance time series annotations) for interpreting time series data. A straightforward method to extract this knowledge is to use LLMs on domain-specific annotations, leveraging their real-world knowledge. However, in practice, many domains lack sufficient high-quality annotations, and domain-specific terminology can further hinder effective extraction.\nTo address these challenges and facilitate knowledge transfer from source to target domains, we introduce a domain decontextualization LLM to convert domain-specific annotations into general annotations by removing domain-specific terminology. This makes it easier to extract common knowledge across domains. Specifically, given a domain-specific annotation $e_s$ in domain $d_i$, the decontextualized annotation $e_d$ is obtained as:\n$e_d = M_d(P_{de}(e_s, d_i))$, (2)\nwhere $M_d$ is the domain decontextualization LLM.\nText Feature Extraction. After decontextualization, we use an LLM $M_l$ to extract textual features from multiple source domains. Formally, given a set of decontextualized annotations ${e_d}^i_{i=1}^{n_d}$ and the text feature extractor $M_l$, the extracted textual features are denoted as:\n$F_l = {f_l^1,\u2026, f_l^{n_l}} = M_l(p_l({e_d}^i_{i=1}^{n_d}))$, (3)\nwhere $p_l$ is the prompt for text feature extraction."}, {"title": "Adaptive Feature Selection", "content": "With a diverse set of features extracted from time series and text data, it becomes essential to focus on the most relevant ones to ensure the generated annotations remain concise and interpretable. Moreover, repeatedly querying LLMs with both the old and new data each time wastes computational resources and incurs additional costs, especially when using non-open-source models.\nTo address these issues, we propose a hybrid strategy for adaptive feature selection that combines Offline LLM-based Feature Selection with Incremental Reinforcement Learning-based Feature Selection. The incremental method builds on the offline approach, minimizing the need to re-query LLMs with both old and new data as it arrives.\nOffline LLM-based Feature Selection. Leveraging LLMs' reasoning abilities, we introduce a feature selection method using LLM-generated feature importance scores to identify the top-k most important text-series-wise and text-wise features. Features mentioned more frequently either explicitly or implicitly\u2014in annotations are assigned higher importance scores.\nSpecifically, given an LLM as the feature selector $M_{sel}$, we prompt $M_{sel}$ with domain-decontextualized annotations ${e_d}^i_{i=1}^{n_d}$ and the extracted features ${f_t^i}^{n_t}_{i=1}$ and ${f_l^i}^{n_l}_{i=1}$ to generate numerical feature importance scores: $s_t= [s_1,\u2026, s_{n_t}]$ for time-series-wise features and $s_l= [s_1,\u2026\u2026, s_{n_l}]$ for text-wise features.\n$s_j = M_{sel}(p_{score}(f_t^i, {e_d}^i_{i=1}^{n_d})), \u2200j \u2208 {1,\u2026,n_t}$,\n$s_k = M_{sel}(p_{score}(f_l^i, {e_d}^i_{i=1}^{n_d})), \u2200k \u2208 {1,\u2026,n_l}$, (4)\nHere, $p_{score}$ is the prompt used to score feature importance. Higher scores, $s_j$ and $s_k \u2208 R^+$, indicate that the features $f_t^j$ and $f_l^k$ appear more frequently, either explicitly or implicitly, in the domain-decontextualized annotations ${e_d}^i_{i=1}^{n_d}$. To ensure that explicitly mentioned features receive higher importance scores, we instruct $M_{sel}$ to assign greater weight to features that are explicitly referenced in the annotations. Further details are in Appendix D.1.\nIncremental Reinforcement Learning-based Feature Selection. When new data arrives, the offline LLM-based approach requires re-querying both old and new data, which becomes burdensome due to LLMs' limited context window. As annotations increase, re-querying all data becomes impractical and costly, leading to higher resource consumption and reduced cost-effectiveness.\nTo address the limitations of the offline approach, we propose an Incremental Reinforcement Learning-based Feature Selection method that is more cost-effective for dynamic environments with evolving data. Specifically, we introduce a multi-agent reinforcement learning (MARL) framework to train two policy networks, $F_t$ and $F_l$, to select the top-k most important time-series-wise and text-wise features, respectively. These policy networks store knowledge from existing annotations and are incrementally updated as new data arrives. This reduces the need to re-query the LLM with all the data, requiring only the new data during updates. As shown in Figure 1, each policy network is initialized with the first three layers of a small LLM, such as GPT-2 (Radford et al., 2019), which remain frozen during training. A trainable multi-head attention layer and a language model (LM) head from GPT-2 follow these layers, using the smallest version of GPT-2 with 124M parameters.\nDuring training, only the multi-head attention layer is updated. For time-series-wise features, given the candidate features ${f_t^i}^{n_t}_{i=1}$ and their corresponding feature name tokens $Y = {y_1,..., y_{n_t}}$, the policy network $F_t$ computes action-values (Q-values) $q_z = [q_z,f_t^1,\u00b7\u00b7\u00b7, q_z,f_t^{n_t}]$ based on the mean logits of the feature names:\n$q_s = F_t({Y_i}^1_{i=1})$, (5)\nA softmax function generates a probability distribution over the features, and the top-k features are selected based on the highest probabilities.\nAt each timestep, the selected top-k features are passed to the LLM $M_{sel}$ to obtain their importance scores $s_i$, \u2200i \u2208 {1,\u2026\u2026,k}. The agent receives a reward $r_t$ defined as:\n$r_t = \begin{cases} \\sum_{i=1}^k s_i, & \text{if } \\sum_{i=1}^k s_i \\ge \u03c4 \\ -0.5, & \text{otherwise,} \\end{cases}$ (6)\nwhere \u03c4 is a threshold to discourage selecting unimportant features. The text-wise feature policy network $F_l$ undergoes a similar training process.\nAfter training, the policy networks are incrementally updated with only new data, eliminating the need to re-query the LLM with both old and new data. This approach improves the scalability and efficiency of feature selection while reducing computational costs, effectively overcoming the offline approach's limitations. By incrementally updating the policy networks, we ensure that feature selection remains scalable and cost-effective in dynamic environments with evolving data."}, {"title": "General Annotation Generation", "content": "After selecting the top-k most important features from both time-series and text, a general annotator is introduced to generate general annotations by analyzing these selected features. An LLM, serving as the general annotator, interprets the given time series data based on the selected features. Formally, given time series data $X = {x_i}^L_{i=1}$ and the selected time-series-wise and text-wise features ${f_t^i}^{k_t}_{i=1}$ and ${f_l^i}^{k_l}_{i=1}$, the generation of a general annotation $e_g$ is represented as:\n$e_g = M_{gen}(p_{gen}({x_i}^L_{i=1}, {f_t^i}^{k_t}_{i=1}, {f_l^i}^{k_l}_{i=1}))$, (7)\nwhere $p_{gen}$ is the prompt for generating general annotations. By emphasizing the signal from the selected common knowledge, the general annotations capture richer patterns that may be overlooked when directly applying LLMs."}, {"title": "Domain-specific Annotation Generation", "content": "Generating domain-specific annotations for time series is crucial as different domains rely on specialized jargon and context-specific terminology to accurately interpret and understand data. Time series data from financial markets, healthcare systems, or industrial processes can exhibit patterns, trends, and anomalies that are unique to each domain. General annotations may overlook critical nuances, whereas domain-specific annotations capture contextual relevance, improving the precision and reliability of downstream analysis or model predictions. By tailoring annotations to a domain's specific lexicon, we can detect meaningful patterns more accurately and make informed decisions.\nDomain-specific Term Extractor. To address the challenge of learning domain-specific terminology, we introduce a domain-specific term extractor. Given limited domain-specific annotations ${e_t}^i_{i=1}^{n_et}$ from the target domain, an LLM $M_{ext}$ is employed to extract domain-specific terms. We prompt $M_{ext}$ with the annotations ${e_t}^i_{i=1}^{n_et}$ to extract a set of domain-specific terms ${\\Im}^i_{i=1}^{n_\\Im}$:\n${\\Im}^i_{i=1}^{n_\\Im} = M_{ext}(p_{ext}({e_t}^i_{i=1}^{n_et}))$, (8)\nwhere $n_\\Im$ is the number of extracted terms, and $p_{ext}$ is the prompt for domain-specific term extraction.\nDomain-specific Annotator. To ensure alignment between domain-specific and general annotations, an LLM $M_{spe}$, acting as a domain-specific annotator, applies the extracted terms ${\\Im}^i_{i=1}^{n_\\Im}$ to general annotations $e_g$, converting them into target-domain annotations $e_t$. Formally, this is represented as:\n$e_t = M_{spe}(p_{spe}(e_g, {\\Im}^i_{i=1}^{n_\\Im}))$, (9)\nwhere $p_{spe}$ is the prompt for generating domain-specific annotations.\nAnnotation Reviewer. To improve the quality of domain-specific annotations and ensure better alignment with general annotations, we introduce an annotation reviewer. This LLM, $M_{rev}$, reviews the generated annotations and extracted terms, providing feedback $e_f$ to the extractor and annotator:\n$e_f = M_{rev}(p_{rev}(e_g, e_t, {\\Im}^i_{i=1}^{n_\\Im}))$, (10)\nwhere $p_{rev}$ is the prompt for reviewing annotations. This feedback loop ensures more precise term extraction and better alignment between general and domain-specific annotations. Based on the feedback, the extractor $M_{ext}$ refines the extraction process, and the annotator $M_{spe}$ enhances its annotations accordingly."}, {"title": "Experiments", "content": "This section presents the experimental results. We first evaluate the TESSA's annotations in downstream tasks and on a synthetic dataset, then examine domain-specific annotations, and finally assess the contribution of key TESSA components."}, {"title": "Experimental Setup", "content": "Dataset. To evaluate the effectiveness of TESSA, five real-world datasets from distinct domains are considered: Stock, Health, Energy, Environment, and Social Good. Specifically, the stock dataset includes 1,935 US stocks with the recent 6-year data, collected by ourselves. The other four datasets come from the public benchmark Time-MMD (Liu et al., 2024a). In this paper, the Stock and Health datasets serve as the source domains, while the Energy, Environment, and Social Good datasets are treated as the target domains. Additionally, we generate a synthetic dataset containing both time series and ground-truth annotations to directly assess the quality of general annotations. More details on these datasets can be found in Appendix E.1. The rest four datasets are from a public benchmark Time-MMD. Additionally, we create a synthetic dataset with both time series and ground-truth annotations to directly evaluate the quality of general annotations. More details of these datasets are in Appendix E.1.\nLLMs. Our experiments utilize one closed-source model, GPT-4o (Achiam et al., 2023) and two open-source models, LLaMA3.1-8B (Dubey et al., 2024) and Qwen2-7B (Yang et al., 2024)."}, {"title": "Evaluating General Annotations in Downstream Tasks", "content": "In this subsection, to evaluate the quality of the general annotations, we apply the generated annotations to the multi-modal downstream tasks (i.e., time series forecasting and imputation) by following the experimental setup in Time-MMD (Liu et al., 2024a). The implementation details are provided in Appendix F.1.\nBaselines. To the best of our knowledge, TESSA is the first work to study cross-domain multi-modal time series annotation. To demonstrate its effectiveness, we compare it with several representative methods, including No-Text, Time-MMD (Liu et al., 2024a), and DirectLLM as baselines. More details on these methods are in Appendix E.2.\nEvaluation Metrics. For the time series forecasting task, we use MSE (Mean Squared Error) and MAE (Mean Absolute Error) as evaluation metrics, where lower values for both MSE and MAE mean better annotations."}, {"title": "Evaluating General Annotations in Synthetic Datasets", "content": "We construct a synthetic dataset with time series data and ground-truth annotations to validate TESSA's performance. Implementation details are provided in Appendix G.1.\nEvaluation Metrics. We apply the LLM-as-a-judge approach (Bubeck et al., 2023; Dubois et al., 2024), evaluating two metrics: Clarity and Comprehensiveness. Two distinct LLMs score the generated annotations on a scale of 1 to 5 for each metric, with an overall score calculated as the mean of the two metrics."}, {"title": "Domain-specific Annotation Evaluation", "content": "In this subsection, we evaluate the quality of domain specific annotations. Similar to Section 4.3, we adopt a LLM-as-a-Judger strategy to evaluate the performance of domain-specific annotation agent from three perspectives: Clarity, Comprehensiveness, and Domain-relevance. The overall score is the average of these three metrics. Further details on these metrics are provided in Appendix H.1."}, {"title": "In-depth Dissection of TESSA", "content": "Adaptive Feature Selections. We compare our two feature selection methods: offline LLM-based selection and incremental RL-based selection. To assess their effectiveness in selecting the top-k most important features, we evaluate the quality of the generated general and domain-specific annotations, following the procedures in Sections 4.2 and 4.4, respectively. Environment is set as the target domain, with results shown in Fig. 2. The results indicate that TESSA performs comparably in both general and domain-specific annotation generation using either selection method. Specifically, as shown in Fig. 2(a), both approaches achieve MSE and MAE around 0.46 and 0.44 for general annotations. Similarly, in Fig. 2(b), both methods score consistently high across all domain-specific metrics, demonstrating their effectiveness in selecting important features. However, incremental RL-based selection proves more cost-effective by reducing redundant re-querying of previously used data.\nAblation Studies. We perform ablation studies to assess the importance of domain decontextualization and adaptive feature selection in TESSA. To evaluate domain decontextualization, we introduce a variant, TESSA/D, which bypasses the domain decontextualization LLM and directly extracts text-wise features from domain-specific annotations. Table 11 shows that TESSA/D captures irrelevant features, such as higher prices over time"}, {"title": "Case Study of TESSA", "content": "We conduct a case study to further validate the effectiveness of TESSA. A representative time series from the Social Good domain (Fig. 3(b)) is selected, and both TESSA and DirectLLM are applied to generate general and domain-specific annotations, summarized in Table 14. To assess the quality of the annotations, we use an LLM-as-a-judger to evaluate the domain-specific annotations from both methods, with results shown in Table 12. Our findings indicate that: (1) TESSA's general annotations capture more meaningful patterns, aiding user understanding and downstream tasks, whereas DirectLLM only highlights basic trends; and (2) TESSA's domain-specific annotations consistently outperform DirectLLM across all metrics, offering clearer, more comprehensive, and contextually relevant insights. More case studies of multivariate time series data are provided in Appendix J."}, {"title": "Conclusion", "content": "In this work, we introduce TESSA, a multi-agent system for automatic general and domain-specific time series annotation. TESSA incorporates two agents, a general annotation agent and a domain-specific annotation agent, to extract and leverage both time-series-wise and text-wise knowledge from multiple domains for annotations. TESSA overcomes the limitations of directly applying LLMs, which often capture only basic patterns and may hallucinate, by effectively identifying and emphasizing significant patterns in time series data. Our experiments on synthetic and real-world datasets from diverse domains demonstrate the effectiveness of TESSA in generating high-quality general and domain-specific annotations."}, {"title": "More Related Works", "content": "LLMs for Time Series Analysis. The rapid advancement of LLMs in natural language processing has unveiled unprecedented capabilities in sequential modeling and pattern recognition, which can be leveraged for time series analysis. Three primary approaches are commonly adopted (Jiang et al., 2024): direct querying of LLMs (Xue and Salim, 2023; Yu et al., 2023; Gruver et al., 2024), fine-tuning LLMs with task-specific modifications (Chang et al., 2023; Cao et al., 2024; Jin et al., 2024; Sun et al., 2024), and incorporating LLMs into time series models to enhance feature extraction (Li et al., 2024).\nDirect querying involves using LLMs to generate predictions or identify patterns from the data without modifying the underlying architecture. For example, PromptCast (Xue and Salim, 2023) applies LLMs to time series forecasting through a sentence-to-sentence paradigm. Yu et al. explore the use of LLMs for domain-specific tasks like financial time series forecasting (Yu et al., 2023), while LLMTime (Gruver et al., 2024) demonstrates how LLMs can function as effective learners by tokenizing time series data in a text-like format.\nFine-tuning LLMs enables them to better capture the intricacies of time series data by adapting them to specific datasets or tasks. For instance, LLM4TS (Chang et al., 2023) shows that fine-tuning pre-trained models can enhance forecasting performance. Additionally, TEMPO (Cao et al., 2024) and TEST (Sun et al., 2024) introduce architectures tailored for time series prediction, further demonstrating the power of specialized designs.\nLastly, LLMs can also act as feature enhancers within traditional time series models, enriching data representations and boosting performance. For example, (Li et al., 2024) illustrates how a frozen LLM can augment zero-shot learning for ECG time series analysis, highlighting the potential of LLMs to provide valuable features for complex datasets.\nDomain Specialization of LLMs. Domain specialization of LLMs refers to the process of adapting broadly trained models to achieve optimal performance within a specific domain. This is generally categorized into three approaches: prompt crafting (Ben-David et al., 2022; Zhang et al., 2023; Xu et al., 2024), external augmentation (Izacard et al., 2023), and model fine-tuning (Malik et al., 2023; Pfeiffer et al., 2020). One of the earliest efforts in this area is PADA (Ben-David et al., 2022), which enhances LLMs for unseen domains by generating domain-specific features from test queries and using them as prompts for task prediction. Auto-CoT (Zhang et al., 2023) advances domain specialization by prompting LLMs with the phrase \"Let's think step by step,\" helping guide the models in generating reasoning chains. Additionally, Izacard et al. (2023) propose integrating a relatively lightweight LLM with an external knowledge base, achieving performance comparable to much larger models like PaLM (Chowdhery et al., 2023). These studies highlight the flexibility of LLMs in adapting to specific domains through various strategies for domain adaptation."}, {"title": "Notations", "content": "Table 5 presents all the notations we used in this paper."}, {"title": "More Details of Multi-modal Feature Extraction", "content": "Time-series Feature Extraction\nGiven a time series data $X = {(X_1,\u2026\u2026,X_L)}$, we develop a time series extraction toolbox ${f_t^1,..., f_t^{n_t}}$ to extract time-series-wise features from X. Specifically, we include seasonality, trend,"}, {"title": "Adaptive Feature Selection", "content": "In some cases, we cannot input all the annotations to LLMs for calculating scores. We may split the annotations into several small batches and input the annotations in the small batches to calculate the score using Eq. (4). After that, we will accumulate the scores from all batches to get the final scores of each feature/token and then select the features with the top-k highest scores."}, {"title": "Experimental Settings", "content": "Datasets. To evaluate the effectiveness of TESSA, five real-world datasets from distinct domains are considered: Stock, Health, Energy, Environment, and Social Good. Specifically, the stock dataset includes 1,935 US stocks with the recent 6-year data. The other four datasets come from the public benchmark Time-MMD (Liu et al., 2024a). The dataset statistics are summarized in Table 6.\nAdditionally, we generate a synthetic dataset containing both time series and ground-truth annotations to directly assess the quality of the general annotations. The synthetic dataset is created by combining several key components from the time-series data:\n\u2022 Trend: Introduces an overall direction, which can be upward, downward, or mixed.\n\u2022 Seasonality: Adds cyclical patterns, modeled using sine waves.\n\u2022 Fourier Feature: Incorporates complex periodic behavior by combining multiple sine and cosine waves.\n\u2022 Noise: Adds Gaussian noise to simulate random fluctuations and real-world imperfections.\n\u2022 Rolling Window Features: Captures smoothed trends (mean) and local variability (max/min).\n\u2022 Lag Features: Uses past values to capture auto-correlation in the time series.\nGround-truth annotations are then generated by summarizing the key components of the synthetic time series.\nIn our synthetic dataset, we conduct 100 times random generation of each components and then combine them together to get 100 synthetic time series data, each with corresponding textual annotation."}, {"title": "Baseline Methods", "content": "Three baselines"}]}