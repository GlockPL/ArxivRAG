{"title": "Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios", "authors": ["Shantanu Jaiswal", "Debaditya Roy", "Basura Fernando", "Cheston Tan"], "abstract": "Complex visual reasoning and question answering (VQA) is a challenging task that requires compositional multi-step processing and higher-level reasoning capabilities beyond the immediate recognition and localization of objects and events. Here, we introduce a fully neural Iterative and Parallel Reasoning Mechanism (IPRM) that combines two distinct forms of computation \u2013 iterative and parallel \u2013 to better address complex VQA scenarios. Specifically, IPRM's \u201citerative\" computation facilitates compositional step-by-step reasoning for scenarios wherein individual operations need to be computed, stored, and recalled dynamically (e.g. when computing the query \"determine the color of pen to the left of the child in red t-shirt sitting at the white table\"). Meanwhile, its \u201cparallel\" computation allows for the simultaneous exploration of different reasoning paths and benefits more robust and efficient execution of operations that are mutually independent (e.g. when counting individual colors for the query: \u201cdetermine the maximum occurring color amongst all t-shirts\"). We design IPRM as a lightweight and fully-differentiable neural module that can be conveniently applied to both transformer and non-transformer vision-language backbones. It notably outperforms prior task-specific methods and transformer-based attention modules across various image and video VQA benchmarks testing distinct complex reasoning capabilities such as compositional spatiotemporal reasoning (AGQA), situational reasoning (STAR), multi-hop reasoning generalization (CLEVR-Humans) and causal event linking (CLEVRER-Humans). Further, IPRM's internal computations can be visualized across reasoning steps, aiding interpretability and diagnosis of its errors. Source code to be released at: https://github.com/shantanuj/IPRM_Iterative_ and_Parallel_Reasoning_Mechanism", "sections": [{"title": "1 Introduction", "content": "Visual reasoning and question answering (VQA) at its core requires a model to identify relevant visual operations, execute those operations, and then combine their results to make an inference. Complex visual reasoning scenarios (depicted in fig. 1) are particularly challenging in this regard. They require models to reason compositionally over a large number of reasoning steps and to engage in a variety of higher-level reasoning operations such as causal linking, logical reasoning, and spatiotemporal processing that extend beyond core perception capabilities.\nIn this context, two powerful computational priors exist \u2013 iterative and parallel. While each has its own limitations, when combined, they can complement each other and effectively address the challenges of complex VQA tasks. Specifically, iterative computation, wherein individual operations are identified and composed in a step-by-step manner, is a beneficial prior for multi-step reasoning scenarios explored by past VQA works [28, 25, 18]. However, pure iterative computation can exhibit"}, {"title": "2 Iterative and Parallel Reasoning Mechanism", "content": "Our proposed iterative-and parallel-reasoning mechanism (IPRM) is a fully-differentiable neural architecture. Given visual features $X_v \\in \\mathbb{R}^{N_v \\times D_v}$ and language or task-description features $X_L \\in \\mathbb{R}^{N_L \\times D_L}$, IPRM outputs a \u201creasoning result\" $y_s \\in \\mathbb{R}^{D_m}$ and, optionally, a set of \u201creasoning result tokens\" $Y_R \\in \\mathbb{R}^{N_m\\times D_m}$. As previously mentioned, IPRM operates iteratively for $T$ reasoning steps and internally, maintains an explicit memory $M : {M_{op}, M_{res}}$. The memory is modelled as a set of \u201coperation states\" $M_{op} \\in \\mathbb{R}^{N_{op} \\times D_m}$, keyed to which are \u201cresult states\" $M_{res} \\in \\mathbb{R}^{N_{op} \\times D_m}$ as shown in fig. 2. Here, $N_{op}$ denotes the number of parallel operations to be computed while $D_m$ denotes the mechanism's internal feature dimension. On a high level, at each reasoning step (denoted by $t\\in {1,\\dots, T}$), IPRM performs the following computations:\n1. First, conditioned on the existing operation states $M_{op,t}$, we retrieve relevant information from language or task-description features $X_L$ to form a new set of latent operations $Z_{op,t} \\in \\mathbb{R}^{N_{op} \\times D_m}$. We term this computation as \u201cOperation Formation\".\n\n$Z_{op,t} = Op\\_Form(X_L; M_{op,t})$\n\n2.  Then, conditioned on the latent operations $Z_{op,t}$ and the existing result state $M_{res,t}$, we attend and retrieve relevant information from visual features $X_v$ which represents a new set of latent results $Z_{res,t} \\in \\mathbb{R}^{N_{op}\\times D_m}$ corresponding to $Z_{op,t}$. We term this computation as \"Operation Execution\".\n\n$Z_{res,t} = Op\\_Exec(X_v; [Z_{op,t}, M_{res,t}])$\n\n3.  Finally, to facilitate interaction amongst parallel operations, we perform inter-operation attention. Here, each operation $Z_{op\\_k,t}; k \\in {1, ..., N_{op}}$, is composed with other oper- ations in $Z_{op,t}$ as well as prior operation states $M_{op[t-W:t]}$ within a lookback-window $W$. The corresponding result $Z_{res\\_k,t}$ is similarly composed with other results $Z_{res,t}$ and prior result states denoted as $M_{res[(t-W):t]}$. We term this computation as \u201cOperation Composition\"\n\n$M_{t+1} = Op\\_Comp({Z_{op,t}, Z_{res,t}}, M_{[(t-W):t]})$\n\nAs shown in eq. (3), this output is the new memory state $M_{t+1} : {M_{op}, M_{res}}$.\nThe overall computation flow is illustrated in fig. 2, and we provide specific details and intuitions behind these computations in the following sub-sections."}, {"title": "2.1 Operation Formation", "content": "The \"operation formation\" stage conceptually models a reasoner that based on its prior set of operations, decides what language features to retrieve in order to form the next set of relevant opera- tions. This can be effectively implemented through conventional attention mechanisms. Specifically, the cumulative set of prior operations (maintained in $M_{op,t}$) can be projected to form the 'query' $Q_{L,t} \\in \\mathbb{R}^{N_{op}\\times D_m}$ representing \u201cwhat features to look for\". The language features $X_L$ can be projected to form the \u2018key' $K_L \\in \\mathbb{R}^{N_L\\times D_m}$ and 'value' $V_L \\in \\mathbb{R}^{N_L\\times D_m}$. Finally, the new set of latent operations $Z_{op,t}$ can be retrieved by computing $attn(Q_{L}, K_{L}, V_{L})$. These steps are formally represented below:\n\n$Q_{L,t} = W_{L,q2} (Tanh(W_{L,q1} (M_{op,t}))), K_L = W_{L,k}(X_L), V_L = W_{L,v}(X_L)$\n\n$Z_{op,t} = attn(Q_{L,t}, K_L, V_L)$\n\nHere, $W_{L,q2} \\in \\mathbb{R}^{D_m\\times D_m}$, $W_{L,q1} \\in \\mathbb{R}^{D_m\\times D_m}$, $W_{L,k} \\in \\mathbb{R}^{D_m\\times D_l}$ and $W_{L,v} \\in \\mathbb{R}^{D_m\\times D_1}$.\nNote $K_L$ and $V_L$ are not computation-step dependent and only computed once. We use a simple linear-modulated formulation (with appropriate broadcasting and projection weight $W_a \\in \\mathbb{R}^{D_kX1}$) to implement $attn(.)$ (further details in appendix sec. 13)."}, {"title": "2.2 Operation Execution", "content": "In the \"operation execution\" stage, the reasoner determines what visual features need to be retrieved depending on both the newly formed operations and existing result states. To model the constituent visual attention mechanism, we draw insights from existing recurrent visual reasoning methods [28, 69] that incorporate feature modulation for memory-guided attention. Specifically, we retrieve a set of feature modulation weights $S_{v,t} \\in \\mathbb{R}^{N_{op}\\times D_m/r}$ through a joint projection of the new operations $Z_{op,t}$ and prior results $M_{res,t}$ as shown in eq. (6).\n\n$S_{v,t} = W_{v,s}([W_{V,op}(Z_{op,t}), W_{v,res} (M_{res,t})])$\n\nHere, $r$ is a feature reduction ratio [23, 31]. $S_{v,t}$ is then applied dimension wise to a projection of $X_v$ to retrieve an intermediate attention key $K'_{v,t} \\in \\mathbb{R}^{N_{op}\\times N_k\\times D_m/r}$. The final attention key $K_{v,t}$ is then obtained through a joint multi-layer-projection of $K'_{v,t}$ and the previously projected representation of $X_v$ as shown in eq. (7).\n\n$K'_{v,t} = S_{v,t} \\otimes W_{v,k1}(X_v), K_{v,t} = W_{V,k3}(\\phi(W_{V,k2}([W_{V,k1}(X_v), K'_{v,t}])))$\n\nFinally, the attention query and value are formed through separate projections of $Z_{op,t}$ and $X_v$ respectively. These are then fed together with $K_{v,t}$ to the attention function to retrieve the new operation results $Z_{res,t}$ as shown in eq. (8). Intuitively, the overall process allows for both prior results and the new set of operations to jointly guide visual attention.\n\n$Q_{v,t}, V_{v,t} = W_{V,q}(Z_{op,t}), W_{v,v}(X_v), Z_{res,t} = attn(Q_{v,t}, K_{v,t}, V_{V,t})$\n\nHere, $W_{v,op} \\in \\mathbb{R}^{D_m/r\\times D_m}$, $W_{v,res} \\in \\mathbb{R}^{D_m/r\\times D_m}$, $W_{vs} \\in \\mathbb{R}^{D_m/r\\times 2D_m/r}$, $W_{v.k1} \\in \\mathbb{R}^{D_m/r\\times D_v}$, $W_{V,k2} \\in \\mathbb{R}^{D_m/r\\times 2D_m/r}$, $W_{v,k3} \\in \\mathbb{R}^{D_m/r\\times D_m/r}$, $W_{v.q} \\in \\mathbb{R}^{D_m/r\\times D_m}$ and $W_{v,v} \\in \\mathbb{R}^{D_m\\times D_m}$."}, {"title": "2.3 Operation Composition", "content": "Finally, in the \u201coperation composition\" stage, the reasoner first integrates the executed operations $Z_{op,t}$ and their results $Z_{res,t}$ into the existing memory state $M_t$ through a simple recurrent update as shown in eqs. (9) and (10). Then, to mitigate redundancy amongst parallel operations and to retrieve relevant knowledge from prior-step operations, it dynamically composes individual operation states $M_{op,t+1}$ with other operation states in $M_{op,t+1}$ and also prior operation states in $M_{op,t-W:t}$. Here, $W$ is an attention look-back window.\nThis composition is achieved through computing inter-operation attention as illustrated in fig. 3. Specifically, $M_{op,t+1}$ is projected to obtain a set of queries $Q_{op,t}$, while the token-wise concatenation of $M_{op,t+1}$ and $M_{op,t-W:t}$ are projected to obtain the operation attention keys $K_{op,t}$ and values $V_{op,t}$. A second set of values $V_{res,t}$ are also formed through projection of respective result states\n\n$M_{op,t+1} = W_{opU}(Z_{op,t}) + W_{opH}(M_{op,t})$\n\n$M'_{res,t+1} = W_{resU} (Z_{res,t}) + W_{resH}(M_{res,t})$\n\n$Q_{op,t} = W_{op,q}(M_{op,t+1})$\n\n$K_{op,t} = W_{op,k}([M_{op,t+1}; M_{op,t-W:t}])$\n\n$V_{op,t} = W_{op,v}([M_{op,t+1}; M_{op,t-W:t}])$\n\n$V_{res,t} = W_{res,v}([M'_{res,t+1}; M_{res,t-W:t}])$\n\n$M_{op,t+1}, A_{op,t} = attn(Q_{op,t}, K_{op,t}, V_{op,t}, mask=I_{N_{op}})$\n\n$M_{op,t+1} = M_{op,t+1} + W_{op,u2} (M_{op,t+1})$\n\n$M_{res,t+1} = A_{op,t}(V_{res,t})+ W_{res, v2} (M'_{res,t+1})$\n\nObtaining Reasoning Summary As mentioned before, our proposed mechanism outputs a set of \u201creasoning result tokens\" $Y_R$ and a \u201creasoning result\" $y_s$. $Y_R$ is simply equivalent to the last memory result states $M_{res,T+1}$. To obtain $y_s$, we perform attention on the last operation states $M_{op,T+1}$ by utilizing a summary representation $l_s \\in \\mathbb{R}^{D_l}$ of $X_L$ as the attention-query.\nWe set $l_s$ to be the first token in case of transformer-based language backbones and as last hidden state in case of LSTM-based language backbones. As shown in eq. (18), $l_s$ is projected to obtain a single-token attention query $p_q$ while $M_{op,T+1}$ is projected to obtain the attention keys $p_k$. The attention value is simply the result states $M_{res,T+1}$, and the output of the attention function is the \"reasoning result\". Intuitively, this computation corresponds to the reasoner deciding which final oper- ation states in $M_{op,T+1}$ are most relevant to the summary of the input language or task-description $X_L$, based on which corresponding result states $M_{res,T+1}$ are weighted and retrieved.\n\n$P_q, P_k = W_{pq,q} (l_s), W_{pk,k}(M_{op,T+1})$\n\n$y_s = attn(p_q, P_k, M_{res,T+1})$\n\nHere, $W_{pq,q} \\in \\mathbb{R}^{D_m\\times D_l}$ and $W_{pk,k} \\in \\mathbb{R}^{D_m \\times D_m}$.\nReasoning mechanism general applicability. Our proposed iterative and parallel reasoning mechanism is an end-to-end trainable neural module. It can be conveniently applied on top of different vision and language backbones, and be trained directly as a new computational block (similar to a transformer block receiving vision and language tokens) with no specific adjustments. Further, the base version of IPRM (used in this paper) is weight-tied across iterative steps which means that its number of parameters is constant regardless of number of computation steps and parallel operations. We provide parameter and computational details along with module implementation details in appendix sec. C."}, {"title": "4 Related Work", "content": "Visual reasoning methods and vision-language models. Multiple prior works have introduced effective visual reasoning methods in context of image and video question answering [35, 52, 2, 85, 15, 50, 69, 56, 32, 54, 53, 76]. Prominent works include NMN[2], FILM [60], NSM [27], MAC [28], MCAN [87], NS-VQA [85], ALOE [12], VR-DP [13], SHG-VQA [68], MIST [18] and OCRA [76]. In contrast to these works that show applicability of methods for particular VQA benchmarks/tasks, our work explores a more general direction of integrating parallel and iterative computation in a reasoning framework that we show to be effective for multiple complex VQA benchmarks and reasoning scenarios. More recently, vision-language models [44, 73, 37, 66, 45, 46, 74] and multimodal large-language-models [57, 47] with transformer-based mechanisms have shown impressive reasoning capabilities at scale. Notable examples include CLIP [61], GPT [57], Gemini [67], MDETR [36], LXMERT [66], VinVL [88], BLIP [44, 43], Flamingo [1], LlavA [47], BEIT [74] and All-in-One [72]. We believe our work is complimentary to these developments, as it"}, {"title": "5 Conclusion", "content": "We introduced a novel fully-differentiable and end-to-end trainable iterative and parallel reasoning mechanism (IPRM) to address complex VQA scenarios. We comprehensively evaluated IPRM on various complex image and video VQA benchmarks testing distinct reasoning capabilities, and found it improves state-of-arts on multiple such benchmarks. We also performed quantiative ablations to study individual impacts of parallel and iterative computation besides qualitative analysis of IPRM's reasoning computation visualization."}, {"title": "6 Limitations and Future work", "content": "Here, we note possible limitations of IPRM. Similar to existing VQA and deep-learning methods, IPRM may reflect biases that are present in the training distribution of VQA benchmarks. This may lead it to overfit to certain image inputs or question forms and possibly provide skewed answers in such scenarios. Further, the utilized vision-language backbones in our experiments may also entail visual, language and cultural biases in their original training distribution which may permeate to IPRM upon integration for VQA scenarios. In this regard, we hope the capability to visualize intermediate reasoning of IPRM and diagnose its error cases (as shown in section 3.3) can serve a useful tool to benefit interpretability in VQA and identify possible reasoning biases that may emerge in the model.\nFor future work, scaling the IPRM architecture to a foundational video-language model by integrat- ing it with large-scale transformer-based vision-language models and relevant instruction-tuning approaches presents an exciting research opportunity. Moreover, while we designed and evaluated IPRM in the context of complex VQA, we believe it has the potential to operate as a general reasoning mechanism applicable to tasks beyond visual reasoning and question answering, such as language processing and embodied reasoning."}, {"title": "D Potential Negative Impact", "content": "In relation to VQA and deep-learning methods in general, the deployment of IPRM in real-world applications without thorough consideration of dataset or training distribution biases, could inadver- tently reinforce existing vision, language and cultural biases present in the data, leading to erroneous outcomes or skewed answers. Further, the deployment of VQA methods such as IPRM in sensitive domains such as healthcare or scene/footage analysis could raise ethical concerns, including privacy violations, algorithmic reliability, and the potential for unintended consequences stemming from erroneous or biased predictions."}]}