{"title": "Leveraging Priors via Diffusion Bridge for Time Series Generation", "authors": ["Jinseong Park", "Seungyun Lee", "Woojin Jeong", "Yujin Choi", "Jaewook Lee"], "abstract": "Time series generation is widely used in real-world applications such as simulation, data augmentation, and hypothesis test techniques. Recently, diffusion models have emerged as the de facto approach for time series generation, emphasizing diverse synthesis scenarios based on historical or correlated time series data streams. Since time series have unique characteristics, such as fixed time order and data scaling, standard Gaussian prior might be ill-suited for general time series generation. In this paper, we exploit the usage of diverse prior distributions for synthesis. Then, we propose TimeBridge, a framework that enables flexible synthesis by leveraging diffusion bridges to learn the transport between chosen prior and data distributions. Our model covers a wide range of scenarios in time series diffusion models, which leverages (i) data- and time-dependent priors for unconditional synthesis, and (ii) data-scale preserving synthesis with a constraint as a prior for conditional generation. Experimentally, our model achieves state-of-the-art performance in both unconditional and conditional time series generation tasks.", "sections": [{"title": "Introduction", "content": "Synthetic time series have expanded their applications by serving roles in data augmentation (Wen et al. 2021), privacy-preserving synthesis (Wang et al. 2024), continuous data compression (Zhou et al. 2023), and simulation of specific hypothesis (Xia et al. 2024) for the data streams. With the advancement of deep generative architectures, time series generative models have been actively investigated such as variational autoencoders (VAE) (Desai et al. 2021; Naiman et al. 2024), generative adversarial networks (GANs) (Yoon, Jarrett, and Van der Schaar 2019; Jeon et al. 2022; Xia et al. 2024), and diffusion models (Ho, Jain, and Abbeel 2020; Karras et al. 2022).\nDiffusion models are well-known for producing high-quality and diverse time series data samples (Tashiro et al. 2021; Alcaraz and Strodthoff 2023). The effectiveness of these models relies on iteratively estimating small steps of the reverse trajectory, which corresponds to the forward process towards standard Gaussian priors (Song et al. 2021).\nHowever, modeling diffusion with constraints is not flexible due to the predetermined mapping from noise to data"}, {"title": "Preliminaries", "content": "Consider the multivariate time series x defined within the sample space $X = R^{d \\times \\tau}$, where d represents the dimensionality of the variables and $\\tau$ denotes the time length. For each index $i \\in \\{1, ..., N\\}$ among the N samples, each time series is denoted as $x^i = (x^i(1),...,x^i(\\tau)) \\in X$, given the data $x^i(k) \\in R^d$ for each time step $k \\in \\{1, ..., \\tau\\}$. For diffusion steps $t \\in \\{0, . . ., T\\}$, we denote intermediate time series samples as $x_t$, and for a specific instance i as $x_t^i$.\nDiffusion process. In standard diffusion models (Ho, Jain, and Abbeel 2020), the diffusion process is constructed by gradually injecting noise into samples $x_0$ drawn from the data distribution $p_0$, forwarding them into a standard Gaussian distribution $p_T = N(0, I)$. The corresponding stochastic differential equation (SDE) (Song et al. 2021) is:\n$dx_t = f(x_t, t)dt + g(t)dw_t$,\nwhere $f : R^d \\times [0, T] \\rightarrow R^d$ represents the drift function, $g : [0,T] \\rightarrow R$ denotes the diffusion coefficient, and $w_t$ is the Wiener process. The reverse SDE is formulated as follows:\n$dx_t = [f(x_t, t) - g(t)^2\\nabla_{x_t}log p(x_t)]dt + g(t)dw_t$,\nwhere $p(x_t)$ refers to the probability density of $x_t$ (Anderson 1982) and $\\nabla_{x_t}log p(x_t)$ is a score function to match."}, {"title": "Diffusion bridge", "content": "The aforementioned standard diffusion models cannot be extended to more general tasks due to their inflexibility in choosing a prior distribution. Instead, to construct the diffusion process towards any endpoint, Doob's h-transform (Doob and Doob 1984) can be applied to Equation (1). With a fixed endpoint y, the forward process is:\n$dx_t = [f(x_t, t) + g(t)^2h(x_t, t, y,T)]dt + g(t)dw_t,$\n$x_0 \\sim q_{data}(x), x_T = y,$\nwhere $h(x, t, y,T) = \\nabla_{x_t}log p(x_T|x_t)|_{x_t=x, x_T=y}$. For a given data distribution $q_{data}(x, y)$, the diffusion bridge (Schr\u00f6dinger 1932; De Bortoli et al. 2021) aims to transport $x_0$ to $x_T$ where $(x_0, x_T) = (x, y) \\sim q_{data}(x, y)$ while following above forward process. Similar to the standard diffusion model, we can construct the reverse SDE as follows:\n$dx_t = [f(x_t, t) - g(t)^2(s(x_t, t, y, T) - h(x_t, t, y, T))]dt + g(t)dw_t,$\nwhere $x_T = y$ and its score function is calculated as\n$s(x, t, y, T) = \\nabla_{x_t}log q(x_t|x_T)|_{x_t=x, x_T=y}$\nFor training, it is sufficient to learn the score $s(x, t, y, T)$. The Schr\u00f6dinger bridge (Schr\u00f6dinger 1932) is a closely related concept, focusing on the path optimizations between two arbitrary distributions. Diffusion bridge models have become popular due to their abilities (i) to flexibly map in various domains, including image-to-image translation (Li et al. 2023; Shi et al. 2022) and text-to-speech synthesis (Popov et al. 2021; Chen et al. 2023b), and (ii) to achieve efficient sampling with fewer diffusion steps (Zhou et al. 2024; Chen et al. 2023b)."}, {"title": "Reparametrization of diffusion bridge", "content": "To approximate the score s(x, t, y, T) in Equation (5), Zhou et al. (2024) recently proposed a Denoising Diffusion Bridge Model (DDBM), integrating the diffusion bridge model in the framework of standard diffusion models. DDBM enables tractable marginal sampling of $x_t$, by designing $x_t = \\alpha_t x_0 + \\sigma_t \\epsilon$, where $\\alpha_t$ and $\\sigma_t$ are the noise schedule functions and $\\epsilon \\sim N(0, I)$. For the variance-preserving (VP) schedule, the marginal distribution at time t is as follows:\n$q(x_t|x_0,x_T) = N (\\hat{\\mu}_t, \\hat{\\sigma}I),$\n$\\hat{\\mu}_t = \\frac{SNR_t \\alpha_t x_T + \\alpha_T x_0 (1 - \\frac{SNR_t}{SNR_T})}{\\alpha_t + \\alpha_T (1 - \\frac{SNR_t}{SNR_T})},$\n$\\hat{\\sigma} = \\sigma_t^2 (1 - \\frac{SNR_t}{SNR_T}),$\nwhere signal-to-ratio (SNR) is defined as $SNR_t = \\alpha_t^2/\\sigma_t^2$. Since the mean $\\hat{\\mu}_t$ is a linear combination of $x_0$ and $x_T$ at time t, the data scale is preserved for translation. With the reparametrization of Elucidating Diffusion Models (EDM) (Karras et al. 2022), we can match the score as follows:\n$\\nabla_{x_t}log q(x_t|x_T) \\approx s(D_0, x_t, t, x_T,T) := \\frac{x_T - (\\alpha_T x_T + \\alpha_t D_0(x_t, t, x_T)(1 - \\frac{SNR_t}{SNR_T}))}{\\sigma_t^2 (1 - \\frac{SNR_t}{SNR_T})},$\nwhere $D_0 (x_t, t, x_T)$ is the model output as a denoiser."}, {"title": "Problem Statement", "content": "In this section, we cover two types of time series synthesis tasks, i.e., unconditional and conditional generations.\nUnconditional generation considers the total dataset containing N samples, denoted as $D = \\{x^i\\}_{i=1}^N$. Our goal is to train a deep generative model to mimic the input dataset and yield synthetic samples statistically similar to a given time series dataset. We aim to make the distribution $p(x)$ of synthetic data x, similar to the input data distribution $p_0(x)$.\nConditional generation further utilizes the paired condition y to guide time series generation of each data sample x as $D = \\{(x^i, y^i)\\}_{i=1}^N$. Thus, we try to make the conditional distribution $p(x|y)$ of synthetic data x closely mirror the conditional distribution of input data $p_0(x|y)$. Considering that the condition in time series is often also a time series as $y \\in X$, Coletta et al. (2024) divided the constraints into two types: (i) soft constraint to guide the time series generation such as trend, or (ii) hard constraint where we should follow during synthesis such as fixed points. In a broader view, well-known time series tasks can be considered as conditional generation, e.g., the unmasked part in imputation or the historical data of forecasting as conditions.\nWith the diffusion bridge framework, we hypothesize that the standard Gaussian prior might not be the optimal choice. Thus, we investigate a general time series synthesis framework by analyzing the prior selection in diffusion models based on the following research questions (RQs):\nRQ1. Are data-dependent priors effective for time series?\nRQ2. How to model temporal dynamics with priors?\nRQ3. Can constraints be used as priors for diffusion?\nRQ4. How to preserve data points with diffusion bridge?"}, {"title": "Diffusion Prior Design with TimeBridge", "content": "To address these questions, we first explore suitable prior designs for diffusion synthesis. Afterward, we propose TimeBridge, a general diffusion bridge model that leverages the aforementioned prior selections, elaborating its application in both unconditional and conditional tasks."}, {"title": "Data- and Time-dependent Priors for Time Series", "content": "For unconditional tasks, we examine the benefit of better priors depending on data and temporal properties of time series.\nData-Dependent Prior. To investigate RQ1, we suggest using data prior to having a data-dependent distribution to better approximate the data distribution. Sampling from $N(0, I)$ lacks data-specific information, which can be enhanced by enforcing that the priors capture data scale and temporal dependencies. Recently, the usages of data-dependent prior have been investigated for audio (Popov et al. 2021; Lee et al. 2022) and image (Yu et al. 2024) domains. Therefore, we test the use of data-dependent prior for time series by setting the prior distribution as follows:\n$x_T \\sim N(\\mu, diag(\\sigma^2))$,\nwhere $\\mu$ and $\\sigma^2$ are independently calculated in the same dimension of the time series x in $R^{\\tau \\times d}$. $diag(\\sigma^2)$ indicates the diagonal covariance matrix of each element."}, {"title": "Time-Dependent Prior", "content": "As a response to RQ2, we focus on capturing temporal dependencies in noise addition towards prior distribution. Recently, Bilo\u0161 et al. (2023) injected stochastic processes instead of random noise to preserve the continuity of temporal data and Han, Zheng, and Zhou (2022) utilized the pre-trained model for prediction and set the mean of prior as the output for regression.\nAs a solution, we employ Gaussian processes (GPs) for prior selection, which is effective for modeling temporal data (Bilo\u0161 et al. 2023; Ansari et al. 2024). GPs are distributions over functions with time input k characterized by the mean function m(k) and the positive definite kernel K(k, k'). We use the radial basis function (RBF) kernel, defined as $K(k, k') = exp(-\\gamma|k - k'|^2)$, to ensure a stationary process. To construct a time-dependent prior based on data-dependent properties, we add a variance term to the kernel function to encode data information into the prior distribution. Consequently, we construct the prior incorporating of the temporal dynamics as follows:\n$x_T \\sim GP(m, K + \\Sigma),$\nwhere $\\Sigma = [\\Sigma_{ij}]$ is a correlation matrix function in which $\\Sigma_{ij}$ represents the correlation of the data between timestamps i and j. For simplified version of Equation (10), we set m as $\\mu$ and $\\Sigma$ as $diag(\\sigma^2)$ to align with the data-dependent prior, i.e., $x_T \\sim N(\\mu, K + diag(\\sigma^2))$."}, {"title": "Scale-preserving Sampling for Conditional Priors", "content": "We now consider the condition $y \\in X$, provided as a time series, which is the most prevalent case for real-world settings. For example, a given trend can guide the synthesis output or fixed points at specific time steps can determine the properties of synthetic data. We explore the use of a prior on the same data scale instead of relying solely on conditional embedding in standard diffusion.\nSoft constraints with given trends. To answer RQ3, we argue that the diffusion bridge is well-suited for preserving a given condition by setting the prior as the condition. The diffusion bridge has shown its effectiveness in conditional tasks such as coloring (Zhou et al. 2024), image-to-image translation (Li et al. 2023; Liu et al. 2023), and improving naive base predictions (Chen et al. 2023b; Lee et al. 2024).\nConditional generation based on trend is straightforward by (i) setting the pair-wise condition to prior $x_T = y$ and (ii) training the translation from trend to data. As the expected mean of $x_t$ is a linear combination of $x_0$ and $x_T = y$ in Equation (6), the model learns the correction starting from the trend samples, utilizing the same data scale. In contrast, standard diffusion models are unsuitable for translating conditions into synthetic data; instead, they generate new data based on a specific trend condition, typically requiring additional steps such as providing guidance or correcting intermediate data samples $x_t$ during sampling. Thus, we can also eliminate the need for additional penalty functions or value correction during sampling.\nHard constraints with fixed points. In RQ4, preserving data points of hard constraints can be important in conditional tasks. To ensure the fixed point condition during"}, {"title": "TimeBridge: Bridge Modeling for Time Series", "content": "Based on the aforementioned prior selections, we propose TimeBridge, a general diffusion bridge model for time series synthesis. Our approach yields high-quality samples with fewer sampling steps and is compatible with existing time series diffusion frameworks.\nFor score matching in Equation (9), we build the denoiser $D_\\theta$ for time series. For architecture design, we adopt the backbone of Diffusion-TS (Yuan and Qiao 2024), i.e., Fourier-based loss function (Zhou et al. 2022; Liu et al. 2024; Nguyen et al. 2022; Yi et al. 2024) and transformer encoder-decoder models with seasonal-trend decomposition (Zeng et al. 2023; Wu et al. 2021). With the bridge diffusion framework, our objective function is formulated as follows:\n$L_\\theta = E_{t,x_0} [w_t (||x_0 - D_\\theta(x_t, t, x_T)||^2 + \\lambda||FFT(x_0) - FFT(D_\\theta(x_t,t,x_T))||^2)].$\n$w_t$ indicates the weight scheduler of the loss function and $\\lambda$ indicates the strength of fast Fourier transform (FFT). We construct the bridge model to produce output by decomposing the input into trend and seasonal components as follows:\n$D_\\theta = V_{tr}(\\theta, x_t, t, x_T) + \\sum_{i=1}^K S_{i,t}(\\theta, x_t, t, x_T) + R(\\theta, x_t, t, x_T),$\nwhere $V_{tr}$ is the output for the trend synthesis layer, $S_{i,t}$ is the output for each seasonal synthesis layer i among K layers and R is the output for estimated residual.\nAs time series data possess stochasticity (Shen and Kwok 2023), we directly predict $D_\\theta$ rather than using parametrization technique from noise prediction used in other domains"}, {"title": "Related Works", "content": "Standard diffusion models for time series. Time series diffusion models have become the de facto method in synthesis tasks. Rasul et al. (2021) initially used diffusion networks for time series based on recurrent networks. For imputation, SSSD (Alcaraz and Strodthoff 2023) and CSDI (Tashiro et al. 2021) considered time series imputation similar to image inpainting tasks. TimeDiff (Shen and Kwok 2023), LDT (Feng et al. 2024), and TMDM (Li et al. 2024) focused on time series forecasting tasks. Recently, Coletta et al. (2024) investigated guiding the sampling with a penalty function called DiffTime. Diffusion-TS (Yuan and Qiao 2024) introduced a decomposition architecture to disentangle time series data into trend and seasonal components and applied a Fourier-based loss term. They also expanded their unconditional model to conditional generation tasks by guiding the diffusion sampling toward the input conditions.\nExisting time series bridge models. Recently, some researchers have investigated the use of the diffusion bridge in the time series domain. Chen et al. (2023a) investigated the convergence analysis of the Schr\u00f6dinger bridge algorithm and applied them to time series imputation tasks. Ham-douche, Henry-Labordere, and Pham (2023) used kernel estimation for the Schr\u00f6dinger bridge combined with feed-forward and LSTM networks, especially for the application of deep hedging on real-data sets. Garg, Zhang, and Zhou (2024) relaxed the Schr\u00f6dinger bridge by setting a geometric mixture of targets and other distributions for the prior distribution and applied their method to time series. However, most of them used iterative solutions for bridge problems, which required a heavy computational burden and incompatibility with recent standard diffusion models. Furthermore, none of them suggested a general framework for both unconditional and conditional time series generation."}, {"title": "Experimental Setup", "content": "We assess the performance of TimeBridge using widely used time series datasets. We use two simulation datasets: Sines with 5 features of different frequencies and phases of sine functions and MuJoCo of multivariate advanced physics simulation with 14 features. For real-world datasets, we use ETT (Electricity Transformer Temperature) for long-term electric power with 7 features, Stocks of Google stock prices and volumes with 6 features, Energy, UCI dataset with appliances energy use in a low energy building with 28 features, and fMRI of the blood oxygen level-dependent functional MR imaging with 50 selected features.\nFor measures, we assess four metrics for unconditional time series generation on a normalized (0-1) scale. Context-Fr\u00e9chet Inception Distance (Context-FID) score (Jeha et al."}, {"title": "Unconditional Generation", "content": "We demonstrate the unconditional generation tasks with a length of 24 for the aforementioned datasets and evaluate the performance of the previous measures. For baseline methods, we compare with TimeVAE (Desai et al. 2021), TimeGAN (Yoon, Jarrett, and Van der Schaar 2019), Cot-GAN (Xu et al. 2020), Diffwave (Kong et al. 2021), DiffTime (Coletta et al. 2024) with SSSD (Alcaraz and Strodthoff 2023) backbone, and Diffusion-TS (Yuan and Qiao 2024). For the proposed TimeBridge, we choose the data-dependent settings in Equation (10) and TimeBridge-GP denotes the time-dependent GP prior in Equation (11). Results are shown in Table 1. We observe that the proposed method outperforms the previous methods. The performance gain is significant in context-FID and discriminative scores, reducing the scores of 40.70% and 18.05% on average, respectively. Notably, the strength of data and time-dependent prior is shown in real datasets, such as ETTh and Stocks. This indicates that our prior selection and modeling help in approximating the data distribution during synthesis.\nTo visualize the generated data, we plot the t-SNE plots with the generated data in Figure 3. The red original data and the blue synthetic data show the overlapped plot in almost all datasets. The generated samples well approximate the distribution properties in complex datasets, such as the Stocks and Energy datasets. For further visualization con-"}, {"title": "Synthesis with Trend Condition", "content": "We now consider trend-guided generation, where the data samples are in $D = \\{(x^i, y^i)\\}_{i=1}^N$. For the trend y, we consider three types: linear, polynomial with degree of three, and Butterworth filter (Butterworth et al. 1930), a signal processing filter that captures trends (Xia et al. 2024). Using these trends as conditions, we reimplement the experiments in Table 1 with the ETTh and Energy datasets. To apply the condition to baseline methods, we feed the trend into a conditional embedding and concatenate it with the original embedding. For the proposed TimeBridge, we set the prior $x_T = y$ upon the conditional embedding for translation.\nExperimental results are shown in Table 2. The rows with trend baseline indicate the results with the trends without any synthesis. After synthesis with trend conditions, we observe that the quality metrics such as Context-FID, correlation, and discriminative scores diminished compared to the unconditional generation in Table 1, indicating that trend conditions help the model generate more realistic data samples. With the trend condition in $x_T$, the proposed TimeBridge achieves the best performance in Context-FID in all settings and the best discriminative score in 5 out of 6 settings while maintaining correlation scores at similar levels to Diffusion-TS. This indicates that the proposed framework can be broadly adopted by different conditioning settings, without controlling any details other than the prior distribution. The sampling path of TimeBridge from trend to data is shown in Figure 4a."}, {"title": "Imputation with Fixed Masks", "content": "For imputation, which is a common setting of hard conditions, we compare the proposed method with imputation diffusion models and the conditional Langevin sampler of Diffusion-TS (Yuan and Qiao 2024). We use random masks for the Mujoco dataset and the geometric mask from Zerveas et al. (2021) for the ETTh and Energy datasets, with a given missing ratio as in Yuan and Qiao (2024). We set the total length of imputation to 100 for the Mujoco dataset, matching the data length, and split the ETTh and Energy datasets into 48 time steps. For the proposed TimeBridge, we preserve the observed values and set the prior $x_T$ as the interpolated values of the condition c as in Equation (12).\nThe imputation results are shown in Table 3 and Table 4. Both tables demonstrate that TimeBridge achieves the lowest measures in imputation tasks. Since the Langevin sampler of Diffusion-TS does not require individual training for various missing ratios, we ensure a fair comparison by evaluating TimeBridge with the same training setting, TimeBridge-75%, which uses 75% missing values for training. The performance of TimeBridge-75% even outperforms the models trained with equal missing values, highlighting the generalization capability of TimeBridge in imputation tasks. The visualization of imputation results is shown in Figure 5, and the sampling path can be found in Figure 4b."}, {"title": "Computation efficiency", "content": "Table 6 illustrates the strength of the proposed method, particularly when using the second-order Heun sampler during sampling. With a lower number of score function evaluations (NFE), our sampler achieves fast synthesis without compromising quality. We use 40 steps for sampling in all experiments, resulting in 119 NFEs with our settings. For comparison, we evaluate the Diffusion-TS using the same NFE. Given the trade-off of performance and sampling time in Diffusion-TS, our experimental results outperform previous methods. The marginal increase in time for TimeBridge is due to the time required to build the prior distribution."}, {"title": "Conclusion", "content": "In this paper, we highlight the effectiveness of selecting prior distributions for time series generation using diffusion bridges. To address the limitations in the standard Gaussian prior of the diffusion models, we propose a framework that allows for the selection of any prior suitable for various tasks. As a limitation, we leave experiments on other types of conditions, such as class labels or time-independent conditions, for future work. We believe that incorporating sampling guidance can address these types of conditions. As the proposed method can cover a wide range of scenarios in time series diffusion models, we believe this work significantly advances the investigation of prior distributions for developing a general time series generation framework."}, {"title": "Diffusion Bridge", "content": "We introduce the details of the diffusion bridge mathematically, and how this framework is embraced in recent deep learning architectures.\nSchr\u00f6dinger Bridge. Standard diffusion models suffer from limitations such as the requirement of a sufficiently long time for the prior distribution to approximate a standard Gaussian distribution and the computational expense associated with sample generation. To address these challenges, diffusion Schr\u00f6dinger bridge models have been recently introduced (De Bortoli et al. 2021; Chen, Liu, and Theodorou 2021). The goal of the Schr\u00f6dinger bridge problem is to find an entropic optimal transport between two probability distributions in terms of Kullback-Leibler divergence on path spaces within a finite horizon (Chen, Liu, and Theodorou 2021; L\u00e9onard 2012). While standard diffusion models and flow matching models are not guaranteed to provide optimal transport, Schr\u00f6dinger bridge models aim to find paths that recover entropy-regularized versions of optimal transport (Shi et al. 2024). The formulation of the Schr\u00f6dinger bridge problem is as follows:\n$\\min_{P \\in P[0,T]} D_{KL}(P||P_{ref}), s.t. P_0 = P_{data}, P_T = P_{prior},$\nwhere $P[0,T]$ refers to the space of path measures on [0, T], $P_{ref}$ denotes the reference path measure, and $P_0, P_T$ represents the marginal distributions of p at each time step 0, T, respectively. The diffusion bridge algorithm is introduced by utilizing pref from the forward SDE in Equation (1) (Vargas et al. 2021; Chen, Liu, and Theodorou 2021; De Bortoli et al. 2021). Setting the reference path as the forward SDE, the problem becomes equivalent to following forward-backward SDEs:\n$dx_t = [f(x_t, t) + g^2(t)\\nabla log \\Psi_t(x_t)]dt + g(t)dw_t,$\n$x_0 \\sim P_{data},$\n$dx_t = [f(x_t, t) - g^2(t)\\nabla log \\hat{\\Psi}_t(x_t)]dt + g(t)dw_t,$\n$x_T \\sim P_{prior},$\nwhere $\\nabla log \\Psi_t(x_t)$ and $\\nabla log \\hat{\\Psi}_t(x_t)$ are described by following PDEs:\n$\\frac{\\partial \\Psi}{\\partial t} = -\\nabla f \\Psi - \\frac{1}{2} tr(g^2 \\nabla^2 \\Psi)$\n$\\frac{\\partial \\hat{\\Psi}}{\\partial t} = -\\nabla (f\\hat{\\Psi}) + \\frac{1}{2} tr(g^2 \\nabla^2 \\hat{\\Psi})$\ns.t. $\\Psi_0\\hat{\\Psi}_0 = P_{data}, \\Psi_T\\hat{\\Psi}_T = P_{prior},$\n$P_t = \\Psi\\hat{\\Psi}_t \\forall t \\in [0, T].$\nThe Schr\u00f6dinger bridge problem can be addressed using iterative methods (Kullback 1968; Fortet 1940). At the outset, several early studies employed iterative methods to simulate trajectories converging to the Schr\u00f6dinger bridge problem (Vargas et al. 2021; Shi et al. 2024; De Bortoli et al. 2021). However, these methods rely on expensive iterative approximation techniques and have seen limited empirical application (Zhou et al. 2024)."}, {"title": "Denoising Diffusion Bridge Models", "content": "Alternatively, Chen, Liu, and Theodorou (2021) leveraged the theory of forward-backward SDEs to derive an exact log-likelihood expression for the Schr\u00f6dinger bridge, which accurately generalizes the approach for score generative models. Drawing inspiration from the perspective that the framework of diffusion bridges, employing forward-backward SDEs, encompasses the paradigms of score matching diffusion models (Song, Meng, and Ermon 2020) and flow matching optimal transport paths (Lipman et al. 2022), Zhou et al. (2024) demonstrated that reintroducing several design choices from these domains becomes feasible. In particular, the reparameterization techniques outlined in Karras et al. (2022) are utilized.\nFor noise scheduling, common options include variance-preserving (VP) and variance-exploding (VE) diffusion (Song, Meng, and Ermon 2020). We express the reparameterization for the VP and VE bridge of Zhou et al. (2024) in Table 7. In our study, we choose to utilize the VP bridge, as our experimental findings indicate its superiority of VP over VE.\nFor score matching, Karras et al. (2022) proved that if a denoiser function D(x; \u03c3) minimizes the expected L2 error for samples drawn from Pdata, then D(x;o) becomes the linear function of score function, i.e., if D(x; \u03c3) minimizes $E_{y \\sim p_{data}} E_{\\epsilon \\sim N(0,\\sigma^2I)} ||D(y + \\epsilon; \\sigma) - y||_2^2$, then $\\nabla log p(x; \\sigma) = (D(x; \\sigma) - x)/\\sigma^2$. Thus, we can train De directly instead of training the score function. In general, training a neural network to model De directly is ineffective, due to its vast variance, which heavily depends on the noise level. Thus, Karras et al. (2022) reconstructed De with \u03c3-dependent skip connection as follows:\n$D_0(x; \\sigma) = C_{skip}(\\sigma)x + C_{out}(\\sigma)F_\\theta(C_{in}(\\sigma)x; C_{noise}(\\sigma)),$\nand trained $F_\\theta$ instead of $D_\\theta$ with the neural network. However, for time series data, estimating the diffusion noise poses greater challenges due to the presence of highly irregular noisy components (Shen and Kwok 2023; Yuan and Qiao 2024). Consistent with these studies, our experiments demonstrate that D-matching is more effective than F-matching. Detailed experimental results for comparison of VP with VE and D-matching with F-matching are provided in the ablation study in the Appendix."}, {"title": "Details of Datasets and Baselines", "content": "We first illustrate time series generation situations on various conditions in Figure 6.\nData explanation. Table 8 presents the statistics of the datasets along with their online links.\nBaseline methods. For the unconditional baseline methods, we compare with TimeVAE (Desai et al. 2021), TimeGAN (Yoon, Jarrett, and Van der Schaar 2019), Cot-GAN (Xu et al. 2020), Diffwave (Kong et al. 2021), DiffTime (Coletta et al. 2024), and Diffusion-TS (Yuan and Qiao 2024). For Table 1, we note the quality evaluation results in Diffusion-TS. We reimplement Diffusion-TS using their official GitHub in https://github.com/Y-debug-sys/Diffusion-"}, {"title": "Experimental settings", "content": "Model Architecture\nWe utilized Diffusion-TS (Yuan and Qiao 2024) as the based model", "follows": "n$V_{i", "cP": "for c = [0", "1": "T/\u03c4. We recover seasonal and error components using Fourier bases as follows:\n$A_{i,t}^{(k)} = F(w_{i,t}^{(k)}), \\Phi_{i,t}^{(k)} = \\phi (F(w_{i,t})_k),$\n$K_{i,t} = argTopK_K \\{ A_{i,t}^{(k)} \\},\nk \\in \\{1,..."}]}