[{"title": "Leveraging Priors via Diffusion Bridge for Time Series Generation", "authors": ["Jinseong Park", "Seungyun Lee", "Woojin Jeong", "Yujin Choi", "Jaewook Lee"], "abstract": "Time series generation is widely used in real-world applications such as simulation, data augmentation, and hypothesis test techniques. Recently, diffusion models have emerged as the de facto approach for time series generation, emphasizing diverse synthesis scenarios based on historical or correlated time series data streams. Since time series have unique characteristics, such as fixed time order and data scaling, standard Gaussian prior might be ill-suited for general time series generation. In this paper, we exploit the usage of diverse prior distributions for synthesis. Then, we propose TimeBridge, a framework that enables flexible synthesis by leveraging diffusion bridges to learn the transport between chosen prior and data distributions. Our model covers a wide range of scenarios in time series diffusion models, which leverages (i) data- and time-dependent priors for unconditional synthesis, and (ii) data-scale preserving synthesis with a constraint as a prior for conditional generation. Experimentally, our model achieves state-of-the-art performance in both unconditional and conditional time series generation tasks.", "sections": [{"title": "Introduction", "content": "Synthetic time series have expanded their applications by serving roles in data augmentation (Wen et al. 2021), privacy-preserving synthesis (Wang et al. 2024), continuous data compression (Zhou et al. 2023), and simulation of specific hypothesis (Xia et al. 2024) for the data streams. With the advancement of deep generative architectures, time series generative models have been actively investigated such as variational autoencoders (VAE) (Desai et al. 2021; Naiman et al. 2024), generative adversarial networks (GANs) (Yoon, Jarrett, and Van der Schaar 2019; Jeon et al. 2022; Xia et al. 2024), and diffusion models (Ho, Jain, and Abbeel 2020; Karras et al. 2022).\nDiffusion models are well-known for producing high-quality and diverse time series data samples (Tashiro et al. 2021; Alcaraz and Strodthoff 2023). The effectiveness of these models relies on iteratively estimating small steps of the reverse trajectory, which corresponds to the forward process towards standard Gaussian priors (Song et al. 2021).\nHowever, modeling diffusion with constraints is not flexible due to the predetermined mapping from noise to data (Harvey et al. 2022; Du et al. 2023). In the time series domain, Coletta et al. (2024) highlighted that generating data under specific bull or bear market patterns requires tailored embedding or penalty functions for sampling guidance. Additionally, the sequential order and data scales (Kim et al. 2022) must be considered when modeling the statistics of historical data and time dependencies (Zeng et al. 2023).\nRecently, altering standard Gaussian noise in diffusion models has been explored by modeling the data-dependent distribution in both forward and reverse processes (Han, Zheng, and Zhou 2022; Yu et al. 2024). Specifically, in the time series domain, the noising process should consider not only the diffusion time steps but also the time stamps of temporal data to maintain the continuity of generated data and time dependency (Bilo\u0161 et al. 2023; Chen et al. 2023a). In this paper, we adopt the diffusion bridge (Schr\u00f6dinger 1932; De Bortoli et al. 2021; Zhou et al. 2024) to utilize diverse diffusion priors. By learning the optimal transport between the data distribution and fixed priors, diffusion bridges make use of fixed priors and have proven effective in conditional tasks such as image-to-image translation (Li et al. 2023) and text-to-speech synthesis (Chen et al. 2023b).\nTo push further, we investigate the various prior designs proper for time series tasks. Then, we propose TimeBridge, a flexible time series generation framework that addresses multiple time series synthesis with the prior design of diffusion bridge, as shown in Figure 1. We then deeply investigate various experimental settings from unconditional to conditional tasks, achieving state-of-the-art performance across multiple datasets, as illustrated in Figure 2. Our contributions to various situations are summarized as follows:\n\u2022 We explore the prior selections on time series and propose TimeBridge, a diffusion bridge model specifically designed to leverage the priors for time series synthesis.\n\u2022 For unconditional generation, we propose prior distributions tailored to time series synthesis by leveraging data- and time-dependent properties.\n\u2022 For conditional generation, we integrate the constraints into the pair-wise prior to maintaining data scales. We also introduce point-preserving sampling to preserve constraints, extending its application to imputation tasks."}, {"title": "Preliminaries", "content": "Consider the multivariate time series x defined within the sample space \\(X = \\mathbb{R}^{d\\times \\tau}\\), where d represents the dimensionality of the variables and \\(\\tau\\) denotes the time length. For each index \\(i \\in \\{1, ..., N\\}\\) among the N samples, each time series is denoted as \\(x^{i} = (x^{i}(1),...,x^{i}(\\tau)) \\in X\\), given the data \\(x^{i}(k) \\in \\mathbb{R}^{d}\\) for each time step \\(k \\in \\{1, ..., \\tau\\}\\). For diffusion steps \\(t \\in \\{0, . . ., T\\}\\), we denote intermediate time series samples as \\(x_{t}\\), and for a specific instance i as \\(x_{t}^{i}\\).\nDiffusion process. In standard diffusion models (Ho, Jain, and Abbeel 2020), the diffusion process is constructed by gradually injecting noise into samples \\(x_{0}\\) drawn from the data distribution \\(p_{0}\\), forwarding them into a standard Gaussian distribution \\(p_{T} = \\mathcal{N}(0, I)\\). The corresponding stochastic differential equation (SDE) (Song et al. 2021) is:\n\\[d x_{t}=f\\left(x_{t}, t\\right) d t+g(t) d w_{t},\\qquad(1)\\]\nwhere \\(f : \\mathbb{R}^{d} \\times [0, T] \\rightarrow \\mathbb{R}^{d}\\) represents the drift function, \\(g : [0, T] \\rightarrow \\mathbb{R}\\) denotes the diffusion coefficient, and \\(w_{t}\\) is the Wiener process. The reverse SDE is formulated as follows:\n\\[d x_{t}=\\left[f\\left(x_{t}, t\\right)-g(t)^{2} \\nabla_{x_{t}} \\log p\\left(x_{t}\\right)\\right] d t+g(t) d w_{t},\\qquad(2)\\]\nwhere \\(p(x_{t})\\) refers to the probability density of \\(x_{t}\\) (Anderson 1982) and \\(\\nabla_{x_{t}} \\log p(x_{t})\\) is a score function to match."}, {"title": "Diffusion bridge", "content": "The aforementioned standard diffusion models cannot be extended to more general tasks due to their inflexibility in choosing a prior distribution. Instead, to construct the diffusion process towards any endpoint, Doob's h-transform (Doob and Doob 1984) can be applied to Equation (1). With a fixed endpoint y, the forward process is:\n\\[d x_{t}=\\left[f\\left(x_{t}, t\\right)+g(t)^{2} h\\left(x_{t}, t, y, T\\right)\\right] d t+g(t) d w_{t},\\qquad(3)\\]\n\\[x_{0} \\sim q_{\\text {data }}(X), \\quad x_{T}=y,\\]\nwhere \\(h(x, t, y, T)=\\nabla_{x_{t}} \\log p\\left(x_{t} \\mid x_{T}\\right) \\mid x_{t}=x, x_{T}=y\\). For a given data distribution \\(q_{\\text {data }}(x, y)\\), the diffusion bridge (Schr\u00f6dinger 1932; De Bortoli et al. 2021) aims to transport \\(x_{0}\\) to \\(x_{T}\\) where \\((x_{0}, x_{T})=(x, y) \\sim q_{\\text {data }}(x, y)\\) while following above forward process. Similar to the standard diffusion model, we can construct the reverse SDE as follows:\n\\[d x_{t}=\\left[f\\left(x_{t}, t\\right)-g(t)^{2}\\left(s\\left(x_{t}, t, y, T\\right)-h\\left(x_{t}, t, y, T\\right)\\right)\\right] d t+g(t) d w_{t},\\qquad(4)\\]\nwhere \\(x_{T}=y\\) and its score function is calculated as\n\\[s(x, t, y, T)=\\nabla_{x_{t}} \\log q\\left(x_{t} \\mid x_{T}\\right) \\mid x_{t}=x, x_{T}=y\\qquad(5)\\]\nFor training, it is sufficient to learn the score \\(s(x, t, y, T)\\). The Schr\u00f6dinger bridge (Schr\u00f6dinger 1932) is a closely related concept, focusing on the path optimizations between two arbitrary distributions. Diffusion bridge models have become popular due to their abilities (i) to flexibly map in various domains, including image-to-image translation (Li et al. 2023; Shi et al. 2022) and text-to-speech synthesis (Popov et al. 2021; Chen et al. 2023b), and (ii) to achieve efficient sampling with fewer diffusion steps (Zhou et al. 2024; Chen et al. 2023b)."}, {"title": "Reparametrization of diffusion bridge", "content": "To approximate the score \\(s(x, t, y, T)\\) in Equation (5), Zhou et al. (2024) recently proposed a Denoising Diffusion Bridge Model (DDBM), integrating the diffusion bridge model in the framework of standard diffusion models. DDBM enables tractable marginal sampling of \\(x_{t}\\), by designing \\(x_{t}=\\alpha_{t} x_{0}+\\sigma_{t} \\epsilon\\), where \\(\\alpha_{t}\\) and \\(\\sigma_{t}\\) are the noise schedule functions and \\(\\epsilon \\sim \\mathcal{N}(0, I)\\). For the variance-preserving (VP) schedule, the marginal distribution at time t is as follows:\n\\[q\\left(x_{t} \\mid x_{0}, x_{T}\\right)=\\mathcal{N}\\left(\\hat{\\mu}_{t}, \\hat{\\sigma} I\\right),\\qquad(6)\\]\n\\[\\hat{\\mu}_{t}=\\frac{\\mathrm{SNR}_{T} \\alpha_{t} x_{T}+\\alpha_{T} x_{0}}{\\mathrm{SNR}_{t}},\\left(1-\\frac{\\alpha_{t}}{\\mathrm{SNR}_{t}}\\right)\\qquad(7)\\]\n\\[\\hat{\\sigma}_{t}^{2}=\\sigma_{t}^{2}\\left(1-\\frac{\\sigma_{t}^{2}}{\\mathrm{SNR}_{t}}\\right),\\qquad(8)\\]\nwhere signal-to-ratio (SNR) is defined as \\(\\mathrm{SNR}_{t}=\\alpha_{t}^{2} / \\sigma_{t}^{2}\\). Since the mean \\(\\hat{\\mu}_{t}\\) is a linear combination of \\(x_{0}\\) and \\(x_{T}\\) at time t, the data scale is preserved for translation. With the reparametrization of Elucidating Diffusion Models (EDM) (Karras et al. 2022), we can match the score as follows:\n\\[\\nabla_{x} \\log q\\left(x_{t} \\mid x_{T}\\right) \\approx s\\left(D_{0}, x_{t}, t, x_{T}, T\\right):=\\frac{\\left(x_{T}-\\alpha_{T} x_{t}+\\alpha_{T} D_{0}\\left(x_{t}, t, x_{T}\\right)\\left(1-\\frac{\\sigma_{t}^{2}}{\\mathrm{SNR}_{t}}\\right)\\right)}{\\sigma_{t}^{2}\\left(1-\\frac{\\sigma_{t}^{2}}{\\mathrm{SNR}_{t}}\\right)},\\qquad(9)\\]\nwhere \\(D_{0}\\left(x_{t}, t, x_{T}\\right)\\) is the model output as a denoiser."}, {"title": "Problem Statement", "content": "In this section, we cover two types of time series synthesis tasks, i.e., unconditional and conditional generations.\nUnconditional generation considers the total dataset containing N samples, denoted as \\(\\mathcal{D} = \\{x^{i}\\}_{i=1}^{N}\\). Our goal is to train a deep generative model to mimic the input dataset and yield synthetic samples statistically similar to a given time series dataset. We aim to make the distribution \\(p(x)\\) of synthetic data x, similar to the input data distribution \\(p(x)\\).\nConditional generation further utilizes the paired condition y to guide time series generation of each data sample x as \\(\\mathcal{D} = \\{(x^{i}, y^{i})\\}_{i=1}^{N}\\). Thus, we try to make the conditional distribution \\(p(x|y)\\) of synthetic data x closely mirror the conditional distribution of input data \\(p(x|y)\\). Considering that the condition in time series is often also a time series as \\(y \\in X\\), Coletta et al. (2024) divided the constraints into two types: (i) soft constraint to guide the time series generation such as trend, or (ii) hard constraint where we should follow during synthesis such as fixed points. In a broader view, well-known time series tasks can be considered as conditional generation, e.g., the unmasked part in imputation or the historical data of forecasting as conditions.\nWith the diffusion bridge framework, we hypothesize that the standard Gaussian prior might not be the optimal choice. Thus, we investigate a general time series synthesis framework by analyzing the prior selection in diffusion models based on the following research questions (RQs):\nRQ1. Are data-dependent priors effective for time series?\nRQ2. How to model temporal dynamics with priors?\nRQ3. Can constraints be used as priors for diffusion?\nRQ4. How to preserve data points with diffusion bridge?"}, {"title": "Diffusion Prior Design with TimeBridge", "content": "To address these questions, we first explore suitable prior designs for diffusion synthesis. Afterward, we propose TimeBridge, a general diffusion bridge model that leverages the aforementioned prior selections, elaborating its application in both unconditional and conditional tasks."}, {"title": "Data- and Time-dependent Priors for Time Series", "content": "For unconditional tasks, we examine the benefit of better priors depending on data and temporal properties of time series.\nData-Dependent Prior. To investigate RQ1, we suggest using data prior to having a data-dependent distribution to better approximate the data distribution. Sampling from \\(\\mathcal{N}(0, I)\\) lacks data-specific information, which can be enhanced by enforcing that the priors capture data scale and temporal dependencies. Recently, the usages of data-dependent prior have been investigated for audio (Popov et al. 2021; Lee et al. 2022) and image (Yu et al. 2024) domains. Therefore, we test the use of data-dependent prior for time series by setting the prior distribution as follows:\n\\[x_{T} \\sim \\mathcal{N}(\\mu, \\operatorname{diag}(\\sigma^{2})),\\qquad(10)\\]\nwhere \\(\\mu\\) and \\(\\sigma^{2}\\) are independently calculated in the same dimension of the time series x in \\(\\mathbb{R}^{\\tau \\times d}\\). \\(\\operatorname{diag}(\\sigma^{2})\\) indicates the diagonal covariance matrix of each element."}, {"title": "Time-Dependent Prior", "content": "As a response to RQ2, we focus on capturing temporal dependencies in noise addition towards prior distribution. Recently, Bilo\u0161 et al. (2023) injected stochastic processes instead of random noise to preserve the continuity of temporal data and Han, Zheng, and Zhou (2022) utilized the pre-trained model for prediction and set the mean of prior as the output for regression.\nAs a solution, we employ Gaussian processes (GPs) for prior selection, which is effective for modeling temporal data (Bilo\u0161 et al. 2023; Ansari et al. 2024). GPs are distributions over functions with time input k characterized by the mean function m(k) and the positive definite kernel \\(K(k, k')\\). We use the radial basis function (RBF) kernel, defined as \\(K(k, k') = \\exp(-\\gamma|k \u2013 k'|^{2})\\), to ensure a stationary process. To construct a time-dependent prior based on data-dependent properties, we add a variance term to the kernel function to encode data information into the prior distribution. Consequently, we construct the prior incorporating of the temporal dynamics as follows:\n\\[x_{T} \\sim \\mathcal{G P}(m, K+\\Sigma),\\qquad(11)\\]\nwhere \\(\\Sigma = [\\Sigma_{i j}]\\) is a correlation matrix function in which \\(\\Sigma_{i j}\\) represents the correlation of the data between timestamps i and j. For simplified version of Equation (10), we set m as \\(\\mu\\) and \\(\\Sigma\\) as \\(\\operatorname{diag}(\\sigma^{2})\\) to align with the data-dependent prior, i.e., \\(x_{T} \\sim \\mathcal{N}(\\mu, K+\\operatorname{diag}(\\sigma^{2}))\\)."}, {"title": "Scale-preserving Sampling for Conditional Priors", "content": "We now consider the condition \\(y \\in X\\), provided as a time series, which is the most prevalent case for real-world settings. For example, a given trend can guide the synthesis output or fixed points at specific time steps can determine the properties of synthetic data. We explore the use of a prior on the same data scale instead of relying solely on conditional embedding in standard diffusion.\nSoft constraints with given trends. To answer RQ3, we argue that the diffusion bridge is well-suited for preserving a given condition by setting the prior as the condition. The diffusion bridge has shown its effectiveness in conditional tasks such as coloring (Zhou et al. 2024), image-to-image translation (Li et al. 2023; Liu et al. 2023), and improving naive base predictions (Chen et al. 2023b; Lee et al. 2024).\nConditional generation based on trend is straightforward by (i) setting the pair-wise condition to prior \\(x_{T}=y\\) and (ii) training the translation from trend to data. As the expected mean of \\(x_{t}\\) is a linear combination of \\(x_{0}\\) and \\(x_{T}=y\\) in Equation (6), the model learns the correction starting from the trend samples, utilizing the same data scale. In contrast, standard diffusion models are unsuitable for translating conditions into synthetic data; instead, they generate new data based on a specific trend condition, typically requiring additional steps such as providing guidance or correcting intermediate data samples \\(x_{t}\\) during sampling. Thus, we can also eliminate the need for additional penalty functions or value correction during sampling.\nHard constraints with fixed points. In RQ4, preserving data points of hard constraints can be important in conditional tasks. To ensure the fixed point condition during sampling, we introduce a novel point-preserving sampling method that prevents adding noise to the values to be fixed. This is a unique trait of our framework because standard diffusion models cannot preserve identity trajectories to the constrained values from Gaussian noise. We make details of the proposed sampling method in Appendix .\nThe direct case of fixed point constraints is imputation. With the mask m indicating the missing value as 0, we construct the condition c = ym for imputation. However, unlike soft constraints that apply to all values in \\(y \\in \\mathbb{R}^{\\tau \\times d}\\), the condition for imputation \\(c \\in \\mathbb{R}^{\\tau \\times d}\\) has missing values, which should be interpolated to data-dependent values rather than zeros. Therefore, we build local linear models for condition-to-time series imputation. We adopt linear spline interpolation (De Boor 1978) to generate a prior for fixed point constraints. In a feature-wise manner, let us consider \\(x \\in \\mathbb{R}\\) to be a single dimensional input of \\(x \\in \\mathbb{R}^{d}\\). With a set K of feature-wise observed times for x, the prior \\(x_{T}\\) with the interpolation of each feature with timestamp k is:\n\\[x_{T}(k)=x\\left(k_{j-1}\\right)+\\frac{x\\left(k_{j}\\right)-x\\left(k_{j-1}\\right)}{k_{j}-k_{j-1}}\\left(k-k_{j-1}\\right),\\qquad(12)\\]\n\\[k_{j-1} \\leq k<k_{j}, \\quad j=1, ..., |K|-1.\\]\nNote that before the first observation, \\(k<k_{0}, x_{T}(k)=x(k_{0})\\). After the last observation, \\(k \\geq k_{|K|-1}, x_{T}(k)=x(k_{|K|-1})\\). The detailed Algorithm is in Appendix ."}, {"title": "TimeBridge: Bridge Modeling for Time Series", "content": "Based on the aforementioned prior selections, we propose TimeBridge, a general diffusion bridge model for time series synthesis. Our approach yields high-quality samples with fewer sampling steps and is compatible with existing time series diffusion frameworks.\nFor score matching in Equation (9), we build the denoiser \\(D_{\\theta}\\) for time series. For architecture design, we adopt the backbone of Diffusion-TS (Yuan and Qiao 2024), i.e., Fourier-based loss function (Zhou et al. 2022; Liu et al. 2024; Nguyen et al. 2022; Yi et al. 2024) and transformer encoder-decoder models with seasonal-trend decomposition (Zeng et al. 2023; Wu et al. 2021). With the bridge diffusion framework, our objective function is formulated as follows:\n\\[\\mathcal{L}_{\\theta}=\\mathbb{E}_{t, x_{0}}\\left[w_{t}\\left(\\left\\|x_{0}-D_{\\theta}\\left(x_{t}, t, x_{T}\\right)\\right\\|_{2}^{2}+\\lambda\\left\\|\\mathrm{FFT}\\left(x_{0}\\right)-\\mathrm{FFT}\\left(D_{\\theta}\\left(x_{t}, t, x_{T}\\right)\\right)\\right\\|_{2}^{2}\\right)\\right].\\qquad(13)\\]\n\\(w_{t}\\) indicates the weight scheduler of the loss function and \\(\\lambda\\) indicates the strength of fast Fourier transform (FFT). We construct the bridge model to produce output by decomposing the input into trend and seasonal components as follows:\n\\[D_{\\theta}=\\mathcal{V}_{\\mathrm{tr}}\\left(\\theta, x_{t}, t, x_{T}\\right)+\\sum_{i=1}^{K} \\mathcal{S}_{i, t}\\left(\\theta, x_{t}, t, x_{T}\\right)+\\mathcal{R}\\left(\\theta, x_{t}, t, x_{T}\\right),\\qquad(14)\\]\nwhere \\(\\mathcal{V}_{\\mathrm{tr}}\\) is the output for the trend synthesis layer, \\(\\mathcal{S}_{i, t}\\) is the output for each seasonal synthesis layer i among K layers and \\(\\mathcal{R}\\) is the output for estimated residual.\nAs time series data possess stochasticity (Shen and Kwok 2023), we directly predict \\(D_{\\theta}\\) rather than using parametrization technique from noise prediction used in other domains (Karras et al. 2022; Zhou et al. 2024). Moreover, we utilize the second-order Heun sampler (Ascher and Petzold 1998; Karras et al. 2022), achieving both quality and efficiency. Overall, our TimeBridge model leverages the flexibility of prior selection as a diffusion bridge designed for time series."}, {"title": "Related Works", "content": "Standard diffusion models for time series. Time series diffusion models have become the de facto method in synthesis tasks. Rasul et al. (2021) initially used diffusion networks for time series based on recurrent networks. For imputation, SSSD (Alcaraz and Strodthoff 2023) and CSDI (Tashiro et al. 2021) considered time series imputation similar to image inpainting tasks. TimeDiff (Shen and Kwok 2023), LDT (Feng et al. 2024), and TMDM (Li et al. 2024) focused on time series forecasting tasks. Recently, Coletta et al. (2024) investigated guiding the sampling with a penalty function called DiffTime. Diffusion-TS (Yuan and Qiao 2024) introduced a decomposition architecture to disentangle time series data into trend and seasonal components and applied a Fourier-based loss term. They also expanded their unconditional model to conditional generation tasks by guiding the diffusion sampling toward the input conditions.\nExisting time series bridge models. Recently, some researchers have investigated the use of the diffusion bridge in the time series domain. Chen et al. (2023a) investigated the convergence analysis of the Schr\u00f6dinger bridge algorithm and applied them to time series imputation tasks. Ham-douche, Henry-Labordere, and Pham (2023) used kernel estimation for the Schr\u00f6dinger bridge combined with feed-forward and LSTM networks, especially for the application of deep hedging on real-data sets. Garg, Zhang, and Zhou (2024) relaxed the Schr\u00f6dinger bridge by setting a geometric mixture of targets and other distributions for the prior distribution and applied their method to time series. However, most of them used iterative solutions for bridge problems, which required a heavy computational burden and incompatibility with recent standard diffusion models. Furthermore, none of them suggested a general framework for both unconditional and conditional time series generation."}, {"title": "Experiments", "content": "We assess the performance of TimeBridge using widely used time series datasets. We use two simulation datasets: Sines with 5 features of different frequencies and phases of sine functions and MuJoCo of multivariate advanced physics simulation with 14 features. For real-world datasets, we use ETT (Electricity Transformer Temperature) for long-term electric power with 7 features, Stocks of Google stock prices and volumes with 6 features, Energy, UCI dataset with appliances energy use in a low energy building with 28 features, and fMRI of the blood oxygen level-dependent functional MR imaging with 50 selected features.\nFor measures, we assess four metrics for unconditional time series generation on a normalized (0-1) scale. Context-Fr\u00e9chet Inception Distance (Context-FID) score (Jeha et al."}, {"title": "Unconditional Generation", "content": "We demonstrate the unconditional generation tasks with a length of 24 for the aforementioned datasets and evaluate the performance of the previous measures. For baseline methods, we compare with TimeVAE (Desai et al. 2021), TimeGAN (Yoon, Jarrett, and Van der Schaar 2019), Cot-GAN (Xu et al. 2020), Diffwave (Kong et al. 2021), DiffTime (Coletta et al. 2024) with SSSD (Alcaraz and Strodthoff 2023) backbone, and Diffusion-TS (Yuan and Qiao 2024). For the proposed TimeBridge, we choose the data-dependent settings in Equation (10) and TimeBridge-GP denotes the time-dependent GP prior in Equation (11). Results are shown in Table 1. We observe that the proposed method outperforms the previous methods. The performance gain is significant in context-FID and discriminative scores, reducing the scores of 40.70% and 18.05% on average, respectively. Notably, the strength of data and time-dependent prior is shown in real datasets, such as ETTh and Stocks. This indicates that our prior selection and modeling help in approximating the data distribution during synthesis.\nTo visualize the generated data, we plot the t-SNE plots with the generated data in Figure 3. The red original data and the blue synthetic data show the overlapped plot in almost all datasets. The generated samples well approximate the distribution properties in complex datasets, such as the Stocks and Energy datasets. For further visualization concerning PCA and kernel density plots, refer to Appendix ."}, {"title": "Synthesis with Trend Condition", "content": "We now consider trend-guided generation, where the data samples are in \\(\\mathcal{D} = \\{(x^{i}, y^{i})\\}_{i=1}^{N}\\). For the trend y, we consider three types: linear, polynomial with degree of three, and Butterworth filter (Butterworth et al. 1930), a signal processing filter that captures trends (Xia et al. 2024). Using these trends as conditions, we reimplement the experiments in Table 1 with the ETTh and Energy datasets. To apply the condition to baseline methods, we feed the trend into a conditional embedding and concatenate it with the original embedding. For the proposed TimeBridge, we set the prior \\(x_{T} = y\\) upon the conditional embedding for translation.\nExperimental results are shown in Table 2. The rows with trend baseline indicate the results with the trends without any synthesis. After synthesis with trend conditions, we observe that the quality metrics such as Context-FID, correlation, and discriminative scores diminished compared to the unconditional generation in Table 1, indicating that trend conditions help the model generate more realistic data samples. With the trend condition in \\(x_{T}\\), the proposed TimeBridge achieves the best performance in Context-FID in all settings and the best discriminative score in 5 out of 6 settings while maintaining correlation scores at similar levels to Diffusion-TS. This indicates that the proposed framework can be broadly adopted by different conditioning settings, without controlling any details other than the prior distribution. The sampling path of TimeBridge from trend to data is shown in Figure 4a."}, {"title": "Imputation with Fixed Masks", "content": "For imputation, which is a common setting of hard conditions, we compare the proposed method with imputation diffusion models and the conditional Langevin sampler of Diffusion-TS (Yuan and Qiao 2024). We use random masks for the Mujoco dataset and the geometric mask from Zerveas et al. (2021) for the ETTh and Energy datasets, with a given missing ratio as in Yuan and Qiao (2024). We set the total length of imputation to 100 for the Mujoco dataset, matching the data length, and split the ETTh and Energy datasets into 48 time steps. For the proposed TimeBridge, we preserve the observed values and set the prior \\(x_{T}\\) as the interpolated values of the condition c as in Equation (12).\nThe imputation results are shown in Table 3 and Table 4. Both tables demonstrate that TimeBridge achieves the lowest measures in imputation tasks. Since the Langevin sampler of Diffusion-TS does not require individual training for various missing ratios, we ensure a fair comparison by evaluating TimeBridge with the same training setting, TimeBridge-75%, which uses 75% missing values for training. The performance of TimeBridge-75% even outperforms the models trained with equal missing values, highlighting the generalization capability of TimeBridge in imputation tasks. The visualization of imputation results is shown in Figure 5, and the sampling path can be found in Figure 4b."}, {"title": "Ablation study", "content": "Table 5 evaluates the effectiveness of training design with three variants: (1) variance exploding (VE) scheduler, (2) noise matching instead of directly predicting \\(D_{\\theta}\\) as in Zhou et al. (2024), and (3) without Fourier-based loss during training. Further experimental results of the ablation study are presented in Appendix ."}, {"title": "Computation efficiency", "content": "Table 6 illustrates the strength of the proposed method, particularly when using the second-order Heun sampler during sampling. With a lower number of score function evaluations (NFE), our sampler achieves fast synthesis without compromising quality. We use 40 steps for sampling in all experiments, resulting in 119 NFEs with our settings. For comparison, we evaluate the Diffusion-TS using the same NFE. Given the trade-off of performance and sampling time in Diffusion-TS, our experimental results outperform previous methods. The marginal increase in time for TimeBridge is due to the time required to build the prior distribution."}, {"title": "Conclusion", "content": "In this paper, we highlight the effectiveness of selecting prior distributions for time series generation using diffusion bridges. To address the limitations in the standard Gaussian prior of the diffusion models, we propose a framework that allows for the selection of any prior suitable for various tasks. As a limitation, we leave experiments on other types of conditions, such as class labels or time-independent conditions, for future work. We believe that incorporating sampling guidance can address these types of conditions. As the proposed method can cover a wide range of scenarios in time series diffusion models, we believe this work significantly advances the investigation of prior distributions for developing a general time series generation framework."}, {"title": "Diffusion Bridge", "content": "We introduce the details of the diffusion bridge mathematically", "follows": "n\\[\\min _{P \\in P[0, T"}]}, {}, {}, [0, "T"], {}, "p_{0"]