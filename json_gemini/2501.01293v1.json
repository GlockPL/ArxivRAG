{"title": "LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite Networks", "authors": ["Zheng Lin", "Yuxin Zhang", "Zhe Chen", "Zihan Fang", "Cong Wu", "Xianhao Chen", "Yue Gao", "Jun Luo"], "abstract": "Recently, the increasing deployment of LEO satellite systems has enabled various space analytics (e.g., crop and climate monitoring), which heavily relies on the advancements in deep learning (DL). However, the intermittent connectivity between LEO satellites and ground station (GS) significantly hinders the timely transmission of raw data to GS for centralized learning, while the scaled-up DL models hamper distributed learning on resource-constrained LEO satellites. Though split learning (SL) can be a potential solution to these problems by partitioning a model and offloading primary training workload to GS, the labor-intensive labeling process remains an obstacle, with intermittent connectivity and data heterogeneity being other challenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL design tailored for satellite networks to combat these challenges. Leveraging SS learning to handle (labeled) data scarcity, we construct an auxiliary model to tackle the training failure of the satellite-GS non-contact time. Moreover, we propose a pseudo-labeling algorithm to rectify data imbalances across satellites. Lastly, an adaptive activation interpolation scheme is devised to prevent the overfitting of server-side sub-model training at GS. Extensive experiments with real-world LEO satellite traces (e.g., Starlink) demonstrate that our LEO-Split framework achieves superior performance compared to state-of-the-art benchmarks.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, more and more low earth orbit (LEO) satellites have been sent into space to build satellite-enabled broadband Internet and sensing networks [1]\u2013[4]. Specifically, to build the LEO satellite meta-constellation, commonly known as Starlink, SpaceX plans to launch approximately 42,000 LEO satellites [5]\u2013[7]. Beyond providing enhanced broadband internet connectivity [8], these LEO satellites are equipped with multimodal sensors capable of collecting a wide spectrum of informative sensor data, typically including spectral data [9], [10] and terrestrial images [11], [12]. More importantly, LEO satellites suggest a new type of infrastructure for networking and sensing in the space, and multiple tenants (e.g., mobile operators and remote sensing companies) may build their services based on this infrastructure [1], [13].\nTo fully take advantage of the rich information provided by various sensors of LEO satellites, these tenants can leverage deep learning (DL) to resolve many global challenges such as food safety [14], disease spread [15], and climate change [16]. However, deploying DL frameworks over satellite networks still faces significant obstacles. Firstly, the excessive costs in both manpower (experts with interdisciplinary expertise) and time render the manual annotation of vast satellite datasets impractical. Consequently, sensing data from LEO satellites is commonly unlabeled, substantially deteriorating data utilization and analysis efficacy. Secondly, the conventional centralized DL paradigm entails raw data gathering and processing at a central server [17], [18]. In satellite networks, due to constrained power, size, and mass of a satellite, the computing power is often less than a central server (e.g., LEO satellite of Planet Labs equipped with common NVIDIA Jetson [19] has only 1/10 of RTX 3090's GPU cores), and it is shared among multiple tenants. Meanwhile, raw data sharing among different tenants is hindered by regulation restrictions, commercial interests, and data ownership concerns [20]\u2013[23]. Last but not least, though federated learning (FL) [24], [25] offers a potential solution to these challenges, training large neural network models on resource-constrained LEO satellites remains a prohibitive task as DL models scale up [26]\u2013[29].\nTo tackle these challenges, a straightforward thought could be to combine split learning (SL) [30], [31] with semi-supervised learning (SSL) for LEO satellite networks. On the one hand, the SL framework partitions a neural network model into client-side and server-side sub-models, placing them in satellites and ground station (GS) to train together. Since the size of a client-side sub-model is much smaller than that of a server-side one, SL can substantially reduce the training load on satellites [31], [32]. After training, the GS aggregates multiple versions of the server-side and client-side sub-models for the next training round. On the other hand, SSL can be built on top of SL to reduce extensive labeling labor. SSL leverages a small number of labeled data, and large amounts of unlabeled data to train a neural network model. State-of-the-art SSL frameworks predominantly rely on pre-defined fixed pseudo-label thresholds for unlabeled data to guide model training [33]\u2013[36].\nHowever, directly combining SL with SSL faces three"}, {"title": "II. CHALLENGES AND MOTIVATION", "content": "In this section, we investigate the impacts of i) intermittent connectivity, ii) SSL cumulative biases caused by SL model aggregation, and iii) data heterogeneity among different satellites on SL performance."}, {"title": "A. Intermittent and Limited Connectivity", "content": "Conventional SL [17], [30] trains client-side and server-side sub-models separately, thus demanding continuous smashed data (i.e., activations and gradients generated by models from both sides) exchanges between the two sides. However, as LEO satellite networks offer only intermittent connectivity and low transmission rates between satellites and GS, conventional SL may fail to train models properly. We set up motivating experiments, as shown in Figure 2a, via Starlink, the real-world commercial LEO satellite communication system, in order to illustrate the challenge of developing SL on it. We use the well-known network measurement tool, Iperf [42], to test and record uplink/downlink rates. To evaluate SL performance, we adopt one of the most representative SL frameworks, SFL [43], and take the well-known neural network VGG-16 [44] as the global model. We select the spatial modulation recognition dataset GBSense [45] as training dataset. Unless otherwise specified, this experimental setup remains the same thoughout the whole Section II.\nAccording to Starlink constellation configuration information [37], the contact time between satellites and GS at different orbital altitudes can be derived. As shown in Fig. 2b, the contact time occupies only a small fraction (roughly 5%) of its orbital period. Meantime, we measure the transmission rates of Starlink; the results shown in Fig. 2c indicate mean downlink and uplink rates of roughly 100 Mbps and 12 Mbps, respectively. Considering the sizes of smashed data being about 170GB (equally shared between activation and gradient) when training VGG-16, a Starlink satellite at 547 km orbit with 4.2-minute contact time can only exchange approximately 29.5% (resp. 3.5%) of activations (resp. gradients) during the contact time with a GS. Consequently, conventional SL cannot train VGG-16 completely. Although specialized GS (with multi-million US dollars cost [46], [47]) offers Gbps- and hundreds of Mbps-level rates respectively for downlink and uplink, multiple tenants need to share these rates, resulting in very limited per-tenant rates and thus severely restricting their smashed data exchanges. Moreover, recent work [48] also utilizes distributed GSs of low-cost commodity hardware to receive satellite downlink signals. Therefore, results produced by our commercial GS are sufficiently representative.\nWe then input the traces of Starlink (e.g., contact time, uplink, and downlink rates) collected by us to our LEO satellite networks emulator (see Section IV for more details) to evaluate the performance of real SFL training in three different contact times. For the sake of comparison, we also consider an ideal case without contact time constraint, i.e., a satellite can continuously communicate with GS throughout any satellite orbital period. The results in Figure 2d demonstrate that the convergence speeds under intermittent connectivity are roughly ten-time slower than that of the ideal case. This dramatic slowdown of model convergence occurs because models can be trained only during contact time. Therefore, existing SL frameworks cannot be directly applied to LEO satellite networks, necessitating a LEO-tailored SL design."}, {"title": "B. Accumulated Biases from SSL", "content": "Although SSL can lessen the need for labeled data in LEO satellite networks, it still introduces model bias due to incomplete training data. Unfortunately, the model aggregation process native to SL tends to accumulate and thus amplify such biases, further diminishing the model's tolerance to them. To gain deeper insights into the impact of SSL on SL training performance, we build a naive semi-supervised SL using a direct combination of the seminal SSL \u03c0-model [49] and SFL. We then conduct motivating experiments with two different settings: i) different number N of satellites (i.e., N = 1,5,10) and a fixed labeling rate of 10%; ii) three different labeling rates 1%, 10%, and 100% for each satellite, and sub-models from 5 satellites are aggregated. We can clearly observe from Figure 3a that, with more satellite sub-models being aggregated, the test accuracy decreases significantly: the global model aggregating 10 sub-models has 8.5%, and 4.9% accuracy losses compared to the counterparts with 1 and 5 sub-models being aggregated. As expected, the global model with complete labeling (i.e., labeling rate = 100%), significantly outperforms others (sub-models with 1% or 10% labeling rate) in Figure 3b.\nThe results reveal two potential reasons for the infeasibility of directly combining SL and SSL. On the one hand, the sub-models from different satellites contribute their own biases to the global model during aggregation. These individual biases mostly do not offset each other but rather get accumulated within the global model, leading to a significant decrease in the test accuracy (on unseen data) as the number of sub-models (resp. labeling rate) increases, indicating a degradation in the model generalization. On the other hand, the limited labeled data of each satellite forces its model gradients closely aligned to the local optimal direction, rather than the global one. Consequently, our preliminary tests seem to suggest a risk of drastic drop in performance upon aggregating a much larger amount of sub-models (e.g., N \u2265 100), indicating the possibility of overfitting on individual client-side datasets."}, {"title": "C. Data Class and Quantity Imbalance", "content": "Data collected by LEO satellites are inherently heterogeneous, mainly in terms of class imbalance and quantity imbalance [50]. Satellites in different orbits may cover very distinct geographical regions, each having its unique characteristics (e.g., climate, geology, and vegetation types). Therefore, classes covered by individual satellite can be highly imbalance as several classes can go missing in the dataset collected by a certain satellite. Additionally, disparities in storage and sensing capabilities limit the amount of data each satellite can retain, causing data quantity imbalance across satellites. To understand how data heterogeneity impacts the performance of SL, we conduct two experiments for class and quantity imbalance. For class imbalance, we leverage Dirichlet data partitioning [51] to set three levels of class imbalance controlled by parameter \u03b1, where a larger \u03b1 indicates a more balanced class across satellites. For quantity imbalance, we configure three local dataset size ratios of 1:1:1 (balance), 1:2:4 (mild imbalance), and 1:5:10 (severe imbalance), while keeping \u03b1 = 0.2.\nFigure 4a and 4b both indicate a certain level testing accuracy degradation due to class imbalance (e.g., \u03b1 = 0.2 causes an accuracy drop of 8.1%) and quantity imbalance (an accuracy drop of 8.5% under severe imbalance). Though the negative impact of data heterogeneity may appear to be mild, it can still be significant for certain LEO-enabled applications [52], [53]. Since satellites train their sub-models on local datasets, the existence of class imbalance (e.g., skewing towards certain classes) may result in overfitting the dominant classes while forgetting the under-represented ones [38], [39], [54]. Similarly, quantity imbalance can mislead the updated global model towards data-rich satellites while neglecting the data-poor ones [40], [41]; the model performance can hence be degraded if the neglected part actually contain crucial information."}, {"title": "III. SYSTEM DESIGN", "content": "In this section, we present LEO-Split, a semi-supervised split learning framework specifically tailored for LEO satellite networks. To combat the aforementioned challenges faced by deploying deep learning for LEO satellite networks, we meticulously design three key components for LEO-Split:\nTo significantly eliminate the dependence of SL training on continuous satellite-GS connectivity, we design an auxiliary model (Section III-B1) with the same output dimension as the global model, allowing the satellite to train independently.\nTo combat the cumulative biases and the catastrophic forgetting induced by data heterogeneity, we propose a pseudo-labeling algorithm (Section III-B2); it dynamically adjusts a threshold based on data distribution across satellites throughout model training, so as to achieve class-balanced model training.\nTo effectively mitigate the performance degradation introduced by overfitting, we develop an adaptive activation interpolation (Section III-C) that interpolates the limited activations at GS to minimize the discrepancy in training data volume between satellites and GS.\nAs shown in Figure 5, the global model W is split into client-side and server-side sub-models, W and Ws, respectively, with W deployed on the i-th satellite and Ws on the GS. The training workflow of LEO-Split for one training round (defined as one satellite orbital period) follows three steps: i) each satellite utilizes the auxiliary model, and pseudo-labeling algorithm to train its sub-model, ii) satellites transmit activations to the GS as much as possible during contact time, and finally iii) GS leverages the adaptive activation interpolation to achieve data argumentation and then train its sub-model for entering the next training round. This last step also involves aggregating sub-models into the global one, but it can be conducted less often. Note that LEO-Split is built upon a common SL framework [43], where aggregation refers to a weighted average of several sub-models, similar to the behavior of FedAvg [24] but only acting on part of the global model that gets trained on the client side."}, {"title": "B. Client-side Design", "content": "We hereby set up the nomenclature for the client-side. To train on partially labeled data, the client-side of LEO-Split leverages a classical SSL framework, Mean Teacher [55], consisting of two branches: i) student model W = [W; Wa] (see Section III-B1), and ii) teacher model H = [H; Ha] (see Section III-B2), where W (resp. H) and Wa (resp. Ha) are student (resp. teacher) sub-model and auxiliary model, respectively; the auxiliary model is meant to cope with the intermittent connectivity. For each training round, the i-th satellite aims to minimize its loss function \\(L = L_A + \\lambda L_U + \\lambda_C L_C\\), where LA, LU, and LC represent the auxiliary loss, unsupervised loss, and contrastive loss, respectively, with details provided in the following sections. The hyper-parameters \\(\\lambda\\) and \\(\\lambda_c\\) are balance weights for LA and LC. During training, the student model updates its weights by minimizing L, while the teacher model's weights are updated using the exponential moving average (EMA) [55] of the student model's weights. For the next training round, the weights of the updated teacher model are assigned to the student model as the initial weights.\n1) Auxiliary Model: As explained in Section II-A, limited contact time and transmission rate cause training failure during satellite-GS non-contact time. To overcome this challenge, we empower satellites with independent client-side training capability during non-contact time. As shown in Figure 6, for conventional SL, the output layer is positioned at the final layer of the model, necessitating data flow to go through the entire model, thus incurring frequent data exchanges between clients and the server. To eliminate the SL's inherent reliance on continuous data exchange, we borrow the idea of multi-exit neural networks [56] to customize early outputs in the shallow layers for facilitating flexible partial model updates. To this end, we add two layers (one CNN and one FC) before the output layer, and we align their input dimensions with the client-side sub-model's last layer while matching their output dimensions with that of the server-side sub-model. This compact neural model, termed auxiliary model, thus allows LEO-Split to be trained without fully interacting with the GS.\nIn order to properly train the auxiliary model, we construct an auxiliary loss against the student model Wa as:\n\\[L_A = \\frac{1}{D_L}\\sum_{k=1}^{D_L} H(p(x_k; W), p(x_k; W_A)),\\]\nwhere \\(D_L = \\{x_k, y_k\\}_{k=1}^{D_L}\\) is the local labeled dataset of the i-th satellite, xk and yuk denote the k-th input data and its corresponding label, \u03a8(\u00b7) is the weak data augmentation operation (e.g., using only flip-and-shift data augmentation [33]), p(x; w) maps the relationship between input data x and its predicted class distribution given model parameter w, and H(\u00b7, \u00b7) denotes the cross-entropy. Subsequently, each satellite may independently trains its sub-model via minimizing auxiliary loss LA during non-contact time.\nRecall that satellites can transmit only a small fraction (e.g., 10%) activations to the GS during contact time, due to the limited downlink rate. To this end, LEO-Split randomly selects activations to saturate the downlink during the contact time, leaving the issue of incomplete interaction handle by the server side (see Section III-C). Intuitively speaking, such incomplete interactions could severely slow down the convergence: at least 10 rounds (given 10% downloadable activations every round) would be needed on average to make one epoch progress in training. Fortunately, because the activations are inherently sparse, our later results (see Section V) demonstrate that LEO-Split can achieve much faster convergence than the expected linear scaling. The rationale behind this promising outcome goes very close to how dropout [57] leverages a small subset of neurons in a model to achieve better performance. In other words, since deep neural models involve a huge amount of elements (e.g., neurons and activations), the chance of having only a small fraction of them being independent is high. Therefore, LEO-Split's partial yet random updates could go a much longer way than one would expect."}, {"title": "2) Pseudo-Labeling Algorithm:", "content": "According to the discussions in Sections II-B and II-C, directly combining SSL with SL (as in Section III-B1) can cause accumulated biases and catastrophic forgetting. Fortunately, our earlier discussions also reveal that the shortage of labeled data and data heterogeneity across satellites are two fundamental reasons: solving them could lead to a seamless welding of SL and SSL. Common strategies to handle these problems often resort to certain forms of label generation [33]; generating meaningful labels apparently compensates labels shortages, while class imbalance can also be alleviated if classes without (or with few) labeled data can acquire sufficient labeling.\nHowever, a single model cannot generate pseudo-labels by itself, otherwise using them for training will lead to zero yet meaningless loss. To this end, a commonly adopted approach is to involve another model to generate pseudo-labeled data for training the original model, and the teacher-student co-training framework is arguably the most successful way to coordinate these two models [55]. In particular, the additional model and the original model are treated as teacher and student models, respectively. As shown in Figure 5, unlabeled data is put into the teacher model to predict a class label with a confidence score that reflects the probability of that prediction being correct. The confidence score is then compared against a predefined threshold: the label is chosen as the pseudo-label if the score exceeds the threshold; otherwise it is dropped. These pseudo-labeled are taken by the student model to enhance its semi-supervised training.\nIn terms of defining the threshold, state-of-the-art proposals mostly stick to a static threshold that remains constant during the whole co-training process [33], [34]. This is apparently not reasonable in the face of inherent class imbalance, because it fails to adapt to discrepancy in data distribution across clients in distributed training and distribution variation during individual model training. We conduct two experiments using a conventional fixed-threshold pseudo-labeling method [33], and set the fixed thresholds as \\(\\tau(m) \\in \\{0.5, 0.7, 0.9\\}\\) for any class. Figure 7a illustrates the optimal thresholds vary under different labeling rates (i.e., 5%, 10%, and 50%), and Figure 7b shows that different thresholds show varying performance in the two heterogeneous data settings (\\(\\alpha = 0.2,0.5\\) with local dataset ratios 1:5:10 and 1:2:4, respectively). The results show that static thresholds fail to perform adequately under the shortage of labeled data and data heterogeneity, implying the incompetence of static threshold to adapt to diverse data distributions.\nFor enhancing adaptability, dynamically adjusting thresholds to suit different data distributions appears to be imperative. Consequently, we focus on fine-tuning the threshold to balance the data heterogeneity that includes both class and quantity imbalance. Intuitively, for class imbalance, we set lower thresholds for classes with more data, generating fewer pseudo-labels to counteract imbalance; for data quantity imbalance, we use higher thresholds for unlabeled datasets with more training data generating fewer pseudo-labels. Extending from these basic intuitions, we propose a pseudo-labeling algorithm to achieve the adaptive threshold \\(\\tau_{i,t}(m)\\) for the m-th class of the i-th satellite at the t-th round. Algorithm 1 consists of local dataset status collection, and adaptive threshold customization stages. In the first stage, the i-th satellite uses the teacher model to generate pseudo-labeled data, and calculates the number of labeled and pseudo-labeled data \\(\\theta_{i,t}^L(m) + \\theta_{i,t}^U(m)\\) for the m-th class (Lines 4 to 5). Afterward, all satellites transmit their \\(\\theta_{i,t}(m)\\) to GS during contact time. In the second stage, we calculate the empirical class distribution of the m-th data class, and the proportion of its amount within the total amount (Lines 8 to 9). Meanwhile, we introduce the standard deviation of data classes to adjust our thresholds, speeding up the generation of balanced pseudo-labels in cases of extreme imbalance (Line 10). The \\(\\tau_h\\) ensures that \\(\\tau_{i,t}(m)\\) does not become extremely high, avoiding the loss of pseudo-labeling capability for certain classes (Lines 11 to 12).\nAfter generating the pseudo-labels, we can utilize these pseudo-labels to train the student model in a supervised manner. Therefore, we employ unsupervised loss LU with the same form as auxiliary loss LA, to update the student model along with LA. Moreover, although the intrinsic features of low-confidence data are not obvious and thus cannot directly generate pseudo-labels, they can still facilitate model training by self-supervised training [58]. The classical contrastive learning framework is used to explore their intrinsic features [58] at the i-th satellite. Low-confidence dataset \\(D_i^U\\) are inputted into student sub-model W and teacher sub-model H to extract their corresponding activations (a.k.a., features) \\(Z_{i,k}^U\\) and \\(Z_{i,k}^{U'}\\), respectively. The principle of our contrastive learning is to make the \\(Z_{i,k}^U\\) and \\(Z_{i,k}^{U'}\\) of the same data as close as possible, and them from different data as far apart as possible. Therefore, the self-supervised contrastive loss InfoNCE [58] is in the following:\n\\[L_C = -log \\frac{exp(Z_{i,k}^{U'} \\cdot Z_{i,k}^{U} / \\phi)}{\\sum_{j=1}^{D'U} I(i\\neq k)exp(Z_{i,k}^{U'} \\cdot Z_{j,k}^{U} / \\phi)}\\]\nwhere denotes dot product operation and \u03c6 is a temperature hyperparameter [59]."}, {"title": "C. Server-side Design", "content": "The client-side of LEO-Split comprises solely server-side model Ws. For each training round, GS updates the server-side model by minimizing the server-side loss Ls, with details as evenly as possible. To achieve this, we adopt a class-cycling activation selection approach, where GS cycles through all classes and downloads the largest activation from each class in turn (Lines 3 to 7). It is noted that GS only downloads activations from more informative labeled and pseudo-labeled data.\nTo ensure consistent convergence between client-side and server-side sub-models, activation interpolation aims to increase the number of activations while retaining alignment between the data distributions of satellites and GS throughout the training process. Without loss of generality, we consider the j-th data interpolation operation at t-th training round for analysis. For notational simplicity, the training round index t is omitted. The GS first randomly selects an activation and its label \\((a_{k_1,(j)}, y_{k_1,(j)})\\) from the selected activation set \\(D_A^{(j)} = \\{(a_{k,(j)}, y_{k,(j)})\\}\\), where \\(a_{k,(j)}\\) and \\(y_{k,(j)}\\) represent the k-th selected activations and its corresponding label (Line 11). Then, GS computes the class empirical distribution \\(\\tilde{q}^{(j+1)}(m; k_1, k_2)\\) resulting from interpolating \\(a_{k_1,(j)}\\) with all remaining activations \\(a_{k_2,(j)}\\) in \\(D_A^{(j)}\\) (Lines 12 to 19). Following this, \\(a_{2,(j)}\\) with the closest interpolated empirical class distribution to client-side class distribution \\(\\tilde{q}(m)\\) in Section III-B2 (Line 20) is selected. Finally, \\(a_{k_1,(j)}\\) and \\(a_{k_2,(j)}\\), and their corresponding labels \\(y_{k_1,(j)}\\) and \\(y_{2,(j)}\\) are linearly interpolated and interpolated activation and label \\(\\bar{a}^{(j)}\\) and \\(\\bar{y}^{(j)}\\) are updated to \\(D_A^{(j+1)}\\) (Lines 21 to 22)."}, {"title": "IV. IMPLEMENTATION", "content": "In this section, we first elaborate on the implementation of LEO-Split, and we then introduce the experiment setup."}, {"title": "A. Implementing LEO-Split", "content": "We implement LEO-Split prototype for satellite-GS communication and neural network training based on a microservices architecture, as illustrated in Figure 9. The system is deployed in an H3C UniServer R5300 G3 server equipped with eight NVIDIA GeForce RTX 3090 GPUs, dual Intel Xeon Silver 4210R processors (10 cores, 2.84 GHz each), and 8x32 GB DDR4 RAM, running Ubuntu 18.04.6 LTS. The software stack includes Python 3.7 and PyTorch 1.9.1, which are used for implementing the training of space modulation recognition and remote sensing image classification applications. We leverage a Kernel-based Virtual Machine (KVM) to emulate a LEO satellite or a GS. The satellite and GS link is emulated and configured by Open vSwitch [62]. The traffic conditions are controlled by tc [63], according to the traces of Starlink collected by us. The server-side and client-side sub-models are deployed within Linux containers (LXC) on the GSs, and satellites, respectively. The training procedures of both sub-models were executed on separate RTX 3090 GPUs within the R5300 G3 server. Real-world satellite orbits (e.g., two-line element) are integrated into the system to enable real-time path computing and network routing, implementing dynamic connections between GS and satellites. This setup closely mirrors actual satellite communication links, and a 3D interface is used to visualize the connections and data exchanges between satellites and GSs via the emulation."}, {"title": "B. Experiment Setup", "content": "1) Dataset: We adopt the space modulation recognition dataset GBSense [45] and the remote sensing image dataset EuroSAT [64] to evaluate the performance of LEO-Split. The GBSense dataset consists of sampled signals with 13 modulation types such as BPSK, QPSK, and 8PSK, containing 16000 training and 4000 test samples. EuroSAT comprises 10 distinct categories of remote sensing images, including industrial, highway, and forest, with 21600 training samples and 5400 test samples. We consider both IID (independent and identically distributed) and non-IID data settings in our experiment. In the IID setting, data samples are shuffled and evenly distributed to participating satellites. In the non-IID setting, we set \u03b1 = 0.5 and divide satellites into three groups, with the local dataset size ratio of 1:2:4 across groups.\n2) Model: To implement LEO-Split, we employ the well-known VGG-16 network [44] as the global model. VGG-16 is a classical deep convolutional neural network comprised of 13 convolution layers and 3 fully connected layers. In our experiment setup, the first four layers of VGG-16 are designated as the client-side sub-model, while the remaining layers are assigned as the server-side sub-model.\n3) Benchmarks: To comprehensively evaluate the performance of LEO-Split, we compare LEO-Split against the following alternatives:\nFM-SL is the SL variant of FixMatch [33], which employs the pre-defined fixed threshold to generate high-confidence pseudo-labels on weakly augmented unlabeled data for guiding model training.\nMM-SL is the SL variant of MixMatch [34], which utilizes distribution sharpening and pre-defined fixed threshold for generating pseudo-labels and enhances the raw data richness by interpolating labeled and unlabeled data.\nMT-SL is the SL variant of Mean Teacher [55], which updates the model based on the output consistency of the teacher and student model, and the teacher model is the exponential moving average of the student model.\n\u03c0-SL is the SL version of \u03c0-model [49], which utilizes diverse data augmentation and dropout techniques to generate self-supervised signals, and employs Euclidean distance between outputs from different network branches for model training.\n4) Hyper-parameters: In our experiments, we deploy a constellation of N = 10 satellites orbiting the Earth and set labeling rates to 10% by default unless specified otherwise. The transmission rate between satellites and GS is consistent with Section II-A. The computing capabilities of the satellites and GS are set to 1 TFLOPS (peak performance of an iPhone 12 Pro [65]) and 4 \u00d7 35.6 TFLOPS (peak performance of four NVIDIA RTX 3090), respectively. For the training process, we employ an SGD optimizer with a learning rate of 0.005 for each satellite, the batch size is set to 128, and \u03c4h is 0.95."}, {"title": "V. PERFORMANCE EVALUATION", "content": "In this section, we evaluate the performance of LEO-Split from three aspects: i) comparisons with four benchmarks to demonstrate the superiority of LEO-Split; ii) investigating the impact of SL-related hyper-parameter on the model performance; iii) ablation study to show the necessity of each meticulously designed component in LEO-Split, including auxiliary model (AM), pseudo-Labeling algorithm (PA), and adaptive activation interpolation (AAI)."}, {"title": "A. Superiority of LEO-Split", "content": "This section conducts a comprehensive comparison of LEO-Split against four benchmarks in terms of test accuracy and convergence speed.\n1) Training Performance of LEO-Split: Figure 10 presents the superior performance of LEO-Split compared to four other benchmarks on GBSense and EuroSAT datasets. LEO-Split exhibits significantly faster convergence than the other benchmarks. This is primarily attributed to the design of AM, which empowers independent client-side sub-model updates and thus overcomes the training failure. In contrast, the limited contact time and transmission rate cause model training failure of four benchmarks in non-contact time, thereby substantially slowing down model convergence. The sophisticated design of PA and AAI enables LEO-Split to achieve the highest converged accuracy. To be specific, PA customizes pseudo-label thresholds for individual satellites to balance data class and quantity across satellites throughout model training, and AAI linearly interpolates limited activations to increase the number of activations while guaranteeing data distribution consistency between satellites and the GS to mitigate server-side overfitting. Conversely, MT-SL and \u03c0-SL demonstrate the worst training accuracy since they rely solely on feature-level consistency regularization and ignore the predictive probability distributions of models. Although FM-SL and MM-SL leverage the model's probability distributions for pseudo-labeling, their fixed pseudo-label thresholds fail to effectively combat data class and quantity imbalance, resulting in lower converged accuracy.\n2) The Converged Accuracy and Time of LEO-Split: Figure 11 shows the converged accuracy and time of LEO-Split and four benchmarks on GBSense and EuroSAT datasets. It is seen that LEO-Split outperforms other benchmarks in both converged accuracy and time under IID and non-IID settings. For converged accuracy, Figure 11a and Figure 11c present that LEO-Split achieves 96.3% and 91.1% in test accuracy on the GBSense and EuroSAT datasets under the IID setting, nearly 1.6% and 2.6% higher on average than other benchmarks. This improvement is even more pronounced in the non-IID setting, approximately 4.8% and 4.2% on GBSense and EuroSAT datasets. The reason for this is two-fold: one is that PA is capable of effectively balancing data classes and quantity during pseudo-labeling to mitigate catastrophic forgetting and accumulated biases, and the other is that AAI expands the number of activations while guaranteeing data class distribution consistency between the satellites and GS. For converged time, Figure 11b and Figure 11d show that LEO-Split converges at a staggering 4.4 and 4.6 (4.6 and 4.7) on average times faster than other benchmarks on the GB-Sense (EuroSAT) dataset under IID and non-IID settings. This significant improvement in convergence speed is attributed to LEO-Split's AM design, which eliminates the dependence on continuous satellite-GS connectivity, thereby preventing training failure of the satellite-GS non-contact time."}, {"title": "B. Micro-benchmarking", "content": "In this section", "Rate": "Figure 12 illustrates the impact of labeling rate on converged test accuracy on GBSense dataset under IID and non-IID settings. It is seen that LEO-Split and four benchmarks consistently exhibit worse performance with a lower labeling rate. LEO-Split experiences only a slight reduction in converged accuracy as the labeling rate drops from 10% to 2%. This is because LEO-Split leverages not only high-confidence pseudo-labeled data but also the rich information in low-confidence data that cannot generate pseudo-labels. Moreover, the design of PA in LEO-Split balances data class and quantity across satellites to enhance training performance, especially in the non-IID setting substantially. Conversely, the exclusive dependence of MT-SL and \u03c0-SL on feature-level consistency regularization significantly limits their ability to extract information from unlabeled data, leading to the severe accuracy deterioration, e.g., 5.1% (10.4%) and 5.5% (11.7%) accuracy loss under IID (non-IID) setting"}]}