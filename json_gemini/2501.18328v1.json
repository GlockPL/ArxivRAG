{"title": "CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes", "authors": ["Yicheng Wu", "Tao Song", "Zhonghua Wu", "Zongyuan Ge", "Zhaolin Chen", "Jianfei Cai"], "abstract": "MRI imputation aims to synthesize the missing modality from one or more available ones, which is highly desirable since it reduces scanning costs and delivers comprehensive MRI information to enhance clinical diagnosis. In this paper, we propose a unified model, CodeBrain, designed to adapt to various brain MRI imputation scenarios. The core design lies in casting various inter-modality transformations as a full-modality code prediction task. To this end, CodeBrain is trained in two stages: Reconstruction and Code Prediction. First, in the Reconstruction stage, we reconstruct each MRI modality, which is mapped into a shared latent space followed by a scalar quantization. Since such quantization is lossy and the code is low dimensional, another MRI modality belonging to the same subject is randomly selected to generate common features to supplement the code and boost the target reconstruction. In the second stage, we train another encoder by a customized grading loss to predict the full-modality codes from randomly masked MRI samples, supervised by the corresponding quantized codes generated from the first stage. In this way, the inter-modality transformation is achieved by mapping the instance-specific codes in a finite scalar space. We evaluated the proposed CodeBrain model on two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments demonstrate that our CodeBrain model achieves superior imputation performance compared to four existing methods, establishing a new state of the art for unified brain MRI imputation. Codes will be released.", "sections": [{"title": "1. Introduction", "content": "Magnetic resonance imaging (MRI) is widely utilized in clinical practice due to its non-invasive capacity to distinguish between various tissue types, providing essential information for medical diagnosis and understanding brain development [13, 27, 47]. It includes a range of protocols, each producing distinct modalities that highlight specific regions of interest. For instance, T1-weighted scans are commonly used to display anatomical structures, while fluid-attenuated inversion recovery (FLAIR) scanning is a popular lesion identification tool. Additionally, contrast-enhanced modalities, such as the T1-weighted images with gadolinium (T1Gd), can further enhance abnormal regions. However, collecting a complete set of MRI modalities is often impractical, since full-modality MRI screenings are both time-consuming and costly, and the use of contrast agents may lead to potential health risks [14, 45]. Furthermore, artifacts, often arising from motion, are typically unavoidable in clinical settings, resulting in subject-level registration errors [1]. These limitations motivate the development of Al models to impute missing MRI modalities to improve clinical accessibility and diagnostic completeness.\nDeep learning-based imputation models [11] have made great progress in recent years, hypothesizing that different imaging modalities for a given subject should have similar and transformable information. This assumption is widely supported in the medical domain [35]. For instance, pancreatic cancers can be segmented from non-contrast CT images even when they are undetectable to human experts [4]. Similarly, SynthStrip [19] and SynthSeg [2] achieve robust"}, {"title": "2. Related Work", "content": "2.1. MRI Imputation\nData incompleteness is a prevalent challenge in large-scale medical AI applications. Considering the limited accessibility of comprehensive medical data, deep learning models have been applied to generate various missing targets [3, 11, 16, 33]. For example, [34] proposed a 3D cycle-GAN model to generate the corresponding PET from MRI to aid in Alzheimer's disease diagnosis. An image-to-image translation model was developed in [45] to synthesize the cerebral blood volume data from standard MRI modalities, enhancing its clinical applicability.\nRecent studies have focused on unified MRI imputation. For instance, MMSYN [6] proposed to learn a modality-invariant latent representation that can be decoded into different MRI modalities. MMGAN [38] employed GAN models to improve the synthesis quality. MMT [29] utilized a Swin Transformer with modality-dependent queries to generate missing modalities. M2DN [30] applied a diffusion model with binary conditional codes to specify different imputation tasks. Furthermore, [54] explored the use of modality-shared and modality-specific modules to improve unified imputation performance, and [9] incorporated federated learning for multi-modal MRI synthesis.\nNevertheless, most existing unified approaches still operate in a pixel-to-pixel inter-modality transformation scheme. In contrast, our proposed CodeBrain framework performs inter-modality transformation at the quantized latent code level and synthesizes a missing modality with the predicted code plus the extracted common features, which achieves a more robust mapping across modalities and eliminates the need for modality-specific modules."}, {"title": "2.2. Quantization-based Image Generation", "content": "Quantization projects complex data into a low-dimensional representation, facilitating multi-modal alignment, mapping, and fusion [18]. Data-driven vector quantization (VQ) techniques have been widely applied in image-based reconstruction, transformation, and generation tasks [12]. Solutions like the straight-through estimator (STE) [43] and Gumbel-softmax [23] are commonly used to back-propagate gradients in quantization modules, enabling end-to-end training. For example, VQVAE [43] introduced the neural discrete representation, while VQGAN [12] employed a patch GAN model to capture fine details and a transformer to predict code indices for high-resolution image synthesis. To improve the image generation quality, residual designs [28], multi-scale codebooks [36, 46], dynamic token embeddings [21], code splitting [55] have been proposed to improve VQ representation capacity. Recently, beyond region-level VQ, global quantization methods [41, 52] have been explored to disentangle multi-level visual signals. Furthermore, VQ has been incorporated with the diffusion and transformer models [15, 37, 40] to produce high-fidelity generation results.\nHowever, training a robust codebook with high utilization is still a critical challenge in VQ-related studies [31]. Therefore, implicit codebooks have been introduced, where the decoder reconstructs targets from code indices only instead of relying on the nearest learnable code vectors [17]. For example, MAGVIT [51] introduced lookup-free quantization [5] by compressing data into binary indices, achieving superior video generation results. FSQ [31] proposed"}, {"title": "3. Method", "content": "Fig. 2 provides an overview of our proposed CodeBrain model. The core idea is to perform inter-modality transformations as predicting scalar-quantized codes in a latent quantized space Q while recovering any missing modality by decoding the recovered code with the extracted common feature from any available modality. Specifically, the training process consists of two stages. Stage I constructs a finite scalar space and quantizes each sample into a specific code map. Then, Stage II predicts full-modality codes from various incomplete MRI inputs, supervised by individual codes reconstructed in Stage I, to achieve a unified brain MRI imputation. During inference, given any incomplete inputs, we use the prior encoder Eprior to predict all modality codes. For any missing modality, we feed the corresponding predicted code, together with the common features extracted by the encoder Es, to the decoder Da to recover it."}, {"title": "3.1. Stage I: Reconstruction", "content": "The purpose of Stage I is to reconstruct an anchor modality Ma from a source modality Ms (Ms \u2260 Ma) via an encoder (Es) and decoder (Ea) architecture. The uniqueness is the design of the bottleneck feature representation,"}, {"title": "which consists of a common feature extracted from Ms by Es and an anchor code extracted from Ma by a posterior encoder Eposterior, aiming to capture modality-agnostic detailed information and anchor-specific coarse information respectively. Specifically, given a subject containing N distinct MRI modalities, we randomly select one modality as the source Ms and another one as the anchor Ma. Following [31], we first encode Ma into a low-dimensional latent feature Fa of size d\u00d7h\u00d7w by Eposterior, and then element-wise scalar-quantize it to L values as", "content": "Fa = Eposterior (Ma)\nZa = [L/2] * tanh(Za)\n\u017ba = Round(Za),"}, {"title": "where \u017ba is the bounded feature and Za is the scalar-quantized anchor code by the rounding operation Round. To enable end-to-end training, following STE [43], \u017ba can be represented as (Za+sg[Za-Za]), where sg[.] is the stop-gradient operation, essentially copying the gradients before and after Round. Note that, Za has d dimensions and the scalar quantization is performed for each dimension respectively. Along each dimension, there are L possible integer values, and the total possible number of quantized vectors (i.e., code elements) is Ld. Additionally, each element in \u017ba indicates a quantized representation for an image patch in Ma. Such a simple scalar quantization introduced in [31] does not require learning an explicit codebook with no auxiliary losses [50] needed to regularize the training.", "content": "Since the quantization process is lossy and the code is low-dimensional, it cannot restore all details of anchor Ma. To improve the synthesis quality, we exploit a source encoder Es to generate common features Fe from Ms. Note that Fe is modality-agnostic, which is achieved by extracting Fe from any randomly selected modality and enforcing it to contribute to any other modality. Specifically, we have"}, {"title": "Fc = Es(Ms),\nMa = Da(Concat[\u0179a, Fc])", "content": "where Ma denotes the corresponding reconstructed results from Da with the concatenated input of both \u017ba and Fe. Finally, we train the Stage I model with the following overall reconstruction loss:"}, {"title": "Lrec = Lpsnr(Ma, Ma) + 1 \u00d7 Lgan(Ma, Ma)", "content": "where Lpsnr is a differentiable loss of Peak Signal-to-Noise Ratio (PSNR) [7], Lgan is a PatchGAN loss [22] to capture the image details, and A is a hyper-parameter to balance the two terms during training.\nOur Stage I designs have several advantages: 1) Different modalities are projected to a shared latent space, reducing the inter-modality gap; 2) \u017ba is instance-specific and"}, {"title": "denotes inter-modality differences, while F aims to capture the modality-shared information, bridging both Stage I and Stage II; 3) The model training only relies on two losses, avoiding the complex tuning of many optimization targets.", "content": "3.2. Stage II: Code Prediction\nOnce Stage I learns instance-specific codes in the quantized space, the purpose of Stage II is to establish inter-modality transformations by directly predicting the codes of missing modalities. For Stage II training, we first concatenate different modalities along the channel dimension together to get the full-modality data Mfull and then perform random masking by a randomly generated binary list BL to simulate various imputation scenarios. Specifically, we randomly mask k channels out of N channels/modalities of Mfull, with 0 < k < N:"}, {"title": "Mmask = {Mfull if BL(i) = 1 ,0 if BL(i) = 0", "content": "where i is the channel index. Then, a prior encoder Eprior is trained to generate full-modality codes from the masked input Mmask.\nThe training of Stage II is supervised by \u0179full, a channel-wise concatenation of quantized codes of N MRI modalities from Stage I:"}, {"title": "\u017dfull = Eprior(Mmask)\nLpred = D(\u017dfull, \u017dfull)", "content": "where D measures the distance between \u017dfull and \u017dfull. Here the target is to predict full-modality codes so that the model is trained by the imputation of missing modalities and the reconstruction of available ones at the same time, similar to [30].\nSince each code dimension can only be one of the L integers, we can directly set D as a L-class cross-entropy loss to train the Stage II model. However, this design ignores the clustering characteristic of the quantized space and the distances among the pre-defined L classes. Therefore, we design a grading loss for D, which is defined as an ordinal binary cross-entropy loss:"}, {"title": "Lpred = Lbce(Bfull, Bfull)", "content": "s.t., Bfull = T(\u017dfull, L), Bfull = T(\u017dfull, L)\nwhere T denotes a transformation from an integer value to an array of binary values of length (L \u2013 1), indicating the ordinal relationships for the original class labels. For example, given a class label y in the range of {0, ..., L - 1}, we create By as:"}, {"title": "B^{y}_j = {1 if j < y ,0 else", "content": "4. Experiments and Results\n4.1. Datasets\nWe evaluated the proposed CodeBrain model on the IXI\u00b9 and BraTS 20232 [32] datasets. IXI contains non-skull-stripped MRI samples from 577 healthy subjects, which were scanned from three London hospitals using three different MRI machines. Each subject contains T1, T2, and Proton Density-weighted (PD) modalities. Since these modalities are not spatially-registered, we then use ANTsPy\u00b3 to register T1 and PD to T2 rigidly. Then, following [29, 30], 90 transverse brain slices are extracted from the middle of each 3D volume. We then crop these slices into a fixed size of 256 \u00d7 256 and randomly select 500 subjects for training, 37 for validation, and the remaining 40 for testing.\nBraTS 2023 comprises multi-site multi-parametric MRI (mpMRI) scans of brain tumor patients, including T1, T2, FLAIR, and T1Gd modalities. Each sample is skull-stripped and rigid-registered. As [29, 30], we extract the middle 80 transverse slices in our experiments, which are further cropped to a fixed size of 240 \u00d7 240. The training, validation, and testing sets include 500, 40, and 40 randomly selected subjects, respectively."}, {"title": "4.2. Implementation Details", "content": "We first normalized these MRI slices into a fixed intensity range of 0-1 by a min-max normalization, making the voxel intensities across different subjects and different modalities comparable, same as [54]. For both datasets, we set X to 1, the batch size to 48, d to 7, and L as 5 as suggested in [31]. We selected the NAFNet [7] as the backbone and used the Adam optimizer with an initial learning rate (LR) of 1e-3. A cosine scheduler with a minimum LR 1e-5 was used to tune the LR to stabilize the model training. All experiments were conducted in an identical environment for fair comparisons (Hardware: 8\u00d7 NVIDIA GeForce 4090 GPUs; Software: PyTorch: 2.1.2, CUDA: 11.8, Random Seed: 1234). We trained the CodeBrain model for 150 epochs in each stage. The total computational complexity of our Code-"}, {"title": "brain Model is 94.85 GMACs with 96.44 M parameters. The total training time is around 36 and 42 hours on the IXI and BraTS 2023 datasets, respectively.", "content": "We use three metrics to evaluate the performance: PSNR, Structural Similarity Index (SSIM), and Mean Absolute Error (MAE). Since the MRI data has a large range of voxel intensities, we use the float type rather than the 8-bit one to calculate these metrics. The data range of PSNR is fixed to 0-1. We compare our CodeBrain with two public unified models: MMSYN [6] and MMGAN [38], and two recent methods: transformer-based MMT [29] and diffusion-based M2DN [30], where the latter two were implemented according to their works. We will release our experimental settings to establish a public benchmark for unified brain MRI imputation.\n4.3. Imputed Results in Different Scenarios\nTable 1 gives quantitative synthesis results in different settings. Overall, PD is easier to synthesize from other modalities and T2 contains more hard-to-restore details, indicating the importance of T2 examinations in clinical practice. Furthermore, different one-to-one imputation results (i.e., the top of Table 1) show that T1 can be better transformed from PD than T2, while T2 and PD are highly related, as stated in [27, 29]. Fig. 3 further shows brain MRI imputation results in different scenarios on the IXI dataset. We can see that: 1) The Stage I of our CodeBrain model captures most target details for reconstruction (i.e., the 2nd column in Fig. 3); 2) Our Stage II model generates accurate and plausible anatomic structures for different missing modalities in different scenarios for brain MRI."}, {"title": "4.4. Comparisons", "content": "Table 2 gives the comparison results of our model and four existing models [6, 29, 30, 38] for unified brain MRI imputation on the IXI (Top) and BraTS 2023 (Bottom) datasets. It reveals that the CodeBrain model outperforms other methods with a significant performance gain. For example, on the IXI dataset, our CodeBrain improves the PSNR value by 0.47 dB than the second-best work [29]. Besides, for various One-to-One and Many-to-One scenarios (i.e., \"O\u2192O\" and \"M\u2192O\" in Table 2), CodeBrain achieves superior synthesis performance on both datasets. Furthermore, in \u201cM\u2192O\" scenarios, we can set Ms as the most relevant modality to the target, for which the mean PSNR performance can be further improved by 0.25 dB on the IXI dataset, see top (.) in the last column of Table 2. Note that, the relevance between different modalities can be captured in our Stage I via a data-driven scheme, providing a new perspective to analyze the similarity of different MRI modalities. Table 2 (Bottom) further indicates our model outperforms other methods on BraTS 2023. Here, our SSIM performance is inferior to [29, 30] since CodeBrain does"}, {"title": "not involve any structure-related supervision for training and BraTS samples contain inconsistent region information (e.g., with or without clear tumor boundaries). Future work will explore structure embeddings [39] to ensure invariant anatomical representation among different modalities.", "content": "Fig. 4 provides visual comparison results on the IXI dataset. It can be seen that our model has fewer synthesis errors than other methods (e.g., the brain tissues in Fig. 4), and either the reconstruction or the imputation stage establishes a high-quality target synthesis, which is essential to reduce the scanning time and improve the feasibility of full-"}, {"title": "modality MRI diagnosis.", "content": "4.5. Ablation Studies\nTable 3 gives an ablation study of our CodeBrain on the IXI dataset. In Stage I, using FSQ [31] can produce higher reconstruction performance than the original VQ [12] with the same dimension of d = 5. Increasing the code dimension to d = 7 (FSQ+) improves the performance and adding common features Fc (i.e., w/ Fc) further boosts the performance by an additional 1.47 dB in PSNR. Furthermore, compared with using the traditional L-class cross-entropy loss (i.e.,"}, {"title": "5. Discussions", "content": "5.1. Quantized Codes\nTo visualize the distributions of quantized codes in the latent space, we further show two 32 \u00d7 32 code maps of our CodeBrain (#3 and #5) in Fig. 5 on the IXI dataset. We can see that, without any training regularization, the code distributions in both stages of our CodeBrain model exhibit clustering characteristics. These clustered codes reflect coarse anatomical structures of the brain, potentially bridging image synthesis and perception tasks (e.g., brain segmentation [53]). Besides, the Stage II model can accurately predict most of the corresponding codes and synthesize high-quality brain MRI modalities on the IXI dataset.\n5.2. Selection of Code Dimensions d\nThe code dimension d controls the complexity of the space Q in our CodeBrain model. Fig. 6 gives the results with different values of d on the IXI dataset. We can see that a growing d can improve the reconstruction performance (e.g., a 2.16 dB PSNR gain from d = 4 to d = 7) while the imputation performance is relatively stable (e.g., an 0.1 dB PSNR gain from d = 4 to d = 7). Therefore, we finally select d = 7 to construct the quantized space on IXI."}, {"title": "5.3. Effect of Common Features Fe", "content": "We further investigate the reconstruction effect of extracting common features Fe from different modalities in our Code-Brain Stage I, with the results shown in Table 4. We can see that each modality can contribute to other modalities' reconstruction and can be generated from other modalities, although with different relevance effects [27]."}, {"title": "We have presented CodeBrain, a unified model for brain MRI imputation. The key idea is to cast the inter-modality transformation task into two stages: reconstruction and", "content": "code prediction. In the first stage, each MRI modality is compressed to a quantized code, which is augmented by common features from other modalities and then decoded to itself. In the second stage, CodeBrain predicts full-modality codes from an incomplete MRI sample. Extensive experiments on two public datasets demonstrated the effectiveness of our CodeBrain, achieving superior performance of unified brain MRI imputation.\nLimitation and Future Work. Fig. 4 shows that despite achieving the SOTA imputation performance, our Code-Brain model still cannot fully recover the high-frequency and texture details of the original MRI scans. Further improvements could come from new estimation metrics and training losses such as those under-explored in related MRI"}, {"title": "synthesis tasks [24, 42]. On the other hand, considering different modalities share basic priors, another direction is to explore the disentanglement [20] between the modality-shared and modality-specific information [44] from the MRI physical theory.", "content": "Societal Impacts. Our proposed CodeBrain model was trained and evaluated on two publicly available but limited datasets, where the dataset bias [49] may result in unconvincing predictions in clinical applications."}]}