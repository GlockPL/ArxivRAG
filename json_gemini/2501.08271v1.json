{"title": "Comparative Analysis of Efficient Adapter-Based Fine-Tuning\nof State-of-the-Art Transformer Models", "authors": ["Saad M. Siddiqui", "Mohammad Ali Sheikh", "Muhammad Aleem", "Kajol Ramesh Singh"], "abstract": "In this work, we investigate the efficacy of various\nadapter architectures on supervised binary classification\ntasks from the SuperGLUE benchmark as well as a\nsupervised multi-class news category classification task\nfrom Kaggle. Specifically, we compare classification\nperformance and time complexity of three transformer\nmodels, namely DistilBERT, ELECTRA, and BART, using\nconventional fine-tuning as well as nine state-of-the-art\n(SoTA) adapter architectures. Our analysis reveals\nperformance differences across adapter architectures,\nhighlighting their ability to achieve comparable or better\nperformance relative to fine-tuning at a fraction of the\ntraining time. Similar results are observed on the new\nclassification task, further supporting our findings and\ndemonstrating adapters as efficient and flexible\nalternatives to fine-tuning. This study provides valuable\ninsights and guidelines for selecting and implementing\nadapters in diverse natural language processing (NLP)\napplications.", "sections": [{"title": "1. Introduction", "content": ""}, {"title": "1.1. Problem and Existing Approaches", "content": "Transformer-based [1] models represent the SoTA in\nterms of NLP applications, having consistently\ndemonstrated unparalleled performance across a wide range\nof tasks. These models are characterized by deep neural\nnetwork architectures with large number of learnable\nparameters, often approaching the order of hundreds of\nmillions, which are optimized through a combination of\nsupervised, self-supervised, and semi-supervised pre-\ntraining on vast corpora.\nNLP researchers and practitioners are interested in\nleveraging the capabilities of transformer models for\ndomain-specific tasks, which invariably require updating or\ntuning learnable parameters. A popular method for doing so\nis fine-tuning [2], which involves updating either all\nlearnable parameters or a subset of learnable parameters\nwith methods such as gradient descent using loss from a\nsmaller, task-specific labeled dataset. Another popular\nmethod is domain-adaptation [3], which involves\naligning model performance with a target domain\nthrough adversarial training, domain-invariant\nrepresentations, and other techniques which involve a\nmix of source and target domain data.\nDespite their efficacy, both methods are often expensive\nin terms of time, often taking prohibitively long owing to\nthe large number of learnable parameters that need to be\nupdated. There is also an additional risk of overfitting and\nissues with gradient flow, as the absence of appropriate\nregularization techniques and architectural constructs for\ngradient flow augmentation can cause problems with\ngeneralization when tuning a large number of parameters\nusing small, labeled corpora."}, {"title": "1.2. Adapters", "content": "Adapters [4] are lightweight task-specific modules,\ntypically consisting of projection layers or small neural\nnetworks, that can be inserted between layers in transformer\nmodels. During training, only adapter parameters are\nupdated while the rest of the model parameters remain\nunchanged. This makes adapters a parameter-efficient\nalternative to finetuning and domain adaptation: adapters\nallow the retention of generalizable knowledge learned\nfrom large corpora as encoded by original model weights\nwhile still allowing the parameters of the adapter to learn\ntask-specific information. Contrary to fine-tuning a\nclassification head in conventional transfer learning, the\ninsertion of adapter modules is not limited to later layers in\nthe network.\nSuccessful integration of adapters in transformer-based\nmodels can significantly reduce computational resources\nand time needed for modifying models for a specific domain\nor task, even with limited labeled data [5]. This parameter\nefficiency means that adapting models to new tasks will\nbecome faster, cheaper, and more accessible, particularly\nfor researchers and practitioners with limited resources. It\nwill enable more widespread use of powerful language\nmodels in various applications, fostering innovation and\nadvancing the field of NLP by making state-of-the-art tools\navailable to a broader audience."}, {"title": "1.3. Objectives", "content": "This project aims to empirically validate and quantify the\nclassification and time complexity benefits of adapters as\nparameter-efficient alternatives to fine-tuning transformer\nmodels for supervised classification. Additionally, the project\nalso aims to identify which adapter designs perform best across\na range of transformer models and tasks to understand their\nadvantages over traditional methods."}, {"title": "1.4. Datasets", "content": "We have used two main datasets in this project.\n1. SuperGLUE [6]: A collection of diverse and challenging\nNLP tasks designed to evaluate the performance of\nlanguage models on understanding and reasoning. It\nconsists of tasks such as question answering, natural\nlanguage inference, and co-reference resolution,\nproviding a comprehensive assessment of model\ncapabilities.\n2. News Category Classification [7]: A Kaggle dataset\nconsisting of 210,294 records of news articles from the\nHuffington Post collected between 2012 and 2022 with\npredictors such as headlines, authors, and publication date\nmapped to 42 highly imbalanced categories such as\nPolitics, Wellness, Entertainment, to name a few.\nIn the SuperGLUE benchmark, we specifically focus on the\nfollowing binary classification tasks, partly due to their\nsimplicity and partly due to time and resource constraints."}, {"title": "2.\nApproach", "content": "At a high level, our approach to the problem involved\nrunning experiments to benchmark model performance\nin terms of validation set accuracy as well as training\ntime with and without one each of nine adapters across\nthree SuperGLUE tasks as well as the News\nClassification task using base variants of three SoTA\ntransformer models, namely DistilBERT [9],\nELECTRA [10], and BART [11]."}, {"title": "2.1.\nAddressing the Problem", "content": "To address our problem, we used HuggingFace's\nTransformers [12] library along with the Adapters library\n[13] from Adapter Hub. The Adapters repository provided\ncode for running simple sequential bottleneck adapters on\nGLUE tasks, but we needed to extend this code to fit our\nrequirements. Specifically, we added support for\nSuperGLUE, incorporating tasks from Table 1 and extended\nthe repository to include various adapter architectures such\nas sequential bottleneck, stacked sequential bottleneck Mix\nand Match, iA3, LoRA, Prefix Tuning, Compacter++,\nPrompt Tuning, and UniPELT [14]. Additionally, we\nmodified the code to return epoch-level estimates of\ntraining and validation loss as well as task-specific model\nevaluation metrics such as accuracy, F1 score, and\nMatthew's Correlation. Lastly, we added support to handle\ntraditional fine-tuning on the same tasks for comparison.\nOur approach therefore resulted in a single pipeline that\ncould be used to benchmark model performance in terms of\nevaluation metrics and time complexity with and without\nadapters across all three transformer models of interest."}, {"title": "2.2.\nNovelty", "content": "Firstly, our approach is novel in its comprehensive\nevaluation of a wide range of adapter architectures on state-\nof-the-art models and complex benchmarks. By including a\npractical news classification task, we aimed to provide real-\nworld insights into the effectiveness of adapters,\nhighlighting their potential for efficient and flexible\nadaptation of pre-trained language models to new tasks.\nAdditionally, by considering epoch-level learning curves\nin conjunction with end-of-training point estimates of\nmodel performance with and without adapters, the\ncomprehensive nature of our analysis provides clear\ninsights into the efficiency and effectiveness of adapters,\ndemonstrating their potential as a viable alternative to full\nmodel fine-tuning in NLP tasks. By evaluating a wide range\nof adapter architectures, we contributed new knowledge to\nthe field, offering practical guidelines for selecting and\nimplementing adapters in various NLP applications."}, {"title": "2.3.\nHypotheses", "content": "We hypothesize that adapters will offer a more efficient\nand flexible way to modify all three transformer models for"}, {"title": "3.\nExperiments", "content": ""}, {"title": "3.1. Setup and Hyperparameters", "content": "We began our investigation with SuperGLUE\nbinary classification tasks and set up a grid of experiments\nwhich were delegated across team members. Specifically,\nwe added each of 9 adapter configurations to each of 3\ntransformer models and trained the resulting architecture\nfor 10 epochs, using HuggingFace and AdapterHub defaults\nfor the hyperparameter including a learning rate of 5e-5,\nbatch size of 8, and an Adam optimizer with \u03b2\u2081 = 0.9, \u03b22 =\n0.999, and \u025b = 1e-08. This allowed us to isolate differences\nin performance exclusively to the presence or absence of\nadapter configurations while also ensuring each\narchitecture had a good probability of convergence\nregardless of architecture. Custom code was written to cater\nto bespoke data preprocessing, tokenization, and evaluation\nrequirements for each SuperGLUE task as well as the\nintegration of a range of adapters across all 3 models.\nWe anticipated some tasks would take very long to\ntrain due to the large size of the datasets and models, as well\nas the former's inherent preprocessing complexity.\nConsequently, we used an NVIDIA L4 GPU on Google\nCoLab and NVIDIA A 100 GPU on PACE-ICE to accelerate\ntraining. Our initial experiments with a simple sequential\nbottleneck adapter on the BoolQ task yielded significantly\nsubpar results, which we anticipated due to the relative\nsimplicity of the adapter architecture. This incentivized us\nto expand our initial search space of adapters to include\nmore advanced architectures such as LoRA, iA3 and Mix-\nand-Match."}, {"title": "3.2.\nModel Selection", "content": "We used three base models: DistilBERT, BART, and Electra\nbase. These models were chosen for their relatively lightweight\narchitecture and the fact that they have not been extensively\ntested with adapters in existing literature. DistilBERT, with\napproximately 66 million parameters, is a distilled version of\nBERT, making it more efficient while retaining a substantial\nportion of BERT's performance capabilities. BART, with\naround 140 million parameters, is a transformer model that\ncombines bidirectional and autoregressive transformers,\nsuitable for a wide range of NLP tasks including classification.\nELECTRA, consisting of about 110 million parameters, is\nknown for its efficiency and performance in pre-training tasks.\nThese models are well-suited for NLP-based binary\nclassification tasks due to their architecture, which\neffectively captures contextual information necessary for\nmaking accurate predictions."}, {"title": "3.3.\nAdapter Methods", "content": "We experimented with various adapter methods to\nunderstand how they modify the architecture of each model\nand their impact on performance. The adapters used were\nseq bn (Sequential Bottleneck), prefix tuning, MAM (Mix\nand Match) adapter, compacter plusplus (Compacter++),\nUniPELT (Unified Prompt and Embedding Learning\nTechniques), LoRA (LowRank Adaptation), stacked seq bn\n(stacked sequential bottleneck with tanh and ReLU\nactivations), IA3 (Improved Adapter Architecture with\nActivation and Attention) and prompt tuning. These\nadapters were selected based on their innovative approaches\nto modifying the model architecture, aiming to enhance\nperformance with minimal additional parameters.\nWe deliberately experimented with a wide range of\nadapters because of the diversity of architectural\nmodifications and preference biases they exhibited. For\ninstance, adapters like Sequence Bottleneck and its stacked\nvariant introduce bottleneck layers that reduce\ncomputational overhead while capturing essential\ninformation. As such, these adapters were expected to be\nmore generic in their preference biases and utility across\nSuperGLUE tasks. In contrast, prefix tuning and prompt\ntuning focus on optimizing input prompts rather than model\nweights, and were expected not to perform well as generic,\nout-of-the-box adapters for classification tasks. MAM and\nCompacter++ utilize advanced parameter sharing and low-\nrank decomposition techniques to improve efficiency.\nUniPELT integrates prompt tuning with embedding\nlearning, while LoRA and iA3 introduce low-rank\napproximations and improved activation functions,\nrespectively, to boost performance."}, {"title": "3.4.\nMetrics", "content": "To compare the performance of fine-tuning and adapter\ntraining, we used three primary metrics: time to train,\nevaluation dataset accuracy and training and validation loss\nby epoch. These metrics were chosen to provide a clear\ncomparison of the computational efficiency and predictive\nperformance of each method. Time to train is crucial for\nunderstanding the resource requirements of each approach,\nwhile evaluation accuracy directly reflects the effectiveness\nof the models and adapters in making accurate predictions.\nAdditionally, training and validation loss by epoch helps\nmonitor the learning process and convergence behavior,\nproviding insights into the stability and efficiency of each\ntraining method. Other metrics such as floating point\noperations were also observed and reported on, but the\nprimary focus of our analysis revolves around the three\nmetrics mentioned previously."}, {"title": "3.5.\nNews Classification", "content": "All three models were first fine-tuned on the News\nClassification dataset. Owing to time and resource\nconstraints, we chose to focus on only one of the three\nmodels, specifically ELECTRA, for adapter tuning\nexperimentation with the Mix-and-Match and Sequential\nBottleneck (ReLU) adapters, partly because these adapters\nhad demonstrated good performance on concurrent\nSuperGLUE experiments and partly because their\narchitecture seemed to be intuitively compatible with the\nrequirements of the News Classification problem. The data\nwas converted to the Hugging Face Dataset format, and a\ntokenization function was applied to the text headlines,\ntruncating and padding them to a maximum length of 128\ntokens. This prepared the data for efficient training with\ntransformer models. We also implemented custom\npreprocessing and metrics specific to the news\nclassification task and analyzed the accuracy, F1-score, and\nloss curves for each configuration, in addition to measuring\ntraining time."}, {"title": "4.\nResults and Discussion", "content": ""}, {"title": "4.1.\nSuperGLUE Performance \u2013 Results", "content": ""}, {"title": "4.2.\nSuperGLUE Performance \u2013 Discussion", "content": "Sequential Bottleneck Adapters\nResults from validate many of our\ninitial hypotheses. Firstly, they demonstrate that adapters\nconsistently lead to performance comparable to or, in some\ncases, even better than traditional fine-tuning in terms of\nevaluation accuracy while requiring significantly less\ntraining time. This trend holds across all three models\ninvestigated in the experiments. In particular, the Sequential\nBottleneck adapter and its variants tend to achieve >= 90%\nof the validation accuracy of full fine-tuning with\nELECTRA despite requiring, <= 70% of the training time.\nThe improvement in training time is true for all adapters,\nalthough different adapters achieve varying degrees of\nsuccess in terms of validation set accuracy. This is\nattributable to their architectural biases and how they lend\nthemselves to binary classification. For instance, the\nsuperior performance of sequential bottleneck adapters can\nbe attributed to their two sequential bottleneck layers,\nallowing the model to capture more intricate patterns."}, {"title": "Prompt Tuning and Prefix Tuning", "content": "In contrast, Prompt tuning and Prefix tuning adapters\nperform poorly across tasks due to their task-specific nature,\nconsistently leading to a very low accuracy relative to fine-\ntuning despite little evidence of overfitting. These adapters are\nless effective for classification tasks because their design suits\ntasks needing rich contextual understanding and generation\nrather than straightforward classification. Prefix Tuning\nprepends trainable prefix tokens to the input, modifying keys\nand values in the multi-head attention mechanism. While this\nenhances sequence-to-sequence tasks like text generation, it\ndoes not benefit classification tasks, which rely on fixed input\nrepresentation and feed-forward layers to capture\ndiscriminative features. Likewise, Prompt Tuning adds a fixed\nprompt to the input, which is effective for generative tasks but\nless so for classification, which needs a clear decision boundary\nand efficient feature transformation."}, {"title": "Mix-and-Match", "content": "Conversely, the Mix and Match (MAM) adapter performs\nwell because it balances simplicity and complexity by\ncombining Prefix Tuning and parallel bottleneck adapters. This\narchitecture allows it to dynamically adapt to different task\nrequirements, making it suitable for classification. Parallel\nBottleneck Adapters are lightweight modules within the\ntransformer's feed-forward blocks, efficiently handling\ncomplex transformations through a combination of down-\nprojection, non-linearities, and up-projection layers, helping\nthem extract relevant features from the input data."}, {"title": "Overfitting with Fine-Tuning", "content": "Training and validation set loss for most adapters generally\ndoes not show a large divergence, as is evident from Figures 1\nand 3. However, fine-tuning almost always results in training\nloss converging to 0 while validation set loss diverges. This\nvalidates our earlier hypotheses about fine-tuning of large\ntransformer models having a higher likelihood of overfitting\ncompared to adapters. Interestingly, validation set accuracy\nwith fine-tuning remains at or above adapter performance\ndespite poor validation loss. This suggests fine-tuning results\nin models that become increasingly confident about wrong\npredictions in probabilistic inference space, but do not\nnecessarily make more or fewer wrong predictions [15]."}, {"title": "Floating Point Operations", "content": "The addition of adapters to a model does not result in a\nsubstantial increase in its median floating-point operations.\nHowever, as shown in Figure 1, the distribution of\noperations with adapters is generally skewed towards a\nhigher number of operations. This intuitively makes sense\nas the inclusion of submodules with varying degrees of\ncomplexity within existing architectures should ideally\nincrease the number of operations performed during\nforward passes. This suggests that while adapters do make\nsubstantial improvements in training time, they do so at a\npotential cost of incurring more computations."}, {"title": "4.3.\nNews Classification Dataset - Results and\nDiscussion", "content": "Based\non promising results from the\nSuperGLUE experiments, we selected the ELECTRA\nmodel and the MAM and Sequential Bottleneck (ReLU)\nadapters for experiments with the News Classification\ndataset, as discussed in 3.5.  shows the\nperformance of this set of experiments, with learning\ncurves shown in Figure 4 - 6.\nThe fine-tuned model (without adapters) achieved\nthe highest evaluation accuracy (0.637) and F1 score\n(0.625), indicating that full fine-tuning can still be the\nmost effective method for achieving high performance\non the news classification task and also had the lowest\nevaluation loss (1.325), which aligns with its higher\naccuracy and F1 score. The MAM adapter showed a\nslight drop in performance with an evaluation accuracy\nof 0.614 and F1 score of 0.596. It also had slightly\nhigher validation set loss. Sequential bottleneck (ReLU)\nhad the lowest accuracy (0.567), F1 score (0.532)\namong the three configurations and higher validation set\nloss (1.575). Training time was significantly reduced\nwith adapters. The fine-tuned model took 2 hours and 33\nminutes, whereas the MAM adapter reduced the time to"}, {"title": "5.\nConclusions", "content": "The results from our experiments demonstrate that adapter-\nbased training provides an effective and efficient alternative to\ntraditional fine-tuning for NLP classification tasks. Adapters\ngenerally achieve comparable performance to full fine-tuning\nwhile significantly reducing training time in both binary and\nmulti-class classification contexts.\nSpecific adapters, such as Mix-and-Match (MAM), exhibit\nrobust performance on SuperGLUE, balancing modeling\ncomplexity parameter efficiency. Sequential bottleneck\nadapters with tanh and ReLU activations show a tendency\nperform well on more complex SuperGLUE tasks despite some\nevidence of minor overfitting, suggesting that their stacked\narchitecture is better suited for handling more intricate datasets.\nConversely, adapters like LoRA and Sequential Bottleneck\nadapters without ReLU and tanh activations tend to underfit on\ncomplex tasks, indicating a need for further refinement in their\ndesign for such applications. Furthermore, adapters like Prompt\nTuning and Prefix Tuning, while powerful for generative tasks,\ndo not align well with the requirements of classification due to\ntheir architectural focus on contextual understanding and\nsequence generation rather than feature extraction and\ndecision-making. As such, we successfully verified our\nhypothesis of adapters yielding variable performance\ndepending on their architectures. Most importantly, however,\nthe benefit of adapters seems to be consistent across multiple\ntransformer-based models, thereby establishing them as a\ngenerally viable alternative to fine-tuning across multiple\ncontexts. However, due to time and resource constraints, we\nwere unable to explore if the same trends held on other tasks\nwithin the SuperGLUE benchmark, which is an area for\nfuture work."}, {"title": "6.\nFuture Work", "content": "Future experiments could expand on these findings by\nexploring larger transformer models with more parameters,\nsuch as BERT-large or GPT-3, to evaluate the scalability and\neffectiveness of adapters. Additional hyperparameter tuning\ncould provide deeper insights into optimizing adapter\nperformance across various tasks. However, scaling for\nmodels with millions of parameters presents significant\ncomputational challenges. Solutions like NVIDIA\nTensorRT could be employed to optimize inference\nperformance and reduce the computational load. Moreover,\nthe difficulties associated with smaller datasets such as CB\nunderscore the need for high-quality, well-annotated\ndatasets to achieve reliable performance evaluations. Future\nwork could involve curating or identifying more robust\ndatasets with higher evidence and clearer task definitions to\nmitigate these challenges. Additionally, leveraging\nadvanced data augmentation techniques and synthetic data\ngeneration could help improve the robustness and\ngeneralizability of models trained with adapters.\nIn conclusion, while adapter-based training offers\nsignificant advantages in terms of efficiency and\nperformance, future research should focus on scaling these\nmethods to larger models, optimizing hyperparameters, and\naddressing dataset-related challenges to fully realize their\npotential in NLP classification tasks. These efforts will not\nonly enhance model performance but also contribute to the\ndevelopment of more efficient and scalable solutions for a\nwide range of NLP applications."}, {"title": "Appendix I", "content": ""}, {"title": "A. Project Code", "content": "All code for our project, including experimental notebooks,\nintermediate artifacts, and data, are stored in our project's\nGATech GitHub repository, which can be found at:\nhttps://github.gatech.edu/ssiddiqui60/CS7643-final-project\nThe SuperGLUE experiment code used a combination of\nthe HuggingFace Transformers and Adapters libraries with\nexample code from HuggingFace and Adapter-Hub\ndocumentation. We specifically modified the code to\n1. Support SuperGLUE instead of the default GLUE\n2. Add a mapping between SuperGLUE tasks and\ntheir corresponding DatasetDict I/Os\n3. Made the script generalizable enough to run for\nany binary classification task with any supported\ntransformer model on the HuggingFace Hub and\nany Adapter supported through Adapters Library\n4. Log training and validation set loss as well as task-\nspecific metrics such as accuracy, F1 score, and\nMatthew's correlation at an epoch level as well.\n5. Generate learning curves of validation and training\nloss by epoch\n6. Export epoch level and end-of-training point\nestimates of time complexity and model\nevaluation metrics to a centralized repository for\nlogging\n7. Generating all visualizations used in the report as\nwell as supplementary visualizations by task and\nmodel for extracting model insights.\nFor the News Classification task, we used HuggingFace and\nAdapter Hub documentation to write a custom script from\nscratch to load data, tokenize it for each model, and run\nsimilar experiments as for SuperGLUE. Code for plotting\nand visualizing loss curves was developed as well along\nwith code for logging epoch-level metrics."}, {"title": "B. Work Division", "content": "Delegation of tasks among team members was crucial for\nsuccessful execution of the project. Muhammad Ali Sheikh\ncollected results for the SuperGLUE RTE and CB tasks\nalong with an auxiliary SuperGLUE RECORD task that we\nchose not to include in the main report due to limited space.\nAli also worked Muhammad Aleem initially found the base\ncode for GLUE tasks using a single adapter and extended it\nto include SuperGLUE and other adapters. Saad M.\nSiddiqui modified the base code for data collection,\nvisualization, and aggregation which informed most of the\nresults and conclusions presented in the report. In addition\nto collecting data for the BoolQ task, Saad also contributed\nto the report. Kajol R Singh focused on the news\nclassification task, reusing the existing base code and\nmaking necessary modifications to adapt it for the news\nclassification dataset, and conducting experiments with\nall three models, including their adapter configurations."}, {"title": "Appendix II \u2013 Additional Results", "content": ""}, {"title": "A. Average Validation Set Accuracy by Adapter \u2013 GLUE vs SuperGLUE", "content": "As mentioned in Section 1.4, we also explored model performance with and without adapters on binary classification\ntasks from the GLUE benchmark. We found the SuperGLUE tasks to be more discriminative in terms of model\nperformance and training time, which is why we chose to focus our analyses on the SuperGLUE tasks. Our results from\nthe GLUE experiments are included for completeness."}]}