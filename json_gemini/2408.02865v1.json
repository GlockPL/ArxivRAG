{"title": "VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge", "authors": ["Zihan Li", "Diping Song", "Zefeng Yang", "Deming Wang", "Fei Li", "Xiulan Zhang", "Paul E. Kinahan", "Yu Qiao"], "abstract": "The need for improved diagnostic methods in ophthalmology is acute, especially in the less developed regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient interaction, making it a highly versatile tool for initial ophthalmic disease screening. VisionUnite can also serve as an educational aid for junior ophthalmologists, accelerating their acquisition of knowledge regarding both common and rare ophthalmic conditions. VisionUnite represents a significant advancement in ophthalmology, with broad implications for diagnostics, medical education, and understanding of disease mechanisms.", "sections": [{"title": "Introduction", "content": "In recent years, the global prevalence of ophthalmic diseases has surged beyond 2.2 billion, with over 1 billion individuals experiencing visual impairment due to limited access to essential medical services for conditions like myopia, hyperopia, glaucoma, and cataracts. The critical situation primarily stems from a shortage of ophthalmologists in low-income and middle-income regions, resulting in inadequate provision of ophthalmic services. Compounding this challenge, the World Health Organization's World Vision Report estimates a staggering cost of 14.3 billion to address these issues, underscoring the financial burden. Consequently, there is an escalating need for swift and precise comprehensive diagnoses facilitated by existing artificial intelligence technology. Notably, numerous researchers have made strides in developing vision models for diagnosing eye diseases, exemplified by works such as the works1\u20136. The development of deep learning models for ophthalmology faces three significant challenges. Firstly, these models are often tailored to diagnose specific diseases and lack the ability to provide comprehensive assessments for multiple conditions simultaneously. It is a critical shortfall, as patients frequently suffer from multiple ailments at once, particularly in older populations. For instance, as reported by the American Academy of Ophthalmology7, it is not uncommon for individuals aged 65 and above to have more than one eye disease. Secondly, there is a persistent issue with effective user interaction, which is essential for practical clinical implementation. Thirdly, many of these AI models lack interpretability in their diagnostic results, which is crucial for trust and reliability in medical settings. In an ideal scenario, a large vision-language model would seamlessly navigate diverse clinical scenarios, including disease screening, diagnostic process optimization, and junior ophthalmologist training. Such a model would integrate visual and linguistic data effectively, aligning closely with the diagnostic criteria used by medical professionals and adhering to clinical consensus guidelines. This approach would ideally involve identifying the lesion area and type before proceeding with a diagnosis, enhancing both the interpretability and accuracy of medical evaluation. However, prevailing models primarily fall short of this ideal. Current vision models such as RETFound, typically only offer diagnostic predictions by identifying the types of diseases without providing the necessary explanatory context. It results in a lack of interpretability that diverges from the standards of medical practice. Similarly, while there are advanced language models referenced in the study, there remains a significant gap in effective vision-language interaction. This deficiency is also noted in the lack of comprehensive user interaction which hinders the model from further understanding the needs of users, as described in work10. These limitations impair the accuracy and universality of diagnoses made by models, underscoring the need for an integrated approach that can effectively combine visual and linguistic information.\nTo meet the above requirements, we introduce VisionUnite, a Large Vision Language Model tailored for Ophthalmology with Clinical Knowledge. As depicted in Figure 1 (a), VisionUnite addresses three critical challenges: 1) the inability to predict open-ended"}, {"title": "Results", "content": "The architecture of VisionUnite is illustrated in Figure 1 (c), featuring a Transformer-based vision encoder, a Vision adapter, and a Large language model fine-tuned on LLaMA model11. Our innovative approach incorporates three distinct training objectives-visual text contrastive learning loss (CLIP Loss), visual classification loss (CLS Loss), and text generation loss (LLM Loss)\u2014to facilitate the convergence of large-scale visual language models and enhance text generation quality. For the pre-training of VisionUnite, we construct a comprehensive dataset of 1.19 million pretrained image-text pairs including natural image-text pairs 12 and biomedical image-text pairs13\u201315. The architecture of VisionUnite incorporates three training objectives: image-text contrastive learning, classification supervised learning, and text-generation supervised learning. These objectives help refine vision encoders, accurately categorize features, and guide the final text output. During pre-training, we use text-generation supervised learning as the primary objective to build robust connections between images and corresponding texts. Furthermore, we construct a Multimodal Fundus fine-tuning dataset (MMFundus) to train and evaluate the diagnostic performance of VisionUnite. The MMFundus dataset is currently the largest multimodal fundus dataset, including 296,379 sets of fundus image-text pairs and corresponding constructed 889,137 rounds of dialogue. The MMFundus dataset covers the eight main categories of fundus image as shown in Figure 1 (b) and its image part includes 53 datasets, such as ODIR16, APTOS17, DeepDRiD18, and REFUGE19 dataset, etc. We design six sign categories including \"Vascular\", \"Macular\", \"FBC (Fundus Boundary Color)\", \"OCD (Optical Cup Disc)\", \"FHE (Fundus Hemorrhages Examination)\", and \"Other\" for each image. The six signs encompass all critical regions and functionalities of the eye, thereby providing a comprehensive assessment of eye health and disease. Each category targets a specific aspect of eye anatomy or pathology: Vascular for blood flow, Macular for central vision, FBC for boundary conditions, OCD for optic nerve health (glaucoma), FHE for retinal bleeding and exudates, and Other for miscellaneous conditions. We generate the corresponding sign labels for each image using the original label information of fundus images based on the above standards. More details of signs and dataset construction are in the methods section.\nTo assess the efficacy of models, we extract 180 samples and corresponding 540 rounds of questions from the MMFundus dataset to constitute the test dataset. It encapsulates a wide range of fundus conditions, ranging from health to illness status, encompassing Myopia, Macular Edema, Age-related Macular Degeneration, Glaucoma, Tessellation, Branch Retinal Vein Occlusion, Cataract, Optical Disc Pallor, Optical Disk Cupping, Drusen, Diabetic Retinopathy, Central Serous Retinopathy, and Arteriosclerotic Retinopathy, thirteen distinct ophthalmic diseases or conditions in total. We benchmark the performance of models against clinical diagnostics, focusing on two pivotal criteria: diagnostic accuracy and diagnostic relevance. Diagnostic accuracy gauges the precision of the diagnostic interpretations, encompassing disease identification and categorization. However, diagnostic accuracy cannot fully reflect the performance of large vision-language models in many cases. Different models may provide different explanations when giving the same diagnostic prediction, and we need relevant criteria to quantify this difference. Therefore, we design the diagnostic relevance, which provides insights into the response quality and further differentiates their differences. Our experimental framework involves presenting a fundus image and three related questions to a junior ophthalmologist (possessing one year of clinical experience). Concurrently, responses are elicited from three large vision-language models, 1. GPT-4V20 (gpt-4-1106-vision-preview), 2. Gemini Pro21 (gemini-pro-vision) and 3. our proposed VisionUnite. These sets of responses are then presented to three senior ophthalmologists (boasting over ten years of clinical experience), tasked with appraising the diagnostic accuracy and relevance. It is worth noting that we mandate the integration of responses into coherent paragraphs without extraneous AI-generated content to ensure an impartial evaluation. The details of diagnostic accuracy and relevance are as follows:\nDiagnostic accuracy: We follow the clinical evaluation standards. 1. In cases of extra answers, we consider them to be incorrect answers. 2. In case of just missing unnecessary diagnostic information, we still consider the responses to be correct. 3. In our evaluation, the answer must only include all diagnosable diseases to be considered correct. We use the Wilson method22 to estimate 95% confidence interval of diagnostic accuracy and calculate the p-value with the two-sided T-test and the above Wilson estimation.\nDiagnostic relevance: 1. Senior ophthalmologists rank the response sets based on their alignment with the accurate diagnosis, scoring the most consistent response as four and the least as one. 2. Diagnostic relevance is designed to refine performance evaluation by taking into account factors that mere diagnostic accuracy might miss. For example, a response could be accurate in diagnostic prediction yet fail to adhere strictly to diagnostic criteria, which will diminish its overall diagnostic relevance. The bootstrap23 method is used to estimate the 95% confidence interval of diagnostic relevance and the two-sided T-test is applied to calculate its P-value."}, {"title": "Comparison of Diagnosis between Ophthalmologist and Large Vision-Language Models", "content": "In our study, we conduct a comprehensive assessment of the diagnostic capabilities exhibited by the junior ophthalmologist compared with the large vision-language models, which include Gemini Pro, GPT-4V, and our proposed VisionUnite. The evaluation is performed on the designated test dataset. It encompasses a detailed analysis of diagnostic proficiency at both the disease and sign levels. Additionally, we explore an intricate examination of the diagnostic relevance between the responses provided by the junior ophthalmologist and those generated by the large vision-language models. As shown in Figure 2, the overall diagnostic accuracy of VisionUnite is over 45% and 28% higher than Gemini Pro and GPT-4V respectively, and the corresponding p-value is less than 0.001. The diagnostic relevance of VisionUnite is also over 62% and 25% higher than Gemini Pro and GPT-4V respectively with the p-value less than 0.001. Compared to the junior ophthalmologist, the overall accuracy of VisionUnite is approximately 4.5% higher with the p-value being 0.0876, and the diagnostic relevance is 0.75% higher (from 2.915 to 2.937). In each round of result evaluation, we find that the performance of VisionUnite in the first round of diagnostic Q&A is higher than that of junior ophthalmologist, with an accuracy rate of approximately 7.2% and a diagnostic relevance of 2.6%. The result indicates that VisionUnite has stronger analytical and reasoning abilities for fundus images, thus achieving superior performance in the first round of diagnosis. In the second and third rounds of diagnostic Q&A, the performance of VisionUnite is comparable to that of a junior ophthalmologist and far superior to GPT-4V and Gemini Pro. It also indicates that VisionUnite has the same problem-solving and interpretation abilities as doctors."}, {"title": "Diagnosis of Specific Disease or Condition and Sign", "content": "Building upon the foundational analysis of average diagnostic performance, our investigation further explores the nuanced diagnostic capabilities of the model and junior ophthalmologist for both healthy ocular conditions and a spectrum of specific ophthalmological pathologies. This segment of our study was particularly comprehensive, encompassing a wide array of conditions ranging from commonplace to rare. Specifically, our focus extended to seven distinct conditions including Age-Related Macular Degeneration, Branch Retinal Vein Occlusion, Cataract, Diabetic Retinopathy, Drusen, Glaucoma, and Myopia. This exhaustive approach allowed for a holistic understanding of the diagnostic acumen of these prevalent and less common ophthalmological diseases or conditions. In addition, we also explore the diagnostic performance involving different signs. As shown in Figure 3, VisionUnite and ophthalmologists perform significantly better than Gemini Pro and GPT-4V in six common and rare ophthalmic diseases or conditions except for Cataracts. The performance of GPT-4V on cataracts is comparable with VisionUnite and ophthalmologists. Among the diseases or conditions, VisionUnite has the best diagnostic performance in four diseases or conditions: Age-Related Macular Degeneration (AMD), Diabetic Retinopathy (DR), Drusen, and Glaucoma. Ophthalmologists perform better in three types of diseases or conditions: Branch Retinal Vein Occlusion, Cataract, and Myopia. For Cataract and Drusen, due to the small number of test samples, there may be significant differences in performance between different models. For AMD and DR, due to the tendency of ophthalmologists to provide incorrect disease staging, their performance is not as good as VisionUnite. Regarding diagnosis involving different signs, VisionUnite and ophthalmologists are also better than GPT-4V and Gemini Pro. As shown in Figure c1-c5, VisionUnite and ophthalmologists perform similarly on OCD (Optical Cup Disc). For FHE (Fundus Hemorrhages Examination) and Macular, VisionUnite performs better. For FBC (Fundus Boundary Color) and Vascular, ophthalmologists perform better. In terms of diagnostic relevance except for macular, ophthalmologists are better than all other models, which also reflects the ability of ophthalmologists to explain the results."}, {"title": "Joint Diagnosis of Multiple Diseases or Conditions and Signs", "content": "When dealing with fundus images from patients suffering from multiple diseases, the overlapping or combined manifestations of these conditions can significantly complicate the diagnostic process7. Each disease may present with specific signs on fundus examination, such as hemorrhages, exudates, or abnormalities in the vascular architecture. However, when multiple pathologies coexist, these signs can lead to a complex visual presentation that challenges even the most experienced ophthalmologists. For instance, diabetic retinopathy and arteriosclerotic Retinopathy both exhibit retinal vessel changes, but their specific impacts on the retina might slightly differ. Diabetic retinopathy typically shows more microaneurysms and hemorrhages, whereas arteriosclerotic retinopathy involves changes in the retinal arterioles, such as narrowing and tortuosity as shown in Figure 15. Therefore, we explore the joint diagnostic performance of the model and junior ophthalmologist for multiple diseases and multiple signs, including 15 diseases or conditions and 5 signs. As shown in Figure 7, the overall performance of VisionUnite is superior to GPT-4V and Gemini Pro in the presence of multiple diseases in fundus images. Compared to ophthalmologists, the overall accuracy of VisionUnite is similar, but the diagnostic relevance is lower. In the first two rounds, VisionUnite performs better than junior ophthalmologists, achieving a diagnostic accuracy of 44.44% and 77.78% with diagnostic relevance of 3.333 and 3.0. Gemini Pro performs the worst and cannot accurately make diagnosis predictions in the first round, with a diagnostic relevance of only 1.222. The results of multi-sign diagnosis are shown in Figure 7 d9-d16. Ophthalmologists perform best with 75% diagnostic accuracy and a diagnostic relevance of 3.375, followed by VisionUnite with 8.33% diagnostic accuracy of 5 and a diagnostic relevance of 2.917. The performance of GPT-4V and Gemini Pro is worse, with diagnostic accuracy of 35.42% and 12.5% respectively, and diagnostic relevance of 2.271 and 1.438. The results also reflect that the ability of models to analyze images is not as good as that of humans, making it difficult to obtain a correct diagnosis when encountering fundus images with multiple signs."}, {"title": "Misdiagnosis Analysis in Healthy Conditions", "content": "We also investigate the misdiagnosis of various models and ophthalmologists in the face of healthy fundus images. We use misdiagnosis rate and diagnostic relevance as evaluation indicators in this kind of scenario, where the misdiagnosis rate equals 1 - diagnostic accuracy, and diagnostic relevance remains as defined earlier. Since healthy samples often outnumber abnormal"}, {"title": "Diagnostic Correction in Patient Interaction", "content": "We assume that an advanced vision-language model ought to possess the capability for diagnostic rectification. It implies the inherent ability of models to autonomously amend inaccuracies in their initial diagnosis upon receiving additional information during a subsequent patient interaction. In this context, we focus on assessing the diagnostic precision of these models. Our evaluation encompassed a rigorous examination of the testing performance exhibited by VisionUnite, GPT-4V, and Gemini Pro. Notably, VisionUnite demonstrated superior corrective accuracy, which significantly surpassed GPT-4V and Gemini Pro. The superior performance of VisionUnite not only highlights the potential of domain-specific large models to exhibit heightened sensitivity in problem recognition within the medical field but also underscores their capacity for agile adaptation. Specifically, the ability of VisionUnite to swiftly recalibrate its responses in alignment with nuanced shifts in problem context, which we term as 'problem sensitivity', stands as a testament to its refined diagnostic acumen and adaptability. As shown in Figure 9, we calculate the proportion of correct answers in the second or third round for each model in the case of incorrect answers in the first round. Overall refers to answering correctly in the second or third round. Round 2 refers to answering correctly in the second round, while Round 3 refers to answering correctly in the third round. The results demonstrate that VisionUnite has an overall correction accuracy of 86.67%, which is 24% and 39% higher than GPT-4V and Gemini Pro respectively with the p-value less than 0.001. In the second round, VisionUnite can perform much better than GPT-4V and Gemini Pro, with a second-round correction accuracy of 66.67% for VisionUnite, while GPT-4V and Gemini Pro can only achieve 28.04% and 20.69% respectively with the p-value also less than 0.001. We also evaluate the diagnostic accuracy of each model for both correct and incorrect predictions as shown in Figure 10. The findings demonstrate that VisionUnite's performance in understanding problems and following instructions is equivalent to that of a junior ophthalmologist, independent of whether its predictions are correct or incorrect. The details can be seen in the appendix."}, {"title": "Diagnostic Errors Analysis between Ophthalmologist and Large Vision-Language Models", "content": "To analyze the outputs of the models thoroughly, we investigate their diagnostic errors and contrast them with assessments from ophthalmologists. We categorize these errors into two types: missed and incorrect errors. Missed errors pertain to incomplete yet correct diagnoses, while incorrect errors include entirely wrong diagnoses and partially correct diagnoses with additional, irrelevant errors. Our goal is to evaluate the completeness and accuracy of responses, noting omissions or irrelevant inclusions. Additionally, we assess error severity, which is categorized as error-free, minor, or major errors based on the impact on clinical judgment and treatment specificity. Minor errors encompass overlooked partial signs not critical to overall judgment or unnecessary diagnoses, as well as generalized treatment recommendations that lack specificity. Major errors include significantly incorrect responses. We also evaluate the potential for physical or mental harm resulting from the answers, grading potential health risks based on severity and likelihood. No harm is considered error-free, mild or moderate harm is considered minor errors, and serious harm or deaths are considered major errors. As shown in Figure 4, we analyze the diagnostic errors in five different situations: overall, single disease, multiple diseases, single sign, and multiple signs. Specifically, VisionUnite and ophthalmologists perform better than Gemini Pro and GPT-4V in various situations. Overall, VisionUnite performs the best in the \"incorrect error\" dimension, achieving an error-free rate of 78.15%, which is 4.82% higher than that of ophthalmologists with a corresponding p-value of 0.035. Ophthalmologists perform better in the \"missed error\" dimension, achieving an error-free rate of 71.85%. At the same time, we find that incorrect errors are more associated with major errors than minor errors. Compared to missed errors, incorrect errors often lead to more serious consequences. However, the results also indicate that there is still a certain gap between VisionUnite and ophthalmologists in the diagnosis of multiple diseases, and it also indicates that the understanding and reasoning abilities of models for images need to be further improved to reach the level of ophthalmologists. The details of the classification criteria for diagnostic errors are in Table 2 in the appendix."}, {"title": "Diagnostic analysis of VisionUnite assisted junior ophthalmologist", "content": "To further explore the clinical decision-support capabilities of VisionUnite, we assess its impact on the diagnostic performance of primary ophthalmologists. We specifically evaluate the improvements in diagnostic accuracy and efficiency when VisionUnite assists ophthalmologists in the initial round which includes 180 questions. Our analysis shows that VisionUnite facilitates a 26.98% reduction in diagnostic time and a 29.44% increase in diagnostic accuracy overall illustrated in Figure 5. Notably, the use of VisionUnite leads to a remarkable 50% improvement in diagnostic accuracy and a 33% reduction in diagnostic time in cases of diabetic retinopathy. Furthermore, we examine the impact of VisionUnite on diagnoses involving five distinct physical signs detailed in Figure 5 i8-i12. The assistance is most significant for fundus hemorrhage exudates (FHE) and optic disc cupping (OCD), where VisionUnite helps improve diagnostic accuracy by 39.02% and 35.49%, respectively, while also reducing diagnostic time by 35.63% and 24.25%."}, {"title": "Consistent Interpretation of Visual and Language Features", "content": "In our final analytical phase, we scrutinize the coherent integration of visual and textual elements within large vision-language models, an endeavor aimed at assessing the alignment between the textual descriptions and the corresponding visual data. Our investigation addresses the potential for 'illusory discrepancies' within the output of these sophisticated models. Specifically, we list the outputs of VisionUnite against those from other large vision-language models, with a keen focus on discerning any textual outputs that may depict features not present within the associated imagery. For instance, in cases where the diagnosis pertained to age-related macular degeneration, and the imagery exclusively showcases the presence of drusen, we should evaluate whether the textual narrative unjustifiably extended the diagnosis to encompass additional manifestations such as hemorrhaging and exudation. Such a detailed examination is pivotal for understanding and mitigating the tendency of these models to over-interpret or misalign textual descriptions with their visual counterparts, thereby ensuring a more accurate and reliable diagnostic output. Meanwhile, we also present a series of evaluation criteria for the responses of the models and junior ophthalmologists in this section. More cases from Junior Ophthalmologist and Large Vision-Language Models including VisionUnite, GPT-4V, and Gemini Pro can be seen in the extended data Figure 12-15 and the corresponding expert evaluation is in Figure 16."}, {"title": "Discussion", "content": "The diagnostic performance of VisionUnite in ophthalmic diseases has undergone thorough validation across diverse clinical scenarios, encompassing open-ended multiple disease diagnoses, clinical interpretation, and interactive inquiry from the patients. The integration of VisionUnite is poised to significantly elevate the precision of initial screenings for ophthalmic diseases, consequently enhancing the diagnostic efficiency of healthcare professionals, which is a particularly pivotal advancement in underdeveloped regions or those plagued by scarce ophthalmic services. Experimental findings underscore that VisionUnite's capabilities in image analysis, disease diagnosis, and clinical explanations have achieved parity with ophthalmic junior doctors and, in certain instances, even surpassed their performance in diagnosing specific diseases. As attested by ophthalmologists, VisionUnite outshines general large vision-language models like GPT-4V and Gemini Pro, not only in delivering more accurate disease diagnoses but also in furnishing clear diagnostic foundations, which is an aspect often lacking in typical large vision-language models. In specific clinical applications, VisionUnite proves adaptable for patient inquiries while concurrently assisting doctors in optimizing diagnoses and facilitating ophthalmic training. Besides, VisionUnite expedites doctors' disease diagnosis, offers question-and-answer services for patients to address their queries, and delivers tailored responses. For healthcare professionals, VisionUnite serves as a valuable auxiliary diagnostic tool, leveraging extensive ophthalmic knowledge and adeptness in disease diagnosis. Different from CT or X-ray, fundus photography typically lacks accompanying textual descriptions. Traditionally, the patient's understanding of fundus imaging results primarily derives from doctors' verbal explanations, which can sometimes lead to inadvertent omissions. However, the introduction of VisionUnite offers a robust solution by generating comprehensive fundus photography reports. The integration of verified VisionUnite reports not only aids doctors in the diagnostic process but also mitigates the risk of potential oversights.\nWhile VisionUnite demonstrates diagnostic capabilities comparable to those of a junior ophthalmologist, certain limitations warrant consideration. Notably, the clinical interpretation of fundus images by VisionUnite can at times be overly broad, hindering the delivery of more precise information and consequently constraining performance enhancements in subsequent second-rounded diagnoses. Furthermore, the current proficiency of VisionUnite is confined to processing fundus images due to constraints in available data modalities. Future endeavors aim to expand its capabilities to include additional ophthalmic modalities such as FA, OCT, OCTA, and others. We also believe that the augmentation of data and modalities during the training process will enhance VisionUnite's ability to handle diverse modalities and bolster its diagnostic performance. Another limitation lies in the current granularity of sign-level labels, as we have only categorized fundus features into six sign labels. It has led to omissions in predictions for images exhibiting multiple sign labels. To address the limitation, we plan to involve more fine-grained sign-level labels to better discern between various fundus images and enhance diagnostic accuracy. As computational power advances and more high-quality annotated data becomes available, we aspire for VisionUnite to extend its diagnostic reach to encompass a broader spectrum of rare diseases. Simultaneously, we envision its role as a valuable tool for ophthalmologists, aiding in identifying and classifying lesions and diseases. By introducing a more extensive array of sign-level labels, we also believe VisionUnite has the potential to unveil previously elusive links between signs and diseases. In addition, it could contribute to the identification of novel diagnostic evidence, fostering the evolution of diagnostic standards and protocols. Lastly, we expect VisionUnite to unveil connections between ocular features and systemic diseases. Contemporary research increasingly proposes the utility of ocular examinations for early screening of systemic conditions, such as Alzheimer's disease. Leveraging the heightened visual encoding capabilities of large vision-language models and incorporating data on systemic diseases, we envision VisionUnite bridging ophthalmic data with information from other body regions. It is poised to expedite the revelation of hitherto undiscovered connections between ocular features and diseases manifesting in other organs."}, {"title": "Methods", "content": "The architecture of VisionUnite To enable the model to achieve open-ended prediction and have user interaction and clinical interpretation capabilities, we propose a large vision-language foundation model with clinical knowledge. Illustrated in 1 (c), our proposed VisionUnite comprises a Transformer-based vision encoder, a vision adapter tailored for signs-level classification, a vision projector for aligning visual embeddings and text tokens, and a substantial language model fine-tuned on llama-7B. The large language model is different from the vision model in that it can achieve open-ended prediction. By leveraging the combination of image-text pairs and extensive dialogue information, VisionUnite excels in responding to user questions based on visual data. We introduce visual features with sign-level features into the model, which improve its ability to understand images and further enhance its clinical interpretation ability. This design can extend its ability to analyze fundus images for medical diagnosis. Our model is also designed for multi-round dialogues, which helps the model follow user instructions and achieve better problem-understanding ability. Harnessing the synergy of image-text pairs and multi-round dialogue information, VisionUnite demonstrates proficiency in addressing user queries grounded in visual data, extending to the capability of diagnosing fundus images. The holistic architecture of VisionUnite enables nuanced and contextually informed responses, ensuring a comprehensive understanding of the inquiries based on visual elements. We leverage the EVA02 model25 with the CLIP Vision Encoder to collectively serve as the Transformer-Based Vision Encoder. Our configuration incorporates 12 layers of EvaBlock25 within the original vision Transformer Block26. Notably, in our design, the GELU activation27 in the original vision Transformer Block is replaced with the more advanced SwiGLU28. It is also widely used in PaLM29, LLaMA\u00b9\u00b9, and LLaMA-Adapter30 to enhance the performance of the FFN (position feedforward network) layer in the Transformer architecture. Specifically, images, text descriptions, and questions are fed into corresponding encoders to form visual embeddings, text embeddings, and dialogue tokens during the training phase. In this phase, we use contrastive learning to supervise the encoding formation of visual embeddings and text embeddings. Afterward, we use the vision adapter to perform secondary encoding and sign classification on visual embeddings, which form six corresponding sign category embeddings. Then we concatenate the visual embeddings and sign category embeddings together as inputs to the vision projector. For the text features, we embed the corresponding sign categories (CLS tokens) and concatenate them to form text features with the dialogue tokens based on the prediction of the signs.\nThe vision encoder in our VisionUnite model facilitates the extraction of visual embeddings, serving as input for the vision adapter to generate classification predictions at the sign level. The prediction of signs by VisionUnite plays a pivotal role in improving both diagnostic accuracy and efficiency. By accurately identifying and localizing signs such as lesions in the images, the model narrows the diagnostic possibilities and guides the model toward more targeted investigations. It not only optimizes the diagnostic process by reducing the need for too broad differential diagnoses but also enhances accuracy by focusing on the most relevant clinical signs. In addition, the model is designed with the flexibility to accommodate an expanding array of sign categories, enhancing its adaptability for future developments. To further refine VisionUnite, we plan to increase the universality of the model by supporting a wider range of sign-level labels. The expansion is aimed at improving the granularity of diagnostic evidence and enhancing the interpretability of the outputs from the models. VisionUnite can recognize six primary signs, which cover a wide range of ophthalmic diseases. This kind of capability aligns closely with the diagnostic process used by medical professionals, who often start their diagnosis by identifying specific signs before narrowing down to a more precise disease identification. Furthermore, the ability of VisionUnite to leverage these predicted image signs effectively distinguishes it from other ophthalmic deep learning models. By focusing on sign-level features, VisionUnite ensures that the diagnostic suggestions it provides are both precise and clinically relevant, offering a robust tool that supports clinicians in making informed decisions. Following the acquisition of predicted sign-level data, VisionUnite seamlessly employs the vision projector to synchronize visual embeddings with dialogue tokens. It entails concatenating the visual embedding with the class embedding, which is then fed into the vision projector. The alignment process involves matching the resulting feature dimensions with CLS (class) and dialogue tokens via concatenation, with the CLS token encapsulating the predicted sign-level feature. The multimodal features, which combine visual embeddings, CLS tokens, and dialogue tokens, are the inputs for training the final text generation response within the framework of the large language model. Overall, the integration of state-of-the-art components contributes to the overall efficacy and robustness of VisionUnite, positioning it as a cutting-edge vision-language model."}, {"title": "The training objectives of VisionUnite", "content": "In the architecture of our proposed VisionUnite, we have crafted three distinct training objectives to enhance convergence: image-text contrastive learning, classification supervised learning, and text-generation supervised learning. The utilization of image-text contrastive learning facilitates the refinement of visual encoders, aiding them in more effectively aligning fundus image features. Meanwhile, the application of classification-supervised learning contributes by furnishing accurate feature categories for both visual embeddings and dialogue tokens. We utilize the accuracy to guide the model training process, enhancing its overall performance. Finally, text-generation supervised learning plays a role in guiding the output of the language model, which is pivotal for achieving accurate and open-ended disease diagnoses. Unlike previous vision models, large language models are trained on vast amounts of diverse textual data. The extensive textual data enables them to generate context-aware responses and detailed explanations that are coherent and clinically relevant. We can achieve open-ended disease diagnosis using the generated text from language models. In contrast, the prediction categories of vision models are limited and concentrated on specific diseases. Collectively, these three training objectives synergistically contribute to the robust convergence and performance of VisionUnite, making it a versatile and effective tool in the field of ophthalmic diagnostics."}, {"title": "Image-text contrastive learning", "content": "To attain seamless alignment between image and text features, we employ image-text contrastive learning. In particular, we leverage the CLIP loss methodology to quantify the similarity between image embeddings and text embeddings, as illustrated in Figure 1 (d) and below:\n$L_{CLIP} = (L_{img} + L_{text})/2$\n$L_{img} = -\\frac{1}{N} \\sum_{i=1}^{N} [t_i \\log(P_{img,i})]$\n$L_{text} = -\\frac{1}{N} \\sum_{i=1}^{N} [t_i \\log(P_{text,i})]$\nwhere N denotes the number of samples in each batch. $P_{img,i}$ is the cosine similarities of image i to all N text embeddings and $P_{text,i}$ is the cosine similarities of text i to all N image embeddings. $t_i$ denotes the soft label representation {$P_1, P_2, P_3, ..., P_i, ...P_n$} of the corresponding image and text pairs along which the cross-entropy loss is computed."}, {"title": "Classification supervised learning", "content": "In facilitating the acquisition of sign-level features, we employ conventional classification learning to guide the training of VisionUnite. Notably, we embrace multi-label classification while acknowledging that each sample may encompass more than one predicted category. This approach aligns more closely with the intricacies of real-world clinical scenarios, where patients may concurrently experience multiple types of eye diseases. For each category under supervision, we apply cross-entropy loss, aggregating these individual losses to derive the ultimate classification loss, as depicted in Figure 1 (e) and below:\n$L_{CLS} = \\sum_{k=1}^{M} L_{CLS_k} = -\\frac{1}{MN} \\sum_{k=1}^{M} \\sum_{i=1}^{N} [y_{ki} \\log(p_{k,i}) + (1 - y_{ki}) \\cdot \\log(1 \u2013 p_{k,i})]$\nwhere M and N denote the number of categories and samples respectively. $L_{CLS,k}$ signifies the cross-entropy loss for category K. And $y_{k,i}$ denotes whether sample i belongs to class K, and $p_{k,i}$ represents the probability of sample i belonging to class K."}, {"title": "Text-generation supervised learning", "content": "Within the framework of VisionUnite, we employ text-generation supervised learning to guide the text output of LLM, which is an essential aspect given that the generated text must intuitively articulate the diagnostic results and their underlying rationale. The LLM loss aims to train the model to generate text closely resembling patterns in its training data as shown in Figure 1 (f), and its formulation is articulated as follows:\n$L_{LLM} = -\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{T_i} \\sum_{j=1}^{T_i} \\log p_{\\theta}(t_{i,j}|t_{i,<j},x_i)$\nwhere N represents the number of samples in the training set, $T_i$ denotes the target sequence length for the i-th sample, and $t_{i, j}$ signifies the j-th target word in the i-th sample. $t_{i,<j}$ corresponds to the preceding j-1 target words in the i-th sample. $x_i$ is the input sequence for the i-th sample, and $p_{\\theta}(t_{i,j}|t_{i,<j},x_i)$ denotes the probability assigned by the model to the j-th target word in the i-th sample, with \u03b8 representing the model parameters."}, {"title": "Datasets for pre-training VisionUnite", "content": "In the pre-training phase of VisionUnite, we utilize a dataset comprising 1.24 million image-text pairs, encompassing around 616,435 natural image-text pairs sourced from the COCO dataset12 and around 623,816 biomedical image-text pairs sourced from the PMC-OA dataset13,14 (6"}]}