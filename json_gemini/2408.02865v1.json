{"title": "VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge", "authors": ["Zihan Li", "Diping Song", "Zefeng Yang", "Deming Wang", "Fei Li", "Xiulan Zhang", "Paul E. Kinahan", "Yu Qiao"], "abstract": "The need for improved diagnostic methods in ophthalmology is acute, especially in the less developed regions with limited access to\nspecialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology\nenhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text\npairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative\nfoundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists.\nVisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient\ninteraction, making it a highly versatile tool for initial ophthalmic disease screening. VisionUnite can also serve as an educational\naid for junior ophthalmologists, accelerating their acquisition of knowledge regarding both common and rare ophthalmic conditions.\nVisionUnite represents a significant advancement in ophthalmology, with broad implications for diagnostics, medical education, and\nunderstanding of disease mechanisms.", "sections": [{"title": "Introduction", "content": "In recent years, the global prevalence of ophthalmic diseases has surged beyond 2.2 billion, with over 1 billion individuals experiencing\nvisual impairment due to limited access to essential medical services for conditions like myopia, hyperopia, glaucoma, and cataracts.\nThe critical situation primarily stems from a shortage of ophthalmologists in low-income and middle-income regions, resulting in\ninadequate provision of ophthalmic services. Compounding this challenge, the World Health Organization\u2019s World Vision Report\nestimates a staggering cost of 14.3 billion to address these issues, underscoring the financial burden. Consequently, there is an\nescalating need for swift and precise comprehensive diagnoses facilitated by existing artificial intelligence technology. Notably,\nnumerous researchers have made strides in developing vision models for diagnosing eye diseases, exemplified by works such as\nthe works1\u20136\n. The development of deep learning models for ophthalmology faces three significant challenges. Firstly, these models\nare often tailored to diagnose specific diseases and lack the ability to provide comprehensive assessments for multiple conditions\nsimultaneously. It is a critical shortfall, as patients frequently suffer from multiple ailments at once, particularly in older populations.\nFor instance, as reported by the American Academy of Ophthalmology7\n, it is not uncommon for individuals aged 65 and above to\nhave more than one eye disease. Secondly, there is a persistent issue with effective user interaction, which is essential for practical\nclinical implementation. Thirdly, many of these AI models lack interpretability in their diagnostic results, which is crucial for trust\nand reliability in medical settings. In an ideal scenario, a large vision-language model would seamlessly navigate diverse clinical\nscenarios, including disease screening, diagnostic process optimization, and junior ophthalmologist training. Such a model would\nintegrate visual and linguistic data effectively, aligning closely with the diagnostic criteria used by medical professionals and adhering\nto clinical consensus guidelines. This approach would ideally involve identifying the lesion area and type before proceeding with a\ndiagnosis, enhancing both the interpretability and accuracy of medical evaluation. However, prevailing models primarily fall short of\nthis ideal. Current vision models such as RETFound8\n, typically only offer diagnostic predictions by identifying the types of diseases\nwithout providing the necessary explanatory context. It results in a lack of interpretability that diverges from the standards of medical\npractice. Similarly, while there are advanced language models referenced in the study9\n, there remains a significant gap in effective\nvision-language interaction. This deficiency is also noted in the lack of comprehensive user interaction which hinders the model from\nfurther understanding the needs of users, as described in work10. These limitations impair the accuracy and universality of diagnoses\nmade by models, underscoring the need for an integrated approach that can effectively combine visual and linguistic information.\nTo meet the above requirements, we introduce VisionUnite, a Large Vision Language Model tailored for Ophthalmology with\nClinical Knowledge. As depicted in Figure 1 (a), VisionUnite addresses three critical challenges: 1) the inability to predict open-ended"}, {"title": "Results", "content": "The architecture of VisionUnite is illustrated in Figure 1 (c), featuring a Transformer-based vision encoder, a Vision adapter, and a\nLarge language model fine-tuned on LLaMA model11. Our innovative approach incorporates three distinct training objectives-visual\ntext contrastive learning loss (CLIP Loss), visual classification loss (CLS Loss), and text generation loss (LLM Loss)\u2014to facilitate\nthe convergence of large-scale visual language models and enhance text generation quality. For the pre-training of VisionUnite, we\nconstruct a comprehensive dataset of 1.19 million pretrained image-text pairs including natural image-text pairs 12 and biomedical image-\ntext pairs13\u201315. The architecture of VisionUnite incorporates three training objectives: image-text contrastive learning, classification\nsupervised learning, and text-generation supervised learning. These objectives help refine vision encoders, accurately categorize\nfeatures, and guide the final text output. During pre-training, we use text-generation supervised learning as the primary objective\nto build robust connections between images and corresponding texts. Furthermore, we construct a Multimodal Fundus fine-tuning\ndataset (MMFundus) to train and evaluate the diagnostic performance of VisionUnite. The MMFundus dataset is currently the largest\nmultimodal fundus dataset, including 296,379 sets of fundus image-text pairs and corresponding constructed 889,137 rounds of\ndialogue. The MMFundus dataset covers the eight main categories of fundus image as shown in Figure 1 (b) and its image part includes\n53 datasets, such as ODIR16, APTOS17, DeepDRiD18, and REFUGE19 dataset, etc. We design six sign categories including \"Vascular\",\n\"Macular\", \"FBC (Fundus Boundary Color)\", \"OCD (Optical Cup Disc)\", \"FHE (Fundus Hemorrhages Examination)\", and \"Other\" for\neach image. The six signs encompass all critical regions and functionalities of the eye, thereby providing a comprehensive assessment\nof eye health and disease. Each category targets a specific aspect of eye anatomy or pathology: Vascular for blood flow, Macular for\ncentral vision, FBC for boundary conditions, OCD for optic nerve health (glaucoma), FHE for retinal bleeding and exudates, and\nOther for miscellaneous conditions. We generate the corresponding sign labels for each image using the original label information of\nfundus images based on the above standards. More details of signs and dataset construction are in the methods section.\nTo assess the efficacy of models, we extract 180 samples and corresponding 540 rounds of questions from the MMFundus dataset\nto constitute the test dataset. It encapsulates a wide range of fundus conditions, ranging from health to illness status, encompassing\nMyopia, Macular Edema, Age-related Macular Degeneration, Glaucoma, Tessellation, Branch Retinal Vein Occlusion, Cataract,\nOptical Disc Pallor, Optical Disk Cupping, Drusen, Diabetic Retinopathy, Central Serous Retinopathy, and Arteriosclerotic Retinopathy,\nthirteen distinct ophthalmic diseases or conditions in total. We benchmark the performance of models against clinical diagnostics,\nfocusing on two pivotal criteria: diagnostic accuracy and diagnostic relevance. Diagnostic accuracy gauges the precision of the\ndiagnostic interpretations, encompassing disease identification and categorization. However, diagnostic accuracy cannot fully reflect\nthe performance of large vision-language models in many cases. Different models may provide different explanations when giving the\nsame diagnostic prediction, and we need relevant criteria to quantify this difference. Therefore, we design the diagnostic relevance,\nwhich provides insights into the response quality and further differentiates their differences. Our experimental framework involves\npresenting a fundus image and three related questions to a junior ophthalmologist (possessing one year of clinical experience).\nConcurrently, responses are elicited from three large vision-language models, 1. GPT-4V20 (gpt-4-1106-vision-preview), 2. Gemini\nPro21 (gemini-pro-vision) and 3. our proposed VisionUnite. These sets of responses are then presented to three senior ophthalmologists\n(boasting over ten years of clinical experience), tasked with appraising the diagnostic accuracy and relevance. It is worth noting that\nwe mandate the integration of responses into coherent paragraphs without extraneous AI-generated content to ensure an impartial\nevaluation. The details of diagnostic accuracy and relevance are as follows:\nDiagnostic accuracy: We follow the clinical evaluation standards. 1. In cases of extra answers, we consider them to be incorrect\nanswers. 2. In case of just missing unnecessary diagnostic information, we still consider the responses to be correct. 3. In our\nevaluation, the answer must only include all diagnosable diseases to be considered correct. We use the Wilson method22 to estimate\n95% confidence interval of diagnostic accuracy and calculate the p-value with the two-sided T-test and the above Wilson estimation.\nDiagnostic relevance: 1. Senior ophthalmologists rank the response sets based on their alignment with the accurate diagnosis,\nscoring the most consistent response as four and the least as one. 2. Diagnostic relevance is designed to refine performance evaluation\nby taking into account factors that mere diagnostic accuracy might miss. For example, a response could be accurate in diagnostic\nprediction yet fail to adhere strictly to diagnostic criteria, which will diminish its overall diagnostic relevance. The bootstrap23 method\nis used to estimate the 95% confidence interval of diagnostic relevance and the two-sided T-test is applied to calculate its P-value.\nAdditionally, we design a multiple-choice evaluation experiment to evaluate the diagnostic performance across various models\nobjectively. Specifically, we provide four distinct options from all disease labels in the MMFundus dataset that models should select\nfor the most likely diagnostic outcome based on the provided fundus image. The other three options are randomly selected except the\ncorrect options. We conduct experiments on 2233 cases and calculate the multiple-choice accuracy of ten diseases for two models:\nLLaVA-Med24 (LLaVA-Med with finetuning) and VisionUnit. The results are in the appendix and Figure 11."}, {"title": "Comparison of Diagnosis between Ophthalmologist and Large Vision-Language Models", "content": "In our study, we conduct a comprehensive assessment of the diagnostic capabilities exhibited by the junior ophthalmologist compared\nwith the large vision-language models, which include Gemini Pro, GPT-4V, and our proposed VisionUnite. The evaluation is\nperformed on the designated test dataset. It encompasses a detailed analysis of diagnostic proficiency at both the disease and sign\nlevels. Additionally, we explore an intricate examination of the diagnostic relevance between the responses provided by the junior\nophthalmologist and those generated by the large vision-language models. As shown in Figure 2, the overall diagnostic accuracy of\nVisionUnite is over 45% and 28% higher than Gemini Pro and GPT-4V respectively, and the corresponding p-value is less than 0.001.\nThe diagnostic relevance of VisionUnite is also over 62% and 25% higher than Gemini Pro and GPT-4V respectively with the p-value\nless than 0.001. Compared to the junior ophthalmologist, the overall accuracy of VisionUnite is approximately 4.5% higher with the\np-value being 0.0876, and the diagnostic relevance is 0.75% higher (from 2.915 to 2.937). In each round of result evaluation, we\nfind that the performance of VisionUnite in the first round of diagnostic Q&A is higher than that of junior ophthalmologist, with an\naccuracy rate of approximately 7.2% and a diagnostic relevance of 2.6%. The result indicates that VisionUnite has stronger analytical\nand reasoning abilities for fundus images, thus achieving superior performance in the first round of diagnosis. In the second and third\nrounds of diagnostic Q&A, the performance of VisionUnite is comparable to that of a junior ophthalmologist and far superior to\nGPT-4V and Gemini Pro. It also indicates that VisionUnite has the same problem-solving and interpretation abilities as doctors."}, {"title": "Diagnosis of Specific Disease or Condition and Sign", "content": "Building upon the foundational analysis of average diagnostic performance, our investigation further explores the nuanced diagnostic\ncapabilities of the model and junior ophthalmologist for both healthy ocular conditions and a spectrum of specific ophthalmological\npathologies. This segment of our study was particularly comprehensive, encompassing a wide array of conditions ranging from\ncommonplace to rare. Specifically, our focus extended to seven distinct conditions including Age-Related Macular Degeneration,\nBranch Retinal Vein Occlusion, Cataract, Diabetic Retinopathy, Drusen, Glaucoma, and Myopia. This exhaustive approach allowed for\na holistic understanding of the diagnostic acumen of these prevalent and less common ophthalmological diseases or conditions. In\naddition, we also explore the diagnostic performance involving different signs. As shown in Figure 3, VisionUnite and ophthalmologists\nperform significantly better than Gemini Pro and GPT-4V in six common and rare ophthalmic diseases or conditions except for\nCataracts. The performance of GPT-4V on cataracts is comparable with VisionUnite and ophthalmologists. Among the diseases\nor conditions, VisionUnite has the best diagnostic performance in four diseases or conditions: Age-Related Macular Degeneration\n(AMD), Diabetic Retinopathy (DR), Drusen, and Glaucoma. Ophthalmologists perform better in three types of diseases or conditions:\nBranch Retinal Vein Occlusion, Cataract, and Myopia. For Cataract and Drusen, due to the small number of test samples, there may be\nsignificant differences in performance between different models. For AMD and DR, due to the tendency of ophthalmologists to provide\nincorrect disease staging, their performance is not as good as VisionUnite. Regarding diagnosis involving different signs, VisionUnite\nand ophthalmologists are also better than GPT-4V and Gemini Pro. As shown in Figure c1-c5, VisionUnite and ophthalmologists\nperform similarly on OCD (Optical Cup Disc). For FHE (Fundus Hemorrhages Examination) and Macular, VisionUnite performs\nbetter. For FBC (Fundus Boundary Color) and Vascular, ophthalmologists perform better. In terms of diagnostic relevance except for\nmacular, ophthalmologists are better than all other models, which also reflects the ability of ophthalmologists to explain the results."}, {"title": "Joint Diagnosis of Multiple Diseases or Conditions and Signs", "content": "When dealing with fundus images from patients suffering from multiple diseases, the overlapping or combined manifestations of these\nconditions can significantly complicate the diagnostic process7. Each disease may present with specific signs on fundus examination,\nsuch as hemorrhages, exudates, or abnormalities in the vascular architecture. However, when multiple pathologies coexist, these\nsigns can lead to a complex visual presentation that challenges even the most experienced ophthalmologists. For instance, diabetic\nretinopathy and arteriosclerotic Retinopathy both exhibit retinal vessel changes, but their specific impacts on the retina might slightly\ndiffer. Diabetic retinopathy typically shows more microaneurysms and hemorrhages, whereas arteriosclerotic retinopathy involves\nchanges in the retinal arterioles, such as narrowing and tortuosity as shown in Figure 15. Therefore, we explore the joint diagnostic\nperformance of the model and junior ophthalmologist for multiple diseases and multiple signs, including 15 diseases or conditions\nand 5 signs. As shown in Figure 7, the overall performance of VisionUnite is superior to GPT-4V and Gemini Pro in the presence of\nmultiple diseases in fundus images. Compared to ophthalmologists, the overall accuracy of VisionUnite is similar, but the diagnostic\nrelevance is lower. In the first two rounds, VisionUnite performs better than junior ophthalmologists, achieving a diagnostic accuracy of\n44.44% and 77.78% with diagnostic relevance of 3.333 and 3.0. Gemini Pro performs the worst and cannot accurately make diagnosis\npredictions in the first round, with a diagnostic relevance of only 1.222. The results of multi-sign diagnosis are shown in Figure 7\nd9-d16. Ophthalmologists perform best with 75% diagnostic accuracy and a diagnostic relevance of 3.375, followed by VisionUnite\nwith 8.33% diagnostic accuracy of 5 and a diagnostic relevance of 2.917. The performance of GPT-4V and Gemini Pro is worse,\nwith diagnostic accuracy of 35.42% and 12.5% respectively, and diagnostic relevance of 2.271 and 1.438. The results also reflect\nthat the ability of models to analyze images is not as good as that of humans, making it difficult to obtain a correct diagnosis when\nencountering fundus images with multiple signs."}, {"title": "Misdiagnosis Analysis in Healthy Conditions", "content": "We also investigate the misdiagnosis of various models and ophthalmologists in the face of healthy fundus images. We use\nmisdiagnosis rate and diagnostic relevance as evaluation indicators in this kind of scenario, where the misdiagnosis rate equals\n1 - diagnostic accuracy, and diagnostic relevance remains as defined earlier. Since healthy samples often outnumber abnormal"}, {"title": "Diagnostic Correction in Patient Interaction", "content": "We assume that an advanced vision-language model ought to possess the capability for diagnostic rectification. It implies the\ninherent ability of models to autonomously amend inaccuracies in their initial diagnosis upon receiving additional information\nduring a subsequent patient interaction. In this context, we focus on assessing the diagnostic precision of these models. Our\nevaluation encompassed a rigorous examination of the testing performance exhibited by VisionUnite, GPT-4V, and Gemini Pro.\nNotably, VisionUnite demonstrated superior corrective accuracy, which significantly surpassed GPT-4V and Gemini Pro. The superior\nperformance of VisionUnite not only highlights the potential of domain-specific large models to exhibit heightened sensitivity in\nproblem recognition within the medical field but also underscores their capacity for agile adaptation. Specifically, the ability of\nVisionUnite to swiftly recalibrate its responses in alignment with nuanced shifts in problem context, which we term as 'problem\nsensitivity', stands as a testament to its refined diagnostic acumen and adaptability. As shown in Figure 9, we calculate the proportion\nof correct answers in the second or third round for each model in the case of incorrect answers in the first round. Overall refers to\nanswering correctly in the second or third round. Round 2 refers to answering correctly in the second round, while Round 3 refers to\nanswering correctly in the third round. The results demonstrate that VisionUnite has an overall correction accuracy of 86.67%, which\nis 24% and 39% higher than GPT-4V and Gemini Pro respectively with the p-value less than 0.001. In the second round, VisionUnite\ncan perform much better than GPT-4V and Gemini Pro, with a second-round correction accuracy of 66.67% for VisionUnite, while\nGPT-4V and Gemini Pro can only achieve 28.04% and 20.69% respectively with the p-value also less than 0.001. We also evaluate the\ndiagnostic accuracy of each model for both correct and incorrect predictions as shown in Figure 10. The findings demonstrate that\nVisionUnite's performance in understanding problems and following instructions is equivalent to that of a junior ophthalmologist,\nindependent of whether its predictions are correct or incorrect. The details can be seen in the appendix."}, {"title": "Diagnostic Errors Analysis between Ophthalmologist and Large Vision-Language Models", "content": "To analyze the outputs of the models thoroughly, we investigate their diagnostic errors and contrast them with assessments from\nophthalmologists. We categorize these errors into two types: missed and incorrect errors. Missed errors pertain to incomplete yet\ncorrect diagnoses, while incorrect errors include entirely wrong diagnoses and partially correct diagnoses with additional, irrelevant\nerrors. Our goal is to evaluate the completeness and accuracy of responses, noting omissions or irrelevant inclusions. Additionally, we\nassess error severity, which is categorized as error-free, minor, or major errors based on the impact on clinical judgment and treatment\nspecificity. Minor errors encompass overlooked partial signs not critical to overall judgment or unnecessary diagnoses, as well as\ngeneralized treatment recommendations that lack specificity. Major errors include significantly incorrect responses. We also evaluate the\npotential for physical or mental harm resulting from the answers, grading potential health risks based on severity and likelihood.\nNo harm is considered error-free, mild or moderate harm is considered minor errors, and serious harm or deaths are considered major\nerrors. As shown in Figure 4, we analyze the diagnostic errors in five different situations: overall, single disease, multiple diseases,\nsingle sign, and multiple signs. Specifically, VisionUnite and ophthalmologists perform better than Gemini Pro and GPT-4V in various\nsituations. Overall, VisionUnite performs the best in the \"incorrect error\" dimension, achieving an error-free rate of 78.15%, which is\n4.82% higher than that of ophthalmologists with a corresponding p-value of 0.035. Ophthalmologists perform better in the \"missed\nerror\" dimension, achieving an error-free rate of 71.85%. At the same time, we find that incorrect errors are more associated with\nmajor errors than minor errors. Compared to missed errors, incorrect errors often lead to more serious consequences. However, the\nresults also indicate that there is still a certain gap between VisionUnite and ophthalmologists in the diagnosis of multiple diseases, and\nit also indicates that the understanding and reasoning abilities of models for images need to be further improved to reach the level of\nophthalmologists. The details of the classification criteria for diagnostic errors are in Table 2 in the appendix."}, {"title": "Diagnostic analysis of VisionUnite assisted junior ophthalmologist", "content": "To further explore the clinical decision-support capabilities of VisionUnite, we assess its impact on the diagnostic performance of\nprimary ophthalmologists. We specifically evaluate the improvements in diagnostic accuracy and efficiency when VisionUnite assists\nophthalmologists in the initial round which includes 180 questions. Our analysis shows that VisionUnite facilitates a 26.98% reduction\nin diagnostic time and a 29.44% increase in diagnostic accuracy overall illustrated in Figure 5. Notably, the use of VisionUnite leads\nto a remarkable 50% improvement in diagnostic accuracy and a 33% reduction in diagnostic time in cases of diabetic retinopathy.\nFurthermore, we examine the impact of VisionUnite on diagnoses involving five distinct physical signs detailed in Figure 5 i8-i12.\nThe assistance is most significant for fundus hemorrhage exudates (FHE) and optic disc cupping (OCD), where VisionUnite helps\nimprove diagnostic accuracy by 39.02% and 35.49%, respectively, while also reducing diagnostic time by 35.63% and 24.25%."}, {"title": "Consistent Interpretation of Visual and Language Features", "content": "In our final analytical phase, we scrutinize the coherent integration of visual and textual elements within large vision-language models,\nan endeavor aimed at assessing the alignment between the textual descriptions and the corresponding visual data. Our investigation\naddresses the potential for 'illusory discrepancies' within the output of these sophisticated models. Specifically, we list the outputs\nof VisionUnite against those from other large vision-language models, with a keen focus on discerning any textual outputs that\nmay depict features not present within the associated imagery. For instance, in cases where the diagnosis pertained to age-related\nmacular degeneration, and the imagery exclusively showcases the presence of drusen, we should evaluate whether the textual narrative\nunjustifiably extended the diagnosis to encompass additional manifestations such as hemorrhaging and exudation. Such a detailed\nexamination is pivotal for understanding and mitigating the tendency of these models to over-interpret or misalign textual descriptions\nwith their visual counterparts, thereby ensuring a more accurate and reliable diagnostic output. Meanwhile, we also present a series of\nevaluation criteria for the responses of the models and junior ophthalmologists in this section. More cases from Junior Ophthalmologist\nand Large Vision-Language Models including VisionUnite, GPT-4V, and Gemini Pro can be seen in the extended data Figure 12-15\nand the corresponding expert evaluation is in Figure 16."}, {"title": "Discussion", "content": "The diagnostic performance of VisionUnite in ophthalmic diseases has undergone thorough validation across diverse clinical scenarios,\nencompassing open-ended multiple disease diagnoses, clinical interpretation, and interactive inquiry from the patients. The integration\nof VisionUnite is poised to significantly elevate the precision of initial screenings for ophthalmic diseases, consequently enhancing\nthe diagnostic efficiency of healthcare professionals, which is a particularly pivotal advancement in underdeveloped regions or those\nplagued by scarce ophthalmic services. Experimental findings underscore that VisionUnite's capabilities in image analysis, disease\ndiagnosis, and clinical explanations have achieved parity with ophthalmic junior doctors and, in certain instances, even surpassed their\nperformance in diagnosing specific diseases. As attested by ophthalmologists, VisionUnite outshines general large vision-language\nmodels like GPT-4V and Gemini Pro, not only in delivering more accurate disease diagnoses but also in furnishing clear diagnostic\nfoundations, which is an aspect often lacking in typical large vision-language models. In specific clinical applications, VisionUnite\nproves adaptable for patient inquiries while concurrently assisting doctors in optimizing diagnoses and facilitating ophthalmic training.\nBesides, VisionUnite expedites doctors' disease diagnosis, offers question-and-answer services for patients to address their queries,\nand delivers tailored responses. For healthcare professionals, VisionUnite serves as a valuable auxiliary diagnostic tool, leveraging\nextensive ophthalmic knowledge and adeptness in disease diagnosis. Different from CT or X-ray, fundus photography typically lacks\naccompanying textual descriptions. Traditionally, the patient's understanding of fundus imaging results primarily derives from doctors'\nverbal explanations, which can sometimes lead to inadvertent omissions. However, the introduction of VisionUnite offers a robust\nsolution by generating comprehensive fundus photography reports. The integration of verified VisionUnite reports not only aids\ndoctors in the diagnostic process but also mitigates the risk of potential oversights.\nWhile VisionUnite demonstrates diagnostic capabilities comparable to those of a junior ophthalmologist, certain limitations warrant\nconsideration. Notably, the clinical interpretation of fundus images by VisionUnite can at times be overly broad, hindering the delivery\nof more precise information and consequently constraining performance enhancements in subsequent second-rounded diagnoses.\nFurthermore, the current proficiency of VisionUnite is confined to processing fundus images due to constraints in available data\nmodalities. Future endeavors aim to expand its capabilities to include additional ophthalmic modalities such as FA, OCT, OCTA, and\nothers. We also believe that the augmentation of data and modalities during the training process will enhance VisionUnite's ability\nto handle diverse modalities and bolster its diagnostic performance. Another limitation lies in the current granularity of sign-level\nlabels, as we have only categorized fundus features into six sign labels. It has led to omissions in predictions for images exhibiting\nmultiple sign labels. To address the limitation, we plan to involve more fine-grained sign-level labels to better discern between various\nfundus images and enhance diagnostic accuracy. As computational power advances and more high-quality annotated data becomes\navailable, we aspire for VisionUnite to extend its diagnostic reach to encompass a broader spectrum of rare diseases. Simultaneously,\nwe envision its role as a valuable tool for ophthalmologists, aiding in identifying and classifying lesions and diseases. By introducing a\nmore extensive array of sign-level labels, we also believe VisionUnite has the potential to unveil previously elusive links between\nsigns and diseases. In addition, it could contribute to the identification of novel diagnostic evidence, fostering the evolution of\ndiagnostic standards and protocols. Lastly, we expect VisionUnite to unveil connections between ocular features and systemic diseases.\nContemporary research increasingly proposes the utility of ocular examinations for early screening of systemic conditions, such as\nAlzheimer's disease. Leveraging the heightened visual encoding capabilities of large vision-language models and incorporating data\non systemic diseases, we envision VisionUnite bridging ophthalmic data with information from other body regions. It is poised to\nexpedite the revelation of hitherto undiscovered connections between ocular features and diseases manifesting in other organs."}, {"title": "Methods", "content": "The architecture of VisionUnite To enable the model to achieve open-ended prediction and have user interaction and clinical\ninterpretation capabilities, we propose a large vision-language foundation model with clinical knowledge. Illustrated in 1 (c), our\nproposed VisionUnite comprises a Transformer-based vision encoder, a vision adapter tailored for signs-level classification, a vision\nprojector for aligning visual embeddings and text tokens, and a substantial language model fine-tuned on llama-7B. The large language\nmodel is different from the vision model in that it can achieve open-ended prediction. By leveraging the combination of image-text\npairs and extensive dialogue information, VisionUnite excels in responding to user questions based on visual data. We introduce\nvisual features with sign-level features into the model, which improve its ability to understand images and further enhance its clinical\ninterpretation ability. This design can extend its ability to analyze fundus images for medical diagnosis. Our model is also designed for\nmulti-round dialogues, which helps the model follow user instructions and achieve better problem-understanding ability. Harnessing\nthe synergy of image-text pairs and multi-round dialogue information, VisionUnite demonstrates proficiency in addressing user queries\ngrounded in visual data, extending to the capability of diagnosing fundus images. The holistic architecture of VisionUnite enables\nnuanced and contextually informed responses, ensuring a comprehensive understanding of the inquiries based on visual elements.\nWe leverage the EVA02 model25 with the CLIP Vision Encoder to collectively serve as the Transformer-Based Vision Encoder. Our\nconfiguration incorporates 12 layers of EvaBlock25 within the original vision Transformer Block26. Notably, in our design, the GELU\nactivation27 in the original vision Transformer Block is replaced with the more advanced SwiGLU28. It is also widely used in PaLM29,\nLLaMA\u00b9\u00b9, and LLaMA-Adapter30 to enhance the performance of the FFN (position feedforward network) layer in the Transformer\narchitecture. Specifically, images, text descriptions, and questions are fed into corresponding encoders to form visual embeddings,\ntext embeddings, and dialogue tokens during the training phase. In this phase, we use contrastive learning to supervise the encoding\nformation of visual embeddings and text embeddings. Afterward, we use the vision adapter to perform secondary encoding and\nsign classification on visual embeddings, which form six corresponding sign category embeddings. Then we concatenate the visual\nembeddings and sign category embeddings together as inputs to the vision projector. For the text features, we embed the corresponding\nsign categories (CLS tokens) and concatenate them to form text features with the dialogue tokens based on the prediction of the signs.\nThe vision encoder in our VisionUnite model facilitates the extraction of visual embeddings, serving as input for the vision adapter\nto generate classification predictions at the sign level. The prediction of signs by VisionUnite plays a pivotal role in improving both\ndiagnostic accuracy and efficiency. By accurately identifying and localizing signs such as lesions in the images, the model narrows the\ndiagnostic possibilities and guides the model toward more targeted investigations. It not only optimizes the diagnostic process by\nreducing the need for too broad differential diagnoses but also enhances accuracy by focusing on the most relevant clinical signs. In\naddition, the model is designed with the flexibility to accommodate an expanding array of sign categories, enhancing its adaptability\nfor future developments. To further refine VisionUnite, we plan to increase the universality of the model by supporting a wider range\nof sign-level labels. The expansion is aimed at improving the granularity of diagnostic evidence and enhancing the interpretability\nof the outputs from the models. VisionUnite can recognize six primary signs, which cover a wide range of ophthalmic diseases.\nThis kind of capability aligns closely with the diagnostic process used by medical professionals, who often start their diagnosis by\nidentifying specific signs before narrowing down to a more precise disease identification. Furthermore, the ability of VisionUnite\nto leverage these predicted image signs effectively distinguishes it from other ophthalmic deep learning models. By focusing on\nsign-level features, VisionUnite ensures that the diagnostic suggestions it provides are both precise and clinically relevant, offering a\nrobust tool that supports clinicians in making informed decisions. Following the acquisition of predicted sign-level data, VisionUnite\nseamlessly employs the vision projector to synchronize visual embeddings with dialogue tokens. It entails concatenating the visual\nembedding with the class embedding, which is then fed into the vision projector. The alignment process involves matching the\nresulting feature dimensions with CLS (class) and dialogue tokens via concatenation, with the CLS token encapsulating the predicted\nsign-level feature. The multimodal features, which combine visual embeddings, CLS tokens, and dialogue tokens, are the inputs for\ntraining the final text generation response within the framework of the large language model. Overall, the integration of state-of-the-\nart components contributes to the overall efficacy and robustness of VisionUnite, positioning it as a cutting-edge vision-language model.\nThe training objectives of VisionUnite In the architecture of our proposed VisionUnite, we have crafted three distinct training\nobjectives to enhance convergence: image-text contrastive learning, classification supervised learning, and text-generation supervised\nlearning. The utilization of image-text contrastive learning facilitates the refinement of visual encoders, aiding them in more effectively\naligning fundus image features. Meanwhile, the application of classification-supervised learning contributes by furnishing accurate\nfeature categories for both visual embeddings and dialogue tokens. We utilize the accuracy to guide the model training process,\nenhancing its overall performance. Finally, text-generation supervised learning plays a role in guiding the output of the language\nmodel, which is pivotal for achieving accurate and open-ended disease diagnoses. Unlike previous vision models, large language\nmodels are trained on vast amounts of diverse textual data. The extensive textual data enables them to generate context-aware responses\nand detailed explanations that are coherent and clinically relevant. We can achieve open-ended disease diagnosis using the generated\ntext from language models. In contrast, the prediction categories of vision models are limited and concentrated on specific diseases.\nCollectively, these three training objectives synergistically contribute to the robust convergence and performance of VisionUnite,\nmaking it a versatile and effective tool in the field of ophthalmic diagnostics.\nImage-text contrastive learning: To attain seamless alignment between image and text features, we employ image-text contrastive\nlearning. In particular, we leverage the CLIP loss methodology to quantify the similarity between image embeddings and text"}, {"title": "embeddings", "content": "embeddings", "below": "n$L_{CLIP"}, "L_{img}+L_{text})/2$ (1)\n$L_{img} = -\\frac{1}{N}\\sum_{i=1}^{N}[t_i log(P_{img,i})"], "learning": "In facilitating the acquisition of sign-level features, we employ conventional classification\nlearning to guide the training of VisionUnite. Notably, we embrace multi-label classification while acknowledging that each sample\nmay encompass more than one predicted category. This approach aligns more closely with the intricacies of"}