{"title": "Towards Inference-time Category-wise Safety Steering for Large Language Models", "authors": ["Amrita Bhattacharjee", "Shaona Ghosh", "Traian Rebedea", "Christopher Parisien"], "abstract": "While large language models (LLMs) have seen unprecedented advancements in capabilities and applications across a variety of use-cases, safety alignment of these models is still an area of active research. The fragile nature of LLMs, even models that have undergone extensive alignment and safety training regimes, warrants additional safety steering steps via training-free, inference-time methods. While recent work in the area of mechanistic interpretability has investigated how activations in latent representation spaces may encode concepts, and thereafter performed representation engineering to induce such concepts in LLM outputs, the applicability of such for safety is relatively under-explored. Unlike recent inference-time safety steering works, in this paper we explore safety steering of LLM outputs using: (i) category-specific steering vectors, thereby enabling fine-grained control over the steering, and (ii) sophisticated methods for extracting informative steering vectors for more effective safety steering while retaining quality of the generated text. We demonstrate our exploration on multiple LLMs and datasets, and showcase the effectiveness of the proposed steering method, along with a discussion on the implications and best practices.", "sections": [{"title": "Introduction", "content": "With the growing accessibility of large language models and conversational agents, there is an increasing focus on how to make these models safer while retaining helpfulness. Most LLMs undergo extensive alignment training whereby models are trained to \u2018align' their behavior with human preferences. Such alignment techniques require large human-annotated or synthetically-generated training datasets and immense compute in order to perform Reinforcement Learning with Human feedback (RLHF) [Bai et al., 2022], with AI Feedback (RLAIF) [Lee et al., 2024b] or supervised fine-tuning (SFT) among others. While the resulting \u2018aligned' models are considerably less harmful than unaligned counterparts, even aligned models can be compromised to elicit harmful responses [Carlini et al., 2024]. Furthermore, there is evidence that once these aligned models are fine-tuned for downstream tasks, they may lose their alignment and can be easily made to spew harmful outputs [Qi et al., 2023, Kumar et al., 2024]. Given the fragility of these alignment methods, there is a need to have more modular, plug-and-play-type safety steering methods, such as inference-time steering or alignment. Furthermore, in cases where safety and moderation policies may need to be updated on the fly, it is infeasible to re-train LLM alignment from scratch given the scale of resources required for training. In such cases, given white-box access to the LLM, can we steer LLM generations to safety using some gradient-free, inference-time steering method?\nIn this work, we explore inference-time safety steering of LLMs, without any additional training or fine-tuning. We do this by computing steering vectors that correspond to the concept of 'harmless-ness'\u00b3 and intervene on intermediate layers using this vector during inference to steer the generation. Unlike previous work in this direction [Rimsky et al., 2023, Turner et al., 2023, Arditi et al., 2024], we focus on (i) category-wise steering, whereby we compute steering vectors for specific categories of harm for additional fine-grained control; and (ii) additional refinement of steering vectors by investigating different ways of extracting informative signals from model activations in order to steer. Following previous work [Zhou et al., 2024, Li et al., 2024b], our key assumption is that over the course of the pre-training and instruction-tuning stages, the LLM has learnt enough information about safety, and the steering step essentially guides the LLM to sample from specific subspaces that are 'safe'. We propose category-wise inference time steering via activation engineering where the categories are various critical safety risks or hazards that arise from human-LLM interactions. Our method uses a single forward pass at inference time, during which the model activations from strategic hidden states are steered from 'unsafe' regions to 'safe' non-refusal regions. This allows the model to deflect harmful prompts by generating a harmless response."}, {"title": "Related Works", "content": "Recently there has been a lot of effort in understanding the inner workings of large language models from the perspective of mechanistic interpretability. Building on the idea of the linear representation hypothesis for LLMs [Park et al., 2024], that says concepts and features in LLMs may be represented along linear directions in the representation space, recent work has tried extracting weights or regions to manipulate the degree of these features or concepts [Cunningham et al., 2023]45. Following this there have been efforts in probing language models and performing activation engineering to manipulate behaviors or elicit latent knowledge [Burns et al., 2023, Marks and Tegmark, 2023]. Given this background, here we mostly focus on recent work relevant to the application of mechanistic interpretability in safety steering.\nActivation Engineering & Model Editing. Several recent efforts have used activation manipulation [Zou et al., 2023] for steering language model outputs, such as by using pairs of input prompts [Turner et al., 2023, Rimsky et al., 2023], spectral editing during inference-time [Qiu et al., 2024], layer-specific editing for defending against jailbreaks [Zhao et al., 2024], identifying and editing sub-space for detoxifying models [Uppaal et al., 2024], etc. The idea of task arithmetic has been explored for enabling models to perform tasks or even enable multi-task performance via task-specific vectors [Ilharco et al., 2023]. This is, in theory, similar to the concept of safety steering using vectors and vector geometry, such as with in-context vectors Liu et al. [2024]. Given that steering models may negatively impact the quality of generation, some recent work controls for the degree of degradation while retaining benefits [Stickland et al., 2024].\nLinear Probes. Linear probes are essentially small linear classifiers [Li et al., 2024a, Lee et al., 2024a, von R\u00fctte et al., 2024] or regressors [Kossen et al., 2024] trained on the model activa-tions. Such probes are capable of capturing and differentiating behaviors in LLMs such as truthful-ness/factuality [Marks and Tegmark, 2023, Mallen and Belrose, 2024], toxicity [Lee et al., 2024a, Wang et al., 2024a], etc. Although these are largely cost-effective methods, one of the disadvantages of linear probe methods lie in requiring explicitly labelled datasets and additional training of the linear probe layers or modules.\nSparse Auto-Encoders. Sparse Auto-Encoders (SAE) [Lieberum et al., 2024, Cunningham et al., 2023] or dictionary learning methods are effective techniques for unsupervised explorations of"}, {"title": "Category-wise Safety Steering for LLM Outputs", "content": "In this section, we first provide a brief overview of the preliminary concepts and background to familiarize readers on the problem. Then we describe the two-step steering methodology we use to perform category-wise safety steering of model outputs at inference time. Our overall framework for computing steering vectors and performing the subsequent steering is shown in Figure 1."}, {"title": "Preliminaries", "content": "In our work, we investigate generative language models, especially recent LLMs that are capable of generating text responses based on a text prompt input by the user. We focus on transformer-based language models [Vaswani, 2017], with several layers, i.e., transformer blocks, and several billion parameters. Typically, LLMs are pretrained with massive internet scale corpora of text for the task of text completion, and then further instruction-tuned to understand and follow user instructions effectively. Most recent LLMs also undergo safety training through techniques such as reinforcement learning with human feedback (RLHF). We denote the LLM being evaluated as M. Given a user prompt $x_{user}$ and an optional system prompt $x_{sys}$, the model response is the output of the model, $response = M(x_{user}, (x_{sys}))$, typically obtained through decoding where each token is sampled from a distribution via some sampling method $s(.)$ and temperature $\u03c4$. Given the massive training performed on these models, LLMs are often viewed as approximations of the data on which they were trained, and larger models are able to perform tasks that were not part of the training regime [Wei et al., 2022].\nMuch work in understanding and interpreting language models have posited that LLMs may represent concepts linearly as directions in the representation space [Park et al., 2024]. Recent work has also explored how model activations may encode concepts. Some efforts use SAE-based methods for disentangling these features or concepts [Cunningham et al., 2023]6, but these methods require additional training data to learn massive SAEs [Gao et al., 2024]. Unlike these works, in this paper, inspired by activation engineering efforts that explore concepts via LLM activations, we hypothesize that for the purposes of inference-time safety steering, vector differences in the activation space are sufficient to obtain steering signals for safety steering of an LLM."}, {"title": "Computing Category-specific Steering Vectors", "content": "We describe two methods for obtaining the category-wise safety steering vectors: (i) unsupervised, and (ii) guided."}, {"title": "Unsupervised Steering Vectors", "content": "In this step, we aim to capture how the model activations differ between harmful text versus harmless text prompts. To achieve this, we need to have white-box access to the model we aim to steer, M. For each input x in the dataset of unsafe texts, $x \u2208 D_{unsafe}$, with category $c_i \u2208 {C_1, ..., C_k }$ the list of harm categories, we perform a forward pass over M and record all activations from all layers. Specifically, we record activations at attention, MLP, residual stream, and at the block output level. We do the same with a forward pass using the dataset of paired safe texts $D_{safe}$. We obtain the safety steering vector for category $c_i$ by taking the mean difference of these activations:\n$\\frac{w_i^{(l)} = \\frac{1}{\\vert D_{safe} \\vert} \\sum_{j=1}^{\\vert D_{safe} \\vert} [act(x_{safe}^{(l)})] - \\frac{1}{\\vert D_{unsafe} \\vert} \\sum_{j=1}^{\\vert D_{unsafe} \\vert} [act(x_{unsafe}^{(l)})]}{}$   (1)\nNote that we compute $w_i^{(l)}$ for all L layers, and we omit layer notations in the equation for simplicity. We compute these steering vectors for all the categories we use in our experiments. Out of the four types of activations we record, following prior work [Li et al., 2024a, Arditi and Obeso, 2023], we use the attention activations in all our experiments."}, {"title": "Guided Steering Vectors", "content": "Most recent models already undergo some degree of safety training whereby models learn to refuse to respond to harmful queries or abstain from engaging with the user query. Since this is a behavior we would want to encourage, in this guided setting we also consider the text completions of the model to filter out which intermediate representations actually resulted in harmful output. In order to do this, we first input each prompt xp into the model M and extract the activations7 from all layers for every token that is generated. We get each layer activation by averaging out over all tokens generated. We perform this extraction for both safe and unsafe datasets and store these activations. We also store the text generated by M during this process, since this will be used to evaluate whether each corresponding activation is 'safe' or 'unsafe'. Detailed pseudo-code for this extraction is shown in Algorithm 1. Once this extraction step is done, we iterate over the saved activations and the corresponding generated text, and evaluate the safety label of each generated text using a safety labeler model S (Algorithm 2). In our experiments, we use OpenAI's GPT-4 to perform this labeling but this can be swapped with any other safety classifier, such as Llama Guard [Inan et al., 2023]. The exact prompt we use for this is in Appendix C. Based on the 'safe' or 'unsafe' label for each completion, we add the corresponding activation into either the 'safe activations' bucket or 'unsafe"}, {"title": "Pruned Activations for Enhanced Steering Signals", "content": "For the unsupervised setting, we also experiment with a simple pruning method to filter out noisy steering signals. To do this we use the pairwise mean differences between harmful and harmless activations, compute the median of the L2 norms of such differences, and retain only the differences with norms that are greater than the median, i.e., top 50% of the pairwise differences. In the 'pruned activation' setting of the experiments, we compute the steering vector using only these mean differences. The rationale behind this is that we would want to retain only the activation differences that provide the most signal, while ignoring ones that are not that significant, i.e., with lower L2 norms. Since the topics of the harmful and harmless text pairs are often similar, a smaller difference in their activations might mean that the LLM cannot effectively disentangle the harm feature from the content feature, therefore having similar activations. Hence these specific activation differences may not be informative enough for the steering."}, {"title": "Generation with Steering Vectors", "content": "Once we have the steering vector $w_i^{(l)}$ computed for each layer $l\u2208 {0, 1, ..., L}$ and category $C_i \u2208 {C_1, ..., C_k }$, we can simply retrieve these during inference time to steer model outputs towards safe regions in the latent space. To do this at, e.g., layer l and category $c_i$, we simply add the steering vector to the self-attention weights at layer l at all token positions during forward pass, as shown in 2, where att are the self-attention weights at layer l, $w_i^{(l)}$ is the steering vector (output from Algorithm 2), and m is a scalar multiplier to control the degree of steering.\n$O_{attn}^{(l)} = attn + m \u00d7 w_i^{(l)}$  (2)\nNote that we use the same layer for computing the vector and for the intervention; this is intentional since using steering vectors from other layers in layer I may affect the semantics as language models have been shown to process input information and semantic differently across different layers - deeper layers may hold more semantic concepts whereas earlier layers learn structures of the tokens and token relationships. All the models we use in our experiments have 32 layers, numbered from 0-31."}, {"title": "Methodology and Experimental Settings", "content": ""}, {"title": "Datasets", "content": "We use the following datasets: CategoricalQA [Bhardwaj et al., 2024], BeaverTails [Ji et al., 2023], and Alpaca Instructions [Taori et al., 2023b].\nCategoricalQA (CatQA) This is a dataset of harmful questions, dividing into 11 categories, with 50 samples in each category. Since this dataset only has harmful questions, we generate category-specific harmless counterparts using GPT-4. For each harmful question in CatQA, we use a prompting strategy similar to Bhattacharjee et al. [2024], in order to generate pairwise harmless questions with the same topic as the the original harmful question. The exact prompt we use for generation is in Appendix B. Some examples of pairwise harmful and generated harmless prompts or questions are in Table 7. Out of the 11 categories in CatQA, we choose 3 representative categories for our experiments, due to resource constraints.\nBeaverTails This is a massive dataset of 330k samples consisting of user prompts and LLM responses. Each sample is labeled as either safe or unsafe, using multiple labels with 14 harm categories. Due to resource constraints, we use 3 representative categories, and 1,500 samples per category, from the train split, for extracting the activations. For steered generation we use the test\nAlpaca Instructions For experiments involving a generic harmless dataset, we use the prompts from Alpaca Instructions. While these may have varied topics and style relative to the harmful counterparts from CatQA, this allows us to investigate whether steering the generation towards an area of the latent space which corresponds to more generic notions of harmlessness is beneficial over category-specific."}, {"title": "Evaluation Metrics", "content": "For inference time safety steering, we would want the generated text to be (i) safe, and (ii) high quality (i.e. helpful). Ideally this method of steering should reduce the percentage of unsafe responses generated by the LLM, while maintaining the utility or quality of the generated text. In theory, these two objectives would be in a trade-off where the extremes could be that the LLM either generated gibberish and therefore scores low on text quality metrics, or the LLM follows harmful instructions in the prompt and generates unsafe text while scoring high on text quality. For (i), we use drop in percentage of unsafe responses (%UR) for steered generation over naive generation as the metric. Similar to the guided setting, we use OpenAI's GPT-4 as the safety classifier. The detailed prompt for the GPT-4 classification is in Appendix C. In all experimental tables, the drop in % unsafe responses is depicted using the following notation: $S(M(D_{test})_{naive}) \\rightarrow S(M(D_{test})_{steered})$, where the term on the left is %UR for naive or completion using model M for $D_{test}$. The term on the right is the %UR when the model M generates completions with the proposed steering method. Ideally we would want this drop in %UR to be as large as possible. For evaluating text quality, we use two scores, helpfulness and coherence as measured by a fine-tuned reward model. More specifically, we use NVIDIA's Nemotron-340B reward model [Wang et al., 2024c] 8 for obtaining these scores."}, {"title": "Results and Discussion", "content": "In order to explore the effectiveness of our proposed category-specific steering method, we aim to answer the following research questions.\nRQ1: Does category-specific steering help reduce harmfulness while retaining text quality?\nWe show the results of steering with category-specific vectors for both Llama2-7B and Llama3-8B in Table 1. For CatQA we show results for three representative categories in all our experiments: {'Adult Content', 'Hate Harass Violence', 'Physical Harm'}. We report the drop in %UR from naive to steered generation as the main metric for understanding how the steering affects the degree of safety\nat inference time. We see that while the %UR are very high for naive generation, steering does help in reducing this. Interestingly, our proposed method works better for Llama3-8B than Llama2-7B, and overall the performance varies across different harm categories. As expected with most steering methods, we do see a trade-off between the reduction in unsafe responses and the quality of the generated text in terms of helpfulness and coherence scores. These scores are also represented to indicate the change from naive to steered generation, i.e., $score(naive) \\rightarrow score(steered)$.\nRQ2: Does pruning help improve steering performance over the vanilla unsupervised setting?\nOur aim is to explore if the pruning method introduced in Section 3.2 helps in getting better, more informative signals for steering the generation. We show the main results for this in Figure 2. We see that for all three categories, for both LLMs, using pruned activations results in better safety scores, i.e. lower %UR. Interestingly we also see that even with this improvement in safety scores, the text quality is often retained or even improved over using all activations. This is especially the case for Llama3-8B. This may imply that even a simple pruning method to remove noise helps to improve the performance trade-off between safety and text quality, in the absence of any external supervision or signal.\nRQ3: Does steering towards regions of \u2018generic' harmless help over using category-specific harmless data?\nThe motivation for this experiment is that we may often want the LLM to steer its generation towards more generic safe outputs when prompted with an unsafe query, instead of generating a category specific response. For example, the LLM may choose to refuse to answer the unsafe user query, instead of staying withing the topic of the category but dodging the unsafe query. We explore whether this is a better strategy for safety steering, and therefore try to steer generations using a steering vector computed from harmful activations of one category and activations of generic harmless data. For this experiment, we again consider the unsupervised setting for extracting activations. We show results for both CatQA and BeaverTails dataset. For CatQA, instead of using the GPT-4 generated harmless counterparts to compute the steering vector, we use 'generic' harmless data from the Alpaca Instructions dataset. For BeaverTails, the dataset already contains a generic \u2018safe' category which we use as the harmless counterpart for computing the steering vectors. Results for this experiment with CatQA and BeaverTails are presented in Tables 2 and 3 respectively. For CatQA, we see that when we use generic harmless data for activations, the steering is more effective in reducing the %UR, while mostly retaining or sometimes even improving the generated text quality in terms of helpfulness and coherence. This is promising since this may imply that generic harmless instruction data can be used effectively in our framework and there may not a need to generate closely paired\nRQ4: Does the additional guidance in the 'guided' setting improve steering performance?\nFinally in this experiment we explore whether some additional signal regarding whether extracted activations result in 'safe' or 'unsafe' generations help in improving quality/informativeness of the steering vector, and hence the quality of steered generations. We show results for CatQA in Table 4 and for BeaverTails in Table 5. For CatQA, compared to Table 1, we see that while using guided activations help in reducing the %UR, helpfulness and coherence get affected, implying the generated text may be of poor quality. Interestingly, for BeaverTails, using guided activations helps significantly for Llama3-8B, where alongside reducing %UR to 0, the helpfulness scores also improve and coherence stays consistent with naive generation."}, {"title": "Conclusion and Future Work", "content": "In this work, we explore category-specific inference time safety steering for large language models. We do this by extracting model activations for harmful and harmless data in two ways: (i) unsuper-vised, and (ii) guided. In the latter, we filter out activations on the basis of whether the extracted activation results in an unsafe text, as labeled by an external safety classifier. Steering vectors are computed from these harmful and harmless activations and stored for use during inference. During inference these vectors are used to intervene on model attention weights in the specified layer in order to steer the generation towards regions of 'safety' even when the user prompt is unsafe.\nWhile our exploration provides informative results and best practices for safety steering using model activations, there are several directions in which further exploration can be done. First, we specifically used attention activations to perform the steering. We do this following prior work as well as based on our preliminary experiments that showed harmful and harmless activations showed the most separability in the attentions out of all types of activations including: MLPs, residual stream, attentions and block outputs. Future work may look at other types of activations or combinations of activation types. For pruning the unsupervised activations, we used a simple thresholding approach with the L2 norms. Given that even this simple method helped significantly future work may look at better or more sophisticated ways to perform this pruning and potentially get even cleaner steering signals without any external safety classifier. When it comes to controlling for text quality, in our work, we do not optimize for text quality in any way. In order to get better trade-off values between the safety scores and the quality of generated text, future work could explore ways to add additional constraints to the steered generation."}, {"title": "Dataset Statistics", "content": "Here we describe the categories of the three datasets we use.\nCategorical Harmful QA (CatQA). This is a dataset of 550 harmful questions across 11 harm categories. Each harm category has 50 questions. The full list of harm categories is: {Illegal Activity, Adult Content, Child Abuse, Hate/Harass/Violence, Malware Viruses, Physical Harm, Economic Harm, Fraud/Deception, Political Campaigning, Privacy Violation Activity, Tailored Financial Advice}. Categories in bold are the ones we use for our steering experiment. Each category is also divided into 5 sub-categories with 10 questions in each. For more details on the sub-categories see Bhardwaj et al. [2024].\nBeaverTails. We show the dataset statistics for the BeaverTails 330k dataset in Table 6. This dataset has 14 harm categories along with a generic 'safe' category as well. Categories in bold are ones we use in our experiments. For the purposes of out experiments we use 1,500 samples from the train split during extracting activations. Steering is performed on first 150-200 samples of the test split for each category."}, {"title": "CatQA Harmless Counterpart Generation", "content": "As mentioned in Section 4.1, we use GPT-4 to generate the category-specific harmless counterpart data for the harmful questions in CatQA. The exact prompt we use for the generation is:\nThe following question is a harmful question. Make minimal changes to it so that the content of the questions stays the same as much as possible, but it is now harmless.\nHarmful question: {$X_p$}\nOutput only the modified input.\nHere xp is a harmful question from CatQA dataset. Examples of generated examples are in Table 7."}, {"title": "Safety Evaluation Prompt", "content": "As mentioned in Section 3.2.2 and Section 4.2, we use GPT-4 for evaluating safety of generated text. The exact prompt we use for this is:\nYou are a safety evaluator for chatbot outputs. For the given question, please evaluate whether the response is 'safe' or 'unsafe'. If you are unsure, respond with 'unsure'. Do not output anything other than the three options.\nQuestion: {$x_p$}\nChatbot Response: {$X_{out}$}\nEvaluation:"}]}