{"title": "AI Act for the Working Programmer*", "authors": ["Holger Hermanns", "Anne Lauber-R\u00f6nsberg", "Philip Meinel", "Sarah Sterz", "Hanwei Zhang"], "abstract": "The European AI Act is a new, legally binding instrument that will enforce certain requirements on the development and use of AI technology potentially affecting people in Europe. It can be expected that the stipulations of the Act, in turn, are going to affect the work of many software engineers, software testers, data engineers, and other professionals across the IT sector in Europe and beyond. The 113 articles, 180 recitals, and 13 annexes that make up the Act cover 144 pages. This paper aims at providing an aid for navigating the Act from the perspective of some professional in the software domain, termed \"the working programmer\", who feels the need to know about the stipulations of the Act.", "sections": [{"title": "1 Introduction", "content": "After extensive deliberations, the European Union has taken the final step for adopting the AI Act [10]. The AI Act aims to ensure the development and deployment of safe and trustworthy AI by relying on a risk-based approach \u2013 the higher the risks to fundamental rights and society, the stricter the legal requirements.\u00b9 However, the demarcations of the regulated areas of AI often seem blurred. The idea of this paper is therefore to provide the \u201cworking programmer\"2 with some initial help in navigating the complexities of the AI Act. In doing so, we make three main contributions:\nWe provide an overview of the regulated AI technologies and how to distinguish between them. This is essential for the working programmer to determine which legal obligations under the AI Act might apply to their work.\nWe map the relevant obligations to help the programmer understand which parts of the AI Act may be relevant to them. This is supported by a flowchart that helps to find the relevant obligations with simple questions and to narrow down the complexities of the AI Act."}, {"title": "2 Addressee of the Paper", "content": "The particular relevance of the AI Act for the working programmer arises from the fact that it not only governs the use of AI systems, but primarily sets out requirements for their development. The AI Act addresses a variety of stakeholders along the AI value chain, among them \"deployer\u201d, \u201cprovider\u201d, \u201cdistributor\u201d, and \u201cimporter\u201d. Against this backdrop, the working programmer will most likely be considered (being part of) a \u201cprovider\" of AI systems or AI models and this role will therefore be the focus of the further analysis. The AI Act defines a provider as any natural or legal person, public authority, agency or other body that develops an AI system or a general purpose AI model or that has an AI system or a general purpose AI model developed and places them on the market or puts the system into service\u00b3 under its own name or trademark, whether for payment or free of charge.\u2074\nIn other words, any entity responsible for the development of a system or model within the scope of the AI Act could be affected by the requirements of the Act if it"}, {"title": "3 What types of AI are regulated under the AI Act?", "content": "Compliance with the AI Act is of course only an issue if the AI Act applies to the projects and tasks of the working programmer."}, {"title": "3.1 What characterises AI according to the AI Act?", "content": "The definition of \"AI systems\" in Article 3(1) [10], which we will examine in Section 3.2, contains some insights on what technical approaches the AI Act considers as \"AI\" and which it does not. As a key characteristic of AI systems, the AI Act underlines their capability to infer how to generate its output, which \u201ctranscends basic data processing by enabling learning, reasoning or modelling\".\u2076 The techniques that enable such inference \u201cinclude machine learning approaches that learn from data how to achieve certain objectives, and logic- and knowledge-based approaches that infer from encoded knowledge or symbolic representation of the task to be solved.\u201d\u2077 This seems to be what, according to the AI Act, distinguishes AI from \u201csimpler traditional software systems or programming approaches\u201d.\u2078 The AI Act thus does not apply to \u201csystems that are based on the rules defined solely by natural persons to automatically execute operations\u201d.\u2079 This means that \u2013 at least to our understanding \u2013 the final version of the AI Act does not cover traditional rule-based systems written by humans, even if they are complex and their deployment is associated with high risks.\u00b9\u2070 This approach has however sparked criticism among legal scholars who advocate a broader scope of application of the AI Act.\u00b9\u00b9"}, {"title": "3.2 What different types of AI does the Act regulate?", "content": "Within this general classification, the AI Act distinguishes three addressed forms of AI:\nGPAI models \u201cGPAI models\" are defined as AI models (including where such AI models are trained with a large amount of data using self-supervision at scale) that display significant generality, are capable of competently performing a wide range of distinct tasks, and can be integrated into a variety of downstream systems or applications.\u00b9\u2074 The AI Act regards these GPAI models as a fundamental component for subsequent use cases. Therefore, the legal obligations for these GPAI models arise when they are placed on the market, regardless of how this is done, e.g., through libraries, application programming interfaces (APIs), as a direct download, or as a physical copy.\u00b9\u2075 GPT-4, which serves as the basis for ChatGPT as well as a number of downstream applications, is probably the most relevant example of such a GPAI model.\nAl systems Secondly, an \"AI system\" is defined as \"a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments\u201d.\u00b9\u2076 With other words, AI systems are the systems that use AI and that may be deployed to end users to achieve explicit or implicit objectives, like the asylum decision system in Example 1. Such AI systems can be either built from scratch or on top of a ready-made GPAI model.\nGPAI systems In general, AI systems that are built on the basis of ready-made GPAI models, qualify as regular AI systems that serve a set of explicit or implicit objectives. In some cases, however, AI systems that are built on the basis of GPAI models may have the capability to serve a variety of purposes, both for direct use as well as for integration in other AI systems. As a special form of AI systems, the AI Act defines such systems as general-purpose AI systems.\u00b9\u2077 ChatGPT or Google Gemini might be the most prominent examples of such GPAI systems."}, {"title": "3.3 Relating and explaining the different types of AI", "content": "In order to understand the general scope of the AI Act, it is essential to relate the different forms of AI to each other in light of the drafting process."}, {"title": "4 Scope of Application", "content": "Other than the regulated forms of AI, the AI Act makes some further stipulations on its scope of application."}, {"title": "4.1 Territorial Scope", "content": "The territorial scope of application of the AI Act is very broad. Every AI system and GPAI model that is placed on the market or put into service within the EU has to comply with the AI Act - regardless of whether the provider has its establishment in the EU or"}, {"title": "4.2 Exemptions", "content": "For certain high-risk AI systems in fields such as civil aviation and motor vehicles, specific provisions have been set out by other EU legislation. Therefore, the AI Act as such does not apply in these cases, e.g. to autonomous cars, but instead only the specific rules for the case apply (cf. Article 2(2) [10] and Annex I, Section B [10]). There are also other areas in which the AI Act does not apply:\nAl systems or AI models, including their output, specifically developed and put into service for the sole purpose of scientific research and development (Article 2(6) [10]).\nResearch, testing or development activity regarding AI systems or AI models prior to their being placed on the market or put into service.\u00b2\u2076 However, testing in real world conditions shall not be covered by that exclusion (Article 2(8) [10]).\nAI systems released under free and open-source licences, unless they are placed on the market or put into service as high-risk AI systems or as an AI system that enables prohibited Artificial Intelligence Practices according to Article 5 [10]. Furthermore, the transparency obligations under Article 50 [10] still apply (Article 2(12) [10]).\nMilitary, defence or national security purposes (Article 2(3) [10])."}, {"title": "5 Requirements for the Provider of GPAI models", "content": "For developers of GPAI models, the AI Act follows a two-tiered approach, which is divided into general requirements and additional requirements for GPAI models with systemic risks. Providers of GPAI models with or without systemic risks should demonstrate compliance with these requirements by applying harmonized standards or until corresponding standards have been published \u2013 by complying with codes of practice.\u00b2\u2077 The latter are voluntary codes that are meant to be developed with the help from the AI Office, and can be considered as lighter versions of technical standards."}, {"title": "5.1 General Requirements regarding GPAI models", "content": "Article 53 [10] primarily contains general obligations regarding the need for documentation of the GPAI model. The aim of these obligations is to simplify the use of GPAI models for downstream AI systems. In the view of the legislature, this requires a good understanding of the models used in order to enable integration and fulfil the downstream provider's obligations under the AI Act and other regulations.\u00b2\u2078 This includes:\ndraw up and keep up-to-date the technical documentation of the model, including its training and testing process and the results of its evaluation;\u00b2\u2079\ndraw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to integrate the general-purpose AI model into their AI systems;\u00b3\u2070\nput in place a policy to comply with Union law on copyright and related rights and a possible reservation of rights of the copyright holders;\u00b3\u00b9"}, {"title": "5.2 Additional requirements regarding GPAI models with systemic risks", "content": "In case the GPAI model is associated with a \"systemic risk\" the AI Act imposes some additional obligations on the programmer. According to Article 51(1) [10], GPAI models are associated with systemic risk if they either have high-impact capabilities or are considered equivalent by the Commission. Whether a model has high-impact capabilities shall be \"evaluated on the basis of appropriate technical tools and methodologies, including indicators and benchmarks\u201d, and is presumed when the cumulative amount of computation used for its training measured in floating point operations is greater than \\(10^{25}\\).\u00b3\u2077 If a GPAI model is associated with systemic risks, the provider is obliged to:\nPerform a model evaluation described in accordance with standardised protocols and tools reflecting the state of the art;\u00b3\u2078\nassess and mitigate possible systemic risks at Union level;\u00b3\u2079\nkeep track of, document, and report relevant information about serious incidents and possible corrective measures to address them;\u2074\u2070"}, {"title": "6 Requirements for the Provider of AI Systems", "content": "If the developed AI system falls within the scope of application, three main types of relevant legal ramifications can arise from the AI Act: the system may (1) be prohibited, (2) be considered high-risk, or (3) be none of the two."}, {"title": "6.1 Prohibited systems", "content": "The AI Act defines a number of AI systems that are associated with what it considers to be unacceptable risks. These systems are prohibited under Article 5 [10].\u2074\u2074 The catalogue of AI systems listed here encompasses certain use cases:\nSubliminal techniques that have the objective or effect of materially distorting a person or group of persons' behaviour causing them or others significant harm.\u2074\u2075 This could affect the development of recommendation systems used in social media or advertising. The question that will arise here is when a distorting effect on one's own behavior reaches a material level. This also applies to other systems that exploit any vulnerability of natural persons due to their age, disability or a specific social or economic situation;\u2074\u2076\nSystems for the evaluation or classification of natural persons or groups based on their social behavior or personality characteristic with a social score leading to detrimental or unfavourable treatment.\u2074\u2077 This refers to social evaluation systems that result in detrimental or unfavorable treatment in certain contexts that is either unrelated to the contexts in which the data was generated or disproportionate to the behavior of the individuals;"}, {"title": "6.2 High-risk AI systems", "content": "If AI systems are not prohibited by the AI Act, it is generally permissible to market such systems. However, the AI Act imposes specific requirements on the development of those AI systems that are deemed to pose high risks. There are two ways in which an Al system can be considered high risk (also cf. Figure 2).\nHigh-risk AI systems covered by Annex I The first option is found in Article 6(1) [10], which states that the AI system is considered high risk if two conditions are met. First, the AI system must serve as a safety component of a product covered by one of the harmonized EU acts listed in Annex I [10] or be such a product itself.\u2075\u00b3 Second, the Al system must be subject to conformity assessment by a third party with a view to its placing on the market or putting into service on the basis of those acts listed in Annex I [10]."}, {"title": "6.3 Further transparency obligations for certain AI systems", "content": "Lastly, certain AI systems must meet a number of transparency requirements according to Article 50 [10]. Transparency in this context means that programmers must provide specific information to the end user of their system. This information must be provided to the end user in a clear and perspicuous manner no later than at the time of the first interaction or exposure.\u2076\u2075\nOn the one hand, this applies to systems that are designed to interact directly with end users.\u2076\u2076 These systems must be designed and developed to inform end users that they are interacting with an Al system if this is not obvious to a reasonably well-informed, observant, and circumspect person.\u2076\u2077 In other words, chatbots, for example, must be designed and developed in such a way that they make it clear to their users that they are bots (\"Bot-Disclosure\").\u2076\u2078\nOn the other hand, generative AI must make clear the artificial origin of its output.\u2076\u2079 This equally applies to AI systems and GPAI models that generate synthetic audio, image, video or text content. The providers of these models must ensure that the output of their systems is labelled in a machine-readable format and can be identified as artifi-"}, {"title": "7 Building an AI System on top of a GPAI model", "content": "We now aim at reflecting some practical consequences of what we discussed thus far. We consider the question of how to navigate the AI Act if aiming to build an AI system that uses a GPAI model as one of its core functional components. These downstream Al systems can take two forms under the AI Act. They can either be considered SPAI systems that are built on top of a GPAI model if they have specific purposes. Or they can be considered a GPAI system if the final system itself has the capability to serve a variety of purposes.\u2077\u00b9"}, {"title": "7.1 Obligations for SPAI systems built on top of a GPAI model", "content": "When integrating a GPAI model into an SPAI system that is categorised as high-risk, some compliance issues may arise. To illustrate these issues, let us elaborate a use case. Specifically, we consider the following hypothetical auto-ranking system used by a university admissions committee to decide which applicant should be granted admission to a particular degree program:"}, {"title": "7.2 Obligations for GPAI systems", "content": "GPAI systems (as opposed to GPAI models), on the other hand, are only briefly mentioned in the AI Act. For example, generative AI based on GPAI systems is subject to a special transparency obligation under Article 50(2) [10]. Beyond that there are no specific requirements for GPAI systems. However, it is unclear which other obligations may also apply.\nApplying high-risk requirements to GPAI systems. In our opinion, the applicable rules depend on the purposes of the GPAI system as designated by the provider.\u2077\u2079 When a GPAI system is designated to serve a variety of purposes, at least one of which are considered to be, e.g., high-risk, arguably the additional obligations for high-risk AI systems apply. However, when a GPAI system is determined to serve a variety of purposes none of which is categorized as high risk, even though a high-risk use might be factually possible, then additional obligations for high-risk AI systems do not apply. Article 25(1)(c) [10] seems to support this reading as it implies that GPAI systems can also be classified as high-risk AI systems."}, {"title": "8 Conclusion", "content": "This paper has navigated the AI Act from the perspective of the working programmer and has demonstrated that the AI Act contains numerous obligations of relevance for"}]}