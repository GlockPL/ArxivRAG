{"title": "DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for type classification", "authors": ["Hanna Abi Akl"], "abstract": "We introduce semantic towers, an extrinsic knowledge representation method, and compare it to intrinsic knowledge in large language models for ontology learning. Our experiments show a trade-off between performance and semantic grounding for extrinsic knowledge compared to a fine-tuned model's intrinsic knowledge. We report our findings on the Large Language Models for Ontology Learning (LLMs4OL) 2024 challenge.", "sections": [{"title": "1 Introduction and related work", "content": "Large language models (LLMs) have seen widespread applications across different tasks in the fields of Natural Language Processing and Knowledge Representation. Particularly, LLM-based systems are used to tackle ontology-related tasks such as ontology learning [1], knowledge graph construction [2], ontology matching [3][4] and ontology generation [5]. Retrieval-Augmented-Generation (RAG) systems, which build on the capabilities of LLMs by enhancing retrieval using external knowledge sources, have also shown promising results in tasks involving the use of ontologies [6]. On the other hand, symbolic methods like semantic representation using primes and universals [7] form another research frontier in the area of knowledge representation which is at the heart of ontologies [8].\nIn this work, we evaluate and compare the performance of fine-tuned models on Task A of the LLMs4OL [9][10][11] 2024 challenge\u00b9 using intrinsic LLM knowledge and external knowledge sources we define as semantic towers. The rest of the work is organized as follows. In section 2, we present our methodology. Section 3 describes our experimental framework. In section 4, we report our results and discuss our findings. Finally, we conclude in section 5."}, {"title": "2 Methodology", "content": "This section describes the methodology for creating a semantic tower ST which we define as:\n$ST = {S1, S2, .., Sn}$,\n(1)\nwhere s is a domain semantic primitive pointing to a semantic property for a given domain and n is the minimal number of primitives needed to define the domain. The rest of this section details the construction of domain semantic towers from semantic primitives."}, {"title": "2.1 Domain semantic primitives", "content": "For each domain, we use the Wikidata Query Service\u00b2 to retrieve semantic information for each term type category. This body of information, or semantic set, serves as the base for the domain semantic primitives.\nThe WordNet semantic set consists of: {subclass,instance,part,represents,description}.\nThe GeoNames semantic set consists of: {subclass,instance,part,category,description}."}, {"title": "2.2 Semantic towers", "content": "The construction scheme of semantic towers is domain-invariant and summarized in the following steps:\n1. The values of the semantic set for each term type are tokenized into a bag of words, cleaned and normalized through lowercase transformation and stop word removal.\n2. The result is transformed to a comma-separated list.\n3. Empty values and duplicates are pruned from the list.\n4. The list of primitives is transformed to vector embeddings of size 1024 using the gte-large\u00b3 model by Google [12].\n5. The resulting domain vector embeddings are stored in a MongoDB\u2074 collection to form a vector store, i.e. the semantic tower.\n6. The semantic tower is indexed on embeddings search for optimized performance."}, {"title": "3 Experiments", "content": "This section describes our experiments in terms of data, models and training process."}, {"title": "3.1 Dataset description", "content": "We consider two datasets for our experiments: WordNet and GeoNames. Both datasets are used for training and testing our models in the respective subtasks (A.1 and A.2). The dataset descriptions are detailed in the following subsections."}, {"title": "3.2 System description", "content": "This section describes the models as well as the setup of our experiments."}, {"title": "3.2.1 Models", "content": "We train one model for each subtask. We use the same base flan-t5-small\u2075 model and fine-tune it on the subtask datasets respectively. The training hyperparameters for both models are configured identically: {learning_rate: 1e-05, train_batch_size: 4, eval_batch_size: 4, num_epochs: 5, question_length: 512, target_length: 512, optimizer: Adam}. For subtask A.1, the model is trained on 70% of the provided WordNet dataset and the remaining 30% is used for validation."}, {"title": "3.2.2 Features", "content": "The same feature engineering method is applied for both models. It consists in embedding input text into vectors of size 1024 using the gte-large model. For the flan-t5-small-wordnet model, the input is the concatenation of the term and the sentence when provided. For flan-t5-small-geonames, the input text is the term."}, {"title": "3.2.3 Setup", "content": "We conduct two experiments per subtask for a total of four.\nFor subtask A.1, the first experiment (WN1) consists in prompting the fine-tuned WordNet model on the test split of the provided dataset which is used as an unofficial test set ahead of the official submission. The prompt used for the model is: Give the entity for the term X. Select the answer from this list Y, where X is dynamically replaced by the input term and Y is replaced by the list of possible term types.\nThe second experiment (WN2) leverages the RAG pipeline shown in Figure 4 in conjunction with a user prompt to retrieve the best term type for each input term. The input is vectorized and compared to the embeddings of the WordNet semantic tower for each term type. A cosine similarity score is used to determine the closest type from the semantic tower vector store to return the top 1 candidate. The answer is then used as an additional input to the user prompt given to the model: Give the entity for the term X. Select the answer from this list Y relying on the search result Z, where X and Y are as previously defined and Z represents the best-matched term type from the semantic tower.\nFor subtask A.2, both experiments GN1 and GN2 mimic WN1 and WN2 respectively. For GN1, the fine-tuned GeoNames model is evaluated on the test split of the curated dataset. The user prompt for the model is the same as that of WN1, with the only changes being the X term values and the Y list of types which now refers to the geographical categories.\nIn experiment GN2, the same pipeline from Figure 4 is reproduced with the only difference being the replacement of the WordNet semantic tower with the GeoNames semantic tower. The user prompt used for the fine-tuned model is the same as that of WN2, with the Y list reflecting the geographical categories. All experiments are conducted on a Google Colab instance using a L4 High-RAM GPU. The code for our experimental setup is publicly available on GitHub."}, {"title": "4 Results", "content": "Table 3 shows our experimental results on the WordNet test set. The results of the GeoNames experiments are presented in Table 4. The F1 scoring metric reflects the criteria of performance assessment set by the task organizers.\nExperiments WN1 and GN1 perform better than WN2 and GN2 respectively, with a performance gain close to 10%. At first inspection, the results seem to suggest that the flan-t5 model, with a little fine-tuning, can rely on its existing knowledge regarding the dataset domains to correctly classify terms by type. The use of an external knowledge base, such as a semantic tower, seems to create more errors in the model answers. However, closer examination of a subset of the outputs reveals that semantic towers effectively ground certain semantic notions in the model that are otherwise lost if the model only relies on its existing knowledge. Examples include correctly classifying the term into the bargain as adverb with the aid of the WordNet semantic tower (as opposed to classifying it as noun without it). While the word bargain dominates the term in the example, the flan-t5-small-wordnet model misses out on the correct classification which attributes an important weight to the adverb into that becomes more prominent with the semantic tower embeddings representation. A similar case can be made for the GeoNames experiments, where the usage of the semantic tower in conjunction with the model improves the classification choice for plural categories (e.g. terms classified as mountains, peaks, streams). The outputs of experiment GN1 show that the model alone has a tendency to choose the singular forms of these categories which count for incorrect classifications. Moreover, experiment GN2 also shows that the semantic tower helps ground nuances between categories (e.g. stream versus section of stream) which leads to a more fine-grained (and accurate) typing.\nFor the official test sets released by the task organizers, we evaluate only the A.1 subtask using WN1 and WN2 and present our results in Table 5. Both WN1 and WN2 demonstrate a slight drop in performance of around 1% but perform competitively well. The results demonstrate that the model training as well as the WordNet semantic tower construction are sound enough to avoid catastrophic drift.\nWe refrain from submitting to the other subtasks, most notably A.2, because of the length of the official test set which is extremely challenging to run on our available resources."}, {"title": "5 Conclusion", "content": "In this shared task, we investigate and compare intrinsic knowledge in LLMs with external semantic sources for ontology learning. While the introduction of semantic towers proves there is still some way to go to achieve semantic resonance in LLMs, it shows promising results in grounding these models semantically and fine-graining their knowledge. Our fine-tuned models demonstrate that ontology term typing is a task within the reach of LLMs based on their existing knowledge. In future work, we will explore the potential of semantic towers and expand their implementation to existing LLM-based systems."}]}