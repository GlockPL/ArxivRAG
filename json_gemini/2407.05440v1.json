{"title": "Explainable AI: Comparative Analysis of Normal and\nDilated ResNet Models for Fundus Disease Classification", "authors": ["P.N.Karthikayana", "Yoga Sri Varshan V", "Hitesh Gupta Kattamuri", "Umarani Jayaraman"], "abstract": "This paper presents dilated Residual Network (ResNet) models for disease\nclassification from retinal fundus images. Dilated convolution filters are used\nto replace normal convolution filters in the higher layers of the ResNet model\n(dilated ResNet) in order to improve the receptive field compared to the nor-\nmal ResNet model for disease classification. This study introduces computer-\nassisted diagnostic tools that employ deep learning, enhanced with explain-\nable AI techniques. These techniques aim to make the tool's decision-making\nprocess transparent, thereby enabling medical professionals to understand\nand trust the AI's diagnostic decision. They are particularly relevant in\ntoday's healthcare landscape, where there is a growing demand for trans-\nparency in AI applications to ensure their reliability and ethical use. The\ndilated ResNet is used as a replacement for the normal ResNet to enhance\nthe classification accuracy of retinal eye diseases and reduce the required\ncomputing time. The dataset used in this work is the Ocular Disease Intelli-\ngent Recognition (ODIR) dataset which is a structured ophthalmic database\nwith eight classes covering most of the common retinal eye diseases. The\nevaluation metrics used in this work include precision, recall, accuracy, and\nF1 score. In this work, a comparative study has been made between normal\nResNet models and dilated ResNet models on five variants namely ResNet-\n18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. The dilated ResNet\nmodel shows promising results as compared to normal ResNet with an av-\nerage F1 score of 0.71, 0.70, 0.69, 0.67, and 0.70 respectively for the above\nrespective variants in ODIR multiclass disease classification.", "sections": [{"title": "1. Introduction", "content": "In 2022, according to World Health Organization (WHO) globally, at\nleast 2.2 billion people have a near or distance vision impairment. These\npeople include those with moderate or severe distance vision impairment or\nblindness due to unaddressed refractive error (88.4 million), cataract (94 mil-\nlion), age-related macular degeneration (8 million), glaucoma (7.7 million),\ndiabetic retinopathy (3.9 million), as well as near vision impairment caused\nby unaddressed presbyopia (826 million) [1]. The percentage of global eye\ndisease as reported in Global Health Matters [2] is shown in Figure 1.\nThe leading causes of vision impairment are i) cataract ii) age-related\nmacular degeneration iii) diabetic retinopathy iv) glaucoma v) uncorrected\nrefractive errors. There is substantial variation in the causes between and\nwithin countries according to the availability of eye care services, their af-\nfordability, and the eye care literacy of the population [1].\nFundus images provide details about a patient's retina and are captured\nwith a specialized fundus camera. The retinal macula, optic disc, fovea, and\nblood vessels constitute some of the features noticed in the image. Because\nof these features, the fundus imaging method is more cost-effective and bet-\nter suited for non-invasive screening. Fundus photography is a screening"}, {"title": "1.1. Need for Automatic Disease Classification", "content": "Analyzing fundus images is an exhausting and tedious task even for\ntrained experts as these images comprise complex components such as optic\ndisc and cup, blood vessels, macula, and so on. The vascular structure of the\nretina is so complex and diseases that affect these structures are so subtle. As\na result, it is difficult to identify these small changes by human experts, which\nmakes the task of manual disease classification so difficult. Also, manual dis-\nease classification is laborious and time-intensive, vulnerable to inter-rater\nvariability, and has reduced efficiency. Furthermore, the increasing number\nof patient data adds to the challenge of clinical routines such as diagnosis,\ntreatment, and monitoring. As a result, automated approaches for disease"}, {"title": "1.2. Gaps in Research", "content": "Deep convolution neural network models have been used extensively for\nimage classification tasks. The ResNet model is one such model that is\nwidely used. Having said that, normal ResNet models, while effective for a\nwide range of computer vision tasks, may not be the most suitable choice for\ndisease classification from retinal fundus images due to several reasons:\nRestricted Receptive Field: Normal convolutional layers, which are\ncommonly used in ResNet models, have a limited receptive field. In the\ncontext of retinal images, where diseases may manifest in subtle or localized\nfeatures across the image, models with a restricted receptive field may find\nit difficult to extract important contextual information required for precise\ndisease classification.\nSpatial Hierarchies and Context: Retinal images contain complex\nstructures at multiple spatial scales, including blood vessels, optic disc, and\nmacula. A normal ResNet model may not effectively capture these hierar-\nchical spatial relationships and contextual cues, leading to poor performance\nin distinguishing between different retinal diseases.\nLoss of Spatial Resolution: Down sampling in deep ResNet models\noften result in small output feature maps at the end of the network, which\nmay lead to a loss of spatial resolution. Preserving fine-grained spatial details\nin retinal images is crucial in detecting subtle changes related to disease."}, {"title": "1.3. Contributions of the work", "content": "The important contributions of this work are highlighted below,\n\u2022 A comparative study between normal ResNet models and dilated ResNet\nmodels for disease classification on the ODIR dataset has been con-\nducted.\n\u2022 Five different ResNet variants of varying network depth have been used\nin the study, namely ResNet-18, ResNet-34, ResNet-50, ResNet-101\nand ResNet-152.\n\u2022 The accuracy and F1 score results obtained in dilated ResNet models\noutperform normal ResNet models. It has been observed that the av-\nerage accuracy has improved from 0.70 to 0.79 while the F1 score has\nimproved from 0.58 to 0.70 in the case of ResNet-152.\n\u2022 The explainability of the ResNet models for disease classification has\nbeen studied with LIME, RISE, and GradCAM techniques. Study-\ning the explainability of correct classifications helps understand if the\nmodel is making decisions based on meaningful and relevant features\nin the data.\nThe rest of the paper is organized as follows. Related work is discussed in\nSection 2. Section 3 discusses the preliminaries and the proposed methodol-\nogy is explained in Section 4. Experimental results and analysis are discussed\nin Section 5. Finally, conclusions are given in Section 6."}, {"title": "2. Related work", "content": "Various retinal fundus image datasets have been used for training deep\nlearning models for disease classification. One of the datasets extensively"}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Normal Convolution (2D)", "content": "Normal convolution in 2D involves sliding a filter (kernel) over the input\nimage or feature map in both the horizontal and vertical directions. At each\nposition, the filter is multiplied element-wise with the corresponding region\nof the input, and the results are summed to produce a single output value\nfor that position. This process is performed for every position in the input,\nresulting in an output feature map. Figure 3(a) shows normal convolution.\nThe mathematical equation for normal convolution in two-dimension is given\nbelow,\n$(f*g)(t, u) = \\sum_{\\tau=-\\infty}^{+\\infty}\\sum_{\\phi=-\\infty}^{+\\infty} f(\\tau, \\phi).g(t - \\tau, u - \\phi)$"}, {"title": "3.2. Dilated Convolution (2D)", "content": "Dilated convolution in 2D extends the receptive field of normal convolu-\ntion by introducing gaps between the kernel elements in both the horizontal\nand vertical directions. These gaps are determined by dilation factors, which\nspecify how far apart the kernel elements are spaced. Figure 3(b) shows"}, {"title": "3.3. Advantages of Dilated Convolution", "content": "The proposed approach is based on dilated convolutions in the ResNet\nmodel also known as dilated ResNet. Unlike the normal ResNet which con-\nsists of normal convolution and is slow, the proposed model consists of dilated\nconvolutions. The advantages of dilation convolution are listed below,\n\u2022 Increase in dilation rate of the filter will increase its receptive field.\n\u2022 Helps capture more global context from input without increasing the\nsize of parameters along with lesser memory and computation time."}, {"title": "3.4. Explainable AI(XAI)", "content": "Explainable AI (\u03a7\u0391\u0399) techniques aim to find the decision-making process\nof AI models, particularly in complex tasks such as medical image analysis.\nBy providing interpretable explanations, XAI methods enhance transparency\nand trust in AI systems, enabling users to understand the rationale behind\nmodel predictions. XAI techniques can be used to generate activation maps\nthat highlight the areas where the model focuses on when making a clas-\nsification decision. This visualization helps in understanding the regions of\ninterest for different classes.\nVarious XAI techniques used in this work are explained below.\n\u2022 LIME: (Local Interpretable Model-agnostic Explanations) has been\nintroduced by Ribeiro et al. in 2016 [16]. It approximates complex\nmodel decisions around specific instances by locally fitting interpretable\nmodels, providing intuitive explanations.\n\u2022 RISE: (Randomized Input Sampling for Explanation) has been intro-\nduced by Petsiuk et al. in 2018 [17]. It generates pixel-wise expla-\nnations by systematically occluding random image patches, providing\nfine-grained insights into the importance of different image regions for\nmodel predictions."}, {"title": "3.5. Activation Maps", "content": "Activation maps are used to visualize and interpret the features that a\nneural network, particularly convolutional neural networks (CNNs), learns at\nvarious layers. They provide insight into the model's decision-making pro-\ncess by highlighting which regions of an input image contribute most to its\npredictions. This aids in debugging and improving model performance by\nidentifying under performing layers or irrelevant features. Additionally, acti-\nvation maps allow for the comparison of different model architectures, such\nas normal convolution versus dilated convolution, and help in understanding\nmodel sensitivity and robustness to input variations."}, {"title": "4. Proposed Methodology", "content": "The flow diagram of the proposed work is shown in Figure 4. It consists\nof two important phases: training and testing. Training consists of three\nmodules namely, i) preprocessing, ii) dilated CNN, and iii) loss function.\nDuring testing, the learned weights are used to classify the disease images.\nEach of these modules is explained in detail below."}, {"title": "4.1. Preprocessing", "content": "The retina from ODIR-5K dataset is captured using various cameras with\ndifferent resolutions. Therefore, it is resized to 224 \u00d7 224 for preprocessing.\nAdditionally, all the images are assigned an eight category vector based on\ntheir labels to train the models."}, {"title": "4.2. Dilated CNN: Dilated ResNet Model", "content": "The proposed model is based on dilated ResNet over normal ResNet.\nFive different versions of ResNet models of varying network depth have been\nused in the study, namely ResNet-18, ResNet-34, ResNet-50, ResNet-101,\nand ResNet-152. ResNet-18 consists of 18 layers, which include convolu-\ntional layers, pooling layers, and fully connected layers. Specifically, it has\n17 convolutional layers and one fully connected layer at the end. The first"}, {"title": "4.3. Loss Function", "content": "The loss function used in the proposed approach is the Sparse Categor-\nical Cross Entropy (SCCE). Sparse Categorical Cross Entropy (SCCE) loss\nfunction is shown in Figure 8. The formula for the softmax function is given\nbelow:\n$f(s_i) = \\frac{e^{s_i}}{\\sum_{j=1}^{C}e^{s_j}}$\nThe output of the softmax is given as input to the Sparse Categorical\nCross Entropy (SCCE). SCCE is the loss function that computes the differ-\nence between the actual value $f(s_i)$ with the ground truth $t_i$\n$SCCE = - \\sum_{i=1}^{C} t_i log(f(s_i)$\\\nThe advantage of using SCCE is that it is more efficient for multiple\ncategories or classes. Unlike SCCE, the CCE (Categorical Cross Entropy)\nwould consume a huge amount of RAM if one-hot encoded When the cat-\negories or classes are more. Also, SCCE saves memory as well as speed up\nthe computation process."}, {"title": "5. Experimental Results", "content": ""}, {"title": "5.1. Dataset", "content": "In this experiment, the ODIR-5K multi-label retinal image dataset [19] is\nused. Some sample images are shown in Figure 9. This dataset contains a set\nof fundus images collected from patients by Shanggong Medical Technology\nCo. Ltd. from different hospitals/medical centers in China."}, {"title": "5.2. Metrics Used", "content": "Metrics such as accuracy, precision, recall, and F1 score are used to eval-\nuate the proposed methodology.\n\u2022 Accuracy is the ratio of correctly predicted instances to the total in-\nstances. It is calculated as:"}, {"title": "5.3. Results", "content": "Table 2 shows the results of the ResNet-18 model without and with dila-\ntion trained and tested on the ODIR dataset. Two models were used in this"}, {"title": "5.4. Explainable AI Techniques", "content": "In the context of the ResNet models trained on the ODIR dataset for\nclassifying eye diseases, \u03a7\u0391\u0399 can:\n\u2022 Visualize Attention: Show which parts of the retinal images the model\nfocuses on when making a classification (e.g., glaucoma or myopia).\n\u2022 Explain Misclassifications: Provide insights into why certain images\nare misclassified, helping improve model accuracy."}, {"title": "6. Conclusions", "content": "In conclusion, this paper presents dilated Residual Network models for\ndisease classification from retinal fundus images, demonstrating the effec-\ntiveness of dilated convolutions in improving the receptive field compared to\nnormal convolutions in ResNet models. By incorporating computer-assisted\ndiagnostic tools that employ deep learning enhanced with explainable AI\ntechniques, this study aims to make the decision-making process of AI trans-\nparent, enabling medical professionals to understand and trust the AI's diag-\nnostic decisions. This approach is particularly relevant in today's healthcare\nlandscape, where there is a growing demand for transparency to ensure the\nreliability and ethical use of AI applications.\nThe dataset used in this work is the Ocular Disease Intelligent Recogni-\ntion (ODIR), a structured ophthalmic database with eight classes covering\nmost common retinal eye diseases. Evaluation metrics used include precision,\nrecall, accuracy, and F1 score. A comparative study has been conducted by\napplying dilation to five variants of the ResNet model: ResNet-18, ResNet-34,\nResNet-50, ResNet-101, and ResNet-152. The dilated ResNet model showed\npromising results compared to the normal ResNet, with average F1 scores\nof 0.71, 0.70, 0.69, 0.67, and 0.70, respectively, for the five different versions\nin ODIR multiclass disease classification. These results indicate that the di-\nlated ResNet model offers a significant improvement in feature capture and\ndiagnostic accuracy for retinal eye diseases.\nThe study also shows how the dilated ResNet focuses on different regions\nfor different diseases. For instance, it focuses on the optic disc for glaucoma,\nblood vessels for diabetes and hypertension, and the macula for degenera-"}]}