{"title": "Self-Evaluation for Job-Shop Scheduling", "authors": ["Imanol Echeverria", "Maialen Murua", "Roberto Santana"], "abstract": "Combinatorial optimization problems, such as scheduling and route planning, are crucial in various industries but are computationally intractable due to their NP-hard nature. Neural Combinatorial Optimization methods leverage machine learning to address these challenges but often depend on sequential decision-making, which is prone to error accumulation as small mistakes propagate throughout the process. Inspired by self-evaluation techniques in Large Language Models, we propose a novel framework that generates and evaluates subsets of assignments, moving beyond traditional stepwise approaches. Applied to the Job-Shop Scheduling Problem, our method integrates a heterogeneous graph neural network with a Transformer to build a policy model and a self-evaluation function. Experimental validation on challenging, well-known benchmarks demonstrates the effectiveness of our approach, surpassing state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "Combinatorial optimization problems (COPs) are fundamental to tasks such as scheduling, resource allocation, and route planning, influencing key decisions across various industries. Given their inherent complexity, most COPs are NP-hard (Papadimitriou, 1998), making exact methods impractical for large-scale or real-time decision-making. Traditionally, specialized heuristics\u2014often grounded in domain expertise\u2014have been employed to provide feasible solutions under tight time constraints (Blum & Roli, 2003).\nNeural Combinatorial Optimization (NCO) (Vinyals et al., 2015; Bello et al., 2017) has emerged as a data-driven framework for deriving heuristics by leveraging recurring patterns in problem instances. Most NCO approaches adopt a constructive viewpoint, treating solution building as a sequential decision-making process naturally framed by a Markov Decision Process (MDP). This formulation allows for training policies that iteratively determine the next choice in a sequence of decisions, ultimately arriving at a complete solution. Nevertheless, constructing solutions through a sequence of individual choices can pose significant challenges, as each local decision can limit or skew subsequent options, making it increasingly difficult to ensure a high-quality final outcome.\nA related and rapidly evolving area is that of Large Language Models (LLMs), which face the challenge of generating coherent sequences. One promising direction in LLM research is self-evaluation, where an auxiliary function assesses outputs, identifies weaknesses, and guides the refinement of subsequent steps (Kadavath & et al., 2022; Xie et al., 2024). Inspired by these advances, we propose applying self-evaluation principles to COP decoding. However, this design significantly departs from the typical approach in LLMs, as the constraints involved in constructing the optimal solution of a combinatorial problem are of a fundamentally different nature.\nThe problem we aim to address is generating subsequences of actions closer to the optimal solution, a core challenge in many COPs. Conventional methods, which produce one action at a time, often fail to generate coherent and optimal subsequences, as they do not directly evaluate the quality of action subsets using a dedicated evaluation mechanism. To address this, our framework introduces a novel mechanism that evaluates subsets of actions collectively at each step. By jointly considering multiple actions, the approach improves entire subsequences, enhancing solution quality compared to traditional stepwise methods. This shift from isolated moves to collective evaluations enables a richer and more effective decision-making process.\nOur framework integrates two complementary models: a policy model and a self-evaluation model. The policy model assigns probabilities to possible actions using supervised learning, sampling sets of actions that are then scored by the self-evaluation model based on their overall quality. This allows for selecting the most promising subsequences during inference and redefines COPs as MDPs, transitioning from single-action spaces to subsequence-based spaces."}, {"title": "2. Related Work", "content": "An early breakthrough in NCO leveraged supervised learning with recurrent neural networks, applied to COPs such as the Traveling Salesman Problem (TSP) (Vinyals et al., 2015). Although exact solvers are limited to small instances, they provided high-quality data to train neural networks capable of generalizing to larger problems, demonstrating the potential of learning-based methods for COPs.\nSubsequent work, particularly in routing problems, shifted towards deep reinforcement learning (DRL) to train policies (Kool et al., 2018; Kwon et al., 2020). These methods introduced various improvements in neural network architectures and strategies to exploit symmetries inherent in routing problems. Typically, they followed a constructive approach, incrementally building solutions by selecting the next element\u2014such as the next city in the TSP-until the solution was complete.\nBuilding on the successes in routing, scheduling problems have also been addressed predominantly with deep reinforcement learning (DRL), incorporating various adjustments in policy training and state modeling (Song et al., 2022; Wang et al., 2023). In addition to recurrent neural networks and transformers (Tassel et al., 2023; Pirnay & Grimm, 2024), commonly used in routing problems, scheduling methods leverage architectures specifically designed to handle the unique challenges of these problems. Unlike routing, scheduling involves more complex entities\u2014such as operations, jobs, and machines\u2014where HGNNs are often utilized (Song et al., 2022). HGNNs effectively represent multiple node and edge types, capturing the intricate relationships inherent in scheduling problems. However, most scheduling and routing methods adopt a constructive approach, where operations are sequentially assigned, and policies are trained to make one assignment at a time. This severely limits their ability to capture broader dependencies between assignments, leading to suboptimal solutions. Additionally, research on hybridizing different architectures remains limited, as current approaches tend to exclusively rely on either recurrent neural networks or transformers, instead of combining their strengths to better handle the complexity of scheduling problems.\nAlthough less common, neural improvement methods provide an alternative to constructive approaches by refining an existing solution rather than building it step by step (Garmendia et al., 2023). For example, in the JSSP (Zhang et al., 2024), these methods learn policies often via DRL variants that rearrange operations pair by pair to optimize the execution sequence. However, this approach, where the action space is a set rather than a subsequence, faces similar challenges to constructive methods-namely, the difficulty of building subsequences of changes that are closer to the optimal solution.\nRecently, researchers have begun exploring alternatives to standard DRL, in favor of self-supervised strategies (Corsini et al., 2024; Pirnay & Grimm, 2024), a return to supervised methods (Drakulic et al., 2024), or offline RL (Echeverria et al., 2024b), to overcome DRL's limitations, most notably the extensive interactions required for exploration. Some approaches have started assigning more than one action simultaneously, such as the method in (Tassel et al., 2023), which combines policy gradient methods with imitation learning, and the approach in (Echeverria et al., 2024a), which relies on supervised learning. However, these methods lack a self-evaluation mechanism and do not move beyond the standard MDP framework. Actions are assigned solely based on the probabilities provided by the policy model, without evaluating how close the generated sequence is to the optimal solution. This can result in action sets lacking internal coherence, as they are not directly evaluated using a dedicated evaluation function.\nThe main contribution of this paper is to depart from generating solutions action by action and, instead, produce and evaluate subsequences collectively using a self-evaluation mechanism inspired by the success of LLMs. This approach avoids the traditional stepwise paradigm of predicting a single move without supervision. Additionally, we integrate HGNNs with Transformers, both to generate the policy and to define the self-evaluation function."}, {"title": "3. Preliminaries", "content": "Job Shop Scheduling Problem. The JSSP is a classical combinatorial optimization problem defined as follows. Let $J = {J_1, J_2, ..., J_n}$ represent a set of $n$ jobs, and let $M = {M_1, M_2,..., M_m}$ denote a set of $m$ machines. Each job $J_i$ consists of a sequence of $k_i$ operations, ${O_{i1}, O_{i2},..., O_{ik_i}}$, where each operation $O_{ij}$ requires exclusive use of a specific machine $M_k \\in M$ for a fixed processing time $p_{ij} > 0$. Operations within a job must follow a predefined order, meaning that $O_{ij}$ must complete before $O_{i(j+1)}$ begins for all $j = 1, . . ., k_i - 1$. Additionally, each machine $M_k$ can process at most one operation at a time, and once an operation starts on its assigned machine, it must run without interruption until completion.\nThe objective is to determine a feasible schedule $S$ that minimizes the makespan, $C_{max}$, defined as the total time required to complete all jobs:\n$C_{max} = \\max_{i=1,...,n} C_i$,\nwhere $C_i$ is the completion time of job $J_i$.\nSelf-Evaluation in Large Language Models. Breaking down complex tasks into intermediate steps, often referred to as reasoning chains, has proven highly effective for enhancing the performance of LLMs on multi-step problems (Brown et al., 2020; Wei et al., 2022). By structuring tasks into logical sub-components, these models can process and solve problems more systematically. However, as reasoning chains grow longer, errors can accumulate across intermediate steps, significantly reducing the accuracy of the final outcomes (Chen et al., 2024)."}, {"title": "4. Method", "content": "In this section, we present our contribution, which introduces a novel approach to applying the concept of self-evaluation to scheduling problems. We begin by explaining how the problem is defined as a Markov Process and then detail the generation of the policy and self-evaluation models."}, {"title": "4.1. JSSP as a Markov Process", "content": "The JSSP is modeled as a Markov Process, with states representing scheduling progress and transitions corresponding to sets of job-machine assignments. Below, we outline the state space, action space, and transition function.\nState Space. The state space is modeled as a heterogeneous graph $G_t = (V_t, E_t)$, following the approach described in (Song et al., 2022). A simplified version, as proposed in (Echeverria et al., 2024a), is used in this work. At each timestep $t$, $G_t$ includes three types of nodes: operations, jobs, and machines, as well as three distinct types of edges (directed and undirected) that capture relationships such as precedence constraints between operations, which operations belong to a specific job, and which operations can be executed by specific machines.\nAction Space. At each timestep $t$, the set of feasible job-machine pairs is denoted as $IM_t$. The action space is defined as the power set of all pairs $(j, m) \\in IM_t$ such that no machine is assigned more than one job simultaneously:\n$A_t = {A \\subset JM_t | \\forall(j, m), (j', m) \\in A, j = j'}$.\nThis formulation allows multiple job-machine assignments to be made at once, provided they respect machine capacity constraints. This represents a significant shift from prior work (Echeverria et al., 2024a) and many other scheduling (Wang et al., 2023; Tassel et al., 2023) or routing (Kwon et al., 2020; Drakulic et al., 2024) approaches, where the action space is typically defined as a single assignment set. In scheduling, prior to this work, the action space would generally be defined as $IM_t$.\nTransition Function. The state transitions are determined by the selected assignments. Specifically, at timestep $t$, the selected action $A \\in A_t$ updates the graph $G_t$ by modifying the attributes of nodes and edges to reflect the assignments made. Following the work in (Ho et al., 2024; Echeverria et al., 2025), completed operations are removed to simplify the state space.\nIn this work, we do not explicitly define a reward function, as both the policy and self-evaluation models are trained using supervised learning on optimal solutions."}, {"title": "4.2. Self-Evaluation", "content": "We refer to our method as SEVAL (Self-evaluation), which introduces a novel mechanism for evaluating subsets of actions collectively, enhancing solution quality compared to traditional step-by-step approaches. SEVAL relies on two complementary models: a policy model that proposes candidate assignments, and a self-evaluation model that scores these sets of assignments, enabling the selection of the most promising ones during inference. In this section, we describe the dataset generation process, the training procedure for both models, and the inference mechanism."}, {"title": "4.2.1. DATASET GENERATION", "content": "To train both models, a supervised learning approach is employed, requiring a dataset. Some methods, like those in (Kool et al., 2019; Drakulic et al., 2024), rely solely on expert trajectories, which can limit data diversity and hinder performance in complex scenarios. The dataset $D$ is constructed by solving small-scale JSSP instances with an efficient solver, transforming each solution into sequences of states and actions:\n$D = {(s, A) | s \\in S, A \\subseteq A_t}$,\nwhere $s$ is a heterogeneous graph of the scheduling state, and $A$ represents feasible job-machine assignments.\nA key issue with supervised learning is poor generalization (Ross & Bagnell, 2010), especially when policies encounter underrepresented states during testing."}, {"title": "4.2.2. POLICY MODEL TRAINING", "content": "The policy model predicts the probability distribution $\\pi_{\\theta}(a | s)$ over feasible job-machine assignments $a \\in IM$ for a given state $s$. To compute assignment probabilities, the model processes the state $s$ in two stages. First, a HGNN generates embeddings for the graph's nodes. The HGNN is parameterized by $\\Phi$, uses $L$ propagation layers, and incorporates the attention mechanisms of GATv2 (Brody et al., 2021) to effectively capture the structural and relational information of the state.\n$\\mathbf{h}^j_i, \\mathbf{h}^m_k = \\text{HGNN}_{\\Phi}(s)$.\nNext, a Transformer processes these embeddings to compute the assignment probabilities. The input to the Transformer is a sequence that includes all feasible assignments $a \\in IM$. For each assignment, the input comprises the concatenation of the job embedding $\\mathbf{h}^j_i$, the machine embedding $\\mathbf{h}^m_k$, and the corresponding edge features $\\mathbf{h}_{m_kji}$. The Transformer then outputs the probability of the assignment:\n$\\mu_{\\theta}(a|s) = \\text{Transformer}_{\\theta} (\\mathbf{h}^j_i, \\mathbf{h}^m_k, \\mathbf{h}_{m_kji})$.\nThe model is trained via supervised learning, minimizing the Kullback-Leibler (KL) divergence loss, as suggested in (Rusu et al., 2015), to achieve more robust probability distributions. This loss measures the difference between the predicted distribution $\\pi_{\\theta}(a|s)$ and the target distribution $\\pi_{\\text{solver}}(a|s)$, which is derived from solver-generated optimal assignments:\n$\\mathcal{L}_{\\text{policy}} = KL(\\pi_{\\theta}(a|s), \\pi_{\\text{solver}}(a|s))$.\nFollowing this calculation, the weights of both the Transformer and the HGNN are updated to align the model's predictions with the solver-provided target distribution."}, {"title": "4.2.3. SELF-EVALUATION MODEL TRAINING", "content": "The self-evaluation model assigns a scalar score between 0 and 1 to subsets of job-machine assignments $A \\subset IM$, representing the proportion of optimal assignments in the subset. In other words, a subset with no optimal assignments would receive a score of zero, while a subset where all assignments are optimal would receive a score of one.\nOnce the model is trained, these scores are used during inference to select high-quality sets of actions, ensuring that the generated solutions are closer to the optimal.\nDuring training, the model leverages the embeddings $\\mathbf{h}^j_i$ and $\\mathbf{h}^m_k$ for jobs and machines, generated by the HGNN as part of the policy model training. Additionally, the embeddings for job-machine edges, $\\mathbf{h}_{m_kji}$, are included to encode relational information.\nTo mathematically represent sets of assignments, we model them as binary vectors. For each state $s$ with its corresponding set of feasible job-machine assignments $A \\subseteq IM$, a binary vector BV(A) is generated. This vector indicates whether each specific job-machine assignment is included in the subset, where a value of 1 denotes inclusion and 0 denotes exclusion.\nDuring the training phase, for a given state $s$ and its corresponding set of optimal assignments $A_{opt}$, a binary vector BV($A_{sub}$) is randomly generated to simulate a subset of assignments. This vector indicates whether each job-machine pair is included in the subset and is compared against the optimal assignment vector to measure their similarity. The objective of the self-evaluation model is to learn to predict this similarity, which represents the degree to which a given subset resembles the optimal solution. This degree of similarity, referred to as the true score TrueScore($A_{sub}, A_{opt}$), acts as the target during training and is computed using ground-truth information from the dataset.\nTo compute the predicted score for a given subset, the binary vector BV($A_{sub}$) is concatenated with the embeddings $\\mathbf{h}^j_i$, $\\mathbf{h}^m_k$, and $\\mathbf{h}_{m_kji}$. These concatenated representations are processed by a Transformer architecture, which aggregates the information and applies mean pooling to produce a scalar score. The training process thus enables the self-evaluation model to assess the quality of any subset by estimating its proximity to the optimal solution.\n$\\text{SE}(A_{sub}) = \\text{MeanPooling} (\\text{Transformer}_{\\phi} [\\mathbf{h}^j_i, \\mathbf{h}^m_k, \\mathbf{h}_{m_kji}, \\text{BV}(A_{sub})])$.\n Figure 2 shows the overall architecture of the self-evaluation framework. The self-evaluation model is trained to minimize the Mean Squared Error (MSE) loss, aligning the predicted score $\\text{SE}(A_{sub})$ with the true score TS($A_{sub}, A_{opt}$):\n$\\mathcal{L}_{\\text{self-eval}} = \\frac{1}{|D|} \\sum_{i=1}^{|D|} (\\text{SE}(A_{sub}) - \\text{TrueScore}(A_{sub}, A_{opt}))^2$.\nIn essence, the self-evaluation model aims to discern how closely a given subset resembles an optimal subset, providing critical feedback for selecting promising solutions during inference. The training process is summarized in Algorithm 1, covering both the policy and self-evaluation models.\nDuring inference, the policy model and the self-evaluation model work together to determine the best subsets of actions at each timestep. At timestep t, the policy model generates $n$ candidate subsets by sampling from the learned probability distribution $\\pi_{\\theta}(a|s)$. Each subset is composed of $k$ feasible actions.\nThe self-evaluation model scores each of these subsets based on their optimality. The subset with the highest score is then selected for execution:\n$A^* = \\arg \\max_{A \\in A_t} \\text{SE}(A)$.\nwhere $A_t$ denotes the set of all candidate subsets generated by the policy model at timestep $t$, and $\\text{SE}(A)$ is the scalar score assigned to each subset by the self-evaluation model."}, {"title": "5. Experiments", "content": "In this section, we evaluate SEVAL, our proposed method.\nThe code and implementation details will be made publicly available upon acceptance of the paper."}, {"title": "5.1. Experimental Setup", "content": "JSSP Dataset. The training dataset comprises 40,000 instances, with 90% of the data used for training and 10% for validation. All instances were solved using Google OR-Tools (Perron & Didier), with a computation time limit of 60 seconds per instance. The data generation process followed the methodology proposed by Taillard (Taillard, 1993), generating instances with the number of jobs and machines ranging from 12 to 20 and processing times were uniformly sampled from the range [1, 99].\nJSSP Test Benchmarks. The performance of our method has been evaluated using two widely recognized benchmark datasets. The first is the Taillard dataset (Taillard, 1993), which contains 80 instances ranging from 15 jobs and 15 machines (225 operations) to 100 jobs and 20 machines (2000 operations). The second is the Demirkol dataset (Demirkol et al., 1998), which includes 80 instances with varying configurations, ranging from 20 to 50 jobs and machines. Notably, the Demirkol dataset introduces additional diversity by not following the same distribution as the Taillard dataset. The inclusion of both benchmarks allows us to evaluate the generalization capabilities of our approach on larger and more diverse instances than those used during training. \nJSSP Baselines. We compared our approach against several state-of-the-art methods. Among the DRL approaches, we included the actor-critic framework (L2D) proposed by (Zhang & et al., 2020), representing one of the pioneering works in this field, as well as the more recent HGNN-based approach, ResSch, introduced in (Ho et al., 2024).\nFrom the methods allowing multiple assignments without employing self-evaluation mechanisms, we considered RLCP (Tassel et al., 2023), which uses a Transformer-based architecture and combines imitation learning with policy gradient methods. Additionally, we evaluated the approach proposed in (Echeverria et al., 2024a), which employs HGNNS and supervised learning and serves as the basis for this work. We also included emerging self-supervised learning approaches, such as SPN (Corsini et al., 2024) and SI GD (Pirnay & Grimm, 2024). For the comparison with SEVAL and other algorithms, the greedy strategy was chosen, selecting the action with the highest probability at each step, as our method generates a single solution without relying on sampling strategies.\nFor non-constructive methods, the improvement-based methods L2S (Zhang et al., 2024) was included, using its 500-step solution improvement variant to achieve comparable execution times. Finally, we compared our approach against the CP-SAT solver from Google OR-Tools, setting a time limit of 3600 seconds as a reference for a traditional optimization method. \nPerformance Metric. The evaluation metric used is the optimal gap (OG), defined as:\n$OG = (\\frac{C_p}{C_{ub}} - 1) 100$,\nwhere $C_p$ is the makespan obtained by the policy, and $C_{ub}$ denotes the optimal or best-known makespan for the instance.\nModel Configuration. The HGNN consisted of six layers, each with three attention heads and a hidden dimension of 32. The policy and self-evaluation models utilized a Transformer architecture with four layers, eight attention heads, latent and feed-forward dimensions of 128, and GeLU as the activation function. The number of subsets evaluated by the self-evaluation model was set to 16."}, {"title": "5.2. Experimental results", "content": "Results on the Taillard Benchmark. The experimental results on the Taillard benchmark, presented in Table 1, show the outcomes for 80 instances grouped into sets of 10 by size. SEVAL, our proposed method, demonstrates superiority across all subsets except one. \nNotably, in the largest group of instances (100\u00d720), SEVAL achieves pseudo-optimal results with a mean gap of just 0.5%, outperforming those obtained by OR-Tools, even when the latter is allocated an execution time of one hour per instance.\nAnother noteworthy aspect is the remarkable improvement in performance metrics for this problem in recent years."}, {"title": "6. Conclusion and Future Work", "content": "In this paper, we propose a self-evaluation mechanism applied to the JSSP, achieving state-of-the-art performance on two widely recognized benchmarks. By evaluating subsets of actions collectively and leveraging a hybrid architecture of HGNNs and Transformers, our approach demonstrates strong generalization and robustness, outperforming classic methods like OR-Tools on larger instances. Future work includes combining supervised learning with reinforcement and self-supervised approaches, integrating beam search for improved inference, and extending the methodology to other combinatorial optimization problems, such as vehicle routing and resource allocation."}, {"title": "A. Features and edges of the state", "content": "For job-type and operation-type nodes, the feature set is defined as:\n\u2022 Binary Completion Indicator: For jobs, $b_j \\in {0,1}$, where $b_i = 1$ if job $j$ is completed; otherwise, $b_j = 0$.\n\u2022 Operation Readiness Indicator: For operations, $b_o \\in {0,1}$ indicates whether operation $o$ is ready to be scheduled.\n\u2022 Completion Time: $t_j \\in \\mathbb{R}_{>0}$ is the timestamp of the last completed operation for job $j$.\n\u2022 Mean Processing Time: $p_o \\in \\mathbb{R}_{>0}$ represents the expected duration of operation $o$.\n\u2022 Remaining Operations Count: $r_j = |\\mathcal{O}_{rem}^j|$ denotes the number of pending operations for job $j$.\n\u2022 Remaining Workload: $s_j = \\sum_{o \\in \\mathcal{O}_{rem}^j} p_o$, where $p_o \\in \\mathbb{R}_{>0}$ is the processing time for operation $o$.\n\u2022 Job Completion Time Difference: $t_j - t_{min}$, where $t_{min}$ is the minimum completion time across all jobs.\nFor machine-type nodes, the feature set is derived from machine-related metrics, capturing key information about machine states and performance:\n\u2022 Number of Pending Operations: $|\\mathcal{O}_{pending}^m|$, the total number of operations yet to be assigned to machine $m$.\n\u2022 Assignable Operations Count: $|\\mathcal{O}_{assignable}^m|$, the number of first operations from each job that can currently be assigned to machine $m$.\n\u2022 Sum of Processing Times: $\\sum_{o \\in \\mathcal{O}_{assignable}^m} p_{o,m}$, the total processing time of all operations assignable to machine $m$.\n\u2022 Last Assigned Completion Time: $t_{mal}^m$, the completion time of the last operation assigned to machine $m$.\nEdges in the graph capture relationships between nodes and are defined as:\n\u2022 Undirected Edges (Machine-Operation): Represent the compatibility between machines and operations, carrying features such as processing time.\n\u2022 Directed Edges (Operation-Job): Define the relationship between operations and their jobs, indicating job ownership.\n\u2022 Directed Edges (Operation-Operation): Enforce precedence constraints between dependent operations.\n\u2022 Directed Edges (Machine-Job): Connect machines to the first pending operation of each job, including processing time features.\nTwo edge types carry specific features: operation-machine edges and job-machine edges. The features for operation-machine edges include:\n\u2022 Processing Time: $p_{o,m}$, the time required to execute operation $o$ on machine $m$.\n\u2022 Processing Time Relative to Job Workload: $\\frac{p_{o,m}}{\\max_{o' \\in O_j} p_{o'}}$ , where $O_j$ is the set of remaining operations for job $j$, and $p_{o'}$ is the processing time of $o'$.\n\u2022 Processing Time Compared to Machine Capability: $\\frac{p_{o,m}}{\\max_{o' \\in O_m} p_{o'}}$ where $O_m$ is the set of operations machine $m$ can process, and $p_{o'}$ is the processing time of $o'$."}, {"title": "B. Example of Action Space Transition and State Transitions", "content": "This section provides an illustrative example of the transitions in the action space and state representations for a JSSP instance.\nThe set of possible assignments in the initial step is given by:\n$IM = {(j_1, m_4), (j_2, m_4), (j_3, m_3), (j_4, m_2)}$.\nThe corresponding power set, ensuring that no machine is assigned more than one job simultaneously, defines the action space:\n$A = {{(j_1, m_4)}, {(j_2, m_4)}, . . ., {(j_1, m_4), (j_3, m_3)}, {(j_1, m_4), (j_4, m_2)}, ..., {{(j_1, m_4), (j_3, m_3), (j_4, m_2)}}}$.\nAn element of A is selected in each step, and the actions within this subset are executed. The corresponding operations are then removed from the graph, and the features of the remaining nodes are updated. This process repeats until all operations have been assigned."}, {"title": "C. Dataset Generation Process", "content": "A diverse dataset is created by solving small-scale JSSP instances with an efficient solver. Solutions are converted into sequences of scheduling states, represented as heterogeneous graphs, along with their corresponding feasible job-machine assignments. Perturbing optimal solutions introduces variety, enhancing the model's generalization and performance in complex scenarios."}, {"title": "D. Model architecture", "content": "In this section, we illustrate how embeddings are calculated within the HGNN model, using the machine embeddings as an example. The embedding process begins with a linear transformation applied to the initial features of machine, job, and operation nodes, denoted as $\\mathbf{h}_m$, $\\mathbf{h}_o$, and $\\mathbf{h}_j$, respectively. These features represent the states of machines, operations, and jobs after the linear transformation, which ensures dimensional consistency. The embeddings are then iteratively updated across L layers of the HGNN. At each layer, the new output is added to the input embedding, ensuring that the embeddings are progressively refined while retaining information from previous layers.\nMachines aggregate information from connected operations. The attention coefficient between a machine $m_i$ and a connected operation $o_{ij} \\in O_{m_i}$ is computed as:\n$e_{o_{ijk}}^{mo} = \\alpha^{mot} \\text{LeakyReLU} (W^{mo} \\mathbf{h}_{m_i} + W^{mo} \\mathbf{h}_{o_{ij}} + W^{mo} \\mathbf{h}_{p_{ijk}})$,\nwhere $\\alpha^{mot} \\in \\mathbb{R}^{3d_{mo}}$ is a learnable vector, and $W^{mo}, W^{mo}, W^{mo} \\in \\mathbb{R}^{d_{mo} \\times d_{mo}}$ are trainable weight matrices. The edge embedding $\\mathbf{h}_{p_{ijk}}$ contains features of the task connecting the machine and the operation.\nSimilarly, machines also aggregate information from jobs if the first operation of a job j is ready to be assigned to machine $m_i$. For these connections, the attention coefficient between a machine $m_i$ and a job $j \\in J$ is computed as:\n$e_{m_{jik}}^{mi} = \\alpha^{mit} \\text{LeakyReLU} (W^{mi} \\mathbf{h}_{m_i} + W^{mi} \\mathbf{h}_j + W^{mi} \\mathbf{h}_{m_{jij}})$, \nwhere $W^{mit}$ and $W^{mi}, W^{mi}, W^{mi}$ are learnable parameters, and $\\mathbf{h}_{m_{jij}}$ represents the features of the edge connecting the machine and job nodes. These edges are only considered if the first operation of the job is ready and can be assigned to the machine.\nOnce the attention coefficients are normalized using a softmax function, the updated embedding for a machine is calculated as:\n$\\mathbf{h}_{m_i}' = \\text{ELU} ( \\sum_{o_{ij} \\in O_{m_i}} A_{o_{ijk}}^{mo} (W^{mo} \\mathbf{h}_{o_{ij}} + W^{mo} \\mathbf{h}_{p_{ijk}}) + \\sum_{j \\in J_{m_i}} A_{m_{jik}}^{mi} (W^{mi} \\mathbf{h}_j + W^{mi} \\mathbf{h}_{m_{jij}}))$,\nwhere $A_{o_{ijk}}^{mo}$ and $A_{m_{jik}}^{mi}$ are the normalized attention coefficients for machine-operation and machine-job edges, respectively.\nThis process is repeated for L layers of the HGNN. At each layer l, the input embedding $\\mathbf{h}_{m_i}^{(l-1)}$ is updated as:\n$\\mathbf{h}_{m_i}^{(l)} = \\mathbf{h}_{m_i}^{(l-1)} + \\mathbf{h}_{m_i}'$,\nwhere $\\mathbf{h}_{m_i}$ is the initial embedding."}, {"title": "E. Execution time comparation", "content": "Tables 4 and Table 5 present the computation times (in seconds) for deep learning methods applied to the Taillard and Demirkol benchmark problems. For all methods, the open-source implementations were utilized. It is important to note that execution time comparisons depend significantly on the specific implementation and the configuration of the computational environment.\nIn general, methods such as L2D and SPN exhibit faster execution times, primarily due to their simpler neural network architectures. Other methods require slightly longer computation times, which reflects the increased complexity of their architectures and algorithms."}]}