{"title": "Blockchain-based Crowdsourced Deep Reinforcement\nLearning as a Service", "authors": ["Ahmed Alagha", "Hadi Otrok", "Shakti Singh", "Rabeb Mizouni", "Jamal Bentahar"], "abstract": "Deep Reinforcement Learning (DRL) has emerged as a powerful paradigm for solving\ncomplex problems. However, its full potential remains inaccessible to a broader au-\ndience due to its complexity, which requires expertise in training and designing DRL\nsolutions, high computational capabilities, and sometimes access to pre-trained mod-\nels. This necessitates the need for hassle-free services that increase the availability of\nDRL solutions to a variety of users. To enhance the accessibility to DRL services, this\npaper proposes a novel blockchain-based crowdsourced DRL as a Service (DRLaaS)\nframework. The framework provides DRL-related services to users, covering two types\nof tasks: DRL training and model sharing. Through crowdsourcing, users could benefit\nfrom the expertise and computational capabilities of workers to train DRL solutions.\nModel sharing could help users gain access to pre-trained models, shared by workers in\nreturn for incentives, which can help train new DRL solutions using methods in knowl-\nedge transfer. The DRLaaS framework is built on top of a Consortium Blockchain to\nenable traceable and autonomous execution. Smart Contracts are designed to manage\nworker and model allocation, which are stored using the InterPlanetary File System\n(IPFS) to ensure tamper-proof data distribution. The framework is tested on several\nDRL applications, proving its efficacy.", "sections": [{"title": "1. Introduction", "content": "The recent advancements in Deep Reinforcement Learning (DRL) have demon-\nstrated significant potential in applications such as robot swarms [1], autonomous driv-\ning [2], and video games [3]. DRL is a paradigm within the realm of Artificial Intelli-\ngence (AI) that seamlessly blends the power of Deep Learning (DL) and Deep Neural\nNetworks (DNNs) with the principles of Reinforcement Learning (RL). In DRL, agents\nare able to develop their own intelligence through rewarded interactions with the envi-\nronment, based on a previously designed learning algorithm that translates the collected\nexperiences into an action policy."}, {"title": "2. Related Work", "content": "Since the proposed DRLaaS framework is the first to tackle the DRL problem in a\ncrowdsourcing manner, this section gives an overview of existing works in the fields of\n1) MLaaS and 2) Crowdsourcing for Machine Learning, which lay the foundations of\nthe proposed framework."}, {"title": "2.1. Machine Learning as a Service", "content": "The advent of MLaaS provided several platforms in the industry that allow users\nwith varying skill sets to leverage its capabilities. Several MLaaS providers offer pre-\nbuilt ML models and/or tools that enable MLaaS users to obtain ML models. Google\nCloud is one platform providing users with services, such as AutoML and Vertex AI.\nAutoML provides a user-friendly interface for users with limited experience, where\nusers can upload their labeled datasets and the service takes care of model training\nand optimization. AutoML offers services for a varying set of tasks, including image\nrecognition, natural language processing, and video analysis. Vertex AI, on the other\nhand, is an end-to-end ML platform that offers a set of tools for building and training\nML models. Similar to AutoML, Amazon SageMaker and Microsoft Azure facilitates\nthe end-to-end ML process by offering a range of tools that cater to different skill lev-\nels. Google Colab provides a cloud-based development environment for ML through a\nJupyter Notebook interface. Colab is mainly known for providing access to powerful,\nbut limited without paid subscription, GPUs and TPUs (Tensor Processing Units) for\nfaster training. While SageMaker, Azure, and Colab can be used to train DRL sys-\ntems, the higher complexities of DRL problems hinder their usability, especially for\ninexperienced users."}, {"title": "2.2. Crowdsourcing for Machine Learning", "content": "The intersection between Crowdsourcing and ML is twofold: some works utilize\nML and DRL in designing crowdsourcing systems [26, 27, 28], while other works\ncrowdsource certain ML tasks [9, 29, 11, 30, 13]. We present an overview of the sec-\nond set of works in the literature, since our work aims to crowdsource DRL tasks.\nExisting works using crowdsourcing for ML mainly use the crowd for data collection\nand processing. In [9], a dataset of breathing and coughing sounds that are collected\nvia crowdsourcing is used for COVID-19 diagnosis. The authors in [29] propose a ML\nframework for flood forecasting systems using data collected through crowdsourcing,\nwhere workers report data about actual flooding incidents, such as rainfall intensity lev-\nels, continuing rainfall duration, and drainage ability. The authors in [11] develop a ML\nmodel to evaluate the performance of workers in categorizing neurotypical and autis-\ntic children. The data are collected through a crowdsourcing platform, where workers\nwatch videos of children and fill out a series of questions about the child's behavior. In\n[30], the authors propose a collaborative crowdsourcing system for ML data labeling.\nIn the proposed system, groups of workers collaborate in labeling data through three\nstages: Vote (choosing the label), Explain (reasoning behind the label), and Catego-\nrize (review other workers' explanations). The authors in [13] propose a crowdsourced\nannotation framework for data in sound event detection applications. The goal is to\nestimate strong labels, i.e. data labels with high confidence, using weak labels that go\nthrough methods such as majority voting."}, {"title": "3. Background", "content": "This section gives a general formulation of the DRL problem, along with the main\naspects to be considered when designing and training DRL solutions."}, {"title": "3.1. DRL Formulation", "content": "In RL, agents interact with the environment based on their observations, and learn\nto alter their behaviors in response to rewards received [31]. In this setup, an au-\ntonomous agent observes a state $s_t$ from its environment at time step $t$, and interacts\nwith the environment by taking action $a_t$. Based on this action, the environment re-\nturns a reward value $r_t$, and transitions into a new state $s_{t+1}$. The goal for the agent\nis to learn a policy $\\pi$ that maximizes the collected rewards by trying to take the best\nsequence of actions. A policy takes as input the agent's observations and returns the\nbest action or a probability distribution over the possible actions. The recent advance-\nments in DNN have led to their integration in DRL, where most DRL policies come in\nthe form of DNNs. Formally, DRL is described as a Markov Decision Process (MDP),\nwhich consists of:\n\u2022 S denotes the finite set of states;\n\u2022 A denotes the finite set of actions;\n\u2022 P is a set of Markovian state transition probabilities, where $P(s_{t+1}|s_t, a_t)$ is the\nprobability that taking an action $a_t$ in state $s_t$ results in a transition to state $s_{t+1}$;\n\u2022 $R(s_t, a_t, s_{t+1})$ denotes a reward function;\n\u2022 and $\\gamma \\in [0, 1]$ is the discount factor for the trade-off between instantaneous and\nfuture rewards.\nGenerally, the policy $\\pi$ maps a state to a probability distribution over the possible ac-\ntions $\\pi : S \\rightarrow P(A = a|S)$. In most real-world problems, an agent does not have a\nfull observation of the environment. In such cases, the problem is defined as a Partially\nObservable MDP (POMDP), where a policy maps the agent's observations to actions\n[5]."}, {"title": "3.2. DRL Design and Training Requirements", "content": "The design and training of DRL models are complex tasks, requiring specialized\nexpertise, computational capabilities, and often compatible pre-trained models. This\nsection explains these needs, which are to be met by the proposed crowdsourcing\nframework."}, {"title": "3.2.1. Expertise: Environment, Reward, and Optimization", "content": "The process of designing and training DRL solutions requires expertise in several\nareas. This process can be summarized in three main steps, namely 1) Environment\nDesign, 2) Reward Engineering, and 3) Policy Optimization. The Environment Design\nstep is concerned with modeling the problem of interest in an environment that follows\nthe DRL formulation. This requires the designer to have expertise in developing en-\nvironments that mimic real-world dynamics and encapsulate the possible interactions\n(actions) between the agent(s) and the environment. For example, in a simple environ-\nment for the game of chess, the designer needs to fully understand the rules that govern\nthe game, which are needed to define the possible actions for the chess agent. Addi-\ntionally, the state of the chess environment could be modeled as a grid image of the\nboard, or by numerical features representing the locations of the pieces. Such a choice\nis critical to be made by the designer, as it affects the training process. The Reward En-\ngineering step is critical since the behavior of the agents in DRL is directly influenced\nby the reward function. Reward functions require considerable engineering, which if\ndone poorly could lead to local optima [18]. This requires the designer to have knowl-\nedge in the domain of interest, as well as expertise in reward shaping [32, 33]. For the\ngame of chess, for example, a simple reward function is to assign a positive value only\nfor a checkmate. However, this complicates the learning since the agent does not get\nregular feedback, and hence calls for the need for more complex reward functions that\nrequire expertise in this domain. In the Policy Optimization step, the aim is to design\npolicies that translate the agent's observations into actions, which are then optimized\nusing the collected rewards. In DRL, policies are represented as DNNs, and the choice\nof architecture depends on several attributes, such as the type and dimensionality of the\ninput and the problem complexity, which require expertise in DL. On the other hand,\nthe optimization algorithm determines how the collected experiences (observations and\nrewards) update and optimize the agent's policy. Designing and choosing the appropri-\nate optimization algorithm requires expertise in the domain of the application, as well\nas in hyperparameter tuning, which is essential in finding optimal solutions. For exam-\nple, in the aforementioned chess environment, convolutional neural networks (CNNs)\ncould be used as policy architectures if the state is represented as an image, while feed\nforward networks (FFNs) could be used if the state is represented in numerical features.\nThese could be optimized using methods such as Proximal Policy Optimization (PPO),\nDeep Q-Networks (DQN), or Deep Deterministic Policy Gradients (DDPG). All of the\naforementioned are decisions to be made by the designer."}, {"title": "3.2.2. Computational Capabilities", "content": "Due to the complexity of DRL, the training process usually requires heavy com-\nputational resources. While the choice of optimization algorithm has a significant ef-\nfect on the learning speed and convergence, the availability of computational resources\nholds equal significance. For example, the availability of GPUs could significantly\nspeed up the execution of DNNs, which in turn speeds up the learning, especially for\nimage-based applications (2D data with Convolutional Neural Networks). Addition-\nally, many policy optimization algorithms for DRL can harness parallel processing\non multiple processing units (CPU cores or GPUs) to speed up the learning process\nby parallelizing the experience collection process. Moreover, most DRL optimization"}, {"title": "3.2.3. Model Availability and Compatibility", "content": "The DRL training process has been recently approached using methods from imita-\ntion learning. Recent works [34, 35, 18] proposed using demonstrations from an expert\nin guiding the learning of DRL agents. An expert model is a pre-trained model that is\nalready familiar with the environment (or with a similar environment), and can help the\nagents collect better experiences, or can be used partially in Behavioral Cloning (BC)\nto enhance the DRL policy. Here, the availability of such expert models becomes an\nimportant issue to address. Additionally, the compatibility of the expert models with\nthe current environment of interest is another crucial challenge. If the demonstrations\nsuggested by the expert introduce variances to the current DRL training problem, this\nintroduces difficulties in the learning convergence. This necessitates the availability of\nmodels that are similar to the current environment.\nAll the aforementioned needs are to be met by the proposed crowdsourcing frame-\nwork in this work. The complicated design processes call for the need of expertise,\nwhich can be obtained through crowdsourcing. Additionally, crowdsourcing could\nhelp in providing computational resources through workers that have access to multiple\nCPUs and GPUs. The recent methods enhancing DRL through expert demonstrations\ncall for the need of compatible expert models, which can also be obtained through\ncrowdsourcing."}, {"title": "4. Blockchain-based DRLaaS Framework", "content": "The proposed framework aims to target two types of DRLaaS tasks that can be\nrequested by users, namely DRL training and model sharing. In DRL training tasks,\nusers can request the design and training of DRL solutions to be done by expert work-\ners, i.e. workers with experience in tackling problems in the domain of interest using\nDRL. This task is highly dependent on the worker's expertise and computational capa-\nbilities, as discussed in Section 3.2. On the other hand, in model sharing tasks, users\ncan request pre-trained models in certain domains, which can be shared by other expert\nworkers. This task depends on the efficiency and compatibility of the available mod-\nels with the requester's environment. The flow of the proposed framework is shown\nin Fig. 2, which is to be detailed in this section by discussing the different DRLaaS\ntasks, the recruitment metrics and processes, and the design of the smart contracts. In\nthis framework, users register in the system by submitting information about their at-\ntributes. Users can then submit task requests by indicating their requirements. For DRL\ntraining tasks, workers are selected following the designed recruitment process. The re-\ncruitment is done based on the workers' attributes and their expected Quality of Service\n(QoS), which considers metrics related to the expertise and computational capabilities\nof the candidate workers. A recruited worker then performs the training task, which in-\ncludes steps such as environment design, reward engineering, policy optimization, and\nmodel tuning, before submitting the resultant model. For DRL model sharing, workers"}, {"title": "4.1. Problem Formulation", "content": "Each task type, i.e. DRL training and model sharing, has its own requirements\nand recruitment metrics to be used when selecting workers. Generally, given a set\nof tasks $T = \\{T_1, T_2, T_3, ...\\}$ submitted by requesters and a pool of candidate workers"}, {"title": "4.2. Worker Recruitment Parameters", "content": "Each of the two task types requires a dedicated worker recruitment process to select\nthe most suitable set of workers to execute the task. To address this, we propose two\ndifferent Quality of Service (QoS) metrics which are used to assess candidate workers\nand select the most suitable for each type of tasks. The proposed QoS metrics take into\nconsideration the task requirements and worker attributes, and ensure that the selected\nworkers meet the desired expectations."}, {"title": "4.2.1. DRL Training Tasks", "content": "A worker here is asked to design a suitable environment for the task and engineer\nan efficient reward function. Additionally, the worker is required to choose a suit-\nable DRL optimization algorithm, train a policy for the task at hand, and fine-tune\nthe trained model (policy). The worker's expertise is essential for designing the en-\nvironment and choosing a suitable policy architecture (type of neural network) and\noptimization method based on the provided environment. The availability of compu-\ntational resources is also crucial for training and fine-tuning the model in a reasonable\ntime."}, {"title": "4.2.2. DRL Model Sharing Tasks", "content": "In model sharing tasks, a user shares information about their pre-trained model with\nthe platform, with the hope of receiving incentives if the model is requested by other\nusers. For a given model sharing task $T_i$, a requester specifies the Problem Description\n(DT), Problem Domain (Domi), Reputation Requirement (Repmin), Time Constraint\n(TCT), and Number of Workers (NWT). In Di, the requester specifies information\nabout the environment they hope to train, which are essential to find suitable shared\nmodels. The number of workers indicates the desired number of models to be shared\nwith the requester, as some DRL methods require multiple models to assist in the learn-\ning [38]."}, {"title": "4.3. Recruitment Optimization Process", "content": "Given a task $T_i$ and a pool of candidate workers, the recruitment optimization pro-\ncess aims to recruit a group of NWT workers that maximize the expected QoS, while\nmeeting the task requirements and constraints. To do so, a Greedy algorithm for worker\nrecruitment is used, where the optimization process is treated as a knapsack problem.\nGreedy algorithms are the common in crowdsourcing recruitment works [39, 19], due\nto their simplicity and scalability. While other algorithms, such as Genetic algorithm\nand Particle Swarm Optimization, could be used, the simplicity of Greedy algorithm\nmakes it more suitable for deployment on the blockchain. In this problem, the aim is\nto maximize the weight of items filled in the knapsacks without violating its maximum\ncapacity. In the context of worker recruitment, the maximum capacity represents the\nspecified group size NW\u00b9, while the weights represent the individual QoS values of\nthe workers. Regardless of the task type (training or model sharing), the recruitment\nprocess is the same, with the QoS evaluation and task requirements being the difference\nbetween the task types. Once a task is pushed to a worker, they are given a time limit\nto accept, after which the task request is retracted and given to the next best worker.\nThe recruitment process used in this work is described in Fig. 3."}, {"title": "4.4. Smart Contracts Implementation", "content": "In this work, the crowdsourcing platform is built on top of a Consortium Blockchain.\nThe blockchain is responsible for managing users' registration, task requests, task al-\nlocation, and feedback through smart contracts. A blockchain is used instead of a cen-\ntralized management system to provide a decentralized, transparent, and autonomous\nplatform for crowdsourcing with no repudiation. A Consortium Blockchain, specifi-\ncally, is used due to its ability to offer increased privacy, shared control, efficiency, cost\nsavings, and trust for multiple organizations or entities collaborating on a project or\nsharing data [37]. A Consortium Blockchain is operated by a group of entities, which\nintroduces increased privacy and trust when compared to public Blockchains, and more\ncollaboration allowance when compared to private Blockchains, making them suitable\nfor crowdsourcing. Recent works explored the utilization of blockchain with DRL\n[38, 40], where agents cooperate to train models on the chain, but none provided solu-\ntions or services for users.\nTo execute the proposed crowdsourcing framework on the blockchain, three smart\ncontracts are designed: 1) Users Manager Contract (UMC), Tasks Manager Contract\n(TMC), and Models Manager Contract (MMC). The users interact with UMC to reg-\nister in the system by providing information. Task requesters interact with the TMC\nto submit tasks and provide requirements. The TMC is responsible for the worker re-\ncruitment, task allocation, task submission, and feedback processes. If users decide to"}, {"title": "4.5. Framework Time Sequence", "content": "Figure 4 shows a time sequence diagram for scenarios under the proposed blockchain-\nbased crowdsourced DRLaaS. It discusses the interactions between the users and the\nsmart contracts constituting the framework. For DRL training tasks, these interactions\nare given as follows:\n\u2022 User Registration: Users register to the UMC by invoking the addWorker() and\naddRequester() functions. Each user, worker or requester, has a unique Ethereum\naddress linked to their account. Workers provide information related to their\ndomains and computational capabilities. The rest of the attributes are initialized\nand updated following task executions.\n\u2022 Task Request and Allocation: A request creates a task by interacting with the\nTMC through the addTask() function and providing the necessary information\nand the required payment. The TMC allocates the task to suitable workers\nthrough the allocateTask() function, and workers perform the task and return\ntheir outcomes through IPFS to TMC through the submitOutcome() function.\nThe outcomes are then forwarded to the task requester."}, {"title": "5. Simulation and Evaluation", "content": "This section presents and discusses several experiments conducted to validate the\nproposed methods. The experiments are conducted on several Multi-agent DRL en-\nvironments, namely Target Localization [34, 17], Fleet Coordination for Autonomous\nVehicles [20], and Multi-Agent Maze Cleaning [21]. We opted to employ Multi-agent\nDRL instead of single-agent DRL due to its increased complexity, which allows for a\nmore rigorous examination of the robustness and adaptability of our proposed methods.\nAll the simulations have been conducted using an Intel E5-2650 v4 Broadwell work-\nstation equipped with 128 GB RAM, 800 GB SSD, and NVIDIA P100 Pascal GPU (16\nGB HBM2 memory)."}, {"title": "5.1. DRL Application Environments", "content": "The DRL environments used in to validate the proposed methods are:\n\u2022 Target Localization [34, 17]: this is a multi-agent problem in which the location\nof a certain target is to be identified, based on sensory readings collected by the\nsensing agents. This is common in applications related to radiation monitoring,\nsearch and rescue, and path-finding. In this problem, the sensing agents (robots\nor UAVs) observe the environment, collect data readings, and communicate with\neach other in order to cooperate and locate the unknown target. The learning\nproblem is complicated since the agents need to learn how to communicate and\ncoordinate, in addition to how to take the best set of actions in order to reach\nthe target as fast as possible. We model the DRL environment as discussed and\npresented in [17]. The agents' observations are modeled as 2D heatmaps and\nfed to a Convolutional Neural Network (CNN) that acts as an actor network in a\nProximal Policy Optimization (PPO) algorithm. A sample scenario of the target\nlocalization problem is shown in Fig. 5a.\n\u2022 Multi-Agent Maze Cleaning [21]: in this problem, a group of agents is placed\nin a maze with the task of cleaning it as fast as possible. Initially, the maze\nis entirely dirty, and each spot covered by an agent is considered to have been\ncleaned. This problem requires coordination between the agents to allocate tasks,\nand for them to learn how to quickly clean the maze. Each agent observes its\nown location, the location of the other agents, as well as the status of the map,\nin 2D format. The observations are fed to a CNN as the actor network in a PPO\nalgorithm. A sample scenario of the maze cleaning problem is shown in Fig. 5b.\n\u2022 Fleet Coordination for Autonomous Vehicles [20]: in this problem, a team of\nautonomous vehicles is tasked with picking up and dropping customers at spe-\ncific locations. Customers are randomly distributed in a certain area of interest,\nwith each customer having a certain desired destination. Vehicles have a limit\nin terms of the number of customers accommodated simultaneously. The team's\ngoal is to minimize the time needed to pick up and drop off customers off at their\ndestinations. The complexity of the learning comes from the fact that the agents\n(i.e. the vehicles) need to cooperate to properly allocate tasks, such as the time\nis minimized. This is seen as a multi-objective DRL problem, since each agent"}, {"title": "5.2. Performance Analysis", "content": "This section evaluates the methods in the proposed DRLaaS framework. To val-\nidate the proposed framework, the key attributes in the worker selection metrics are\nevaluated, including agent's computational capabilities $CCW_j$; and model similarity\n$S(m_k^W, T_i)$. The learning convergence in the following experiments is evaluated in\nterms of Episode Length, which is the time it takes for the agents to finish the task.\nThe episode length is tracked throughout the learning, to reflect how fast the agents"}, {"title": "5.2.1. DRL Training Tasks", "content": "As discussed in Section 4.2.1, most common DRL algorithms utilize parallelized\nprocessing to run copies of the environment for experience collection during training.\nTo verify the importance of considering the worker's CPU capabilities CCCpu during\nthe recruitment stage for DRL training tasks, Fig. 6 presents the effect of varying\nthe number of CPU cores on the training convergence of DRL for the 3 applications.\nFor many applications, parallelizing the data collection in DRL does not significantly\naffect the learning convergence, which is the case for Target Localization (Fig. 6a) and\nFleet Coordination (Fig. 6c). However, in some applications where the task has a long\nhorizon (takes generally longer time steps to finish), increasing the number of parallel\nprocesses brings benefits, as seen in Fig. 6b for the Maze Cleaning environment. This is\nbecause parallelizing the data collection enhances the exploration process, since more\nunique experiences in different parallel environments are being collected, resulting in\nbetter training."}, {"title": "5.2.2. DRL Model Sharing", "content": "To study the importance of the similarity metric proposed in Eq. 7 for the DRL\nmodel sharing tasks, Fig. 8 shows the learning performance when using different ex-\npert models in assisting DRL, using Demonstration Cloning (DC) [34]. In DC, expert\nmodels help current agents collect better experiences with more exposure to reward\nvalues, resulting in better learning. For the target localization problem (Fig. 8a), a\nteam of agents is to be trained on environment with 3 agents and 3 walls (3A3W), with\nthe help of 3 expert models that have been previously trained on different environments,"}, {"title": "5.2.3. Recruitment Optimization", "content": "To validate the choice of the greedy algorithm for the optimization of the recruit-\nment process, the proposed method is benchmarked against common methods in crowd-\nsourcing works, such as Genetic Algorithm (GA) [42], Particle Swarm Optimization\n(PSO) [43], and Ant Colony Optimization (ACO) [44]. In GA-based methods, a pop-\nulation of candidate solutions (i.e. possible sets of workers) is created and iteratively"}, {"title": "5.2.4. Blockchain and Smart Contracts Complexity Analysis", "content": "To analyze the complexity and feasibility of the proposed smart contracts, Table 5\npresents the gas cost of the deployment and execution of the smart contracts and their\nfunctions. Gas cost is defined as the computational effort required to execute operations\nsuch as smart contracts. In public blockchains, the gas cost can be used to determine\nthe fees to be paid for executing transactions or running smart contracts based on the\ngas price. Since a Consortium Blockchain is proposed, the deployment and execution\nof the smart contracts do not require any payments by the users since the gas price is\nzero. However, the gas cost is a good measure of the complexity of the smart contracts\nand its functions to indicate their feasibility. As seen in the table, the gas costs are\nlow, reflecting the feasibility of the proposed contracts. For reference, we present a\nbenchmark of the gas cost of deploying and executing a similar UMC as discussed in\n[37]."}, {"title": "6. Conclusion and Future Directions", "content": "In this paper, a novel blockchain-based framework for crowdsourced Deep Rein-\nforcement Learning as a Service (DRLaaS) is proposed. The proposed framework aims\nat providing DRL-related services in terms of expertise and computational resources\nfor users in the DRL domain. The proposed framework encapsulates two possible task\ntypes: DRL training and model sharing. In this framework, users submit tasks to be\nallocated to appropriate workers, who then return task outcomes as per the require-\nments. Specific recruitment metrics and processes are designed, aiming to find the\nbest set of workers for a given task, while considering expertise, computational capa-\nbilities, reputation, and model similarity to the target environment. On a Consortium\nBlockchain, smart contracts are designed to manage worker recruitment and model\nsharing processes using Greedy methods. Experiments on several DRL applications,\nsuch as target localization, fleet coordination, and maze cleaning show the efficacy of\nthe proposed recruitment metrics, especially in terms of computational capabilities and\nmodel similarity.\nIn addition to the contributions made in this paper regarding worker recruitment\nfor crowdsourcing DRL training and model sharing tasks, there are several avenues\nfor future research and improvements. One important direction involves incorporat-\ning quality control mechanisms to ensure the reliability and accuracy of the submitted\noutcomes. Another direction is to investigate diverse incentive mechanisms through\ngame theoretic approaches to incentivize worker engagement and maintain participa-\ntion. All the aforementioned is while considering the special nature of DRL tasks and\ntheir needs."}]}