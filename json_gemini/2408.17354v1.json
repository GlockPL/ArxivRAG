{"title": "Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage", "authors": ["Md Rafi Ur Rashid", "Jing Liu", "Toshiaki Koike-Akino", "Shagufta Mehnaz", "Ye Wang"], "abstract": "Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of pre-trained models, allowing anyone to publish without rigorous verification. This scenario creates a privacy threat, as pre-trained models can be intentionally crafted to compromise the privacy of fine-tuning datasets. In this study, we introduce a novel poisoning technique that uses model-unlearning as an attack tool. This approach manipulates a pre-trained language model to increase the leakage of private data during the fine-tuning process. Our method enhances both membership inference and data extraction attacks while preserving model utility. Experimental results across different models, datasets, and fine-tuning setups demonstrate that our attacks significantly surpass baseline performance. This work serves as a cautionary note for users who download pre-trained models from unverified sources, highlighting the potential risks involved.", "sections": [{"title": "Introduction", "content": "In recent times, the traditional way of training a language model (LM) from scratch has been largely replaced by the introduction of pre-trained foundation models (Touvron et al. 2023; Chiang et al. 2023). For example, the Hugging Face Hub\u00b9 is a platform with over 120k open-source models, readily available for download and any registered user can contribute by uploading their own model. However, there are serious security and privacy risks associated with downloading such models from any untrusted sources and further fine-tuning them for some downstream applications as they could be maliciously crafted (Tram\u00e8r et al. 2022; Kandpal et al. 2023; Hu et al. 2022). Additionally, the public release of large language models (LLMs) fine-tuned on potentially sensitive user data could lead to privacy breaches, as these models have been found to memorize verbatim text from their training data (Carlini et al. 2019, 2021). In this paper, we combine the notion of poisoning a pre-trained LLM and causing privacy leakage of the fine-tuned model. More specifically, we introduce a novel model poisoning algorithm that aims to manipulate a pre-trained LLM in order to disclose more of the private data used during its fine-tuning.\nAt its core, our approach leverages machine unlearning (Cao and Yang 2015; Guo et al. 2019) to poison the pre-trained LLM. The original objective of unlearning is to make the model forget specific data points that it has seen during training so that it produces a high loss for those data points, and it becomes difficult to reconstruct those samples (Gu et al. 2024). Motivated by data augmentation that reduces overfitting, we discovered that unlearning on some noisy version of fine-tuning data points can promote overfitting of the original data during the fine-tuning process.\nHowever, it is important to have control over the process of loss maximization; otherwise, the model might become unusable and the poisoning attempt would be easily detectable. Hence, we propose bounded unlearning as a poisoning tool, where we maximize loss in a controlled manner on the pre-trained model for some noisy data points to increase privacy leakage of the fine-tuned LLM without compromising its utility.\nTo measure the privacy leakage caused by our proposed method, we consider two standard privacy attacks: membership inference (MIA) (Shokri et al. 2017a; Carlini et al. 2022a) and data extraction (DEA) (Nasr et al. 2023; Rashid et al. 2023). In MIA, the model is queried to evaluate whether a specific target data point that the attacker possesses was indeed part of the finetuning dataset. On the contrary, DEA aims to extract verbatim texts from the fine-tuning dataset with partial/zero prior knowledge. We evaluate our proposed method for both of these attacks on a range of language models (Llama2-7B, GPT-Neo 1.3B), datasets (MIND, Wiki-103+AI4Privacy), fine-tuning methods (Full-FT, LORA-FT, QLoRA-FT), and defense (differential privacy). Overall, our method significantly boosts the MIA and DEA attack performance over the baselines in almost all scenarios and maintains its stealth by preserving model utility. Prior works that deal with privacy leakage through pre-trained model poisoning pose some strong assumptions on the adversary's capability, as discussed in the Related Work section of the paper. Our proposed method, on the other hand, with a more practical threat model and weaker adversarial ability, substantially enhances the attack success rate and still remains stealthy."}, {"title": "Related Work", "content": "The privacy risks of LLMs have been extensively studied in prior works. A pioneering study by Carlini et al. (2021) introduced an attack method that successfully extracted publicly available internet texts by querying a pre-trained GPT-2 model. Earlier, Carlini et al. (2019) had brought attention to the issue of unintended memorization within LLMs. They introduced canaries\u2014deliberately inserted data points\u2014into the training dataset and used a metric called 'exposure' to assess the likelihood of these canaries being leaked.\nSubsequent research by Kim et al. (2023) and Lukas et al. (2023) developed algorithms to evaluate how much of the information memorized by LLMs constitutes sensitive personally identifiable information (PII) and examined the effectiveness of existing defenses in preventing such leakage. On a different front, Nasr et al. (2023) presented a scalable data extraction attack that forces production-level language models into deviating from their aligned behavior, leading them to output significant amounts of training data without requiring any prior knowledge.\nIn addition to these studies, several works have focused on membership inference attacks against LLMs. Rather than using reference-based attacks as seen in works like (Carlini et al. 2022a; Tram\u00e8r et al. 2022; Zarifzadeh, Liu, and Shokri 2024), Mattern et al. (2023) proposed neighborhood attacks. This method infers membership by comparing the model's output scores for a specific sample against those of synthetically generated neighboring texts. In the domain of clinical language models, Jagannatha, Rawat, and Yu (2021) conducted membership inference attacks and also compared the extent of privacy leaks between masked and autoregressive language models.\nWhile our study shares similar goals with the aforementioned works, the threat model we employ, particularly regarding the adversary's capabilities, differs significantly.\nThe idea of poisoning machine learning (ML) models has been largely applied in designing security attacks (Chen et al. 2017; Liu et al. 2020). However, a recent line of research has introduced the idea of poisoning/backdooring ML models in order to cause privacy leaks. Feng and Tram\u00e8r (2024) tampers with initial model weights and creates some data traps to compromise the privacy of future finetuning data. However, they assume access to the fine-tuned model weights to extract the trapped training data, whereas, in our work, we consider a black-box API access to the fine-tuned model. Tram\u00e8r et al. (2022) introduced a targeted poisoning attack that inserts mislabeled data points in the training dataset to cause higher membership inference leakage. Write access to the finetuning dataset is a strong assumption of the adversary's capability in real-world scenarios. Conversely, in our work, we consider a weaker threat model where an adversary can poison only the initial model. Liu et al. (2024) has served a similar purpose to ours by harnessing the memorization level of the pre-trained model. However, unlike our threat model, they assume that the adversary has side knowledge of the trainable modules during the finetuning process, and their auxiliary dataset needs to be drawn from the same distribution as the downstream training dataset. Apart from that, a very recent work (Wen et al. 2024) applied a more straightforward poisoning technique by minimizing the loss on the pre-trained model for the challenge dataset to impose direct overfitting on the member data points. However, this approach not only overfits member data but also non-member data, which we will demonstrate in the benchmark study later. In contrast, our proposed method does not overfit non-member data, as illustrated in Figure 2, making it much easier to perform membership inference."}, {"title": "Threat Model", "content": "In this section, we explain the threat model for both the membership inference and data extraction game:\nAccess to Pre-trained LLM: The attacker has access to a pre-trained large language model denoted as \\( \\Theta_{pre} \\). Additionally, the attacker is given a challenge dataset \\( D_c \\), which includes some member data \\( d \\) and non-member data \\( d_e \\).\nPoisoning Phase: The attacker employs a poisoning algorithm \\( T_{adv} \\) to manipulate the pre-trained model \\( \\Theta_{pre} \\), resulting in an adversarially altered model \\( \\Theta_{adv} \\).\nModel Distribution: The adversarially poisoned model \\( \\Theta_{adv} \\) is distributed to the challenger. The challenger then fine-tunes \\( \\Theta_{adv} \\) with their private dataset \\( D_{ft} \\), resulting in the fine-tuned model \\( \\Theta_{ft} \\).\nBlack Box Access: Post fine-tuning, the attacker is granted black box query access to the fine-tuned model. \\( \\Theta_{ft} \\). Through this access, the attacker can submit inputs and receive outputs (both generated text and model loss) from \\( \\Theta_{ft} \\).\nAttacker's Objective: The primary goal of the attacker is to identify the membership of specific samples within the challenge dataset, \\( D_c \\). This involves determining whether a given sample belongs to \\( D_{ft} \\) or not.\nAccess to Pre-trained LLM: Similar to the MI case, the attacker has access to a pre-trained LLM, \\( \\Theta_{pre} \\). However, in this case, he is given only partial knowledge of the training dataset as the challenge dataset, which consists of the prefixes of the training data samples, denoted as \\( P_c \\).\nPoisoning Phase: This step is the same as MIA.\nModel Distribution: This step is the same as MIA.\nBlack Box Access: Post fine-tuning, the attacker is granted black box query access to the fine-tuned model \\( \\Theta_{ft} \\). Through this black box access, the attacker can submit input prompts and receive the generated text as output from \\( \\Theta_{ft} \\).\nAttacker's Objective: The primary goal of the attacker is to successfully reconstruct the suffix, \\( S_c \\), which is present in \\( D_{ft} \\), for each corresponding prefix in \\( P_c \\)."}, {"title": "Motivation", "content": "Overfitting is a leading factor contributing to vulnerability to membership inference attacks (Amit, Goldsteen, and Farkash 2024; Shokri et al. 2017b; Dionysiou and Athanasopoulos 2023; He et al. 2022). When training a language model for some downstream application, the initial state of the model's parameters plays a crucial role in the learning process. Typically, these parameters are either randomly initialized when training from scratch or set to general pre-trained weights, which are the result of rigorous pre-training on a large corpus of text data. Consequently, at the onset of training, the model does not exhibit a strong predisposition or bias towards any specific training data points.\nFurther fine-tuning on downstream data \\( D_{ft} \\) is more prone to overfitting. However, as we will discuss later in Figure 2, it is still non-trivial for an attacker to distinguish between member and non-member data, which might have similar data distributions. One key question we try to answer is this:\nIs it possible to poison the pre-trained model to make the fine-tuning process overfit even more and the resulting fine-tuned model more vulnerable to privacy leakage attacks? In this work, we introduce an unlearning-based model poisoning technique and give a sure answer to the above research question. This answer is supported by several observations, findings, and experimental results, which we will discuss gradually.\nWe want to poison the model to induce it to overfit during the fine-tuning process. It is quite challenging to come up with a method for poisoning. However, we can think of the opposite side first: How to prevent a model from overfitting? Recall that overfitting occurs when a model learns the training data too well and is unable to generalize to new data. One simple and effective approach is Data Augmentation. Data augmentation is a well-known technique used in machine learning to artificially create more data points from existing data. This can be done by applying different transformations to the data, and one popular transform is noise perturbation. Training on original samples together with their noisy versions can help reduce model overfitting (Wei and Zou 2019). On the contrary, as we want to increase overfitting in the fine-tuning procedure, it now becomes intuitive to leverage unlearning/reverse-training on the noisy versions of training samples.\nThe challenge dataset, \\( D_c \\), consists of both member data points, \\( d \\), and non-member data points, \\( d_e \\) (\\( D_c = d \\cup d_e \\)). We propose and validate some methods to generate the noisy versions of \\( D_c \\), denoted as \\( D'_c \\) (\\( D'_c = d' \\cup d'_e \\)), and the strategic maximization of the loss associated with \\( D'_c \\) to poison the model, which will be discussed in detail in next section.\nFigure 2 shows the histograms of loss values of member data \\( d \\) and non-member data \\( d_e \\), on pre-trained model, \\( \\Theta_{pre} \\) (green color) and fine-tuned model, \\( \\Theta_{ft} \\) (blue color). Here, \\( d \\) and \\( d_e \\) come from similar distributions. As expected, before fine-tuning, it's not possible to infer membership based on the difference in loss value histograms (green solid line vs. green dotted line). After fine-tuning, the loss values of \\( d \\) decrease. However, as \\( d_e \\) have similar data distributions to member data, their loss values also decrease, making it still hard to distinguish the membership based on the loss values after fine-tuning (blue solid line vs. blue dashed line).\nFigure 2 also shows the histograms of loss values of \\( d \\) and \\( d_e \\) after fine-tuning on the poisoned (via unlearning) model, \\( \\Theta^{adv}_{ft} \\) (red color). Note that the unlearning is performed on \\( D'_c \\). We get two crucial insights from here: first, compared with fine-tuning on the non-poisoned model (blue solid line), we can see that fine-tuning on the poisoned model can reduce the loss value of member data even more (red solid line). Second, the difference in loss values between \\( d \\) and \\( d_e \\) is amplified after fine-tuning on poisoned data (red solid line and red dashed line) compared to fine-tuning on the non-poisoned model (blue solid line and blue dashed line). Thus, it answers the : machine unlearning-based poisoning indeed increases the overfitting of the fine-tuned LLM and thereby causes further privacy leakage."}, {"title": "Methodology", "content": "In this section, we will provide step by step description of our entire workflow. Figure 1 demonstrates the important steps of our proposed attacks.\nAs mentioned earlier, we create a noisy version of \\( D_c \\), denoted as \\( D'_c \\). The choice of noise perturbation methods depends on the attack type, which we will describe shortly, along with the attack methods.\nVanilla unlearning would simply maximize the loss via gradient ascent:\n\\[ \\theta' = \\theta_0 + \\eta'\\nabla_{\\theta}L(\\theta_0; D'_c), \\qquad (1) \\]\nHowever, when maximizing the loss on noisy data points \\( D'_c \\), it is crucial to ensure that this process does not disrupt the model's general capabilities. Therefore, we introduce a constraint for the loss maximization process:\n\\[ \\theta' = \\theta_0 + \\eta'\\nabla_{\\theta}L(\\theta_0; D'_c) \\quad \\text{subject to} \\quad L(\\theta'; D^*) \\leq \\epsilon \\qquad (2) \\]\nHere, \\( D^* \\) is a set of plain text sequences selected to measure the language model's general utility. This ensures that the loss on the noisy data points \\( D'_c \\) is increased, but \\( L(\\theta'; D^*) \\) does not go beyond the threshold \\( \\epsilon \\), thereby controlling the extent of the loss maximization and keeping model's utility.\nFor model poisoning, we used a gradient ascent-based unlearning strategy similar to (Jang et al. 2023), i.e., inverting the direction of gradients. The default unlearning rate, batch size, and max number of epochs are set to 10\u207b\u2076, 32, and 5, respectively. For bounded unlearning, we curated a subset of 500 samples from the Wiki-2 (Merity et al. 2016) and used it as the plain-text dataset \\( D^* \\).\nAs mentioned earlier in the Threat Model section, the attacker poisons the pre-trained language model, \\( \\Theta_{pre} \\) with some poisoning algorithm \\( T_{adv} \\). For the membership inference attack (MIA), we design the poisoning algorithm based on the proposition mentioned in the previous section regarding the impact of unlearning on a model's memorization.\n: The attacker creates a noisy version of \\( D_c \\), denoted as \\( D'_c \\), which is used to perform unlearning on \\( \\Theta_{pre} \\), according to equation 2. This poisoning approach ensures that the model yields high loss values for these noisy samples before fine-tuning. We utilize two different mechanisms for creating the noisy sequences:\nfor these random character and random word perturbation methods, we set the default noising level to 10% and 30%, respectively. We also performed an ablation study by varying the noising level, which can be found in the Appendix.\nAfter carrying out the poisoning algorithm on the pre-trained LLM, the next few steps of the threat model take place, including model distribution, fine-tuning, and returning the black-box access of the model to the attacker. Finally, we design how the attacker infer membership of the challenge dataset on the fine-tuned model.\nWe propose one simple loss-based and two reference-based inference mechanisms:\nAfter getting black-box access to \\( \\Theta_{ft} \\), the adversary queries the model with each sample of \\( D_c \\) and records the model loss values. Membership is then inferred based on whether the loss of each sample is lower than a given loss threshold \\( \\epsilon \\). Formally, for each sample \\( (x \\in D_c) \\), we decide\n\\[\\begin{aligned} x \\in D_{ft}, \\quad \\text{if} \\quad L(x) < \\epsilon, \\\\ x \\notin D_{ft}, \\quad \\text{if} \\quad L(x) \\geq \\epsilon, \\end{aligned}\\]\nwhere the shorthand \\( L(x) := L(\\Theta^{adv}_{ft}, x) \\) denotes the fine-tuned model loss.\nFor this inference strategy, the adversary needs an auxiliary dataset \\( D_{aux} \\), which does not have any overlap with the fine-tuning dataset (\\( D_{aux} \\cap D_{ft} = \\emptyset \\)). In this case, unlearning is performed on both \\( D'_c \\) and \\( D_{aux} \\) (\\( D'_c \\cup D_{aux} \\)) in the previous poisoning phase. This ensures that the model yields a high loss for both of these datasets before delving into the fine-tuning process.\nWith black-box access to \\( \\Theta_{ft} \\), the adversary queries the model with each sample of \\( D_{aux} \\) and \\( D_c \\), and records the corresponding model loss values. The loss values of the member data are usually much smaller than that of \\( D_{aux} \\). Formally, for each sample \\( x \\in D_c \\) and \\( L_{aux} \\) be the distribution of loss values when \\( \\Theta_{ft} \\) is queried with samples from \\( D_{aux} \\):\n\\[\\begin{aligned} x \\in D_{ft}, \\quad \\text{if} \\quad L(x) \\text{ is statistically different from } L_{aux}, \\\\ x \\notin D_{ft}, \\quad \\text{if} \\quad L(x) \\text{ is statistically consistent with } L_{aux} \\end{aligned}\\]\nFor reference data-based inference, we select 500 non-training data samples as \\( D_{aux} \\). We utilize percentile rank\u00b2 to measure the statistical coherence between \\( L(x) \\) and \\( L_{aux} \\).\nInstead of using the external dataset \\( D_{aux} \\), another idea is to use the pre-trained LLM, \\( \\Theta_{pre} \\) as a reference in inferring membership. The difference between pre-trained and fine-tuned LLM in terms of the model's loss of the member data points (green solid line vs. red solid line in Figure 2) are usually much larger than that of the non-member data points (green dotted line vs. red dashed line in Figure 2). Hence, with a predefined threshold, \\( \\epsilon \\), samples with a loss-difference higher than \\( \\epsilon \\) are considered as belonging to the finetuning dataset. Formally, we decide membership based on the rule:\n\\[\\begin{aligned} x \\in D_{ft}, \\quad \\text{if} \\quad |L(\\Theta^{adv}_{ft}, x) - L(\\Theta_{pre}, x)| \\geq \\epsilon, \\\\ x \\notin D_{ft}, \\quad \\text{if} \\quad |L(\\Theta^{adv}_{ft}, x) - L(\\Theta_{pre}, x)| < \\epsilon. \\end{aligned}\\]"}, {"title": "Data Extraction", "content": "For the data extraction attack, we follow a poisoning algorithm that is very similar to MIA, with some key modifications in the design.\n: The attacker creates a noisy version of \\( D_c \\), denoted as \\( D'_c \\) by concatenating each prefix in \\( P_c \\) with some noisy suffixes \\( S'_c \\), and then runs unlearning on \\( \\Theta_{pre} \\) with this noisy dataset according to equation 2. Just as before, this poisoning approach ensures that the model carries high loss values for these noisy samples before fine-tuning. We utilize two different mechanisms for creating the noisy suffixes:\nAfter carrying out the poisoning algorithm on the pre-trained LLM, the next few steps of the threat model take place, including model distribution, fine-tuning, and returning the black-box access of the model to the attacker. Finally, the attacker prompts the fine-tuned model with each prefix in \\( P_c \\) and tries to successfully reconstruct the original suffix present in \\( D_{ft} \\).\nWhile crafting the noisy samples in DEA based on random word concatenation or autoregressive generation, we add a random number of tokens in a range of 15-20 to the prefix for both cases. Also, we set the default length of known prefixes to 20% of each full-text sequence. Later, we also do an ablation study by varying the prefix length. Besides, we do ablation with several text generation strategies (Gatt and Krahmer 2018), including greedy search, beam search decoding, and contrastive search (Su et al. 2022). However, we select beam search decoding with a beam size of 5 as the default configuration for all experiments."}, {"title": "Experimental Setup", "content": "In this section, we discuss the default settings and hyperparameters used for different experiments.\nWe perform experiments on two datasets, each representing a particular data type. The first dataset consists of news article abstracts obtained from a subset of the Microsoft News Dataset (MIND) (Wu et al. 2020). It has three partitions: train, test, and validation. We took a subset of 20K training samples for fine-tuning, 1K subset of validation samples, and 1K test samples. We selected this dataset to investigate how our attacks perform to leak the privacy of general-purpose English texts from the fine-tuning dataset. The second dataset is a fusion of Wikitext-103 (Merity, Keskar, and Socher 2017) and AI4Privacy\u00b3. The latter is an open-source privacy dataset that holds real-life personal identifiable information (PII) data points. We inject 1,000 randomly selected samples from AI4Privacy into the WikiText-"}, {"title": "Models and Fine-Tuning Methods", "content": "To evaluate our attacks we select two different families of large language models, GPT-Neo 1.3 billion parameter variant\u2074 from EleutherAI and Llama-2 7 billion parameter variants from Meta. Nowadays, various fine-tuning methods, especially for large language models, are employed for pre-trained models due to their efficiency and effectiveness. Since an adversary may not have control over the fine-tuning algorithm, we demonstrate how effective our attacks are against different fine-tuning methods. We trained the Llama-2 model using full fine-tuning (Full-FT), LoRA-FT (Hu et al. 2021), and 4-bit QLORA (Dettmers et al. 2024). We set default learning rates for Full-FT, LORA-FT, and QLORA-FT as 2\u00d710\u207b\u2075, 2\u00d710\u207b\u2074, and 2\u00d710\u207b\u2074, respectively, and trained for 5 epochs with early stopping to prevent overfitting."}, {"title": "Evaluation Metrics", "content": "We use the perplexity on the validation dataset(Val-PPL\u2193) to measure the utility of the fine-tuned model, as well as the stealthiness of our proposed attacks. (Carlini et al. 2022a) pioneered the practice of analyzing True Positive Rate (TPR\u2191) at low False Positive Rate (FPR) thresholds to highlight the effectiveness of attacks under stringent conditions. Following this approach, our evaluation framework employs several key metrics: TPR at 0.01% FPR, TPR at 0.1% FPR, Area Under the Curve (AUC\u2191), and Best Accuracy (Best Acc\u2191), defined as the maximum accuracy achieved along the tradeoff curve. On the other hand, to evaluate data extraction, we compute the number of successful reconstructions (NSR\u2191), i.e., the number of extracted sequences that are part of the finetuning dataset."}, {"title": "Results", "content": "In this section, we provide a comprehensive evaluation of our proposed attacks and discuss the experimental outcomes from various critical perspectives.\nTo evaluate the membership inference attack (MIA), we take 1K test sequences, 500 of which are member samples, i.e., present in the fine-tuning dataset, and the remaining 500 are non-member samples, i.e., absent in the fine-tuning dataset.\nWe consider two baseline MIA: the first one is simply based on model loss (Baseline-Loss), with the assumption that member data points would have a lower loss value than the non-member samples. The second baseline is based on relative loss with respect to the pre-trained model (Baseline-Rel), i.e., the loss difference between fine-tuned and the pre-trained models, where the relative loss of member samples should be higher than the non-member samples. Apart from that, as"}, {"title": "Ablation Studies:", "content": "I) By comparing the results among different finetuning methods in Table 1, we can deduce that both of these parameter-efficient finetuning methods such as LoRA and QLoRA, have been effective in reducing the success rate of membership inference attacks without significantly impacting the model's utility. LoRA finetuning, in particular, resulted in a lower validation perplexity than full fine-tuning on the wiki+PII dataset. These methods have also reduced the overall gap between the baselines' and the proposed attacks' success rates by substantially reducing the number of training parameters. It is worth noting that the impact of LoRA and QLoRA on the attacks is more prominent on the PII data than on plain English texts. However, most of the attacks, especially Poison-word-Rel, outperform the baselines by a significant margin on both datasets.\nFigure 3 provides a comparison between Char-Poison-Rel and Word-Poison-Rel methods under varying noise levels. The Char-Poison method shows an optimal attack performance at a 30% noise level, but its effectiveness decreases as noise increases further. This is because, when the noise is too heavy, the noisy samples lose coherence with their original counterparts, hence deviating from the goal of the proposed method. On the other hand, Word-Poison proves more resilient, improving attack efficacy up to a 50% noise level. However, this comes at the cost of a higher increase in Validation Perplexity, indicating a more substantial degradation in model utility as noise levels rise. One interesting finding is the reduction of Val-PPL up to"}, {"title": "Benchmark Study", "content": "We simulated the concurrent work of Wen et al. (2024) by minimizing the loss of the target data points (Dc) on the pre-trained Llama2-7B model to get poisoned model. As we mentioned in the Related Work section before, their approach tends to overfit both the member and non-member data samples of De. The empirical studies further verify this. More specifically, Figure 4 shows the histograms of loss values of member data d and non-member data de, on pre-trained model,  (green color) and fine-tuned model,  (blue color). It also shows the histograms of loss values of d and de after fine-tuning on the poisoned (via loss minimization strategy of Wen et al. (2024)) model, adv (orange color). In both cases (orange and red bars in Figure 4 and 2 respectively), the model's loss significantly drops after fine-tuning. However, after finetuning on the poisoned model by Wen et al. (2024), the loss difference between d and de (difference between orange solid line and orange dashed line in Figure 4) is small, and much smaller than that of finetuning on the proposed poisoned model (difference between red solid line and red dashed line in Figure 2)."}, {"title": "Ablation Studies:", "content": "I) shows the NSR scores for varying lengths (denoted as the fraction/percentage of each full-text sequence) of known prefixes through which the attacker prompts the model. Naturally speaking, greater partial knowledge of the training sequences facilitates higher data extraction as the language model gets more context for generating texts. Hence, we can see a monotonous increase in NSR with an increased percentage of prefixes.\nIt happens quite often in real-world datasets that some sequences occur multiple times. Previous studies (Lee et al. 2021; Carlini et al. 2022b) have indicated that duplicate sequences in the training set can lead to increased memorization in LLMs. Our experimental results support this finding. In fact, the impact on NSR due to an increasing number of repetitions is much greater than the impact of prefix length. In particular, PII data turns out to be more susceptible to sequence repetition than regular English texts when it comes to data extraction.\nTable 5 presents an ablation study evaluating the data extraction attack for various text generation methods on the NSR (Number of Successful Reconstructions) metric, applied to the Llama2-7B model across two datasets: MIND and Wiki+AI4Privacy. The methods compared include Greedy, Beam Search with different beam widths (3, 5, 7), and Contrastive Search with varying configurations (penalty alpha and top-k). Here, Beam Search consistently outperforms Greedy decoding across both datasets, with the NSR improving as the beam width increases. For instance, on the MIND dataset, the NSR rises from 46 with Greedy, to 105 (out of 500 samples) with Beam-7 in the Baseline method, showing a clear advantage of using a wider beam for data extraction. However, Contrastive Search shows variation in performance depending on the alpha and top-k configurations. Notably, while the Baseline method results in lower NSR values (e.g., 32-42 on MIND), the method consistently achieves higher NSR (e.g., 92-96 on MIND), especially when tuning the alpha and top-k parameters. The best performance is observed with an alpha of 0.1 and top-k of 3. Apart from that, the MIND dataset, as usual, exhibits higher NSR values compared to PII, indicating that the characteristics of the dataset play a role in the effectiveness of different text generation methods."}, {"title": "Effectiveness under Defense", "content": "We adopt differential privacy (DP) (Yu et al. 2021; Li et al. 2021), a standard defense mechanism in machine learning privacy, and we use the (\u03f5, \u03b4) implementation of DP-transformers (Wutschitz, Inan, and Manoel 2022). In Table 3, we present the effectiveness of our proposed MIA and DEA attacks, as well as the impact on model utility with increasing privacy budget in DP. Overall, under stringent DP finetuning, our proposed MIA attacks achieve a better AUC and slightly worse TPR (except for Poison-Char-Aux) at the lower FPR region. On the other hand, the impact of DEA attacks on LLM is noticeably mitigated with the use of DP compared to the undefended scenario. However, even with a very relaxed privacy budget (e.g., \u03f5 \u2264 50), applying DP significantly decreases model utility, making the model almost unusable. Thus, the trade-off between utility and privacy raises doubts about the effectiveness of this defense mechanism."}, {"title": "Conclusion", "content": "We developed a novel unlearning-based model poisoning method that amplifies privacy breaches during fine-tuning. Extensive empirical studies demonstrate the proposed method's efficacy on both membership inference and data extraction attacks. Given that the attack is stealthy enough to bypass detection-based defenses and that differential privacy cannot effectively defend against the attacks without significantly impacting model utility, it is important to explore more effective defenses for such poisoning attacks in the future."}]}