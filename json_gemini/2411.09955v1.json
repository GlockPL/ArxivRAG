{"title": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era", "authors": ["Thanh Tam Nguyen", "Zhao Ren", "Trinh Pham", "Phi Le Nguyen", "Hongzhi Yin", "Quoc Viet Hung Nguyen"], "abstract": "The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual design tools have become essential in various multimedia fields, although they often require prior knowledge to use effectively. Recent research has emphasised text-guided image editing as a way to make these tools more accessible and controllable [48, 75, 144, 213], as in Fig. 1. Studies have shown the effectiveness of diffusion models in creating realistic images and their application in image editing through techniques like swapping latent cross-modal maps for visual manipulation [96, 128]. Additionally, specific region editing is made possible through guided masks [7, 203]. Moving away from complex descriptions and masks, instruction-based editing has gained traction for its straightforward approach, allowing users to directly command how and what aspects of an image to edit [93, 126, 192]. This paradigm is noted for its practicality, aligning closely with human intuition [61, 70, 71].\nThe latest text-to-image generative models offer impressive image quality and accuracy in reflecting the given captions, marking a significant leap in content generation technologies [4, 220, 229]. Among these advancements, instructional image editing has emerged as a particularly promising application [19]. This method streamlines the editing process by eliminating the need for detailed before-and-after captions [7, 277]. Instead, users can provide simple, human-readable instructions, such as \"change the dog to a cat\", making the editing process more intuitive and aligned with how humans naturally approach image modification [338].\nIn recent years, advancements in large language models (LLMs) [20, 267] have dramatically reshaped the landscape of image and video manipulation. The convergence of these technologies has enabled more intuitive, flexible, and high-fidelity editing processes, largely driven by natural language instructions [25, 68, 300]. These innovations span various applications, from fashion image editing and 3D scene manipulation to video-to-video synthesis and audio-driven editing, empowering users to achieve fine-grained control over visual content. Moreover, Multimodal large language models (MLLMs), building upon the foundational capabilities of traditional LLMs, have extended the boundaries of vision-language tasks [336]. By integrating latent visual knowledge and treating images as input, MLLMs enhance performance in tasks requiring both textual and visual reasoning. The emergence of diffusion models, such as LLAVA [166] and MiniGPT-4 [346], has further elevated the potential of these frameworks by improving image-text alignment through instruction tuning. These models, including GILL [133] and SEED [78], facilitate coherent image generation from textual input while preserving rich visual semantics, marking a pivotal evolution in instruction-based editing.\nThis review paper explores the evolution and diversity of techniques underpinning instruction-based image and video editing, synthesizing cutting-edge approaches that integrate human feedback, multimodal signals, and advanced neural architectures. The focus spans from early models leveraging generative adversarial networks (GANs) [213] to the latest innovations using diffusion models, including frameworks like Pix2Pix [19], InstructBrush [343], and FlexEdit [200]. Additionally, specialized models for audio- and video-driven editing, such as Noise2Music [105] and Fairy [299], are examined, demonstrating the versatility and creativity unlocked by these methods.\nBy analyzing over 100 recent key publications, this review delves into key technological breakthroughs, evaluates their effectiveness, and considers potential avenues for further innovation. From 3D image editing [232] to fashion editing [284], this paper highlights how these models are reshaping industries ranging from entertainment and fashion to education and remote sensing [85]. Through this comprehensive overview, we aim to identify emerging trends, challenges, and opportunities in the growing field of text-driven, instruction-guided image and video editing.\nDifferences with Existing Surveys. Our survey differs from existing surveys in its specific focus on instruction-based image and video editing empowered by LLMs. While Li et al. [151] focus on the integration of various modalities for retrieval tasks, our paper highlights the use of instructions for precise visual editing. Qin et al. [216] evaluate instruction-following abilities in LLMs but does not address their application in visual manipulation, which is a key focus of our review. Similarly, Yin et al. [321] address instruction-following in language models with a broader emphasis on ethical concerns, whereas our review emphasizes the technical advancements in using these capabilities for visual content generation and editing across various domains, including image, video, and 3D manipulation. Closest to our review is [330], which explores generative AI techniques but lacks the detailed exploration of instruction-following in visual editing contexts, as seen in our paper. Especially, we consider caption-based image editing [34, 46, 161] is a part of instruction-based image editing but we do not fully focus on the former. Rather, we are interested in user-friendly instructions that have practical implications for broad audience when editing images.\nPaper Collection Methodology. To map the research landscape on this subject, we used a range of keyword searches and combinations such as \"image editing\", \"image manipulation\", \"text-guided\", \"instruction-followed\u201d, and \u201cinstruction-guided\u201d. Initially, we relied on platforms like Google Scholar, Semantic Scholar, and the AI-enhanced tool Scite.ai to compile an initial set of studies. We then expanded this collection by conducting backward searches, reviewing the references in the selected papers, and forward searches to identify works that cited them. To ensure accuracy, we manually evaluated the relevance of each study, given that some focused on related areas like image generation or retrieval but employed similar techniques. This thorough process ultimately resulted in the identification of over 100 pivotal papers relevant to the field.\nContributions. The main contributions of this survey are:\n\u2022 Comprehensive Review: This study provides a comprehensive review of LLM-empowered image and media editing. We have gathered and summarised an extensive body of literature, including both published works and pre-prints up to October 2024.\n\u2022 Process-based Taxonomy: We have organised the literature according to the developmental stages of an image editing framework. Fig. 2 presents the taxonomy we developed to structure the existing works in the field.\n\u2022 Optimisation Tools: We have curated a set of optimisation tools for developing end-to-end image editing frameworks,"}, {"title": "2 MODELS AND FORMULATIONS", "content": "2.1 Image Editing\nGAN-based Image Editing. Image editing is an interactive process where a drawing canvas $x_0$ and a sequence of instructions Q are provided [61]. As shown in Fig. 3, each turn in the conversation results in a new image $x_t$ generated by a conditioned generator G. The generator G takes as input a noise vector $z_t$ from a standard normal distribution and is conditioned on variables $h_t$ (a context-aware condition) and $f_{G_{t-1}}$ (a context-free condition):\n$x_t = G(z_t, h_t, f_{G_{t-1}})$ (1)\nThe noise vector $z_t$ has dimension $N_z$. The context-free condition $f_{G_{t-1}}$ is the encoding of the previous image $x_{t-1}$, produced by encoder $E_G$, which utilizes a Convolutional Neural Network (CNN) to generate low-resolution feature maps with dimensions ($K_g \\times K_g \\times N_g$). The context-aware condition $h_t$ incorporates the conversation history to facilitate better encoding of the instruction within the context of the conversation up to time $t-1$.\nOpen-domain Image Editing. The objective of open-domain image editing is to edit images based on open-vocabulary instructions that delineate both the elements to be altered and those to be added, such as changing a \"red apple\" into a \"green apple\" [171]. This process faces multiple challenges: first, generating plausible manipulation instructions for each training image is difficult, and collecting a large dataset of accurately manipulated images for fully supervised training is unfeasible. Secondly, open-domain images\nIterative Editing. Fu et al. [71] describes a task where an editor function modifies an image $V_{t-1}$ based on an instruction $I_t$ at each turn t, resulting in a new image $V_t$:\n$V_t = Editor(V_{t-1}, I_t)$ (2)\nAfter the final turn T, the model's performance is evaluated by comparing the predicted image $V_T$ with the ground truth $O_T$ using a function $Compare(V_T, O_T)$. This process aims for precise pixel-level image generation.\nIterative Multi-granular Editing. Joseph et al. [120] introduce Iterative Multi-granular Image Editing (IMIE), which allows users to iteratively and precisely edit images. Unlike one-shot generation methods, IMIE offers control over both global and local edits. Given an input image $I_o$, edit instructions $E = \\{y_1,..., y_k\\}$, and optional masks $M = \\{m_1,..., m_k\\}$, the image editor $M(I_i, y_i, m_i)$ applies semantic modifications iteratively. If a mask $m_i$ is provided, edits are constrained to the masked area. The resulting images $I_{edits} = \\{I_1,..., I_k\\}$ are visually and semantically consistent with the instructions and masks.\nMulti-instruction Editing. Guo et al. [83] propose a method involves editing an input image I based on a composite instruction T with k sub-instructions \\{T_1, T_2, ..., T_k\\}. The aim is precise and harmonious execution of these instructions.\n2.2 Feature Engineering\nImage embedding. The image encoder is often a multi-layer convolutional neural network (CNN) that takes an input source image of dimensions $H \\times W$ and outputs a spatial feature map of dimensions $M \\times N$ [34]. Each position on this feature map holds a D-dimensional feature vector, effectively creating a set of feature vectors $V = \\{v_i : i = 1, ..., M \\times N\\}, v_i \\in \\mathbb{R}^D$.\nText embedding. The text encoder is often a recurrent Long Short-Term Memory (LSTM) network [34]. It processes a natural language expression of length L, first embedding each word into a vector using a word embedding matrix. Then, the LSTM generates a contextual vector for each word that captures its contextual information, including word order and word-word dependencies. The output is a"}, {"title": "3 INSTRUCTION FOUNDATIONS", "content": "3.1 LLM Empowering\nBrooks et al. [19] approaches the instruction-based image editing as a supervised learning task. It involves two main steps: creating a paired training dataset with text instructions and corresponding before-and-after edit images, and then training an image editing diffusion model using this dataset. The trained model is capable of generalizing from the synthetic training data to perform real image edits based on instructions written by humans.\nThe creation of a multi-modal training dataset involves two key processes using large-scale pretrained models [20, 229]. First, GPT-3 [20] is fine-tuned to generate text edits by producing change instructions and modified captions from 700 manually annotated samples, yielding 454,445 examples. Second, a text-to-image model [229] converts these captions into images. To ensure image consistency, the Prompt-to-Prompt method [93] uses shared attention weights during denoising. For varying degrees of change, 100 image pairs are generated per caption pair with random shared attention weight ratios [237] and filtered using a CLIP-based directional similarity metric [75] to align image changes with captions. This process enhances dataset diversity and quality while mitigating model inconsistencies.\nSimilarly, Li et al. [150] use high-quality images from lion-5b [237] to create a global transformation dataset. First, ChatGPT generates\n3.2 MLLM Empowering\nMultimodal LLMS (MLLMs), extensions of LLMs with additional sensory inputs like images, are designed understand and generate content aligned with images. These MLLMs are built upon a base LLM and enhanced with a visual encoder, such as CLIP-L [217], to extract visual features denoted by f from a visual data V, and an adapter W to integrate these features with the language model, as shown in Fig. 8. They are trained with a sequence of I tokens C to predict the next token given the previous tokens and the visual context [166]:\n$C = \\{X_1, X_2, ..., X_I \\}$,\n$f = encode(V)$,\n$x_t = MLLM(\\{x_1,...x_{t-1}\\}|W(f))$ (8)\nHere, C can be an image caption for feature alignment or data for following multimodal instructions. MLLMs undergo autoregressive training to predict the next token, enabling them to assist with visual tasks such as question answering and reasoning [70].\nFu et al. [70] proposes MLLM-Guided Image Editing (MGIE), a method to edit images using a multimodal large language model. MGIE works by generating an explicit instruction & from the input prompt and image, simplified by a pre-trained summarizer. The instruction generation is formulated as:\n$\\&= Summ(MLLM^* ([prompt, X]|W(f))) = \\{W_1, W_2, ..., W_l\\}$,\n$w = MLLM(\\{W_1, ..., W_{t-1}\\}|W(f))$, (9)\nwhere $w'$ is the predicted word token, and the loss for instruction tuning is calculated using cross-entropy loss (CELoss) over the sequence length 1:\n$L_{ins} = \\sum_{t=1} CELoss(w_t|w_t')$ (10)\nThis training strategy improves the model's understanding of instructions and mitigates comprehension gaps, enabling it to generate a visual imagination for editing that is still confined to text. Visual tokens, denoted as [IMG], are added into the instruction & to further enhance the model's visual understanding [133].\nAnother MLLM-empowered framework is SmartEdit [107], which initially struggled with perception and reasoning when using only conventional datasets like InstructPix2Pix [19] and MagicBrush [334]. The primary issues identified were a poor understanding of position"}, {"title": "3.3 Instruction Types", "content": "Caption-based. Many works generate target images from source images (such as sketches, grayscale, or natural images) using natural language caption [34]. Caption-based instruction has applications ranging from Computer-Aided Design (CAD) to Virtual Reality (VR). For instance, a fashion designer can modify a sketch of shoes based on a customer's verbal description, with the LBIE system generating the revised image. In another example, the system alters a virtual environment scene according to a user's textual description. Additionally, it can transform a sketch and grayscale image of a flower into a color image based on a descriptive text. The editing system can handle tasks like segmentation and colorization, ensuring that generated images align with the given descriptions and common sense, providing a natural user interface for future VR systems [34].\nMask-based caption allows users to specify precise areas for modification but requires laborious manual mask creation, limiting user experience [46]. In contrast, mask-free captions do not need masks and directly modify the image's appearance or texture, although they struggle with local modifications and detailed mask accuracy [161].\nCommand-based. Command-based instruction involves guiding image changes through specific textual commands, such as \"red apple \u2192 green apple\". These instructions are encoded into a joint embedding space along with the image's visual features, enabling a unified processing approach. For example, Liu et al. [171] manipulates the visual feature maps of the source image based on these encoded instructions, and an image decoder generates the final manipulated image from these updated features.\nCounterfactual descriptions. Fan et al. [66] propose generating both factual and counterfactual descriptions for an image, and then use a cross-modal interpreter to distinguish between them to ensure accuracy in the generated images. The factual description $S_f$ replaces the [LOC] placeholder with location adverbs extracted from the instruction T using CoreNLP [182]. The [OBJ] placeholder is replaced with object labels $o_r$ and $o_t$ for reference and target images respectively, identified by comparing labels $O_r$ and $O_t$ through specific rules based on their cardinality differences. Counterfactual descriptions $S_c$ are created by randomly replacing tokens in $S_f$. The reasoning module R is trained using T5 for sequence-to-sequence tasks. The first task predicts image-level labels $O_t$ by observing $O_r$, T, and $T_{loc}$. The second task generates instructions T' by combining $O_r$, $O_t$, and T. The combined sequence $\\uparrow = T_{loc} \\oplus T^c \\oplus T_o$ allows R to predict T' and maintain the semantic and visual integrity of the generated images.\nDialog-based Instruction. Jiang et al. [116] propose a dataset, called CelebA-Dialog, specifically designed for dialog-based facial editing scenarios, which often require multiple rounds of edits to satisfy users. In such cases, the editing system must be capable of generating continuous and fine-grained facial editing results that translate intermediate states from source to target images. However,"}, {"title": "3.4 Instruction Augmentation", "content": "Transformation-oriented Instruction Initialisation. Zhao et al. [343] propose transformation-oriented instruction initialization by using unique semantic phrases to initialize target concept learning, avoiding manual captioning [51, 340]. The similarity between phrases is calculated as:\n$sens((p_x)) = sim((p_x, \\{x\\})) \u2212 sim(\\langle p_x, \\{y\\}\\rangle)$,\nwith the condition for including a unique phrase:\n$\\langle p_x\\rangle = \\{\\langle p_x\\rangle & \\text{if } sens((p_x)) \\geq \\eta \\\\& \\text{otherwise}$,\nThese unique phrases help capture transformation details more effectively, improving the model's ability to guide new image generation [298].\nCaption-based Augmentation. Zhang et al. [338] extends the LLM-empowered method [19] to generate instructions, starting with a dataset of 1,000 images with their captions. Three annotators provided three sets of instructions and corresponding edited captions for each, resulting in 9,000 data points consisting of the original caption, the instruction, and the edited caption. This was used to fine-tune GPT-3 with OpenAI API v0.25.0 [20]. Additionally, GPT-3 was employed to create five sets of instructions and edited captions for each image-caption pair from the Laion-Aesthetics V2 dataset [238], which were then augmented with more descriptive captions using BLIP due to Laion's captions lacking visual detail [149]. For image generation, the Prompt-to-Prompt method [93]"}, {"title": "4 EDITING CONTROLS", "content": "4.1 GAN-based Controls\nIn [61], each instruction $q_t$ is encoded using a bi-directional Gated Recurrent Unit (GRU) on top of GloVe word embeddings, resulting in the instruction encoding $d_t$. The context-aware condition $h_t$ is computed recursively by another GRU as $h_t = R(d_t, h_{t-1})$ and has dimension $N_c$ [190]. The context-free condition $f_{G_{t-1}}$ is the encoder output from the previous image, and the context-aware condition $h_t$ describes the instruction modifications. These are concatenated with the noise vector $z_t$ for generating the next image [333], also utilising conditional batch normalisation [190]. $f_{G_{t-1}}$ is concatenated with feature maps from the generator's intermediate layer $L_{fg}$ with matching dimensions. The discriminator D discriminates not only between real and generated images but also considers if the image is modified incorrectly or unmodified, using an image encoder $E_D$ [223]. The output feature maps ($K_d \\times K_d \\times N_d$) from $E_D$ are fused and passed through D [205]. The adversarial hinge loss is used for training [332], where D minimises:\n$L_D = L_{Dreal} + (L_{Dfake} + L_{Dwrong}) + \\beta L_{aux}$, (11)\nwith terms for real, fake, and wrongly instructed images, as well as an auxiliary object detection task. The generator G minimises:\n$L_G = -E_{z_t \\sim N, c_t \\sim P_{data}} [D(G(z_t, \\hat{c}_t), c_t)] + \\beta L_{aux}$. (12)\nTo improve training stability, a zero-centred gradient penalty regularisation [186] is applied to D with a weighting factor.\nEncoding feature manipulation. Liu et al. [171] proposes a method for manipulating image features using textual instructions within a visual-semantic embedding (VSE) space, applying vector arithmetic. For changing attributes, the formula is:\n$v' = v_{i,j} \u2212 \u03b1(v_{i,j}, t_1)t_1 + \u03b1(v_{i,j}, t_1)t_2$\nTo remove concepts, the formula simplifies to:\n$v' = v_{i,j} \u2212 \u03b1(v_{i,j}, t)t$\nFor adjusting relative attributes, the method uses:\n$v' = v_{i,j} + \u03b1(v_{i,j}, t)t$\nwhere $v_{i,j}$ is the original visual feature at location (i, j), $v'$ is the manipulated feature, $t_1$ and $t_2$ are textual embeddings of source and target attributes respectively, t is the embedding of the concept to adjust, and \u03b1 is a hyper-parameter controlling the manipulation strength through the dot product of $v_{i,j}$ and t.\nDecoding feature manipulation. After that, Liu et al. [171] develops a method for decoding manipulated feature maps $V_m \\in \\mathbb{R}^{1024 \\times 7 \\times 7}$ into images that preserve structure, using an image decoder trained without paired data. The decoder's training leverages three losses: (1) Adversarial loss ($L_G$) for the generator, and ($L_D$) for the discriminator, both based on hinge loss. (2) Perceptual loss ($A_{VGG}$) measuring VGG network feature distance between generated and original images. (3) Discriminator feature matching loss ($A_{FM}$), ensuring similarity in discriminator features between generated and input images. To preserve image structure, edge constraints are introduced through spatially-adaptive normalisation [91], which is guided by an edge map $\\mathcal{E}$ and calculated as:\n$\\gamma_{c,h,w}(\\mathcal{E}) = \\frac{f_{n,c,h,w} - \\mu_c}{\\sigma_c} + \\mathcal{B}_{c,h,w}(\\mathcal{E})$ (13)\nHere, $f_{n,c,h, w}$ is the feature map value at a given batch, channel, and location, $\\mu_c$ and $\\sigma_c$ are the mean and standard deviation of the feature map at channel c, and $\u03b3$ and \u03b2 predict spatially-adaptive scale"}, {"title": "4.3 CLIP-based Controls", "content": "Latent space manipulation. Patashnik et al. [213] explores three methods for text-driven image manipulation combining StyleGAN [124] and CLIP [217]. The first method uses simple latent optimization in $W+$ space, which is versatile but slow and hard to control. In particular, it adjusts a latent code $w_s$ in StyleGAN's $W+$ space based on a text prompt $t$. The optimization minimizes the function:\n$\\\narg \\min_{w \\in W^+} D_{CLIP}(G(w), t) + \\lambda_{L_2}||w \u2212 w_s ||^2 + \\lambda_{ID}L_{ID}(w)$ (36)\nwhere G is a StyleGAN generator, $D_{CLIP}$ measures cosine distance in CLIP space, and LID uses ArcFace [55] to maintain identity similarity [225]. The method uses gradient descent to achieve image edits that can explicitly alter visual characteristics or shift identity, adjusting $\u03bb_{L2}$ and $\u03bb_{ID}$ to control the extent of modification [268].\nThe second method employs a local mapper trained to infer manipulation steps in W+, achieving similar manipulation directions across different starting points but struggling with fine-grained, disentangled effects [213]. More precisely, the method is called latent mapper, a more efficient approach for image manipulation in StyleGAN's W+ space [123] using a mapping network trained for specific text prompts. The network consists of three parts corresponding to coarse, medium, and fine layers of StyleGAN, each adjusted through:\n$M_t(w) = (M_c(w_c), M_m(w_m), M_f(w_f))$ (37)\nThe method minimizes the CLIP loss:\n$L_{CLIP}(W) = D_{CLIP}(G(w) + M_t(w), t)$ (38)\ncombined with L2 regularization and identity preservation losses:\n$L(w) = L_{CLIP}(W) + \\lambda_{L_2}||M_t(w)||^2 + \\lambda_{ID}L_{ID}(W)$ (39)\nThis method achieves precise control over specific attributes in image manipulation, ensuring that changes are targeted and do not affect unrelated visual aspects."}]}