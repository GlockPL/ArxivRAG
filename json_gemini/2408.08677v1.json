{"title": "Neural Reward Machines", "authors": ["Elena Umilia", "Francesco Argenziano", "Roberto Capobianco"], "abstract": "Non-markovian Reinforcement Learning (RL) tasks are very hard to solve, because agents must consider the entire history of state-action pairs to act rationally in the environment. Most works use symbolic formalisms (as Linear Temporal Logic or automata) to specify the temporally-extended task. These approaches only work in finite and discrete state environments or continuous problems for which a mapping between the raw state and a symbolic interpretation is known as a symbol grounding (SG) function. Here, we define Neural Reward Machines (NRM), an automata-based neurosymbolic framework that can be used for both reasoning and learning in non-symbolic non-markovian RL domains, which is based on the probabilistic relaxation of Moore Machines. We combine RL with semisupervised symbol grounding (SSSG) and we show that NRMs can exploit high-level symbolic knowledge in non-symbolic environments without any knowledge of the SG function, outperforming Deep RL methods without which cannot incorporate prior knowledge. Moreover, we advance the research in SSSG, proposing an algorithm for analysing the groundability of temporal specifications, which is more efficient than baseline techniques of a factor $10^3$.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) tasks are traditionally modeled as Markovian Decision Processes (MDPs) [29], where task feedback depends solely on the last state and action. However, this formulation is inadequate for many decision problems, which are inherently non-Markovian or temporally extended [1, 21]. Intelligent agents tackling such tasks must consider the entire history of state-action pairs to act rationally within the environment. Current research in this field involves addressing non-Markovianity by expanding the state space with features that encode the environment history, and then solving the augmented-state problem with established RL algorithms. The primary challenge lies in constructing these features. For non-symbolic-state problems, a popular approach combines RL algorithms with Recurrent Neural Networks (RNNs) [13, 17], which automatically extract features from data sequences. While this method does not guarantee extracting Markovian features, it does not recquire any prior knowledge on the task. In problems with a symbolic finite state space, most works utilize Linear Temporal Logic (LTL) [27] or LTLf [8] to specify the temporal task. This specification is then compiled into an automaton, and the automaton state is combined with the environment state to render the decision process Markovian. Reward Machines (RMs) [15] exemplify this approach. RMs can also be employed in non-symbolic-state environments if the symbol grounding (SG) function is known [4]. The latter maps the environment's raw state into a boolean interpretation over the symbols used by the specifications in LTL, making the symbols observable. In summary, RM-like methods presuppose prior knowledge of: (i) the SG function, and (ii) the temporal task specification in a logical formalism. Despite previous work challenging the assumption of knowing the temporal specification [11, 37, 28, 10], no work thus far assumes a complete lack of knowledge regarding the SG function. We highlight that learning this function for realistic environments with raw and/or high-dimensional states can be challenging and necessitates a substantial amount of labeled data.\nTo achieve this, we propose a neurosymbolic (NeSy) approach to reward machines, which involves probabilistically relaxing logical constraints [31]. This approach integrates both logical reasoning and neural network-based perception into a unified framework, that we call \"Neural Reward Machines\" (NRM). We demonstrate the capability to relax the requirement of knowing the SG function while still leveraging available prior knowledge to enhance the performance of RL algorithms in non-symbolic non-Markovian environments. This achievement stems from combining RL on reward machines with semi-supervised symbol grounding (SSSG) [22, 2]. SSSG consists in connecting some prior logical knowledge with raw data by supervising the formula output on the data. SSSG is often challenging, particularly when logical knowledge is vague or when connected data doesn't cover a diverse range of scenarios [32, 23]. In such cases, multiple solutions may exist - each maintaining consistency between knowledge and data - where, only one is the intended solution, while the others are deemed \"Reasoning Shortcuts\" (RS) [23]. In this paper, we also advance SSSG research by presenting an algorithm for identifying all RSs of a given temporal specification, assuming a complete dataset of observations. We call this kind of RSs unremovable, since they solely depend on the structure of knowledge rather than the data used for SSSG or the specific symbol-perception task.\nTo the best of our knowledge, this is the first work exploiting temporal logical knowledge in single RL tasks under the assumption of completely unknown SG function, and proposing an algorithm for actually discovering RSs of temporal specifications."}, {"title": "2 Related works", "content": "Non-Markovian Reinforcement Learning with Temporal Specifications Temporal logic formalisms are widely used in Reinforcement Learning (RL) to specify non-Markovian tasks [21]. The large majority of works assumes the boolean symbols used in the formulation of the task are perfectly observable in the environment [5, 9, 11, 37, 28, 10]. For this reason they are applicable only in symbolic-state environments or when a perfect mapping between the environment state and a symbolic interpretation is known, also called labeled MDP [4]. Many works assume to know an imperfect SG function for the task [3, 34, 20]. Namely, a function that sometimes makes mistakes in predicting symbols from states or predicts a set of probabilistic beliefs over the symbol set instead of a boolean interpretation. These works represent a step towards integration with non-symbolic domains. However, they do not address the problem of learning the SG function, but only how to manage to use a pre-trained imperfect symbol grounder. Only one work in the literature assumes the same setting of ours [18], namely, that the agent observes sequences of non-symbolic states and rewards, and is aware of the LTL formula describing the task but without knowing the meaning of the symbols in the formula. This work employs a neural network composed of different modules, each representing a propositional symbol or a logical or temporal operator, which are interconnected in a tree-like structure, respecting the syntactic tree of the formula. This network learns a representation of states, that can be easily transferred to different LTL tasks in the same environment. However, the key distinctions between our and their work are: (i) Kuo et al. [18] learn a subsymbolic uninterpretable representation, while we learn precisely a mapping between states and symbols; (ii) their method provides benefits only in a multitask setting and is unable to expedite learning on a single task, while ours learns and leverages the symbol grounding in the individual task.\nSemisupervised Symbol Grounding Much prior work approaches the SG problem by assuming as inputs: (i) some prior symbolic logical knowledge, and (ii) a set of raw data that are annotated with the output of the knowledge (for example data are divided in positive and negative sample accordingly to the formula)[36, 2, 22, 32, 7, 14, 30]. This practice is known as semi-supervised symbol-grounding (SSSG). SSSG is tackled mainly with two families of methods. The first approach [36, 2, 22, 32] consists in embedding a continuous relaxation of the available knowledge in a larger neural networks system, and training the system so to align the outputs of the relaxation with the high-level labels. The second approach [7, 14, 30] instead maintains a crisp boolean representation of the logical knowledge and uses a process of logic abduction to correct the current SG function, that is periodically retrained with the corrections. Much work in this area does not take into account temporal aspects, and only few works focus on learning symbol grounding using temporal logics. In particular, [32] employs an extension of Logic Tensor Networks (LTNs) to represent DFAs, which exploits for classifying sequences of images. Our approach can be categorized in the first family of mentioned methods. Differently from [32], which employs LTNs and apply to classification domains, we use probabilistic Moore Machines and our main application is to RL. Furthermore our framework is more versatile than [32], as the temporal component can be both imposed and learned through the network.\nGroundability and Reasoning Shortcuts In [23], the authors define reasoning shortcuts and outline a method for counting deterministic optima, assuming knowledge of: the logical knowledge, the support dataset, and the SG function. This method is inapplicable in our scenario since we assume the SG unknown. Conversely, [32] introduces the concept of \"ungroundability\" as a property solely dependent on the logical knowledge and not on the data or the SG function. However, it gives only theoretical definitions and does not provide a method to actually discovering the RSs that cause the ungroundability. In this paper, we bridge these two concepts and we introduce the Unremovable Reasoning Shortcuts (URS), as the RSs identifiable when considering all possible observations. Beyond offering a theoretical definition, we develop an algorithm for actually calculating URS for a temporal property, which is missing in the litterature."}, {"title": "3 Background", "content": "3.1 Moore Machines\nA Moore machine $M$ is a tuple $(P, Q, O, q_0, \\delta_e, \\delta_o)$, where $P$ is a finite alphabet of input propositional symbols, $Q$ is a finite set of states, $O$ is a finite set of output symbols, $q_0 \\in Q$ is the initial state, $\\delta_e : Q \\times P \\to Q$ is the transition function, and $\\delta_o : Q \\to O$ is the output function. Let $P^*$ be the set of all finite strings over $P$, and $ \\epsilon $ the empty string. The transition function over strings $ \\delta : Q \\times P^* \\to Q$ is defined recursively as\n$\\delta (q, \\epsilon) = q$\n$\\delta (q, p + x) = \\delta_e (\\delta (q, p), x)$   (1)\nWhere $p \\in P$ is a symbol, $x \\in P^*$ is a string, and $p + x$ is the concatenation of $p$ and $x$. Consequently, the output function over strings $ \\delta : Q \\times P^* \\to O$ is defined as\n$\\delta_o (q, x) = \\delta_o (\\delta (q, x))$   (2)\nWe also define the string of output $ \\delta^{**} : Q \\times P^* \\to O^*$ as\n$\\delta^{**}(q, x) = [\\delta_o (q, x_1), \\delta_o (q, x_2), ..., \\delta_o (q, x_{|x|})]$   (3)\nWhere $T$ is the length of string $x$, and $x_{1:i}$ is the string $x$ up to index $i$. Let $x_p = [p(1), p(2), ..., p(T)]$ be the input string, where $p(t)$ is the t-th character in the string, we denote as $x_q = [q(0), q(1), ...q(T)]$ and as $x_o = [o(1), o(2), ...o(T)]$ respectively the sequence of states and output symbols produced by the automaton while processing the string, namely $q(0) = q_0$ and $q(t) = \\delta_e (q(t-1), x(t))$ and $o(t) = \\delta_o (q(t))$ for all $t > 0$.\n3.2 Non-Markovian Reward Decision Processes and Reward Machines\nIn Reinforcement Learning (RL) [29] the agent-environment interaction is generally modeled as a Markov Decision Process (MDP). An MDP is a tuple $(S, A, t, r, \\gamma)$, where $S$ is the set of environment states, $A$ is the set of agent's actions, $t : S \\times A \\times S \\to [0, 1]$ is the transition function, $r: S \\times A \\to \\mathbb{R}$ is the reward function, and $ \\gamma \\in [0, 1]$ is the discount factor expressing the preference for immediate over future reward.\nIn this classical setting, transitions and rewards are assumed to be Markovian - i.e., they are functions of the current state only. Although this formulation is general enough to model most decision problems, it has been observed that many natural tasks are non-Markovian [21]. A decision process can be non-markovian because markovianity does not hold on the reward function $r: (S \\times A)^* \\to \\mathbb{R}$, or the transition function $t: (S \\times A)^* \\times S \\to [0, 1]$, or both. In this work we focus on Non-Markovian Reward Decision Processes (NMRDP) [12]. Learning an optimal policy in such settings is hard, since the current environment outcome depends on the entire history of state-action pairs the agent has explored from the beginning of the episode; therefore, regular RL algorithms are not applicable. Rather than developing new RL algorithms to tackle NMRDP, the research has focused mainly on how to construct Markovian state representations of NMRDP. An approach of this kind are the so called Reward Machines (RMs).\nRMs are an automata-based representation of non-Markovian reward functions [15]. Given a finite set of propositions $P$ representing abstract properties or events observable in the environment, RMs"}, {"title": "4 Neural Reward Machines", "content": "In this section, we formalize Neural Reward Machines, elaborate on their implementation using neural networks, and explore the reasoning and learning tasks achievable with NRMs. Subsequently, we will specifically delve into the integration of semi-supervised symbol grounding and RL through NRMs. Finally, we present our algorithm for identifying Unremovable Reasoning Shortcuts for a temporal property.\n4.1 Definition and Notations\nWe define a Neural Reward Machine as a tuple $NRM = (S, P, Q, R, q_0, \\delta_{tp}, \\delta_{rp}, sg)$, where $S$ is the set of environment states, possibly infinite and continuous, the machine can process; $P$ is a finite set of symbols; $Q$ is a finite set of states, $R$ is a finite set of rewards; $q_0$ is the initial state; $\\delta_{tp} : Q \\times P \\times Q \\to [0, 1]$ is the machine transition probability function; $\\delta_{rp}: Q \\times R \\to [0, 1]$ is the reward probability function and $sg : S \\times P \\to [0,1]$ is the symbol grounding probability function.\nLike an RM, an NRM produces a temporal state and a reward from a sequence of environment states. However, unlike an RM, the input sequences do not necessarily need to be symbolic; they can be of any type. The symbol grounding function maintains the link with the symbolic representation, assigning to a data instance $s \\in S$ a probability value for each symbol in the alphabet $P$. Given a time sequence of environment observations $x_s = [s(1), s(2), ..., s(T)]$ the symbol grounding function grounds probabilistically each state in the set of symbols $P$ producing a sequence of symbol probability vectors $X_{pp} = [p_1, p_2, ..., p_T]$, where the k-th component of $p^{(t)}$ is equal to $sg(s(t), p_k)$. Note that probabilities are present both in the machine and the grounding: in our setting, the Reward Machine is a probabilistic Moore Machine taking as input a probabilistic symbols. We represent the stochastic Moore machine in matrix representation, as:(i) the initial state probability vector $q^{(0)} \\in [0, 1]^{|Q|}$, containing at index i a 1 if $q_0 = i$ and a 0 otherwise; (ii) a transition matrix $M_t \\in [0, 1]^{P \\times Q \\times Q}$ containing at index $(p, q, q')$ the value of $\\delta_{tp} (q, p, q')$; (iii) a reward matrix $M_r \\in [0, 1]^{|Q| \\times |R|}$ representing the reward function and containing at index $(q, r)$ the value of $\\delta_{rp} (q, r)$. The sequence of probabilistic symbols returned by the grounder is processed by the probabilistic Moore Machine, which produces a sequence of state probability vectors $x_q = [q^{(1)}, q^{(2)}, ...q^{(T)}]$ and a sequence of reward probability vectors $X_{rp} = [r^{(1)}, r^{(2)}, ...r^{(T)}]$.\n$p^{(t)} = sg(s(t))$\n$q^{(t)} = \\sum_{i=1}^{i=|P|} p[i](q^{(t-1)} M_t[i])$  (5)\n$r^{(t)} = q^{(t)} M_r$\nWhere we denote with $V[i] (v[i])$, the component i of matrix (vector) $V (v)$. We show the model in Figure 1(c). Summarizing, given a time sequence of environment states $x_s$, the model in Equation 5 can be applied recursively on $x_s$ producing: a time sequence of input symbols $X_{pp}$, one of machine states $x_q$, and a one of reward values $x_r$. Subscript $p$ denotes that sequences are probabilistically grounded, namely, each point in the sequence is a probability vector. While we denote with $X, x_q, X_r$, without subscript $p$, the symbolic time sequences. Namely $x_p = [p(1), p(2), ..., p(T)]$, $x_q = [q(1), q(2), ..., q(T)]$, and, $X_r = [r(1), r(2), ..., r(T)]$, with each point in the sequence being a symbol in the finite set $P, Q$ and $R$ respectively. Symbolic sequences can also be obtained from the probabilistic ones choosing the most probable symbol at each time step. Namely $p(t) = p_i \\in P$, with $i = arg \\max_i (p_p(t) [i])$; $q(t) = q_j \\in Q$, with $j = arg \\max_j (q_p(t) [j])$; $r(t) = r_k \\in R$, with $k = arg \\max_k (r_p(t) [k])$. We next build our neural framework by using continuous parametric models based on this structure.\n4.2 Implementation\nIn this section, we discuss the implementation of the formalism described above using Neural Networks (NN). Notably, we employ probabilistic relaxations and matrix representations of functions to facilitate this integration. Neural Networks are inherently continuous as they learn through a gradient-based optimization process, therefore they can face challenges in learning a 'crispy' boolean logical model like a Moore Machine. In our case, we aim for each function in the NRM definition-namely, $sg, \\delta_{tp}$, and $ \\delta_{rp}$-to be both initialized with prior knowledge and learned from data.\nFor the SG function, it can take any form, with the only constraint being that it must return a probability vector. Therefore, it can be"}, {"title": "4.3 Reasoning and Learning with NRM", "content": "As anticipated the framework is designed to allow both reasoning and learning and both initialization with background knowledge and train-ing with data of each module of the machine. Specifically, different configurations of prior knowledge on the NRM parameters $( \\Theta_{sg}, M_t, and M_r)$ and of the possibility to observe rewards in the environment or not result in different possible uses of NRMs, that we describe here.\nPure Reasoning By feeding the machine with a time sequence of states $x_s$ observed in the environment, we can calculate the sequence of probabilities on the machine states, $x_q = Q(x_s|\\Theta_{sg}, M_t)$, and reward values, $x_r = R(x_s|\\Theta_{sg}, M_t, M_r)$ which can be used in the same manner as RMs do: for designing non-Markovian reward functions, for augmenting the environment state, for Counterfactual Experience or hierarchical RL [15]. This process requires prior knowledge of all the NRM parameters $ \\Theta_{sg}, M_t, and M_r$ and does not require any reward feedback from the environment.\nPure Learning In case we do not know any of the NRM parameters but we can observe in the environment sequences of states $x_s$ and ground truth sequences of rewards $x_r$ we can exploit the latter to learn the NRM modules entirely from data. It is obtained by minimizing the cross-entropy loss (or other classification loss functions) between the rewards probabilities predicted by the NRM and the ground-truth rewards.\n$\\mathcal{L}(x_s, x_r) = cross\\text{-}entropy(R(x_s|\\Theta_{sg}, M_t, M_r), X_r)$   (7)\nLearning and Reasoning Integration If only some of the NRM modules are known, and sequences of states and rewards are observable in the environment, we can train the missing modules to align with both prior knowledge and data. This entails initializing the known modules with the available knowledge and training the remaining parameters using loss function in Equation 7. Various configurations are feasible in this scenario. Here, we specifically focus on symbol grounding: learning $\\Theta_{sg}$ while assuming $M_t$ and $M_r$ are known. Another reasoning-learning configuration, which we are currently exploring, is that of automata learning, where $ \\Theta_{sg}$ is known while $M_t$ and $M_r$ are unknown, which we let for future research.\n4.4 Exploiting NRMs for non-Markovian RL\nIn this paper, we assume that an agent has to learn to perform a temporally extended task by optimizing a non-Markovian reward signal. As is standard in RL, the agent interacts with the environment by taking an action and it receives a new state of the environment and a new reward in response. Additionally, we assume that the agent"}, {"title": "4.5 Groundability Analysis of Temporal Specifications", "content": "In our approch RL and Symbol Grounding are closely intertwined and mutually influence each other. SG relies on RL as we collect data through RL exploration for SG training. Simultaneously, RL relies on SG, as it utilizes an estimated representation of the automaton's state, which becomes more accurate as SG accurately predicts symbols. At the same time, regardless of the specific application, all types of semisupervised SG are affected by some reasoning shortcuts, especially when the knowledge is somewhat 'trivial' - including formulas that are either trivially false or trivially true across most instances - or contains specific symmetries [32].\nTherefore, in this section, we delve into the issue of ungroundability. We first review the definitions of ungroundability and reasoning shortcuts and we then device our own algorithm to find all the RS of a given temporal specification depending solely on the specifics, that we call for this reason 'unremovable'.\nReasoning Shortcuts Given the problem of grounding symbols of the alphabet $P$ of a certain logical knowledge $\\phi$ exploiting the knowledge and a dataset $D$ of couples (data, output of the formula), a RS is a renaming of symbols, $\\alpha: P \\to P$, different from the identity, $\\exists p \\in P | \\alpha(p) \\neq p$, which maintains perfect alignment of $ \\phi o \\alpha$ with the data in $D$. In [23] they estimate the number of reasoning shortcuts by assuming prior knowledge of the groundtruth SG function $sg^*$, the logical knowledge $\\phi$ and the support dataset $D$.\n$\\#RS(\\phi, D, sg^*) = \\sum_{\\alpha \\in A} \\mathbb{I} {\\Big[\\sum_{d \\in D} \\mathbb{I}\\{( \\phi o \\alpha)(sg^*(d)) = \\phi(sg^*(d))\\} \\Big]}$   (8)\nWhere $A$ is the set of all possible mappings $\\alpha$. Only the $ \\alpha $ mapping each symbol to itself is indeed the correct solution, therefore the knowledge is said to admit RSs on $D$ if $ \\#RS(\\phi, D, sg^*) > 1 $\nUngroundability The following definition of ungroundability is given in [32]: a symbol $p_i \\in P$ is ungroundable through a formula $ \\phi $ if there exists a mapping $ \\alpha: P \\to P$ that maps $p_i$ to $p_j$ with $p_i \\neq p_j$ such that $ \\phi = (\\phi o \\alpha)$. Where $ \\equiv $ denotes logical equivalence. Notice that ungroundability is a property of a symbol in the formula, and does not depend on the particular supporting dataset used in the application, nor it needs the knowledge of $sg^*$ to be verified. However, [32] does not define a method to actually find the mappings $ \\alpha $ which maintain logical equivalence between $ \\phi $ and $ \\phi o \\alpha $ for a specific $ \\phi $. Nonetheless, these mappings could be identified using a naive brute-force method. This involves examining the semantic equivalence between $ \\phi $ and $ \\phi o \\alpha $ for every possible mapping $ \\alpha \\in A $ and returning the mapping that yields a positive check. Given that the number of possible mappings is $|A| = |P|^{|P|}$ and equivalence checking has an exponential cost, the brute-force approach is very time consuming and can become impracticable for long formulas defined over large alphabets. Here, we introduce a smarter algorithm for identifying knowledge-preserving mappings. This algorithm leverages certain stopping criteria, which we will delineate later, to enhance speed by a factor of $10^3$. We term the mappings discovered by this algorithm as Unremovable Reasoning Shortcuts (URS), referring to the RS that are pathological of the logical knowledge, which would be never eliminated, even considering an ideal support dataset including all the possible observations of the domain.\nUnremovable Reasoning Shortcuts (URS) To calculate URSS we focus on the complete support $D^*$. Notice that, since the support is complete, we can directly create synthetically the complete symbolic dataset $D_{sym}$, and count the number of RS on that without knowing the ground truth symbol grounding function $sg^*$. We define the number of reasoning shortcuts under complete support assumption as\n$\\#RS^* (\\phi) = \\sum_{\\alpha \\in A} {\\mathbb{I} {\\Big[\\sum_{d_{sym} \\in D_{sym}} \\mathbb{I}\\{( \\phi o \\alpha)(d_{sym}) = \\phi(d_{sym})\\} \\Big]}}$  (9)"}, {"title": "5 Experiments", "content": "In this section, we report the experiments validating our framework. Our code is available on github: https://github.com/KRLGroup/NeuralRewardMachines.\nApplication We test NRMs on two types of environment inspired by the Minecraft videogame, similar to those considered by [6] and [20] and decribed in the example of Section 3.2, in which we assume the SG function is unknown.\nStates Based on this application we construct two environments that showcase different levels of difficulty in terms of symbol grounding: (i) The map environment, in which the state is the 2D vector containing the x and y current agent location, (ii) The image environment, in which the state is an image of 64x64x3 pixels showing the agent in the grid, like the one shown in Figure 1(a).\nRewards We express the non-Markovian reward as an LTLf formula, that is then transformed into a DFA and, ultimately, into a Moore Machine. In the last process, each state q is assigned a reward value, which is maximum if q is a final states and gradually decreases as the distance from q to the final state increases (as detailed in Sec-tion 4.4 and shown in Figure 1(b)). Reward values are scaled so to give maximum cumulative reward always equal to 100. We focus on patterns of formulas that are popular in non-Markovian RL [15, 33], that we call here using the categorization given in [24]. Visit formulas: we denote as Visit(p1, p2, , pn) the LTLf formula F p1 AF p2A ... AF pn, expressing that the agent has to make true at least once each pi in the formula in any possible order. Sequenced Visit: we denote with Seq_Visit(p1, p2, ..., pn) the LTLf formula F(p1 F(p2\u2227.\u06f0.\u2227 F(pn))), expressing that we want the symbol pi+1 made true at least once after the symbol pi has become True. Global Avoidance: we denote with Glob_Avoid(p1, p2, ..., pn) the formula G (\u00acp\u2081) ^ G (\u00acp2)^...\u2227G (\u00acpn), expressing that the agent must always avoid to make True the symbols p1, p2, ..., pn. F and G are temporal operators called respectively 'eventually' and 'globally', we refer the reader to [27] for the formal semantics of these operators that we explain here only intuitively. Based on these patterns of formulas we construct tasks of increasing difficulty that we aggregate in two classes. The first class is a set of tasks obtained as a conjunction of visit and sequenced visit formulas. The second class contains conjunctions of visit, sequenced visit and global avoidance formulas. We report in the appendix the formulas considered.\nComparisons Given the absence of alternative methods in the literature for leveraging ungrounded prior knowledge in non-Markovian"}, {"title": "6 Conclusions and Future Work", "content": "In conclusion, we introduce Neural Reward Machines, a neurosim-bolic framework providing non-Markovian rewards for non-symbolic-state environments. NRMs possess great versatility and applicability across various learning configurations, with a particular focus here on their capacity for grounding within environments. We address the challenge of semisupervised symbol grounding through temporal specifications and propose an algorithm for identifying Unremovable Reasoning Shortcuts in temporal tasks. We demonstrate the ability of NRMs to integrate the machine's prior knowledge and leverage it to outperform Deep RL methods, achieving nearly equivalent rewards to RMs, even without possessing full prior knowledge. We leave exploration of other reasoning-learning scenarios, such as the integration of automata learning with RL, for future research."}, {"title": "A Training setting", "content": "Implementation and Hardware The code is entirely written in Python, utilizing PyTorch as our preferred framework, and benefiting from CUDA acceleration. Training was conducted using an NVIDIA 3070 Graphics Card.\nNeural Networks Design The Critic and Actor networks comprise 3 fully connected layers with Tanh activation functions, each of hidden size equal to 120. Additionally, the Actor network concludes with an"}, {"title": "B Theorems proof", "content": "Here we demonstrate theorems provided in the main paper.\nTheorem 1. Let $D_{sym"}, "L)$ denote the set of all strings over $P$ with maximum length equal to L. If $L_1 \\textless L_2$, then $ \\#RS^* (\\phi, D_{sym}(L_2)) \\leq \\#RS^* (\\phi, D_{sym}(L_1))$.\nProof. If a mapping $\\alpha$ preserves the target output of the formula on a string of length $L_2$, then it also preserves it on the subtrace of length $L_1 \\textless L_2$, the opposite is not generally true, if a mapping preserves the output up to $L_1$ it is not said that it preserves it also up to $L_2$. Therefore the number of reasoning shortcuts can only decrease as we increase the horizon of the complete symbolic support dataset. More formally. Let us denote as $x$ the string of length $L_1$ and $x+y$ the string of length $L_2$. we have\n$\\delta^{**}(q_0, x+y) = \\delta^{**}(q_0, (x+y) o \\alpha) \\Rightarrow \\delta^{**}(q_0, x) = \\delta^{**}(q_0, x o \\alpha)$\nTherefore\n$ \\Rightarrow RS(\\phi, D_{sym}(L_2)) \\textless RS(\\phi, D_{sym}(L_1))$\nCorollary 2. The number of RS calculated on any complete support with finite horizon L is an upper bound for the number of URS. $ \\#RS^* (\\phi) \\leq \\#RS^* (\\phi, D^*_{ym}(L)) \\forall L \\textless \\infty$.\nProof. Since the number of Unremovable Reasoning Shortcuts is the number of RS on the complete (infinite) symbolic dataset, which we denote as $D_{sym}(\\infty)$, we have\n$\\#RS^*(\\phi) = \\#RS^* (\\phi, D^*_{ym}(\\infty)) \\leq \\#RS^* (\\phi, D^*_{ym}(L))$\nbecause $L \\textless \\infty$ and Theorem 1\nTheorem 3. Let $\\mathcal{A} \\subset Q$ be the set of absorbing states of $ \\phi$. If: (i) a string x has reached an absorbing state, $\\delta (q_0, x) \\in \\mathcal{A}$. (ii) for a certain mapping a, also $x o \\alpha$ has reached an absorbing state, $ \\delta_e (q_0, x o \\alpha) \\in \\mathcal{A}$. (iii) $\\alpha$ is a working map for x, namely $\\delta^{**} (q_0, x) = \\delta^{**} (q_0, x o \\alpha)$. Then $\\alpha$ is a working map also for all the strings having x as prefix, namely $\\delta^{**} (q_0, x + y) = \\delta^{**} (q_0, (x + y) o \\alpha)\\forall y \\in P^*$.\nProof. Since x has reached an absorbing state (point (i)), no matter how we expand the string, the state, and consequently the output that we get in the last step will be always the same. Namely\n$\\delta_o (q_0, x) = \\delta_o (q_0, x + y)) \\forall y \\in P^*$\nThe same holds for the string $x o \\alpha$, for point (ii).\n$\\delta_o (q_0, x o \\alpha) = \\delta_e (q_0, (x o \\alpha) + y))\\forall y \\in P^*$\nwe can bring $ +y$ inside the mapping $ \\alpha $\n$\\delta_o (q_0, x o \\alpha) = \\delta_e (q_0, (x + y) o \\alpha) \\forall y \\in P^*$\nfor point (iii) we have\n$\\delta_o (q_0, x) = \\delta_o (q_0, x o \\alpha)$\ntherefore\n$\\delta_o (q_0, x + y) = \\delta_e (q_0, (x + y) o \\alpha \\forall y \\in P^*$\nwhich demonstrates the thesis\n$\\delta^{**} (q_0, x + y) = \\delta^{**} (q_0, (x + y) o \\alpha) \\forall y \\in P^*$\nTheorem 4. Given that: (i) $\\alpha$ is a working map, for the string x + z, with $x, z \\in P^*$, $\\delta^{**} (q_0, x + z) = \\delta^{**} (q_0, (x + z) o \\alpha)$; (ii) $\\exists p \\in P$ such that both x and x + p reach the same state and $x o \\alpha$ and (x + p) o a reach the same state, $\\delta_e (q_0, x) = \\delta_e (q_0, x + p) = q \\wedge \\delta_e (q_0, x) = \\delta_e (q_0, x + p)$. Then $\\alpha$ works for the string x + y + z, $\\delta"]}