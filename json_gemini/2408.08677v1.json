{"title": "Neural Reward Machines", "authors": ["Elena Umilia", "Francesco Argenziano", "Roberto Capobianco"], "abstract": "Non-markovian Reinforcement Learning (RL) tasks are very hard to solve, because agents must consider the entire history of state-action pairs to act rationally in the environment. Most works use symbolic formalisms (as Linear Temporal Logic or automata) to specify the temporally-extended task. These approaches only work in finite and discrete state environments or continuous problems for which a mapping between the raw state and a symbolic interpretation is known as a symbol grounding (SG) function. Here, we define Neural Reward Machines (NRM), an automata-based neurosymbolic framework that can be used for both reasoning and learning in non-symbolic non-markovian RL domains, which is based on the probabilistic relaxation of Moore Machines. We combine RL with semisupervised symbol grounding (SSSG) and we show that NRMs can exploit high-level symbolic knowledge in non-symbolic environments without any knowledge of the SG function, outperforming Deep RL methods without which cannot incorporate prior knowledge. Moreover, we advance the research in SSSG, proposing an algorithm for analysing the groundability of temporal specifications, which is more efficient than baseline techniques of a factor $10^3$.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) tasks are traditionally modeled as Markovian Decision Processes (MDPs) [29], where task feedback depends solely on the last state and action. However, this formulation is inadequate for many decision problems, which are inherently non-Markovian or temporally extended [1, 21]. Intelligent agents tackling such tasks must consider the entire history of state-action pairs to act rationally within the environment. Current research in this field involves addressing non-Markovianity by expanding the state space with features that encode the environment history, and then solving the augmented-state problem with established RL algorithms. The primary challenge lies in constructing these features. For non-symbolic-state problems, a popular approach combines RL algorithms with Recurrent Neural Networks (RNNs) [13, 17], which automatically extract features from data sequences. While this method does not guarantee extracting Markovian features, it does not recquire any prior knowledge on the task. In problems with a symbolic finite state space, most works utilize Linear Temporal Logic (LTL) [27] or LTLf [8] to specify the temporal task. This specification is then compiled into an automaton, and the automaton state is combined with the environment state to render the decision process Markovian. Reward Machines (RMs) [15] exemplify this approach. RMs can also be employed in non-symbolic-state environments if the symbol grounding (SG) function is known [4]. The latter maps the environment's raw state into a boolean interpretation over the symbols used by the specifications in LTL, making the symbols observable. In summary, RM-like methods presuppose prior knowledge of: (i) the SG function, and (ii) the temporal task specification in a logical formalism. Despite previous work challenging the assumption of knowing the temporal specification [11, 37, 28, 10], no work thus far assumes a complete lack of knowledge regarding the SG function. We highlight that learning this function for realistic environments with raw and/or high-dimensional states can be challenging and necessitates a substantial amount of labeled data.\nTo achieve this, we propose a neurosymbolic (NeSy) approach to reward machines, which involves probabilistically relaxing logical constraints [31]. This approach integrates both logical reasoning and neural network-based perception into a unified framework, that we call \"Neural Reward Machines\" (NRM). We demonstrate the capability to relax the requirement of knowing the SG function while still leveraging available prior knowledge to enhance the performance of RL algorithms in non-symbolic non-Markovian environments. This achievement stems from combining RL on reward machines with semi-supervised symbol grounding (SSSG) [22, 2]. SSSG consists in connecting some prior logical knowledge with raw data by supervising the formula output on the data. SSSG is often challenging, particularly when logical knowledge is vague or when connected data doesn't cover a diverse range of scenarios [32, 23]. In such cases, multiple solutions may exist - each maintaining consistency between knowledge and data - where, only one is the intended solution, while the others are deemed \"Reasoning Shortcuts\" (RS) [23]. In this paper, we also advance SSSG research by presenting an algorithm for identifying all RSs of a given temporal specification, assuming a complete dataset of observations. We call this kind of RSs unremovable, since they solely depend on the structure of knowledge rather than the data used for SSSG or the specific symbol-perception task.\nTo the best of our knowledge, this is the first work exploiting temporal logical knowledge in single RL tasks under the assumption of completely unknown SG function, and proposing an algorithm for actually discovering RSs of temporal specifications."}, {"title": "2 Related works", "content": "Non-Markovian Reinforcement Learning with Temporal Specifications Temporal logic formalisms are widely used in Reinforcement Learning (RL) to specify non-Markovian tasks [21]. The large majority of works assumes the boolean symbols used in the formulation of the task are perfectly observable in the environment [5, 9, 11, 37, 28, 10]. For this reason they are applicable only in symbolic-state environments or when a perfect mapping between the environment state and a symbolic interpretation is known, also called labeled MDP [4]. Many works assume to know an imperfect SG function for the task [3, 34, 20]. Namely, a function that sometimes makes mistakes in predicting symbols from states or predicts a set of probabilistic beliefs over the symbol set instead of a boolean interpretation. These works represent a step towards integration with non-symbolic domains. However, they do not address the problem of learning the SG function, but only how to manage to use a pre-trained imperfect symbol grounder. Only one work in the literature assumes the same setting of ours [18], namely, that the agent observes sequences of non-symbolic states and rewards, and is aware of the LTL formula describing the task but without knowing the meaning of the symbols in the formula. This work employs a neural network composed of different modules, each representing a propositional symbol or a logical or temporal operator, which are interconnected in a tree-like structure, respecting the syntactic tree of the formula. This network learns a representation of states, that can be easily transferred to different LTL tasks in the same environment. However, the key distinctions between our and their work are: (i) Kuo et al. [18] learn a subsymbolic uninterpretable representation, while we learn precisely a mapping between states and symbols; (ii) their method provides benefits only in a multitask setting and is unable to expedite learning on a single task, while ours learns and leverages the symbol grounding in the individual task.\nSemisupervised Symbol Grounding Much prior work approaches the SG problem by assuming as inputs: (i) some prior symbolic logical knowledge, and (ii) a set of raw data that are annotated with the output of the knowledge (for example data are divided in positive and negative sample accordingly to the formula)[36, 2, 22, 32, 7, 14, 30]. This practice is known as semi-supervised symbol-grounding (SSSG). SSSG is tackled mainly with two families of methods. The first approach [36, 2, 22, 32] consists in embedding a continuous relaxation of the available knowledge in a larger neural networks system, and training the system so to align the outputs of the relaxation with the high-level labels. The second approach [7, 14, 30] instead maintains a crisp boolean representation of the logical knowledge and uses a process of logic abduction to correct the current SG function, that is periodically retrained with the corrections. Much work in this area does not take into account temporal aspects, and only few works focus on learning symbol grounding using temporal logics. In particular, [32] employs an extension of Logic Tensor Networks (LTNs) to represent DFAs, which exploits for classifying sequences of images. Our approach can be categorized in the first family of mentioned methods. Differently from [32], which employs LTNs and apply to classification domains, we use probabilistic Moore Machines and our main application is to RL. Furthermore our framework is more versatile than [32], as the temporal component can be both imposed and learned through the network.\nGroundability and Reasoning Shortcuts In [23], the authors define reasoning shortcuts and outline a method for counting deterministic optima, assuming knowledge of: the logical knowledge, the support dataset, and the SG function. This method is inapplicable in our scenario since we assume the SG unknown. Conversely, [32] introduces the concept of \"ungroundability\" as a property solely dependent on the logical knowledge and not on the data or the SG function. However, it gives only theoretical definitions and does not provide a method to actually discovering the RSs that cause the ungroundability. In this paper, we bridge these two concepts and we introduce the Unremovable Reasoning Shortcuts (URS), as the RSs identifiable when considering all possible observations. Beyond offering a theoretical definition, we develop an algorithm for actually calculating URS for a temporal property, which is missing in the litterature."}, {"title": "3 Background", "content": "A Moore machine $M$ is a tuple $(P, Q, O, q_0, \\delta_e, \\delta_o)$, where $P$ is a finite alphabet of input propositional symbols, $Q$ is a finite set of states, $O$ is a finite set of output symbols, $q_0 \\in Q$ is the initial state, $\\delta_e: Q \\times P \\rightarrow Q$ is the transition function, and $\\delta_o: Q \\rightarrow O$ is the output function. Let $P^*$ be the set of all finite strings over $P$, and $e$ the empty string. The transition function over strings $: Q \\times P^* \\rightarrow Q$ is defined recursively as\n$\\delta(q, e) = q$  \n$\\delta(q, p + x) = \\delta_e(\\delta(q, p), x)$  \n(1)\nWhere $p \\in P$ is a symbol, $x \\in P^*$ is a string, and $p + x$ is the concatenation of $p$ and $x$. Consequently, the output function over strings $: Q \\times P^* \\rightarrow O$ is defined as\n$\\delta_o(q, x) = \\delta_o(\\delta(q, x))$  \n(2)\nWe also define the string of output $*: Q \\times P^* \\rightarrow O^*$ as\n$\\delta^{**}(q, x) = [\\delta_o(q, x_1), \\delta_o(q, x_2), ..., \\delta_o(q, x_{|x|} )]$\n(3)\nWhere T is the length of string x, and $x_i$ is the string x up to index i. Let $x_p = [p(1), p(2), ..., p(T)]$ be the input string, where $p(t)$ is the t-th character in the string, we denote as $x_q = [q(0), q(1), ...q(T)]$ and as $x_o = [o(1), o(2), ...o(T)]$ respectively the sequence of states and output symbols produced by the automaton while processing the string, namely $q(0) = q_0$ and $q(t) = d_e(q(t-1), x(t))$ and $o(t) = do(q(t))$ for all t > 0.\nIn Reinforcement Learning (RL) [29] the agent-environment interaction is generally modeled as a Markov Decision Process (MDP). An MDP is a tuple $(S, A, t, r, \\gamma)$, where $S$ is the set of environment states, $A$ is the set of agent's actions, $t : S \\times A \\times S \\rightarrow [0, 1]$ is the transition function, $r: S \\times A \\rightarrow R$ is the reward function, and $\\gamma \\in [0, 1]$ is the discount factor expressing the preference for immediate over future reward\nIn this classical setting, transitions and rewards are assumed to be Markovian - i.e., they are functions of the current state only. Although this formulation is general enough to model most decision problems, it has been observed that many natural tasks are non-Markovian [21]. A decision process can be non-markovian because markovianity does not hold on the reward function $r: (S \\times A)^* \\rightarrow R$, or the transition function $t: (S \\times A)^* \\times S \\rightarrow [0, 1]$, or both. In this work we focus on Non-Markovian Reward Decision Processes (NMRDP) [12]. Learning an optimal policy in such settings is hard, since the current environment outcome depends on the entire history of state-action pairs the agent has explored from the beginning of the episode; therefore, regular RL algorithms are not applicable. Rather than developing new RL algorithms to tackle NMRDP, the research has focused mainly on how to construct Markovian state representations of NMRDP. An approach of this kind are the so called Reward Machines (RMs).\nRMs are an automata-based representation of non-Markovian reward functions [15]. Given a finite set of propositions $P$ representing abstract properties or events observable in the environment, RMs specify temporally extended rewards over these propositions while exposing the compositional reward structure to the learning agent. Formally, in this work we assume the reward can be represented as a Reward Machine RM = $(P, Q, R, q_0, \\delta_t, \\delta_r, L)$, where P is the automaton alphabet, Q is the set of automaton states, R is a finite set of continuous reward values, $q_0$ is the initial state, $\\delta_t: Q \\times P \\rightarrow Q$ is the transition function, $\\delta_r: Q \\rightarrow R$ is the reward function, and $L: S \\rightarrow P$ is the labeling (or symbol grounding) function, which recognizes symbols in the environment states.\nLet $x_s = [s(1), s(2), ..., s(t)]$ be a sequence of states the agent has observed in the environment up to the current time instant t, we define the labeling function over sequences $L^* : S^* \\rightarrow P^*$ as\n$L^*(x_s) = [L(s(1)), L(s(2)), ..., L(s(t))]$  \n(4)\nWe denote with $\\delta_t$ and $\\delta_r$ the transition and reward function over strings, which are defined recursively, analogously to Equations 1 and 2. Given $x_s$, the RM produces an history-dependent reward value at time t, $r(t) = \\delta_r(q_0, L^*(x_s))$ and an automaton state at time t, $q(t) = \\delta_t(q_0, L^* (x_s))$. The reward value can be used to guide the agent toward the satisfaction of the task expressed by the automaton, while the automaton state can be used to construct a Markovian state representation. In fact it was proven that the augmented state $(s(t), q(t))$ is a Markovian state for the task expressed by the RM [9]."}, {"title": "4 Neural Reward Machines", "content": "In this section, we formalize Neural Reward Machines, elaborate on their implementation using neural networks, and explore the reasoning and learning tasks achievable with NRMs. Subsequently, we will specifically delve into the integration of semi-supervised symbol grounding and RL through NRMs. Finally, we present our algorithm for identifying Unremovable Reasoning Shortcuts for a temporal property.\nWe define a Neural Reward Machine as a tuple $NRM = (S, P, Q, R, q_0, t_p, d_{rp}, sg)$, where $S$ is the set of environment states, possibly infinite and continuous, the machine can process; $P$ is a finite set of symbols; $Q$ is a finite set of states, $R$ is a finite set of rewards; $q_0$ is the initial state; $d_{tp} : Q \\times P \\times Q \\rightarrow [0, 1]$ is the machine transition probability function; $d_{rp}: Q \\times R \\rightarrow [0, 1]$ is the reward probability function and $sg : S \\times P \\rightarrow [0,1]$ is the symbol grounding probability function.\nLike an RM, an NRM produces a temporal state and a reward from a sequence of environment states. However, unlike an RM, the input sequences do not necessarily need to be symbolic; they can be of any type. The symbol grounding function maintains the link with the symbolic representation, assigning to a data instance $s \\in S$ a probability value for each symbol in the alphabet $P$. Given a time sequence of environment observations $x_s = [s(1), s(2), ..., s(T)]$ the symbol grounding function grounds probabilistically each state in the set of symbols $P$ producing a sequence of symbol probability vectors $X_{pp} = [p^{(1)}, p^{(2)}, ..., p^{(T)}]$, where the k-th component of $p^{(t)}$ is equal to $sg(s(t), p_k)$. Note that probabilities are present both in the machine and the grounding: in our setting, the Reward Machine is a probabilistic Moore Machine taking as input a probabilistic symbols. We represent the stochastic Moore machine in matrix representation, as:(i) the initial state probability vector $q^{(0)} \\in [0,1]^{|Q|}$, containing at index i a 1 if $q_0 = i$ and a 0 otherwise; (ii) a transition matrix $M_t \\in [0,1]^{P \\times Q \\times Q}$ containing at index $(p,q, q')$ the value of $d_{tp}(q, p, q')$; (iii) a reward matrix $M_r \\in [0, 1]^{|Q||R|}$ representing the reward function and containing at index (q, r) the value of $d_{rp}(q, r)$. The sequence of probabilistic symbols returned by the grounder is processed by the probabilistic Moore Machine, which produces a sequence of state ate probability vectors $x_{qp} = [q^{(1)}, q^{(2)}, ...q^{(T)}]$ and a sequence of reward probability vectors $X_{rp} = [r^{(1)}, r^{(2)}, ..., r^{(T)}]$.\n$p^{(t)} = sg(s(t))$  \n$q^{(t)} = \\prod_{i=1}^{i=|P|} p[i](q^{(t-1)} M_t[i])$  \n$r^{(t)} = q^{(t)} M_r$  \n(5)\nWhere we denote with $V[i] (v[i])$, the component i of matrix (vector) $V (v)$. We show the model in Figure 1(c). Summarizing, given a time sequence of environment states $x_s$, the model in Equation 5 can be applied recursively on $x_s$ producing: a time sequence of input symbols $X_{pp}$, one of machine states $x_{qp}$, and a one of reward values $x_{rp}$. Subscript p denotes that sequences are probabilistically grounded, namely, each point in the sequence is a probability vector. While we denote with $X, x_q, X_r$, without subscript p, the symbolic time sequences. Namely $x_p = [p(1), p(2), ..., p(T)], x_q = [q(1), q(2), ..., q(T)]$, and, $X_r = [r(1), r(2), ..., r(T)]$, with each point in the sequence being a symbol in the finite set $P$, $Q$ and $R$ respectively. Symbolic sequences can also be obtained from the probabilistic ones choosing the most probable symbol at each time step. Namely $p(t) = p_i \\in P$, with $i = arg \\max_i (p^{(t)} [i])$; $q(t) = q_j \\in Q$, with $j = arg \\max_j (q^{(t)} [j])$; $r(t) = r_k \\in R$, with $k = arg \\max_k (r^{(t)} [k])$. We next build our neural framework by using continuous parametric models based on this structure."}, {"title": "4.2 Implementation", "content": "In this section, we discuss the implementation of the formalism described above using Neural Networks (NN). Notably, we employ probabilistic relaxations and matrix representations of functions to facilitate this integration. Neural Networks are inherently continuous as they learn through a gradient-based optimization process, therefore they can face challenges in learning a 'crispy' boolean logical model like a Moore Machine. In our case, we aim for each function in the NRM definition-namely, sg, tp, and drp-to be both initialized with prior knowledge and learned from data.\nFor the SG function, it can take any form, with the only constraint being that it must return a probability vector. Therefore, it can be implemented with any NN featuring softmax activation on the output layer. Conversely, learning the Moore Machine with backpropagation is more challenging. Our approach is based on the intuition that Probabilistic Moore Machines (PMM) are closely related to Recurrent Neural Networks (RNN), as they calculate the next state and output using multiplications between continuous vectors and matrices, similar to RNNs. Even though (deterministic) machines can be represented in matrix form with only one-hot row vectors. Following this idea, we define the recurrent module as a parametric PMM whose representation can be driven to be close to one-hot during training, enabling logical induction through backpropagation. We achieve this effect using an activation function that smoothly approximates discrete output, as seen in prior work [16, 35, 19]. In particular, we use a modified version of the classical softmax activation function: $softmax_\\tau(x, \\tau) = softmax(x/\\tau)$, where $0 < \\tau < 1$ is a temperature value controlling the activation steepness. When $\\tau$ is close to 0, the $\\tau$-softmax returns outputs close to one-hot vectors; when $\\tau$ is 1, the function is the standard softmax.\nThe NRM implementation with parametric models is represented as follows:\n$p^{(t)} = sg(s(t); \\theta_{sg})$  \n$q^{(t)} = \\prod_{i=1}^{i=|P|} p[i](q^{(t-1)} (softmax_\\tau(M_t, \\tau))[i])$  \n$r^{(t)} = q^{(t)} softmax_\\tau(M_r, \\tau)$  \n(6)\nwhere the symbol grounding function sg is now a parametric function with learnable parameters $\\theta_{sg}$, and $M_t$ and $M_r$ are matrices of parameters. We process $M_t$ and $M_r$ with softmax, before using them in order to ensure they tend to represent a (deterministic) Moore Machine as the temperature decreases. The discretization of parameters in $M_t$ and $M_r$ through temperature decreasing is unnecessary and can be omitted if the machine is initialized with a known Moore Machine. It is only required when learning the machine from data. A scheme of the proposed implementation is shown in Figure 1(c). The model in equation 6 can be seen as three parametric functions (NNs) producing the three sequences of probability vectors as output, when given a sequence of states $x_s$ as inputs, namely: $X_{pp} = G(x_s|\\theta_{sg})$, $X_{qp} = Q(x_s|\\theta_{sg}, M_t)$, and $x_{rp} = R(x_s|\\theta_{sg}, M_t, M_r)$."}, {"title": "4.3 Reasoning and Learning with NRM", "content": "As anticipated the framework is designed to allow both reasoning and learning, and both initialization with background knowledge and training with data of each module of the machine. Specifically, different configurations of prior knowledge on the NRM parameters $(\\theta_{sg}, M_t,$ and $M_r)$ and of the possibility to observe rewards in the environment or not result in different possible uses of NRMs, that we describe here.\nBy feeding the machine with a time sequence of states $x_s$ observed in the environment, we can calculate the sequence of probabilities on the machine states, $x_{qp} = Q(x_s|\\theta_{sg}, M_t)$, and reward values, $x_{rp} = R(x_s|\\theta_{sg}, M_t, M_r)$ which can be used in the same manner as RMs do: for designing non-Markovian reward functions, for augmenting the environment state, for Counterfactual Experience or hierarchical RL [15]. This process requires prior knowledge of all the NRM parameters $\\theta_{sg}, M_t,$ and $M_r$ and does not require any reward feedback from the environment.\nIn case we do not know any of the NRM parameters but we can observe in the environment sequences of states $x_s$ and ground truth sequences of rewards $x_r$ we can exploit the latter to learn the NRM modules entirely from data. It is obtained by minimizing the cross-entropy loss (or other classification loss functions) between the rewards probabilities predicted by the NRM and the ground-truth rewards.\n$L(x_s, x_r) = cross-entropy(R(x_s|\\theta_{sg}, M_t, M_r), X_r)$  \n(7)\nIf only some of the NRM modules are known, and sequences of states and rewards are observable in the environment, we can train the missing modules to align with both prior knowledge and data. This entails initializing the known modules with the available knowledge and training the remaining parameters using loss function in Equation 7. Various configurations are feasible in this scenario. Here, we specifically focus on symbol grounding: learning $\\theta_{sg}$ while assuming $M_t$ and $M_r$ are known. Another reasoning-learning configuration, which we are currently exploring, is that of automata learning, where $\\theta_{sg}$ is known while $M_t$ and $M_r$ are unknown, which we let for future research."}, {"title": "4.4 Exploiting NRMs for non-Markovian RL", "content": "In this paper, we assume that an agent has to learn to perform a temporally extended task by optimizing a non-Markovian reward signal. As is standard in RL, the agent interacts with the environment by taking an action and it receives a new state of the environment and a new reward in response. Additionally, we assume that the agent has a purely symbolic knowledge of the task it is executing. For example, in a scenario like the Minecraft game depicted in Figure 1(a), the task for the agent could be \"reach the door, the pickaxe, and the lava in any possible order.\" In this case, the agent would have access to the Moore Machine shown in Figure 1(b). However, it lacks knowledge on how to recognize a pickaxe, a gem, or a door within its environment. In this setting, traditional RMs are inapplicable because of the missing symbol grounding function, while purely deep-learning-based approaches have no way to exploit the symbolic knowledge and will rely only on rewards. With our framework we exploit both symbolic knowledge and data from the environment by interleaving RL and semi-supervised symbol grounding of the NRM. In particular, we initialize the parameters $M_t$ and $M_r$ with the task prior knowledge, and the parameters of the grounder $sg$ with random weights. We record each episode of interaction, as a sequence of states $x_s$ and a sequence of rewards $x_r$. We calculate the machine state sequence $x_{qp}$ with the NRM as $Q(x_s|\\theta_{sg}, M_t, M_r)$, and we augment each environment state with the predicted machine state (reasoning step). The interaction is leaded by a RL algorithm running on the augmented state $(s(t), q^{(t)})$. Since the symbol grounder is initially randomized, the predicted machine state initially deviates from the ground truth, and the state representation is not perfect. However, at regular intervals, we update the symbol grounder by training on sequences xt and xr collected in the environment so to minimize the loss in Equation 7 (learning step). As the agent observes more scenarios in the environment, the symbol grounding function of the model gradually becomes more similar to the unknown ground truth, and so does the distribution of machine states. In case the perfect SG function is learned the NRM becomes equivalent to a RM.\nSince the reward values serve as supervision for the NRM, we have observed that overly sparse rewards are not very effective for symbol grounding. For instance, consider the automaton shown in Figure 1(b), with 8 states, of which only one is final (state 2). If we employ a sparse signal, rewarding the agent solely upon task completion, this entails assigning a positive reward to state 2 and 0 reward to all the other states. Subsequently, all the unsuccessful episodes would provide no feedback to the symbol grounder. Moreover, in case the agent accomplishes the task, the grounder would glean feedback solely to discern that the last observation leading to winning reward ought to be associated with one of the three symbols bringing from a non rewarding state to state 2: lava (L), door (D), or pickaxe (P). However, it would lack the means to distinguish among these three symbols in the last observation, or discern which symbols the agent has encountered in the previous observations. We, therefore, assume a potential-based reward shaping [26], where the potential function varies based on the distance on the automaton from the nearest final state. This partitions the states of the machine in figure according to 4 reward values, $r_1, r_2, r_3$ and $r_4$. Reward shaping is very commonly applied in RM applications to facilitate policy learning through RL [15]. Note that this type of reward generally does not provide distinct feedback for each single state of the machine. Consequently, while the resulting grounding process is feasible, it is by no means straightforward."}, {"title": "4.5 Groundability Analysis of Temporal Specifications", "content": "In our approch RL and Symbol Grounding are closely intertwined and mutually influence each other. SG relies on RL as we collect data through RL exploration for SG training. Simultaneously, RL relies on SG, as it utilizes an estimated representation of the automaton's state, which becomes more accurate as SG accurately predicts symbols. At the same time, regardless of the specific application, all types of semisupervised SG are affected by some reasoning shortcuts, especially when the knowledge is somewhat 'trivial' - including formulas that are either trivially false or trivially true across most instances - or contains specific symmetries [32].\nTherefore, in this section, we delve into the issue of ungroundability. We first review the definitions of ungroundability and reasoning shortcuts and we then device our own algorithm to find all the RS of a given temporal specification depending solely on the specifics, that we call for this reason 'unremovable'.\nGiven the problem of grounding symbols of the alphabet $P$ of a certain logical knowledge $\\phi$ exploiting the knowledge and a dataset $D$ of couples (data, output of the formula), a RS is a renaming of symbols, $a: P \\rightarrow P$, different from the identity, $\\exists p \\in P|a(p) \\neq p$, which maintains perfect alignment of $\\phi \\circ a$ with the data in $D$. In [23] they estimate the number of reasoning shortcuts by assuming prior knowledge of the groundtruth SG function $sg^*$, the logical knowledge $\\phi$ and the support dataset $D$.\n$\\#RS(\\phi, \\mathcal{D}, sg^*) = \\sum_{\\alpha \\in \\mathcal{A}} I\\left{ \\bigwedge_{d \\in \\mathcal{D}} (\\phi \\circ a)(sg^*(d)) = \\phi(sg^*(d)) \\right}$  \n(8)\nWhere $\\mathcal{A}$ is the set of all possible mappings a. Only the a mapping each symbol to itself is indeed the correct solution, therefore the knowledge is said to admit RSs on $\\mathcal{D}$ if $\\#RS(\\phi, \\mathcal{D}, sg^*) > 1$\nThe following definition of ungroundability is given in [32]: a symbol $p_i \\in P$ is ungroundable through a formula $\\phi$ if there exists a mapping $a: P \\rightarrow P$ that maps $p_i$ to $p_j$ with $p_i \\neq p_j$ such that $\\phi = (\\phi \\circ a)$. Where $\\equiv$ denotes logical equivalence. Notice that ungroundability is a property of a symbol in the formula, and does not depend on the particular supporting dataset used in the application, nor it needs the knowledge of $sg^*$ to be verified. However, [32] does not define a method to actually find the mappings a which maintain logical equivalence between $\\phi$ and $\\phi \\circ a$ for a specific $\\phi$. Nonetheless, these mappings could be identified using a naive brute-force method. This involves examining the semantic equivalence between $\\phi$ and $\\phi \\circ a$ for every possible mapping $a \\in \\mathcal{A}$ and returning the mapping that yields a positive check. Given that the number of possible mappings is $|\\mathcal{A}| = |P|^{|P|}$ and equivalence checking has an exponential cost, the brute-force approach is very time consuming and can become impracticable for long formulas defined over large alphabets. Here, we introduce a smarter algorithm for identifying knowledge-preserving mappings. This algorithm leverages certain stopping criteria, which we will delineate later, to enhance speed by a factor of $10^3$. We term the mappings discovered by this algorithm as Unremovable Reasoning Shortcuts (URS), referring to the RS that are pathological of the logical knowledge, which would be never eliminated, even considering an ideal support dataset including all the possible observations of the domain.\nTo calculate URSS we focus on the complete support $\\mathcal{D}^*$. Notice that, since the support is complete, we can directly create synthetically the complete symbolic dataset $\\mathcal{D}_{sym}$, and count the number of RS on that without knowing the ground truth symbol grounding function $sg^*$. We define the number of reasoning shortcuts under complete support assumption as\n$\\#RS^* (\\phi) = \\sum_{\\alpha \\in \\mathcal{A}} \\left{ \\bigwedge_{d_{sym} \\in \\mathcal{D}_{sym}} (\\phi \\circ a)(d_{sym}) = \\phi(d_{sym}) \\right}$  \n(9)"}, {"title": "5 Experiments", "content": "In this section", "github": "https://github.com/KRLGroup/NeuralRewardMachines.\nApplication We test NRMs on two types of environment inspired by the Minecraft videogame", "6": "and [20", "grounding": "i) The map environment", "33": "that we call here using the categorization given in [24", "formulas": "we denote as $Visit(p_1", "Visit": "we denote with $Seq_Visit(p_1, p_2, ..."}]}