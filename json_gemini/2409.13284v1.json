{"title": "Time Distributed Deep Learning models for Purely Exogenous Forecasting. Application to Water Table Depth Prediction using Weather Image Time Series", "authors": ["Matteo Salis", "Abdourrahmane M. Atto", "Stefano Ferraris", "Rosa Meo"], "abstract": "Groundwater resources are one of the most relevant elements in the water cycle, therefore developing models able to accurately predict them is a pivotal task in the sustainable resources management framework. Deep Learning (DL) models have been revealed very effective in hydrology, especially by feeding directly spatially distributed data (e.g. raster data). In many regions, hydrological measurements are difficult to obtain regularly or periodically in time, and in some cases, last available data are not up to date. Reversely, weather data, which have significant impacts on water resources, are usually more available and with higher quality. More specifically, we have proposed two different DL models to predict the water table depth in the Grana-Maira catchment (Piemonte, IT) using only exogenous weather image time series. To deal with the image time series, both the model are made of a first Time Distributed Convolutional Neural Network (TDC) which encodes the image available at each time step into a vectorial hidden representation. The first model, TDC-LSTM uses then a Sequential Module based on an LSTM layer to learn temporal relations and output the predictions. The second model, TDC-UnPWaveNet uses instead a new version of the WaveNet architecture, adapted here to output a sequence shorter and completely shifted in the future with respect to the input one. To this aim, and to deal with the different sequence lengths in the UnPWaveNet, we have designed a new Channel Distributed layer, that acts like a Time Distributed one but on the channel dimension, i.e. applying the same set of operations to each channel of the input. TDC-LSTM and TDC-UnPWaveNet have shown both remarkable results. However, the two models have focused on different learnable information: TDC-LSTM has focused more on lowering the bias, while the TDC-UnPWaveNet has focused more on the temporal dynamics maximising correlation and KGE.", "sections": [{"title": "1. Introduction", "content": "Water resources management is a key element in the sustainable development framework, even more in the case of climate change [1]. Freshwater is essential for sanitation and hygiene standards but also for granting food avail- ability and economic stability [2]. Groundwater resources are the second most relevant component of the water cycle, accounting for roughly 30% of freshwater resources all over the world, just after glacier and ice caps which account for 68.7% [3]. Furthermore, groundwater resources prove to be a more stable source of freshwater with respect to others like rivers and lakes, whose water storage varies promptly with the current weather conditions [4]. In fact, In Europe, 65% of drinking water and 25% of irrigation comes from groundwater resources [5] and these percentages are expected to increase given the expected effects of climate change [1, 6, 2, 7].\nWater management policies are a pivotal tool to pursue and achieve sustainable development [8, 9] and fulfil the needs of water both in terms of drinkable water for humans and irrigation of crops. Accounting for the water resources of a region is of utmost relevance to plan water policies, and for this, it is essential to develop models which can quantify groundwater resources. The phenomena concerning groundwater are extremely complex and very related to the physical properties of the context [10, 11], not by chance physical models need a lot of information of the region of concern and are very scale-dependent [12, 13]. In the last years, Deep Learning (DL) techniques have been applied to groundwater modeling, and they have proved to be very efficient in obtaining remarkable results without needing all the physical knowledge of the region under study [14, 15, 16, 17, 18, 19, 20, 21]. In particular, in addition to autoregressive terms providing information on the past states of the variable to predict (target variable), most of the DL studies use exogenous weather variables (e.g. precipitation, temperature, etc.) as the major input motivated by domain knowledge and data availability. In fact, other types of data, like the anthropogenic pressure on water resources (e.g. water abstractions, irrigations), are used only by a minority of studies because of the scarce availability in many catchments, and in general at the large scale [22, 23, 24].\nTo quantify groundwater resources, groundwater level (GWL) is usually adopted as the target variable [20]. GWL represents the distance between the sea level and the higher surface of the groundwater body, which often is defined as the water table, i.e. the surface of the phreatic aquifer. Another possibility of quantification is to measure the water table depth, i.e. the distance between the ground surface and the water table. In this way, a decrease in the water table depth means an increase in groundwater resources, reversely an increase of the water table depth means a decrease of the water stored in the phreatic aquifer. The more widespread data type adopted to predict the water table depth is tabular data (i.e one-dimensional time series) for both the input and the output, as in [25, 19, 21]. However, [26, 27] revealed the usefulness of using spatially distributed input data (e.g. raster data) as input data for hydrological data-driven modeling, letting the DL model find the most useful relations among all the input variables spread over the region of interest (ROI). This could be done using as input, a time series of weather raster images (i.e. a multidimensional time series), in which an image for each considered weather variable is retrieved\u00b9 at each time"}, {"title": "1.1. Related Works", "content": "DL techniques have already been applied proficiently to groundwater resource forecasting. In [25] authors made a comparison of different DL architectures, namely NARX [57], 1D CNN and LSTM to predict GWLs on 17 sensors in the Upper Rhine Graben (URG) region. NARX is a neural network architecture specifically designed to model autoregressive terms (i.e. past values of the target) and exogenous data in a nonlinear fashion. They trained local models for each sensor using Bayesian optimization and fed as input, weather variables measured by nearby sensors (i.e. in a tabular structure), furthermore, for some of the sensors, also an autoregressive term. They found CNN faster in training and inference than other methods. LSTM was revealed to be the worst performing, while NARX performed the best, but this could be thanks to the intrinsic use of an autoregressive component, not explicitly provided to CNN and LSTM. However, in many other works, LSTM performed better than other methods like ARIMA, \u0391\u039d\u039d[58], Random Forest [18]. In [59] authors made a comparison over many models and they found LSTM and NARX as the best models without a clear winner between the two. Notwithstanding, LSTM seems to be more established and widespread in the DL community and also in hydrological applications, which more frequently adopt LSTM as the reference model for data-driven modeling [60, 61, 62, 63].\nMost of the studies on groundwater modeling deals with tabular data to model their target. This means retrieving input and output data directly from the measurement sensor network over the region of interest (i.e. geospatial data in more statistical terms). In this way, each observation in the dataset is indexed by the time of acquisition, and longitude and latitude of the sensor by which it is measured. Spatially distributed data instead are represented in a grid format (i.e. raster) in which each portion of the area under study is represented by a square of the grid (or a pixel if one looks at the raster as an image). This type of data represents the variables of interest all over the region under study, and not only on the coordinate in which the sensors are located. Spatially distributed data could be created either by spatial interpolation of tabular data or by using other sensors like satellites (e.g. GRACE data [64]). In both cases, in the DL framework, using spatially distributed data as input could facilitate the model in understanding spatial relations and"}, {"title": "1.2. Case Study Description", "content": "Groundwater resources in Italy are even more exploited than European average statistics. In fact, in Italy 85% of drinking water [68] is extracted from groundwater sources. In Piemonte, an administrative Region in the north-west of Italy, nearly half of total water abstractions are for the agriculture sector [69, 70] which extensively adopts irrigation to meet the water needs of crops. It is estimated that 83% of irrigable lands are effectively irrigated [71], a fact that states the limited use of seasonal precipitation as a direct source of water in place of human abstractions.\nPiemonte is a very heterogeneous region from the geographical point of view, with Alps near the western, northern and southern borders, hills extending from the centre to south-est, and plains which cover an area from the Cuneo"}, {"title": "1.2.1. Data", "content": "We retrieved three time series of the water table depth from sensors in the municipalities of Vottignasco, Sav- igliano, and Racconigi\u00b3. These sensors are part of the measurement network of the Regional Environmental Agency (ARPA4) and are freely available by request. Table 1 reports different information and summary statistics for the three series, and Figure 2 shows the weekly average series for the three sensors. This figure highlights a problem of missing data and irregularities between the series. In fact, some huge missing periods are present and, furthermore, these gaps are not the same for the three series. Given the large extension of some of these missing periods, it was considered more sensible not to impute any data, but to try to build deep learning models able to learn without making imputation."}, {"title": "2. Methods", "content": "This work has aimed to train local models for each sensor independently of the others. This is because it is consistent with the literature (see for example [25, 27, 21]) but also because of the irregularities between the series explained in Section 1.2. In fact, if one wanted to train a global model using the data from all the sensors, it would require getting only the data from non-missing dates available jointly to all three sensors, discarding in this way a lot of information. Furthermore, as shown in Section 1.2, the three series show different dynamic behaviour and a general global model could be too restrictive and of scarce utility for the domain application.\nThe local models we have developed take as input a multivariate and multidimensional time series, i.e. a video, of time length T, spatial extent H \u00d7 W (i.e. height and width of each frame), and P weather features. Each local model forecasts the water table depth at a weekly time step t in a sliding windows fashion. Formally, a local model has to learn the relation f which links $y_t = f(X^{H,W,P,T}_t) + e_t$, where e is the irreducible error term, and the input $X^{H,W,P,T}_t$ is defined as the multivariate image sequence $X^{H,W,P,T}_t = \\{X_{(h,w,p,\\tau)} : h \\in [1;H], w \\in [1;W], p \\in [1;P], \\tau \\in [t - T + 1;t]\\}$, where X denotes the tensor composed by the full length image time series. Given that groundwater phenomena y could have a very long memory, we have used a very long input weather image time series length, setting T to 104 (i.e. 2 years in weekly terms) and letting the DL models use the most relevant past information.\nAs already stated, the proposed models are made of two modules. The first module is a Time Distributed CNN (TDC) whose structure is identical in the two models. The TDC is responsible for learning spatial relations and"}, {"title": "2.1. Distributed Layers", "content": "it outputs a Time Distributed Hidden Representation of the input image time series (Figure 3), i.e. it encodes each frame of the video into a vector of dimension D; in other words, it extracts a classical multivariate (of D variables) time series from the input. The second module, the Sequential Module, is what characterizes the two different models called TDC-LSTM and TDC-UnPWaveNet. In the following, the building boxes of the two models are explained, especially the modification and novelties carried out in the development of the UnPWaveNet, as the Channel Distributed layer."}, {"title": "2.1.1. Time Distributed Layers", "content": "Generally speaking, inside a standard Neural Network, a hidden layer is made of many neurons, each of which is responsible of computing an affine transformation of the input and applying a non-linear activation function g. Formally $A_{k,l} = g(W_{k,l} A_{l-1} + \\beta_{k,l})$ where $A_{k,l}$ is the output of a general neuron k in layer l, and $A_{l-1}$ is the output matrix of the layer 1 \u2013 1; W and \u03b2 contain the parameters to be learned. It is possible to substitute the \"simple\" neuron with more complex operations, still made of neurons, but encapsulated in a so-called cell, while the formal neurons inside the cell are referred to as \"units\". An exemplification concerning sequential data could be RNNs, and in particular, the LSTM layer, which is made of as many cells as the number of elements in the input sequence (or time step in the case of temporal data). Every cell is responsible for extracting long and short-term temporal dependencies and passing this information to the subsequent cell (through recurrent connection) [35, 74]. However, in the LSTM layer, and in RNN in general, the weights used by neurons in a cell are the same for every cell in the layer. In other words, each element of the input sequence is processed by a cell with the same parameters, what differs between the cells of an LSTM layer are the inputs of every cell.\nThe concept of applying the same set of operations to every element of the input sequence is not applied only for RNNs, but it is a general way of proceeding also in other architectures. A layer that works in this way is usu- ally referred to as a Time Distributed (TD) layer. Not by chance, in Keras there is a specific layer-class named"}, {"title": "2.1.2. Channel Distributed Layers", "content": "While analyzing the TD layers, a question caught our attention: Why not apply the behaviour of TD layer to chan- nels (i.e. variables) instead of time? This implies processing each channel individually and performing computations on the temporal dimension. In this fashion, the channel-wise information is preserved, while the sequential (temporal) information is processed with the same cell for each channel. This brought us to develop the Channel Distributed (CD) cell-based layers that, as explained in the following, have been adopted in the development of the UnPWaveNet architecure.\nInstead of looking at the multivariate time series Z as a series of T time-indexed vectors each with C element, it is possible to interpret it as a set of C univariate time series. Then, taking a univariate series $c_j$ with $j \\in [1;C]$, it is feasible to feed the $c_i$ series into a cell and apply the same cell $V_{c_j}$, $j \\in [1;C]$. Figure 4b represents a CD layer with a fully connected cell. If the number of neurons T* in the cell is less than the number of elements in the input univariate series (i.e. T < T*), the CD layer compresses the time dimension of the input sequence, leaving untouched the channel dimension which still contains C variables. Reversely, if T > T* the CD layer will expand the time dimension. This means that the CD layer squeezes or dilates the time dimension of all channels with the same cell; that is exactly what the TD layer does but acting instead on the time dimension. As for the TD layer, the cell could take any form. However, in our application we found the fully connected cell to produce already satisfying results in the CD layer of the UnPWaveNet."}, {"title": "2.2. WaveNet & UnPWaveNet", "content": "In the last year, many studies have tried to develop new convolutional models for temporal-sequential data tasks to compete with RNN[37, 38, 39, 42, 43, 76]. Most of these works are based on the dilated convolution, which enables the exponential expansion of the receptive field of the network over the input sequence (i.e. look far away in the past of the input series) [45]. WaveNet [46] is exactly based on this concept, and it also integrates a causal constraint using"}, {"title": "2.3. Proposed models", "content": ""}, {"title": "2.3.1. TDC module", "content": "The TDC module is responsible for learning a vectorial representation of the images available at each time step. Thus, it converts the input image time series (i.e. video) into a classical multivariate time series. Figure 7 depicts the architecture of the TDC module. It is made up of a TD CNN which takes the image with P channels available at every time 7 and feeds it into a 4-layer CNN with filters of size 2 and leaky-ReLU activation function. These 4 layers reduce the spatial dimension and increase the channels (see the number of filters shown in Figure 7). The last max pooling layer is then responsible for squeezing the spatial extent and outputs a vector with 16 elements.\nAlong with the convolutional operations, the one hot encoding (OHE) of the corresponding month of t is computed using 11-dimensional vector; this lets the network to take account of seasonality behaviours."}, {"title": "2.3.2. TDC-LSTM model", "content": "The TDC-LSTM model uses the TDC module and then a Sequential Module as depicted in Figure 8. In detail, a first bottleneck layer made of a TD fully connected followed by a Leaky-ReLU activation reduces the channel dimension to 16. This decreases the number of parameters in the subsequent layers, and thus it helps also in mitigating overfitting [31, 43]. Then, a 1D spatial dropout with probability 0.15 is employed as a regularization technique. Instead of the classical dropout, the spatial dropout zero-out entire channels and not single element inside channels; this is done for facing correlation issues between consecutive elements in a channel [80]. A single LSTM layer with 32 units is then adopted to model the temporal relations, leaky-ReLU and sigmoid are employed as activation functions for the gates inside the LSTM cells. Leaky-ReLU has proved to be more effective than tanh in this task providing better results. A fully connected layer with 8 neurons and leaky-ReLU is then used to reduce the dimensionality of the 32-dimensional output of the LSTM. The last output layer computes an affine transformation and outputs the water table depth. The TDC-LSTM model has in total 9705 parameters, 6272 of which are from the LSTM layer."}, {"title": "2.3.3. TDC-UnPWaveNet model", "content": "The structure of the TDC-UnPWaveNet is very similar to the TDC-LSTM. The TDC module is still the same, however, the Sequential Module adopts the UnPWaveNet for learning temporal relations. Figure 6b depicts the ar-"}, {"title": "2.4. Implementation details", "content": ""}, {"title": "2.4.1. Preprocessing", "content": "The raw weather raster images covered the entire Piemonte region, to focus on the catchment all the images were clipped on the ROI maintaining a squared shape which is easier to handle with CNN. A box with a lower-left corner in coordinate (6.90\u00b0E;44.35\u00b0N) and higher-right in (7.79\u00b0E;44.84\u00b0N) was adopted to clip the images. Concerning the temporal resolution, we set a weekly time step for the predictions, and then both the target and features were aggregated computing weekly averages.\nIn a time series task in which lagged features are employed, inserting gaps between the training, validation and test sets is common to prevent data leakage and performance overestimation. For example, in the case of an autoregressive"}, {"title": "3. Results", "content": "Many performance metrics have been computed following Equations 2 3 4 5 6 7 89. In these equations y, ymin, Ymax represent respectively the target mean, minimum and maximum computed over the training set. NSE and KGE have been frequently adopted in hydrological modeling studies. More in detail, NSE shows how a model performs with respect to a naive estimator, which is usually the mean (y). The NSE has an intrinsic benchmark set at 0, that is when the model performs as well as the naive estimator. The KGE [82] is a different concept, it is the the euclidean distance between the vector defined by metrics p (as in Equation 7),a and, \u03b2 and the vector with the best achievable metrics (p = 1, a = 1 and, \u03b2 = 1). In practice, the KGE is a measure which takes into account more aspect of the prediction, i.e. the Pearson correlation, the bias and the variance. The KGE has no intrinsic benchmark as the NSE and, as pointed out in [83], these two metrics are not directly comparable. In [83] authors stated that if the benchmark is set as the NSE. i.e. a fixed mean estimation, then the cut-off point of the KGE is -0.41, after which the model is performing better than the naive estimator. For both NSE and KGE the maximum value is 1 and the higher the values are the better a model is performing.\nTable 3 shows all the performance metrics computed over the test sets for the TDC-LSTM and TDC-UnPWaveNet models, and Figures 11a 11b 11c show the ensemble mean prediction. It is difficult to define a clear winner between the TDC-LSTM and TDC-UnPWaveNet, also because it seems that the two models have captured different aspects of the phenomenon to be modelled. In the case of Vottignasco and Racconigi sensors, the TDC-LSTM has performed better in terms of RMSE, BIAS, MAPE and NSE, however, the TDC-UnPWaveNet has been better for correlation and KGE. The TDC-UnPWaveNet has appeared to be more able to predict the actual temporal evolution of the ground truth, while the TDC-LSTM has been better in terms of the biasedness of predictions. This is very clear in Figure 11c in which the TDC-UnPWaveNet follow accurately the temporal evolution (\u03c1 = 0.95) of the ground truth but a bias is clearly visible. Another example could be the drop in the Vottignasco series (Figure 11a) around 2022-10-01. Here, the TDC-UnPWaveNet has predicted correctly a more prolonged drop and more in line with the actual values, instead the TDC-LSTM has predicted a very accurate decrease of the depth, however, it incorrectly has predicted the"}, {"title": "4. Discussion", "content": "Concerning the test period, it has to be stressed out that 2022 was a very particular year in terms of weather conditions. In fact, our ROI suffered from a severe drought in the summer which lasted until the autumn of 2022. Thus predicting the water table depth, especially in Vottignasco and Racconigi, has been a very difficult task for our models, which have to predict an uncommon drop. This has been even more difficult given the absence of an explicit autoregressive term, which would have helped the model in anchoring to the most recent actual water table depth values.\nAs stated in Section 1.2 the choice of not using autoregressive terms in our proposed models has been guided only by a practical fact, i.e. water table depth data are updated on a semester basis. Then, using an autoregressive term would have made our model unusable in a practical scenario to predict the next week's depth value. Furthermore, no anthropogenic pressure proxy (e.g. human water consumption) has been fed to our model, and this is because of the lack of such data in our ROI. Even if our models have yielded satisfactory results, they could be enhanced by the introduction of these additional inputs (as in [65, 16, 23, 25]), especially to improve the performance in anoma- lous scenarios like the summer and autumn of 2022. In fact, predicting such a drop could be very difficult looking only at the weather variable. For example, if the precipitations are scarce, likely, the anthropogenic pressure on the groundwater resource increases making groundwater resources decrease even more.\nIn [25] authors found the LSTM-based model more robust against initialization effects than CNN. We have found soft evidence of this. In fact, in our case study, this could be true for Vottignasco and Racconigi series, in which the ensemble standard deviation of the TDC-LSTM models (shadows in Figure 11a and 11c) seem to be lower than the TDC-UnPWaveNet ones. However, this is not true for the Savigliano test predictions, in which the TDC-UnPWaveNet ensemble standard deviation appears to be lower. Furthermore, in our case study, the more variability related to the initialization effect could be also caused by the higher number of parameters and the deeper architecture of the TDC- UnPWaveNet.\nIn terms of performance metrics, our models are in line with, and in some cases even better than, other hydrology"}, {"title": "5. Conclusion", "content": "We have proposed two different DL models for predicting, in a many-to-one fashion, the water table depth of three sensors located in the Grana-Maira catchment (Piemonte, IT) from weather image time series. These models are made of two modules: a first Time Distributed CNN (TDC) and a Sequential Module. The TDC is the same for the two proposed models, and it extracts a vectorial representation (Time Distributed Hidden Representation) of the input image time series, i.e. it encodes each image available at each time step into a vector forming a hidden multivariate time series. For the TD-LSTM model, the Sequential Module is based on a classical LSTM layer; instead for the TD-UnPWaveNet model the sequential model is based on a new version of the WaveNet adapted here to output a series completely in the future and shorter than the input one - actually a many-to-one scenario for this case study.\nIn developing the UnPWaveNet, and facing the issue of different sequence lengths inside the architecture, we have designed a new Channel Distributed (CD) layer. The CD layer applies the same transformations to each channel individually (i.e. a translation of the concept of Time Distributed layer to channels). In this way, a sequence with many channels could be transformed into a sequence of a different length maintaining the channel-wise dimension. The CD layer, implemented in the UnPWaveNet with a fully connected cell, has proved to be efficient and effective: it has enabled to achieve very satisfactory results limiting the total number of parameters.\nBoth the DL models have shown remarkable performance, revealing that, in our ROI, it is possible to predict the water table depth using only exogenous weather information with satisfactory results. The TD-LSTM has appeared to be better in terms of bias, but the TD-UnPWaveNet has outperformed the previous in terms of correlation and KGE, appearing to be better in modeling the temporal dynamics of the target. This means that the UnPWaveNet model could be considered as a new possible competitor for recurrent models. Future works are required to investigate better the performance of the UnPWaveNet in other case studies and against other types of DL architecture, e.g. Transformers [84], here not included because of the already consistent work done in developing and adapting the proposed models"}]}