{"title": "Research on Predicting Public Opinion Event Heat Levels Based on Large Language Models", "authors": ["Yi Ren", "Tianyi Zhang", "Weibin Li", "DuoMu Zhou", "Chenhao Qin", "FangCheng Dong"], "abstract": "In recent years, with the rapid development of large language models, serval models such as GPT-40 have demonstrated extraordinary capabilities, surpassing human performance in various language tasks. As a result, many researchers have begun exploring their potential applications in the field of public opinion analysis. This study proposes a novel large-language-models-based method for public opinion event heat level prediction. First, we preprocessed and classified 62,836 Chinese hot event data collected between July 2022 and December 2023. Then, based on each event's online dissemination heat index, we used the MiniBatchKMeans algorithm to automatically cluster the events and categorize them into four heat levels (ranging from low heat to very high heat). Next, we randomly selected 250 events from each heat level, totalling 1,000 events, to build the evaluation dataset. During the evaluation process, we employed various large language models to assess their accuracy in predicting event heat levels in two scenarios: without reference cases and with similar case references. The results showed that GPT-40 and DeepseekV2 performed the best in the latter case, achieving prediction accuracies of 41.4% and 41.5%, respectively. Although the overall prediction accuracy remains relatively low, it is worth noting that for low-heat (Level 1) events, the prediction accuracies of these two models reached 73.6% and 70.4%, respectively. Additionally, the prediction accuracy showed a downward trend from Level 1 to Level 4, which correlates with the uneven distribution of data across the heat levels in the actual dataset. This suggests that with the more robust dataset, public opinion event heat level prediction based on large language models will have significant research potential for the future.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the emergence of large language models (LLMs), such as OpenAI's ChatGPT[1], has brought profound changes to the field of natural language processing (NLP). With their powerful few-shot[2] and zero-shot learning capabilities, these models exhibit remarkable generalization performance, enabling them to handle various complex language tasks and generate coherent, logically consistent responses. As a result, LLMs have demonstrated broad potential applications across multiple fields.\nAlthough ChatGPT remains closed-source, the development of the open-source community has provided researchers with more alternatives. With the introduction of a series of high-performance open-source models like LLaMA[3], Qwen[4], and ChatGLM[5], researchers now have more opportunities to explore the application of LLMs in different domains, achieving notable results. For example, in the medical field[6], Wang et al. proposed the BenTsao[7] model, which integrates medical knowledge graphs and literature and utilizes a Chinese medical instruction tuning dataset generated via the ChatGPT API to fine-tune models like LLaMA. In the legal field, Zhou et al. developed LawGPT[8], which underwent secondary pretraining and instruction tuning on a large-scale Chinese legal corpus, endowing it with robust legal question-answering capabilities. In the field of remote sensing[9, 10], Kuckreja et al. proposed GeoChat, the first multimodal large model[11] capable of understanding various types of remote sensing images. In the hydrology field, Ren et al. introduced WaterGPT[12], which, based on Qwen-7B-Chat and Qwen2-7B-Chat, underwent large-scale secondary pretraining and instruction tuning on domain-specific data, enabling professional knowledge Q&A and intelligent tool usage. In the field of sentiment analysis, Pe\u00f1a et al. experimentally evaluated the performance of four Spanish LLMs in classifying public affairs documents, demonstrating that LLMs can effectively handle and understand complex language documents, classifying them into up to 30 topics, thus providing technical support for promoting transparency, accountability, and civic participation[13].\nDespite extensive research exploring the application of LLMs in specialized domains, studies focused on predicting the influence of trending events remain limited. We propose a public opinion event heat level prediction method based on LLMs to address this. In this study, we first preprocessed and classified 62,836 data points covering trending events in China"}, {"title": "II. PROPOSED METHODS", "content": "In this study, we propose a method for predicting the heat level of public opinion events based on large models. This method consists of three main modules: data processing, public opinion event heat level classification, and model prediction. The detailed process is illustrated in Figure 1.\nFirst, in the data processing module, we organize and refine the content of each public opinion event and categorize them based on their attributes. Then, in the public opinion event heat level classification module, we use a 62,836 public opinion events dataset and apply the MiniBatchKMeans algorithm to cluster them based on their online dissemination heat index automatically. Through this process, we classify public opinion events into four heat levels: low, medium, high, and very high.\nFinally, in the model prediction module, we use the categorized dataset from the data processing module to train the bge-large-zh-1.5[16] embedding model. When utilizing the evaluation dataset, the model recalls ten similar public opinion events based on the content of the given event using the bge- large-zh-1.5 model. Subsequently, we fill the content of these similar events into a predefined template and input it into a large language model. The model then generates the final predicted heat level of the public opinion event, which is recorded for further analysis."}, {"title": "B. Data processing module", "content": "Our original dataset contains 62,836 records covering hot events in China from July 2022 to December 2023. In the data processing module, we first crawled detailed information for each public opinion event based on its title from the internet. After filtering out events with empty content or garbled text, we retrieved detailed information for 40,081 public opinion events.\nNext, we used the DeepSeekV2 API to extract summaries from the detailed content of each event, condensing it into a concise description to serve as the content representation of the event. For events where detailed information could not be retrieved, we used the event's title as its content description.\nWe manually categorized each event into one of 20 categories, including transportation, sports, agriculture, healthcare, and others. We then constructed a dataset for training the embedding model. To balance the data distribution, the number of events in each category was capped at 3,000, with excess entries being discarded. The entries in the dataset were used to create positive and negative samples based on the main content of the events: entries in the same category were treated as positive samples, while entries in different categories were treated as negative samples. Each training sample consisted of the event's content, content from another event in the same category (positive sample), and content from an event in a different category (negative sample). Ultimately, the training dataset contained 33,864 records, with the specific distribution and proportion of categories shown in Figure 2."}, {"title": "C. Public opinion event heat rating module", "content": "In this module, we applied the MiniBatchKMeans algorithm for automated clustering based on the online propagation heat index of public opinion events. The events were categorized into four heat levels: low,, medium, high, and very high(ranging from level one to level four). We randomly selected 250 events from the public opinion event pool from each heat level, resulting in a total of 1,000 events to construct an evaluation dataset for large language models.\nThe clustering process is as follows:\n(1) Calculation of the Sum of Squared Errors (SSE):\n$SSE = \\sum_{j=1}^{P} \\sum_{V \\epsilon C_j} ||v - \\mu_j||^2$\nHere, P represents the number of clusters, $C_j$ denotes the jth cluster, and uj is the centroid of the jth cluster. Vi represents the vector belonging to the jth cluster. The SSE measures the sum of the distances between data points and their respective cluster centroids, serving as one of the indicators to evaluate clustering performance. A smaller SSE indicates that the points within a cluster are more tightly grouped. By plotting the SSE values for different numbers of clusters P, one can preliminarily assess the reasonable range for the number of clusters.\n(2) Calculation of the Silhouette Coefficient:\n$s(i) = \\frac{b(i)-a(i)}{max(a(i),b(i))}$\nHere, a(i) represents the average distance from data point i to all other points within the same cluster, and b(i) represents the average distance from data point i to the nearest points in a different cluster. The silhouette coefficient S for the entire dataset is the average of the silhouette scores s(i) for all data points:\n$S = \\frac{1}{N} \\sum_{i=1}^N s(i)$"}, {"title": "D. Model prediction module", "content": "In this module, we will use the training dataset generated in the previous data processing stage to train the bge-large-zh-1.5 model. After training is completed, the model will retrieve similar events for each public opinion event in the evaluation dataset. Specifically, the model will recall 10 similar public opinion events for each input event and output their content information, online heat propagation index, and heat level.\nBased on the retrieval results, we will construct a template as shown in Appendix, and the content of this template will be input into the model.\nWe employed six of the most advanced large language models currently available on the market, including API-based and locally deployed models. Details are provided in Table 2."}, {"title": "III. EXPERIMENTS AND ANALYSIS", "content": "We evaluated the ability of different large language models to predict the heat levels of public opinion events, primarily based on two approaches: direct prediction without any case references and prediction after referencing ten similar event cases. Our specific template is shown in the Appendix. We employed a multiple-choice format, allowing the language model to choose one of the four heat levels. This method helps standardize the model's output.\nOur evaluation results are divided into overall prediction accuracy and the level-specific prediction accuracy for 250 events in each heat level category."}, {"title": "B. Embedding Model Training Setup", "content": "To achieve similar event recall, we trained the bge-large- zh-1.5 model using the training dataset introduced in Section 2.1. The model was trained for one epoch, and the trained model was mixed with the original model in a 1:1 ratio. This approach balances specialized and general capabilities. The specific training parameters are shown in Table 3."}, {"title": "C. Large language model prediction results", "content": "To comprehensively evaluate the performance of large language models in predicting the heat levels of public opinion events, we carefully designed two scenarios: with and without case references. In the \"without case references\" scenario, the model predicts the heat level based on the event content. The \"with case references\" scenario has two setups: one with actual similar events and one with simulated cases. In the actual case setup, we used the trained embedding model to recall ten similar events for each event in the evaluation dataset, which were then integrated into a designed prompt and fed into the model for prediction. In the simulated case setup, due to the uneven distribution of the real dataset, we randomly selected three events from each of the heat levels above the medium level (for a total of nine events), incorporated them into a designed prompt, and fed it into the model for prediction. This approach simulates the model's prediction results when the dataset is more balanced. The experimental results are shown in Table 5 and Figure 3."}, {"title": "IV. CONCLUSION", "content": "This study evaluated the performance of various large language models in predicting the heat levels of public opinion events, focusing on analyzing changes in prediction accuracy with and without reference to similar cases. The experimental results show that the direct prediction performance of large language models without reference cases was relatively poor, with the best model, GPT-4o, achieving only 28.10% accuracy. Nevertheless, certain models performed better in specific heat level categories. For instance, GLM4 reached 70.0% accuracy for high-heat events, and Qwen2-7B-instruct achieved 65.6% accuracy for very high heat events.\nWhen similar case references were available, the overall prediction performance of the models improved, with both GPT-40 and DeepSeek-V2 achieving 30.30% accuracy. In the simulated case scenario, especially, the prediction accuracy of GPT-40 and DeepSeek-V2 reached 41.40% and 41.50%, respectively. However, it is important to note that the prediction accuracy of the models decreased as the heat level increased, particularly for events at medium heat levels and above. This decline is closely related to the uneven distribution of the dataset, where the lack of sufficient samples at the higher heat levels resulted in poorer prediction performance. For low- heat events, GPT-40 and DeepSeek-V2 performed exceptionally well, achieving prediction accuracies of 73.6% and 70.4%, respectively. In contrast, for very high-heat events, the models generally performed poorly, which may be due to the insufficient quality of similar cases and the models' tendency towards conservative predictions.\nOverall, although large language models still face challenges in predicting the heat levels of public opinion events, such as uneven data distribution and difficulties in matching similar cases, their strong performance in predicting low-heat events and the overall improvement in prediction accuracy suggest that public opinion analysis based on large language models has significant research potential. Future research could improve prediction accuracy for events at different heat levels by optimizing dataset distribution and enhancing the mechanism for matching similar cases."}, {"title": "APPENDIX", "content": "No case prompt:\nThe event content is {event}### Please predict the popularity level that the event will reach based on the above event content. Please output the options. Please select only the most relevant level. \n{options}\nThere are similar cases prompt:\nThe event content is {event}### Please predict the popularity level that the event will reach based on the above event content. Please output the options. Please select only the most relevant level. \n{options} Refer to similar event information as {Case}.\nOptions=\"\"\"Option: A, heat level 1, heat index range is (0.000000,8.777964)\nOption: B, heat level 2, heat index range is (8.777964,21.462457)\nOption: C, heat level 3, heat index range is (21.462457,42.399911)\nOption: D, heat level 4, heat index range is (42.399911, Inf)\"\"\""}]}