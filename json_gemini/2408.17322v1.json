{"title": "Investigating Neuron Ablation in Attention Heads: The Case for Peak Activation Centering", "authors": ["Nicholas Pochinkov", "Ben Pasero", "Skylar Shibayama"], "abstract": "The use of transformer-based models is growing rapidly throughout society. With this growth, it is\nimportant to understand how they work, and in particular, how the attention mechanisms represent\nconcepts. Though there are many interpretability methods, many look at models through their neuronal\nactivations, which are poorly understood. We describe different lenses though which to view neuron\nactivations, and investigate the effectiveness in language models and vision transformers though various\nmethods of neural ablation: zero ablation, mean ablation, activation resampling, and a novel approach\nwe term 'peak ablation'. Through experimental analysis, we find that in different regimes and models,\neach method can offer the lowest degradation of model performance compared to other methods, with\nresampling usually causing the most significant performance deterioration. We make our code available\nat https://github.com/nickypro/investigating-ablation", "sections": [{"title": "1. Introduction", "content": "Understanding how language models make decisions is important to ensure that their use\ncan be trusted. Mechanistic interpretability offers one lens through which to understand how\ntransformer architecture models [1] perform the computations required to get an output. An\noft-used tool in mechanistic interpretability is to attribute individual network parts to specific\ncapabilities by ablating those parts and observing capability degradation.\nHowever, choosing how to ablate neurons in language models is still an unsolved problem.\nThe traditional closed-form methods are zero ablation and mean ablation [2, 3], as well as an\nadditional, more randomised method of activation resampling in the case of causal scrubbing\n[3, 4], but little empirical analysis has been done to optimise these methods [4].\nUnderstanding exactly how neuron activations deviate, and what baseline they deviate from, is\na broadly applicable question that is underexplored, and has the potential to improve techniques\nfor model pruning and analysis into model sparsity.\nIn this paper, we 1) describe a simplistic working model of neuron activations, 2) suggest an\nimproved, closed-form method of neuron ablation using modal activation, called 'peak ablation',\nand 3) run experimental analysis on various ablation methods to compare the degree to which\nthey harm model performance."}, {"title": "2. Related Work", "content": "Mechanistic interpretability is a field of research focusing on understanding how neural\nnetwork models achieve their outputs. [5, 3, 6, 7, 8, 9]. A common method used in mechanistic\ninterpretability, is 'ablate and measure' [3]. We investigate more precisely how different ablation\nmethods affect performance, and propose 'peak ablation' as another possible method.\nMost relevantly, recent research [4] investigates hyperparameter selection to optimise activa-\ntion patching for causal scrubbing. Our research differs; instead of interpolating activations\nbetween similar inputs, we set neurons' values for all inputs, and do not limit only to resampling.\nPruning: Model pruning [10] is a common practice wherein reduced neural network pa-\nrameter counts lessen memory and performance costs. In particular, structured pruning of\nlarge features [11] is interested in the removal on the scale of neurons and attention heads, and\ncan often achieve a large reduction in parameter count [12]. Our work seeks to question the\nassumption of using masks that set neuron values to zero.\nModularity: Research into activation sparsity [13], modularity [14], mixture of experts\n[15, 14, 16], and unlearning by pruning [17, 18] all investigate how different subsets of activations\nare responsible for different tasks. These implicitly set activations to zero."}, {"title": "3. Method", "content": "3.1. Pre-Trained Models and Datasets\nWe work with two causal text models, Mistral 7B [19] and Meta's OPT 1.3B [20], a masked text\nmodel, ROBERTa Large [21], and a vision transformer, ViT Base Patch16 224 [22].\nTo get a general sense of performance, the above models were evaluated by looking at top1\nprediction accuracy\u00b9, as well as cross-entropy loss on various datasets. For text models, we\nassess on EleutherAI's 'The Pile' [23]. For image models, we assess on Imagenet-1k [24], an\nimage dataset with 1000 different classes. We evaluate on deterministic subsets of 100,000 text\ntokens and 1000 images respectively\n3.2. Neurons\nThe objects of study are attention pre-out neurons, sometimes called \u2018z\u2019-hook activations. We\ndefine attention pre-out neuron activations $y_i = f(x_i) = preout(x_i)$ as $Y_i = \\sum_j A_{i,j}W_vx_j$,\nwhere $A_{i,j} = softmax((W_Kx_i) \\cdot (W_qx_j))$, where $W_Q$, $W_K$, $W_v$ are the attention query, key,\nand value matrices respectively. We focus on attention neurons rather than MLP neurons, as\nthese do not have an activation function that privileges positive activations, making analysis\nmore difficult. To ablate a neuron, we replace $y_i = f(x_i)$ with some constant.\nIn Figure 1, we showcase some plots of neuron probability distributions. We see an example\nof many attention pre-out neuron activation distributions within the same layer. We note that\nmost neurons follow a roughly Gaussian or double-exponential distribution about zero, but\nnote that there is a minority of neurons that are not distributed at zero. As most neurons are\nzero-centred and symmetric, it makes sense that zero and mean ablation work quite well."}, {"title": "3.3. A Working Model of Neuron Activation", "content": "Our hypothesis, based on activation profiles such as those seen in Figure 1 is that neurons have\na 'baseline' or 'default-mode' activation (typically at zero), when the input contains no relevant\nfeatures, which is then deviated from as neurons fire in proportion to various features they\nare tuned to pick up. In residual stream models [25], information is limited to the width of the\nresidual stream [26], and as the residual stream typically grows exponentially in size [27], noise\ncan become amplified. This is supported by the common redundancy of many circuits [28],\neven in transformer models trained without the use of dropout [29].\nIn particular, we expect that ablating neurons should have two contributors to reduced\nperformance. These are 1) removing the relevant contextual computed information that the\nneuron is providing, and 2) taking the model activation out of distribution, by adding 'noise'.\nAblating neurons to a constant value should cause some constant increase in loss for term 1, and\ndifferent constant should contribute to different values of term 2. As we increase the distance\nfrom the 'default-mode' value, the neuron would further degrade the performance by taking\nthe residual stream further out of distribution, thus in some sense, 'adding noise'."}, {"title": "3.4. Ablation Methods", "content": "We choose four main methods of ablating neurons, see Table 1 for a summary. These are:\nZero ablation: The most common form of ablation, which involves replacing a neuronal\nactivation of any with a zeroed out activation. That is, setting $\u2200x_j : f_i(x_j) = 0.0$"}, {"title": "Mean Ablation", "content": "A still relatively-common method of ablation, which involves first collecting\nactivations of various neurons on a distribution of inputs, and averaging the activations to find\na mean activation. That is, for some dataset D, for $x_j \u2208 D$, let $f_i(x_j) = \\sum_j f_i(x_j)$"}, {"title": "Activation Resampling", "content": "Inspired by [3, 4], we also try general neuron resampling, by\nsetting activations to those found by giving a randomised input. 2 For text model, we take\nactivations by a) sampling random generated characters, b) sampling random tokens, and c)\nusing OPT to generate a random text. For ViT, we use randomly generated pixel values."}, {"title": "Naive Peak Ablation", "content": "Observing that neuronal activations frequently exhibit a prominent\npeak, we propose an ablation method targeting their modal activation. For bin size e, the neuron\ni activations $f_i(x_j)$ for each $x_j \u2208 D$ are sorted into bins $N_i[k]$ such that $y_k \u2264 f_i(x_j) < Y_k + \u03f5$.\nThe bin $N_i[k_{max}]$ with the highest occurrence is selected, and $f_i(x_j)$ is set to $Y_{k_{max}} + \\frac{\u03f5}{2}$"}, {"title": "3.5. Ablation Experiments", "content": "Under the working model described in Section 3.3, we expect that ablating neurons to different\nvalues should have different impacts to performance, with there being a value which leads to\nsome minimal drop in performance due to minimal noise being added to the residual stream.\nWe randomly select attention neurons in increments of 10% and ablate them until the model\nis fully pruned, and at each step, assess performance by evaluating the Top1 accuracy and\nCross-Entropy Loss in the chosen dataset with each ablation method, described in Table 1. The\nneurons are selected deterministically across three separate seeds, summarised in Table 2"}, {"title": "4. Results", "content": "4.1. Causal Text Models\nIn Figure 2, we see the results for random pruning of OPT 1.3b and Mistral 7b with the different\nmethods of ablation. We can see that Peak ablation has the most consistent pattern, causing the\nlowest amount of degradation, with mean ablation and zero ablation coming a close second\nand third, and Random resampling causes by far the most degradation. Of the three resampling\nmethods, choosing random tokens causes the lowest degradation.\n4.2. Other Transformers\nIn Figure 3, we see that for ViT, zero, mean, and peak ablation have statistically insignificant\ndifferences in performance, while resampling causes some small additional degradation. We\ncan see that almost all of the performance loss is based on the specific neurons being selected\nrather than the ablation method being chosen, even between zero, mean, and peak ablation.\nIn RoBERTa, we see that in the first 75% of pruning, the three main methods of peak, mean and\nzero ablation are very close, with Peak edging slightly better performance. Beyond 75%, the three\nmethods become more noisy; resampling of IDs ends up having the best performance overall in\nboth Top1 and Cross-Entropy Loss at the task of token unmasking and de-randomization."}, {"title": "4.3. Overall Comparison", "content": "In Table 2, we see that in different models and in different regimes, the different methods have\ndifferent merits in reducing performance, with Peak ablation working overall best in the most\ncases. Surprisingly, although Random resampling seems to add a lot of noise to the activations,\nrandom token ID resampling can sometimes work well, such as in RoBERTa."}, {"title": "5. Discussion", "content": "The analysis presented seems to suggest that when evaluating and understanding neurons in\nthe attention layers of language models, the ideal centring method seems to depend significantly\non the model. In decoder models, a good method is to find the largest peak, with a close second\nbeing zero ablation. This similarity is expected, as most neurons are centred at zero. This has\ndownstream effects on improving the way we can look at one of the most crucial aspects of\nhow neural networks work - their activations.\nWe have seen that neurons can have activations that are: non-Gaussian, non-symmetric,\nmulti-modal, non-zero-centred. We hypothesise that taking into consideration this fact has the\npotential to make interpretability analysis into more fruitful, and centring activations by their\npeak seems a potential natural method.\nFuture work could: 1) investigate other potential better methods for neuron recentring, 2)"}, {"title": "7. Appendix", "content": "7.1. Resampling Methods\nAs we are not performing resampling based on a single possible input, but rather comparing\nmany methods, we seek a few possible inputs to gain an in-distribution sample of what model\nactivations might look like. We consider three methods. The first is to generate a string of\nrandom alphanumeric characters. The second is to generate a random selection of possible input\ntokens from the model. The last is to attempt to generate a random in-distribution text based\non a single token. We see some extracts in Table 3. The three methods are plotted together, as\ntheir performances were relatively similar."}]}