{"title": "Multimodal Medical Disease Classification with LLaMA II", "authors": ["Christian Gapp", "Elias Tappeiner", "Martin Welk", "Rainer Schubert"], "abstract": "Medical patient data is always multimodal. Images, text, age, gender, histopathological data are only few examples for different modalities in this context. Processing and integrating this multimodal data with deep learning based methods is of utmost interest due to its huge potential for medical procedure such as diagnosis and patient treatment planning. In this work we retrain a multimodal transformer-based model for disease classification. To this end we use the text-image pair dataset from OpenI consisting of 2D chest X-rays associated with clinical reports. Our focus is on fusion methods for merging text and vision information extracted from medical datasets. Different architecture structures with a LLaMA II backbone model are tested. Early fusion of modality specific features creates better results with the best model reaching \u2248 97.10% mean AUC than late fusion from a deeper level of the architecture (best model: \u2248 96.67% mean AUC). Both outperform former classification models tested on the same multimodal dataset. The newly introduced multimodal architecture can be applied to other multimodal datasets with little effort and can be easily adapted for further research, especially, but not limited to, the field of medical AI.", "sections": [{"title": "1 Introduction", "content": "Deep learning from multimodal data, i.e. two or more modalities such as images, text, age, gender, histopathological data, becomes more and more interesting in medical diagnosis and can have a positive impact on patient treatment planning [1]. We train a transformer-based model for disease classification, using a text-image pair dataset from OpenI [2], [3] consisting of 2D chest X-rays combined with clinical reports.\n\nThe multimodal dataset was formerly processed in [4] for automated image annotation. With TransCheX [5] \u2013 published in MONAI (Medical Open Network AI) \u2013 a transformer-based network for disease classification was trained on the same data [2]. For TransCheX BERT (Bidirectional Encoder Representations from Transformers) [6] is used as backbone language model. The Transformer architecture was first introduced in [7] for text processing. With Vision Transformer [8], networks like CNNs for image processing (segmentation, classification, etc.) were partly outperformed."}, {"title": "2 Multimodal Medical Chest X-Ray Dataset", "content": "The multimodal dataset used in our work is released by OpenI [2], [3]. The dataset consists of 2D chest X-rays (256\u00d7256), a clinical report and target classes (diseases, findings) for each patient. Training (3199 image-text pairs), validation (101) and test datasplits (377) are used equivalently as in [5]. The distributions in their classification labels 0 to 13 (= 14 classes) are depicted in Fig. 1. Twelve labels depict diseases (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12), one is for Support-Devices (13) and one for No Finding (8).\n\nAs one patient can have several diseases, multiple classes are possible, except there was No Finding. Fig. 2 shows an example 2D chest X-ray with the associated clinical report."}, {"title": "3 Model \u2013 Architecture", "content": "When processing multimodal data the fusion of two or more modalities is a central part. Different fusion methods involve different architectures. Our main focus is on the cross layer, i.e. the layer that merges the two modalities. In order to demonstrate the different fusion strategies and their architectures, we first outline the details of the feature extraction from text and vision separately."}, {"title": "3.1 Feature Extraction", "content": "The multimodal data in its original form can not be analyzed from a deep learning model. Therefore, at first both clinical report (text) and 2D chest X-rays (vision) must be preprocessed before being"}, {"title": "3.2 Fusion Strategies", "content": "We want to introduce different fusion strategies for merging the two modalities, text and vision. All of our strategies have cross layers for multimodal data fusion in common. Regarding the location of these cross layers, early, late or mixed fusion is possible."}, {"title": "3.2.1 Cross Layer: Multimodal Fusion", "content": "Text and vision layers process one modality each. With cross layers, the modalities are fused together with the purpose of exchanging information from the data. Text, and vision and cross layers consist of attention blocks that compute\n\nAttention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V,\n\nwith matrix Q containing the queries, K containing the keys and V the values. \\sqrt{d_k} is a scaling factor, with $d_k$ representing the dimension of keys. Multi-head attention is realized with an additional linear layer. Multiple attention-heads produced with Eq. (1) are concatenated and projected using another linear layer $W_o$. With\n\nmulti\\_head\\_attention(Q, K, V) = Concat(head_1, ...,head_h)W_o,\n\nwhere $head_i$ = Attention($Q_i$, $K_i$, $V_i$) with $i = 1, ..., h$ (number of heads), the model is capable of attending to different information in parallel [7].\n\nText, vision and cross layers further contain a feed forward module\n\nfeed\\_forward(x) = W_2(silu(W_1(x)W_3(x))),\n\nwith $W_1$, $W_2$, $W_3$ representing linear layers and $silu(x) = x \\sigma(x)$, being the Sigmoid Linear Unit function with the logistic sigmoid $\\sigma(x)$. Taking the attention block's output\n\ny = x + multi\\_head\\_attention(L^2norm(x)),\n\nwith input x, the final text, vision layers' output is computed by\n\nout = y + feed\\_forward(L^2norm(y)).\n\nThe cross layer gets a query from one modality and a key-value pair from the other modality. Hence we need at least two cross layers (1: query (vision), key-value (text), 2: query (text), key-value (vision)) per level. This can be interpreted as asking for text (key, value) from vision (query) (1) and vice versa (2)."}, {"title": "3.2.2 Parallel Pipeline \u2013 Early Fusion", "content": "The parallel architecture is depicted in Fig. 3. The cross layers are parallel to the text and vision layers at each level. Herein the untouched text and vision features are fed to the cross layers' input. As the fusion of the modalities is already done at the first level, parallel architectures generate an early fusion of multimodal information."}, {"title": "3.2.3 Serial Pipeline \u2013 Late Fusion", "content": "With the serial architecture, see Fig. 4, the cross layer is put after the two modality specific layers (text, vision layer). Thereby the processed text and vision features are fed to the cross layers instead of using the untouched ones as done in the parallel workflow. Late Fusion combines the multimodal information after the last level of single modality specific layers.\n\nThe pipeline after the cross layers is equal to the parallel one as described above."}, {"title": "3.2.4 Mixed Pipeline \u2013 Mixed Fusion", "content": "As a last scenario a mixed version of the parallel and serial pipeline is trained. Starting from the parallel architecture a cross layer is added immediately before the Dense Linear layers. In order to let the size of parameters unchanged one cross layer was removed from the former parallel architecture. In other words, one parallel cross layer is replaced by a serial one."}, {"title": "4 Training Methodologies", "content": "The enormous size of large language models generally leads to difficulties during training, as they require a lot of GPU memory. Therefore, we introduce a parameter efficient fine tuning method that reduces GPU memory requirement and thus makes it possible to train architectures with a huge number of model parameters."}, {"title": "4.1 Low Rank Adaptation (LoRA)", "content": "LORA [11] enables to train a small amount of model parameters by freezing the pretrained model weights and injecting trainable rank decomposition matrices. This reduces both gpu memory requirement and computation time whilst maintaining the same or resulting in an even better performance. Let $W \\in \\mathbb{R}^{d \\times k}$ contain the weights for one matrix in the layer L, then $A \\in \\mathbb{R}^{r \\times k}$ and $B \\in \\mathbb{R}^{d \\times r}$ represent the two additional LoRA matrices for this matrix W in L. Instead of training all $d \\times k$ parameters, now only the $r \\times k$ parameters from A and the $d \\times r$ parameters from B are updated during training routine and added to the frozen matrix W. The weights in B are initialized with zeros, whereas Random Gaussian initialization is applied for A. For input x, formerly the corresponding output was h = Wx. Now we have\n\nh = W_ox + \\Delta Wx = W_ox + BAx,\n\nwhere $W_o$ is the frozen weight matrix at initial state. The hyperparameter r < d, k directly impacts the parameter reduction. Hereby memory requirement is enormously reduced by factor $r(1/d+1/k)$ for each lora-layer. Additional LoRA parameters are the lora_dropoutrate $\\in \\mathbb{R} \\cap [0, 1]$ and scaling factor $\\alpha \\in \\mathbb{N}$. We set them constant as mentioned in the Ablation Studies afterwards.\n\nIn our implementation we use a model wrapper from Parameter Efficient Fine Tuning (PEFT) [13] in order to handle the LoRA layers."}, {"title": "4.2 Ablation Studies", "content": "All architectures are of the same size regarding number of parameters. They contain three text, three vision and three cross layers. Note that for the mixed architecture we positioned two cross layers parallel to the text and vision layers, and one cross layer serial to all of them. The feature extraction (input generation) and the cross layers' output processing \u2013 Dense Linear \u2192 Tanh() \u2192 Dropout \u2192 Dense CLS \u2192 Sigmoid \u2013 to finally compute the output probabilities remains unchanged in all scenarios.\n\nBy varying the position of the cross layers seven models (three for serial, three for parallel, one for mixed pipeline) are trained, tested on the test datasplit and compared using the AUC (area under the ROC (receiver operating characteristic) curve) metric. The LoRA parameter r is modified for the serial and parallel pipelines with $r\\in \\{2, 4, 8\\}$, resulting in three models each. Additionally a mixed version with r = 2 is trained. The other LoRA parameters are set as follows: lora_dropoutrate = 0.1, $\\alpha$ = 32."}, {"title": "4.3 Training Details", "content": "We implemented our models in python with the use of Pytorch [14]. The models were trained on a Nvidia Titan RTX 24 GB GPU with 15 epochs and batch_size = 20. For optimization the Adam Optimizer with learning_rate = 10-4 and weight_decay = 10-5 was used. We applied binary cross entropy as loss function for our multi-class classification problem that outputs probabilities \u2208 [0,1] for each class."}, {"title": "5 Results", "content": "TransCheX's [5] model's evaluation statistics on the test dataset from OpenI [2], [3] resulted in 96.29% mean AUC. Our results for parallel, serial and mixed architectures will be presented below.\n\nParallel Pipeline The model with parallel pipeline and LoRA parameter r = 2 has the best performance on the test datasplit with 97.10% mean AUC and hence outperforms the TransCheX's model by 0.81%, see Table 3. In Fig. 5 training and validation details are depicted.\n\nSerial Pipeline The model with LoRA parameter r = 4 performs best with 96.67% mean AUC on the test datasplit and thus slightly better than the TransCheX's model by 0.38%, see also Table 3. Training and validation details can be viewed in Fig. 6.\n\nMixed Pipeline With a mean AUC = 96.00%, the mixed pipeline performs marginally worse than the other architectures. The class specific AUC can be viewed in Table 3 (second last column)."}, {"title": "6 Discussion", "content": "In our tests, five out of seven architectures \u2013 all three parallel (LoRA: $r \\in \\{2, 4, 8\\}$) and two serial ones (LoRA: $r \\in \\{2, 4\\}$) \u2013 outperformed former models tested on the dataset from OpenI with regard to the mean AUC. Solely the serial with LoRA: r = 8 and the mixed pipeline performed worse. All in all the parallel architecture with LoRA: r = 2 leads to the best result with mean AUC = 97.10%, outperforming the current state of the art. Regarding the best parallel (LoRA: r = 2) and the best serial pipeline (LoRA: r = 4) the early fusion (parallel architecture) of the two modalities text (clinical reports) and vision (2D chest X-rays) seems to perform slightly better than the late fusion (serial architecture). In the case of the multimodal dataset processed here, this means, the early fusion of the raw, untouched features lead to better classification performance than processing the modality specific outputs from a deeper level as done with late fusion.\n\nThe replacement of BERT with the larger language model LLaMA II 7B, fine tuning with LoRA, and the optimization of the position to merge multimodal information build the central part to gain such great performance for disease classification on the text-image pair dataset from OpenI. This can have remarkable impact on the future patient treatment planning in medicine [1]."}, {"title": "7 Summary and Outlook", "content": "In this paper we trained a multimodal transformer-based model with a LLaMA II backbone for medical disease classification. The underlying multimodal dataset from OpenI consists of 2D chest X-rays with corresponding clinical reports. We focused on different fusion strategies for merging the text with vision data. Herein the position of the cross layer, i.e. the layer that fuses the multimodal information, was central. Utilizing LoRA (Low Rank Adaptation) we trained seven models and tested them on the test datasplit from OpenI. Five out of seven models outperformed recent works (TransCheX [5]) tested on the same dataset. Early fusion realized with the parallel architecture performed slightly better than late or mixed fusion. Our models can be applied to other multimodal datasets with little effort. Furthermore, our models build the foundation to study interpretability methods on multimodal data, which is a special interest in the medical domain."}]}