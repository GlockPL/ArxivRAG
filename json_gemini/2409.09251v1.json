{"title": "ETAGE: ENHANCED TEST TIME ADAPTATION WITH INTEGRATED ENTROPY AND\nGRADIENT NORMS FOR ROBUST MODEL PERFORMANCE", "authors": ["Afshar Shamsi", "Rejisa Becirovic", "Ahmadreza Argha", "Ehsan Abbasnejad", "Hamid Alinejad-Rokny", "Arash Mohammadi"], "abstract": "Test time adaptation (TTA) equips deep learning models to handle unseen test data that deviates from the training distribution, even when source data is inaccessible. While traditional TTA methods often rely on entropy as a confidence metric, its effectiveness can be limited, particularly in biased scenarios. Extending existing approaches like the Pseudo Label Probability Difference (PLPD), we introduce ETAGE, a refined TTA method that integrates entropy minimization with gradient norms and PLPD, to enhance sample selection and adaptation. Our method prioritizes samples that are less likely to cause instability by combining high entropy with high gradient norms out of adaptation, thus avoiding the overfitting to noise often observed in previous methods. Extensive experiments on CIFAR-10-C and CIFAR-100-C datasets demonstrate that our approach outperforms existing TTA techniques, particularly in challenging and biased scenarios, leading to more robust and consistent model performance across diverse test scenarios. The codebase for ETAGE is available on https://github.com/afsharshamsi/ETAGE.", "sections": [{"title": "1. INTRODUCTION", "content": "Deep learning models [1-3] have demonstrated significant success across various tasks, particularly when both training and testing data share the same distribution. In real-world applications, however, such models often face data that deviates from training distribution [4, 5], a challenge known as domain shift or dataset shift. This discrepancy between the training (source) data and the testing (target) data can severely undermine model performance [6], thereby limiting their practical efficiency. Tackling this issue requires innovative strategies that enable models to adapt to new data distributions without requiring additional training data or supervision.\nLiterature Review: Unsupervised Domain Adaptation (UDA) [7, 8] has been one approach to address the aforementioned issue. Generally speaking, in the UDA approach, the knowledge from labeled source data is transferred to unlabeled target data using both datasets during the training. By analyzing the distribution of the target set, UDA allows the model to learn domain-invariant features [9] that generalize well across different distributions. While UDA has shown promise in various scenarios, it still relies on the availability of source data during the adaptation process. Test Time Adaptation (TTA) [10] bridges this gap by enabling models to adapt solely at test time, without access to the source data. This makes TTA particularly appealing in situations where retraining is impractical due to privacy concerns, computational constraints, or the urgent need for adaptation.\nTest-Time Entropy Minimization (TENT) [11] is among the first strategies proposed for TTA, which focused on reducing the entropy of model predictions during test time. TENT enhances model robustness to corrupted datasets and domain adaptation scenarios by directly minimizing prediction entropy, i.e., updating model parameters through entropy minimization without altering the training process. Entropy-Aware Test-time Adaptation (EATA) [12] is another noteworthy TTA approach that extended TENT by stabilizing model weights to address the \"forgetting\" phenomenon, i.e., degradation of adapted models' performance on in-distribution test samples. EATA starts with an initial pass over the target data to estimate the distribution, applying Fisher-based [13] weighting to identify crucial parameters for adaptation. This ensures that critical model weights remain stable and minimizes the impact of noisy or irrelevant samples. Sharpness-Aware Minimization (SAR) [14] further improves stability by optimizing only the most reliable features. By guiding reliable samples towards a flat minimum, it enhances the model's stability and resilience, even under severe distribution shifts. SAR is based on the insight that models trained in flatter regions of the loss landscape tend to be more robust to perturbations [15], thereby, ensuring consistent performance even under challenging conditions.\nDespite the advancements made, existing approaches to TTA focus primarily on entropy as the key measure of confidence. This focus, however, has limitations, particularly in scenarios where spurious correlation shift makes entropy an unreliable confidence metric [16]. Recently, DeYO [17] proposed the concept of Pseudo Label Probability Difference (PLPD) to better identify harmful samples that may compromise model's performance. DeYO enhances performance by intentionally degrading the shape of objects in images, ensuring the model's judgments are based on generalizable features rather than misleading patterns.\nContributions: Building upon SAR's focus on mitigating noisy samples through sharpness aware minimization and DeYO's use of shape information in TTA, we propose the enhanced test time adaptation with integrated entropy and gradient norms (ETAGE) method. ETAGE introduces a refined test-time adaptation strategy that integrates entropy minimization with gradient norms and PLPD directly addresses the limitations of SAR and DeYo. By considering gradient norms, we capture the model's sensitivity to noisy samples more effectively, enhancing the stability and efficacy of the adaptation process. Additionally, we provide a mathematical analysis"}, {"title": "2. PRELIMINARIES", "content": "In this section, we provide the required background used for development of the proposed ETAGE method.\n2.1. Distribution Shift\nDomain shift refers to differences between the distributions of training and testing data, which can be formally described as follows\n$S = {(x, y)|(x, y) \\sim p_s}$,\n$f_s: f_s(x) = y$\n$T = {(x, y)|(x, y) \\sim p_T}$,\n$f_T: f_T(x) = y$\nwhere $(x, y)$ represents the source data, which follows a probability distribution denoted by $p_s$. Function $f_s(.)$ is a mapping that assigns $x$ to $y$. The target data, $T$, is defined in a similar fashion. Consider a Neural Network (NN) denoted by $f(x, \\theta): x \\rightarrow y$, where $f(.)$ represents a fixed architecture with parameters $\\theta$. Generally speaking, the objective is to minimize the loss function given by\n$\\varepsilon_T(\\theta) = E_{(x,y)\\sim P_T}[l(f(x, \\theta), y)].$\nSuch a minimization task becomes particularly challenging in a TTA setup, where access to the source data is restricted.\n2.2. Entropy Minimization\nTo tackle the above mentioned issue, where the model is required to adapt to new, unseen data during inference without access to the source data, one effective strategy is entropy minimization. The key idea behind this approach is to adjust the model's parameters in real-time, focusing on minimizing the entropy of the output predictions. Entropy, in this context, measures the uncertainty of the model's predictions. By minimizing entropy, the model becomes more confident in its predictions, effectively adapting to the new distribution presented by the test data. To achieve this, typically, the focus is on adapting only specific layers within the model, particularly normalization layers, which are believed to serve as proxies for the source data by retaining certain statistics of the source domain. This approach was first introduced with Batch Normalization (BN) layers in TENT, and has since been extended to other normalization techniques such as Group Normalization (GN) and Layer Normalization (LN) [14]. By adjusting these normalization layers during test time, the model can adapt to the new data distribution without requiring access to the source data, therefore, maintaining its performance across varying domains.\n2.3. Pseudo Label Probability Difference (PLPD)\nPLPD focuses on estimating the probability associated with an input for the same class before and after applying noise or augmentation, and is computed as follows\n$PLPD_{\\theta}(x, x') = (P_{\\theta}(x) - P_{\\theta}(x'))$,\n(1)\nwhere $x$ and $x'$ are the sample input before and after, respectively, applying the augmentation/noise. The PLPD measures the sensitivity of the model's predictions to slight changes in the input. For example, the patch shuffle technique, as shown in Figure 1, rearranges small regions (patches) in the input to alter spatial dependence within the image. By evaluating the PLPD under such conditions, one can assess the accuracy of the model when key information is disrupted. The core idea is that if the model still assigns the same class to the input after noise or augmentation, it may have learned spurious correlations from the source, rather than focusing on the shape of the object in the input image. The noise or augmentation is, typically, designed to disrupt or destroy objects within the image. The desired scenario is a high PLPD, which indicates that after the object is corrupted, the model is unable to assign the same class. This scenario is crucial as it identifies samples that should be used later for adaptation, ensuring the model's predictions are based on more meaningful features."}, {"title": "3. THE ETAGE METHOD", "content": "In this section, we introduce the proposed ETAGE, that leverages both a gradient norm threshold and PLPD to enhance sample filtering. We premise that a high gradient norm indicates that the model is highly sensitive to small perturbations in the input space. Such samples, even if they produce an acceptable PLPD, might be misleading because the underlying instability (captured by the high gradient norm) is not reflected in the PLPD calculation. To explore this premise, a mathematical counterexample is provided below.\nETAGE, first applies a gradient norm threshold in addition to an entropy threshold to ensure that only stable samples proceed to the PLPD calculation. The gradient norm is used to identify samples that may cause instability in the model's predictions, and is defined as\n$||\\nabla_{\\theta}L(x, \\theta) || = || \\frac{\\partial P(y | x, \\theta)}{\\partial x} ||$,\n(2)\nA high gradient norm indicates that the model's output is highly sensitive to small changes in the input, which can be a sign of overfitting to noise or instability in the learned decision boundary. Now, consider a sample $x$ where the gradient norm is very high, i.e.,\n$||\\nabla_{\\theta}L(x, \\theta) || >> \\tau_{Grad}$,\nwhere $\\tau_{Grad}$ is the gradient norm threshold. A high gradient norm suggests that even a small change in x will cause a significant change in the model's prediction. Suppose we apply a small perturbation $\\delta$ to $x$, resulting in $x' = x + \\delta$. Using the first-order Taylor expansion, the predicted probability is approximated as\n$P(y | x', \\theta) \\approx P(y | x, \\theta) + \\nabla_x P(y | x, \\theta) . \\delta$.\n(3)\nEquation (1) can then be approximated as\n$PLPD_{\\theta}(x, x') = (P(y | x, \\theta) - (P(y | x, \\theta) + \\nabla_x P(y | x, \\theta) . \\delta))$,\n(4)\nwhich simplifies to\n$PLPD_{\\theta}(x, x') \\approx - (\\nabla_x P(y | x, \\theta) . \\delta)$.\n(5)\nThe value of PLPD can be further estimated as\n$PLPD_{\\theta}(x, x') \\approx ||\\nabla_x P(y | x, \\theta) || ||\\delta|| cos(\\theta)$,\n(6)\nwhere $\\theta$ is the angle between $\\nabla_x P(y | x, \\theta)$ and $\\delta$. The PLPD values primarily depend on two components: the gradient norm $||\\nabla_x P(y | x, \\theta) ||$ and the perturbation magnitude $||\\delta||$. Now, consider a scenario where the input is highly sensitive to small perturbations. In such cases, as illustrated in Equation (2), these samples will generate high gradient norms, meaning the PLPD is largely influenced by $||\\nabla_x P(y | x, \\theta) ||$. Consequently, even with low perturbations, the PLPD values will be disproportionately high. This undermines the reliability of the PLPD since it is meant to reflect the difference in predicted probabilities before and after perturbation. In other words, for noisy gradient samples, the reason the PLPD is high (and why such samples might pass the threshold) is due to the high gradient norm. To tackle this issue, we refine the sample selection to exclude those with high gradient norms, i.e.,\n$S'_e(x) = {x | Ent_{\\theta}(x) > \\tau_{Ent}, ||\\nabla_{\\theta}L(x, \\theta) || < \\tau_{Grad}, PLPD_{\\theta}(x, x') > \\tau_{PLPD}}$\nWe remove these noisy, high-gradient samples from the set, leading to a more stable and effective adaptation process. The final entropy loss to be minimized is given by\n$L_{final} (\\theta) = \\sum_{x\\in S'(x)} Ent_{\\theta}(x)$.\n(7)"}, {"title": "4. SIMULATIONS AND RESULTS", "content": "To evaluate performance of the ETAGE method, we have used CLIP [18], which is a large pre-trained model that utilizes contrastive learning to connect image and text data. This approach was first introduced in the groundbreaking CLIP research [19]. CLIP is composed of two main components: a text encoder and an image encoder. The text encoder is based on a Transformer architecture, which effectively processes and understands natural language input. The image encoder, on the other hand, can be implemented using either a Vision Transformer (ViT) (the one that we have utilized is the ViT-B/32) or a Convolutional Neural Network (CNN). Together, these encoders are trained to align images with their corresponding text descriptions, enabling the model to perform various tasks involving both visual and textual information. For the CLIP model we modify LN layers while keeping the rest parameters freezed in the adaptation phase.\n4.1. Datasets\nWe employed CIFAR-10-C and CIFAR-100-C datasets in our experiments to evaluate the ETAGE method. Below, we provide a brief yet comprehensive introduction to these datasets, highlighting their key characteristics and relevance to our research objectives.\n4.1.1. CIFAR-10-C\nThis dataset shares its training samples with CIFAR-10 [20], but the test set, CIFAR-10-C [21], is created by applying various distortions to the original CIFAR-10 test images. These modifications include a range of corruptions and noises, such as blur, Gaussian noise, and shot noise. CIFAR-10-C presents these corruptions at five different levels of severity, resulting in a total of 50,000 test images for each type of noise. This dataset is particularly valuable for assessing the performance of image recognition models across diverse real-world scenarios, therefore, contributing to the enhancement of their reliability and robustness.\n4.1.2. CIFAR-100-C\nThis dataset extends CIFAR-100 by applying a range of corruptions (similar to that of CIFAR-10-C) to its test set creating challenging scenarios for evaluating image recognition models. This dataset is commonly used in computer vision tasks to assess the performance of different models under distribution shifts.\n4.2. Results\nWe estimate and compare ETAGE with some state-of-the-art methods in the literature namely, DeYo, EATA, SAR, and TENT. The results presented in Table 1 and Table 2 demonstrate that ETAGE consistently outperforms the other methods in the literature across a variety of corruption types. Specifically, on the CIFAR-10-C dataset (Table 1), ETAGE achieves the highest average accuracy, surpassing all other models. Similarly, on the CIFAR-100-C dataset (Table 2), ETAGE also leads with the highest average performance, indicating its robustness and effectiveness in handling diverse corruptions. These results validate the superiority of our approach, reinforcing ETAGE's capability to adapt effectively under varying conditions compared to state-of-the-art methods. It is also worth nothing that for TTA, lowering the batch size is challenging (lower batch sizes such as 1, 2, 4 are known as wild scenarios). This is because TTA relies on calculating statistics (mean and variance) over normalization layers (BN, GN, LN). When the batch size is small, these statistics may be less representative of the data distribution, leading to instability in the model's performance. The performance of different methods for different batch sizes are shown in Figure 2(b) for Gaussian noise corruption with severity 5 of CIFAR-10-C. ETAGE retains its accuracy even with low batch sizes, while its counterparts' performance drop severely as batch size decreases. This underscores the effect of identifying and filtering out the noisy gradient samples which has been described in more details in Section 3.\nTable 3 provides a comprehensive comparison of different methods under Gaussian noise on CIFAR-10-C, including TENT, DEYO, SAR, EATA, and ETAGE, evaluated across key performance metrics: Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Brier score, and Area Under the Receiver Operating Characteristic curve (AUROC). Lower ECE and MCE values suggest that a model's probability estimates are more dependable. Similarly, a lower Brier score indicates higher accuracy in probability predictions, while a higher AUROC value reflects better discrimination between classes. Notably, ETAGE outperforms others across most metrics, showcasing superior calibration and discriminative capability which in turn underscores the effectiveness of ETAGE in handling Gaussian noise while maintaining reliable predictions."}, {"title": "5. CONCLUSION", "content": "This study introduce ETAGE, an improved method for TTA, addressing the limitations of existing approaches that primarily rely on entropy as a confidence metric. By integrating gradient norms with the PLPD, our approach effectively filters out noisy samples, leading to more stable and reliable model adaptation. The application of this method to the CIFAR-10-C and CIFAR-100-C datasets demonstrated its effectiveness in handling various distribution shifts, with consistent performance improvements over baseline methods. Our findings emphasize the importance of considering gradient information alongside entropy in TTA, providing a pathway to more robust model adaptation. As part of future work, we plan to extend our method to wild setups involving lower batch sizes and evaluate its performance on the IMAGENET-C dataset. These efforts will further assess the scalability and generalization of our approach in more diverse and challenging environments."}]}