{"title": "How to Choose a Reinforcement-Learning Algorithm", "authors": ["Fabian Bongratz", "Vladimir Golkov", "Lukas Mautner", "Luca Della Libera", "Frederik Heetmeyer", "Felix Czaja", "Julian Rodemann", "Daniel Cremers"], "abstract": "The field of reinforcement learning offers a large variety of concepts and methods to tackle sequential\ndecision-making problems. This variety has become so large that choosing an algorithm for a task at hand\ncan be challenging. In this work, we streamline the process of choosing reinforcement-learning algorithms\nand action-distribution families. We provide a structured overview of existing methods and their properties,\nas well as guidelines for when to choose which methods. An interactive version of these guidelines is available\nonline at https://rl-picker.github.io/.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) has long been known as a meaningful approach to tackling problems that\ncould be cast into a sequential decision-making process together with a reward-like performance measure.\nWith many fundamental concepts of RL being already established for several decades, the strong progress in learning meaningful state representations with deep feedforward neural networks\n[61, 32] paved the way for high-impact machine learning systems outperforming humans in a variety of game-\nrelated tasks [104, 95, 5]. Modern applications of RL are game-playing [100], robotics [56], communication\nsystems [66], and finance [12], to name just a few.\nNowadays, deep reinforcement learning encompasses a large number of methods and algorithms with\nvarying properties. Unfortunately, the information about which method properties to choose in which situ-\nations is distributed across the considerable length of standard courses and textbooks; and the information\nabout which method has which properties is distributed across dozens of papers that propose the methods.\nExisting surveys about RL typically provide a compact overview of methods\nand application fields but lack structured guidelines for when to choose which methods. On the other hand,\npractical recommendations for RL algorithm design mostly focus on a few environmental\nconditions like simulated locomotion benchmarks.\nIn this work, we streamline the process of choosing RL algorithms. We achieve this by bringing the\ninformation about which algorithmic properties to choose in which situations into a quickly accessible form.\nIn addition, we elaborate on reasons that make certain methods advantageous over others in certain envi-\nronments based on practical considerations and existing literature in the field.\nUnless stated otherwise, we consider a Markov decision process with a set S of possible states $s \\in S$,\na set A of possible actions $a \\in A$, state-transition probabilities $p(s' | s, a) = p(s_{t+1} = s' | s_t = s,a_t = a)$,\nand rewards $r_t$, where $t \\in \\mathbb{N}_0$ is a timepoint, $s, s' \\in S$ and $a \\in A$. For control tasks, the goal is to learn a\npolicy $\\pi(a | s)$, mapping from each state s to a mixed next action, i.e. a distribution over actions that aims\nto maximize the expectation $E[\\Sigma r_{t+1}]$ of the cumulative reward $\\Sigma r_{t+1}$ (the cumulative reward is\nalso called return) across an episode, where $r_t$ is the reward at time t and $t_{max}$ is the duration of an episode.\nValue functions assign the expected future cumulative reward to either a single state (state-value function\n$V(s) = E[\\Sigma r_{t+1} | S_t = s])$ or to state-action pairs (action-value function $Q(s, a) = E[\\Sigma r_{t+1} | S_t =\ns, a_t = a)$ and advantage function $A(s, a) = Q(s, a) - V(s))$, assuming a fixed policy for all future decision\nsteps. The state-value function V(s) is the expectation of Q(s, a) with respect to the probability measure\n$\\pi(a | s)$. Note that, for decision processes with infinite time horizon, i.e. $t_{max} \\rightarrow \\infty$, the cumulative reward\nneeds to be calculated as $\\Sigma \\gamma^t r_{t+1}$, where the discount factor 0 < $y$ < 1 ensures convergence of the\nseries. We ignore this fact for the sake of simplicity wherever possible.\nThe rest of this paper is structured as follows. Section 2 provides an overview of RL algorithms and\nguidelines for choosing them based on their key properties. Section 3 provides an overview of action-\ndistribution families that can be used in the RL algorithms as a blueprint for the policy and guidelines\nfor choosing them. Final selection steps are in Section 4. Moreover, we provide a list of deep neural-\nnetwork architectures that are popular in RL in Section 5 and practical guidance for training those models\nin Section 6. Finally, we conclude with a brief discussion about present challenges and promising directions\nfor future research as well as possible obstacles that might be encountered during our streamlined decision\nprocess.\nWe could imagine different ways to go through this overview: An experienced RL practitioner may take\na direct look at the tables, combining algorithmic properties and action-distribution families according to\nthe needs for a concrete problem at hand or to discover new approaches and unresolved challenges. A less\nexperienced reader might also be interested in the short, rather intuitive, explanations of properties. Even\nreading crisscross focusing only on some tables/sections should be possible since they are often independent\nof each other. Regarding the decisive steps we propose, it could even make sense to change their order\nfor concrete tasks at hand. In particular, if some environmental properties are easier/clearer to assess,\nit probably makes sense to first make the \"evident\" decisions and to conduct individual experiments to\ncomplete less clear choices (e.g. it might be easier to assess if the environment has discrete or continuous\nstates than to evaluate whether sample efficiency or training stability are more important)."}, {"title": "2. RL algorithms", "content": "Table 1 contains a list of RL algorithms and their respective properties. Algorithms with certain proper-\nties can be inappropriate in certain situations. Tables 2 to 13 explain these relationships between situations\nand algorithm properties and thus help choose an algorithm (based on the properties listed in Table 1)\nthat is appropriate for a given situation. More specifically, Tables 2 to 13 describe properties of situations\n(\"if\"), desirable method properties that are appropriate for respective situations (\"then\"), and correspond-\ning reasons (\"because\"). Having chosen a certain RL algorithm, there exist usually still various options for\nthe action-distribution family that can often be used interchangeably under constraints imposed by the RL\nalgorithm and its properties. We consider the possible choices under the respective constraints in Tables 14\nto 16."}, {"title": "2.1. Model-free vs. model-based reinforcement learning", "content": "Considerations for deciding between model-free and model-based RL are given in Table 2 and explana-\ntions of terms are provided in the following.\nRL algorithms differ in how they deal with the environment dynamics, i.e. with the rewards $r_t$ and\nstate-transition probabilities $p(s' | s, a)$. The options to choose from are:\n\u2022 Model-free RL algorithms aim to learn a policy and/or a value function directly from interacting\nwith the environment without being aware of the underlying environment dynamics. That is, a model\nof the environment dynamics is not involved at any point in the algorithmic procedure.\n\u2022 Model-based RL algorithms model the environment dynamics explicitly, i.e. they include an envi-\nronment model somehow in the algorithmic procedure. This model can either be given in advance, or\nit can be learned simultaneously with the policy. The variety of model-based approaches ranges from\nmethods that slightly augment an otherwise model-free RL approach with an environment model (e.g.\nsimulating environmental transitions from the environment model in addition to real-world experience)\nto methods that infer an entire policy from planning based on the environment model.\nIn the following, we will primarily deal with the selection of model-free RL algorithms. In situations\nfor which Table 2 recommends using model-based methods, an overview of model-based RL methods and\nways to incorporate a model into RL algorithms can be found in recent surveys and overview papers [74, 73,\n114, 39, 85]. For an introduction to pure planning algorithms, in the sense of searching for the best action\nsequence in the space of all possible action sequences based on a given environment model (i.e. model-based\nbut not RL due to lack of trainable parts), see for example [60]."}, {"title": "2.2. Hierarchical RL", "content": "Selection criteria for and against a hierarchical approach can be found in Table 3 and a short introduction\nto the terms is provided in the following.\n\u2022 In hierarchical RL, the agent performs a coarse-to-fine cascade of several consequent decisions about\nthe next action. The hierarchy of these decisions is often based on temporal and/or spatial abstractions\nthat are hardwired by the programmer or learned by the agent. It is probably most convenient to\nconsider a concrete example: Assume we have a robot that can move around with multiple wheels.\nThen, a high-level action (sometimes also called skill in this context) could be \"driving an S-curve\"\nwhile a lower-level action describes how exactly to do that, i.e. how to turn which wheel. Consequently,\nmultiple different policies with different levels of abstraction can be trained. In some cases, it is even\npossible to transfer skills from one task to another while training other skills from scratch or to fine-\ntune them. In the above example, a hierarchical RL approach would allow to first train the skill\n\"driving an S-curve\" independently of other skills and then, potentially in a new environment, train a\nhigher-level policy that chooses among multiple skills, e.g. \"driving an S-curve\" and \"driving straight\nahead\".\n\u2022 Non-hierarchical algorithms do not contain such a hierarchical structure explicitly (although parts\nof the neural network might learn to approximately perform a hierarchy of decisions). In the litera-\nture about hierarchical RL, non-hierarchical RL algorithms are sometimes called algorithms with flat\npolicy [62]."}, {"title": "2.3. Imitation learning", "content": "Table 4 summarizes the considerations about when to apply imitation learning in an RL task. Terms\nrelated to imitation learning are explained in the following.\n\u2022 A field closely related to reinforcement learning is imitation learning. While RL algorithms aim to\nsolve a decision-making problem based on some environmental feedback (the rewards r), imitation-\nlearning algorithms learn a policy based on observing the behavior of an expert. While a detailed\noverview of imitation-learning algorithms is beyond the scope of this work (see related literature about\nimitation learning, e.g. [3, 109, 127, 48, 57, 91]), our Table 1 solely lists methods that combine imitation\nlearning with RL, as well as RL methods that do not perform imitation learning at all, but our table\ndoes not list methods that perform imitation learning without RL. Notably, the performance of an\nagent trained with pure imitation learning is quite bounded by the performance of the expert. In\ncontrast, an algorithm that uses both concepts (RL and imitation learning) jointly might have better\nchances of outperforming the expert at the end of the training phase [46].\nJoint reinforcement and imitation learning usually means to pre-train an agent first solely on an\nimitation loss based on the (dis)similarity between the behavior of an expert and the RL agent,\nand subsequent reward-based training (as in common RL) with an additional imitation loss term.\nTheoretically, it would be possible to remove the imitation loss after the imitation pre-training phase,\nbut as the authors of [46] argue, this seems to be sub-optimal.\n\u2022 Algorithms without imitation learning listed in Table 1 usually do not use expert trajectories in\nthe learning process of the agent.\nNote that, even though it might be tempting to simply augment a replay buffer in an off-policy algorithm\n(cf. Section 2.6.3) with expert trajectories, this can have a devastating impact on the training process without\nfurther modifications [30] but can also work well [76]. If such augmentation is an option, e.g. because the\nexpert trajectories do already exist and an off-policy algorithm with replay buffer seems to be a good choice\nas per Table 8, a closer look into the literature about the respective algorithm and/or additional experiments\nmight be required."}, {"title": "2.4. Distributed algorithms", "content": "Considerations for deciding for or against a distributed (not to be confused with distributional) RL\nalgorithm are given in Table 5 and an explanation of this term is provided in the following. The two options\nto choose from are:\n\u2022 In distributed algorithms, there exist two or more actors, i.e. parallel sub-processes of which each\nacts in a separate instance of the environment and creates training data, and one or more learners,\ni.e. parallel sub-processes that are responsible for updating parameters, at training time. The number\nof learners can be smaller, equal, or larger than the number of actors. Especially if the acquisition of\nexperience is slow, the parallel execution of actor processes can significantly reduce training time since\nmore experience is acquired in less time. Multiple learners allow, for example, for an asynchronous\nand parallel computation of gradients [72], thus also speeding up training."}, {"title": "2.5. Distributional algorithms", "content": "Considerations to decide for or against a distributional (not to be confused with distributed) algorithm\nare in Table 6 and related terms are explained in the following.\n\u2022 Distributional approaches estimate the probability distribution of the cumulative reward $\\Sigma r_t$\ninstead of merely its expected value $E[\\Sigma r_t]$ (i.e. the value function), where $r_t$ is the reward at time\nt. Distributional approaches are usually combined with value-based or actor-critic methods since pure\npolicy-based methods do not learn a value function (and value-based and actor-critic methods do).\n\u2022 Non-distributional approaches only consider the expectation of the cumulative reward as introduced\npreviously in Section 2.8."}, {"title": "2.6. On-policy vs. off-policy learning", "content": "Considerations for deciding between on- and off-policy algorithms are given in Table 8 and explanations\nof these terms are provided in the following.\n2.6.1. Target policy, behavior policy, test-time policy\nWe define the terms target policy, behavior policy, and test-time policy as follows:\n\u2022 The target policy (at each given timepoint during training) is a policy that is being trained, and\nfrom which the behavior policy and the test-time policy are computed as explained below. The term\ntarget policy is not to be confused with the term output target (values that a neural network tries to\nlearn to output, but does not output precisely)."}, {"title": "2.6.2. Stochastic vs. deterministic target policy", "content": "Considerations for deciding between a stochastic and a deterministic target policy are given in Table 7\nand explanations of these terms are provided in the following.\n\u2022 A stochastic policy is defined by a conditional probability distribution $ \\pi_{\\theta}(a | s)$, where $\\theta$ are the\nlearnable parameters. Given a state s, the action a is sampled from this probability distribution. The\nprobability distribution over actions (conditional, given the state) can differ from state to state.\n\u2022 A deterministic policy is defined by a deterministic function $\\mu_{\\theta}(s)$ that maps each state s to an\naction, where $\\theta$ are the learnable parameters. Note that a deterministic policy is a special case of a\nstochastic policy where $ \\pi_{\\theta}(a | s) = 1$ if $a = \\mu_{\\theta}(s)$."}, {"title": "2.6.3. Definition of on-policy and off-policy learning", "content": "Having the above distinction between target and behavior policy in mind, the difference between on- and\noff-policy algorithms can be summarized as follows:\n\u2022 In on-policy algorithms, the behavior policy and the target policy are always equal and only experi-\nence collected with the current target/behavior policy is used to update the target policy. That is, the\nexperience used for updating (\"learning about\") the target policy has been collected using the target\npolicy in its current state (right before the parameter update). After the parameter update, the previ-\nously collected experience becomes worthless for the on-policy algorithm since the policy under which\nthe experience has been acquired is then different from the current target policy, and new experience\nwith the updated target policy needs to be collected.\n\u2022 Off-policy algorithms allow the behavior policy to differ from the target policy. In contrast to on-\npolicy methods, experience collected under a behavior policy that was different than the current target\npolicy can be used for updating the parameters of the target policy. A typical example is to increase\nthe amount of exploration by using a behavior policy that has more randomness than the current target\npolicy. Also, the allowed discrepancy between the two policies is often exploited by using a replay\nbuffer, in which experience acquired under the behavior of multiple older versions of the behavior\npolicy is stored. It is also possible to integrate some expert trajectories into the replay buffer, leading\nto a form of imitation learning, see also Section 2.3. Finally, the experience in a replay buffer can be\nprioritized such that training samples that seem to be promising for the training process are replayed\nmore frequently than potentially less important ones [94].\nNote that it is no problem to use experience collected under the current policy (\"on-policy data\") in off-\npolicy algorithms, see, e.g. [76, 53]. In contrast, the opposite (using off-policy data for training in on-policy\nalgorithms) is typically not possible."}, {"title": "2.7. Value-based vs. policy-based vs. actor-critic", "content": "Considerations for the decision between a value-based, policy-based, or actor-critic algorithm can be\nfound in Table 9. If this table indicates a value-based algorithm, Table 11 lists considerations for deciding\nbetween the different subcategories based on the size of the state and action spaces. On the other hand, if\na policy-based or actor-critic approach is required, Table 10 contains considerations for the decision about\na learned critic. The terminology and underlying concepts are explained in the following.\nThe primary goal of RL algorithms for control tasks is to learn a policy $\\pi(a | s)$ that maximizes the\ncumulative reward $\\Sigma r_t$ (while in RL algorithms for prediction tasks, the goal is to learn to predict the\noutcome of a given policy in terms of rewards for an unknown environment model). In general, RL algorithms\nfor control tasks differ in the way the agent's policy is defined and, therefore, also in the way the policy is\nlearned. The options with respective subcategories are:\n\u2022 Value-based algorithms infer the policy directly from a learned action-value function $Q(s,a) =$\n$E(\\Sigma r_{t+1} | S_t = s,a_t = a)$ that assigns an expected value of subsequent cumulative rewards\nto every possible state-action pair (a more accurate name would be \"state-action-value function\" but\n\"action-value function\" is widely established). See Section 2.8 for methods to learn such an action-value\nfunction. In value-based algorithms, the agent selects an action $a_t = argmax Q(s_t, a)$ at each time\nstep t. Due to the deterministic computation of the argmax, this leads to a primarily deterministic\npolicy (if multiple actions maximize Q at a certain state, one of them may be selected randomly).\nHowever, there exist ways to make the actions stochastic (cf. Section 3.1), leading to policies that\ndeviate from $argmax Q(s_t, a)$ but are still built upon this definition and therefore considered value-\nbased. Value-based algorithms can further be grouped into the following subcategories.\nTabular value-based methods define an action value for each possible state-action pair (s, a) in\na tabular way, i.e. a table that represents the action-value function Q(s, a). For tabular action-\nvalue-based methods (in contrast to non-tabular methods, see below), very good convergence\nguarantees exist and exact maximization is usually possible [119], [107, Chapters 5,6]. Tabular\nmethods are not related to deep learning and we will not consider them further; a detailed\ndescription can be found in [107, Chapter I]. For the sake of completeness, we indicate in Table 9\nin which situation a tabular value-based approach might be most appropriate.\nNon-tabular value-based methods approximate the action-value function with a non-tabular\nfunction Q(s, a), where $\\theta$ are trainable parameters. In most cases, $Q_{\\theta}$ is represented by a deep\nneural network and an action at time t is given by $a_t = argmax_a Q_{\\theta}(s_t, a)$.\nWe further subdivide non-tabular value-based algorithms according to the following categories\nthat are also used by [111]:\n* In value-based methods with exact maximization over actions, the action a is given by\nthe one that exactly maximizes Q(s, a) at the current state s.\n* Value-based methods with approximate maximization and fixed search procedure\ncompute an action that might not maximize the action-value function exactly, using a fixed\nprocedure such as the cross-entropy method [92].\n* Value-based methods with approximate maximization and learned search proce-\ndure compute an action (that might not maximize the action-value function exactly) using\nanother learned function that proposes a set of promising actions [111]. The idea is to reduce\nthe potentially large set of possible actions to a smaller set of which the best action can be\neasily determined. Such methods are conceptually similar to actor-critic methods (defined\nbelow). The difference is that the executed action is determined by the action-value function\nfrom the set of proposals in a value-based method with approximate maximization and a\nlearned search procedure. In contrast, in actor-critic methods, an action proposed by the\nactor is directly executed without having its action value assessed before.\n\u2022 Policy-based algorithms, on the other hand, parameterize the policy explicitly. That is, they have a\nset of learnable parameters $\\theta$ defining the policy $\\pi_{\\theta}(a | s)$. At each time step t, the agent selects an\naction $a_t$ ~ $ \\pi_{\\theta}(a | s_t)$. Note that the policy does not necessarily need to be stochastic it can also\nbe a deterministic function a = $ \\mu_{\\theta}(s)$, i.e. $ \\pi(a | s) = 1$ iff a = $ \\mu_{\\theta}(s)$. We ignore this detail for the sake\nof simplicity unless stated otherwise."}, {"title": "2.8. Value-function learning", "content": "In Table 12, we provide some considerations that might be helpful for choosing the sub-algorithm for\nvalue-function learning.\nThe choice of the sub-algorithm for value-function learning, i.e. the sub-algorithm responsible for esti-\nmating the state-value function V(s), the action-value function Q(a, s), or the advantage function A(a, s) =\nQ(a, s) - V(s) is a key design choice in value-based and actor-critic methods. Intuitively, action-value and\nadvantage functions provide an assessment of the quality of the current policy in terms of the future cumu-\nlative reward $\\Sigma r_t$ (also called return), where $r_t$ is short for the reward at time t that could be expected\nfrom following the current policy $\\pi(a | s)$ after state $s_t$, i.e. from time t onward.\nAs a reminder, the action-value function Q(s, a) is defined as the expected value of cumulative rewards\nfor state-action pairs, i.e. Q(s, a) = E($\\Sigma \\gamma^{t+1} r_{t+1} | S_t = s,a_t = a$), where $r_t$ is the reward at time t,\n$t_{max}$ is the maximal duration of an episode, 0 < $y$ < 1 is the discount factor that ensures convergence of the\nseries for $t_{max} \\rightarrow \\infty$, and $s_t$ and $a_t$ are the state and action at time t, respectively. The state-value function\nV(s) is defined analogously with an additional summation over all possible actions $a_t$ (or an integral in the\ncase of continuous actions).\nThese sub-algorithms fundamentally determine how the agent learns how to act in the environment since\nthe policy is either directly inferred from the value function (value-based methods) or the parameter updates\nof a parametric policy rely on a value function (actor-critic methods). Even for policy-based methods without\na learned critic, where the updates of the policy do not directly rely on a learned value function, a state-\nvalue function is usually involved in the form of a baseline to increase training stability (see Section 2.7 and\nSection 6.1).\nIn the following, we provide a brief overview using a high-level classification of the respective sub-\nalgorithms. A detailed consideration of the sub-algorithms is beyond the scope of this work. We refer to the\nliterature cited in Table 12 for further details and to [107] for an introduction to value-function learning. In\naddition, we would also like to refer to [2] for a comparison of some estimators, namely n-step, GAE, and\nV-trace, in an on-policy algorithm applied in a continuous environment.\nThe classes of algorithms we consider are:\n\u2022 Monte Carlo (MC) algorithms learn the value functions from cumulative rewards observed during\nentire sampled trajectories. Therefore, the parameters of the value function can only be updated after\ncompleted trajectories. The sample mean of the cumulative rewards obtained from multiple sampled\nepisodes is an unbiased estimator of the expectation of the cumulative reward [107, Chapter 5]. It can,\ntherefore, be used for learning the value function in a supervised manner.\n\u2022 One-step bootstrapping algorithms, often summarized under the term temporal-difference (TD)\nlearning, follow a different approach. Instead of observing cumulative rewards of entire episodes, a\nbootstrapping target (in the sense of an \"output target\") for the value function estimator is created\nfrom the last observed reward and an approximation of the value function calculated with the current\nparameterization (e.g. the current neural network), hence bootstrapped. As a simple example, assume\nwe want to learn the discounted state-value function V for some fixed policy $\\pi$. Then, a gradient-based\nTD update rule for the estimator $V_{\\phi}$ is given by $\\phi$ \u2190 $\\phi$+$\\alpha$($r_{t+1}$+$\\gamma V_{\\phi}$($s_{t+1}$) - $V_{\\phi}(s_t)$)$\\nabla_{\\phi}V_{\\phi}(s_t)$, where\n$\\alpha$ is the learning rate and the transition $s_t$\u2192 $s_{t+1}$ is due to an action a ~ $\\pi$ [107, Chapter 9]. Some\nof the most popular RL algorithms like Q-learning [119] and SARSA [93] are based on the concept of\ntemporal-difference learning.\n\u2022 Eligibility traces, n-step methods, and related algorithms usually provide a trade-off between\none-step bootstrapping and MC learning in the sense that they consider the reward of multiple steps\ntogether with a bootstrapped value as the learning objective. For instance, replacing the objective\n$r_{t+1}$+$\\gamma V_{\\phi}(s_{t+1})$ by $r_{t+1}$ + $\\gamma r_{t+2}$ + $\\gamma^2V_{\\phi}(s_{t+2})$ in the example of the previous bullet point would make\nit a 2-step method. In general, MC and TD methods are special cases of eligibility traces and n-step\nmethods. That is MC and TD methods can usually be recovered for certain values of an additional\nhyperparameter (e.g., n = \u221e for MC and n = 1 for TD in n-step methods) [107, Chapter 12],\n[75, 112]. That way, the benefits of one-step bootstrapping and MC methods can often be combined\nin one algorithm. Examples of such methods are:\nTD(n) (see [107, Chapter 7], for instance)\nTD(\u03bb) [106]\nLSTD-Q(\u03bb) [10]\nQ(\u03bb) [119, 82]\nSarsa(\u03bb) [93]\nT\u0392(\u03bb) [86]\n\u0395\u03a4(\u03bb) [112]\nGAE(\u03bb) [97]\nRetrace(\u03bb) [75]\nGTD(\u03bb) [67]\nV-trace(n) [23]"}, {"title": "2.9. Entropy regularization", "content": "In Table 13, we provide considerations about when to apply entropy regularization. In the following, we\nexplain the main idea together with various realizations.\nThe well-known dilemma in reinforcement learning is to decide between exploration of new policies\n(find unexplored actions/states through randomness) and exploitation of already discovered, well-working\npolicies (good performance). In general, this trade-off is addressed by a behavior policy that contains, by\nits definition, a certain degree of randomness. In practice, a common problem is to \u201ctune\u201d the randomness,\ni.e. the amount of noise, of the behavior policy in a way that accounts well for the exploration/exploitation\ntrade-off. The concept of entropy regularization or maximum-entropy RL addresses this problem in that\nit incorporates the policy's degree of randomness (i.e. how diverse the distribution of proposed actions\nis) into its training objective. While the standard objective of the policy is to maximize the expected\nfuture cumulative reward, the policy's objective in maximum-entropy RL is to maximize the reward plus\nan entropy term (representing the policy's randomness). Furthermore, the entropy objective often prevents\nearly, potentially sub-optimal convergence of the modes of the action distribution [37].\nFor maximum-entropy RL (in the form of the options listed in the bullet list below) to be applicable, the\npolicy (which can be the 'target policy' in policy-based and actor-critic algorithms, or a 'proposal function'\nin value-based algorithms; for the sake of simplicity, we refer to both of these options with 'policy' here)\nneeds to be parameterized explicitly (not implicitly as in standard Q-learning). We refer to the parameters\nmaking up the policy, e.g. the weights of a neural network that parameterizes the action distribution, as\n'policy parameters'.\nIn practice, there exist several ways towards maximum-entropy RL; we list the options to choose from\nin the following.\n\u2022 A per-state entropy regularization term in the objective function of the policy can be used [122,\n72]. More precisely, a term of the form $H( \\pi_{\\theta}(\u00b7 | s_t))$ can be added to the reward in order to enhance\nthe information entropy H of the policy in the current state $s_t \\in S$.\n\u2022 While the regularization term discussed in the previous bullet point enhances the entropy of the\npolicy at the current state st, soft Q-learning [37, 76] aims at enhancing the entropy of entire policy\ntrajectories, i.e. $\\Sigma H( \\pi_{\\theta}(\u00b7 | s_t))$, by considering the expected future cumulative entropy in addition\nto the observed rewards.\n\u2022 A Kullback-Leibler divergence regularization term in the objective that is minimized with respect\nto policy parameters, i.e. a term $D_{KL}( \\pi_{\\theta}(\u00b7 | s_t) || q(\u00b7))$, where $q(\u00b7)$ can be any probability distribution\nand $D_{KL}$ denotes the Kullback-Leibler divergence between two probability distributions. In practice,\n$q(\u00b7)$ can be chosen to be the old action distribution [99], i.e. the policy before updating the parameters\naccording to the objective. This can be interpreted as a relative entropy constraint on the parameter\nupdate because strong changes in the entropy of the stochastic policy are penalized and policies are\nusually initialized to have high entropy at the beginning of training (potentially drifting towards low\nentropy after some iterations).\n\u2022 Mutual information (MI) between states and latent variables can also be used as an entropy\nregularizer. In [27], maximization of the MI yields a diverse set of skills in a hierarchical policy. More\nprecisely, in this case, the policy $ \\pi_{\\theta}(\u00b7 | s_t, c)$ depends on the current state st and a hierarchically\nhigher skill (see Section 2.2), represented as latent variable c. While H($ \\pi_{\\theta}(\u00b7 | s_t)$) is intractable due\nto the integration over c, the MI given by H(c) \u2013 H(c | st) can be computed. Note that the goal\nhere is to have a unique action associated with a certain latent code (the entropy H(c | st) needs to\nbe reduced to this end), which is different from the goal of learning a diverse set of actions described\nin the above bullet points; hence, the application of MI regularization to a specific problem probably\nrequires a very detailed examination of the policy (and potentially involved latent variables)."}, {"title": "3. Action-distribution families", "content": "The goal of RL is to find an optimal policy $\\pi(a | s)$ that maps state s to a probability for action a in a\ncertain environment. In this section, we discuss which action-distribution family to use in the RL algorithm\nselected from Table 1. While the policy defines a specific behavior of the agent (which is optimized during\ntraining), the action-distribution family is the \"blueprint\" for the policy and needs to be chosen a priori.\nMathematically, the action-distribution family is a family of probability distributions over actions, often\ncharacterized by a certain shape but without specific parameters (e.g. a Gaussian without having its mean\nand variance parameters specified). The goal of the RL training process is to find these parameters, i.e.\nto select/learn the optimal policy from the action-distribution family in the given environment. Hence,\nthe choice of the action-distribution family is important as it pre-defines the possible behavior of the agent\nduring training (behavior-action-distribution family) and the possible result of the training process (target-\naction-distribution family).\n3.1. Value-based action-distribution families (action-distribution families for value-based algorithms)\nIf Table 9 indicates a value-based algorithm, a value-based action-distribution family needs to be chosen.\nWe list options for value-based action-distribution families in Table 14 and explain them in the following.\nWe further describe"}]}