{"title": "How to Choose a Reinforcement-Learning Algorithm", "authors": ["Fabian Bongratz", "Vladimir Golkov", "Lukas Mautner", "Luca Della Libera", "Frederik Heetmeyer", "Felix Czaja", "Julian Rodemann", "Daniel Cremers"], "abstract": "The field of reinforcement learning offers a large variety of concepts and methods to tackle sequential decision-making problems. This variety has become so large that choosing an algorithm for a task at hand can be challenging. In this work, we streamline the process of choosing reinforcement-learning algorithms and action-distribution families. We provide a structured overview of existing methods and their properties, as well as guidelines for when to choose which methods. An interactive version of these guidelines is available online at https://rl-picker.github.io/.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) has long been known as a meaningful approach to tackling problems that could be cast into a sequential decision-making process together with a reward-like performance measure. With many fundamental concepts of RL being already established for several decades [9, 106, 119, 118, 93], the strong progress in learning meaningful state representations with deep feedforward neural networks [61, 32] paved the way for high-impact machine learning systems outperforming humans in a variety of game- related tasks [104, 95, 5]. Modern applications of RL are game-playing [100], robotics [56], communication systems [66], and finance [12], to name just a few.\nNowadays, deep reinforcement learning encompasses a large number of methods and algorithms with varying properties. Unfortunately, the information about which method properties to choose in which situ- ations is distributed across the considerable length of standard courses and textbooks; and the information about which method has which properties is distributed across dozens of papers that propose the methods. Existing surveys about RL [63, 126, 51, 125, 52, 120, 113] typically provide a compact overview of methods and application fields but lack structured guidelines for when to choose which methods. On the other hand, practical recommendations for RL algorithm design [2, 89, 44, 22] mostly focus on a few environmental conditions like simulated locomotion benchmarks.\nIn this work, we streamline the process of choosing RL algorithms. We achieve this by bringing the information about which algorithmic properties to choose in which situations into a quickly accessible form. In addition, we elaborate on reasons that make certain methods advantageous over others in certain envi- ronments based on practical considerations and existing literature in the field.\nUnless stated otherwise, we consider a Markov decision process with a set S of possible states $s \\in S$, a set A of possible actions $a \\in A$, state-transition probabilities $p(s' | s, a) = p(s_{t+1} = s' | s_t = s,a_t = a)$, and rewards $r_t$, where $t \\in \\mathbb{N}_0$ is a timepoint, $s, s' \\in S$ and $a \\in A$. For control tasks, the goal is to learn a policy $\\pi(a | s)$, mapping from each state s to a mixed next action, i.e. a distribution over actions that aims to maximize the expectation $E[\\sum r_{t+1}]$ of the cumulative reward $\\sum r_{t+1}$ (the cumulative reward is also called return) across an episode, where rt is the reward at time t and $t_{max}$ is the duration of an episode. Value functions assign the expected future cumulative reward to either a single state (state-value function $V(s) = E(\\sum r_{t+1} | S_t = s)$) or to state-action pairs (action-value function $Q(s, a) = E(\\sum r_{t+1} | S_t = s, a_t = a)$ and advantage function $A(s, a) = Q(s, a) \u2013 V(s)$), assuming a fixed policy for all future decision"}, {"title": "2. RL algorithms", "content": "Table 1 contains a list of RL algorithms and their respective properties. Algorithms with certain proper- ties can be inappropriate in certain situations. Tables 2 to 13 explain these relationships between situations and algorithm properties and thus help choose an algorithm (based on the properties listed in Table 1) that is appropriate for a given situation. More specifically, Tables 2 to 13 describe properties of situations (\"if\"), desirable method properties that are appropriate for respective situations (\u201cthen\u201d), and correspond- ing reasons (\"because\"). Having chosen a certain RL algorithm, there exist usually still various options for the action-distribution family that can often be used interchangeably under constraints imposed by the RL algorithm and its properties. We consider the possible choices under the respective constraints in Tables 14 to 16.\n2.1. Model-free vs. model-based reinforcement learning\nConsiderations for deciding between model-free and model-based RL are given in Table 2 and explana- tions of terms are provided in the following.\nRL algorithms differ in how they deal with the environment dynamics, i.e. with the rewards rt and state-transition probabilities $p(s' | s, a)$. The options to choose from are:\n\u2022 Model-free RL algorithms aim to learn a policy and/or a value function directly from interacting with the environment without being aware of the underlying environment dynamics. That is, a model of the environment dynamics is not involved at any point in the algorithmic procedure.\n\u2022 Model-based RL algorithms model the environment dynamics explicitly, i.e. they include an envi- ronment model somehow in the algorithmic procedure. This model can either be given in advance, or it can be learned simultaneously with the policy. The variety of model-based approaches ranges from methods that slightly augment an otherwise model-free RL approach with an environment model (e.g. simulating environmental transitions from the environment model in addition to real-world experience) to methods that infer an entire policy from planning based on the environment model.\nIn the following, we will primarily deal with the selection of model-free RL algorithms. In situations for which Table 2 recommends using model-based methods, an overview of model-based RL methods and ways to incorporate a model into RL algorithms can be found in recent surveys and overview papers [74, 73, 114, 39, 85]. For an introduction to pure planning algorithms, in the sense of searching for the best action sequence in the space of all possible action sequences based on a given environment model (i.e. model-based but not RL due to lack of trainable parts), see for example [60]."}, {"title": "2.2. Hierarchical RL", "content": "Selection criteria for and against a hierarchical approach can be found in Table 3 and a short introduction to the terms is provided in the following.\n\u2022 In hierarchical RL, the agent performs a coarse-to-fine cascade of several consequent decisions about the next action. The hierarchy of these decisions is often based on temporal and/or spatial abstractions that are hardwired by the programmer or learned by the agent. It is probably most convenient to consider a concrete example: Assume we have a robot that can move around with multiple wheels. Then, a high-level action (sometimes also called skill in this context) could be \u201cdriving an S-curve\u201d while a lower-level action describes how exactly to do that, i.e. how to turn which wheel. Consequently, multiple different policies with different levels of abstraction can be trained. In some cases, it is even possible to transfer skills from one task to another while training other skills from scratch or to fine- tune them. In the above example, a hierarchical RL approach would allow to first train the skill \u201cdriving an S-curve\u201d independently of other skills and then, potentially in a new environment, train a higher-level policy that chooses among multiple skills, e.g. \u201cdriving an S-curve\u201d and \u201cdriving straight ahead\u201d.\n\u2022 Non-hierarchical algorithms do not contain such a hierarchical structure explicitly (although parts of the neural network might learn to approximately perform a hierarchy of decisions). In the litera- ture about hierarchical RL, non-hierarchical RL algorithms are sometimes called algorithms with flat policy [62]."}, {"title": "2.3. Imitation learning", "content": "Table 4 summarizes the considerations about when to apply imitation learning in an RL task. Terms related to imitation learning are explained in the following.\n\u2022 A field closely related to reinforcement learning is imitation learning. While RL algorithms aim to solve a decision-making problem based on some environmental feedback (the rewards r), imitation- learning algorithms learn a policy based on observing the behavior of an expert. While a detailed overview of imitation-learning algorithms is beyond the scope of this work (see related literature about imitation learning, e.g. [3, 109, 127, 48, 57, 91]), our Table 1 solely lists methods that combine imitation learning with RL, as well as RL methods that do not perform imitation learning at all, but our table does not list methods that perform imitation learning without RL. Notably, the performance of an agent trained with pure imitation learning is quite bounded by the performance of the expert. In contrast, an algorithm that uses both concepts (RL and imitation learning) jointly might have better chances of outperforming the expert at the end of the training phase [46].\nJoint reinforcement and imitation learning usually means to pre-train an agent first solely on an imitation loss based on the (dis)similarity between the behavior of an expert and the RL agent, and subsequent reward-based training (as in common RL) with an additional imitation loss term. Theoretically, it would be possible to remove the imitation loss after the imitation pre-training phase, but as the authors of [46] argue, this seems to be sub-optimal.\n\u2022 Algorithms without imitation learning listed in Table 1 usually do not use expert trajectories in the learning process of the agent.\nNote that, even though it might be tempting to simply augment a replay buffer in an off-policy algorithm (cf. Section 2.6.3) with expert trajectories, this can have a devastating impact on the training process without further modifications [30] but can also work well [76]. If such augmentation is an option, e.g. because the expert trajectories do already exist and an off-policy algorithm with replay buffer seems to be a good choice as per Table 8, a closer look into the literature about the respective algorithm and/or additional experiments might be required."}, {"title": "2.4. Distributed algorithms", "content": "Considerations for deciding for or against a distributed (not to be confused with distributional) RL algorithm are given in Table 5 and an explanation of this term is provided in the following. The two options to choose from are:\n\u2022 In distributed algorithms, there exist two or more actors, i.e. parallel sub-processes of which each acts in a separate instance of the environment and creates training data, and one or more learners, i.e. parallel sub-processes that are responsible for updating parameters, at training time. The number of learners can be smaller, equal, or larger than the number of actors. Especially if the acquisition of experience is slow, the parallel execution of actor processes can significantly reduce training time since more experience is acquired in less time. Multiple learners allow, for example, for an asynchronous and parallel computation of gradients [72], thus also speeding up training."}, {"title": "2.5. Distributional algorithms", "content": "Considerations to decide for or against a distributional (not to be confused with distributed) algorithm are in Table 6 and related terms are explained in the following.\n\u2022 Distributional approaches estimate the probability distribution of the cumulative reward $\\sum_t r_t$ instead of merely its expected value $E[\\sum_t r_t]$ (i.e. the value function), where rt is the reward at time t. Distributional approaches are usually combined with value-based or actor-critic methods since pure policy-based methods do not learn a value function (and value-based and actor-critic methods do).\n\u2022 Non-distributional approaches only consider the expectation of the cumulative reward as introduced previously in Section 2.8."}, {"title": "2.6. On-policy vs. off-policy learning", "content": "Considerations for deciding between on- and off-policy algorithms are given in Table 8 and explanations of these terms are provided in the following.\n2.6.1. Target policy, behavior policy, test-time policy\nWe define the terms target policy, behavior policy, and test-time policy as follows:\n\u2022 The target policy (at each given timepoint during training) is a policy that is being trained, and from which the behavior policy and the test-time policy are computed as explained below. The term target policy is not to be confused with the term output target (values that a neural network tries to learn to output, but does not output precisely)."}, {"title": "2.6.2. Stochastic vs. deterministic target policy", "content": "Considerations for deciding between a stochastic and a deterministic target policy are given in Table 7 and explanations of these terms are provided in the following.\n\u2022 A stochastic policy is defined by a conditional probability distribution $ \\pi_{\\theta}(a | s)$, where $\\theta$ are the learnable parameters. Given a state s, the action a is sampled from this probability distribution. The probability distribution over actions (conditional, given the state) can differ from state to state.\n\u2022 A deterministic policy is defined by a deterministic function $ \\mu_{\\theta}(s)$ that maps each state s to an action, where $\\theta$ are the learnable parameters. Note that a deterministic policy is a special case of a stochastic policy where $ \\pi_{\\theta}(a | s) = 1$ if $a = \\mu_{\\theta}(s)$."}, {"title": "2.6.3. Definition of on-policy and off-policy learning", "content": "Having the above distinction between target and behavior policy in mind, the difference between on- and off-policy algorithms can be summarized as follows:\n\u2022 In on-policy algorithms, the behavior policy and the target policy are always equal and only experi- ence collected with the current target/behavior policy is used to update the target policy. That is, the experience used for updating (\u201clearning about\u201d) the target policy has been collected using the target policy in its current state (right before the parameter update). After the parameter update, the previ- ously collected experience becomes worthless for the on-policy algorithm since the policy under which the experience has been acquired is then different from the current target policy, and new experience with the updated target policy needs to be collected.\n\u2022 Off-policy algorithms allow the behavior policy to differ from the target policy. In contrast to on- policy methods, experience collected under a behavior policy that was different than the current target policy can be used for updating the parameters of the target policy. A typical example is to increase the amount of exploration by using a behavior policy that has more randomness than the current target policy. Also, the allowed discrepancy between the two policies is often exploited by using a replay buffer, in which experience acquired under the behavior of multiple older versions of the behavior policy is stored. It is also possible to integrate some expert trajectories into the replay buffer, leading to a form of imitation learning, see also Section 2.3. Finally, the experience in a replay buffer can be prioritized such that training samples that seem to be promising for the training process are replayed more frequently than potentially less important ones [94].\nNote that it is no problem to use experience collected under the current policy (\u201con-policy data\u201d) in off- policy algorithms, see, e.g. [76, 53]. In contrast, the opposite (using off-policy data for training in on-policy algorithms) is typically not possible."}, {"title": "2.7. Value-based vs. policy-based vs. actor-critic", "content": "Considerations for the decision between a value-based, policy-based, or actor-critic algorithm can be found in Table 9. If this table indicates a value-based algorithm, Table 11 lists considerations for deciding between the different subcategories based on the size of the state and action spaces. On the other hand, if a policy-based or actor-critic approach is required, Table 10 contains considerations for the decision about a learned critic. The terminology and underlying concepts are explained in the following.\nThe primary goal of RL algorithms for control tasks is to learn a policy $\u03c0(a | s)$ that maximizes the cumulative reward $ \\sum_t r_t$ (while in RL algorithms for prediction tasks, the goal is to learn to predict the outcome of a given policy in terms of rewards for an unknown environment model). In general, RL algorithms for control tasks differ in the way the agent's policy is defined and, therefore, also in the way the policy is learned. The options with respective subcategories are:\n\u2022 Value-based algorithms infer the policy directly from a learned action-value function $Q(s,a) = E(\\sum_t r_{t+1} | S_t = s,a_t = a)$ that assigns an expected value of subsequent cumulative rewards to every possible state-action pair (a more accurate name would be \u201cstate-action-value function\u201d but \u201caction-value function\u201d is widely established). See Section 2.8 for methods to learn such an action-value function. In value-based algorithms, the agent selects an action $a_t = argmax Q(s_t, a)$ at each time step t. Due to the deterministic computation of the argmax, this leads to a primarily deterministic policy (if multiple actions maximize Q at a certain state, one of them may be selected randomly). However, there exist ways to make the actions stochastic (cf. Section 3.1), leading to policies that deviate from argmax Q(st, a) but are still built upon this definition and therefore considered value- based. Value-based algorithms can further be grouped into the following subcategories.\n* Tabular value-based methods define an action value for each possible state-action pair (s, a) in a tabular way, i.e. a table that represents the action-value function Q(s, a). For tabular action- value-based methods (in contrast to non-tabular methods, see below), very good convergence guarantees exist and exact maximization is usually possible [119], [107, Chapters 5,6]. Tabular methods are not related to deep learning and we will not consider them further; a detailed description can be found in [107, Chapter I]. For the sake of completeness, we indicate in Table 9 in which situation a tabular value-based approach might be most appropriate.\n* Non-tabular value-based methods approximate the action-value function with a non-tabular function $Q(s, a)$, where $\\theta$ are trainable parameters. In most cases, $Q_{\\theta}$ is represented by a deep neural network and an action at time t is given by $a_t = argmax_a Q_{\\theta}(s_t, a)$.\nWe further subdivide non-tabular value-based algorithms according to the following categories that are also used by [111]:\n* In value-based methods with exact maximization over actions, the action a is given by the one that exactly maximizes Q(s, a) at the current state s.\n* Value-based methods with approximate maximization and fixed search procedure compute an action that might not maximize the action-value function exactly, using a fixed procedure such as the cross-entropy method [92].\n* Value-based methods with approximate maximization and learned search proce- dure compute an action (that might not maximize the action-value function exactly) using another learned function that proposes a set of promising actions [111]. The idea is to reduce the potentially large set of possible actions to a smaller set of which the best action can be easily determined. Such methods are conceptually similar to actor-critic methods (defined below). The difference is that the executed action is determined by the action-value function from the set of proposals in a value-based method with approximate maximization and a learned search procedure. In contrast, in actor-critic methods, an action proposed by the actor is directly executed without having its action value assessed before.\n\u2022 Policy-based algorithms, on the other hand, parameterize the policy explicitly. That is, they have a set of learnable parameters \u03b8 defining the policy $\u03c0_\u03b8(a | s)$. At each time step t, the agent selects an action $a_t \u223c \u03c0_\u03b8(a | s_t)$. Note that the policy does not necessarily need to be stochastic it can also be a deterministic function a = \u03bc\u0473(s), i.e. \u03c0(\u03b1 | s) = 1 iff a = \u03bc\u0473(s). We ignore this detail for the sake of simplicity unless stated otherwise."}, {"title": "2.9. Entropy regularization", "content": "In Table 13, we provide considerations about when to apply entropy regularization. In the following, we explain the main idea together with various realizations.\nThe well-known dilemma in reinforcement learning is to decide between exploration of new policies (find unexplored actions/states through randomness) and exploitation of already discovered, well-working policies (good performance). In general, this trade-off is addressed by a behavior policy that contains, by its definition, a certain degree of randomness. In practice, a common problem is to \u201ctune\u201d the randomness, i.e. the amount of noise, of the behavior policy in a way that accounts well for the exploration/exploitation trade-off. The concept of entropy regularization or maximum-entropy RL addresses this problem in that it incorporates the policy\u2019s degree of randomness (i.e. how diverse the distribution of proposed actions is) into its training objective. While the standard objective of the policy is to maximize the expected future cumulative reward, the policy\u2019s objective in maximum-entropy RL is to maximize the reward plus an entropy term (representing the policy\u2019s randomness). Furthermore, the entropy objective often prevents early, potentially sub-optimal convergence of the modes of the action distribution [37].\nFor maximum-entropy RL (in the form of the options listed in the bullet list below) to be applicable, the policy (which can be the 'target policy' in policy-based and actor-critic algorithms, or a 'proposal function' in value-based algorithms; for the sake of simplicity, we refer to both of these options with 'policy' here) needs to be parameterized explicitly (not implicitly as in standard Q-learning). We refer to the parameters making up the policy, e.g. the weights of a neural network that parameterizes the action distribution, as 'policy parameters'.\nIn practice, there exist several ways towards maximum-entropy RL; we list the options to choose from in the following.\n\u2022 A per-state entropy regularization term in the objective function of the policy can be used [122, 72]. More precisely, a term of the form $H(\\pi_{\\theta}(\\cdot | s_t))$ can be added to the reward in order to enhance the information entropy H of the policy in the current state $s_t \\in S$.\n\u2022 While the regularization term discussed in the previous bullet point enhances the entropy of the policy at the current state st, soft Q-learning [37, 76] aims at enhancing the entropy of entire policy trajectories, i.e. $\\sum_t H(\\pi_{\\theta}(\\cdot | s_t))$, by considering the expected future cumulative entropy in addition to the observed rewards.\n\u2022 A Kullback-Leibler divergence regularization term in the objective that is minimized with respect to policy parameters, i.e. a term $D_{KL}(\\pi_{\\theta}(\\cdot | S_t) || q(\\cdot))$, where $q(\\cdot)$ can be any probability distribution and $D_{KL}$ denotes the Kullback-Leibler divergence between two probability distributions. In practice, $q(\\cdot)$ can be chosen to be the old action distribution [99], i.e. the policy before updating the parameters according to the objective. This can be interpreted as a relative entropy constraint on the parameter update because strong changes in the entropy of the stochastic policy are penalized and policies are usually initialized to have high entropy at the beginning of training (potentially drifting towards low entropy after some iterations).\n\u2022 Mutual information (MI) between states and latent variables can also be used as an entropy regularizer. In [27], maximization of the MI yields a diverse set of skills in a hierarchical policy. More precisely, in this case, the policy $\u03c0_\u03b8(\u00b7 | st, c)$ depends on the current state st and a hierarchically higher skill (see Section 2.2), represented as latent variable c. While $H(\u03c0_\u03b8(\u00b7 | st))$ is intractable due to the integration over c, the MI given by $H(c) \u2013 H(c | st)$ can be computed. Note that the goal here is to have a unique action associated with a certain latent code (the entropy H(c | st) needs to be reduced to this end), which is different from the goal of learning a diverse set of actions described in the above bullet points; hence, the application of MI regularization to a specific problem probably requires a very detailed examination of the policy (and potentially involved latent variables)."}, {"title": "3. Action-distribution families", "content": "The goal of RL is to find an optimal policy $\u03c0(a | s)$ that maps state s to a probability for action a in a certain environment. In this section, we discuss which action-distribution family to use in the RL algorithm selected from Table 1. While the policy defines a specific behavior of the agent (which is optimized during training), the action-distribution family is the \u201cblueprint\u201d for the policy and needs to be chosen a priori. Mathematically, the action-distribution family is a family of probability distributions over actions, often characterized by a certain shape but without specific parameters (e.g. a Gaussian without having its mean and variance parameters specified). The goal of the RL training process is to find these parameters, i.e. to select/learn the optimal policy from the action-distribution family in the given environment. Hence, the choice of the action-distribution family is important as it pre-defines the possible behavior of the agent during training (behavior-action-distribution family) and the possible result of the training process (target- action-distribution family)."}, {"title": "3.1. Value-based action-distribution families (action-distribution families for value-based algorithms)", "content": "If Table 9 indicates a value-based algorithm, a value-based action-distribution family needs to be chosen. We list options for value-based action-distribution families in Table 14 and explain them in the following. We further describe how they can be parameterized by a NN. Recall from Section 2.7 that the action-value function estimates the expected future cumulative reward, i.e. $Q(s, a) = E(\\sum_t r_t | S_t = s, a_t = a)$.\n\u2022 The greedy value-based action-distribution family comprises all policies that maximize an action- value function in a certain state, i.e. $a_t = argmax_a Q(s_t, a)$. The computation of the argmax can be exact or approximated according to the choice made in Table 11 (value-based variants)."}, {"title": "3.2. Parametric action-distribution families (action-distribution families for policy-based and actor-critic algorithms)", "content": "If Table 9 indicates a policy-based or actor-critic algorithm, a parametric action-distribution family needs to be chosen. We list options for parametric action-distribution families in Table 15. We explain these options, their parameterization with a NN, and related terms in the following. Recall from Section 2.7 that, in policy-based and actor-critic algorithms, the policy \u03c0\u03b8 is parameterized directly with parameters 0 and not inferred from an action-value function (as in value-based RL, see Section 3.1).\n\u2022 The categorical distribution provides probabilities for a discrete set of actions. The NN maps observations to the logits of the action bins. The distribution can be arbitrarily factorized, i.e. assuming conditional independence between arbitrary (subsets of) actions.\n\u2022 The Gaussian distribution is continuous with a single mode. The NN maps observations to the mean and covariance matrix of the Gaussian. The covariance matrix can be restricted (e.g. full, diagonal, state-independent).\n\u2022 The Gaussian mixture distribution is continuous with an arbitrary number of modes (depending on the number of Gaussians that are mixed). The NN maps observations to the means, covariance matrices, and weights of multiple Gaussians. The covariance matrices can be restricted (e.g. full, diagonal, state-independent).\n\u2022 Normalizing flows can represent complex continuous probability distributions with an arbitrary number of modes. The first layers of the NN map observations to the parameters of a simple base distribution (e.g. Gaussian), which is then transformed by a normalizing flow. Many variants exist, e.g. Real-NVP or radial flows.\n\u2022 Stochastic networks and black-box distributions can represent an arbitrary probability distribu- tion. The NN maps observations stochastically to actions, e.g. by using stochastic weights or random noise as an additional input. Many variants can be created by varying the transformation, i.e. by changing the NN architecture that represents this transformation. However, it can be difficult to compute the probability density in closed form.\n\u2022 The tanh function can be used to incorporate boundaries into continuous action-distribution families.\n\u2022 The beta distribution is a bounded probability distribution. The NN maps observations to the parameters of the beta distribution. One of the parameters could be fixed or learned directly rather than output by the NN.\n\u2022 The deterministic action-distribution family comprises all policies that assign a probability of one to exactly one action. The NN maps observations to a single action.\n\u2022 action-distribution families with added noise rely on a deterministic prediction to which they add noise. Compared to other stochastic parametric action-distribution families, the parameters of the noise distribution are usually chosen manually and are not learnable. Popular choices for the distri- bution of the sampled noise are the Ornstein-Uhlenbeck process [110] (yielding temporally correlated noise) or Gaussian distributions (yielding non-correlated noise)."}, {"title": "4. Final selection steps", "content": "If the selection of methods as per the previous sections still leaves us with more than one option to choose from, the following approaches can be recommended for making a final choice:\n\u2022 Run experiments with several of those methods.\n\u2022 Try methods that perform well on similar problems, according to literature (for example literature that introduces recent methods and compares them to other methods, or literature that uses recent methods)."}, {"title": "5. Neural-network architectures", "content": "In this section, we list several examples of neural network (NN) architectures that are frequently im- plemented in RL to approximate a policy and/or one or multiple value functions, i.e. Q, V, and/or A. In addition, we give a brief overview of situations, when those architectures are applied in existent work. Note that, in general, those functions do not necessarily need to be represented by an NN (they can also be tabular or linear functions etc.). Still, neural networks have shown to be best suited for complex and challenging tasks in recent years. Apart from architectural subtleties, the training of deep RL models follows common deep learning practices. Popular references in this regard include [32] and [14].\nFully Connected NNs. Fully connected architectures are usually applied if the state space is rather small, e.g. a set of atomic measurements. Examples: [96, 64].\nTime-Recurrent NNs. Recurrent architectures are usually applied in partially observable Markov decision processes (MDPs) where the current observation does not contain all information about the current state. Time-recurrent NNs have also been shown to perform well for fully observable MDPs. Examples: [54, 23, 111, 71].\nCNNs. Convolutional architectures and/or the attention mechanism are usually applied whenever state observations consist of Euclidean-grid data like images or video sequences, i.e. when feature extraction should be local and equally good regardless of the location of features. Examples: [71, 72, 96, 64]."}, {"title": "6. Training of neural networks in RL", "content": "In this section, we provide a list of approaches that can be applied during NN training to tackle two typical objectives in RL: a stable training process and good test performance of a trained model.\n6.1. Improving stability of the learning process\nA general problem in RL is that the learning process suffers from instability. This means that the performance of an agent is not increasing monotonically during training and different random seeds could lead to entirely different results [44].\nSutton and Barto [107] identified three factors that increase the risk of unstable training, while the effect is most severe if they are all present at a time: The approximation of the action-value function Q (as opposed to tabular approximation) (e.g. through a neural network Q with trainable parameters $ \\theta$ ), bootstrapping (Section 2.8), and off-policy training (Section 2.6.3). They call a combination of these factors the deadly triad and argue that going for an on-policy method in order to remove at least one of the three factors is often a reasonable way to achieve more training stability [107, Chapter 11].\nFurther approaches that have proven beneficial for more stability in the training process and which can be applied to many deep RL algorithms, often independent of the algorithm, include:\n\u2022 Trust regions [96, 123] that constrain the parameter updates and/or entropy regularization (KL regularization can be interpreted as a soft form of trust regions and Soft Q-learning seems to also have a positive impact on training stability [38])\n\u2022 Clipping parts of the objective functions [99]\n\u2022 Clipping the gradients of the policy and/or value function parameters with respect to the objectives (e.g. cumulative reward, entropy)\n\u2022 Target networks [71, 64, 38], i.e. updating parameters used for the computation of bootstrapping tar- gets in one-step or n-step bootstrapping methods (see Section 2.8) less frequently than the parameters that are updated using this objective\n\u2022 Implicit parameterization of stochastic action distributions using the reparameterization trick [26, 50], e.g. used in [38] for a Gaussian distribution\n\u2022 Learned state- or time-dependent baselines in policy objectives, usually related to the cumulative reward [121, 72, 21]\n\u2022 Weight averaging during training [78]"}, {"title": "6.2. Improving test results", "content": "If a working RL algorithm has been chosen for a certain task at hand", "include": "n\u2022 Learning a diverse set of policies: This technique has already been mentioned in Table 5 in the context of distributed learning. However", "5": ".", "motivation": "In the standard RL setting", "105": ".", "4": ".", "59": ".", "augmentation": "While the concept of data augmentation is commonly used in domains like computer vision [102"}]}