{"title": "Creativity in AI: Progresses and Challenges", "authors": ["METE ISMAYILZADA", "DEBJIT PAUL", "ANTOINE BOSSELUT", "LONNEKE VAN DER PLAS"], "abstract": "Creativity is the ability to produce novel, useful, and surprising ideas, and has been widely studied as a crucial aspect of human cognition. Machine creativity on the other hand has been a long-standing challenge. With the rise of advanced generative AI, there has been renewed interest and debate regarding AI's creative capabilities. Therefore, it is imperative to revisit the state of creativity in AI and identify key progresses and remaining challenges. In this work, we survey leading works studying the creative capabilities of AI systems, focusing on creative problem-solving, linguistic, artistic, and scientific creativity. Our review suggests that while the latest AI models are largely capable of producing linguistically and artistically creative outputs such as poems, images, and musical pieces, they struggle with tasks that require creative problem-solving, abstract thinking and compositionality and their generations suffer from a lack of diversity, originality, long-range incoherence and hallucinations. We also discuss key questions concerning copyright and authorship issues with generative models. Furthermore, we highlight the need for a comprehensive evaluation of creativity that is process-driven and considers several dimensions of creativity. Finally, we propose future research directions to improve the creativity of AI outputs, drawing inspiration from cognitive science and psychology.", "sections": [{"title": "1 Introduction", "content": "Creativity, the ability to produce novel, useful, and surprising ideas [Boden 2004], is one of the major hallmarks of human intelligence [Guilford 1967]. Since the invention of the first known general-purpose mechanical computer (known as Analytical Engine) designed by Babbage [Babbage 1837], the question of whether machines can truly think or create anything new has intrigued the scientific community [Newell et al. 1959; Turing 1950; Wang et al. 2024b]. Ada Lovelace, recognized as the first programmer by many, famously stated that the Analytical Engine has no pretensions to originate anything [Lovelace 1843] and Alan Turing, who laid the foundations of computer science, asserted that machines can never take us by surprise [Turing 1950]. Nevertheless, alongside the development of personal computers and advancements in Artificial Intelligence (AI), several symbolic-based and stochastic approaches were developed to endow machines with story generation [Lebowitz 1983; Meehan 1977; Turner 1994; y P\u00e9rez and Sharples 2001], poetry writing [Masterman 1971; Racter 1984] and music composition skills [Brooks et al. 1957; Hiller and Isaacson 1958]. However, these early approaches could not generalize beyond a set of limited domains [Colton et al. 2012; Ji et al. 2020; Yao et al. 2019].\nFast-forward to now, the advent of the Transformer architecture [Vaswani et al. 2017] and the development of large language models (LLMs) [Zhao et al. 2023c] in the past decade ushered a new age of intelligent systems with remarkable generative, reasoning, coding, mathematical and"}, {"title": "2 Creativity", "content": "While creativity as a concept seems intuitively easy to understand on the surface, there is still no consensus on what constitutes true creativity. This is primarily due to the subjective nature of creativity, as what is deemed novel and of quality can vary significantly across cultures, disciplines, and periods. Aleinikov et al. [2000] lists more than 100 proposed definitions, and the number keeps growing. Despite the lack of global consensus, there is one definition of creativity that has seen wide adoption by many philosophers and psychologists and has been dubbed as the \u201cstandard definition\" [Barron 1955; Runco and Jaeger 2012; Stein 1953]. According to this definition, creativity requires novelty (a.k.a originality, uniqueness, etc.) and value (a.k.a utility, effectiveness, usefulness, appropriateness, relevance, meaningfulness, etc.).\nThe novelty criterion is typically self-explanatory to the point that people equate it to creativity in everyday life. This is generally known as intrapersonal or personal creativity (often denoted as P-creativity), i.e. the product is novel within the frame of a person's life [Boden 2004; Stein 1953; Weisberg 1986]. Researchers distinguish it from the interpersonal or historical creativity (often denoted as H-creativity), i.e. the product is novel with respect to the entire history of people such as Einstein's general relativity theory.\nMany theorists have argued that novelty is insufficient for creativity, and value dimension is needed to filter out original nonsense, such as something generated by a truly random process. While value is generally understood as something inherently \u201cgood\u201d for the respective audience, there appears to be such a thing as malevolent or \u201cdark\u201d creativity. For instance, one can be creative in producing torture instruments or in committing terrorist atrocities [Gaut 2010]. Therefore, the interpretation of the value of a product as being \u201ceffective\u201d towards its intended end, regardless of whether that end is morally good or bad, has been suggested as a better alternative [Livingston"}, {"title": "2.1 Definitions", "content": "While creativity as a concept seems intuitively easy to understand on the surface, there is still no consensus on what constitutes true creativity. This is primarily due to the subjective nature of creativity, as what is deemed novel and of quality can vary significantly across cultures, disciplines, and periods. Aleinikov et al. [2000] lists more than 100 proposed definitions, and the number keeps growing. Despite the lack of global consensus, there is one definition of creativity that has seen wide adoption by many philosophers and psychologists and has been dubbed as the \u201cstandard definition\" [Barron 1955; Runco and Jaeger 2012; Stein 1953]. According to this definition, creativity requires novelty (a.k.a originality, uniqueness, etc.) and value (a.k.a utility, effectiveness, usefulness, appropriateness, relevance, meaningfulness, etc.).\nThe novelty criterion is typically self-explanatory to the point that people equate it to creativity in everyday life. This is generally known as intrapersonal or personal creativity (often denoted as P-creativity), i.e. the product is novel within the frame of a person's life [Boden 2004; Stein 1953; Weisberg 1986]. Researchers distinguish it from the interpersonal or historical creativity (often denoted as H-creativity), i.e. the product is novel with respect to the entire history of people such as Einstein's general relativity theory.\nMany theorists have argued that novelty is insufficient for creativity, and value dimension is needed to filter out original nonsense, such as something generated by a truly random process. While value is generally understood as something inherently \u201cgood\u201d for the respective audience, there appears to be such a thing as malevolent or \u201cdark\u201d creativity. For instance, one can be creative in producing torture instruments or in committing terrorist atrocities [Gaut 2010]. Therefore, the interpretation of the value of a product as being \u201ceffective\u201d towards its intended end, regardless of whether that end is morally good or bad, has been suggested as a better alternative [Livingston"}, {"title": "2.2 Types", "content": "While creativity manifests itself in various forms across domains, even within a particular domain, different types of creativity can be distinguished based on the difficulty level of the inherent creative process involved. Four-C model of creativity is one such popular theory that differentiates between four types of creativity corresponding to four levels of difficulty involved in producing creative artifacts [Kaufman and Beghetto 2009]. The first major type of creativity is known as little-c creativity which is what we find in everyday life as solutions to minor problems. Examples might include combining unusual ingredients to make a new type of meal or using a hand-held vacuum cleaner on the ceiling to remove flies. Almost everyone possesses this type of creativity in one way or another. The second main type of creativity in this model is the Big-C creativity that includes major works of scientific, technological, social, or artistic importance. Examples could be Darwin's theory of evolution, the invention of the printing press, or Leonardo Da Vinci's painting of the Mona Lisa. In addition to these two major categories, Kaufman and Beghetto [2009] also defines two minor categories of creativity. First is the mini-c creativity for very small-scale cases of"}, {"title": "2.3 Evaluation", "content": "Evaluating creativity remains a challenging task in artificial intelligence due to its inherently subjective nature [Lamb et al. 2018]. Interestingly, some research work even argued against the quantitative evaluation of creativity, suggesting it is either too domain-specific to be measured effectively [Baer 2012], or that creativity is an inherently human trait that cannot be accurately modeled computationally [Boden 1991; Minsky 1982]. However, an overwhelming majority of the scientific community favors the possibility of computational modeling and evaluation of creativity [Veale and Cardoso 2019]. Hence, numerous evaluation methods have been proposed in the past [Lamb et al. 2018]. However, most of the proposed metrics are either formal frameworks that are hard to implement in practice or manual psychometric creativity tests that require costly human involvement [Kim 2006] or automated metrics that are too domain-specific [Fran\u00e7a et al. 2016]. We refer the reader to Franceschelli and Musolesi [2021b] and Lamb et al. [2018] for more details on formal evaluation frameworks, and here we briefly summarize some of the relevant manual and automated metrics for creativity."}, {"title": "2.3.1 Manual Evaluation", "content": "Since creative products vary greatly in their forms and are hard to characterize with objective measures, the simplest and most common way to evaluate them is to ask other humans to manually rate them based on some criteria associated with creativity, which differs from task to task [Lamb et al. 2015]. For example, in story generation, humans are typically asked to rate a generated story on aspects such as interestingness, coherence, relevance, humanlikeness and etc. [Goldfarb-Tarrant et al. 2020; Rashkin et al. 2020; Yang et al. 2022]. In other tasks where the goal is to produce multiple responses such as common psychometric creativity tests Alternative Uses Task (AUT) [Guilford 1967] and Torrance Tests of Creative Thinking (TTCT) [Torrance 1974], evaluation is centered around four dimensions of creativity: fluency (the total number of meaningful, and relevant ideas generated in response to the stimulus), flexibility (the number of different categories of relevant responses), originality (the uniqueness or rarity of responses) and elaboration (the amount of detail in the responses).\nWhile it is common and straightforward to conduct human evaluation with ordinary humans, some have argued that people who are not experts on a kind of creative artifact might not be good judges of those artifacts [Gerv\u00e1s 2019; Lamb et al. 2015, 2018; Mirowski et al. 2022; Veale 2015]. This typically results in poor interrater reliability and even when they agree, their judgments do"}, {"title": "2.3.2 Automated Evaluation", "content": "While creativity is generally evaluated by humans, several attempts have also been made to devise automated measures of it [Cook and Colton 2015; Fran\u00e7a et al. 2016; Jordanous et al. 2015; Maher and Fisher 2012]. These measures often target a specific dimension of creativity. Below, we review some automated measures for three dimensions of creativity: novelty, value, and surprise.\nNovelty. It is typically defined as the measure of how different an artifact is from other known artifacts in its class [Maher 2010]. Then a distance metric is established to quantify this difference based on the attributes of the artifact and the task space. For example, in the text generation task, a notion of semantic distance is commonly employed as a distance measure [Beaty and Johnson 2020; Dunbar and Forster 2009; Harbison and Haarmann 2014; Johnson et al. 2022; Prabhakaran et al. 2013]. More specifically, the text is embedded into a vector in semantic space and some distance or dissimilarity metric (e.g. typically 1-cosine_similarity) is used to compute how much semantically different is one text from another. However, the granularity of the text can differ from task to task. For example, in the story generation task, Karampiperis et al. [2014] defines the novelty of a story as the average semantic distance between the dominant terms included in the textual representation of the story, compared to the average semantic distance of the dominant terms in all stories where distance is measured based on the embeddings of terms.\nNovelty can also be characterized by the degree an artifact differs from the previously produced works that one has already seen [Elgammal and Saleh 2015; Gunkle and Berlyne 1975]. This definition has inspired the development of Creative Adversarial Networks (CANs) [Elgammal et al. 2017] similar to the popular Generative Adversarial Networks (GANs) [Goodfellow et al. 2014a]. In CANs, the generator tries to fool the discriminator into thinking its generation is \u201cart\u201d and at the same time, the style of its generation is nothing known to the discriminator. Consequently, the score assigned by the discriminator (more specifically, 1-score) can be used to measure the novelty of the generated artifact as suggested by Franceschelli and Musolesi [2022].\nValue. This dimension is generally the hardest to evaluate as it depends on the subjective utility or performance of the artifact which is typically judged by domain experts of that artifact and can radically change across domains [Maher 2010]. In visual arts, this might correspond to \u201cbeauty\u201d, whereas in science to \u201clogical correctness\u201d. Therefore, a metric appropriate for its domain should be employed. For example, in open-ended story generation, a minimally useful story can be defined as a relevant, coherent, and meaningful story. In this sense, automated metrics measuring the overall quality of a story can be leveraged [Chen et al. 2022b; Guan and Huang 2020; Xie et al. 2023a], however, it is often challenging to measure coherence [Laban et al. 2021; Zhao et al. 2023b]. Another more general evaluation of utility has been suggested by Franceschelli and Musolesi [2022] based on the discriminator score in GANS.\nSurprise. Also known as unexpectedness, surprise measures the artifact's degree of deviation from what is expected [Maher 2010]. Therefore, automatic metrics for surprise tend to be information-theoretic [Bunescu and Uduehi 2022; Kuznetsova et al. 2013] and estimate the violation of expectation based on uncertainty reduction [Frank 2010; Hale 2006]. However, semantic distance-based measures of surprise have also been suggested. For example, in the story generation task, Karampiperis et al. [2014] conceptualizes surprise as the average semantic distances between the consecutive fragments of a given story. Recent work has also suggested an automated measure based on the Bayesian theory of surprise [Baldi and Itti 2010; Franceschelli and Musolesi 2022]."}, {"title": "3 Linguistic Creativity", "content": "The creative aspect of language in linguistics has been discussed since the early days [Chomsky 1965]. Chomsky, in this paper, attributes creativity mainly to the essential property of language to provide means to express many thoughts indefinitely. However, several linguists since Chomsky have argued against using this characterization since it does not align with the everyday definition of creativity [Bergs 2019; Sampson 2017; Zawada 2006]. Chomsky's theory of grammar might generate an infinite number of sentences; it, however, relies on a fixed set of rules, while creativity requires deviation from rules. In this sense, Sampson [2017] suggests distinguishing between F-creativity (fixed) and E-creativity (extending), where F-creativity refers to the Chomskian interpretation of linguistic creativity (a.k.a productivity in morphology) and E-creativity corresponds to the real linguistic innovation such as metaphors, jokes, neologisms, etc. Some recent works have explored the F-creativity of large language models and found that this task is challenging in general and even harder in more morphological complex languages [Anh et al. 2024; Ismayilzada et al. 2024; Weissweiler et al. 2023]. Most past works however have focused on studying the E-creativity of AI systems which we review in the following sections."}, {"title": "3.1 Humor", "content": "Humor is one of the most common ways humans creatively use language to express their ideas and feelings. Early works to model humor focused on hand-crafted linguistic templates and wordplay [Raskin and Attardo 1994; Stock and Strapparava 2005; Taylor and Mazlack 2004]. Subsequent works have leveraged language's lexical and syntactic properties as humor-specific features for humor detection [Liu et al. 2018b; Yang et al. 2015; Zhang and Liu 2014]. The growing interest in computational humor in recent years has resulted in several shared tasks organized by the NLP community [Castro et al. 2018; Hossain et al. 2020a; Meaney et al. 2021; Miller et al. 2017; Potash et al. 2017; Van Hee et al. 2018]. Latest works have developed methods based on neural networks and language models to generate and detect humorous content [Amin and Burghardt 2020; Annamoradnejad and Zoghi 2020; Arora et al. 2022; Bertero and Fung 2016; Chen and Soo 2018; Hossain et al. 2019; Peyrard et al. 2021; Ravi et al. 2024; Ziser et al. 2020], jokes [Horvitz et al. 2024; Ren and Yang 2017; Tang et al. 2022; Weller and Seppi 2019; Xie et al. 2021], puns [He et al. 2019; Mittal et al. 2022; Yu et al. 2018], and sarcasm [Chakrabarty et al. 2020a, 2022c].\nSeveral datasets have also been proposed to benchmark the humor capacity of LLMs in several languages including English [Horvitz et al. 2024; Hossain et al. 2019, 2020b; Jain et al. 2024; Meaney et al. 2021; Miller et al. 2017; Tang et al. 2022], Chinese [Zhang et al. 2019b], Italian [Buscaldi and Rosso 2007], Spanish [Castro et al. 2017], Dutch [Winters and Delobelle 2020] and Russian [Blinov et al. 2019]. Computational humor has also been explored in multimodal settings involving images, audio, and video in addition to text [Bertero and Fung 2016; Hasan et al. 2019; Hessel et al. 2023a; Radev et al. 2016; Shahaf et al. 2015; Xie et al. 2023b]. While the latest methods particularly LLMs show an impressive ability to generate and detect humorous content, recent work has also shown that these models still fail to reliably understand humor [Borji 2023a; G\u00f3es et al. 2023a; Hessel et al. 2023a; Koco\u0144 et al. 2023] and generated jokes typically lack diversity [Jentzsch and Kersting 2023] which has been attributed to training on less diverse humor datasets [Baranov et al. 2023]. Creative training frameworks have also been developed to improve the humor generation capabilities of LLMs [Zhong et al. 2023]. We refer the reader to Amin and Burghardt [2020] for an in-depth survey on computational humor."}, {"title": "3.2 Figurative Language", "content": "Figurative language is a term in language studies encompassing various figures of speech like hyperbole, similes and metaphor [Paul 1970; Roberts and Kreuz 1994; Veale et al. 2016]. These elements can be used to achieve a range of communicative goals. Figurative language generation involves transforming a text into a specific figure of speech while maintaining the original meaning [Lai and Nissim 2024]. Generating figurative language requires an understanding of abstract concepts, commonsense reasoning, and an ability to make analogies and deviate from literal meaning. Recent works have shown that language models with injected commonsense knowledge can generate textual and visual metaphors [Chakrabarty et al. 2023f, 2021b], similes [Chakrabarty et al. 2020b; He et al. 2023], idioms [Chakrabarty et al. 2021a] and hyperboles [Tian et al. 2021]. Chakrabarty et al. [2023a] reveals that metaphors generated by large language models are often incoherent or cliched. Chakrabarty et al. [2023a] highlights the following example of such a metaphor generated by an LLM:\u201c- However, she managed to laugh louder and louder until her laughter transformed into an embrace of the sun's atmosphere.\u201d We refer the reader to Lai and Nissim [2024] and Abulaish et al. [2020] for an in-depth survey on the automatic generation and detection of figurative language."}, {"title": "3.3 Lexical Innovation", "content": "Understanding and generating novel words or word compounds is a challenging linguistic task that often requires creativity, commonsense knowledge, and an ability to generalize over seen concepts [Costello and Keane 2000; Wisniewski 1997]. Similar noun compounds might have different meanings based on our common understanding. For example, knowing that \u201cchocolate"}, {"title": "4 Creative Problem-Solving", "content": "Creative problem-solving is the mental process of searching and coming up with creative solutions to a given problem [Duncker and Lees 1948]. It is a challenging task for machines as it not only requires creativity but also commonsense reasoning, and compositional generalization [Davidson et al. 2022]. In addition, creatively solving a problem is usually characterized by two kinds of thinking, namely, convergent and divergent thinking, and involves deep abstraction and analogy-making abilities."}, {"title": "4.1 Convergent Thinking", "content": "Convergent thinking models creativity in terms of an ability to produce a single optimal solution for a given problem [Guilford 1967]. This type of creativity requires one to be able to associate seemingly remote ideas and converge to a unified solution. To evaluate this type of thinking in humans, psychologists have come up with several creativity tests such as Remote Associates Test (RAT) [Mednick 1962] and insight problems [Webb et al. 2017]. For example, the goal in RAT is to connect several unrelated words with one concept, e.g. words \u201cbroken\u201d, \u201cclear\u201d and \u201ceye\u201d can be connected with the word \u201cglass\u201d.\nLanguage models have recently been evaluated on problems that require convergent thinking. Lin et al. [2021] tests language models on solving riddles that require creativity and commonsense and finds a significant gap between model and human performance. Naeini et al. [2023] uses the popular British quiz show Only Connect's Connecting Wall that mimics RAT formulation with built-in, deliberate red herrings (i.e. misleading stimuli or distractors) and evaluates large language models such as GPT-4 on these problems. They report poor model performance and show that models are highly susceptible to distractors in the input and manifest a form of fixation effect (a.k.a functional fixedness or Einstellung effect). This type of cognitive bias forces the model to fixate on its past knowledge and prevents it from thinking \u201cout-of-the-box\u201d. The same effect is also found when models are evaluated on everyday problems involving unconventional use of objects [Tian et al. 2023]. Very recently, large language models such as GPT-40 have been evaluated on the popular New York Times game Connections and have been found to struggle with associating encyclopedic and linguistic knowledge at an abstract level [Samadarshi et al. 2024]."}, {"title": "4.2 Divergent Thinking", "content": "Divergent thinking requires one to conceptualize multiple, often seemingly disconnected ideas [Guilford 1967]. It essentially plays the opposite role to convergent thinking and therefore, the goal is to start with a unified idea and diverge from this idea into the space of all ideas to find the ones that are relevant to the task at hand. Psychologists have also devised creativity tests to evaluate humans' divergent thinking abilities, such as Alternate Uses Test (AUT) [Guilford 1967] and Torrance Tests of Creative Thinking (TTCT). AUT tests creativity based on whether the participant can come up with unusual (creative) uses for an everyday object and the results are typically evaluated either manually or using semantic distance. For example, a \u201cbrick\u201d can be used as a \u201cpaperweight\u201d or \u201cto break a window\u201d and \u201ccoffee cup\u201d can be used as \u201csmall bowl\u201d, or \u201ca hat for an elf", "thinking out-of-the-box": "For example, recent works have found that defying default commonsense associations and modeling unexpected or unlikely situations are challenging for large language models [Jiang et al. 2023; Tian et al. 2023; Zhao et al. 2023a]. Figure 4a illustrates a creative problem-solving example from Tian et al. [2023] that involves unconventional use of everyday objects."}, {"title": "4.3 Abstraction and Analogy-Making", "content": "Conceptual abstraction and analogy-making lie at the core of human cognition and intelligence [Chollet 2019; Hofstadter 2001; Mitchell 2021]. These are abilities that enable humans to generalize to new domains, invent novel concepts, and make useful and often surprising connections between concepts. In other words, abstraction and analogy-making serve as foundational building blocks for creative thinking."}, {"title": "4.3.1 Abstraction", "content": "As the cornerstone of human intelligence, abstraction, and abstract reasoning are typically evaluated using visual IQ tests in humans. Popular examples of these tests are RAVEN progressive matrices [Raven 1938], Bongard problems [Bongard 1970] and recently introduced Kandinsky Patterns [Holzinger et al. 2019], Abstraction and Reasoning Corpus (ARC) [Chollet 2019] and its variations [Moskvichev et al. 2023]. These tests require the participants to identify and complete an abstract visual pattern based on given examples. Although several attempts have been made to solve these tasks using both symbolic-based and neural network-driven approaches [Hu et al. 2023; Lorello et al. 2024; Mirchandani et al. 2023; Santoro et al. 2018; Xu et al. 2022], modern AI systems still struggle with solving RAVEN-like [Ahrabian et al. 2024; Gendron et al. 2023; Odouard and Mitchell 2022; Zhang et al. 2019a] and ARC-like tasks [Mitchell et al. 2023; Moskvichev et al. 2023; Odouard and Mitchell 2022; Xu et al. 2023; Zhang et al. 2021]. Analysis of abstraction via a serial reproduction task [Langlois et al. 2021] where participants are asked to produce a textual stimulus for the next participant upon observing a visual stimulus and vice versa, has suggested that GPT-4 unlike humans relies heavily on linguistic representations even in vision-only paradigm [Kumar et al. 2024]. Figure 5 illustrates an example from the ARC task [Chollet 2019]. The problems in this corpus are quite hard to solve to the extent that this task has been recognized as the de facto benchmark for measuring progress towards Artificial General Intelligence (AGI) and a public competition with a grand prize of $1, 000, 000 has recently been launched\u00b2. At the time of writing this paper, the highest score is 49.5% far from from the passing threshold of 85% (human-level)."}, {"title": "4.3.2 Analogy-Making", "content": "In its basic form, analogy-making is the ability to identify a relation between two concepts and apply it to a new concept. For example, Paris is to France as Tokyo is to Japan (i.e. capital:country relation). Early approaches to computational analogy-making were symbolic-based and required extensive hand-coded input i.e. structured representations of both the entities and their relations [Falkenhainer et al. 1989; Gentner 1983; Turney 2008]. Later, word embedding models based on neural networks were shown to exhibit analogy-making abilities at the word level [Gladkova et al. 2016; Jacob et al. 2023; Marquer et al. 2022; Mikolov et al. 2013; Petersen and van der Plas 2023; Ushio et al. 2021]. However, word-level analogies are generally simple and do not reflect the typical analogical reasoning humans perform in everyday life about complex situations.\nWhile some works have argued for emergent analogical reasoning abilities of large language models [Hu et al. 2023; Webb et al. 2022; Yasunaga et al. 2023], other works have shown that these models lack the robustness and generality exhibited by humans when it comes to long text analogies [Wijesiriwardene et al. 2023; Zhu and de Melo 2020], scientific analogies [Czinczoll et al. 2022; Yuan et al. 2023], story analogies [Jiayang et al. 2023; Nagarajah et al. 2022; Sourati et al. 2023; Sultan and Shahaf 2022], visual analogies [Bitton et al. 2022; Opielka et al. 2024; Zhang et al. 2019a] and complex analogical reasoning [Lewis and Mitchell 2024; Musker et al. 2024]. Figure 4b illustrates an example from the analogical reasoning over narratives benchmark [Sourati et al. 2023]."}, {"title": "5 Artistic Creativity", "content": "Artistic creativity is the ability to produce original, imaginative, and expressive works in various art forms, such as creative writing, poetry, visual arts, music, dance, theater, and more. In this section, we will focus on the advancements made in AI to produce creative stories, poetry, visual, and musical content automatically and also point out the remaining challenges."}, {"title": "5.1 Story Generation", "content": "Storytelling is at the heart of human communication, a powerful tool for connecting and conveying ideas effectively [Suzuki et al. 2018]. It requires creativity, particularly when crafting an engaging and compelling narrative. Early approaches to this task focused on algorithmic planning based on character traits and social and physical constraints [Lebowitz 1984; Meehan 1977]. With the advent of powerful neural networks, the focus shifted to machine learning-based data-driven approaches [Akoury et al. 2020; Du and Chilton 2023; Fan et al. 2018; Hong et al. 2023; Louis and Sutton 2018].\nWhile these networks are trained on large datasets of stories and prompted to directly generate a new story, often producing locally coherent narratives, they suffer from long-term coherence, irrelevance to premise, and repetitive text problems [Yao et al. 2019]. Latest approaches have addressed these problems by using content planning and recursive prompting techniques where a high-level plan of the story is first generated, followed by iterative prompting that aims to generate the story in multiple steps based on the plan [Goldfarb-Tarrant et al. 2020; Yang et al. 2022; Yao et al. 2019]. Since language models are designed for open-ended text generation, controlling the attributes of its generations (e.g. topic, characters) is another major challenge [Dathathri et al. 2019]. While several methods have been developed towards controllable text generation [Chung et al. 2022; Dathathri et al. 2019; Pascual et al. 2021; Paul and Frank 2021; Rashkin et al. 2020; Tambwekar et al. 2018], language models still struggle with following constraints [Sun et al. 2023]. In addition, long-term factual inconsistency and hallucinations still remain as major issues in language model generated texts [Banerjee et al. 2024; Elazar et al. 2021; Tam et al. 2022; Zhang et al. 2023].\nLanguage models have also been evaluated on their ability to produce and judge creative content as a professional writer. Chakrabarty et al. [2023b] generates short stories from LLMs based on the plots of popular fictional stories published in the New York Times and conducts a fine-grained"}, {"title": "5.2 Poetry", "content": "Poetry is a form of literary expression that uses rhythmic and often condensed creative language to evoke emotions, convey ideas, or tell stories. Early approaches to poetry generation have been based on hand-crafted templates, heuristics, and linguistic features of"}]}