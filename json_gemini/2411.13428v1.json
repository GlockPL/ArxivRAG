{"title": "SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers", "authors": ["Hojjat Karami", "David Atienza", "Anisoara Ionescu"], "abstract": "Generating synthetic Electronic Health Records (EHRs) offers significant potential for data augmentation, privacy-preserving data sharing, and improving machine learning model training. We propose a novel tokenization strategy tailored for structured EHR data, which encompasses diverse data types such as covariates, ICD codes, and irregularly sampled time series. Using a GPT-like decoder-only transformer model, we demonstrate the generation of high-quality synthetic EHRs. Our approach is evaluated using the MIMIC-III dataset, and we benchmark the fidelity, utility, and privacy of the generated data against state-of-the-art models.", "sections": [{"title": "Introduction", "content": "Electronic Health Records (EHRs) are comprehensive digital repositories of patient health information, encompassing a wide range of data types. In in-patient settings, EHRs include structured data such as demographics and International Classification of Diseases (ICD) codes, time series data (e.g., vital signs and lab results), and unstructured data (e.g., clinical notes and radiology images). These records provide an invaluable resource for developing machine learning models, offering rich datasets that can be harnessed to enhance patient care, predict outcomes, and support clinical decision-making.\nAccess to real-world EHRs is often limited due to privacy regulations such as HIPAA and GDPR, as well as technical challenges and a lack of incentives for data sharing. Even pseudo-anonymized data remains susceptible to re-identification by malicious actors. Techniques like federated learning [1] and differential privacy [2] are being explored to mitigate these risks. Synthetic data provides a promising alternative by replicating the statistical properties of real data without being tied to actual individuals, thereby circumventing privacy regulations. It can be securely shared among stakeholders, facilitating hypothesis generation, AI model pre-training, and data exploration without exposing sensitive information. This approach not only alleviates privacy concerns but also reduces the effort involved in accessing real data, allowing researchers to focus on validating more probable hypotheses before turning to original datasets for confirmation.\nGenerating synthetic EHR data presents several challenges. EHRs comprise multiple data types with distinct characteristics. For instance, clinical events like ICD codes are high-dimensional and sequential, while time series data are relatively low-dimensional but feature irregular sampling and significant missingness, which is often informative and not random [3, 4]. Additionally, patients may have multiple visits or admissions, with data across visits being correlated. While many studies focus on generating single data types, such as discrete ICD codes ([5, 6, 7, 8]) or time series data ([9, 10, 11]), few addresses the generation of multiple data types across multiple visits simultaneously ([12]).\nIn the era of Large Language Models (LLMs), many have been adapted to the clinical domain for tasks such as diagnosis prediction [13], text embeddings [14], and medical reasoning [15, 16]. While these models can interpret structured data and numerical values presented in text format, they are not directly applicable for generating synthetic structured EHR data. This limitation arises because LLMs are typically not trained on large structured EHR datasets, particularly those containing complex numerical data.\nIn this work, we present SynEHRgy, a framework designed to Synthesize mixed-type structured EHRs, including covariates, ICD codes, and irregularly-sampled time series across multiple patient visits. We introduce a novel tokenization strategy for structured EHR data, particularly for numerical variables, allowing decoder-only transformer models to capture underlying data patterns within a causal language modeling framework and generate high-quality synthetic EHRs. Using a small GPT model, we demonstrate the efficacy of our approach on the MIMIC-III dataset [17], comparing the generated data's quality against state-of-the-art models in terms of fidelity, utility, and privacy. The key contributions of this work are as follows:\n\u2022 We propose a novel tokenization strategy for mixed-type structured EHR data (covariates, ICD codes, irregularly-sampled time series) across multiple visits.\n\u2022 We empirically demonstrate that a GPT-like decoder-only transformer model can generate high-quality structured EHR data.\n\u2022 We conduct a comprehensive evaluation of the proposed methodology on the MIMIC-III dataset, comparing the generated data's quality to state-of-the-art models in terms of fidelity, utility, and privacy.\n\u2022 Our framework shows exceptional performance in generating irregularly-sampled time series data, a challenging task due to high missingness and irregular time points."}, {"title": "Related Works", "content": "Generating clinical events, particularly ICD codes, has garnered significant research interest in recent years. In contrast, generating clinical time series presents a greater challenge due to irregularly- sampled time points and substantial missingness.\nICD Code Generation A substantial body of work focuses on generating ICD codes using various frameworks. Generative Adversarial Networks (GANs) have been employed for this purpose ([18, 6, 19, 20]), as have Variational Autoencoders (VAEs) ([21]) and diffusion models ([22, 23, 24, 25]). Additionally, Large Language Models (LLMs) have been explored due to the sequential nature of ICD codes and the feasibility of straightforward tokenization. For instance, PromptEHR [8] employs an encoder-decoder transformer model with prompt learning to generate ICD codes conditioned on numerical and categorical demographic features. HALO [12] utilizes a transformer encoder coupled with a linear autoregressive module for generating ICD codes at both the visit and code levels. CEHR-GPT [26] leverages GPT to produce sequences of visits based on demographic prompts and visit time intervals.\nTime Series Generation Research on time series generation has predominantly focused on regularly-sampled time series, which are more common across various domains. Techniques such as Generative Adversarial Networks (GANs) have been widely applied ([27, 28, 29, 30, 31]), as well as diffusion models ([32, 33, 34]). However, generating irregularly-sampled time series is less explored. Notable efforts in this area include RTSGAN [9], EHR-Safe [31], and TimEHR [11] using GANs, and TS-Diffusion [35] and TimeDiff [10] using diffusion models. Additionally, research on leveraging language models for time series generation remains limited. While pre-trained LLMs have demonstrated effectiveness with numerical values in text format-e.g., LIFT [36] and GReaT [37] excel in tabular data generation\u2014they primarily treat numerical values as text. Some approaches propose defining specific numerical tokens and training LLMs from scratch. For example, CodeAR [38] utilizes a Vector Quantized Autoencoder (VQ-VAE) for codebook creation, followed by an LSTM with next-token prediction loss. Chronos [39] discretizes numerical values into bins for time series forecasting using encoder-decoder transformers, a strategy also employed by HALO [12] for clinical time series and by [40] for financial time series. To the best of our knowledge, no existing work, other than HALO [12], explores the generation of irregularly-sampled time series using LLMs."}, {"title": "Methodology", "content": ""}, {"title": "Problem Formulation", "content": "An EHR dataset can be characterized as $D = \\{P_i\\}_{i=1}^N$, where N denotes the number of patients and $P_i$ represents the data for the i-th patient. Each patient may have one or more visits, denoted as $P_i = \\{V_{ij}\\}$, with each visit represented as $V_j = \\{c_j, y_j, E_j, T_j\\}$. Here, $c_j$ denotes the set of covariates (e.g., demographics), yj represents the set of clinical outcomes (e.g., in-hospital mortality), $E_j = \\{e_m\\}_{m=1}^{|E_j|}$ is a sequence of clinical events such as ICD codes, and $T_j$ is the time series data, including vital signs and laboratory variables. The time series data for each visit is represented as\n$T_j = \\{t_k, \\{(n_{kp}, x_{kp})\\}_{p=1}^{L_k}\\}_{k=1}^{T_j}$, where $T_j$ denotes the total number of measurement times, and Lk indicates the number of available data points per measurement. In this representation, $t_k \\in \\mathbb{R}_{\\geq 0}$ is the timestamp of the k-th measurement, while nkp and xkp are the variable name and value pairs."}, {"title": "Tokenization", "content": "For numerical values, we employ uniform quantization to discretize numerical variables into equal- sized bins, similar to Chronos [39]. Each bin is assigned a unique token. For instance, the token represents heart rate values in the range of 55-66 beats per minute. For timestamps, we compute the time intervals between consecutive measurements and discretize these intervals into bins. For non-numerical values such as ICD codes and categorical variables, each code or category is assigned a unique token.\nWe also use special tokens to delineate different sections of the data: and for the start and end of a sequence, respectively; , , , and  to signify the end of covariates, labels, time series, and admissions. The padding token () is employed to ensure that all tokenized sequences are of uniform length. If a patient has multiple admissions, the data for new admissions is appended after the token . An example of tokenized patient data is illustrated on the right side of Figure 1.\nThis tokenization strategy offers several advantages: First, it is scalable with respect to the number of variables. For instance, when integrating a new dataset with additional variables, we simply update the tokenization dictionary by incorporating new tokens. Second, numerical tokenization is more efficient than treating numerical values as text. For example, the GPT-2 tokenizer would generate seven tokens for 'Lactate:2.5', whereas our method generates only one token."}, {"title": "Model Architecture", "content": "Decoder-only Transformer models, such as the ones used in language modeling (e.g., GPT [41]), typically employ a causal language modeling (CLM) objective as their loss function. This objective is also known as autoregressive language modeling. The core idea is to predict the next token in a sequence given all previous tokens. The model generates a probability distribution over the vocabulary for the next token, and the loss function measures the discrepancy between the predicted distribution and the actual next token. For an tokenized patient sequence $x = [x_1,x_2,..., x_T]$, the model predicts the probability distribution over the vocabulary for each token in the sequence, conditioned on all previous tokens:\n$p(x_t | x_1,x_2,...,x_{t-1})$"}, {"title": "EHR Data Generation", "content": "The generation of EHR data follows a process similar to text generation. We initiate the generation with the start token () and continue generating tokens sequentially until the end token () is produced. For the de-tokenization of numerical tokens, we apply uniform sampling within the specified bin range to recover the original numerical values. This approach ensures that the generated values are representative of the underlying distribution of the data. The de-tokenization of ICD codes and categorical variables is straightforward, as each token directly maps to a specific code or category.\nThis method offers several advantages: First, it leverages the well-established text generation frame- work, which allows for seamless integration with existing models and techniques. Second, uniform sampling for numerical values maintains the integrity of the data distribution, providing a realis- tic representation of the original values. Lastly, the straightforward de-tokenization of categorical variables ensures simplicity and efficiency in generating accurate categorical data."}, {"title": "Experiments", "content": ""}, {"title": "Dataset", "content": "We utilize the MIMIC-III dataset for our experiments [17]. Specifically, we use the preprocessing pipeline provided by [42] to extract data from approximately 42,000 patients with multiple hospital visits. The dataset is divided into training, validation, and test sets using a 70-15-15 split. For each patient, we include age and gender as covariates. Additionally, for each visit, we select 25 phenotype labels along with an in-hospital mortality label. We include ICD diagnosis and procedure codes with a frequency greater than five across the entire dataset, resulting in 4,656 unique ICD codes. We also select 41 time series from vital signs and laboratory variables, including five categorical time series. After tokenization, we have a total of 5,127 unique tokens, with the training split consisting of 29.6 million tokens. Each patient's sequence starts with the  token and ends with the  token, comprising demographics, ICD codes, and time series tokens, as explained in Section 3.2."}, {"title": "Evaluation Metrics", "content": "We evaluate the synthetic data based on Fidelity, Utility, and Privacy. Since all of our baselines, except HALO, do not generate multiple data types, we assess our metrics separately for each data type. Additionally, we use the test split as a reference for high-quality synthetic data to evaluate these metrics.\nFidelity For ICD codes, similar to [12], we assess fidelity using unigram, bigram, and trigram probabilities within each visit, as well as sequential bigram probabilities between consecutive visits. For instance, the bigram probability of the sequence [icd1053, icd610] is calculated as its frequency divided by the total number of patients. We report Pearson's correlation between the top-1000 n-gram probabilities of the train and test/synthetic data pairs.\nFor time series data, we create manual embeddings for each patient by computing common statistics (min, max, mean, std) from the first 48 hours of the patient's stay. We then report the precision, recall, density, and coverage (PRDC) metrics [43] to evaluate the fidelity between the embeddings of the train split and the test/synthetic split. Additionally, we calculate the pairwise temporal correlation between all time series variables by concatenating the time series data from all patients and report the mean squared error between the correlation matrices of the train split and the test/synthetic split ($MSE_{corr}$). We also assess correlation accuracy [30] by discretizing the correlation coefficients into five levels: High negative ([-1, -0.5)), Medium negative ([-0.5, -0.2)), Low ([-0.2, 0.2)), Medium positive ([0.2, 0.5)), and High positive ([0.5, 1)). The confusion matrix of these correlation levels is then reported. To assess the fidelity of the missingness patterns, we visualize the co-occurrence of measurements for each pair of time series variables and normalize this by the sum of occurrences for each variable.\nUtility We assess the utility of the synthetic data by evaluating its performance in two tasks: in-hospital mortality prediction (binary classification) and phenotypes classification (multi-label classification). Specifically, we use time series embeddings from the first 48 hours of patient admission, as described earlier, and train a LightGBM model [44]. To examine the impact of data augmentation with synthetic data, we incorporate the entire synthetic dataset into the training data at various ratios (0, 0.1, 0.2, 0.5, 1.0) and evaluate the model on the test split.\nPrivacy We adopt the Membership Inference Attack (MIA) approach as described in EHR-Safe [31], where the goal is to determine whether specific data points were included in the training dataset. To accomplish this, we fit a K-Nearest Neighbors (KNN) model on the synthetic data and calculate the nearest distances for each patient in both the training and test splits. A significant disparity between the distance distributions in the synthetic-train and synthetic-test sets indicates lower privacy. For ICD code sequences, we use the Hamming distance, whereas for time series embeddings, we employ the Euclidean distance. We then fit Gaussian distributions to these distances and assess the differences between the two distributions using the Wasserstein Distance (WD), Jensen-Shannon Divergence (JSD), and Area Under the Receiver Operating Characteristic (AUROC) metrics."}, {"title": "Baselines", "content": "For ICD code generation, we compare our method with HALO [12], a hierarchical autoregressive language model, and PromptEHR [8], which utilizes an encoder-decoder transformer model. For time series generation, we compare our method with RTSGAN [9] and TimEHR [11], both of which are GAN-based models designed for generating irregularly-sampled time series data. Additionally, we include HALO in the comparison for time series generation, as it employs a similar tokenization strategy for numerical variables.\nAblation study. We further demonstrate the effectiveness of numerical tokenization by removing the discretization step and treating the numerical values as raw text. We refer to this model as SynEHRgy0."}, {"title": "Training Details", "content": "We employ a small GPT-2 model from the Transformers library\u00b9. Our model consists of 4 layers, 4 attention heads, and 384 embedding dimensions. After testing various context lengths, we found that 1024 tokens offer a good balance between the quality of generated data and training time (notably, context lengths of 256, 512, 1024, and 2048 correspond to 20%, 34%, 51%, and 68% of the original data, respectively). For the ablated model (SynEHRgy0), we need to use a larger context length of size 4098 due to its inefficient tokenization.\nWe use the Adam optimizer with a learning rate of $3 \u00d7 10^{\u22124}$ and train the model for 20 epochs on a system equipped with an NVIDIA Tesla V100-SXM2-32GB GPU. The batch size is set to 128, and we employ gradient accumulation with a step of two. All other hyperparameters are set to their default values as per the Transformers library. For generation, we use top-k sampling with a temperature of 0.7 and a top-k value of 50. We generate 30,000 synthetic patients which is the same size of training split. Due to computational limitation, we have performed minimal hyperparameter tuning for our model and for the baselines to ensure a fair comparison."}, {"title": "Results", "content": ""}, {"title": "Fidelity", "content": "Table 1 shows the correlation values between n-gram probabilities of the training and test/synthetic splits. While HALO achieves the highest performance for unigram probabilities, our method consis- tently surpasses all baselines, particularly in bigram, trigram, and sequential bigram probabilities. This underscores the ability of our method to generate high-quality synthetic ICD codes."}, {"title": "Utility", "content": "Table 3 and Table 4 present the AUROC scores for the 25-phenotype and in-hospital mortality predic- tion tasks, respectively, across different data augmentation settings. When training exclusively on synthetic data and testing on real data (TSTR), our method surpasses all baseline models and achieves performance closest to that of the Train on Real and Test on Real setting (TRTR). Additionally, our method demonstrates the highest AUROC when synthetic data is incorporated into the training set, although the performance gain diminishes as the proportion of real training data increases. This suggests that while the models leverage the synthetic data effectively, they do not extend beyond the real data distribution to generate new patient profiles that significantly enhance test performance."}, {"title": "Privacy", "content": "Membership Inference Attack (MIA) metrics are reported in Table 5. As can be seen, none of methods shows any potential privacy risk. It should be noted that these privacy metrics are computed based on hamming distance of ICD codes and Euclidean distance of time series data, and the results may vary for different distance metrics."}, {"title": "Conclusion", "content": "In this work, we introduced SynEHRgy, a novel methodology for generating synthetic structured EHR data that incorporates mixed-type data (ICD codes and time series) across multiple visits. A key innovation of our approach is the tokenization strategy for numerical variables, which enhances compatibility with decoder-only transformer models. Despite its advantages, the proposed method has limitations. Specifically, treating each data point (e.g., an ICD code or a heart rate measurement) as a single token may become inefficient for very large sequences due to context length constraints. Future work will explore more efficient tokenization strategies to mitigate this issue. Additionally, integrating high-frequency continuous signals, such as ECG data, is currently not feasible with our tokenization approach. We plan to address this limitation by incorporating patch-based tokenization strategies [45, 39, 46]. Furthermore, we aim to expand our methodology to include other data modalities, such as clinical notes and radiology images, to generate comprehensive multimodal EHR data."}]}