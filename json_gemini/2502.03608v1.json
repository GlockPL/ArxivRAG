{"title": "(GG) MoE vs. MLP on Tabular Data", "authors": ["Andrei Chernov"], "abstract": "In recent years, significant efforts have been directed toward adapting modern neural network architectures for tabular data. However, despite their larger number of parameters and longer training and inference times, these models often fail to consistently outperform vanilla multilayer perceptron (MLP) neural networks. Moreover, MLP-based ensembles have recently demonstrated superior performance and efficiency compared to advanced deep learning methods. Therefore, rather than focusing on building deeper and more complex deep learning models, we propose investigating whether MLP neural networks can be replaced with more efficient architectures without sacrificing performance. In this paper, we first introduce GG MoE, a mixture-of-experts (MoE) model with a Gumbel-Softmax gating function. We then demonstrate that GG MoE with an embedding layer achieves the highest performance across 38 datasets compared to standard MoE and MLP models. Finally, we show that both MoE and GG MoE utilize significantly fewer parameters than MLPs, making them a promising alternative for scaling and ensemble methods.", "sections": [{"title": "1. Introduction", "content": "Supervised machine learning on tabular data is widely applied, and its business value is undeniable, leading to the development of numerous algorithms to address these problems. Gradient Boosting Decision Tree (GBDT) models (Chen & Guestrin, 2016; Ke et al., 2017; Prokhorenkova et al., 2018) have demonstrated superior performance compared to deep learning methods (Shwartz-Ziv & Armon, 2022; Grinsztajn et al., 2022) and remain the most common and natural choice for tabular data prediction. As a result, tabular data remains one of the few domains where neural networks do not yet dominate.\nIn recent years, many researchers have attempted to adapt transformer-based neural network architectures for tabular data (Huang et al., 2020; Somepalli et al., 2021; Song et al., 2019). While these approaches have shown promising results on specific subsets of datasets, they often fail to consistently outperform vanilla Multilayer Perceptron (MLP) neural networks across a wide range of datasets (Gorishniy et al., 2024). This is despite their significantly higher computational requirements and larger number of parameters. Furthermore, a study (Gorishniy et al., 2024) demonstrated that efficient ensembles of MLPs tend to outperform advanced deep learning models.\nThis raises a question that, in our view, has been overlooked in recent research: Is there a neural network architecture that is more efficient than MLPs in terms of parameter count while still achieving comparable performance?\nFraming the problem this way makes investigating the performance of Mixture-of-Experts (MoE) models on tabular data a natural choice. MoE has recently gained popularity in deep learning (Fedus et al., 2022). However, to the best of our knowledge, little research has explored the adaptation of MoE to the tabular deep learning domain or evaluated its performance across a broad range of datasets. In this paper, we aim to address this gap.\nAdditionally, we introduce MoE with a Gumbel-Softmax activation function on the output of the gating network (GG MoE); see Section 3.4 for details. We compare the performance of MoE and GG MoE with MLP across 38 datasets and demonstrate that GG MoE achieves the highest average performance while both MoE and GG MoE are significantly more parameter-efficient than MLP."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Tabular Deep Learning", "content": "Although it is theoretically proven that feedforward neural networks can approximate functions from a wide family with arbitrary accuracy (Cybenko, 1989; Hornik, 1991), in practice, they often underperform compared to GBDT methods in the tabular domain (Gorishniy et al., 2021; Shwartz-Ziv & Armon, 2022; Grinsztajn et al., 2022). To improve performance, extensive research has been conducted. Here, we highlight three main directions."}, {"title": "2.2. Mixture of Experts", "content": "Mixture of Experts (MoE) is not a new architecture, and extensive research has been conducted on it. We encourage readers to refer to the comprehensive survey by (Yuksel et al., 2012) for an overview. MoE consists of two main components: a gating function and expert functions. The experts can be considered an ensemble of different models, which are aggregated into a final prediction using a gating function. A detailed description of the MoE architecture is provided in Section 3.2.\nMoE was not a widely adopted choice in deep learning architectures until its recent application to natural language processing (NLP) (Du et al., 2022) and computer vision (CV) (Puigcerver et al., 2023; Riquelme et al., 2021). Various MoE architectures have been developed and tested for these domains (Fedus et al., 2022). However, to the best of our knowledge, few studies have evaluated the performance of MoE across a broad range of tabular datasets. In this paper, we aim to address this gap."}, {"title": "2.3. Gumbel-Softmax Distribution", "content": "The Gumbel-Softmax distribution is widely used in deep learning for its ability to produce differentiable samples that approximate non-differentiable categorical distributions (Jang et al., 2016). In this paper, we utilize this distribution for a different purpose-primarily to regularize the gating neural network in MoE (see Section 3.4)."}, {"title": "3. Models", "content": "In this paper, we compare the performance of three models: MLP, MoE, and GG MoE. In this section, we provide a brief introduction to the architecture of each."}, {"title": "3.1. Notation", "content": "We formulate the supervised machine learning problem using a probabilistic framework. Given a training dataset consisting of N independent and identically distributed (i.i.d.) observations $x_i \\in \\mathbb{R}^M$, where $i = 1,..., N$ and M is the input dimension, along with their corresponding target values $y_i$, our goal is to model the conditional distribution p(y|x, w)."}, {"title": "3.2. MoE", "content": "Informally, MoE consists of two main components. The first component comprises K experts, which are independent models that learn the target distribution. These experts do not share weights and can be executed in parallel. The second component is a gating function, which maps an input to a probability distribution over the experts. The final output is a weighted average of the experts' outputs, where the weights are determined by the gating function. The Deep Ensemble method from (Lakshminarayanan et al., 2017) can be considered a special case of an MoE model with a constant gating function, g = 1/K.\nMore formally, the target distribution is defined as:\n$p(y | x, w) = \\sum_{i=1}^K p(i | x, w_g)p(y | i,x, W_{e_i}),$ (1)\nwhere K is the number of experts, and $p(i | x, w_g)$ is modeled by the gating function g. To model g, we use a multi-class logistic regression:"}, {"title": "3.3. MLP", "content": "For an MLP neural network, we model the target distribution as\n$p(y | x, w) = \\delta(y - f(x;w))$ (3)\nfor regression tasks, where d is the Dirac delta function. For classification tasks, we define\n$p(y = i | x, w) = softmax_i(f(x; w)).$ (4)\nHere, f(x, w) represents the model function, which, in our case, is a neural network parameterized by w, mapping an input observation x to a target value y.\nThe model function f(x; w) consists of a sequence of n blocks followed by a final linear layer. Each block is defined as\nBlock = Dropout(ReLU(Linear(x; wi))), (5)\nwhere wi represents the parameters of the i-th block. All linear layers within the blocks share the same hidden dimension. The final linear layer produces an output of dimension 1 for regression problems or C for classification tasks, where C is the number of classes.\nAn MLP neural network can be considered a degenerate case of MoE with a single expert and a constant gating function, g 1."}, {"title": "3.4. GG MOE", "content": "The only difference between MoE and GG MoE is that, instead of the standard softmax function, we use the Gumbel-Softmax function in the gating mechanism:\n$g_G(i | x, w_g) = \\frac{exp(\\frac{w_g^T\\cdot [x,1]+s_i}{\\tau})}{\\sum_{j=1}^K exp(\\frac{w_g^T\\cdot [x,1]+s_j}{\\tau})}$ (6)\nwhere $s_1,s_2,...,s_K$ are i.i.d. samples drawn from the Gumbel(0, 1) distribution. As\u03c4 \u2192 +\u221e, the Gumbel-Softmax distribution converges to a uniform distribution, while as \u03c4 \u2192 0, it converges to an argmax distribution. Due to this property, Gumbel-Softmax has been widely used in deep learning to sample from discrete distributions (Jang et al., 2016). However, in this paper, we utilize this distribution primarily for regularization purposes.\nIt is well known that, without intervention during training, the gating function may converge to a degenerate distribution, where one expert receives a weight close to 1, while all others receive weights close to 0. The authors of (Shazeer et al., 2017) proposed adding Gaussian noise to the softmax operation to prevent this behavior. In our research, we prefer Gumbel noise over Gaussian noise or other alternatives because, in our view, Gumbel-Softmax exhibits more suitable asymptotic behavior for this role. Specifically, as \u03c4 \u2192 0, the gating output distribution converges to an argmax distribution, leading to the entropy $h(p) = -\\sum_i^K P_i \\log P_i$ approaching zero. Conversely, as \u03c4 \u2192 +\u221e, the distribution becomes uniform, attaining the highest possible entropy value of log(K). For details on entropy and its properties, we refer to (Conrad, 2004). Informally, entropy can be interpreted as a measure of uncertainty. We tune 7 as a hyperparameter for each dataset (see Section 5.3), effectively selecting the \"optimal level of uncertainty.\"\nA drawback of introducing stochasticity into the gating function is the challenge of handling it during inference. To obtain an unbiased estimation of the target distribution, we apply Monte Carlo (MC) estimation (Graham & Talay, 2013) to approximate the expected value:\n$E[y(x; w)] = \\sum_{i=1}^K g_G(i | x, w_g) f (x; w) \\approx \\frac{1}{N_K} \\sum_{j=1}^{N_K}\\sum_{i=1}^K a_{ji} f(x; w),$ (7)\nwhere $a_{ji}$ are i.i.d. samples from the Gumbel-Softmax distribution. This estimation introduces a minor runtime overhead during inference. Fortunately, the sampling procedure is computationally inexpensive, as we do not need to recalculate logits or expert predictions for different samples. As shown in Section 6, 10 samples are sufficient for a reliable estimation. Furthermore, in Section 7.3.2, we demonstrate that the inference overhead is negligible. During training of GG MoE, we use a single sample to compute gradients, resulting in training times for MoE and GG MoE that are approximately the same, as illustrated in Section 7.3."}, {"title": "4. Datasets", "content": "To evaluate model performance, we used 38 publicly available datasets. Of these, 28 were taken from (Grinsztajn et al., 2022). These datasets are known to be more GBDT-friendly, meaning that deep learning models tend to perform worse on them. However, this collection includes only regression and binary classification problems and consists of small- to medium-sized datasets."}, {"title": "5. Experiment Setup", "content": "The authors of (Gorishniy et al., 2024) provided benchmarks for a diverse set of models, including GBDT and deep learning approaches. We consider comparable benchmarks essential for evaluating models in the tabular domain. Therefore, we fully adopted their experimental setup to ensure the comparability of our results. In this section, we outline the key aspects of this setup."}, {"title": "5.1. Data Preprocessing", "content": "Binary features were mapped to {0,1} without any additional preprocessing. For categorical features, we applied one-hot encoding. Numerical features were preprocessed using quantile normalization (Pedregosa et al., 2011).\nAdditionally, we utilized non-linear piecewise-linear embeddings for numerical features, as proposed in (Gorishniy et al., 2024). The embedding dimension and the number of bins were tuned as hyperparameters (see Section 5.3). All models were evaluated in two configurations: with and without piecewise-linear embeddings. Throughout this paper, we refer to models with embeddings by adding the prefix 'E+' to the model name (e.g., E+MoE)."}, {"title": "5.2. Training", "content": "We minimized the mean squared error (MSE) loss for regression tasks and the cross-entropy loss for classification tasks. All models were trained using the AdamW optimizer (Loshchilov, 2017). The learning rate and weight decay were tuned as hyperparameters (see Section 5.3). We did not modify the learning rate during training. Additionally, global gradient clipping was set to 1.0.\nEach model was trained until no improvement was observed on the validation set for 16 consecutive epochs. This early stopping criterion was applied during both hyperparameter tuning (see Section 5.3) and the final evaluation (see Section 5.4)."}, {"title": "5.3. Hyperparameter Tuning", "content": "We used the Optuna package (Akiba et al., 2019) to tune hyperparameters, setting the number of iterations to 100 for each model. Hyperparameters were tuned using validation sets for every dataset.\nIn Table 1, we present the search space for each hyperparameter for models without embeddings. For MoE-type models, we restricted expert sizes to either 32 or 64 hidden units. However, we allowed a wide range for the number of experts, from 2 to 40. The motivation behind this choice was to encourage the use of multiple weak learners rather than a few strong ones.\nFor GG MoE, we aimed to prevent the Gumbel-Softmax mechanism from converging to an undesirable distribution. Specifically, we sought to avoid convergence to an argmax distribution, as this would mean that only one expert contributes to the output. At the same time, we prevented convergence to a uniform distribution, as this would render the gating network meaningless and reduce the model to a Deep Ensemble (Lakshminarayanan et al., 2017). To address this, we constrained the temperature parameter (7) to be neither too close to zero nor excessively large.\nThe search space was identical across datasets. When using an embedding layer, the search space remained the same, except that the maximum number of blocks was reduced to 5. In Table 2, we provide the search spaces for optimizer and embedding layer hyperparameters for MoE-type models. For MLP and E+MLP models, the search space was the same except for the learning rate, which followed U[3e-5, 0.001]."}, {"title": "5.4. Evaluation", "content": "For classification tasks, accuracy was used as the primary metric for tuning hyperparameters and evaluating final model performance on test sets. For regression tasks, the negative root mean square error (RMSE) served as the primary metric.\nTo rank the models, we followed the approach described in (Gorishniy et al., 2024), which does not count insignificant improvements as wins. Each model was trained from scratch 15 times with tuned hyperparameters, using different random seeds. The average rank and standard deviation over these runs were then computed. Finally, we applied the ranking algorithm outlined in Algorithm 1 separately to each dataset. Informally, this algorithm assigns the same rank to models where the difference between mean scores is smaller than the standard deviation."}, {"title": "6. Results", "content": "We present the rankings for each model in Figure 1. For GG MoE, we applied Monte Carlo (MC) sampling (Section 3.4)"}, {"title": "7. Models Efficiency", "content": null}, {"title": "7.1. Number of Parameters", "content": "In Table 4, we present the average, median, and standard deviation (std) of the number of parameters (in millions) per dataset for each model. GG MoE and MoE models have approximately the same number of parameters, which is significantly lower than that of MLP models. This holds true for both models with and without embeddings.\nHowever, while the difference in the number of parameters between MLP and E+MLP models is negligible, the same does not apply to MoE and GG MoE models. This discrepancy arises because the embedding layer is a fully connected linear layer, which naturally leads to a significant increase in the number of parameters in MoE models.\nAt the same time, the number of parameters in the backbone of all models decreases when an embedding layer is added. See the number of blocks and block dimensions in Table 3."}, {"title": "7.2. Regularization", "content": "In Table 5, we observe that during hyperparameter optimization, the temperature in Gumbel-Softmax (7) was selected to encourage the contribution of every expert and introduce more stochasticity rather than enforcing sparsity. This is an interesting finding, and a possible follow-up in future work could be to increase the number of experts and examine whether 7 starts to decrease.\nThe only parameter that significantly differs between MoE and GG MoE models is dropout (Table 6). We believe this difference is primarily related to the stochasticity in the gating network, which acts as a regularization mechanism. This, in turn, leads to a lower dropout rate in the experts, resulting in stronger performance of GG MoE compared to MoE."}, {"title": "7.3. Computation Time", "content": null}, {"title": "7.3.1. TRAINING TIME", "content": "In Table 7, we present the training times using tuned hyperparameters for three datasets where the number of training rows exceeds 100,000."}, {"title": "7.3.2. GG MOE: INFERENCE TIME FOR DIFFERENT NUMBERS OF SAMPLES", "content": "As discussed in Section 3.4, the Monte Carlo (MC) estimation of the expected value does not introduce any runtime overhead during training. In Table 8, we report the average inference time for each dataset where the number of training rows exceeds 10, 000. There are 15 such datasets.\nFor each dataset, we measured inference time using all available data, i.e., by combining the training, validation, and test sets. To reduce variance in time evaluation, we repeated the measurement 15 times and then computed the average.\nWe observed no difference in runtime between 1, 5, and 10 samples, while computing 100 samples increased inference time by approximately 33%. However, computing 100 samples is unnecessary, as it does not improve accuracy (see Section 6). This result also holds for GG MoE models without embeddings."}, {"title": "8. Conclusion and Future Work", "content": "In this paper, we compared the performance of MoE models and MLP models in the tabular domain. We introduced GG MoE, a mixture-of-experts model in which the gating network employs a Gumbel-Softmax function instead of a standard Softmax function. Our results show that this approach, combined with a piecewise-linear embedding layer, outperforms both standard MoE and MLP models.\nAdditionally, we demonstrated that GG MoE and MoE models are significantly more efficient in terms of parameter count compared to MLP models, making them more suitable for scaling or ensemble-based approaches.\nWe believe that this work highlights the promising potential of MoE models for tabular data in deep learning. However, there are still many avenues for further research. One direction is scaling MoE and GG MoE models, not merely by increasing the number of parameters but also by adopting more efficient ensemble techniques. Furthermore, it would be valuable to explore the performance of both well-known MoE variants, such as Hierarchical MoE, and emerging deep learning architectures, such as sparse or soft MoE."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Datasets Overview", "content": "In Table 9, we present the statistics for each dataset used in the evaluation. The first 28 datasets were taken from (Gorishniy et al., 2022), the last 10 were sourced from (Grinsztajn et al., 2022)."}]}