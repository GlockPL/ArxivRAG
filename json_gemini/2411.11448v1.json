{"title": "Unveiling the Inflexibility of Adaptive Embedding in Traffic Forecasting", "authors": ["Hongjun Wang", "Jiyuan Chen", "Lingyu Zhang", "Renhe Jiang", "Xuan Song"], "abstract": "Spatiotemporal Graph Neural Networks (ST-GNNS) and Transformers have shown significant promise in traffic forecasting by effectively modeling temporal and spatial correlations. However, rapid urbanization in recent years has led to dynamic shifts in traffic patterns and travel demand, posing major challenges for accurate long-term traffic prediction. The generalization capability of ST-GNNs in extended temporal scenarios and cross-city applications remains largely unexplored. In this study, we evaluate state-of-the-art models on an extended traffic benchmark and observe substantial performance degradation in existing ST-GNNs over time, which we attribute to their limited inductive capabilities. Our analysis reveals that this degradation stems from an inability to adapt to evolving spatial relationships within urban environments. To address this limitation, we reconsider the design of adaptive embeddings and propose a Principal Component Analysis (PCA) embedding approach that enables models to adapt to new scenarios without retraining. We incorporate PCA embeddings into existing ST-GNN and Transformer architectures, achieving marked improvements in performance. Notably, PCA embeddings allow for flexibility in graph structures between training and testing, enabling models trained on one city to perform zero-shot predictions on other cities. This adaptability demonstrates the potential of PCA embeddings in enhancing the robustness and generalization of spatiotemporal models. The code is released in code.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in Spatiotemporal Graph Neural Net-works (ST-GNNs) and Transformer models have opened up exciting possibilities for traffic prediction. These models excel at capturing both spatial and temporal dependencies in traffic data, offering promising results under stable conditions by leveraging the structure of transportation networks [1]\u2013[7]. However, the fast-paced urbanization and constant change in modern cities pose unique challenges to accurate traffic predictions [5], [8]\u2013[10]. As cities grow, traffic patterns and demand shift unpredictably, requiring models that can keep up with these dynamics."}, {"title": "II. PROBLEM STATEMENTS", "content": "In traffic forecasting, we define a graph as G = (V, E, A), where V is the set of nodes, E \u2286 V \u00d7 V represents the edges, and A is the adjacency matrix associated with the graph G. At each time step t, the graph is associated with a dynamic feature matrix Xt in the real-number space R|V|\u00d7C, where C indicates the dimensionality of the node features (e.g., traffic volume, traffic speed, time of day and time of week). Traffic forecasting involves developing and training a neural network model fe, formulated as: fo : [Xt, A, E] \u2192 Yt, where E presents the adaptive embedding layer learning from training data, Xt = X(t\u221211):t and Yt = X(t+1):(t+l2), with 11 and 12 representing the lengths of the input and output sequences, respectively."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Overview of Adaptive Embedding Layer", "content": "In traffic forecasting [17], spatial indistinguishability poses a significant challenge, where time series with closely aligned historical patterns over specific observation windows display substantial divergence in their future trajectories. To address this issue, researchers have leveraged Tobler's first law of geography [29], by introducing GNNs into traffic prediction to resolve spatial indistinguishability.\nMessage passing in GNNs operates on the principle of local similarity, where proximate nodes are expected to demonstrate similar traffic patterns [30]. Spatial-temporal graphs for traffic prediction typically use either road connection distances [31] or absolute physical coordinates [19] to calculate edge weights. However, connectivity relationships in these predefined graphs are often incomplete or biased, as they rely heavily on supplementary data and human expertise, which complicates the task of capturing a comprehensive panorama of spatial dependencies.\nTo address this challenge, the adaptive graph approach is proposed to leverage parameter representations of adaptive embeddings, which are continuously updated throughout the training phase to minimize model errors. Adaptive graph aims to identify biases stemming from human-defined concepts and to uncover hidden spatial dependencies within the data. Mainstream adaptive graph learning methods utilize randomly initialized learnable matrices [12], [14], [32]\u2013[37]. Traditionally, the adaptive graph [13] is generated as follows:\n$Aadp = SoftMax (ReLU (EET)),$"}, {"title": "B. The Limitation of Adaptive Embedding", "content": "In this section, we delve into three key limitations inherent to Adaptive Embedding: excessive spatial indistinguishability, lack of inductive capacity, and limited transferability.\n\u2022 Lack of Inductive Capacity: As urban environments evolve, new infrastructure and traffic patterns emerge, rendering previously indistinguishable locations potentially dis-tinct over time. However, Adaptive Embedding, relying on fixed embeddings during inference, is inherently limited in its ability to adapt to such dynamic changes. This con-straint is visually depicted in Figure 5, where the model's performance diminishes as the environment diverges from its training distribution. Figure 5 reveals that while the LSTM model maintains relatively low MAE values across in-distribution and out-of-distribution settings, other SOTA models (GWNet, AGCRN, MTGNN, TrendGCN, STAE-former) exhibit a pronounced sensitivity to distributional shifts, highlighting the fact that adaptive embedding lack of inductive capacity.\n\u2022 Excessive Spatial Distinguishability: As described in [17], adaptive embeddings may be affected by excessive spatial distinguishability, particularly for datasets where spatial relationships are less critical. Empirical analysis presented in Figure 4 indicates that datasets with high spatial indis-tinguishability (such as METR-LA and PEMS-BAY) benefit significantly from trainable adaptive embeddings. However, datasets with low spatial indistinguishability (such as Ex-changeRate and ETTm1) experience performance degrada-tion in models like STID and AGCRN when such com-ponents are introduced. [17] observe that spatially indis-tinguishable samples constitute only a very small portion (approximately one in a thousand) of total observations in datasets requiring spatial differentiation, leading to a risk of overfitting when using adaptive embeddings. Subsequent experiments in Figure 5 reveal that, compared to PCA em-beddings, trainable adaptive embeddings may induce over-distinguishability on datasets where spatial relationships are important, resulting in overfitting to the training data and weakened model generalization capability.\n\u2022 Limited Transferability: A critical limitation of Adaptive Embedding lies in its restricted transferability across dif-ferent scenarios and deployments. As sensor deployments evolve over time, maintaining a fixed graph size becomes increasingly challenging [21], especially when urban en-vironments require modifications to their sensor networks through additions, decommissioning, or temporary failures [38]. The inherent design of adaptive embeddings, being tightly coupled with specific sensor, severely limits their applicability across different cities [39], requiring complete model retraining for each new deployment [40]. This in-flexibility is particularly problematic in rapidly developing urban areas where infrastructure changes are frequent [41], leading to significant computational overhead and resource requirements [6]. While recent research has proposed poten-tial solutions, such as meta-learning frameworks [42] and transfer learning strategies [43], these approaches fail to address the core issue of adaptive embedding. Consequently,"}, {"title": "C. Principal Component Analysis Embedding", "content": "To address the aforementioned limitations of adaptive em-bedding, we propose PCA embedding as an alternative ap-proach. PCA embedding effectively mitigates the three key challenges through its statistical and data-driven nature, while maintaining the ability to capture essential spatiotemporal relationships. Formally, due to the inherent periodicity in traffic data, we first divide each day into equal time slots to obtain Z \u2208 RD\u00d7N\u00d7T, where D is the number of days, N represents the number of nodes, and T is the number of time slots in a day (e.g., with a 5-minute sampling interval, T = 288). We then apply PCA to obtain the embedding matrix for each day: Enca = Zd.P \u2208 RN\u00d7C, d \u2208 {1, ..., D}, where P is the projection matrix generated from PCA, and C is the dimension of PCA embedding. Subsequently, we average the PCA embeddings across all training days to obtain the final node representations:\n$Etrainpca = \\frac{1}{D} \\SigmaERNXC$\nIn the testing phase, the same PCA projection matrix, P, is utilized to ensure consistency in feature extraction.\n$Etestpca Zval. P.$\nTo mitigate information leakage, a small subset (5%) of the validation set is designated as the validation subset. PCA is then applied to distill the key features that capture the system's spatiotemporal dynamics.\n1) Advantage of PCA Embedding: The PCA embedding method effectively overcomes three key constraints of adaptive embeddings through its rigorous statistical basis and data-driven adaptability. Below, we detail how PCA embedding addresses each limitation:\nFirstly, in terms of limited inductive capacity, PCA's gen-eralization capabilities are notable, as it captures essential statistical features of the input data rather than depending on fixed, trainable parameters. As environmental dynamics shift, PCA inherently adapts to distribution changes by recalculating features from updated data inputs, thereby preserving its performance across varying conditions. This contrasts with static embeddings that may require retraining to adapt.\nSecondly, PCA mitigates the challenge of excessive spa-tial distinguishability via its orthogonal basis representation, ensuring that extracted spatial features remain mutually in-dependent. By appropriately selecting a subset of principal components that explain a sufficient fraction of the total vari-ance (e.g., \u03a3=1 \u03bb\u03af/\u03a3=1 \u03bb\u03af \u2265 0), PCA maintains a balanced spatial representation without overfitting the training data. This characteristic makes it especially robust for scenarios requiring spatial generalization. For experimental comparison, please refer to Figure 5 and the detailed description in the experimental part.\nThirdly, the limitation of poor transferability is addressed effectively by PCA's inherent statistical methodology, which allows for seamless application across different scenarios"}, {"title": "IV. EXPERIMENT", "content": "In this section, we analyze the performance of PCA-embedding by addressing the following research questions:\n\u2022 RQ1: Is the performance of PCA-embedding consistent with Adaptive-embedding?\n\u2022 RQ2: How does PCA-embedding perform in zero-shot generalization?\n\u2022 RQ3: How does PCA-embedding perform when facing spatial shifts?\n\u2022 RQ4: Why PCA-embedding work in zero-shot generaliza-"}, {"title": "A. Comparsion with Adaptive Embedding (RQ1)", "content": "The primary research question we aimed to address was whether the use of PCA embeddings, as opposed to learnable embeddings, would lead to a decline in model performance during training. To explore this, we conducted a comparative analysis of model performance on the PEMS benchmark, utilizing both PCA embeddings and learnable embeddings.\nAs shown in Figure 5, our results demonstrate that the use of PCA embeddings does not significantly reduce model effectiveness compared to learnable embeddings, suggesting that manual dimensionality reduction can achieve outcomes comparable to those produced by learnable approaches. No-tably, in certain cases, PCA embeddings even exhibited su-perior performance. We hypothesize that this improvement is largely due to the greater ability of PCA embeddings to mitigate overfitting in the model. Recent studies [5], [9], [10] have identified spatial-shift issues, despite the original test data being collected only three weeks apart. We observe that recurrent neural network architectures, such as AGCRN and TrendGCN, are more prone to overfitting, and transformer architectures also encounter this issue on the PEMS03 dataset."}, {"title": "B. Zero-Shot Performance (RQ2)", "content": "While the utilization of PCA embeddings does not nec-essarily yield improvements in in-distribution performance, our work has unveiled a significant advantage in terms of model interpretability. It allows us to transcend previous training paradigms, which typically confined model validation to identical test sets. Our approach enables model validation across diverse datasets, irrespective of variations in node num-bers. Specifically, as illustrated in the Table I, we employed PCA embeddings for training on the PEMS03 and PEMS07 datasets. We use A\u2192B to indicate that the model is trained on the dataset A and tested on the dataset B. During the testing phase, we substituted the embeddings by applying the projection matrices W derived from PEMS03 and PEMS07, respectively, to generate the embedding E of the training samples from PEMS04 and PEMS08.\nTable I reveal that STAEformer (transformer-based archi-tectures) exhibit remarkable zero-shot generalization capabili-ties. Furthermore, our findings indicate that leveraging larger datasets, such as PEMS07's 883 nodes compared to PEMS03's 358, leads to superior performance, which suggests a positive correlation between dataset size and generalization capacity. Based on these results, we posit that PCA embeddings may emerge as a unifying paradigm for future large-scale traffic models.\nSimilarly, we conducted experiment in LargeST [6] in Table I. In a manner consistent with the previous experiments, we trained the model separately on the San Diego and Bay Area datasets, then tested it on the remaining scenarios. We observed that the zero-shot capabilities of the ST-Model were significantly underestimated. The previous training paradigms did not fully unlock the model's potential. In contrast, our results demonstrate that the zero-shot performance is not substantially inferior to that of the model trained on these datasets. We believe that the PCA-embedding approach paves"}, {"title": "V. RELATED WORK", "content": ""}, {"title": "A. Adaptive Embeddings in Spatiotemporal Models.", "content": "Adaptive Embeddings address the limitations of traditional graph construction by using adaptive learning strategies to uncover implicit relationships within spatiotemporal data. Ex-isting approaches can be divided into random initialization-based and feature initialization-based methods. In the ran-dom initialization approach, GWNet [12] introduced adaptive adjacency matrices via two learnable embedding matrices, laying the foundation for later work. MT-GNN [46] expanded this idea by incorporating nonlinear transformations and an-tisymmetric operations to capture more complex relation-ships. CCRNN [47] and DMSTGCN [48] introduced layer-wise adaptive graph learning and tensor decomposition-based methods, respectively, enhancing the model's ability to capture dynamic spatial dependencies. More recently, STID [11] and STAEformer [16] simplified the process by directly optimizing Adaptive Embeddings, bypassing graph construction while maintaining competitive performance. Feature initialization methods have shown promising results in capturing dynamic spatial relationships. DGCRN [49] employs a recurrent mecha-nism to dynamically generate graph structures based on hidden states, enabling the model to adapt to temporal variations. GTS [1] advances this approach by utilizing a probabilistic framework to generate graph structures from input features, enabling more data-driven and adaptive modeling. These methods demonstrate superior performance in scenarios where spatial relationships evolve significantly over time."}, {"title": "B. Spatiotemporal Forecasting.", "content": "Spatiotemporal forecasting has been extensively researched due to its crucial role in various real-world applications [6], [7], [50]\u2013[55]. Traditional methods, including ARIMA [56], VAR [57], k-NN [58], and SVM [59], often fail to capture the complex spatiotemporal dependencies in data due to their inherent limitations in modeling non-linear relationships and high-dimensional feature spaces. In recent years, GCNs have been integrated with temporal models, leading to improved performance in spatiotemporal forecasting. Notable examples include STGCN [31], which combines graph convolutions with gated temporal convolutions, and DCRNN [19], which inte-grates diffusion convolution with recurrent neural networks. These pioneering works have inspired numerous developments in the field [11]\u2013[13], [60]. Additionally, innovative meth-ods have been proposed to address specific challenges in spatiotemporal forecasting [1]\u2013[5], [61], such as long-term dependencies, multi-scale temporal patterns, and heteroge-neous spatial relationships. However, current research primar-ily evaluates ST-GNNs within short timeframes, neglecting the dynamics of data distribution shifts and long-term pattern changes."}, {"title": "C. Distribution Shift in GNNs.", "content": "Graph Neural Networks have significantly advanced graph representation learning, achieving state-of-the-art results across various graph-related tasks [62]\u2013[67]. However, recent studies have revealed suboptimal performance of GNNs on out-of-distribution data for both node and graph classification tasks [68]\u2013[75]. These findings highlight the vulnerability of GNNs to distribution shifts and the importance of developing robust models. Recent pioneering efforts [5], [9], [10] have made significant strides in addressing the challenge of out-of-distribution scenarios in traffic forecasting. CaST [5] intro-duces a causal structure learning framework to enhance model robustness, while [9] proposes invariant learning techniques to maintain prediction stability. However, these studies either manually construct the spatial-shift or validate it only within short-term scenarios, limiting their applicability to real-world situations. Although distribution shift has been observed across various fields, in the domain of traffic prediction, spatial-shift has yet to be widely validated in real-world scenarios, partic-ularly in the context of long-term forecasting and evolving urban environments. The gap between theoretical advances in handling distribution shift and practical applications in traffic forecasting remains significant, particularly in scenarios involving long-term temporal evolution and complex spatial dynamics. This highlights the need for more comprehen-sive approaches that can effectively address both spatial and temporal distribution shifts in real-world traffic forecasting applications."}, {"title": "VI. CONCLUSION", "content": "We reveal the limitations of current ST-models in adapting to evolving urban spatial relationships, leading to spatial-shift performance degradation in traffic forecasting. We propose a novel, PCA embedding enabling training-free adaptation to new scenarios. Our approach, implemented in existing architectures, significantly improves prediction accuracy and generalization capability. It also demonstrates potential for cross-dataset zero-shot predictions. Our work advances traffic forecasting methodology and addresses challenges posed by rapid urbanization. Future work will focus on refining the adaptive framework, exploring its applicability to other dy-namic prediction tasks."}]}