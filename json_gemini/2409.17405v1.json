{"title": "AI Enabled Neutron Flux Measurement and Virtual Calibration in Boiling Water Reactors", "authors": ["A. Tunga", "J. Heim", "M. Mueterthies", "J. Thomas Gruenwald", "J. Nistor"], "abstract": "Accurately capturing the three-dimensional power distribution within a reactor core is vital for ensuring the safe and economical operation of the reactor, compliance with Technical Specifications, and fuel-cycle planning (safety, control, and performance evaluation). Offline (that is, during cycle planning and core design), a three-dimensional neutronics simulator is used to estimate the reactor's power, moderator, void, and flow distributions, from which margin to thermal limits and fuel exposures can be approximated. Online, this is accomplished with a system of local power range monitors (LPRMs) designed to capture enough neutron flux information to infer the full nodal power distribution. Certain problems with this process, ranging from measurement and calibration to the power adaption process, pose challenges to operators and limit the ability to design reload cores economically (e.g., engineering in insufficient margin or more margin than required). Artificial intelligence (AI) and machine learning (ML) are being used to solve the problems to reduce maintenance costs, improve the accuracy of online local power measurements, and decrease the bias between offline and online power distributions, thereby leading to a greater ability to design safe and economical reload cores. We present ML models trained from two deep neural network (DNN) architectures, SurrogateNet and LPRMNet, that demonstrate a testing error of 1.1% and 3.0%, respectively. Applications of these models can include virtual sensing capability for bypassed or malfunctioning LPRMs, on-demand virtual calibration of detectors between successive calibrations, highly accurate nuclear end-of-life determinations for LPRMs, and reduced bias between measured and predicted power distributions within the core.", "sections": [{"title": "1. INTRODUCTION", "content": "In boiling water reactors (BWRs), an array of fixed in-core detectors (LPRMs) [1] is used and supplemented with movable in-core detectors (TIPs), to measure the local power distribution. These detectors provide perhaps the most fundamental set of measurements within a reactor core. Accurately capturing the full power distribution within a reactor core is vital for ensuring the safe and economical operation of the reactor, compliance with Technical Specifications, and fuel-cycle planning. Offline (that is, during cycle planning and core design), a three-dimensional neutronics simulator is used to estimate the reactor's power, moderator, void, and flow distributions, from which margin to thermal limits and fuel exposures can be approximated. Online, this is accomplished with the system of LPRMs to capture enough neutron flux information to infer the full nodal power distribution.\nCertain problems with this process pose challenges to operators and limit the ability to design reload cores economically (e.g., engineering in insufficient margin or more margin than required). These include (i) lack"}, {"title": "2. BACKGROUND", "content": ""}, {"title": "2.1. LPRM", "content": "The Local Power Range Monitoring (LPRM) System [1] provides signals proportional to the local neutron flux, and the individual LPRMs measure the local flux at various radial and axial locations within the core. There are typically 43 radially located LPRM assemblies (strings) in a large BWR, with each assembly containing four detectors spaced at 36-inch intervals. Each LPRM assembly is adjacent to a hollow dry instrument tube for the Traversing Incore Probe (TIP) System. This system is used to periodically calibrate the LPRMs to correct for instrument drift and to collect more spatially-resolved axial power distributions (referred to as traces) throughout the core. Mostly, the distribution of LPRMs is such that they are symmetric across a diagonal axis. In Figure 1, the LPRMs in sets A and B are symmetric, and set C forms the symmetry axis. The readings of the symmetric partners are very similar, given symmetric operation of the fuel cycle. As a representative example, Figure 3 shows the readings of 1A and 6A, which are symmetric partners.\nTo be able to reliably construct the axial flux distribution of the core, the four detectors are spaced at three-foot intervals. The lowest detector, Detector A, is located 18 inches above the bottom of the active fuel. The remaining detectors are spaced 36 inches apart with the D detector located 24 inches from the top of the fuel assembly.\nThe LPRM detector is a miniature fission chamber, with the case and collector fabricated from titanium and insulated from one another by a ceramic material. The inner surface of the case is coated with a U3O8 coating which contains several isotopes of uranium. This will include 18% U-235, 78% U-234 and 4% various isotopes of uranium (primarily U-238). The fill gas used in the LPRM detector is argon with an internal pressure of 14.7 psia (atmospheric). Thermal neutrons impinging on the detector have a high probability of causing uranium atoms to fission. The resulting fission process releases fission fragments, neutrons, and gamma radiation into the detector volume. This causes ionization of the gas and an electrical discharge between cathode and anode. Gamma radiation from sources external to the detector can also cause ionization within the detector.\nThe mixed uranium isotopes (U-235 and U-234) in the U3O8 coating provides two functions. The U-235 has a high probability of fission which causes the primary ionization of the argon gas within the LPRM detector. If U-235 were the only uranium isotope in the LPRM detector, the uranium coating would be rapidly depleted. This would result in sensitivity of the detector decreasing rapidly at normal operating power levels. Earlier BWR models were equipped with this type of LPRM detector (non-regenerative). The addition of U-234 provides a method of replacing or regenerating U-235 lost by fission. U-234 has a very low probability of fission but a high probability of adsorption and transmutation to U-235."}, {"title": "2.2. DEEP LEARNING", "content": "Recent advances in AI like image recognition, autonomous driving, voice recognition, and language models are possible due to Deep Learning (DL). In DL, a large model learns to perform a task (usually classification or regression) directly from the data. DL models can learn intricate structures in high-dimensional data, and representations of data required to perform a given task [3]. In this paper, we use a form of DL, called supervised learning, with a large dataset consisting of input features and targets. In the DL model's training process, it has access to the input features and desired targets. The DL model starts off with random adjustable weights, which are applied to the input features, to produce an output. The model's output is compared with the desired output, and an error measure (loss) is computed between the two. In the training process, the weights of the model are adjusted to minimize the error between the predicted output and the"}, {"title": "2.2.1. Fully connected networks", "content": "A fully-connected neural network is the simplest DL model, where it takes a fixed-size one-dimensional vector as an input, performs non-linear transformation, and outputs a fixed-size one-dimensional network. A fully connected network has an input layer, an output layer, and multiple hidden layers. It is called fully connected because every neuron in one layer is connected to every other neuron in the subsequent layer."}, {"title": "2.2.2. Convolutional neural networks", "content": "Recently, CNNs have shown tremendous performance improvements in image processing tasks such as classification [4], segmentation [5], and detection [6]. CNNs are particularly good at processing data with multiple array-like structures. For example, images made of three 2D arrays, one for each channel \u2013 red, green, and blue. The main idea behind CNNs is local connectivity \u2013 each neuron is only connected to a local region instead of all neurons [3]. In a typical CNN, a kernel with learnable weights slides over the array, and computes the dot product between its weights and the values in an array, in a particular region. The dot product produces a value indicative of how strongly a specific pattern or feature is present in that region. In most cases, multiple features might be required to learn a specific task, so multiple kernels are learned, and dot products are computed across the image to produce feature maps. These feature maps are stacked and passed through another set of convolutional layers, depending on the depth of the network. Usually, the initial layers focus on the local relationships, while the later layers focus on the global relationships in data. Since the structure of data from a NPP core is very similar to images, with multiple 2D arrays stacked on top of each other (images usually have 3 arrays stacked, while the data from NPPs usually has 25 nodal layers stacked), CNNs are well suited to process this data. Also, recent work [7] has shown success in using CNNs to predict pin powers in pressurized water reactors."}, {"title": "2.2.3. Attention", "content": "One of the important properties of CNNs is local connections, which help in reducing the number of parameters in the network. However, they introduce a major challenge: modeling long-range relationships. Recent works have proposed methods like using multi-scale dilatation convolutions to aggregate contextual"}, {"title": "3. DATA", "content": "The data used for all experiments discussed in this work are from a currently operating large BWR-4 (with Mark I containment) NPP. The dataset includes measured LPRM readings, measured core parameters, and parameters computed by a commercial core simulator. The parameters used as input features are Nodal Power ($NP \\in \\mathbb{R}^{H \\times W \\times D}$), Nodal Blade Depletion ($NBD \\in \\mathbb{R}^{H \\times W \\times D'}$), and Rod Pattern ($RP \\in \\mathbb{R}^{H \\times W}$). We also use one-dimensional parameters \u2013 thermal power, core inlet subcooling, and core flow. The measured LPRM values are the targets that the models predict using the input features.\n\u2022 Nodalize rod pattern to intermediate rod variable (RV') by representing the insertion by 1s and Os in the D' dimension. For example, if at a given location, the rod is 50% inserted, then half of the values in D' dimension at that location will have 1s, rest Os.\n\u2022 Weight the nodalized array with segment depletion:\n$s_i = 1 - NBD_i, RV_i = s_i * RV'_i,$\nwhere $NBD_i \\in NBD, RV'_i \\in RV'$ and $RV_i \\in RV$ at position $i$, and $s_i$ represents the fraction of the blade not depleted at position $i$. In a typical large-size BWR reactor H = W = 30, D = 25, and D' = 24."}, {"title": "4. PROPOSED ARCHITECTURE", "content": "In this section, we discuss the architectures and modeling methodology of the SurrogateNet and LPRMNet models, which are developed for real-time predictions and more accurate offline predictions, respectively."}, {"title": "4.1. SurrogateNet", "content": "SurrogateNet is a neural network-based architecture that uses information from multiple LPRM detectors resident in the core to predict the value of any other LPRM detector in the core contemporaneously. We exploit the symmetry in the core to predict the LRPM detector values. We divide the LPRMs in the core into three distinct sets\u2014A (green) with 19 strings and 76 detectors, B (red) with 19 strings and 76 detectors and C (blue) with 5 strings and 20 detectors, as shown in Figure 1. Every detector in set A has a symmetric partner in set B, as indicated by the dotted lines in Figure 1. The set C forms the line of symmetry, and thus the detectors in C do not have symmetrical partners.\nTo predict the values of the detectors in sets A and B, one of the sets is used as an input set and the other is used as the output set, respectively. The input set of 76 detectors is passed through six fully connected layers. For the fully connected layers, GELU [13] is used as the non-linear activation function, and for each layer batch normalization is performed. The optimal parameters for the network are identified through hyper-parameter optimization. The same architecture is used for the model where the input and output sets are reversed. Using the two models, 152 of the 172 detectors can be calculated in real-time. To account for the situations where the detectors are bypassed or otherwise deemed faulty, data augmentation is performed on the fly when training the network, which is explained in Section 4.3.\nThe detectors in set C do not have symmetrical partners, so a modified model and different inputs have been used to train the model. For every detector in C, all but the target detector is used as inputs to the model (N-1 detectors are used as inputs, where N is the total number of detectors in the core). The N-1 detectors are passed through six fully connected layers before predicting the final detector value. The activation function and batch normalization are similar to the previously defined models. As these models are based on simple fully connected networks, they can be used for real-time predictions, even on a CPU, thereby enabling real-time monitoring and diagnostics."}, {"title": "4.2. LPRMNet", "content": "LPRMNet is a CNN-based model, which uses the reactor state data to predict the LPRM values. In the implementation of LPRMNet, we extract information from NP and RV separately using CNNs and use fully connected networks to extract the information from the scalar variables. NP and RV are separately passed to individual CNNs, and the output from the individual CNNs is stacked to form a feature map L. The scalar features are passed through two fully connected layers to produce a feature vector S.\nThe feature map L is passed through the axial-attention [12] module to impart contextual knowledge across the horizontal and vertical axes. Axial-attention enables a global receptive field, compared to the receptive field achieved by only using CNNs. Similar to [11], in the axial-attention module, 1 \u00d7 1 convolution is applied on feature map L to generate feature maps Q and K of the same spatial dimensions as L. From Q and K, an attention map M is obtained via the Affinity operation. From every spatial position of Q, a vector Qj is extracted along the channel dimension of Q. From K, a feature set $j is extracted from the same row or column as j. The Affinity operation is defined as:\n$a_{i,j} = Q_j^T i_j$\nwhere $a_{i,j} \\in A$, and a softmax layer is applied to A to get the attention map M. Further, a 1 \u00d7 1 convolution is again applied on feature map L, to generate V. Similar to Qj, from every spatial location of V, a vector Vj is obtained, and a set of features which are in the same row or column of j is obtained as \u03a9j. Using the Aggregation operation, the contextual information is collected:\n$L'_j = \\sum_i M_{i,j}l_{i,j} + L_j,$\nwhere $L'$ is a feature vector in $L'$ at position j. $L'$ is the contextual information-rich feature map derived from L. The output from the axial-attention module, $L'$, is concatenated with the feature map L. Further,"}, {"title": "4.3. IMPLEMENTATION DETAILS", "content": "The proposed models - SurrogateNet and LPRMNet - have been implemented in PyTorch [14], and trained using mean squared error loss. For training SurrogateNet, AdamW [15] optimizer is used with a weight decay of 0.01. 1-cycle policy [16] is used for the learning rate with max_lr = 0.005. Since the LPRM detectors are predicted using other LPRM detectors, it is possible that the model might not be robust when some of the detectors are bypassed when faulty (they read 0). To overcome this challenge, while training the SurrogateNet, with a probability of 20%, the input detectors' are replaced with 0s. We observed that by randomly replacing detector values with Os, the model is stable during inference even when one or more detectors are bypassed.\nLPRMNet is trained with AdamW [15] optimizer with a weight decay of 0.01. Similarly, a 1-cycle policy is used for the learning rate with max_lr = 0.08. Since each detector has an individual LPRMNet model, a total of 172 different models have been trained. To speed up the training, 4 models have been trained in parallel on a single GPU. All the models have been trained on a single 24 GB NVIDIA A30 GPU."}, {"title": "5. EXPERIMENTS AND RESULTS", "content": ""}, {"title": "5.1. DATA SPLITS", "content": ""}, {"title": "5.1.1. SurrogateNet data split", "content": "SurrogateNet is developed to predict real-time LPRM readings, so it is beneficial to evaluate it across all the cycles. From the entire dataset, 70% of the data is used for training, and 20% of the data is used for validation. The remaining 10% of the data is used for independent testing and is never seen in the training or hyper-parameter optimization process. All the results in Table 1 are reported on the independent test set."}, {"title": "5.1.2. LPRMNet data split", "content": "LPRMNet predicts the LPRM readings from offline core parameters. The model predicts the LPRM values for new fuel cycles, which will aid in better planning of the fuel cycles. To test the robustness of such a"}, {"title": "5.2. RESULTS", "content": "To evaluate the models, the Root Mean Square Error (RMSE) between the model predictions and the detector measurements is calculated for the test sets, for all the detectors. The average RMSE, as well as the maximum MSE for all the detectors, can be seen in Table 1, for SurrogateNet and LPRMNet, respectively. In addition to the average RMSE, the average RMSE at each detector level - A, B, C, and D is also shown.\nOn comparing the results from the SurrogateNet and the LPRMNet, though it might appear that the results from SurrogateNet are better than the results from LPRMNet, it should be noted that the test sets are vastly different between the two. In the case of SurrogateNet, although the test data are unseen, they come from the same distribution as the training set. However, in the case of LPRMNet, the test data is both unseen and from a new distribution as compared to the training data. Also, SurrogateNet has access to online core conditions, when the measurements are taken. The different testing methodologies are intentionally chosen based on the use cases of both models. Though both the models predict the LPRM detector values, they are useful in different situations - the SurrogateNet for online predictions and virtual readings, while the LPRMNet for offline and future predictions."}, {"title": "6. CONCLUSIONS", "content": "In this paper we demonstrate that by using the available data from a NPP, spanning 4 fuel cycles, we can reliably predict the values of the LPRM detectors using deep learning modeling methodologies. Two different models were developed: (i) an online model, which uses the values of other LPRM detectors to predict the value of a given LPRM detector with a mean RMSE of 0.5, and (ii) an offline prognostic model, which can be used for future planning, that uses core projection data to predict the values of the LPRM with a mean RMSE of 1.35. This level of performance corresponds to an average percent error of 1.1% and 3.0% for type (i) and (ii) models, respectively. The models have shown more than 2\u00d7 reduction in uncertainty when compared to the relevant commercial core simulator.\nApplications of these models include virtual sensing capability for bypassed or malfunctioning LPRMs, on-demand virtual calibration of detectors between successive TIP calibrations, highly accurate nuclear"}]}