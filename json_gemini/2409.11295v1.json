{"title": "EIA: ENVIRONMENTAL INJECTION ATTACK ON\nGENERALIST WEB AGENTS FOR PRIVACY LEAKAGE", "authors": ["Zeyi Liao", "Lingbo Mo", "Chejian Xu", "Mintong Kang", "Jiawei Zhang", "Chaowei Xiao", "Yuan Tian", "Bo Li", "Huan Sun"], "abstract": "Recently, generalist web agents have evolved rapidly and demonstrated remarkable\npotential in autonomously completing a wide range of tasks on real websites,\nwhich can significantly boost human productivity and capability. However, there\nare also unprecedented safety risks associated with these web agents, which are\nnearly unexplored so far in the literature. In this work, we aim to narrow this\ngap by conducting the first study on the privacy risks of generalist web agents\nin adversarial environments. First, we present a threat model that discusses the\nadversarial targets, constraints, and attack scenarios for a realistic privacy attack.\nIn particular, we consider two types of adversarial targets: stealing users' specific\npersonally identifiable information (PII) or stealing the entire user request. To\nachieve these objectives, we propose a novel attack method, termed Environmental\nInjection Attack (EIA). This attack injects malicious content designed to adapt\nwell to different environments where the agents operate, causing them to perform\nunintended actions. This work instantiates EIA specifically for the privacy scenario.\nIt inserts malicious web elements alongside persuasive instructions that mislead\nweb agents into leaking private information, and can further leverage CSS and\nJavaScript features to remain stealthy. We collect 177 actions steps that involve\ndiverse PII categories on realistic websites from the Mind2Web dataset (Deng\net al., 2023), and conduct extensive experiments using one of the most capable\ngeneralist web agent frameworks to date, SeeAct (Zheng et al., 2024). The results\ndemonstrate that EIA achieves up to 70% attack success rate (ASR) in stealing\nusers' specific PII at an action step. For example, our attack can deceive the\nagent to enter the user's name into a malicious text field as shown in Figure 1.\nStealing full user requests is more challenging, but a relaxed version of EIA can\nstill achieve 16% ASR. Despite these concerning results, it is important to note that\nthe attack can still be detectable through careful human inspection, highlighting a\ntrade-off between high autonomy and security. This leads to our detailed discussion\n(Section 6) on the efficacy of EIA under different levels of human supervision as\nwell as implications on defenses for generalist web agents", "sections": [{"title": "INTRODUCTION", "content": "The web hosts a multitude of websites, tools, and content that span every aspect of the digital world.\nTo make these resources more accessible and boost human productivity, significant research efforts\nhave been invested in the development of web agents, particularly generalist web agents (Deng\net al., 2023) that can perform a wide range of tasks on realistic websites. Recent advancements in\nlarge language models (LLMs) and large multimodal models (LMMs) have significantly boosted the\nperformance of generalist web agents, making them closer to real-world deployment. Specifically,\nimprovements in long context capability (Yang et al., 2024a; Su et al., 2024), visual information\nunderstanding (Liu et al., 2023b;c), and enhanced reasoning and planning capabilities (Achiam et al.,"}, {"title": "RELATED WORK", "content": "Direct and Indirect Prompt Injection. Prompt injection attacks refer to manipulating the input\nmessage to Al systems to elicit harmful or undesired behaviors (Greshake et al., 2023a; Liu et al.,\n2023d; Perez & Ribeiro, 2022; Toyer et al., 2023; Yi et al., 2023; Pedro et al., 2023; Zeng et al.,\n2024). One type of prompt injection is directly inserted by users to jailbreak the guardrails of LLMs.\nIt could either be crafted by humans (Schulhoff et al., 2023; Wei et al., 2023; Mo et al., 2024a) or\ngenerated by LLMs automatically (Shah et al., 2023; Yu et al., 2023; Liao & Sun, 2024). Besides,\nGreshake et al. (2023a) introduces the novel concept of indirect prompt injection, which attacks\nLLMs remotely rather than directly manipulating the input messages. In particular, they design\nseveral attack strategies for LLM-integrated applications and alter the behaviors of LLMs by injecting\nmalicious instructions into the information retrieved from different components of the application.\nWeb Agents. There are various definitions of web agents in the literature. Some works (Nakano\net al., 2021; Wu et al., 2024b) consider web agents to be LLMs augmented with retrieval capabilities\nover the websites. While useful for information seeking, this approach overlooks web-specific\nfunctionalities, such as booking a ticket directly on a website, thereby limiting the true potential of\nweb agents. Recent works (Yao et al., 2022; Deng et al., 2023) have developed web agents that take\nraw HTML content as input and can directly perform tasks in simulated or realistic web environments\nbased on human instructions. However, HTML content can be noisier compared to the rendered\nvisuals used in human web browsing and provides lower information density. Given this, (Zheng\net al., 2024) proposes SeeAct, a two-stage framework that incorporates rendered screenshots as input,\nyielding stronger task completion performances. Although there exist other efforts towards generalist\nweb agents, including one-stage frameworks (Zhou et al., 2023) and those utilizing Set-of-Mark\ntechniques (Yang et al., 2023), these approaches either have much lower task success rate or need\nextra overhead compared to SeeAct, making them less likely to be deployed in practice. Therefore, in\nthis work, we focus on attacking SeeAct as our target agent. It is important to note that our proposed\nattack strategies are readily applicable to all web agents that use webpage screenshots and/or HTML\ncontent as input.\nExisting Attacks against Web Agents. To the best of our knowledge, there exists only a limited\nbody of research examining potential attacks against web agents. Yang et al. (2024b) and Wang et al.\n(2024) investigate the insertion of backdoor triggers into web agents through fine-tuning backbone\nmodels with white-box access, aiming to mislead agents into making incorrect purchase decisions.\nWu et al. (2024a) explores the manipulation of uploaded item images to alter web agents' intended"}, {"title": "ENVIRONMENTAL INJECTION ATTACK AGAINST WEB AGENTS", "content": "3.1 BACKGROUND ON WEB AGENT FORMULATION\nGiven a website (e.g., American Airlines) and a task request T (e.g., \u201cbooking a flight from CMH\nto LAX on May 15th with my email abc@gmail.com\"), a web agent needs to produce a sequence\nof executable actions {a1, a2, ..., an} to accomplish the task T on the website. Particularly, at each\ntime step t, the agent generates an action at based on the current environment observation st, the\nprevious actions At = {a1,a2,..., at\u22121}, and the task T, according to a policy function \u03c0. We\nselect SeeAct (Zheng et al., 2024) as our target agent, which considers both the HTML content ht\nand the corresponding rendered screenshot it of the current webpage as its observation st:\n$a_t = \\pi(S_t, T, A_t) = \\pi({h_t, i_t}, T, A_t)$\nAfter executing action at, the website is updated accordingly.\nWe omit notion t in subsequent equations for brevity, unless otherwise stated. In order to perform\nan action a on the real website, the agent formulates the action at each step as a triplet (e, o, \u03c5),\nrepresenting the three required variables for browser events. Specifically, e denotes the identified\ntarget HTML element, o specifies the operation to be performed, and v represents the values needed\nto execute the operation. For example, to perform the action of filling a user's email on the American\nAirlines website, SeeAct will TYPE (o) \u201cabc@gmail.com\u201d (v) into the email input field (e).\nSeeAct is designed with two stages to generate the action: action generation and action grounding.\nThe action generation stage involves textually describing the action to be performed at the next step:\n$(e, o, v) = \\pi_1({i}, T, A)$\nwhere underlined variables correspond to their respective textual descriptions. i represents the\nscreenshot rendered from HTML content h, i.e. i = \\phi(h) where \\phi denotes the rendering process.\nThe action grounding stage grounds the described action into the corresponding web event on the\nwebpage by:\n$(e, o, v) = \\pi_2({i, h}, (e, o, v), T, A)$\nNote that in our work, we follow the default implementations in SeeAct: (1) only the screenshot i is\nused for action generation (i.e., no HTML content is needed at this stage), (2) the approach of textual\nchoices is used for action grounding. Examples of the two stages in SeeAct can be found in Figure\n13 and 14.\n3.2 THREAT MODEL\nAdversarial Targets. We consider two types of adversarial targets. (1) The first target is to leak the\nuser's specific PII, such as the email address and credit card information. (2) The second target is to\nleak the user's entire task request T, as it contains sensitive data along with the additional context\nthat reveal more personal information. For instance, a full user request, \"booking a flight from CMH\nto LAX on May 15th with my email abc@gmail.com\" on the American Airlines website, reveals\ndetailed information about the user's travel plan such as dates, location, and transportation type,\nposing significant privacy risks.\nAttack Constraints. We assume that attackers have no prior knowledge of the user's task T or the\npreviously executed actions A. This condition ensures that the attack remains general and applicable"}, {"title": "ENVIRONMENTAL INJECTION ATTACK STRATEGIES", "content": "General Methodology. Based on the threat model we proposed above, we introduce EIA. EIA aims\nto manipulate the agent's behavior via injection of persuasive instructions (PI) into the benign HTML\ncontent h. Generally, it can be formulated as:\n$h* = E(h, PI, \\beta, \\alpha)$\nwhere:\n\u2022 h represents the original benign HTML content without any injections.\n\u2022 PI denotes the specific persuasive instructions designed for different adversarial targets.\n\u2022 $\\beta \\in {P_{-\\infty}, ..., P_{-1}, P_0, P_1, ..., P_{+\\infty}}$ is a parameter vector that defines the injection loca-\ntions within h.\nPo: The position of the target element where the specific PII is intended to be entered\nin the original webpage. It serves as a reference point (See Figure 2 for an example).\nP+n (n > 0): Inject PI at positions that are n levels above Po in the HTML structure.\nFor example, P+1 would indicate insertion immediately above the target element within\nthe same  pairs.\nP_n (n > 0): Inject PI at positions that are n levels below Po. For instance, P_1\nrepresents insertion immediately below the target element within the same  \npairs.\nP+\u221e and P-\u221e: The highest and lowest possible injection positions on the webpage,\nrespectively.\nThis positioning design aims to systematically explore vulnerabilities at various levels within\nthe HTML structure. In this work, we focus on positions three levels adjacent to the target\nelement (i.e., P_3, P_2, P_1, P1, P2, P3), and the two extreme positions, P-\u221e and P+0\u221e.\n\u2022 \u03b1 \u2208 [0, 1] is a parameter that controls the visibility of the injected content. This is achieved\nby adjusting the CSS opacity property of the injected elements. The value of a corresponds\ndirectly to the opacity, where 0 represents complete invisibility and 1 represents full visibility.\n\u2022E:hxPIX {$P_{-\\infty}, ..., P_{+\\infty}$} \u00d7 [0, 1] \u2192 h* is the strategy that injects PI into the benign\nHTML h, according to the position parameter  using the visibility parameter a. The\nresulting HTML after injection is denoted as h*. This process is depicted in Figure 2."}, {"title": "EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETTINGS\nBackbone LMMs of Web Agents. SeeAct (Zheng et al., 2024), as a SOTA web agent framework, can\nbe powered by different LMMs. Specifically, we experiment with the closed-source GPT-4V (Achiam\net al., 2023), open-source Llava-1.6-Mistral-7B (Liu et al., 2023a) and Llava-1.6-Qwen-72B (Li et al.,\n2024), which are presented as LlavaMistral7B and LlavaQwen72B for brevity in the later experiments.\nAll experiments are conducted using A6000 48GB GPUs.\nEvaluation Data Collection. We collect evaluation data from Mind2Web (Deng et al., 2023), a\nwidely used dataset for developing and evaluating web agents. This dataset spans 137 real websites"}, {"title": "EIA TO STEAL SPECIFIC PII", "content": "To mislead the agent to leak specific PII, we experiment with two injection strategies, i.e., FI and MI,\nwith two variants of FI: FI (text) and FI (aria), as outlined in Section 3.3. Both strategies utilize the\nprompt designed to leak the user's PII with the default opacity value, i.e., a = 0.\nPerformance of EIA. The attack performance using different injection strategies in different positions\nis shown in Table 1. Note that different backbone LMMs vary substantially in their general capabilities"}, {"title": "\u0395\u0399\u0391 \u03a4\u039f STEAL FULL USER REQUESTS", "content": "We further inject prompts designed to leak full user requests with an opacity value of a = 0 by\ndefault but encounter failures (we will explain why shortly). Then, we propose the approach of\nRelaxed-EIA to set a low but non-zero opacity value to influence the reasoning process of action\ngeneration. Besides evaluating the ASR of leaking full user requests (both the selection of the injected\nelement and the typed values are correct), we also assess cases where the injected element is correctly\nselected no matter whether the typed values are correct or not, which is denoted as ASR\u3002.\nFailures of EIA. Our experiments reveal that the injection fails consistently across all tested positions\nand three different backbone models, yielding an ASR of zero for EIA. Interestingly, upon examining\nthe ASR, we find that web agents are indeed misled into selecting the injected element as presented\nin Figure 5a, but fail to type the full user request into them. We identify that the failure occurs\nbecause action grounding is predominantly guided by the reasoning steps and the generated textual"}, {"title": "ATTACK DETECTION AND MITIGATION", "content": "5.1 STEALTHINESS EVALUATION\nMaintaining stealthiness is crucial for the success of any attack strategy. In this section, we evaluate\nthe stealthiness of the EIA and Relaxed-EIA methods from the perspectives of both traditional web\nsecurity and agent functional integrity. We focus on using GPT-4V in this section.\nTraditional Web Security Tool. Web security has been studied for years with many successful and\nuseful detection tools. Particularly, we use VirusTotal (VirusTotal, 2023), a classical web malware\ndetection tool, to identify suspicious and malicious components within webpages. For both EIA (two\nFI variants and one MI) and Relaxed-EIA, we inject the PI at 8 different positions, generating a total\nof 5,664 malicious webpages (4,248 with EIA and 1,416 with Relaxed-EIA). Notably, none of these\nwebpages were flagged as malicious or suspicious by VirusTotal, as shown in Table 2.\nSuch failure of detection stems from the unique nature of the malicious content we introduced. Unlike\nprevious web threats typically associated with malicious executable code, our approach involves\ninserting seemingly innocuous natural language into HTML content, which will be overlooked by\nthose traditional web security tools.\nAgent Functional Integrity. Maintaining the agent's functional integrity is important for the attack's\nstealthiness. Any disruption in normal operation could alert users to potential issues, possibly leading\nthem to abandon the site and thus preventing the theft of their private information. To assess this\naspect, we trace whether the agent can continue performing the user task normally after leaking the\nuser's private information (i.e., SR of at+1 following the successful attack at at), denoted as ASRpt.\nFigure 6 illustrates the ASR and ASRpt performance for both EIA and Relaxed-EIA.\nThe results show that ASRpt is very close to ASR in both EIA and Relaxed-EIA, indicating that\nour attacks barely affect subsequent actions of the web agent, partly due to the auto-submission\nmechanism we designed. This finding suggests that malicious websites employing these attack\nmethods can steal users' private information in a highly stealthy manner, without noticeably affecting\nthe agent's functional integrity or the user interaction experience."}, {"title": "MITIGATION BY DEFENSIVE SYSTEM PROMPT", "content": "In addition to evaluating the stealthiness of EIA above, we further assess if the risks posed by our\nattack could be easily mitigated by a defensive system prompt. Based on the insights discussed at\nthe end of Section 4.3, we explicitly add the following instructions to the end of the SeeAct default\nsystem prompt:"}, {"title": "DISCUSSIONS", "content": "Levels of Human Supervision. Web agents can be applied in various scenarios, each characterized\nby different levels of human supervision. Such varying degrees of human supervision present a\ntrade-off between autonomy and security. In scenarios with high demands for autonomy, webpages\nare often not presented directly to the user, and web agent ends up operating in environments with\nminimal human supervision over them. This allows attackers to design more explicit attacks without\nconcern for stealthiness, making the agents highly vulnerable. On the other hand, when humans\ndirectly monitor the accessed websites, it becomes easier for them to spot abnormal visual changes,\nsuch as those caused by Relaxed-EIA or the extra white space introduced by EIA. However, sustained\nvisual attention unavoidably introduces extra burdens to the users.\nA balanced approach is to tailor the level of supervision based on task types. For tasks involving PII,\nclose supervision over the web agent is imperative to ensure safety, including requiring permission or\nverification when entering sensitive information. For tasks focused on information seeking, higher\nautonomy is generally preferable to reduce user burden. However, implementing this approach\npresents challenges. For instance, if a user is trying to book a flight (a task involving PII) while\ndriving, maintaining constant supervision becomes impractical. Additionally, while information-\nseeking tasks may not directly involve private data, unauthorized leakage of such activities, such as\nthrough Relaxed-EIA, can still violate users' privacy rights.\nGiven these challenges, we turn our attention to defense strategies on both the websites and web\nagents in the following section. Particularly, we discuss their applicability in ensuring the safety of\nweb agents without relying on human supervision.\nImplications over Defenses in Pre- and Post-Deployment of Websites. We have discussed one of\nthe most well-known tools, VirusTotal, to examine webpages in Section 5, which could be seen as\npotential defenses at the website pre-deployment stage. The failures of detection highlight the need\nfor more advanced and dedicated web malware detection tools to combat the unique threats, natural\nlanguage injection, arising from LLM- and LMM-based web agents. One possible solution is to use a\npredefined list of sensitive keywords to filter webpage content. However, the persuasive instructions\nin our attack primarily consist of normal sentences that simply mislead the agent. For example, a\nphrase like \"This is the right place to type ...\" might appear as a benign guidance message on the web,\nmaking it hard for keyword filtering to detect. Moreover, expanding the keyword list can increase\nthe risk of false positives and may lead to the misclassification of benign elements. Another defense\napproach is to filter out non-visible elements with zero opacity. However, many legitimate elements\ninitially have zero opacity for reasons like transitions or animations, before becoming visible and\ninteractive. Distinguishing between benign and malicious elements in such cases is difficult. Blanket"}, {"title": "CONCLUSION", "content": "In this paper, we explore potential privacy leakage issues posed by generalist web agents. We first\ndevelop a threat model with adversarial targets, attack constraints, and realistic scenarios for the\nattacks to be implemented. Then, we introduce a novel attack approach, dubbed EIA, and apply it to\none of the SOTA generalist web agent frameworks, SeeAct. Our experiments demonstrate the efficacy\nof our attacks in leaking users' specific PII and full requests by implementing two distinct injection\nstrategies and examining various injection positions on the webpages. Additionally, we show that\nthese attacks are challenging to detect and mitigate. Furthermore, we point out the need for various\nlevels of human supervision and discuss the implications on defenses during both the pre-deployment\nand post-deployment stages of the websites. Overall, our study calls for further in-depth explorations\nof privacy leakage risks associated with generalist web agents.\nMore importantly, although we develop EIA with a particular focus on injections into the web envi-\nronment, this approach opens avenues for broader exploration of injections into various environments\nthat agent operate on. Future studies can investigate injections in other digital contexts, such as\nmobile app environments or virtual reality environments, to assess their impact on corresponding\nagents. This expansion of research scope can not only reveal new vulnerabilities but also inspire the\ndevelopment of more comprehensive security measures across different types of digital agents."}, {"title": "LIMITATIONS", "content": "We introduce EIA as a method to mislead web agents into leaking users' private information,\ndemonstrating its attack efficacy. Specifically, we conduct offline evaluations using action steps\nadapted from Mind2Web, following their methods to assess step success rate and designing ASR for\neach action step containing PII. To fully assess the web agent's capabilities and associated risks, it is\ncrucial to evaluate success rate in completing user requests end-to-end within a real-time interactive\nweb environment while monitoring ASR throughout the entire process. Currently, to measure the\nstealthiness of EIA, we check the immediate next step to see if the agent can continue the intended\ntask as normal after the attack. In the future, this can be expanded to include all of the subsequent\nsteps leading to task completion. Additionally, we instantiate EIA in this work by injecting malicious\nelements into web environments, but this represents just one possibility. There is ample room for\nfurther exploration, including injections at multiple points within a webpage, compositional injections\nto attack multi-turn interactions, or injections implemented at the post-deployment stage rather than\nbeing embedded before deployment."}, {"title": "ETHICS STATEMENT", "content": "This work introduces a new type of attack, EIA, which could potentially mislead web agents to leak\nusers' private information, posing a security risk if exploited by attackers. However, it is crucial to\nemphasize that our research methodology is designed to investigate this risk without compromising\nreal user privacy. Our evaluation data is derived from the Mind2Web dataset (Deng et al., 2023)"}]}