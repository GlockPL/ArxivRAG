{"title": "Not All Diffusion Model Activations\nHave Been Evaluated as Discriminative Features", "authors": ["Benyuan Meng", "Qianqian Xu", "Zitai Wang", "Xiaochun Cao", "Qingming Huang"], "abstract": "Diffusion models are initially designed for image generation. Recent research\nshows that the internal signals within their backbones, named activations, can\nalso serve as dense features for various discriminative tasks such as semantic\nsegmentation. Given numerous activations, selecting a small yet effective subset\nposes a fundamental problem. To this end, the early study of this field performs a\nlarge-scale quantitative comparison of the discriminative ability of the activations.\nHowever, we find that many potential activations have not been evaluated, such as\nthe queries and keys used to compute attention scores. Moreover, recent advance-\nments in diffusion architectures bring many new activations, such as those within\nembedded ViT modules. Both combined, activation selection remains unresolved\nbut overlooked. To tackle this issue, this paper takes a further step with a much\nbroader range of activations evaluated. Considering the significant increase in\nactivations, a full-scale quantitative comparison is no longer operational. Instead,\nwe seek to understand the properties of these activations, such that the activa-\ntions that are clearly inferior can be filtered out in advance via simple qualitative\nevaluation. After careful analysis, we discover three properties universal among\ndiffusion models, enabling this study to go beyond specific models. On top of\nthis, we present effective feature selection solutions for several popular diffusion\nmodels. Finally, the experiments across multiple discriminative tasks validate the\nsuperiority of our method over the SOTA competitors. Our code is available at this\nurl.", "sections": [{"title": "Introduction", "content": "Diffusion models [20, 9, 34, 33] are powerful generative models that progressively reconstruct images\nfrom Gaussian noises through a series of denoising steps. Typically, a U-Net [35] is trained as\nthe noise predictor backbone to perform denoising. Recently, the impressive generative capability\ninspires the application to discriminative tasks such as semantic segmentation [49, 55] or semantic\ncorrespondence [28, 53]. In this direction, diffusion feature is one simple yet effective approach,\nwhere the intermediate signals, named activations, are extracted from the pre-trained diffusion U-Net\nas dense features [2, 55, 53, 28, 50, 12, 13]."}, {"title": "Related Work", "content": "Diffusion models for discrimination. We discuss three mainstream ways to use diffusion models\nfor discrimination. (i) Diffusion classifier [25, 6] utilizes Bayes' theorem to transform a pre-trained\ndiffusion model into an image classifier. This method enjoys a theoretical guarantee and does\nnot need additional training. However, it is limited to image-level tasks. (ii) The second way is\nto model discriminative tasks as image-to-image generation tasks with diffusion models [21, 22].\nThis method is suitable for various dense vision tasks but requires heavy training. (iii) Diffusion\nfeature [2, 55, 53, 38, 28], the focus of this paper, follows the traditional practice of feature extraction\nto pursue the balance between wider applicability and less training. This makes it adaptable for\ndifferent tasks and alleviates the training needs for the diffusion model. Only small downstream\nmodels may require training.\nThe diffusion feature approach has seen various improvements. Some techniques toggle the input\nsettings of diffusion models, such as seeking better timesteps [57, 28]. The others add trainable\nparameters outside the diffusion model to refine the outputs or provide an efficient fine-tuning\nalternative [55, 26, 43]. Additionally, some studies explore completely training-free methods that\nutilize spatial attention information [48, 40].\nDespite the progress, previous methods only consider activations between neighboring modules,\nleaving many potential candidates unevaluated. Our work shows that such an overlook can hinder\nmodel performance. By introducing a more comprehensive feature selection solution, our method\ncould generically enhance both existing and future diffusion feature approaches.\nAnalysis on model properties. The inner properties of neural networks have consistently received\nmuch attention [4, 31]. For transformers, Geva et al. [14] show that the feed-forward layers act as\nkey-value memories and are interpretable for humans. Amir et al. [1] extend this insight to vision\ntransformers and put it into practical applications such as image classification, while Vilas et, al. [42]\ntry to make more detailed interpretation of ViT activations.\nThe methodology of these studies provides valuable guidance for our research. However, since\ndiffusion models are trained for generation, our study relies more on qualitative analysis and feature\nvisualization compared to previous work."}, {"title": "Preliminaries: Architecture of Diffusion U-Nets", "content": "Diffusion models typically involve a forward pass and a reverse pass [20]. During the forward\npass, noise is gradually added to a clean image $x \\in R^{3 \\times w \\times h}$ until the image resembles Gaussian\nnoise, where w and h denote the width and height, respectively. This process can be denoted by\n$x_t \\sim q(x_t | x_0)$, where q represents the noise posterior and t is the timestep. In the reverse pass, an\nend-to-end neural network $e_\\theta$, as parameterized by $\\theta$, learns to predict the noises and thus reconstruct\nthe image. One such denoising step can be denoted as $ \\epsilon = \\epsilon_\\theta(x_t, t, c)$, where $ \\epsilon$ is the predicted\nnoise, and c is the condition that describes the expected image content. Although there are alternative\nformulations for diffusion models [36, 27, 37], they all rely on this neural network backbone $ \\epsilon_\\theta$. This\nstudy focuses on this backbone, typically implemented as U-Net [35], rather than other components\nof diffusion models. Next, we will detail the architecture of the diffusion U-Net and standardize the\nterminology referring to different parts of the model, as shown in Figure 2.\nWe start with an overview. Following the initial U-Net design [35], the U-Net has three main stages:\ndown-stage, mid-stage, and up-stage. The down-stage reduces the resolution of activations, while the\nup-stage increases it. Both stages contain multiple resolutions, in each of which the activations share"}, {"title": "Distinct Properties of Diffusion U-Nets", "content": "The diffusion U-Net has many interesting properties, but now we only focus on those distinct from the\nknowledge of traditional U-Net [35] or ViTs [41]. As shown in Figure 3, this section highlights three\nnoticeable properties, each of which corresponds to a different level of the diffusion U-Net architecture\ndescribed in Section 3. Notably, these properties can be universally observed in different samples\nand diffusion models, though all visualization in the main content is conducted on the same\nimage and SDXL for consistency. Additional visualization is provided in Appendix A. Besides, the\nomitted properties are available in Appendix E."}, {"title": "Asymmetric Diffusion Noises", "content": "The first property is at the macro level and closely related to the overall diffusion process. It is\ncommon that high-frequency signals are typically noisy [4, 31]. This phenomenon can also be\nobserved in diffusion U-Nets, especially within the increment activations of residual connection.\nHowever, this does not mean that low-frequency signal activations in diffusion U-Nets are free from\nnoises. As shown in Figure 3(a), the diffusion process introduces a new type of noise that also impacts\nlow-frequency signals. This is not surprising since the diffusion process requires the backbone to\nprocess noisy inputs and predict noises as outputs. As a result, activations near the inputs or outputs,\nregardless of their frequency, also suffer from such noises. Considering the special cause, we name\nsuch noises diffusion noises.\nHow does the influence of diffusion noises spread across diffusion U-Nets? As shown in Figure 3(a),\ndiffusion noises exist throughout the entire down-stage, with a decreasing magnitude. Remarkably,\nduring the early half of the up-stage, activations are rather clean, with no perceivable diffusion noises.\nOnly in the latter half do diffusion noises start to resurface. As shown in Appendix A, such an\nasymmetric pattern is consistent across activations in different diffusion models. It is noticeable that,\nalthough being universal among diffusion U-Nets, this property was not reported in the research of\ntraditional U-Nets [54], to the best of our knowledge.\nSimilar to common high-frequency noises, diffusion noises can degenerate feature quality. Hence,\nthis property can serve as a criterion for identifying and filtering out sub-optimal activations, which\nwe will delve into in Section 5."}, {"title": "In-Resolution Granularity Changes", "content": "The second property is at the in-resolution level and closely related to recent advances in diffu-\nsion architectures. Specifically, the design of U-Net follows the idea of resolution hierarchy [35].\nConsequently, the overall architecture displays a fine-coarse-fine granularity trend, looking like\nthe alphabet \"U\u201d. Traditional U-Nets implement this architecture with a relatively large number of\nresolutions, while each resolution is typically small, equipped with two or three simple convolutional"}, {"title": "Locality without Positional Embeddings", "content": "For the third distinct property, we delve into the sub-module level, i.e., the blocks in embedded ViT\nmodules. Positional embeddings, which are widely used in language transformers [41] and ViTs [11],\naim to provide spatial information for each input token. Consequently, the activations of traditional\nViTs display strong positional information, where the latent pixel resembles nearby pixels more than\nthose that are semantically similar but far away [1]. This phenomenon is significant for the layers\nclose to the inputs. When the layer goes deeper, the tokens are refined with semantic information,\nmaking the activations display less positional information. However, only in the last few layers, such\npositional information becomes negligible.\nIn contrast, ViT modules in diffusion U-Nets do not use positional embeddings [34, 33], perhaps\nbecause the other U-Net components have provided sufficient spatial cues. This change results in\ndistinct properties of the activations. On one hand, positional information is negligible for most\nactivations despite how near they are to the inputs. On the other hand, the queries and keys of self-\nattention still display non-negligible positional information, marked with orange circles in Figure 3(c).\nSpecifically, the latent pixel on the horse's neck is a light blue color, similar to the pixels to its left\nthat actually represent the background. In contrast, the pixels above the circle are in purple color,\nthough they also represent the horse's neck. Such comparison shows that a latent pixel is more similar\nto other pixels that are spatially near it than those semantically closer to it.\nWe name this phenomenon locality since it has a different mechanism from that induced by positional\nembeddings. As pointed out in [11], self-attention allows ViT to integrate global and local information\neven in the shallow layers, and the attention scope enlarges w.r.t. layer depth. Even without positional\nembeddings, self-attention activations are generally consistent with this insight, but the magnitude\nhas greatly reduced. As shown in Figure 3(c), in shallow activations, locality exists but is inherently\nweaker than that induced by positional embeddings, such that more semantic information is reserved.\nAs the layer goes deeper, locality quickly diminishes.\nSince positional information can degrade the quality of activations [1], its absence has the potential\nto enhance the activations in ViT modules. Moreover, locality can play a special role in activation\nfiltering, as presented in Section 5."}, {"title": "Universality of Three Properties", "content": "Although the visualization in Figure 3 is conducted only on SDXL, the scope of these properties is\nnot limited to the specific architecture. Further evidence supporting the following claims is available\nin Appendix A.\n(i) Diffusion noises directly arise from the diffusion process. Hence, it is promising to extend this\nproperty to other diffusion backbones.\n(ii) In-resolution granularity changes come from the \"fatness\u201d of U-Nets, making it applicable to\nmore traditional U-Nets.\n(iii) Locality originates from the self-attention mechanism in ViT architectures, so it is broadly\napplicable to standalone or embedded ViTs where positional embeddings are absent."}, {"title": "Enhanced Feature Selection from Diffusion U-Nets", "content": "So far, we have had a more comprehensive understating of the properties of diffusion U-Nets. All\nthese properties, especially in-resolution granularity changes, encourage us to reconsider the feature\nselection solution, with a special emphasis on the activations in ViT modules. With these properties,\nwe are also able to filter out many low-quality activations qualitatively, followed by a thus simplified\nquantitative comparison."}, {"title": "Qualitative Filtering", "content": "Avoiding Diffusion Noises. As shown in Figure 4(a), diffusion noises tend to degrade the quality\nof activations. Hence, it is natural to filter out the activations severely affected by diffusion noises\nfrom the candidate pool. Specifically, according to the asymmetric trend of diffusion noises, we only\nconsider activations in the early half of the up-stage, which are rather clean. This approach will\nsignificantly reduce the number of candidate activations and simplify the quantitative comparison.\nAvoiding Self-Attention Locality. The locality in self-attention modules is another important factor\nthat can degrade the activations. The empirical evidence in Figure 4(b) demonstrates that these\nactivations are generally inferior to the others, such as those from cross-attention layers or the outputs\nof ViT basic blocks. Consequently, it is rather safe to filter out most activations in self-attention\nmodules from our candidate pool.\nUsing Locality to Suppress Diffusion Noises. So far, the activations in the candidate pool are\nclean and free from locality. However, all these activations are low-resolution ones since high-\nresolution activations are generally noisy and thus filtered out. This is unfavorable since some\ndetailed information might only exist in high-resolution activations. To address this issue, we exploit\na side effect of self-attention locality. Specifically, as indicated in Figure 4(c), locality can help\nsuppress diffusion noises via its focus on spatial structures. Although locality is sub-optimal, it is\nstill superior to severe diffusion noises. In view of this, the candidate pool reserves self-attention\nactivations extracted from the later half of the up-stage.\nWe have filtered out many candidate activations based on the distinct properties of diffusion U-Nets.\nAdditionally, all increment activations in residual connection can be further filtered out since they\nintroduce high-frequency noises [4, 31]. After such qualitative filtering, a small but high-quality\ncandidate pool is available. Taking SDXL as an example, the number of candidates decreases from\n279 to 63, i.e., a 78% reduction. Next, we explain how to conduct this quantitative comparison briefly."}, {"title": "Quantitative Comparison", "content": "The quantitative comparison follows the protocol of [2]. Specifically, given an input image xo, we\nfirst perform the forward pass with a pre-defined timestep t to generate the noisy image xt. Then,\nthe U-Net backbone conducts one denoising step. Instead of collecting the model output e, we\ngather U-Net activations and consolidate them to the candidate pool, as described in Section 5.1.\nAfterward, each activation is individually fed to a downstream model to evaluate its discriminative\npotential, with the details of the model described in Appendix C. Such comparison is made fair by\nusing the same dataset for all activations. Finally, we can obtain the capability ranking of activations\nfrom each resolution, according to which features can be selected wisely. Notably, we conduct such\ncomparisons on multiple datasets to guarantee generalizability, since the best activations may differ\namong different scenes [2]. Thanks to qualitative filtering, the time cost for each (model, dataset)\npair has been reduced by more than one week, equipped with Nvidia(R) RTX 3090 GPUs.\nGiven the capability ranking, feature selection is in fact flexible, as it is possible to combine multiple\nactivations and specifically tune the choice for a task. For practicality, we provide generic combina-\ntions of features for SDv1.5 and SDXL in Appendix B which are likely to perform well generally,\naccording to the results detailed in Appendix D. Each of them consists of four activations mainly\nfrom ViT modules rather than inter-module positions. Taking SDv1.5 as an example, one of the\nfour selected features is from one self-attention layer in the highest resolution, which utilizes our\nobservation in Section 4."}, {"title": "Experimental Validation on Multiple Discriminative Tasks", "content": "To validate the effectiveness of our feature selection solution, the experiments are conducted on three\npopular discriminative tasks: semantic correspondence, semantic segmentation, and label-scarce\nsegmentation. The SOTA methods for each task are selected as competitors. Besides, we also\ncompare with two baselines that select the conventional inter-module activations as features [2],\nnamed Legacy-v1.5 and Legacy-XL. For our method, we provide the following implementations:\n\u2022 Ours-v1.5 & Ours-XL: Features extracted from SDv1.5 and SDXL, respectively.\n\u2022 Ours-XL-t: For fairness, we further enhance SDXL features with some additional techniques\nthat are also adopted by SOTAs. The techniques we select, i.e., attention maps [55] and feature\namalgamation [2, 28, 57], are relatively simple and lightweight.\nMore experimental details are in Appendix C, such as task information, evaluation metrics, and\nimplementation details. Besides, Appendix D presents additional results not covered here."}, {"title": "Empirical Results on Semantic Correspondence", "content": "We present the experimental results for semantic correspondence in Table 1. From the results, we\nhave the following observations:"}, {"title": "Empirical Results on Semantic Segmentation", "content": "As shown in the left part of Table 2, Ours-XL-t and Ours-XL achieve state-of-the-art performance\non the semantic segmentation task (45.71 and 43.45 v.s. 40.89 on ADE20K), demonstrating its\neffectiveness and generalizability. Furthermore, the most competitive SOTA, Meta Prompts [43],\nintroduces a large number of trainable parameters and uses the diffusion U-Net recurrently, which is\nrather time-consuming. In contrast, our method delivers superior results with efficiency maintained.\nUnlike the results in semantic correspondence, Legacy-XL outperforms Legacy-v1.5 on CityScapes\n(71.67 v.s. 64.01), and the performance gap between Legacy-v1.5 and Ours-v1.5 is narrow (64.01 v.s.\n64.10). This is because this task utilizes a large-scale downstream model, which can significantly\nrefine the input features and thus reduce the gap in feature quality. Nevertheless, Ours-XL still\nachieves a significant improvement over Legacy-XL (74.47 v.s. 71.67)."}, {"title": "Empirical Results on Label-Scarce Segmentation", "content": "One advantage of diffusion features is the applicability to label-scarce scenarios [2]. For validation\nunder such conditions, we experiment on the label-scarce segmentation task, with results presented in\nthe right part of Table 2. The observations are generally similar to those on the semantic correspon-\ndence task. For example, Ours-v1.5 outperforms Legacy-v1.5 (60.2 v.s. 59.4), Legacy-XL is inferior\nto Legacy-v1.5 (53.0 v.s. 59.4), and Ours-XL is better than Ours-v1.5 (62.7 v.s. 60.2).\nNext, we focus on the comparison with the SOTA method, DDPM [2]. Although DDPM is a\nrelatively early study, it outperforms the other competitors and our most implementations. This result\nhas two reasons. On one hand, DDPM performs the diffusion process directly in the image space"}, {"title": "Conclusion and Future Work", "content": "In this study, we revisit the fundamental problem of feature selection from diffusion U-Nets. We\npoint out that prior arts only consider a limited range of potential activations. In contrast, we consider\na much wider range of activations as candidates, especially those extracted from the embedded ViT\nmodules. Given the large volume of the candidate pool, we first analyze the properties of diffusion\nU-Nets. The properties we find are universal such that our observations are not limited to the specific\ndiffusion architecture. Based on these properties, we qualitatively filter out many activations with low\nquality, facilitating the following quantitative comparison. On top of this, concrete feature selection\nsolutions are proposed for two popular diffusion models, i.e., SDv1.5 and SDXL. Finally, extensive\nexperiments on three discriminative tasks validate the effectiveness of our method.\nHowever, we are not sure whether our observations can generalize well to recently-developed DiT\nmodels [32] since it has a markedly different architecture from diffusion U-Nets. Thus, analyzing\nDiT models is a promising topic for future research."}, {"title": "Details of Our Method", "content": "In this section, we index all diffusion U-Net components in the order in which they are activated\nduring a network forward run. Besides descriptions in natural language, we also index the activations\nusing the same notations as used in our code implementation."}, {"title": "Feature Selection Solution for SDv1.5", "content": "We select four activations from SDv1.5, maintaining the same total feature channels as the conven-\ntional way to extract all output activations from each resolution.\n(i) The cross-attention query activation from the 2nd ViT, the 2nd resolution. This provides coarse\ninformation for simple scenes (up-levell-repeat1-vit-block0-cross-q).\n(ii) The inter-module activation after the 3rd ResModule, the 2nd resolution. This provides coarse\ninformation for complex scenes (up-level1-repeat2-res-out).\n(iii) The cross-attention query activation from the 2nd ViT, the 3rd resolution. This provides finer\ninformation with a higher resolution (up-level2-repeat1-vit-block0-cross-q).\n(iv) The self-attention key activation from the 1st ViT, the 4th resolution. This extracts features\nfrom the highest resolution, harnessing the noise suppression effect of self-attention locality\n(up-level3-repeat0-vit-block0-self-k).\nWe omit the index of basic blocks in ViTs, as SDv1.5 only contains one-layer ViTs. Additionally,\nwe totally ignore the lowest resolution, as its activations have very low resolution (8 \u00d7 8 if the input\nimage is 512 \u00d7 512), suggesting inferiority, as supported by the quantitative comparison."}, {"title": "Feature Selection Solution for SDXL", "content": "Four activations are selected from SDXL, trying to get similar total feature channels to the SDv1.5\nfeature selection solution.\n(i) The output activation after the 8th basic block, the 1st ViT, the 1st resolution. This provides\nrelatively coarse information for simple scenes (up-level0-repeat0-vit-block7-out).\n(ii) The output activation after the 6th basic block, the 1st ViT, the 1st resolution. This provides\nrelatively coarse information for complex scenes (up-level0-repeat0-vit-block5-out).\n(iii) The cross-attention query activation from the 1st basic block, the 1st ViT, the 2nd resolution.\nThis provides relatively fine information for simple scenes (up-level1-repeat0-vit-block0-cross-\nq).\n(iv) The output activation after the 1st basic block, the 1st ViT, the 2nd resolution. This provides\nrelatively fine information for complex scenes (up-levell-repeat0-vit-block0-out).\nWe ignore the highest resolution since it is affected by diffusion noises and lacks ViTs from which\nwe can extract self-attention activations. However, if the downstream model is strong enough to\nlearn to suppress noises, it may be possible to additionally extract inter-module activations from this\nresolution to harness more information."}, {"title": "Feature Selection Solution with Additional Techniques", "content": "For the setting Ours-XL-t, we mainly utilize two simple techniques that are also adopted in some\nSOTA methods: (i) We additionally extract attention maps, i.e., the similarity scores of cross-attention\nquery and key, as dense features [55, 49]. Such attention maps are closely related to the semantics of\nprompts, thus providing important supplementary information. Despite its usefulness, this technique\nonly adds a few additional channels to the features. (ii) We amalgamate features from different\nmodels [2, 28, 57] through simple concatenation. This technique is a common practice and can be\nseen as an extension of amalgamating different activations together as a whole feature. We next\nexplain what activations are selected to implement the two techniquess.\nWe first extract features according to the feature selection solution for SDXL. Afterward, we extract\nadditional features from SDv1.5:"}, {"title": "Alternative Feature Selection Solution with Additional Techniques", "content": "Large-scale datasets for semantic segmentation mostly consist of images of complex scenes. In such\ncases, we find attention maps can be too noisy to be useful. Therefore, we discard the attention map\ntechnique and the entire SDv1.5 model, as it is weaker compared to the newer SDXL and Playground\nv2 models. To compensate for the loss of activations, we select additional activations from SDXL\nand Playground v2.\nFrom SDXL, we select all the activations as described in the feature selection solution for SDXL and\nselect one additional activation: the upsampler output activation from the 2nd resolution, to harness\nhigh-resolution information (up-levell-upsampler-out). From Playground v2, we select the following\nactivations:\n(i) The output activation after the 6th basic block, the 1st ViT, the 1st resolution (up-level0-repeat0-\nvit-block5-out).\n(ii) The cross-attention query activation from the 1st basic block, the 1st ViT, the 2nd resolution\n(up-level1-repeat0-vit-block0-cross-q).\n(iii) The upsampler output activation from the 2nd resolution (up-levell-upsampler-out)."}, {"title": "Experimental Details", "content": ""}, {"title": "Semantic Correspondence", "content": "Task and Dataset. Semantic correspondence [17] involves finding a pixel in an image that semanti-\ncally matches another keypoint pixel in a reference image, such as the hind legs of two different cats.\nWe conduct experiments on the SPair-71k dataset [29].\nEvaluation Metric. PCK@0.1img(\u2191) and PCK@0.1bbox(\u2191) are used, following the widely-adopted\nprotocol reported in [29]. These two metrics mean the percentage of correctly predicted keypoints,\nwhere a predicted keypoint is considered to be correct if it lies within the neighborhood of the\ncorresponding annotation with a radius of 0.1 \u00d7 max(h, w). For PCK@0.1img/PCK@0.1bbox, h, w\ndenote the dimension of the entire image/object bounding box, respectively.\nSOTA Competitors. We provide the results from four SOTA methods: DINO [1] and DHPF [30] as\nnon-diffusion-feature methods, as well as DIFT [38] and DHF [28] as diffusion feature methods.\nImplementation Details. The semantic correspondence task can be done via the nearest neighbor\nalgorithm [39], which is unsupervised and training-free [29]. We add one additional trainable\nconvolutional layer before applying the nearest neighbor algorithm to refine the input features, which\nis also adopted by some SOTAs including DHF. The model is trained for two epochs, each containing\n5,000 sample pairs, following conventional settings. Our implementation is derived from DHF, and\nwe keep all hyper-parameters at their default settings."}, {"title": "Semantic Segmentation", "content": "Task and Dataset. Semantic segmentation [17] is essentially pixel-level classification. For this task,\nwe choose the ADE20K dataset [56] with over 20k annotated images of 150 semantic categories, and\nthe CityScapes dataset [7], which contains 5,000 fine annotated images of urban street scenes.\nEvaluation Metric. We use mIoU metric, which is the mean over the IoU performance across all\nsemantic classes [17]. For each image, IoU (Intersection over Union, \u2191) is defined by #(overlapped\npixels between the prediction and the ground truth) / #(union pixels of them).\nSOTA Competitors. We choose three diffusion feature SOTA methods as competitors. ODISE [49]\nis an early method with a simple implementation. VPD [55] is another early study of this field, which\nintroduces additional text adapter modules for improvement. Meta Prompts [43] is a newer method\nand shows significant improvements. We also report the performance of MaskCLIP [10], which is\nincluded as a competitor in the ODISE study.\nImplementation Details. Both VPD and Meta Prompts perform full-scale fine-tuning on the diffusion\nU-Net using feedback from the discriminative task. This heavy fine-tuning does not entirely comply\nwith the motivation of diffusion feature, which seeks a balance between wider applicability and less\ntraining, and is hard to extend to the larger SDXL model. Therefore, we keep the entire diffusion\nmodel frozen instead. As the setting has been changed, the performance of VPD and Meta Prompts\nin Table 2 is based on our experiments, not the reported results from the original papers. Our\nimplementation directly uses the hyper-parameters reported in Meta Prompts."}, {"title": "Label-Scarce Segmentation", "content": "Task and Dataset. Using features from a pre-trained diffusion model ensures good performance\neven when labeled training data is scarce [2]. For this setting, we use a dataset collected in [2] and\nexperiment on its Horse-21 subset, the data of which is sourced from LSUN [51]. This subset contains\nonly 30 labeled training images to be consistent with the intuition. The semantic segmentation in the\nlabel-scarce scenario also uses the mIoU metric.\nSOTA Competitors. We select the SOTA diffusion feature approach, DDPM [2], as the major\ncompetitor. We also include other representative segmentation methods: DatasetDDPM, MAE [18],\nSwAV [3], which are all reported in [2].\nImplementation Details. Following DDPM [2], the downstream model is an ensemble of ten simple\nMLP networks, each conducting pixel-wise classification. The simplicity of the model is intended to\ndemonstrate the innate capability and generalizability of diffusion models. Our implementation is\nderived from DDPM with only batch size changed among all hyper-parameters. We use a larger batch\nsize for faster experiments as a smaller one does not improve performance. Additionally, this is\nalso the setting for the quantitative comparison, as the compact size of the dataset can enhance\nefficiency."}, {"title": "Additional Experimental Results", "content": ""}, {"title": "Generalizability across Different Scenes", "content": "As stated in Section 5, it is preferable to conduct the quantitative comparison across multiple datasets\nand choose activations that are optimal for each. This approach can enhance the generalizability of\nthe selected features. To evaluate the generalizability of our features, we conducted an additional\nexperiment, with results presented in Table 3.\nIn this experiment, we design an alternative feature selection solution for SDXL, based solely on\nquantitative results from a single dataset consisting of simple scenes. In this solution, we extract both\noptimal and slightly sub-optimal activations, maintaining the same total number of feature channels\nas the standard solution. The alternative solution achieves higher performance on the simple scene\nit is based on but performs significantly worse on the other scene. Therefore, we conclude that our\nstandard feature selection solution achieves generalizability across different scenes, albeit with a\nslight performance drop compared to features specifically selected for each scene."}, {"title": "Quantitative Comparison Results", "content": "In this part, we present all the quantitative comparison results obtained following the protocol\ndescribed in Section 5. These results are from a dataset of simple scenes (Horse-21 [2]) and a\ndataset of complex scenes (Bedroom-28 [2]). Since SDXL and Playground v2 share the same U-Net\narchitecture, their results are shown in the same tables. We display the results from the lowest\nresolution in Table 4 and the results from the middle resolution in Table 5. We have also done a\nquantitative comparison on SDv1.5, and the results are shown in Table 6."}, {"title": "Other Properties of Diffusion U-Nets", "content": ""}, {"title": "High-Frequency Noises in Increment Activations", "content": "Typically, high-frequency signals are usually noisy [4, 31]. This applies to the diffusion U-Net, and\nwe can further examine what activations are more vulnerable to such noises. To be specific, the\ndiffusion U-Net contains many residual connection structures, and their increment activations are\nhigh-frequency signals prone to noises. Such increment activations include:\n(i) The increment branch of ResModule. These activations are moderately noisy and thus usually\nless effective than inter-module activations.\n(ii) The feed-forward layer activations within ViTs. These activations are also moderately noisy.\nHowever, this results from their significantly more channels compared to other activations,\nwhich can reduce their noise magnitude.\n(iii) The self-attention value activations within ViTs. These activations are severely noisy and suffer\nsignificant degradation as they are the nested inner increments within embedded ViTs.\nThese activations are visualized in Figure 16 for a clearer illustration. Additionally, the residual\nactivations within ViTs are at the same time also increments to the main inter-module residual.\nHowever, these activations are not obviously affected by high-frequency noise, possibly due to their\ndual role as ViT residuals."}, {}]}