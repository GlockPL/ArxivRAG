{"title": "A Grey-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse", "authors": ["Zhongliang Guo", "Lei Fang", "Jingyu Lin", "Yifei Qian", "Shuai Zhao", "Zeyu Wang", "Junhao Dong", "Cunjian Chen", "Ognjen Arandjelovi\u0107", "Chun Pong Lau"], "abstract": "Recent advancements in generative AI, particularly Latent Diffusion Models (LDMs), have revolutionized image synthesis and manipulation. However, these generative techniques raises concerns about data misappropriation and intellectual property infringement. Adversarial attacks on machine learning models have been extensively studied, and a well-established body of research has extended these techniques as a benign metric to prevent the underlying misuse of generative AI. Current approaches to safeguarding images from manipulation by LDMs are limited by their reliance on model-specific knowledge and their inability to significantly degrade semantic quality of generated images. In response to these shortcomings, we propose the Posterior Collapse Attack (PCA) based on the observation that VAEs suffer from posterior collapse during training. Our method minimizes dependence on the white-box information of target models to get rid of the implicit reliance on model-specific knowledge. By accessing merely a small amount of LDM parameters, in specific merely the VAE encoder of LDMs, our method causes a substantial semantic collapse in generation quality, particularly in perceptual consistency, and demonstrates strong transferability across various model architectures. Experimental results show that PCA achieves superior perturbation effects on image generation of LDMs with lower runtime and VRAM. Our method outperforms existing techniques, offering a more robust and generalizable solution that is helpful in alleviating the socio-technical challenges posed by the rapidly evolving landscape of generative AI.", "sections": [{"title": "Introduction", "content": "The field of generative artificial intelligence has witnessed unprecedented advancements, particularly in the domain of image synthesis and manipulation. State-of-the-art methodologies, exemplified by diffusion models such as Stable Diffusion (Rombach et al. 2022), have demonstrated remarkable capabilities in producing photorealistic imagery with exceptional fidelity and diversity. These technological breakthroughs have not only revolutionized creative industries but have also democratized access to sophisticated image editing tools, empowering users across various domains.\nHowever, the proliferation of such powerful technologies inevitably engenders a concomitant set of ethical quandaries and potential security vulnerabilities. Of particular concern is the prospect of data misappropriation and intellectual property infringement. The facility with which diffusion-based models can be employed to manipulate existing visual content presents a significant challenge to the integrity of digital assets. For instance, malicious actors could exploit Stable Diffusion to edit copyrighted images, effectively \"laundering\" them by removing or altering identifying features (Rombach et al. 2022). This process of \u201cde-copyrighting\u201d not only undermines the rights of content creators but also poses a threat to the economic ecosystems built around digital imagery, potentially destabilizing industries ranging from photography to digital art.\nIn the realm of machine learning security, adversarial attacks have emerged as a critical area of study (Lau et al. 2023a). These attacks aim to perturb the input of machine learning models in ways that are imperceptible to humans but can significantly disrupt the model's output. Interestingly, this concept of adversarial attacks provides a potential solution to the misuse of generative AI. By introducing carefully crafted, acceptable perturbations to data that could be misused, it may be possible to impede the ability of diffusion models to edit or manipulate such content effectively.\nCurrent protection techniques often rely heavily on extensive prior knowledge, specifically requiring full white-box access to the parameters of the target models (Liang et al. 2023; Salman et al. 2023; Liang and Wu 2023; Xue et al. 2024). This assumption is largely impractical in real-world scenarios, especially given the rapid pace of technological advancement in generative AI. The proliferation of model architectures and constant iterations of backbone networks create a dynamic landscape where protection methods quickly become obsolete. Developing a universal method that can effectively protect against the vast majority of generative models without detailed knowledge of their internal structures is exceedingly difficult. Furthermore, the diversity of model architectures means that a method designed for one particular model or family may be entirely ineffective against others. This lack of generalizability severely limits the practical applicability of such approaches in a rapidly changing technological environment.\nSecondly, while existing approaches successfully prevent infringer-desired alterations and introduce visible watermark-like artifacts in post-editing images as indicators of unauthorized manipulation (Liang et al. 2023; Xue et al. 2024), these approaches fail to comprehensively disrupt the semantic quality of generation as edited adversarial samples still remain the perceptual integrity with original clean image. Consequently, these methods struggle to achieve the goal of thoroughly degrading the semantic quality of manipulated outputs.\nGiven these limitations in current protective measures and the narrow scope of existing attack methods on diffusion models, addressing these critical gaps in the field requires a solution that meets several key criteria:\n\u2022 To ensure broader applicability and resilience against rapid technological advancements, the method should minimize dependence on white-box information of the target models or merely rely on white-box information that is public across the most popular current methods.\n\u2022 The method should cause significant degradation to the semantic generation quality of the most popular image editing and generation methods within the bounds of human visual sensitivity.\n\u2022 The method should exhibit strong transferability, easily generalizing across various models and architectures.\nIn light of these challenges, we aim to propose a new protection method targeting Variational Autoencoders (VAEs) hinge on a key observation: the vast majority of state-of-the-art generative models leveraging VAEs, where VAEs perform as a upstream bottleneck. This commonality presents a promising opportunity to design a more universal attack strategy targeting the LDMs, potentially addressing the need for a generalizable protection method.\nIn literature, existing attacks on VAEs primarily focus on corrupting the decoding process to lead to deteriorated reconstruction (Kuzina, Welling, and Tomczak 2022), which does not align with our objective of perturbing downstream diffusion process. Notably, the encoding posterior distribution of VAEs, often overlooked in previous attack methods, provides a more comprehensive representation of the latent space, which is the bottleneck of all downstream tasks, potentially leading to more effective and generalizable attacks.\nDrawing inspiration from the phenomenon of posterior collapse in VAEs' training (Razavi et al. 2019), where the latent variables fail to capture meaningful information about the input data, we suggest to intentionally induce this collapse as an attack strategy. Thus, we propose Posterior Collapse Attack (PCA), inducing collapse in the posterior distribution with our proposed new loss function, resulting worse semantic quality of edited image. As illustrated in Figure 1 and 2, our method effectively perturbs downstream LDMs in a near black-box manner (only 3.39% paprameters). This approach not only aligns with our goal of disrupting downstream tasks but also offers a more robust and transferable solution. Unlike previous methods that rely heavily on model-specific knowledge, our method merely leverages encoder of VAEs, providing a more universally applicable protection mechanism against unauthorized image manipulation across various generative AI architectures.\nOur contributions can be summarized as follows:\n\u2022 We introduce the Posterior Collapse Attack that merely exploits the common VAE structure in LDMs, utilizing a novel loss function to cause the posterior collapse. Our method induces a significant semantic collapse of LDM-edited images, with minimal model-specific knowledge and less computational resources.\n\u2022 Our experimental results demonstrate that PCA, operating with near black-box adversarial information, effectively prevent LDM-based image editing across various architectures and resolutions, outperforming existing methods in terms of transferability and robustness."}, {"title": "Related Work", "content": "Generation Models\nDiffusion Probabilistic Model (DPM) (Ho, Jain, and Abbeel 2020) has achieved state-of-the-art results in density estimation (Kingma et al. 2021) and sample quality (Dhariwal and Nichol 2021). These models are powerful due to their U-Net backbone, which suits the inductive biases of image-like data. However, they face challenges in inference speed and training costs, especially for high-resolution images. To address these limitations, researchers have developed approaches like Denoising Diffusion Implicit Models (DDIM) (Song, Meng, and Ermon 2021) to enhance sampling speeds, and explored two-stage processes. Latent Diffusion Models (LDM) (Rombach et al. 2022) use autoencoding models to learn a perceptually similar space with lower computational complexity, while VQ-VAEs (Yan et al. 2021; Razavi, Van den Oord, and Vinyals 2019) and VQGANs (Yu et al. 2022; Esser, Rombach, and Ommer 2021) utilize discretized latent spaces and adversarial objectives to scale to larger images. These advancements aim to overcome the challenges of complex training, data shortages, and computational costs associated with diffusion models, particularly for high-resolution image synthesis.\nAdversarial Attack\nThe field of adversarial machine learning was catalyzed by the seminal work of Szegedy et al. (2014), who first uncovered the vulnerability of neural networks. Subsequent research has led to the development of various attack methodologies (Goodfellow, Shlens, and Szegedy 2015; Madry et al. 2018). The implications of adversarial attacks raised significant concerns in critical applications. This has spurred the development of defensive strategies, such as adversarial training (Lau et al. 2023b) and input transformation (Lin et al. 2020b). We have seen the expansion of adversarial attack research into more complex domains (Liu et al. 2023; Guo et al. 2024a; Li et al. 2024; Zhao et al. 2024).\nAttack on VAEs. Adversarial attacks on VAEs have attracted significant research attention. Most existing research on VAE attacks focused on manipulating the input or latent space to affect reconstruction output (Tabacof, Tavares, and Valle 2016; Gondim-Ribeiro, Tabacof, and Valle 2018; Kos, Fischer, and Song 2018; Kuzina, Welling, and Tomczak 2022), aiming to generate adversarial examples that would be reconstructed as images from different classes. Our approach, however, differs significantly from these previous works. We specifically target the disruption of downstream tasks based on VAE encodings, such as diffusion models, rather than the VAEs' own reconstruction. Furthermore, we attack the parameters of the approximate posterior distribution produced by the encoder, instead of the latent variables themselves. This approach allows us to explore vulnerabilities in VAE-based systems, particularly in scenarios where VAEs serve as feature extractors or in applications relying on the distributional properties of encoded representations.\nAttack on Diffusion-based Image Editing. Recent research has paid attention to adversarial attacks against diffusion models for image editing. Liang et al. (2023) targeted"}, {"title": "Method", "content": "Problem Definition\nAdversarial attack aims to craft an imperceptible perturbation \u03b4, added on the clean image x as the adversarial sample xadv, resulting in the wrong or disruptive output of machine learning models. The key concept of the adversarial attack against LDM-based image editing can be summarized as two objectives:\nObjective 1: $min_\\delta D(f(x + \\delta), x + w)$ s.t. $||\\delta||_p \\le \\epsilon$,\nObjective 2: $max_\\delta D(f(x + \\delta), f(x))$ s.t. $||\\delta||_p \\le \\epsilon$,\nwhere $f(\u00b7)$ is a kind of LDM-based image editing method; w refers to a kind of watermark artifact; D(\u00b7) measures the perceptual distance between two inputs, indicating the visual consistency of two images in human visual perspective; $||\u00b7 ||_p$ means applying a constraint on the vector, in most cases, this serves to maintain the visual integrity of the adversarial sample, following the $l_\\infty$ norm. Notably, Objective 1 will cause similar results to Objective 2, i.e., there will be some distance in D's space between $f(x + \\delta)$ and x.\nExisting methods in the literature typically address either Objective 1 or Objective 2. However, both approaches often require extensive white-box information about the target model, particularly access to the neural backbone U-Net of the LDM. This heavy reliance on model-specific details limits their transferability and applicability across different LDM architectures, requiring more computing resource.\nOur method focuses primarily on Objective 2, but takes a fundamentally different approach. Instead of relying on detailed knowledge of the entire LDM pipeline, we exploit the inherent characteristics of LDM-based editing by targeting the VAE component, which is common across various LDM architectures. This strategy allows us to achieve the goal of maximizing the disparity between $f(x_{adv})$ and f(x) without requiring extensive access to model-specific information, particularly the compute-intensive and model-specific U-Net component. By concentrating on the VAE, our approach aligns more closely with real-world scenarios where full model access may not be available, providing an efficient solution to prevent infringers from exploiting LDM-based image editing outputs.\nPosterior Collapse\nVariational Autoencoder. A key observation driving our approach is the ubiquity of VAEs in the architecture of LDMs. VAEs serve as a foundational component across different LDM implementations, often with only minor variations between models. For instance, the VAE used in Stable Diffusion 2.0 is essentially a fine-tuned version of the one employed in Stable Diffusion 1.5. This commonality presents a strategic opportunity: by focusing on the VAE, we can potentially affect a wide range of LDMs without requiring detailed knowledge of their specific architectures.\nVAE can be considered as a probabilistic generative extension of the ordinary autoencoders. The encoder of a VAE aims at approximating the intractable posterior distribution of the latent variable z by a Gaussian distribution $q(z|x) = N(\\mu, diag(\\sigma^2))$, where diag denotes a diagonal matrix formed with vector $\\sigma^2$ as the diagonal entries. The training proceeds by minimising the KL divergence between the approximating variational distribution q(z) and the true posterior p(z|x), and the learning objective is:\n$L(x) = -E_{q(z|x)} [log p(x, z) \u2013 ln q(z|x)]$.(4)\nThe above loss is also known as negative ELBO. By observing the generating process p(x, z) = p(z)p(x|z), the loss can be written alternatively as:\n$L(x) = -E_{q(z|x)} [lnp(x|z)] + D_{KL}(q(z|x) || p(z))$,(5)\nwhere the first term is known as reconstruction error and the second term, the KL divergence between the variational distribution and the prior distribution, usually a standard normal distribution, that is p(z) = N(0, I). The KL divergence has a regularisation effect such that the variational distribution is \"close\" to a standard spherical Gaussian distribution.\nBy general consensus, training of VAE usually suffers an optimization issue called posterior collapsing (Razavi et al. 2019), that is the KL divergence term of Equation 5 dominates the overall loss such that all posteriors collapse to the uninformative prior and the reconstruction error or the likelihood term is ignored. We draw inspiration from this vulnerability and aim to propose adversarial attacks to cause the posterior collapse during inference stage.\nPosterior Collapse Loss Function. We aim to leverage the divergence measure between q(z|x) and a target distribution p*(z). The objective is to generate $x_{adv}$ by minimising the KL divergence between q(z|x) and p*(z). The KL divergence of two multivariate Gaussian distributions is:\n$D_{KL}(N_1 || N_2) = \\frac{1}{2} [n_2 \u2212 d + tr{\\Sigma_2^{-1}\\Sigma_1} + (\\mu_2 \u2212 \\mu_1)^T\\Sigma_2^{-1} (\\mu_2 \u2212 \\mu_1)]$,(6)\nwhere $N_1 = N(\\mu_1, \\Sigma_1),N_2 = N(\\mu_2, \\Sigma_2)$.\nFor this work, we set the attack target p*(z) to a zero mean Gaussian distribution p(z) = N(0, vI), where v > 0 is hyper-parameter that controls the disruptive effect. Since both p*(z) and q(z|x) are diagonal multivariate Gaussian distributions, Equation 6 becomes:\n$L_{KL}(x) = D_{KL}(q(z|x) || p^{*}(z))$\n$= \\frac{1}{2} (\\sum_{i=1}^{d} \\frac{\\sigma_i^2 + \\mu_i^2}{v} + - d + \\sum_{i=1}^{d} ln(\\frac{v}{\\sigma_i^2}))$.(7)\nTherefore, the attack formulation becomes:\n$min_\\delta L_{KL} (x + \u03b4)$ s.t. $||\u03b4||_\u221e < \u03f5$.(8)\nNote that the attack target distribution is independent from the input image x, making it a more general attack method. Successful attacks should make the encoded posterior uninformative or collapsed for a perturbed image, consequentially, the downstream image generation, i.e. Objective 2, will be disrupted accordingly as well.\nNote that we choose to introduce the loss $L$ with f-divergence for its generality. Both forward KL and reverse KL are specific cases of the general divergence measure. For example, the reverse KL divergence is recovered when we set f = ln. Comparing with the forward KL, the reverse KL, which is used in defining the VAE training loss and therefore also contributes to the posterior collapse issue, is found to perform better in disrupting the image generation. To see why, the reverse KL's expectation is taken w.r.t q(z|x) = N(\u03bc, diag(\u03c3\u00b2)), making the attack objective more dependent on the encoding mean and variance rather than the fixed hyperparameter v. Three different loss measures, mean squared error, forward KL and reverse KL, are plotted in Figure 3, where the attack variance hyperparameter v is set to unit for convenience. As expected, all three losses have the same minimum: \u03bc = 0, \u03c3\u00b2 = 1. However, it can be observed that the reverse KL provides a more nuanced loss compared with the symmetric MSE error. In comparison with the forward loss, reverse KL's loss surface provides more geometric information to guide the attack, which is manifested in the steeper gradient directions across the input domain. Thus, adversary generation can be achieved by maximizing the distributional gap using gradient ascent.\nTo craft adversarial samples, we use projected sign gradient ascent (Madry et al. 2018) for iterative updates. Our approach is inspired by the counterintuitive finding from Xue et al. (2024), where minimizing their loss function, contrary to the intuitive maximization, yielded better results. Following this insight, we explore both gradient directions by altering the sign of our final loss function, which will be discussed in appendix. Our update process follows the form:\n$x^{t+1} = P_{\\epsilon_x} (x^{t} + a sign\\nabla_{x^{t}} I (x^{t}))$,(9)\nwhere $P_{\\epsilon_x} (\u00b7)$ will apply the projection on $\u03b4_t$ in the $\\epsilon$-ball of $l_x$ norm, a is a hyperparameter to adjust the learning rate during the optimization."}, {"title": "Experiments Setup", "content": "Dataset. In our experiments, we utilized a 1000-image-subset of the ImageNet (Deng et al. 2009), as selected by Lin et al. (2020a). This choice aligns with established conventions in adversarial attack research. All images were resized to 512 \u00d7 512, ensuring consistency across evaluations.\nBaselines. We compared our approach against several state-of-the-art methods, AdvDM (Liang et al. 2023), PhotoGuard (PG) (Salman et al. 2023), MIST (Liang and Wu 2023), and SDS (Xue et al. 2024). To ensure a fair comparison, we use the gradient ascent version of SDS, aligning with the optimization settings of other methods. We fix t = 40 and \u03f5 = 16 for all methods, the rest left default. We implement our method with \u03b1 = 2, v = 1 \u00d7 10-8, the optimization follows the goal defined in Equation 8, which the opposite one will be illustrated in Appendix.\nVictim Models. Our experiments primarily focused on popular LDMs, specifically the lightweight Stable Diffusion 1.4 (SD14) and 1.5 (SD15). These models were chosen due to their widespread accessibility and higher potential for misuse, making them relevant targets for our study on protecting against unauthorized image manipulation. To assess the transferability across different resolution and architectures, we also tested the first 100 images on Stable Diffusion 2.0 (SD20) with 768\u00d7768, and Stable Diffusion XL (SDXL) with 1024 \u00d7 1024 for all methods. All reported results of our method leverage the VAE of SD15 as the surrogate model.\nVaried Prompts. To further evaluate the robustness and versatility of our method, we conducted inference using a diverse set of prompts. These prompts include: P1: an empty prompt; P2: a caption generated by BLIP (Li et al. 2022) with the clean image; P3: \"add some snow\" to represent weather modifications; P4: \"apply sunset lighting\" to test lighting adjustments; P5: \"make it like a watercolor painting\" to examine style transfer capabilities. Through this varied set of prompts, we aimed to assess our method's effectiveness across a wide range of potential image manipulation scenarios.\nImage Quality Assessment (IQA). To quantitatively assess the quality of the edited images pre- and post-attack, we employed five different IQAs. Peak Signal-to-Noise Ratio (PSNR) provides insight into the overall distortion introduced. Fr\u00e9chet Inception Distance (FID) offers a measure of the perceptual quality and generation diversity (Heusel et al. 2017). Structural Similarity Index (SSIM) captures changes in local patterns of pixel intensities (Wang et al. 2004). Learned Perceptual Image Patch Similarity (LPIPS) approximately measures human perceptual similarities (Zhang et al. 2018). Aesthetic Color Distance Metric (ACDM) evaluates the overall color consistency (Guo et al. 2024b).\nAgainst Defending Methods. To evaluate our attack's robustness against potential defenses, we considered two methods. First, we examined Adv-Clean, a filter-based approach designed to remove adversarial perturbations in diffusion models without requiring additional training. Second, we applied 3 \u00d7 3 Gaussian blur to simulate potential degradation of protected images during the distribution. We apply these defense on our attacked images and evaluate on SD15."}, {"title": "Results", "content": "Image Editing Results with Varied Prompts\nOur experimental results demonstrate the superior performance of our proposed PCA compared to existing methods. Figure 2 provides a visual comparison, while Table 1 presents quantitative results.\nAs observed from Figure 2, our method significantly outperforms existing approaches in terms of semantic disruption. Most other methods largely maintain an image structure similar to that of the clean image after editing. In contrast, our approach fundamentally alters the semantic information of the image, resulting in edited outputs that bear almost no resemblance to the infringer-desired images.\nEven when prompts containing image structure information are provided, our method continues to demonstrate superior performance. The generated images show significant discrepancies from the expected outcomes, effectively thwarting attempts at unauthorized manipulation. Moreover, our approach introduces quite noise patterns in the generated images, substantially degrading their quality. This degradation renders the manipulated images unsuitable for their intended purposes, whether it be unauthorized use, redistribution, or further editing. Our method not only disrupts the semantic content of the image but also compromises its overall visual quality. This comprehensive degradation ensures that even if an attacker manages to partially reconstruct some elements of the original image, the result remains unusable from both a semantic and aesthetic standpoint.\nThe quantitative results in Table 1 further corroborate the effectiveness of our method. Notably, our Posterior Collapse Attack achieves state-of-the-art results while requiring minimal knowledge of the opponent model, less computational time, and lower GPU memory usage compared to existing methods (as illustrated in Figure 1).\nThese results demonstrate that, our approach effectively disrupts image semantics while maintaining efficiency and generalizability. This strategy challenges generative models\u2019 ability to produce coherent edits, operating with minimal model-specific information in a near black-box scenario.\nTransferability\nAs shown in Figure 4, our method demonstrates superior transferability compared to existing approaches when tested on both SD20 and SDXL models. Across all five IQA metrics, our approach consistently achieves the highest normalized scores of 1.0, indicating the best attack performance. In contrast, other methods show varying degrees of effectiveness, with PhotoGuard and MIST performing moderately well, while AdvDM and SDS exhibit limited transferability. These results validate our hypothesis regarding the limitations of current protection techniques that rely heavily on white-box access to target models. Our method, by minimizing dependence on model-specific knowledge, achieves better performance across diverse model architectures. This generalizability is crucial in addressing the challenges posed by rapidly evolving generative AI models, offering a more practical and enduring solution in this dynamic field.\nAgainst Defense\nAs shown in Figure 5, our method demonstrates robust performance against the Adv-Clean defense, consistently achieving the highest normalized scores of 1.0 across all five IQA metrics. This indicates superior resilience to this type of defense compared to other methods. PhotoGuard and MIST show moderate effectiveness, while AdvDM and SDS exhibit limited resistance to Adv-Clean. Against 3 \u00d7 3 Gaussian blur, our original method shows comparable performance, particularly in PSNR and ACDM, indicating resilience in overall image quality and color manipulation. With a simple adaptive attack technique (denoted as ours*), our method achieves superior performance across all metrics, consistently outperforming other approaches. This improvement demonstrates our approach's flexibility and robustness. While the non-adaptive version has already performed well in some metrics, the adaptive version's dominance across all metrics highlights our method's ability to maintain effectiveness under various defenses. This adaptation allowing our method to quickly recover its original performance when under specific defense strategy. These results underscore the versatility of our attack strategy, proving its capability to overcome diverse defense mechanisms with or without minor adjustments."}, {"title": "Conclusion", "content": "In this paper, we proposed the Posterior Collapse Attack, a novel method for disrupting LDM-based image editing, requiring fewer parameters \u2013 merely 3.39% of the whole model, Which makes our method operates faster and consumes less VRAM than existing approaches. We leverage a novel loss function targeting the VAE encoder of LDMs, induces collapse in the VAE's posterior distribution to significant disrupt the image semantic during LDM editing. Experimental results demonstrate that PCA significantly degrades the quality of LDM-edited images, causing a collapse in their semantic content. This approach proves highly effective in protecting images from unauthorized manipulation across various LDM architectures. Our work contributes to the ongoing efforts to secure digital assets in an era of rapidly advancing generative AI, alleviating the socio-technical challenges posed by malicious AI misuse."}, {"title": "Appendix", "content": "Derivation of the Loss Function\nFor the Equation 6 to Equation 7 in the main paper, we provide a derivation to demonstrate how we obtained Equation 7 from Equation 6:\nGiven two multivariate Gaussian distribution $N_1(\\mu_1, \\Sigma_1)$ and $N_2(\\mu_2, \\Sigma_2)$, where $N_1$ and $N_2$ refer to the posterior distribution q(z|x) and our target prior distribution p*(z). We set the target attack p*(z) to a zero mean Gaussian distribution p*(z) = N(0, vI). Since both p* (z) and q(z|x) are diagonal multivariate Gaussian distributions, equation can be derived as:\n$D_{KL}(N_1 || N_2) = \\frac{1}{2} [ln(\\frac{|\\Sigma_2|}{|\\Sigma_1|}) - d + tr{\\Sigma_2^{-1}\\Sigma_1} + (\\mu_2 - \\mu_1)^T\\Sigma_2^{-1} (\\mu_2 - \\mu_1)]$\n$= \\frac{1}{2} [ln(\\prod_{i=1}^{d} \\frac{\\sigma_i^{2}}{v}) - d + \\sum_{i=1}^{d} \\frac{\\sigma_i^{2}}{v} + \\sum_{i=1}^{d} \\frac{\\mu_i^{2}}{v}]$\n$= \\frac{1}{2} [- d + \\sum_{i=1}^{d} \\frac{\\sigma_i^{2} + \\mu_i^{2}}{v} + \\sum_{i=1}^{d} ln(\\frac{v}{\\sigma_i^{2}})]$\n$= \\frac{1}{2} [\\sum_{i=1}^{d} (\\frac{\\sigma_i^{2} + \\mu_i^{2}}{v} - ln\\frac{\\sigma_i^{2}}{v} - 1)].$\nNotably, in our implementation, we ignore the ln v term, because ln v is a constant, which will not influence the performance. Therefore, the final loss function in our implementation is:\n$D_{KL}(N_1 || N_2) ~ L_{KL}(x) = \\frac{1}{2} \\sum_{i=1}^{d} (\\frac{\\mu_i^{2} + \\sigma_i^{2}}{v} - lno_i^{2} - 1 + ln(\\frac{v}{\\sigma_i^{2}}))$.\nGradient Direction\nAs briefly mentioned in the main paper, our investigation was motivated by the work of Xue et al. (2024), leading us to examine both the minimization (-) and maximization (+) of the loss function $L_{KL}$.\nTo provide context for our analysis, we revisit the two primary objectives outlined in the main paper:\nObjective 1: ming D(f(x + \u03b4), x + w) s.t. $||\u03b4||_p \u2264 \u03f5$,\nObjective 2: max8 D(f(x + d), f(x)) s.t. $||\u03b4||_p \u2264 \u03f5$.\nIt is crucial to note that these objectives, while seemingly different, share a common goal: to prevent the infringer-desired output. This commonality allows us to consider Objective 1 as a variant of Objective 2.\nFor a comprehensive evaluation of our method's performance, we employ two distinct approaches:\n\u2022 Evaluation of Objective 1:\nWe compute IQAs between clean images and edited adversarial samples, represented as IQA(x, f(xadv)). This approach aligns with some baselines studies (Liang et al. 2023; Xue et al. 2024).\n\u2022 Evaluation of Objective 2:\nWe calculate IQAs between edited clean images and edited adversarial samples, denoted as IQA(f(x), f(xadv)). This metric is consistent with the results presented in the main paper.\nTwo Variants of Posterior Collapse\nPosterior collapse in VAEs is a phenomenon that can manifest in two primary forms: diffusion collapse and concentration collapse. Let X be the input space and Z \u2282 Rd be the latent space of a VAE. The posterior distribution in a VAE is denoted by q(z | x), where x \u2208 X and z \u2208 Z.\nHere we give definitions of two kind of collapses and explain when the phenomena will occur:\n1. Diffusion Collapse:\nWe witness diffusion collapse when the posterior distribution becomes overly dispersed:\nq(zx) \u2248 U(Z) as \u03c32 \u2192 \u221e,(10)\nwhere U(Z) denotes the uniform distribution over Z. In this case, the encoder produces highly uncertain latent representations for all inputs.\n2. Concentration Collapse:\nIn this scenario, the posterior distribution becomes excessively concentrated, approaching a Dirac delta function:\nq(z | x) \u2192 \u03b4(z \u2013 \u03bc\u03bf) as \u03bc, \u03c32 \u2192 0,(11)\nwhere \u03b4 is the Dirac delta function, and \u00b5o is typically the mean of the prior distribution. This collapse results in the encoder mapping all inputs to nearly identical latent representations.\nBased on the above definitions, we propose two strategies to cause these two collapse phenomena:\n1. Maximization Strategy (PCA+):\nConsider the optimization problem:\nmax LKL (x + \u03b4) s.t. $||\u03b4||_\u221e < \u03f5$.(12)\n\u03b4\nWhen v = 1, the KL divergence simplifies to:\n$L_{KL}(x) = \\frac{1}{2} \\sum_{i=1}^{d} (-lno_i^2 \u2212 1 + \u03bc_i^2 + \u03c3_i^2)$.(13)\nAnalysis shows that maximizing this leads to:\n\u03bc_i \u2192 \u221e and \u03c3_i2 \u2192 \u221e for all i,(14)\nresulting in diffusion collapse.\nPCA+ optimization pushes the latent distribution N(\u03bc, \u03c3\u00b2) away from the prior distribution N(0, I), effectively creating an out-of-distribution (OOD) scenario. This divergence introduces a significant domain shift for the downstream diffusion model, moving the data distribution into regions where the model's knowledge is limited or inaccurate. Consequently, the diffusion model struggles to perform proper inference on the transformed data, as its fundamental assumptions about the data distribution are violated.\nThis scenario can be conceptualized as an expansion of the latent space, where data is forced to extend beyond the normal domain of the downstream diffusion model's understanding. This manipulation can be seen as a method to \"confuse\u201d the diffusion model by presenting it with data that falls outside its expected input distribution. The effect is analogous to creating a domain shift in the context of VAEs, where the latent space is manipulated to fall outside the expected distribution that the decoder was trained on.\nIntuitively the downstream LDM will pay attention to move those OOD points into the known distribution, causing results consistent with Objective 1 expectations.\n2. Minimization Strategy (PCA-):\nConsider the optimization problem:\nmin LKL (x + d) s.t. $||\u03b4 ||_\u221e < \u03f5$.(15)\n\u03b4\nAs v \u2192 0+, the dominant term becomes:\n$\\frac{\\mu_i^2 + \\sigma_i^2}{\u03c5}$(16)\nMinimizing this expression leads to:\n\u03bci \u2192 0 and \u03c3i2 \u2192 0 for all i,(17)\nresulting in concentration collapse.\nPCA- forces the posterior distribution to converge towards N (0, 0), effectively collapsing the distribution into a point mass at z = 0, a Dirac delta function centered at the origin of the latent space. In this context, as the distribution collapses, all latent representations are concentrated at z = 0, leading to a severe loss of dimensionality in the latent space. The normal distribution N (0, 0) ceases to be a valid probability distribution for describing the data, instead becoming a degenerate case where the distribution has zero variance in all dimensions.\nWhen this collapsed distribution is passed through a LDM, it results in a complete breakdown of semantic information in the edited images. The LDM, expecting a meaningful distribution in the latent space, fails to generate coherent outputs from this degenerate input.\nThis collapse represents a total loss of information about the original data variability, matching the Objective 2, which makes it impossible for the LDM to recover meaningful features or structures."}, {"title": "Notations", "content": "Throughout this section", "nomenclature": "n\u2022 \u201cPCA-\u201d and \u201cours-\u201d: Refer to minimize the LKL (consistent with Table 1 in the main paper) with v = 1 \u00d7 10-8\n\u2022 \"PCA+\" and \"ours+\": Refer to maximize the LKL with v = 1\nIn the subsequent tables (Table 2 and Table 3)", "conventions": "nGray background : Indicates methods not compared with others\n\u2022 Bold text: Denotes the best results\n\u2022 Underlined text: Highlights the second-best results\nOur extensive experimentation is consistent with the hypothesis: PCA+ is good at Objective 1, while PCA- excels in addressing Objective 2. The following sections delve deeper into these findings.\nAnalysis of PCA+ Performance (Objective 1)\nTable 2 presents a comprehensive comparison of IQA(x, f(xadv)) across various methods, highlighting the efficacy of our PCA+ approach. The results demonstrate a consistent superior performance of our method across multiple Image Quality Assessment (IQA) metrics"}]}