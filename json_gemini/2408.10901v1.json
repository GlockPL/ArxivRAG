{"title": "A Grey-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse", "authors": ["Zhongliang Guo", "Lei Fang", "Jingyu Lin", "Yifei Qian", "Shuai Zhao", "Zeyu Wang", "Junhao Dong", "Cunjian Chen", "Ognjen Arandjelovi\u0107", "Chun Pong Lau"], "abstract": "Recent advancements in generative AI, particularly Latent Diffusion Models (LDMs), have revolutionized image synthesis and manipulation. However, these generative techniques raises concerns about data misappropriation and intellectual property infringement. Adversarial attacks on machine learning models have been extensively studied, and a well-established body of research has extended these techniques as a benign metric to prevent the underlying misuse of generative AI. Current approaches to safeguarding images from manipulation by LDMs are limited by their reliance on model-specific knowledge and their inability to significantly degrade semantic quality of generated images. In response to these shortcomings, we propose the Posterior Collapse Attack (PCA) based on the observation that VAEs suffer from posterior collapse during training. Our method minimizes dependence on the white-box information of target models to get rid of the implicit reliance on model-specific knowledge. By accessing merely a small amount of LDM parameters, in specific merely the VAE encoder of LDMs, our method causes a substantial semantic collapse in generation quality, particularly in perceptual consistency, and demonstrates strong transferability across various model architectures. Experimental results show that PCA achieves superior perturbation effects on image generation of LDMs with lower runtime and VRAM. Our method outperforms existing techniques, offering a more robust and generalizable solution that is helpful in alleviating the socio-technical challenges posed by the rapidly evolving landscape of generative AI.", "sections": [{"title": "Introduction", "content": "The field of generative artificial intelligence has witnessed unprecedented advancements, particularly in the domain of image synthesis and manipulation. State-of-the-art methodologies, exemplified by diffusion models such as Stable Diffusion (Rombach et al. 2022), have demonstrated remarkable capabilities in producing photorealistic imagery with exceptional fidelity and diversity. These technological breakthroughs have not only revolutionized creative industries but have also democratized access to sophisticated image editing tools, empowering users across various domains.\nHowever, the proliferation of such powerful technologies inevitably engenders a concomitant set of ethical quandaries and potential security vulnerabilities. Of particular concern is the prospect of data misappropriation and intellectual property infringement. The facility with which diffusion-based models can be employed to manipulate existing visual content presents a significant challenge to the integrity of digital assets. For instance, malicious actors could exploit Stable Diffusion to edit copyrighted images, effectively \"laundering\" them by removing or altering identifying features (Rombach et al. 2022). This process of \u201cde-copyrighting\" not only undermines the rights of content creators but also poses a threat to the economic ecosystems built around digital imagery, potentially destabilizing industries ranging from photography to digital art.\nIn the realm of machine learning security, adversarial attacks have emerged as a critical area of study (Lau et al. 2023a). These attacks aim to perturb the input of machine learning models in ways that are imperceptible to humans but can significantly disrupt the model's output. Interestingly, this concept of adversarial attacks provides a potential solution to the misuse of generative AI. By introducing carefully crafted, acceptable perturbations to data that could be misused, it may be possible to impede the ability of diffusion models to edit or manipulate such content effectively.\nCurrent protection techniques often rely heavily on extensive prior knowledge, specifically requiring full white-box access to the parameters of the target models (Liang et al. 2023; Salman et al. 2023; Liang and Wu 2023; Xue et al. 2024). This assumption is largely impractical in real-world scenarios, especially given the rapid pace of technological advancement in generative AI. The proliferation of model architectures and constant iterations of backbone networks create a dynamic landscape where protection methods quickly become obsolete. Developing a universal method that can effectively protect against the vast majority of generative models without detailed knowledge of their internal structures is exceedingly difficult. Furthermore, the diversity of model architectures means that a method designed for"}, {"title": "Related Work", "content": "Generation Models\nDiffusion Probabilistic Model (DPM) (Ho, Jain, and Abbeel 2020) has achieved state-of-the-art results in density estimation (Kingma et al. 2021) and sample quality (Dhariwal and Nichol 2021). These models are powerful due to"}, {"title": "Method", "content": "Adversarial attack aims to craft an imperceptible perturbation \u03b4, added on the clean image x as the adversarial sample xadv, resulting in the wrong or disruptive output of machine learning models. The key concept of the adversarial attack against LDM-based image editing can be summarized as two objectives:\nObjective 1: $ming D(f(x + \u03b4), x + w)$ s.t. $||\u03b4||p \u2264 \u20ac$,\nObjective 2: $maxs D(f(x + d), f(x))$ s.t. $||\u03b4||p \u2264 \u20ac$,\nwhere f(\u00b7) is a kind of LDM-based image editing method; w refers to a kind of watermark artifact; D(\u00b7) measures the perceptual distance between two inputs, indicating the visual consistency of two images in human visual perspective; $||\u00b7 ||p$ means applying a constraint on the vector, in most cases, this serves to maintain the visual integrity of the adversarial sample, following the l\u221e norm. Notably, Objective 1 will cause similar results to Objective 2, i.e., there will be some distance in D's space between f(x + d) and x.\nExisting methods in the literature typically address either Objective 1 or Objective 2. However, both approaches often require extensive white-box information about the target model, particularly access to the neural backbone U-Net of the LDM. This heavy reliance on model-specific details limits their transferability and applicability across different LDM architectures, requiring more computing resource.\nOur method focuses primarily on Objective 2, but takes a fundamentally different approach. Instead of relying on detailed knowledge of the entire LDM pipeline, we exploit the inherent characteristics of LDM-based editing by targeting the VAE component, which is common across various LDM architectures. This strategy allows us to achieve the goal of maximizing the disparity between f(xadv) and f(x) without requiring extensive access to model-specific information, particularly the compute-intensive and model-specific U-Net component. By concentrating on the VAE, our approach aligns more closely with real-world scenarios where full model access may not be available, providing an efficient solution to prevent infringers from exploiting LDM-based image editing outputs."}, {"title": "Posterior Collapse", "content": "Variational Autoencoder. A key observation driving our approach is the ubiquity of VAEs in the architecture of LDMs. VAEs serve as a foundational component across different LDM implementations, often with only minor variations between models. For instance, the VAE used in Stable Diffusion 2.0 is essentially a fine-tuned version of the one employed in Stable Diffusion 1.5. This commonality presents a strategic opportunity: by focusing on the VAE, we can potentially affect a wide range of LDMs without requiring detailed knowledge of their specific architectures.\nVAE can be considered as a probabilistic generative extension of the ordinary autoencoders. The encoder of a VAE aims at approximating the intractable posterior distribution of the latent variable z by a Gaussian distribution q(z|x) = N(\u03bc, diag(\u03c3\u00b2)), where diag denotes a diagonal"}, {"title": "Experiments Setup", "content": "Dataset. In our experiments, we utilized a 1000-image-subset of the ImageNet (Deng et al. 2009), as selected by Lin et al. (2020a). This choice aligns with established conventions in adversarial attack research. All images were resized to 512 \u00d7 512, ensuring consistency across evaluations.\nBaselines. We compared our approach against several state-of-the-art methods, AdvDM (Liang et al. 2023), PhotoGuard (PG) (Salman et al. 2023), MIST (Liang and Wu"}, {"title": "Image Editing Results with Varied Prompts", "content": "Our experimental results demonstrate the superior performance of our proposed PCA compared to existing methods. Figure 2 provides a visual comparison, while Table 1 presents quantitative results.\nAs observed from Figure 2, our method significantly outperforms existing approaches in terms of semantic disruption. Most other methods largely maintain an image structure similar to that of the clean image after editing. In contrast, our approach fundamentally alters the semantic information of the image, resulting in edited outputs that bear almost no resemblance to the infringer-desired images.\nEven when prompts containing image structure information are provided, our method continues to demonstrate superior performance. The generated images show significant discrepancies from the expected outcomes, effectively thwarting attempts at unauthorized manipulation. Moreover, our approach introduces quite noise patterns in the generated images, substantially degrading their quality. This degradation renders the manipulated images unsuitable for their intended purposes, whether it be unauthorized use, redistribution, or further editing. Our method not only disrupts the semantic content of the image but also compromises its overall visual quality. This comprehensive degradation ensures that even if an attacker manages to partially reconstruct some elements of the original image, the result remains unusable from both a semantic and aesthetic standpoint.\nThe quantitative results in Table 1 further corroborate the effectiveness of our method. Notably, our Posterior Collapse Attack achieves state-of-the-art results while requiring minimal knowledge of the opponent model, less computational time, and lower GPU memory usage compared to existing methods (as illustrated in Figure 1).\nThese results demonstrate that, our approach effectively disrupts image semantics while maintaining efficiency and generalizability. This strategy challenges generative models\u2019 ability to produce coherent edits, operating with minimal model-specific information in a near black-box scenario."}, {"title": "Transferability", "content": "As shown in Figure 4, our method demonstrates superior transferability compared to existing approaches when tested on both SD20 and SDXL models. Across all five IQA metrics, our approach consistently achieves the highest normalized scores of 1.0, indicating the best attack performance. In contrast, other methods show varying degrees of effectiveness, with PhotoGuard and MIST performing moderately well, while AdvDM and SDS exhibit limited transferability. These results validate our hypothesis regarding the limitations of current protection techniques that rely heavily on white-box access to target models. Our method, by minimizing dependence on model-specific knowledge, achieves better performance across diverse model architectures. This generalizability is crucial in addressing the challenges posed by rapidly evolving generative AI models, offering a more practical and enduring solution in this dynamic field."}, {"title": "Conclusion", "content": "In this paper, we proposed the Posterior Collapse Attack, a novel method for disrupting LDM-based image editing, requiring fewer parameters merely 3.39% of the whole model, Which makes our method operates faster and consumes less VRAM than existing approaches. We leverage a"}, {"title": "Appendix", "content": "For the Equation 6 to Equation 7 in the main paper, we provide a derivation to demonstrate how we obtained Equation 7 from Equation 6:\nGiven two multivariate Gaussian distribution N\u2081(\u03bc1, \u03a31) and N2(\u03bc2, \u03a32), where N\u2081 and N2 refer to the posterior dis-tribution q(z|x) and our target prior distribution p*(z). We set the target attack p*(z) to a zero mean Gaussian distribution p*(z) = N(0, vI). Since both p* (z) and q(z|x) are diagonal multivariate Gaussian distributions, equation can be derived as:\n$DKL(N1 || N2) = {1 \\over 2} [In{\\left| \\Sigma_2 \\right| \\over \\left| \\Sigma_1 \\right|} - d + tr{\\Sigma_2^{-1}\\Sigma_1} + (\u03bc_2 - \u03bc_1)^{T} \u03a32^{-1}(\u03bc_2 - \u03bc_1)]$\n$= {1 \\over 2} In{v^d \\over \\prod_{i=1}^d \u03c3_i^2} - d + \\sum_{i=1}^d  {\u03c3_i^2 \\over v} +  \\sum_{i=1}^d  {\u03bc_i^2 \\over v}$\n$= {1 \\over 2} [dln v - \\sum_{i=1}^d ln(\u03c3_i^2) - d + \\sum_{i=1}^d  {\u03c3_i^2 \\over v} +  \\sum_{i=1}^d  {\u03bc_i^2 \\over v}]$\n$= {1 \\over 2} \\sum_{i=1}^d [-ln\u03c3_i^2 +{\u03c3_i^2 \\over v} +{\u03bc_i^2 \\over v} -1 + ln v]$\n$= {d \\over 2} + \\sum_{i=1}^d {1 \\over 2} (ln {v \\over \u03c3_i^2}  )({\u03bc_i^2 + \u03c3_i^2 \\over v})$\nNotably, in our implementation, we ignore the In v term, because ln v is a constant, which will not influence the performance. Therefore, the final loss function in our implementation is:\n$DKL(N1 || N2) \\sim LKL(x) = \\sum_{i=1}^d {1 \\over 2} (lno_i - 1 + {\u03bc_i^2 + \u03c3_i^2 \\over \u03c5})$.\nGradient Direction\nAs briefly mentioned in the main paper, our investigation was motivated by the work of Xue et al. (2024), leading us to examine both the minimization (-) and maximization (+) of the loss function LKL.\nTo provide context for our analysis, we revisit the two primary objectives outlined in the main paper:\nObjective 1: $min_\u03b4 D(f(x + \u03b4), x + w)$ s.t. $||\u03b4||_p \u2264 \u20ac$,\nObjective 2: $max_\u03b4 D(f(x + d), f(x))$ s.t. $||\u03b4||_p \u2264 \u20ac$.\nIt is crucial to note that these objectives, while seemingly different, share a common goal: to prevent the infringer-desired output. This commonality allows us to consider Objective 1 as a variant of Objective 2.\nFor a comprehensive evaluation of our method's performance, we employ two distinct approaches:\n\u2022 Evaluation of Objective 1:\nWe compute IQAs between clean images and edited adversarial samples, represented as IQA(x, f(xadv)). This approach aligns with some baselines studies (Liang et al. 2023; Xue et al. 2024).\n\u2022 Evaluation of Objective 2:\nWe calculate IQAs between edited clean images and edited adversarial samples, denoted as IQA(f(x), f(xadv)). This metric is consistent with the results presented in the main paper."}, {"title": "Two Variants of Posterior Collapse", "content": "Posterior collapse in VAEs is a phenomenon that can manifest in two primary forms: diffusion collapse and concentration collapse. Let X be the input space and Z \u2282 Rd be the latent space of a VAE. The posterior distribution in a VAE is denoted by q(z | x), where x \u2208 X and z \u2208 Z.\nHere we give definitions of two kind of collapses and explain when the phenomena will occur:\n1. Diffusion Collapse:\nWe witness diffusion collapse when the posterior distribution becomes overly dispersed:\nq(z|x) \u2248 U(Z) as \u03c3\u00b2 \u2192 \u221e,\nwhere U(Z) denotes the uniform distribution over Z. In this case, the encoder produces highly uncertain latent representations for all inputs."}, {"title": "Notations", "content": "Throughout this section, we use the following nomenclature:\n\u2022\n\u2022 \u201cPCA-\u201d and \u201cours-\u201d: Refer to minimize the LKL (consistent with Table 1 in the main paper) with v = 1 \u00d7 10-8\n\u2022 \"PCA+\" and \"ours+\": Refer to maximize the LKL with v = 1\nIn the subsequent tables (Table 2 and Table 3), we employ the following conventions:\n\u2022 Gray background : Indicates methods not compared with others\n\u2022 Bold text: Denotes the best results\n\u2022 Underlined text: Highlights the second-best results\nOur extensive experimentation is consistent with the hypothesis: PCA+ is good at Objective 1, while PCA- excels in address-ing Objective 2. The following sections delve deeper into these findings.\nAnalysis of PCA+ Performance (Objective 1)\nTable 2 presents a comprehensive comparison of IQA(x, f(xadv)) across various methods, highlighting the efficacy of our PCA+ approach. The results demonstrate a consistent superior performance of our method across multiple Image Quality Assessment (IQA) metrics. Notably, PCA+ achieves the best scores in PSNR, FID, SSIM, and LPIPS across all evaluated scenarios, indicating a robust ability to maintain similarity between the edited adversarial image and the original image.\nWhile the PCA+ method does not lead in ACDM scores, it remains highly competitive, consistently ranking among the top-tier performers. This comprehensive performance across various metrics underscores the method's effectiveness in addressing Objective 1.\nVisual analysis, as evidenced in Figures 6 and Figure 7, reveals that PCA+ produces results visually comparable to estab-lished methods such as SDS and AdvDM. A characteristic feature observed in the edited images is the appearance of watermark artifacts, which is consistent across these methods. However, PCA+ distinguishes itself through enhanced resistance to unde-sired edits. This superiority is not only reflected in the quantitative metrics but also apparent in qualitative analysis. A striking example is observed in the P3 scenario, where PCA+ successfully prevents the appearance of snow-like artifacts in f(xadv), a feat not achieved by other methods."}, {"title": "Implementation Details", "content": "Our proposed method is implemented using PyTorch (Paszke et al. 2019) and the Diffusers library (von Platen et al. 2022). We maintain consistency across all experiments by fixing the random seed to 3407 for PyTorch's generator (Picard 2021), ensuring reproducibility in all stable diffusion image generations."}]}