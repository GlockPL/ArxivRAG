{"title": "Learning Visually Grounded Domain Ontologies via Embodied Conversation and Explanation", "authors": ["Jonghyuk Park", "Alex Lascarides", "Subramanian Ramamoorthy"], "abstract": "In this paper, we offer a learning framework in which the agent's knowledge gaps are overcome through corrective feedback from a teacher whenever the agent explains its (incorrect) predictions. We test it in a low-resource visual processing scenario, in which the agent must learn to recognize distinct types of toy truck. The agent starts the learning process with no ontology about what types of trucks exist nor which parts they have, and a deficient model for recognizing those parts from visual input. The teacher's feedback to the agent's explanations addresses its lack of relevant knowledge in the ontology via a generic rule (e.g., \"dump trucks have dumpers\"), whereas an inaccurate part recognition is corrected by a deictic statement (e.g., \"this is not a dumper\"). The learner utilizes this feedback not only to improve its estimate of the hypothesis space of possible domain ontologies and probability distributions over them, but also to use those estimates to update its visual interpretation of the scene. Our experiments demonstrate that teacher-learner pairs utilizing explanations and corrections are more data-efficient than those without such a faculty.", "sections": [{"title": "Introduction", "content": "The field of eXplainable Artificial Intelligence (XAI) aims to develop AI systems that can explain their decisions, such that any identified knowledge gaps can be closed to improve performance on various dimensions: e.g., task success rates, faster convergence, or closer alignment with human knowledge. However, many XAI works focus only on identifying knowledge gaps, not exploiting those gaps in a principled way as learning opportunities (Weber et al. 2023). Furthermore, the currently most popular ML models that require explanations are deep neural networks (DNNs), which generally need many iterations of parameter updates to incur significant behavior changes. Providing human feedback to every single model explanation would be infeasible for DNNs, so most existing work on XAI-based model improvements aims to involve as little human involvement as possible during training.\nIn the meantime, feedback by a human-in-the-loop in response to XAI explanations could prove much more valuable when the learner has to quickly adapt to an unfamiliar domain with a continuously changing hypothesis space. Consider, for example, a general-purpose robotic agent deployed in an assembly plant where task concepts may be frequently added or modified in unforeseen ways, and supervision by a human domain expert (perhaps without ML expertise) is available via natural dialogues. Concepts in the domain ontology may be niche and specific to this particular plant, making it difficult to address the learner's initial ignorance of the domain ontology with some existing pre-trained ML models. In such scenarios, it would be desirable for the expert's corrective feedback to the agent's explanations (as demonstrated in Fig. 1b) to cause immediate behaviour changes without interrupting its operation, so as not to exhaust or irritate the human teacher. This motivation is particularly pertinent when dealing with highly specialized, low-resource domains, where it is difficult to prepare automated feedback to agent explanations.\nIn this paper, we propose a framework for explanatory interactive learning (XIL; cf. Teso et al. 2023), suited to a family of knowledge-based neurosymbolic AI systems. The primary strength of our approach is that the learner can start"}, {"title": "Explanatory Interactive Learning", "content": "Testbed task: fine-grained visual classification\nThe main challenge of fine-grained visual classification (FGVC) is that instances of fine-grained subcategories exhibit small inter-class variance and large intra-class variance (Wei et al. 2021). Recent development of powerful vision foundation models has enabled strong FGVC performance with only visual features and label annotations, adding a classification head on top of the pre-trained models with weights frozen (Oquab et al. 2023), which we will employ as a 'vision-only' baseline in this study.\nFormally, each input to our FGVC task is a pair $x_i = (I_i, m_i)$ with an RGB image $I_i$ and a binary mask $m_i$. Thus, $x_i$ essentially refers to an object in an image. The anticipated task output $y_i \\in C_{fg\\_type}$ is the correct concept of the object instance $x_i$, where $C_{fg\\_type}$ is a set of fine-grained visual concepts. One important difference between our problem setting versus traditional FGVC is that for us $C_{fg\\_type}$ is not known to the learner at the start-it must be acquired through interaction with the teacher. The classification target concepts in our experiments are fine-grained types of toy truck as illustrated in Fig. 1a. Some features define the fine-grained types of truck and thus are crucial for their successful classification (types of load and cabin parts), while other irrelevant features may vary across the types and act as distracting signals (e.g., colours of different body parts, numbers and sizes of wheels). The learner starts out knowing that toy trucks have loads and cabins, albeit with imperfect recognition capability."}, {"title": "Dialogue Strategies", "content": "Fig. 2 depicts dialogue flows that we aim to cover with our proposed approach and baselines. In effect, each episode of interaction is a single round of FGVC inference and learning with a training example, unrolled into a sequence of NL utterances exchanged between the teacher and the learner. An interaction episode starts with the teacher presenting an instance $o$ of one of the target visual concepts in $C_{fg\\_type}$. The teacher asks a probing question \"What kind of truck is this?\" along with an accurate binary mask specifying the object demonstratively referenced by \"this\". The learner answers with its best estimate of the fine-grained type in $C_{fg\\_type}$; e.g., \"This is a dump truck\u201d. This answer may be right or wrong; if the object is indeed a dump truck, then the episode terminates, and the teacher and learner proceed to the next episode with a new example. If the answer is wrong, then the teacher corrects the learner's answer by uttering, for instance, \u201cThis is not a dump truck. This is a missile truck\u201d.\nAfter the teacher's correction, the dialogue branches according to whether the learner relies on direct neural prediction only or utilizes generic knowledge about distinctive part properties as well. The former case is a baseline, which we refer to as Vis-Only. No further interactions ensue, and the episode terminates. Otherwise, the teacher investigates the source of the error by asking \"Why did you think this$_{\\circ}$ is a dump truck?\u201d. This introduces another branching point"}, {"title": "Neurosymbolic Agent Architecture", "content": "This section outlines our neurosymbolic architecture, assuming it is deployed to learn and perform image analysis tasks. However, note that in principle our framework is designed for knowledge-intensive tasks with any input modality (image, video, language, audio, etc.) where dialogue participants can naturally refer to parts of inputs. Fig. 3 illustrates the system components and their coordination in inference mode. As depicted, the agent architecture has four components with distinct responsibilities: vision processing, language processing, long-term memory and symbolic reasoning. The architecture is highly modular and can employ any existing methods for each component as long as they satisfy the feature requirements described in the subsections below."}, {"title": "The vision processing module", "content": "The vision processing module is responsible for parsing raw pixel-level inputs perceived by the system's vision sensor into structured representations. We use an object-centric graph-like data structure for summarizing visual scenes, which we will refer to as scene graphs hereafter. Scene graphs encode a set of salient objects in a visual scene and their conceptual relations (e.g., $dumpTruck(o_i)$, $have(o_i, o_j)$) along with likelihood scores. Each object in a scene graph is also associated with a demonstrative reference to it by segmentation mask. A scene graph serves as an internal, abstract representation of an image that will be later processed by symbolic reasoning methods.\nThe functional requirement we impose on the vision processing module is twofold. First, given a raw RGB image and a set of n binary masks ${m_1,..., m_n}$ referring to scene objects, the vision module should be able to perform few-shot classification on visual concepts it's aware of, returning the estimated probabilities of concept membership as its output. Second, given a raw RGB image and a visual concept $\\gamma$, the vision module should be able to localize and segment instances of $\\gamma$ in the visual scene in few shot (i.e., $\\gamma$ is specified by a moderately sized set of positive exemplars $X^+$), producing a set of corresponding binary masks"}, {"title": "The language processing module", "content": "The language processing module handles NL interactions with human users. Its core responsibilities include:\n\u2022 Parsing free-form NL inputs from users into their semantic contents and pragmatic intents (natural language understanding; NLU).\n\u2022 Incrementally updating semantic representations of states of ongoing dialogues and deciding which speech acts to perform from current states (dialogue management; DM).\n\u2022 Generating NL utterances according to the DM's decisions about the agent's next speech act (natural language generation; NLG).\nWe draw on formal semantic representations for the NLU, DM and NLG components. In the scope of this work, each clause in the dialogues is either a proposition or a question. Suppose we have a standard first-order language L defined for describing scene graphs, including constants referring to objects and predicates referring to visual concepts. We will represent the semantic content of an indicative NL sentence with an antecedent-consequent pair of L-formulae; we will refer to such a unit as a PROP hereafter. If $\\psi$ is a PROP comprising an antecedent L-formula $Ante$ and a consequent L-formula $Cons$, then $\\psi$ has the form $Ante \\Rightarrow Cons$. We refer to $Ante$ and $Cons$ of $\\psi$ as $Ante(\\psi)$ and $Cons(\\psi)$ respectively. When a PROP $\\psi$ is a non-conditional, factual statement, $Ante(\\psi)$ is empty and we omit the $Ante \\Rightarrow$ part, leaving $Cons$. For example, the NL sentence \"o is a dump truck\" is represented as:\n$dumpTruck(o)$\nQuantified PROPS are marked with matching quantifiers. In this work, we are primarily concerned with generic statements and will let G denote the generic quantifier. Thus, for example, the NL sentence \"Dump trucks (generally) have dumpers\" can be represented as follows:\n$G_x.dumpTruck(x) \\Rightarrow (\\exists_y.have(x,y) \\wedge dumper(y))$\nHowever, we go further and use a skolem function f in L that maps an instance of dump truck to its dumper:\n$G_x.dumpTruck(x) \\Rightarrow have(x, f(x)) \\wedge dumper(f(x))$\n(3) is more compatible with the logic programming formalism that we adopt in the symbolic reasoning module.\nOur semantic representation of questions, QUES hereafter, resembles the notation from Groenendijk and Stokhof (1982). A wh-question is represented as $?x. \\psi(x)$ where $\\psi$ is a PROP in which the variable x is free. It's (partial) answers provide values a for x such that $[x/a] \\psi$ evaluates to true. For example, we represent the NL sentence \"What kind of truck is o?\" by the following QUES:\n$?\\lambda P. P(o) \\wedge  (P, truck)$\nby taking the liberty of interpreting the NL question as meaning \"Which concept P has o as an instance and entails that o is a truck?\". Here, $F_{KB}$ is a reserved predicate such that $F_{KB}^{type}(p_1, p_2)$ evaluates to true if and only if $p_1$ is a subtype of $p_2$ according to the domain ontology represented by a knowledge base KB.\nAnother important type of question in the interactions from Fig. 2 is why-questions. We simply introduce another symbol $?why_{DP}$ for representing why-questions that query the source of belief by some dialogue participant DP. For example, we represent the NL question \u201cWhy did you think o is a dump truck?\u201d addressed to the learner agent agt as:\n$?why_{agt}.dumpTruck(o)$"}, {"title": "The long-term memory module", "content": "The long-term memory module stores interpretable fragments of knowledge, which the learner has acquired from the teacher over the course of interaction. Three types of storage subcomponents are needed: the visual exemplar base (XB), the symbolic knowledge base (KB) and the lexicon.\nThe visual XB stores collections of positive and negative concept exemplars $X^+_{\\gamma}$ and $X^-_{\\gamma}$ for each visual concept $\\gamma$. $X^+_{\\gamma}$ and $X^-_{\\gamma}$ naturally induce a binary classifier for the concept $\\gamma$, which is used by $f_{clf}$ in the vision module. $X^+_{\\gamma}$ and $X^-_{\\gamma}$ are updated whenever the learner encounters an object and deems it worth remembering as a positive/negative sample of $\\gamma$. Whenever the learner encounters an unforeseen concept $\\gamma$ via the teacher's utterance that features a word that's outside the learner's vocabulary (i.e., a neologism), it is registered as a new concept, and new empty sets $X^+_{\\gamma}$ and $X^-_{\\gamma}$ are created in the XB.\nThe symbolic KB stores a collection of generic rules about relations between concepts, represented as G-quantified PROPS like (3). Generic rules are acquired via interactions with a teacher and stored in the KB. In our setting, the generic statements express 'have-a' relations between whole vs. part concepts.\nThe lexicon stores a mapping between visual concepts and NL content word strings. Neologisms referring to novel visual concepts are added to the lexicon when first mentioned by a teacher, accordingly updating the visual concept vocabulary and the XB."}, {"title": "Symbolic reasoning module", "content": "The symbolic reasoning module combines subsymbolic perceptual inputs and symbolic relational knowledge. The 'first impressions' formed by the vision module are adjusted by the generic rules in KB to yield final estimations of the current image's interpretation. To cope with estimation of quantitative uncertainties on logical grounds, we use probabilistic graphical models for symbolic reasoning.\nFig. 4 illustrates a small example of how we represent an instance of a visual classification problem as a factor graph model. Our factor graph representations feature two types of variable nodes and two types of factor nodes. Concept membership variables represent whether an object (or an ordered tuple of objects) is 'truly' an instance of a concept. Evidence variables represent evidence with quantitative uncertainty, obtained through a process external to the scope of the reasoning problem (virtual evidence; cf. Pearl 1988), namely visual perception and neural prediction here. Each evidence variable is uniquely defined for and causally linked to the corresponding concept membership variable. Evidence factors connect each matching pair of a concept membership variable and an evidence variable. The strengths of observed evidence, which are exactly the probability values returned by $f_{clf}$ functions, are entered as input to corresponding evidence factors as likelihood ratios: e.g., $Pr(ev\\_dumper(p) = T | dumper(p) = T) : Pr(ev\\_dumper(p) = T | dumper(p) = F)$. Rule factors connect concept membership variables and represent KB entries that encode relational knowledge like (3).\nWe construct a graphical model for inference by first translating the scene graph and the symbolic KB entries into a probability-weighted normal logic program. The primary reason we employ normal logic programs as an intermediate representation is that we would like the generic PROPS in the KB to receive minimal model interpretations. In other words, we want to infer a rule consequent $Cons$ only when some rule antecedent $Ante$ that entails $Cons$ is proven to hold, as opposed to material implication in propositional logic and Markov logic (Richardson and Domingos 2006). This enables inference in the abductive direction, e.g., raising the probability of an object being a dump truck after recognizing a dumper as its part due to the rule \"Dump trucks"}, {"title": "The agent's inference and learning procedures", "content": "We now describe how the architecture components coordinate in service to the four cognitive processes integral to our framework: inference, explanation, instance-level learning and rule-level learning.\nInference The agent performs FGVC inference upon parsing a probing QUES like (4) from the teacher. The vision module first prepares a scene graph that includes the classification target object o (referenced as \"this,\" in Fig. 2). If the symbolic KB is not empty, potential instances of relevant part subtypes as determined by the KB are searched by $f_{seg}$ and added to the scene graph as well. Then, each concept membership probability for the scene objects are estimated by $f_{clf}$, completing the scene graph SG. SG and the symbolic KB are each translated into probability-weighted normal logic programs $II_V$ and $II_K$ respectively. Note that $II_K = \\emptyset$ for the Vis-Only baseline. $II_V \\cup II_K$ is converted into a factor graph, and the symbolic reasoning module executes its inference algorithm on the graph. Marginal probabilities are queried afterwards, then the candidate concept with the highest probability is selected as answer.\nExplanation The explanation process is triggered when the agent parses a why-QUES like (5) from the teacher, upon which it invokes a dedicated procedure for reflecting upon its reasoning process to select a sufficient reason (Darwiche and Hirth 2020). For example, if the agent knows the generic rule that fire trucks have ladders, then identifying a ladder with high confidence would be a sufficient reason for recognizing the whole object as a fire truck in our domain. We implement a modified version of Koopman and Renooij (2021)'s algorithm for finding sufficient reasons from our factor graphs. We only consider the evidence variables in the factor graph (e.g., $ev\\_dumper(p)$), which are observable, as potential explanans candidates. If the algorithm returns \u2018direct perceptions\u2019 as (part of) sufficient reasons, e.g., selecting $ev\\_dumpTruck(o)$ as sufficient reason of the agent's answer $dumpTruck(o)$, they are deemed not informative and thus omitted from the explanation. The intuition is that explanations like 'I thought o is a dump truck because it looked like a dump truck' do not reveal a knowledge gap that the teacher can exploit as a learning opportunity.\nInstance-level learning Instance-level learning updates the sets $X^+_{\\gamma}$, $X^-_{\\gamma}$ stored in the XB and thus contributes to updating $f_{clf}$ and $f_{seg}$. As the sizes of $X^+_{\\gamma}$ get larger, the accuracy of these functions should increase.\n$X^+_{\\gamma}$ and $X^-_{\\gamma}$ are updated whenever the agent makes an incorrect prediction. Specifically, the teacher's correction \u201cThis is not a dumper truck. This is a missile truck\u201d will add the referenced instance to $X^-_{dumpTruck}$ and $X^+_{missileTruck}$.\nOn any occasion where the agent made a correct prediction that an instance is a dump truck, but was not highly confident in that prediction, the instance is added to $X^+_{dumpTruck}$. In this case, any recognized instances of relevant part concepts, as determined by the agent's current KB, are added to the XB as well; e.g., $X^+_{dumper}$ and $X^+_{quadCabin}$ would also be expanded in our example. Note that the estimated binary masks for the part instances may be faulty and thus allow introduction of bad exemplars in the XB. In fact, it is precisely these sources of error that we aim to mitigate via the teacher's corrective feedback in Vis+Genr+Expl dialogues. That is, the corrective feedback \u201cThis is not a quad cabin\u201d to the explanation \u201cBecause I thought this is a quad cabin\u201d will add the misclassified non-instance to $X^-_{quadCabin}$. This enables joint refinement of whole and part concepts.\nRule-level learning For rule-level learning, the symbolic KB is updated whenever the teacher utters a generic statement after the learner fails to provide some verbal explanation for its incorrect prediction. Each NL generic statement, like \"Dump trucks have dumpers\", is translated into a corresponding G-quantified PROP like (3) and added to the KB"}, {"title": "Experiments", "content": "Setup\nWe conduct a suite of experiments to assess the data-efficiency of the three different interaction strategies during FGVC training: Vis-Only, Vis+Genr and Vis+Genr+Expl. We only compare against ablative baselines because no other existing neurosymbolic approaches are designed to cope with incremental learning combined with a lack of symbolic domain ontology after deployment. We consider two FGVC domains with differing difficulties:\n\u2022 single_4way: Four types of trucks can be distinguished solely by their load types.\n\u2022 double_5way: Five types of trucks can be distinguished by two dimensions of part properties (load and cabin), where some parts may be shared between truck type pairs and thus can fail to serve as distinguishing.\nWe also vary the part-recognition performance of the vision model with which the learner starts each experiment, by controlling initial XB entries, in order to study how the initial recognition capability affects the learning process. We run experiments with three initial part recognition accuracies: LQ/MQ/HQ (low-/medium-/high-quality). Our evaluation metric is cumulative regret; i.e., the accumulated number of mistakes made across a series of 120 interaction episodes, where subtypes and visual features are randomly sampled for each training instance. We report cumulative regret curves averaged over 30 random seeds with 95% confidence intervals. Due to space limits, the Appendix offers more details on experiment setup and agent implementation.\nResults and discussion\nPlots in Fig. 5 show how teacher-learner pairs adopting the three strategies acquire the target concepts at different rates. Fig. 5a shows that the order for data efficiency of learning in single_4way setting (starting from LQ) is: Vis+Genr+Expl, Vis+Genr, Vis-Only, from the most to the least efficient. Two-sample t-tests on their final cumulative regret values show these differences are all statistically significant (p < 10-6), also attested by the wide separations between the 95% confidence intervals in Fig. 5a. Generic rules allow the learner to generalize better by attending to more important visual features (e.g., part types) while mitigating distractions by irrelevant features (e.g., colors). Teacher feedback to learner explanations provides even further synergies through improving part recognition models.\nThe results for the double_5way experiments are nuanced. In particular, notice in Fig. 5b how Vis+Genr performs worse than Vis-Only. This may be due to the fact that the cabin parts (quad vs. hemtt) are more difficult to tell apart: they look more similar to each other and have varying colors as distracting features. This suggests that depending on task difficulty, generic part-whole rules may actually cause an adverse effect on the learning progress if"}, {"title": "Related Work", "content": "A line of research investigates a framework often referred to as explanatory interactive learning (XIL), which focuses on the corrective role of explanations in teacher-learner interactions (Teso et al. 2023). CAIPI (Teso and Kersting 2019) is a model-agnostic method that converts local explanations to counterexamples, but it cannot analytically state how agent explanations should be corrected at the feature level. Schramowski et al. (2020) extend CAIPI by allowing users to interact with explanations given as input gradients at the expense of model-agnosticity. ProtoPDebug (Bontempelli et al. 2023) makes explicit references to visual parts for human-in-the-loop debugging of neural FGVC models, but not in the form of logical rules. NeSy XIL (Stammer, Schramowski, and Kersting 2021) bears a strong resemblance to our approach in that it implements a neurosymbolic architecture which admits rule-based correction from local explanations. However, NeSy XIL heavily relies on the traditional DNN training scheme and thus is more suitable for static task domains where human inspections are needed only occasionally. Michael (2019) and Tsamoura, Hospedales, and Michael (2021) are another relevant line of work with a heavy emphasis on neurosymbolic architectures and argumentation-based model coaching, but they are also chiefly concerned with guiding parameter updates of neural models in static domains. HELPER (Sarch et al. 2023) is another closely relevant approach to building customizable agents with memory-augmented large neural models, but it is designed to learn novel user-specific action routines, while our approach focuses on enabling a pretrained neural model to acquire concepts it was completely unaware of.\nInteractive task learning (ITL; Laird et al. 2017) is a ML paradigm motivated by scenarios where an AI system needs to cope with unforeseen changes after deployment. Learning in ITL takes place primarily through natural interactions with a human domain expert. Therefore, it is natural for ITL to utilize insights from linguistic phenomena, such as discourse coherence (Appelgren and Lascarides 2020) and complex referential expressions (Rubavicius and Lascarides 2022). We contribute to this body of work by extending the neurosymbolic ITL architecture proposed in Park, Lascarides, and Ramamoorthy (2023), which exploits the truth-conditional semantics of generic statements; our extension adds corrective feedback to the agent's explanations as further learning opportunities."}, {"title": "Conclusion and Future Directions", "content": "In this paper, we have proposed an explanatory interactive learning framework for neurosymbolic architectures designed to solve knowledge-intensive tasks. It uses natural language dialogues through which the teacher provides feedback to the learner's explanations of its (incorrect) predictions. This feedback provides piecemeal information of domain ontologies, enabling incremental refinement of imperfect visual concept boundaries as and when needed. Symbolic reasoning enhances generalizability by explicitly highlighting important feature concepts, while correction of inaccurate part-based explanations ensures the referenced visual evidence is of good quality. Our proof-of-concept experiments show that timely exploitation of explanations can significantly boost learning efficiency in tasks where knowledge of part-whole relations is integral, although such improvements may be conditional upon the quality of the part recognition model. Potential future research directions include: in-depth exploration of exception-admitting generics (Pelletier and Asher 1997), extension to continuous regression tasks (Letzgus et al. 2022) and long-horizon planning tasks (Fox, Long, and Magazzeni 2017)."}, {"title": "Technical Appendix", "content": "Vision processing module:\nDetailed formal specifications\nFormally, suppose the vision module is provided with a raw RGB image input with width W and height H as $I \\in [0, 1]^{3\\times H\\times W}$. The agent's current vocabulary of unary and binary visual concepts are represented with sets $C_u$ and $C_b$ respectively. We do not make any exclusivity assumption among concepts, so $C_u$ and $C_b$ can contain concepts that are supertypes or subtypes of others. We represent a scene graph containing N objects as a pair of a vertex set and an edge set SG = (V, E) such that:\n\u2022 V = {($c_i$,$m_i$) | i \u2208 [1..N]}, where $c_i \\in [0,1]^{|C_u|}$ represents the probabilistic beliefs of whether the object indexed by i classifies as an instance of unary concepts in $C_u$, and $m_i \\in {0,1}^{H\\times W}$ a binary segmentation mask referring to the object.\n\u2022 E = {$r_{i,j}$ | i,j \u2208 [1..N], i \u2260 j}, where $r_{i,j} \\in [0,1]^{|C_b|}$ represents the probabilistic beliefs of whether the ordered pair of objects indexed by (i, j) classifies as an instance of binary concepts in $C_b$.\nSG serves as an internal, abstract representation of I that will be later processed by symbolic reasoning methods.\nThe functional requirement we impose on the vision processing module is twofold. First, given a raw RGB image $I \\in [0, 1]^{3\\times H\\times W}$ and an ordered set of n binary masks $m_1,..., m_n \\in {0,1}^{H\\times W}$ referring to scene objects, the vision module should be able to perform few-shot binary classification on known visual concepts, returning the estimated probabilities of concept membership as its output. That is, if we denote sets of positive and negative examples of a visual concept $\\gamma$ as $X_{\\gamma}^+$ and $X_{\\gamma}^-$ respectively, the vision module should provide a function $f_{clf}$ for each concept $\\gamma$ such that $f_{clf}(I, (m_1,..., m_n), X_{\\gamma}^+, X_{\\gamma}^-)$ returns the estimated probability that the n-tuple of objects referenced by (m1,\u2026, mn) is an instance of $\\gamma$. We steer clear of the closed-world assumption by estimating a binary distribution for each individual concept instead of a multinomial distribution over a fixed set of concepts, accommodating incremental learning of an open vocabulary of concepts. In our setting, we need $f_{clf}$ defined for n = 1 (for $C_u$) and n = 2 (for $C_b$); the former estimates $c_i$ in V for each object i in the scene, the latter estimates $r_{i,j}$ in E for each object pair (i, j).\nSecond, given a raw RGB image $I \\in [0, 1]^{3\\times H\\times W}$ and a visual concept $\\gamma$ specified by a set of positive concept examples $X_{\\gamma}^+$, the vision module should be able to localize and segment instances of $\\gamma$ in the visual scene as binary masks $m_1,..., m_p \\in {0,1}^{H\\times W}$ for some p. This 'search' functionality is needed for recognizing object parts as evidence that would significantly affect later symbolic reasoning. As with the classification function $f_{clf}$, the segmentation function $f_{seg}$ is required to be few-shot as well. The set of p proposal masks returned by $f_{seg}(I, X_{\\gamma}^+)$ are fed into $f_{clf}$ for re-evaluation (with $X_{\\gamma}^-$ also provided as needed), from which the top-scoring proposal(s) can be selected as best match and added to the scene graph. In our setting, we are only interested in searching for instances of unary concepts in $C_u$, object part concepts in particular."}, {"title": "Further implementational details\non experiment setup", "content": "Evaluation scheme As mentioned in the main text, our suite of experiments is designed to assess the data-efficiency of the three different interaction strategies during FGVC training: Vis-Only, Vis+Genr and Vis+Genr+Expl. We consider two FGVC domains with differing difficulties:\n\u2022 single_4way: Four types of trucks can be distinguished solely by their load types.\n\u2022 double_5way: Five types of trucks can be distinguished by two dimensions of part properties (load and cabin), where some properties may be shared between truck type pairs and thus can fail to serve as distinguishing.\nEach strategy is evaluated by exposing the learner to se-quences of interaction episodes as delineated in Fig. 2 in the main text. We use 30 different sequences of interac-tion episodes per domain, where each sequence consists of 120 training examples sampled by randomizing both relevant and distracting features. For relevant features, we vary the subtypes of the truck load and cabin subparts. For dis-tracting features, we vary colors of parts, number and size of wheels, and position and orientation of trucks (see Fig. 1a in the main text). A fixed set of example sequences from 30 random seeds are shared across the strategies for controlled sampling.\nThe learner starts each episode sequence with zero knowl-edge of the domain ontology and a deficient model for recognition of parts. The quality of recognition model for each part concept $\\gamma$ can be controlled by the sizes of $X_{\\gamma}^+$ and $X_{\\gamma}^-$ in the visual XB. We indirectly control sizes of the part exemplar sets by running a prior training sequence in Vis-Only mode with varying numbers of episodes. The more part examples the agent has witnessed before the interac-tive FGVC task, the larger $X_{\\gamma}^+$ and $X_{\\gamma}^-$ it will have, and so the higher the quality of the part recognition model is. The prior part models we used in our experiments yielded aver-aged classification accuracy of 74.83%/88.86%/98.17% af-ter 20/100/200 part training episodes. We will refer to each of them as LQ/MQ/HQ (low-/medium-/high-quality) models.Our evaluation metric is cumulative regret; i.e., the accu-mulated number of mistakes made across each 120-episode"}, {"title": "", "content": "sequence. We want to minimize cumulative regret (at each step in the sequence) in order to claim better training data-efficiency. This choice of evaluation metric reflects the nature of our interactive learning framework, in which training and inference are closely intertwined, and each piece of user feedback should take effect right after it is provided. We report cumulative regret curves averaged over 30 seeds with 95% confidence intervals.\nImplementation We randomly generate visual scene images from a simulated environment implemented in Unity.\nIn the single_4way domain, $C_{fg\\_type} \\overset{\\text { def }}{=} C_{4way} \\overset{\\text { def }}{=}$ {baseTruck, dumpTruck, missileTruck, fireTruck}. For double_5way, $C_{5way} \\overset{\\text { def }}{=} C_{4way} \\cup$ {containerTruck}. See Fig. 1a in the main text for the distinguishing part properties of each truck type. The teacher only refers to generic rules about load types in single_4way, while rules about both load and cabin types are taught in double_5way. We implement simulated teachers in our experiments that can respond to the learner's NL utterances, just so that the dialogue flows described in Fig. 2 in the main text are correctly followed.\nWe implement our neurosymbolic architecture by employing existing technology stacks that are available off-the-shelf or readily extensible. For the vision module, we use DINOv2-Small (Oquab et al. 2023) for patch- and instance-level feature extraction, and SAM (Kirillov et al. 2023) for instance segmentation. $f_{clf}$ functions are implemented as a distance-weighted kNN classifier on [CLS]-pooled vector outputs from DINOv2, which are obtained by feeding visual-prompted image segments (Yang et al. 2024). $f_{seg}$ functions are implemented by coordinating DINOv2 and SAM as done in Matcher (Liu et al. 2024), a training-free framework for few-shot instance segmentation.The language processing module employs a large-coverage parser of the English Resource Grammar (Copestake and Flickinger 2000), whose outputs are heuristically translated into our PROP and QUES formalisms. For symbolic reasoning, we extend the loopy belief propagation algorithm (Murphy, Weiss, and Jordan 1999) for approximate inference, modified to comply with the semantics of logic programs."}]}