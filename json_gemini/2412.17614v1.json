{"title": "Emerging Security Challenges of Large Language Models", "authors": ["Herv\u00e9 Debar", "Sven Dietrich", "Pavel Laskov", "Emil C. Lupu", "Eirini Ntoutsi"], "abstract": "Large language models (LLMs) have achieved record adoption in a short period of time across many different sectors including high importance areas such as education [4] and health-care [23]. LLMs are open-ended models trained on diverse data without being tailored for specific downstream tasks, enabling broad applicability across various domains. They are commonly used for text generation, but also widely used to assist with code generation [3], and even analysis of security information, as Microsoft Security Copilot demonstrates [18]. Traditional Machine Learning (ML) models are vulnerable to adversarial attacks [9]. So the concerns on the potential security implications of such wide scale adoption of LLMs have led to the creation of this working group on the security of LLMs. During the Dagstuhl seminar on \"Network Attack Detection and Defense - AI-Powered Threats and Responses\", the working group discussions focused on the vulnerability of LLMs to adversarial attacks, rather than their potential use in generating malware or enabling cyberattacks. Although we note the potential threat represented by the latter, the role of the LLMs in such uses is mostly as an accelerator for development, similar to what it is in benign use. To make the analysis more specific, the working group employed ChatGPT as a concrete example of an LLM and addressed the following points, which also form the structure of this report:\n\n1.  How do LLMs differ in vulnerabilities from traditional ML models?\n2.  What are the attack objectives in LLMs?\n3.  How complex it is to assess the risks posed by the vulnerabilities of LLMs?\n4.  What is the supply chain in LLMs, how data flow in and out of systems and what are the security implications?\n\nWe conclude with an overview of open challenges and outlook.", "sections": [{"title": "What is specific to LLMs regarding adversarial vulnerabilities?", "content": "Adversarial Machine Learning, is an area of study concerned with the vulnerabilities and robustness\u00b9 of ML models to adversarial attacks. Although, the first vulnerabilities were identified a number of years ago, e.g., [9], the contributions to this area have increased exponentially in recent years and entire conferences such as IEEE SaTML are devoted to this topic as well as regular sessions and many papers in both security and ML conferences. Furthermore, ensuring the security of AI systems constitutes a key pillar of responsible AI, alongside principles such as fairness, interpretability, and transparency. In light of this, discussions have focused on the aspects in which LLMs differ from adversarial aspects in traditional ML. Traditional ML models are vulnerable to adversarial attacks, which can be categorized along several dimensions (see recent taxonomy [25]), including the stage of the learning process targeted (training vs. inference), the attacked capabilities (e.g., control over training data or models), attacker knowledge of the system (white-box, black-box, or gray-box attacks), and attacker goals (e.g., privacy compromise). While most conventional adversarial attacks focus on predictive models [6], aiming to manipulate the input and deceive the model into incorrect predictions, there is growing interest in adversarial attacks to generative models.\n\nLLMs are large-scale, statistical language models based on neural networks. Pre-trained lan-guage models are task-agnostic trained on Web-scale unlabeled text corpora for general tasks that learn to predict the most likely next word based on a given sequence of words. These models can be finetuned to specific tasks using small amounts of (labeled) task-specific data [19]. Due to their probabilistic nature, LLMs are prone to what is known as hallucinations [14,20], defined as \"the generation of content that is nonsensical or unfaithful to the provided source\" [15]. Hallucinations can be intrinsic, directly conflicting with the source material introducing factual inaccuracies or log-ical inconsistencies, or extrinsic, which, while not contradicting the source, are unverifiable against the source and include speculative or unconfirmable elements. While these hallucinations usually do not have malicious cause or intent but are due to the probabilistic nature of LLMs, they do raise concerns about the trustworthiness of LLMs, and the potential objectives an attacker might pursue to exploit these vulnerabilities and carry out malicious actions.\n\nLLMs are based on the transformer architecture [26]. While significant research has been con-ducted on the performance and applications of transformers, and some studies have investigated their security vulnerabilities [16], comprehensive analyses remain limited, highlighting a critical need for further research in this area. This challenge is partly due to the architectural complexity of transformers, with interdependent components like multi-head attention and feed-forward layers making vulnerability analysis particularly difficult. Additionally, training LLMs, such as Chat-GPT, is particularly expensive, both financially and in terms of the data required. As a result, base models are trained on extensive public datasets that can be easily poisoned, i.e., data intentionally chosen by an attacker can be included in the training set. However, given the vast amount of these datasets, the proportion of poisoned data is likely to remain relatively small. This makes it difficult for an adversary to universally damage the model, though targeted attacks focusing on specific con-texts remain feasible [27]. Equally importantly, the training data is in large parts available to the attacker to construct the poisoned data points. As a result, the attacker has the ability to exploit data sparseness and amplify features in the training set.\n\nA second consequence of the high cost of pre-training LLMs, is that this process is likely to be"}, {"title": "Attack objectives", "content": "Adversarial attacks are attacks against ML systems, that alter the input of a model in subtle ways, so that to a human it would trigger the same response, but mislead the ML model in providing a different response than expected [6]. A typical example comes from computer vision [7]: when a slightly modified image is presented to a human, the human does recognise the image that is presented (animal, person, road sign, ...), but the model missclassifies it. There is extensive research on adversarial attacks [25] focusing on understanding how they occur, detecting them, and developing strategies to defend against them.\n\nThe semantics of existing adversarial attacks need to be critically reassessed in the context of LLMs. Some of the existing attack objectives may not be feasible, while others appear plausible. In the following, we present exemplary considerations that have arised during our discussions in the workshop. Obviously, LLMs are implemented in software, and software has bugs. We consider that"}, {"title": "The complexity of security risk assessment in LLMs", "content": "Evaluating the security of LLMs presents a multifaceted challenge due to several factors:\n\nData quality and origin The training data consists of massive amounts of data, largely scrapped from the Web, including human-generated content, introducing various data quality challenges such as biases, outdated information, misinformation and errors. Most of this data is publicly accessible, lacks clear provenance, is known to attackers and may already contain several vulnerabilities. Inspecting or curating such data at scale is impractical."}, {"title": "The supply chain of LLMs", "content": "The role of data in AI systems is of paramount importance. In this section, we focus on under-standing how data flow in and out of an LLM system and the associated security implications."}, {"title": "LLM Supply Chain Components", "content": "Figure 1 provides a high-level perspective of the data supply in LLMs. While one could delve into various stages of this pipeline, such as data collection and pre-processing, we consider this granularity sufficient for the purpose of this study.\n\nThe supply chain consists of the following components:\n\nThe LLM model LLM models can be categorized into two types: i) pre-trained models like Chat-GPT, which can be used off-the-shelf, and ii) fine-tuned models which are pre-trained models further trained on task-specific or application-specific data, such as financial data.\n\nTraining data These are large, diverse datasets from various sources used to train pre-trained models."}, {"title": "LLM Supply Chain Vulnerabilities", "content": "Different components and their interfaces (see Figure 1) can serve as potential vulnerabilities for security threats. In the following, we provide an overview of these vulnerability spots, offering examples and referring to related work. We categorize attacks into two types based on their impact on the resulting model: i) training-time attacks and ii)testing-/inference-time attacks. Training-time attacks result in permanent model poisoning, while inference attacks affect the model output during a user session but do not alter the model itself, resulting in ephemeral effects."}, {"title": "Challenges and Outlook", "content": "The security of LLMs is a crucial topic. We outline in the following key open challenges, organized into three main categories: attacking LLMs, defending LLMs, and assessing attack impact."}, {"title": "Attacking LLMs", "content": "Here we discuss various ways LLMs could be attacked, broken down by a series of questions.\n\nHow to attack an LLM? What specific parts can be poisoned, instances, features, labels, or feedback? Looking at the diagram (c.f., Figure 1), it is a matter of choosing the proper location to insert the disruption. This begs the question of how those individual disruption points can be effectively identified and exploited?\n\nAre there better ways to attack transformer models? Given the initial thoughts of poisoning the various disruption points, did we overlook a better way to attack these models? Could there be improvements over those starting points, or possibly a combination of those points, or a completely new approach?\n\nIs it possible to systematically attack an LLM through methods such as self-learning and inducing a decline in quality over time? Through the feedback loops could one degrade the model over time, by forcing it to drift away from the original trained model?\n\nHow long does it take to attack a model? How much time or poisoned data is needed? As a way of quantifying the disruption of these attacks, what is the level of effort required to execute them, in terms of time spent or amount of poisoned data to be inserted or added at various locations.\n\nAutomated attacks at scale If we consider the extension of the conceptual attacks, can we proceed to automate them, i.e. go away from the ad-hoc nature of the attack and aim for a systematic mechanism? So i) Can we produce attacks at scale, and while one create attacks, do they actually scale to very large LLMs (e.g. ChatGPT), or are they limited to toy problems? And ii) Can we automate attacks, e.g., machine-generated attacks, and while a proof-of-concept attack would be worth noting, to what extent can we automate these attacks, in terms of simplicity, reproducibility, and efficiency? And lastly, iii) Self-attacks: Can we generate machine-against-the-machine attacks or apocalyptic attacks? In other words, can we use the existing tools on themselves to disrupt the models?"}, {"title": "Defending LLMs", "content": "Here we take the other side, considering the defensive stance for LLMs. Again we address this by asking a series of questions.\n\nCan backdoor attacks be detected? If indeed an attacker manages to backdoor an LLM, how could that be detected, and how fast?\n\nCan we respond to the attacks/repair the model? Assuming that one has detected that an LLM model has been attacked, possibly backdoor, or otherwise compromised, how would one go about responding to these attacks? Is a repair of the model possible, and how soon could it be remediated?"}, {"title": "Attack impact assessment", "content": "The challenge here is how to assess the damage that has occurred in the context of an attack. We try to list the pertaining questions.\n\nWho are the affected users? Which applications are targeted? In looking at the damage done, it is important to understand the impact of the attack: how will suffer from the attack, as in potential users of the model, or particular applications that ingest the model?\n\nCan we assess the extent of the damage? Is there a qualification or quantification of the damage done? What would the specific criteria be?\n\nTypes of harm/damage Here we consider different types of harm and damage, with a spin on bias and discrimination: i) Damage in a specific context: For example, targeted attacks to specific population (sub) groups that might lead to allocation or representational harm. ii) Please note that different subgroups are likely to ask different prompts, so as to trigger particular responses aimed at those targeted users. This could be based on stylometry, cultural context and grammar, and even particular keywords."}, {"title": "Conclusions", "content": "Salzer and Schroeder's principle of \"economy of mechanism\" [21] is well known to security re-searchers. So, it is noticeable that many of the discussions in the working group on the security of LLMs were dominated by their complexity. This complexity manifests itself at multiple levels: the architecture itself, the training data and the training process, the supply chain, the deployment of the models and the user queries and input. From this complexity arise multiple possibilities to compromise the models in deliberate ways to evade their alignment, and to bias their output in in-discriminate or targeted ways. Many potential vulnerabilities were discussed during the workshop. Some may be only hypothetical at this stage. However, the recent floury of articles in computer security conferences and journals bring them into the spotlight, shows that the concerns are well founded, and that such vulnerabilities are indeed present.\n\nSo far, the research literature and the community response seems to be focusing mostly on attacks and demonstrating, one by one, that the vulnerabilities of LLMs can be exploited concretely. We expect this trend to continue, and to see many more papers demonstrating how LLMs can be compromised. In contrast, work on mitigating vulnerabilities is scarce at present. Perhaps, this is only a matter of time and once the more salient attacks have been amply demonstrated, the interests will shift towards mitigations. Although some problems, like the detection of the presence of backdoors are known to be intrinsically difficult to solve. Furthermore, the rapid adoption of LLMs gives us little time and leaves us exposed in the meantime and the richness of applications for which LLMs are being used makes predicting the actual impact of attacks a very difficult, if not impossible task.\n\nBeyond specific vulnerabilities and attacks, a more in-depth analysis of the systemic vulner-abilities of LLMs is still needed and we would like to encourage the community to work in this direction. Indeed, little appears to be known about the systemic vulnerabilities of the transformer architecture, or the processes (including RL and reward models) used for fine-tuning. Moreover,"}]}