{"title": "Efficient Quantization-Aware Training on\nSegment Anything Model in Medical Images and\nIts Deployment", "authors": ["Haisheng Lu", "Yujie Fu", "Fan Zhang", "Le Zhang"], "abstract": "Medical image segmentation is a critical component of clin-\nical practice, and the state-of-the-art MedSAM model has significantly\nadvanced this field. Nevertheless, critiques highlight that MedSAM de-\nmands substantial computational resources during inference. To address\nthis issue, the CVPR 2024 MedSAM on Laptop Challenge was estab-\nlished to find an optimal balance between accuracy and processing speed.\nIn this paper, we introduce a quantization-aware training pipeline de-\nsigned to efficiently quantize the Segment Anything Model for medical\nimages and deploy it using the OpenVINO inference engine. This pipeline\noptimizes both training time and disk storage. Our experimental results\nconfirm that this approach considerably enhances processing speed over\nthe baseline, while still achieving an acceptable accuracy level.", "sections": [{"title": "Introduction", "content": "Drawing inspiration from the remarkable achievements of foundation models in\nnatural language processing, researchers at Meta FAIR introduced a versatile\nfoundation model for image segmentation, termed the Segment Anything Model\n(SAM). It is widely recognized that foundation models in any domain often\nconfront challenges stemming from limited data diversity. Despite the consider-\nable scale of the dataset utilized to train SAM (referred to as the SA-1B dataset),\ncomprising over one billion masks, the model's performance fell short in med-\nical image segmentation tasks. This shortfall can be attributed in part to\nthe composition of the SA-1B dataset, which primarily comprises photographs of\nnatural scenes captured by cameras, thus lacking the nuanced features character-\nistic of medical images. In response to this challenge, Ma et al. curated a diverse\nand extensive medical image segmentation dataset encompassing 15 modalities,\nupon which they fine-tuned SAM. Their refined model, dubbed MedSAM,\nrepresents a significant step forward in addressing this discrepancy. However,\ndespite its advancements, MedSAM still grapples with several unresolved chal-\nlenges. For instance, the training dataset suffers from extreme modality im-\nbalances, the model encounters difficulties in accurately segmenting vessel-like\nbranching structures, and the practicality of text prompts remains limited.\nThe focus of the CVPR 2024 MedSAM on Laptop Challenge is on enhancing\nthe inference speed of MedSAM. The Segment Anything Model comprises three\ncore components: an image encoder responsible for transforming input images\ninto image embeddings, a prompt encoder that converts prompts into prompt\nembeddings, and a mask decoder tasked with generating low-resolution masks\nfrom image embeddings and prompt embeddings. Notably, in the initial proto-\ntype of MedSAM, the image encoder is notably more resource-intensive than the\nother two components. Consequently, various alternative backbones have been\nproposed to replace the original image encoder, such as the ViT-Tiny archi-\ntecture adopted by MobileSAM and EfficientViT in Efficient ViT-SAM. \nThe challenge's baseline model (LiteMedSAM) incorporates a distilled ViT-Tiny\nimage encoder, albeit with slight adjustments compared to MobileSAM."}, {"title": "2 Method", "content": "We propose to quantize the baseline model LiteMedSAM using QAT. While neu-\nral networks consist of various components beyond just matrix multiplications,\nit's within these operations that the peak of computational complexity resides.\nTherefore, nearly every QAT method focuses on quantizing inputs and weights\nduring matrix multiplications, such as in linear layers, convolution layers, and\nattention blocks. In contrast, operations involving biases, activation layers, and\nnormalization layers are typically performed per element. While the quantiza-\ntion of these layers can be selective, in our proposed quantized model, we opt to\nretain all these layers as floating-point, with only matrix multiplications in the\nimage encoder and the mask decoder being quantized. The reason we choose not\nto quantize the prompt encoder lies in the fact that its parameter size is over\n1000 times smaller than the other two modules, as indicated in Table 1. Some\nof the most common quantized sub-structures are illustrated in Figure 1.\nSince quantization is non-differentiable, we employ the straight-through es-\ntimator (STE) methodology, as demonstrated in previous works . In STE,\nincoming gradients are directly passed through a threshold operation to become\noutgoing gradients. For each quantization node, we propose an 8-bit symmetric\nper-tensor signed integer activations quantizer with a learned floating-point scale\nfactor. This scale factor is initialized from runtime statistics.\nUpon completion of quantization-aware training, Brevitas provides exceptional\ntoolchains for exporting quantized models to diverse backends.\nWhile the standard QuantizeLinear-DeQuantizeLinear (QCQ) representation\nfor quantization in ONNX exists, Brevitas has extended this to QuantizeLinear-\nClip-DeQuantizeLinear (QCDQ). With this extension, researchers can confine\nthe range of quantized values. Therefore, we propose exporting the quantized\nLiteMedSAM to ONNX in the QCDQ representation.\nWhile numerous inference engines support the ONNX format, not all of them\nare compatible with QCDQ. Given that the challenge mandates CPU inference,\nwe narrow down the options to ONNX Runtime and OpenVINO. An experiment\non inference speed between these two inference engines is detailed in Section 4.1.\nBased on the results, we ultimately opt for OpenVINO. Model caching is also\nsupported by OpenVINO. This strategy can reduce the resulting delays at appli-\ncation startup, making it considerably suitable for accelerating in this challenge."}, {"title": "Preprocessing", "content": "The dataset comprises three types of medical images: grayscale images, RGB\nimages, and 3D images. 3D images are split into individual 2D clips along the\nz-axis, with each clip treated as a grayscale image. To standardize the grayscale\nformat with the RGB format, grayscale images are duplicated across the red,\ngreen, and blue channels. Subsequently, RGB images are resized, padded to\ndimensions of 256 \u00d7 256, and finally normalized. It's important to note that in\nthe baseline approach, RGB images undergo normalization before padding with\nzeros. In this case, the padded value is equivalent to the minimum value of the\nimage instead of zero.\nWe've implemented some optimizations in the dataloader to enhance effi-\nciency during both training and inference. For the training process, in the base-\nline approach, all compressed 3D npz files are decompressed along the z-axis,\nwhich demands approximately 10TB of disk storage. This overhead is signifi-\ncantly disproportionate to the size of the original dataset, which is only around\n160GB. To mitigate this inefficiency, we propose indexing each 3D clip along\nthe z-axis and employing a binary search algorithm to locate the target 2D clip\nwhen necessary. By adopting this strategy, we distribute the decompression time\nacross each batch of training data, resulting in substantial savings in disk stor-\nage. Additionally, considering that our machine typically processes one batch\nof data in approximately one second, the computational cost of decompression\nbecomes negligible.\nIn terms of inference, the baseline method iterates through each 3D prompt\nbox individually. However, when 3D boxes intersect along the z-axis, the base-\nline recalculates image features. Given that the image encoder constitutes the\nmost computationally intensive aspect of SAM, we propose to preprocess all the\n3D boxes into 2D boxes corresponding to 2D clips. This approach ensures that\nthe image embedding of each 2D clip is computed only once, optimizing com-\nputational resources. In addition, the challenge has an 8GB limit on the Docker"}, {"title": "Proposed method", "content": "We propose to quantize the baseline model LiteMedSAM using QAT. While neu-\nral networks consist of various components beyond just matrix multiplications,\nit's within these operations that the peak of computational complexity resides.\nTherefore, nearly every QAT method focuses on quantizing inputs and weights\nduring matrix multiplications, such as in linear layers, convolution layers, and\nattention blocks. In contrast, operations involving biases, activation layers, and\nnormalization layers are typically performed per element. While the quantiza-\ntion of these layers can be selective, in our proposed quantized model, we opt to\nretain all these layers as floating-point, with only matrix multiplications in the\nimage encoder and the mask decoder being quantized. The reason we choose not\nto quantize the prompt encoder lies in the fact that its parameter size is over\n1000 times smaller than the other two modules, as indicated in Table 1. Some\nof the most common quantized sub-structures are illustrated in Figure 1.\nSince quantization is non-differentiable, we employ the straight-through es-\ntimator (STE) methodology, as demonstrated in previous works . In STE,\nincoming gradients are directly passed through a threshold operation to become\noutgoing gradients. For each quantization node, we propose an 8-bit symmetric\nper-tensor signed integer activations quantizer with a learned floating-point scale\nfactor. This scale factor is initialized from runtime statistics."}, {"title": "Model Inference and Post-processing", "content": "Upon completion of quantization-aware training, Brevitas provides exceptional\ntoolchains for exporting quantized models to diverse backends.\nWhile the standard QuantizeLinear-DeQuantizeLinear (QCQ) representation\nfor quantization in ONNX exists, Brevitas has extended this to QuantizeLinear-\nClip-DeQuantizeLinear (QCDQ). With this extension, researchers can confine\nthe range of quantized values. Therefore, we propose exporting the quantized\nLiteMedSAM to ONNX in the QCDQ representation.\nWhile numerous inference engines support the ONNX format, not all of them\nare compatible with QCDQ. Given that the challenge mandates CPU inference,\nwe narrow down the options to ONNX Runtime and OpenVINO. An experiment\non inference speed between these two inference engines is detailed in Section 4.1.\nBased on the results, we ultimately opt for OpenVINO. Model caching is also\nsupported by OpenVINO. This strategy can reduce the resulting delays at appli-\ncation startup, making it considerably suitable for accelerating in this challenge"}, {"title": "3 Experiments", "content": "We employed the challenge dataset for training, while the evaluation dataset\nwas obtained by partitioning it at a ratio of one-tenth. The dataset comprises\n11 modalities, and their sizes (prior to partitioning into training and evaluation\ndatasets) are summarized in Table 2. An evident issue arises from the significant\nimbalance in sample numbers across modalities. To address this imbalance and\nprevent bias or overfitting in the quantized model, as well as to expedite training,\nwe propose randomly sampling N, 2D clips from each modality in each epoch.\nAdditionally, these samples undergo random horizontal and vertical flips for data\naugmentation."}, {"title": "Dataset and sampler", "content": "We employed the challenge dataset for training, while the evaluation dataset\nwas obtained by partitioning it at a ratio of one-tenth. The dataset comprises\n11 modalities, and their sizes (prior to partitioning into training and evaluation\ndatasets) are summarized in Table 2. An evident issue arises from the significant\nimbalance in sample numbers across modalities. To address this imbalance and\nprevent bias or overfitting in the quantized model, as well as to expedite training,\nwe propose randomly sampling N, 2D clips from each modality in each epoch.\nAdditionally, these samples undergo random horizontal and vertical flips for data\naugmentation."}, {"title": "Metrics and loss functions", "content": "The accuracy of the model is evaluated using the Dice Similarity Coefficient\n(DSC) and the Normalized Surface Distance (NSD), while efficiency is measured\nthrough running time analysis. These metrics are collectively utilized to compute\nthe ranking. In the training phase, we mainly employ a combination of the\nDice loss and focal loss. This decision is based on the robustness demonstrated\nby compound loss functions in various medical image segmentation tasks, as\nevidenced in prior research ."}, {"title": "Training protocols", "content": "The training procedure includes three stages."}, {"title": "Environment settings", "content": "The development environments and requirements are presented in Table 4."}, {"title": "4 Results and discussion", "content": "The challenge evaluates models on an Intel Xeon W-2133 CPU (6c12t@3.8GHz),\nwhile we use an Intel Core i7-8750H CPU (6c12t@4.1GHz) that offers comparable\nperformance because we do not have an identical environment. We test each\nvariant with a single image and a prompt box. The inference speeds of various\nmethods are detailed in Table 5."}, {"title": "Inference speeds of different engines", "content": "The challenge evaluates models on an Intel Xeon W-2133 CPU (6c12t@3.8GHz),\nwhile we use an Intel Core i7-8750H CPU (6c12t@4.1GHz) that offers comparable\nperformance because we do not have an identical environment. We test each\nvariant with a single image and a prompt box. The inference speeds of various\nmethods are detailed in Table 5."}, {"title": "Quantitative results on validation set", "content": "Table 7 presents the performance of the proposed three stages in comparison\nwith the baseline model on the public validation dataset."}, {"title": "Qualitative results on validation set", "content": "Two sets of successful segmentation results are depicted in Figure 2. It can be\nobserved that the proposed quantized model performs better in matching the\nROI than the floating-point counterpart. Figure 3 illustrates two sets of chal-\nlenging cases. In these cases, the segmentation results of the proposed quantized\nmodel align more closely with the ground truth ROI compared to the baseline.\nHowever, since the baseline prediction results were significantly distant from the\nground truth, the correction was unsuccessful."}, {"title": "Ablation Study", "content": "Training a Segment Anything Model from scratch requires a huge mass of\ndata. However, the proposed quantization-aware training procedure starts with\na pre-trained model. Reducing the number of samples $N_s$ from each modality,\nespecially from the larger modalities, certainly benefits in saving training time.\nHowever, it still raises questions about its influence on the precision of the quan-\ntized model. In this section we propose an ablation study to explore the balance\nbetween efficiency and accuracy.\nTo describe the variation of samples from different modalities clearly, we\nwill use $N(m)$ to represent the number of samples from modality m. The total\nsamples of modality m is denoted by $N_m(m)$, and the complete set of modalities\nis denoted by $M$. The strategy of the proposed method can be described as\n$N_s(m) = \\min_{i \\in M} N_m(i)$.\nThe ablation study introduces a strategy that enlarges $N_s(m)$ to one-tenth of\n$N_m(m)$, in particular,\n$N_s(m) = \\max {\\frac{N_m(m)}{10} , \\min_{i \\in M} N_m(i)}$.\nThe metrics of the three stages in the ablation study are summarized in\nTable 9. Compared with Table 7 (we provide the average metrics of the pro-\nposed method in the last row of Table 9), the results indicate that increasing $N_s$\ndoes not result in a significant improvement, underscoring the efficiency of the\nproposed QAT pipeline in terms of training time."}, {"title": "Results on final testing set", "content": "The testing results are summarized in Table 10. The proposed quantized model\nexhibits a marginal decrease but much more balance in the average accuracy.\nAdditionally, the inference efficiency has been significantly optimized under the\nsame backbone. Compared with Table 7, we can observe that the model's perfor-\nmance on different modalities varies between the validation set and the testing\nset. However, the trend of balance across modalities remains consistent."}, {"title": "Limitation and future work", "content": "Experimental results have shown a significant decrease in performance in certain\nmodalities with larger amounts of data, and the accuracy of the least accurate\nmodalities still lags far behind the average. Hence a more accurate and modality-\nbalanced quantization is expected. On the other hand, the floating-point model"}, {"title": "Conclusion", "content": "In this paper, we present an efficient pipeline for quantizing LiteMedSAM and\ndeploying it on the OpenVINO inference engine. Objective experiments have\nconclusively shown that our method significantly accelerates the baseline while\nmaintaining an acceptable level of accuracy. Future endeavors will focus on en-\nhancing the speed of the floating-point backbone, further alleviating the im-\nbalance across different modalities, and deploying the quantized model on cus-\ntomized hardware platforms."}]}