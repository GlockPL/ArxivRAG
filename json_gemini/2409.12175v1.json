{"title": "EXPANDING EXPRESSIVITY IN TRANSFORMER MODELS WITH M\u00d6BIUSATTENTION", "authors": ["Anna-Maria Halacheva", "Mojtaba Nayyeri", "Steffen Staab"], "abstract": "Attention mechanisms and Transformer architectures have revolutionized Natural Language Processing (NLP) by enabling exceptional modeling of long-range dependencies and capturing intricate linguistic patterns. However, their inherent reliance on linear operations in the form of matrix multiplications limits their ability to fully capture inter-token relationships on their own. We propose M\u00f6biusAttention, a novel approach that integrates M\u00f6bius transformations within the attention mechanism of Transformer-based models. M\u00f6bius transformations are non-linear operations in spaces over complex numbers with the ability to map between various geometries. By incorporating these properties, M\u00f6biusAttention empowers models to learn more intricate geometric relationships between tokens and capture a wider range of information through complex-valued weight vectors. We build and pre-train a BERT and a RoFormer version enhanced with M\u00f6biusAttention, which we then finetune on the GLUE benchmark. We evaluate empirically our approach against the baseline BERT and RoFormer models on a range of downstream tasks. Our approach compares favorably against the baseline models, even with smaller number of parameters suggesting the enhanced expressivity of M\u00f6biusAttention. This research paves the way for exploring the potential of M\u00f6bius transformations in the complex projective space to enhance the expressivity and performance of foundation models.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformers (Vaswani et al., 2017) have revolutionized various areas of machine learning, becoming the foundation for groundbreaking models in Natural Language Processing (NLP) like text generation (GPT3 (Brown et al., 2020), BERT (Devlin et al., 2019), Mistral 7B(Jiang et al., 2023)) and computer vision (ViT (Dosovitskiy et al., 2021) utilized in SAM (Kirillov et al., 2023), DINO (Caron et al., 2021) and the multi-modal CLIP (Radford et al., 2021)). At the heart of their success lies the attention mechanism (Vaswani et al., 2017), a powerful tool that enables them to identify relationships between different parts of the data, be it words in a sentence or image patches in a scene.\nDespite their remarkable impact, current transformers face limitations. A key constraint is the inherent linearity of the attention mechanism, which primarily relies on weights learned through linear transformations, matrix multiplications, and the softmax function. While softmax is a non-linear operation, it is only used to produce a probability distribution over the elements signaling their relative importance in comparison to the others, and not to introduce non-linear interdependencies. Predominantly linear operations restrict the ability of models to capture complex linguistic dependencies, leading to potential information loss within each attention layer as shown by recent research (Zhang, 2023). Simply increasing the depth of the architecture does not fully solve this issue as it has"}, {"title": "2 RELATED WORK", "content": "The landscape of attention mechanism research is rich and multifaceted, with various approaches aiming to improve different aspects.\nA significant portion of research focuses on enhancing the time and memory efficiency of attention, e.g., HyperAttention (Han et al., 2024), FlashAttention (Dao et al., 2022) and other notable works (Shen et al., 2021; Child et al., 2019). These works prioritize maintaining functional and mathematical equivalence to the standard attention mechanism.\nSeveral works explore incorporating non-linear functions within the attention algorithm. Linear Transformers (Katharopoulos et al., 2020), Skyformer (Chen et al., 2021), Performers(Choromanski et al., 2021), Cosformer (Qin et al., 2022) and Kerformer (Gan et al., 2023) propose attention mechanisms with reduced computational requirements and comparable or better performance. These approaches utilize non-linear kernels on the learned weight vectors, essentially replacing the softmax function within attention. In comparison, our approach targets the weight representation and learning process itself to enhance its information capture capabilities.\nSeveral approaches introduce non-linear kernels to replace the standard dot-product similarity operation on the Q and K vectors (e.g., Rymarczyk et al. (2021); Tsai et al. (2019); Kim et al. (2019)). In contrast, our approach focuses on modifying the weight representation during the learning process itself, intending to facilitate the acquisition of information-richer vectors.\nROPE (Su et al., 2024) and NeuralAttention (Zhang, 2023) exhibit the most similarity to our work. Both papers introduce non-linear transformations on the Q, K, V weights vectors through rotation or learning via a non-linear MLP activation. However, these methods are limited to mapping within a single geometric space, lacking the flexibility to handle diverse geometries like elliptic, circular, or loxodromic, crucial for capturing intricate inter-token relationships. Furthermore, while both methods operate in real space, our approach leverages the complex domain and operations naturally supported by complex numbers, facilitating the modeling of various phenomena, including cyclical patterns."}, {"title": "2.2 COMPLEX-VALUED TRANSFORMER", "content": "The exploration of complex-valued models has gained significant traction in recent years, with applications emerging across various domains (Vasudeva et al., 2022; Li et al., 2020; Barrachina et al., 2021; Trabelsi et al., 2018; Nayyeri et al., 2021; Azizi et al., 2022). While complex-valued Transformers have been proposed (Complex Transformer (Yang et al., 2020), Signal Transformer (Peng et al., 2024) and C-Transformer (Eilers & Jiang, 2023)), these works primarily focus on the signal processing field and aim for a complete adaptation of the Transformer architecture to the complex domain, without introducing any alterations to the attention mechanism that are not necessary for the transition from real to complex space. Our work delves deeper into the core component of Transformers, the attention mechanism, seeking novel enhancements.\nA complex-valued Transformer specifically for NLP is developed by Wang et al. (Wang et al., 2020) where they define word embeddings as continuous functions over the position of the words in the complex domain, allowing for word representations to change gradually as positions increase. While our work shares the goal of capturing ordered relationships (a facet of geometric properties), we employ a distinct strategy by leveraging transformations which can represent a very broad variation of behavior based on only few learnable parameters. Additionally, our work adopts a new approach to position embeddings, as we use token embeddings as the real part of the input into the model, and the corresponding position embeddings - as the imaginary one. This targeted focus on position embeddings differentiates our approach from existing works."}, {"title": "3 BACKGROUND", "content": "This section presents all the necessary mathematical background essential for introducing our model."}, {"title": "Coordinate in Projective Geometry", "content": "Projective geometry employs homogeneous coordinates, representing N-dimensional coordinates with N + 1 parameters. For instance, a point in 2D Cartesian coordinates, [X, Y], transforms into [x, y, k] in homogeneous coordinates, where X = x/k and Y = y/k."}, {"title": "Projective Line", "content": "A Projective Line serves as the foundational space for projective geometry. To hold the axiom that \"two parallel lines intersect at infinity\", projective geometry necessitates the inclusion of a point at infinity. Consequently, an extended line P1(K) (with K representing the real line) is constructed, incorporating both K and a point at infinity, topologically resembling a circle. Formally, the projective line is expressed as the set { [x,1] \u2208 P\u00b9(K)|x \u2208 K }, augmented by an additional element [1: 0] representing the point at infinity. In this paper, we are interested in the Complex projective line denoted by CP\u00b9 (K = C) due to its favorable geometric properties introduced in the subsequent sections."}, {"title": "Riemann Sphere", "content": "The Riemann Sphere, depicted in Figure 2b, extends the concept of the complex plane (Figure 2a) by including a point at infinity. It is constructed by mapping the points on the complex plane onto a sphere by using the stereographic projection, where poles represent 0 and \u221e. In projective geometry, the Riemann Sphere serves as a complex projective line, offering valuable insights for projective transformations."}, {"title": "Projective and M\u00f6bius Transformations", "content": "A Projective Transformation involves mapping the Riemann Sphere onto itself. Suppose [x : y] \u2208 CP\u00b9 be a point in the Complex projective line, represented in the homogeneous coordinates. A projective transformation in CP\u00b9 can be denoted by a mapping T : CP\u00b9 \u2192 CP\u00b9 which is a matrix-vector multiplication:\nT([x, y]) = M\n, M = \n, (1)\nwhere the matrix M must be invertible (det(M) \u2260 0). Identifying CP\u00b9 with \u0108 = CU\u221e, a projective transformation is represented by a fractional expression through a sequence of bringing a point in the complex plane to homogeneous coordinate, applying a transformation, and bringing back from"}, {"title": "homogeneous coordinate to the Complex space as:", "content": "\nx\n[x1] \u2192 \n\nX\n\u2192 \n\n(2)\nwhere the mapping M : \u0108 \u2192 \u0108 is the M\u00f6bius transformation defined as:\nM(x) = , ad - bc \u2260 0, a, b, c, d, x \u2208 C. (3)"}, {"title": "M\u00f6bius Group", "content": "The M\u00f6bius Group comprises all M\u00f6bius transformations, forming the projective linear group PGL(2, C). It consists of all invertible 2 \u00d7 2 matrices with matrix multiplication as the operation in a projective space. Denoted by Aut(\u0108), it serves as the automorphism group of the Riemann sphere \u0108, or equivalently, CP\u00b9. Due to the isomorphism between the projective linear group PGL(2, C) and the M\u00f6bius group, denoted as PGL(2, C) = Aut(\u0108) (Kisil, 2012), the characteristics specified for Equation 3 also hold true for Equation 1."}, {"title": "Variants Of M\u00f6bius Transformation", "content": "Each M\u00f6bius transformation yields a maximum of two fixed points, 1 and 2, on the Riemann sphere, determined by solving M(\u03b3) = \u03b3 (Richter-Gebert, 2011)\n\u00a51,2 = . (4)\nwhere \u2206 = (trM)2 \u2013 4 det M. Based on the number of fixed points, M\u00f6bius transformations are categorized into Parabolic or Circular (one fixed point), Elliptic, Hyperbolic, and Loxodromic (two fixed points) transformation functions. Refer to Figure 1 and Table 1 for detailed conditions. Each group of transformations forms a subgroup that is isomorphic to the group of matrices listed under the Isomorphic row in Table 1.\nEach transformation possesses a characteristic constant k = ea+i\u00df, signifying the sparsity/density of the transformation. The parameter \u1e9e represents the expansion factor, delineating the repulsive nature of the fixed point 1 and the attractive quality of the second fixed point 2. Meanwhile, a serves as the rotation factor, dictating the extent to which a transformation rotates the plane counterclockwise around 1 and clockwise around 2."}, {"title": "4 M\u00d6BIUS ATTENTION", "content": "In this section, we introduce our novel attention mechanism centered around the M\u00f6bius transformation. We present our approach through the following components: a) token and position representation, b) query, key, and value computation, and c) attention calculation."}, {"title": "Token and Position Representation", "content": "Let T = {wi}i=1N be a set of N input tokens. Each token wi has a position in a text, denoted by pw. Each token wi and its position pw\u2081 are embedded as a d dimensional real vector, denoted by wi, pw; \u2208 Rd. A pair token-position pi = (Wi, Pw\u2081) is represented as a d dimensional complex number, i.e., pi = wi + ipw; \u2208 Cd. Thus, each element of Pij, j = 1, ..., d is a point in the Complex plane (Figure 2a), i.e., Pij \u2208 C."}, {"title": "Query Representation", "content": "Each element of the attention matrix, e.g., auv, determines the similarity between the source token wu and the target token ww. The vanilla transformer defines the query"}, {"title": "q(x) and the key functions k(x) as a linear function.", "content": "In contrast, we propose to define the query function as an element-wise M\u00f6bius transformation. We present the query function in two equivalent representations:\nM\u00f6bius Query Representation We define the M\u00f6bius query function Mq(x) = [Mq1, ..., Mqa] \u2208 Cd as follows\nMqj (Pij) = , di, j = 1,..., d, (5)\nwhere aq;, bqj, qi, dq; \u2208 C. Because PGL(2, C) \u2243 Aut(C), we can present the projective representation of the query function as follows.\nProjective Query Representation To gain better insight and model interpretation, we introduce the projective representation of the query function, denoted as Tq(x) = [Tq1(x),...,Tqa(x)]. \u03a4\u03bf apply the projective transformation, we first bring the pair token-position representation p\u2081 into a homogeneous coordinate, represented by ph\u2208 CPd. With this, the projective query function is defined as follows:\nTaj (p) = Maj j = 1, ..., d, (6)\nwhere Mq3 = . The Equation 6 shows that the query calculation can be done in matrix- vector products in the projective space, enabling efficient implementation through tensor products."}, {"title": "Key and Value Representation", "content": "A complex linear transformation is used for key K(.) and value V(.) functions as follows\nKj(pij) = WkjPij, j = 1, ..., d, (7)\nVj (pij) = WvjPij, j = 1, ..., d. (8)"}, {"title": "M\u00f6bius Attention", "content": "Similar to Vaswani et al. (2017), we compute the M\u00f6bius attention as follows\nAtt(Tq, K, V) = softmax(). (9\nwhere O is the attention matrix. Defining the Complex query matrix Q and the key matrix K with elements Qij = Tq; (pij), Kij = Kj(pij), i = 1, . . ., N, j = 1, . . ., d, we compute the matrix O by QKT. The rest of the architecture is similar to the one introduced in Vaswani et al. (2017). In this paper, we integrate the M\u00f6bius attention in the BERT model (Devlin et al., 2019), and into RoFormer (Su et al., 2024), essentially a BERT model with rotary positional embeddings (RoPe). The detailed architecture is presented in the experiments section."}, {"title": "Geometric Interpretation", "content": "In this section, we provide a geometric interpretation of our attention model and highlight its advantages compared to existing models."}, {"title": "Capturing Local Information", "content": "The set of all query matrices Mq; in Equation 6 constitutes the generalized linear group GL(2, C). If we impose the condition det Mq\u2081 = 1, we obtain the special linear group SL(2, C), which preserves both volume and orientation. Consequently, the set of source token-position pairs in a sequence can be mapped to the set of target token-position pairs, capturing local dependencies between tokens within the attention matrix."}, {"title": "Capturing Global Information", "content": "When det Mq; \u2260 1, the transformation alters both volume and orientation. This property, combined with the M\u00f6bius transformation's capability to map lines to circles and vice versa, results in a more expressive attention matrix. This enhanced expressiveness captures more intricate relationships between tokens and understands complex linguistic patterns."}, {"title": "Time and Space Complexities", "content": "Despite the favorable characteristics of our model, it is efficient in terms of time and space complexities. The time complexity of M\u00f6bius attention is O(n\u00b2d + nd\u00b2) where d is the token vector size and n is the number of tokens in the sequence in the case of self- attention. The space complexity of M\u00f6biusAttention is similar to the vanilla attention O(n\u00b2). We will later show in our experiment that our approach requires fewer layers than the vanilla model and is more efficient in memory and time."}, {"title": "5 EXPERIMENTS", "content": "We integrate M\u00f6biusAttention into the BERT and RoFormer architectures (Devlin et al., 2019; Su et al., 2024) using the MosaicBERT framework (Portes et al., 2023), licensed under the Apache 2.0 License, instead of the original BERT framework, which was also used for RoFormer. This choice is motivated by several factors, including its ease of adaptation, extensibility to additional models, and suitability for training on the C4 dataset. See Appendix A.1 for further details on our motivation.\nFor training, we employ a cluster with four A100-40GB GPUs. The software environment consists of PyTorch version 1.13.1, CUDA version 11.7, Python version 3.10, and Ubuntu version 20.04.\nGiven that the results reported in Portes et al. (2023) were obtained using a setup of 8 A100-80GB GPUs, while our setup consists of 4 A100-40GB GPUs, we opted to train the baseline ourselves rather than directly adopting their results. Following the specifications of the framework used, we pretrain all models for 70,000 steps with a batch size of 4096."}, {"title": "Datasets", "content": "In contrast to the BookCorpus (Zhu et al., 2015) and English Wikipedia combination used for pre-training BERT and RoFormer (Devlin et al., 2019; Su et al., 2024), we leverage the more recent and larger Colossal Clean Crawled Corpus (C4) dataset (Raffel et al., 2020), licensed ODC-By, for our pre-training stage. This aligns with the recent trend of training NLP models on increasingly vast datasets, a strategy demonstrably leading to performance improvements witnessed in models succeeding BERT (e.g., Liu et al. (2019); Raffel et al. (2020); Liu et al. (2021); Lee-Thorp et al. (2022)). By adopting the C4 dataset, we not only benefit from this advancement but also ensure consistency with the MosaicBERT framework, which is specifically optimized for this data source."}, {"title": "Models", "content": "Our study employs two pre-trained transformer models as baselines: BERT (Devlin et al., 2019) and RoFormer (Su et al., 2024). BERT was selected due to its popuarity and frequent adoption as the foundation for numerous high-performing models (e.g., Liu et al. (2019); He et al. (2021); Lan et al. (2019)). RoFormer serves as our second baseline, chosen for its derivation from BERT and its integration of rotary positional embeddings (RoPe). RoPe introduces a geometric dimension to the model by rotating the query and key vectors according to token positions, employing circular geometry. Additionally, RoPe has been integrated in a multitude of novel LLMs such as LLAMA 1, 2 and 3 (Touvron et al., 2023a;b; Dubey et al., 2024), the Falcon series (Almazrouei et al., 2023), PaLM (Chowdhery et al., 2023), GPT-NeoX (Black et al., 2022), etc. Additional details on our motivation for these selections can be found in Appendix A.1.\nWe use the implementation from the Hugging Face Transformers library for PyTorch\u00b3. We use the base uncased version without any modifications to the architectures.\nWe also created our BERT and RoFormer versions enhanced with M\u00f6biusAttention - M\u00f6biusBERT and MobRoFormer. The M\u00f6bius transformation boasts high expressivity, but a Transformer solely comprised of M\u00f6biusAttention blocks would likely suffer from overfitting, as we show in our ablation study in Section 5. To address these limitations, we strategically integrate M\u00f6biusAttention - M\u00f6biusBERT and MobRoFormer utilize M\u00f6biusAttention only in the first and the last layer while"}, {"title": "relying on standard Transformer blocks for the remaining layers.", "content": "Additionally, we allow for adjustment of the percentage of M\u00f6biusAttention not only on layer-level but also on head-level, so it can range from zero to full utilization. We propose combining M\u00f6biusAttention with vanilla attention within the same layer by introducing an architecture that allows us to set the percentage of heads using M\u00f6biusAttention. We used an equal split of 50% vanilla attention heads (6 heads) and 50% M\u00f6bius Attention heads. Other variants with different placements of M\u00f6biusAttention are offered for M\u00f6biusBERT in our ablation study.\nEach block utilizing M\u00f6bius Attention operates in the Complex space, necessitating several architectural adjustments. Specifically, (a) the linear layers within the block are doubled, with separate layers for the imaginary and real channels, and (b) residual connections are introduced for each channel individually. For additional details on the architecture and design, please refer to Appendix A.2.\nTo construct the real and imaginary input channels for the first layer, we separate the word and positional encodings of the token-position pairs to represent the real and imaginary components, i.e., pi = (Wi, Pw\u2081) gives us the input to the model I = {pi}i=1N with pi = wi + ipw; \u2208 \u0108d. This strategy is not applicable for the last block which again uses M\u00f6biusAttention, since it is preceded by vanilla Transformer layers operating in the real space. For this last layer, we build the complex input by taking the real-valued output of the preceding layer as input to the real channel and the token embeddings from the real channel of the first block, i.e., the other M\u00f6biusAttention block, as input to the imaginary channel. A visualization of the input construction is provided in Fig. 4b in Appendix A.2.\nThe output of the complex M\u00f6biusAttention layers is converted back to real space by adding the real and imaginary outputs, i.e., for first and last layers l \u2208 {1, L} the output of the specific layer is out\u2081 = (Or,1 + Oi,l).\nThe chosen approach grants a stronger emphasis on positional encodings by separating them into a distinct channel. The key advantage of framing vanilla attention with M\u00f6biusAttention lies in its ability to leverage both the two input channels and the expressive power of M\u00f6bius transformations. Furthermore, the presence of two input channels before the final block allows for a residual connection to earlier layers using the imaginary channel without affecting the significance of the previous block's output, which remains in the real channel. Figure 4b illustrates the architecture of M\u00f6biusBERT."}, {"title": "Low-Dimensional M\u00f6bius Models", "content": "The usage of complex-valued parameters and M\u00f6bius transformation introduces additional parameters. Accordingly, we reduce the depth of the M\u00f6bius models in order to ensure a fair comparison to match the parameter count of the BERT baseline. As an alternative, we also create a low-dimensional M\u00f6biusAttention version which uses less parameters than the original one. Here the M\u00f6bius transformation is applied after the linear query transformation, thus lowering the number of parameters."}, {"title": "Tasks", "content": "To ensure maximal comparability between the models, we adhere to the setup for the pre-training and finetuning of BERT as specified in Devlin et al. (2019). The only deviation on our end is choosing Masked Language Modeling (MLM) as the only pre-training objective, leaving out Next Sentence Prediction (NSP) objective. We pre-train all models with this setup (BERT, RoFormer and the M\u00f6bius models). This choice of ours is in correspondence to newer research works showing that the NSP task is obsolete given MLM (Conneau & Lample, 2019; Liu et al., 2019; Joshi et al., 2020; Raffel et al., 2020; Yang et al., 2019; Su et al., 2024). With this exception, the remainder of decisions regarding pre-training are adopted from BERT, and, correspondingly, the setup for BERT-Base in the framework of our choice: masking ratio 15%, and dropout 0.1.\nTo assess the performance of the pre-trained models for different NLP tasks, we fine-tune and evaluate them on the GLUE benchmark (Wang et al., 2018). Details on individual tasks are in Appendix A.4."}, {"title": "Results and Analysis", "content": "Our models achieve superior performance to the baselines across several GLUE tasks, namely MNLI, QQP, QNLI, SST-2 and RTE as shown in Table 2. Both M\u00f6biusBERT models also outperform their host model, BERT, in the MRPC task. However, none of the reviewed models, including RoFormer, outperform BERT for the STS-B task. We also note that only one variant of our models outperforms BERT on SST-2, a sentiment classification task for movie reviews. Upon examining the two datasets, STS-B and SST-2, we found inconsistencies which we report in Appendix A.5. Due to the detected issues, we consider both training datasets to be of insufficient"}, {"title": "Analysis of M\u00f6biusAttention", "content": "To analyze the M\u00f6biusAttention mechanism, we closely examine the learned M\u00f6bius weights and the resulting attention values. Our observations are as follows:\na) Learned Geometries: Our analysis of the learned M\u00f6bius weights reveals that the model captures a diverse range of complex geometries, as illustrated in Fig. 3. The heatmap displays the distribution of these geometries across different attention heads in the first and last layers of the model. Key observations include:\nLayer-level Specialization: The model decides for a layer-level geometry specialization by clearly favouring the circular and elliptic geometries in the last layer, but neglecting the circular one in the first layer.\nHead-level Specialization: The distribution varies on a head-level too, a sign for specialization of the heads. This is also evident in Fig. 6 and 7, examples of how geometries might change after finetuning on different tasks.\nBeyond Circular Geometry: Consistent with RoFormer's RoPe, the model emphasizes circular geometry, particularly in the last layer. However, it extends beyond the circular geometry supported by RoFormer, especially in the first layer, facilitating more complex reasoning.\nb) Learning to \"Forget\": The examination of the attention values obtained via M\u00f6biusAttention show that vanilla attention and M\u00f6biusAttention adopt different approaches to detecting important information. As seen in the attention heatmaps in Fig. 8 in the Appendix, vanilla attention almost never assigns zero attention score to a token pair. In contrast, M\u00f6biusAttention gives most of the pairs zero score and only a few a non-zero one. Accordingly, instead of learning on what to \"focus\", M\u00f6biusAttention learns what to \"forget\". The vanilla model has difficulty to give zero value to entries of the attention matrix due to using linear transformation on a group of tokens. Accordingly, it is hard for the model to forget elements, but M\u00f6bius can give a zero value to elements as the transformation can deform the distribution. As a result, the M\u00f6bius transformation is not limited to keep some irrelevant entries non-zero to keep the group similarities. Therefore, the mixture of M\u00f6bius and vanilla attention shows very promising results.\nMemory and Time Complexity M\u00f6biusBERT demonstrates comparable pre-training efficiency to our BERT baseline, requiring the same pre-training duration of 26 hours. However, M\u00f6biusBERT achieved this performance with a reduced memory footprint. Specifically, M\u00f6biusBERT utilized 104 million parameters, whereas BERT required 110 million parameters. It is important to note that for"}, {"title": "M\u00f6biusAttention, we counted the real and imaginary components of the complex-valued parameters separately, rather than combining them into a single parameter.", "content": "To ensure maximal comparability with the BERT baseline, we adopted all hyperpa- rameters used in the original BERT model without any further optimization or tuning (see Appendix A.3 and A.6 for details). Consequently, our ablation study focuses solely on variations within the M\u00f6biusBERT architecture. Additionally, we maintain the same number of parameters by adjusting the number of layers.\nWe investigated the impact of M\u00f6bius attention placement within the transformer architecture. We experimented with four configurations: a) Top Layer (10 Layers): A single M\u00f6bius attention layer positioned at the very beginning of the transformer stack, b) Stacked Layers (9 Layers): Two consecutive M\u00f6bius attention layers at the beginning of the stack, c) Framed Architecture (9 Layers): M\u00f6bius attention layers flanking the transformer stack (one at the beginning and one at the end), and d) Alternating Layers (8 Layers): Three M\u00f6bius attention layers interspersed throughout the stack, each separated by two vanilla attention layers.\nOur findings, listed in Appendix A.7, revealed that both the stacked and alternating configurations yielded inferior performance compared to the framed model. This suggests potential overfitting within these architectures. Conversely, the framed architecture appears to introduce complexity in a controlled manner, mitigating overfitting. The initial M\u00f6bius attention captures intricate patterns, followed by vanilla attention layers that focus on specific aspects within those patterns and refine them. The final M\u00f6bius attention leverages the refined representation for even more complex reasoning."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce M\u00f6biusAttention, a novel attention mechanism based on M\u00f6bius transformations. It offers greater expressiveness compared to traditional attention by leveraging M\u00f6bius transformations' unique capabilities. These transformations enable mappings between different geometries like line to line or circle, representing various shapes such as Circular, Hyperbolic, Loxodromic, Parabolic, and Elliptic. We integrated M\u00f6biusAttention into BERT and RoFormer, forming M\u00f6biusBERT and MobRoFormer, and evaluated their performance on GLUE benchmarks. Results show our models outperform their baseline on various tasks and on average with our M\u00f6biusBERT models having fewer parameters (about 104M versus 110M) and no increase in training time. Our ablation study found that combining M\u00f6biusAttention with traditional attention achieved the best performance among various architectural options."}, {"title": "A APPENDIX", "content": "This section provides supplementary materials for our paper titled \"Leveraging M\u00f6bius Transfor- mations for Improved Attention in Transformer Models\". It includes details on our motivation for the experimental setup, the architecture of M\u00f6biusBERT, the hyperparameter settings used for pre- training, an introduction to the GLUE tasks, the hyperparameter specifications for GLUE fine-tuning, results from the ablation study, visualizations of standard attention mechanisms, and analysis of M\u00f6biusAttention. We also provide a discussion on the limitations, A.9, and broader impact, A.10 of M\u00f6bius Attention."}, {"title": "A.1 MOTIVATION OF THE EXPERIMENTAL SETUP", "content": "We integrate M\u00f6biusAttention within the BERT architecture (Devlin et al., 2019) for several reasons. Firstly, BERT's widespread adoption and popularity make it a common benchmark for comparison in NLP tasks, e.g., Liu et al. (2019); Su et al. (2024); He et al. (2021). Secondly, BERT serves as the foundation for numerous high-performing models, such as RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2021), Albert (Lan et al., 2019), etc. Finally, BERT's size allows for efficient training with limited resources compared to larger models that would require significantly longer training times (e.g., GPT3 (Brown et al., 2020), Mistral 7B (Jiang et al., 2023)). Our primary goal is to demonstrate performance improvements with M\u00f6biusAttention without incurring substantial increases in memory and time complexity. As achieving state-of-the-art results often demands significantly more computational resources, BERT presents itself as the ideal candidate for our study.\nInstead of the original BERT framework (Devlin et al., 2019), which was also followed in RoFormer (Su et al., 2024), we choose the newer MosaicBERT framework (Portes et al., 2023). BERT is originally trained on TPUs, while our hardware configuration consists of GPUs. The MosaicBERT framework offers training optimizations for BERT specifically designed for A100 GPUs, aligning perfectly with our setup. Additionally, the Toronto BookCorpus dataset (Zhu et al., 2015) used for pre-training BERT was not publicly available during our research timeframe. The authors of the MosaicBERT framework have accordingly chosen to work on the C4 dataset and provided the used codebase for full reproducibility of their BERT baseline, alleviating the dataset-mismatch challenge."}, {"title": "A.2 ARCHITECTURE AND DESIGN", "content": "In this section we provide details on the architecture and design of our M\u00f6biusAttention-enhanced models. Specifically, we present the mixed-head version as it achieves the highest performance. However, we also explain all alterations required for a layer with only M\u00f6biusAttention heads.\nEach block using M\u00f6biusAttention is realized in Complex space. This causes several adjustments to the block which we build as follows:\nGiven our complex input I = Ir + I\u00bf \u00b7 i, we first pass the values in the real and imaginary channels through a single LayerNorm instance (LN\u2081) before performing M\u00f6biusAttention and vanilla attention (Eq. (10-12)). We add the real and imaginary parts of the M\u00f6biusAttention output in order to obtain only one channel, allowing us to concatenate the outputs from the two types of attention heads (Eq. (13)). Next, we apply a linear layer (LL) and perform dropout, Eq. (14), add the residual connections, Eq. (15), and apply again LayerNorm (LN2 instance), Eq. (16).\nI' := I + I \u00b7 i = LN1(Ir) + LN1(I\u00bf) \u00b7 i (10)\nAr + A\u00bf \u00b7 i = MobAtt(T\u2084(I'), K(I'), V(I')) (11)\nAvanilla = Att(Qvanilla(I', + I), Kvanilla(I', + I), Vvanilla(I + I)) (12)\nAjoined = Avanilla||(Ar + Ai) (13)\nA' = Dropout(LL(Ajoined)) (14)\nA'+ = I (15)\nA' = LN2(A'). (16)"}, {"title": "We then pass A' through a Feed Forward Layer as defined in BERT (FFL), Eq. (17).", "content": "Finally, we add again residual connections, Eq. (18), and apply a LayerNorm (LN3) to get the output O.\nA\" = FFL(A') (17)\nA\"+ = A' (18)\nO = LN3(A\"). (19)\nThose adaptations are visually shown in Figure 4a. We note that we do not follow the Complex version of backpropagation (Benvenuto & Piazza, 1992) or use complex-valued normalization layers (Eilers & Jiang, 2023; Trabelsi et al., 2018) for computational efficiency.\nThe remaining blocks (excluding the first and last) are standard Transformer blocks.\nWe note that in our mixed-head models we add the real and imaginary parts of the M\u00f6biusAttention output before the application of the linear layer, as shown in Eq. (13). This is required to obtain only one channel, allowing to concatenate the outputs from the two types of attention heads. For architectures with layers using M\u00f6biusAttention heads, it is possible to add the two channels at the end of the block. Benefits of this approach are that we can apply two linear projections separately on the channels, as well as separate"}]}