{"title": "MLC-GCN: Multi-Level Generated Connectome Based GCN for AD Analysis", "authors": ["Wenqi Zhu", "Yinghua Fu", "Ze Wang"], "abstract": "Alzheimer's Disease (AD) is a currently incurable neurodegeneartive disease. Accurately detecting AD, especially in the early stage, represents a high research priority. AD is characterized by progressive cognitive impairments that are related to alterations in brain functional connectivity (FC). Based on this association, many studies have been published over the decades using FC and machine learning to differentiate AD from healthy aging. The most recent development in this detection method highlights the use of graph neural network (GNN) as the brain functionality analysis. In this paper, we proposed a stack of spatio-temporal feature extraction and graph generation based AD classification model using resting state fMRI. The proposed multi-level generated connectome (MLC) based graph convolutional network (GCN) (MLC-GCN) contains a multi-graph generation block and a GCN prediction block. The multi-graph generation block consists of a hierarchy of spatio-temporal feature extraction layers for extracting spatio-temporal rsfMRI features at different depths and building the corresponding connectomes. The GCN prediction block takes the learned multi-level connectomes to build and optimize GCNs at each level and concatenates the learned graphical features as the final predicting features for AD classification. Through independent cohort validations, MLC-GCN shows better performance for differentiating MCI, AD, and normal aging than state-of-art GCN and rsfMRI based AD classifiers. The proposed MLC-GCN also showed high explainability in terms of learning clinically reasonable connectome node and connectivity features from two independent datasets. While we only tested MLC-GCN on AD, the basic rsfMRI-based multi-level learned GCN based outcome prediction strategy is valid for other diseases or clinical outcomes.", "sections": [{"title": "I. INTRODUCTION", "content": "LZHEIMER'S disease (AD) is a progressive neurode- generative disease characterized by hallmark patholog- ical depositions and cognitive impairments such as memory decline and executive dysfunction [1]. Due to the lack of an effective cure for AD and the unclear etiology, a top research focus is on early disease diagnosis as the best hope of treatment or interventions for AD is to delay or slow down its progression in the early stage. Functional magnetic resonance imaging (fMRI) is a non-invasive imaging technique that has been increasingly used in AD research due to its ability to probe regional brain function alterations and interregional functional connectivity (FC) and subsequently the whole brain connectome [2], [3]. Over the past decade, several studies have investigated resting state fMRI (rsfMRI)-revealed FC in AD diagnosis using traditional classification methods such as generalized linear regression [4], random forest [5] and support vector machine (SVM) [6]. These methods are often limited by the dependence of prior knowledge, the empirical and complicated feature selection as well as the inability to extract and use data features at different hierarchies, which leads to sub-optimal classification performance. The most recent devel- opment in this research topic highlights the use of deep neural network (DNN), which is the state-of-art in machine learning that is free from the above-mentioned drawbacks of traditional \"shallow\" machine learning. The initial work of DNN in AD prediction mainly is based on convolutional neural networks (CNNs), a popular and powerful DNN framework [7]\u2013[9]. Ramzan et al. used ResNet-18 to extract image features from 2D fMRI image slices to build an AD multi-classification predictor [8]. Kam et al. built a 3D CNN AD classifier using the 3D fMRI images as the input [7]. Parmar et al. used the 4D rsfMRI data (five consecutive rsfMRI image volumes) to build a CNN-based multi-class (normal aging, early and late mild cognitive impairment, and AD) AD classifier [9]. FC was not explicitly considered in these studies although the ICA process included in [7] can implicitly utilize FC.\nThe whole brain FC matrix (connectome) has long been used to model the brain connection network in a graph defined by the spatial node and inter-node connections. A natural way to combine connectome and DNN for AD or other disease prediction is to build a network graph using the functional connectome and then input the graph to a graph convolutional network (GCN) [10]\u2013[16]. In an early study [10], Parisot et al. presented a group level GCN including individual rsfMRI data and phenotypic data to form a population level graph. Functional connectome is included as the node feature and subsequently condensed using graph Fourier transform in the populational graph. Kazi et al. [11] proposed an extension of the GCN by Parisot et al. [10] through changing the receptive field of the filters so that the inter- and intra-graph heterogeneity can be better captured. Song et al. [12] proposed an improved GCN by changing the node similarity calculation to consider the disease status difference between different categories and similarity difference between data modalities. Another variation of the GCN by Parisot et al was proposed by Huang and Chung [13]; they used Monte Carlo simulations to drop out non-effective edges in the population graph. Song et al. [17] incorporated multi-center and structural and functional connectivity into the population graph. Instead of directly using the individual subject's FC as the input features of the population GCN, Jiang et al. [18] used a pre-processing GCN to condense the FC features of each individual and then input them to the population GCN. Zhang et al. [19] published a similar approach but used an attention module to select the top k nodes from the individual level GCN output as the input features to the population GCN. These combined individual and population GCN approach may be able to extract better represented individual FC features and reduce the dimension of the input data to the population GCN but they should still be grouped into the population GCNs. Overall, the population GCNs achieve encouraging AD and other disease prediction accuracy, which may be a significant contribution by the inter-subject relationship encoding through the phenotypic data enhanced inter-node (inter-subject) association and populational graph learning. A big limitation of these methods is that the population graph needs to be created from the entire cohort which limits the generalizability and scalability. Pooling all subjects together and learning their features for classification in a single network is similar to a global data decomposition process, which risks being overfitted to the included subjects. The explicit inter-subject relationship encoding may further escalate this issue. When the number of subjects increases, there is an exponential increase of computation complexity. Meanwhile, the populational graph would need to be regenerated for a new subject and need to be retrained, which is nearly impractical. Another issue is that the population GCNs are difficult to interpret in the original brain space. These issues can be addressed using the individual GCNs.\nRecently, a few individual GCN studies have been published [14]\u2013[16]. In Xing et al. [14], the dynamic brain connectome- generated GCNs were used for AD and biological value pre- diction. Yao et al. [15] then proposed a mutual learning-based multi-scale triplet-based GCN to combine structural connec- tome and functional connectome for various brain disorder prediction. Yang et al [16] published a similar but simpler functional connectome and structural connectome and mutual learning based GCN based brain disorder classifier. Instead of using the Pearson correlation as the connectome association to form the network structure, a few studies have proposed network structure learning strategies through modeling the potential nonlinear spatio-temporal inter-regional relationship [20]\u2013[22]. Thus far, only the lowest level connectivity has been considered.\nNeuronal signal is known to have a multi-scale structure, as does inter-regional connectivity. To investigate the multi-scale multi-level property of functional connectome, we propose a novel multi-level feature extraction based GCN working on individual subject's rsfMRI data. We dub this technique as the multi-level generated connectome GCN (MLC-GCN). To gen- erate the connectomes at different scales of the input BOLD signals, we introduce a hierarchy of spatio-temporal feature extractors (STFEs) to extract spatio-temporal representations of the input BOLD signals at different level and use them to generate connectomes at different levels. The generated connectomes are then input into multiple GCNs to further learn the feature representation, and outputs of all different GCNs are concatenated and finally sent to a classifier to predict the disease status. The multi-classification task involves more discriminative features than bi-classification, so we take AD multi-classification to train the proposed MLC-GCN archi- tecture. The contributions of this paper are summarized as follows:\n1) A novel GCN architecture (MLC-GCN) for AD multi- classification is proposed to obtain rich temporal and regional correlations by combining the generated conec- tomes at different levels and multiple independent GCN encoders.\n2) STFE is specially designed to extract spatio-temporal features for different levels of time series data, which includes more significant multi-scale information than previous architectures.\n3) The experimental results on the public medical datasets Alzheimer's Disease Neuroimaging Initiative (ADNI) and Open Access Series of Imaging Studies-3 (OASIS-3) demonstrate that MLC-GCN achieves the state-of- the-art performance. Extensive ablation experiments are also conducted to discuss the effectiveness of modules in MLC-GCN."}, {"title": "II. METHODS", "content": "Fig. 1 illustrates the flowchart of the proposed MLC-GCN. The first module (a) is for fMRI preprocessing. The right two modules are the multi-graph generator and the multi-level GCNs predictor included in the MLC-GCN. The multi-graph generator is designed to construct a brain connectome using the fMRI time series extracted through the embedding and STFE module at each different level. Each generated connec- tome (graph) is then encoded into an embedding vectors by an independent GCN. The output of all GCNs are concatenated into a vector which is input to a multi-layer perceptron (MLP) for predicting AD status (or other clinical outcome for other disease).\nA. Multi-Graph Generator\nThe multi-graph generator contains a hierarchy of feature extraction and graph generation module. At the lowest level, the preprocessed fMRI time series with a length of L from n ROIs are directly used to calculate a n\u00d7n Pearson correlation coefficient matrix. Matrix element at the i-th row and j-th column is the correlation coefficient between the fMRI time series of the i-th ROI and the j-th ROI (i, and j are from 1 to n). This matrix is used to build the brain graph: the connectome F. At each higher level of the hierarchy i, the output of the lower level STFE module is sent to a new STFE module to extract new temporal features of the fMRI signal hi. These features will be used to calculate a correlation coefficient matrix and form a graph (the brain connectome) A(i). The temporal features are also sent to the upper level for further processing. At the second level, an embedding layer is added in front of the STFE module, which is made of a 1D- CNN designed to extract the compact feature representation for each of all n time series in the successive STFE module and to form the feature matrix with size n\u00d7l (l < L). Details are provided below.\n1) Input Embedding: The embedding layer is used to extract abstract temporal features from the discretely sampled points. For ROI p, denote the corresponding time series by Xp (length is L). After 1D temporal convolution through multiple kernels in the 1D-CNN, a new series Zp with a length of l is obtained. For the preprocessed BOLD signals from fMRI X = {X1, X2, \u2026\u2026\u2026, Xn} where n denotes the number of ROIs,\nthe embedded feature of the whole brain Z is obtained through a fully-connected layer with the following formulas:\n$Z = \u03c3(Flatten(Conv(X))W) + PE$ (1)\n$PE(pos, 2q) = sin(\\frac{pos}{10000^{2q/h}})$, (2)\n$PE(pos, 2q + 1) = cos(\\frac{pos}{10000^{2q/h}})$,\nwhere Conv() denotes a 1D-CNN with size t and m kernels involving, $W \u2208 R^{mL\u00d7l}$ the learnable weight matrix of the lin- ear layer, $\u03c3(\u00b7)$ the activation function, $Z = {Z_1, Z_2,..., Z_n}$\nthe hidden representation of time series. PE in (2) is the constant position embedding [23], where pos is the token\nposition and q is the embedding dimension which is set to be n in this paper. $X \u2208 R^{n\u00d7L}$ is transformed into the hidden\nrepresentation $Z \u2208 R^{n\u00d7l}$, where $Z_p$ is the embedded vector of the p- ROI.\n2) The STFE Module: Fig. 2 illustrates the architecture of a STFE module, which consists of two parallel pathways: one for spatial feature extraction (SFE), the other for temporal feature extraction (TFE). The SFE pathway is composed of the encoder of a transformer [23], consisting of a multi-head attention layer and a feed forward layer. Given the input feature $h_{i\u22121}$ which is the output of the preceding level STFE,\nSFE at the i-th level can be described as:\n$h_{Trans} = Trs(h_{i-1})$ (3)\nwhere Trs() means the encoder of a transformer consisting of a multi-head attention layer and a multilayer perceptron (MLP) layer. We use a transformer in the SFE pathway to specifically encode the node order information during spatial feature extraction.\nThe TFE pathway contains a linear decomposition block and a feed forward layer connected and followed by a nor- malization (Norm) block. The linear decomposer (DLinear) is designed to extract the hidden features through a trend- cycle operator and seasonal variation operator [24], which has shown good performance for feature extraction for univariate time series. For the input feature $h_{i\u22121}$, TFE will perform the following operations:\n$h_{trend} = AvgPool(h_{i-1})$, (4)\n$h_{seasonal} = h_{i-1} - h_{trend}$,\n$h_{DLinear} = MLP(\u03c3(W_t h_{trend}) + \u03c3(W_s h_{seasonal}))$\nwhere $h_{trend}$ refers to the trend-cycle features, $h_{seasonal}$ means the seasonal variation features, $W_t$ and $W_s$ are the learnable matrices. AvgPool() with size t is implemented with padding operation to keep the size of $h_{i\u22121}$.\nThe output of SFE and TFE are then fused to form the output of the i-th level STFE and can be described through Equations (5) and (6) below:\n$h_i = MLP(h_{DLinear} + h^T_{Trans})$ (5)\n$h_{i+1} = STFE(h_i), h_0 = Z$ (6)\nwhere hi, i = 0,1,2,..., K, denotes the output of the i-th layer, and k is the total number of STFE blocks.\nAfter normalization, $h_i$ can be used to build the i-th level connectome (the adjacency matrix A) through dot product:\n$A^{(i)} = h_i (h_i)^T$ (7)\nThrough the K levels of STFE, we will get K sets of n features (note that we keep the spatial dimension after feature extraction through the SFE pathway): h = {$h_1,h_2,\u2026,h_k$}\nand the corresponding set of K adjacency matrices: G =\n{$G_1, G_2,\u2026\u2026,G_K$}.\nB. Multi-Level GCNs-based Predictor\nThe standard brain connectome $G_0$ (calculated from the preprocessed fMRI time series) and the generated graphs\nG = {$G_1, G_2,\u2026\u2026,G_K$} at K levels are sent to a K + 1 level GCNs-based predictor as the input. For each of the K+1\nlevels, a GCN will be learned based on the corresponding input $G_i, i = 0,1,2,..., K$ to generate the output graph embedding\n$E_i, i = 0,1,2,\u2026, K$. Denote the input temporal features of the nodes for the j-th layer of the i-th GCN by $h^{(i)}$:\n$h^{(i)} = [h_1^{(i)},h_2^{(i)},...,h_n^{(i)}]^T$. The operation of the j-th layer of this GCN can be described by:\n$h_1^{(i)} = \u03c3((A^{(i)}h)W^{(j)}), (8)\n$h^{(i)} = F, A^{(i)} = A^{(i)} + I$\nwhere $A^{(i)} \u2208 R^{n\u00d7n}$ denotes the adjacency matrix of the generated graph, n is the number of nodes, I is an identity matrix performing as self-connections, $W^{(j)}$ is a trainable weight matrix of the jth layer and $\u03c3(\u00b7)$ the activation function. F is Pearson correlation coefficient matrix. For simplicity, the same adjacency matrix $A^{(2)}$ is used for all layers of GCNs,\nand the number of layer of GCN is set to 2.\nThe output of the last graph convolutional layer of the k-th GCN will be embedded into a 1D vector $E_k$ through an MLP operation and concatenated according to the order of k in all\nK GCNs into a the final multi-level graph embedded vector\n$E = {E_0, E_1,\uff65\uff65\uff65, E_k}$. This fused vector is then passed to a\nMLP for clinical outcome prediction:\n$Y = Softmax(MLP(Concat(E_0, E_1,\uff65\uff65\uff65, E_k)))$ (9)\nwhere Y = [$y_o, Y_1,\u2026\u2026, Y_c$]^T is the final output of MLC-GCN referring to the disease state of the input images, c the number of class categories.\nC. Objective Function\nFor prediction performance, we adopt the cross entropy loss\n$L_{CE}$ to constraint MLC-GCN:\n$L_{CE} = -\\frac{1}{N} \\sum_{i=1}^{c} 1(y=i)log(\\hat{y})$ (10)\nwhere \u0177 and y represent the predicted results and the ground- truth labels, respectively. N is the number of samples and 1(\u00b7) is the indicator function.\nTo force the graph generator of the MLC-GCN to learn mu- tually different connectomes and the corresponding graphs, we add an intra-group loss to minimize the intra-group difference\nas described by (11) below:\n$L_{group} = \\frac{1}{K} \\sum_{i=1}^{k} \\sum_{c\u2208C} \\frac{1}{|S_c|} \\sum_{u\u2208S_c} \\sum_{u\u2208S_c} (A_{\\mu_c}^{(i)} - A_{\u03bc_c}^{(i)})^2$ (11)\n$\u03bc_c = \\frac{1}{|S_c|} \\sum_{u\u2208S_c} A_u^{(i)}$\nwhere C denotes the set of class labels; $S_c = {u|Y_{u,c} = 1}$ is the set of samples with label c; K is the number of levels of STFE; $\u03bc_c$ is the mean of the learnable adjacency matrix $A^{(i)}$ in class c and layer i within a batch.\nThe final loss is a combination of LCE and the intra-group loss $L_{group}$:\n$L_{total} = L_{CE} + \u03b1L_{group}$ (12)\nwhere \u03b1 is hyper-parameter and is set to 1.0 in this paper."}, {"title": "III. EXPERIMENTS, RESULTS AND DISCUSSION", "content": "In this section, we present several comparative experiments and an ablation study on the public datasets ADNI and OASIS- 3 to validate the effectiveness of MLC-GCN. Furthermore, we investigate the brain graphs generated by MLC-GCN to ex- plore their consistency with existing neuroscience discoveries.\nTable II presents the results of the bi-classification task differentiating NC from AD on ADNI. The proposed MLC- GCN in this paper was trained using 6, 12, and 24 levels of STFE denoting MLC_GCN6, MLC_GCN12 and MLC_GCN24 respectively. It can be seen that the machine learning methods perform poorly compared to other types of methods due to the complexity of fMRI. CNN [27] has better performance in the classification between NC and AD than brain GNNs because the Pearson graph may include the unrelated brain information of AD, which even outperforms the SOTA brain GNN model MMTGCN [15]. LG-GCN [19] as a hybrid GNN as well as FBNetGNN [21] and DABNet [33] as the graph generation method have got the better performance than brain GNNs and DNNs without graph. LG-GCN gets a little higher performance than FBNetGNN in Acc, AUC, Spe and Sen, but has the similar performace with DABNet. Moreover, MLC- GCN with the basic 6-layer STFE is superior to all other methods on all metrics, and the performance also increases with the number of layers of STFE.\nTable III gives the results of the multi-classification task on the ADNI and OASIS-3. In this case, the accuracy of machine learning methods is greatly reduced indicating that hand- crafted extractors is hard to capture the representative features when samples become increasingly complex. The performance of DNNs and CNNs also decreases with the highest accuracy lower than 75% and FCNet even lower than 70% on ADNI. Several GNN-based methods still outperform BrainnetCNN and DNN, which indicates that their architectures capture the representative features even with the increasing data com- plexity. It should be noted that FBNetGNN performs worse than GCN taking Pearson graph on ADNI, as the stronger feature extractors to construct the brain graph are needed when the data becomes more various and complex. LG-GCN still obtains the best performance among the published methods because of the sophisticated architecture. For the proposed architecture in this paper, MLC-GCN with 12 ATSFEs has an accuracy of over 80% and with 24 ATSFEs reaches 82.26% on ADNI, higher almost 10% than GCN. The performance of MLC-GCN shows an upward trend with the increasing ATSFEs indicating effectiveness of the deep features when the data become complex for the multi-classification task. The result of OASIS-3 is similar with ADNI. The strategy of multi-level feature extraction to construct connectomes intends to simultaneously expand the depth of information exploration and the division of feature levels in the model. Table III indicates the model performance improves as the levels of STFE upsend, showing effectiveness of the sparse advanced features. MLC-GCN with three different stacks MLC_GCN6, MLC_GCN12 and MLC_GCN24 get the best performance among all the compared methods. It should be noted that the results on OASIS-3 are nearly overall higher than those on ADNI as the labels number on OASIS-3 is 3 in our experiments.\nD. Ablation Experiments\nTable IV shows the ablation experiments to validate the designed components for the graph generator in MLC-GCN. The performance of MLC-GCN decreases when removing TFE or SFE, which implies their strong temporal and spatial feature extraction separately in the graph generator. In addi- tion, the performance reduces more by removing SFE than by TFE, which may indicate that the temporal features are less important than the interregional correlation. These results prove the appropriateness of taking the spatial connectivity into account during the graph generation proposed in the paper. The regularization Lgroup also improves the performance of the model to a certain extent by controlling the generator to cluster the graphs according to the labels. MLC-GCN with TFE, SFE and Lgroup achieves the highest performance on all metrics indicating that each component contributes to improve of the architecture.\nFig 3 shows a comparison of the results of MLC-GCN including 6 ATSFEs with the other two models to verify the usage of multi-layer GCN encoders. GCNap includes 7 identical layers as all edges of the graphs in different layers are composed of Pearson correlation matrices, and MLC-GCNal adopts the same architecture with MLC-GCN, but all graphs are generated from the output of the last STFE. MLC-GCN is higher than GCNap across all metrics on ADNI and OASIS- 3 showing that the generated graphs capture even better feature than that of Pearson matrices. Moreover, the difference between MLC-GCN and MLC-GCNal on performance also indicates different levels of features more effectively represent the fMRI data than the single-layer features.\nFig 4 presents the ablation results on the different number of levels in graph generators of MLC-GCN with 6 ATSFEs. The results on two datasets demonstrate that the concatenating brain graphs at different levels can effectively improve the performance of classification. MLC-GCN with 6 graphs from each STFE far outperforms MLC-GCN6 with a single brain graph generated from the 6th level, and still is higher than the architectures including a part of the levels. MLC-GCN2,4,6 involving the 2nd, 4th and 6th levels gets a lightly higher score on Accuracy and Spe than MLC-GCN on OASIS-3, revealing the certain inter level gap may also be beneficial to feature selection and the more relaxed architecture may alleviate overfitting. These experiments imply the multi-level graphs proposed in this paper capture more the hierarchical information of the brain in fMRI than the single or a few ones.\nFig 5 displays the results of different numbers of embed- ding length l for MLC-GCN. The accuracy on two datasets increases with the increasing numbers of embedding length from 16 to 64, and then reversely decrease. The performance of the model reaches its peak when the embedding length is selected between 32 and 128. The reason can be that a smaller length can lead to information loss during the embedding process, making it unable to contain enough useful information and a larger length, especially longer than the original time series, contains too much loose information, which hinders the model's ability to integrate feature information. In this observation, we set the numbers of embedding length to 64 in all other experiments.\nIn fact, MLC-GCN has some universality, and we only adopt the standard GCN as the encoder, which can be further improved by considering more powerful GNN encoders.\nE. Model visualization\nTo check what the MLC-GCN learned under the regulariza- tion of both cross entropy loss and intra-class graph dissimilar- ity, we visualized the mean learned connectome graphs of the multi-graph generator module after MLC-GCN training. Fig. 6 (a) and Fig. 6 (b) display the mean correlation coefficient matrix of the non-STFE processed fMRI time series ($G_0$)) and the mean of all generated graphs of all subjects ($G_1$ to $G_K$). The generated graph shows higher sparsity than the graph of the non-temporal feature processed (non-STFE processed) time series with high connectivity mainly located in prefrontal and temporal cortex (Fig. 6 (c)). Fig. 6 (c) shows the graph in the 3D brain space. Network visualization was performed using the BrainNet Viewer (http://www.nitrc.org/projects/bnv/) [39]. The graph is generated by the top 1% most connected edges of the generated connectomes shown in Fig. 6 (b). The size of node indicates the number of connected edges after thresholding and the color indicates different brain ROIs.\nFig. 7 visualizes the top 20 most important brain regions associated with AD in the MLC-GCN models built for ADNI and OASIS-3 separately. The importance of ROIs is quantified by summing the edge weights of each node in the average generated graph. The two independently trained MLC-GCN models show over-lapped top 20 nodes in superior frontal gyrus (SFG), middle frontal gyrus (MFG), inferior frontal gyrus (IFG), paracentral lobule (PCL), superior temporal gyrus (STG) and middle temporal gyrus (MTG).\nIV. CONCLUSION\nIn this paper, we proposed a stack of spatio-temporal feature extraction and graph generation based clinical outcome prediction model. Through the intra-class graph dissimilarity regularization, the multi-level STFEs and GCNs are trained to learn different sparse brain graphs at different scales. Through independent cohort validations, MLC-GCN shows better performance for differentiating MCI, AD, and normal aging than state-of-art GCN and rsfMRI based AD classifiers. The proposed MLC-GCN also showed high explainability in terms of learning clinically reasonable connectome node and connectivity features from two independent datasets. While we only tested MLC-GCN on AD, the basic rsfMRI-based multi- level learned GCN based outcome prediction strategy is valid for other diseases or clinical outcomes."}]}