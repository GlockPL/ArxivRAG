{"title": "MLC-GCN: Multi-Level Generated Connectome Based GCN for AD Analysis", "authors": ["Wenqi Zhu", "Yinghua Fu", "Ze Wang", "Alzheimer's Disease Neuroimaging Initiative"], "abstract": "Alzheimer's Disease (AD) is a currently incurable neurodegeneartive disease. Accurately detecting AD, especially in the early stage, represents a high research priority. AD is characterized by progressive cognitive impairments that are related to alterations in brain functional connectivity (FC). Based on this association, many studies have been published over the decades using FC and machine learning to differentiate AD from healthy aging. The most recent development in this detection method highlights the use of graph neural network (GNN) as the brain functionality analysis. In this paper, we proposed a stack of spatio-temporal feature extraction and graph generation based AD classification model using resting state fMRI. The proposed multi-level generated connectome (MLC) based graph convolutional network (GCN) (MLC-GCN) contains a multi-graph generation block and a GCN prediction block. The multi-graph generation block consists of a hierarchy of spatio-temporal feature extraction layers for extracting spatio-temporal rsfMRI features at different depths and building the corresponding connectomes. The GCN prediction block takes the learned multi-level connectomes to build and optimize GCNs at each level and concatenates the learned graphical features as the final predicting features for AD classification. Through independent cohort validations, MLC-GCN shows better performance for differentiating MCI, AD, and normal aging than state-of-art GCN and rsfMRI based AD classifiers. The proposed MLC-GCN also showed high explainability in terms of learning clinically reasonable connectome node and connectivity features from two independent datasets. While we only tested MLC-GCN on AD, the basic rsfMRI-based multi-level learned GCN based outcome prediction strategy is valid for other diseases or clinical outcomes.", "sections": [{"title": "I. INTRODUCTION", "content": "ALZHEIMER\u2019S disease (AD) is a progressive neurode-generative disease characterized by hallmark patholog-ical depositions and cognitive impairments such as memorydecline and executive dysfunction [1]. Due to the lack ofan effective cure for AD and the unclear etiology, a topresearch focus is on early disease diagnosis as the best hope oftreatment or interventions for AD is to delay or slow down itsprogression in the early stage. Functional magnetic resonanceimaging (fMRI) is a non-invasive imaging technique that hasbeen increasingly used in AD research due to its abilityto probe regional brain function alterations and interregionalfunctional connectivity (FC) and subsequently the whole brainconnectome [2], [3]. Over the past decade, several studieshave investigated resting state fMRI (rsfMRI)-revealed FC inAD diagnosis using traditional classification methods such asgeneralized linear regression [4], random forest [5] and supportvector machine (SVM) [6]. These methods are often limitedby the dependence of prior knowledge, the empirical andcomplicated feature selection as well as the inability to extractand use data features at different hierarchies, which leads tosub-optimal classification performance. The most recent devel-opment in this research topic highlights the use of deep neuralnetwork (DNN), which is the state-of-art in machine learningthat is free from the above-mentioned drawbacks of traditional\"shallow\" machine learning. The initial work of DNN in ADprediction mainly is based on convolutional neural networks(CNNs), a popular and powerful DNN framework [7]\u2013[9].Ramzan et al. used ResNet-18 to extract image features from2D fMRI image slices to build an AD multi-classificationpredictor [8]. Kam et al. built a 3D CNN AD classifier usingthe 3D fMRI images as the input [7]. Parmar et al. used the 4DrsfMRI data (five consecutive rsfMRI image volumes) to builda CNN-based multi-class (normal aging, early and late mildcognitive impairment, and AD) AD classifier [9]. FC was notexplicitly considered in these studies although the ICA processincluded in [7] can implicitly utilize FC.\nThe whole brain FC matrix (connectome) has long beenused to model the brain connection network in a graph definedby the spatial node and inter-node connections. A natural wayto combine connectome and DNN for AD or other diseaseprediction is to build a network graph using the functionalconnectome and then input the graph to a graph convolutionalnetwork (GCN) [10]\u2013[16]. In an early study [10], Parisot etal. presented a group level GCN including individual rsfMRIdata and phenotypic data to form a population level graph.Functional connectome is included as the node feature andsubsequently condensed using graph Fourier transform in thepopulational graph. Kazi et al. [11] proposed an extensionof the GCN by Parisot et al. [10] through changing thereceptive field of the filters so that the inter- and intra-graphheterogeneity can be better captured. Song et al. [12] proposedan improved GCN by changing the node similarity calculationto consider the disease status difference between differentcategories and similarity difference between data modalities.Another variation of the GCN by Parisot et al was proposed byHuang and Chung [13]; they used Monte Carlo simulations todrop out non-effective edges in the population graph. Song etal. [17] incorporated multi-center and structural and functionalconnectivity into the population graph. Instead of directlyusing the individual subject's FC as the input features ofthe population GCN, Jiang et al. [18] used a pre-processingGCN to condense the FC features of each individual andthen input them to the population GCN. Zhang et al. [19]published a similar approach but used an attention moduleto select the top k nodes from the individual level GCNoutput as the input features to the population GCN. Thesecombined individual and population GCN approach may beable to extract better represented individual FC features andreduce the dimension of the input data to the population GCNbut they should still be grouped into the population GCNs.Overall, the population GCNs achieve encouraging AD andother disease prediction accuracy, which may be a significantcontribution by the inter-subject relationship encoding throughthe phenotypic data enhanced inter-node (inter-subject) as-sociation and populational graph learning. A big limitationof these methods is that the population graph needs to becreated from the entire cohort which limits the generalizabilityand scalability. Pooling all subjects together and learningtheir features for classification in a single network is similarto a global data decomposition process, which risks beingoverfitted to the included subjects. The explicit inter-subjectrelationship encoding may further escalate this issue. When thenumber of subjects increases, there is an exponential increaseof computation complexity. Meanwhile, the populational graphwould need to be regenerated for a new subject and need tobe retrained, which is nearly impractical. Another issue is thatthe population GCNs are difficult to interpret in the originalbrain space. These issues can be addressed using the individualGCNs.\nRecently, a few individual GCN studies have been published[14]\u2013[16]. In Xing et al. [14], the dynamic brain connectome-generated GCNs were used for AD and biological value pre-diction. Yao et al. [15] then proposed a mutual learning-basedmulti-scale triplet-based GCN to combine structural connec-tome and functional connectome for various brain disorderprediction. Yang et al [16] published a similar but simplerfunctional connectome and structural connectome and mutuallearning based GCN based brain disorder classifier. Instead ofusing the Pearson correlation as the connectome associationto form the network structure, a few studies have proposednetwork structure learning strategies through modeling thepotential nonlinear spatio-temporal inter-regional relationship[20]\u2013[22]. Thus far, only the lowest level connectivity has beenconsidered.\nNeuronal signal is known to have a multi-scale structure, asdoes inter-regional connectivity. To investigate the multi-scalemulti-level property of functional connectome, we propose anovel multi-level feature extraction based GCN working onindividual subject's rsfMRI data. We dub this technique as themulti-level generated connectome GCN (MLC-GCN). To gen-erate the connectomes at different scales of the input BOLDsignals, we introduce a hierarchy of spatio-temporal featureextractors (STFEs) to extract spatio-temporal representationsof the input BOLD signals at different level and use themto generate connectomes at different levels. The generatedconnectomes are then input into multiple GCNs to furtherlearn the feature representation, and outputs of all differentGCNs are concatenated and finally sent to a classifier to predictthe disease status. The multi-classification task involves morediscriminative features than bi-classification, so we take ADmulti-classification to train the proposed MLC-GCN archi-tecture. The contributions of this paper are summarized asfollows:\n1) A novel GCN architecture (MLC-GCN) for AD multi-classification is proposed to obtain rich temporal andregional correlations by combining the generated conec-tomes at different levels and multiple independent GCNencoders.\n2) STFE is specially designed to extract spatio-temporalfeatures for different levels of time series data, whichincludes more significant multi-scale information thanprevious architectures.\n3) The experimental results on the public medical datasetsAlzheimer's Disease Neuroimaging Initiative (ADNI)and Open Access Series of Imaging Studies-3 (OASIS-3) demonstrate that MLC-GCN achieves the state-of-the-art performance. Extensive ablation experiments arealso conducted to discuss the effectiveness of modulesin MLC-GCN.\nThe remainder of this paper is organized as follows. SectionII presents the architecture of MLC-GCN in detail. SectionIII describes the experimental results of MLC-GCN on thedatasets ADNI and OASIS-3 compared with other methods,the ablation experiments are described to indicate the effec-tiveness of the proposed modules, and the generated graphsin the multi-classification task are visualized and analyzed forassociation with existing brain research related to AD. Finally,we conclude the paper and offer suggestions for further studyin Section IV."}, {"title": "II. METHODS", "content": "Fig. 1 illustrates the flowchart of the proposed MLC-GCN.The first module (a) is for fMRI preprocessing. The righttwo modules are the multi-graph generator and the multi-levelGCNs predictor included in the MLC-GCN. The multi-graphgenerator is designed to construct a brain connectome usingthe fMRI time series extracted through the embedding andSTFE module at each different level. Each generated connec-tome (graph) is then encoded into an embedding vectors by anindependent GCN. The output of all GCNs are concatenatedinto a vector which is input to a multi-layer perceptron (MLP)for predicting AD status (or other clinical outcome for otherdisease).\nA. Multi-Graph Generator\nThe multi-graph generator contains a hierarchy of featureextraction and graph generation module. At the lowest level,the preprocessed fMRI time series with a length of L from nROIs are directly used to calculate a n\u00d7n Pearson correlationcoefficient matrix. Matrix element at the i-th row and j-thcolumn is the correlation coefficient between the fMRI timeseries of the i-th ROI and the j-th ROI (i, and j are from1 to n). This matrix is used to build the brain graph: theconnectome F. At each higher level of the hierarchy i, theoutput of the lower level STFE module is sent to a newSTFE module to extract new temporal features of the fMRIsignal hi. These features will be used to calculate a correlationcoefficient matrix and form a graph (the brain connectome)A(i). The temporal features are also sent to the upper levelfor further processing. At the second level, an embedding layer isadded in front of the STFE module, which is made of a 1D-CNN designed to extract the compact feature representationfor each of all n time series in the successive STFE moduleand to form the feature matrix with size n\u00d7l (l < L). Detailsare provided below.\n1) Input Embedding: The embedding layer is used toextract abstract temporal features from the discretely sampledpoints. For ROI p, denote the corresponding time series by Xp(length is L). After 1D temporal convolution through multiplekernels in the 1D-CNN, a new series Zp with a length of lis obtained. For the preprocessed BOLD signals from fMRIX = {X1, X2, \u2026\u2026\u2026, Xn} where n denotes the number of ROIs,the embedded feature of the whole brain Z is obtained througha fully-connected layer with the following formulas:\n$Z = \u03c3(Flatten(Conv(X))W) + PE$ (1)\n$PE(pos, 2q) = sin(\\frac{pos}{10000^{2q/h}}),$\n$PE(pos, 2q + 1) = cos(\\frac{pos}{10000^{2q/h}})$ (2)\nwhere Conv() denotes a 1D-CNN with size t and m kernelsinvolving, $W \u2208 R^{mL\u00d7l}$ the learnable weight matrix of the lin-ear layer, \u03c3(\u00b7) the activation function, Z = {$Z_1, Z_2,..., Z_n$}the hidden representation of time series. PE in (2) is theconstant position embedding [23], where pos is the tokenposition and q is the embedding dimension which is set tobe n in this paper. $X \u2208 R^{n\u00d7L}$ is transformed into the hiddenrepresentation Z \u2208 $R^{n\u00d7l}$, where $Z_p$ is the embedded vectorof the p- ROI.\n2) The STFE Module: Fig. 2 illustrates the architecture ofa STFE module, which consists of two parallel pathways: onefor spatial feature extraction (SFE), the other for temporalfeature extraction (TFE). The SFE pathway is composed ofthe encoder of a transformer [23], consisting of a multi-headattention layer and a feed forward layer. Given the inputfeature $h_{i\u22121}$ which is the output of the preceding level STFE,SFE at the i-th level can be described as:\n$h_{Trans} = Trs(h_{i-1})$ (3)\nwhere Trs() means the encoder of a transformer consistingof a multi-head attention layer and a multilayer perceptron(MLP) layer. We use a transformer in the SFE pathway tospecifically encode the node order information during spatialfeature extraction.\nThe TFE pathway contains a linear decomposition blockand a feed forward layer connected and followed by a nor-malization (Norm) block. The linear decomposer (DLinear)is designed to extract the hidden features through a trend-cycle operator and seasonal variation operator [24], which hasshown good performance for feature extraction for univariatetime series. For the input feature $h_{i\u22121}$, TFE will perform thefollowing operations:\n$h_{trend} = AvgPool(h_{i-1}),$\n$h_{seasonal} = h_{i-1} - h_{trend},$\n$h_{DLinear} = MLP(\u03c3(W_th_{trend}) + \u03c3(W_sh_{seasonal}))$ (4)\nwhere $h_{trend}$ refers to the trend-cycle features, $h_{seasonal}$means the seasonal variation features, Wt and Ws are thelearnable matrices. AvgPool() with size t is implemented withpadding operation to keep the size of $h_{i\u22121}$.\nThe output of SFE and TFE are then fused to form theoutput of the i-th level STFE and can be described throughEquations (5) and (6) below:\n$h_i = MLP(h_{DLinear} + h_{Trans}^T)$ (5)\n$h_{i+1} = STFE(h_i), h_0 = Z$ (6)\nwhere hi, i = 0,1,2,..., K, denotes the output of the i-thlayer, and k is the total number of STFE blocks.\nAfter normalization, h\u00b2 can be used to build the i-th levelconnectome (the adjacency matrix A) through dot product:\n$A^{(i)} = h_i(h_i)^T$ (7)\nThrough the K levels of STFE, we will get K sets of nfeatures (note that we keep the spatial dimension after featureextraction through the SFE pathway): h = {$h_1,h_2,\u2026,h_k$}and the corresponding set of K adjacency matrices: G ={$G_1, G_2,\u2026\u2026,G_K$}.\nB. Multi-Level GCNs-based Predictor\nThe standard brain connectome $G_0$ (calculated from thepreprocessed fMRI time series) and the generated graphsG = {$G_1, G_2,\u2026\u2026,G_K$} at K levels are sent to a K + 1level GCNs-based predictor as the input. For each of the K+1levels, a GCN will be learned based on the corresponding input$G_i, i = 0,1,2,..., K$ to generate the output graph embedding$E_i, i = 0,1,2,\u2026, K$. Denote the input temporal featuresof the nodes for the j-th layer of the i-th GCN by $h^{(i)}_j$:\n$h^{(i)}_0 = [h_1,h_2,...,h_n]^T$. The operation of the j-th layerof this GCN can be described by:\n$h_{l+1}^{(i)} = \u03c3(\\tilde{A}^{(i)}h_l^{(i)}W^{(l)}),$\n$\\tilde{A}^{(i)} = A^{(i)} + I$ (8)\nwhere $A^{(i)} \u2208 R^{n\u00d7n}$ denotes the adjacency matrix of thegenerated graph, n is the number of nodes, I is an identitymatrix performing as self-connections, $W^{(l)}$ is a trainableweight matrix of the jth layer and \u03c3(\u00b7) the activation function.F is Pearson correlation coefficient matrix. For simplicity,the same adjacency matrix $A^{(2)}$ is used for all layers of GCNs,and the number of layer of GCN is set to 2.\nThe output of the last graph convolutional layer of the k-thGCN will be embedded into a 1D vector $E_k$ through an MLPop operation and concatenated according to the order of k in allK GCNs into a the final multi-level graph embedded vectorE = {$E_0, E_1,\uff65\uff65\uff65, E_k$}. This fused vector is then passed to aMLP for clinical outcome prediction:\n$Y = Softmax(MLP(Concat(E_0, E_1,\u00b7\u00b7\u00b7, E_k)))$ (9)\nwhere Y = {$y_0, y_1,\u2026\u2026, y_c$}$^T$ is the final output of MLC-GCNreferring to the disease state of the input images, c the numberof class categories.\nC. Objective Function\nFor prediction performance, we adopt the cross entropy loss$L_{CE}$ to constraint MLC-GCN:\n$L_{CE} = -\\frac{1}{N}\\sum_{i=1}^C 1(y=i)log(\\hat{y}_i)$ (10)\nwhere \u0177 and y represent the predicted results and the ground-truth labels, respectively. N is the number of samples and 1(\u00b7)is the indicator function.\nTo force the graph generator of the MLC-GCN to learn mu-tually different connectomes and the corresponding graphs, weadd an intra-group loss to minimize the intra-group differenceas described by (11) below:\n$L_{group} = \\frac{1}{K} \\sum_{i=1}^k \\sum_{c\u2208C} \\sum_{u\u2208S_c} \\frac{1}{|S_c|} \\sum_{v\u2208S_c} ||A_{\\mu v}^{(i)} - \\mu_c^{(i)}||^2$\n$\\mu_c^{(i)} = \\frac{1}{|S_c|} \\sum_{u\u2208S_c} A_{\\mu}^{(i)}$ (11)\nwhere C denotes the set of class labels; $S_c$ = {u|$Y_{u,c}$ = 1}is the set of samples with label c; K is the number of levelsof STFE; $\\mu_c^{(i)}$ is the mean of the learnable adjacency matrix$A^{(i)}$ in class c and layer i within a batch.\nThe final loss is a combination of $L_{CE}$ and the intra-group loss $L_{group}$:\n$L_{total} = L_{CE} + \u03b1L_{group}$ (12)\nwhere \u03b1 is hyper-parameter and is set to 1.0 in this paper."}, {"title": "III. EXPERIMENTS, RESULTS AND DISCUSSION", "content": "In this section, we present several comparative experimentsand an ablation study on the public datasets ADNI and OASIS-3 to validate the effectiveness of MLC-GCN. Furthermore, weinvestigate the brain graphs generated by MLC-GCN to explore their consistency with existing neuroscience discoveries.\nTable II presents the results of the bi-classification taskdifferentiating NC from AD on ADNI. The proposed MLC-GCN in this paper was trained using 6, 12, and 24 levels ofSTFE denoting MLC_GCN6, MLC_GCN12 and MLC_GCN24respectively. It can be seen that the machine learning methodsperform poorly compared to other types of methods due tothe complexity of fMRI. CNN [27] has better performancein the classification between NC and AD than brain GNNsbecause the Pearson graph may include the unrelated braininformation of AD, which even outperforms the SOTA brainGNN model MMTGCN [15]. LG-GCN [19] as a hybrid GNNas well as FBNetGNN [21] and DABNet [33] as the graphgeneration method have got the better performance than brainGNNs and DNNs without graph. LG-GCN gets a little higherperformance than FBNetGNN in Acc, AUC, Spe and Sen, buthas the similar performace with DABNet. Moreover, MLC-GCN with the basic 6-layer STFE is superior to all othermethods on all metrics, and the performance also increaseswith the number of layers of STFE.\nTable III gives the results of the multi-classification task onthe ADNI and OASIS-3. In this case, the accuracy of machinelearning methods is greatly reduced indicating that hand-crafted extractors is hard to capture the representative featureswhen samples become increasingly complex. The performanceof DNNs and CNNs also decreases with the highest accuracylower than 75% and FCNet even lower than 70% on ADNI.Several GNN-based methods still outperform BrainnetCNNand DNN, which indicates that their architectures capture therepresentative features even with the increasing data com-plexity. It should be noted that FBNetGNN performs worsethan GCN taking Pearson graph on ADNI, as the strongerfeature extractors to construct the brain graph are needed whenthe data becomes more various and complex. LG-GCN stillobtains the best performance among the published methodsbecause of the sophisticated architecture. For the proposedarchitecture in this paper, MLC-GCN with 12 ATSFEs has anaccuracy of over 80% and with 24 ATSFEs reaches 82.26%on ADNI, higher almost 10% than GCN. The performanceof MLC-GCN shows an upward trend with the increasingATSFEs indicating effectiveness of the deep features whenthe data become complex for the multi-classification task.\nThe result of OASIS-3 is similar with ADNI. The strategy ofmulti-level feature extraction to construct connectomes intendsto simultaneously expand the depth of information explorationand the division of feature levels in the model. Table IIIindicates the model performance improves as the levels ofSTFE upsend, showing effectiveness of the sparse advancedfeatures. MLC-GCN with three different stacks MLC_GCN6,MLC_GCN12 and MLC_GCN24 get the best performanceamong all the compared methods. It should be noted thatthe results on OASIS-3 are nearly overall higher than thoseon ADNI as the labels number on OASIS-3 is 3 in ourexperiments.\nD. Ablation Experiments\nTable IV shows the ablation experiments to validate thedesigned components for the graph generator in MLC-GCN.The performance of MLC-GCN decreases when removingTFE or SFE, which implies their strong temporal and spatialfeature extraction separately in the graph generator. In addi-tion, the performance reduces more by removing SFE thanby TFE, which may indicate that the temporal features areless important than the interregional correlation. These resultsprove the appropriateness of taking the spatial connectivityinto account during the graph generation proposed in the paper.The regularization $L_{group}$ also improves the performance ofthe model to a certain extent by controlling the generator tocluster the graphs according to the labels. MLC-GCN withTFE, SFE and $L_{group}$ achieves the highest performance on allmetrics indicating that each component contributes to improveof the architecture.\nFig. 3 shows a comparison of the results of MLC-GCNincluding 6 ATSFEs with the other two models to verifythe usage of multi-layer GCN encoders. GCNap includes 7identical layers as all edges of the graphs in different layers arecomposed of Pearson correlation matrices, and MLC-GCNaladopts the same architecture with MLC-GCN, but all graphsare generated from the output of the last STFE. MLC-GCN ishigher than GCNap across all metrics on ADNI and OASIS-3 showing that the generated graphs capture even betterfeature than that of Pearson matrices. Moreover, the differencebetween MLC-GCN and MLC-GCNal on performance alsoindicates different levels of features more effectively representthe fMRI data than the single-layer features.\nFig 4 presents the ablation results on the different numberof levels in graph generators of MLC-GCN with 6 ATSFEs.The results on two datasets demonstrate that the concatenatingbrain graphs at different levels can effectively improve theperformance of classification. MLC-GCN with 6 graphs fromeach STFE far outperforms MLC-GCN6 with a single braingraph generated from the 6th level, and still is higher than thearchitectures including a part of the levels. MLC-GCN2,4,6involving the 2nd, 4th and 6th levels gets a lightly higherscore on Accuracy and Spe than MLC-GCN on OASIS-3,revealing the certain inter level gap may also be beneficialto feature selection and the more relaxed architecture mayalleviate overfitting. These experiments imply the multi-levelgraphs proposed in this paper capture more the hierarchicalinformation of the brain in fMRI than the single or a fewones.\nFig 5 displays the results of different numbers of embed-ding length l for MLC-GCN. The accuracy on two datasetsincreases with the increasing numbers of embedding lengthfrom 16 to 64, and then reversely decrease. The performanceof the model reaches its peak when the embedding length isselected between 32 and 128. The reason can be that a smallerlength can lead to information loss during the embeddingprocess, making it unable to contain enough useful informationand a larger length, especially longer than the original timeseries, contains too much loose information, which hindersthe model's ability to integrate feature information. In thisobservation, we set the numbers of embedding length to 64 inall other experiments.\nIn fact, MLC-GCN has some universality, and we onlyadopt the standard GCN as the encoder, which can be furtherimproved by considering more powerful GNN encoders.\nE. Model visualization\nTo check what the MLC-GCN learned under the regulariza-tion of both cross entropy loss and intra-class graph dissimilar-ity, we visualized the mean learned connectome graphs of themulti-graph generator module after MLC-GCN training. Fig.6 (a) and Fig. 6 (b) display the mean correlation coefficientmatrix of the non-STFE processed fMRI time series ($G_0$) andthe mean of all generated graphs of all subjects ($G_1$ to $G_K$).The generated graph shows higher sparsity than the graphof the non-temporal feature processed (non-STFE processed)time series with high connectivity mainly located in prefrontaland temporal cortex (Fig. 6 (c)). Fig. 6 (c) shows the graphin the 3D brain space. Network visualization was performedusing the BrainNet Viewer (http://www.nitrc.org/projects/bnv/)[39]. The graph is generated by the top 1% most connectededges of the generated connectomes shown in Fig. 6 (b). Thesize of node indicates the number of connected edges afterthresholding and the color indicates different brain ROIs.\nFig. 7 visualizes the top 20 most important brain regionsassociated with AD in the MLC-GCN models built for ADNIand OASIS-3 separately. The importance of ROIs is quantifiedby summing the edge weights of each node in the averagegenerated graph. The two independently trained MLC-GCNmodels show over-lapped top 20 nodes in superior frontalgyrus (SFG), middle frontal gyrus (MFG), inferior frontalgyrus (IFG), paracentral lobule (PCL), superior temporal gyrus(STG) and middle temporal gyrus (MTG)."}, {"title": "IV. CONCLUSION", "content": "In this paper, we proposed a stack of spatio-temporalfeature extraction and graph generation based clinical outcomeprediction model. Through the intra-class graph dissimilarityregularization, the multi-level STFEs and GCNs are trainedto learn different sparse brain graphs at different scales.Through independent cohort validations, MLC-GCN showsbetter performance for differentiating MCI, AD, and normalaging than state-of-art GCN and rsfMRI based AD classifiers.The proposed MLC-GCN also showed high explainability interms of learning clinically reasonable connectome node andconnectivity features from two independent datasets. While weonly tested MLC-GCN on AD, the basic rsfMRI-based multi-level learned GCN based outcome prediction strategy is validfor other diseases or clinical outcomes."}]}