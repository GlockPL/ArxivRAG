{"title": "UNIGEM: A UNIFIED APPROACH TO GENERATION\nAND PROPERTY PREDICTION FOR MOLECULES", "authors": ["Shikun Feng", "Yuyan Ni", "Yan Lu", "Zhi-Ming Ma", "Wei-Ying Ma", "Yanyan Lan"], "abstract": "Molecular generation and molecular property prediction are both crucial for drug\ndiscovery, but they are often developed independently. Inspired by recent stud-\nies, which demonstrate that diffusion model, a prominent generative approach,\ncan learn meaningful data representations that enhance predictive tasks, we ex-\nplore the potential for developing a unified generative model in the molecular\ndomain that effectively addresses both molecular generation and property predic-\ntion tasks. However, the integration of these tasks is challenging due to inherent\ninconsistencies, making simple multi-task learning ineffective. To address this,\nwe propose UniGEM, the first unified model to successfully integrate molecular\ngeneration and property prediction, delivering superior performance in both tasks.\nOur key innovation lies in a novel two-phase generative process, where predic-\ntive tasks are activated in the later stages, after the molecular scaffold is formed.\nWe further enhance task balance through innovative training strategies. Rigorous\ntheoretical analysis and comprehensive experiments demonstrate our significant\nimprovements in both tasks. The principles behind UniGEM hold promise for\nbroader applications, including natural language processing and computer vision.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence, particularly through deep learning technologies, is advancing various appli-\ncations in drug discovery. This encompasses two major tasks: molecular property prediction (Zaidi\net al., 2022; Feng et al., 2023a; Ni et al., 2023; 2024) and molecule generation (Hoogeboom et al.,\n2022; Guan et al., 2023; Gao et al., 2024). The objective of molecular property prediction is to learn\nfunctions that accurately map molecular samples to their corresponding property labels, which can\nfacilitate the virtual screening process. Meanwhile, molecule generation aims to estimate the under-\nlying molecular data distribution, offering significant potential for automatic drug design. Although\nconsiderable research has been conducted in these areas, they have largely progressed independently,\noverlooking their intrinsic correlations.\nIn our opinion, the essence of these two tasks lies in molecular representations. On the one hand,\nthe effectiveness of various molecular pre-training methods demonstrates that molecular property\nprediction relies on robust molecular representations as a foundation. On the other hand, molecule\ngeneration requires a deep understanding of molecular structures, enabling the creation of good\nrepresentations during the generation process. Recent research findings provide support for our per-\nspective. For instance, researchers in the computer vision field have shown that diffusion models\ninherently possess the ability to learn effective image representations (Chen et al., 2024; Hudson\net al., 2024; Mittal et al., 2023). In the molecular domain, studies have indicated that generative pre-\ntraining can enhance molecular property prediction tasks (Liu et al., 2023; Chen et al., 2023a), par-\nticularly through data augmentation using diffusion models. However, these methods often require\nadditional fine-tuning to achieve optimal predictive performances. Additionally, while predictive\ntasks can guide molecule generation (Bao et al., 2022; Gao et al., 2024), it remains unclear whether"}, {"title": "2 METHOD", "content": ""}, {"title": "2.1 UNIGEM FRAMEWORK", "content": "UniGEM is a two-phase diffusion based generative approach for property prediction and molecular\ngeneration. For the convenience of comparing with the traditional joint diffusion approach, we adopt\na notation scheme consistent with the E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al.,\n2022). We denote a 3D molecule with M atoms as z = (x, h), where x = (x1,\u2026,x\u043c) \u2208 R3M\nrepresents the atomic positions, and h = (h1,\u2026,h\u043c) \u2208 {\u0435\u043e,\u2026\u2026, \u0435\u043d}M encodes the atom type\ninformation. Each hi is a one-hot vector of dimension H, corresponding to the type of the atom i,\nwhere H is the total number of distinct atom types in the dataset."}, {"title": "2.1.1 TWO-PHASE GENERATIVE PROCESS", "content": "In contrast to traditional diffusion based model, UniGEM adopts diffusion process to generate only\nthe atomic coordinates x rather than the whole molecule z. We define the forward process for adding\nnoise to the molecular coordinates as follows:\n$$q(x_{0:T}) = q(x_0) \\prod_{t=1}^{t_n} q(x_t|x_{t-1}) \\prod_{t=t_n+1}^{T} q(x_t|x_{t-1}),$$\nwhere tn denotes the nucleation time to indicate when the molecule forms its scaffold. We define the\ngrowth phase as t \u2208 [0, tn] and the nucleation phase as t \u2208 [tn, T]. During the growth phase, we will\nincorporate atom type and property information, as these attributes are clearly defined at this stage.\nThe conditional distribution is defined as $q(x_t|x_0) = N_x(x_t| \\alpha_t x_0, \\sigma_t^2 I)$, t = 1,\u2026\u2026 ,T, where \u03b1t\nand \u03c3t represent the noise schedule following Hoogeboom et al. (2022) and satisfy the condition\n$\\alpha_t^2 + \\sigma_t^2 = 1$, with at decreases monotonically from 1 to 0. zo = (x0, h0) represent an unperturbed\nmolecule in the dataset. Nr represents the Gaussian distribution in the zero center-of-mass (CoM)\nsubspace satisfying $\\sum_{i}^{M} x_i = 0$, to ensure translation equivariance."}, {"title": "2.1.2 TRAINING STRATEGIES", "content": "In our framework, the growth phase occupies a small portion of the overall training process, with\nthe best-performing configuration being tn/T = 0.01. If we follow the standard diffusion training\nprocedure and sample time steps uniformly, the number of iterations for the predictive task only\ntakes 1% of the total training process, which will significantly degrade the model's performance\non this task. Therefore, to ensure sufficient training for the predictive task, we oversample the time\nsteps in the growth phase. Thus, the final loss can be written as the following equation:\n$$L = E_{t \\sim \\frac{1}{2}U(1,t_n] + \\frac{1}{2}U(t_n,T]} (L^{(x)} + L^{(h)} + L^{(c)})$$"}, {"title": "2.1.3 INFERENCE OF UNIGEM", "content": "In UniGEM, molecular generation is achieved by reversing the forward diffusion process through\na Markov chain to reconstruct atomic coordinates, followed by predicting atom types based"}, {"title": "2.2 THEORETICAL ANALYSIS", "content": "In this section, we present our theoretical results addressing two questions: first, why diffusion is\ncapable of learning representations while the direct multi-task approach fails; and second, why the\nproposed UniGEM effectively enhances performance in the challenging generation task."}, {"title": "2.2.1 INCONSISTENCY BETWEEN MOLECULE GENERATION AND PROPERTY PREDICTION", "content": "In UniGEM, we incorporate property prediction during the diffusion process, specifically after the\nnucleation time. We analyze the reason why training with denoising diffusion loss can effectively\nlearn molecular representations, as well as why property prediction is significantly more beneficial\nafter nucleation time.\nTo facilitate this study, we analyze the problem from an Information Maximization (Info-\nMax) (Linsker, 1988; Oord et al., 2018) perspective, which has been shown to closely relate to\nthe quality of representation learning. According to the InfoMax theory, effective latent representa-\ntions in the diffusion-based molecule generation approach are achieved by maximizing the mutual\ninformation (MI) between the original molecular coordinates 20 and $, which are derived from the\nnoisy coordinates xt in the intermediate layers of the denoising network. We theoretically demon-\nstrate that good representations can be learned during denoising diffusion training, but are only\nachieved at smaller time steps, indicating that the latent representations learned from later time steps\nare insufficient to support property prediction. These findings not only provide a foundation for\nusing diffusion to learn molecular representations but also expose the inherent inconsistency be-\ntween molecule generation and property prediction tasks. This explains the failure of the multi-task\napproach and emphasizes the need for a two-phase modeling process.\nTheorem 2.1. The mutual information between xo and \u03b6 can be expressed as follows, with a sub-\nsequent lower bound:\n$$I(\\zeta, x_0) = I(x_0; X_t) \u2013 E_{q(X_t,\\zeta)} [D_{KL}(q(x_0|X_t)||q(x_0|\\zeta))]$$\n$$> I(x_0; x_t) - E_{q(X_t,\\zeta)} [D_{KL}(q(x_0|X_t)||p(x_0|\\zeta))],$$\nwhere q(x0, xt) are data distribution defined by the forward process of diffusion, $q(\\zeta|x_t) = \\delta_{g_\\theta(x_t)}$\nand $p(x_0|\\zeta)$ represent the estimated representation and denoising distributions by the denoising net-\nwork. In practice, our denoising network models $g_\\theta(x_t)$ and the mean of the denoising distribution\n$$E_{p(x_0|g_\\theta(x_t))} := \\frac{x_t - \\sigma_t \\phi_\\theta^{(x)}(x,t)}{\\alpha_t}$$\nWe analyze the lower bound on the mutual information. As we show in appendix D.3, when\n$p(x_0|\\zeta)$ and $q(x_0|x_t)$ follow Gaussian distributions with the same variance $\u03c3$, minimizing the\nKL divergence is equivalent to minimizing the denoising diffusion loss. Thus, the second term\n$E_{q(X_t,\\zeta)} [D_{KL}(q(x_0|x_t)||p(x_0|\\zeta))]$ can be minimized during the diffusion training process. This\nsuggests that effective representations can be implicitly learned throughout this diffusion process."}, {"title": "2.2.2 THEORETICAL ANALYSIS ON GENERATION ERROR", "content": "To further investigate why UniGEM improves performance on the generation task, we conduct a\ntheoretical analysis of the generation error. Specifically, we derive the upper bound of the generative\nerror for two molecular generation approaches: UniGEM and traditional diffusion-based models,\nbased on results from Chen et al. (2023b). While comparing these upper bounds may not completely\nreflect real-world behavior, it provides a close approximation that yields valuable insights, enhancing\nour understanding of the relative performance of these methods.\nTheorem 2.2 (Generative Error Analysis). With mild assumptions on the data distribution q pro-\nvided in appendix E.1, the molecular generative error measured by total variation between the\nUniGEM generated data distribution po(zo) and ground-truth data distribution q(zo) is bounded\nby the following terms.\n$$TV(p_\\theta(z_0), q(z_0)) \\le \\sqrt{KL(q(z_T)||p_\\theta(z_T))e^{-T}} + (L_z\\sqrt{d_z}l + L_zm_1) \\sqrt{T}$$\n$$+ \\sqrt{\\sum_{t=1}^{T} L_t^{(x)}} + \\sqrt{\\sum_{t=1}^{T} L_t^{(h)}},$$\nwhere $m_1 = E_q(z_0) ||\\cdot ||^2$ is the second moment of q(\u00b7). Traditional diffusion models generate both\natomic coordinates and atom types simultaneously, as detailed in Appendix C. The total variation\ndistance between the molecular distribution generated by the traditional diffusion model, denoted\nas ro(zo), and the ground-truth data distribution q(zo) can be bounded by the following terms.\n$$TV(r_\\theta(z_0), q(z_0)) \\le \\sqrt{KL(q(z_T)||r_\\theta(z_T))e^{-T}} + (L_z\\sqrt{d_z}l + L_zm_1) \\sqrt{T}$$\n$$+ \\sqrt{\\sum_{t=1}^{T} L_t^{(x,h)}} + \\sqrt{\\sum_{t=1}^{T} L_t^{(h)}},$$\nOur theoretical analysis identifies four factors contributing to generation errors in both UniGEM and\ntraditional diffusion-based models: prior distribution error, discretization error, coordinate score\nestimation error, and atom type score estimation (or prediction) error. The prior distribution and\ndiscretization errors both decrease with lower data dimensionality. Since UniGEM only generates\natom coordinates, unlike traditional models that jointly generate both coordinates and atom types,\nUniGEM deals with significantly smaller data dimensionality, leading to reduced errors. As for the\nlatter two errors, it's harder to compare through theory alone, so we supplement with experimental\nresults showing that UniGEM outperforms traditional models in these aspects:\nA critical issue with traditional diffusion-based models is their treatment of discrete atom types as\ncontinuous variables, which can lead to instability during generation. For example, in our experi-\nments, we observe that even at the later stage of generation, the predicted atom type (determined by\nthe highest probability) oscillates between two categories, indicating a suboptimal learned distribu-\ntion. Since the coordinates in the next step depend on both the current coordinate and atom type,\nthis oscillation introduces additional errors in coordinate estimation. In contrast, UniGEM avoids"}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 MAIN EXPERIMENTS", "content": "Datasets In our experiments, we utilize two widely used datasets. The QM9 dataset (Ruddigkeit\net al., 2012; Ramakrishnan et al., 2014) serves both molecular generation and property prediction\ntasks, comprising approximately 134,000 small organic molecules with up to nine heavy atoms,\neach annotated with quantum chemical properties such as LUMO (Lowest Unoccupied Molecular\nOrbital), HOMO (Highest Occupied Molecular Orbital), HOMO-LUMO gap, polarizability (a).\nThese properties are used as ground-truth labels for property prediction. In contrast, the GEOM-\nDrugs dataset (Axelrod & Gomez-Bombarelli, 2022) focuses on drug-like molecules, providing a\nlarge-scale collection of molecular conformers. It includes around 430,000 molecules, with sizes\nranging up to 181 atoms and an average of 44.4 atoms per molecule.\nThe splitting strategies for both benchmarks follow the previous practices. For the QM9 dataset, we\nadopt the same split as in prior methods (Hoogeboom et al., 2022; Satorras et al., 2021; Anderson\net al., 2019), dividing the data into training (100K), validation (18K), and test (13K) sets. Simi-\nlarly, for the GEOM-Drugs dataset, we follow the approach outlined in Hoogeboom et al. (2022) to\nrandomly split the dataset into training, validation, and test sets in an 8:1:1 ratio.\nImplementation Details We adopt EGNN (Satorras et al., 2021) as our backbone and modify it into\na multi-branch network. As illustrated in Figure 3, different branches handle the diffusion loss at\ndifferent ranges of time steps, with shared layers preceding the branches. In the branch for t \u2264tn,\nwe add predictive losses, including molecular property prediction and atom type prediction. During\ntraining, we apply uneven sampling for each batch. For the first half of the batch, the sampling\nrange for t is [tn, T], and for the second half, the sampling range for t is [0, tn], ensuring that each\nbranch receives the same samples and is sufficiently trained. All losses are optimized in parallel,\nwith each loss weighted equally at 1. We implement UniGEM with 9 layers, each consisting of 256\ndimensions for the hidden layers. The shared layer count is set to 1, while each separate branch,\ncorresponding to different ranges of time steps, has 8 layers. Optimization is performed using the\nAdam optimizer, with a batch size of 64 and a learning rate of 10-4. We train UniGEM on QM9\nfor 2,000 epochs, while training on the GEOM-Drugs dataset is limited to 13 epochs due to the\nlarger scale of its training data. We set the nucleation time tn to 10 and the total time steps T to\n1000, ensuring that the prediction of atom types and property occurs within the first 10 time steps.\nThe atom type and molecular property predicted in the last time step are used as the final predictive\nresult."}, {"title": "3.1.1 EXPERIMENTAL RESULTS FOR MOLECULE GENERATION", "content": "Baselines UniGEM is implemented based on the codebase of a classic 3D molecular diffusion al-\ngorithm, EDM (Hoogeboom et al., 2022). Thus, our baseline comprises EDM and its variants,\nGDM (Hoogeboom et al., 2022), EDM-Bridge (Wu et al., 2022) and GeoLDM (Xu et al., 2023).\nGDM utilizes non-equivariant networks for training, EDM-Bridge enhances EDM through the tech-\nnique of diffusion bridges. GeoLDM introduces an additional autoencoder to encode molecular\nstructures into latent embeddings, where the diffusion process is conducted in the latent space. Ad-\nditionally, we include G-Schnet (Gebauer et al., 2019), an autoregressive generation method, and\nEquivariant Normalizing Flows (E-NF) (Garcia Satorras et al., 2021) in our comparisons.\nMetrics Following the approach of these baselines, we sample 10,000 molecules and evaluate atom\nstability, molecule stability, validity, and uniqueness of valid samples. Specifically, we utilize the\ndistances between each pair of atoms to predict the bond type-whether it is single, double, triple,\nor non-existent. Atom stability is calculated as the ratio of atoms exhibiting correct valency, while\nmolecule stability reflects the fraction of generated molecules in which each atom maintains stability.\nValidity and uniqueness are assessed using RDKit by converting the 3D molecular structures into\nSMILES format, with uniqueness determined by calculating the ratio of unique generated molecules\namong all valid samples after removing duplicates.\nResults The results in Table 2 show that UniGEM consistently outperforms all baselines across\nnearly all evaluation metrics for both QM9 and GEOM-Drugs. Notably, compared to EDM variants,\nUniGEM is significantly simpler, as it neither relies on prior knowledge nor requires additional\ntraining for an autoencoder, yet it achieves superior performance compared to EDM-Bridge and\nGeoLDM, highlighting UniGEM's superiority."}, {"title": "3.1.2 EXPERIMENTAL RESULTS FOR MOLECULAR PROPERTY PREDICTION", "content": "Baselines The property prediction module of UniGEM is built on the widely used equivariant neural\nnetwork EGNN (Satorras et al., 2021). We compare UniGEM against the EGNN model trained from\nscratch, referred to as EGNN without confusion, as well as several recent pre-training methods, that\nhave demonstrated superiority in learning molecular representations, thereby improving the property\nprediction performance. These methods include GraphMVP (Liu et al., 2021), 3D Infomax (St\u00e4rk\net al., 2022), GEM (Fang et al., 2022), and 3D-EMGP (Jiao et al., 2023). It is important to note that\nall baseline methods utilize the same backbone and data splits to ensure a fair comparison."}, {"title": "3.1.3 \u0421\u043eMPARISONS WITH GENERAL UNIFIED APPROACHES", "content": "In this section, we compare UniGEM with two general approaches for unifying generation and\nproperty prediction tasks. Table 4 outlines these approaches: the first treats generation and prop-\nerty prediction as a multi-task joint training network, referred to as \u2018Multi-task\u2019. Additionally, we\nincorporate the property prediction task into a pre-trained generation network for continued train-\ning, specifically freezing the backbone to preserve the network's generation capability, referred to\nas 'Gen Pre-train'."}, {"title": "3.2 ABLATION STUDIES", "content": "We conduct three ablation studies to assess the impact of predictive loss, time sampling strategy,\nand nucleation time setting using the QM9 dataset. These studies include validation on both gener-\nation and property prediction tasks, comparing our results to the generation baseline EDM and the\nproperty prediction baseline EGNN trained from scratch."}, {"title": "3.2.1 IMPACT OF PREDICTIVE LOSS ON PERFORMANCE", "content": "In UniGEM, we incorporate two types of predictive loss: atom type prediction loss and molecular\nproperty prediction loss. To validate their impact, we compare our model against two baselines:\none utilizing only atom type prediction loss (referred to as ATP) and the other using only molecular\nproperty prediction loss (referred to as MPP). The comparison results are presented in Table 5, with\nLUMO selected as the target property for performance evaluation.\nAs shown in Table 5, both ATP and MPP enhance generation performance compared to previous\nbaseline models, with ATP demonstrating a greater improvement than MPP. The combination of\nthese two losses functions yields the best results, as evidenced by UniGEM. These findings under-\nscore the importance of both predictive losses, particularly highlighting the critical role of atom type\nprediction.\nIn molecular property prediction, MPP enhances the original performance, and incorporating the\natom type prediction loss further improves results. This improvement may stem from atom type pre-\ndiction facilitating the learning of better molecular representation, which ultimately benefits property\nprediction."}, {"title": "3.2.2 ANALYSIS OF TRAINING STRATEGIES IN UNIGEM", "content": "To ensure sufficient training steps for property prediction, we oversample the time steps after the\nnucleation time. Additionally, we design a multi-branch network to mitigate the negative impact of\noversampling on generation performance. We conduct ablation studies to demonstrate the necessity\nof oversampling and branch splitting strategies in ensuring superior performance for both generation\nand prediction tasks."}, {"title": "3.2.3 EFFECT OF NUCLEATION TIME", "content": "Nucleation time is defined as the moment when the molecular scaffold forms, after which the molec-\nular coordinates experience only minor adjustments. In practical applications, accurately determin-\ning the true nucleation time is challenging. Therefore, we conduct an empirical analysis to assess its\nimpact on model performance. We set the total training step to T=1000, and compare models with\nnucleation times of 1, 10, and 100.\nResults are outlined in Figure 2. Across all generation\nand property prediction criteria, a nucleation time of\n10 achieves optimal performance. Setting the\nnucleation time too large may incorporate time steps\nprior to the complete formation of the molecular\nscaffold, leading to suboptimal outcomes. Conversely,\nusing a nucleation time that is too small also degrades\nperformance, potentially due to insufficient input\nnoise, which is crucial for learning a more robust type\nand property mapping. From the perspective of force\nlearning interpretation of the denoising task, as\ndiscussed in Appendix D.4, a moderate level of noise\nis advantageous for capturing molecular\nrepresentations, thus enhancing the predictive tasks."}, {"title": "4 CONCLUSION", "content": "This paper introduces UniGEM, the first effective unified model that significantly improves the per-\nformance of both molecular generation and property prediction tasks. The underlying philosophy is\nthat these traditionally separate tasks are highly correlated due to their reliance on effective molec-\nular representations. The traditional inconsistencies of these tasks are overcome by a two-phase\ngenerative process with atom type and property prediction losses activated in the growth phase.\nUniGEM's enhanced performance is supported by solid theoretical analysis and comprehensive ex-\nperimental studies. We believe that the innovative two-phase generation process and its correspond-\ning models offer a new paradigm that may inspire the development of more advanced molecule\ngeneration frameworks and benefit more specific applications of molecular generation."}, {"title": "C INTRODUCTION OF JOINT DIFFUSION FOR MOLECULES", "content": "A 3D molecule is represented by coordinates and atom types. Formally, we denote a molecule with\nMatoms as z = (x, h), where x = (x1,\u2026,XM) \u2208 R3M represents the atomic positions, and\nh = (h1,\u2026,h\u043c) \u2208 {\u0435\u043e,\u2026\u2026, \u0435\u043d}M encodes the atom type information. Each h\u2081 is a one-hot\nvector of dimension H, corresponding to the type of the atom i, where H is the total number of\ndistinct atom types in the dataset.\nRecently, several works have applied diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020)\nto 3D molecular data, employing joint diffusion to simultaneously generate molecular coordinates\nand atom types (Hoogeboom et al., 2022; Guan et al., 2023; Gao et al., 2024). The diffusion model\nindependently injects noise to the coordinates and atom types respectively via a forward process:\n$$q(z_t|z_0) = N_x(x_t|\\alpha_t x_0, \\sigma_t^2 I)\u00b7N(h_t|\\alpha_t h_0, \\sigma_t^2 I), t = 1,\u2026,T$$\nwhere zo is a molecule in the dataset, at and ot represent the noise schedule and satisfy a\u00b2 + \u03c3? = 1\nmonotonically decrease from 1 to 0. N represents the Gaussian distribution in the zero center-of-\nmass (CoM) subspace satisfying $\\sum_{i}^{M} x_i = 0$. \u00b7 refers to joint distribution.\nTo generate samples, the forward process is reversed using a Markov chain re (20:T) =\nro(ZT) 110(Zt-1|zt) with a noise term approximated by a neural network (h) (zt, t):\n$$ro(z_{t-1}|z_t) = N_x (x_{t-1}| \\mu_\\theta^{(x)}(z,t)), \\tilde{\\sigma}_{t}^2 I) N (h_{t-1}|\\mu_\\theta^{(h)}(z,t)), \\tilde{\\sigma}_{t}^2 I)$$\nwhere $(\\mu^{(i)}_t, \\phi^{(i)}_\\theta)(z,t)) = \\frac{\\alpha_t (x_t - \\phi^{(i)}(z,t))}{\\sigma_t},  for i \u2208 {x, h}, \\tilde{\\sigma}_{t-1} = \\sigma_t - \\frac{\\sigma_t \\alpha^2_t}{\\alpha_{t-1}},$ \n$\\tilde{\\alpha}_{t\\t-1} = \\alpha_t/\\alpha_{t-1} and \\tilde{\\sigma}^2_{t\\t-1} = \\sigma^2_t - \\alpha^2_t$$\nThe prior distribution is approximated by p(zn) = Nx(xv|0, I)\u00b7N(hn|0, I). The noise predic-\ntion network is trained using a mean squared error (MSE) loss min\u0117 Et\u223cU(0,T]Lt:\n$$L_t = E_{q(z_0,z_t)} ||p_\\theta(z_t, t) \u2013 \\epsilon_t||^2 \\sim E_{q(z_0,z_t)} ||p_\\theta(z_t, t) \u2013 (-\\sigma_t\\nabla_{z_t} log q(z_t))||^2,$$\nwhere et = (zt \u2013 atzo)/0t is the standard Gaussian noise injected to zo and \u2207z\u2081 log q(zt)) refers\nto the score function. The equivalent loss is derived using the equivalence between score matching\nand conditional score matching (Vincent, 2011), with a formal proof provided in Ni et al. (2024);\nFeng et al. (2023a). Thus Lt is termed denoising loss or equivalently score estimation loss. Loss\nin equation 12 can be further decomposed as atom type and coordinate noise prediction: Lt =\n$E_{q(z_0,z_t)} ||\\phi^{(x)}(z_t, t) - \\epsilon_t^{(x)} ||^2 + || \\phi^{(h)}(z_t, t) - \\epsilon_t^{(h)} ||^2 := L^{(x/h)} + L^{(h/x)}$"}, {"title": "D REPRESENTATION LEARNING ANALYSIS FOR MOLECULAR COORDINATE\nGENERATION", "content": "We analyze from the perspective of information maximization (InfoMax) (Linsker, 1988; Oord et al.,\n2018) the reasons why training with denoising diffusion loss can effectively learn molecular repre-\nsentations that enhance property prediction, as well as why property prediction is more effective\nafter ready time."}, {"title": "D.1 INFOMAX TARGET", "content": "In accordance with the principles of information maximization (InfoMax), our objective is to select\na representation ( that maximizes the mutual information (MI) between the input data and its repre-\nsentation. In the context of UniGEM, this objective translates to maximizing the mutual information\nbetween the original molecular coordinates x and the learned latent representations (. Here, ( is\nderived from the intermediate layers of the denoising network, with the input consisting of the noisy\ncoordinates xt at time step t. This latent representation ( is subsequently utilized in the denoising\ntask."}, {"title": "D.2 VARIATIONAL UPPER BOUND ON THE KL DIVERGENCE TERM", "content": "Since $E_{q(x_t,\\zeta)} [D_{KL}(q(x_0|\\zeta)||p(x_0|\\zeta))] \\ge 0$, the following inequality holds:\n$$\\int q(x_0|\\zeta) log q(x_0|\\zeta) dx_0 \\ge \\int q(x_0|\\zeta) log p(x_0|\\zeta) dx_0$$\nThus, we obtain the following expression:\n$$E_{q(x_t,\\zeta)} log \\frac{q(x_0xt)}{q(x_0\\zeta)} \\le E_{x_0,x_t,\\zeta} log \\frac{q(x_0xt)}{p(x_0\\zeta)} = E_{q(x_t,\\zeta)} [D_{KL}(q(x_0|X_t)||p(x_0|\\zeta))]$$\nConsequently, by minimizing this variational upper bound in the right hand side of equation 15, we\ncan effectively reduce the KL divergence term in equation 13."}, {"title": "D.3 RELATION TO DENOISING LOSS", "content": "In this section, we discuss the relationship between $E_{q(x_t,\\zeta)} [D_{KL}(q(x_0|X_t)||p(x_0|\\zeta))]$ and the de-\nnoising loss. Given that $q(x_t|x_0) = N_x(\\alpha_t x_0, \\sigma_t^2 I)$, we can apply Tweedie's formula (Efron, 2011;"}, {"title": "D.4 THE BENEFITS OF DIFFUSION FOR PROPERTY PREDICTION: A FORCE LEARNING\nPERSPECTIVE", "content": "The denoising task, particularly at specific time steps during diffusion training, has been shown to\neffectively learn meaningful molecular representations, as demonstrated in prior work (Zaidi et al.,\n2022; Feng et al., 2023a; Ni et al., 2024; Arts et al., 2023). This process can be interpreted as"}, {"title": "E GENERATIVE ERROR ANALYSIS OF UNIGEM AND JOINT DIFFUSION\nMODEL", "content": ""}, {"title": "E.1 GENERAL DIFFUSION ERROR BOUND", "content": "We follow (Chen et al., 2023b) to derive the error bound for diffusion model and make the following\nmild assumptions on the data distribution q(yo). To distinguish it from the notation of molecules,\nwe use y to represent general data generated by the diffusion model.\nAssumption 1 (Lipschitz score). For all t > 0, the score \u2207 ln q(yt) is L-Lipschitz.\nAssumption 2 (second moment bound). For some \u03b7 > 0, Eq(yo)[||Yo||2+n] is finite. Denote\nm\u00b2 = Eq(yo) [||Yo||2] for the second moment of q(yo).\nT refers to the timestep of the simple Ornstein-Uhlenbeck (OU) process as defined by Chen, de-\nscribed by the equation\n$$dY_t = \u2212Y_t dt + \\sqrt{2}dB_t, t\u2208 [0,T],$$\nwhile our forward process is given by\n$$dY_t = -g(t)^2Y_t dt + \\sqrt{2g(t)} dB_t, t \u2208 [0,T],$$\nwhere g(t) is determined by our noise schedule at. The timesteps of the two processes differ by a\ntime reparameterization. In EDM, T also refers to the discretization steps of the diffusion process.\nTheorem E.1 (Chen et al. (2023b), modified). Suppose that Assumptions 1 and 2 hold. Suppose\nthat the step size l := T/T satisfies l \u2264 1/L, where L > 1. Then, it holds that\n$$TV (p_\\theta(y_0), q(y_0)) \\le \\sqrt{KL(q(y_T)||p_\\theta(y_T)) exp(-\\frac{T}{l})} + (L_y\\sqrt{d_y}l + L_ym_1) \\sqrt{T}+ \\sqrt{\\sum_{t=1}^{T} L_t},$$"}, {"title": "E.2 ERROR BOUND FOR UNIGEM AND JOINT DIFFUSION", "content": "Based on the results above, we can derive the molecular generation error bound for UniGEM"}]}