{"title": "CGI: Identifying Conditional Generative Models with Example Images", "authors": ["Zhi Zhou", "Hao-Zhe Tan", "Peng-Xiao Song", "Lan-Zhe Guo"], "abstract": "Generative models have achieved remarkable performance recently, and thus model hubs have emerged. Existing model hubs typically assume basic text matching is sufficient to search for models. However, in reality, due to different abstractions and the large number of models in model hubs, it is not easy for users to review model descriptions and example images, choosing which model best meets their needs. Therefore, it is necessary to describe model functionality wisely so that future users can efficiently search for the most suitable model for their needs. Efforts to address this issue remain limited. In this paper, we propose Conditional Generative Model Identification (CGI), which aims to provide an effective way to identify the most suitable model using user-provided example images rather than requiring users to manually review a large number of models with example images. To address this problem, we propose the Prompt-Based Model Identification (PMI), which can adequately describe model functionality and precisely match requirements with specifications. To evaluate PMI approach and promote related research, we provide a benchmark comprising 65 models and 9100 identification tasks. Extensive experimental and human evaluation results demonstrate that PMI is effective. For instance, 92% of models are correctly identified with significantly better FID scores when four example images are provided.", "sections": [{"title": "1 Introduction", "content": "Deep generative models [Jebara, 2012], such as variational autoencoders (VAE) [Kingma and Welling, 2014; Kingma and Welling, 2019; Parmar et al., 2021], generative adversarial networks (GAN) [Goodfellow et al., 2014; Sohn et al., 2015; Creswell et al., 2018], flow-based models [Rezende and Mohamed, 2015], and diffusion models [Sohl-Dickstein et al., 2015; Dhariwal and Nichol, 2021; Rombach et al., 2022], have shown significant success in image generation. Generative model hubs like Hugging Face and CivitAI have been established to facilitate the sharing and downloading of models by developers and users, respectively. However, with the rapid growth of available models, finding the most suitable model for specific tasks has become a critical challenge.\nExisting generative model hubs provide basic methods such as model tag filtering, text matching, and download volume ranking [Shen et al., 2023] to help users search for models. However, the complex functionalities of generative models cannot be adequately described using only textual and statistical information [Lu et al., 2023; Luo et al., 2024]. Although model hubs like CivitAI and OpenArt offer example images for each model to assist users in finding their desired models, this solution has two main limitations. First, example images cannot fully represent model functionality, especially when user purposes and offered examples mismatch. Second, users still need to manually review example images repeatedly, which is time-consuming and heavily relies on their expertise. Therefore, model selection remains challenging for users, as they must iteratively review, download, and test multiple models before finding a suitable one.\nConditional Generative Model Identification (CGI). The above limitation inspires us to consider the following question: Can we describe the functionality of conditional generative models in a precise format that enables efficient and accurate model identification by matching their functionalities with user requirements? Following the learnware paradigm [Zhou and Tan, 2022], we call the functionality description the model's specification. An important characteristic of this problem is the requirement to assign a specification to the model upon uploading it to the model hub which allows future users to simply compute the similarity between the specification and their requirements to search for models. This is fundamentally different from the scenario where a query and a large set of candidate models are provided, and the objective is to learn the ranking. We call this novel setting Conditional Generative Model Identification (CGI). To the best of our knowledge, this problem has not been studied yet. presents an illustration of the CGI problem and different from the traditional model selection process.\nChallenges of CGI. The challenges of CGI come from two main sources: (1) How to assign specifications to adequately describe the functionalities of different conditional generative models as well as the user requirements, and (2) How to match the users' requirements with the models' specifica-"}, {"title": "2 Related Work", "content": "This study is related to the following two aspects:\nLearnware and Model Selection. With the development of model hubs, users have the option to search for and adapt pre-trained models that satisfy their specific needs. Learn-ware [Zhou, 2016] offers a paradigm to identify models for the users. Recently, Wu [Wu et al., 2023] proposed to de-scribe the model's functionality by approximating the training data distribution and searching for models by comparing the approximated distribution distance. Guo [Guo et al., 2023] proposed to describe the model's functionality by approxi-mating the model's parameters with a linear proxy model and enabling the model search by comparing the proxy model's parameters. However, these methods are not applicable to the generative model search. A few works are related to the generative model search. For example, [Shen et al., 2023] proposed to describe the model's functionality via natural language (e.g., model tags, model architectures, resources requirement) and adopted ChatGPT as a model selector to search useful models that meet user's requirements from the HuggingFace platform. Stylus [Luo et al., 2024] describes the model functionality with vision-language models, and identify conditional generative models with large language models. However, only natural language can not describe the model's functionality adequately, more precise descrip-tion needs to be studied. [Lu et al., 2023] proposed a content-based search method that can be applied to unconditional gen-erative models. Therefore, existing works are not applicable to the CGI problem, and this paper presents the first attempt to solve this problem.\nGenerative Models. In recent years, generative models have become one of the most widely discussed topics in the field of artificial intelligence for their promising results in image generation, exemplified by models such as Genera-tive Adversarial Networks [Goodfellow et al., 2014; Arjovsky et al., 2017; Brock et al., 2019; Choi et al., 2020], Vari-ational Autoencoders [Kingma and Welling, 2014; van den Oord et al., 2017; Vahdat and Kautz, 2020], Diffusion Mod-els [Nichol and Dhariwal, 2021; Dhariwal and Nichol, 2021; Rombach et al., 2022], etc. With the development of the gen-erative model, various generative model hubs, e.g., Hugging-Face and Civitai, have been developed to enable model de-velopers to share models. These numerous generative models show different specialties and functionality. Our goal is not"}, {"title": "3 Preliminary", "content": "In this section, we first introduce the problem setup of CGI problem. Then, we present the problem analysis to show the core challenge of CGI problem."}, {"title": "3.1 Problem Setup", "content": "M\nAssume the model hub has $M$ conditional generative mod-els $\\{f_m\\}_{m=1}^M$. Each model is associated with a corresponding specification $S_m$ to describe its functionalities for future model identification. There are two stages in the CGI setting: the submitting stage for model developers and the identification stage for future users.\nSubmitting Stage. The model developer submits a model $f_m$ to the model hub, and then we assign a specification $S_m$ to the model. Formally, the specification $S_m$ is generated by a specification assignment algorithm $A_s$ using the model $f_m$, i.e., $S_m = A_s(f_m)$. It is important to note that uploaded models are anonymous with no mandatory constraints, which means we cannot access their training data and developers are not guaranteed to provide required model information.\nIdentification Stage. For any user task $\\tau$, models are identified from the model hub using one or a few example images $X_{\\tau} = \\{x_{\\tau}\\}_{i=1}^{N_{\\tau}}$. When users upload example images to describe their needs, the model hub generates the requirement represented in the specification space $R_{\\tau} = A_r(X_{\\tau})$ using a requirement generation algorithm $A_r$. Then, we match the requirement $R_{\\tau}$ with model specifications $\\{S_m\\}_{m=1}^M$ using an evaluation algorithm $A_e$ and compute the matching score $s_m = A_e(S_m, R_{\\tau})$ for each model $f_m$. Finally, return the best-matched model with the maximum score or a list of models sorted by $\\{s_m\\}_{m=1}^M$ in descending order.\nM\nThe two main problems for addressing CGI are: (a) How to design $A_s$ and $A_r$ to fully characterize the functionality of submitted conditional generative models and user requirements? (b) How to design $A_e$ to effectively identify the most suitable model for users' specific needs using the specifications and requirements?"}, {"title": "3.2 Problem Analysis", "content": "Learnware [Zhou, 2016] provides an effective framework for describing the functionality of discriminative models. It de-scribes model functionality by approximating the training data distribution and performs model search by matching the approximated training and testing data distributions.\nSpecifically, the learnware methods [Wu et al., 2023] use Kernel Mean Embedding (KME) techniques to represent training data distributions by mapping a probability distribution $P$ defined on $\\mathcal{X}$ into a reproducing kernel Hilbert space (RKHS) as\n$\\mu_k(P) := \\int_{\\mathcal{X}} k(x,\\cdot)d\\mathbb{P}(x) \\qquad(1)$\nwhere $k : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is a kernel function with associated RKHS $\\mathcal{H}$. In cases of finite training data, the empirical KME"}, {"title": "4 Our Approach", "content": "In this section, we present our solution Prompt-Based Model Indetification (PMI) for the CGI setting. As illustrated in Figure 2, PMI consists of three key modules. Automatic Specifi-cation Assignment and Requirement Generation project the model's functionality and user requirements into a unified model matching space, addressing the first challenge. Task-Specific Matching adjusts the specification in the matching space according to the requirement and identifies the most suitable model with the highest similarity score, addressing the second challenge. We first describe the three key modules in detail. Then, a further analysis is provided as follows."}, {"title": "4.1 Automatic Specification Assignment", "content": "Automatic specification assignment aims to automatically generate a specification for each conditional generative model to describe its functionality. Its core idea is to prompt the con-ditional generative model $f_i$ to generate data for describing the model functionality based on a pre-defined prompt set $\\mathcal{P}$ or developer-provided prompt set $\\mathcal{P}_i$.\nWhen model developer submit a model $f_i$ to the model hub, the automatic specification assignment algorithm $A_s$ generates its corresponding specification $S_i$ using a $N$-size prompt set $\\mathcal{P}$ which is pre-defined in the model hub. A set of images $\\mathcal{X}_i$ is generated by the model $f_i$ and the prompt set $\\mathcal{P}$ to describe model functionality conditioned on $\\mathcal{P}$:\n$\\mathcal{X}_i = \\{x_j\\}_{j=1}^N = f_i(p_j) | p_j \\in \\mathcal{P}\\qquad(3)$\nThen, the prompt set $\\mathcal{P}$ and the generated images $\\mathcal{X}_i$ are en-coded to a unified model matching space using textual en-coder $T(\\cdot)$ and vision encoder $G(\\cdot)$ from a pre-trained vision-language model respectively:\n$\\begin{aligned} \\mathcal{Z}_i &= \\{z_j\\}_{j=1}^N = G(x) | x \\in \\mathcal{X}_i\\},\\\\ \\mathcal{Q}i &= \\{q_j\\}_{j=1}^N = T(p_j) | p_j \\in \\mathcal{P}\\,. \\end{aligned} \\qquad(4)$"}, {"title": "4.2 Requirement Generation", "content": "Requirement generation aims to produce the requirements $R_{\\tau}$ for the user task $\\tau$ to select the most suitable model. Its core idea is to decompose the complex model functionality into the difference between user-provided example images $X_{\\tau} = \\{x_i^{\\tau}\\}_{i=1}^{N_{\\tau}}$ and corresponding textual descriptions, enabling the future model identification to be more accurate.\nSpecifically, requirement generation algorithm $A_{\\tau}$ transforms $X_{\\tau}$ into feature representations:\n$Z_{\\tau} = \\{z_i = G(x_i^{\\tau})\\}_{i=1}^{N_{\\tau}}\\qquad(6)$\nusing the vision encoder $G(\\cdot)$. Then, the textual description $P_{\\tau}$ is generated by a vision-language model $\\text{VLM}(\\cdot)$ to describe each example image and be mapped to the model"}, {"title": "4.3 Task-Specific Matching", "content": "Task-specific matching aims to identify the most suitable model for the user task $\\tau$ by calculating the similarity score between the user requirement $R_{\\tau}$ and the specification $S_m$ of each model $f_m$. Its core idea is to transform the specification $S_m$ corresponding to the user requirement $R_{\\tau}$ in the model matching space to better match the functionality.\nSpecifically, matching algorithm $A_e$ calculates the similar-ity score between the user requirement $R_{\\tau} = \\{Z_{\\tau},Q_{\\tau}\\}$ and the specification $S_m = \\{Z_m,Q_m\\}$ for each model $f_m$ using the following formula:\n$A_e(S_m, R_{\\tau}) = \\frac{1}{N_{\\tau} N_m}\\sum_{j=1}^{N_m}\\sum_{i=1}^{N_{\\tau}} \\frac{k(z_{m_j},z_i^{\\tau})}{\\|z_{m_j}\\|_\\mathcal{H}\\|z_i^{\\tau}\\|_\\mathcal{H}}-k(q_{m_j},q_i^{\\tau})\\qquad(9)$\nwhere $\\langle \\cdot, \\cdot \\rangle_{\\mathcal{H}}$ measures the similarity between the $j$-th specification prompts for model $f_m$ and the textual descrip-tions of example image $x_i^{\\tau}$, transforming organial specifica-tions to task-specific specifications, which are more focused"}, {"title": "4.4 Discussion", "content": "It is evident that our proposal for the CGI scenario achieves a higher level of accuracy and efficiency when compared to model search techniques employed by existing model hubs.\nAccuracy. Our proposal elucidates the functionalities of generated models by capturing both the distribution of gen-erated images and prompts. This approach allows for more accurate identification of suitable models for users, as op-posed to the traditional model search method that relies on download counts and star ratings for ranking models.\nEfficiency. Suppose that the model hub generates one re-quirement in $T_r$ time and calculates the similarity score for each model in $T_s$ time. The time complexity of our proposal for one identification is $O(T_r + MT_s)$ time. Moreover, with accurate identification results, users can save the efforts of browsing and selecting models, as well as reducing the con-sumption of network and computing. This is linearly cor-related to the number of models on the model hub (which can be reduced by filtering by tags). Additionally, our ap-proach also has the potential to achieve further acceleration through the use of a vector database [Guo et al., 2023] such as Faiss [Johnson et al., 2019]."}, {"title": "5 Experiments", "content": "To verify the effectiveness of our proposed method PMI for CGI problem, we first build a novel conditional generative model identification benchmark based on stable diffusion models [Rombach et al., 2022], and then conduct experiments on this benchmark. Below, we first introduce the details of the benchmark and evaluation metrics. Then, we present the ex-perimental results on this benchmark and human evaluation results to emphasize the importance of CGI problem as well as the effectiveness of our PMI."}, {"title": "5.1 CGI Benchmark", "content": "In this section, we describe the our constructed CGI bench-mark and corresponding evaluation metrics.\nModel Hub and Task Construction. In practice, we ex-pect model developers to submit their models to the model hub, allowing users to identify models that meet their spe-cific needs. To enhance the realism of the evaluation, we constructed a model hub and user identification tasks to sim-ulate this scenario. For the model hub construction, we man-ually collected $M = 65$ different stable diffusion models $\\{f_1, ..., f_m,..., f_M\\}$ from CivitAI, representing uploaded conditional generative models on the hub. These models be-long to the same category to mimic the real process where users first apply category filters before selecting models. For model specification generation, we created 61 prompts $\\{p_1,...,p_{61}\\}$ as a pre-defined prompt set $\\mathcal{P}$ in the model"}, {"title": "5.2 Experimental Settings", "content": "In this section, we introduce the experimental settings, in-cluding comparison methods and implementation details.\nComparison Methods. First, we compare our proposal with baseline method, which always uses the model with the highest downloading volume as the best-matched model [Shen et al., 2023]. The performance of baseline can be identified by a reasonable lower bound of CGI prob-lem. Then, we also consider the basic implementation of the RKME specification [Wu et al., 2023] as a comparison method, namely, RKME, for the CGI problem to evaluate whether learware techniques can be applied to the CGI.\nImplementation Details We adopt the official code in [Wu et al., 2023] to implement the RKME method and the official code in [Radford et al., 2021] to implement the pre-trained vision-language model. We follow the default hyperparam-eter setting of RKME in previous studies [Guo et al., 2023], setting the size of the reduced set to 1 and choosing the RBF kernel [Xu et al., 1994] for RKHS. The hyperparameter $\\gamma$ for calculating RBF kernel and similarity score is tuned from {0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05} and set to 0.02. For all experiments without additional notes, we assume that the specification is generated with developer-provided prompts. Our experiments are conducted on Linux servers with NVIDIA A800 GPUs."}, {"title": "5.4 Further Analysis", "content": "In this section, we analyze the model identification perfor-mance when using default prompts and present qualitative re-sults through visualization.\nDefault Prompt Set. When model developers do not pro-vide prompts, the model hub uses a default prompt set to generate specifications, making it more adaptable to differ-ent conditional generative models. shows the FID scores when using default prompts for all models. Our PMI achieves significantly lower FID scores compared to RKME and baseline methods, demonstrating its ability to generate"}, {"title": "6 Conclusion", "content": "In this paper, we study a novel problem setting called Con-ditional Generative Model Identification, whose objective is to describe the functionalities of conditional generative mod-els and enable the model to be accurately and efficiently identified for future users. To this end, we present a sys-tematic solution including three key components. The Au-tomatic Specification Assignment and Requirement Genera-tion respectively project the model functionality and user re-quirements into a unified matching space. The Task-Specific Matching further builds the task-specific specification in the matching space to precisely identify the most suitable model. To promote relevant research, we open-sourced a benchmark"}]}