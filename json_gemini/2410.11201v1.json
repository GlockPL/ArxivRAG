{"title": "TREE OF ATTRIBUTES PROMPT LEARNING FOR VISION-LANGUAGE MODELS", "authors": ["Tong Ding", "Wanhua Li", "Zhongqi Miao", "Hanspeter Pfister"], "abstract": "Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully leverage the rich context indicated in the category name. To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a \"concept - attribute - description\" structure for each category, and then learn the hierarchy with vision and text prompt tokens. Unlike existing methods that merely augment category names with a set of unstructured descriptions, our approach essentially distills structured knowledge graphs associated with class names from LLMs. Furthermore, our approach introduces text and vision prompts designed to explicitly learn the corresponding visual attributes, effectively serving as domain experts. Additionally, the general and diverse descriptions generated based on the class names may be wrong or absent in the specific given images. To address this misalignment, we further introduce a vision-conditional pooling module to extract instance-specific text features. Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods on the zero-shot base-to-novel generalization, cross-dataset transfer, as well as few-shot classification across 11 diverse datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in vision-language models (VLMs) like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) merge the capabilities of visual perception with linguistic understanding, which have revolutionized the landscape with their zero-shot learning abilities. They proficiently handle tasks on unseen data, bypassing the conventional requirement for task-specific training. This feature has enabled a plethora of applications, ranging from content-based image retrieval to complex visual question answering, setting new benchmarks in the domain. A crucial development in this domain is the concept of prompt learning, which has significantly influenced both natural language processing (NLP) (Lester et al., 2021; Li & Liang, 2021; Liu et al., 2021) and vision-only models (Jia et al., 2022; Wang et al., 2022a;b; Zhang et al., 2022). This approach leverages learnable prompts to guide model understanding, tailoring responses to specific tasks or datasets.\nPrompt learning, particularly in vision-language models, has garnered considerable interest due to its parameter efficiency and rapid convergence (Zhou et al., 2022b;a; Zhu et al., 2023; Derakhshani et al., 2023; Lu et al., 2022). Techniques like CoOp (Zhou et al., 2022b) optimize learnable continuous prompts for few-shot image recognition, enhancing model performance significantly. Recent efforts have expanded to multimodal prompt learning, optimizing prompts in both visual and language domains (Khattak et al., 2023a;b; Shi & Yang, 2023; Lee et al., 2023). Despite their success, these models rely on simplistic text prompts, typically formatted as \"a photo of a {class}\", illustrated in Fig. 1 (a). While functional, this approach lacks depth, failing to encapsulate the intricacies and finer details inherent in visual data. Such limitations hinder the model's ability to fully leverage the rich, descriptive potential offered by more detailed and contextually relevant textual information.\nIn parallel, another stream of research has been exploring the utilization of large language models (LLMs) to generate more elaborate and descriptive text prompts for enhancing zero-shot learning capabilities (Menon & Vondrick, 2023; Pratt et al., 2023; Roth et al., 2023; Kim et al., 2023; Parkhi et al., 2012; Yan et al., 2023; Yang et al., 2023; Roy & Etemad, 2024; Zheng et al., 2023; Tian"}, {"title": "2 RELATED WORK", "content": "Prompt Learning for Vision-Language Models. Prompt learning bridges linguistic understanding and visual perception by guiding VLMs with text prompts, a concept originated in NLP (Lester et al., 2021; Li & Liang, 2021; Liu et al., 2021) and adapted to vision-only (Jia et al., 2022; Wang et al., 2022a;b; Zhang et al., 2022) and multimodal contexts (Zhou et al., 2022b;a; Khattak et al., 2023a;b; Shi & Yang, 2023; Lee et al., 2023; Tian et al., 2023; Rasheed et al., 2023; Roy & Etemad, 2024; Zheng et al., 2023; Zhu et al., 2023; Bulat & Tzimiropoulos, 2023; Lu et al., 2022). In the textual domain, CoOp (Zhou et al., 2022b) optimizes learnable continuous prompts in CLIP's language branch\nfor few-shot image recognition, while CoCoOp (Zhou et al., 2022a) addresses CoOp's overfitting issues by conditioning prompts on visual features. In the visual domain, Visual Prompt Tuning (VPT) (Bahng et al., 2022) and Dual-modality Prompt Tuning (DPT) (Xing et al., 2023) enhance CLIP's vision encoder by learning visual prompts in pixel space and dynamically generating prompts through cross-attention, respectively. TransHP (Wang et al., 2023b) leverages category hierarchy for prompt learning to improve classification performance. LoGoPrompt (Shi & Yang, 2023) enhances classification by incorporating synthetic images with class name text as auxiliary visual prompts. MaPLe (Khattak et al., 2023a) explores multimodal prompt learning, jointly optimizing prompts in both vision and language branches. Other recent works have focused on regularizing prompt learning to leverage the knowledge from base VLMs effectively, demonstrating enhanced generalization in varied downstream visual tasks (Khattak et al., 2023b; Bulat & Tzimiropoulos, 2023; Roy & Etemad, 2024). PromptSRC, for instance, introduced a self-regulating method that restricts both the vision and text prompt, demonstrating improved generalization. Distinct from these approaches, PLOT (Chen et al., 2023) and ALIGN (Wang et al., 2023a) leverage Optimal Transport to align multiple prompts with local visual features, either from the multi-head self-attention layer or at a token level. Our work diverges from these methods by introducing a hierarchical \"Tree of Attribute\" framework derived from LLMs to structure textual descriptions and guide the learning of specialized \"domain expert\" tokens for attribute-level understanding.\nEnhancing model's understanding using visual attributes. There's a growing emphasis on enhancing model's understanding of the class names by integrating visual attributes into the model. A common to do that is to use detailed visual descriptions from LLMs, moving beyond generic prompts (Zhou et al., 2022b;a). These descriptions, like the \u201cfur pattern\" or \"tail shape\" of a cat, provide fine-grained and distinctive characteristics. The use of LLMs like GPT-3 (Brown et al., 2020), allows for efficient generation of a broad spectrum of class-specific descriptions, offering an advantage over manually crafted templates. While this approach has been extensively researched in zero-shot contexts (Kim et al., 2023; Menon & Vondrick, 2023; Parkhi et al., 2012; Roth et al., 2023; Yan et al., 2023; Yang et al., 2023; Fabian et al., 2023; Pratt et al., 2023; Novack et al., 2023), its application in conjunction with prompt learning for few-shot tasks remains relatively unexplored(Mao et al., 2023; Lee et al., 2023; Tian et al., 2023; Zheng et al., 2023; Zhang et al., 2024; Liu et al., 2024). Additionally, another line of research integrates the visual attributes through data augmentations. For example, AAPL (Kim et al., 2024) uses adversarial token embedding to mitigate bias and enhance attribute-specific learning through augmented image features. However, we note the following limitations of previous methodologies. First, in the use of LLMs, existing methods have largely utilized unstructured descriptions, which lacks an organized framework for effective utilization. Additionally, while data augmentation can improve learning of invariant visual attributes, it is constrained by the limitations of image data augmentation, which cannot cover all possible variations. Our approach diverges by structuring the visual descriptions into a \u201cTree of Attribute\", coupled with learnable visual prompts as domain experts. Additionally, LLM-generated descriptions often cover a wide range of potential class descriptions, of which not all may be pertinent to a given image, pointing to the need for a selective pooling mechanism to ensure optimal image-text alignment. We further introduce a vision-conditional pooling layer for refined image-text alignment. This structured approach not only enhances the interpretability of the model's learning process but also significantly improves alignment accuracy between image content and descriptive text."}, {"title": "3 METHODOLOGY", "content": "3.1 PRELIMINARY\nCLIP. Our approach is built on the pre-trained vision-language model, CLIP (Radford et al., 2021). Formally, let (x, c) denote the dataset, where x is an image and c\u2208 {1,..., C'} are the class labels. For an image x, the vision encoder h\u2081(\u00b7) transforms it into a feature vector f = h(x). Simultaneously, each class label c is mapped to a text prompt tc = a photo of a {c}, and converted into textual feature vectors ft = hr(tc). The predicted class \u0177 is given by:\n\u0177 = argmax cos(f, ft)  (1)\nC\nwhere cos() denotes cosine similarity.\n3.2 OVERALL FRAMEWORK\nWe rethink the descriptions by LLM De as nodes in knowledge graphs. While previous methods generate an unstructured set of descriptions, we distill structured knowledge graphs for each class c from LLM, in which the root node is the class name c, capturing the highest level semantics, and the leaf nodes are the detailed descriptions capturing fine-grained details. In this framework, previous paradigms only generate the leaf nodes of the graph, with the edges and graph structure missing, where the rich and inherent structure from the descriptions is overlooked. To address this limitation, we formulate our approach as a Tree of Attribute, which follows the \"concept - attribute - description\" structures, as illustrated in Fig. 1 (c).\nBesides weighting the descriptions equally, previous works align descriptions that describe images from different aspects and at different granularities with a singular CLS token from the image encoder. However, while the use of a single CLS token is effective in certain contexts, we note that the CLS token is designed to capture the global information of an input image x (Dosovitskiy et al., 2021). As a result, even though this helps to further inform global understanding, it may fail to effectively capture the nuances and variances at the attribute level, which leads to suboptimal use of the rich descriptions. We address this by introducing a set of learnable prompt tokens that serve as domain experts in the vision branch, each of which aligns with a specific attribute-level textual embedding.\nAdditionally, close inspection of the LLM-generated descriptions indicates limited contextual rele- vance and a high degree of diversity. Previous works (Roth et al., 2023) reflect the issue of descriptions that are likely not co-occurring e.g. \"steam\" and \"fried\". We further identify cases where the de- scriptions are technically correct but irrelevant to certain images, such as describing \u201clong tail\" in frontal images of cats, underscoring the need for a selective pooling mechanism. Thus, we introduce a vision-conditional pooling layer to extract instance-specific text features for each attribute for selecting the most applicable descriptions.\nOverall, TAP leverages the tree structure in two key ways: first, a top-down process generates attributes and corresponding descriptions for each class in a structured and contextually relevant manner. This ensures that the descriptions are structured and contextually relevant. Second, a bottom-"}, {"title": "3.3 TREE OF ATTRIBUTE GENERATION BY LLMS", "content": "We redefine the process of integrating LLM-generated descriptions by introducing a knowledge graph Ge = {Vc, Ec} for each class c, where Ve denotes the set of nodes, and Ec denotes the edges that capture the semantic relationship between nodes. In previous works, Ve is the set of descriptions De, while Ec is missing. We argue that such methods overlook the inherent structure among the descriptions and thus do not exploit the richness of these descriptions effectively. To better leverage knowledge from LLMs, we introduce an attribute layer to link the root node class name, and the leaf node descriptions. The attribute nodes include visual attributes generated by LLMs, such as color and shape, for systematically guiding description generation as illustrated in Fig. 1 (c). Each branch of this \"tree\" represents a specific attribute, with the subsequent \u201cleaves\u201d fleshing out the descriptions with finer details. In this framework, Ge includes the class name which is the root node, the set of attributes such as color and shape being the intermediate layer, and lastly the set of descriptions under each attribute node. E includes the edges that build up the hierarchy. This structure allows for a nuanced representation of class information, spanning from general concepts down to specific attributes and detailed descriptions.\nTo this end, we introduce the Tree of Attribute (ToA), where we use a tree structure to model the relationship and structure of the descriptions. Let A denote the set of attributes, and for each attribute ac \u2208 Ac, we denote its leaf nodes as Da. Each set De contains descriptions that specifically pertain to attribute a for class c, which is denoted as\nD = {da,1,da,2,...,da,n},  (3)\nwhere da, represents the i-th description for attribute a of class c and n is the number of descriptions per attribute.\nThe process of generating a Tree of Attribute (ToA) unfolds in three steps: 1) Attribute Generation: We first query LLMs with the dataset information and ask it to generate a set of attributes A which are considered relevant and characteristic of the dataset. 2) Example Generation: We then ask LLMs to generate descriptions for a randomly sampled class in the dataset, using the attributes A identified in the previous step. Each description takes the format of \u201cclass, which {is/has/etc} {description}\". 3) Description Generation for All Classes: Building upon the Q&A template from the previous step, the LLM is then tasked with generating descriptions for all classes in the dataset.\nAdditionally, we incorporate a \"global context\u201d attribute which is aligned with the CLS token in the vision encoder. The descriptions are the 7 standard templates provided in (Radford et al., 2021)."}, {"title": "3.4 LEARNING TAP WITH LEARNABLE EXPERT TOKENS", "content": "To fully exploit the structured Tree of Attribute, we introduce learnable visual expert tokens p in the vision branch to learn from each of the attribute nodes a \u2208 A. Unlike traditional methods that rely on a single CLS token for alignment, these expert tokens enable focused learning on specific image attributes, such as color or shape, enhancing the model's performance and interpretability.\nWe denote the set of introduced visual expert tokens as IPU = {pala \u2208 A}. Akin to the idea of visual prompt tuning (VPT) (Jia et al., 2022), we insert PU into the input sequence of the vision encoder, forming the prompted input sequences Xp = {ecLs, PU, Epatch}, where ecls is the input CLS token, and Epatch denotes the embedded patch tokens. To further boost the model's capacity for nuanced attribute representation, we employ deep prompting by introducing a zero-initialized layer residual for each prompt token across transformer layers, which provides more explicit attribute guidance across transformer layers. In parallel, we adopt a set of m learnable context tokens"}, {"title": "3.5 VISION-CONDITIONAL POOLING", "content": "To mitigate issues of misalignment and potential misleading information from the broad spectrum of LLM-generated descriptions, we proposed an adaptive vision-conditional pooling layer, applicable to each set of attribute descriptions Da shared across all classes to dynamically pool the most applicable descriptions based on the visual content of the image x using its corresponding visual expert token denoted as pax. For ease of expression, we will proceed without explicitly mentioning x, though it's important to note that both the expert token and the resulting attribute-level embeddings are dependent on the visual information. Intuitively, VCP uses attention to calculate the similarity between p and all embedded descriptions in attribute Da, which are then used as weights for a weighted sum of the original description embeddings. Formally, for each attribute a and its associated expert token pu, the pooled attribute-level embedding va for class c and attribute a is:\nQuery = WqPa\nKey = Wk Emb(DC),\nAttention Score = softmax(Query \u00b7 Key),\nv = Attention Score \u00b7 Emb(DC),  (4)\nwhere Wq and We are learnable weights \u2208 Rd\u00d7d, Emb(\u00b7) denotes the embedding function, and softmax() is the Softmax function. This layer mirrors cross-attention but omits W to maintain the output within the CLIP V-L space."}, {"title": "3.6 TRAINING AND INFERENCE", "content": "Training objective. During training, each visual expert token p is aligned with its associated attribute-level embedding vo, trained with the following contrastive objective:\nLcon (Pa, v) =  log ( exp(cos(p, v)/\u315c)/ \u03a3i=1 exp(cos(p, v)/\u03c4) )  (5)\nwhere N represents the number of training samples, and 7 is the learned temprature of CLIP. The total classification loss Lclass is the average of the contrastive loss from each expert token as well as the CLS token, defined as:\nLclass = (Leon (Pa, v)))  (6)\nSimilar to (Khattak et al., 2023b) and (Bulat & Tzimiropoulos, 2023), we regularize the vision CLS token, text feature, and the prediction logits from each attribute using the vanilla CLIP model. We denote the regularization loss as Lreg, where the details can be found in Appendix. The overall training objective is Ltotal = Lclass + Lreg.\nPrediction fusion. During inference, we integrate the prediction by each attribute expert pair by a weighted sum, formulated as follows:\n\u1ef9 = argmax(a cos(FCLS, VCLS) + (1-\u03b1)/(A-1) cos(pa, v) )  (7)\nwhere a is a hyperparameter that signifies the weight assigned to the global context provided by the CLS token, balancing its contribution with that of the attribute-specific expert prompts."}, {"title": "4 EXPERIMENTS", "content": "We extensively evaluate our method in three settings: 1) Base-to-novel class generalization, where the datasets are equally split into base and novel classes. We train the model on the base classes only and"}, {"title": "4.1 BASE-TO-NOVEL GENERALIZATION", "content": "In base-to-novel generalization, we equally split the classes into base and novel classes. Initial training and evaluations are conducted on the seen base classes, followed by evaluation on the unseen novel classes in a zero-shot manner. TAP surpasses prior state-of-the-art models in terms of the base and novel class accuracy, as well as their harmonic mean across most of the 11 datasets, with an average increase of 1.53% in the zero-shot novel class prediction, and a 1.07% increase in the"}, {"title": "4.2 CROSS-DATASET TRANSFER", "content": "To further investigate the generalization capability of TAP, we train on ImageNet with 16 shots per class, and directly test on the other 10 datasets under zero-shot without further tuning. As shown in Table 2, TAP demonstrates better generalizability on 8/10 target datasets compared to PromptSRC (Khattak et al., 2023b), and achieves an average performance increase of 0.75%. Additionally, while the performance increase of previous methods on target datasets come with costs on the source dataset (-0.49% for CoCoOP and -0.24% for PromptSRC) as compared to CoOp (Zhou et al., 2022b), TAP also outperform previous methods on the source dataset with 1.03% increase compared to PromptSRC (0.79% incrase compared to CoOp), demonstrating TAP 's robustness in domain generalization without sacrifice on source dataset performance."}, {"title": "4.3 FEW-SHOT CLASSIFICATION", "content": "In few-shot classification, TAP also outperforms existing methods in 9 out of the 11 datasets. Detailed in Table 3, we achieve an average accuracy of 83.37 across the 11 datasets, surpassing the previous state-of-the-art methods by 0.5%, further demonstrating the effectiveness of our method."}, {"title": "4.4 ABLATION STUDY", "content": "Effects of Tree of Attribute. A core inquiry is whether structuring descriptions into a Tree of Attribute (ToA) offers advantages over an unstructured aggregation of LLM-generated descriptions. To evaluate, we revert to aligning a mixed, unstructured set of descriptions with the CLS token - a common practice in prior studies (Mao et al., 2023; Lee et al., 2023; Tian et al., 2023; Zheng et al., 2023), while keeping the same number of visual prompt tokens. According to Table 4, substituting the ToA with an unstructured set results in significant performance decreases of 1.86%, 2.31%, and 2.11% across the average base, novel, and their harmonic mean performances, respectively. This stark contrast underscores the ToA's critical role in enhancing model efficacy."}, {"title": "4.5 VISUALIZATION", "content": "Expert tokens focus on attribute-related regions. We further investigate the effects of vision domain experts by visualizing their class activation maps from three illustrative examples using GradCAM (Selvaraju et al., 2017), as shown inFig. 3. These visualizations underscore the precision with which each expert token concentrates on the image regions pertinent to its designated attribute. Take the first cat image as an example. The \u201cfur pattern\" expert distinctly highlights the animal's fur texture, whereas the \u201cear\" and \"eye\" experts focus precisely on the respective anatomical features. This pattern of attribute-specific attention is consistent across the evaluated examples, reinforcing the conceptualization of expert tokens as dedicated \u201cdomain experts\u201d within the visual field.\nVCP layer pools the most applicable descriptions. The inherently interpretable nature of the VCP layer, thanks to its attention mechanism, allows for insightful visualizations of its operational process. Through the examination of attention weights assigned by the VCP layer to different attributes in a given image, we elucidate the layer's capability to discern and prioritize the most applicable descriptions. As illustrated in Fig. 4 with a \u201cdumplings\u201d image, the VCP layer adeptly allocates higher attention weights to descriptions accurately reflecting the observed instance (e.g., assigning weights of 0.92 to \"round with a pleated edge\u201d under the \"Shape\" attribute and 0.95 to \"soft and chewy texture\" under the Texture\u201d). In contrast, less relevant descriptions for the specific image context (e.g., \"crescent-shaped\" for Shape and \u201ccrispy texture from pan-frying\" for Texture) receive significantly lower weights. This discernment is crucial, given the class dumplings\u201d encompasses a broad variety of appearances based on cooking methods, yet not all descriptions are fitting for every instance. These visualizations compellingly demonstrate the VCP layer's effectiveness in refining description relevance, thereby enhancing the model's interpretative alignment with the visual content."}, {"title": "5 CONCLUSION", "content": "This paper introduces Tree of Attribute Prompt learning (TAP), a novel method that integrates detailed, LLM-generated descriptions within VLMs, achieving state-of-the-art performance in base-to-novel generalization, cross-dataset transfer, and few-shot image classification tasks across 11 diverse datasets. TAP leverages a hierarchical \"Tree of Attribute\" framework, distilling structured knowledge graphs from LLMs for nuanced representation of visual concepts, and employs learnable \"domain expert\" tokens and a vision-conditional pooling module for optimal image-text alignment. While promising, we note that the reliance on LLMs presents challenges in fine-grained datasets where similar classes require nuanced differentiation, in which cases LLMs generate identical descriptions for distinct classes, impacting novel class prediction performance. It highlights the current limitations of LLMs in discerning highly fine-grained distinctions. Addressing this challenge through enhanced LLM capabilities or alternative strategies will be a key focus of future research."}, {"title": "A APPENDIX", "content": "A.1 MODEL REGULARIZATION\nDenote the frozen image feature from CLIP vision encoder as fu, the frozen text feature for description d from CLIP text encoder as f, and the zero-shot logit prediction from CLIP as \u0177. Additionally, denote the trained image feature as f\", the trained text feature for description d as f, and the logit prediction from attribute a after training as \u1ef9a. The losses are as follows:\nLcon-T = \u2211 1/2 log (exp(cos(f, f)) /(\u03a3ked exp(cos(f, f))))  (8)\nLL\u2081-V = ||f\" \u2013 f\"||\u2081  (9)\nLKL-attr = 1/( \u03a3 \u03b1\u0395\u0391 EA D(CC(Y, Ya))  (10)\nThe regularization loss is then:\nLreg = \u00b5\u2081LL1-V + \u00b52LKL-attr + \u00b53Lcon-T,  (11)\nOur overall training objective is thus given by:\nLtotal = Lclass + Lreg  (12)\nTo investigate the effectiveness of model regularization, we compare TAP against existing methods with and without regularization. As evidenced in Table 9, the proposed model regularization helps in both base and novel performance, with an increase of 1.62% in average harmonic mean. Comparing to existing methods, TAP is consistently better than other baselines in both settings, demonstrating the robustness of our method."}, {"title": "A.2 ADDITIONAL IMPLEMENTATION DETAILS", "content": "Because the number of attributes vary across the 11 datasets which results in different number of learnable parameters, we group the datasets into two and apply two sets of learning rates to balance generalizability and performance. For DTD, Oxford Flowers, Stanford Cars, UCF101, and Caltech101 datasets, which have fewer attributes, we use a low learning rate of 0.002 for the text encoder to avoid overfitting and a high learning rate of 0.006 for the vision encoder to facilitate the learning process. A high \u00b53 = 3 is also used to regularize the text encoder for preventing overfitting. For the remaining 6 datasets, which have more attributes, the learning rates for both text and vision encoders are set as 0.004, with \u00b52 = 1.5, \u00b5\u2081 = 10, and \u00b52 = 2.5 are used for all datasets.\nFor base-to-novel generalization and few-shot classification evaluations, we use an adaptive approach for generating the attributes, in which the attributes vary across datasets. Although it turns out to be better than using a fixed set of attributes as shown in Table 7, it is not applicable to the cross-dataset transfer experiment as both VCP layers and visual expert tokens are specific to their corresponding attributes. Therefore, for cross-dataset transfer, we use the following fixed set of 4 attributes that are applicable to all 11 datasets: Pattern, Texture, Shape, and Context.\nWe use PyTorch Paszke et al. (2017) to implement all experiments on a single NVIDIA A100-80GB GPU. Our code is developed based on the implementation of CoOp Zhou et al. (2022b), which is available at https://github.com/KaiyangZhou/CoOp and released under the MIT license. Our code is also released under the MIT license. Baseline results for the three tasks are taken from their respective publications. For the \"global context\" attribute which is aligned with the CLS token in the vision encoder, we use the following 7 selected templates provided in Radford et al. (2021).\n\"itap of a {class}.\"\n\"a bad photo of the {class}.\"\n\"a origami {class}.\"\n\"a photo of the large {class}.\"\n\"a {class} in a video game.\"\n\"art of the {class}.\"\n\"a photo of the small {class}.\""}, {"title": "A.3 ROBUSTNESS OF LLMS", "content": "To investigate the robustness of our methods against different LLMs, we additionally generate the descriptions using a locally-served small LLM - Qwen-2-7B-Instruct (Yang et al., 2024), in which the results are comparable."}, {"title": "A.4 PROMPTS FOR TREE-OF-ATTRIBUTE GENERATION", "content": "As introduced in Section 3.3, we generate the Tree-of-Attribute with the following three steps: 1) Attribute Generation, 2) In-Context Example Generation, and 3) Description Generation for All Classes. The prompts for each step are as follows:\n1) Attribute Generation:\n{Dataset Description.}\nVisual attributes refer to observable, describable features of the images that can include color, shape, size, texture, and any specific patterns or markings, which can help differentiate between classes for the dataset. They should be consistently observable across multiple images of the same class. Your task is to generate a list of visual attributes (less than 10) for the {Dataset Name} dataset. Ensure this list is clear, concise, and specific to the dataset's needs. Avoid generic attributes that do not contribute to distinguishing between classes.\n2) In-Context Example Generation\nDescribe describe what a \"{Random Class Name}\" class in the {Dataset Name} dataset look like using the generated visual attributes.\nYou must follow the following rules:\n1. For each visual attribute, describe all possible variations as separate sentences. This approach allows for a detailed and clear presentation of each attribute's range.\n2. Provide a maximum of five descriptions for each visual attribute to maintain focus and relevance. Also, aim to provide at least two descriptions to ensure a comprehensive overview of the attribute.\n3. The descriptions should provide clear, distinguishable features of each class to support image classification tasks.\n4. Descriptions for each attribute are independent from each other, and they should not serve as context for each other.\n5. Each description describes an image independetly. If certain description is possible for a class, please just list that description, and do not use words like \"may have\" or \"sometimes have\".\n6. Reply descriptions only. Do not include any explanation before and after the description.\n7. The descriptions should follow the format of \"classname, which ...\", where \"...\" is the description of the visual attribute.\n3) Description Generation for All Classes\n{Dataset Description.}\nYour task is to write detailed descriptions for various classes within the {Dataset Name} dataset, using the provided visual attributes such as color and shape. These descriptions will help in accurately classifying and understanding the unique features of each class.\nYou must follow the following rules:\n1. For each visual attribute, describe all possible variations as separate sentences. This approach allows for a detailed and clear presentation of each attribute's range.\n2. Provide a maximum of five descriptions for each visual attribute to maintain focus and relevance. Also, aim to provide at least two descriptions to ensure a comprehensive overview of the attribute.\n3. The descriptions should provide clear, distinguishable features of each class to support image classification tasks.\n4. Descriptions for each attribute are independent from each other, and they should not serve as context for each other.\n5. Each description describes an image independetly. If certain description is possible for a class, please just list that description, and do not use words like \"may have\" or \"sometimes have\".\n6. Reply descriptions only. Do not include any explanation before and after the description.\n7. The descriptions should follow the format of \"classname, which ...\", where \"...\" is the description of the visual attribute.\nQ: Describe what a \"{Random Class Name}\" in the {Dataset Name} look like using the following visual attributes: {Visual Attributes from Step 1.}\nA: {Answer from Step 2.}\nQ: Describe what a \"{Target Class Name}\" in the {Dataset Name} look like using the following visual attributes: {Visual Attributes from Step 1.}\nA:\nIn the prompt templates, \"Dataset Description\" is the description of the dataset from their official website, \"Random Class Name\" is a randomly sampled class name in the dataset for in-context example generation, and \"Target Class Name\" is the class name of interest for the current query. While step 1 and 2 are made in two consecutive calls to provide contexts which are queried once per dataset, step 3 is queried independently for each of the remaining classes in the dataset. Our carefully designed prompts for step 1 and 2 guide the LLM in generating high-quality examples. Human review further confirms that the generated in-context examples from these prompts are of high quality even without any manual intervention."}]}