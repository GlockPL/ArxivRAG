{"title": "UNIFIED PARAMETER-EFFICIENT UNLEARNING FOR LLMS", "authors": ["Chenlu Ding", "Jiancan Wu", "Yancheng Yuan", "Jinda Lu", "Kai Zhang", "Alex Su", "Xiang Wang", "Xiangnan He"], "abstract": "The advent of Large Language Models (LLMs) has revolutionized natural language processing, enabling advanced understanding and reasoning capabilities across a variety of tasks. Fine-tuning these models for specific domains, particularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like LoRA, has become a prevalent practice due to its efficiency. However, this raises significant privacy and security concerns, as models may inadvertently retain and disseminate sensitive or undesirable information. To address these issues, we introduce a novel instance-wise unlearning framework, LLMEraser, which systematically categorizes unlearning tasks and applies precise parameter adjustments using influence functions. Unlike traditional unlearning techniques that are often limited in scope and require extensive retraining, LLMEraser is designed to handle a broad spectrum of unlearning tasks without compromising model performance. Extensive experiments on benchmark datasets demonstrate that LLMEraser excels in efficiently managing various unlearning scenarios while maintaining the overall integrity and efficacy of the models.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) demonstrate remarkable capabilities in knowledge understanding and complex reasoning (Li et al., 2023; Zhang et al., 2024b; Li, 2024; Li et al., 2024; Lee et al., 2024), having sparked increasing interest in adapting LLMs to specific domains through fine-tuning techniques (Li & Liang, 2021; Dettmers et al., 2023; Zhang et al., 2023; Zaken et al., 2022). Among them, Parameter-Efficient Fine-Tuning (PEFT) (Li & Liang, 2021; Liu et al., 2021), such as LoRA (Hu et al., 2022), has emerged as the mainstream paradigm, offering significant reductions in resource costs by fine-tuning only a small subset of parameters. While highly effective, the reliance on domain-specific data for fine-tuning raises concerns regarding data leakage and privacy (Lu et al., 2024; Blanco-Justicia et al., 2024), such as potentially memorizing or propagating sensitive, biased, copyrighted, or harmful information (Liu et al., 2024c; Qu et al., 2024). In this light, researchers have introduced unlearning techniques (Jang et al., 2023; Kurmanji et al., 2023; Kumar et al., 2023) into LLMs, to \"forget\" specific data without requiring the time-consuming and resource-intensive process of retraining.\nPrior efforts in exploring unlearning in LLMs primarily focus on removing specific concepts (Kassem et al., 2023; Jang et al., 2023). A typical example is the erasure of LLM's ability to recall information related to the Harry Potter series (Eldan & Russinovich, 2023). While these efforts yield valuable insights, they risk inadvertently affecting related concepts, such as other novels with similar titles. In this work, we broaden the scope by investigating instance-wise unlearning tasks, which allow us to target more nuanced aspects of model behavior. To this end, we first present various instance-wise unlearning tasks for LLMs, as illustrated in Figure 1. More case studies can be found in Appendix B. Specifically, consider a training instance z = (x, y) in a supervised fine-"}, {"title": "2 PRELIMINARY", "content": "This section introduces key concepts underpinning our methodology. We cover instruction tuning to enhance LLMs' understanding of human instructions, followed by PEFT, highlighting LoRA for efficient updates. Lastly, we discuss the influence function, which analyzes parameter changes from data perturbations. These foundations set the stage for the techniques discussed later."}, {"title": "2.1 INSTRUCTION TUNING", "content": "Instruction tuning is a key technique that leverages carefully curated datasets of human-annotated instructions and corresponding responses to enhance LLMs' capacity to comprehend and respond to human instructions (Wei et al., 2022; Liu et al., 2023b; Sanh et al., 2022). Given a downstream task dataset Z = {z|z = (x,y)} containing n instances, where x represents a description of the human instruction and y is the corresponding response, LLMs are fine-tuned using the following autoregressive (Brown et al., 2020; Touvron et al., 2023a) objective:\n$\\max _{\\Phi} \\sum_{(\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{Z}} \\sum_{t=1}^{|\\mathbf{y}|} \\log (P (y_{t} | \\mathbf{x}, y_{<t}; \\Phi)),$\nwhere I is LLMs' parameters, yt is the t-th token of y, and y<t represents tokens preceding yt."}, {"title": "2.2 PARAMETER-EFFICIENT FINE-TUNING", "content": "LLMs typically consist of billions of parameters, making full fine-tuning computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) addresses this challenge by updating only a small number of the parameters while still achieving satisfactory performance. Among them, LoRA (Hu et al., 2022) stands out as particularly effective, which freezes the original pretrained parameters while introducing pairs of low-rank-decomposition weight matrices to simulate parameter updates. Formally, the optimization objective for LoRA is expressed as follows:\n$\\max _{\\Theta} \\sum_{(\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{Z}} \\sum_{t=1}^{|\\mathbf{y}|} \\log (P (y_{t} | \\mathbf{x}, y_{<t}; \\Phi + \\Delta\\Phi(\\Theta))),$\nwhere is the trainable parameters that is significantly smaller in size compared to \u03a6."}, {"title": "2.3 INFLUENCE FUNCTION", "content": "The influence function was first applied in machine learning by Koh & Liang (2017) to analyze the outputs of black-box models. For the dataset Z, we focus on the following empirical risk minimization (Shalev-Shwartz & Ben-David, 2014; Vapnik, 1998; Bartlett & Mendelson, 2002) problem:\n$\\Theta \\in \\underset{\\Theta}{\\text{argmin}} \\Big{\\vert} \\mathbb{R}(\\mathcal{Z}; \\Theta)\\Big{\\vert} \\mathbb{R}(\\mathcal{Z}; \\Theta) := \\frac{1}{n} \\sum_{(\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{Z}} \\mathcal{L} ((\\mathbf{x}, \\mathbf{y}); \\Theta)$\nwhere O is the trainable model parameter and \u2295 is the minimizer of Equation 3. L (\u00b7; \u0398) is the loss function, and for Equation 2, it is defined as:\n$\\mathcal{L} ((\\mathbf{x}, \\mathbf{y}); \\Theta) = \\sum_{t=1}^{|\\mathbf{y}|} \\log (P (y_{t} | \\mathbf{x}, y_{<t}; \\Phi + \\Delta\\Phi(\\Theta))).\nWhen a training example (x, y) is upweighted by an infinitesimal amount e, the perturbed loss for Onew (6) can be expressed as:\n$\\hat{\\Theta}_{new} (\\epsilon) \\in \\text{argmin} \\Big{\\mathcal{L} (\\mathcal{Z}, (\\mathbf{x}, \\mathbf{y}), \\epsilon; \\Theta) \\vert \\hat{\\mathcal{L}} (\\mathcal{Z}, (\\mathbf{x}, \\mathbf{y}), \\epsilon; \\Theta) := \\mathbb{R}(\\mathcal{Z}; \\Theta) + \\epsilon\\mathcal{L} ((\\mathbf{x}, \\mathbf{y}); \\Theta)\\Big\\}.\nWhen \u20ac \u2248 0, the parameter change \u0394\u0398(\u20ac) = \u00d4new (\u20ac) \u2013 \u00d4 can be approximately calculated by applying a Taylor expansion of Equation 3. Please refer to (Koh & Liang, 2017) for detailed derivation. Specifically, \u0394\u0398(\u03b5) can be written as:\n$\\Delta\\Theta(\\epsilon) \\approx -\\mathbf{H}_{\\Theta}^{-1} \\nabla_{\\Theta} \\mathcal{L} ((\\mathbf{x}, \\mathbf{y}); \\hat{\\Theta}),$\nwhere H = VR(Z; \u00d4) is the Hessian matrix, VoL((x, y); \u00d4) represents the gradient of L w.r.t. parameters \u0398, evaluated at \u2295."}, {"title": "3 METHOD", "content": "In this work, we propose LLMEraser, a framework that updates the PEFT adapter parameters to handle various instance-wise unlearning tasks. As shown in Figure 2, our approach leverages the influence function to directly estimate the parameter changes for various unlearning tasks, circumventing the resource-consuming fine-tuning or retraining procedures. Moreover, we present a novel algorithm to accelerate the computation of the inverse Hessian-vector-product in the influence function, enabling its efficient implementations in LLMs. Finally, we summarize how LLMEraser works."}, {"title": "3.1 \u03a4\u0391\u03a7\u039f\u039d\u039fMY OF LLM UNLEARNING TASKS", "content": "We focus on instance-wise unlearning tasks for LLMs, specifically for PEFT that uses domain-specific data. For an instance z = (x,y), where x represents the query and y is the response, we propose a taxonomy of unlearning tasks based on the operation applied to the target instance.\nInstance Removal (IR). When a specific instance z = (x, y) is either restricted from use or contains harmful content, it necessitates complete elimination from the training set, along with its associated influence on the model.\nQuery Modification (QM). This category involves modifying the query x, transforming z = (x, y) into z' = (x', y). It could not only delete outdated or incorrect tokens in the query x, such as noisy interactions from a user's history, but also update erroneous or outdated tokens with correct ones.\nResponse Correction (RC). Here, the focus is on rectifying the output component y of the instance z. That is, replacing z = (x, y) with z' = (x, y'). For binary classification tasks, such as answering \"Yes\" or \"No\", it corrects mislabeled outputs by flipping the labels. For other tasks, such as multi-class classification or question answering, it is applied to rectify inaccurate responses.\nOur proposed taxonomy expands the concept of LLM unlearning beyond the removal of entire instances. It introduces a more fine-grained categorization defined at the token level within both queries and responses, allowing for nuanced control of model behavior."}, {"title": "3.2 LLMERASER", "content": "The key strength of LLMEraser lies in its capacity to directly estimate the adapter's parameter changes caused by various unlearning tasks. For the sake of clarity and without sacrificing generality, we employ the loss function in LoRA (cf. Equation 4) as our example, while other alternatives would yield similar formulations.\nTo develop a unified approach for solving all unlearning tasks in our taxonomy, we begin by considering a general case where perturbations are applied to both the query (x) and response (y) components of an instance z. This generalized framework allows us to model each specific unlearning task as a special case of this perturbation scenario. Formally, we define the perturbation & applied to z as zs = (x + dx, y + dy), where \u03b4\u03b5 and dy represent perturbations to the query and response, respectively. We now formulate the perturbed empirical risk minimization problem as:\n$\\Theta_{\\delta}(\\epsilon) \\in \\text{argmin} \\Big{\\mathbb{R}(\\mathcal{Z}; \\Theta) + \\epsilon\\mathcal{L} ((\\mathbf{x} + \\delta x, \\mathbf{y} + \\delta y); \\Theta) - \\epsilon\\mathcal{L} ((\\mathbf{x}, \\mathbf{y}); \\Theta)\\Big\\},$\nwhere\n(6) is the minimizer of the optimization problem after applying a perturbation 8 of magnitude e to the sample z. Following the derivation in (Koh & Liang, 2017), when the sample size n is sufficiently large, by taking \u20ac = = (i.e., \u0454 \u2248 0), we can safely estimate the parameter change \u0394\u0398\u03b4 as follows:\n$\\Delta\\Theta_{\\delta} \\approx (\\nabla_{\\Theta}^2 \\mathbb{R}(\\mathcal{Z}; \\hat{\\Theta}))^{-1} \\frac{1}{n} (G(\\mathbf{x}, \\mathbf{y}) - G(\\mathbf{x} + \\delta x, \\mathbf{y} + \\delta y)),$\nwhere G(x, y) is an abbreviations for VL ((x, y); ). Next, we present the perturbations and corresponding parameter changes for different unlearning tasks.\n\u2022 Instance Removal. The deletion of data corresponds to the perturbation function in Equation 5. By setting \u20ac = - like Equation 6, it is equivalent to removing instance z. The set of deleted instances is denoted as SIR. By aggregating the gradients of all deleted instances, the parameter change AOIR can be expressed as follows:\n$\\Delta\\Theta_{IR} \\approx (\\nabla_{\\Theta}^2 \\mathbb{R}(\\mathcal{Z}; \\hat{\\Theta}))^{-1} \\frac{1}{n} \\sum_{(\\mathbf{x}, \\mathbf{y}) \\in S_{IR}} G(\\mathbf{x}, \\mathbf{y}).$\n\u2022 Query Modification. Modifying certain tokens in the query x is equivalent to perturbing x with \u03b4\u03b1, where \u03b4\u03b1 represents deleting noisy tokens or correcting inaccurate tokens, while keeping the response unchanged (i.e., dy = 0). Hence, the perturbed instance z is represented as 28 = (x+dx,y), with the set of instances requiring the removal or modification of specific tokens represented by SQM. By aggregating the gradients of all instances in Sqm, the parameter change AeQ\u2122 induced by query modification can be shown as follows:\n$\\Delta\\Theta_{QM} \\approx (\\nabla_{\\Theta}^2 \\mathbb{R}(\\mathcal{Z}; \\hat{\\Theta}))^{-1} \\frac{1}{n} \\sum_{(\\mathbf{x}, \\mathbf{y}) \\in S_{QM}} \\nabla_{\\Theta} \\mathcal{L}((\\mathbf{x} + \\delta x, \\mathbf{y})).$"}, {"title": "4 EXPERIMENT", "content": "In this section, we carry out extensive experiments to assess the performance and efficiency of LLMEraser. The experiments are designed to explore the following key research questions: RQ1: How does LLMEraser perform across various unlearning tasks? RQ2: How does LLMEraser perform at different unlearning ratios? RQ3: How does the efficiency of LLMeraser compared to other unlearning methods?"}, {"title": "4.1 EXPERIMENTAL SETUPS", "content": "We conduct experiments on both LLMs and Multimodal Large Language Models (MLLMs), focusing specifically on LLMs for Recommendation (LLM4Rec) (Bao et al., 2023; Liao et al., 2024) and MLLM relation mining tasks (Wu et al., 2024; Ye et al., 2024), to validate the effectiveness of our proposed LLMEraser. We choose LLaMA2-7B (Touvron et al., 2023b) as our backbone LLM and LLaVA 1.5-7B (Liu et al., 2023a) for the MLLM experiments. Comprehensive details on task, datasets, baselines, and evaluation metrics for our proposed LLMEraser can be found in Appendix D.1."}, {"title": "4.2 RESULTS ANALYSIS FOR VARIOUS UNLEARNING TASKS (RQ1)", "content": "We design a variety of comprehensive experiments to thoroughly validate the effectiveness of LLMEraser across the three unlearning tasks we have proposed."}, {"title": "4.2.1 RESULTS ANALYSIS ON INSTANCE REMOVAL", "content": "For instance removal, we directly delete a proportion of training instances and subsequently evaluate the performance of each unlearning method. The experimental results on LLM4Rec are shown in Table 2. We can find that: (1) LLMEraser closely mirrors the performance of Retrain. The performance gap between LLMEraser and Retrain is merely 0.0038, constituting only 0.6% of Retrain's performance. This can be attributed to our method's direct estimation of the parameter changes between the retrained model and the original model, allowing for a highly accurate calculation of these changes. (2) Other unlearning methods exhibit notable declines in model performance. Specifically, Gradient Ascent and E2URec show average decreases of 2.7% and 2.4%, respectively, as they do not explicitly aim to approximate the Retrain model during the fine-tuning process."}, {"title": "4.2.2 RESULTS ANALYSIS ON QUERY MODIFICATION & RESPONSE CORRECTION", "content": "Adversarial attack experiments are widely employed to assess the efficacy of data modification for unlearning techniques (Wu et al., 2023; Moon et al., 2024; Cha et al., 2024). The core idea is first randomly introducing corrupted instances into the dataset, which inevitably leads to a decline in model performance, and then leveraging unlearning techniques to correct these noisy data on the model. Following this setting, we evaluate the performance of LLMEraser in both query modification and response correction tasks.\nFor query modification, we conduct experiments on the LLM4Rec task by adding adversarial noise to the user interaction sequences, i.e., randomly deleting some items from the sequences (Interaction Removal) or replacing them with corrupt ones (Interaction Replacement), and then using LLMEraser to rectify the data. Table 3 presents the experimental results. We can observe that: (1) LLMEraser brings a substantial utility gain to the model compared to the corrupted baseline, significantly reducing the negative impact of noisy data. Specifically, it achieves an average improvement of 5.1% compared to the corrupted model in both settings, with a peak increase of 5.5% in interaction removal setting. Moreover, its performance is closest to that of Retrain, demonstrating its effectiveness in correcting inaccurate input information. (2) SISA and RecEraser fail to improve performance. Their average results in both settings decreased by 7.0% and 31.3% compared to the corrupted baseline. The reasons may lie in their dataset partitioning and submodel retraining strategy, potentially leading to a loss of crucial contextual information and introducing inconsistencies in learned representations. (3) RecEraser underperforms SISA in most cases. Designed on traditional recommendation models, RecEraser relies on users' collaborative signals to optimize shard partitioning; however, this strategy fails to effectively adapt to LLM4Rec.\nFor response correction, we introduce noise into the training data of the MLLMs task by randomly assigning incorrect labels to a portion of the samples. In the spurious biases task for MLLMs, we reverse 40% the original \"yes/no\" labels. For the hard hallucination mining task in MLLMs, we assign random labels to 40% of the samples. We leverage LLM unlearning to mitigate the negative impact of such noisy data, aiming to approximate the performance of retraining with clean data. The experimental results of response correction unlearning task on spurious biases task and"}, {"title": "4.3 RESULTS ANALYSIS FOR DIFFERENT UNLEARNING RATIOS (RQ2)", "content": "To assess the sensitivity of various unlearning methods to different scales of unlearning data, we conduct experiments using different unlearning ratios in instance removal and query modification tasks. For the instance removal, we employ TallRec as the LLM4Rec framework, where 5% and 10% of instances are removed. Meanwhile, for query modification, LLARA is utilized as the backbone, where 5% and 10% of user interactions are deleted. The experimental results are shown in Figure 3. From these results, we can find that: (1) In the instance removal task, LLMEraser consistently performs closest to Retrain across different unlearning ratio settings, with an average performance decline of only 1.18%. This indicates that LLMEraser can effectively delete data while minimizing the negative impact on model performance. (2) In the query modification task, LLMEraser consistently achieves the best performance across various unlearning ratios, with an average improvement of 4.9% compared to corrupted method. Notably, at an unlearning ratio of 10%, the relative improvement reaches 5.1%. The average difference between LLMEraser and Retrain is only 0.0079. In comparison to SISA and RecEraser, LLMEraser demonstrates a superior ability to maintain model utility. This highlights the effectiveness of LLMEraser, demonstrating its robust performance across varying unlearning demands. (3) We observe an interesting phenomenon in query modification task under adversarial attack settings, with a sufficiently high unlearning ratio (in this case, 5% and 10%),"}, {"title": "4.4 RESULTS ANALYSIS FOR UNLEARNING EFFICIENCY (RQ3)", "content": "Efficiency is a key metric in evaluating unlearning techniques, particularly for LLMs. We here conduct experiments, comparing our proposed LLMEraser against existing techniques. For a fair comparison, we report the execution time in the QM task, where 5% of users have items replaced with noisy interactions. All methods are run on a single Nvidia A100 GPU. Table 6 presents the results. We can observe that: (1) Due to the parallel training of sub-models, the retraining time of both SISA and RecEraser can be reduced to some extent. However, RecEraser requires data partitioning based on similarity, which introduces additional computational overhead. Moreover, both methods remain highly inefficient as unlearning requests necessitate retraining of the adapters. (2) In contrast, our proposed LLMEraser exhibits remarkable efficiency in handling unlearning tasks. By directly modifying model parameters, LLMEraser achieves a speedup of approximately 31.25 times compared to retraining, requiring only about 1.4 \u00d7 103seconds to update the parameters. This reduction in execution time demonstrates the effectiveness of our approach in accelerating the computation of parameter changes. Additional experimental results and related analyses on the memory usage and execution time of LLMEraser can be found in Appendix F."}, {"title": "5 LIMITATIONS", "content": "LLMEraser offers efficient parameter updates without the need for retraining, making it versatile across different unlearning tasks while also reducing computational overhead. Despite the improvements brought by LLMEraser, its potential shortcomings should not be overlooked. Calculating parameter changes for different unlearning tasks requires accessing the gradient information of the target data and assumes the availability of the training set. Furthermore, the influence function's reliance on the first-order Taylor expansion of the optimization objective leads to inevitable estimation errors, representing an inherent limitation of such an approach."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "This paper introduces LLMEraser, a unified parameter-efficient unlearning framework. By systematically categorizing and addressing various unlearning tasks, LLMEraser leverages influence functions for parameter adjustments, circumventing the cumbersome retraining processes common in traditional methods. Extensive experiments on benchmark datasets show that LLMEraser excels in efficiently handling various unlearning tasks while preserving the overall integrity and efficacy of the models. Additionally, LLMEraser opens new avenues for future research, encouraging the exploration of enhanced unlearning techniques and their implications in diverse applications, such as data privacy and ethical AI. Future studies could explore the broader applicability of LLMEraser and potential optimizations for its computational efficiency and accuracy."}, {"title": "ETHICS STATEMENT", "content": "This work is primarily foundational in instance-wise unlearning for LLMs, focusing on the development of a more efficient parameter-efficient unlearning framework. Its primary aim is to contribute to the academic community by enhancing the understanding and implementation of LLM applications in specific domains. We do not foresee any direct, immediate, or negative societal impacts stemming from the outcomes of our research."}, {"title": "A OVERVIEW OF EXISTING LLM UNLEARNING", "content": "\u2022 SISA (Bourtoule et al., 2021): It works by dividing the training dataset into partitions, allowing for targeted unlearning of specific instances. The methodology typically involves the following steps: data partitioning, retraining, and aggregation. However, a notable limitation of SISA is that it does not preserve the model architecture and requires retraining of sub-models, which can lead to increased computational costs.\n\u2022 FairSISA (Kadhe et al., 2023): FairSISA improves upon SISA by incorporating fairness enhancements. It still relies on the paradigm of retraining sub-models to handle unlearning requests. This approach inherently alters the model architecture and necessitates the retraining of the sub-models, which can limit the flexibility and efficiency of the unlearning process.\n\u2022 APA (Hu et al., 2024b): This study introduces the first exact unlearning approach for large language model-based recommendation (LLMRec), focusing on the removal of personal data to comply with privacy regulations. The Adapter Partition and Aggregation (APA) method is proposed, which combines data partitioning with parameter aggregation to reduce inference latency while maintaining performance. This approach enables efficient unlearning without incurring the extra costs typically associated with traditional methods. However, it can affect the integrity of the adapter structure and necessitates retraining of sub-models.\n\u2022 Gradient Ascent: It utilizes the gradient of the target instance to fine-tune the adapter by moving in the direction of the negative gradient of the deleted data. However, this approach is not effective for input modification and output correction tasks, as gradient ascent of target instances cannot adequately handle these scenarios.\n\u2022 EUL (Chen & Yang, 2023): This work introduces a lightweight approach for LLMs to efficiently forget specific information without complete retraining. It incorporates unlearning layers into transformer architectures, utilizing a selective teacher-student formulation, and employs a fusion mechanism to combine multiple unlearning layers into a unified layer. This enables LLMs to dynamically handle a sequence of deletion requests while maintaining model performance. The introduction of adapters alters the model's structure, and the KL divergence-based methods are only effective for instance removal tasks, as obtaining a model trained on clean data is not feasible.\n\u2022 E2URec (Chen & Yang, 2023): This method uses lightweight LoRA modules and a teacher-student framework to forget specific data while maintaining performance. However, the extra LORA module changes the original model architecture, and the teacher-student framework requires pretraining on both retained and forgotten data, which is intricate and cannot perform well on other tasks like editing."}]}