{"title": "PRIVACY RISKS OF SPECULATIVE DECODING IN LARGE LANGUAGE MODELS", "authors": ["Jiankun Wei", "Abdulrahman Abdulrazzag", "Tianchen Zhang", "Adel Muursepp", "Gururaj Saileshwar"], "abstract": "Speculative decoding in large language models (LLMs) accelerates token generation by speculatively predicting multiple tokens cheaply and verifying them in parallel, and has been widely deployed. In this paper, we provide the first study demonstrating the privacy risks of speculative decoding. We observe that input-dependent patterns of correct and incorrect predictions can be leaked out to an adversary monitoring token generation times and packet sizes, leading to privacy breaches. By observing the pattern of correctly and incorrectly speculated tokens, we show that a malicious adversary can fingerprint queries and learn private user inputs with more than 90% accuracy across three different speculative decoding techniques \u2013 BiLD (almost 100% accuracy), LADE (up to 92% accuracy), and REST (up to 95% accuracy). We show that an adversary can also leak out confidential intellectual property used to design these techniques, such as data from data-stores used for prediction (in REST) at a rate of more than 25 tokens per second, or even hyper-parameters used for prediction (in LADE). We also discuss mitigation strategies, such as aggregating tokens across multiple iterations and padding packets with additional bytes, to avoid such privacy or confidentiality breaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have transformed natural language processing (NLP), enabling machines to gener-ate and understand human language at an unprecedented scale. LLMs typically generate text using auto-regressive decoding, where the generation happens serially and each token depends on all the previous ones. Unfortunately, this serial process causes a significant bottleneck in the LLM response latency and under-utilizes the avail-able hardware-level parallelism, limiting token generation throughput and latency.\nSpeculative decoding addresses this problem without impacting model accuracy. It uses smaller models or heuristics such as retrieval or self-drafting to inexpensively generate tokens speculatively, which the larger target model verifies in parallel in a single iteration. By tuning the heuristics to maintain a high rate of correct speculations, such techniques provide 2x to 5x speedups in inference latency and throughput.\nHowever, the adoption of speculative techniques is not with-out risks. Speculative execution in CPUs, which inspired speculative decoding, has led to considerable security vulnerabilities in processors, such as Spectre, Melt-down, and their numerous variants. These side-channel vulnerabilities exploit the side-effects of pro-cessor mis-speculations, such as timing variations intro-duced by mis-speculations, to leak private and confiden-tial data that become accessible to a processor during mis-speculations. This raises the question, does speculative decoding in LLMs also introduce new risks to privacy?\nIn this paper, we provide the first study of the privacy and confidentiality risks of speculative decoding in LLMs, in-cluding leakage of private user inputs or confidential data.\nProblem. We observe two key issues in speculative decod-ing implementations: (1) the pattern of correct and incorrect speculations of output tokens is dependent on the input, and (2) input-dependent speculation patterns can be inferred from the variations in packet sizes: upon correct specula-tion, more tokens generated per iteration leads to a larger packet and so observing the packet size leaks the degree of correct speculation. Consequently, an adversary capable of measuring the number of tokens generated per iteration or packet sizes can gain access to input-dependent speculation-patterns based on variation in packet sizes and leak out private input attributes, or even entire inputs and outputs.\nConsider a response generated by TinyLlama 1.1B Chat V1.0 with the state-of-the-art specula-tive decoding technique, Lookahead Decoding (LADE) as an example. The red tokens are"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "We provide background on speculative decoding in LLMs, and then cover recent side-channel attacks on LLMs.\n2.1 Background on Speculative Decoding\nSpeculative decoding addresses inefficiencies in auto-regressive decoding by allowing multiple token predictions to be verified in parallel, reducing overall latency. It works by generating speculative token guesses, which are then verified by the main model. If correct, multiple tokens are accepted in one step, significantly speeding up the process. If incorrect, the model falls back to standard autoregressive decoding to ensure accuracy.\nSpeculation Using Smaller Models. Many works pro-pose using smaller, much faster draft models to generate sequences of tokens speculatively and then verify a single sequence or a tree of such sequences in parallel using the larger target model. The draft model can be a smaller version of the model family or a pruned larger model. In this paper, we use BiLD as a representative example of this type. Two key policies-fallback and rollback-ensure the quality of tokens is maintained in BiLD: the fallback policy hands control to the larger model if confidence drops, while rollback corrects tokens exceeding a set distance metric, like cross-entropy loss.\nSpeculation using Self-Drafting. Another speculative de-coding paradigm caches previous output tokens and retrieves them for speculation in future iterations, with the target model verifying these speculations. Look-Ahead Decoding (LADE) implements this by maintaining a cache where keys are tokens and values are N-grams previously generated after those tokens. LADE populates its cache using jacobi decoding, selecting random tokens from the input and gen-erating token chains. When a key token reappears in the output stream, LADE uses these N-grams for speculation.\nSpeculation Using Retrieval. Unlike other paradigms re-lying on model generations, Retrieval-Based Speculative Decoding (REST) speculates by retrieving data from a datastore containing contextually relevant text or code. REST's datastore contains prefixes and continu-ations from a corpus, matching input suffixes to generate draft outputs. If there are too many drafts, REST selects high-quality subsets and organizes them in a tree, applying a tree attention mask during decoding with the target model.\n2.2 Recent Side-Channel Attacks on LLMS\nRecent works have demonstrated side-channel vulnerabil-ities in LLM inference systems leading to critical privacy risks. A recent attack demonstrated a token-length side-channel on streaming LLMs, where network-based attackers can leak a large fraction of private response tokens from an LLM chatbot, by observing the size of encrypted packets in the network traffic. By identify-ing individual tokens based on packet sizes and leveraging inter-sentence context, this attack reconstructs a significant portion of private responses from platforms like ChatGPT and Microsoft Copilot that stream responses token-by-token. Another attack demonstrated a tokenizer side-channel, where a malicious user constructs inputs that translate to different number of tokens based on the tokenizer's vocabulary, which is used to leak out the vocabulary and any private data in the vocabulary.\n2.3 Goal: Speculative Decoding Side-Channel\nSpeculative execution techniques in processors, deployed for improving performance, have resulted in many potent side-channel attacks capable of leaking information, such as Spectre and Meltdown and their many variants. Consequently, this work seeks to explore whether the deployment of speculative decoding in LLMs similarly for improving inference perfor-mance introduces any new side-channels capable of privacy breaches."}, {"title": "3 THREAT MODEL", "content": "Like prior works , we focus on streaming LLMs, where the user sends requests to an LLM running on the server, and the server sends back the response token-by-token. Here, we study the following attacker models.\n1. Network Based Adversary. We assume the attacker is able to observe the network packet size and the time at which they are transmitted, similar to . The goal of the attacker is to leak private user queries. Although network packets containing LLM response tokens are encrypted, the packet size is still observable. The attacker can also observe the time be-tween each packet to identify which iteration it belongs to. We show this allows an adversary to fingerprint and leak private user queries in Section 4.\n2. Malicious User. In this setting, we assume the attacker is a malicious user interacting with the LLM, similar to , with the goal of leaking confidential data used by LLM's speculative decoding mechanism. Here, the user can craft arbitrary inputs and inspect the output, and also monitor the number of tokens generated per iteration for the output tokens. We show this allows an adversary to leak out any con-fidential data used for the speculative decoding or its hyperparameters in Section 5."}, {"title": "4 QUERY FINGERPRINTING ATTACK", "content": "4.1 Attack Overview\nAttack Scenario. We consider a scenario where a user interacts with an AI chatbot using an LLM. The LLM is pro-grammed to answer a predefined set of user queries through the system prompt. This can occur in scenarios such as a patient in a healthcare setting interacting with an LLM to get medical queries answered, or a user in a customer service setting sharing private information with the chatbot before being redirected to a human. The queries are sent from a client device, over a network, to a server that hosts the LLM equipped with speculative decoding. This leads to the threat of a network-based attacker trying to eavesdrop on the queries and responses, as described in Section 3.\nGoals and Capabilities. Figure 2 shows the overview of our attack. Our attacker over the network seeks to learn the private query asked by the user. While the query and response packets are encrypted, the attacker can intercept the response packets, measure the time intervals between packets, and count the number of tokens per packet or simply use the packet size to approximate the number of tokens.\nMechanism. Since the input set is limited, the attacker fin-gerprints inputs based on the pattern of output token counts per iteration influenced by speculative decoding. There are two phases to the attack: offline and online. In the offline phase, the attacker profiles the number of output tokens gen-erated per iteration vs iteration-id (we call this a trace) for each input in the input set, and trains a random forest clas-sifier to predict the input based on the trace. In the online phase, the attacker collects the trace for an unknown input and uses the classifier to identify the input. Next, we provide more details about how these fingerprints are obtained, and then describe how the attack works on various speculative decoding techniques (LADE, REST, BiLD).\n4.2 Fingerprinting Using Speculation Patterns\nSpeculative decoding introduces variation in the number of tokens generated per iteration. In iterations with correct speculation, multiple tokens are verified in a single iteration and sent back together in a single packet. However, if the speculation is incorrect, a single token is generated per iter-ation. The attacker uses this variation to create a fingerprint for a particular output sequence.\nAs shown in Figure 2, from each packet containing outputs from a single iteration, the attacker measures the number of tokens generated per iteration. Subsequently, by plotting the token numbers vs iteration ID, it gets the trace for an input/output (prompt-response) tuple.\nFigure 3 shows the trace of tokens per iteration for two dif-ferent prompts with LADE across different runs. From this, we observe two key properties that make these traces suit-able for usage to identify and leak the prompt and response:\n1. Traces are unique to a prompt and response - dif-ferent prompts result in different traces of tokens per iteration, as shown in Figure 3a and Figure 3b.\n2. Traces are reproducible - repeating the same prompt produces similar patterns in the trace of token counts, as shown in the two iterations in Figure 3a or the two iterations in Figure 3b.\nThese observations demonstrate that the number of tokens generated per iteration with speculative decoding is a unique identifier for a prompt/response pair. These patterns can leak the prompt/response via side-channels to an adversary.\nHowever, in existing LLM deployments on servers such as OpenAI's ChatGPT, the logits get decoded into charac-ter strings by the detokenization at the server-side before being streamed to the user in encrypted packets. Because the packets are encrypted, the tokens in each packet cannot be directly inferred. Nevertheless, there is a strong corre-lation between the packet size and the number of tokens per iteration. Figure 4 shows the trace of token count per iteration for a prompt with LADE, and the corresponding packet size in bytes per iteration match considerably. Thus, for our fingerprinting attack, the adversary can use the trace of packet size per iteration as a proxy for the token count per iteration, given that it is easily observable.\nNext, we develop attacks to fingerprint and leak user inputs, based on the traces of packet sizes per iteration.\n4.3 Attack Experiment Design\nWe perform our attacks on an AI Chatbot designed to answer common medical queries from a patient. The user is a patient interacting with the chatbot and the attacker seeks to learn which query the user asks. We conduct the following three fingerprinting attacks:\nExperiment 1. The first experiment studies a scenario where the attacker has exact knowledge of the complete set of queries, allowing the attacker to profile the exact queries offline that will be leaked out in the online phase. In this, we use a set of 50 prompts from real world human-LLM interactions acquired from . These prompts ask a variety of questions about different common diseases (the list of prompts is provided in the Appendix).\nExperiment 2. In this experiment, we choose 50 prompts with commonplace diseases from our dataset that all starts with \"What are the symptoms of\". Therefore, all the prompts have a similar structure. We design this experi-ment to see if our attack can distinguish similar prompts.\nThe training and testing prompts of both Experiment 1 and 2 are the same. This illustrates the case where the attacker has exact knowledge of the set of predefined prompts.\nExperiment 3. The last experiment studies a scenario where the attacker has only approximate knowledge of the set of queries asked by the user. To emulate this, the attacker profiles queries that are semantically similar to the queries asked by the user (e.g., profiled query: \u201cFor how long should you take zolpidem?", "user": "How long is zolpidem typically prescribed for?\"). We reuse the prompts from Experiment 1 as our training prompts, but for testing, i.e., the user prompts to be leaked, we rephrase the prompts in the training set using ChatGPT-40 by prompting it as \"Can you rephrase the following 50 questions while keeping the diseases in each question the same?\"\n4.4 Methodology\nTarget Models. We target three recent speculative de-coding techniques: LADE that uses self-drafting for speculation, REST that uses retrieval from datastore for speculation, and BiLD which uses a smaller model for speculation. However, our results are as applicable to other techniques using speculation. We demonstrate results for LADE with TinyLlama 1.1B Chat V1.0, for REST with Vicuna 7B as the target model and ShareGPT data-store containing 120K conversations used for speculation, and for BiLD with Llama2 7B as the target model and TinyLlama 1.1B as the smaller model for speculation. We run LADE and REST on NVIDIA RTX4090 (24GB) and BiLD on NVIDIA A100 (40GB) GPUs with Question-Answer tasks, to simulate a chatbot under attack.\nDatasets and Classifier. For each experiment, we use 50 queries of varying complexity, length, and semantics (Ap-pendix lists all the prompts). For our training set, in the offline profiling phase of the attack, we run each query 5 to 30 times and extract the packet size traces (5 to 30 traces per query) and train a Random Forest classifier using them. For the test set in the online phase, we use the corresponding 50 queries and 5 new traces per query, collecting 250 new data points for evaluation, and report accuracy and F1-scores. For our attack, we use scikit-learn's Random Forest with\""}, {"title": "4.5 Results", "content": "Table 1 shows the results for the Query Fingerprinting At-tack on LADE, REST, and BiLD.\nExact Knowledge Scenarios (EK). In both Experiment 1 and 2, the attacker leaks the input prompt with high accu-racy: with accuracy of up to 70.8%, 100%, and 77.6% for LADE, REST, and BiLD respectively, compared to random guessing (2% for a set of 50 prompts). In Experiment 1, where we study randomly selected prompts and the set of profiled and leaked prompts are identical, the attack success rates improve as the number of traces per query (TPQ) pro-filed increases, reaching up to 68% accuracy and an F1 score of 0.66 with 30 TPQ for LADE, reaching 99.6% accuracy with 30 TPQ and an F1 score of 1.0 for REST, and reaching 52% accuracy with 30 TPQ and an F1 score of 0.55 for BiLD. In Experiment 2, where all our prompts have a sim-ilar structure, the results are similar, with attack accuracy for LADE, REST, and BiLD reaching 70.8%, 99.6%, and 77.6% respectively. This indicates that our attack success rate is not influenced by the structure of the prompt (input), as long as the response (output) is different.\nApproximate Knowledge Scenarios (AK). In Experiment 3, where the leaked prompts are not exactly the same as the profiled prompts, the attack success rates are lower (up to 18.8% for LADE, 20% BiLD, and 40% for REST), but still significantly better than random-guessing (2%). This demonstrates that models using speculative decoding are vulnerable to query fingerprinting attacks, even when the attacker just has approximate knowledge about the queries.\nOverall, all of the attacks are more successful on REST than BiLD and LADE. This is because REST demonstrates higher correct speculation rates compared to BiLD and LADE, and therefore has more prompt-dependent varia-tions in the tokens per iteration and the packet sizes, leading to the attack achieving higher accuracy on REST.\n4.6 Ablation Studies\nTable 2 presents an ablation study studying the attack accu-racy as the temperature of our target model varies.\nFor LADE and BiLD, as the temperature increases from 0.3 to 1.0, the attack accuracy for both Experiment 1 and Exper-iment 2 decreases. This is because at lower temperatures, the generations and speculation patterns are more stable and alike. Therefore, the output tokens for the same prompt and the packet size traces are more similar across different runs, making the attack more successful.\nFor REST, the attack accuracy is relatively stable for both Experiment 1 and 2 despite the increasing temperature. This is likely because the features are so pronounced that it with-stands the perturbations caused in the output by the increas-ing temperature.\nThe attack accuracy in Experiment 3 has an interesting trend with temperature. Since the attacker only has approximate knowledge of the prompts (train and test set are different), the accuracy of this attack is much lower than Experiment 1 and 2. However, unlike Experiment 1 and 2, for Experi-ment 3 the attack accuracy has a moderate increase as the temperature of the model increases. This is because, at low temperatures, with limited variation in the training data, the classifier overfits on the training set, and at higher temper-atures, this problem does not exist, allowing the attack to reach slightly higher accuracies."}, {"title": "5 INTELLECTUAL PROPERTY LEAKAGE", "content": "Speculative decoding techniques may use carefully selected data or tuned hyper-parameters for achieving the highest correct speculation rates. In this section, we focus on how a malicious user can craft inputs to recover such data or hyper-parameters. We assume the malicious user can examine the output and count the number of tokens streamed back per iteration, as outlined in the threat model in Section 3-2.\n5.1 Leaking Datastore Used for Speculation\nTechniques like REST rely on retrieval from an external datastore to generate speculative tokens. These datastores can be populated with proprietary information to tailor model outputs to specific domains, or data collected from user interactions that is private. Patterns of correct speculations arising from such data can be exploited to leak the sensitive information in such datastores.\n5.1.1 Background on REST's Speculation\nREST builds a datastore of phrases in the format of context and continuation pairs, where a preceding context and its following continuation are stored. During generation, REST retrieves the longest context matching with the tokens gen-erated so far, and uses it to construct a speculative draft by selecting top prefixes from the retrieved continuations. This speculative draft is verified using a tree attention mechanism by the target model.\n5.1.2 Attack Design\nWe observe that any tokens that are correctly speculated, i.e., returned as a group of tokens per iteration have to exist in the datastore. We propose an attack where a malicious user crafts inputs and observes which tokens are correctly speculated and all such tokens can thus be leaked from the datastore. With this, the attacker can systematically extract sequences from the datastore.\nWe study the following attacker strategies for input genera-tion to achieve high token leakage rates:\n1. Random Generation: The user prompts the LLM to generate random paragraphs of text.\n2. Leveraging Common Words. The LLM is prompted to generate paragraphs containing the most frequently oc-curring words, taken from a dataset of top 10,000 English words. This is based on the in-sight that there is a high chance of phrases in the datastore containing these words.\n3. Reusing Leaked Sequences. The LLM is prompted to generate paragraphs that extend phrases leaked out so far, as shown in Figure 5. This approach is based on the insight that the sequence that is already leaked out may be a part of a longer phrase in the datastore. Therefore, with such iterative prompting, additional tokens from the phrase can be leaked out in a focused manner, leading to more unique tokens being leaked with a similar number of queries.\n5.1.3 Results\nWe test our attacks on a REST implementation using the ShareGPT datastore of 120K conversations for speculation on NVIDIA RTX 4090. Figure 6 shows the unique se-quences leaded by each of the three attack strategies, aver-aged over three runs per strategy.\nRandom generation leaks 20,000 unique sequences in 20 minutes, but subsequently the rate slows down leaking only 70,000 sequences in 3 hours. In comparison, prompting with frequently occurring words leaks 31,000 unique sequences in 20 minutes, which increases linearly to 190,000 after 3 hours. Lastly, prompting with leaked sequences results in the most efficient leakage, leaking about 35,000 unique sequences in 20 minutes and about 200,000 after 3 hours, highlighting the amplification potential of feedback. These results show that an attacker can easily leak REST's datas-tore when they can observe correct speculation patterns."}, {"title": "5.2 Leakage of Speculation Hyper-parameters", "content": "Speculative decoding mechanisms rely on hyper-parameters to control speculation, directly impacting correct specula-tion rates and performance. By analyzing traces of these patterns, we can reverse-engineer the hyper-parameters used. We demonstrate this attack on LADE.\n5.2.1 Background on LADE's Implementation\nLADE generates speculative tokens by caching and reusing prior n-grams generated by the model. LADE has two main parameters: N for n-gram size, and G for guess set size. Its cache is structured as a key-value store, where the key is a token, and the value is a list of G candidate (N-1)-grams following the key token, that are used for speculation. These are replaced in the Least-Recently-Used (LRU) order. During execution, LADE use the last token of the previous iteration or the input to query this cache, and the associated G candidate (N-1)-grams are used as speculation candidates, and the best match with target model generation is accepted.\n5.2.2 Leaking Hyper-parameter N in LADE\nAs each iteration outputs at most N-1 tokens under correct speculation, we expect the maximum number of tokens per iteration to be N-1. Therefore, we prompt LADE with a sim-ple prompt, \"Repeat Letter \u2018A' 60 times\u201d that should sustain the maximum correctly speculated tokens per iteration. We observe the maximum number of tokens per iteration with this prompt to deduce N-1, and thus learn N.\n5.2.3 Leaking Hyper-parameter G in LADE\nTo leak G, the number of candidate predictions consid-ered for speculation, we force LADE to incur targeted mis-speculations. Here, we prompt the model to repeat a se-quence of phrases with the same prefix (\u201cI run\u201d), i.e., \u201cI run to the grocery shop. I run as fast as cars. I run with my best friend. I run from the lecture hall.", "run\", resulting in it being able to correctly speculate the token that follows \\\"run\" in each of these phrases. However, if the number of phrases are greater than G, then with LRU replacement, newer n-grams evict older n-grams for the key \\\"run\" from the set of candidates which has a limited capacity of G. This causes the token following \\\"run\" to always be mis-speculated.\"\n    },\n    {\n      \"title\"": "6 MITIGATION STRATEGIES"}, {"content": "In this section, we discuss possible mitigation strategies for our attacks and evaluate their impact on the attack success rate as appropriate."}, {"title": "6.1 Mitigating Query Fingerprinting Attacks", "content": "For the query fingerprinting attacks, we propose the follow-ing mitigations:\n1. Padding Token Packets: Padding the network packet size with additional bytes can mask the actual number of tokens per iteration and conceal the differences between correct and mis-speculations to prevent the attack.\nThe padding can be added in two ways:\n1. Constant Length Padding. Every packet can be padded to a constant size. This size can be selected based on the maximum number of tokens per iteration, multiplied by the maximum bytes per token. We pick this size as 1024 bytes, without loss of generality.\n2. Variable Length Padding. Every packet can be padded with a random number of bytes selected from a uniform distribution of $e \\sim Unif(0, D)$ where D is a pre-defined upper bound of padding and e is an integer.\n2. Reduce Granularity of Attacker Observation: An alternative approach is to aggregate output tokens over mul-tiple iterations at the server and return them to the client all together. This can limit observability of fine-grain specu-lation patterns from the packet sizes for the attacker, and reduce the effectiveness of fingerprinting attacks."}, {"title": "6.2 Mitigating Datastore or Hyper-parameter Leaks", "content": "For preventing confidential data leakage from datastore or parameter leakage attacks, in addition to the two mitigations discussed previously, we can consider one other mitigation:\n3. Using Only Public Data for Speculative Decoding: Any data-store used for speculation (e.g., in REST) should be constructed only with public data, and any private data or personal identifiable information (e.g., usernames, address, etc.) should be scrubbed from it before it is used. This minimizes the risk of sensitive data leakage through any speculation patterns that may be observable despite padding or token aggregation. Similarly, parameters used for specu-lation (such as N and G in LADE) should be assumed to be public as they can be easily leaked out."}, {"title": "7 RELATED WORK", "content": "In this section, we describe other speculative decoding tech-niques that may be exploitable and related LLM perfor-mance optimizations that may lead to side-channel leakage.\n7.1 LLM Speculative Decoding Techniques\nSpeculative Decoding with Tree Attention. SpecInfer and SpecTr observe that when a token early in the speculation draft is rejected, the entire draft gets discarded, resulting in wasted compu-tation. Instead, these works propose multiple drafts per iteration organized in a tree structure to provide alterna-tives for speculative tokens at each position, increasing the likelihood of tokens advancing to the next step and yield-ing greater speedup by increasing correct speculation rates. These drafts are organized in a tree structure with a cus-tom tree attention mask to prevent inter-tree interactions and verified in parallel. Such works (similar to REST which also uses tree attention) increase the correct speculation rate of tokens and introduce more input-dependent speculation patterns in the output. Hence these are likely to be more vulnerable to our query fingerprinting attacks, similar to REST, which is highly susceptible to our attack, as we show in this paper.\nSpeculative Decoding using Target Models. Other works reuse the target model itself for speculative decoding by uti-lizing the feature layer (second-to-top layer). Medusa trains multiple extra heads, each responsible for a token position in the draft sequence, to generate spec-ulative tokens. EAGLE and EAGLE-2 propose a method to perform auto-regressive decoding on the feature layers itself to generate speculative tokens. These techniques are conceptually no different from using an al-ternative smaller model for speculation like BiLD. Hence, they are equally susceptible to query fingerprinting attacks.\n7.2 Performance Optimizations in LLMs\nOther performance optimization techniques in LLMs that can introduce timing variation, may also be susceptible to leakage via timing side-channels.\nAdaptive KV-Cache Management. The KV-cache is the bottleneck for token generation latency for the memory-bound LLM inference . Recent works, like H2O that performs input-dependent cache eviction or TriForce that performs retrieval-based KV-cache selection for drafting may have input-dependent timing variations. These can in-troduce timing side-channels allowing query fingerprinting, similar to our attacks. However, as such attacks rely on timing, they are likely to be more susceptible to noise.\nPrompt Caching A significant portion of prompts sent to LLMs are duplicates, including system prompts, repeated inputs, and others. Prompt caching, adopted by major LLM providers such as Anthropic and OpenAI, precomputes attention states for these repetitive prompt components to enable reuse, and substantially reducing latency for the first token when there is reuse. Such techniques can leak the nature of the query based on the timing. Other works cache the re-sponses for prompts with similar semantic meanings . Recent attacks demonstrate query fingerprinting attacks similar to ours, when such prompt caches are shared between users. In contrast, our attacks are effective even without any explicit sharing of caches."}, {"title": "8 CONCLUSION", "content": "This paper reveals significant privacy risks associated with speculative decoding in Large Language Models. Across multiple speculative decoding techniques, we demonstrate that attacks can exploit speculation patterns to infer user inputs with > 90% accuracy via fingerprinting attacks and also leak data from datastores used for predictions. These leakages highlight the careful consideration needed when deploying speculative decoding techniques to ensure that performance does not come at the cost of reduced privacy."}]}