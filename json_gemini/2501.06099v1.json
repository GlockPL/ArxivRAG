{"title": "Explaining Deep Learning-based Anomaly Detection in Energy Consumption Data by Focusing on Contextually Relevant Data", "authors": ["Mohammad Noorchenarboo", "Katarina Grolinger"], "abstract": "Detecting anomalies in energy consumption data is crucial for identifying energy waste, equipment malfunction, and overall, for ensuring efficient energy management. Machine learning, and specifically deep learning approaches, have been greatly successful in anomaly detection; however, they are black-box approaches that do not provide transparency or explanations. SHAP and its variants have been proposed to explain these models, but they suffer from high computational complexity (SHAP) or instability and inconsistency (e.g., Kernel SHAP). To address these challenges, this paper proposes an explainability approach for anomalies in energy consumption data that focuses on context-relevant information. The proposed approach leverages existing explainability techniques, focusing on SHAP variants, together with global feature importance and weighted cosine similarity to select background dataset based on the context of each anomaly point. By focusing on the context and most relevant features, this approach mitigates the instability of explainability algorithms. Experimental results across 10 different machine learning models, five datasets, and five XAI techniques, demonstrate that our method reduces the variability of explanations providing consistent explanations. Statistical analyses confirm the robustness of our approach, showing an average reduction in variability of approximately 38% across multiple datasets.", "sections": [{"title": "1. Introduction", "content": "The rising global demand for energy, particularly electricity, is causing significant environmental impacts, including increased greenhouse gas emissions and the depletion of vital resources [1]. Global electricity demand is anticipated to rise by nearly 80% by 2040 [2]. Residential and commercial buildings, accounting for one-third of global energy consumption, are major contributors to these environmental impacts [1]. Improving building energy efficiency, especially in terms of electricity usage, is essential to mitigate the adverse effects of growing energy consumption [3]. Achieving this goal relies heavily on detecting and correcting anomalies in electricity consumption. These anomalies, defined as irregularities or deviations from normal energy behavior, include unusual consumption patterns caused by faulty device operations, user negligence (e.g., leaving windows or refrigerator doors open), theft, or non-technical losses. If not addressed promptly, these issues can result in energy waste, increased power consumption, and devices running longer than necessary due to inefficiencies or malfunctions, leading to additional energy waste and potential equipment damage [4, 5, 6].\nData-driven approaches have proven effective in identifying anomalies, offering reliable alerts to analysts and energy managers [7, 8]. Anomaly detection has evolved from traditional statistical methods, which struggle with complex structures and large datasets, to Deep Learning (DL) methods that automatically learn from time series data [9, 10, 11]. DL algorithms excel in identifying abnormal electricity consumption patterns due to their ability to model complex non-linear relationships and leverage multi-layered architectures for hierarchical feature extraction [12, 13]. Despite their superior accuracy, DL models face challenges in transparency and explainability, crucial factors for building trust and ensuring successful real-world deployment [14]. Providing clear, instance-specific explanations for anomalies is needed to enhance expert trust, support informed decisions, and facilitate the adoption of complex DL models in the energy sector [15].\nExplainable Artificial Intelligence (XAI) aims to enhance transparency and provide explanations, allowing users to understand and trust them [16]. While XAI methods have been primarily used in computer vision to provide visual interpretability and explain decisions in tasks such as object recognition, they are equally important for time series data to understand decision-making processes. As sensors become more affordable and ubiquitous, generating vast amounts of electricity consumption time series data, analysis of these data can automate tasks such as energy usage monitoring to enhance maintenance and reduce inefficiencies. For DL-based time series models, such as Long Short-Term Memory (LSTM) networks or Transformers, data are typically transformed into time-series segments using the sliding window technique, necessitating the adaptation of current explainability approaches to provide meaningful explanations for electricity consumption analysis based on this segmented data structure. Converting time points into features for local feature importance and visualizing relevance, similar to saliency masks in images, will provide experts with insights into the decision-making process and facilitate energy improvement tasks [17].\nWhile XAI approaches are generally categorized as either model-specific, tailored to particular model architectures, or model-agnostic, applicable to any model by focusing on input-output relationships, this study focuses on model-agnostic methods due to their versatility and broad applicability across diverse machine learning models. A prominent model-agnostic approach is SHapley Additive exPlanations (SHAP), which is widely recognized for its ability to provide consistent and interpretable feature importance scores using Shapley values from game theory [18]. SHAP has gained considerable attention due to its solid theoretical foundation and its effectiveness in delivering reliable explanations across various domains, making it a widely accepted method for local explanations [19].\nDespite the great success of SHAP, its application to large datasets is often hindered by its computational complexity. Calculating exact SHAP values is not only time-consuming but can also be impractical for many widely used models. To address these challenges, approximation methods such as Kernel SHAP have been proposed. Kernel SHAP estimates SHAP values by solving a weighted linear regression on a sample of perturbed instances, providing a more computationally efficient means of obtaining SHAP value estimates with fewer evaluations of the original model [20].\nHowever, while Kernel SHAP offers improvements in efficiency, it introduces a new challenge: instability in explanations [21]. Kernel SHAP, as well as its variants (Partitioning, and Sampling), depend on the choice"}, {"title": "2. Related Work", "content": "This section reviews XAI techniques in energy systems and time series applications, as well as in anomaly and fault detection."}, {"title": "2.1. XAI in Energy Systems and Time Series Applications", "content": "Several studies have leveraged XAI to provide insights into workings of energy load forecasting models and other building energy management systems. Moon et al. [23] proposed Explainable Electrical Load Forecasting (XELF) methodology for educational buildings, emphasizing the importance of understanding and interpreting the factors that influence electrical load predictions. By incorporating external factors such as weather and internal building data, they trained various tree-based models and utilized SHAP to provide interpretable explanations for the energy predictions made by these models. Similarly, Chung and Liu [24] analyzed input variables for deep learning models predicting building energy loads, comparing the XAI techniques Local Interpretable Model-Agnostic Explanations (LIME) and SHAP, and found that SHAP outperformed LIME by maintaining prediction accuracy with fewer input variables. Joshi et al. [25] presented a data-driven approach for benchmarking energy usage in Singapore, employing ensemble tree models and XAI techniques such as SHAP for a detailed analysis of the impact of building attributes on energy consumption.\nThe integration of XAI in power systems and renewable energy has been also been explored. Zhang et al. [26] utilized SHAP to explain deep reinforcement learning models for power system emergency control, generating SHAP values to quantify the impact of each system variable and clarify how different factors influenced emergency control decisions. Tan et al. [27] proposed an explainable Bayesian neural network for probabilistic transient stability analysis in power systems, using the Gradient SHAP algorithm for explanations. Their approach provided insights at both global and local levels, with global explanations offering a comprehensive understanding of factors influencing overall model behavior and local explanations detailing individual predictions.\nLeuthe et al. [28] explored XAI in building energy consumption forecasting and compared transparent and black-box models. They considered linear regression, decision tree, and QLattice as transparent prediction models and applied four XAI methods - partial dependency plots, Accumulated Local Effects (ALE), LIME, and SHAP - to an artificial neural network using a real-world dataset of residential buildings. Their findings indicate that appropriate XAI methods can significantly improve decision-makers' satisfaction and trust in machine learning models for energy forecasting. Mueller et al. [29] examined the use of XAI to explain characteristics of vehicle power consumption, referring to the energy consumption within a vehicle's low-voltage electrical system. The study applied methods such as ALE and Permutation Feature Importance (PFI) for global insights, and LIME and SHAP for local analysis."}, {"title": "2.2. XAI in Anomaly/Fault Detection", "content": "XAI has been instrumental for enhancing the explainability of anomaly detection and fault detection models. Roshan and Zafar [34] explored the use of SHAP for feature selection in an unsupervised anomaly detection setting. Their approach leveraged SHAP to improve the performance of autoencoders by identifying key features responsible for anomalies, and retraining the model using only benign data. While the SHAP-based feature selection showed improved results over other methods, the paper faced limitations in computational cost due to Kernel SHAP's complexity and potential sampling bias from using a subset of the CICIDS2017 dataset, which could affect generalizability. On the other hand, Antwarg et al. [15] focused on applying the SHAP framework, traditionally used in supervised learning, to explain anomalies detected by unsupervised autoencoders. Their approach emphasizes understanding the relationships between features with high reconstruction errors and those most critical to anomaly detection. Kernel SHAP is utilized to calculate feature importance, offering detailed insights into why certain anomalies occur, by identifying both contributing and offsetting features. However, challenges such as selecting the appropriate background dataset and further validation across various autoencoder architectures remain areas for future research. While Roshan and Zafar's work primarily aimed at enhancing model prediction accuracy through feature selection, Antwarg et al. focused more on interpreting feature contributions and relationships in the context of anomaly explanations.\nKim et al. [35] proposed an explainable anomaly detection framework for maritime main engine sensor data, combining SHAP with hierarchical clustering to interpret and group common anomaly patterns. By transforming SHAP values based on their distributions, Kim et al. were able to identify and isolate key sensor variables contributing to anomalies, allowing for more precise segmentation and analysis of the detected anomalies. This method provides insights into the causes of anomalies by visualizing and grouping similar patterns, offering an improved understanding of the conditions leading to engine failures. A two-layer patient monitoring system employing Kernel SHAP for anomaly detection and explanation in healthcare data was"}, {"title": "3. Background", "content": "This section first introduces core concepts in explaining black box models. Next, classical SHAP and Kernel SHAP are introduced, as our approach leverages these techniques."}, {"title": "3.1. Explaining Black-Box Models", "content": "Understanding and interpreting the decisions made by Artificial Intelligence (AI) models, especially those that function as black-box systems, is crucial for ensuring their legitimacy and reliability in sensitive applications [39]. Modern AI models such as Deep Neural Networks (DNNs) are complex systems with many parameters, making them difficult to interpret [40]. To address this challenge XAI has emerged, offering two key types of explainability: local and global. Local explainability focuses on explaining the decision for a specific instance, offering detailed insights into how a particular prediction was made. In contrast, global explainability provides an overarching view of the model's decision-making process, giving a broader understanding of the AI system's behavior across its entire input space [41, 42]. Both local and global explainability play important roles in making AI systems more transparent, accountable, and understandable, thereby enhancing trust and facilitating decision-making [42].\nIn this research, we focus on local explainability because it allows for a deeper understanding of individual anomalies detected within the data, assisting in determining the root cause of each anomaly. Both LIME and SHAP provide local explanations. LIME explains one prediction at a time by constructing a simple"}, {"title": "3.2. Classic SHAP and Kernel SHAP", "content": "Shapley value estimation, based on cooperative game theory, calculates the contribution of each feature by comparing the model's predictions with and without that feature across all possible feature combinations [18]. However, this is computationally expensive, especially with large feature sets. To address this, sampling methods approximate Shapley values without requiring retraining for every combination of features [44]. Despite these approximations, the process remains resource-intensive for large datasets, leading to the development of more efficient methods such as Kernel SHAP.\nBuilding on the classic Shapley value estimation, Kernel SHAP provides a more practical approach to approximate Shapley values by solving a Weighted Least Squares (WLS) problem. This weighted approach assigns different importance levels to each possible subset of features. Kernel SHAP utilizes sampling techniques to approximate Shapley values while reducing computational burden, thus making it feasible for models with many features while maintaining their theoretical integrity [18, 21]. This balance of practicality and rigor makes Kernel SHAP well suited for energy applications; nevertheless, it still suffers from instability.\nThe Kernel SHAP estimates the contribution of each feature through the following WLS problem:\n$\\min_{\\Phi_0,..., \\phi_F} \\sum_{S \\subseteq F} k(F, S) \\left( v(S) - \\Phi_0 - \\sum_{i \\in S} \\phi_i \\right)^2$ (1)\nHere F represents the total number of features in the model, and S denotes a subset of features. The term v(S) refers to the value of the predictive model when only the features in subset S are considered. The term \\Phi_0 is the base value representing the average prediction over the entire background dataset. The contribution of feature j indicated by \\phi_j is calculated as part of the WLS optimization process by balancing the error terms across all feature subsets. The weights k(F, S) in Equation 1, are given by:\n$k(F, S) = \\frac{(|F|-1)}{(|S| \\cdot (|F| - |S|)}$ (2)\nHere $\\binom{|F|-1}{|S|}$ is the binomial coefficient representing the number of ways to choose |S| features from the total F features. A subset S refers to any possible combination of features from the full set F. For example, given three features {A, B, C'}, possible subsets include {A}, {B, C'}, {A, B, C'}, and the empty set {}. The kernel weights k(F, S), defined in Equation 2, are derived from cooperative game theory, where the contribution of each feature is evaluated by considering all possible subsets S of features. These weights k(F, S) ensure that each feature's contribution is assessed in a balanced manner by giving different levels of importance to subsets of different sizes. More specifically, the weighting ensures that features are not biased by their position in a subset and that smaller subsets are given fair consideration [21]. This weighting scheme balances the evaluation, ensuring that each feature's contribution is assessed within a meaningful and fair context.\nBoth Classic SHAP and Kernel SHAP determine feature importance by excluding features from the model to observe their impact on predictions. Since the model is already trained and features cannot be physically removed, features are substituted with alternative values to reduce their influence. This substitution typically involves using values from a designated background dataset. When calculating the model's prediction without a specific feature, the real value of that feature is replaced with a value from the background dataset. The background dataset can consist of either the entire training data or, in the case of large datasets, a representative subset. This allows the model to simulate the absence of the feature and assess its contribution by comparing changes in predictions when the feature is excluded."}, {"title": "4. Methodology", "content": "This section presents the proposed approach for explaining the anomaly detection model for energy consumption data by leveraging variants of SHAP, such as Kernel SHAP, but improving stability and consistency through the targeted selection of the background dataset (baseline) using a weighted cosine similarity technique. The approach is specifically designed for prediction-based anomaly detection techniques where a black-box model is used to generate energy predictions, which are in turn compared to actual energy values. If the difference exceeds the threshold, the sample is deemed anomalous. After detecting anomalies, we proceed to the explanation phase, which involves multiple steps to provide clear feature contributions for each anomaly. The overview of the complete process of detecting anomalies and explaining underlying features is presented in Figure 1, while the details of each component are presented in the following subsections."}, {"title": "4.1. Prediction Model", "content": "This first step involves getting the prediction-based model ready for the anomaly detection. As seen from Figure 1, it consists of feature engineering, data preparation, and model training and tuning."}, {"title": "4.1.1. Feature Engineering", "content": "Energy consumption data obtained from smart meters or other sensors typically consists of energy consumption and the reading date-time recorded in hourly or similar intervals. For better anomaly detection, we extract the following features from the reading date-time: the hour of the day, the day of the week, the day of the month, the day of the year, the month, and an indicator for weekends. Weather-related features such as temperature, humidity, and wind speed are also incorporated, along with previous energy consumption readings as input features, while the energy consumption remains the target variable. Data are scaled using Min-Max scaling to bring all features to a similar range and avoid dominance of large numbers:\n$X' = \\frac{X - X_{min}}{X_{max} - X_{min}}$ (3)\nwhere X represents the original feature values, $X_{min}$ and $X_{max}$ are the minimum and maximum values of the feature, and X' are the scaled values."}, {"title": "4.1.2. Data Preparation", "content": "Next, the dataset is chronologically split, with the first 80% used for training, the next 10% for validation, and the final 10% for testing. To capture temporal dependencies, the sliding window technique is applied, moving a fixed-length window along the time series, advancing one record at a time. As shown in Figure 2, this approach creates sequences of inputs and corresponding outputs for the model to learn temporal relationships. For each window i, the input sequence is represented as a matrix of time steps and features:\n$\\begin{bmatrix}\nX_{1,t_1} & X_{2,t_1} & ... & X_{F,t_1}\\\\\nX_{1,t_2} & X_{2,t_2} & ... & X_{F,t_2}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\nX_{1,t_I} & X_{2,t_I} & ... & X_{F,t_I}\n\\end{bmatrix}$ (4)\nwhere I represents the number of time steps in the sliding window, F is the number of features, and $X_{f,t}$ is the value of feature f at time t.\nThe output sequence for window i is represented as:\n$O_i = [Y_{t+1}, Y_{t+2},..., Y_{t+h}]$ (5)\nwhere h is the forecasting horizon and $[Y_{t+1}, Y_{t+2},..., Y_{t+h}]$ are the target energy consumption values for time steps t +1 to t+h. In this paper, we used four time steps (I = 48) for inputs and one step ahead forecasting horizon (h = 24) as shown in Figure 2, but explanation only provided for first horizon (h = 1). The prediction model supports anomaly detection through comparison of predicted and actual values."}, {"title": "4.1.3. Model Training and Tuning", "content": "Here, the prediction model, regardless of which type of architecture is employed, is trained and tuned. The selection of hyperparameters that need to be tuned depends on the selected model, but the tuning process remains the same. Bayesian optimization, specifically using the Tree-structured Parzen Estimator (TPE) [45], is employed for tuning due to its resource efficiency, although other techniques could be used as well. Tuning is carried out with the validation set."}, {"title": "4.2. Anomaly Detection", "content": "As already mentioned, prediction-based anomaly detection methods, including those employed in this study, identify the anomalies by comparing the predicted values with the actual values. Normal data is expected to have small deviations, while anomalous samples are expected to result in larger discrepancies. As seen from Figure 1, anomaly detection involves calculating prediction error, determining an anomaly threshold, and classifying samples as anomalous or non-anomalous.\nNext, we calculate the prediction error for the entire training dataset. The prediction error is defined as the difference between the actual and predicted values for each data point within a single prediction window. By computing the prediction errors across all data points in the training dataset, we obtain a comprehensive distribution of errors.\nNext, the anomaly threshold is determined using the Interquartile Range (IQR) method calculated based on the prediction errors from all training data, as it is well-suited for identifying outliers in skewed data distributions [46]. The IQR method sets a range that defines normal data behavior, as shown in Equation 6:\n$Q_1 - 1.5 \\times IQR <e < Q_3 + 1.5 \\times IQR$ (6)\nwhere Q1 represents the first quartile (25th percentile) of the prediction errors, Q3 is the third quartile (75th percentile), and IQR is the interquartile range, calculated as Q3- Q1. Each prediction error is compared against the established threshold. If the prediction error exceeds this threshold, the data point is classified as an Anomalous Sample, suggesting that it deviates significantly from expected behavior. Conversely, if the prediction error falls within the threshold, the data point is classified as a Probably Normal Sample. This classification enables the identification of potential issues in energy consumption."}, {"title": "4.3. Background Dataset Selection for Anomalies", "content": "In the context of SHAP value explanations, the background dataset serves as the baseline, representing the expected or average behavior against which individual predictions are compared. According to Chen et al. [22], selecting an appropriate baseline is crucial as it defines how absent feature values are handled during Shapley value calculations. Different baseline strategies, such as fixed baselines or distributional baselines (marginal and conditional), influence the resulting SHAP values in distinct ways. Our approach of selecting a similar background dataset using weighted cosine similarity aligns with the concept of distributional baselines, where the background samples are chosen to reflect the statistical context of the anomalies being"}, {"title": "4.3.1. Transforming Data", "content": "As our approach leverages the SHAP algorithm, it is necessary to first transform the windowed time series data into a format compatible with this method, which operates on tabular datasets with row vectors as samples. This transformation process is the first step of Algorithm 1. For anomaly detection, the energy data is represented in a windowed format as shown in Equation 4. These data are transformed into a tabular format (1\u00d7 I \u00d7 F), representing the multi-dimensional input into a one-dimensional vector. The transformed data are expressed as follows:\n$[X_{11}, X_{12}, ..., X_{1F}, X_{21}, X_{22}, ..., X_{2F}, ..., X_{IF}]_{(1xIxF)}$ (7)\nFor example, when the model uses inputs with a window length of 48 and 10 features, this transformation results in a 480-dimensional vector representing the input, with explanations provided for each feature.\nThis paper focuses on explaining one-step-ahead predictions, corresponding to the first horizon of the output sequence $O_1 = [Y_{t+1}]$. By setting the output window h to 1, this approach facilitates early anomaly detection. However, the same approach can be extended to explain predictions for multiple horizons. To do this, one only needs to substitute the first horizon with the desired horizon (e.g., the second horizon) and repeat the same transformation and evaluation steps to provide explanations for subsequent horizons."}, {"title": "4.3.2. Global Feature Importance", "content": "After data are transformed into one-dimensional vectors, a surrogate model is used to calculate global feature importance (step 2 of of Algorithm 1). While various models can serve this purpose, we selected Random Forest due to its speed and computational effectiveness. Instead of using approaches that focus primarily on local explanations, such as SHAP, we employed Random Forest to provide faster global feature importance estimates. This is because it uses Gini importance, which does not require calculating local feature importance for each individual training data point. This advantage makes it well-suited for large datasets while maintaining high accuracy [47]. Moreover, Random Forest's ability to handle high-dimensional datasets and robust performance across various datasets made it the an excellent choice for this anomaly explanation step [48].\nTo further strengthen the feature importance derived from the Random Forest model, the exponential transformation is applied to the importance scores, defined as $GFI = exp(GFI_i)$. This transformation increases the differences in importance among features, giving more important features greater impact in subsequent analyses. As a result, the exponential scaling makes similarity measures, specifically cosine similarity, pay more attention to important features, improving the selection of relevant neighbors for SHAP explanations."}, {"title": "4.3.3. Selecting Neighbors for Anomalies", "content": "In step 3 of Algorithm 1, weighted K-Nearest Neighbors (KNN) is employed to select neighbors for each detected anomaly point under analysis. The similarity score between an anomaly point $x_a$ and each point $x_c$ in the training set is computed using the weighted cosine similarity metric:\n$S(x_c, x_a) = \\frac{\\sum_{i=1}^{F} GFI_i X_{ci} X_{ai}}{\\sqrt{\\sum_{i=1}^{F} (GFI_i X_{ci})^2 \\sum_{i=1}^{F} (GFI_i X_{ai})^2}}$ (8)\nHere, $X_{ai}$ and $x_{ci}$ represent the values of feature i for the anomalous sample and another sample from the training dataset, respectively, and F denotes the total number of features. The wight of feature i, $GFI_i$, is the global feature importance calculated determined as described in Subsection 4.3.2. For each anomaly"}, {"title": "4.4. SHAP Integration for Anomalies", "content": "After the SHAP values are calculated, features need to be categorized into those that contribute to the anomaly (referred to as contributors) and those act as offsets. This categorization is based on the comparison between the actual observed output and the predicted value for a specific instance. Such categorization based on SHAP values help us understand how much each feature pushes the model's prediction away from or towards the real value. The prediction for a sample is expressed as [21]:\n$Prediction = BaseValue + \\sum_{i=1}^{F} \\phi_i$ (9)\nHere, the term BaseValue represents the average prediction made using the background dataset, and the SHAP value \\phi_i explains how much feature i contributes to the prediction. It is important to note that SHAP values are not used during the training of the model but are computed afterward to explain how features influence the final prediction.\nThe features are categorized according to the following logic:\n\u2022 If Real > Predicted: This indicates that the model under-predicted the outcome. In this case:\n$Real > Base Value + \\sum_{i=1}^{F} \\Phi_i$ (10)\nHere, positive SHAP values (i > 0) indicate features that pull the prediction closer to the real value, thereby acting as offsets. Negative SHAP values ($\\phi_i < 0$) indicate features that push the prediction further away from the true value, and these are considered contributors to the anomaly.\n\u2022 If Predicted > Real: This indicates that the model over-predicted the outcome. In this case:\n$BaseValue + \\sum_{i=1}^{F} \\phi_i > Real$ (11)"}, {"title": "5. Results and Discussion", "content": "This section present the outcomes of our explainable anomaly detection approach, including data preparation, model optimization, prediction performance, and feature importance. The results highlight the improvements achieved through hyperparameter tuning, the impact of key features on model predictions, and the advantages of using our approach for enhancing model explainability. Statistical comparisons further support the robustness of our approach, demonstrating its effectiveness in producing reliable and consistent results."}, {"title": "5.1. Dataset Description and Preparation", "content": "To conduct a comprehensive analysis of the proposed method, we use energy consumption datasets from five different consumer types: a residence, a manufacturing facility, a medical clinic, a retail store, and an office building. A residence dataset provided by London Hydro [49], comprises energy consumption records from a residence in London, Ontario, Canada, spanning from January 1, 2002, to December 31, 2004, with hourly energy consumption values. To enhance the predictive capabilities of anomaly detection and improve explanations as discussed in Subsection 4.1.1, we incorporated additional date-time and weather-related features from the Government of Canada's historical climate data [50]. The remaining four datasets are from Building Data Genome Project 2 [51], covering January 2016 to December 2017. Weather-related information for these datasets was already included in the repository.\nThe dataset was divided into training, validation, and test sets, following an 80-10-10 split. The model was trained based on windows sequences data. This configuration captures temporal dependencies and patterns within the data, significantly contributing to accurate anomaly detection in the analysis."}, {"title": "5.2. Optimization and Training Results", "content": "This study employs various deep learning architectures, including different variants of Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformer-based models. The RNN variants utilized are Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and their bidirectional counterparts (BLSTM and BGRU), which effectively capture temporal dependencies in sequential data. Within the CNN category, we used one-dimensional CNN (1D-CNN), Dilated CNN (DCNN), Temporal Convolutional Networks (TCN), and WaveNet, which efficiently capture local temporal patterns and enhance feature extraction through convolutional operations. Additionally, Transformer-based models such as the Temporal Fusion Transformer (TFT) and Time Series Transformer (TST) were employed, leveraging self-attention mechanisms to dynamically weigh the importance of time steps and features.\nHyperparameter optimization was conducted for all algorithms using the Tree-structured Parzen Estimator (TPE) from Bayesian optimization. A total of 50 trials were performed, with each trial representing a different set of hyperparameters evaluated on the validation set. The best combination was selected to minimize validation loss. A summary of the selected hyperparameters for models trained on the residential dataset and performance metrics is presented in Table 2. For the remaining datasets, the same process was followed, but selected hyperparameters are not included for conciseness and Table 3 only includes performance metrics."}, {"title": "5.3. Global Feature Importance", "content": "In this section, we present the global feature importance results for 10 features across 48 time sequences, offering a comprehensive view of each feature's contribution to model prediction. As detailed in the methodology, we applied an exponential transformation to the Random Forest results to better highlight"}, {"title": "5.4. Explanation Step: Comparing Random Background Dataset and Similar Background Dataset Selection", "content": "In this section, we provide a detailed comparison between the random and similar background dataset selection methods by analyzing a specific anomaly point identified using the LSTM model on the residential data, as shown in Figures 6 and 7.\nAs discussed in Section 5.3, these feature importance have dimensions of 48\u00d710, where the features are represented in the rows, and their sequences are displayed along the columns. The features are sorted by the absolute value of their SHAP values, with the most impactful features positioned at the top. For example, in Figure 6, the date and time features, such as the month, are identified as the most important in explaining the anomaly. In contrast, in Figure 7, energy consumption and weather-related features are more significant contributors. This demonstrates that the similar background dataset selection can capture different aspects of the data, such as weather information, due to its temporal alignment.\nAnother important aspect of these heatmaps is their relevance to the reliability of the results. Before explaining this further, it is important to clarify the role of the line plots and the dotted horizontal line present at the top of each heatmap. As outlined in Equation 9, the prediction function f(x) (represented by the line plot) is the sum of the SHAP values, showing how each feature shifts the prediction either away from or towards the base value (dotted horizontal line). Given that the dataset consists of 10 features and 48 time steps, the prediction function is defined as:\n$f(x) = BaseValue + \\sum_{t=1}^{48} \\sum_{i=1}^{10} SHAP_t(i)$ (12)"}, {"title": "5.5. SHAP Density Plot Analysis", "content": "As seen from the density plots in Figure 8, based on anomalies detected by the LSTM model on residential dataset, a notable reduction in the variation of SHAP values is observed for date and time-related features (Hour, DayOfWeek, DayOfMonth, Month, DayOfYear) when utilizing similar background dataset. This reduction underscores the ability of this approach to filter out less relevant features, thereby allowing the model to focus on factors more closely linked to the detected anomalies, such as weather information. However, while the variation is significantly reduced, these features are not entirely disregarded. In cases where energy consumption is unusual at specific times, these features still contribute to the model's prediction, albeit with less fluctuation.\nThis behavior indicates that the similar background dataset essentially serves as a filter, helping to emphasize the most impactful features while minimizing noise from less relevant ones. This filtering effect is particularly beneficial when working with high-dimensional data, where many features may not be directly related to the anomalies. By concentrating the SHAP values near zero for these date and time features, the model can more effectively highlight the key drivers of unusual energy consumption, such as weather conditions or specific patterns in energy use."}, {"title": "5.6. Stability of SHAP Explanations across Multiple Datasets and Explainability Approaches", "content": "Analysis of results so far focused on a residential dataset, demonstrating the benefit of our technique for achieving robust explanations. Building on this, here we expand the analysis to the remaining four datasets- a manufacturing facility, a medical clinic, a retail store, and an office building-to examine the portability of our technique across diverse energy consumer types. Additionally, for each dataset, we consider diverse XAI algorithms to illustrate the wide applicability of our technique.\nMoreover, to further evaluate the impact of our approach for background dataset selection methods on SHAP values, we conducted a comparison between random and similar background dataset approaches across different deep learning models and datasets from diverse energy consumer types. This analysis explored various explainability algorithms, including SHAP variants (Kernel, Partition, and Sampling), as well as LIME and Permutation, applied across all five datasets.\nTable 4 presents the results. For each dataset and each XAI method, the table reports the mean and standard deviation (Mean \u00b1 SD) of SHAP/LIME value variability for random baseline and our baseline, calculated as the standard deviation of feature importance across detected anomalies. For all datasets and all XAI methods, our technique achieves lower variability than the random baseline, highlighting its ability to provide more stable and reliable explanations compared to the random baseline. Additionally, the table includes the percentage reduction in variability, calculated as the relative decrease in the similar baseline's mean compared to the random baseline, to quantify the effectiveness of the proposed method. Bartlett's test results (P-Value) [52"}]}