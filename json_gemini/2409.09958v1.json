{"title": "An Offline Adaptation Framework for Constrained Multi-Objective Reinforcement Learning", "authors": ["Qian Lin", "Zongkai Liu", "Danying Mo", "Chao Yu"], "abstract": "In recent years, significant progress has been made in multi-objective reinforcement learning (RL) research, which aims to balance multiple objectives by incorporating preferences for each objective. In most existing studies, specific preferences must be provided during deployment to indicate the desired policies explicitly. However, designing these preferences depends heavily on human prior knowledge, which is typically obtained through extensive observation of high-performing demonstrations with expected behaviors. In this work, we propose a simple yet effective offline adaptation framework for multi-objective RL problems without assuming handcrafted target preferences, but only given several demonstrations to implicitly indicate the preferences of expected policies. Additionally, we demonstrate that our framework can naturally be extended to meet constraints on safety-critical objectives by utilizing safe demonstrations, even when the safety thresholds are unknown. Empirical results on offline multi-objective and safe tasks demonstrate the capability of our framework to infer policies that align with real preferences while meeting the constraints implied by the provided demonstrations.", "sections": [{"title": "Introduction", "content": "In the standard reinforcement learning (RL) setting, the primary goal is to obtain a policy that maximizes a cumulative scalar reward [Sutton and Barto, 2018]. However, in many real-world applications involving multiple objectives, a desired policy must not only strike a balance among potentially conflicting objectives but also consider the constraints imposed on specific safety-critical objectives. Such requirements motivate the research of Multi-objective RL (MORL) [Liu et al., 2014, Mossalam et al., 2016] and safe RL [Gu et al., 2022, Achiam et al., 2017]. In addition to reward maximization, the former aims to enable policies that cater to a target preference indicating the trade-off between competing objectives, while the latter focuses on reducing the cost measures of learned policies within a given safety threshold.\nDespite the development of meticulous and effective mechanisms to achieve these goals, most existing MORL and safe RL algorithms rely on predefined target preferences or safety thresholds. These elements need to be carefully designed by human experts with prior knowledge, generalized from a large number of demonstrations through observation. Taking autonomous driving as an example, defining aggressive, stable, and conservative strategies through different preferences between minimizing energy consumption and increasing driving speed may require extensive human participation, which means that researchers need to categorize driving data into different styles based on human experience and observe the energy consumption-speed ratio to estimate appropriate preference weights and safety thresholds for different strategies. Moreover, it is uncertain whether policies based on manually designed preferences will exhibit expected behaviors or whether feasible policies exist under given safety constraints. The challenge of designing appropriate preferences or safety thresholds becomes more pronounced as the number of objectives increases.\nCompared to manually designing target preferences or safety thresholds based on human knowledge, it is more natural to infer expected behaviors through a few demonstrations that implicitly indicate the trade-off between multiple objectives and the constraints of safety. For instance, selecting demonstrations with conservative behaviors from the driving dataset can be easier than inferring preferences that lead to conservative policies. Therefore, in this work, we formulate a novel offline adaptation problem for constrained MORL, with the goal to leverage a few demonstrations with expected behaviors, rather than relying on handcrafted target preferences and safety thresholds, to generate a target policy that achieves desired trade-offs across various objectives and meets the constraints on safety-critical objectives under offline settings.\nTo achieve this, we first focus on unconstrained MORL scenarios and propose a simple yet effective offline adaptation framework Preference Distribution Offline Adaptation (PDOA), which includes: 1) learning a set of policies that respond to various preferences during training, and 2) adapting a distribution of target preferences based on a few given demonstrations during deployment. Specifically, we initialize the first part with existing state-of-the-art offline MORL algorithms, and then in the second part, we propose to align the adapted policies with expected behaviors by modeling the posterior preference distribution regarding demonstrations. Moreover, we show that our framework can be extended to constrained MORL settings by converting a constrained RL problem into an unconstrained MORL counterpart, and incorporating a conservative estimate of preference weights on constrained objectives to mitigate the potential constraint violations. Lastly, we conduct several empirical experiments on classical MORL, safe RL tasks and a novel constrained MORL environment under offline settings, demonstrating the capability of our framework in generating policies that align with the real preferences and meet the constraints implied in demonstrations."}, {"title": "Related Work", "content": "Multi-objective RL Many existing MORL methods explicitly maintain a set of policies tailored to various given preferences to approximate the Pareto front of optimal policies. These works either apply a single-policy algorithm individually for each candidate preference [Roijers et al., 2014, Mossalam et al., 2016], employ evolutionary algorithms to generate a population of diverse policies [Handa, 2009, Xu et al., 2020] or simultaneously learn a set of policies represented by a single network [Abels et al., 2019, Basaklar et al., 2022]. Furthermore, due to the potential costs and risks associated with extensive online exploration, several studies [Zhu et al., 2023, Lin et al., 2024] have been proposed to leverage only offline datasets for MORL by extending offline return-conditioned methods [Chen et al., 2021, Emmons et al., 2021] or offline policy-regularized methods [Fujimoto and Gu, 2021, Wang et al., 2022] to MORL settings. Despite the ability to obtain a set of well-performing policies for various preferences, most existing MORL methods overlook the process of acquiring target preferences for identifying appropriate policies during practical deployment. In this work, we assume no online interactions and no target preferences but several demonstrations generated with expected behaviors, which are easier to access than meticulously designed target preferences.\nSafe RL While maximizing the expected reward, classical safe RL methods restrict the cumulative cost to stay within a predefined safety threshold through Lagrangian primal-dual methods [Stooke et al., 2020, Chow et al., 2017] or primal approaches [Xu et al., 2021, Sootla et al., 2022]. Recently, several studies have focused on learning a set of policies that respond to various safety thresholds in both online settings [Yao et al., 2024] and offline settings [Liu et al., 2023a, Lin et al., 2023]. Similar to MORL, these works also assume access to well-designed safety thresholds that ensure safe behaviors. Unlike prior research, our framework relies solely on safe demonstrations to indicate the implicit constraints and offers a mechanism to mitigate constraint violations through conservatism.\nRL with Offline Adaptation Several studies have applied meta-learning techniques to address MORL [Chen et al., 2019] or safe RL problems [Guan et al., 2024], but they require online interactions for task adaptation and suffer from low sample efficiency. Additionally, other research endeavors explore offline adaptation methods to mitigate environmental uncertainty by modelling a posterior distribution over all possible Markov Decision Processes (MDPs) [Ghosh et al., 2022] or considering varying confidence levels in conservative value estimates [Hong et al., 2022]. [Mitchell et al., 2021, Xu et al., 2022] focus on offline adaptation for multi-task problems, aiming to achieve fast adaptation to new downstream tasks after offline training on multi-task experience. Among these methods, offline meta RL [Mitchell et al., 2021] addresses a problem similar to ours, which aims to train a meta policy that can adapt to a new task with limited data. Prompt-DT [Xu et al., 2022] achieves"}, {"title": "Preliminaries", "content": "Both multiple-objective RL and safe RL can be discussed based on a uniform formulation: constrained multi-objective MDP (CMO-MDP) proposed by LP3 [Huang et al., 2022]. A CMO-MDP is defined as a tuple (S, A, P, r, c, \u03b2, \u03b3) with state space S, action space A, transition distribution P(s'|s, a), vector reward functions r \u2208 \\mathbb{R}^N for N unconstrained objectives, vector cost functions c\u2208 \\mathbb{R}^K, safety thresholds \u03b2 \u2208 \\mathbb{R}^K for K constrained objectives and discount factor \u03b3\u2208 [0,1]. The goal is to maximize the rewards on unconstrained objectives while ensuring the costs on constrained objectives remain within the safety threshold \u03b2. Since it is typically infeasible to maximize all task objectives simultaneously, preferences \u03c9 \u2208 \u03a9 and preference functions f(r) which map the reward r to a scalar utility under a specific preference w, are introduced to control the trade-off between unconstrained objectives. Given preferences w \u2208 \u03a9 and safety thresholds B, the goal can be formulated as follows:\nmax\u0395\u03c0\u03c9,\u03b2 [Rw], s.t. C\u2264\u03b2, (1)\n\u03c0\u03c9,\u03b2\nwhere Rw = \u2211t fw(rt) and C = \u2211t ct represent cumulative utility and vector cost over time t, respectively. We denote \u03c0\u03c9,\u03b2 as a policy conditioned on w, B. In this paper, we consider the linear preference setting (i.e., fw(r) = wTr where w \u2208 \\mathbb{R}^N and ||w||1 = 1), which is widely studied and applied [Mossalam et al., 2016, Abels et al., 2019] and also serves as a bridge between unconstrained and constrained MORL, as shown in Section 4.3. A CMO-MDP problem can degenerate to a standard safe RL problem when N = 1 and to a standard multi-objective problem when K = 0."}, {"title": "Offline MORL", "content": "Under offline MORL settings, an offline dataset D = {(s, a, s', r, c, w)} is the only data available for training, which is generated by a set of behavior policies \u03c0w(\u00b7|s, w) with diverse behavioral preferences w. One straightforward yet effective approach to learn a set of policies for various preferences is to adapt offline single-objective RL methods for MORL settings. An example of this approach is the multi-objective version of Diffusion-QL (MODF) [Lin et al., 2024], which incorporates a preference-conditioned policy (i.e., \u03c0(as, w)) and a multi-dimensional value function for N objectives (i.e., Q(s, a, w) = Q1(s, a, w), ..., QN (s, a, w)) into Diffusion-QL [Wang et al., 2022]:\nL\u03c0 = \u2212E(s,a,w)\u223cD [Ea'\u223c\u03c0(\u22c5|s,w) [wTQ(s, a', w)] \u2212\nkEi\u223cU,\u03f5\u223cN(0,I) [||\u03f5 \u2212 \u03f5\u03b8(\u221a\u03b1ia + \u221a1 \u2212 \u03b1i\u03f5, s, i)||2] ],\nLQ = E(s,a,r,s',w)\u223cD [(r + \u03b3Ea'\u223c\u03c0(\u22c5|s,w)Q(s', a', w) \u2212 Q(s, a, w))2], (2)\nwhere i is the diffusion timestep, k is the regularization weight, \u03b1i are pre-defined parameters of diffusion model and \u03f5\u03b8(\u00b7) is a denoiser model. The diffusion policy generates the actions by iteratively using \u03f5\u03b8(\u00b7) to recover actions from noise. The second term in the actor loss of Eq. (2) is a diffusion reconstruction loss, which serves as a regularization term to align the actions of diffusion policy with the behavioral actions in the dataset.\nPareto-Efficient Decision Agents (PEDA) [Zhu et al., 2023] is another MORL method based on supervised RL, which trains a policy conditioned on both target preferences and vector returns through a supervised paradigm:\nL = \u2212E(\u03c4t:\u03c4w)\u223cD [log \u03c0\u03b8 (at|st, gt, w)], (3)\nwhere \u03c4t:T = {(st, at, rt), ..., (sT, aT, rT)} is a trajectory segment, gt = \u2211Tt=t rt is the target vector return (a.k.a., return-to-go) and w is the behavioral preference of \u03c4t:T. It is worth noting that despite the significant performance of the above two methods in offline MORL problems, both require human-provided target preferences during deployment to achieve the desired behavior, and therefore cannot be directly applied to the setting in this paper."}, {"title": "An Offline Adaptation Framework for Constrained Multi-Objective RL", "content": "In this paper, we focus on a novel offline adaptation problem for constrained MORL, with the goal to leverage only a few demonstrations to generate the policies that exhibit expected behaviors. During training, an offline dataset D = {(s, a, s', r, c, w)} is provided for policy training. During deployment, we have access to a demonstration set corresponding to a target G, i.e.,\nBG = {xi \u223c \u03c0G}Mi=1, (4)\nwhere xi is defined as a tuple (si, ai, s'i, ri, ci) and M is the total number of transition demonstrations. The target G can be preferences in MORL problems (G = wG), safety thresholds in safe problems (G = \u03b2G) or a combination of both (G = (wG, \u03b2G)), and \u03c0\u2217G is the expert policy that achieves the best utility and meet the constraints under the preference wG and safety threshold \u03b2G of the target G. In our setting, the real target G is inaccessible, and the goal is to obtain an adapted policy for the target G that ensures high utility fw\u2217G(r) and meets the constraints with safety threshold \u03b2G by leveraging demonstration set BG during the deployment phase."}, {"title": "Offline Adaptation for the Unconstrained Case", "content": "First, we set aside the constraints in CMO-MDP and focus on the unconstrained version. Our proposed framework, Preference Distribution Offline Adaptation (PDOA), solves the offline adaptation problem under unconstrained settings in two steps: 1) learning a set of policies that respond to various preferences during training; and then 2) adapting a distribution of target preferences based on given demonstrations during deployment.\nIn the first part, we directly apply existing offline MORL algorithms on the dataset D to obtain a set of policies \u03c0w that respond to varying preferences w. In the second phase, we propose to model the distribution of target preferences and then utilize this distribution to obtain a reliable estimation of target preference. Specifically, we consider the posterior probability of the target preference wG with regard to demonstration set BG, i.e., P(wG|BG,D) = P(BG|wG,D)P(wG|D). Here P(BG|wG, D) represents the probability that the optimal wG generates samples BG in the real environment P(s', r|s, a). Due to the inaccessibility of P\u2217 and \u03c0\u2217 under offline settings, we replace them with the empirical dynamics P\u03c0w(s'|s, a) and its corresponding optimal policy \u03c0\u2217w, which can be obtained using offline data D during training. Therefore, the preference posterior distribution can be approximated by\nP(wG|BG, D) =\nP(BG|wG,D)P(w|D) =\nP(BG|wG, \u03c0w, D)P(w|D).\nP(BG|D) P(BG|D)\nM\n\u221d P(w|D) \u220f P\u03c0w(si)P\u03c0w(ai|si) P\u03c0w(s'i, ri|si, ai),\ni=1\n(5)\nwhere (si, ai, ri, s'i) \u2208 BG. One challenge in Eq. (5) is the requirement of explicitly modeling the state distribution P\u2217(si) and the transition probability P\u03c0w(s'i, ri|si, ai). Another concern is that demonstrations BG with the target preference wG can be out-of-distribution samples for the estimation of P\u2217 and \u03c0\u2217, leading to considerable discrepancy between estimated P\u03c0w(si), P\u03c0w(s'i, ri|si, ai) and their real-world counterparts. This challenge is pronounced in multi-objective settings due to significant differences in the trajectory distributions of the optimal policy under various preferences. Therefore, following previous offline adaptation works [Ghosh et al., 2022, Hong et al., 2022], we opt to approximate log P\u03c0w(si) P\u03c0w(s'i, ri|si, ai) with a surrogate defined by TD error of value models of \u03c0w in M\u03c0w:\nrTD(s, a, r, s') = \u2212\u03b4||Q(s, a, w) \u2212 (r + \u03b3V(s, w))||2, (6)\nwhere \u03b4 is a hyperparameter. This approximation makes sense because rTD(s, a, r, s') not only measures how likely the sample (s, a, r, s') occurred during training for preference w but also aligns with the estimated dynamics P\u03c0w. In other words, rTD(s, a, r, s') has a high value if (s, a, r, s') is"}, {"title": "Extension to Constrained Settings", "content": "Then, we consider a natural extension of our MORL adaptation framework to constrained settings. Under the linear preference setting, the constrained MORL problem in Eq. (1) can be converted to its dual form with zero duality gap [Paternain et al., 2019]:\nmin max \u0395 [\u2211 \u03c4 \u03c9T rt + \u03bbT (\u2211 ct \u2212 \u03b2G)],\n\u03bb\u2208RK \u03c0 AERK\nt\nt\n(8)\nDenoting \u03bb\u2217 as the solution of this problem, the dual problem 8 can be rewritten as:\nmax \u0395 [\u03c0G] [\u03c9\u2217T rt \u2212 \u03bb\u2217T ct] .\n(9)\n\u03c0\nt\nThis formulation means that the constrained MORL problem under safety threshold \u03b2 is uniquely equivalent to an unconstrained problem of finding the optimal policy under an extended preference \u03c9\u2217e = [\u03c9\u2217, \u03bb\u2217]T/||[\u03c9\u2217]T, \u03bb\u2217T||1 among extended N + K objectives re = [rt]T, \u2212ctT. However, solving for the extended preference \u03c9e is challenging and requires the real safety thresholds, which are inaccessible in our setting. Nevertheless, Section 4.2 provides an approach to infer the target preferences from the demonstration set, helping us circumvent this challenge. Therefore, we can convert a constrained MORL problem to an unconstrained MORL problem, where the vector reward is defined as r = [rt, -ct] and dataset D is augmented to De. Here, r1:N = rt corresponds to N unconstrained objectives, while rN+1:N+K = -ct associated with K constrained objectives. One issue with this approach is how to set the behavior preference for augmented dataset De. We present an effective scheme for approximating behavioral preferences in Appendix A.5."}]}