{"title": "Tackling Selfish Clients in Federated Learning", "authors": ["Andrea Augello", "Ashish Gupta", "Giuseppe Lo Re", "Sajal K. Das"], "abstract": "Federated Learning (FL) is a distributed machine learning paradigm facilitating participants to collaboratively train a model without revealing their local data. However, when FL is deployed into the wild, some intelligent clients can deliberately deviate from the standard training process to make the global model inclined toward their local model, thereby prioritizing their local data distribution. We refer to this novel category of misbehaving clients as selfish. In this paper, we propose a Robust aggregation strategy for the FL server to mitigate the effect of Selfishness (in short RFL-Self). RFL-Self incorporates an innovative method to recover (or estimate) the true updates of selfish clients from the received ones, leveraging robust statistics (median of norms) of the updates at every round. By including the recovered updates in aggregation, our strategy offers strong robustness against selfishness. Our experimental results, obtained on MNIST and CIFAR-10 datasets, demonstrate that just 2% of clients behaving selfishly can decrease the accuracy by up to 36%, and RFL-Self can mitigate that effect without degrading the global model performance.", "sections": [{"title": "1 Introduction", "content": "With an aim to train a Machine Learning (ML) model in a privacy-preserving manner, the researchers in [26] introduced Federated Learning (FL) framework, and since then it has drawn interest from the ML community. Being a distributed learning paradigm, FL enables each participant (or client) to train the model locally and send only model weights (parameters) to a central server for aggregation, thereby enabling learning from distributed heterogeneous devices without sharing their sensitive data. FL has been adopted in many contexts, such as, medical records management [31], activity recognition [10], and smart homes [17]. Despite the advantages of FL, the lack of oversight over the training process can have serious implications. For instance, a client may deviate from the normal training [23], negatively affecting the underlying model. The deviation may be caused by a malicious client who wants to disrupt training [19], or by a normal client who has insufficient resources [16, 30]. Malicious clients may introduce noise in the model to prevent convergence, as in byzantine attacks, or optimize for a secondary objective, e.g., a backdoor [34]. As the server lacks control over the clients, preventing their harmful actions can be challenging.\nThe issue of malicious or adversarial clients is a very active field of FL. Broadly, two common approaches exist: (i) detect and remove the clients deviating from normal behavior [23, 37] and (ii) mitigate the impact of the misbehaving clients via robust aggregation [4, 25]. For instance, the server in [14] excluded the contribution of colluding"}, {"title": "2 Preliminaries and Problem Description", "content": "In FL, the goal of the server is to find an optimal global model w* satisfying the optimization problem:\n$w^* = \\arg \\min_{w} F(w) = \\frac{1}{k} \\sum_{i \\in [k]} F_i(w)$.\nwhere $F_i(w)$ is the loss function of i-th client and $[k]$ is the set of all clients. To solve this optimization problem, at each communication round t, each client i computes a local update d through Stochastic Gradient Descent (SGD). The updates are then sent to the server which computes a global update $\\delta_{[k]} = \\sum_{i \\in [k]} d$ as the average of all the local updates and obtains a new global model as $w_{t+1} = w^* + \\delta_{[k]}$ for next round t + 1. To avoid overburdening the notation, from now on we omit the round superscript when clear from context."}, {"title": "2.1 Selfish Client", "content": "The optimization objective in Eq. (1) aims to learn a globally optimal model rather than providing an optimal model for each client. In non-IID settings, the local optima of each client might differ from the global one. Some intelligent clients might notice this difference and be interested in obtaining a global model closer to their local optima. We refer to these savvy clients as selfish in FL. Through their actions, the global model ceases to be optimal for the whole system.\nIt is worth noting that using a personalized model for each client [33] would ensure that each client obtains a locally optimal, for instance by training multiple models for different groups of similar clients [2]. However, since selfish clients try to influence the global model at inference time, personalization techniques are unsuitable for their goals, as local fine-tuning has no impact on the performance of the global model. Thus, in this work we focuses on the case where a single global model is trained for all clients, which is the most common FL setup.\nSelfish versus malicious clients: The notion of selfish clients completely differs from the malicious ones (adversaries) that are well defined in the literature [7]. Selfish behavior does deviate from normal behavior, but the objective is not explicitly in contrast with the global objective. Selfish clients lack nefarious intent, such as engaging in model poisoning attacks or preventing model convergence, as malicious clients do. Unlike backdoor attackers [34], selfish clients do not optimize the model for an additional objective different from the main task. The term \"selfish\" has been used in the context of FL in a complementary way in [24], where it refers to a server that strives to favor a subset of clients over others, which is the opposite problem to the one addressed in this work. Additionally, in certain works, \"selfish\" denotes typical malicious clients [36].\nFurther, in communication networks, selfish clients are those wishing to reap benefits from a system without contributing [29]. In FL, these clients are commonly defined as \"free-riders\" [13] who wish to obtain a model without engaging in the training process, exhibiting a completely different behavior than the one addressed in this work. A subset of researchers has utilized the term \"selfish\" to denote free-riders in their work [1, 32]. However, the problem tackled in these studies is unrelated to the one addressed in this paper. As far as we are aware, this study is the first to address a similar issue."}, {"title": "2.2 Problem Description", "content": "A selfish client $s \\in [k]$ mainly aims to craft local updates such that, after aggregating the local updates from all the clients, the global model incurs a lower loss on its local data. The client s can formulate the objective for their local update as\n$\\delta_s = \\arg \\min_{\\delta} \\{F_s(w + \\frac{\\delta_{[k] \\setminus \\{s\\}} + \\delta}{k})\\}$,\nwhere the input to the objective function $F_s(.)$ is not the global weights w but the crafted weights w and two additional additive terms. The first term $[k] \\setminus \\{s\\}$ involves knowing the sum of all the local updates of the rest of the clients, which is generally not known. The first term can be easily computed for any given k, but determining the optimal d is not straightforward.\nThough the selfish clients do not intentionally aim to pollute the training process, their actions are not innocent. When clients have non-IID data, the updates from selfish clients can considerably diverge the global model from the global optimum, causing performance degradation for normal clients.\nTo this end, the problem at hand is two-fold:\n1.  From selfish client perspective: solving the optimization problem in Eq. (2) given the global model weights w and k. Similar to [5], we assume clients know the total number of FL participants (k).\n2.  From server perspective: Alleviating the effect of selfishness on the global model through robust aggregation. Since selfish behavior differs from existing works on outliers and malicious clients, separate analysis and countermeasures are warranted.\nDue to the non-poisonous intent of the selfish clients, we can neither totally omit their updates from the aggregation nor include them directly, which makes the problem interesting and challenging compared to dealing with malicious clients."}, {"title": "3 Proposed Approach", "content": "To introduce selfish clients in the FL context, we first propose a novel way to estimate selfish model updates and then present a robust aggregation method to mitigate the effect of selfishness on the global model. This work considers two types of clients: normal and selfish. Each client has heterogeneous data and selfish clients do not collude."}, {"title": "3.1 Client Side: Computing Selfish Updates", "content": "To improve accuracy on its local data distribution, a selfish client needs to reduce the distance between global model updates $\\delta_{[k]}$ (see Eq. (2)) and the local model updates $\\delta_s$ trained on local data. To do"}, {"title": "3.2 Server Side: RFL-Self", "content": "When selfish clients participate in FL, the objective of the server is to identify them before aggregation to mitigate their impact on the global model. However, even if these clients are detected correctly, they should not be completely excluded from the aggregation. Doing so leads to a model that performs poorly for the users (not available during training) having data distribution similar to the selfish clients. The only viable solution for the selfishness issue is estimating the true updates of selfish clients and using them in aggregation instead of the ones received from selfish clients. This section proposes a robust aggregation strategy, RFL-Self, to remove the effect of selfishness from the training process. Each round, RFL-Self identifies suspected selfish clients and attempts to recover their true update."}, {"title": "3.2.1 Identifying Selfish Clients", "content": "Given a set of received updates from all the clients, RFL-Self first computes the L2 norm of each update and finds a median norm Nmed. Since a selfish client aims to increase its influence on the global model to deviate the training toward its local model, it is obvious that selfish updates are larger in magnitude than those from other clients. This intuition, confirmed by Theorem 1, suggests that for any client i, if $||d_i||_2 > N_{med}$ holds, then client i might be selfish. In this case, the received update $d_i$ would be the crafted $\\hat{d}_s$. This detection strategy is intentionally crafted to be simple and lightweight, but it is fully supported by the theoretical analysis in Theorem 1, and the experimental results confirm its effectiveness."}, {"title": "3.2.2 Recovering True Updates of Selfish Clients", "content": "Upon successful identification of the selfish clients, there are three possible ways in which the server can deal with them:\n*   Drop: exclude suspicious updates from the aggregation."}, {"title": "3.2.3 Robust Aggregation", "content": "Finally, by replacing the received updates of the set of suspected selfish clients S with the recovered ones, the server aggregates the updates as expressed below:\n$\\delta_{[k]} = \\frac{1}{k} (\\sum_{i \\in [k] \\setminus S} d_i + \\sum_{j \\in S} \\delta'_j)$\nLater, the global model for the next round is computed as $w = w + \\delta_{[k]}$. By substantially mitigating the effect of selfishness, our aggregation process offers a robust FL framework against the selfish participants while strategically utilizing their updates to achieve a more generalized global model."}, {"title": "3.2.4 Theoretical Guarantees", "content": "This section presents rigorous theoretical insights about the RFL-Self, focusing on the soundness of the detection and recovery mechanisms.\nTheorem 1. If the true update is similar in magnitude to the average update of the normal clients, then an effective estimated update of a selfish client is always larger in magnitude than the true update.\nProof. Let us assume that there exists some a such that $||\\hat{\\delta}_s||^2 = ||{\\delta}_s||^2$, making the norm of the estimated update indistinguishable from the true update. Such values of a can be determined by using Eq. (4). In the norm space, this condition can be written as\n$||a k d_s + (1 - ak) \\delta_{[k]\\setminus{s}} ||^2 = ||{\\delta}_s||^2$,\n$(ak||d_s||)^2 + 2ak(1-ak)\\langle{\\delta_s,\\delta_{[k]\\setminus{s}}}\\rangle + (1-ak)^2 ||{\\delta_{[k]\\setminus{s}}} ||^2 = ||{\\delta_s}||^2$,\nwhere $\\langle,\\rangle$ is the inner product. After solving the above, we get\n$\\alpha = \\frac{-\\langle d_s, \\delta_{[k] \\setminus \\{s\\}} \\rangle + \\sqrt{\\langle d_s, \\delta_{[k] \\setminus \\{s\\}} \\rangle^2 - (||d_s||^2 - ||\\delta_{[k] \\setminus \\{s\\}} ||^2)}}{k(||d_s||^2 - 2\\langle{\\delta_s,\\delta_{[k]\\setminus{s}}}\\rangle + ||{\\delta_{[k]\\setminus{s}}} ||^2)}$\nand\n$\\alpha = \\frac{-\\langle d_s, \\delta_{[k] \\setminus \\{s\\}} \\rangle - \\sqrt{\\langle d_s, \\delta_{[k] \\setminus \\{s\\}} \\rangle^2 - (||d_s||^2 - ||\\delta_{[k] \\setminus \\{s\\}} ||^2)}}{k(||d_s||^2 - 2\\langle{\\delta_s,\\delta_{[k]\\setminus{s}}}\\rangle + ||{\\delta_{[k]\\setminus{s}}} ||^2)}$  \nFor $\\alpha = 0$ the $\\hat{\\delta}_s = d_s$, i.e., the estimated update coincides with the true update. The other value for $\\alpha$ if $||{\\delta_s}||^2 \\sim ||\\delta_{[k] \\setminus \\{s\\}} ||^2$, corresponds to $\\alpha \\approx 0$, i.e. $\\hat{\\delta}_s \\sim {\\delta_{[k] \\setminus \\{s\\}}}$. Thus, the only way to avoid updates with a larger magnitude (larger norm) than the true ones, is by not behaving selfishly, while effective selfish updates are always larger in magnitude than the true updates.\nRemark. Theorem 1 does not ensure that all clients exhibiting larger update norms are necessarily acting selfishly. Consequently, RLF-Self can incur false positives. This aspect is taken into account in the recovery process, ensuring that the simplicity of the detection mechanism does not undermine performance."}, {"title": "4 Experimental Evaluation", "content": "This section empirically analyzes the impact of selfish clients on FL system performance and evaluates the effectiveness of our RFL-Self method. Under an image classification task, we use two benchmark datasets widely adopted to assess FL algorithms, MNIST [11] and CIFAR-10 [20], with 50 clients varying the level of selfishness a with up to 20% of the clients being selfish. We chose to assess RFL-Self on these datasets due to their widespread use as benchmarks for FL and their varying complexities. Larger datasets like CIFAR-100 were not included because, in non-IID settings, the increased diversity in data distribution compels selfish clients to transmit even larger updates. Consequently, detecting selfish clients using our method becomes straightforward. To simulate the non-IID scenario, we partition the dataset so that each client has data for two randomly selected classes, which is the most challenging setting [21], as few clients have overlapping classes. The tested non-IID conditions represent one of the most challenging scenarios in FL. In less severe non-IID conditions, the influence of selfish clients is expected to be less pronounced. Additional experiments on a smaller FL setup involving 5 clients are detailed in the supplementary material.\nExperimental setup: For the MNIST dataset, we train a CNN with 2 convolutional layers followed by 2 fully connected layers. For the CIFAR-10 dataset, the trained CNN has 3 convolutional layers instead. Our investigation of FL is focused solely on the perspective of selfishness, rather than maximizing accuracy. Therefore, these shallow models are suitable enough for our task. The hyper-parameters are optimizer: SGD, batch size: 128/256, and learning rate: 0.01 and 0.1 for MNIST and CIFAR-10 datasets, respectively. We train the models for 30 communication rounds, with five local epochs per round at each client, assuming full participation of all the clients in every round. All the experiments are implemented in Python using a well-known library PyTorch 1.12.1 [27] on a Windows 11 powered with an NVIDIA RTX A5000 GPU. The source code for this work is available at [3]."}, {"title": "4.1 Impact of Selfish Clients on Model Performance", "content": "At first, we carry out experiments to analyze the impact of selfish client(s) on the global model accuracy without using RFL-Self, and report results in Figure 6 for MNIST dataset. Similar results were discussed in Figure 3 for CIFAR-10 to ease the understanding of the context therein. Though the mean test accuracy (shown by 'green' box plots) across the normal clients keeps decreasing as the selfishness level a increases, it does not seem to favor the selfish clients. In part (b) of Figure 6, for a > 0.3 the two selfish clients also start losing accuracy. It is interesting to observe that in the presence of two selfish"}, {"title": "4.2 Performance of RFL-Self", "content": "Next, we evaluate the performance of RFL-Self in the presence of selfish clients and make a fair comparison with two standard strategies: median and downscaling. The median is also a robust aggregation strategy in which the median is used instead of the mean when aggregating, it is commonly used in FL systems to deal with outliers and byzantine clients [35], and is representative of the \"drop\" strategy that completely excludes the updates of the suspected selfish clients from the aggregation process. In the downscaling strategy, whenever a client is suspected to be selfish in a given round, instead of using Eq. (5), we scale each suspected update by $N_{med}/||\\delta_s||$. The authors in [16] adopted downscaling to deal with unreliable clients, here we use it as a representative of the \"mitigate\" strategy. While sophisticated malicious client detection mechanisms have been proposed in the literature, in our scenario, Theorem 1 proves that our criterion,"}, {"title": "4.3 Performance with More Selfish Clients", "content": "Finally, we investigate how the performance of the considered strategies changes by increasing the number of selfish clients with $\\alpha \\in \\{0.2, 0.3, 0.4\\}$. We choose these values of a with the help of part (b) of Figure 3, where at a = 0.2, the model achieves maximum accuracy even with two selfish clients and right after that it starts degrading and at a = 0.4, we notice a steep drop in accuracy. Table 1 reports the results on the CIFAR-10 dataset under an FL setup with 50 clients including the selfish clients (0% to 20%). On the CIFAR-10 dataset, at $\\alpha = 0.3$ with 10% selfish clients, the downscaling loses to our method by a substantial 4.2% margin (60.84-56.64). Regardless of the percentage of selfish clients, RFL-Self outperforms both downscaling and median and maintains a fair balance between the accuracy of normal and selfish clients. Notably, RFL-Self outperforms the other strategies even in the absence of selfish clients, supporting that it does not harm the performance of normal clients."}, {"title": "5 Conclusion", "content": "We introduced a novel notion of selfish clients who can deviate the overall FL training in their favor. From the server perspective, we proposed a robust aggregation strategy, RFL-Self, to mitigate the impact of these clients on the global model. With rigorous analysis, we established that selfish clients can severely affect the training process and potentially deviate it to no convergence point. By recovering true updates of selfish clients, the RFL-Self offered a strong robust aggregation strategy against selfishness. By conducting extensive empirical analysis using two benchmark datasets with varying levels of selfishness, we observed that RFL-Self can handle the selfishness without degrading the model accuracy for normal clients and it is superior to other standard strategies like downscaling and median. In the future, we plan to investigate adaptive selfishness, collusion among selfish clients, and the impact of selfish clients on fairness."}, {"title": "Appendix A: Experiments with 5 clients", "content": "To assess the impact of selfish behavior and the effectiveness of RFL-Self in scenarios with few participating entities (such as data silos), we perform the experimental evaluations on an FL setup with 5 clients. It is worth noting that, with so few clients, the selfish clients are the only ones holding data pertaining to their classes and it is thus crucial that any mitigation strategy manages to give good performance to all the clients. Moreover, as only 5 clients are considered, having 2 selfish clients in the system already represents a 40% concentration of misbehaving clients."}, {"title": "A1: Impact of selfishness on model performance", "content": "Figures 10 and 11 show that, with few clients, a selfish client can more easily improve its performance at the expense of other clients. Unsurprisingly, if $\\alpha < $ the selfish clients experience poor performances, as the global model receives no information pertaining to the classes in the selfish clients' datasets."}, {"title": "A2: Performance of RFL-Self", "content": "On the CIFAR-10 dataset, the results in Figure 13 are roughly similar to those reported for 50 clients. An interesting point to notice is that, in the case of two selfish clients, the median, albeit underperforming RFL-self, is a preferable alternative to the downscaling strategy, yielding better accuracy for normal clients as a increases.\nThe experiments on the MNIST dataset, reported in Figure 14, are more surprising. Indeed, mitigation strategies other than RFL-self"}, {"title": "Appendix B: Estimation of k", "content": "The soundness of the proposed solution relies on the following assumptions:\n1.  The average update of the normal clients $\\delta_{[k]\\setminus{s}}$ is approximately equal between consecutive communication rounds $\\delta^{\\text{St-1}}\n2.  The number of clients k is fixed at the beginning of the FL process and does not change over time.\nThe two assumptions make it possible for the selfish client to estimate the number of clients k in the FL system and their average update vector $\\delta_{[k]\\setminus{s}}$ by observing the past global update vectors.\nSpecifically, by taking into consideration the past n global update"}]}