{"title": "Articulation Work and Tinkering for Fairness in Machine Learning", "authors": ["MIRIAM FAHIMI", "MAYRA RUSSO", "KRISTEN M. SCOTT", "MARIA-ESTHER VIDAL", "BETTINA BERENDT", "KATHARINA KINDER-KURLANDA"], "abstract": "The field of fair Al aims to counter biased algorithms through computational modelling. However, it faces increasing criticism for perpetuating the use of overly technical and reductionist methods. As a result, novel approaches appear in the field to address more socially-oriented and interdisciplinary (SOI) perspectives on fair AI. In this paper, we take this dynamic as the starting point to study the tension between computer science (CS) and SOI research. By drawing on STS and CSCW theory, we position fair Al research as a matter of 'organizational alignment': what makes research 'doable' is the successful alignment of three levels of work organization (the social world, the laboratory and the experiment). Based on qualitative interviews with CS researchers, we analyze the tasks, resources, and actors required for doable research in the case of fair AI. We find that CS researchers engage with SOI to some extent, but organizational conditions, articulation work, and ambiguities of the social world constrain the doability of SOI research. Based on our findings, we identify and discuss problems for aligning CS and SOI as fair Al continues to evolve.", "sections": [{"title": "1 INTRODUCTION", "content": "Fair artificial intelligence (fair AI) has emerged as a novel research field for computer scientists and technologists to devise algorithmic interventions. The new field, in broad terms, is concerned with research that supports the development of trustworthy, responsible, ethical artificial intelligence (AI) [78, 79], by proposing novel methods to incorporate fairness notions into these systems, among other types of interventions.\nAs of the present moment, fair Al finds itself at an intermediary phase of disciplinary evolvement [33, 50]. Notably, fair Al is navigating internal dissent and external challenges about its future orientation [51, 77]. This dynamic is marked by the interplay of two contrasting research paradigms: one rooted in computer science (CS), the origin discipline of fair AI, and another that is more socially-oriented and interdisciplinary (SOI) [7, 10, 65]. On one side, there is a defense of mathematically rigorous fairness approaches by CS researchers\u00b9, and an urgent call to not discredit them \u00b2, particularly in light of an increasing presence of AI technologies. On the other side, extensive critiques persistently challenge CS-oriented fair AI research. This criticism often stems from the perception that these approaches view solutions to discrimination and social inequality through a techno-optimistic lens, relying on computational methods without sufficient engagement with the reality of a socially stratified and diverse society [19]. Scholars from various disciplines [3, 19, 76] have contributed to these critiques, contributing to the formation of the SOI paradigm. Boundaries"}, {"title": "Contribution.", "content": "The objectives of this paper are threefold. First, from a conceptual angle, we seek to reintroduce the longstanding and classical approach of articulation work by Fujimura [25], inspired by the work of Corbin and Strauss [18], into CS and CSCW. Despite Fujimura's significant contributions to the social studies of science\u00b3 and the concept's past significance for CSCW [64], we have noticed that it has somewhat faded from contemporary discussions. Notwithstanding, we believe it is still an important lens for CSCW and CS today because it enables a closer examination of research and its (shifting) constraints in times of the increasing uncertainty of academic labor [67]. It is also important because it foregrounds that mundane and tacit organizational efforts are always necessary if novel technologies (such as fair AI techniques) are to become integrated into already situated practices and organizational settings [28].\nThe concept of articulation work also applies to other areas of CS research, as well as to research in general. However, studying the case of fair Al is particularly interesting given the increasing role of social world actors, such as regulatory and policy bodies. Our second contribution is thus to provide a deeper understanding of fair Al research as the emergent product of effectively aligning one's research with these social world actors. We have personally experienced the effects that these actors can have on laboratory practices, and we consider it crucial to thoroughly examine the influence of such interactions.\nThird, we are acutely aware that all too often, there is far too little time for a meaningful reflection on our daily work. The insight that our research is contingent on what is \"doable\" may initially seem quite straightforward. However, it also raises the question of how we can potentially expand the scope of doability. Connecting this to the identified gap between CS and SOI, we thirdly contribute with a reflection about how SOI can become a more doable problem."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Algorithmic-decision systems that draw on AI and machine learning (ML) techniques are proposed and deployed for a myriad of tasks across domains, some with high-stakes implications, e.g., healthcare treatment allocation, credit assessment, or job suitability. Their implementation is supported in great part by the promise \"to bring greater discipline to decision-making\" [6]. However, alongside the potential societal advantages attributed to the use of these systems, their deployment presents a challenge to society: they can perpetuate, and render invisible, structural forms of discrimination, such as sexism, racism, classism, and ageism. Extensively researched instances of algorithmic discrimination [4, 14, 47, 51] and media coverage of cases of algorithmic bias4 5 6 have highlighted the imperative to detect and mitigate such harms.\nThis has been paralleled by a growth in publications about Al ethical principles and guidelines [37], the proposal of AI regulation across the globe [16, 22, 56, 81], and the flourishing of related research communities across all disciplines. In this specific conjuncture, the field of fair Al has expanded across differing AI and computational sub-fields.\nFair Al is currently situated at an \"intermediate stage of disciplinary evolvement\" [33, 50]. In this stage, there is a growing importance of SOI, placing CS research on fair Al more and more at the intersection of these two research paradigms. As Benbouzid [7] notes in regard to the increasing SOI demands for transparency, CS researchers \u201cface two difficult and often distinct types of demands: first, for reliable computational techniques, and second, for transparency, given the constructed, politically situated nature of quantification operations\".\nTransparency is a characteristic that is asked of fair Al research, however within our context it is only one of the many indicators of the sociopolitical dimensions of this type of research. To expand on our introductory definition, we\""}, {"title": "3 ARTICULATION WORK AND THE DOABILITY OF RESEARCH", "content": "Science encompasses more than knowledge, it involves work. This basic insight stems from research in CSCW and STS, that emerged in the early 1970s as a new way of thinking about science [60]. There are two distinct forms of scientific work. The first, known as production work, involves translating specific research interests into concrete research outcomes. This involves applying scientific methods, conducting experiments, analyzing data, and, in a general sense, engaging in thinking [25, 39]. On the other hand, there is a second type of work conceptualized as articulation work, also described as \"work to make work work\" [59, 63]. Articulation work involves daily tasks like planning, organizing, monitoring, evaluating, adjusting, coordinating, and integrating activities."}, {"title": "Facilitators and constraints.", "content": "Fujimura [25] examines three conditions of scientific research that facilitate or decrease articulation work. The leeway provided by available resources, and a clear division of labor facilitate articulating alignment. Conversely, uncertainty limits researchers' ability to plan. Consequently, more articulation of work is then needed to conduct research spontaneously. As for the PhD student, constraints to performing their xAI user study could be the unavailability of courses during the semester, or the unwillingness of potential interview participants to remain throughout the length of the study. The student also needs to complete a convincing research proposal to get the support of their supervisor, regularly correspond with the hosting PI, and engage and survey relevant literature to support their findings. Suppose the hosting PI does not see value in performing the study, or the supervisor faces time constraints, articulation work for the student increases significantly. If alignment with important social world actors is not possible, the research will not be doable."}, {"title": "Tinkering.", "content": "Constraints and uncertainties evoke tinkering, whereby scientists cope with difficult situations and unexpected problems [46]. Tinkering is an idiosyncratic, situated scientific practice in which local opportunities are made to work to solve a problem [39, 55]. Considering the scenario in which the supervisor initially hesitates due to time constraints, the PhD student might tactfully adjust the research timeline or propose more flexible arrangements. The student might also suggest using video updates to maintain engagement without the need for simultaneous, time-intensive meetings with the supervisor. Such small adjustments exemplify how tinkering can introduce creative solutions to unforeseen challenges.\nTo summarize, the doability of a research problem is contingent on the successful alignment across three levels of work organization, and shaped by facilitators and constraints, scientists' actions and positions [30, 31, 45]. If an"}, {"title": "4 METHOD AND DATA ANALYSIS", "content": "Selbst et al. [65] identify CS researchers working in fair Al as powerful. This is because the form a technology ultimately takes is shaped by the perspectives and practices of those that develop it: \"as fair ML researchers seek to define the 'best' approach to fairness, we also implicitly decide which problems and relevant social groups are important to include in this process. Our choices prioritize certain views over others, exerting power in ways that must be accounted for\" [65]. Not least for this reason, we study the gap between SOI and CS starting from the influential perspectives and practices of CS researchers.\nData collection and interview partners. Our qualitative study was completed within the context of an EU-funded project, dedicated to researching and developing interdisciplinary methods for fair Al systems. In addition to this particular affiliation, our interview partners were selected following these concise criteria: (i) they held a PhD in computer science, (ii) they held the role of PI in at least one research project, (iii) they worked on research topics about fair Al to varying degrees of involvement, (iv) they were based in different CS laboratories across different university departments or research centers. Our initial list was made up of twenty prospective participants, all identified through the directory of the project network. Over five months, we progressively contacted all of them via e-mail, introducing them to the general idea of the study, the methodology, and disclosing possible publication purposes.\nTo our interview requests, we received eleven positive answers, with ten following through with the interview. The resulting group was therefore made up of established, fairly senior CS researchers, who are currently actively involved in fair Al research in Europe. Table 1 displays relevant information about our interview partners, such as the institutional position, country of institution, and gender.\nEven though almost half of our cohort indicated they had been involved in fair Al research for a long time, some of our interview partners were relatively new to the field and even mentioned that fair AI research was not their primary area of expertise. In the latter instance, their CS sub-fields of principal research included computational social science, social network analysis, information retrieval, semantic web, or data mining, to name a few.\nTo further characterize the professional trajectory and disciplinary alignment of our interview partners, we performed a quantitative analysis of their scientific research. For this, we looked at a measure for scientific research output (i.e., h-index [34]) and performed an assessment of their scholarly literature in a broad sense. Regarding the inclusion and utility of the h-index, we acknowledge both the limitations and the criticism directed towards such measures. For that reason, our objective is only to provide an overview of the cumulative impact and perceived relevance of the scientific research output for all our interview partners in the last five years via a generally acknowledged indicator in academia. In Figure 2 we display the distribution of this index for our participants. In Figure 3, we also present a Sankey Diagram generated by executing the Python library pyBibX10. This visualization enables us to cumulatively analyze raw data files representing the individual author's profiles for all ten of our interview partners, sourced from the scientific database Scopus11. Concisely, the diagram illustrates the flow of 450 published works by our interview partners for the years 2009-2023, between the top 20 conferences and workshops the papers were accepted at and the research sub-areas the papers belong to. In addition to capturing this document flow, the analysis also makes it"}, {"title": "Interview process and qualitative data coding.", "content": "The completed qualitative study follows an episodic narrative interview approach [52]. This method is used to improve the understanding of a phenomenon, in our case fair AI, by inviting the interview partners to tell their individual stories of life and work experience around said phenomenon. The interview format allows interview partners enough time for self-reflection and for the opportunity to share in richer detail their direct experiences. Collected data is thus useful to uncover the narratives around fair AI and related activities, which then require analysis and interpretation [68]. Our interviews were held throughout the year 2022, with the duration of each interview ranging from one to one and a half hours. Following every interview, the recordings were manually transcribed and pseudonymized. All personal identifiers were removed from our interview data and all our interview partners are referred to by pseudonyms i.e., R1, R2, etc. Subsequently, the interview material was interpreted using an explorative approach, informed by an inductive analysis that facilitates the possibilities of finding novel and"}, {"title": "Research Ethics.", "content": "All participants in our study were provided with and acknowledged a consent form before the interviews. The document outlined the objectives of our project, the data collection process, transcription, and analysis procedures. It explicitly highlighted voluntary participation, emphasizing participants' right to terminate interviews at any point (reiterated before the interview). We conducted all interviews through an open-source video conferencing platform hosted on the server of one of our academic institutions. Furthermore, all data was stored on servers located in Europe, aligning with the regulations of the EU General Data Protection Regulation (GDPR) legislation."}, {"title": "5 FINDINGS", "content": "Our findings are structured into three subsections. We start with two questions: First, what does the fair Al world want? Second, zoomed in on the CS lab, what do CS researchers do? Of course, the fair Al world also 'does things', and CS researchers also 'want things'. Yet in this particular case, our focus lies on the (SOI) demands of the fair Al world to understand whether and how they align with what is done in the situated practice of the CS lab. In the last subsection, we further zoom into researchers' micro-strategies for including and tinkering with SOI."}, {"title": "5.1 Social worlds: what does the fair Al world want?", "content": "In this section, we introduce the most prominent actors in the social world of fair AI that we identified in our interviews. We refer to it herein as the fair Al world. Our first finding is that there are three important collective actors: the CS community, project partners and regulatory bodies. These actors display an increasing orientation towards the SOI paradigm, but with different needs, expectations, and interpretations attached to it.\nCS community. The first significant actor that emerged in the interviews is the CS community. However, there was a time when our early-contributors to fair AI found it challenging to align their research with the then-research interests of the CS community. Particularly, the CS community was mainly concerned with research problems that tackled scalability or efficiency, as retold by R8, a computer science professor at an Italian university. It was, especially"}, {"title": "Project partners.", "content": "The second significant actor was the 'project partner'. Typically, we found a project partner to be a private entity, and in our interviews, banks, credit agencies, and insurance companies were the ones most frequently mentioned. Public project partners were institutions such as hospitals, police, public broadcasters, or other research institutions. While there are many challenges for researchers in applied partnerships due to, for example, privacy concerns or licensing disputes [11, 35], private and public actors were perceived as particularly valuable because they can be a source of funding, interesting cases, datasets and models to study fairness. The latter was understood to be important for fair Al research because it required societal datasets containing information about people. A successful alignment with such data-providing partners was termed \u2018real-world' research, and demonstrating real-world impact, was considered to be exceptionally relevant by our interview participants.\n\u201cI will talk about this project because it is one of the latest and it's still running and it is a real project, so I'm proud, compared to projects which are theoretical, lab-based.\" (R8)\nOur interview participants attentively observed their project partners' (and, in alignment with this, also their experiments and CS laboratories') increasing orientation towards fairness and ethics in general, with R2 describing it as a paradigm shift.\n\"I actually see this as kind of paradigm shift. Several years back when I was a young researcher, ethical aspects in our kind of research were kind of an afterthought and a necessary evil but right now it's kind of the first thing to consider and it's always there\" (R2).\nOther interview participants expressed a more negative perspective, characterizing an orientation towards fairness as a competitive advantage for institutions, that allowed them to strategically position themselves as socially responsible and attentive to the societal impact of their Al systems. They had witnessed instances where project partners guided\""}, {"title": "Regulatory bodies.", "content": "Regulatory bodies, including various institutions within the EU, emerged as the third important social world actor for our interview participants. Not only did they serve as (novel) funding bodies for fair Al research, but it was expected that they would also progressively be taking on roles as project partners themselves. Across the globe, proposals for fair and responsible AI are being introduced and debated, with the European Union's AI Act being the first of its kind with regulatory enforceability in the EU [16, 22, 56, 81].\nThe majority of our interview partners anticipated advantages for their research once such regulations were implemented. For instance, R1 emphasized the necessity for \"some common ground where you can stand on, and these are standards\". In a similar vein, R8 suggestd standardization bodies like the International Organization for Standardization (ISO) to control and audit AI development. He also proposed compensation mechanisms for \u201cpeople who have been harmed by such systems\", and the introduction of general reporting mechanisms. These reporting systems would, as R8 explained, enable potentially disadvantaged individuals to denounce and record their cases.\nDespite this predominantly expressed desire for standards and reporting mechanisms, at the time of the interviews, our participants mainly engaged with regulatory bodies through the perspective of their project partners. R8 noted how current regulatory attempts shifted his project partners' interest towards identifying and mitigating algorithmic bias. He had observed a similar pattern in the past regarding (computational) privacy issues, wherein significant interest in the topic only emerged after the implementation of regulations.\n\"I have to say that the [company's] interest in these [fairness] techniques was motivated by the rules, by the law. This is a general behaviour, for instance, [if you] think about privacy twenty years ago, nobody cared about privacy issues until the regulations came out.\" (R8)\nA similar viewpoint was shared by R7, regarding a project with an insurance company to build risk models and implement fairness techniques.\n\u201cNow, their interest is, as far as I can see, not per se because they want to make what is currently there, fair. They do not necessarily consider it unfair, let's put it that way. But they want to be prepared and even steer the discussion if at a certain point legislation may come that will force them to become, to obey some fairness measures.\" (R7)\""}, {"title": "5.2 Laboratories and experiments: what do CS researchers do?", "content": "In this section, we zoom in on the level of the CS lab. We elaborate on the different tasks and factors that interview participants gathered to do fair Al research. We found that the organizational conditions of the lab steered researchers' attention towards other, more pragmatic concerns.\nProjects and staff. At the level of the lab, our interview participants were often and understandably occupied with more pressing tasks and concerns that did not necessarily pertain to tending to the broader social claims that motivated their research. They needed to spend most of their time dividing their tasks within projects, acquiring new projects, and strategically pursuing new projects with partners to gain access to new resources. For instance, R5 conveyed that her current research projects on fair AI also offered secure employment opportunities.\n\"To be honest, it's not really about the projects. Right? So the projects are just a funding tool, right?\" (R5)\nIn a similar vein, fair Al projects were also a way to secure positions for preceding and younger researchers, and R6 remarked that the fair Al research projects he was currently working on were also important because they secured his students' future positions in research. This is not to say that their research was not important to our interview participants, but, that on top of it, fair Al work had other dimensions beyond the undertaking of ethical and social justice-oriented claims. A fair Al laboratory is a workplace and also a site of career building for people. We witnessed that this reality manifested through practices such as placing staff on projects due to pre-existing working contracts or attempting to redefine work, to make it fit to existing knowledge and skills of the staff in the lab.\n\"So you were sitting together with the configuration of people that is somehow given because they are there, and they are supposed to work now on something.\" (R4)\nApart from shouldering the responsibility to keep the staff in their labs employed, some of our interviewed participants also saw how tending to that created an unexpected incongruity. Here, we refer to the call to integrate diversity and inclusion (D&I) considerations in Al systems, and how in pertaining to problems of unfairness and bias, these can be"}, {"title": "Limited resources.", "content": "Project-based funding mechanisms also provided a temporary constraint to the distribution of resources, as projects' duration was always limited, typically spanning three to five years. Such time constraints therefore called for savvy project management skills, which would be better reflected towards the end of a project life cycle. Inadvertently, this led to prioritizing research problems that contributed to achieving project objectives on time, but that perhaps required \"less effort\" from an experimental setup and execution point of view. For instance, although some interview participants attempted to set up user studies in their fair Al research, they expressed dissatisfaction as the actual engagement with users remained limited due to time constraints. R2 emphasized the typicality of such situations.\n\u201cI think this is quite typical for research projects that you don't get as far as to involve a real end user with your research software. Sometimes you manage to set up a kind of experimental setup but yeah I cannot think of any example that would have involved this [...] idea.\" (R2)\nSome fair Al research problems, that were not deemed as required or necessary for successful project completion, were thus postponed, or not conducted at all."}, {"title": "Noise and (other) papers.", "content": "According to our interview participants, the first step of the acquisition of a project involved the conception of an idea, followed by the publication of a corresponding paper. We witnessed that a published scientific paper also served as a legitimization of a novel research problem because it provided a first proof of doability. R6, among the well-experienced and influential researchers, described this legitimizing role of scientific papers as follows:\n\"I mean typically, the story is that you're interested in an area, you try to have maybe like a first initial paper and then you get funding.\" (R6)\nYet, our interview participants often grappled with keeping up with the overwhelming amount of new scientific papers and information that emerged in the fair Al research area. When we asked about their primary tasks, we heard that a significant amount of time was allotted to following, reviewing, and reading the numerous scientific papers, to remain informed and up-to-date.\n\"Still one big point is really to figure out what other people do in this topic. It's currently a hype topic and if it is a hype topic you have more publications than you can ever read.\" (R2)\nArticulation work encompassed not only the task of monitoring and reading the papers that were published but also the discernment of which papers were merely contributing to such 'noise', e.g. \u201cgo[ing] on and on about the same things\" (R10), and those significant for fair AI research. In that regard, early-contributor R7 stated that \"as soon as a particular few become hyped, over-hyped, many people jump on it and they aim to publish rather than to do something useful\" (R7).\""}, {"title": "Production work.", "content": "We now focus on the production work of fair Al research, and related experiments. Early-contributors recounted that their first experiments in fair Al research were conducted on an ad-hoc basis, such as training a classifier on a biased dataset to experiment with the results. After obtaining results, they would convince their colleagues (and themselves) that fairness was a relevant research problem by putting a valuable object in the foreground: the (biased) dataset.\n\"And it was one of those datasets that they used and that they showed that you could find [...] subgroups of people that were discriminated against.\" (R5)\nAlong with the dataset, other objects were placed in the foreground for production work. Over the past years, numerous fairness metrics and other bias mitigation techniques had been defined and developed, making it possible to \"package\" [25] them into fairness toolkits [5]. These collections of metrics, methods, and practical instructions were intended to ease their use in research, but more importantly, streamline their use in industry, where often inexperienced practitioners were the ones tasked with building and 'de-biasing' ML systems [5, 21]. These objects thus facilitated alignment between the CS community and their project partners. And while the use of fairness metrics allowed for measuring, quantifying and comparing the output of models, at the same time, they could instill the idea that only what can be captured by such metrics should be defined and identified as fair (or unfair).\nR9, a professor at a university in Belgium and another early-contributor to the field, emphasized that the primary objective of fair Al research should be to focus on model accuracy by \"concentrat[ing] mainly on the distorted vision that is there [in the data used]\". Accuracy, for him, was part of addressing fundamental computational and statistical questions that the CS lab was supposed to pose, and not \u201cpolitical ones\u201d. With this demarcation approach, interview participants like R9 suggested that CS can and should be more autonomous from socially-oriented research and political actors. While such interpretations of the role of technology could be considered clear deviations from an SOI approach, we recalled that a focus on datasets and on computationally relevant questions such as accuracy had allowed for alignment with the initially dismissive CS community. In this way, fair Al research could now be \"stored\" [71] (e.g. in a dataset, in a computer, in a metric), shared across different levels of work organization, and made comparable. Such practices of storing, sharing and comparing, further facilitated alignment with other social world actors. For example, the creation of fairness metrics allowed for project partners to request that they be implemented into their own systems and provided a potential method to communicate standards adherence to regulatory bodies.\""}, {"title": "5.3 Openness to the SOI paradigm and tinkering", "content": "We conclude our findings by pointing out some interview participants' micro-strategies and attempts to integrate some SOI perspectives in their everyday research practices.\nOpenness to the SOI Paradigm. Some of our interview participants, notably the two professors, R4 and R5, both situated in German universities along with R8, the professor from an Italian university, and R10, a professor from a UK university, expressed their wish for fair Al research to become more inclusive of SOI perspectives. Some of them acknowledged their own agency and responsibility in attaining this. In particular, R10 emphasized how he tried to negotiate and shape the conditions of his CS lab to align with a more inclusive research approach. This, as he explained, included actively taking responsibility for conducting more research with interdisciplinary actors, such as scholars from the social sciences. As a senior researcher, he had experienced that such collaborations did not emerge spontaneously or by themselves but required proactive efforts by researchers, such as to \u201cnegotiate and have a more active role in your occupation to be able to keep the things that you're happy with.\u201d (R10)\nR5 invoked that the CS community must learn to perceive and do fair Al research differently than \"just another machine learning problem.\" On the level of the lab and everyday research, this would include more reflection on fundamental questions in contrast to the more practical and technical aspects encountered in everyday tasks, for instance about the significance of the gender binary within datasets.\n\"Maybe thinking [of] fundamental questions again and kind of thinking from scratch or with a new perspective might be useful. I don't know how this perspective can come. I think looking at [fairness] as another machine learning problem is not a new perspective.\" (R5)\nWe further witnessed how several other researchers, such as R1 from the German research lab, R2 from the Greek university, and R7 from the Dutch university displayed some surface-level engagement with SOI actors in their research. They also called for greater inclusion of SOI actors, particularly, those people most affected (and potentially harmed) by a given Al system. In that regard, they all noted how \u201cthe user\u201d13, as they described people at the receiving end of AI and ML models, had become more important. Alignment with the user happened mostly through their representation by civil society organizations in collaborative research projects. For example, R2 described one of his projects, where partners included government-affiliated (disabled) product user organizations. However, as previously detailed, user studies were still perceived as an additional task rather than an inherent step of CS research.\""}, {"title": "Summary.", "content": "Our findings show that there was a desire to accomplish impactful (fairness) research that coexisted with varying degrees of integration of SOI into everyday research practices. Even researchers with a more critical stance towards SOI demonstrated some (surface-level) engagement with it. Yet, due to the organizational constraints of the CS lab, SOI was adjusted or modified, e.g. tinkered with, to sidestep additional articulation work."}, {"title": "6 DISCUSSION", "content": "Bringing Fujimura's study in dialogue with our findings, we now discuss the gap between CS and SOI as a problem of 'organizational alignment'.\nDoing fair AI because it works. Our first argument is that computational researchers focus on the CS paradigm of fairness largely \"because it works\", as to quote Knorr-Cetina [40]. On the contrary, adopting the SOI paradigm would necessitate acquiring new material resources and staff, reconfiguring publication venues, and dedicating additional time to novel methodological processes.\nIn the case of biomedical cancer research, as elaborated in Section 3, Fujimura [25] identifies two facilitators of making a problem doable (and thus, decreasing articulation work). The first facilitator is the leeway provided by available resources, such as equipment, space, technology, time, staff, and skills that need to be available for researchers to enable a working routine. Second, a clear division of labor, such as clear task-person and task-organization divisions, decreases articulation work, e.g. work of deciding who should do what, when, and how.\nIn our study, we identified similar resources as crucial for making the CS paradigm of fairness research work. Much like in Fujimura's observations, computational researchers pragmatically leveraged comparable resources for fairness research just as they would for other research problems. This included acquiring funding and utilizing technologies, staff and skills similar to how they approached \"other ML problems\" (R5). What partly set fairness research apart from other ML problems was its particular reliance on societal data that presented compelling societal cases (or 'real-world scenarios'). Computational researchers encountered challenges in legitimizing fairness research without access to such data, highlighting that technical resources shaped what was (still) considered a meaningful advancement in the fair AI field."}, {"title": "Uncertainties and constraints of fair AI.", "content": "As Fujimura [25] highlights, uncertainty increases articulation work as it constrains researchers' possibilities to plan. In our case study, uncertainties posed constraints on computational researchers. The 'projectification' of research, with limited time frames, created general challenges for researchers in planning for the long-term. Information overload (e.g. 'noise') added to these uncertainties, as researchers were challenged to decide on their own accord which contributions to the field were important enough to extend, incorporate or reference in their work. In that regard, the lack of regulatory and technical standards (e.g. benchmarks, toolkits,"}, {"title": "Making SOI (more) doable.", "content": "If uncertainties and burdensome articulation work prevented computational researchers from fully engaging with SOI, how can SOI become a 'doable problem' for them? From our theoretical standpoint, we now examine this question for each of the three organizational levels.\nThe doability of a research problem greatly relies on the positions of the social worlds. Fujimura [25] and Clarke and Star [15] have noted that at times, there may be multiple intersecting social worlds, each with its own interests. This can increase the complexity of the articulation work required. As previously discussed, the fair Al world faces new emerging actors, such as regulatory bodies, becoming more important. Furthermore, different interpretations within social worlds existed regarding what is considered doable. This does not necessarily imply that fairness loses credibility because it is \"empty\" [38, 43, 48], but that it can rather hold varying significance for different individuals (e.g., a so-called 'boundary object' [72]). From this perspective, the much-requested call for standards, as sometimes indicated by our respondents, could potentially hinder interdisciplinary collaborations and alignment with diverse social worlds. Perhaps instead of such ethical or regulatory standards on fairness for computational researchers to translate into local (lab) practices, we need more alignment between different social worlds (e.g. CS community, regulatory bodies, companies, marginalized communities). This could include explicit stances from the social worlds' actors acknowledging the SOI paradigm of fairness and making the SOI perspective an integral part of what is deemed as \"success\" [40].\nOn the level of the laboratory, the criteria of how research is evaluated, what counts as a successful publication, and which venues are treated as important, could be re-evaluated. The success of research is still mostly defined via publications (and within these, the 'weight' relies on rigid and decontextualized hierarchies of venue quality). The quest for other forms of visibility is, and will remain, an open problem for both scientometrics theory and academic institutions' practices. Academic institutions' ongoing developments of evaluation and indicator systems (which are, to some extent, inherently self-defeating) continues14. One potential approach to follow is the inclusion of meaningful SOI related goals into project proposals, so that project timelines explicitly account for research initiatives such as thorough user studies. To the same effect, it is also important to account for the fact that an enhanced SOI focus of a project will likely also require an enhanced role for SOI actors within the project.\nWe have made the point that much of what drives the current situation is a need to limit articulation work for overburdened (computational) researchers. Therefore, we acknowledge that suggesting changes on the individual or experimental level could actually increase articulation work. However, we need alternative strategies that individual researchers can take if they wish to impact the kind of research that gets done. One strategy that we propose is taking a conscious awareness of how we allocate time to various tasks and a thoughtful consideration of how tasks are driven by optimizing productivity and efficiency. From our own experience in collaborating for this paper, we would like to emphasize that interdisciplinary alignment takes a considerable amount of time and that there are misses along the"}, {"title": "7 LIMITATIONS AND FUTURE RESEARCH", "content": "We hope that our study opens some novel avenues for investigating the evolvement of fair Al from the perspective of the CS researchers involved in it. Yet, our study is limited to only a small group of researchers. While we were especially interested in the perspectives and experiences of these computational researchers, this can reduce the scope of our work, and hence limit the generalizability of the claims made regarding CS laboratories at large. Future research could integrate the perspectives of other relevant actors in the laboratory. Beyond CS, the doability of fair Al depends on the collaborative efforts of engineers, data scientists, ethicists, research group leaders or IT experts who may be all situated in the (same) laboratory. Further research could investigate these micro-level actors' interactions with each other and meso-level actors more closely.\nSecond, we would find it interesting to further systematically explore our interviews regarding our respondents' definitions of \"fair AI\" compared to a rather more flexible and open definition adopted while talking about their ongoing research projects and professional routines. Related to that, we were"}]}