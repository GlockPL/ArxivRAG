{"title": "Articulation Work and Tinkering for Fairness in Machine Learning", "authors": ["MIRIAM FAHIMI", "MAYRA RUSSO", "KRISTEN M. SCOTT", "MARIA-ESTHER VIDAL", "BETTINA BERENDT", "KATHARINA KINDER-KURLANDA"], "abstract": "The field of fair Al aims to counter biased algorithms through computational modelling. However, it faces increasing criticism for perpetuating the use of overly technical and reductionist methods. As a result, novel approaches appear in the field to address more socially-oriented and interdisciplinary (SOI) perspectives on fair AI. In this paper, we take this dynamic as the starting point to study the tension between computer science (CS) and SOI research. By drawing on STS and CSCW theory, we position fair Al research as a matter of 'organizational alignment': what makes research 'doable' is the successful alignment of three levels of work organization (the social world, the laboratory and the experiment). Based on qualitative interviews with CS researchers, we analyze the tasks, resources, and actors required for doable research in the case of fair AI. We find that CS researchers engage with SOI to some extent, but organizational conditions, articulation work, and ambiguities of the social world constrain the doability of SOI research. Based on our findings, we identify and discuss problems for aligning CS and SOI as fair Al continues to evolve.", "sections": [{"title": "1 INTRODUCTION", "content": "Fair artificial intelligence (fair AI) has emerged as a novel research field for computer scientists and technologists to devise algorithmic interventions. The new field, in broad terms, is concerned with research that supports the development of trustworthy, responsible, ethical artificial intelligence (AI) [78, 79], by proposing novel methods to incorporate fairness notions into these systems, among other types of interventions.\nAs of the present moment, fair Al finds itself at an intermediary phase of disciplinary evolvement [33, 50]. Notably, fair Al is navigating internal dissent and external challenges about its future orientation [51, 77]. This dynamic is marked by the interplay of two contrasting research paradigms: one rooted in computer science (CS), the origin discipline of fair AI, and another that is more socially-oriented and interdisciplinary (SOI) [7, 10, 65]. On one side, there is a defense of mathematically rigorous fairness approaches by CS researchers\u00b9, and an urgent call to not discredit them 2, particularly in light of an increasing presence of AI technologies. On the other side, extensive critiques persistently challenge CS-oriented fair AI research. This criticism often stems from the perception that these approaches view solutions to discrimination and social inequality through a techno-optimistic lens, relying on computational methods without sufficient engagement with the reality of a socially stratified and diverse society [19]. Scholars from various disciplines [3, 19, 76] have contributed to these critiques, contributing to the formation of the SOI paradigm. Boundaries"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Algorithmic-decision systems that draw on AI and machine learning (ML) techniques are proposed and deployed for a myriad of tasks across domains, some with high-stakes implications, e.g., healthcare treatment allocation, credit assessment, or job suitability. Their implementation is supported in great part by the promise \"to bring greater discipline to decision-making\" [6]. However, alongside the potential societal advantages attributed to the use of these systems, their deployment presents a challenge to society: they can perpetuate, and render invisible, structural forms of discrimination, such as sexism, racism, classism, and ageism. Extensively researched instances of algorithmic discrimination [4, 14, 47, 51] and media coverage of cases of algorithmic bias4 5 6 have highlighted the imperative to detect and mitigate such harms.\nThis has been paralleled by a growth in publications about Al ethical principles and guidelines [37], the proposal of AI regulation across the globe [16, 22, 56, 81], and the flourishing of related research communities across all disciplines. In this specific conjuncture, the field of fair Al has expanded across differing AI and computational sub-fields.\nFair Al is currently situated at an \"intermediate stage of disciplinary evolvement\" [33, 50]. In this stage, there is a growing importance of SOI, placing CS research on fair Al more and more at the intersection of these two research paradigms. As Benbouzid [7] notes in regard to the increasing SOI demands for transparency, CS researchers \u201cface two difficult and often distinct types of demands: first, for reliable computational techniques, and second, for transparency, given the constructed, politically situated nature of quantification operations\".\nTransparency is a characteristic that is asked of fair Al research, however within our context it is only one of the many indicators of the sociopolitical dimensions of this type of research. To expand on our introductory definition, we"}, {"title": "3 ARTICULATION WORK AND THE DOABILITY OF RESEARCH", "content": "Science encompasses more than knowledge, it involves work. This basic insight stems from research in CSCW and STS, that emerged in the early 1970s as a new way of thinking about science [60]. There are two distinct forms of scientific work. The first, known as production work, involves translating specific research interests into concrete research outcomes. This involves applying scientific methods, conducting experiments, analyzing data, and, in a general sense, engaging in thinking [25, 39]. On the other hand, there is a second type of work conceptualized as articulation work, also described as \"work to make work work\" [59, 63]. Articulation work involves daily tasks like planning, organizing, monitoring, evaluating, adjusting, coordinating, and integrating activities.\nAccording to Fujimura [25]'s ethnographic research in the field of biomedical cancer research, articulation work is crucial for rendering a scientific problem doable. Doability is achieved by establishing alignment across three levels: the experiment, the laboratory, and the social world [18, 74]. On a micro level, the experiment encompasses a set of tasks conducted within the laboratory. The laboratory serves as the physical space for experiments and related activities. Expanding to the macro level, the social world is the broader context where experiments and laboratories are situated. The social world is the arena where all collective and individual actors involved in a specific research problem interact. A social world is characterized by specific activities, sites and requirements, and existing or new technologies that enable the fulfillment of the social world's requirements [15, 73].\nTo provide a clear understanding of these abstract concepts, let us imagine a PhD student in a computer science laboratory (CS lab). The PhD student wants to validate their explainable ML (XAI) framework with a user study in collaboration with a principal investigator (PI) who is an expert in user research. The student successfully gathers several factors that are situated at the experiment and laboratory level, e.g., attending user experience design courses, designing the study, recruiting interview participants, and obtaining a recording device and software for transcriptions. In alignment with the social world, the student uses a common explainability technique acknowledged by the CS community. The CS community also supports user studies as a valid method for assessing the interpretability of explanations. Further, the student's research connects to other academic disciplines such as social sciences and psychology, and civil society groups' concerns on opaque algorithms. The student's efforts to combine all these factors, and connect them to the social world's requirements, is thus what makes their research doable. Over time, the student's research objectives can also change to maintain alignment [28].\nFacilitators and constraints. Fujimura [25] examines three conditions of scientific research that facilitate or decrease articulation work. The leeway provided by available resources, and a clear division of labor facilitate articulating alignment. Conversely, uncertainty limits researchers' ability to plan. Consequently, more articulation of work is then needed to conduct research spontaneously. As for the PhD student, constraints to performing their xAI user study could be the unavailability of courses during the semester, or the unwillingness of potential interview participants to remain throughout the length of the study. The student also needs to complete a convincing research proposal to get the support of their supervisor, regularly correspond with the hosting PI, and engage and survey relevant literature to support their findings. Suppose the hosting PI does not see value in performing the study, or the supervisor faces time constraints, articulation work for the student increases significantly. If alignment with important social world actors is not possible, the research will not be doable.\nTinkering. Constraints and uncertainties evoke tinkering, whereby scientists cope with difficult situations and unexpected problems [46]. Tinkering is an idiosyncratic, situated scientific practice in which local opportunities are made to work to solve a problem [39, 55]. Considering the scenario in which the supervisor initially hesitates due to time constraints, the PhD student might tactfully adjust the research timeline or propose more flexible arrangements. The student might also suggest using video updates to maintain engagement without the need for simultaneous, time-intensive meetings with the supervisor. Such small adjustments exemplify how tinkering can introduce creative solutions to unforeseen challenges.\nTo summarize, the doability of a research problem is contingent on the successful alignment across three levels of work organization, and shaped by facilitators and constraints, scientists' actions and positions [30, 31, 45]. If an"}, {"title": "4 METHOD AND DATA ANALYSIS", "content": "Selbst et al. [65] identify CS researchers working in fair Al as powerful. This is because the form a technology ultimately takes is shaped by the perspectives and practices of those that develop it: \"as fair ML researchers seek to define the 'best' approach to fairness, we also implicitly decide which problems and relevant social groups are important to include in this process. Our choices prioritize certain views over others, exerting power in ways that must be accounted for\" [65]. Not least for this reason, we study the gap between SOI and CS starting from the influential perspectives and practices of CS researchers.\nData collection and interview partners. Our qualitative study was completed within the context of an EU-funded project, dedicated to researching and developing interdisciplinary methods for fair Al systems. In addition to this particular affiliation, our interview partners were selected following these concise criteria: (i) they held a PhD in computer science, (ii) they held the role of PI in at least one research project, (iii) they worked on research topics about fair Al to varying degrees of involvement, (iv) they were based in different CS laboratories across different university departments or research centers. Our initial list was made up of twenty prospective participants, all identified through the directory of the project network. Over five months, we progressively contacted all of them via e-mail, introducing them to the general idea of the study, the methodology, and disclosing possible publication purposes.\nTo our interview requests, we received eleven positive answers, with ten following through with the interview. The resulting group was therefore made up of established, fairly senior CS researchers, who are currently actively involved in fair Al research in Europe. Table 1 displays relevant information about our interview partners, such as the institutional position, country of institution, and gender.\nEven though almost half of our cohort indicated they had been involved in fair Al research for a long time, some of our interview partners were relatively new to the field and even mentioned that fair AI research was not their primary area of expertise. In the latter instance, their CS sub-fields of principal research included computational social science, social network analysis, information retrieval, semantic web, or data mining, to name a few.\nTo further characterize the professional trajectory and disciplinary alignment of our interview partners, we performed a quantitative analysis of their scientific research. For this, we looked at a measure for scientific research output (i.e., h-index [34]) and performed an assessment of their scholarly literature in a broad sense. Regarding the inclusion and utility of the h-index, we acknowledge both the limitations and the criticism directed towards such measures. For that reason, our objective is only to provide an overview of the cumulative impact and perceived relevance of the scientific research output for all our interview partners in the last five years via a generally acknowledged indicator in academia. In Figure 2 we display the distribution of this index for our participants. In Figure 3, we also present a Sankey Diagram generated by executing the Python library pyBibX10. This visualization enables us to cumulatively analyze raw data files representing the individual author's profiles for all ten of our interview partners, sourced from the scientific database Scopus11. Concisely, the diagram illustrates the flow of 450 published works by our interview partners for the years 2009-2023, between the top 20 conferences and workshops the papers were accepted at and the research sub-areas the papers belong to. In addition to capturing this document flow, the analysis also makes it"}, {"title": "5 FINDINGS", "content": "Our findings are structured into three subsections. We start with two questions: First, what does the fair Al world want? Second, zoomed in on the CS lab, what do CS researchers do? Of course, the fair Al world also 'does things', and CS researchers also 'want things'. Yet in this particular case, our focus lies on the (SOI) demands of the fair Al world to understand whether and how they align with what is done in the situated practice of the CS lab. In the last subsection, we further zoom into researchers' micro-strategies for including and tinkering with SOI.\n5.1 Social worlds: what does the fair Al world want?\nIn this section, we introduce the most prominent actors in the social world of fair AI that we identified in our interviews. We refer to it herein as the fair Al world. Our first finding is that there are three important collective actors: the CS community, project partners and regulatory bodies. These actors display an increasing orientation towards the SOI paradigm, but with different needs, expectations, and interpretations attached to it.\nCS community. The first significant actor that emerged in the interviews is the CS community. However, there was a time when our early-contributors to fair AI found it challenging to align their research with the then-research interests of the CS community. Particularly, the CS community was mainly concerned with research problems that tackled scalability or efficiency, as retold by R8, a computer science professor at an Italian university. It was, especially"}, {"title": "5.2 Laboratories and experiments: what do CS researchers do?", "content": "In this section, we zoom in on the level of the CS lab. We elaborate on the different tasks and factors that interview participants gathered to do fair Al research. We found that the organizational conditions of the lab steered researchers' attention towards other, more pragmatic concerns.\nProjects and staff. At the level of the lab, our interview participants were often and understandably occupied with more pressing tasks and concerns that did not necessarily pertain to tending to the broader social claims that motivated their research. They needed to spend most of their time dividing their tasks within projects, acquiring new projects, and strategically pursuing new projects with partners to gain access to new resources. For instance, R5 conveyed that her current research projects on fair AI also offered secure employment opportunities.\nLimited resources. Project-based funding mechanisms also provided a temporary constraint to the distribution of resources, as projects' duration was always limited, typically spanning three to five years. Such time constraints therefore called for savvy project management skills, which would be better reflected towards the end of a project life cycle. Inadvertently, this led to prioritizing research problems that contributed to achieving project objectives on time, but that perhaps required \"less effort\" from an experimental setup and execution point of view. For instance, although some interview participants attempted to set up user studies in their fair Al research, they expressed dissatisfaction as the actual engagement with users remained limited due to time constraints. R2 emphasized the typicality of such situations.\nNoise and (other) papers. According to our interview participants, the first step of the acquisition of a project involved the conception of an idea, followed by the publication of a corresponding paper. We witnessed that a published scientific paper also served as a legitimization of a novel research problem because it provided a first proof of doability. R6, among the well-experienced and influential researchers, described this legitimizing role of scientific papers as follows:"}, {"title": "5.3 Openness to the SOI paradigm and tinkering", "content": "We conclude our findings by pointing out some interview participants' micro-strategies and attempts to integrate some SOI perspectives in their everyday research practices.\nOpenness to the SOI Paradigm. Some of our interview participants, notably the two professors, R4 and R5, both situated in German universities along with R8, the professor from an Italian university, and R10, a professor from a UK university, expressed their wish for fair Al research to become more inclusive of SOI perspectives. Some of them acknowledged their own agency and responsibility in attaining this. In particular, R10 emphasized how he tried to negotiate and shape the conditions of his CS lab to align with a more inclusive research approach. This, as he explained, included actively taking responsibility for conducting more research with interdisciplinary actors, such as scholars from the social sciences. As a senior researcher, he had experienced that such collaborations did not emerge spontaneously or by themselves but required proactive efforts by researchers, such as to \u201cnegotiate and have a more active role in your occupation to be able to keep the things that you're happy with.\u201d (R10)\nR5 invoked that the CS community must learn to perceive and do fair Al research differently than \"just another machine learning problem.\" On the level of the lab and everyday research, this would include more reflection on fundamental questions in contrast to the more practical and technical aspects encountered in everyday tasks, for instance about the significance of the gender binary within datasets.\nTinkering with SOI methods. Some interview participants built their own accounts of doing SOI work by gradually deviating from usually performed CS tasks. Drawing on Fujimura [25] and Knorr [39], we understand these micro-strategies as tinkering. We especially observed tinkering with SOI methods. For instance, R7, the professor at the Dutch University engaged his PhD students in conducting \u201cshort ethnographic studies\", as he coined them, at field sites of their collaborative partners. At the same time, the ethnographic study was further adapted and tinkered with, so that"}, {"title": "6 DISCUSSION", "content": "Bringing Fujimura's study in dialogue with our findings, we now discuss the gap between CS and SOI as a problem of 'organizational alignment'.\nDoing fair AI because it works. Our first argument is that computational researchers focus on the CS paradigm of fairness largely \"because it works\", as to quote Knorr-Cetina [40]. On the contrary, adopting the SOI paradigm would necessitate acquiring new material resources and staff, reconfiguring publication venues, and dedicating additional time to novel methodological processes.\nIn the case of biomedical cancer research, as elaborated in Section 3, Fujimura [25] identifies two facilitators of making a problem doable (and thus, decreasing articulation work). The first facilitator is the leeway provided by available resources, such as equipment, space, technology, time, staff, and skills that need to be available for researchers to enable a working routine. Second, a clear division of labor, such as clear task-person and task-organization divisions, decreases articulation work, e.g. work of deciding who should do what, when, and how.\nIn our study, we identified similar resources as crucial for making the CS paradigm of fairness research work. Much like in Fujimura's observations, computational researchers pragmatically leveraged comparable resources for fairness research just as they would for other research problems. This included acquiring funding and utilizing technologies, staff and skills similar to how they approached \"other ML problems\" (R5). What partly set fairness research apart from other ML problems was its particular reliance on societal data that presented compelling societal cases (or 'real-world scenarios'). Computational researchers encountered challenges in legitimizing fairness research without access to such data, highlighting that technical resources shaped what was (still) considered a meaningful advancement in the fair AI field.\nUncertainties and constraints of fair AI. As Fujimura [25] highlights, uncertainty increases articulation work as it constrains researchers' possibilities to plan. In our case study, uncertainties posed constraints on computational researchers. The 'projectification' of research, with limited time frames, created general challenges for researchers in planning for the long-term. Information overload (e.g. 'noise') added to these uncertainties, as researchers were challenged to decide on their own accord which contributions to the field were important enough to extend, incorporate or reference in their work. In that regard, the lack of regulatory and technical standards (e.g. benchmarks, toolkits,"}, {"title": "7 LIMITATIONS AND FUTURE RESEARCH", "content": "We hope that our study opens some novel avenues for investigating the evolvement of fair Al from the perspective of the CS researchers involved in it. Yet, our study is limited to only a small group of researchers. While we were especially interested in the perspectives and experiences of these computational researchers, this can reduce the scope of our work, and hence limit the generalizability of the claims made regarding CS laboratories at large. Future research could integrate the perspectives of other relevant actors in the laboratory. Beyond CS, the doability of fair Al depends on the collaborative efforts of engineers, data scientists, ethicists, research group leaders or IT experts who may be all situated in the (same) laboratory. Further research could investigate these micro-level actors' interactions with each other and meso-level actors more closely.\nSecond, we would find it interesting to further systematically explore our interviews regarding our respondents' definitions of \"fair AI\" compared to a rather more flexible and open definition adopted while talking about their ongoing research projects and professional routines. Related to that, we were explicitly interested in how to align CS research with the SOI paradigm, rather than exploring alignment from the reverse perspective. To gain a comprehensive understanding of the current engagements of SOI scholars with fair AI, along with identifying constraints and facilitators, and possible alignment with CS, future research is needed.\nThe third limitation pertains to the interview setting employed in our study. While qualitative interviews are valuable for exploring narratives, recounting stories, and uncovering constraints and challenges - aligning with our research interest - they also capture retrospective rationalizations of practices and events. Recognizing this, we encourage future research endeavors to conduct (more) ethnographic studies within the laboratory. Engaging in ethnographic studies allows investigating the doability of tasks in,real-time', observing activities as they unfold. We think that this will allow for a deeper understanding of the ambiguities of doability, and an exploration of the more tacit aspects of fair ML, which for those we interviewed might have already been ordinary and thus less noteworthy.\nFourth, the advantage of examining researchers' accounts in qualitative settings lies in providing interview partici- pants with the opportunity to articulate ambivalence, correct their perspectives, and even pose questions in return. However, to expand generalizability, future research could broaden the scope of inquiry by incorporating quantitative surveys that could facilitate the collection of data from a larger, and ideally, a more diverse group of participants. Notwithstanding, future research could also consider introducing alternative ways to collect qualitative data, such as the elaboration of hypothetical scenarios (e.g., vignettes, speculative writing) in tandem with our interview partners to reflect upon different real-world situations. Fifth, we delineated two paradigms, but it is important to consider the possibility of additional paradigms existing, emerging and changing [15]. This insight also extends to the actors within the social world and their alignments with the CS lab. Notably, investigating actors excluded from the fair Al world, e.g."}, {"title": "8 OUTLOOK", "content": "To understand fair AI, it is essential to shift the perspective towards its actors, their everyday practices, and their challenges. By investigating the conditions and constraints that CS researchers experience, as well as their frustrations, needs, and desires, we gain important insights into how fair Al is currently made doable, and can derive novel inspirations about how else it could be. This points to a need for the consideration of how to embrace more interdisciplinary and social-oriented research into our work and not be confined by what is deemed doable. It also requires exploring avenues that challenge the organizational (and epistemic) status quo.\nAs CSCW and STS theories remind us, scientific research is, fundamentally, a form of work. Of course, engaging in fair AI could also extend beyond the formal boundaries of work. This suggestion is not intended to replace integrating SOI into computational practices, but rather serves as a reminder of the diverse possibilities for meaningful engagement with fairness and social justice across various aspects of our lives [9, 23], extending beyond professional responsibilities.\nSome of the issues we have identified, and the proposals for addressing them, mirror long-standing struggles and some constructive developments in academic research more generally. For that reason, this current moment of fair AI research could be seen as a springboard to motivate progress in academic practices and knowledge production beyond the CS paradigm."}]}