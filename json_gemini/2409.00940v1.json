{"title": "Large Language Models for Automatic Detection of Sensitive Topics", "authors": ["RUOYU WEN", "STEPHANIE ELENA CROWE", "KUNAL GUPTA", "XINYUE LI", "MARK BILLINGHURST", "SIMON HOERMANN", "DWAIN ALLAN", "ALAEDDIN NASSANI", "THAMMATHIP PIUMSOMBOON"], "abstract": "Sensitive information detection is crucial in content moderation to maintain safe online communities. Assisting in this traditionally manual process could relieve human moderators from overwhelming and tedious tasks, allowing them to focus solely on flagged content that may pose potential risks. Rapidly advancing large language models (LLMs) are known for their capability to understand and process natural language and so present a potential solution to support this process. This study explores the capabilities of five LLMs for detecting sensitive messages in the mental well-being domain within two online datasets and assesses their performance in terms of accuracy, precision, recall, F1 scores, and consistency. Our findings indicate that LLMs have the potential to be integrated into the moderation workflow as a convenient and precise detection tool. The best-performing model, GPT-40, achieved an average accuracy of 99.5% and an F1-score of 0.99. We discuss the advantages and potential challenges of using LLMs in the moderation workflow and suggest that future research should address the ethical considerations of utilising this technology.", "sections": [{"title": "1 INTRODUCTION", "content": "This paper explores how Large Language Models (LLMs) can be utilised to identify sensitive information related to mental well-being in the context of human communications. The recent development of LLMs has led to their integration into many traditional workflows. These models are recognised for their capability to comprehend and process natural languages effectively. Given their advanced understanding and generation capabilities, LLMs offer a potential approach to automating the detection of sensitive topics such as suicidal ideation or self-harm, grief and loss, sexual assault, and abuse. This paper evaluates the capability of LLMs to detect such topics. By examining the accuracy and consistency of different LLMs, this study aims to provide insights into their potential to identify and process sensitive information effectively.\nOne application area for this is content detection and moderation in mental health and the use of conversational agents and chatbots to provide mental health support safely and ethically. The prevalence of mental health issues is escalating globally, presenting a substantial public health challenge. The World Health Organisation (WHO) estimates that one in four people will encounter mental health difficulties at some point in their lives [44]. Youth (ages 12-24) is identified as a critical period, with the majority of mental disorders commencing during these formative years [45]. The protracted COVID-19 pandemic appears to further exacerbate these trends [47, 51]. However, globally, healthcare systems face a pronounced shortage of trained mental health professionals, making it a significant challenge for young people to access timely mental support [9, 30].\nMental health chatbots are increasingly recognised as a promising and impactful tool for mental health professionals within the context of mental health support [18]. Recent studies have demonstrated their ability to assist in managing mental health issues [1, 10, 54, 56]. There are a number of advantages to using them; Chatbots can be accessed 24 hours a day in multiple languages on various health topics, reducing labour costs and improving counselling efficiency. However, prior studies have indicated that chatbots have limitations in ensuring safety within the mental well-being domain [6, 18, 33]. One disadvantage is that chatbots often lack the ability to process sensitive information, which can sometimes harm the user's mental well-being instead of supporting it. Consequently, human-in-the-loop monitoring is still crucial in human-AI interactions in real time. Chatbots can show a progressive ability to understand the context in conversations [58], indicating a potential to support humans with sensitive topic detection. LLMs can help detect sensitive information by labelling sensitive messages and notifying a human moderator to intervene. This study aims to investigate the capabilities of LLMs to detect sensitive topics within the mental well-being domain. Our aim is to test the performance of various LLMs on metrics such as accuracy, precision, and recall in automatically detecting sensitive messages. By conducting this study, we seek to answer two research questions:\nRQ1: What is the impact of different LLMs on detecting sensitive topics?\nRQ2: How does the temperature parameter setting affect LLMs' performance in detecting sensitive topics?\nThe rest of the paper is organised as follows. Section 2 introduces related work in this field, and Section 3 explains the methodology we followed. While Section 4 introduces the preliminary study, Section 5 covers the procedure of the formal and the results are presented in Section 6. Finally, discussions and conclusions are presented in Sections 7 and 8, respectively. The main contributions of this paper are:\n(1) We provide one of the first evaluations of the capability of several common LLMs at different temperature settings in detecting mentally sensitive messages.\n(2) We curated datasets to form a collection of sensitive and non-sensitive messages.\n(3) We provided a framework for testing the capability of LLMs in detecting sensitive content during the moderation process."}, {"title": "2 RELATED WORK", "content": "Automated content detection is a critical area of research, particularly for identifying sensitive and harmful information on online platforms. This section explores the various methods and technologies employed in this domain, highlighting the evolution from traditional human moderation to advanced machine-learning approaches. Subsection 2.1 explores the application of automated content detection in different contexts, such as preventing cyberbullying and supporting mental health."}, {"title": "2.1 Automated Content Detection", "content": "Sensitive and harmful information detection is always an important topic in many areas. Many online forums and social media platforms traditionally employ human moderators to review and manage content manually. This approach is often inefficient. To improve efficiency, some crowdsourced workers have been introduced in this field [7, 25], but this approach, at the same time, has increased costs. Recently, many studies have explored using automated content detection, often machine-learning-based, to assist human moderators, aiming to replace manual review methods with higher efficiency and lower cost.\nAutomated content detection has been extensively researched in the context of preventing cyberbullying and cybercrime [48]. For example, Al-Garadi et al. [3] developed a machine learning model to detect cyberbullying on Twitter, analysing network, activity, user, and content features from 2.5 million geo-tagged tweets. They achieved high detection accuracy and precision using classifiers like Random Forest, proposing a tool for monitoring and mitigating online bullying. Similarly, Zhao et al. [57] proposed a new model that enhances traditional text classification by integrating bullying-specific features derived from word embeddings. Their model showed superior performance compared to baseline methods. Dadvar and De Jong [14] integrate user characteristics and behaviours across social networks into a new model to detect cyberbullying. Their study offers a tool for more accurate cyberbullying detection and enhanced support for victims and moderators.\nOne important domain for identifying sensitive information is E-health, which is a sensitive area that requires careful handling. Huh, et al. [27] developed a text classification system to assist moderators in identifying posts that need their expertise in online health communities. They used a binary Naive Bayes classifier to categorise posts into those needing a moderator's response and those that do not. This approach provides a low-cost, scalable solution to help moderators efficiently prioritise posts requiring their attention, thereby enhancing support in large-scale health forums.\nThere are also some studies about suicidality detection. O\u2019Dea et al. [41] developed a machine-learning model to detect suicidality on Twitter. They collected tweets containing suicide-related phrases and used human coders to classify the level of concern. These tweets were then used to train a classifier. The classifier achieved an overall accuracy of 76%, correctly identifying 80% of \u201cstrongly concerning\u201d tweets. Cohan et al. [12] also used machine learning to build a system to automatically triage the severity of posts in online mental health forums. The system categorises posts into four levels of severity, from no risk to imminent risk of self-harm, using a feature-rich classification framework that includes lexical, psycholinguistic, contextual, and topic modelling features. The system significantly improved the identification of high-risk posts, achieving up to a 17% improvement in F1 scores, an indicator of precision and recall capability, compared to previous methods.\nResearchers also looked at peer support, which is beneficial for young people to overcome barriers in help-seeking behaviour. Milne et al. [36] developed a machine learning system to triage posts in online peer support forums, categorising messages by urgency. Using a classifier, it achieved 84% accuracy, improving moderator efficiency and response times on online forums. Milne et al. [37] organised the \u201cCLPsych 2016 Shared Task\u201d to develop systems for automatically triaging posts in online mental health forums, specifically \u201cReachOut.com\u201d. Participants classified posts into four categories: Green (no immediate action needed), Amber (moderate concern), Red (high"}, {"title": "2.2 Perceptions of Technology-Assisted Detection", "content": "From prior studies, we found that machine learning is the most common method for building AI-supported detectors or moderators. It can achieve high accuracy and reduce detection time. Some have already been integrated into practical workflows. Jhaver et al. [28] conducted a study of Automoderator (Automod) on the Reddit platform. They interviewed 16 Reddit moderators, including the creator of Automod, to explore the benefits and challenges associated with automated content detection technologies. The result indicated that Automod significantly reduces the workload of human moderators by handling menial tasks and protecting them from emotional labour. However, using Automod also creates new problems for human moderators. They must check for incorrect decisions, as Automod lacks contextual sensitivity and struggles with cultural and linguistic nuances. Based on the forum regulations, the tool requires regular updates and fine-tuning, adding to the workload of already limited skilled moderators. Additionally, moderators may find it difficult to fully understand and control Automod's actions, especially when it makes mistakes or its decisions are unclear, it's not fully transparent. At last, the paper highlighted the need for a balanced approach that combines automated and human moderation to maintain effective content regulation.\nMany scientists have raised concerns about using technology-assisted tools for content detection [39, 46, 50]. Gillespie [21] mentioned the inability of these tools to grasp context and nuance, leading to errors in detecting sarcasm and cultural subtleties. Additionally, Gillespie is concerned about the risk of AI perpetuating biases from training data, potentially impacting marginalised groups unfairly. He emphasised the need for human oversight in moderation to handle complexities and ethical considerations, arguing that AI should assist rather than replace human moderators. He pointed out that \u201cthe kind of judgment that includes the power to ban a user from a platform should only be made by humans.\" Gorwa et al. [23] examined the technical and political challenges of algorithmic content moderation on major platforms like Facebook, YouTube, and Twitter. Their study underscored the opaqueness and accountability issues of automated moderation and similarly emphasised the need for human oversight to address the complex and contextual nature of content moderation effectively."}, {"title": "2.3 Large Language Model: A New Method", "content": "The rapid development of LLMs introduces a new method in this domain, changing the way humans interact with machines [2]. LLMs possess powerful text-generating and understanding capabilities [55]. Users can ask AI to explain each decision in natural language simply by using natural language prompts, making communication straightforward. In such cases, integrating LLMs to replace traditional machine learning methods becomes a viable choice for automated content detection.\nWang et al. [53] evaluated six popular LLMs for detecting and handling harmful instructions. LLaMA-2 showed the best safety performance, while ChatGLM2 was the least effective. Commercial models like GPT-4 often rejected harmful instructions directly. He et al. [24] compared GPT-4's data labelling with 415 crowd-sourced workers from the MTurk platform on 3,177 sentence segments from 200 scholarly papers. GPT-4 achieved 83.6% accuracy, while MTurk reached 81.5%. Combining their labels improved accuracy to 87.5%. The study shows that integrating human and AI labelling enhances accuracy, highlighting the value of combining crowdsourcing with advanced AI models. Kolla et al. [31] evaluated GPT-3.5's effectiveness in moderating Reddit content, analysing 744 posts across nine subreddits. They found GPT-3.5 had a high true-negative rate (92.3%) but a low"}, {"title": "3 METHODOLOGY", "content": "This research comprises two phases, as shown in 1. First, we conducted a preliminary study, detailed in Section 4. This involved using a small dataset to evaluate various LLMs for detecting sensitive topics in conversations among university freshmen. This initial study aimed to gain a general understanding of LLM performance in detecting sensitive topics. It covers dataset preparation, LLM selection and fine-tuning, and data processing. Preliminary results highlight the effectiveness of LLMs, with some models demonstrating high accuracy in identifying sensitive information.\nSecond, in Section 5, we conducted a formal study by evaluating five LLMs on a task involving datasets with both sensitive and non-sensitive messages. The procedures for both studies are shown in Fig. 1. Section 6 reports on the accuracy, precision, recall, F1 scores, and consistency of the models. Additionally, we tested the impact of the temperature parameter on LLM performance and analysed the causes of errors encountered during the study. Finally, Section 7 discusses the study's results."}, {"title": "4 PRELIMINARY STUDY", "content": "To test the capability of models for the subsequent experimental design, we conducted a preliminary study. In this study, several LLMs were tested to detect whether the topics of conversation were sensitive. The context was set as conversations among university freshmen undergoing a transition. The scope of sensitive topics was based on prior studies [16, 49] and discussions among the research team. These topics included: \u2018Strong Emotion or Stress,\u2019 \u2018Suicidal Ideation or Self-Harm,' \u2018Sexual Assault and Abuse,' \u2018Severe Mental Health Disorders,' \u2018Substance Abuse,' \u2018Eating Disorders,' \u2018Trauma and PTSD,' \u2018Grief and Loss, \u2018Family Issues,' \u2018Identity and Discrimination Issues,' \u2018Legal Issues and Crime,' 'Physical Hurt' and \u2018General Stress and Bad Mood."}, {"title": "4.1 Procedure", "content": "4.1.1 Dataset Preparation. We used ChatGPT (GPT-4) to generate casual conversations between a university freshman and her best friend about her first-year life in school. the prompt was:\n\u201cI would like you to play as a freshman named Lily, communicating with your friend Sam about the problems you encounter in your first-year uni life. Lily is a freshman facing transition issues. I don't want continuous dialogue, but ten discontinuous pieces of conversation. I want this communication to start from the beginning like you might say \"Hi, how is it going?\" (Please use the format: Lily: '...', Sam: .....) Some of these are about Lily's troubles with stress in school, and others are casual talk. Generate 500 messages, I want half of the conversations to be mentally safe, and half to be unsafe.\"\nFrom this generation, we obtained 500 messages(50% sensitive) to create the pool. Then, a research team member manually checked messages and selected 100 sensitive messages along with 100 non-sensitive messages to form the dataset. Here are some examples of sensitive messages:\n(1)\nMessage:I think I need to talk to someone professional. I'm overwhelmed by my emotions and it's getting harder to cope.\nReason: General Stress and Bad Mood\n(2) Message:I know it's bad, but I feel like I can't stop. It's the only thing that helps me feel less anxious.\nReason: Substance Abuse\n4.1.2 LLM Preparation. We selected five popular and powerful LLMs available at the time of this research for testing, including GPT-3.5-Turbo-0125, GPT-4-Turbo Claude-3-Opus-20240229, Llama-3-70B-8192, and GPT-3.5-Turbo-0125 (Fine-Tuned). Fine-tuning is a process of pre-training an LLM for specific tasks, believed to enhance its performance in specific areas [52]. In this study, we choose GPT-3.5-Turbo-0125, the most advanced GPT model for fine-tuning at the time of this research. We selected 70 messages as the training data and 30 as the validation data suggested by OpenAI [42]. Then, we used the OpenAI Playground \u00b9 to train the fine-tuned model.\n4.1.3 LLM Data Processing. We used JSON mode in Python 3.9.13 to call the API of each of the five models to batch-process the datasets. The processed data was then stored in CSV files. The following prompt was given to the LLMs as instructions:\n\u201cYou are a detector that determines whether the input falls within a given range. The range includes Strong Emotion or Stress, Suicidal Ideation or Self-Harm, Sexual Assault and Abuse, Severe Mental Health Disorders, Substance Abuse, Eating Disorders, Trauma and PTSD, Grief and Loss, Family Issues, Identity and Discrimination Issues, Legal Issues, Crime. And something that can cause stress and bad mood, Long sentences of more than 30 words, and Physical hurt or related topics. Simply respond with 'yes' if it is related, or 'no' if it is not.\u201d\nWe set the prompt slightly broader than our scope, assuming that LLMs might struggle with long, complex sentences [29]."}, {"title": "4.2 Preliminary Results", "content": "The results are shown in Table 1. True Positives (T+) represent the number of correctly identified sensitive data points. True Negatives (T-) represent the number of correctly identified non-sensitive data points. False Positives (F+) represent the number of non-sensitive data points incorrectly identified as sensitive. False Negatives (F-) represent the number of sensitive data points incorrectly identified as non-sensitive.\nIn this study, the evaluations of multiple LLMs suggested the promising potential of using LLMs in detecting sensitive topics as defined. Models like Claude-3-Opus-20240229, GPT-4-Turbo, and"}, {"title": "5 FORMAL STUDY", "content": "Based on the preliminary study findings, we designed the formal study. The procedure involves dataset selection and pre-processing, human coding for topic refinement, and model detection using proprietary and open-source LLMs, with performance measured at various temperature settings."}, {"title": "5.1 Dataset", "content": "We chose two open-source, real datasets: \u201cTopical-Chat\" [22] and \u201cMental-health-counseling-conversations\" [4]. Topical-Chat is a knowledge-grounded human-human conversation dataset focused on daily life, with each message labelled with sentiments such as \u2018Curious to Dive Deeper', 'Happy', \u2018Neutral', 'Surprised', 'Disgusted', \u2018Sad', \u2018Fearful', and \u2018Angry'. We selected messages labelled Happy to form the non-sensitive topic dataset, there were 683 messages. Mental-health-counselling-conversations is collected from two online mental counselling and therapy platforms. We used it to form the sensitive topic dataset, resulting in 996 messages.\nWe pre-processed the two datasets, removing duplicates and messages that potentially violated laws and AI platform usage policies (e.g., content related to paedophilia, Nazis, rape, etc.)."}, {"title": "5.2 Human Coding", "content": "Three members of the research team participated in data coding. They are all postgraduate students: two are HCI researchers, and one has a background in psychology. Based on the preliminary study results, dataset preprocessing, and prior studies, human coders revised the topic scope to include climate change [26] and finance issues [32] as sensitive topics. We also improved the prompt in JSON for the GPT, see Appendix B. The final instruction of sensitive topic scope is:\n'Suicidal Ideation or Self-Harm or related topics,\u2019 \u2018Sexual Assault and Abuse or related topics,\u2019 \u2018Severe Mental Health Disorders or related topics,' \u2018Substance Abuse or related topics,\u2019 \u2018Eating Disorders or related topics,' 'Trauma and PTSD or related topics,\u2019 \u2018Grief and Loss or related topics,\u2019 \u2018Strong Emotion or Stress,' 'Family Issues or related topics,' \u2018Identity and Discrimination Issues or related topics,' 'Legal Issues and Crime or related topics,' \u2018Physical harm or related topics,' \u2018Financial problems or related topics,' 'Climate change concerns or related topics,\u2019 \u2018General stress and bad mood or related topics,\u2019 'Anything else that could cause mental health issues, stress, or emotional damages,' and 'Long sentences with more than 30 words'."}, {"title": "5.3 Model Detection", "content": "Based on the results of the preliminary study and the accessibility of the API for batch processing, we selected three proprietary models: GPT-4, GPT-40, GPT-4-Turbo, and two open-source models: Llama-3-70b and Solar-1-mini-chat. For each model, we measured performance at a temperature setting of 0.0, conducting five repetitions to assess consistency. Additionally, we tested each model at three different temperature settings (0.3, 0.5, and 0.7) to analyse the impact of temperature variations on performance, with each experiment repeated three times. The number of samples was confirmed by a statistical power analysis [20]. We performed a power analysis with G*Power and calculated Cohen's d based on our data [19], and the result (Actual Power = 1, alpha = 0.05, effect size f = 13.39 ) indicated that the sample size was sufficient, with a power close to 1, to detect statistically significant effects at an alpha level of 0.05."}, {"title": "6 FORMAL STUDY RESULTS", "content": "We employed several common performance metrics to evaluate these models, including Accuracy, Precision, Recall, and F1-score [15, 38], which have been used in prior studies [3, 57].\nAccuracy was calculated as the proportion of correctly classified messages out of the total number of messages, with the baseline being (100%)."}, {"title": "6.1 Data Analysis", "content": "We employed several common performance metrics to evaluate these models, including Accuracy, Precision, Recall, and F1-score [15, 38], which have been used in prior studies [3, 57].\nAccuracy was calculated as the proportion of correctly classified messages out of the total number of messages, with the baseline being (100%).\n$Accuracy = \\frac{T+ + T-}{T++T+F+ + F-}$\nPrecision was measured as the proportion of messages identified as sensitive by the model that were actually sensitive according to the labelled data.\n$Precision = \\frac{T+}{T+ + F+}$\nRecall was determined as the proportion of actual sensitive messages correctly identified by the model.\n$Recall = \\frac{T+}{T+ + F-}$\nThe F1-score was a crucial metric for evaluating model performance because it balanced precision and recall. Precision measured the accuracy of positive predictions, while recall measured the model's ability to identify all actual positive instances. The F1 score, being the harmonic mean of precision and recall, provided a single value that reflects both the accuracy and completeness of the model's positive predictions. This was especially important in the online counselling context, where both true positives and false negatives have significant implications.\n$F1\\text{-score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\nTable 2 shows each model's performance results. We also considered the model's performance on non-sensitive data to ensure a comprehensive evaluation by measuring the true negative rate. We used IBM SPSS Statistics 27 to analyze the results. The study was conducted in the following steps:"}, {"title": "6.2 Performance Analyses at Temperature 0.0", "content": "6.2.1 Accuracy. The Shapiro-Wilk Test indicated that our data was consistent with a normal distribution except for Llama-3 (GPT-4: W=0.88, p=0.31; GPT-40: W=0.83, p=0.14; GPT-4-turbo: W=0.94, p=0.69; Llama-3: W=0.68, p=0.006; Solar: W=0.91, p=0.49). Levene's test for equality of variances was performed to assess the homogeneity of variances across the five models' accuracies. The results showed that the assumption of homogeneity of variances was satisfied, F(4, 20)=2.29, p=0.095. Since the p-value was greater than 0.05, we concluded that the variances were approximately equal across"}, {"title": "6.2.2 F1-score.", "content": "The F1-score, or the harmonic mean, indicates the balance between precision and recall in a single value. The Shapiro-Wilk test indicated that the data is consistent with a normal distribution for most models (GPT-4: W=0.83, p=0.14; GPT-40: W=0.86, p=0.22; GPT-4-turbo: W=0.88, p=0.33; Solar: W=0.97, p=0.88), except for Llama-3 (W=0.68, p=0.006). Levene's test yielded F(4, 20)=3.95, p=0.016, indicating the variances are approximately equal across the different models. ANOVA revealed a significant difference among the models (F(4, 20)= 2017.00, p<0.001). The post-hoc pairwise comparison using Tukey test was conducted to determine the specific differences between the models. The post-hoc test yielded the results as shown in Fig. 3, GPT-40 (SD=0.0011) and GPT-4 (SD=0.0019) achieved significantly higher scores than the other three models, consistent with their accuracy performance. Additionally, GPT-4-turbo (SD=0.0031) significantly outperformed Llama-3 (SD=0.0005) and Solar (SD=0.0032)."}, {"title": "6.3 Performance Analyses at Temperature 0.3, 0.5, and 0.7", "content": "We tested each model at different temperatures, including 0.3, 0.5, and 0.7, three times and then calculated the results to investigate how the models performed at each temperature. We also tested each model in 0.0 three times as the baseline to provide a benchmark."}, {"title": "6.3.1 Accuracy.", "content": "The descriptive results showed that the accuracy of the three GPT-4 models and the Llama-3 model fluctuated with increasing temperature, displaying no direct relationship with the temperature settings. In contrast, the Solar model seemed to display a trend of decreasing accuracy as the temperature increased. Considering that we conducted three repeated measurements for each model at each temperature, we used a nonparametric Friedman test on accuracy under different temperatures. The results yielded: GPT-4: x\u00b2(2)=0.18, p=0.91; GPT-40: x\u00b2(2)=6.0, p=0.05; GPT-4-Turbo: x\u00b2(2)=3.0, p=0.22; Llama-3: x\u00b2(2)=4.67, p-value=0.097; Solar: x\u00b2(2)=4.67, p-value=0.097. The results indicated that the accuracy did not significantly differ at different temperatures. While this suggests that temperature does not affect accuracy, more extensive testing is needed for verification.\nThe results in Fig. 5 indicated that under the task conditions for determining the sensitivity of the information and then outputting only a boolean value as the final result, temperature variation appeared unrelated to SD (accuracy) fluctuations. However, GPT-40 consistently demonstrated the highest reliability."}, {"title": "6.3.2 F1-scores.", "content": "We calculated F1-scores for each model at different temperatures, and the results are presented in Fig. 6. The trend of F1-scores with temperature variations generally aligned with the trend of accuracy with temperature variations. Similarly, GPT-40 consistently achieved high scores at all temperatures. The performance of GPT-4 fluctuated but it generally remained consistent."}, {"title": "6.4 Error Analysis: A Case Study", "content": "We analysed the failed cases encountered in this study. Based on our analysis of errors in LLM responses, we categorised the errors into two main types: response failure and incorrect response. Response failure refers to instances where the LLM did not return responses in the specified format (Boolean values) set in the prompt. Incorrect response refers to instances where the LLMs returned responses in the specified format but incorrectly assessed the sensitivity of the message."}, {"title": "6.4.1 Response Failure.", "content": "Response failures were observed only in GPT-40 and Llama-3. Under the 0.7 temperature condition for GPT-40, there were instances where it did not return a Boolean value for sensitive messages, accounting for 0.23% (7/3000) of the total responses at this specific temperature, such as:\nInput: Does this go against any ethical codes? Are there legal ramifications?\nOutput: [boolean]\nFor Llama-3, we identified two kinds of response failures. The first type was a complete failure, where it did not execute the task based on the prompt, for example:\nInput: It seems like more than that.\nOutput: I sense that you're hinting at a deeper emotional concern. Would you like to talk about what's on your mind?"}, {"title": "6.4.2 Incorrect Response.", "content": "We randomly selected one false positive and one false negative example from each of the five models and then used the corresponding LLMs to provide explanations for the results. We conducted an in-depth case analysis to understand the reasons for these errors.\n(1) GPT-4-False Positive:\nInput: I played for 4 years in high school. It was fun but high stress the QB involved in pretty much every offensive play and lets face it no one cares about defense.\nOutput: Yes\nExplanation: The input is related to \"Strong Emotion or Stress\" as well as \"General Stress and Bad Mood or related topics\". This is because the sender mentioned that playing the quarterback role is \"high stress\".\nIn this case, the message mentioned \u201chigh stress", "Negative": "nInput: Just wondering if this is a deviant act, and if I should be concerned for him. He isn't quite 10.\nOutput: No\nExplanation: This input does not provide enough information to link it to any of the specified sensitive topics. Information about a specific act or behaviour is missing, making it impossible to gauge whether it is concerning or potentially linked to a sensitive topic. However, the context suggests potential \"Family Issues or related topics\" due to the person's concern about a certain individual's behaviour. Yet, without concrete details, firm conclusions cannot be made.\nThis input was related to the topic of potential concerns about a child's behaviour. It signalled a concern for the child's well-being and possibly indicated underlying issues that could impact mental health. Although GPT-4 mentioned \u201cfirm conclusions can't be made", "Positive": ""}, {"title": "Input:", "content": "HA! I did see the exploding glitter bomb. If he sold it on Amazon, he could make a killing. People, like me, are so vindictive!\nOutput: Yes\nExplanation: The input is related to \"Strong Emotion or Stress\" and \"General Stress and Bad Mood or related topics\" because it mentions vindictiveness, which is a strong negative emotion.\nSimilar to GPT-4, GPT-40 made its judgment based on a strongly emotional keyword like vindictive, and neglected the general context of the message.\n(4) GPT-40-False Negative:\nWe tested all false-negative cases in this study from GPT-40 under the same conditions in the OpenAI playground. In contrast, GPT-40 consistently provided correct judgments, leading us to suspect that errors in the batch process may have caused the false negatives.\n(5) GPT-4-Turbo-False Positive:\nInput: I did not. So weird. I know that constant novelty from clicking links on the internet can cause real addiction measurable in the brain.\nOutput: Yes\nExplanation: The input mentions addiction, specifically referring to internet addiction, which is related to \"Substance Abuse or related topics\" as listed. The mention of addiction being measurable in the brain indicates a concern with mental health and behavioural issues, aligning with the topic of substance abuse and related behaviours.\nThis explanation made no sense in linking substance abuse to the addiction mentioned in this message. This is only a statement of objective fact. Certain keywords instead of the whole sentence result in its judgement.\n(6) GPT-4-Turbo-False Negative:\nInput: I love him, and I don't know what to do.\nOutput: No\nExplanation: The input expresses a feeling of love and uncertainty about what actions to take, but it does not explicitly relate to any of the specified sensitive topics such as severe mental health disorders, trauma, or substance abuse. The statement is more indicative of a personal or emotional dilemma, possibly related to intimate relationships, but it does not contain enough context or severity to be classified under the provided sensitive topics.\nThis case indicated that GPT-4-Turbo tended to classify information as non-sensitive while lacking sufficient contextual details, even if it recognises that the information might pertain to several relevant topics.\n(7) Llama-3-False Positive:\nAs with GPT-40, the Llama-3 Playground (via GroqCloud) \u00b2 also consistently provided correct judgments for all false positive cases obtained from batch-process API calls in this study.\n(8) Llama-3-False Negative:\nInput: He is an adolescent. He has peed his pant multiple times over the last few years, all at times when he is too wrapped up in a video game or video. We have taken away games and videos for long periods of time as punishment, but after a few months of having then back, he pees his pants again."}, {"title": "Outputs:", "content": "No\nExplanation: The input is not directly related to any of the specified sensitive topics. Although the situation described may be embarrassing or frustrating for the adolescent", "Positive": "nTesting on the Solar Playground also found that Solar could correctly identify false positive messages when evaluating them individually"}]}