{"title": "ADDRESSING DATA HETEROGENEITY IN\nFEDERATED LEARNING WITH ADAPTIVE\nNORMALIZATION-FREE FEATURE RECALIBRATION", "authors": ["Vasilis Siomos", "Sergio Naval-Marimont", "Jonathan Passerat-Palmbach", "Giacomo Tarroni"], "abstract": "Federated learning is a decentralized collaborative training paradigm that pre-\nserves stakeholders' data ownership while improving performance and generaliza-\ntion. However, statistical heterogeneity among client datasets poses a fundamen-\ntal challenge by degrading system performance. To address this issue, we propose\nAdaptive Normalization-free Feature Recalibration (ANFR), an architecture-level\napproach that combines weight standardization and channel attention. Weight\nstandardization normalizes the weights of layers instead of activations. This is\nless susceptible to mismatched client statistics and inconsistent averaging, thereby\nmore robust under heterogeneity. Channel attention produces learnable scaling\nfactors for feature maps, suppressing those that are inconsistent between clients\ndue to heterogeneity. We demonstrate that combining these techniques boosts\nmodel performance beyond their individual contributions, by enhancing class se-\nlectivity and optimizing channel attention weight distribution. ANFR operates\nindependently of the aggregation method and is effective in both global and per-\nsonalized federated learning settings, with minimal computational overhead. Fur-\nthermore, when training with differential privacy, ANFR achieves an appealing\nbalance between privacy and utility, enabling strong privacy guarantees without\nsacrificing performance. By integrating weight standardization and channel atten-\ntion in the backbone model, ANFR offers a novel and versatile approach to the\nchallenge of statistical heterogeneity. We demonstrate through extensive exper-\niments that ANFR consistently outperforms established baselines across various\naggregation methods, datasets, and heterogeneity conditions.", "sections": [{"title": "1 INTRODUCTION", "content": "Federated learning (FL) (McMahan et al., 2017) is a decentralized training paradigm which enables\nclients to jointly develop a global model without sharing private data. By preserving data privacy\nand ownership, FL holds significant promise for applications across various domains, including\nhealthcare, finance, and mobile devices. However, a fundamental challenge in FL is statistically\nheterogeneous, i.e. non-independent and identically distributed (non-IID) client datasets. This het-\nerogeneity can degrade the performance of the global model and slow down convergence (Li et al.,\n2020b; Hsu et al., 2019). Addressing it is thus critical for the success of FL in real-world scenarios.\nMost prior relevant research has focused on developing novel aggregation algorithms that either aim\nto reduce client drift (Karimireddy et al., 2020) or tailor specific layers of the model to individual\nclients to accommodate data variations (Zhang et al., 2023). While these algorithmic strategies have\nadvanced the field, they often overlook the impact of the underlying model architecture on perfor-\nmance under heterogeneity. Of particular interest is the role of normalization layers. Batch Normal-\nization (BN) (Ioffe & Szegedy, 2015), although effective in centralized settings, has been shown to\nnegatively affect performance in heterogeneous FL due to mismatched client-specific statistics and\ninconsistent parameter averaging (Wang et al., 2023; Guerraoui et al., 2024). In response, the use"}, {"title": "2 RELATED WORK", "content": "Since McMahan et al. (2017) introduced FL, most research has focused on developing novel ag-\ngregation algorithms to address challenges like data heterogeneity. In global FL (GFL), methods\nsuch as proximal regularization (Li et al., 2020a) and cross-client variance reduction (Karimireddy\net al., 2020) aim to reduce client drift. Techniques like discouraging dimensional collapse through\ncorrelation matrix norm regularization (Shi et al., 2023), adopting relaxed adversarial training (Zhu\net al., 2023), and performing amplitude normalization in frequency space (Jiang et al., 2022) have\nalso been proposed. Recent promising ideas include constructing global pseudo-data to de-bias\nlocal classifiers and features (Guo et al., 2023), introducing concept drift-aware adaptive optimiza-\ntion (Panchal et al., 2023), and using hyperbolic graph manifold regularizers (An et al., 2023). In\npersonalized FL (pFL), personalizing specific layers of the model can mitigate heterogeneity. The\nsimplest approach shares all model parameters except the classification head (Arivazhagan et al.,\n2019). More advanced methods replace lower layers and mix higher ones (Zhang et al., 2023) or\nadjust mixing ratios based on convergence rate approximations (Jiang et al., 2024). While these\nalgorithmic approaches have advanced both GFL and pFL, they often overlook the impact of the\nunderlying architecture on performance.\nOur work addresses this gap by examining how specific model components can enhance FL perfor-\nmance. This approach is orthogonal to algorithmic advancements, representing a crucially underde-\nveloped area. Previously, Qu et al. (2022) demonstrated performance gains by switching from a pre-\ntrained convolutional network to a pre-trained vision transformer. Studies by Pieri et al. (2023) and\nSiomos et al. (2024) evaluated multiple architectures under different aggregation methods, show-\ning that changing the backbone model can improve performance more than altering the aggregation\nmethod. However, these works did not design models specifically tailored to combat heterogeneity."}, {"title": "3 ADAPTIVE NORMALIZATION-FREE FEATURE RECALIBRATION", "content": "We consider a FL setting with C clients, each owning a dataset of image-label pairs $D_i = \\{(x_k, y_k)\\}$\nand optimizing a local objective $L_i(\\theta) = E_{(x,y)\\sim D_i}[l(x, y; \\theta)]$, where $l$ is a loss function and $\\theta$ the\nmodel parameters. The global objective is learning a model $f(\\theta)$ that minimizes the aggregate loss:\n$\\min_\\theta L(\\theta) = \\frac{1}{|D|} \\sum_{i=1}^C |D_i| L_i(\\theta)$                                                                                                                                                     (1)\nHeterogeneity among $D_i$ can degrade the global model performance and slow convergence (Kairouz\net al., 2021). In this study, we are interested in modifying the backbone model to address it. As\nthey are the most widely used family, and they perform better or on par with others (Pieri et al.,\n2023; Siomos et al., 2024), we focus specifically on convolutional neural networks (CNNs). Let\n$X \\in R^{B\\times C_{in}\\times H\\times W}$ represent a batch of B image samples with $C_{in}$ channels and dimensions H$\\times$W."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETTINGS\nDatasets. We evaluate our approach on five classification datasets, including Fed-ISIC2019\n(Ogier du Terrail et al., 2022) containing dermoscopy images from 6 centers across 8 classes where\nlabel distribution skew and heavy quantity skew is present; FedChest, a novel chest X-Ray multi-\nlabel dataset with 4 clients and 8 labels with label distribution skew and covariate shift; a partition-\ning of CIFAR-10 (Krizhevsky et al., 2009) which simulates heavy label distribution skew across\n8 clients using the Kolmogorov-Smirnov (KS) \u2018split-2' as presented in Qu et al. (2022); CelebA\n(Liu et al., 2015) from the LEAF suite (Caldas et al., 2018) which is a binary classification task in\na cross-device setting with a large number of clients, covariate shift and high quantity skew; and\nFedPathology, a colorectal cancer pathology slide dataset with 9 classes derived from Kather et al.\n(2019), featuring challenging concept drift as the images, which we do not color-normalize, were\nproduced using two different staining protocols. FedChest contains images from PadChest (Bustos\net al., 2020), CXR-14 (Wang et al., 2017) and CheXpert Irvin et al. (2019), which present one or\nmore of 8 common disease labels. For FedPathology which we use for DP training in Section 4.3,\nwe use Dirichlet distribution sampling (Hsu et al., 2019) with \u03b1 = 0.5 to simulate a moderate label\ndistribution skew and partition the data among 3 clients. Each task covers a different aspect of the\nmulti-faceted problem of data heterogeneity in FL, including different domains and sources of het-\nerogeneity, to provide a robust test bed. Additional dataset details are presented in Appendix A.1,\nincluding instructions to replicate FedChest in B.1.\nCompared models. We compare our approach with a typical ResNet (utilizing BN), a ResNet where\nBN is replaced by GN, a SE-ResNet (Hu et al., 2018), and a NF-ResNet. This selection isolates the\neffects of our architectural changes compared to using BN, using its popular substitution GN, and\nusing weight standardization and CA separately. We choose a depth of 50 layers for all models to\nbalance performance with computational expense. All models are pre-trained on ImageNet (Rus-\nsakovsky et al., 2015) using the timm (Wightman, 2019) open-source library. ANFR follows the\nstructure of NF-ResNet, with the addition of CA blocks in the same position as SE-ResNet. Except\nfor Section 4.4, we employ Squeeze-and-Excitation (Hu et al., 2018) as the attention mechanism.\nAdditional model details are provided in Appendix A.3.\nEvaluated methods. We use 4 global FL (GFL) and 2 personalized FL (pFL) aggregation methods\nas axes of comparison for the models, each representing a different approach to model aggregation:\nthe seminal FedAvg (McMahan et al., 2017) algorithm, FedProx (Li et al., 2020a), which adds a\nproximal loss term to mitigate drift between local and global weights, SCAFFOLD (Karimireddy\net al., 2020), which corrects client drift by using control variates to steer local updates towards the\nglobal model, FedAdam (Reddi et al., 2021), which decouples server-side and client-side optimiza-\ntion and employs the Adam optimizer (Kingma & Ba, 2017) at the server for model aggregation,\nFedBN (Li et al., 2021) which accommodates data heterogeneity by allowing clients to maintain\ntheir personal batch statistics, and by construction is only applicable to models with BN layers, and\nFedPer (Arivazhagan et al., 2019) which personalizes the FL process by keeping the weights of the\nclassifier head private to each client. We note our proposal is an architectural one which is aggre-\ngation method-agnostic, thus we selected these widely known aggregation methods to represent a\nspectrum of strategies, from standard averaging to methods addressing client drift and personaliza-\ntion. This provides a robust comparison concentrated on the model architectures.\nEvaluation metrics. For Fed-ISIC2019, we report the average balanced accuracy due to heavy\nclass-imbalance as in (Ogier du Terrail et al., 2022). For FedChest, a multi-label classification task\nwith imbalanced classes, we report the mean AUROC on the held-out test set to account for varying\nclass distributions, in addition to more extensive results in Appendix B.2. We report the average\naccuracy for the other 3 datasets. In pFL settings, the objective is providing good in-federation\nmodels. Consequently, we report the average metrics of the best local models, as suggested in\n(Zhang et al., 2023).\nImplementation Details. We choose training hyper-parameters separately for each dataset by tun-\ning the vanilla ResNet using the ranges detailed in Appendix A.2 and then re-using the same pa-\nrameters for all models. In Fed-ISIC2019 clients use Adam with a learning rate of 5e-4 and a batch\nsize of 64 to train for 80 rounds of 200 steps. In FedChest clients use Adam with a learning rate of\n5e-4 and a batch size of 128 to train for 20 rounds of 200 steps. For DP-training in FedPathology,"}, {"title": "4.2 PERFORMANCE ANALYSIS AND COMPARISON", "content": "GFL scenario. Average results for all datasets, models, and GFL aggregation methods are presented\nin table 1. First, we observe that GN does not consistently outperform the vanilla ResNet, support-\ning our pursuit of a more reliable alternative. For instance, GN is outperformed by BN in half of\nthe tested aggregation methods on Fed-ISIC2019 and FedChest. Second, the sub-optimality of CA\noperating on BN-normalized features is evident, as the SE model frequently performs worse than\nBN-ResNet, notably across all aggregation methods on FedChest. NF-ResNet shows strong perfor-\nmance across all tasks and methods, confirming the potential of replacing activation normalization\nwith weight standardization in FL. However, our proposed ANFR model consistently outperforms\nNF-ResNet, often by a considerable margin. For example, on Fed-ISIC2019 with SCAFFOLD,\nANFR surpasses NF-ResNet's mean balanced accuracy by more than 3%. For the FedChest dataset,\nwe employ a large batch size of 128 to maximize the probability that all classes are represented in\neach batch, following best practices. This choice incidentally biases the setting in favor of BN mod-\nels since larger batches reduce inconsistent averaging of mini-batch statistics and BN parameters.\nDespite this bias, ANFR emerges as the top-performing model across aggregation methods. These\nresults indicate that integrating CA with SWS networks provides significant performance gains,\nsuggesting that channel attention is a crucial component in designing effective FL models."}, {"title": "4.3 SAMPLE-LEVEL DIFFERENTIALLY PRIVATE TRAINING", "content": "In privacy-preserving scenarios involving differential privacy (DP) BN cannot be used because cal-\nculating mini-batch statistics violates privacy-preservation rules, and is customarily replaced by GN.\nWe demonstrate the utility of ANFR compared to GN in such settings using the FedPathology set-\nup described in Section 4.1. We first train with strict sample-level privacy guarantees, employing a\nprivacy budget of \u03b5 = 1, followed by training without privacy constraints (\u03b5 = \u221e) to illustrate the\nprivacy/utility trade-off of each model.\nFrom the results presented in Table 4, we observe that\nwith an unrestricted privacy budget, GN and ANFR per-\nform comparably. However, when a strict budget is en-\nforced GN suffers a sharp performance decrease of 17%,\nas expected following previous research (Klause et al.,\n2022), whereas ANFR's average accuracy is reduced by\nonly 3%. ANFR's robustness under DP may be attributed\nto its reliance on weight standardization, which has been\nshown to benefit from additional regularization (Brock\net al., 2021b; Zhuang & Lyu, 2024) such as that provided\nby DP-SGD's gradient clipping and gradient noising. Our\nexperiments show DP training induces a regularization ef-"}, {"title": "4.4 ATTENTION MECHANISM COMPARISON", "content": "Next, we investigate the impact of different attention mechanisms on performance. We compare\nthe SE module used in previous sections with ECA (Wang et al., 2020), and CBAM (Woo et al.,"}, {"title": "5 CONCLUSION", "content": "We introduce ANFR, a simple approach combining the strengths of weight standardization and\nchannel attention to address the challenges of data heterogeneity and privacy at a design level in\nFL. ANFR fills a gap by being the first method to simultaneously work in GFL, pFL, and private\nFL scenarios while being compatible with any aggregation method and offering a robust increase\nin performance. Extensive experiments demonstrate the superior adaptability and performance of\nANFR, as it consistently surpasses the performance of baseline architectures, regardless of the ag-\ngregation method employed. Our results position ANFR as a compelling backbone model suitable\nfor both global and personalized FL scenarios where statistical heterogeneity and privacy guaran-\ntees are important concerns. Our findings highlight the need to look beyond aggregation methods\nas the core component of federated performance and the critical role of architectural innovations in\nreaching the next frontier in private and collaborative settings."}, {"title": "A ADDITIONAL IMPLEMENTATION DETAILS", "content": "A.1 DATASETS\nSkin Lesion Classification on Fed-ISIC2019. Fed-ISIC2019 (Ogier du Terrail et al., 2022) con-\ntains 23,247 dermoscopy images from 6 centers across 8 classes and is a subset of the ISIC 2019\nchallenge dataset. We follow the original pre-processing, augmentation, loss, and evaluation met-\nric of (Ogier du Terrail et al., 2022). This means the loss function is focal loss weighted by the\nlocal class percentages at each client, and the reported metric is balanced accuracy, as counter-\nmeasures against class imbalance. The augmentations used include random scaling, rotation, bright-\nness changes, horizontal flips, shearing, random cropping to 200 \u00d7 200 and Cutout (DeVries, 2017).\nWe train for 80 rounds of 200 local steps with a batch size of 64. The clients locally use Adam\n(Kingma & Ba, 2017), a learning rate of 5e-4, and a cyclical learning rate scheduler (Smith, 2017).\nIn terms of heterogeneity, Fed-ISIC2019 represents a difficult task due to class imbalance and heavy\ndataset size imbalance, with the biggest client owning more than 50% of the data and the smallest\nclient 3%.\nCIFAR-10. Krizhevsky et al. (2009) consists of 50,000 training and 10,000 testing 32 \u00d7 32 images\nfrom 10 classes. We follow the setup of Pieri et al. (2023), specifically the 'split-2' partitioning\nwhere each client has access to four classes and does not receive samples from the remaining six\nclasses. This means we train for 100 rounds of 1 local epoch with a batch size of 32. Clients use\nSGD with a learning rate of 0.03 and a cosine decay scheduler, in addition to gradient clipping to\n1.0. During training the images are randomly cropped with the crop size ranging from 5% to 100%\nand are then resized to 224 x 224.\nCelebA from LEAF. A partitioning of the original CelebA (Liu et al., 2015) dataset by the celebrity\nin the picture, this dataset contains 200,288 samples across 9,343 clients. The task is binary classi-\nfication (smiling vs not smiling). We follow the setup presented in Pieri et al. (2023), training with\n10 clients each round until all clients have trained for at least 30 rounds. The other settings are the\nsame as those for CIFAR-10.\nFedPathology Slide Classification Dataset. A colorectal cancer pathology slide dataset (Kather\net al., 2019), consisting of 100k training images of Whole Slide Image (WSI) patches with labels\nsplit among 9 classes, is used to simulate a federation of 3 clients. We mimic one of the most\nimportant challenges in the WSI field by not color-normalizing the images, which come from two\ndifferent labs with differences in staining protocols. The original 7k color-normalized validation\nset from Kather et al. (2019) is kept as a common validation set. We follow common practice\n(Hsu et al., 2019) to simulate label skew data heterogeneity by using a Dirichlet distribution with\na = 0.5 to partition the data. Since this artificial partitioning is random, we make sure to use the\nsame seeds across architectures and privacy settings to compare on exactly the same partitioning\ninstances. Our pipeline is built using Opacus (Yousefpour et al., 2022) and (\u03b1, \u03b4)-Renyi Differential\nPrivacy (RDP) (Mironov, 2017). Following good practices, the probability of information leakage \u03b4\nis set to 0.1/|D| where |Di | represents each client's dataset size. The DP-specific hyper-parameters\nof the noise multiplier and gradient max norm are set to 1.1 and 1, respectively. Data augmentation\nincludes random horizontal and vertical flips, random color jittering, and random pixel erasing.\nClients use Adam with a learning rate of 5e-5, training for 500 local steps with a batch size of\n64. Federated training is stopped after 25 rounds, which is the point where both architectures have\nexpended, on average, a privacy budget of \u025b = 1. Finally, we train without using DP under the same\nsettings to form a clearer picture of the privacy/utility trade-off of each model.\nChest X-Ray Multi-Label Classification on FedChest. Please refer to Appendix B.1.\nA.2 HYPER-PARAMETER TUNING\nHyper-parameters were optimized for the BN-ResNet and then the same parameters were used for\nall networks. The ranges were as follows:\n\u2022 Local Steps: {100, 200, 500}\n\u2022 Rounds: {20, 50, 75, 100}\n\u2022 Batch size: 32, 64, 128}"}, {"title": "B FEDCHEST CONSTRUCTION AND ADDITIONAL RESULTS", "content": "B.1 CONSTRUCTION AND HYPER-PARAMETERS\nTo create FedChest we use three large-scale chest X-Ray multi-label datasets: CXR14 (Wang et al.,\n2017), PadChest (Bustos et al., 2020) and CheXpert (Irvin et al., 2019). To derive a common dataset\nformat for all three, we need to take several pre-processing steps:\n1. We remove lateral views where present, keeping only AP/PA views.\n2. We discard samples which do not contain at least one of the common diseases, which are:\nAtelectasis, Cardiomegaly, Consolidation, Edema, Effusion, No Finding, Pneumonia, and\nPneumothorax.\n3. We remove \u201cduplicates\u201d which, in this context, means samples from the same patient that\nhave the same common labels but different non-common labels.\n4. We remove 5% from the edge of each image to avoid blown-out borders and artifacts.\n5. We resize the images to 224x224 pixels.\n6. We apply contrast-limited histogram equalization (CLAHE) to the images.\nIn addition to these common steps, some dataset-specific additional pre-processing steps are neces-\nsary, namely setting NaN and 'uncertain' labels of CheXpert to 0 (not present), removing corrupted\nNA rows from CXR14, and removing corrupted images from PadChest.\nAfter pre-processing, CheXpert has twice as many samples as the other datasets, so we further split\nit into two clients, cxp_young and cxp_old using the median age of the patient population (63 years),\nleading to a total of 4 clients with train/val/test splits of (given in thousands): 23.7/15/10\nfor CXR14, 26/15/10 for PadChest, 29.7/15/7.5 for cxp_old and 31/15/7.5. The task is multi-\nlabel classification across the 8 common classes.\nAfter tuning, clients perform 20 rounds of 200 local steps with a batch size of 128, the loss function\nis weighted Binary Cross-Entropy (BCE), and the optimizer Adam with a learning rate of 5e-4,\nannealed over training. Data augmentation includes random shifts along both axes, random scaling\nand rotation, Cutout, and random cropping."}, {"title": "C TABULAR COMPARISON WITH RELATED WORK", "content": "Table 8: Comparison of desirable attribute between our study and related work.  symbolize\na condition is not met, inconsistently met, and fully met, respectively. ANFR fills a gap by being the\nfirst method to simultaneously work in GFL,pFL, and private FL scenarios while being compatible\nwith any aggregation method and offering a robust increase in performance."}, {"title": "D EXTENDED CSI AND ATTENTION WEIGHT ANALYSIS", "content": "D.1 SETUP DETAILS AND PERFORMANCE\nFL training is performed on the extremely heterogeneous 'split-3' partitioning of CIFAR-10 from\nQu et al. (2022), which consists of 5 clients who each have samples only from 2 classes. The training\nparameters are the same as in Qu et al. (2022) and Section 4.1. All the compared models are pre-\ntrained on ImageNet and have a depth of 50 layers, which results in 16 attention blocks for each\nmodel that uses channel attention. To calculate the channel attention weights and class selectivity\nindex distributions, we use the entire test set of CIFAR-10, passing each class separately through the\nmodels to extract class-conditional activations; this is done both before and after FL training.\nFor channel attention weights, this allows us to store the distributions of weights of each model\nfor each class and channel index. For the CSI, we query the nearest ReLU-activated feature maps\nbefore and after each channel attention block-or the equivalent points for the models that do not\nuse such blocks. In timm (Wightman, 2019) terminology, we are referring to the output of act 2 as\nbefore, and act 3 as after. Comparing before and after distributions for the same network, allows us\nto isolate the effect of CA in the case of SE-ResNet and ANFR, and observe the baseline effect of\nmoving through the convolutional block on the CSI distribution in BN-ResNet and NF-ResNet. Fi-\nnally, the histogram of CSI values for each layer is used to draw an approximation of the continuous\nprobability density function for the layer."}]}