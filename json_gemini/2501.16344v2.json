{"title": "WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning", "authors": ["Rajath Rao", "Adithya V Ganesan", "Oscar Kjell", "Jonah Luby", "Akshay Raghavan", "Scott Feltman", "Whitney Ringwald", "Ryan L. Boyd", "Benjamin Luft", "Camilo Ruggero", "Neville Ryant", "Roman Kotov", "H. Andrew Schwartz"], "abstract": "Current speech encoding pipelines often rely on an additional text-based LM to get robust representations of human communication, even though SotA speech-to-text models often have a LM within. This work proposes an approach to improve the LM within an audio model such that the subsequent text-LM is unnecessary. We introduce WhiSPA (Whisper with Semantic and Psychological Alignment), which leverages a novel audio training objective: contrastive loss with a language model embedding as a teacher. Using over 500k speech segments from mental health audio interviews, we evaluate the utility of aligning Whisper's latent space with semantic representations from a text autoencoder (SBERT) and lexically derived embeddings of basic psychological dimensions: emotion and personality. Over self-supervised affective tasks and downstream psychological tasks, WhiSPA surpasses current speech encoders, achieving an average error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates that it is not always necessary to run a subsequent text LM on speech-to-text output in order to get a rich psychological representation of human communication.", "sections": [{"title": "Introduction", "content": "Human communication is inherently multimodal, but AI integration of modalities is often fragmented (Lazaro et al., 2021; Gu et al., 2017), where speech models, such as Whisper (Radford et al., 2022), are often pipelined into text-based language models (LMs) (Chuang et al., 2020) in order to get the most accurate speech-based representations (see Figure 1). Text-based LMs produce richer semantic representations (Wu et al., 2024; Fu, 2024). This often results in redundant computational costs from having two LMs in the pipeline (one within the audio model and one for the text LM) and representations remain incomplete of the full spectrum of human expressions (Zhang et al., 2023; Lian et al., 2023). y important for psychological and social scientific applications of LMs where reptions from text-based LMs demonstrate superior performance than direct speech repdirect speech(??).\nHere, we seek to bridge the semantic and psychological representation gap between speech-based LMs present in audio models and text-based LMs. We introduce a speech encoding model, WhiSPA\u00b9 (Whisper with Semantic and Psychological Alignment), which aligns a pre-trained speech recognition model, Whisper (Radford et al., 2022), with the latent dimensions from SBERT (Reimers and Gurevych, 2019), intended to better capture semantics and deeper psychological information (V Ganesan et al., 2022; Park et al., 2014). Such alignment reduces computational and memory inefficiencies, circumventing the need for a second text encoder, as it enables a unified cross-modal representation between speech and language models. Still, since text is derivable from speech, speech should intrinsically be mappable to the same rich semantic features from the text.\nOur focus on psychological or human-level tasks reflects a growing demand for foundation models"}, {"title": "Background", "content": "This work builds on top of Whisper (Radford et al., 2022), OpenAI's SotA automatic speech recognition (ASR) foundation model. We chose Whisper over other alternatives such as HuBERT and Wav2Vec2-BERT, since previous works (Kyung et al., 2024; Yang et al., 2023) have shown that Whisper has a stronger language encoding module at capturing speaker attributes.\nRecent advances in foundational speech technologies, like Whisper and HuBERT, have vastly improved the performances on speech recognition tasks (Radford et al., 2022; Hsu et al., 2021). However, they have limited ability to capture deeper semantics and speaker attributes compared to a text-based language model (Chen et al., 2024; Dong et al., 2022). Prior works that have addressed this have targeted a very narrow scope of psychological attributes (Busso et al., 2008). These gaps underscore the need for methodologies that bridge speech encoders' acoustic robustness with the psychological depth of text-based language models-a challenge we address by embedding fundamental psychological dimensions present in one's speech.\nMulti-level fusion architectures leveraging both acoustic and lexical features have shown to improve the performance on downstream tasks. For instance, (Zhao et al., 2022) demonstrates that coattention-based early fusion and late fusion using Wav2Vec2.0 (Baevski et al., 2020; Schneider et al., 2019) and BERT (Devlin et al., 2019) outperform SotA emotion recognition benchmarks.Other recent works inject acoustic nuances into language models using textual descriptions of speech characteristics (Wu et al., 2024) or common-sense reasoning through historical utterances from the speaker (Fu, 2024).However, this approach does not fully leverage the cross-modal dependencies between text and audio, as it remains unimodal, relying solely on textual inputs rather than raw acoustic representations.\nPrior works in cross-modal alignment provide foundational insights for this integration. Compositional Contrastive Learning (Chen et al., 2021) distilled audio-visual knowledge into video representations by aligning teacher-student embeddings across modalities, embedding rich semantics from teacher-audio and image models into the student-video model. In another work, Dong et al. (2022) improved the accuracy of intent classification of spoken language by employing a contrastive loss using both speech and language features. These works highlight that the cross-modal alignment objective embeds information from different modalities into shared spaces to capture their relationships, while contrastive learning aids in grouping related inputs across different modalities (e.g., audio and text segments) while separating unrelated pairs (Ye et al., 2022). Efforts to align text and audio include SpeechBERT (Chuang et al., 2020), which adapted BERT's framework (Devlin et al., 2019) to paired speech-text data, and SLAM (Speech-Language Aligned Models) (Bapna et al., 2022), which optimized joint embedding spaces to improve down-"}, {"title": "Data & Tasks", "content": "Audio Datasets. We utilize two psychological, mental health-focused datasets for training and evaluation: WTC-Segments (WTC) (Kjell et al., 2024) and HiTOP-Segments (HiTOP) (Kotov et al., 2022). WTC recordings were completed by patients in a clinic for World Trade Center (9/11) responders who came for a health monitoring visit. HiTOP interviews were completed by outpatients with psychiatric diagnoses who were recruited by the study team to complete a research interview. Both datasets consist of paired audio-text data, ensuring alignment between spoken content and its corresponding textual transcription.\nFrom its source, WTC was curated from ~6 minute interview recordings, on average, of patients responding to both personal and general questions in a structured manner (Kjell et al., 2024). Contrarily, HiTOP followed a semi-structured format, where patients described experiences on set topics while also organically conversing with the interviewer. Once filtered for audio segments solely spoken by patients, interviews generally ranged from 45 to 90 minutes, yielding a voluminous and broadened set of audio segments (Kotov et al., 2022). The recordings were diarized using NVIDIA NeMo and transcribed with whisper-large-v2.\nPsychological Assessments. For each dataset, psychological measures were collected for each user. For WTC, each subject completed the self-reported PTSD CheckList (PCL), yielding scores for four specific subscales: Re-experiencing (REX), Avoidance (AVO), Negative Alterations in Mood (NAM), Hyperarousal (HYP). For HiTOP, trained interviewers provided ratings for the following six psychopathology scales: Internalizing (INT), Dis-inhibition (DIS), Antagonism (ANT), Somatoform (SOM), Thought-Disorder (THD), and Detachment (DET) (Kotov et al., 2022, 2024).\nTo evaluate the encoding ability of WhiSPA for any given audio segment, we manually anno-"}, {"title": "Methodology", "content": "Aligning audio representations directly with a text-based language model allows us to infuse the audio model's latent space with the rich semantic and affective details typically provided by text representations, thereby eliminating the need for a separate text LM. While this approach does not explicitly leverage the unique acoustic features of speech, it prioritizes efficiency by avoiding redundant processing and consistently delivers a semantically enriched representation\u2014an advantage that is particularly critical for psychological and social scientific applications (Lukac, 2024; Chen et al., 2024).\nModel Architecture. We begin with the Whisper\u00b2 encoder-decoder backbone (Radford et al., 2022), which does not run autoregressively. During training, audio segments are previously transcribed with whisper-large-v2, making it entirely self-supervised. Likewise, SBERT and PsychEmb representations were encoded using these transcriptions. As seen in the Whisper (Student) portion of Figure 2, we apply a mean pooling layer to the last hidden state of Whisper's decoder yielding a singular representation for the input audio. This representation is then pooled using a learnable dense layer, and the output serves as our embedding during alignment. This aggregated representation is aligned to the pooled representations from pre-trained SBERT for semantic alignment and the PsychEmb's dimensions for psychological alignment. Throughout this paper, we denote the pre-trained Whisper model as Whisper-384, where the numeric suffix refers to the embedding dimensionality."}, {"title": "Alignment Objective", "content": "While fusion architectures focus on merging acoustic-textual features throughout layers, we contrast this paradigm by directly aligning cross-modal latent spaces for deeper semantic and psychological representations from audio, bypassing the need for task-specific fusion architectures. Our alignment objective aims to improve the semantic and psychological information encoded in Whisper (student) with the help of the representations from a strong text encoding teacher model like SBERT\u00b3 and PsychEmb. In this work, we explore two suitable candidate objective functions to align speech representations with text, which are described below in detail."}, {"title": "Cosine Similarity Loss (CS)", "content": "The success of the cosine similarity-based approach in building geometrically robust representations in SBERT motivated its use as an alignment objective in this work. We apply cosine similarity loss to the pooled audio embeddings and pooled SBERT embeddings, given by the following equation:\n$C^{CS} = \\sum_{i \\in I}  sim(A_i, T_i)$ (1)\n$sim(A_i, T_i) = \\frac{A_i \\cdot T_i}{||A_i|| ||T_i||}$\nwhere $i \\in I = \\{1...N\\}$ refers to the index of audio/text pair in a batch of N samples. A refers"}, {"title": "Noise Contrastive Estimation Loss (NCE)", "content": "The Noise Contrastive Loss (Equation 2) is optimized to increase the cosine similarity between a pair of audio embedding and its corresponding text embedding while simultaneously increasing the differentiation between the audio embedding and randomly sampled text embeddings in that batch (Ye et al., 2022).\n$\\mathcal{L}^{NCE} = \\sum_{i \\in I}  \\mathcal{L}^{NCE}_i$ (2)\n$\\mathcal{L}^{NCE} = -log \\frac{exp(sim(A_i, T_i)/\\tau)}{\\sum_{T_b \\in B(i)} exp(sim(A_i, T_b)/\\tau)}$\nwhere $\\mathcal{L}^{NCE}$ refers to contrastive loss criteria in which pairwise cosine similarities are calculated for each audio embedding with all text embeddings in that batch. Hence, there is only one positive text embedding that pairs with an audio embedding, while the remaining text embeddings from the batch serve as contrastive samples. Let B(i) \u2208 I, where B(i) represents all other SBERT text embeddings in the batch such that $T_b \\neq T_i$ (Ye et al., 2022; Chen et al., 2020; Khosla et al., 2021). The variable $T_b$ denotes the index of an arbitrary, negative SBERT text embedding sample and \u03c4, temperature, represents a tunable scalar parameter which is set to 0.1."}, {"title": "Whisper Semantically Aligned (WhiSA-384)", "content": "WhiSA leverages a student-teacher model paradigm (Hinton et al., 2015; Sanh et al., 2020) to align Whisper's audio-based embeddings with SBERT's text-based embeddings, which serve as the teacher model. SBERT encodes corresponding text sentences into semantically rich embedding vectors, which serve as T in the above equations during training. Whisper's embeddings (A in the above equations), derived from its decoder's last hidden state, are aligned to these SBERT embeddings using the loss functions described above. This process is aimed at WhiSA to learn robust semantic representations directly from audio inputs by minimizing the cosine distance between Whisper and SBERT embeddings as shown in Figure 2."}, {"title": "Adding Psychological Alignment (WhiSPA)", "content": "WhiSPA extends the WhiSA framework by augmenting PsychEmb dimensions into Whisper's. While maintaining the semantic alignment objective, WhiSPA injects the PsychEmb dimensions into the SBERT embeddings under two settings: (1) with replacement: We adopted a naive strategy of replacing the first ten dimensions of SBERT's embedding with the PsychEmb dimensions to maintain the same number of latent dimensions between both models. We use WhiSPA-384 to refer to this. (2) with projection: We concatenate the PsychEmb dimensions to the text embedding from SBERT. Consequently, this requires a 384 \u00d7 10 learnable projection matrix, P, to transform Whisper embeddings of dimensionality 384 to 394, which is then passed through a TanH activation. This model goes by the name WhiSPA-394.\nTo address the numerical instability issues from modeling the PsychEmb dimensions in its absolute range, we standardize and scale them to match SBERT's distribution of embedding values. Refer to Appendix subsection A.2 for more information on training."}, {"title": "Results & Discussion", "content": "We consider three popular, robust speech encoders as baselines: Wav2Vec2-BERT\u2074 (Communication et al., 2023; Chung et al., 2021), HuBERT\u2075 (Hsu et al., 2021), and Whisper (Radford et al., 2022), which are referred to as W2V2B, HuBERT, and Whisper-384, respectively. We measured the effectiveness of these embeddings by computing Pearson correlation coefficient (r) and mean squared error (mse) over a 10-fold cross-validated ridge regression model for each task. For each model variant, we encode audio segments for each participant and aggregate them with a statistical mean to represent person-level embeddings for the tasks in Table 2 and Table 3."}, {"title": "Alignment improved the models' ability to capture psychological dimensions from language.", "content": "We evaluated the speech-based models' ability to capture the psychological dimensions of language by comparing our models' predictions to PsychEmb derived values at the segment level. As summarized in Table 2, we found that both semantic (WhiSA) and psychological alignments (WhiSPA) significantly outperformed traditional speech-based models (Wav2Vec and Whisper) across all ten dimensions on both metrics. Compared to Whisper, which was evidently a stronger baseline than Wav2Vec2 (Avg\u2206 = 36 Pearson points for WTC & 21 points for HiTOP), Our semantic alignment method showed a marked improvement in performance, with an average of 11 in Pearson points for WTC and 2 in HiTOP. A paired t-test was used to confirm that all improvements over Wav2Vec and all improvements over Whisper, except for 4 outcomes in HiTOP, were statistically significant (p < .05). This result highlighted our alignment methods improved the speech model's ability to capture psychological dimensions in language (PsychEmb).\nInterestingly, deriving psychological estimates from semantic dimensions (WhiSPA-394) was consistently better than the replacement (WhiSPA-384) of 10 semantic dimensions with PsychEmb dimension. This shows the importance of curating the semantic dimensions before replacing them with different embeddings.\nWe also observed that the alignment increased"}, {"title": "Semantic-Psychological alignment is SotA for audio-based psychological assessments.", "content": "Table 3 shows that the improvements brought by our aligned models over traditional models were preserved even when evaluated on a spectrum of downstream psychological assessment tasks. In particular, the alignment showed a stark increase in capturing deeper psychological conditions such as INT (internalizing) (\u2265 16 Pearson points) and DIS (disinhibition) (\u2265 20 Pearson points) from very long durations of speech data. Consistent with behaviours exhibited with PsychEmb dimensions, in Table 2, semantic-psychological alignment from semantically-derived psychological dimensions (WhiSPA-394) performed the best, followed by semantic-psychological alignment from replacement (WhiSPA-384) and finally semantic-only alignment (WhiSA-384). For these tasks, we averaged the segment-level representations of the interview audio file to produce a person-level embedding. These embeddings were used to perform 10-fold cross-validation with a ridge regression model, and its performance was measured using Pearson correlation coefficient (r) and mean squared error (mse).\nThe success of WhiSPA-394 can be attributed to its integration of psychological feature alignment, which complements semantic alignment by explicitly encoding affective dimensions such as valence and arousal. The improvements in outcomes like INT and DIS further support this interpretation since these constructs often rely on subtle vocal"}, {"title": "Contrastive loss criteria led to richer representations of audio.", "content": "Investigation of the choice of alignment objective towards performance (Table 4) revealed that Noise Contrastive Estimation (NCE) consistently produced a better-aligned model than cosine similarity (CS). This is likely because NCE optimizes for discriminative learning, encouraging more separation between positive and negative samples in the embedding space (Ye et al., 2022), enhancing the model's ability to encode nuanced semantic and psychological cues. When comparing WhiSPA-394 and WhiSPA-384, we notice the recurring trend with NCE granting a greater optima during alignment than CS as exemplified in"}, {"title": "WhiSPA captures semantics without the need for appending SBERT representations.", "content": "The last row in Table 5 underscores the marginal increase in correlations after appending SBERT embeddings to WhiSPA. WhiSPA, trained through a student-teacher alignment paradigm, appears to reach a semantic and psychological optimum during convergence. This is evident in its substantial performance gains over Whisper, which lacks the semantic and psychological depth provided by language models. However, the potential of cross-modal alignment may be constrained by the representational efficacy of the teacher model(s). On human-annotated audio segments, all of the WhiSPA variants achieve substantial improvements in capturing acoustic valence. In comparison with Whisper-384, WhiSPA-384, exhibits a gain of +15 Pearson points in VAL (acoustic valence) which exemplifies the reduction in the semantic/psychological gap between audio models and text-based models. Notably in Figure 8, WhiSPA-394 demonstrates clear improvements in specific measures such as INT and VAL, with gains of +3 and +8 Pearson points, respectively, when compared to its teacher, SBERT-384.\nUltimately, these findings highlight two important observations: (1) WhiSPA effectively captures nearly all the information encoded by its text-based teacher model, SBERT. (2) The marginal returns from appending text-based representations indicate that WhiSPA successfully learns to encode the critical semantic and psychological cues provided by its teachers, reflecting the success of the distillation."}, {"title": "WhiSPA's representations are interpretable through language semantically associated with psychological dimensions.", "content": "Table 6a shows that n-grams known to be indicative of PTSD severity from prior studies (Kjell et al., 2024) \u2013 including first-person pronouns, experienced symptoms, psychological distress, and negative affect \u2013 yield significantly higher correlations with WhiSPA's predictions compared to Whisper. In contrast, Table 6b reveals that language discussing relationships and positive affect is more negatively associated with WhiSPA's scores. These findings indicate that the contrastive loss training effectively aligns the latent space with rich semantic and psychological representations, capturing psychologically relevant linguistic markers more robustly. The highly semantic latent spaces of text-based LMs are reflected in WhiSPA's representations, especially for psychological nuances in spoken language. More quantitative analysis of our model can be found in Appendix subsection A.4"}, {"title": "Conclusion", "content": "We claim that WhiSPA is a significant step toward more accurate representations of human communication by addressing the modal gap between text and audio, as language models often outperform audio models in predicting psychological attributes. By aligning WhiSPA's representations with SBERT's representations enriched with PsychEmb, we found consistent improvement for ten self-supervised tasks and significantly greater accuracies over 11 downstream psychological tasks. We observed only marginal improvements when appending SBERT representations to WhiSPA's, implying that the distillation process effectively captures the semantic features provided by the teacher language model. Our findings exemplify WhiSPA's effectiveness in extracting semantic and psychological features from speech, enhancing SotA audio representations for psychological and mental health assessments."}, {"title": "Limitations", "content": "While WhiSPA demonstrates significant advancements in providing semantically enriched audio"}, {"title": "Ethical Implications", "content": "The multimodal WhiSPA model holds significant potential for improving mental healthcare assessments by providing rich insights into individuals' states of mind through speech analysis. However, multimodal approaches increase ethical considerations due to the richer and more diverse forms of personally identifiable information (PII) they capture compared to unimodal models. In addition to text content, the WhiSPA model processes acoustic and prosodic features including tone of voice, speech patterns, and emotional expressions which can inadvertently reveal sensitive details like gender, ethnicity, emotional state, and health conditions. This expanded data scope raises the risk of re-identification, making it essential to implement stringent data security and handling, in-"}, {"title": "Security & Privacy.", "content": "Moreover, the potential for misuse or unauthorized exploitation of such detailed multimodal data necessitates robust ethical guidelines for its storage, processing, and application. Transparency in how these models are trained and used is critical to building trust among clinicians and patients. Finally, ongoing efforts to mitigate algorithmic biases and ensure fairness are important, as errors in multimodal assessments could disproportionately impact vulnerable populations or lead to incorrect diagnoses if not carefully managed.\nThe WTC and HiTOP recordings took place in a clinical setting where each participant gave consent and was fully informed about the study, that it was voluntary to take part, and that they had the right to withdraw at any time without giving a reason or that it would affect their treatment. After the interview, participants were debriefed (for more details about the WTC data collection, see (Kjell et al., 2024); for more details about the HiTOP data, see (Kotov et al., 2022, 2024). The studies and data uses were approved by the Institutional Review Board at an undisclosed university for privacy reasons."}, {"title": "Software.", "content": "Adhering to the ideals of open and reproducible science, we will make the WhiSPA software code base, along with the trained models and secure dimensional representations of the data, openly available. These representations strictly comply with established security protocols, ensuring that no individual can be identified nor any anonymity safeguard compromised. Nevertheless, direct access to the underlying data remains restricted in accordance with privacy and security measures."}, {"title": "Acknowledgments", "content": "Additionally, AI-based tools were employed throughout the project to assist in code development and report formulation, including the use of ChatGPT and other similar consumer generative AI. Such integration aligns with established best practices and guidelines, ensuring that the technical accuracy, integrity, and scientific rigour of the work remain uncompromised while benefiting from enhanced efficiency and streamlined workflows.\nThe work presented in this paper stems from the immense hours of audio recordings from the WTC"}, {"title": "Appendix", "content": "A.1 Data Description\nA.1.1 HiTOP.\nThe HiTOP dataset consists of video-recorded interviews conducted between World Trade Centre responder participants and clinicians. Each recording is annotated with the outcomes derived from the HiTOP structured interview, which includes a standardized set of questions designed to assess a comprehensive set of mental health dimensions, including aspects of internalizing (e.g., questions about distress and fear), dis-inhibited externalizing (e.g., questions about substance abuse and antisocial behaviours) and more.\nOutcomes in HiTOP The HiTOP outcomes were derived from the structured clinical interview (Roman and Meyer, 2024), where we used the total score of the six dimensions including: i) internal-izing (INT; e.g., dysphoria, lassitude), ii) disinhibited externalizing (DIS; e.g., alcohol use, drug use), iii) antagonistic externalizing (ANT; e.g., attention seeking, callousness), iv) somatoform (SOM; e.g., conversion, somatization), v) thought disorder (THD; e.g., psychotic and disorganized thought patterns), vi) detachment (DET; e.g., intimacy avoidance, suspiciousness)\nA.1.2 WTC.\nIn the WTC dataset, participants were recorded in a private room during their clinical visit while responding to questions displayed on a screen as part of an automated clinical interview. These questions prompted participants to reflect on both positive (e.g., What are three things you currently look forward to the most?) and negative aspects of their lives across different time frames (past, present, and future). Topics included general life experiences (e.g., the best and worst experiences, challenges, and support systems) and significant events such as COVID-19 and 9/11 (e.g., How does 9/11 affect you now?). A full list of the questions is provided in (Kjell et al., 2024).\nTo enhance generalizability, the questions were designed to be broad and used everyday language, avoiding clinical jargon or references to specific symptoms. Instructions on the screen advised participants not to read the questions aloud and to aim for at least 60 seconds of response time per question. Throughout the development phase, the questions were refined over three iterations to improve"}, {"title": "Training", "content": "The research done for devising WhiSPA's framework resulted from iterations of tweaking and testing architectures, loss criteria, parameters, and hyperparameters.\nFor the methodology presented in this paper, we provide the following configurations for reproducibility:\nPooling: MEAN. Learning Rate: 1 \u00d7 10-5. Weight Decay: 1 \u00d7 10-2. Temperature (\u03c4): 0.1. Batch Size: 900. Number of Epochs: 50. Number of workers (CPU cores): 16. These configurations result in a total average training time of ~ 20 hours.\nWe discovered that the efficacy of Equation 2 highly depends on the batch size. It should be stated that larger batch sizes allow for greater degrees of repulsion and attraction in the cross-modal embedding space. While training WhiSA and WhiSPA, we utilized a batch size of 900 and distributed them across 3 NVIDIA RTX A6000 devices with 48GB of VRAM each.\nAdditionally, we use open-source licensed pre-trained models from HuggingFace. Our programmatic implementation for deep learning is done with PyTorch. When it comes to evaluation, we utilize Differential Language Analysis Tool Kit (DLATK) for correlating regression results across specified groups (i.e., user_id or segment_id)\nCosine similarity is sensitive to the relative magnitudes of the vectors being compared. If the added ten dimensions of psychological features have a very different scale or distribution from SBERT embeddings, they could dominate or skew the cosine similarity computation. Once either loss function is applied, (1) or (2), WhiSPA embeddings remain semantically aligned with SBERT while also encoding meaningful affective cues for downstream tasks.\nDuring the training of WhiSPA, we experimented with identifying which dimensions of the teacher-model, SBERT, have the lowest correlations with PsychEmb dimensions to replace those dimensions. We decided that this approach may lead to statistical biases when training, and so we naively replaced the first 10 dimensions. One"}, {"title": "Annotations", "content": "Please note that the annotators were expert psychologists and co-authors.\nThe documentation accompanying the iHiTOP interview dataset was utilized to report the coverage of its domains, demographic information, and other relevant details. The dataset's focus on structured psychological interviews and its linguistic properties were described in the paper to contextualize its relevance to this research. This information was presented to ensure transparency and reproducibil-"}, {"title": "Quantitative Analysis", "content": "PsychEmb's lower correlations in Figure 8 should not be mistaken for poor performance. With only 10 dimensions, PsychEmb representations achieve a staggering 24 and 22 Pearson points on INT and DIS respectively, emphasizing its validity as the psychological teacher. WhiSPA's consistent improvement over the audio models is attributed to the semantic and psychological dimensions that SBERT and PsychEmb offer. Notably, WhiSPA exemplifies drastic improvements in prediction accuracy for VAL, INT, THT, and PCL compared to Whisper-384.\nWhile WhiSPA demonstrates substantial ad-vancements, surpassing even its text-based LM teacher, SBERT-384, it remains inherently constrained by the representational capacity of the teacher model. If the teacher's capabilities are limited, these deficiencies inevitably carry over to the student, even after distillation. This is evident in the ARO column, where arousal \u2013 an affective dimension is more accurately conveyed through acoustic cues. However, WhiSPA struggles to capture and preserve the acoustic information, instead predominantly aligning with the semantic representations provided by SBERT, thus limiting its ability to fully represent the nuanced affective content inherent in speech.\nBeyond demonstrating superior alignment with established PTSD markers, Table 6 highlights WhiSPA's enhanced sensitivity to psychologically meaningful language patterns. Table 6a shows that n-grams reflecting personal experiences, self-referential content (e.g., first-person pronouns), and negative affective states correlate more strongly with WhiSPA's predictions than with those of Whisper. WhiSPA appears better attuned to indicators of psychological distress, anxiety, and trauma symptoms--an advantage likely stemming from the contrastive alignment objective with text-based representations. The model's capacity to detect nuanced emotional and cognitive expressions in spoken language is further supported by its higher effect sizes on known PTSD-relevant n-grams, underscoring that semantically oriented embeddings can bolster the recognition of clinically significant markers in audio data.\nMeanwhile, Table 6b points to a distinctive negative association between WhiSPA's predicted severity scores and n-grams referencing positive affect or social relationships. This result suggests that the same semantically focused latent space that amplifies negative or distress-related terms also filters out language tied to more adaptive or supportive experiences. In practical terms, such an effect could be advantageous for screening or early detection: positive affect or relational talk might serve as a buffer or resilience indicator, thereby inversely correlating with predicted symptom severity. Taken together, these findings highlight the unique strength of WhiSPAs in capturing a wide spectrum of psychologically relevant linguistic markers, surpassing the granularity offered by audio models alone."}]}