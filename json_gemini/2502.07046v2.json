{"title": "SnipGen: A Mining Repository Framework for Evaluating LLMs for Code", "authors": ["Daniel Rodriguez-Cardenas", "Alejandro Velasco", "Denys Poshyvanyk"], "abstract": "Large Language Models (LLMs), such as transformer-based neural networks trained on billions of parameters, have become increasingly prevalent in software engineering (SE). These models, trained on extensive datasets that include code repositories, exhibit remarkable capabilities for SE tasks. However, evaluating their effectiveness poses significant challenges, primarily due to the potential overlap between the datasets used for training and those employed for evaluation. To address this issue, we introduce SnipGen, a comprehensive repository mining framework designed to leverage prompt engineering across various downstream tasks for code generation. SnipGen aims to mitigate data contamination by generating robust testbeds and crafting tailored data points to assist researchers and practitioners in evaluating LLMs for code-related tasks. In our exploratory study, SnipGen mined approximately 227K data points from 338K recent code changes in GitHub commits, focusing on method-level granularity. SnipGen features a collection of prompt templates that can be combined to create a Chain-of-Thought-like sequence of prompts, enabling a nuanced assessment of LLMs' code generation quality. By providing the mining tool, the methodology, and the dataset, SnipGen empowers researchers and practitioners to rigorously evaluate and interpret LLMs' performance in software engineering contexts.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated signif- icant success across diverse software engineering (SE) tasks, including code auto-completion [1]\u2013[5], code summarization [6], [7], code review [8], [9], code translation [10], clone detection [11], [12], and program repair [13]\u2013[19]. LLMs are neural models trained on huge datasets including complete GitHub repositories. Common testbeds for evaluating LLMs for code such as HumanEval, MBPP and CodeXGlue are no longer sufficient [20]. In addition, as benchmarks and testbeds are released, new LLMs probably already seen those testbeds. Therefore the testbeds are prone to be outdated as soon as a new LLM is released.\nLLMs perform complex tasks by relying on statistical knowledge acquired from data distributions, a phenomenon described by Wei et al. as emerging capabilities [21]. Given the limited understanding of the nature of this phenomenon, we can formulate an important question: under what conditions LLMs produce the desired output? Prompt engineering ad- dresses this question by harnessing these capabilities, guiding LLMs to make more accurate predictions. Furthermore, given that LLMs can extract rules from the provided context (i.e., in- context learning), prompt engineering is a natural and intuitive way for people to use LLMs.\nRecent studies have demonstrated that LLMs exhibit im- provements in accuracy for downstream tasks when prompts are enhanced and augmented [22], [23]. Moreover, new meth- ods for crafting better prompts are being explored. For ex- ample, Beurer-Kellner et al. [24] introduce the idea of Lan- guage Model Programming (LMP) which combines text-based prompting with scripting. Furthermore, Wei et al. [25] shows that the incorporation of Chain-of-Thought (CoT) significantly improves the ability of LLMs to perform complex reasoning.\nUnderstanding the internal mechanisms of LLMs presents a significant challenge. Current datasets and benchmarks often lack the curated data necessary for thorough performance analyses. Therefore, there is a critical need for consistent data points to effectively evaluate the performance of LLMs across various SE tasks. We argue that well-designed testbeds and prompts are the key to accurately assessing LLMs understand- ing of complex information, such as task-related semantics.\nTo bridge the gap between existing datasets and bench- marks, we developed SnipGen. SnipGen is a framework to collect source code snippets from GitHub. Each snippet is automatically augmented with prompts tailored for various software tasks. Practitioners and researchers can query and generate new prompts according to the SE task and experiment with different configurations for evaluating LLMs for code. Our goal is to provide resources that can more accurately assess the performance of LLMs and aid in the construction of more detailed benchmarks.\nThe contributions of this paper are listed as follows: 1) A Framework for mining software repositories and crafting data points augmented with prompts for specific SE downstream tasks. 2) a generated testbed comprising Python snippets with calculated features from the AST, natural language, and vulnerabilities analysis [26]. 3) Prompt-generated dataset with mutated snippets crafted for Code Completion, Commit generation, and Code summarization. 4) source code and complementary material used in this research are published in an open-source repository [27]."}, {"title": "II. THE SnipGen FRAMEWORK", "content": "SnipGen is a framework to extract snippets at method granularity from GitHub. SnipGen follow steps for curating the extracted raw data and take features from the data such as the number of identifiers, vocabulary, tokens, etc. Features derived from their AST representations and further complementary data. Our dataset can potentially improve the quality of the predictions in downstream tasks by augmenting the prompts, thereby enabling LLMs to perform more effectively."}, {"title": "A. Data Point Feature Structure", "content": "SnipGen can collect a set of Python methods that serve as evaluative data points. Each data point has associated features at seven dimensions as observed at Fig. 2. These seven dimensions describe the static feature from the snippet. We aim to link code fragments with their properties. The first dimen- sion corresponds to snippets' identification, which includes the commit_id (i.e., commit hash), repository name, path, file_name, fun_name, commit_message. The second dimension is related to the associated documentation docstring. The doc- string extended to complementary natural language features such as n_words, vocab_size, language, and n_whitespaces.\nThe third dimension corresponds to the snippet's syntactic in- formation, which includes the actual code base, n_ast_errors,\nn_ast_levels, n_ast_nodes,n_words, vocab_size, token_count, and n_whitespaces. The fourth dimension corresponds to canonical software metrics, which include nloc, complexity,\nn_identifiers. The fifth dimension depicts the span position for vulnerabilities detected from the code snippet. The sixth dimension is associated with the snippet mutation when the code is randomly cut one line after the signature, therefore SnipGen identifies the signature as the original snippetID and cut code. Finally, the seventh dimension comprises the linked features to the generated prompt. SnipGen labels the prompt to the SE task and the prompt configuration."}, {"title": "B. Software Engineering Tesbed Task", "content": "Data curation, pre-processing, and data validation produce a testbed oriented to evaluate a model. For instance, a RandomCut and WithDocString testbeds might evaluate the model to transform the RandomCut version of a method into its complete code (code). P5 is designed to ask the model to generate the commit message from the mutated code and the actual code. Lastly, in code summarization, P6 provides only the code, which the model uses to generate a corresponding summary."}, {"title": "D. SnipGen Prompt Generation and Use", "content": "The SnipGen framework is designed to select a SE task and evaluate a LLM using the testbed with a given context with a designed prompt. Fig. 3 depicts the options a practitioner has to evaluate a LLM. The database supports a query to filter the snippets according to the SE task, for instance for code completion we can sample the snippets with linked docstring with more than 10 words, this provides the task context (see, Fig. 3 section \u24b6). The prompt generation might contain a mutated snippet such as RandomCut to perform the required task. For example, for code completion, we will need a partial code snippet that must be auto-completed by the LLM therefore we need to cut the original snippet smartly. SnipGen can use the RandomCut method to split the code beyond the method signature. Practitioners can still evaluate the model using canonical datasets and metrics to compare against the new collected SnipGen testbed."}, {"title": "III. EXPERIENCE REPORT", "content": "In this section, we describe our experience of using SnipGen for collecting a testbed and generating a set of prompts. We also briefly describe three use cases illustrating how SnipGen was successfully used to evaluate LLMs for code."}, {"title": "A. SnipGen Testbed Generation", "content": "The experience with SnipGen begins by mining reposi- tories from GitHub, as detailed in Sec. II. We focused on the most popular Python repositories, applying the follow- ing query filters: language:Python fork:false size:>= 30000\npushed:>2021-12-31 stars: > 2000. The query gathers the most popular repositories in Python. We selected the top 200 repositories including keras, numpy, pandas, sentry, etc. We extracted the new snippets reported on commits between 2022 and 2023 from selected repositories. Then we used the data curation to remove duplicates and feature extraction to generate and extract the associated features. We configured a 0.7 similarity threshold [35], [36] to de-duplicate snippets using HuggingFace tokenizer BPE. SnipGen saves the raw data and their features into a JSON and a database. We randomly validated 960 out of \u2248 227K data points to confirm the extracted features and the meaningfulness of the Docstring and linked code.\nWe sampled until 5k data points from the RawData testbed to construct six testbeds, each tailored for a specific SE task as described at Sec. II-B. To create RandomCut, we selected data points with more than 10 tokens or 100 characters, and subsequently, each data point was randomly truncated after the method signature. For SummarizationGen and FromCommit, we filtered RawDataDocstring data points with more than\n10 words or 50 characters. Table. II provides information about the SE task associated with each curated testbed, the percentage of detected duplicates, the final size, and the generated number of prompts."}, {"title": "B. Successful Use Cases", "content": "Galeras [31]: Galeras is a benchmark for measuring the causal effect of SE prompts for code completion. Galeras configures a set of treatments to assess the influence of potential confounders on the outcomes of ChatGPT (i.e., GPT- 4). The selected confounders are: prompt_size (from prompts),\nn_whitespaces (from documentation), token_counts, and nloc\n(from code_features). This use case of SnipGen demonstrates that prompt engineering strategies (such as those listed in Table I - processing prompt) have distinct causal effects on the performance of ChatGPT.\nSyntaxEval [33]: In this use case Syn taxEval evaluates the ability of Masked Language Models (i.e., Encoder-based Transformers) to predict tokens associated with specific types in the AST representation (i.e., syntactic features). SyntaxEval used SnipGen to construct a code completion testbed with ap- proximately 50K Python snippets. SyntaxEval aims to account for potential confounders such as ast_data and code_features (illustrated in Fig. 2), the analysis revealed no evidence that the evaluated syntactic features influenced the accuracy of the selected models' predictions.\nASTxplainer [32]: ASTxplainer is an explainability method designed to assess how effectively a LLM (e.g., decoder- based transformers) predicts syntactic structures. ASTxplainer aggregates next-token prediction values through syntactic de- composition, quantified as AsC-Eval values to evaluate the effectiveness. ASTxplainer findings reveal that the ability to predict syntactic structures strongly depends on the LLM's parameter size and fine-tuning strategy. Furthermore, causal analysis controlling for confounding variables (e.g., ast_data and code_features) shows that AsC-Eval values at the snippet level negatively impact the cross-entropy loss of the evaluated LLMs."}, {"title": "IV. SIMILAR DATASETS", "content": "Significant efforts have produced datasets for evaluating LLMs in SE tasks, including DeepFix for program repair [37], CodeContest and CoNaLa for program synthesis [38], [39], and CodeSearchNet for code retrieval [40]. Expansions like CodeXGLUE [41], xCodeEval [42] target broader tasks, while benchmarks such as HumanEval and SecurityEval focus on functional correctness and vulnerabilities [43], [44]. Despite these efforts, existing datasets often suffer from contamination [20], [45], with overlaps between training and evaluation data, and benchmarks are prone to memorization by models [46], limiting their effectiveness in assessing true generalization.\nRecent research work has explored the dynamic generation of prompts and testbeds, for instance, EvoPrompt is a frame- work for automatic discrete prompt optimization that connects LLMs with Evolutionary Algorithms [47]. Evol-instruct is a systematic approach to generate instruction-response pairs by iteratively improving prompts and responses through model self-enhancement [48]. LiveCodeBench is a benchmark for evaluating LLMs designed to generate code [20]. Unlike Snip- Gen, LiveCodeBench addresses issues of data contamination by using continuously updated problems from online coding competitions."}, {"title": "V. LIMITATIONS AND FUTURE WORK", "content": "Documentation Quality Analysis: Meaningfulness evalua- tion for the docstring and linked code can not be automatized and depends on the project context. To handle this limitation, we conducted a manual validation. As part of future work, SnipGen should streamline the manual validation process and ascertain the significance of comments and documentation within the snippets.\nVulnerability Detection: The detection of vulnerabilities is reliant solely on the CodeQL tool and its updates; we did not employ any alternative tools to validate these results.\nAssumption Regarding Snippet Exposure: SnipGen miti- gates to select \u201ccontaminated\" data (i.e., already seen snippets) by selecting snippets from specific commit time windows. A practitioner can specify the time windows depending on the LLM release date. SnipGen aims to reduce data contamination by including a prompt and variating the cut code. However, it's important to note that the extracted code changes might include older lines of code or reused code fragments. Our evaluation does not encompass the entire project history to identify older references.\nFor future work, 1) we propose to extend this dataset to support multiple programming languages; 2) Integrate a wider number of SE tasks. 3) Rank each data point according to the cyclomatic complexity number of AST nodes, documentation, and number of identifiers. The rank will prove better criteria on which snippets are more interesting to evaluate the LLM."}]}