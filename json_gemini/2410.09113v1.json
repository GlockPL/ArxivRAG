{"title": "M\u00b2-ViT: Accelerating Hybrid Vision Transformers with Two-Level Mixed Quantization", "authors": ["Yanbiao Liang", "Huihong Shi", "Zhongfeng Wang"], "abstract": "Although Vision Transformers (ViTs) have achieved significant success, their intensive computations and substantial memory overheads challenge their deployment on edge devices. To address this, efficient ViTs have emerged, typically featuring Convolution-Transformer hybrid architectures to enhance both accuracy and hardware efficiency. While prior work has explored quantization for efficient ViTs to marry the best of efficient hybrid ViT architectures and quantization, it focuses on uniform quantization and overlooks the potential advantages of mixed quantization. Meanwhile, although several works have studied mixed quantization for standard ViTs, they are not directly applicable to hybrid ViTs due to their distinct algorithmic and hardware characteristics. To bridge this gap, we present M\u00b2-ViT to accelerate Convolution-Transformer hybrid efficient ViTs with two-level mixed quantization. Specifically, we introduce a hardware-friendly two-level mixed quantization (M\u00b2Q) strategy, characterized by both mixed quantization precision and mixed quantization schemes (i.e., uniform and power-of-two), to exploit the architectural properties of efficient ViTs. We further build a dedicated accelerator with heterogeneous computing engines to transform our algorithmic benefits into real hardware improvements. Experimental results validate our effectiveness, showcasing an average of 80% energy-delay product (EDP) saving with comparable quantization accuracy compared to the prior work.", "sections": [{"title": "I. INTRODUCTION", "content": "Built upon the self-attention mechanism, Vision Transformers (ViTs) have achieved competitive performance in the computer vision [1] and multi-modality [2] fields. However, their high computational and memory overheads limit their deployment on resource-constrained edge devices [3]. Particularly, the self-attention mechanism has quadratic computational complexity and is widely recognized as a critical hindrance [3]\u2013[5]. To solve this limitation, various works have proposed efficient ViTs [4], [6], which incorporate more efficient attention mechanisms with linear complexity and typically feature Convolution-Transformer hybrid architectures [4], [6]. For example, as depicted in Fig. 1, the state-of-the-art (SOTA) efficient ViT, dubbed EfficientViT [4], mainly comprises lightweight Multi-Scale Attention (MSA) modules and MBConvs [7], offering much higher accuracy and better hardware efficiency than standard ViTs [1]."}, {"title": "II. BACKGROUND", "content": ""}, {"title": "A. The Structure of EfficientViT", "content": "As depicted in Fig. 1, EfficientViT has two key attributes: lightweight attention with linear computational complexity to enhance hardware efficiency and Convolution-Transformer hybrid architecture to boost performance. Specifically, EfficientViT primarily comprises MBConvs [7] for local information processing and lightweight MSAs for global information extraction. Each MBConv consists of two pointwise convolutions (PWConvs) sandwiched by a depthwise convolution (DWConv). Besides, the ReLU-based global attention is the core component in each lightweight MSA. This component substitutes the Softmax function in vanilla self-attention with a ReLU-based similarity function, which enables the utilization of the associative property of multiplications to decrease computational complexity from quadratic to linear [4]."}, {"title": "B. Uniform Quantization", "content": "Uniform quantization is a basic and most widely adopted quantization method, which converts the floating-point X into b-bit integer $X_q$ as follows:\n$X_{Uniform} = clip ([X/S] + Z, 0, 2^b \u2212 1), (1)$\nwhere S and Z are the scaling factor and zero point, respectively, and they can be determined as follows:\n$S = \\frac{max(X) - min(X)}{2^b - 1}, Z = clip (\\lceil-\\frac{min(x)}{S}\\rceil, 2^b-1)$. (2)\nWhen performing convolutions, each input channel is processed by the corresponding weight channel within filters and then summed along the channel dimension to produce output. Thus, to eliminate floating-point computations, input activations are generally layer-wise quantized, using a common scaling factor for all channels and thus allowing summations to be performed in the integer domain [8]. Similarly, for weights, since summations are constrained to channels within filters, they are typically filter-wise quantized, where all weight channels within each filter share the same scaling factor [8]."}, {"title": "III. TWO-LEVEL MIXED QUANTIZATION (M2Q)", "content": ""}, {"title": "A. Observations", "content": "As shown in Fig. 1, EfficientViT primarily comprises three types of layers: (1) DWConvs, where each filter (kernel) has only one channel to process the corresponding input channel for obtaining the output, (2) PWConvs, which is equivalent to generic convolutions with 1 \u00d7 1 kernel size, and (3) matrix multiplications (MatMuls) within lightweight MSAs. Based on operation intensity [12], we divide these layers into two categories: (1) computation-intensive layers, including PWConvs and MatMuls, and (2) memory-intensive layers, such as DWConvs, and investigate their algorithmic properties to explore advanced quantization opportunities."}, {"title": "1) Computation-Intensive Layers", "content": "To identify potential quantization opportunities, we first choose PWConvs as representative layers and visualize their weight distributions. As seen, the weight distributions across different filters vary from Uniform (Fig. 2a) to Gaussian (Fig. 2b). This observation can be also noted in MatMuls. This variation indicates that merely adopting uniform quantization is sub-optimal and enables the integration of Power-of-Two (PoT) quantization to enhance hardware efficiency. Specifically, uniform quantization uniformly distributes quantization bins across all values, making it more appropriate for filters with Uniform distributions. In contrast, PoT quantization (see Fig. 2c), which allocates more quantization bins for smaller values, is more suitable for filters with Gaussian distributions. Formally, it is expressed as:\n$W^{Q}_{POT} = s\u00d72^p, where\ns = sign(W), p = clip(\\lceil log_2|W/S|\\rceil, \u22122^{b-1},0)$. (3)\n$W and W^{Q}_{POT}$ are floating-point and PoT quantized weights, respectively. b denotes quantization bit-width. S represents the scaling factor and can be determined by S=$\\frac{max(W)-min(W)}{2}$, aiming to re-scale W to [0, 1]. For example, if W=-0.26, S=2, b=5, then s=-1 and p=-3. By doing this, multiplications between activations A and PoT quantized weights $W^{Q}_{POT}$ can be substituted with bitwise shifts, as formulated in Eq. (4), thus significantly reducing computational costs.\n$A \u00d7 W^{Q}_{POT} = A \u00d7 s \u00d7 2^p = s \u00d7 (A >> p)$. (4)"}, {"title": "2) Memory-Intensive Layers (DWConvs)", "content": "Each filter in DWConvs features only one weight channel for handling the corresponding input channel to generate output, which significantly lowers computational costs but limits data reuse opportunities and increases bandwidth demand. Thus, the primary challenge in DWConvs is improving data access efficiency. Fortunately, the small amount of weights per filter of DWConvs inherently offers an opportunity to implement low-bit filter-wise quantization, reducing the bandwidth requirement."}, {"title": "B. Two-Level Mixed Quantization Strategy", "content": "Motivated by the above observations, we propose a two-level mixed quantization (M\u00b2Q) strategy for efficient ViTs [4], which includes (1) mixed-scheme quantization (uniform and PoT) for computation-intensive layers that replace partial multiplications with hardware-efficient bitwise shifts to enhance computational efficiency and (2) mixed-precision quantization to reduce memory access overhead of memory-intensive layers. Note that our M2Q is exclusively applied to weights, with activations still using standard 8-bit uniform quantization due to their higher quantization sensitivity [8], [13]."}, {"title": "1) Mixed Quantization Schemes for Computation-Intensive Layers", "content": "As explained in Sec. III-A-1), the heterogeneous weight distributions of computation-intensive layers offer us an opportunity to use PoT quantization to reduce computational costs. However, as demonstrated in Table I, it inevitably yields accuracy drops compared to 8-bit uniform quantization. This is because PoT quantization prioritizes accurately representing smaller values near zero, while overlooking bigger values that contribute significantly to final outputs. To better balance the major small weights and the minor but important big weights within filters exhibiting Gaussian distributions, we shift our attention to additive PoT (APoT) quantization [14], which is essentially the combination of two PoT components:\n$W^{Q}_{APOT} = S X (2^{p_1} + 2^{p_2}), s = sign(W)$, (5)\nwhere p1/p2 are PoT values similarly in Eq. (3). By combining two PoT components, APOT strikes a balance between PoT and uniform quantization (see Fig. 2d), thus enhancing hardware efficiency while maintaining accuracy (see Table I). After identifying appropriate quantization schemes, the key challenge lies in automatically assigning different quantization schemes to filters with distinct distributions. For example, a PWConv in EfficientViT [4] can contain up to 1024 filters, yielding a design space of $2^{1024}$ for determining schemes for filters in even one layer. Thus, to enable automatic allocation and save human efforts, we employ the widely adopted Mean Squared Error (MSE) [11] to select the optimal quantization scheme that minimizes quantization error for each filter:\n$W_{Q} = arg min_W E || W \u2013 W_Q||^2$,\ns.t. W_{Q} \u2208 {W^{APOT}_Q, W^{uniform}_Q}, (6)\nwhere W are floating-point and $W_Q$ are quantized weights."}, {"title": "2) Low-Bit Quantization for Memory-Intensive Layers", "content": "As discussed in Sec. III-A-2), the small amount of weights in each filter of DWConvs enable low-bit quantization to reduce bandwidth requirements. To determine the optimal bit-width, we quantize DWConvs' weights in EfficientViT from 3-bit to 8-bit. As listed in Table II, quantization at 4-bit or greater yields negligible accuracy drops compared to the full-precision counterpart, which supports our low-bit quantization hypothesis. Considering both quantization accuracy and hardware efficiency, we choose 4-bit filter-wise quantization for DWConvs."}, {"title": "IV. M2-VIT'S ACCELERATOR", "content": "The diverse operation types within the hybrid architecture of EfficientViT [4] and the mixed quantization precision (4bit and 8bit) and schemes (uniform and APoT) introduced by our M2Q strategy challenge the translation of our algorithmic benefits into real hardware benefits. Specifically, there are mainly three kinds of operations: (a) memory-intensive layers (DWConvs) with 4-bit uniform quantization; computation-intensive layers (PWConvs/MatMuls) with (b) 8-bit uniform and (c) APOT quantization. However, existing accelerators [9], [10] cannot directly be used due to the lack of tailored computing engines and dataflows, calling for a dedicated accelerator to efficiently perform these operations."}, {"title": "Overall Hardware Architecture", "content": "As depicted in Fig. 3a, our accelerator comprises a controller to provide global control signals, global buffers to store inputs and weights, and L computing cores to process different batches. Specifically, each computing core includes a local controller to support predefined dataflow and an auxiliary buffer for caching intermediate results. Besides, it integrates a Mixed-Precision Multiplication Array (MPMA), which features precision-scaleable multipliers to effectively support (a) 4-bit DWConvs and (b) 8-bit PWConvs/MatMuls, and a Shifters and Adder Tree (SAT) engine equipped with multiple shifters to efficiently handle (c) PWConvs/MatMuls with APoT quantization. We will illustrate the two components - MPMA and SAT, in detail next."}, {"title": "1) Mixed-Precision Multiplication Array (MPMA)", "content": "As shown in Fig. 3b, the MPMA comprises T processing tiles, each containing M PE blocks. Each block features R 4-bit\u00d78-bit (as activations are all represented in 8-bit) multipliers. To accommodate (a) 4-bit DWConvs and (b) 8-bit PWConvs/MatMuls, which differ in both computation patterns and quantization bits, our MPMA is configured to operate in two distinct modes - single mode and merged mode, to respectively handle the aforementioned two types of operations."}, {"title": "a) Single Mode", "content": "As each filter in DWConvs features only one weight channel to handle the corresponding input channel, input reuse is not available across different filters. To seize the available output reuse and weight reuse opportunities, we design a output-parallel dataflow for DWConvs. As illustrated in Fig. 3b, multipliers within each block concurrently process weights from different rows in the same kernel. This allows partial sums to be directly accumulated across cycles within each block, thus enhancing output locality. Blocks within the same PE tile independently compute inputs and weights from different channels, enabling parallelism of M in the channel dimension. Meanwhile, different PE tiles simultaneously handle inputs from adjacent sliding windows with weights broadcast, facilitating weight reuse and enabling parallelism of T in the output pixel dimension. Additionally, as shown in Fig. 3b (left), by simply using shift registers, we exploit the reuse of input pixels from overlaps between adjacent sliding windows."}, {"title": "b) Merged Mode", "content": "To support 8-bit PWConvs and MatMuls (can be treated as PWConvs with large batch size), MPMA operates in merged mode, where two 4-bit\u00d78-bit multipliers in adjacent PE tiles are merged for 8-bit\u00d78-bit multiplications. To facilitate data reuse, we design a filters-parallel dataflow. As depicted in Fig. 3c, all PEs within merged PE tiles perform computations along input channel, providing parallelism of RX M and enhancing output reuse. Meanwhile, different pairs of PE tiles parallelly handle different filters with input broadcast, offering parallelism of T/2 along the output channel while enhancing input reuse."}, {"title": "2) Shifter and Adder Tree (SAT)", "content": "To support PWConvs/ MatMuls with APoT quantization, we develop a dedicated SAT engine comprising multiple shifters. As shown in Fig. 3d, SAT contains S processing tiles, each containing N shifter units (SU) and an adder tree. Each SU comprises two shifters and an adder to support APoT quantization in Eq. (5). To facilitate data reuse, we propose filters-parallel dataflow here, similar to the merged mode of MPMA in Sec. IV-1b. Specifically, all shifter units within the same tile simultaneously process computations along input channel, achieving parallelism of N. The generated partial sums are then directly aggregated by the adder tree to enhance output reuse. Different tiles handle different filters with inputs broadcast, offering parallelism of S along output channel while facilitating input reuse."}, {"title": "Execution Flow", "content": "As our accelerator integrates heterogeneous computing engines to support different types of operations, we adopt pipeline processing to enhance hardware utilization. Specifically, (1) for MatMuls with APoT quantization, which are followed by MatMuls with uniform quantization in our algorithm, APoT-quantized MatMuls are first executed by SAT. The generated outputs are immediately sent to MPMA to serve as inputs for subsequent uniform-quantized MatMuls, enabling parallel processing and enhancing hardware utilization. (2) For DWConvs, which are typically followed by PWConvs in EfficientViT, when MPMA executes DWConvs, the generated outputs are promptly directed to SAT to serve as inputs for the subsequent PWConvs with APoT quantization. Once MPMA finishes DWConvs, it is reallocated to compute filters of PWConvs with uniform quantization, allowing parallel execution with SAT."}, {"title": "V. EXPERIMENTAL RESULTS", "content": ""}, {"title": "A. Experimental Setup", "content": ""}, {"title": "Quantization Setup and Baselines", "content": "Quantization Setup: Our algorithm is built upon [9] to further explore mixed quantization. Following [9], we randomly sample 1024 images from ImageNet's [15] training set as calibration data and test on ImageNet's validation set. To achieve accuracy-efficiency trade-offs, we maintain a 1:1 ratio of APoT to Uniform (8-bit) in our mixed-scheme quantization across all computation-intensive layers and align this ratio with the parallelism in corresponding computing engines to enhance hardware utilization. The quantization is performed offline and the selected schemes are recorded as a series of instructions to guide our accelerator. Baselines: We compare with Trio-ViT [9], a SOTA quantization approach dedicated to EfficientViT [4] using uniform 8-bit quantization, and Auto-ViT-Acc [10], which we apply its mixed quantization method tailored for standard ViTs to EfficientViT [4], in terms of top-1 accuracy."}, {"title": "Hardware Setup and Baselines", "content": "Hardware Setup: The parallelism of computing cores in our accelerator (R \u00d7 M\u00d7T+N\u00d7 S) \u00d7 L (see Fig. 3) is configured to (3 \u00d7 3 \u00d7 16 + 9 \u00d7 8) \u00d7 16. To obtain unit energy and overall power, we synthesize our accelerator with Synopsys Design Compiler under a 28nm TSMC technology and 500MHz clock frequency. For fast and accurate estimations, we follow [5], [9] to develop a cycle-level simulator for our accelerator, which utilizes network structure, hardware architecture, dataflow, and technology-dependent unit energy to measure total energy/throughput/latency. Baselines: We consider five baselines: (i) full-precision EfficientViT [4] (hybrid architecture) executed on the Edge CPU (Qualcomm Snapdragon 8Gen1 CPU); (ii) half-precision EfficientViT executed on the NVIDIA Jetson Nano GPU; (iii) mixed-quantized DeiT [16] and (iv) uniform-quantized Swin-T [17] respectively executed on their dedicated accelerators [10], [18]; and (v) uniform-quantized EfficientViT on its dedicated accelerator [9], which is our most competitive baseline and thus we also implement on ASIC for a fair comparison. We compare them on throughput, energy efficiency, end-to-end latency, energy (excluding off-chip memory), and energy-delay product (EDP)."}, {"title": "B. Results and Discussions", "content": "Evaluation of M\u00b2Q Strategy. As shown in Table III, thanks to the incorporation of both APOT and low-bit quantization (4-bit) in our M2Q strategy, we can reduce an average of \u219331.5% computational energy with comparable accuracy (an average of \u21930.29%) compared to the SOTA quantization baseline Trio-ViT [9]. Besides, M2Q achieves an average \u21910.95% accuracy with \u219110.8% computational energy overhead compared to Auto-ViT-Acc [10], showing our superiority. As shown in Table IV, MBConvs are more quantization-sensitive than attention but offer higher energy savings. Notably, the overall average accuracy drop is smaller than when only MBConvs are quantized, suggesting the model's robustness and compensation."}, {"title": "Evaluation of M\u00b2-ViT's Accelerator", "content": "From Table V we can see that: (i) Compared to EfficientViT-B1-R224 [4] executed on CPU/GPU, we achieve \u219139.3\u00d7~\u219151.3\u00d7 throughput and \u2191537.5\u00d7~\u2191639.9\u00d7 energy efficiency. (ii) When compared with mixed-quantized DeiT [16] and uniform-quantized Swin-T [17] on their dedicated accelerators [10], [18], we offer \u21911.52\u00d7 and \u21935.00\u00d7 throughput, respectively. (iii) As for our most competitive baseline, uniform-quantized EfficientViT on its accelerator [9] and implemented on the same ASIC technology as us, we can gain \u21913.55\u00d7 energy efficiency, 77% energy reduction and 80% EDP saving. As shown in Table VI, our throughput/latency benefits mainly from the area efficiency of our computing units, enabling higher parallelism. The energy/power savings stem from our hardware-efficient computing units and low buffer overhead, which are attributed to the low bit-widths introduced by our quantization scheme."}, {"title": "VI. CONCLUSION", "content": "In this brief, we propose M2-ViT to marry the hardware efficiency of both quantization and hybrid Vision Transformers (ViTs). We first develop a two-level mixed quantization (M\u00b2Q) strategy upon the SOTA EfficientViT [4] to fully exploit its architectural properties. To translate our algorithmic advantages into real hardware efficiency, we further design a dedicated accelerator with heterogeneous computing engines. We finally conduct experiments to validate our effectiveness."}]}