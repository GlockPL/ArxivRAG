{"title": "AttentionHand:\nText-driven Controllable Hand Image Generation\nfor 3D Hand Reconstruction in the Wild", "authors": ["Junho Park", "Kyeongbo Kong", "Suk-Ju Kang"], "abstract": "Recently, there has been a significant amount of research\nconducted on 3D hand reconstruction to use various forms of human-\ncomputer interaction. However, 3D hand reconstruction in the wild is\nchallenging due to extreme lack of in-the-wild 3D hand datasets. Espe-\ncially, when hands are in complex pose such as interacting hands, the\nproblems like appearance similarity, self-handed occclusion and depth\nambiguity make it more difficult. To overcome these issues, we propose\nAttentionHand, a novel method for text-driven controllable hand image\ngeneration. Since AttentionHand can generate various and numerous in-\nthe-wild hand images well-aligned with 3D hand label, we can acquire\na new 3D hand dataset, and can relieve the domain gap between in-\ndoor and outdoor scenes. Our method needs easy-to-use four modalities\n(i.e, an RGB image, a hand mesh image from 3D label, a bounding\nbox, and a text prompt). These modalities are embedded into the latent\nspace by the encoding phase. Then, through the text attention stage,\nhand-related tokens from the given text prompt are attended to high-\nlight hand-related regions of the latent embedding. After the highlighted\nembedding is fed to the visual attention stage, hand-related regions in\nthe embedding are attended by conditioning global and local hand mesh\nimages with the diffusion-based pipeline. In the decoding phase, the final\nfeature is decoded to new hand images, which are well-aligned with the\ngiven hand mesh image and text prompt. As a result, AttentionHand\nachieved state-of-the-art among text-to-hand image generation models,\nand the performance of 3D hand mesh reconstruction was improved by\nadditionally training with hand images generated by AttentionHand.", "sections": [{"title": "1 Introduction", "content": "The goal of 3D hand mesh reconstruction is to recover the 3D hand mesh from a\nsingle RGB image. It becomes difficult when hands are in the wild, due to insuf-\nficiency of in-the-wild 3D hand datasets. Compared to in-the-lab datasets [1-3],\nacquisition in-the-wild datasets is challenging due to unpredictable conditions\nsuch as weather, lighting, cost of sensors, and safety issues on crowded roads\nand public places. Even if an in-the-wild dataset is collected, data diversity\nwould be poor due to the aforementioned severe constraints. Although arbitrary\nlabels can be obtained through pseudo annotation, the precision and accuracy\nis still poor compared to in-the-lab datasets as shown in Fig. 1(a). To tackle\nthis problem, several synthetic datasets [4,5] have introduced. However, since\nthe hand and background images are synthesized out of harmony, they consist\nof unnatural and unrealistic hand images as shown in Fig. 1(b). Hence, it is\ndifficult to overcome the domain gap between indoor and outdoor scenes with\nsynthetic datasets.\nMoreover, when hands are in a complex pose like interacting hands, it be-\ncomes even more challenging to reconstruct 3D hand meshes due to the ap-\npearance similarity, self-handed occclusion and depth ambiguity. Starting with\nInterHand2.6M [7], several works [8-15] have emerged to solve the complex hand"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text-to-Image Generation", "content": "Text-to-image generation aims to synthesize high-resolution image from natu-\nral language descriptions. With the advent of diffusion models, various stud-\nies on text-to-image generation have been conducted in recent years [17-21].\nSpecifically, ControlNet [18] and T2I-Adapter [19] proposed novel approaches\nto incorporate arbitrary condition into the generation process. Recently, Uni-\nControlNet [20] presented a novel approach that allows for the simultaneous\nutilization of various conditions in a flexible and composable manner. Neverthe-\nless, aforementioned models exhibited common limitations in generating hand\nimages, due to the relatively small size of hands within the overall image reso-\nlution."}, {"title": "2.2 Generative Models for Hand", "content": "GANs for Hand. There are several works [22-25] to tackle the hand image\ngeneration problem with the generative adversarial network (GAN) [26]. Specif-\nically, a novel network for image-to-image translation [22] was proposed to make\ngenerated images follow the same statistical distribution as real-world hand im-\nages. GestureGAN [23] was designed to translate hand gesture-to-gesture with"}, {"title": "Diffusion Models for Hand.", "content": "Recently, some works [27-29] have been ad-\ndressed hand-related problems with diffusion models. DiffHand [27] introduced\nthe first diffusion-based framework that approaches hand mesh reconstruction\nas a denoising diffusion process. HandDiffuse [28] proposed a strong baseline for\nthe controllable motion generation of interacting hands using various controllers\nby designing a diffusion-based model. HandRefiner [29] presented an inpainting\npipeline to rectify malformed human hands in generated images with diffusion-\nbased models. However, since these models are not text-driven methods, they\ncannot generate various in-the-wild hand images conditioned on language in-\nstructions."}, {"title": "3 Method", "content": "We introduce AttentionHand, a novel framework for creating various and plau-\nsible hand images. AttentionHand is a SD-based framework that can generate\nnew RGB images infinitely conditioned on hand mesh images and text prompts.\nThe overall pipeline is shown in Fig. 3."}, {"title": "3.1 Data Preparation Phase", "content": "As shown in the first box of Fig. 3, it just requires four inputs to train At-\ntentionHand: (1) a global RGB hand image $I_{RGB} \\in \\mathbb{R}^{3\\times512\\times512}$, (2) a global\nhand mesh image $I_{Mesh} \\in \\mathbb{R}^{3\\times512\\times512}$, (3) a bounding box of the hand region\n$B \\in \\mathbb{R}^{1\\times4}$, and (4) a hand-related text prompt $U$. However, since hands typi-\ncally occupy small areas on in-the-wild scenes, we also obtain a local RGB hand\nimage $I_{RGB}\u2019 \\in \\mathbb{R}^{3\\times512\\times512}$ and a local hand mesh image $I\u2019_{mesh} \\in \\mathbb{R}^{3\\times512\\times512}$ by\ncropping and resizing $I_{RGB}$ and $I_{mesh}$ with $B$. This combination of local and\nglobal information enhances hand image conditioning. Details will be explained\nin the supplementary materials."}, {"title": "3.2 Encoding Phase", "content": "For the diffusion process in latent space, encoding phase for $I_{RGB}, I\u2019_{RGB}$ and\n$U$ is implemented by the encoder $E$. It makes global and local latent image em-\nbeddings $X^G, X^L \\in \\mathbb{R}^{4\\times64\\times64}$ for $I_{RGB}$ and $I_{RGB}$, and a latent text embedding"}, {"title": "3.3 Conditioning Phase", "content": "For generating new hand images conditioned by given text prompt and mesh\nimages, we design the text attention stage (TAS) and the visual attention stage\n(VAS) in the conditioning phase, as shown in the third box of Fig. 3. TAS is a\nstage of paying attention to tokens for the hand and its corresponding gesture\nin a given text. VAS is a stage of training the SD-based model specialized for\nhand image generation by conditioning global and local mesh images."}, {"title": "Text Attention Stage (TAS).", "content": "TAS is a stage of attending tokens which\nrepresent hand or gestures in a given text prompt as shown in Fig. 4. First, by\nadding Gaussian noise to $X^G_0$ and $X^L_0$ with t diffusion steps, the global noisy\nembedding $X^G_t$ and local noisy embedding $X^L_t$ are obtained. For simplicity, we\ndefine as $X_0 = (X^G_0,X^L_0)$ and $X_t = (X^G_t,X^L_t)$. Then, $X_t$ and $K$ are fed to\nTAS as inputs. For the text attention of TAS, we utilize the cross attention [32].\nSpecifically, an attention map $A \\in \\mathbb{R}^{H\\times W\\times N}$ is obtained by calculating the key\n(i.e., $K$) and query (i.e., $Q$, which is the linear projection of intermediate feature"}, {"title": "Visual Attention Stage (VAS).", "content": "VAS is a stage of training SD-based model\nby conditioning the aforementioned global and local mesh image. VAS is com-\nposed of two modules: one is the guidance module $\\phi_g$, and the other is the dif-\nfusion module $\\phi_d$ as shown in Fig. 5. First, the diffusion module $\\phi_d$ is designed\nbased on an U-Net network, consisting of 25 blocks: 8 blocks are downsampling\nand upsampling convolution layers, and the remaining 17 blocks consist of four\nResNet [35] layers and two Vision Transformers [36]. We define the parameter set\nof $\\phi_d$ as $\\theta_d$, which is fixed frozen to maintain the image generation performance\nof SD.\nOn the other hand, the guidance module $\\phi_g$ is also based on an U-Net network\nwith 25 blocks of $\\phi_d$. We define the parameter set of $\\phi_g$ as $\\theta_g$, which is a copied\nversion of $\\theta_d$. Different from $\\theta_d$, $\\theta_g$ is set to be learnable for generating images\nconditioned to $I_{mesh}$ and $I\u2019_{mesh}$. Specifically, $\\phi_g$ has zero convolution $Z$ [18]\nat the front of the network, and last 12 blocks of the network consist of $Z$.\nSince $Z$ is defined as a $1\\times1$ convolution layer whose weights and bias are\ninitialized to zero, the gradients of the weight and bias progressively grow from\nzeros to optimized parameters in a learnable manner. Hence, $Z$ helps generated\nimages to be conditioned on $I^G_{mesh}$ and $I^L_{mesh}$, while maintaining the quality of\nimage generation. More specifically, $\\phi_d$ and $\\phi_g$ share weights at the beginning of\ntraining, because parameter sets of both modules, i.e, $\\theta_d$ and $\\theta_g$, are initialized\nwith the pre-trained SD. However, while continuing with training process, $\\theta_g$\nis updated to learn $I^G_{mesh}$ and $I^L_{mesh}$, whereas $\\theta_d$ is fixed frozen to preserve the"}, {"title": "Optimization.", "content": "Since the diffusion model typically involves both forward and\nreverse processes, our AttentionHand also employs two processes. For the for-\nward process, the noisy embedding $X_t = (X^G_t, X^L_t)$ is obtained by progressively\nperturbing Gaussian noise $\\epsilon = (\\epsilon_G, \\epsilon_L)$ to the initial embedding $X_0 = (X^G_0, X^L_0)$\nby t diffusion steps. $\\epsilon_G$ and $\\epsilon_L$ denote the global and local noise added to $X^G_0$\nand $X^L_0$, respectively. Then, since $X_0$ is updated to $X_t$ by TAS, $\\epsilon$ is also updated\nto $\\hat{\\epsilon} = (\\hat{\\epsilon_G}, \\hat{\\epsilon_L})$. In other words, $\\hat{\\epsilon}$ is considered the residual noise between $X_0$\nand $X_t$. Thus, $\\hat{\\epsilon_G}$ and $\\hat{\\epsilon_L}$ denote the global and local residual noise. For the\nreverse process, AttentionHand learns to gradually remove residual noises with\nglobal and local denoising processes as shown in Fig. 6. Therefore, given text\nembedding $K$, diffusion steps $t$, and mesh images $I_{Gesh}$ and $I_{Lesh}$, the diffu-\nsion training network $\\epsilon_{\\theta}$ is optimized to predicted $\\hat{\\epsilon_G}$ and $\\hat{\\epsilon_L}$ jointly through the\nfollowing objectives:"}, {"title": "3.4 Decoding Phase", "content": "In the decoding phase, we can generate a new RGB hand image $\\hat{I}_{RGB} \\in \\mathbb{R}^{3\\times512\\times512}$\nby passing $Y_a$ through the decoder $D$, as shown in the fourth box of Fig. 3. While\n$E$ encodes $X_0$ by downsampling $I_{RGB}$ in the latent space, $D$ decodes $\\hat{I}_{RGB}$ by\nupsampling $Y_a$ in the pixel space, conditioned to given text prompt and mesh\nimages. The structure of $D$ is similar to the decoder of VQ-GAN. The decoding\nphase is expressed as follows:"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "For the text-to-image generation, we adopted MSCOCO [6]. For the 3D hand\nmesh reconstruction, we adopted Hands-In-Action (HIC) [37], Re:InterHand\n(ReIH) [5], InterHand2.6M (IH2.6M) [7], and MSCOCO. Due to the page limit,\ndetails will be explained in the supplementary materials."}, {"title": "4.2 Evaluation Protocol", "content": "For the text-to-image generation, we adopted FID [38], FID-Hand (FID-H), KID\n[39], KID-Hand (KID-H), the hand confidence score (Hand Conf.) [40], the mean\nsquare error of 2D and 3D keypoints (MSE-2D, 3D), and the user preference\n(User Pref.). For the 3D hand mesh reconstruction, we adopted the mean per-\nvertex position error (MPVPE), the right hand-relative vertex error (RRVE),\nand the mean relative-root position error (MRRPE). Due to the page limit,\ndetails will be explained in the supplementary materials."}, {"title": "4.3 Comparisons with State-of-the-arts", "content": "Text-to-Image Generation. As shown in Table 1, our AttentionHand exhib-\nited the highest performance in all metrics among state-of-the-arts [17-20]. This\nis particularly evident in the comparison of FID(-H) and KID(-H), which sig-\nnify the quality of the generated images being on par with real RGB images."}, {"title": "3D Hand Mesh Reconstruction.", "content": "To verify our AttentionHand extensively,\nwe trained state-of-the-art hand pose networks [8-10, 13, 16] by additionally\nadding new data generated by AttentionHand. As shown in Table 2, the perfor-\nmance of all methods increased for all metrics. Specifically, with respect to the\nMPVPE, AttentionHand showed the dramatic performance improvement with\nInterWild [16] about 3.66% and 7.81% on HIC and ReIH, respectively. With\nrespect to the RRVE, it increased by about 1.17% and 0.65% on HIC and ReIH,\nrespectively. With respect to the MRRPE, it increased by about 6.40% and\n1.47% on HIC and ReIH, respectively. These imply generated hand images help\nincreasing the accuracy of the 3D hand mesh reconstruction. In addition, the\nqualitative performance for in-the-wild scenes is also verified as shown in Fig.\n8. Although MSCOCO mainly contains in-the-wild situations, 3D hand mesh is"}, {"title": "4.4 Ablation Studies", "content": "Text Attention Stage (TAS). We deeply dived into TAS to verify its supe-\nriority. Firstly, as in the last two rows in Table 1, TAS showed its effectiveness\nin all metrics. In addition, as shown in Fig. 9, attention maps are well described\ntheir corresponding tokens in the case of with TAS. It implies that with TAS,\nAttentionHand can reflect hand-related tokens enough. Additional qualitative\nresults are in the supplementary materials.\nSecondly, we conducted more experiments about Gaussian filters as follows:\n(1) no Gaussian filter, (2) random Gaussian filter, and (3) fixed Gaussian filter.\nAs shown in the second, third, and fourth columns of Fig. 10, we found interesting"}, {"title": "Model Design Justification.", "content": "To justify our model's superiority, we compared\nthe characteristics of prior works including our model. As shown in Table 3,\nour model's distinctive and potential features compared to prior works are (1)\nharmonious preservation of locality (i.e., hand) with globality (i.e., in-the-wild\nscene), and (2) selective attention on hand-related tokens by cross attention.\nSpecifically, to harmonize globality and locality, we developed global and local\ndesigns for the visual attention stage (VAS). Moreover, since the global and local\nbranches are designed structurally same, we set them to share their weights for\nreducing the number of training parameters (about 20.2%\u2193) and improving the\ngeneralizability (see two shaded rows in Table 4). We experimentally verified the\neffectiveness of our design as shown in Table 4."}, {"title": "Robustness of Generated Dataset.", "content": "To verify robustness of our generated\ndataset, we generated multiple hand images from same modalities as shown\nin Fig. 11(a). As a result, all generated images are perfectly well-aligned with\ngiven hand mesh images. Moreover, we found the t-SNE distribution [44] of\nAttentionHand is broader than MSCOCO as shown in Fig. 11(b). As a result,\nwe believe that AttentionHand can contribute to the downstream task with our\nextensive in-the-wild hand images, leading to alleviate the domain gap between\nindoor and outdoor scenes."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel text-to-hand image generation model, At-\ntentionHand, which pays attention to the hand-related tokens from the text\nprompt and global and local mesh images. AttentionHand achieved state-of-the-\nart performance in text-to-hand image generation, and we demonstrated that\ntraining with the dataset generated by our AttentionHand improved the perfor-\nmance of 3D hand mesh reconstruction. However, the diversity may decrease as\nthe generative model is trained to optimize hand mesh images. We expect for the\nemergence of outstanding diffusion model to improve the diversity and quality\nof the hand image."}, {"title": "A Preliminary: Latent Diffusion Model", "content": "Latent Diffusion Model (LDM) or Stable Diffusion (SD) [17] is a type of diffusion\nmodel designed for training in a latent space, which is particularly well-suited\nfor likelihood-based generative models. Unlike traditional approaches that utilize\nthe full high-dimensional pixel space, LDM leverages the latent space to concen-\ntrate on the essential and meaningful aspects of the data. This enables training\nin a lower-dimensional space, resulting in significantly improved computational\nefficiency. The objective of LDM is as follows:"}, {"title": "B Details of Data Preparation Phase", "content": "To train AttentionHand, it just requires easy-to-use four modalities: (1) a global\nRGB hand image $I_{RGB} \\in \\mathbb{R}^{3\\times512\\times512}$, which represents in-the-wild scene with\nhand, (2) the corresponding global hand mesh image $I_{Mesh} \\in \\mathbb{R}^{3\\times512\\times512}$, (3) the\ncorresponding bounding box of hand region $B \\in \\mathbb{R}^{1\\times4}$, and (4) the corresponding\nhand-related text prompt U."}, {"title": "Rendering Hand Mesh Images.", "content": "$I_{mesh}$ is obtained by utilizing MANO [45],\nwhich is generally adopted as ground-truth for 3D hand mesh reconstruction.\nSpecifically, the ground-truth root pose $M_{root} \\in \\mathbb{R}^{1\\times3}$, hand pose $M_{hand} \\in\n\\mathbb{R}^{15\\times3}$, shape $M_{shape} \\in \\mathbb{R}^{1\\times10}$, translation $M_{trans} \\in \\mathbb{R}^{1\\times3}$, and hand type $h$ (i.e.,\nleft or right hand) are passed to MANO layer to get the mesh $M_{mesh} \\in \\mathbb{R}^{778\\times3}$\nand face $M_{face} \\in \\mathbb{R}^{1538\\times3}$ of 3D hand as following:"}, {"title": "C Hand-related Tagging", "content": "As we mentioned in the section 3.3 of the main body, we design the hand-\nrelated tagging $H_{tag}$, which is based on part-of-speech tagging [34] from NLTK\nlibrary [48]. Specifically, $H_{tag}$ determines if the input token's part of speech\nindicates \u201cVBG\u201d (i.e., holding, taking, using, etc), or if the input token contains\nthe word hand(s). As a result, we can extract hand-related tokens with $H_{tag}.\nExamples of this process can be found in Fig. B."}, {"title": "DDetails of Experiments", "content": ""}, {"title": "D.1 Dataset", "content": "Text-to-Image Generation. For the train, RGB hand images, mesh images,\nand bounding boxes were utilized from the train set of MSCOCO [6]. Text\nprompts, which represent hand-related descriptions, are obtained by off-the-shelf\ncaptioning model [47]. Note that since two or more people's hands can be seen as\none person's hands on the hand-focused image, we filtered out the case of more\nthan one person for the data preparation. Hence, AttentionHand is induced to\ntrain about single or both hands of only one person. For the test, RGB hand\nimages and mesh images were also utilized from the train set of MSCOCO to\nevaluate the image quality and pose alignment of generated hand images. On\nthe other hand, text prompts were utilized from the validation set of MSCOCO.\nMoreover, we adopted Hands-In-Action (HIC) [37], Re:InterHand (ReIH) [5],\nand InterHand2.6M (IH2.6M) [7] to evaluate the effectiveness of generated hand\nimages for the 3D hand mesh reconstruction."}, {"title": "3D Hand Mesh Reconstruction.", "content": "For the train, we utilized hand mesh images\nfrom ReIH and IH2.6M, which provide accurate 3D hand labels, to generate new\ntraining samples. For the test, we adopted HIC, ReIH, IH2.6M, and MSCOCO\nto evaluate the accuracy of reconstructed 3D hand mesh."}, {"title": "D.2 Evaluation Protocol", "content": "Text-to-Image Generation. To evaluate the image quality, we adopted frechet\ninception distance (FID) [38] and kernel inception distance (KID) [39]. In ad-\ndition, according to [40], we computed FID-Hand (FID-H), KID-Hand (KID-\nHand), and the hand confidence score (Hand Conf.), to measure the quality of\nimages only in the hand regions. To evaluate the pose alignment, we adopted the\nmean square error of 3D keypoints (MSE-3D) for analysis the error between the\nground-truth and predicted keypoints estimated by the off-the-shelf model [16].\nAdditionally, to validate reliability, we evaluated the mean square error of 2D\nkeypoints (MSE-2D) using Mediapipe [49]. Moreover, similar to [17,50], we car-\nried out user preference to evaluate the perceptual plausibility of generated im-\nages. Specifically, we attached 24 samples of results in the Google Forms, and\nreleased it to 30 people. We asked for three questions as shown in Fig. C: (1)\nalignment with the given mesh image, (2) reflection of the given text prompt,\nand (3) overall quality of the generated image. The results of these questions\nwere averaged and quantified in percentage."}, {"title": "3D Hand Mesh Reconstruction.", "content": "We adopted the mean per-vertex position\nerror (MPVPE), the right hand-relative vertex error (RRVE), and the mean\nrelative-root position error (MRRPE), which are representative metrics for the\n3D hand mesh reconstruction."}, {"title": "D.3 Implementation Details", "content": "Text-to-Image Generation. For the text-to-image generation, we adopted\nPyTorch Lightning [51] framework. We set the batch size as 1, and learning rate\nas 10\u22125. We used one RTX 3090."}, {"title": "3D Hand Mesh Reconstruction.", "content": "For the 3D hand mesh reconstruction, we\nmainly referred to InterWild [16]. Specifically, we adopted PyTorch [52] frame-\nwork. We set the batch size as 32, and learning rate as 10\u22124 for the first 4 epochs,\nas 10\u22125 for the rest epochs. We used one RTX 3090."}, {"title": "D.4 Generalizability of AttentionHand", "content": "To verify the generalizability of AttentionHand for 3D hand mesh reconstruction,\nwe additionally generated hand images with state-of-the-arts of text-to-image\ngeneration, utilized them as training sets of the off-the-shelf model [16], which\nis suitable for in-the-wild generalization, and tested on in-the-wild datasets (i.e.,\nHIC and ReIH) and in-the-lab dataset (i.e., IH2.6M.) As a result, AttentionHand"}, {"title": "D.5 More Ablation Study of Text Attention Stage", "content": "We additionally verified the effectiveness of the text attention stage (TAS) as\nshown in Fig. D. Specifically, based on given hand mesh images and text prompts,\nwe visualized attention maps and generated new hand images with three cases.\nWithout TAS, attention of corresponding tokens was not well represented as\nshown in attention maps with red boxes. However, with TAS, attention was more\nhighlighted by reflecting corresponding tokens as shown in attention maps with\ngreen boxes. It implies that with TAS, AttentionHand can reflect hand-related\ntokens enough."}, {"title": "D.6 More Qualitative Results", "content": "Text-to-Image Generation. Compared to state-of-the-arts [17-20], our At-\ntentionHand generated the high-quality hand image which is well-aligned with"}, {"title": "3D Hand Mesh Reconstruction.", "content": "We trained off-the-shelf model [16] by addi-\ntionally adding new data generated by AttentionHand, and tested on MSCOCO\nand ReIH. The performance for in-the-wild scenes is verified as shown in Fig. G.\nAlthough MSCOCO mainly contains in-the-wild situations, 3D hand mesh is re-\nconstructed robustly. It implies that even for difficult situations, the performance\nof reconstruction can be improved by utilizing AttentionHand. In addition, the\nperformance improvement is also verified as shown in Figs. H and I. Note that\nReIH is considered more challenging than other datasets because it consists of\nimages with various backgrounds and complex interacting hands. However, by\nemploying AttentionHand, the 3D hand mesh was reconstructed accurately re-\ngardless of the viewpoint (i.e., egocentric and exocentric view.) In addition, both\ninteracting hands were elaborately recovered even when hands are in self-handed\nocclusion and depth ambiguity."}]}