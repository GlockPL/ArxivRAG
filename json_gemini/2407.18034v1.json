{"title": "AttentionHand:\nText-driven Controllable Hand Image Generation\nfor 3D Hand Reconstruction in the Wild", "authors": ["Junho Park", "Kyeongbo Kong", "Suk-Ju Kang"], "abstract": "Recently, there has been a significant amount of research\nconducted on 3D hand reconstruction to use various forms of human-\ncomputer interaction. However, 3D hand reconstruction in the wild is\nchallenging due to extreme lack of in-the-wild 3D hand datasets. Espe-\ncially, when hands are in complex pose such as interacting hands, the\nproblems like appearance similarity, self-handed occclusion and depth\nambiguity make it more difficult. To overcome these issues, we propose\nAttentionHand, a novel method for text-driven controllable hand image\ngeneration. Since AttentionHand can generate various and numerous in-\nthe-wild hand images well-aligned with 3D hand label, we can acquire\na new 3D hand dataset, and can relieve the domain gap between in-\ndoor and outdoor scenes. Our method needs easy-to-use four modalities\n(i.e, an RGB image, a hand mesh image from 3D label, a bounding\nbox, and a text prompt). These modalities are embedded into the latent\nspace by the encoding phase. Then, through the text attention stage,\nhand-related tokens from the given text prompt are attended to high-\nlight hand-related regions of the latent embedding. After the highlighted\nembedding is fed to the visual attention stage, hand-related regions in\nthe embedding are attended by conditioning global and local hand mesh\nimages with the diffusion-based pipeline. In the decoding phase, the final\nfeature is decoded to new hand images, which are well-aligned with the\ngiven hand mesh image and text prompt. As a result, AttentionHand\nachieved state-of-the-art among text-to-hand image generation models,\nand the performance of 3D hand mesh reconstruction was improved by\nadditionally training with hand images generated by AttentionHand.", "sections": [{"title": "1 Introduction", "content": "The goal of 3D hand mesh reconstruction is to recover the 3D hand mesh from a\nsingle RGB image. It becomes difficult when hands are in the wild, due to insuf-\nficiency of in-the-wild 3D hand datasets. Compared to in-the-lab datasets, acquisition in-the-wild datasets is challenging due to unpredictable conditions\nsuch as weather, lighting, cost of sensors, and safety issues on crowded roads\nand public places. Even if an in-the-wild dataset is collected, data diversity\nwould be poor due to the aforementioned severe constraints. Although arbitrary\nlabels can be obtained through pseudo annotation, the precision and accuracy\nis still poor compared to in-the-lab datasets as shown in Fig. 1(a). To tackle\nthis problem, several synthetic datasets have introduced. However, since\nthe hand and background images are synthesized out of harmony, they consist\nof unnatural and unrealistic hand images as shown in Fig. 1(b). Hence, it is\ndifficult to overcome the domain gap between indoor and outdoor scenes with\nsynthetic datasets.\nMoreover, when hands are in a complex pose like interacting hands, it be-\ncomes even more challenging to reconstruct 3D hand meshes due to the ap-\npearance similarity, self-handed occclusion and depth ambiguity. Starting with\nInterHand2.6M, several works have emerged to solve the complex hand"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text-to-Image Generation", "content": "Text-to-image generation aims to synthesize high-resolution image from natu-\nral language descriptions. With the advent of diffusion models, various stud-\nies on text-to-image generation have been conducted in recent years. Specifically, ControlNet and T2I-Adapter proposed novel approaches\nto incorporate arbitrary condition into the generation process. Recently, Uni-\nControlNet presented a novel approach that allows for the simultaneous\nutilization of various conditions in a flexible and composable manner. Neverthe-\nless, aforementioned models exhibited common limitations in generating hand\nimages, due to the relatively small size of hands within the overall image reso-\nlution."}, {"title": "2.2 Generative Models for Hand", "content": "GANs for Hand. There are several works to tackle the hand image\ngeneration problem with the generative adversarial network (GAN). Specif-\nically, a novel network for image-to-image translation was proposed to make\ngenerated images follow the same statistical distribution as real-world hand im-\nages. GestureGAN was designed to translate hand gesture-to-gesture with"}, {"title": "Diffusion Models for Hand", "content": "Recently, some works have been ad-\ndressed hand-related problems with diffusion models. DiffHand introduced\nthe first diffusion-based framework that approaches hand mesh reconstruction\nas a denoising diffusion process. HandDiffuse proposed a strong baseline for\nthe controllable motion generation of interacting hands using various controllers\nby designing a diffusion-based model. HandRefiner presented an inpainting\npipeline to rectify malformed human hands in generated images with diffusion-\nbased models. However, since these models are not text-driven methods, they\ncannot generate various in-the-wild hand images conditioned on language in-\nstructions."}, {"title": "3 Method", "content": "We introduce AttentionHand, a novel framework for creating various and plau-\nsible hand images. AttentionHand is a SD-based framework that can generate\nnew RGB images infinitely conditioned on hand mesh images and text prompts.\nThe overall pipeline is shown in Fig. 3."}, {"title": "3.1 Data Preparation Phase", "content": "As shown in the first box of Fig. 3, it just requires four inputs to train At-\ntentionHand: (1) a global RGB hand image $I_{RGB} \\in \\mathbb{R}^{3\\times512\\times512}$, (2) a global\nhand mesh image $I_{Gesh} \\in \\mathbb{R}^{3\\times512\\times512}$, (3) a bounding box of the hand region\n$B\\in \\mathbb{R}^{1\\times4}$, and (4) a hand-related text prompt U. However, since hands typi-\ncally occupy small areas on in-the-wild scenes, we also obtain a local RGB hand\nimage $I'_{RGB} \\in \\mathbb{R}^{3\\times512\\times512}$ and a local hand mesh image $I'_{mesh} \\in \\mathbb{R}^{3\\times512\\times512}$ by\ncropping and resizing $I_{RGB}$ and $I_{mesh}$ with B. This combination of local and\nglobal information enhances hand image conditioning. Details will be explained\nin the supplementary materials."}, {"title": "3.2 Encoding Phase", "content": "For the diffusion process in latent space, encoding phase for $I_{RGB}$, $I'_{RGB}$ and\nU is implemented by the encoder E. It makes global and local latent image em-\nbeddings $X^{G}, X^{L} \\in \\mathbb{R}^{4\\times64\\times64}$ for $I_{RGB}$ and $I'_{RGB}$, and a latent text embedding"}, {"title": "3.3 Conditioning Phase", "content": "For generating new hand images conditioned by given text prompt and mesh\nimages, we design the text attention stage (TAS) and the visual attention stage\n(VAS) in the conditioning phase, as shown in the third box of Fig. 3. TAS is a\nstage of paying attention to tokens for the hand and its corresponding gesture\nin a given text. VAS is a stage of training the SD-based model specialized for\nhand image generation by conditioning global and local mesh images.\nText Attention Stage (TAS). TAS is a stage of attending tokens which\nrepresent hand or gestures in a given text prompt as shown in Fig. 4. First, by\nadding Gaussian noise to $X^{G}$ and $X^{L}$ with t diffusion steps, the global noisy\nembedding $X_{t}^{G}$ and local noisy embedding $X_{t}^{L}$ are obtained. For simplicity, we\ndefine as $X_{t} = (X_{t}^{G},X_{t}^{L})$ and $X_{0} = (X_{0}^{G},X_{0}^{L})$. Then, $X_{t}$ and K are fed to\nTAS as inputs. For the text attention of TAS, we utilize the cross attention. Specifically, an attention map $A \\in \\mathbb{R}^{H\\times W\\times N}$ is obtained by calculating the key\n(i.e., K) and query (i.e., Q, which is the linear projection of intermediate feature"}, {"title": "Visual Attention Stage (VAS)", "content": "VAS is a stage of training SD-based model\nby conditioning the aforementioned global and local mesh image. VAS is com-\nposed of two modules: one is the guidance module $\\phi_{g}$, and the other is the dif-\nfusion module $\\phi_{d}$ as shown in Fig. 5. First, the diffusion module $\\phi_{d}$ is designed\nbased on an U-Net network, consisting of 25 blocks: 8 blocks are downsampling\nand upsampling convolution layers, and the remaining 17 blocks consist of four\nResNet layers and two Vision Transformers. We define the parameter set\nof $\\phi_{d}$ as $\\theta_{d}$, which is fixed frozen to maintain the image generation performance\nof SD.\nOn the other hand, the guidance module $\\phi_{g}$ is also based on an U-Net network\nwith 25 blocks of $\\phi_{d}$. We define the parameter set of $\\phi_{g}$ as $\\theta_{g}$, which is a copied\nversion of $\\theta_{d}$. Different from $\\theta_{d}$, $\\theta_{g}$ is set to be learnable for generating images\nconditioned to $I^{G}_{mesh}$ and $I^{L}_{mesh}$. Specifically, $\\phi_{g}$ has zero convolution Z at\nthe front of the network, and last 12 blocks of the network consist of Z.\nSince Z is defined as a 1 \u00d7 1 convolution layer whose weights and bias are\ninitialized to zero, the gradients of the weight and bias progressively grow from\nzeros to optimized parameters in a learnable manner. Hence, Z helps generated\nimages to be conditioned on $I^{G}_{mesh}$ and $I^{L}_{mesh}$, while maintaining the quality of\nimage generation. More specifically, $\\phi_{d}$ and $\\phi_{g}$ share weights at the beginning of\ntraining, because parameter sets of both modules, i.e, $\\theta_{d}$ and $\\theta_{g}$, are initialized\nwith the pre-trained SD. However, while continuing with training process, $\\theta_{g}$\nis updated to learn $I^{G}_{mesh}$ and $I^{L}_{mesh}$, whereas $\\theta_{d}$ is fixed frozen to preserve the"}, {"title": "Optimization", "content": "Since the diffusion model typically involves both forward and\nreverse processes, our AttentionHand also employs two processes. For the for-\nward process, the noisy embedding $X_{t} = (X_{t}^{G}, X_{t}^{L})$ is obtained by progressively\nperturbing Gaussian noise $\\epsilon = (\\epsilon^{G}, \\epsilon^{L})$ to the initial embedding $X_{0} = (X_{0}^{G}, X_{0}^{L})$\nby t diffusion steps. $\\epsilon^{G}$ and $\\epsilon^{L}$ denote the global and local noise added to $X_{0}^{G}$\nand $X_{0}^{L}$, respectively. Then, since $X_{0}$ is updated to $X_{t}$ by TAS, $\\epsilon$ is also updated\nto $\\hat{\\epsilon} = (\\hat{\\epsilon}^{G}, \\hat{\\epsilon}^{L})$. In other words, $\\hat{\\epsilon}$ is considered the residual noise between $X_{0}$\nand $X_{t}$. Thus, $\\hat{\\epsilon}^{G}$ and $\\hat{\\epsilon}^{L}$ denote the global and local residual noise. For the\nreverse process, AttentionHand learns to gradually remove residual noises with\nglobal and local denoising processes as shown in Fig. 6. Therefore, given text\nembedding K, diffusion steps t, and mesh images $I^{G}_{mesh}$ and $I^{L}_{mesh}$, the diffu-\nsion training network $\\epsilon_{\\theta}$ is optimized to predicted $\\hat{\\epsilon}^{G}$ and $\\hat{\\epsilon}^{L}$ jointly through the\nfollowing objectives:"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "For the text-to-image generation, we adopted MSCOCO. For the 3D hand\nmesh reconstruction, we adopted Hands-In-Action (HIC), Re:InterHand\n(ReIH), InterHand2.6M (IH2.6M), and MSCOCO. Due to the page limit,\ndetails will be explained in the supplementary materials."}, {"title": "4.2 Evaluation Protocol", "content": "For the text-to-image generation, we adopted FID, FID-Hand (FID-H), KID,\nKID-Hand (KID-H), the hand confidence score (Hand Conf.), the mean\nsquare error of 2D and 3D keypoints (MSE-2D, 3D), and the user preference\n(User Pref.). For the 3D hand mesh reconstruction, we adopted the mean per-\nvertex position error (MPVPE), the right hand-relative vertex error (RRVE),\nand the mean relative-root position error (MRRPE). Due to the page limit,\ndetails will be explained in the supplementary materials."}, {"title": "4.3 Comparisons with State-of-the-arts", "content": "Text-to-Image Generation. As shown in Table 1, our AttentionHand exhib-\nited the highest performance in all metrics among state-of-the-arts. This\nis particularly evident in the comparison of FID(-H) and KID(-H), which sig-\nnify the quality of the generated images being on par with real RGB images."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel text-to-hand image generation model, At-\ntentionHand, which pays attention to the hand-related tokens from the text\nprompt and global and local mesh images. AttentionHand achieved state-of-the-\nart performance in text-to-hand image generation, and we demonstrated that\ntraining with the dataset generated by our AttentionHand improved the perfor-\nmance of 3D hand mesh reconstruction. However, the diversity may decrease as\nthe generative model is trained to optimize hand mesh images. We expect for the\nemergence of outstanding diffusion model to improve the diversity and quality\nof the hand image."}]}