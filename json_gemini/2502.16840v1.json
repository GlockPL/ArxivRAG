{"title": "IN-CONTEXT LEARNING OF EVOLVING DATA STREAMS WITH TABULAR FOUNDATIONAL MODELS", "authors": ["Afonso Louren\u00e7o", "Jo\u00e3o Gama", "Eric P. Xing", "Goreti Marreiros"], "abstract": "State-of-the-art data stream mining in supervised classification has traditionally relied on ensembles of incremental decision trees. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees across all non-stationary benchmarks. Several promising research directions are outlined in the paper. The authors urge the community to explore these ideas, offering valuable opportunities to advance in-context stream learning.", "sections": [{"title": "Introduction", "content": "Data stream mining is an area in machine learning where the inference and training of the algorithm are performed in real-time, dealing with large volumes of ever-evolving tabular data. Differently to batch learning where all training data necessary to induce a model is available, data streams incrementally arrive at any time. Thus, making a model easily outdated due to the occurrence of concept drifts, i.e., distribution changes over time [1]. To circumvent these challenges, many stream mining algorithms have been proposed with different requirements being imposed: (1) be ready to predict and update the model at any point, and in limited time; (2) process an example at a time, inspecting only once with a limited sized model; (3) be able to adapt to change.\nDecision trees. In fulfilling these requirements, the current state-of-art for supervised learning in tabular data streams has long been incremental decision trees (IDTs), whose success can be attributed to two key factors: approximation-based splitting techniques [2] and effective adaptation strategies [3]. Approximation-based splitting involves updating statistical summaries of entropy-based metrics, e.g. information gain, and determining whether the observed utility of a split is statistically close to its true utility when the data distribution is unknown, e.g. via the Hoeffding bound [4]. However, as tree grows from the root node, the descendant nodes subsequently get fixed to covering particular sub-spaces of the space covered by their parent"}, {"title": "Background", "content": "This section describes the different advancements for decisions trees and transformers that make them now cross roads, as competing solutions for data stream mining. First, describing the different adaptations needed to make ensembles of IDTs the state-of-the art for tabular data stream mining. Second, the different efforts of making transformers capable on working with tabular data and perform ICL."}, {"title": "Incremental decision trees", "content": "Dynamic structural expansion. To incrementally construct a IDT, using contained memory and time per sample, the fundamental algorithm design component is approximation-based splitting [2]. As new instances arrive, they traverse the tree from the root to a leaf node, updating statistics at each node that guarantee for split decisions almost identical to those that conventional batch learning methods would make, despite lookahead and stability issues. Based on these sufficient statistics, IDTs continuously update the heuristic values for each attribute, with a distribution of potential split numeric values being approximate by a single Gaussian [35] and successfully executed according to a statistical Bound of the heuristic difference between attributes [36]. However, while effective for incremental adaptation, this design component does not control for unnecessary tree growth. Thus, posing the risk of: (1) excessive memory consumption, due to multiple redundant splits on all features and increasing running statistics to keep updated; and (2) loss of plasticity, due to interference between concepts and descendant nodes getting subsequently fixed to the space covered by their parent node. To address this, various pre-pruning and pos-pruning techniques have been proposed. Pre-pruning strategies include splitting rules enhancements, adaptive tie breaking, adaptive grace periods, and activity-based expansion modes [37]. Pos-pruning includes various methods that estimate in each decision node whether the corresponding rooted subtree is consistent with the current data and worth pruning, e.g. checking if the current split outperforms the null attribute [38] or directly monitoring the error rate in nodes, e.g. using the Page\u2013Hinckley [39] which is designed to detect abrupt changes in the average of a Gaussian signal [40], or the ADWIN change detector [5]. To reconstruct the tree, one can simply require another attribute to become the top split attribute [38], restructure the otherwise pruned subtree [41], or train an alternative subtree as soon as an inconsistent node is detected, which only eventually replaces the original subtree when its accuracy is superior, and after a user-defined number of examples [5].\nDiversity. While single IDTs need necessarily to be pruned, combining multiple trees into a high diversity ensemble allows to naturally develop decision boundaries that locally adapt to changes in the data distribution [42, 7]. Moreover, ensemble strategies can be combined with change detection methods to dynamically select, delete, add, and combine IDTs optimized for different regions of the solution space. For instance, Adaptive Random Forest (ARF) [43] combines resampling with drift recovery strategies. It uses both online bagging and boosting to train classifiers iteratively, increasing the weight on misclassified instances [44]. If a drift is detected, the background tree becomes the new active tree, and the old one is forgotten. Streaming Random Patches (SRP) [6] combines random subspaces and resampling to leverage diversity among base incremental learners, applying a global subspace strategy that increases diversity across weak learners. Streaming Gradient Boosted Trees (SGBT) [45] resets only some parts of the booster using an internal tree replacement mechanism, instead of externally monitoring each item in the boosting ensemble.\nConcept history. However, such ensembles constantly train all active base classifiers, progressively discarding some of them when a drift is detected. Base learners evolve and the previously learned concepts are forgotten before they reoccur, if that benefits the current concept being learned. Consequently, these models might need to learn old concepts"}, {"title": "Tabular foundational models", "content": "Multi-modal LLMs. Within the discovery of ICL abilities [10], and the subsequent emerging possibilities of multimodal applications, various efforts were made for flexible prompts of tabular information [13]. Initial research efforts focused on complementing multi-modal solutions with table understanding, such as Text2SQL, question answering, fact verification, and natural language inference [56, 57, 58, 59]. For instance, by extending BERT with a masked language model objective and additional embeddings designed to capture tabular structured data [60]. While in simpler architectures, an LLM takes in a query and serialized table as an input prompt, in more involved architectures, the system might be also connected to external spreadsheets, databases or programs, allowing for their manipulation, management, and integration [61, 62, 63].\nLLMs for tabular understanding. In order for a foundational model to efficiently perform these tabular tasks it is important to compress tables, not only because large tables might not fit in the context length, but also due to the slow processing of long sentences caused by the quadratic complexity of self-attention [64]. Furthermore, compressing allows to manage the impact of tabular data challenges such as: noisy information in messy data and adversarial inputs [65, 66]; random ordering of rows and table transpositions [67]; arbitrary, sequential and shuffled column names [67]; semi-structured content with rows as strings of key-value pairs and merged contiguous columns [67]; and feature heterogeneity in understanding large tables. Motivated by these challenges, predefined constraints [66] and other methods [60, 68] can be used to truncate the input based on a maximum sequence length. Moreover, the ability to search and retrieve information relevant to the tabular task can also be crucial to automate prompt compression. Instead of manually curating or randomly selecting k examples [62], one can find k examples based on semantic similarity with a query, using a predefined distance metric of the question and example embedding, e.g. the Euclidean or negative cosine similarity [69]. Domain-specific information, such as table and column names can be masked from this query.\nLLMs for tabular prediction. Leveraging on these LLMs' validated capabilities on table understanding, new efforts went towards directly applying them for supervised learning of tabular data. While the same serialization and prompting strategies apply, tabular data prediction requires more sophisticated computation with numerical data, e.g. estimating threshold and densities. To address this, initial methods focused on training on large tabular metadatasets with masked reconstruction loss [56, 57], using text templates to convert tabular data into instruction-oriented formats [70], fine-tuning LLMs on small collections of linearized tabular data to learn fundamental features before adapting it to a specific prediction [15, 71, 72] and fine-tuning on the specific prediction dataset to specialize its knowledge using Low-Rank Adaptation [14].\nLLM limitations. Despite the potential of these advancements, several limitations hinder the application of LLMs for tabular supervised learning. Firstly, the limited context window restricts the number of examples that can be utilized for few-shot learning and the amount of additional semantic information that can be ingested, particularly in datasets that contain excessive numbers of categorical columns. Secondly, the high count of parameters, quadratic complexity cost of processing the table as string (n samples \u00b7 m features) makes inference computationally expensive. Moreover,"}, {"title": "In-context stream learning", "content": "This sections offers insights into the drivers behind the ICL ability, and how it addresses the core challenges of adaptive learning in dynamic environments. ICL requires a model to implicitly construct a map from in-context examples, of a new concept on which it has not been previously trained, to a predictor without any updates to the model's parameters themselves. Indeed, it has been show empirically that transformers with fixed parameters can learn linear functions comparable to the optimal least squares estimator, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context on the fly [84]. Furthermore, it has been demonstrated that transformers implement more complex function classes implicitly, i.e., sparse linear functions, two-layer neural networks, and decision trees, with performance that matches or exceeds task-specific learning algorithms [85]. However, it remains unclear to what extent these models are able to learn new tasks from in-context examples alone as opposed to indexing into a vast set of known tasks from the training data. In this regard, it has been hypothesized that this emergent"}, {"title": "Context optimization on evolving data streams", "content": "Based on this intuition, the objective of in-context stream learning is to minimize bias in model predictions when faced with new data that drifts over time. This problem occurs in situations where the LTM $f_o$, is applied on-the-fly to a continuous stream of real-world data. The data stream is assumed to be partitioned sequentially into non-overlapping segments, called concepts, such as $C_i$ and $C_{i+1}$, each corresponding to distinct joint data distributions, $p_i(x, y)$ and $p_{i+1}(x, y)$, respectively. Within each concept, the data distribution is assumed static, however, as the data stream progresses, drift between consecutive concepts can introduce discrepancies between past and future data. For instance, if concept $C_1$ has a skewed label distribution $p_1(y)$, and the following concept $C_{i+1}$ has a uniform label distribution $p_{i+1}(y)$, then the predictive model must adapt to account for this shift in label distribution [81]. The goal is to select an optimal localized context $l$, composed of $(x_1, y_1, ..., x_n, y_n, x_{query})$, which helps $f_e$ approximate the correct output $y_{query}$ even for cases in the tail regions of these drifting distributions. In this case, the model's expected prediction error for a new sample $(x_{query}, y_{query})$ is defined by:\n$EPE_{f_e}(x_{query}) = E_I [(f_o(x_{query} | x_1, y_1, ..., x_n, y_n) \u2013 E[y_{query} | x_{query}])^2]$\n$= E_I[((f_o(x_{query} | x_1, y_1, ..., x_n, y_n) \u2013 E_I[f_o(x_{query} | x_1, y_1, ..., x_n, y_n)]) + (E_I [f_o(x_{query} | x_1, y_1, ..., x_n, y_n)] - E[y_{query} | x_{query}]))^2]$\n$= Var[f_o(x_{query} | x_1, y_1,..., x_n, y_n)] + (Bias^2 [f_o(x_{query} | x_1, y_1,..., x_n, y_n)]) + \\sigma^2$\nLTM's variance decreases with more in-context examples. Assume $f_e$ to be c-Lipschitz continuous with a constant $c = (c_1,..., c_n)$ where each $c_i$ scales as $d_i^{-\\alpha}$, $\\delta$ is a positive constant, and $a > 0.5$. Then, for two independently sampled contexts $(x_1, y_1, ..., x_n, y_n, x_{query})$ and $(x'_1, y'_1, \u00b7 \u00b7 \u00b7, x'_n, y'_n, x_{query})$ from an unbounded $D_{stream}$, the inequality $|f_o(x_{query} | (x_1, y_1,..., x_n, y_n)) \u2014 f_o(x_{query} | (x'_1, y'_1,...,x'_n, y'_n))| \\leq \\sum_{i=1}c_i1\\{x_i\\neq x'_i\\}$ holds [97]. Applying McDiarmid's Inequality [101], for any t > 0, the tail probability bound is:\n$Pr(f_o(x_{query} | x_1, y_1,..., x_n, y_1, ..., x_n, y_n)| \\geq t) \\leq 2 exp(\\frac{-2t^2}{||c||^2})$\nwhere $||c||^2 = \\Sigma^n_i(\\delta i^{-\\alpha})^2$, converging due to $a > 0.5$. By the Borel-Cantelli lemma, $f_e(x_{query} | x_1, y_1, ..., x_n, y_n)$ converges almost surely to the expected prediction $E\\_{\\sim I}[f_o(x_{query} | x_1, y_1, ..., x_n, y_n)]$ as n \u2192 \u221e [81]. This setup implies that as more samples are added to the prompt, the model's sensitivity to small input changes diminishes, reducing error volatility."}, {"title": "Experimental study", "content": "This section evaluates the performance of a naive streaming implementation of a large tabular model (LTM) against state-of-art algorithms for data stream mining, namely: Adaptive Random Forest(ARF) [43], Streaming Random Patches (SRP) [6], Boosting-like Online Learning Ensemble (BOLE) [104], Leverage Bagging (LevBag) [105], Extremely Fast Decision Tree (EFDT) [38], and Very Fast Decision Tree (VFDT) [36]. These algorithms were implemented within the Massive Online Analysis (MOA) framework\u00b9, and tuned for each dataset using a grid search strategy, with ensembles of up to 90 components, grace periods set to 100, 400, and 1000, and tie-splitting thresholds configured to 0.01, 0.05, and 0.1 as the hyperparameter settings.\nThe LTM was implemented with the publicly available first version of TabPFN \u00b2 on a T4 GPU. This model relies on causal attention layers typical of a standard decoder-only Transformer architecture [106], making it possible to process a streaming sequence where tokens become available one at a time [95]. However, with a special attention mask, where in-context tokens only attend to each other, with no attention to the query instances. Instead, the target to the query instances are used as a label to minimize the cross-entropy loss [34]. Since tabular columns are permutation invariant, the feature orderings and scalings are shuffled for an ensembled prediction. In these experiments, 4 permutations were performed."}, {"title": "Future research directions", "content": "In the early stages of machine learning, the dominant belief was that the best way to solve a task was to meticulously design a specific dataset and tailor a model exclusively for that task. Over time, it became evident that for certain types of unstructured data, such as image recognition, a general-purpose model could be fine-tuned on specific datasets, yielding effective results. Today, we stand at a point where highly versatile models, such as general-purpose transformers, can be prompted to tackle a wide range of tasks without task-specific training. With this work, we embrace this evolving paradigm. So far, we have identified LTMs as a promising candidate for streaming solutions, warranting further research. The key challenge is not merely the size of the model or processing speed but rather the dynamic interplay between data arrival, training, recovery, and inference. As technology advances, models will naturally become smaller, faster, and more affordable. In the meantime, efforts should focus on designing algorithms that align more closely with the core principles of data stream mining: processing data in its natural order, handling labels that may be correct at the time of request but outdated or even incorrect by the time they become available, minimizing interference among concepts, and so on. Building on this foundation, several promising research directions emerge within this evolving paradigm. The authors encourage the academic community to explore these opportunities, as they hold significant potential for advancing the field. The authors, too, may pursue some of these directions and welcome feedback and collaboration from those interested in contributing to this line of research."}, {"title": "Developing new LTMs", "content": "Since the pre-training stage did not represent the added value of this work, an already existing LTM for batch data was used. However, many benefits could be introduced by scaling LTMs with large-scale drifting tabular datasets. Indeed, it has been shown repeatedly that the construction of modality-specific large-scale datasets allows to significantly increase performance, while using the same model architecture and training procedure from prior work [12]. For instance, recent work filtered large collections of raw tabular data [112] with a mix heuristics and rule-based methods applied at a table,"}, {"title": "Hypernetwork-based LTMs", "content": "Instead of predicting the target directly, hypernetwork-based LTMs predict a set of concept-specific weights for a target model. This target model then performs classification on tabular data, allowing the LTM to remain independent from the inference stage, which is particularly beneficial in memory- and time-constrained scenarios. Examples of such LTMs include HyperFast [28] and MotherNet [29], both of which generate feed-forward neural networks through a single forward pass. In HyperFast, the weights of the main network are generated layer-by-layer by hypernetwork modules with shared and layer-specific parameters. The first layer applies a random features kernel trick combined with PCA to form a fixed-size, permutation-invariant representation. Layers 2 to L \u2013 1 employ MLP-based hypernetworks that process intermediate representations, incorporating residual connections and non-linear activations. For the final layer L, HyperFast avoids directly predicting the weights, instead averaging per-class embeddings to create a lightweight classification weight matrix. In contrast, MotherNet adapts the TabPFN model to generate weights for a compact feed-forward network with a fixed architecture consisting of two hidden layers of 512 units each. MotherNet introduces an attention layer that compresses all activations into a single dataset embedding, based on a learned query vector [29]. This query token is optimized during training and subsequently fixed, similarly to prefix tuning [114], but is applied solely within a top-layer attention module rather than across the entire transformer. For future research, one can draw upon the traditional hypernetwork architectural paradigms, which are typically categorized as either task-conditioned or data-conditioned. Task-conditioned schemes involve providing task-specific inputs, such as task identity/embedding, hyperparameters, or other cues [115], while data-conditioned schemes adapt based on the characteristics of the input data [116, 117]. While hypernetwork-based LTMs have primarily focused on data-conditioned schemes, exploring task-conditioned schemes may offer better information sharing across multiple tasks [115]. However, the conditions under which to model a problem as data-conditioned or task-conditioned remain unclear, and require further exploration depending on the problem, the availability of data, and the number of tasks [118]. Beyond the dynamic nature of input-dependent contexts, future work can investigate alternative output-based strategies. Hypernetwork-based LTMs have primarily focused on generating weights for the entire target model simultaneously [29, 28], but generating a large number of weights for the target model can result in a larger last layer in the hypernetwork. One potential solution is to introduce multiple heads for weight generation, thereby reducing the number of weights required in the final layer by a factor corresponding to the number of heads [115]. Furthermore, adopting a chunk-wise or component-wise weight generation approach could be advantageous. In chunk-wise approaches, some weights may remain unused, with additional embeddings used to distinguish and produce different chunks, which reduces complexity and improves the scalability of hypernetworks [115]. Component-wise approaches go further, generating separate weights for each layer or channel in the target model, allowing the network to focus on specific features or patterns [117]. It is important to note that, unlike traditional hypernetwork architectures [119], which use task-specific, multi-task hypernetworks to generate large target networks, hypernetwork-based LTMs utilize large, transformer-style networks to generate compact tabular classification models. Moreover, hypernetwork-based LTMs should generate all weights in a single forward pass, rather than generating weights for specific layers (e.g., the final layer) while training other parts of the feature extractor [120, 121]."}, {"title": "Localization strategy", "content": "Similar to LLMs, compressing the context for LTMs is essential for ensuring alignment with the context length. This process must account for the fact that different data points exhibit varying levels of potential, with some being more representative or informative than others. Rather than naively selecting a random subsample of the training data as context for the prompt [33, 122], it is more effective to retrieve information that is specifically relevant to the task at hand. Techniques such as compressing the dataset through k-means centroids [79] or employing dataset distillation [80] have been applied. In a streaming setting, however, synopsis techniques, such as histograms, wavelets, sketches are often required to construct geometric and statistical data descriptors as the online phase of clustering-based approaches, capturing the internal structure of the classes [123]. Subsequently, these methods are followed by a offline mining process performed on the stored micro-cluster synopsis whenever a user sends a request. For example, this two-phase learning approach has been used as a temporal extension for clustering feature vectors [124], which was later extended with projection techniques for subspace clustering in high-dimensional data streams [125] and temporal-dependent splitting for long-term operation [126]. Moreover, this synopsis strategy can be utilized to build a density profile of"}, {"title": "Retrieval-based strategy", "content": "Context optimization should not be constrained to a uniform context across all query points. While certain coresets may provide valuable insights for defining the current concept, others could limit the model's adaptability. Instead, a more flexible approach involves storing a larger pool of encoded knowledge that can be dynamically grouped into adaptive local contexts, tailored to each specific query [28]. This retrieval-based strategy facilitates LTMs, which rely on subsampling, to scale more efficiently, even when the number of samples exceeds the maximum context length. Moreover, separating new knowledge from the feature space used to revisit older knowledge reduces interference between old and new subspaces, preserving valuable information that may not contribute immediately but could be useful in future distributions. To construct the most representative context at each iteration, it is essential to define a measure of coreset affinity. One approach involves designing a short-term context-wise query mechanism, where short-term memory with labels is utilized as training data to compute the loss in prediction for each coreset within the larger pool. This key-value pair-based query strategy provides a distribution matching measure for attention, which can then be used to dynamically select the top-k suitable coresets based on their alignment with the current context. Alternatively, one might assume that the most relevant information for classifying a query point is contained within its local vicinity, employing k-NN to identify the nearest domain centers [142]. The number of neighbors governs the model's expressivity and the bias-variance trade-off. Naturally, the varying potentials of coresets become particularly pronounced in practical scenarios involving imbalanced or noisy instances. To address this, a second training set can be constructed in which the sample size in each region is inversely proportional to that of the original set. This allows for the retrieval of neighboring samples from both sets for a new query [81]. When pursuing this research direction, it is essential to note that TabPFN [33] replicates each prompt across multiple permutations of feature shufflings and scalings to ensure invariance with respect to table column order. To address the resulting inefficiencies in the forward pass, TabPFN prompting fits both the test samples and their context simultaneously, thereby accelerating inference. As a result, any retrieval-based strategy built upon TabPFN must align with this batching protocol, wherein multiple test points share the same context."}, {"title": "Meta-representations", "content": "Rather than operating directly within the original data space, an LTM can be applied in a projected space, shifting the focus to learning how to represent relationships between features and labels in an embedding space. This approach addresses the heterogeneity within attribute and class spaces across tabular datasets, which not only complicates joint training of an LTM but also hinders its direct application during inference. To achieve this, LTMs can incorporate a meta-representation module, enabling datasets to be transformed into a uniform format of consistent dimensionality while filtering out redundant or noisy attributes. For example, TabPTM standardizes heterogeneous datasets by defining instances, regardless of their original dimensionality, through their membership to a class in terms of the Euclidean or Manhattan distance to the top-k nearest class-specific prototypes in the training set [30]. This results in a meta-representation of instances as a set of K-dimensional vectors, one for each of the classes, which is then used to train the LTM across multiple tabular datasets to extract class-wise confidence scores. With this trained LTM, one can compute the meta-representation of any downstream dataset and perform generalization-free classification in few-shot scenarios."}, {"title": "Conditional computation", "content": "Rather than executing the full LTM, specific parts of the network can be activated on a per-instance basis. This strategy significantly enhances model capacity without a proportional increase in computational cost. Building on this idea, MixturePFN improves prompt effectiveness by directing new test samples to a specialized prompter context, which is fine-tuned on a cluster of training data most relevant to the query [82]. The clusters are formed by expanding and subsampling K-Means clusters, tailored to the desired context size. The effectiveness of MixturePFN prompts depends on the number of in-context prompters (ICPs), with a larger number required as the complexity and scale of data grow to capture label entropy. MixturePFN employs a tunable hyperparameter, \u03b3, to balance efficiency and effectiveness, where the number of ICPs is given by [YNtrain/B]. Intuitively, increasing y enhances effectiveness but at the cost of efficiency."}, {"title": "Fine-tuning", "content": "While the previous strategies generally improve performance, unexpected drops may occur due to distribution shifts between the true data-generating mechanism during inference and the pretraining dataset prior [33, 122]. To address this, MixturePFN employs bootstrapping and linear adapter layers for parameter-efficient fine-tuning, minimizing the parameter count for each new expert on the assigned context, i.e., training cluster [82]. Additionally, fine-tuning has been shown to significantly improve retrieval-based training performance [83, 142]. It is important to note, however, that methods built on top of TabPFN should recognize that fine-tuning it is nontrivial, as it has been trained on a synthetic dataset prior, rather than a pretraining dataset."}]}