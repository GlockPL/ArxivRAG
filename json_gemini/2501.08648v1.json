{"title": "MAGNET: AUGMENTING GENERATIVE DECODERS WITH REPRESENTATION LEARNING AND INFILLING CAPABILITIES", "authors": ["Savya Khosla", "Kushal Kafle", "Simon Jenni", "Handong Zhao", "John Collomosse", "Jing Shi"], "abstract": "While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning, respectively). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we introduce MAGNET, an adaptation of decoder-only LLMs that enhances their ability to generate robust representations and infill missing text spans, while preserving their knowledge and text generation capabilities. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging future context, (3) retain the ability for open-ended text generation without exhibiting repetition problem, and (4) preserve the knowledge gained by the LLM during pretraining.", "sections": [{"title": "INTRODUCTION", "content": "Language models are computational models designed to understand and generate human language. They have transformed natural language processing, powering applications such as text annotation, machine translation, summarization, speech recognition, and dialogue systems. Traditionally, language models are categorized into three main types: (1) Encoder-only models, which focus on encoding the input into fixed-dimensional representations and excel in tasks like sentiment analysis (sentence-level classification) and named entity recognition (token-level classification). (2) Decoder-only models, which are adept at generating coherent text, thereby specializing in tasks like creative content generation and dialogue systems. (3) Encoder-decoder models, wherein an encoder understands the input and a decoder generates the corresponding output, making this architecture suitable for tasks like machine translation and summarization.\nRecently, the NLP community has increasingly embraced decoder-only architecture (Large Language Models or LLMs) due to their efficient training and scalability to larger datasets, resulting in enhanced performance across various tasks. However, since these models are trained using causal attention and lack bidirectional context, they are less suitable for tasks like (1) sentiment analysis and named entity recognition, which require understanding contextual representations of sentences or words, and (2) text infilling, where predicting missing text spans must maintain coherence with subsequent context. With the goal of leveraging the scalable decoder-only LLMs, some recent efforts have aimed to adapt decoder-only LLMs for these tasks. However, as illustrated in Figure 1, methods that enhance LLMs for text infilling do not enable them to function as effective text encoders, while approaches focused on representation learning hinder their generative capabilities."}, {"title": "RELATED WORKS", "content": "Representation Learning. Text representation learning focuses on understanding contextual relationships within sentences. Traditionally, encoder models dominated this field due to their bidirectional context modeling, using masked language modeling for token-level representations and special tokens with similarity-based optimization for sentence-level understanding. Recent work has explored adapting decoder-only LLMs for representation learning, either by using last-token or mean-pooled representations, or by fine-tuning with masked modeling or label supervision. While some approaches modify the decoder's causal attention to"}, {"title": "TEXT INFILLING", "content": "Text infilling requires considering both left and right context when generating text in the middle of a sequence. Encoder-decoder models can handle this task by encoding available context and decoding infilled text. Other approaches have extended masked language modeling to perform span infilling. Decoder-only models have also been adapted for infilling through various strategies: training models to directly fill marked blanks, rearranging training examples to align with infilling objectives, or using dual generation from both ends of a sentence until convergence. However, while these approaches successfully enhance LLMs with infilling capabilities, none have attempted to simultaneously equip them with both infilling and representation learning abilities, as done by MAGNET."}, {"title": "UNIFYING TEXT UNDERSTANDING AND GENERATION", "content": "Prior works on unifying natural language understanding and generation within a single framework usually focus on proposing pretraining objectives and task formulations. These approaches typically extend traditional masked language modeling, with innovations like permutation-based objectives for bidirectional context modeling, autoregressive blank infilling, multi-directional attention masks, and sequence-to-sequence pretraining. However, these approaches require pretraining new networks from scratch, despite decoder-only models demonstrating exceptional scalability and effectiveness. Instead of starting from scratch, we propose a parameter-efficient method that builds upon the rich representations already learned by existing large language models, transforming them into a unified framework for representation learning, text infilling, and text generation."}, {"title": "METHOD", "content": "Decoder-only models, based on the Transformer architecture, process input sequences through successive blocks of multi-head self-attention, feed-forward networks, and layer normalization. The self-attention mechanism converts the input $x \\in R^{l \\times d}$ into queries Q, keys K, and values V using a linear projections, and computes attention using the formula:\n$Attn: (Q, K, V) = softmax(\\frac{QK^T + M}{\\sqrt{d_k}}) V$\nwhere $Attn_i$ is the $i^{th}$ head of the multi-head self-attention, $d_k$ represents the dimensionality of the keys/queries, and M represents the causal mask.\nMAGNET seeks to update the causal attention mechanism of an LLM by incorporating elements of bidirectionality and thereafter fine-tunes the model using unsupervised objectives. We look at the modifications to the attention mechanism in Section 3.1 and the training objectives in Section 3.2."}, {"title": "MODIFYING ATTENTION", "content": "MAGNET updates the causal attention mechanism of an LLM to introduce bidirectional capabilities within segments of the input sequence, as illustrated in Figure 2. After the input text is tokenized for the language model, we categorize each token as either context tokens or span tokens:\nContext tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens within the sequence. In our implementation, the attention mask is designed with 0s at output positions corresponding to context tokens, allowing each context token to access information from every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model.\nSpan tokens. The span tokens (shown in green in Figure 2) represent a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling"}, {"title": "TRAINING OBJECTIVES", "content": "MAGNET fine-tunes an off-the-shelf LLM using three self-supervised objectives aimed at enhancing the model's ability to learn contextually-rich representations and autoregressively fill in missing spans of text. These objectives are illustrated in Figure 3 and discussed below."}, {"title": "MASKED NEXT TOKEN PREDICTION (MNTP)", "content": "MNTP enables the model to realize its newly enabled bidirectional attention capability. The task is defined as follows: Given an input sequence $x = (x_1, x_2, ..., x_L)$, we select a fraction of the input tokens for masking and train the model to predict these masked tokens. In our setup, we find that selecting 20% of the input tokens for masking works well. Further, following Devlin et al. (2019), we replace 80% of the selected tokens with a [MASK] token, 10% with a random token from the model's vocabulary, and leave the remaining 10% unchanged. Since LLMs are trained to predict the next token in a sequence, we use the token representations from position l to predict a masked token at position l + 1. MNTP is optimized using categorical cross-entropy loss:\n$L_{MNTP} = \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{l=1}^{L} \\sum_{v=1}^{V} 1_{mask}(l + 1) \\cdot (y_{lv} log(\\hat{y}_{lv}))$"}, {"title": "SELF-SUPERVISED CONTRASTIVE LEARNING (SSCL)", "content": "Since LLMs are not explicitly trained to capture the entire input context and generate sentence-level representations, we employ SSCL to transform them into text encoders. The training objective is defined as follows: Given an input sequence x, generate its augmented view x+ and align their"}, {"title": "MISSING SPAN GENERATION (MSG)", "content": "MSG provides text infilling capabilities to the left-to-right autoregressive model. The task is defined as: Given a position p and an input sequence $x = (x_1, ..., x_p, x_q, ..., x_L)$, generate a plausible sequence of m tokens $y = (y_1, y_2, \u2026\u2026\u2026, y_m)$ that fits between $x_p$ and $x_q$. More specifically, in our training setup, this task entails predicting a span token $y_l$ conditioned on all context tokens in x and the preceding span tokens $x_{[1..l-1]}$. We train using categorical cross-entropy loss computed over the predicted span tokens:\n$L_{MSG} = \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{l=1}^{L} \\sum_{v=1}^{V} 1_{span}(l) \\cdot (y_{lv} log(\\hat{y}_{lv}))$"}, {"title": "APPROACH OVERVIEW", "content": "Figure 4 provides an overview of MAGNET. Starting with a training example x, the process unfolds in two parallel streams \u2013 (1) One or more contiguous spans of M tokens in x are marked as span tokens, while a fraction of the remaining tokens (context tokens) is masked to form xm. (2) x is augmented to get x+. The input sequences x, xm and x+ are processed by the base decoder model to produce hidden states h, hm and h\u207a. From hm, a language modeling head generates ym, which is used to compute LMNTP and LMSG. Parallelly, h and h+ are processed using a projection head to get e and e+, which are used to compute LsSCL. The overall loss function is given as:\n$L = \\lambda_1 L_{MNTP} + \\lambda_2 L_{SSCL} + \\lambda_3 L_{MSG}$\nFor processing x to get x+, the decoder utilizes a bidirectional attention mask, illustrated in Figure 2b. While processing xm, the decoder employs an attention mask similar to those depicted in Figures 2c and 2d. In some cases, when all input tokens are marked as span tokens, the attention mask reduces to causal attention, as shown in Figure 2a."}, {"title": "EXPERIMENTS", "content": "In this section, we show that applying MAGNET augments a decoder-only LLM with representation learning and infilling capabilities. All training details are mentioned in Appendix A. Additionally, we present ablation experiments demonstrating the benefits of training a bidirectional model with a causal objective in Appendix C."}, {"title": "WORD-LEVEL TASKS", "content": "We evaluate the token-level representations on three tasks \u2013 (1) chunking, (2) named entity recognition, and (3) part-of-speech tagging \u2013 using the CoNLL-2003 dataset. After applying the training objectives proposed in Section 3.2, we train a linear classifier on top of the frozen representations obtained from the last hidden state of the model. The word-level embeddings"}, {"title": "SENTENCE-LEVEL TASKS", "content": "We evaluate sentence-level representations on multiple semantic similarity and clustering benchmarks. We perform these tasks using the representation corresponding to the last token ([EOS]), without performing any task-specific training. Further, task-specific instructions are used for extracting relevant representations. We compare the text encoding capabilities of MAGNET with other recently proposed methods for transforming decoder models into text encoders, viz. LLM2Vec and Echo Embeddings."}, {"title": "INFILLING TASK", "content": "To test infilling capabilities, we evaluate the perplexity (PPL) of LLaMA-2-7B and MAGNET-adapted LLaMA-2-7B on the test set of ROC Stories and Wikitext-103. For ROC Stories, we randomly mask out a sentence from each 5-sentence story, while for Wikitext-103, we mask up to three spans with lengths ranging from 8 to 32 tokens. Following Donahue et al. (2020), we compute PPL only for the tokens comprising the original masked out spans. The results are presented in Table 4, and they show that the base model (LLaMA-2-7B) exhibits significantly higher perplexity for the masked spans compared to MAGNET, demonstrating that MAGNET effectively augments the base model with text infilling capabilities. This improvement is attributed to MAGNET's ability to incorporate all the surrounding contextual information when infilling text, which increases the likelihood of correctly predicting the original masked content.\nWe also conducted experiments using zero-shot and few-shot learning to enable LLaMA-2-7B to incorporate all the surrounding context when infilling a missing span. We explored various prompting strategies and found that while a zero-shot setup did not yield sensible infillings, a five-shot setup with descriptive prompts resulted in more context-aware infillings (refer Appendix B for details). For a comprehensive analysis, we conducted a human evaluation to compare the quality of infillings generated by the base model, its zero-shot variant, its few-shot variant, and its MAGNET adaptation. In this evaluation, we randomly sampled 100 stories from the ROC Stories dataset, masked out one of their middle sentences, and tasked the models with infilling the missing sentence. Two human annotators on Amazon Mechanical Turk then independently assessed whether each generated sentence was contextually appropriate and contributed to a coherent story. The results are presented in Table 5, and they show that the infillings generated by MAGNET-adapted model are significantly more coherent than those generated by the variants of the base model. We show some qualitative examples in Table 10, demonstrating that MAGNET successfully delivers relevant and contextually appropriate infillings."}, {"title": "OPEN-ENDED TEXT GENERATION AND REPETITION PROBLEM", "content": "The repetition problem in text generation refers to the issue when generative models repeatedly produce the same phrases or sentences. Prior studies have identified that this issue often results from"}, {"title": "TEXT GENERATION AND KNOWLEDGE RETENTION", "content": "In Table 6, we evaluate the impact of MAGNET on the knowledge acquired by the LLM during its pretraining, as measured by the Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) benchmark. We find that adapting the base model using the Wikitext-103 dataset results in a performance decline of 1.6% on average. This degradation can be attributed to the dataset's encyclopedic nature, potentially introducing a knowledge bias that narrows the model's generalization capabilities. To mitigate this limitations, we also adapt the model by fine-tuning on the SlimPajama dataset, which comprises texts from more diverse data sources like Commoncrawl, C4, GitHub, Books, ArXiv, Wikipedia, and StackExchange. With SlimPajama adaptation, the performance drop on MMLU is only 0.4% on average. Some categories (like 'Others')"}, {"title": "CONCLUSION", "content": "In this work, we presented MAGNET, a method to transform causal LLMs into text encoders and infilling language models with bidirectional context-capturing ability. Through extensive experiments, we show that MAGNET uniquely equips LLMs with abilities that are beyond the scope of traditional text encoders or decoders. Thus, MAGNET shows the potential to unify text generation and text encoding within a single framework. Future research could explore scaling MAGNET to multimodal settings."}, {"title": "TRAINING DETAILS", "content": "MAGNET fine-tunes LLaMA-2-7B using LoRA with r = 16 and a = 32. We use the AdamW optimizer with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999 and \u2208 = 1e - 8, apply bfloat16 quantization, and use scaled-dot-product attention (SDPA). All experiments are performed on a single NVIDIA A100 GPU, with the MAGNET adaptation of LLaMA-2-7B taking approximately 7 hours. The hyperparameters for the different training objectives are as follows:\nMNTP. We train for 4200 iterations using the Wikitext-103 train set with a batch size of 32, a learning rate of 3e-5, and a max sequence length of 512. We select 20% of the tokens for masking \u2013 80% of the selected tokens are replaced with a [MASK] token, 10% tokens are replaced with a random token from the model's vocabulary, and 10% tokens are left unchanged. For LLaMA-2-7B, we use \"_\" as the mask token.\nSSCL. We train for 800 iterations with a batch size of 64, a learning rate of 3e-5, and a max sequence length of 128. To extract representations we use the prompt \"Given the sentence, find its representation:\" and extract the representations corresponding to the last token. The training data is created from Wikitext by extracting lines longer than 20 words and paraphrasing them for the positive examples. We set t = 0.1 in equation 3.\nMSG. Similar to MNTP, we train for 4200 iterations using the Wikitext-103 train set with a batch size of 32, a learning rate of 3e-5, and a max sequence length of 512. A training example can have up to 2 missing spans with span length ranging from 4 to 128 tokens.\nOverall Loss. For the first 3400 iterations, we optimize the loss with \u03bb\u2081 = 1, \u03bb\u2082 = 0, and \u03bb\u2083 = 1, and for the next 800 iterations \u03bb\u2081 = 1, \u03bb\u2082 = 9, and \u03bb\u2083 = 1. We initially train with only MNTP and MSG because these objectives help the model learn to capture future context, a capability the base model lacks.\nWord-Level Tasks. Using the frozen representations from the last hidden layer of the base model, we train a linear classifier for the three word-level tasks (Chunking, NER, and POS-tagging). Specifically, we train on the CoNLL-2003 train set for 4000 steps using a batch size of 8, a learning rate of 5e-4, and a dropout rate of 0.1."}, {"title": "CONTEXTUAL PROMPT INFILLING", "content": "To thoroughly evaluate the infilling capability of the base model, we perform zero-shot and few-shot experiments where the model is shown both preceding and following context of a missing span of text."}, {"title": "ZERO-SHOT EVALUATION", "content": "To this end, we experimented with four types of prompts to infill a missing line in five-line stories from the ROC Stories dataset. The four prompting strategies we used are:\nBlank Infilling Prompt. In this setting, we add a blank token at the infilling position and use the following prompt:\nGenerate the missing line represented by _ in the given text: <text>.\nGenerate a single sentence.\nThe missing line is:\nHere, <text> represents the input text with \"_\" in place of a missing sentence.\nContextual Prompt. In this setting, we provide the past and future context of the missing line and use the following prompt:\nFill in the missing sentence between \"<past-context>\" and \"<future-context>\". Generate only one sentence. The missing sentence is:\nPrefix-Suffix Prompt. In this setting, we give the past context of a missing sentence as a prefix and the future context as a suffix and ask the model to generate the middle. Specifically, we use the following prompt:"}, {"title": "FEW-SHOT EVALUATION", "content": "To improve infilling results from the base model, we employed few-shot learning techniques with various prompting styles \u2013 blank infilling, prefix-suffix, and line-by-line. Specifically, we provided five solved examples in the model's context using the chosen prompt format and asked the model to infill the missing line in the sixth example. We observed that more descriptive prompts and examples led to better output from the model, and the line-by-line prompting style seemed to be the most effective in enabling coherent infillings."}, {"title": "TRAINING OBJECTIVE ABLATION ANALYSIS", "content": "We perform ablation experiments to evaluate the effectiveness of our unified training with the three proposed objectives. Specifically, we compare the performance on representation learning tasks after adapting the LLM using different combinations of the objectives. The results are presented in Table 7. We find that while MNTP is the only objective that explicitly trains the model for better token-level representations, adding MSG marginally improves performance on word-level tasks. We conjecture that MSG, being closer to the original next-token prediction objective of the base LLM, acts as a regularizer and helps prevent extreme variations in the token representations"}, {"title": "COMPARING MTP AND MNTP OBJECTIVES", "content": "Traditionally, language models for representation learning are trained to predict a masked token at position l using the output at position l in the final hidden states. This approach is logical because the residual connections in the transformer block incorporate the lth token's input representation into its output representation.\nWe conducted an experiment to test whether we can use LoRA to adapt the base LLM for l-to-l prediction (similar to BERT). The training curves for masked token prediction (MTP) and masked number token prediction (MNTP) are shown in Figure 7. As illustrated, with MTP, the loss converges, but the evaluation accuracy for masked token prediction decreases. This likely occurs because the base model is trained to predict the (l + 1)th token at position l, and shifting to l-to-l prediction introduces a significant distributional shift that the model may struggle to accommodate swiftly. Thus, overall, we find that MNTP is a more effective objective for converting a decoder-only LLM into a text encoder. Additionally, MNTP aligns well with the causal MSG objective and paves way for a unified text generator and encoder."}, {"title": "LIMITATIONS", "content": "While MAGNET better preserves the open-ended generation capability of the base LLM compared to other bidirectional adaptation methods, it still reduces generation quality. For instance, fine-tuning LLaMA-2-7B with MAGNET increases the test set perplexity (PPL) on Wikitext-103 from 6.4 to 7.6. Although qualitative analysis shows no major artifacts in the generated text, the model's confidence in predicting the next word seems to be somewhat diminished.\nIn the infilling task, we only focus on augmenting the base LLM with the ability to consider all the surrounding information to produce contextually coherent infilling. We have observed that the quality of infilling decreases when using the MAGNET-adapted LLM to infill very long sequences in the middle of a text. We believe this observation is a result of the parameters set for the MSG objective, and it can be addressed by the following three ways: (1) Instead of attempting to infill"}]}