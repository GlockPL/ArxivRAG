{"title": "Query-Guided Self-Supervised Summarization of Nursing Notes", "authors": ["Ya Gao", "Hans Moen", "Saila Koivusalo", "Miika Koskinen", "Pekka Marttinen"], "abstract": "Nursing notes, an important component of Electronic Health Records (EHRs), keep track of the progression of a patient's health status during a care episode. Distilling the key information in nursing notes through text summarization techniques can improve clinicians' efficiency in understanding patients' conditions when reviewing nursing notes. However, existing abstractive summarization methods in the clinical setting have often overlooked nursing notes and require the creation of reference summaries for supervision signals, which is time-consuming. In this work, we introduce QGSumm, a query-guided self-supervised domain adaptation framework for nursing note summarization. Using patient-related clinical queries as guidance, our approach generates high-quality, patient-centered summaries without relying on reference summaries for training. Through automatic and manual evaluation by an expert clinician, we demonstrate the strengths of our approach compared to the state-of-the-art Large Language Models (LLMs) in both zero-shot and few-shot settings. Ultimately, our approach provides a new perspective on conditional text summarization, tailored to the specific interests of clinical personnel.", "sections": [{"title": "1. Introduction", "content": "Electronic Health Records (EHRs) document the events that patients go through during their hospitalization. These records consist of both free-text clinical notes and structured data. Among them, nursing notes are important for keeping track of the progression of patients' illnesses, changes in health status, as well as the medications and procedures administered (T\u00f6rnvall and Wilhelmsson, 2008). Nursing notes provide clinicians with comprehensive insights into the patients' conditions, assisting in formulating next-step treatment and care plans, as well as in writing the final discharge summaries. However, a patient's care episode may result in a large number of nursing notes, especially for patients suffering from complex health problems, which causes the problem of information overload (Hall and Walton, 2004). Additionally, the information in nursing notes is usually intricate and highly condensed, making it time-consuming for clinicians to understand (Clarke et al., 2013).\nIn Natural Language Processing (NLP), text summarization techniques can be used to distill the content of nursing notes (Wang et al., 2021) to help clinicians quickly grasp their contents. Automatic clinical note summarization has been extensively studied, with existing approaches categorized into extractive (Pivovarov and Elhadad, 2015; Moen et al., 2016; Tang et al., 2019) and abstractive methods (Zhang et al., 2020b; Liu et al., 2022a; Searle et al., 2023). However, these methods have certain limitations: (1) Extractive methods only retain sentences, important words, or keyphrases from the original note, limiting coherence and fluency of the summary. This presents a challenge for understanding the summarized content. On the other hand, (2) abstractive methods can generate smoother summaries, but most of the existing works on abstractive clinical note summarization require explicit supervision, i.e., a reference summary as the ground truth. Writing the references is time-consuming (O'Donnell et al., 2009), causing a shortage of training data. Moreover, (3) most abstractive note summarization methods focus on a specific type of notes, such as discharge summaries, radiology reports, or the dialogues between doctors and patients, and there is a lack of research on abstractive nursing note summarization.\nTo address these limitations, we propose a novel approach for abstractive nursing note summarization, which does not require reference summaries for training. The nature of the nursing documentation poses additional challenges. For example, as shown in Figure 1, information in nursing notes may lack clarity, be poorly organized, and contain medical jargon that often includes non-standard abbreviations. In the absence of supervised learn- ing signals, guiding the summarization model to understand the semantic information in notes and generate a good summary becomes challenging. Some previous text summariza- tion works (Chu and Liu, 2019; Elsahar et al., 2021) adapt strategies in self-supervised learning (Liu et al., 2021) to such a scenario. In their methods, the training objective is to decrease the semantic distance between a summary and the original text based on the assumption that a good summary is capable of reconstructing the source text. However, simply making the semantic representation of the summary close to that of the original document does not allow controlling the generated summaries, which may result in a lack of relevant information. In clinical domains, we have specific requirements for the content of the summary, where the focus should be on the patient's condition. Thus, methods that rely solely on the semantic similarity may not be fully satisfactory.\nIn this paper, we propose a query-guided self-supervised domain adaptation framework for nursing note summarization, named QGSumm. We formulate a learning objective to adapt the text summarization capability of a pretrained language model to the nursing note domain. Our approach is built on a hypothesis:\nA good summary of a clinical note is centered on the patient's condition. Consequently, when queried about the state of the patient, answers obtained from the summary will be similar to those obtained from the original note.\nFor instance, a query can concern the probability of a patient's condition improving in the near future. We can train a model to answer this query using the current nursing note or its summary as input, and if the summary is accurate these two answers should be similar. Accordingly, we design a learning objective that aims at minimizing the discrepancy between the responses from the summary or from the source note to given queries. To further encourage the model to prioritize the patient's current medical condition, we integrate into the summarization workflow both the patient's metadata as well as information in the previous notes of the patient recorded on the same admission.\nTo the best of our knowledge, our proposed framework is the first on abstractive sum- marization of nursing notes, and there is no previous work on employing the self-supervised learning strategy for clinical note summarization. Our primary contributions are:\n\u2022 The study focuses on nursing notes that play a critical role in clinical settings, filling a gap in previous research by introducing a method for abstractive nursing note sum- marization. Our method's ability to operate effectively without requiring reference summaries highlights its practical applicability.\n\u2022 We propose a novel self-supervised domain adaptation framework. By leveraging patient-related queries to guide the model, we achieve the goal of generating nursing note summaries that prioritize specific content, i.e., patients' conditions and health status, without the need for ground truth.\n\u2022 We conduct a comprehensive automatic evaluation and a manual evaluation by an expert clinician. We compare our approach with state-of-the-art Large Language Models (LLMs) in few- and zero-shot settings. This demonstrates the method's ability to generate high-quality summaries of nursing notes, and additionally provides an independent evaluation of the common LLMs in this task.\nGeneralizable Insights about Machine Learning in the Context of Healthcare\nIn existing healthcare-related NLP research, there is a noticeable gap in addressing nursing notes specifically. Summarizing key patient information within nursing notes could enhance the efficiency of medical personnel's workflow. We provide a new perspective on obtaining summaries of nursing notes through self-supervised domain adaptation without the need for manually written reference summaries. In unconditional text summarization, the generated summary lacks explicit constraints. On the other hand, most conditional text summa- rization methods typically require data annotation (Vig et al., 2022) or the extraction of information related to content conditions (Pagnoni et al., 2023) from the source text, mak- ing them less suitable for our task. We introduce easily applicable patient-related queries as a way to facilitate conditional text summarization of nursing notes, which ensure that the generated summaries contain information required to respond to the query effectively, and are closely linked to the patient's key information. Such constraints and guidance make our method highly suitable for the clinical and healthcare field since the resulting summaries are centered on the information nurses and clinicians are most concerned about."}, {"title": "2. Related Work", "content": "Key Information Extraction and Summarization from Clinical Notes\nResearch in this domain focuses on two approaches, extractive and abstractive summa- rization. The extractive method can preserve faithfulness but results in the inability to paraphrase and difficulties in maintaining coherence. Earlier work primarily used seman- tic similarity-based techniques (Pivovarov and Elhadad, 2015; Moen et al., 2016). The emergence of Transformer models has shifted the focus to attention-based methods to de- termine key information in clinical text with an emphasis on explainability (Tang et al., 2019; Reunamo et al., 2022; Kanwal and Rizzo, 2022).\nRecently, there has been a notable increase in research on abstractive clinical text sum- marization. From an application perspective, these methods mainly target discharge sum- mary generation (Shing et al., 2021; Adams et al., 2022; Searle et al., 2023), radiology report summarization (Zhang et al., 2020b; Van Veen et al., 2023), summarization of doctor-patient conversations (Zhang et al., 2021; Krishna et al., 2021; Abacha et al., 2023) and problem list summarization (Gao et al., 2022, 2023). However, unlike our approach, most of these works depend on data annotation or reference summaries for training and domain adaptation.\nLLMs demonstrate a remarkable capability in clinical text understanding, leading to interest in investigating their performance in clinical text summarization. Van Veen et al. (2024) extensively analyze the clinical text summarization performance of various LLMs with in-context learning (Lampinen et al., 2022) and QLoRA (Dettmers et al., 2024) adap- tation. They compare the performance of LLMs with medical experts, providing insights into the strengths and limitations of LLMs.\nUnsupervised and Self-Supervised Abstractive Text Summarization\nThe scarcity of annotated text for abstractive text summarization tasks has spurred in- terest in unsupervised and self-supervised text summarization. Previous works have relied on source document reconstruction, operating under the assumption that a good summary should be able to reconstruct the source document or capture its essential content (Chu and Liu, 2019). However, reconstructing an entire text using a summary without any guiding signal or prompt is challenging. In contrast, Yang et al. (2020) leverage the lead bias in news articles, by pretraining a model to predict the leading sentences as the target summary. However, this approach requires specific information distribution and text layout, which is not generally applicable. Some works have proposed two-step approaches to first extract important information or entities in the source text and then perform abstractive summa- rization with the guidance of this extracted information (Zhong et al., 2022; Ke et al., 2022; Liu et al., 2022b). However, the quality of the generated summary relies on the effectiveness of the extraction process, and developing a reliable extractor may entail significant costs. Zhuang et al. (2022) propose a contrastive learning strategy, using source documents as positive and edited source documents as negative examples. Their training objective aims at maximizing the semantic similarity between generated summaries and positive examples while minimizing those between generated summaries and negative examples. Hosking et al. (2023) propose an attributable opinion summarization system, which encodes sentences as paths through a hierarchical discrete latent space. Given a specific entity, the system can identify its common subpaths that are decoded as the output summary."}, {"title": "3. Methods", "content": "Next, we introduce QGSumm, a novel framework to automatically summarize and refine clinical notes, with a focus on capturing important patient-centered information in a self- supervised fashion (Figure 2). In line with the hypothesis, we propose a self-supervised domain adaptation strategy applied on the base model presented in Section 3.1. This strat- egy positive-contrastively learns from the original nursing notes, providing the summaries with an ability comparable to the original notes to resolve patient-related queries (Section 3.2). Moreover, we aim for the model to maintain focus on the patient's meta informa- tion while also considering temporal aspects during the generation process. To achieve this, we propose two augmentation blocks, detailed in Section 3.3, to enhance the overall performance. Our model summarizes one nursing note at a time, taking into account its context. Assume a patient PT has a sequence of nursing notes N = {N\u2081, N\u2082,..., Nm} sorted by time. Our objective is to obtain a summary S\u1d62 for note N\u1d62 from the distribu- tion P(S\u1d62 | N\u1d62, PA, {N\u2081, . . ., N\u1d62\u208b\u2081}, U), which is conditioned on the patient's metadata PA, information in the past notes {N\u2081,..., N\u1d62\u208b\u2081}, and the user query U guiding the generation.\n3.1. Base Model\nThe backbone of our framework is an off-the-shelf transformer-based language model with an encoder-decoder structure, denoted by M. Specifically, we leverage a checkpoint M\u02e2\u1d58\u1d50 of M as the base model, which has been fine-tuned for text summarization using publicly available datasets. This allows us to efficiently utilize the extensive resources offered by the pre-trained language model without the effort of training from scratch. Hence, M\u02e2\u1d58\u1d50 has been enriched with task-specific knowledge for improved performance in text summarization. However, the capability of M\u02e2\u1d58\u1d50 to understand clinical text still remains limited. Therefore, additional refinement of M\u02e2\u1d58\u1d50 is necessary to enhance its ability to grasp the complicated semantic information within nursing notes.\nLet N\u1d62 = [t\u2081, t\u2082,..., t\u2099], where t\u1d62, i = 1,...,n, denote tokens in N\u1d62. As a preliminary step, we first train the encoder ENC of the base model M\u02e2\u1d58\u1d50 by reconstructing N\u1d62:\n$\\L_{rec}(N_i, ENC, DEC^{rec}) = \\L_{Cross Entropy}(N_i, DEC^{rec}(ENC(N_i)).$ (1)\nHere, DEC\u02b3\u1d49\u1d9c is the decoder of the original pretrained model M, which remains frozen during the training. This process empowers the encoder with the ability to understand the semantic information and the clinical knowledge embedded in nursing notes, enabling it to encode the notes more effectively. This preparatory step as precedes the primary workflow for nursing notes summarization."}, {"title": "3.2. Training Objective", "content": "Since there is no ground truth summary available, the conventional method to guide the model M\u02e2\u1d58\u1d50 through supervised fine-tuning is not feasible. Instead, we adopt a self- supervised strategy to force the model to generate high-quality, patient-centered summaries that can respond to patient-related queries effectively. We introduce a model R, which serves as a query responder. This model has been trained to generate responses to specific queries concerning the patient. For example, if a query pertains to the patient's readmission status, R is trained to classify patients based on readmission risk using data available in the patient database.\nWhen giving the original nursing note N\u1d62 or its summary S\u1d62 generated by M\u02e2\u1d58\u1d50 as an input to the responder R, the training objective is to minimize the discrepancy between the two responses:\n$\\min \\L_{Cross Entropy}(R(N_i), R(S_i)).$ (2)\nThis formulation ensures that when responding to a certain patient-related query, using the summary will result in a response similar to that obtained using the original nursing note. To prevent M\u02e2\u1d58\u1d50 from generating summaries that are too verbose or direct \"copy- paste\" from the original notes, we introduce a length penalty term. Therefore, the final loss function for nursing notes within one batch becomes:\n$\\L_{summ} = \\frac{1}{K}\\sum_{i=1}^{K} \\L_{Cross Entropy}(R(N_i), R(S_i)) \\times (1 + \\lambda_1 e^{(\\alpha-0.5)}),$ (3)\nwhere\n$\\alpha = \\frac{\\frac{1}{K}\\sum_{r=1}^{K}Len(S_r)}{\\frac{1}{K}\\sum_{i=1}^{K}Len(N_i)}.$ (4)"}, {"title": "3.3. Augmentation Blocks for the Context of the Patient", "content": "Temporal Information Fusion (TIF). A patient typically has multiple nursing notes ordered in time to document the evolution of her condition. Therefore, the key information crucial for summarizing a patient's current status is influenced by the context provided by the prior notes. We regard this as temporal information which should be incorporated during summarization to help the model understand the progression of the patient's condition.\nFor N\u1d62, the embeddings of its previous notes are represented by the embeddings of their respective first tokens, which are special tokens indicating the start of each note. These embeddings are obtained at the last hidden state in the ENC, denoted as {h\u2081, h\u2082,..., h\u1d62\u208b\u2081}, where h\u1d62 \u2208 \u211d\u1d48 and d is the dimension of the hidden state. We aggregate the representations of the past notes by weighted mean pooling such that the most recent notes receive the largest weight. In practice, we determine initial weights \u03b2\u2c7c, j = 1, ..., i \u2212 1 for each past note N\u1d62 based on the position in the sequence, such that \u03b2\u2081 = 1, \u03b2\u2082 = 2, ..., \u03b2\u1d62\u208b\u2081 = i \u2212 1. The weighted mean pooling is performed using normalized weights:\n$\\beta'_j = \\frac{\\beta_j}{\\beta_1 + \\beta_2 + ... + \\beta_{i-1}},$ (5)\nh\u1d40\u1d35\u1da0 = MeanPooling(\u03b2\u2081h\u2081, \u03b2\u2082h\u2082,..., \u03b2\u1d62\u208b\u2081h\u1d62\u208b\u2081), (6)\nwhere h\u1d40\u1d35\u1da0 \u2208 \u211d\u1d48 represents the information fusion of the past notes. As shown in Figure 3, we prepend a special token [TI] at the beginning of the decoder input, representing temporal information with embedding h\u1d40\u1d35\u1da0. Consequently, the initial input to the decoder at the first time step consists of [[TI], [BOS]], where [BOS] is a special token indicating the start of generation. We substitute [TI] with the padding token [PAD] for nursing notes that have no past notes.\nThe model generates subsequent tokens in the summary in an auto-regressive manner. At each time step, the token produced is appended at the end of the decoder input for generation of subsequent tokens. Therefore, the [TI] token contributes to the generation of each token in the summary, serving as a prompt which consistently guides the model to focus on information about the patient's past condition.\nPatient Information Augmentation (PIA). We aim at obtaining summaries focusing on the patient's condition. To aid this, we explicitly incorporate patient-level information into the model through a cross-attention mechanism, which facilitates the interaction of information on different levels of representation learning. A patient's metadata PA typi- cally comprise basic information recorded for the patient's admission, including age, gender, existing diagnoses, and performed procedures. We convert this metadata into patient in- formation in natural language (one example presented in Appendix A.1), and then encode it using ENC to derive patient embedding h\u1d3e\u1d2c \u2208 \u211d\u1dbb\u02e3\u1d48 for patient PT, where z represents the number of tokens in patient information. The encoder also learns the embedding of the source note, h\u1d49\u207f\u1d9c \u2208 \u211d\u207f\u02e3\u1d48, where n denotes the number of tokens in the note given as input to the encoder. On the decoder DEC side, let us assume the tokens input to the decoder at the current timestep are [[TI], [BOS], y\u2081,..., y\u2c7c]. Consequently, the hidden represen- tation passed to the lth decoder layer is h\u1d48\u1d49\u1d9c \u2208 \u211d\u207d\u02b2\u207a\u00b2\u207e\u02e3\u1d48. The hidden representation h\u1d48\u1d49\u1d9c is processed and updated in each decoder layer using the conventional self-attention and cross-attention with h\u1d49\u207f\u1d9c. Furthermore, we augment the decoder layer with patient infor- mation by performing cross-attention also between the hidden representation h\u1d48\u1d49\u1d9c and the patient embedding h\u1d3e\u1d2c. This facilitates the fusion of patient- and note-level information:\n$H_{l+1}^{dec} = MHCA(H^{enc}, MHSA(H^{dec})) + \\lambda_2 \\times MHCA(H^{PA}, MHSA(H^{dec})),$ (7)\nwhere the MHCA and the MHSA respectively denote Multi-Head Cross-Attention and Multi-Head Self-Attention (Vaswani et al., 2017). \u03bb\u2082 \u2208 [0, 1] is a hyperparameter to control the importance of patient meta information. Hdecl+1 is the input to the next decoder layer, or if the lth layer is the final layer, it is the input to the language modeling head.\nWith these two augmentation blocks, the computation of the final decoder state h\u1d48\u1d49\u1d9c \u2208 \u211d\u207d\u02b2\u207a\u00b2\u207e\u02e3\u1d48 for generating the (j + 1)th token in the summary of the note N\u1d62 is abstracted as:\n[h\u1d49\u207f\u1d9c, h\u1d3e\u1d2c] = ENC(N\u1d62, PA), h\u1d48\u1d49\u1d9c = DEC(h\u1d49\u207f\u1d9c, h\u1d3e\u1d2c, [[TI], [BOS], y\u2081,..., y\u2c7c]), (8)"}, {"title": "4. Experiments", "content": "4.1. Data\nWe utilize MIMIC-III (Johnson et al., 2016), a widely used real-world EHRs database, for our experiments. In MIMIC-III, clinical notes in \u201cNOTEEVENTS\" table are organized by admission, and a single patient may have multiple admissions. Since the information in notes from different admissions of the same patient is discontinuous, we treat notes in each admission independently. We focus on nursing notes within the clinical notes. After the preprocessing, filtering and sampling (details in Appendix A.2), the number of nursing notes in the training, validation and test sets is 149015, 10001, 3079 and the corresponding numbers of admissions are 13893, 1000, 1156.\n4.2. Types of Queries\nThis section presents queries used in our experiments, and more details can be found in Appendix A.3. Two principles are followed when determining the queries: (1) The query should be closely related to the patient and learnable by the query responder R; (2) Data required to train R should be easily available without excessive data annotation. Below we propose four different types of queries. In each of these, the query responder R is a classification model, which classifies patients according to a specific aspect of a patient's status. Part of the training data is used to train the query responder R. When using R to guide the summarization, we input the summary and the original note to predict the classification probabilities, and minimize the discrepancy between these predictions, as described in Section 3.2. As an additional query, we include a simple baseline by minimizing the semantic distance between the note and its summary, measured by cosine similarity.\nContrastive Next Note Prediction. Given a nursing note pair (N\u1d62, N\u2c7c), we regard the query about whether N\u2c7c is the successor note of N\u1d62 as a prediction of the patient's future status. To train the query responder R for the next note prediction, we create two note pairs for each nursing note, where the positive pair comprises the note and its successor in the sequence, and the negative pair contains the note and a randomly chosen non-consecutive note. If N\u1d62 is the patient's last nursing note, we use the patient's discharge summary and a random note from other patients to construct the positive and negative pairs. The query is formulated as binary classification, and the output of R is the probability of each pair being the positive pair containing the consecutive notes.\nReadmission Prediction. Readmission information is easily retrieved from the hospi- tal's database and is closely related to the patient. The readmission prediction query is formulated as a 2-class classification task to predict whether the patient will be readmitted within 30 days of discharge, which reflects the patient's future condition.\n$\\nu = LMH(H^{dec}),$ (9)\n$\\nu' = ST-GumbelSoftmax(\\nu).$"}, {"title": "4.3. Experiment Settings", "content": "As the base model, we use BART-Large-CNN, which is a BART model (Lewis et al., 2020) fine-tuned on CNN Daily Mail, specialized in text summarization. As the query responder, we use Clinical-Longformer (Li et al., 2022), chosen for its ability to handle a long context. It is fine-tuned on the selected queries. Hyperparameter settings for QGSumm and the query responder are presented in Appendix A.4.\nSince our method is designed for scenarios where reference summaries are unavailable, we compare it with baselines in both zero-shot and few-shot settings: Zero-Shot: BART- Large-CNN (abbreviated as BART-zs), BioMistral-7B (Labrak et al., 2024) (abbreviated as BioMistral-zs), and GPT-4 (OpenAI, 2023); 10-Shot: BART-Large-CNN (abbreviated as BART-fs), Pegasus (Zhang et al., 2020a), and BioMistral-7B (abbreviated as BioMistral- fs). We use summaries generated by GPT-4 for the 10-shot fine-tuning and use one-shot in-context learning when prompting GPT-4 and BioMistral-7B. We also include results from two extractive methods, TextRank (Mihalcea and Tarau, 2004) and Lead-40%, for reference. In TextRank, we utilize MPNet (Song et al., 2020) to obtain sentence embeddings. In Lead- 40%, we use the first 40% of the content of the note as a summary. More details about baselines and in-context learning/few-shot adaptation can be found in Appendix B.1.\n4.4. Evaluation Metrics\nEvaluating the quality of text summarization is challenging (Bhandari et al., 2020), espe- cially when reference summaries are unavailable. Therefore, we employ multiple metrics covering different aspects of the summaries, providing a comprehensive evaluation.\nAutomatic Evaluation Metrics. Metrics in the automatic evaluation are divided into three categories: 1) predictiveness, 2) factuality and consistency, and 3) conciseness. Metrics for predictiveness assess whether the summary adequately contains patient key information, quantified as the ability to predict the patient's condition using the summary as input. Specifically, we conduct readmission prediction and phenotype classification using summaries from baselines and our method. We employ the summaries generated by different methods to fine-tune the query responder, resulting in multiple predictors, one for each method. For readmission prediction, we report the weighted F1 and F1 of the positive class (\"being readmitted\"), and for phenotype classification, we report the F1-Macro score. For consistency and factuality, we consider: (1) UMLS-Recall. This metric measures the biomedical information consistency by comparing the set of medical concepts in the summary with that in the original note. We employ QuickUMLS (Soldaini and Goharian, 2016) to extract Unified Medical Language System (UMLS) biomedical concepts from the nursing note and its summary. Recall is the proportion of concepts in the original note that are present in the summary. (2) UMLS-FDR. FDR denotes False Discovery Rate. Analogously to UMLS-Recall, we compute the proportion of medical concepts in the sum- mary that do not appear in the original note. (3) FactKB. With FactKB (Feng et al., 2023), an evaluation model measuring the factuality of a summary and its original text, we evaluate whether the summaries are consistent with the nursing notes from the perspective of their overall semantic information. (4) BARTScore. This metric evaluates the general consistency of summaries in a text-generation manner using BART, which also considers aspects such as the structure, coherence, and fluency of the summary (Yuan et al., 2021).\nFinally, we report the length of the generated summary as a percentage of the original note's length to assess conciseness. We do not enforce a strict maximum length for baselines because we believe the model should be capable of determining the appropriate length autonomously.\nMetrics used in the manual evaluation by a clinician Without a reference summary, automatic evaluation metrics may not fully capture the quality of the summary. Therefore, we invite a clinician to conduct manual evaluation of the summaries of 25 nursing notes. The clinician evaluates summaries from multiple methods in a blinded and randomized or- der. Each summary is rated on four aspects: (1) Informativeness: Whether the summary adequately captures essential information regarding the patient's condition in the original note; (2) Fluency: Whether the summary is well-written and easy to understand. (3) Consistency: How well the summary aligns with the original nursing note in factuality. (4) Relevance: It evaluates the conciseness of the summary and whether it contains un- necessary information. The score for each aspect ranges from 1 to 5. More detailed grading criteria are presented in Appendix B.2.\""}, {"title": "5. Results and Discussion", "content": "5.1. Results of the Automatic Evaluation\nPredictiveness. Results are shown in Table 1. In the readmission prediction task, GPT- 4 performs best, producing summaries that enable more accurate prediction of a patient's status. Our method also exhibits excellent performance, surpassing all few-shot methods. The main reference for our method is BART-zs, as it is the base model in our method, and hence represents performance without the proposed novel components. We see that our method outperforms BART-zs significantly in weighted F1 score (84.2 vs. 78.8) and F1 score of the positive class (18.2 vs. 11.1). This shows the effectiveness of the adaptation strategy guiding the model with useful queries. Interestingly, we find that using the summary from GPT-4 for this task outperforms using the original notes. Similarly, the summary from our method has performance close to that of using the original notes, highlighting the importance of high-quality summaries. In phenotype classification, our method with the query focusing on patients' phenotype performs the best, outperforming BART-zs in Macro F1 (25.6 vs. 20.5). Even when using the similarity alone as a guiding signal, our method still is better than BART-zs (22.4 vs. 20.5) or BART-fs (22.4 vs. 21.1). Although specialized in text summarization, Pegasus has weak performance on all predictiveness metrics.\nConciseness, Consistency, and Factuality. As shown in Table 1, there is an expected trade-off between UMLS-Recall and the summary's length. Our method strikes a good bal- ance between medical information consistency (measured by UMLS-Recall) and conciseness. GPT-4 captures more medical information, but achieves this with summaries which are less concise. Conversely, BART-zs can produce concise summaries but fails to adequately cap- ture medical concepts. Even if the 10-shot fine-tuning clearly improves predictiveness and UMLS-Recall, BART-fs still struggles to generate a concise summary. Similar to BART-fs, both BioMistral-zs and BioMistral-fs tend to produce summaries that are not concise.\nSummaries generated from BART-zs maintain high levels in factuality (measured by UMLS-HR and FactKB) and general consistency (measured by BARTScore). Our method also has strong performance on relevant metrics. We find that although LLMs, such as GPT- 4 and BioMistral, excel in language understanding, they do not perform well on factuality and general consistency. One possible reason is their tendency to rephrase or even expand upon the original notes, potentially introducing inconsistent information. However, the metrics can be influenced by text style and layout, which may cause summaries that are more different from the original note to score relatively lower, even if they are more fluent. For this reason, our model also scores lower than the base model BART-zs on some metrics. The results and limitations will be further discussed in Sections 5.4 and 5.5.\nEffectiveness of the Query Guidance. According to the results shown in Table 1, the performance with different queries varies. We can observe: (1) Regarding predictive- ness, employing queries closely related to patients and focusing on readmission and phe- notype information yield superior performance compared to other queries. As expected, the method with phenotype-related queries performs the best in phenotype classification, while the method with readmission-related queries is the best in readmission prediction. This highlights the effectiveness of guiding the summarization with queries, and different queries enable the summary to concentrate on different aspects of the original note. (2) Using similarity as guidance can produce summaries that are more similar to the original notes, resulting in higher scores on general consistency and factuality. However, summaries generated under this configuration tend to be longer and often sacrifice predictiveness and informativeness regarding medical concepts, demonstrating the limitations of the uncon- strained guidance signal. (3) When employing joint guidance with both readmission and phenotype information, our method consistently achieves excellent performance across all metrics. This indicates that combining different guidance signals can help in producing better summaries, and further research is needed to explore this aspect in depth."}, {"title": "5.2. Results of the Manual Evaluation by a Clinician", "content": "To avoid excessive manual work, we select three baselines to include in the manual evalu- ation: BART-zs, GPT-4, and BioMistral-fs. The justification for selecting these methods is two-fold: First, BART-zs is the base model in our method and hence the main baseline, demonstrating the benefits of the novel components. Second, GPT-4 and BioMistral-fs are well-known strong baselines and they had good performance in the automatic evalua- tion. We use \u201cReadmission Prediction and Phenotype Classification\" as the query for our method. Average scores for each method across four metrics are shown in Figure 4.\nQGSumm vs BART-zs. Our method significantly outperforms the base model BART-zs on all four metrics. This indicates that the proposed domain adaptation strategy enables the model to generate higher-quality summaries from the medical personnel's perspective, con- taining refined and important patient information with fewer hallucinations and increased readability. Although on average the summaries generated by our method are longer than those produced by the base model, our method achieves a higher relevance score from the clinician, suggesting the base model struggles to identify key information and focuses on unnecessary details. Our model can effectively enhance this aspect.\nQGSumm vs GPT-4 and BioMistral-fs. Due to the LLMs' strong language under- standing capability and sufficient medical knowledge, GPT-4 and BioMistral-fs can ad- equately summarize key information in nursing notes, receiving approximately the same average score in Informativeness as our method. Additionally, they excel in generating fluent summaries by rephrasing and clarifying abbreviations, receiving a slightly higher Fluency from the clinician than our method. However, the rephrasing can introduce fac- tual inconsistencies, and the tendency to infer additional content may reduce factuality. Consequently, our model has a higher average score in Consistency, which is essential in the clinical setting. Furthermore, it generates more concise summaries, leading to a higher average score in Relevance. However, due to the small sample size, the only statistically significant difference in these comparisons was the improvement of our method compared to Biomistral-fs in consistency, and further work is needed for conclusive results."}, {"title": "5.3. Effectiveness of Augmentation Blocks", "content": "We analyze the effects of the proposed augmentation blocks through an ablation study. We consider three settings: removing the Patient Information Augmentation block (denoted as w/o PIA); removing the Temporal Information Fusion block (denoted as w/o TIF); removing both blocks (denoted as w/o PIA+TIF). We use \u201cReadmission Prediction and Phenotype Classification\" as the query in our method. The results of the ablation study are shown in Table 2 The decreased weighted F1 and macro F1 scores indicate that both augmentation blocks enhance the predictiveness of summaries. This implies that information in patient metadata and previous notes can effectively prompt the inference of the current and future status of patients. Removing the TIF block causes a larger decrease in the F1 scores, suggesting that temporal information is more important than the patient's metadata in guiding the generation of summaries to focus on the progression of the patient's status.\nOn the other hand, the incorporation of patient metadata can lead to more faithful sum- maries, as the removal of PIA degrades more the performance on UMLS-FDR and FactKB, which are related to factuality.In contrast, the TIF block does not have a significant impact on the factuality. However, according to the UMLS-Recall score, it encourages the model to capture more medical information, thereby improving the consistency of the summary."}, {"title": "5.4. Case Study", "content": "One artificial nursing note and its corresponding summaries generated by QGSumm, BART- zs, GPT-4, and BioMistral-fs are presented in Figure 5. In the original nursing note, the content highlighted in blue indicates information included in the summary generated by our approach. We can see that our approach captures most of the important patient information. However, some details, such as cardiovascular and respiratory conditions, are overlooked.\nThe summary from BART-zs only covers information from the first half of the nursing note, suggesting the limitation in understanding long context. Summaries from GPT-4 and BioMistral-fs contain more patient information but lack conciseness. These models achieve fluency by rephrasing notes and expanding abbreviations. However, BioMistral-"}, {"title": "5.5. Discussion", "content": "User need -oriented summarization. A high-quality summary should facilitate effi- cient understanding of the relevant content for users. In the context of nursing notes, this means the summary should capture the patient's condition. Our method employs patient- related queries, indirectly ensuring that the summary centers around the patient's status.\nThe summaries generated with different queries can be seen as coming from distinct con- ditional distributions and parts of the semantic space. As discussed in Section 5.1, the queries can guide summaries to focus on specific aspects of the original note. Therefore, by selecting appropriate queries, we can control preferences for desired content and adjust granularity, which facilitates a more flexible and user need -oriented summary generation. For instance, broad queries about the patient's condition will result in a summary that fo- cuses on the patient's overall condition, while more detailed queries, such as those regarding cardiovascular health, could produce a summary that focuses on that specific aspect.\nDesign choices for information augmentation. One challenge is how to efficiently integrate information into the model while avoiding excessive computational cost. We utilize cross-attention to allow the patient's metadata to efficiently interact on multiple levels with the process of generating the summary. In contrast, for temporal information in previous notes, using cross-attention in a similar manner might make it difficult for the model to balance attention across the current note, past notes, and patient information, in addition to introducing computational challenges with long sequences of notes. Hence, we adopt a simple but effective strategy: representing the temporal information, obtained by weighted mean pooling from previous note representations, as the first token of the decoder's input. This strategy is intuitive, as information from previous notes naturally precedes the summary of the current note.\nInterpretation of the evaluation metrics. The metrics used in the automatic evalu- ation have limitations as they do not conclusively reflect the quality of the summary, and come with trade-offs. For example, a good performance in predictiveness and medical in- formation consistency (UMLS-Recall) may not be due to the high quality of the summary but rather caused by copying the source note, resulting in a lack of conciseness and fluency. Conversely, as the summary becomes more concise, it may become less informative. Fur- thermore, models used to measure factuality and general consistency have inherent biases. As they are based on general semantics, they are potentially weak at recognizing patient- related information due to the dissimilarity between their training domain and clinical data, and they often prioritize text style and structure. Finally, since BARTScore is derived from the BART model, summaries generated by BART have a bias of scoring relatively higher with this metric. We attempt to mitigate the impact of these limitations by comprehen- sively considering multiple metrics, and including the manual evaluation by a clinician, but there remains a need for more conclusive evaluation metrics.\nLimitations. (1) Our current approach produces summaries of individual nursing notes, lacking the long context and support for multiple note summarization. (2) There is room for more exploration on the formulation of the clinical queries. We don't employ generative queries but only queries related to the classification of the patient's status. Also, when investigating the combined effects of multiple queries, further exploration using multi-task learning methods could be beneficial. (3) Due to the workload, the number of notes assessed in the manual evaluation is limited to 25. A larger sample size would allow more conclusive comparisons of the strengths and weaknesses of the methods."}, {"title": "Conclusion", "content": "We presented a novel method for self-supervised nursing note summariza- tion, where the main innovation was the introduction of query guidance, which successfully directed the summaries to include desired content. In the manual evaluation by a profes- sional clinician, our method significantly outperformed a specialized open text summariza- tion model, BART-Large-CNN, in all metrics. Because this model was the base model of our method, the result highlights the usefulness of the novel developments. Of the other baselines, the proprietary GPT-4 had the closest performance to our method and was better than the other baselines. In the automatic evaluation, GPT-4 was better than our method in predictiveness, but, importantly, our method outperformed GPT-4 in factual consistency, having fewer hallucinated facts without sacrificing the correct content. The same trend was seen in the manual evaluation as higher average consistency for our method, although the difference was not statistically significant. Hence, our approach can produce more reli- able summaries, clearing obstacles for responsible clinical use of LLMs. From the machine learning perspective, our method demonstrates the feasibility of domain adaptation for pre-trained text summarization models without explicit supervision, and the effectiveness of self-supervised strategies to guide conditional summarization to specific interests."}]}