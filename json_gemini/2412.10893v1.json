{"title": "BgGPT 1.0:\nExtending English-centric LLMs to other languages", "authors": ["Anton Alexandrov", "Veselin Raychev", "Dimitar I. Dimitrov", "Ce Zhang", "Martin Vechev", "Kristina Toutanova"], "abstract": "We present BGGPT-GEMMA-2-27B-\nINSTRUCT and BGGPT-GEMMA-2-9B-\nINSTRUCT: continually pretrained and\nfined-tuned versions of Google's Gemma-2\nmodels, specifically optimized for Bulgarian\nlanguage understanding and generation. Lever-\naging Gemma-2's multilingual capabilities\nand over 100 billion tokens of Bulgarian and\nEnglish text data, our models demonstrate\nstrong performance in Bulgarian language\ntasks, setting a new standard for language-\nspecific AI models. Our approach maintains\nthe robust capabilities of the original Gemma-2\nmodels, ensuring that the English language\nperformance remains intact. To preserve\nthe base model capabilities, we incorporate\ncontinual learning strategies based on recent\nBranch-and-Merge techniques as well as\nthorough curation and selection of training\ndata. We provide detailed insights into our\nmethodology, including the release of model\nweights with a commercially-friendly license,\nenabling broader adoption by researchers,\ncompanies, and hobbyists. Further, we\nestablish a comprehensive set of benchmarks\nbased on non-public educational data sources\nto evaluate models on Bulgarian language\ntasks as well as safety and chat capabilities.\nOur findings demonstrate the effectiveness of\nfining state-of-the-art models like Gemma\n2 to enhance language-specific AI applications\nwhile maintaining cross-lingual capabilities.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown re-\nmarkable capabilities, particularly in the English\nlanguage, due to the abundance of digitalized En-\nglish text. Moreover, with recent developments\n(Grattafiori et al., 2024; Gemma-Team et al., 2024;\nTeam, 2024; Yang et al., 2024) we are seeing that\nthe open-source/weight community has been clos-\ning the gap compared to closed, black box plat-\nforms in most tasks. A large portion of LLM users,\nhowever, are not native English speakers or re-\nquire text generation in low-resource languages,\nand English-centric models do not always meet\ntheir needs. Some such models do exhibit strong\nmulti-lingual understanding, stemming from their\nlarge, diverse pretraining data, especially in more\npopular European languages, but often do not meet\nthe required standard for text generation nowadays.\nThis work presents the development process\nand evaluation of our language models, spe-\ncialized for Bulgarian and English use, named\nBGGPT-GEMMA-2, based on the Gemma-2 ar-\nchitecture (Gemma-Team et al., 2024). This is the\nthird iteration of Bulgarian models developed by\nINSAIT, designed to be powerful yet accessible\non common consumer hardware."}, {"title": "Advancing Bulgarian while retaining base ca-\npabilities", "content": "The BGGPT-GEMMA-2 models are\ntrained through continued pretraining of strong\nbase models not specialized in Bulgarian. Our\ntraining data also includes English data but has\nan emphasis on Bulgarian local knowledge and\nlanguage understanding. Since the base models\ndemonstrate strong performance in English, cod-\ning, mathematics, and other domains, we have ap-\nplied Branch-and-Merge (Alexandrov et al., 2024),\nwhich significantly reduces catastrophic forgetting\nfrom the base model while minimally impacting the\nlearning of Bulgarian capabilities. Our training pro-\ncedure also incorporates techniques such as curricu-\nlum learning and experience replay (Scialom et al.,\n2022; Zhang et al., 2023; Ibrahim et al., 2024)."}, {"title": "Challenges and goals", "content": "Previous releases of open\nmodels such as Llama, Llama-2, Mistral 7B and\neven GPT-2 (Touvron et al., 2023a,b; Jiang et al.,\n2023; Radford et al., 2019) have led to numer-\nous specialized models targeting non-English lan-\nguages (Pl\u00fcster, 2023; Nikolich et al., 2024; Vo"}, {"title": "2 Self-supervised pretraining", "content": "Our models are based on continued pretraining of\nthe Gemma-2 base model with the Branch-and-\nMerge (Alexandrov et al., 2024) strategy. As this is\na technique to mitigate forgetting and erroneous di-\nvergence from the base model, we are tracking the\nscores, particularly for benchmarks where Gemma-\n2 is already strong \u2013 such as math and reasoning.\nWe split our pretraining dataset into 8 parts, named\nG1 to G8 for short. The odd parts - G1, G3, G5 and\nG7 contain predominantly Bulgarian text, while\nthe even parts - G2, G4, G6 and G8 contain slightly\nmore English in addition to Bulgarian with the\ngoal of maintaining and possibly improving En-\nglish skills. This procedure, described in greater\ndetail in Alexandrov et al. (2024), is very similar\nto what we have already applied for our previous\nBgGPT models.\nIn our training, we use PyTorch (Paszke\net al., 2019) and HuggingFace's transformers li-\nbrary (Wolf et al., 2019) combined with DeepSpeed\nZeRO to parallelize our training runs (Rasley et al.,\n2020; Rajbhandari et al., 2020). All training is per-\nformed on 64 NVIDIA H100s divided into 8 nodes\nof 8 GPUs with Infiniband interconnection. We set\n$10^{-5}$ as the maximum learning rate for the contin-\nued pretraining of all the models combined with a\nbatch size of 512 for continued pretraining and 256\nor 512 for supervised fine-tuning. We use cosine de-\ncay to 0.1-max_lr with max(100, 0.01\u00b7total_steps)\nlinear warmup."}, {"title": "2.1\nPretraining data", "content": "The training data for continued pretaining is com-\npletely identical to the one utilized in Alexandrov\net al. (2024), combining approximate experience\nreplay and high-quality Bulgarian data from vari-\nous sources. To adapt the RedPajama-V2 (Weber\net al., 2024) annotation pipeline we performed a\nfew adjustments due to the use of the Cyrillic alpha-\nbet and the lack of a Bulgarian equivalent of certain\nresources. Any natural-language signal that looks\nat characters has to be adjusted to include Cyril-\nlic and some additional punctuation like a specific\ntype of quotation marks appearing in digitalized\nBulgarian text. The original pipeline includes a\ncount per document of words that are contained\nin the LDNOOBW blocklist, which we replace\nwith our own list of \"bad words\" in Bulgarian.\nRedPajama-V2 also considers ML-based heuristics\nfrom fastText (Bojanowski et al., 2017) classifiers"}, {"title": "2.2 Training procedure", "content": "We summarize the training steps in Figure 1, along-\nside their evaluation performance in Table 2. We\nbegin by continually pretraining the base Gemma-\n2-27b model on the G1 and G2 datasets separately.\nTraining on both datasets results in an overall in-\ncrease in Bulgarian skills and a reduction in En-\nglish skills. By merging the G1 and G2 models,\nwe obtain a model denoted G1 G2 that recovers"}, {"title": "3 Supervised fine-tuning", "content": "3.1\nInstruction fine-tuning data\nFor instruction fine-tuning, we collect a set of trans-\nlated and native Bulgarian conversations and in-\nstructions that are summarized in Table 3.\nTranslated datasets Some datasets were trans-\nlated from English using the Google Translate API.\nWe also manually fixed translations that we de-\ntected as likely incorrect using heuristics. Our\nheuristics search for inconsistencies in the English\nand Bulgarian translations as the alphabets are dif-\nferent, e.g. if a word is kept in English in one sen-\ntence and translated or transliterated in the next one,\nwe flag the example for manual translation. Over-\nall, as the cost of translation is high, the amount\nof Bulgarian instruction data is more limited than\nnative English data.\nChat conversations We also leverage the chat\napplication at https://bggpt.ai/ and collect a\nset of conversations generated by current users. We\ngather conversations from consenting users of our\nchat application to construct an IFT dataset. As part"}, {"title": "Toxic questions", "content": "We assemble a set of unsafe,\ntoxic, and inappropriate questions to create a de-\nnial QA dataset. In particular, we first translate the\nfiltered chat logs using INSAIT's previous model,\nBgGPT-Instruct-v0.2, into English. This allows us\nto use existing open-weight LLMs, to identify toxic\nand inappropriate questions regardless of the lan-\nguage (mostly Bulgarian and English). We employ\nMistral-7B-v0.2 for identification, as empirically\nit worked well to detect toxic content due to its\nalignment training. We then apply exact and near\ndeduplication of the resulting unsafe queries based\non embedding similarity. We give this deduplicated\nset of queries to our annotators to divide into differ-\nent topics, further filter for good-quality examples,\nand finally supply denial answers to them. This\ngives us a total of 550 question-answer pairs to\ninclude in our IFT dataset."}, {"title": "Human preference dataset", "content": "In our endeavors to\napply Direct Preference Optimization (Rafailov\net al., 2023) for Human Alignment, before the\nlaunch of the first chat application, we let more than\n70 people create all kinds of complex queries and\ngathered a total of 1500 prompts, which we then\ngenerated answers for with BgGPT-Instruct-v0.2,\na few different versions of Bulgarian models we\ntrained and other models. Our annotators then ex-\npressed their preference for one of the possible gen-\nerations if there was an appropriate one or edited\nthe one that was closest to being acceptable and set\nthat as the preferred answer. As this process is time-\nconsuming and the improvement gained from DPO\non such a small amount of data was insignificant,"}, {"title": "Synthetic poetry questions", "content": "Rhyming and po-\netry are very difficult skills for a language model\nto obtain because they combine creative writing\nwith generation constraints that are based on sub-\nwords and potential pronunciations, which is some-\nthing tokenizer-based LMs notoriously have a hard\ntime with. We found that our first models of the\nBgGPT series (BgGPT-Instruct-v0.2), which were\nfine-tuned mostly on translated instructions, really\nstruggled to make any rhymes at all, often actually\ntrying to rhyme words that would rhyme in English\nbut when translated into Bulgarian would not. To\ntackle this we used a rhyme dictionary and anno-\ntated thousands of songs songs and poems. We\nthen only took rhyming line cuts or stanzas to cre-\nate questions and answers such as \"Complete the\ngiven stanza and retain the rhyming form {stanza\nlines (0, k\u2212j)} => {stanza lines (k\u2212j, k)}\", \"Give\n{k} words that rhyme with the word {word} =>\n{extracted rhyming words}\", \"Which of the fol-\nlowing word pairs rhyme?\" and others. We found\nthat including too many of these examples in the\nIFT stage can lead to overfitting but adding even a\nsmall amount helps the model improve in creating\nrhyming songs and poems."}, {"title": "3.2 Instruction fine-tuning process", "content": "For our final chat-oriented model we simply fine-\ntune for 2 epochs on the dataset composition\ndescribed in 3. However, this is not the best-\nperforming model in terms of benchmark scores,\nbut the one we find to match our preference, as\nwell as the model with the highest preference of"}, {"title": "4 Benchmark Evaluation", "content": "For our evaluations, we used and now elaborate\non the benchmarks applied in LM Evaluation Har-\nness (Gao et al., 2023). We translated them to Bul-\ngarian using Google Translate API and then applied\nheuristics to identify samples that may be translated\nincorrectly. Then, we asked professional translators\nto review and correct these translations. Addition-\nally, we utilize 2 benchmarks that were originally\ncreated in Bulgarian, namely EXAMS (Hardalov\net al., 2020) and a novel benchmark created from\nofficial exam questions from the Bulgarian Min-\nistry of Education (described more thoroughly in\nSection 5).\nWinogrande challenge (Sakaguchi et al., 2021)\nis a commonsense reasoning benchmark measuring\nan LLM's ability to fill in a blank from a choice of\ntwo entities to logically complete a sentence. The\nset of tasks in reference are the 1767 samples from\nthe validation set of winogrande_xl. As part of\nour translation effort, some samples were rephrased\nas the word-to-word Bulgarian version could reveal\nthe answer through the gender of the words from\nthe rest of the sentence. We evaluate the models on\nthis benchmark in a 0-shot manner.\nHellaSwag (Zellers et al., 2019) is a common-\nsense reasoning benchmark evaluating how well an\nLLM can select a logical continuation of a sentence."}, {"title": "ARC-Easy and -Challenge", "content": "ARC-Easy and -Challenge (Clark et al., 2018)\nis a dataset of science exam questions. We evaluate\non the 2590 hard samples (ARC-Challenge) and\n5197 easy samples (ARC-Easy) in a 0-shot manner."}, {"title": "MMLU", "content": "MMLU (Hendrycks et al., 2021) is a multitask\nlanguage understanding benchmark covering a\nwide range of 57 different tasks. Evaluated on\n14079 test set samples in a 5-shot manner."}, {"title": "MathQA", "content": "MathQA (Amini et al., 2019) is a multiple\nchoice mathematical reasoning benchmark. Evalu-\nated on the 4475 validation set samples."}, {"title": "GSM8K", "content": "GSM8K (Cobbe et al., 2021) is a mathematical\nreasoning benchmark consisting of grade-school\nmath questions for which free-text answers must\nbe provided. Evaluated on 1.3k test set samples.\nWe run GSM8k with a 5-shot chain-of-thought gen-\neration."}, {"title": "TriviaQA", "content": "TriviaQA (Joshi et al., 2017) is a dataset consist-\ning of trivia questions. We evaluate 17.9k valida-\ntion set samples in a 5-shot manner."}, {"title": "EXAMS", "content": "EXAMS (Hardalov et al., 2020) is a high school\nexam question dataset covering a range of subjects.\nEvaluated on 1472 test set samples in Bulgarian.\nWe use 5-shot prompting."}, {"title": "Additional Benchmarks", "content": "In our evaluation, we\nadditionally used the following benchmarks but\nomitted them from this report. While being of rela-\ntively high quality, we found that the Belebele (Ban-\ndarkar et al., 2023) benchmark was very simple,\nwith little to no difference in the performance be-\ntween models, except the distinction between the\nscores of IFT-d models and non-IFT-d ones. We\nalso made use of XNLI (Conneau et al., 2018) for\nBulgarian, as provided in the LM Evaluation Har-\nness, however, we found that the examples in Bul-\ngarian were very confusing and unnaturally worded.\nThe quality may have also been reflected in the\nscores we observed, which seemed very random,\nwith no sign of what drove the numbers higher or\nlower. While models that have seen significant Bul-\ngarian training performed better than their English-\ncentric bases, we found that all of them scored\naround 50%, unrelated to how much or what type\nof Bulgarian training they have received."}, {"title": "5 Instruction-following Evaluation", "content": "In this section, we evaluate the utility of the model\nin chat between a user and an AI assistant users are\nexpected to interact with via its API endpoint or\nchat application. For its chat interactions, BGGPT-\nGEMMA-2 follows the same chat template as the\ninstruction-tuned Gemma-2 models.\nMany of the English and Multilingual bench-\nmarks in such a setting use human judges (Arena,\n2024) or model judges (Zheng et al., 2024). How-\never, we find it challenging and costly to simply\ntranslate these benchmarks to a new language like\nBulgarian and maintain their quality. Instead, we\npropose a set of new benchmarks based on local\nBulgarian data that we collected from a variety of\nsources."}, {"title": "5.1 MON benchmark", "content": "MON is a dataset collected by the Ministry of Ed-\nucation and Science of Bulgaria 2024, which con-\ntains a set of multiple choice questions for school\nexams between 4th and 12th grade. These exams\nare designed as a standardized way to assess the\nknowledge of students across Bulgaria on a range\nof topics. These questions are in Bulgarian and\ncover subjects like literature, mathematics, physics,\nhistory, computer science, and all other subjects\nin the standard school curriculum in Bulgaria. We\ninclude only text-based questions, without images\nor other media, with exactly one correct answer out\nof 4 and as a result, obtain 10, 088 questions with\n4 possible answers. The tests were carefully de-\nsigned and taken by thousands of students; we thus\nbelieve that the dataset is of relatively high quality.\nAt this stage, we do not intend to release the dataset\npublicly, to avoid potential future contamination of\nmodels.\nTo evaluate the quality of the models in a chat set-"}, {"title": "5.2 Chat Preference with a Judge Model", "content": "We have found that even though GPT4o may not be\nperfect for Bulgarian text generation, it can serve as\na great LLM-as-a-judge to give preference between\ntwo outputs from user prompts. Taking advantage\nof the already existing bggpt.ai chat application\nwe collect a dataset of chat interactions from our\nconsenting users to create a set of 3000 prompts\nto use the preference judge on. We apply various\nquality filters to exclude erroneous examples like\nsingle-word prompts and other nonsensical input.\nWe deduplicate the examples based on embedding\nsimilarity and take action to remove inputs includ-\ning sensitive or personally identifiable information.\nAs we aim to evaluate our models on a diverse set\nof representative user queries, we classify the ex-\namples into 82 different topic categories and soft\nsample our 3000 queries to include all topics while\nmaintaining the diversity distribution. It is impor-\ntant to note that these are sampled out of a separate\nchunk of conversations and not from those used\nwithin the IFT dataset. We can see a distribution of\nthe supercategories of these prompts in Figure 2\nand a fine-grained 82-topic distribution in Figure\n4.\nExperimenting with multiple prompts for the\njudge in both Bulgarian and English, we found\nthat GPT40's better abilities to reason in English"}, {"title": "6 Related work", "content": "Our major goal is to create language models that\nare capable of understanding and generating both\nBulgarian and English, which is what we believe\nwould be most useful to propagate the advances of\nNLP in Bulgarian locality. The techniques used to\ncreate strong duolingual language models can be\ndivided into 2 distinct types - ones that construct a\nmodel from scratch and those that build on top of\nexisting language models. The focus of our work\nis on the latter approach, leveraging the already\ninvested resources of the increasingly competitive\nopen-source and open-weight LLM research. It\nhas been shown that transfer learning from one do-\nmain to another can not only reduce the costs of\nmodel development but also bring improved perfor-\nmance on the target domain (Zhuang et al., 2020;\nAnonymous, 2024b). More specifically, in the do-\nmain of language modeling, there is evidence of\ncross-lingual relevance, as models can utilize their\ncapabilities through different languages, particu-\nlarly for low-resource languages (Conneau et al.,\n2020; Wang et al., 2024; He et al., 2024). However,\nthis is not without caveats, such as the curse of\nmultilinguality (Conneau et al., 2020) and catas-\ntrophic forgetting (French, 1999; Goodfellow et al.,\n2014; Gogoulou et al., 2023; Shi et al., 2024). \u03a4\u03bf\novercome these challenges we only work with 2\nlanguages and employ different strategies to allevi-\nate catastrophic forgetting in language transfer."}, {"title": "Language transfer", "content": "There is an abundance of\nworks that show very promising results of language\ntransfer from an English-centric base model to a\ntarget language of lower resource (Pl\u00fcster, 2023;\nNguyen et al., 2023; Csaki et al., 2023; Choud-\nhury et al., 2024; Toraman, 2024; Joshi et al., 2024;\nVo et al., 2024; Luukkonen et al., 2023) through\ndifferent strategies. Most examples focus on im-\nproving on relatively weaker baselines, such as\nLlama-2 (Touvron et al., 2023b), or vastly strug-\ngle with retaining English capabilities. Recent\nmodel releases such as the Gemma-2 model fam-\nily (Gemma-Team et al., 2024) demonstrate very\nstrong multilingual performance even on lower re-\nsource languages such as Bulgarian, which sets a\nhigher new standard for bilingual model improve-\nment.\nSome works rely on extending or altering the\narchitecture of the underlying transformer model\nthrough added model weights, adapters, swapped\nlayers or vocabulary extension (Choudhury et al.,\n2024; Gurgurov et al., 2024; Anonymous, 2024a;\nNguyen et al., 2023), which is something we avoid\ndue to the increased complexity of deployment and\nuse for the community. It is made obvious that tar-\ngeting a single language with a monolingual corpus\nreduces the original English proficiency and that\nexperience/knowledge replay can alleviate it to a\ngreat extent (Zhao et al., 2024; Ibrahim et al., 2024;\nCsaki et al., 2023), however, due to the unavailable\npretraining data we use approximate knowledge\nreplay."}, {"title": "7 Conclusion", "content": "We presented our entire BGGPT-GEMMA-2\npipeline for training strong duolingual language\nmodels, illustrating the steps for the Bulgarian lan-\nguage. The resulting pipeline has been used to\nbuild the best open Bulgarian language model to\ndate.\nOur findings emphasize how to leverage\nstrong English-centric open-weight models through\ndomain-specific adaptation strategies. Specifically,\nour BGGPT-GEMMA-2-27B-INSTRUCT model\nexhibits state-of-the-art performance on Bulgarian\nbenchmarks, outperforming larger, multilingually\ncapable models like Qwen-2.5-72B and Llama-3.1-\n70B on crucial metrics.\nWe highlight the challenges involved in achiev-\ning high-quality generation in the target language\nof interest (e.g., Bulgarian) while maintaining En-\nglish proficiency and the already learned capabili-\nties. This work provides detailed insights into the\nnecessary data construction and curation required\nto train such models, as well as the continual pre-\ntraining and fine-tuning strategies, including lever-\naging the latest research on model merging (i.e.,\nour Branch-and-Merge method), that effectively\npreserve and enhance the bilingual capabilities of\nthe models.\nWe demonstrate the performance of the BGGPT-\nGEMMA-2 models on educational tasks, real-world\nchat applications in addition to a multitude of stan-\ndard benchmarks, translated and naturally gener-\nated. While exhibiting strong performance in all of\nour Bulgarian evaluations, they preserve or even im-"}, {"title": "8 Limitations", "content": "While we show that our methods are efficient in\ncreating duolingual models, we rely on the exist-\ning multilingual capabilities of the base models.\nExtending open-weight models like Gemma-2 and\nLlama-3 to German and Bulgarian is reasonable\nbut such success cannot be guaranteed for signifi-\ncantly lower-resource languages and may require\nfurther cross-lingual advancements to compensate\nfor the lack of training data.\nIn our fine-tuning efforts we find that there is a\nlack of high-quality data and while we have put a\ngreat effort in filling that gap, it is far from enough\nand more work is needed to improve the various\naspects of this training stage.\nThis work describes the development of large\nlanguage model, which is one of the components\nfor building chat assistants, smart applications and\ntools that may be applied in scenarios of various\ncriticality. It is important to note that generative\nlanguage models based on decoder transformers\nmay not be reliable and should be thoroughly tested\nand safety bounded subject to the specific use case."}, {"title": "9 Ethical Considerations", "content": "We believe our work empowers the general pub-\nlic that uses Bulgarian to benefit from the power\nof large language models. However, such mod-\nels can of course also be abused and in particular\nif our approach generalizes beyond language to\ngeneral domain adaptation, malicious practitioners"}]}