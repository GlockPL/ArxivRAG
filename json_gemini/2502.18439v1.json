{"title": "MAPORL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning", "authors": ["Chanwoo Park", "Asuman Ozdaglar", "Seungju Han", "Kaiqing Zhang", "Xingzhi Guo", "Joo-Kyung Kim"], "abstract": "Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs' performance as shown recently. In this paper, we introduce a new post-training paradigm MAPORL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPORL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPORL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multi-agent RL. Unlike existing LLM post-training paradigms, MAPORL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.", "sections": [{"title": "1 Introduction", "content": "Recent advances in large language models (LLMs) have highlighted their potential for collaboration, particularly within the multi-agentic framework (Du et al., 2024; Li et al., 2023; Kim et al., 2024b). The shift from single-agent to multi-agent systems introduces new dimensions and challenges in enabling effective collaboration among LLM agents. Recent approaches to multi-LLM collaboration mostly rely on prompting pre-trained models. However, such approaches struggle with achieving genuine collaboration among the agents. For example, multi-agent debate does not consistently lead to improved performance with additional turns (Huang et al., 2024). This limitation may be somewhat expected \u2014 while LLMs are able to simulate collaboration procedures, they were not explicitly trained to achieve effective cooperation. In theory, it is not hard to imagine that single-agent training is insufficient for collaboration an untrained and non-strategic opponent can fail to act in a way that promotes collaboration. Instead, achieving collaborative behaviors requires interactive training environments where each agent actively engages with others, and dynamically optimizes the strategy (Gagne, 1974; Macy, 1991; Hertz-Lazarowitz et al., 2013). Moreover, conventional approaches such as supervised fine-tuning (SFT), as we will show, are inadequate for this purpose, either: merely mimicking multi-agent interactions from training data may not lead to effective collaboration. To develop more effective collaborative agents, we propose Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning (MAPORL), a co-training paradigm for multiple LLMs using multi-agent reinforcement learning (MARL). In MAPORL, within the pre-defined frameworks for multi-agent collaboration (e.g., the debate framework (Du et al., 2024)), each agent receives rewards for their responses during collaboration, based on the quality of their answers and interactions. The objective for each agent in MAPORL is to maximize their own value function, defined as the expected cumulative sum of rewards over the course of the collaboration. To further encourage collaboration in MAPORL, we incorporate incentives for successful interactions and penalties for failures in collaboration, steering the LLMs towards more effective and cooperative behaviors. Through a simplified game-theoretic analytical example, we validate the following insights: 1) single-agent training alone is insufficient to produce genuinely collaborative agents, and 2) co-trained agents can achieve an equilibrium with collaborative behaviors. To assess the effectiveness of MAPORL, we conduct experiments across diverse tasks and evaluation strategies. Specifically, we train multi-agent LLMs for tasks such as mathematical reasoning (GSM8k (Cobbe et al., 2021)) and natural language inference (ANLI (Nie et al., 2020)), comparing their performance against baseline approaches. Additionally, we evaluate the robustness of our method by testing agents on out-of-domain tasks (e.g., training on a NLI task and evaluating on a math dataset), demonstrating the generalization capabilities of our approach. We also explore the collaboration among agents of varying capabilities, by analyzing the impact of training heterogeneous LLMs together. To the best of our knowledge, this study is among the first works to explore the training of multi-LLM systems as a whole\u00b9, using RL, for multi-LLM collaboration."}, {"title": "2 Related Work.", "content": "Please see Appendix A for more comprehensive discussions about the related works.\nMulti-Agent Reinforcement Learning. Various algorithms have been proposed to address multi-agent reinforcement learning (MARL) (Hernandez-Leal et al., 2019; Zhang et al., 2021), including multi-agent Proximal Policy Optimization (PPO) (Yu et al., 2022), and value function factorization techniques such as QMIX and VDN (Rashid et al., 2020; Sunehag et al., 2018). In the context of language models and collaborative debating we focus on, MARL takes on a particular and unique form. Here, each agent's state is represented by the sequence of previous responses from all the agents, with each agent deciding the next token based on this history. LLMs provide compact state representations through their hidden layers, enabling the use of long debate histories.\nMulti-Agent Collaboration with LLMs. An array of studies have explored effective collaboration frameworks among multiple large language model agents to solve complex tasks (Wu et al., 2023; Li et al., 2024; Zhao et al., 2024). For example, \"role-playing\"-based approaches utilized multi-agent LLMs by assigning a specific role to each LLM (Li et al., 2023), and \u201cmulti-agent debate\"-based approaches prompted each LLM agent to solve the task independently and then discuss (Du et al., 2024; Khan et al., 2024). In a debate, the agents reason through each other's answers to converge on a consensus response, which may improve the factual accuracy, mathematical ability, and reasoning capabilities of the LLM (Du et al., 2024; Liang et al., 2024; Kim et al., 2024b). Similar mult-agentic frameworks include voting (Wang et al., 2023), group discussions (Chen et al., 2024), and negotiating (Fu et al., 2023). However, all of these frameworks rely heavily on prompt engineering, which may lead to sub-optimal results (Huang et al., 2024), and do not consider training LLMs specifically for collaboration. Therefore, while multi-LLM systems seem promising at the first glance, their performance may be limited when using the out-of-the-box (pretrained) LLM with only prompt tuning, which highlights the need for training for better multi-agent collaboration. Recently, Stengel-Eskin et al. (2025) introduced a training framework for accepting or rejecting persuasion in multi-agent systems. Additionally, very recently, Subramaniam et al. (2025) and Zhao et al. (2025) focused on training the entire multi-agent systems using iterative SFT. In contrast, MAPORL employs (multi-agent) RL to train the whole multi-LLM system.\nRL for LLM Training. RL has been widely used in post-training LLMs, e.g., for improving factuality (Tian et al., 2024), code generation (Le et al., 2022), and more recently and significantly, reasoning (Guo et al., 2025). One prevalent approach of RL for LLM training is RL from human feedback (RLHF) (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022; Ahmadian et al., 2024). RL offers a smooth generalization to the multi-turn setting based on the Markov decision process (MDP) model, and there have been attempts to apply multi-turn RL for LLM training, such as RLHF for multi-turn model training to enhance the dialogue abil-"}, {"title": "3 Analytical Insights: Collaborate to Solve Hard Questions", "content": "In this section, we present a simplified model of multi-LLM collaboration and explain (a) why co-training multiple LLMs is necessary compared to training a single agent, and (b) the role of incentives to further enhance collaboration during training. We validate both aspects through experiments in Section 5.\n3.1 Problem Setup\nWe consider questions that inherently require collaboration for a successful solution. For instance, solving complex mathematical problems often requires collaboration among multiple agents (Liang et al., 2024; Du et al., 2024). Beyond mathematics, collaboration can also enhance the performance on tasks related to privacy, factuality, and reliability (Feng et al., 2025). We model the interaction among LLMs as a repeated game with $T$ turns. For simplicity, we assume that in each turn, each agent chooses between two actions: Collaborate $(a_0)$ or Act Independently $(a_1)$. For a given question $q$, we define $C(q)$ as a non-negative integer representing the collaboration threshold. The agents achieve collaborative synergy if, over the course of the $T$-turn interactions, the total number of collaborative actions $(a_0)$ of all the agents meets or exceeds $C(q)$. When collaborative synergy is achieved, each agent receives a reward $R_{\\text{syn}}(q) = 1$, representing a (near-)guaranteed correct solution. Prior to achieving synergy, agents receive rewards based on their chosen actions: a reward of $R_{\\text{col}}(q)$ for choosing to collaborate $(a_0)$ and $R_{\\text{ind}}(q)$ for acting independently $(a_1)$, where $R_{\\text{col}}(q) < R_{\\text{ind}}(q)$ (see Remark 1 for a detailed justification on the setup). This reward structure creates a tradeoff between short-term accuracy and long-term collaborative success. This setup is related to the classical Coordination Games (Cooper, 1999) in game theory if $R_{\\text{syn}}$ is large. We introduce a new collaboration threshold and synergy mechanism that shapes the transition from independent actions to collaborative behavior in multiple turns, to better model the collaboration procedure among multiple LLMs."}, {"title": "3.2 Analytical Observations", "content": "To provide intuition for why co-training is necessary and single-agent training may be inadequate, we analyze the simplest case with $T=2$ and $C(q)=1$ to illustrate the fundamental differences between single and multi-agent training. We provide formal statements and proofs in Appendix B.\nObservation 1. Suppose that the opponent selects action $a_0$ with probability $\\pi(q)$ for each question $q$. Then, the optimal strategy for the agent is as follows: if $(R_{\\text{syn}}(q) \u2013 R_{\\text{ind}}(q))\\pi(q) \\geq R_{\\text{ind}}(q) \u2013 R_{\\text{col}}(q)$, then the optimal strategy for question $q$ is to collaborate $(a_0)$. Otherwise, the optimal strategy is to act independently $(a_1)$.\nThis shows the dependence of the agent's strategy on the opponent's behavior. If the opponent is not collaborative enough and non-strategic, then $\\pi(q)$ will be small, leading the trained model to behave in a non-collaborative way.\nObservation 2 (Informal). If both agents are trained to maximize their individual cumulative rewards with an entropy regularization term scaled by $\\tau$, then as $\\tau \\rightarrow 0$, they will collaborate if:\n$R_{\\text{syn}}(q) > \\text{max}(3R_{\\text{col}}(q) - 2R_{\\text{ind}}(q), 2R_{\\text{ind}}(q) \u2013 R_{\\text{col}}(q))$.\nObservation 2 can be proved by adapting the results of Zhang and Hofbauer (2016), and transforming our setup with $T=2$ into a matrix game. This observation implies that when both agents optimize"}, {"title": "3.3 Toy Experiments with  T = 10, 20 Turns", "content": "We illustrate the benefit of jointly optimized (multi-agent) policies over those obtained from a single-agent approach in our setting, with longer $T = 10, 20$ turns. Each question $q$ is associated with the rewards $R_{\\text{col}}(q), R_{\\text{ind}}(q)$, and $R_{\\text{syn}}(q)$, along with a collaboration threshold $C(q)$. Further details on the choices of these quantities be found in Appendix C. We first consider a single agent interacting with a fixed opponent whose probability of collaborating, $\\pi(q)$, is set at $\\{0.5, 0.6, 0.7\\}$. Despite the relatively high likelihood of collaboration from the opponent, the single-agent policy, which optimizes its response to the fixed opponent, does not result in effective collaboration (Figure 5). Instead of learning to strategically engage with the opponent's behavior, the single-agent policy, which follows a best-response strategy to the fixed opponent, tends to avoid collaboration, highlighting the limitations of a single-agent framework when facing a fixed, non-strategic opponent.\nNext, we consider two jointly optimizing (multi-agent) learners who adapt their policy based on the other's actions. Concretely, we compute an entropy-regularized Nash equilibrium for $\\tau = 0.1$ via backward induction. As shown in Figure 5 (solid red curves), the jointly optimized agents coordinate with significantly higher collaboration rates. Intuitively, this is because their learning process, which fosters high collaboration at larger $t$, shapes their strategic behavior from the very beginning, leading to increased cooperation even at the first turn. These toy results underscore the importance of strategic interactions: when both agents adapt their policies simultaneously, they learn to be more collaborative, despite the temptation of short-term independent rewards."}, {"title": "4 Post-Co-Training with MARL", "content": "We now provide an overview of our new paradigm of Multi-Agent Post-Co-Training with RL (MAPORL) for LLM collaboration. In our framework, each agent's response per turn is evaluated by a verifier, which assigns a score reflecting the answer's valid-"}, {"title": "4.1 Multi-Agent System - Collaborative Debate Formulation", "content": "We follow the collaborative debate system proposed by Du et al. (2024)as an example of our multi-LLM system in the experiments. Note that, MAPORL can be applied to other multi-LLM systems as long as each agent's response can be evaluated-for example, by a verifier that assigns a score reflecting the quality or correctness of the response. The reward for each agent is then determined by summing the verifier scores of all the responses influenced by that agent throughout the multi-agent interaction process. Assume we have a collaborative debate system that runs for $T$ turns and involves $A$ agents. In each turn, an LLM must determine its next response based on the history of its own answers as well as the answers provided by other LLM agents. Let $q$ be the given question, and let $s_{ti}$ denote the solution provided by agent $i$ at turn $t$. We inductively express the solution $S_{(t+1)i}$ as follows:\n$S_{1i} = LLM_{i} (q), S_{(t+1)i} = LLM_{i} (q\\oplus_{j\\in[A],t'\\in[t]} S_{t'j})$"}, {"title": "4.2 Multi-Agent RL Formulation", "content": "where $\\oplus$ denotes token-wise concatenation, $1 < t < T-1$ and $LLM(s)$ represents the function of inputting prompt $s$ into the $LLM_{i}$ which outputs logits over its token space, followed by sampling a token based on these logits. If $A = 1$, then this setup is equivalent to that of self-correcting LMs (Madaan et al., 2024). Now, we define $\\theta = \\{@_{ta}\\}_{t\\in[T],a\\in[A]}$, where $\\theta_{ta}$ represents the parameters of the $a$-th agent at turn $t$. We denote LLM parameterized by $\\theta_{ta}$ as $LLM_{\\theta_{ta}}$\nNext, to implement MAPORL, we define the reward function for the multi-agent RL formulation, using the verifier score. Due to space constraints, we here introduce the Influence-aware Verification Rewards, and defer other choices of the reward functions that we will use in the experiments to Appendix E.\nDefinition 1 (Influence-aware Verification Reward). The influence-aware verification reward function $R_{\\Theta}(q, S_{ta})$ is defined as\n$R_{\\Theta}(q, S_{ta}) = \\mathbb{E} \\Big[ \\sum_{t'\\in[t,T]} \\gamma^{t'-t} Verifier(q, S_{ta}) +  \\sum_{t'\\in[t+1,T]} \\sum_{j\\in[A]}  \\frac{1}{A} \\gamma^{t'-t} (Verifier(q, S_{t'a}) - Verifier(q, S_{t'j} )\\Big]$\nHere, the expectation arises from the randomness of other agents\u2019 answers, which are influenced by the agent's current response, and $\\gamma\\in [0, 1]$ is a discount factor. This reward not only considers the verifier's score for the current solution $s_{ta}$, but also incorporates the impact of this solution on the future answers of all agents. The term $\\sum_{t' \\in [t+1,T]} \\sum_{j\\in[A]} \\frac{1}{A}$ averages the verifier's scores across all the agents, reflecting the influence that $s_{ta}$ has on the overall multi-agent system."}, {"title": "4.3 Reward Shaping to Incentivize Collaboration", "content": "As discussed in Section 3, incorporating additional incentives in the reward can steer agents towards better collaboration. We define four key parameters when implementing such a reward-shaping: parameters $\\alpha_0$ and $\\alpha_1$ correspond to the incentives related to an agent's own revision of the answer, and parameters $\\beta_0$ and $\\beta_1$ correspond to those related to her influence on other agents\u2019 answers. Specifically, $\\alpha_0$"}, {"title": "5 Experiments", "content": "5.1 Datasets\nWe evaluate MAPORL on two benchmark NLP tasks to validate its performance in both mathematical reasoning and logical natural language inference. The details are summarized as follows:\nGSM8K (Cobbe et al., 2021) and TinyGSM (Liu et al., 2023). GSM8K is a benchmark dataset designed to assess a model's mathematical reasoning abilities, requiring models to solve high-school-level mathematics problems. TinyGSM is an augmented version of GSM8K, generated using GPT-3.5-turbo, where solutions are provided in Python."}, {"title": "5.2 Models", "content": "We primarily use the Microsoft Phi-3-mini-128k-instruct (3.4B) model (Abdin et al., 2024), together with Qwen2.5-3B-instruct (Yang et al., 2024) and Llama-3-8B-instruct (Dubey et al., 2024) for the experiments. Due to computational constraints, we mainly use quantized models and fine-tuned them with QLORA (Dettmers et al., 2024). We defer the training details to Appendix F. When evaluating on GSM8K and ANLI, we set the max token length to 300 and 250, respectively."}, {"title": "5.3 Experiment 1: Vanilla Collaboration by Off-the-shelf LLMs Cannot Improve Performance, While MAPORL-Trained LLMs Can", "content": "We first compare the collaboration performance of off-the-shelf LLMs with MAPORL-trained LLMs. The training was conducted with two agents collaborating over three turns. An overview of the trained system is provided in Figure 1. In Experiment 1, we trained the model starting from turn $t \\geq 2$ for two reasons: (a) the first turn primarily focuses on knowledge acquisition from each dataset, and (b) to ensure a fair comparison with off-the-shelf LLMs. We focus on enhancing collaboration skills rather than teaching specific task knowledge. For this experiment, we used Phi-3-mini-128k-instruct and evaluate the trained models in a three-agent and three-turn collaboration environment. We observe that even when the off-the-shelf LLM is allowed to generate longer reasoning (600 tokens, twice the output length of our MAPORL-trained model model), its accuracy did not improve across turns. This aligns with prior findings in the literature, particularly for models that are not sufficiently strong. For instance, Huang et al. (2024, Table 7) provided evidence that additional turns do not necessarily improve the performance significantly. Similarly, our results show that off-the-shelf LLMs' performance may not benefit from additional turns. In contrast, LLMs trained using MAPORL exhibit improved performance as the number of collaboration turns increased, as shown in Figure 2."}, {"title": "5.4 Experiment 2: Reward Shaping with Collaboration Incentives", "content": "In addition to the multi-agent independent PPO framework, we then investigate the auxiliary incentive mechanism designed to enhance collaborative interactions. To analyze the impact of the incentive parameters (a and $\\beta$, Section 4.3), we simplify our experimental setup by limiting the total number of debate turns to 2 and analyze the following cases. Here, $\\alpha_0$ and $\\alpha_1$ correspond to incentives for an agent's own revision, capturing critical reasoning (extracting useful information from incorrect answers) and persuadability (accepting correct information), respectively. Meanwhile, $\\beta_0$ and $\\beta_1$ correspond to incentives for influencing others, where $\\beta_0$ encourages providing incorrect but useful responses, and $\\beta_1$ reflects an agent's ability to persuade others with correct answers. To analyze the impact of the incentive parameters (a and $\\beta$, Section 4.3), we simplify our experimental setup by limiting the total number of debate turns to 2 and analyze the following cases. Here, $\\alpha_0$ and $\\alpha_1$ correspond to the incentives related to an agent's own revision of the answer, while $\\beta_0$ and $\\beta_1$ correspond to the incentives related to the agent's influence on other agents' answers."}, {"title": "5.5 Experiment 3: Collaboration Ability Acquired by MAPORL Is Transferable", "content": "Here, we investigate the transferability of collaboration abilities acquired through MAPORL across different datasets not used during training. We evaluate LLMs trained with MAPORL on one dataset when applied to tasks from other datasets. For instance, we assesse models trained on ANLI when solving tasks from GSM8k, along with other dataset combinations. The results, presented in Table 5, demonstrate that collaboration abilities learned through MAPORL are indeed transferable across datasets. This suggests that the models acquire a meta-capability for effective collaboration, even when encountering novel, unseen tasks."}, {"title": "5.6 Experiment 4: MAPORL with Heterogeneous LLMs Can Help", "content": "In this experiment, we investigate collaborative learning between different foundation models, specifically examining co-training between (Phi3 3.4B and Qwen2.5 3B) and (Phi3 3.4B and Llama3-8B) pairs. In single-model evaluations, both Phi3 and Qwen2.5 3B demonstrate stronger performance compared to Llama3-8B. Due to GPU memory constraints necessitating simultaneous loading of two base models, we conduct experiments in a two-agent, two-turn environment. This setup enables us to explore whether models with heterogeneous capabilities could effectively collaborate to enhance the overall performance (Figure 7). The synergistic effects are particularly evident when models with different strengths worked together, suggesting that diverse model partnerships can yield better outcomes than individual model performance alone when we have MAPORL."}, {"title": "5.7 Experiment 5: Na\u00efve Supervised Fine-Tuning Using High-Quality Collaboration Samples May Not Induce Collaborative Behaviors", "content": "In this experiment, we investigate whether models could learn collaborative behavior through SFT on high-quality debate trajectories. We generated 12,800 trajectories using the multi-agent system (Figure 1) with off-the-shelf LLMs to match the training sample size used in MAPORL for GSM8K. To provide favorable conditions for SFT, we allow a maximum of 600 tokens per response, which exceeded the token limit used in our MAPORL experiments. We selected the top 10% of trajectories using the following criteria: 1) excluding trajectories without well-formatted answers, 2) filtering out trajectories where the final majority voting result was incorrect, and 3) selecting 1,280 trajectories based on the verifier's score of the final answer, which evaluates both correctness and reasoning quality. Interestingly, the results indicate that SFT not only failed to enhance collaborative behaviors, but also led to a decline in performance compared to the off-the-shelf model. Specifically, for turn-2, accuracy dropped to 0.578 ($\\Delta = -0.111$), and for turn-3, it further decreased to 0.525 ($\\Delta = -0.114$). This suggests that either substantially more training data would be required to learn effective collaborative behaviors, or that SFT might not be an effective approach for inducing such behaviors. Contemporaneously, Subramaniam et al. (2025) and Zhao et al. (2025) enhance multi-agent performance by incorporating new techniques into iterative SFT with their own data augmentation to generate effective collaboration examples, demonstrating its potential when combined with additional refinements. In contrast, our approach does not leverage data augmentation, but instead utilizes RL."}, {"title": "6 Conclusion", "content": "In this paper, we have introduced MAPORL, a new post-training paradigm that leverages multi-agent RL to explicitly foster the collaboration among multiple LLMs. Unlike methods that rely solely on prompting or single-agent fine-tuning, MAPORL focuses on co-training multiple LLMs, ensuring that each agent adapts its policy not just to immediate feedback, but also to the strategic behaviors of other agents over multiple interactive turns. By incorporating a verifier network for reward shaping with incentives, the framework guides each agent's responses that account for both short-term correctness and long-term collaborative potential, thus promoting collaborative discussions that lead to more accurate final answers. Through an extensive set of experiments on reasoning-intensive tasks \u2013 such as GSM8K for mathematical problem-solving and ANLI for logical natural language inference our results demonstrate that off-the-shelf LLMs often do not improve the overall performance with additional debate turns. In contrast, MAPORL-trained agents show significant improvements with accuracy increasing as collaboration progresses. Crucially, these collaborative abilities are shown transferable across tasks, suggesting that once LLMs learn to collaborate, they can retain a generalizable \u201ccollaboration skill\" applicable to different domains. Furthermore, our experiments with heterogeneous LLMs highlight that MAPORL can also foster collaborative synergy even among models of varying capabilities. While our study has revealed the promise of MAPORL for boosting LLM performance in multi-agent settings, several avenues for future work remain. First, exploring new reward-shaping strategies - beyond verifier scores to incorporate factual consistency or domain-specific constraints may yield more significant improvement. Second, integrating more specialized collaboration protocols (beyond collaborative debate) may better align with real-world use cases that demand consensus or hierarchical decision-making among multiple LLMs."}, {"title": "Limitations", "content": "Since we use instruction prompts as inputs to the LLMs, the output can vary significantly depending on the prompts. As the first methodology paper, our experiments are conducted on relatively small LLMS (3B to 8B parameters) for fast iteration, and the observed behaviors may differ on larger models. After all turns of multi-LLM interactions, we apply majority voting to determine the final answer. Using alternative mechanisms, such as a manager agent that makes the final prediction based on the responses from multiple agents, may further improve the overall performance."}, {"title": "Potential Risks", "content": "As our proposed approach encourages and facilitates collaboration among multiple LLM agents, when adversarial or malicious agents exist, our method could lead to unintended harmful outcomes by enabling their collaboration with others."}, {"title": "B Deferred Proof of Section 3", "content": "Observation 1. Suppose that the opponent selects action $a_0$ with probability $\\pi(q)$ for each question $q$. Then, the optimal strategy for the agent is as follows: if $(R_{\\text{syn}}(q) \u2013 R_{\\text{ind}}(q))\\pi(q) \\geq R_{\\text{ind}}(q) \u2013 R_{\\text{col}}(q)$, then the optimal strategy for question $q$ is to collaborate $(a_0)$. Otherwise, the optimal strategy is to act independently $(a_1)$.\nProof. For the last turn (t = 2), regardless of whether the opponent selects $a_0$ or not, choosing $a_1$ is an optimal strategy. This is because:\n\u2022 If collaborative synergy has been achieved, the agent will always receive $R_{\\text{syn}}(q)$ regardless of their action in the second turn.\n\u2022 If collaborative synergy has not been achieved, since we know that $R_{\\text{col}}(q) < R_{\\text{ind}}(q)$, the optimal choice is to select $a_1$ in the final turn to maximize the immediate reward.\nTherefore, considering the cumulative reward for the turn $t = 1$, the reward matrix is given as follows:\n$\\begin{array}{c|cc}  & a_0 \\text{ (Collaborate)} & a_1 \\text{ (Act independently)}\\\\ \\hline a_0 \\text{ (Collaborate)} & (R_{\\text{col}}(q) + R_{\\text{syn}}(q), R_{\\text{col}}(q) + R_{\\text{syn}}(q)) & (R_{\\text{col}}(q) + R_{\\text{ind}}(q), 2R_{\\text{ind}}(q))\\\\ a_1 \\text{ (Act independently)} & (2R_{\\text{ind}}(q), R_{\\text{col}}(q) + R_{\\text{ind}}(q)) & (2R_{\\text{ind}}(q), 2R_{\\text{ind}}(q))\\end{array}$      \nSince the opponent chooses $a_0$ with probability $\\pi(q)$, the expected reward for choosing $a_0$ is:\n$(R_{\\text{col}}(q) + R_{\\text{syn}}(q))\\pi(q) + (R_{\\text{col}}(q) + R_{\\text{ind}}(q))(1 \u2013 \\pi(q))$.\nThe expected reward for choosing $a_1$ is $2R_{\\text{ind}}(q)$. To determine the optimal strategy, we compare these two expected rewards. The agent should collaborate $(a_0)$ if:\n$(R_{\\text{col}}(q) + R_{\\text{syn}}(q))\\pi(q) + (R_{\\text{col}}(q) + R_{\\text{ind}}(q))(1 \u2013 \\pi(q)) \\geq 2R_{\\text{ind}}(q)$.\nwhich is equivalent to\n$(R_{\\text{syn}}(q) - R_{\\text{ind}}(q))\\pi(q) \\geq R_{\\text{ind}}(q) \u2013 R_{\\text{col}}(q)$.\nThus, if $(R_{\\text{syn}}(q) \u2013 R_{\\text{ind}}(q))\\pi(q) \\geq R_{\\text{ind}}(q) - R_{\\text{col}}(q)$, the optimal strategy is to collaborate $(a_0)$. Otherwise, the agent should act independently $(a_1)$ to maximize their cumulative expected reward.  \nNow, we provide the formal statement of Observation 2. Before doing so, we define the regularized Nash Equilibrium (NE).\nDefinition 2 (Regularized NE). An entropy-regularized Nash equilibrium is defined as a strategy profile $\\pi^*$ where each player maximizes a regularized objective that combines the expected reward with an entropy term. Specifically, for each player $i$, the equilibrium strategy $\\pi_i^*$ satisfies\n$\\pi_i^* = \\text{arg } \\underset{\\pi_i}{\\text{max}} \\mathbb{E}_{a_i \\sim \\pi_i, a_{-i} \\sim \\pi^*_{-i}}[U_i(a_i, a_{-i})] + \\tau H(\\pi_i)$,\nwhere $\\tau > 0$ is a temperature parameter and $H(\\pi_i) = - \\sum_{a_i} \\pi_i(a_i) \\text{log }\\pi_i(a_i)$ is the Shannon entropy of the strategy, and $u_i$ is the utility function of player $i$. This entropy term smoothens the best response, leading to a softmax (or logit) formula of the optimal strategy:\n$\\pi^*_i(a_i) = \\frac{\\text{exp}(\\frac{1}{\\tau} \\mathbb{E}_{a_{-i} \\sim \\pi^*_{-i}}[U_i(a_i, a_{-i})])}{\\sum_{a' \\in A} \\text{exp}(\\frac{1}{\\tau} \\mathbb{E}_{a_{-i} \\sim \\pi^*_{-i}}[U_i(a', a_{-i})])}$"}, {"title": "C Deferred Details in Section 3.3", "content": "The game is solved using backward induction with the state represented as (turn, count), where count denotes the number of times $(a_0, a_0)$ has occurred in the history of the interactions. Both players choose actions to maximize their expected cumulative utility plus an entropy term times a coefficient $\\tau = 0.1$.\nChoices of $R_{\\text{col}} (q)$, $R_{\\text{ind}}(q)$, $R_{\\text{syn}}(q)$, $C(q)$. Each instance of a question $q$ is associated with parameters drawn as follows: the independent action reward $R_{\\text{ind}}(q)$ is sampled from a uniform distribution $R_{\\text{ind}}(q) \\sim \\text{Unif}(0, 1)$. The collaborative action reward $R_{\\text{col}}(q)$ is then sampled condition on $R_{\\text{ind}}(q)$, following $R_{\\text{col}}(q) \\sim \\text{Unif}(0, R_{\\text{ind}}(q))$. The synergy reward is fixed as $R_{\\text{syn}} (q) = 1$."}, {"title": "D Deferred Details of the Verifier Models", "content": "For a reasoning question $q$, the trained verifiers (reward models) assess the correctness of a complete solution path $"}]}