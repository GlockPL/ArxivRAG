{"title": "Fine-tuning - a Transfer Learning approach", "authors": ["Joseph Arul Raj", "Linglong Qian", "Zina Ibrahim"], "abstract": "Secondary research use of Electronic Health Records (EHRs) is often hampered by the abundance of missing data in this valuable resource. Missingness in EHRS occurs naturally as a result of the data recording practices during routine clinical care, but handling it is crucial to the precision of medical analysis and the decision-making that follows. The literature contains a variety of imputation methodologies based on deep neural networks. Those aim to overcome the dynamic, heterogeneous and multivariate missingness patterns of EHRs, which cannot be handled by classical and statistical imputation methods. However, all existing deep imputation methods rely on end-to-end pipelines that incorporate both imputation and downstream analyses, e.g. classification. This coupling makes it difficult to assess the quality of imputation and takes away the flexibility of re-using the imputer for a different task. Furthermore, most end-to-end deep architectures tend to use complex networks to perform the downstream task, in addition to the already sophisticated deep imputation network. We, therefore ask if the high performance reported in the literature is due to the imputer or the classifier and further ask if an optimised state-of-the-art imputer is used, a simpler classifier can achieve comparable performance.\nThis paper explores the development of a modular, deep learning-based imputation and classification pipeline, specifically built to leverage the capabilities of state-of-the-art imputation models for downstream classification tasks. Such a modular approach enables a) objective assessment of the quality of the imputer and classifier independently, and b) enables the exploration of the performance of simpler classification architectures using an optimised imputer.", "sections": [{"title": "I. INTRODUCTION", "content": "In the era of big data, healthcare data has emerged in diverse formats such as data collected through health monitoring devices, sequencing data generated in labs, clinical data collected from prospective observational or randomized trials, and EHRS collected during hospital visits [1]. EHRs have emerged as a cornerstone of modern healthcare, capturing a comprehensive array of patient information including demographic data, medical history, laboratory results, medications, vital signs, and more. They provide a wealth of information carrying the potential to improve treatment quality and patient outcomes [2]. EHRs serve multiple purposes within the healthcare ecosystem. Primarily, they support clinical care by providing healthcare providers with real-time access to patient information, facilitating efficient diagnosis, treatment planning, and medication management. Additionally, EHRs are instrumental in healthcare administration, enabling tasks such as appointment scheduling, billing, and quality improvement initiatives.\nFurthermore, EHR data has become an invaluable resource for research. The ability to analyse large-scale patient populations has accelerated the discovery of new treatments, prevention strategies, and disease patterns [3]. But, the challenge of missing data persists here. EHR data is generated as a product of patient care. Measurements, symptoms and observations are therefore added at the discretion of clinical workers and according to clinical practices. As a result, missingness prevails in EHR due to many reasons including the following [2]:\n\u2022\n\u2022 Heterogeneity: not all patients undergo the same set of lab tests leading to missing and incomplete data, relative to other patients.\nData Modality: EHRs contain data in various formats, including numerical values (lab results), free text notes (physician consultations), and images (X-rays). Missing data can occur within each modality.\n\u2022 Hidden Relationships: Important patient relationships, such as family history or shared diagnoses, might not be explicitly captured in the data. These relationships can hold valuable information for analysis but are often missing or implicit.\nImputation of missing data is very much needed when we are trying to build a predictive model with that data With-out imputation, one would need to discard samples with missing values, which can lead to a reduction in sample size, limiting the model's ability to learn complex patterns and introducing bias into the model resulting in inaccurate predictions and misleading conclusions [4]. Many methods and strategies have been developed to handle the missing data problem, including conventional statistical, machine learning, and newly developed deep learning methods. Deep learning imputation has garnered significant interest due to its ability to handle complex relationships in the data with minimal dependence on the underlying data distributions, potentially leading to better imputation. Architectures like autoencoders, recurrent neural networks (RNNs), and long short-term memory networks (LSTMs) have demonstrated impressive results in reconstructing missing data by understanding the underlying structures and dependencies, even in complex data types like"}, {"title": "II. RELATED WORK", "content": "The advent of deep learning has spiked significant advancements in imputation techniques, offering promising solutions to address this issue. A plethora of deep learning-based imputers have been developed, demonstrating their potential to effectively recover missing information while preserving data integrity and utility. To remain within the scope of this study, only the literature comprising of deep learning imputers for healthcare were considered and are categorised as follows:\n\u2022 RNN-based Models:\nThe table below summarises the performance of these models across various datasets and tasks, providing insights into their relative strengths and limitations. The performance metrics listed in the table are based on what the original research papers reported. We did not conduct independent evaluations of these models.\nBased on the table and a comprehensive literature review, our analysis reveals several critical gaps in the existing imputation models. These gaps include,\n\u2022 End-to-End Pipelines: Many models seen in the table, integrate imputation directly with predictive modelling in an end-to-end fashion. While this can streamline the process, it often obscures the individual contributions of imputation and classification, making it difficult to isolate and improve each component. Additionally, these end-to-end approaches may not be optimal for all downstream tasks.\n\u2022 Uncertain Improvement Source: It is often unclear whether performance improvements are due to the strength of the imputation model or due to the complexity of the downstream classifier or their interaction. This lack of clarity hampers targeted improvements.\n\u2022 Incomplete Evaluations: Many studies do not conduct comprehensive evaluations across different types of missing data and the required downstream task of interest.\n\u2022 Variability Across Publications: There are no standardised models or benchmarks for comparing model out-comes, leading to varying reported performances across different publications. The reported performance of the most highly-cited models (e.g. BRITS [7], GRU-D [5]) seems to vary per publication."}, {"title": "III. BACKGROUND", "content": "Transfer learning has gathered significant attention in recent years, especially in computer vision and natural language processing. In general, the transfer learning technique can be defined as the improvement of learning a new task through the transfer of knowledge from a related task that has already been learned. Fine-tuning a pretrained network model such as ImageNet [23] on a new dataset is the most common strategy for knowledge transfer in the context of deep learning. Meth-ods have been proposed to fine-tune all network parameters, only the parameters of the last few layers, or to just use the pretrained model as a fixed feature extractor [24]. Recent research has increasingly adopted fine-tuning Large Language Models (LLMs) like BERT, GPT, and Llama as a strategy to enhance performance while simultaneously reducing training costs and time. Despite its recent success in computer vision and natural language processing, transfer learning has been rarely applied to deep learning models for time series data. Since time series data contain one temporal dimension (time), compared to two dimensions for images (width and height), it is only natural to think that if filters can successfully be transferred on images, they should also be transferable across time series datasets [25].\nThe pretraining comprises our foundational model, where the weights are adjusted during the training based on the input data. It is the initial phase of a model's development where it learns general patterns and representations from a vast amount of unlabeled data. This process involves exposing the model to"}, {"title": "III. BACKGROUND", "content": "CSAI is a well-established imputation model and was selected due to its capability to handle missing data in multivariate medical time series, specifically tailored to the complexities of Electronic Health Records (EHRs). The model effectively deals with the imbalanced distribution of missing data, which is a critical issue in EHR datasets. CSAI is an extension of Bidirectional Recurrent Imputation for Time Series (BRITS) [7] which uses a bidirectional RNN for imputing missing values by employing decay factors. Since it is an RNN-based framework utilising the Transformer-like self-attention mechanism, we can leverage the rich hidden states generated by the RNN for downstream tasks. This means that we can directly utilise the extracted hidden representations from our pretrained model as valuable features for our classifier.\nCSAI builds upon BRITS architecture by introducing a non-uniform masking strategy tailored to handle the distinct patterns of missing data often found in medical datasets. It captures temporal relationships by incorporating a conditional knowledge embedding mechanism combined with a decay function to optimise the imputation process. As we can observe from the architecture diagram, the process begins with an input embedding layer that transforms raw data inputs into a dense representation. This is followed by Positional Embedding, which injects temporal information, ensuring that the model"}, {"title": "IV. METHODOLOGY", "content": "To benchmark against the existing models using the two most widely used datasets in\n\u2022\n\u2022 MIMIC-III: a large database comprising de-identified, comprehensive clinical data of patients admitted to the Beth Israel Deaconess Medical Center in Boston, Mas-sachusetts. It contains data associated with 53,423 dis-tinct hospital admissions for adult patients admitted to critical care units between 2001 and 2012, covering a total of 38,597 distinct adult patients and 7870 cases of newborn children admitted between 2001 and 2008 [28]. The MIMIC-III extract used in this work is obtained by following the benchmarking strategy used in the two benchmarking papers [29] [30]. The final extract comprises 14,365 samples with 89 features.\nPhysionet 2012: extracted from the MIMIC-II database, Physionet [31] is a part of the Computing in Cardiology Challenge 2012. It comprises records from 12,000 ICU stays of adult patients lasting at least 48 hours. The extracted data comprises 3997 samples with 35 features\nThe diagram shown in Fig 2, demonstrates a structured approach followed in this work to perform a classification task using missing data. The framework comprises two main stages: pretraining and fine-tuning\n\u2022 Pretraining: Initially, a pretraining phase is conducted to develop a robust imputation model. The well-established strong imputation model, CSAI [27] is chosen as the foundational model for pretraining. The raw medical data, which includes the original records with missing entries, is fed into the imputation model for training to learn to impute missing values by capturing the underlying patterns and relationships within the dataset in light of"}, {"title": "CSAI's embedded knowledge-acquisition strategies [27].", "content": "The output of this stage is a pretrained CSAI model, which has been optimised to handle missing data through imputation.\n\u2022 Fine-tuning: In our second phase, i.e. fine-tuning, we feed the raw data into our pretrained model to obtain the imputed data and hidden states representing the imputed values and their associated uncertainties. These learned features are then fed into our downstream classifier for fine-tuning by either freezing or unfreezing the pretrained weights. Freezing the pretrained weights will use the pretrained imputer with no further updates to its learned weights and biases. On the other hand, unfreezing the weights will enable further fine-tuning of the imputer with respect to the downstream classifier. By perform-ing experiments using the two different setups, we can evaluate the effect of the learned features and weight initialisation from the pretrained model and are leveraging the strengths of our pretrained model, to improve its overall performance on the predictive tasks."}, {"title": "V. EXPERIMENTAL SETUP", "content": "To comprehensively assess the performance of our imputa-tion model, we conducted a series of experiments with differ-ent classifiers and varied configurations. The classifiers used for the downstream task are Multi-Layer Perceptron (2-layer and 5-layer MLP), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) and non-neural network classifiers such as XGBoost and Support Vector Machine (SVM). The different types of strategies used for our downstream classifiers are categorized as,\n\u2022 Freezing the weights of the pretrained model and training the neural network model.\n\u2022 Unfreezing the weights of the pretrained model and training(fine-tuning) them along with the classifier.\n\u2022 Utilising the learned feature on a non-neural network model."}, {"title": "Freezing model weights:", "content": "Our initial approach involved freezing the weights of the pretrained model and leveraging the optimised weights as the initial weight for our downstream"}, {"title": "Unfreezing model weights:", "content": "In this approach, the weights of our pretrained model unfrozen, which allows them to be updated during the backpropagation alongside the downstream classifier's weights. This will evaluate the impact of fine-tuning the pretrained model along with the classifier to improve performance on downstream tasks. This approach of transfer learning is known as fine-tuning, where the model adapts its parameter to better suit the specific downstream task.\nIn both strategies, for MLP-based classifiers, the hidden states from the pretrained model are passed as inputs, whereas for the RNN-based classifiers, we implemented two distinct strategies to handle the inputs."}, {"title": "B. Hyperparameter Tuning", "content": "To optimise the performance of our models and to avoid overfitting we employed hyperparameter tuning strategies for learning rate, which is crucial in training neural networks as it determines the step size at each iteration while moving toward a minimum of the loss function. A too-high learning rate can cause the model to converge too quickly, missing the minima of the loss function whereas a low learning rate can make the training excessively slower and there is also a chance for the model to get stuck in local minima. We implemented the following different hyperparameter tuning strategies,\n\u2022 Cyclical Learning Rate (CyclicLR): This strategy in-volves cyclically varying the learning rates within a specific range. Instead of monotonically increasing or decreasing the learning rates, this method lets the learn-ing rate cyclically vary between the upper bound and lower bound values that are being set [32]. This os-cillation can help the optimizer escape local minima and better explore the loss surface. For our setup, we used the 'base_lr=0.00001' and 'max_lr=0.001', with the 'mode=exp_range'. In the exp_range mode, the learning rate varies between the minimum and maximum bound-aries and each boundary value declines by an exponential factor of the gamma values [32].\n\u2022 ReduceLROnPlateau: This strategy dynamically adjusts the learning rate during training to ensure the model con-tinues to improve its performance even as it approaches a plateau. It helps fine-tune the model by decreasing the learning rate when the monitored metric, such as validation loss in our case, stops improving for a cer-tain number of epochs. For our setup, we started the experiment with a learning rate of 0.001 and set the mode as 'min', as we are expecting to minimise the validation loss(monitored metric). The reduction factor was set as 0.2 with patience=15', which means if after 15 epochs there is no reduction in validation loss, then the learning rate will decrease by a factor of 0.2. We also set 'min_lr=0.00001' so that the learning rate will not drop too low.\n\u2022 Optuna Hyperparameter Optimization: This strategy employs Bayesian optimization by exploring the hyperpa-rameter space by selecting configurations that either max-imise the objective function's performance or minimise the validation loss [33]. Through the interaction with the trail object, Optuna constructs the objective function, dur-ing which the search space will be created dynamically by the trail object's methods. It also has the feature of employing pruning of trials, also known as automated early-stopping wherein the objective function terminates"}, {"title": "C. Early Stopping", "content": "Early stopping is a regularisation technique employed in machine learning to prevent overfitting. Essentially, it involves halting the training process of a model before it reaches its optimal performance on the training data. Early stopping is particularly useful for iterative learning algorithms where the model's parameters are updated repeatedly. By introducing a stopping criterion based on the validation set performance, we can avoid unnecessary computational resources and time spent on training a model beyond its optimal point.\nBefore considering early stopping, we ran some of our experiments with 200 epochs and observed the following, a) The validation performance reaches a plateau after some epochs, b) The validation performances start decreasing and c) The model learns the training set too much such that, the training performance attain the maximum of AUC 1 after certain epochs, clearly denoting the occurrence of overfitting.\nTo mitigate this we followed the early stopping strategy by introducing a callback to our training function. In our experi-ment, we used the patience counter to trigger the callback and the patience value was set to 25, meaning that the training will stop if there's no improvement in validation performance for over 25 epochs. We also followed a strategy to store the model's state at each epoch if it achieved a higher validation performance so that we could utilise the model at its high-performing state."}, {"title": "D. Implementation Details", "content": "For both datasets, we parsed the data by converting it to hourly bins and grouping the attribute values within each hour and then extracting the records from the first 48 hours for each patient. We employed K-Fold cross-validation by dividing the data into five folds, with each fold including independent training, evaluation, and test sets. This ensures that the model can be trained and tested consistently across diverse data subsets. To isolate the impact of fine-tuning in a simpler neural architecture on downstream task performance, we opted for minimal data preprocessing. Beyond the standard normalisation techniques applied during pretraining, we re-frained from additional data processing or feature engineering steps. The processing techniques used during pretraining are as follows,\n\u2022 A custom normalisation function following a standardis-ation approach, which adjusts each data point to have a mean of zero and a standard deviation of one is being used. The mean and standard deviation for each variable are calculated using non-missing data points. Then the data points are normalised by subtracting the mean and dividing by the standard deviation. In cases when the standard deviation is zero, then the normalised data is calculated just by subtracting the mean.\n\u2022 Once the data are normalised, the data loader is prepared to make the data compatible with the pytorch framework. Here Non-uniform masking strategy with 10% missing-ness, as introduced and followed in the CSAI paper [27], is employed to generate masks to simulate missing values. This will be used to train the imputer to fill in the missing records. The final data loader is prepared to include the training, validation and test set in the form tensors."}, {"title": "E. Experimental results", "content": "The main results of our experiments can be summarised in two categories: Performances while freezing the pretrained model's weights and Performances while unfreezing the pre-trained model's weights. Tables II and III, present the down-stream performances across different neural network models on the two datasets. Each model has been trained using three different strategies: Hyper-parameter tuning using Optuna, and learning rate schedulers such as CyclicLR scheduler and ReduceLROnPlateau scheduler.\nFreezing model weights: As observed from Table II, the average highest performance for physionet data with frozen weights was observed in the LSTM model obtaining a val-idation AUC of 0.797 and a test AUC of 0.781, while the lowest performance was observed in 5-layered MLP with a validation AUC of 0.785 and a test AUC of 0.765. For the MIMIC III data, table II shows the highest performance in the LSTM model with a validation AUC of 0.827 and a test AUC of 0.795. The lowest performance was observed in a 2-layered MLP model obtaining a validation AUC of 0.839 and a test AUC of 0.819.\nFreezing model weights: Table III shows that, for Physionet"}, {"title": "VI. DISCUSSION AND CONCLUSION", "content": "By employing different neural network architectures and different classification strategies for our downstream task we aimed to evaluate two things: the impact of strong imputer on the downstream classifier and the impact of model complexity on the performance. Based on the results presented in Tables II and III and by considering our research question in place, the observed key findings and interpretation of the results across the datasets and different models will be described in three sections."}, {"title": "Fine-tuning vs Freezing:", "content": "Fine-tuning the pretrained model generally led to improved performance compared to models trained while freezing the pretrained model weights. As we are freezing the pretrained imputer, we are not allowing the weights and biases of the pretrained model to get updated. As their values are fixed, they do not adapt well to the requirements of the downstream classifier. Whereas in a fine-tuning approach, the unfrozen weights and biases that are initialised from the pretrained saved state get updated with respect to the downstream classifier. This process helps in refining the learned features from the imputation tasks to better align with the specific objectives of the classifier thus achieving improved performance."}, {"title": "Model weights vs Model architecture:", "content": "RNN-based models performed better in all the instances when the pretrained weights were frozen. On the other hand, when the model weights were unfrozen, the MLP-based models performed better in most of the cases. The reason for this might be the nature of the model itself. RNN-based models, such as LSTM and GRU, are specifically designed to handle sequential data by maintaining hidden states that can carry information from one time step to the next and learn temporal dependencies. Whereas MLP-based models focus on learning mappings from input features to outputs, without an inherent capability to manage sequential data. They can process time-series data only by converting the data to a non-sequential format (e.g., using feature extraction or in our case using hidden states from a pretrained model), which eliminates the need for learning and preserving temporal dependencies directly. In theory, since the pertaining is done using the same dataset, freezing the model weights prevents the model from overfitting to the new task by preserving the learned features from the original training. Whereas when the weights are unfrozen, the entire weight space is trainable and so there is a risk of catastrophic forgetting - When we unfreeze the model weights and fine-tune them for the new task, the performance of the task either increases or at least maintains. But when this property fails and there is a high decrease in performance on the previous tasks in favour of the new one, the model is said to suffer from the problem where the prior knowledge is forgotten while learning a new task. This problem is common in transfer learning with fine-tuning in Neural Networks [34]. This contributes to the reason that MLP-based classifiers perform better than RNN-based classifiers when the model weights are unfrozen. As model weights are unfrozen, MLPs have the flexibility to adapt the weights to the new task at hand and since MLPs do not capture the temporal dependencies directly, the concept of catastrophic forgetting doesn't apply in the same way it does to RNNS."}, {"title": "Model Complexity vs Performance:", "content": "Since the fine-tuning with unfrozen weights demonstrated improved performance, this analysis will focus only on com-paring models while keeping the weights unfrozen. As we have observed from Table III, there is not much of a difference between the performance of all 4 classifier models(2MLP, 5MLP, LSTM and GRU). In other words, the performance didn't show a significant improvement when the model com-plexity was increased. In fact, as per our findings, the simplest"}, {"title": "D. Conclusion:", "content": "In summary, our study shows that the use of a strong imputer coupled with relatively simple neural architectures can yield competitive results, even compared to more complex state-of-the-art methods. While in some instances, increas-ing complexity showed improved performance (like the 5-layer MLP achieving higher Test AUC of 0.786 and 0.81 while using LR schedulers with the Physionet 2012 dataset), these improvements are so minimal while considering the substantial increase in the number of parameters leading to the additional computational cost and the risk of overfitting. Our best-performing models were closer to the benchmarks of top-performing complex models like Set Functions for Time Series (0.869 AUC) [35], Interpolation-Prediction Networks (0.864 AUC) [36], and Multi-Time Attention Networks (0.858 AUC) [37]. This is impressive given that our models are less complex, highlighting the efficacy of our pretrained imputer in extracting useful features and supporting our hypothesis that learned features from the imputer have an impact on the performance of the downstream classifier. However, our results demonstrated that traditional machine learning models might not be well-suited for leveraging the rich feature representa-tions learned by deep learning models. While the approach shows promising results, it also emphasises the need for nec-essary improvement, notably in terms of model explainability. Future advances in this area along with model generalisability, could considerably improve the utility and safety of employing such models in real-world healthcare settings. Our findings pave the way for further research into the integration of sophisticated imputation approaches with downstream tasks of interest, intending to develop more accurate and trustworthy tools for medical data analysis."}]}