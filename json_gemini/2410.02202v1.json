{"title": "Can Language Models Take A Hint? Prompting for Controllable Contextualized Commonsense Inference", "authors": ["Pedro Colon-Hernandez", "Claire Yin", "Nanxi Liu", "Chelsea Joe", "Peter Chin", "Yida Xin", "Henry Lieberman", "Cynthia Breazeal"], "abstract": "Generating commonsense assertions within a given story context remains a difficult task for modern language models. Previous research has addressed this problem by aligning commonsense inferences with stories and training language generation models accordingly. One of the challenges is determining which topic or entity in the story should be the focus of an inferred assertion. Prior approaches lack the ability to control specific aspects of the generated assertions. In this work, we introduce \"hinting,\" a data augmentation technique that enhances contextualized commonsense inference. \"Hinting\" employs a prefix prompting strategy using both hard and soft prompts to guide the inference process. To demonstrate its effectiveness, we apply \"hinting\" to two contextual commonsense inference datasets: Para-COMET and GLUCOSE, evaluating its impact on both general and context-specific inference. Furthermore, we evaluate \"hinting\" by incorporating synonyms and antonyms into the hints. Our results show that \"hinting\" does not compromise the performance of contextual commonsense inference while offering improved controllability.", "sections": [{"title": "1 Introduction", "content": "The task of Contextual or Discourse-Aware Commonsense Inference, which consists of generating relevant and coherent commonsense assertions (i.e. facts) for a certain sentence in a story context, while easy for humans, remains challenging for machines. Within this task, we define an assertion as a tuple that contains a subject, a relation, and an object (e.g., a dog, is a, animal), similar to a subject-verb-object triple. An assertion in this task is a contextually specific fact or generally applicable templated fact, that can be inferred from a sentence within a given story context.\nAutomated systems (such as pre-trained transformer-based language models) struggle with generating contextual assertions since there is an implicit assumption that clues for making predictions can always be found explicitly in the text. This becomes problematic because a model for this task is essentially forced to use knowledge that it may not have seen during pre-training. Additionally, models are forced to guess what to predict about (e.g., what the subject of an assertion is), which may lead to decreased performance (e.g., the model generates an assertion about cats when it should have talked about dogs). Below we give an example of contextual commonsense inference with a story, a target sentence, and some corresponding story specific and general inferences. The story is picked directly from the ROCStories corpus."}, {"title": "2 Related Work", "content": "Recently, there has been a shift in paradigm in Natural Language Processing from pre-training and fine-tuning a model, to pre-training, prompting, and predicting. One primary reason for this shift is the creation of ever-larger language models, which have become computationally expensive to fine-tune. Prompting can be described as converting a pre-trained language model input sequence into another sequence that resembles what the language model has seen during pre-training. Overall, most prompting research is focused on formulating the task as a cloze (fill-in-the-blanks) task. However, we consider the task of language generation, an open-ended formulation.\nRecall that prefix prompting modifies the input to a language model, by adding either a hard prompt (additional words to the input sequence) or a soft prompt (i.e., adding trainable vectors that represent, but are not equivalent to, additional words).\nUnlike classic prefix prompting, hinting uses both hard and soft prompts. The soft prompts are in the form of symbols that represent the different parts of the assertion (i.e., subject, relation type, and object), and the hard prompts are in the form of the actual parts of the assertion that are selected to be appended as part of the hint. Our work is similar to KnowPrompt, except that they use a masked language model and soft prompts for relationship extraction. AutoPrompt is also similar, but finds a set of \"trigger\" words that give the best performance on a cloze-related task, whereas we provide specific structured input for the model to guide text generation. We additionally note that although there are prompt-based relation extraction models, we are performing a different task which is contextual commonsense inference. Another recent contribution, P-Tuning, shares similarities with our approach by combining trainable continuous prompt embeddings with discrete prompts. Both hinting and P-tuning share the overarching objective of enhancing prompt learnability. PTR, or prompt-tuning with rules, shares similarities with hinting as it involves encoding prior knowledge about tasks and classes. Similarly to PTR, hinting also introduces additional information to provide the model with contextual understanding of the relationships between words."}, {"title": "2.1 Prompting", "content": "Recently, there has been a shift in paradigm in Natural Language Processing from pre-training and fine-tuning a model, to pre-training, prompting, and predicting. One primary reason for this shift is the creation of ever-larger language models, which have become computationally expensive to fine-tune. Prompting can be described as converting a pre-trained language model input sequence into another sequence that resembles what the language model has seen during pre-training. Overall, most prompting research is focused on formulating the task as a cloze (fill-in-the-blanks) task. However, we consider the task of language generation, an open-ended formulation.\nRecall that prefix prompting modifies the input to a language model, by adding either a hard prompt (additional words to the input sequence) or a soft prompt (i.e., adding trainable vectors that represent, but are not equivalent to, additional words).\nUnlike classic prefix prompting, hinting uses both hard and soft prompts. The soft prompts are in the form of symbols that represent the different parts of the assertion (i.e., subject, relation type, and object), and the hard prompts are in the form of the actual parts of the assertion that are selected to be appended as part of the hint. Our work is similar to KnowPrompt, except that they use a masked language model and soft prompts for relationship extraction. AutoPrompt is also similar, but finds a set of \"trigger\" words that give the best performance on a cloze-related task, whereas we provide specific structured input for the model to guide text generation. We additionally note that although there are prompt-based relation extraction models, we are performing a different task which is contextual commonsense inference. Another recent contribution, P-Tuning, shares similarities with our approach by combining trainable continuous prompt embeddings with discrete prompts. Both hinting and P-tuning share the overarching objective of enhancing prompt learnability. PTR, or prompt-tuning with rules, shares similarities with hinting as it involves encoding prior knowledge about tasks and classes. Similarly to PTR, hinting also introduces additional information to provide the model with contextual understanding of the relationships between words."}, {"title": "2.2 Controllable Generation", "content": "Controllable generation can be described as ways to control a language model's text generation given some kind of guidance. One work that tries to implement controllable generation is CTRL. The authors supply control signals during pre-training of a general language model. A body of work in controllable generation has focused on how it can be used for summarization. Representative work that uses techniques similar to ours is GSum.\nIn contrast to GSum, our method is model independent, allows for the source document to interact with the guidance signal, and contains soft prompts in the form of trainable embeddings that represent the parts of a tuple. The GSum system gives interesting insight into the fact that highlighted sentences, and the provision of triples, does in fact help with the factual correctness of abstractive summarization. We make the distinction that hinting falls more under prompting for the reason that we utilize additionally the trainable soft embeddings rather than purely additional hard tokens and that our task of contextual commonsense generation is not explored in the controllable generation works, whose main focus is on controlling unstructured text generation. Some works that are in this area are also who utilize what they call \"control factors\" as keywords or phrases that are supplied by a human-in-the-loop to guide a conversation."}, {"title": "2.3 Discourse-aware/Contextual commonsense inference", "content": "Commonsense inference is the task of generating a commonsense assertion. Discourse-aware/contextual commonsense inference is the task of, given a certain narrative or discourse, inferring commonsense assertions that are coherent within the narrative. This task is particularly hard because commonsense knowledge may not be explicitly stated in text and the model needs to keep track of entities and their states either explicitly or implicitly. Research into the knowledge that pre-trained language models learn has yielded good results in that they do contain various types of factual knowledge, as well as some commonsense knowledge. The amount of commonsense knowledge in these models can be improved by supplementing sparsely covered subject areas with structured knowledge sources such as ConceptNet.\nKnowing that these pre-trained language models may contain some commonsense information has led to the development of knowledge models such as COMET. This line of research has been extended from the sentence-by-sentence level in COMET, to the paragraph-level in ParaCOMET. Contemporaneously, GLUCOSE builds a dataset of commonsense assertions that are contextualized to a set of stories, and generalized. More recently, the idea of knowledge models or models that can be leveraged to generate commonsense assertions has been gaining track. One recent approach has been kogito. Kogito is a toolkit for commonsense inference, which permits training and access of similar to COMET, along with providing tools for selecting a subject and a relation. Kogito utilizes the same formulation as the ParaCOMET work, in which a subject and a relation are provided. However, kogito does not tackle the more complicated general commonsense inference as in GLUCOSE. With our work, we could provide kogito with a framework to be able to train models that can perform this type of controllable, generalized inference and improve the overall training."}, {"title": "3 Modeling", "content": "We now detail the task of Contextual Commonsense Inference. We are given a story $S$ composed of $n$ sentences, $S = {S_1, S_2, ..., S_n}$, a target sentence from that story, $S_t$, where $S_t \\in S$, and a dimension/relation type $R$. Given all this, we want to generate a tuple in the form of $(subject, R, object)$ that represents an assertion, present or implied, in $S_t$ given the context $S$, and the relation type $R$.\nWe run tests with two variations of this task, one is the ParaCOMET variation and the other the GLUCOSE variation. In the ParaCOMET experiments, we represent $S_t$ with a special token. Additionally, we only generate the object of the tuple. In our GLUCOSE experiments, we represent $S_t$ by marking it with * on the left and right of the actual target sentence. Additionally, we generate at most two subject, R, object tuples: one that is the context-specific tuple, and the other is the general tuple, separated by two asterisks (**). An example of how the inputs look for both datasets can be seen earlier in the Introduction."}, {"title": "3.1 Task", "content": "We now detail the task of Contextual Commonsense Inference. We are given a story $S$ composed of $n$ sentences, $S = {S_1, S_2, ..., S_n}$, a target sentence from that story, $S_t$, where $S_t \\in S$, and a dimension/relation type $R$. Given all this, we want to generate a tuple in the form of $(subject, R, object)$ that represents an assertion, present or implied, in $S_t$ given the context $S$, and the relation type $R$.\nWe run tests with two variations of this task, one is the ParaCOMET variation and the other the GLUCOSE variation. In the ParaCOMET experiments, we represent $S_t$ with a special token. Additionally, we only generate the object of the tuple. In our GLUCOSE experiments, we represent $S_t$ by marking it with * on the left and right of the actual target sentence. Additionally, we generate at most two subject, R, object tuples: one that is the context-specific tuple, and the other is the general tuple, separated by two asterisks (**). An example of how the inputs look for both datasets can be seen earlier in the Introduction."}, {"title": "3.2 Hinting", "content": "The mechanism we present in this work, called hinting, is a kind of mixed/hybrid prompting for generative language models. Prompting is essentially supplying additional text (i.e. prompts) to a language model to aid/guide it in a specific task. In our case, we opt to give a \"hint\", as to what the assertion that we want to predict contains, at the end of our input text. We chose placing the hint at the end of the input for simplicity in dataset processing, but it can be placed anywhere and we"}, {"title": "3.3 An example of Hinting", "content": "A simple example of hinting is the following:\nStory: The hockey game was tied up. The red team had the puck. They sprinted down the ice. They cracked a shot on goal! They scored a final goal!\nTarget sentence: They scored a final goal!\nTarget assertion: (subject: the red team, relation: are capable of, object: winning the game.)\nA hint can be any permutation of the target assertion, except the complete assertion, along with some symbol that indicates which part it is:"}, {"title": "3.4 Hinting with Synonyms and Antonyms", "content": "To explore other ways of hinting, we devised another technique by swapping parts of the hint with synonyms and antonyms. We used the synsets and antsets from the WordNet knowledge base to find synonyms and antonyms respectively. In the case that there is no viable synonym or antonym we keep the original word. To introduce synonyms and antonyms, the process is as follows. When a hint is generated, we copy it and we decide whether to supply it with synonyms and/or antonyms by swapping the words in the hint prompt with their synonyms or antonyms by sampling from a binomial distribution (we explored various values for p, with 0.5 being the most effective) to control the frequency of hints with synonyms/antonyms. The reasoning for this is that if we only supply synonyms/antonyms, we lose the control that hinting provides, so we introduce synonyms and antonyms at a certain rate to have the model still be controllable, while seeing \u201cnew\u201d information through the synonyms/antonyms. After the swapping process, we insert a soft prompt, <|syn|> or <ant|>, at the end of the substituted prompt to signal that the model is using a synonym or an antonym respectively."}, {"title": "3.5 Models", "content": "For our first set of experiments, we utilize the ParaCOMET dataset and the framework with the same GPT-2 model as ParaCOMET, along with a T5 model to observe the effects of hinting in a sequence-to-sequence formulation of the dataset. We use the off-the-shelf pre-trained \"base\" version of these models for efficiency. For our second set of experiments with the GLUCOSE dataset, we also used the T5 model, as was done in GLUCOSE."}, {"title": "4 Experimental Setup", "content": "To show the effectiveness of hinting we use the following setups. First we utilize the original ParaCOMET dataset and setup and adding hints with/without synonyms and antonyms . The ParaCOMET setup consists of given a story $S$ composed of $n$ sentences, $S = {S_1, S_2,..., S_n}$, a relation type $R$, and a target sentence token (i.e. <|sent0|>, <|sent1|>, ..., <|sent(n-1)|>). In the ParaCOMET dataset, we must predict the object of a triple, utilizing implicitly the sentence as a subject and explicitly the supplied sentence symbol and relation $R$ symbol.\nWithin this framework, after the relation $R$, we add our hint between parenthesis (i.e. \u201c([hint])", "of": "a subject symbol (<|subj|>) along with the target sentence to serve as a subject, a relation symbol (<|rel|>) along with the relation $R$, or an object symbol (<obj|>) along with the object of the triple. Using the hockey example a possible hint in this set of experiments would be: \"(<|rel|> <|xEffect|>,<|obj|> they win the game)\". In the case that we add a synonym and/or antonym, we do the appropriate replacement and add the <|syn|> or <|ant|> tags. It is possible, although we leave for future work, to include both the hint and the synonym/antonym hint as part of the prompt.\nIn our GPT-2 experiments, we utilize the same cross-entropy loss as in . We note that we also utilize a sequence-to-sequence formulation for the T5 model. This in contrast to the GPT-2-based system requires encoding a source sequence (i.e., story, target sentence, and relation symbol), and decoding it into a target sequence (i.e., the object of an assertion). For the T5 model, we add the prefix \"source:\" before the story $S$, and the prefix \"hint:\" for placing our hints. In addition, whenever there is a synonym or antonym added, it is added as another prefix (\"synonym:[synonym]\" or \u201cantonym:[antonym]", "heuristic\" dataset as ParaCOMET which utilizes a heuristic matching technique to align ATOMIC triples to story sentences.\nSecondly, we utilize the formulation utilized in GLUCOSE. The formulation utilizes the T5 model in a sequence-to-sequence formulation once more. In this formulation, the source text is composed of a prefix of a dimension to predict $D \\in {1, 2, . . . 10}$, followed by the story $S$ with the marked target sentence. The target sentence, $S_t$, is marked with \"*\" before and after the sentence. An example input is": "1: The first sentence. *The target sentence. * The third sentence.\". This task is slightly different from the ParaCOMET one, in that in addition to predicting a context specific triple, the model has to predict a generalized triple.\nIn the GLUCOSE task we have to infer both general and context specific subject, object and relation elements. For our hints we provide up to five out of these six elements while training, along with a symbol that represents whether it is the subject, object or a relation, and another symbol that represents whether it is part of the general or specific assertion. We add our hint after the story $S$, utilizing the prefix \"hint:\" and supplying the hint between parenthesis. Given the hockey story, an example of a hint for GLUCOSE can be: \u201c(<|general|><|obj|> People_A win a Something_A)\". Hyperparameter configuration details can be seen in the Appendix.\nThirdly, for testing the controllability of the model, we train 5 models on the GLUCOSE data: one without hints, one with hints, one with hints and synonyms, one with hints and antonyms, and one with hints, synonyms, and antonyms. To test the control, we make a synthetic test in which we utilize the GLUCOSE testing data, and supply the model with hints for the specific subject and/or relation and/or the general subject and/or relation. The test measures the overlap between the elements provided in the hint and the elements present in the output. We note that this is a synthetic benchmark which demonstrates that the model is capable of incorporating the hint into its output accordingly.\nLastly, we ran a tiny Mechanical Turk study similar to the one presented in the original ParaCOMET in which a human judges the plausibility of the generated assertion based on the\""}, {"title": "5 Results and Analysis", "content": "The aggregated results for this set of experiments can be found in Table 2. We can see here that on average, hinting does tend to improve the score even if slightly. It seems that providing a hint is beneficial and not detrimental for contextual commonsense inference. Given the way that this task is framed, a possibility that could explain the relative similarity of the performances, is that hinting in this formulation only adds the object of the triple as additional possible data that the model may see during training; the subject and the relation can be repeated with hinting. We note that the performance of the T5 model was less, and we believe that it may be lack of hyperparameter tuning, as it was seen that the model was sensitive to the learning rate and had to use a higher than usual learning rate. In these tests, we also note that hinting with synonyms tends to consistently improve the performance even further than just plain hinting, indicating that the model benefits from making associations of related concepts. We see that hinting with antonyms also tends to be beneficial, but the benefit is not as consistent as with synonyms. Interestingly, we see that combining both synonym and antonym training does not bring the best of both worlds, but more closely an average between the performance of hinting with either."}, {"title": "5.1 Experiment 1: ParaCOMET with hints", "content": "The aggregated results for this set of experiments can be found in Table 2. We can see here that on average, hinting does tend to improve the score even if slightly. It seems that providing a hint is beneficial and not detrimental for contextual commonsense inference. Given the way that this task is framed, a possibility that could explain the relative similarity of the performances, is that hinting in this formulation only adds the object of the triple as additional possible data that the model may see during training; the subject and the relation can be repeated with hinting. We note that the performance of the T5 model was less, and we believe that it may be lack of hyperparameter tuning, as it was seen that the model was sensitive to the learning rate and had to use a higher than usual learning rate. In these tests, we also note that hinting with synonyms tends to consistently improve the performance even further than just plain hinting, indicating that the model benefits from making associations of related concepts. We see that hinting with antonyms also tends to be beneficial, but the benefit is not as consistent as with synonyms. Interestingly, we see that combining both synonym and antonym training does not bring the best of both worlds, but more closely an average between the performance of hinting with either."}, {"title": "5.2 Experiment 2: GLUCOSE with hints", "content": "The aggregated results for this set of experiments can be found in Table 3. Once more we notice that hinting (with and without synonyms and/or antonyms) does tend to improve the performance of the contextual commonsense inference task. This suggests that hinting is indeed beneficial for the task of contextualized commonsense inference, especially when faced with the harder task of generating both a general and context dependent assertion. We believe that this improvement is because hinting gives the model the clues it may need to decide on what to focus or attend to, to generate useful inferences."}, {"title": "5.3 Experiment 3: Controllability", "content": "In Table 4 we see the results of our synthetic controllability test. We see that the model without hinting tends to predict about relevant things (indicated by the BLEU scores above 50), however when hints are injected, the model always predicts about what the hint was suggesting (indicated by the nearly perfect scores). We also see that supplying synonyms and antonyms does not decrease the controllability of the models."}, {"title": "5.4 Experiment 4: Human Judgements", "content": "The results for a tiny Mechanical Turk study for human evaluation of model inferences can be seen in Table 5. Overall we can see here that hinted systems are judged as slightly higher in plausibility. We also see upon looking some of the inferences that the hinted model tends to be more general and provide shorter responses than the non-hinted model (e.g., hinted inference: \"satisfied\" vs. non-hinted inference: \"happy and satisfied\")."}, {"title": "6 Discussion", "content": "From the results of our experiments, we can see that hinting tends to increase the performance of contextualized commonsense inference at least with regards to automated metrics and does not significantly degrade or improve human judgements. This brings the question of: Why hint at all? The primary reason is for controllability in the generation. By supplying these hints, we are teaching the model pay attention and generate inferences about a certain subject, relation, or object. This in turn, after training, can be leveraged by a user or downstream application to guide the model to generate assertions from parts that are manually supplied. Although this is not very clear within the ParaCOMET formulation, it becomes clearer in the GLUCOSE formulation of the problem. We give an illustrative example of the usefulness of hinting in Table 1. We can see that by giving a model the hint, the model could be capable of inferring about information that may not be present in the story. We note that this behavior is useful in downstream tasks such as story understanding and contextual knowledge graph generation in which we may need a model to have a specific subject or object. Lastly, hinting was designed to be simple to implement, and is model independent."}, {"title": "6.1 Why hint?", "content": "From the results of our experiments, we can see that hinting tends to increase the performance of contextualized commonsense inference at least with regards to automated metrics and does not significantly degrade or improve human judgements. This brings the question of: Why hint at all? The primary reason is for controllability in the generation. By supplying these hints, we are teaching the model pay attention and generate inferences about a certain subject, relation, or object. This in turn, after training, can be leveraged by a user or downstream application to guide the model to generate assertions from parts that are manually supplied. Although this is not very clear within the ParaCOMET formulation, it becomes clearer in the GLUCOSE formulation of the problem. We give an illustrative example of the usefulness of hinting in Table 1. We can see that by giving a model the hint, the model could be capable of inferring about information that may not be present in the story. We note that this behavior is useful in downstream tasks such as story understanding and contextual knowledge graph generation in which we may need a model to have a specific subject or object. Lastly, hinting was designed to be simple to implement, and is model independent."}, {"title": "6.2 Is hinting optimal?", "content": "This work was a proof of concept for this technique. We acknowledge there is a large body of research on the area of prompting. The way the hinting mechanism was designed however, leaves much space to explore alternate mechanisms such as AutoPrompt, including additional soft prompts such as those in and , or even replacing the contents of the hint with synonyms or related words. Because of the naivety of the approach, we do not think it is an optimal approach, and there is a large body of research that points to manual templating of prompts being less effective than learned prompts. However, from our tests, our approach does not degrade performance, and only improves it."}, {"title": "7 Conclusion", "content": "In this work we presented hinting, a simple hybrid prompting mechanism that consists of appending parts of a target tuple into an input sequence for the task of contextual commonsense inference. We showed that hinting tends to improve performance in automated metrics and provides comparable performance with human-based judgements. With this, we open the doors for exploring prompting within the realm of contextual commonsense inference.\nThe hinting system design acknowledges areas for improvement, particularly in developing smarter strategies for selecting when and what to hint, and enhancing the hint with additional soft prompts. Future work and exploration is further described in the Appendix F."}, {"title": "8 Ethics Statement", "content": "In this work, we propose a mechanism called \"hinting\" to create a controllable contextual commonsense inference model. Our goal is to improve the usability of contextual commonsense inference models in downstream applications. However, it's important to note that our mechanism may be subject to limitations due to biases existing in the knowledge bases used (e.g., ATOMIC, and GLUCOSE).\nWhile our model is controllable to some extent, it could potentially generate harmful or incorrect assertions as a result of the conditioned biases. Additionally, since our model generates text, it might produce erroneous statements. We did not analyze the degree of these biases or incorrect inferences in this study; however, we utilize well-vetted knowledge bases which should minimize any significant negative impacts on performance."}, {"title": "9 Limitations", "content": "We note that our work has some limitations. One of these is the length of the stories that were given for the task of contextual commonsense inference. Many of these stories are around 5 sentences long. This may in turn harm the generalization of the effectiveness of this technique to longer stories. We also note that this technique was designed for language models that are not extremely large (>7B parameters) which have recently shown their effectiveness in a wide variety of tasks, however, since these are prompts, the hinting technique can be utilized as part of example prompts for these extremely large models. Lastly, one limitation of this method is that the inferences that are produced have no way of being evaluated on their relevance and truthfulness. This is to be addressed in future work with a classifier for these assertions."}, {"title": "F Future Work", "content": "When designing the hinting system certain aspects were formulated to leave space for improvements. One such area is finding a smarter way of selecting when to hint, and finding a smarter way of selecting what to hint. Additionally, more soft prompts could be added to the hint such that they would learn a better virtual template.\nAnother area to explore is providing deeper ablation studies to determine what parts of the hint are more effective and when. This work is more a proof-of-concept that hinting, or more broadly prompting, is useful towards the task of contextual commonsense inference.\nExploring further, another approach, chain-of-thought prompting , is commonly utilized in very large language models-a practice that does not extend to the smaller models within our scope. Considering this, we could explore an analogous approach, such as chain-of-hinting, an adaptation that may enable us to incorporate multi-hop graph reasoning for contextual commonsense inference.\nFurthermore, given that models trained with hinting for contextual commonsense inference can be guided by the information supplied in hints, such models can be utilized in a variety of downstream applications such as story understanding and contextual knowledge graph generation."}]}