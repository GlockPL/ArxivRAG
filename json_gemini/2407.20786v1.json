{"title": "Be aware of overfitting by hyperparameter optimization!", "authors": ["Igor V. Tetko", "Ruud van Deursen", "Guillaume Godin"], "abstract": "Hyperparameter optimization is very frequently employed in machine learning. However, an optimization of a large space of parameters could result in overfitting of models. In recent studies on solubility prediction the authors collected seven thermodynamic and kinetic solubility datasets from different data sources. They used state-of-the-art graph-based methods and compared models developed for each dataset using different data cleaning protocols and hyperparameter optimization. In our study we showed that hyperparameter optimization did not always result in better models, possibly due to overfitting when using the same statistical measures. Similar results could be calculated using pre-set hyperparameters, reducing the computational effort by around 10,000 times. We also extended the previous analysis by adding a representation learning method based on Natural Language Processing of smiles called Transformer CNN. We show that across all analyzed sets using exactly the same protocol, Transformer CNN provided better results than graph-based methods for 26 out of 28 pairwise comparisons by using only a tiny fraction of time as compared to other methods. Last but not least we stressed the importance of comparing calculation results using exactly the same statistical measures.", "sections": [{"title": "Introduction", "content": "The prediction of water solubility is crucial for different chemistry applications and has been a challenge for computational studies since the 1880th. The field is actively developing, and new models and approaches to predict this important property continue to be regularly published. Recently, the first EUOS/SLAS challenge for the prediction of solubility classes measured by nephelometry assay was organized on Kaggle. Since the applicability domain of models critically depends on data, the studies reporting new large datasets with solubility data, such as AqSoIDB, are of considerable interest to the research community. That is why the recent article by Meng et al., which reported on the collection of large sets of solubility values, while also mentioning a significant drop in the RMSE due to the reported data curation, attracted our attention. One of the important methodological approaches reported in the article was the use of a hyperparameter optimization procedure which required a lot of computational power. Therefore, despite the availability of the authors' scripts, the reproduction of results reported by the authors proved to be very challenging due to very high computational demands required to perform the hyperparameter optimization. We were interested in whether similar or better results could be obtained using a moderate amount of computational resources (a level of which would typically be available in academic settings) and whether use of hyperparameter optimisation was really critical for this study.\nTherefore in this work, we critically re-analyzed the previously reported results using the relatively modest computational resources that were available to us.\nOur main contributions are the following:\n\u2022 We show that hyperparameter optimisation may not provide an advantage over using a set of pre-optimised parameters and may be also contribute to overfitting\n\u2022 We demonstrate that the efficiency of TransformerCNN is comparable to graph methods ChemProp and AttentiveFP\n\u2022 We make a clear distinction between cuRMSE and standard RMSE function and highlight the importance of using the same statistical measure when comparing results"}, {"title": "Data", "content": "The authors collected seven datasets, as summarized in Table 1. In the article, there were three versions of the sets dubbed as \"original\u201d (\u201cOrg\u201d), \u201ccleaned\u201d (\u201cCln\u201d) and \"curated\u201d (Cur)."}, {"title": "Original sets (\"Org\")", "content": "The \"original\" sets collected data retrieved by the authors from the respective data sources, as reported in Meng et al. Some of the original sets contained duplicates. For example, On-line CHEmical database and Modelling environment (OCHEM) has a policy of collecting data as it is published in the original articles. This policy allows for easier reproduction of the respective studies. Thus, if some data were repeatedly reused in other publications, many of the records would likely be duplicated. The original Huuskonen data set and its curated version (AQUA set in Table 1) were re-used in practically all publications on the prediction of solubility and made up major parts of the PHYSP, ESOL and OCHEM sets in particular. Moreover, since the data in this set were extracted from the AqSolDB database, they were also present in the AQUASOL set. AQUASOL set, as any other set in this article, is a collection of data from multiple sources. It also contained a number of single heavy atom molecules, such as [CH4], [Mg2+], [Mo], [Re], [H+].[F-] etc., or inorganic complexes [O-2].[O-2].[Mg+2].[Ca+2], [AI+3].[Cl-].[Cl-].[Cl-], etc., which could not be processed by graph-based neural networks due to the fact that there are no bonds between heavy atoms set for those objects to apply graph convolution or/and these atom/molecule/compound types are not supported. The authors removed duplicates and metals during the cleaning and standardization procedure to create \"clean\" and \"curated\u201d sets as described below, and the 192 metal-containing compounds that remained in the \u201cOrg\u201d set were excluded as reported in Table 1."}, {"title": "Clean sets (\"Cln\")", "content": "The cleaning procedure (described in Meng et al.) included SMILES standardization using MoIVS followed by the removal of duplicates (only when the difference between values for the same molecules in two records was less than 0.01 log unit), removing records that followed non-standard experimental protocols (temperature 25 \u00b1 5\u00b0C, pH 7 \u00b1 1) as well as the removal of compounds containing metals.\nOCHEM uses the InChi keys to index molecules as well as rounded (0.01 log unit) property values to calculate unique keys. Using such keys, we identified and eliminated a few additional exact duplicates for \u201cCln\u201d thermodynamic solubility sets, including molecules containing metals, which were initially not detected by the authors (Table 1).\nThe data for kinetic solubility (\u201cKINECT\" dataset) were all downloaded from the OCHEM database. However, the authors did not initially notice that the majority of these records in OCHEM originally came from the PubChem AID1996 assay. OCHEM contains the original data uploaded from this assay as well as data from two articles that used this assay in their studies. These three sources contributed a total number of 161710 records out of 164273 records in the \u201cKINECT Org\u201d set. The different processing procedures (salt elimination, neutralization, aromatisation, etc.) in the aforementioned studies, resulted in different chemical structures. Hence, after the deduplication procedure the authors of this study obtained 82057 records instead of the original 57858, i.e., there were 24199 duplicated measurements. Such a high value of duplicates (>37%) could imply a biased estimation accuracy of the developed models for kinetic solubility.\nThe CHEMBL set had 30099 values reported in the article but 30099 and 31099 records were made available in the repository for \u201cOrg\u201d and \u201cCln\u201d sets, respectively. For the \u201cCln\u201d set, we used 31099 records as provided in the repository (chembl_stand_clean.csv file). The differences in the number of molecules in the datasets could contribute some changes in calculated statistical parameters.\nIn the case of duplicated records for analyzed molecules within the same set, the authors used a weighting to avoid its overrepresentation during learning (so-called \u201cinter-dataset curation\u201d). The authors assigned each record a weight, inversely proportional to the total number of records per molecule. Thus each molecule had a total weight equal to \"1\"."}, {"title": "Curated sets (\"Cur\")", "content": "The authors (Meng et al.) also produced curated datasets. The authors first assigned weights to datasets corresponding to their quality (high quality: AQUA, PHYSP, ESOL with weight \u201c1.0\u201d and OCHEM with weight 0.85 as well as low quality: AQSOL with weight 0.4 and CHEMBL with weight 0.8), which was determined manually based on the performance of ChemProp. In the first step of this analysis, records were assigned a weight based on the sets they originated from. After that, the authors extended each analyzed set with records for the same molecule found in other sets. In cases where several solubility values were present for the same molecule, and the differences between their values were less than d=0.5 log units (corresponding to the estimated experimental accuracy of solubility measurements), their values were merged and the weight of the merged record was updated. Otherwise, records were kept with the weights assigned according to their respective datasets. For more details about this procedure, we refer to the original article by Meng et al. As a result of this procedure, new solubility sets with weights for each record were obtained and provided by the authors, which were named \"curated\u201d (Cur) datasets.\nAccording to the authors, this procedure incorporates a measure of the accuracy of individual records into the analysis and thus, possibly increases the quality of the developed models. The authors mentioned that \u201csearching for and evaluating a better weight assignment requires extremely large compute power\u201d and as such no attempt to search for optimal weights based on model performances was made."}, {"title": "Statistical parameters", "content": "One of the traditional statistical parameters used to estimate the accuracy of models is RMSE (eq 1)\n$RMSE = \\sqrt{\\sum_{i=0}^{n-1} (y_i - \\widehat{y}_i)^2 / n}$ (1)\nHowever, the authors modified RMSE to produce an ad hoc \u201ccurated RMSE\u201d (cuRMSE), which used the weights of records to estimate the performance of models (see eq 2)\n$CuRMSE = \\sqrt{\\sum_{i=0}^{n-1} \\omega_i * (y_i - \\widehat{y}_i)^2 / n}$ (2)\nIn both formulas $\\widehat{y_i}$ are predicted values (which were averaged across several models to improve model accuracy, as stated by Weng et al.) and $y_i$ are experimental values.\nThe cuRMSE is an interesting loss function for training of neural networks, since it decreases the impact of some individual data points. However, this measure depends on the distribution of weights and could provide a bias for the comparison of model performance across different sets. Indeed, it decreases errors for molecules that have more than one value in the dataset. For example, if a molecule has two records, each with the same difference between predicted and experimental values, e.g., 0.6, and each record has the same weight 1/2 = 0.5 for all records (as seen in \"Cln\u201d datasets to account for inter-set redundancy), its cuRMSE will be sqrt((0.5*0.6*0.6 + 0.5*0.6*0.6)/2)=0.3, i.e., its cuRMSE will be artificially halved in comparison to the RMSE for the same molecule. Assignment of small weights to records from datasets with high errors may have an even more pronounced effect."}, {"title": "Methods", "content": "In this section, we examine whether or not hyperparameter optimization provides a significant improvement in the performance of analyzed methods."}, {"title": "Analyzed methods", "content": "Attentive FingerPrint (AttFP), ChemProp and Transformer CNN methods were used for the analysis. The first two methods were based on keras GCNN (KGCNN) repository code. While a ChemProp implementation was available, we preferred to use the KGCNN implementation since it allowed for better control as well as optimization of several hyperparameters by testing the algorithm against datasets from our previous study. The implementation of Transformer CNN is available at. All three methods are available as part of openOCHEM software. Both AttFP and ChemProp are generally among the Top 5 best graph models applied to physical property predictions, including 3D graphs. However, because both methods are based on RDKit, some failed on compounds that could not be processed by this package or by each method. The Transformer CNN is a non-graph method based on the Transformer architecture, which analyzes the representation of compounds as SMILES strings. This method was added for comparison with graph-based architecture. All results of this study are available as  and instructions on how to reproduce the calculation results (and/or assess development models) are provided on the github.\nMeng et al. performed an extensive optimization of hyperparameters using a large GPU cluster. In particular, the authors noted that one round of calculations (solely for results of the ChemProp method) reported in Table 2 of their article (half of which are listed in Table 3 below) took approximately two weeks on a cluster with 1200 compute nodes (38,200 cores and 4800 GPU accelerators). Thus it required >1.8M (4800*24*16) hours of calculations on GPU cards. Moreover, the authors stated that they could not produce results with AttFP for the CHEMBL and KINECT sets since the calculations would be too time-consuming. Since we did not have access to such a powerful cluster, we decided to skip parameter optimization and use default hyperparameters provided by OCHEM developers for the respective methods and implemented in openOCHEM. The employed hyperparameters were selected based on analysis of several small sets used in our previous study. We performed our analysis on a communal cluster with 16 GPU cards, usually running two tasks simultaneously as it was typically faster than running only one task per card. The training of all three models in Table 3 typically required less than 6 hours on all available cards (ca 100 GPU hours), with the exception of the largest \u201cKINECT Org\u201d set which required about 100 GPU hours for the development of models with three analyzed methods. Assuming that the GPU cards we used (Nvidia GeForce RTX 2070, Titan V, Tesla V100) had similar specifications to those used by Meng et al. (the authors did not specify which cards were used for production, but information at  indicated they were likely Nvidia Tesla C2050), all results reported in this study would require about 10,000 times less computational power, i.e., less than 3 minutes of calculations on their GPU cluster.\nThe workflow for molecule processing in OCHEM included standardization, de-salting (keeping the major fragment), and neutralization. We noticed that graph-based methods failed for some compounds in some sets (e.g., AQSOL, CHEMBL, KINECT) because of an RDKit error after the neutralization of molecules. For smaller sets, we noticed that both results of the methods were similar, both with and without neutralization. Therefore for these two methods, the neutralization step was skipped. The partitioning of data on folds (see below) was the same for all three analyzed methods, which allowed direct comparison of results of models developed with different algorithms."}, {"title": "Validation protocols", "content": "The authors Meng et al used random and scaffold partitioning [0.8,0.1,0.1] for training, testing and evaluation, which was repeated five times. We decided to analyze results obtained with random partitioning, since the issues we encountered are also relevant to partitioning based on scaffold splitting. The protocol of the authors was very similar to 10-fold cross-validation (10-CV), which was used in this study for all results reported in openOCHEM. Indeed, in both protocols, 10% of data were excluded from model training and these data were predicted once hyperparameter optimization was finished. OCHEM 10-CV provides prediction for all the data, as in any CV approach. The internal procedure in OCHEM splits data into internal training (81%), early stopping (9%) and evaluation sets (10%) corresponding to [0.81,0.09,0.1] split, which is practically the same as the [0.8,0.1,0.1] split used by the authors.\nThe authors, however, did not run 10-CV to obtain predictions for all data, opting instead for five random data splits. Because of the random split, less than 50% of the data was used to calculate the reported model performance in the evaluation set. This procedure should provide very similar results to the 10-CV. Also, the authors generated an ensemble of eight models for each split and took their average to improve their results. Unfortunately there was no quantitative comparison to estimate how much improvement was provided by ensembling. The apparent disadvantage of the authors' procedure, besides the fact that it does not predict all data, is that it is more computationally expensive, i.e. 5 x 8 = 40 models are developed compared to 10 models required in OCHEM. The eight models were used to calculate confidence intervals which, in our opinion, come at too high a computational price compared to simple bootstrap procedure used in OCHEM. OCHEM splits data using the non-stereochemical part of the InChi hash key (to ignore stereochemistry) rather than with canonical SMILES, which was used by the authors. The OCHEM bootstrap procedure is more reliable since its splits are insensitive to possible errors related to the stereochemistry of the analyzed molecules.\nHowever, the validation procedures used in Meng et al. and this study are very similar and, importantly, both use 90% of data to develop models to predict the respective validation sets used to estimate model performance. This allows for a direct comparison between the results of this and the prior study."}, {"title": "Results", "content": "In order to clarify the effect of hyperparameter optimization and ensemble averaging used by the authors, we compared our results to the published results from Meng et al. using 10-fold CV for \"Org\" sets (Table 3). The bootstrap procedure estimated the confidence intervals for results reported in this study, as described elsewhere. This procedure, in general, provided smaller confidence intervals, which could be attributed to the fact that it used data from the whole set compared to the analysis in ref which used about 50% of data. The additional variance in the results of Meng et al. could be due to hyperparameter selection. The confidence intervals for both analyses decreased with the dataset size."}, {"title": "Analysis of performance of models for \"Org\" sets", "content": "In order to clarify the effect of hyperparameter optimization and ensemble averaging used by the authors, we compared our results to the published results from Meng et al.10 using 10-fold CV for \"Org\" sets (Table 3). The bootstrap procedure estimated the confidence intervals for results reported in this study, as described elsewhere.20 This procedure, in general, provided smaller confidence intervals, which could be attributed to the fact that it used data from the whole set compared to the analysis in ref\u00b9\u00ba which used about 50% of data. The additional variance in the results of Meng et al.10 could be due to hyperparameter selection. The confidence intervals for both analyses decreased with the dataset size."}, {"title": "Analysis of results for \"Cln\u201d dataset", "content": "The following analysis was performed to evaluate the effect of intra-set curation on the performance of developed models.\ncuRMSE values were reported by the authors in their article for the \u201cCln\u201d and \u201cCure\u201d sets. As mentioned, this measure may not allow a faithful comparison of model performances across different sets. Since weights were available for both \u201cCln\u201d and \u201cCure\u201d sets, we used them to calculate cuRMSE to compare models from this study with those previously published by the authors. To this end, we first developed models using the default 10-CV procedure of openOCHEM (split by molecule) using \"Cln\u201d data. Then, values provided by the cross-validation procedure were used to calculate cuRMSE using eq (2). We excluded the KINECT set from this analysis since it had a different number of records after our curation, but still included its results using openOCHEM for model comparison.\nThe cuRMSE was not significantly different from RMSE for all \u201cCln\u201d sets, with the exception of OCHEM and CHEMBL, which had the largest numbers of molecules with several measurements in the dataset.\nPerformances of methods developed in this study without hyperparameter optimization provided lower cuRMSE in 7 of 12 pairwise analyses (Table 4). Only AQSOL results using AttFP calculated by Meng et al.10 had a lower cuRMSE than the results of this study."}, {"title": "Analysis of results for \"Cur\u201d dataset", "content": "In the final analysis, we compared the effect of intraset data curation on model performance. For this analysis, we reused cross-validation results from the \"Cln\u201d set and re-calculated cuRMSE using weights and experimental values provided by the authors (predictions for the weighted set were taken from 10-fold CV and molecules in both sets were matched using InChi).\nOnly the PHYSP set results calculated in this study had higher RMSE than those reported by Meng et al. (Table 5). However, as previously mentioned, the PHYSP set for \u201cCln\u201d and \u201cCur\u201d studies was exactly the same, so the results reported for \u201cCur\u201d could be biased.\nIf we exclude this set, results calculated in this study with default hyperparameters had lower cuRMSE in 6 out of 9 cases compared to those reported by the authors. These results confirm our previous finding that models that have undergone hyperparameter optimization did not yield better results than models using a fixed set of pre-optimised hyperparameters, as investigated in this study."}, {"title": "Discussion", "content": "As we were intrigued by the high performance of models reported by the authors, we sought to investigate their proposed methodology in-depth and hoped to reproduce their results. However, since we did not have access to the same level of computational power as the authors, we analyzed the results calculated without hyperparameter optimization.\nFirst, our analysis of the original sets (\u201cOrg\u201d) showed that the methods used in this study gave rise to similar RMSE values to those reported by the authors, despite their extensive hyperparameter optimization.\nMoreover, another analysis of results from the \u201cCln\u201d set (results for \u201cCur\u201d were obtained from those for \"Cln\u201d) also showed that hyperparameter optimization did not provide consistent improvement compared to the use of a fixed set of pre-optimized hyperparameters. While hyperparameter optimization is recommended for some of algorithms used in this study, the availability of a cluster with hundreds GPU cards is a luxury rather than a typical situation for researchers and the use of such clusters would go against the Green Al principles. Indeed, all results performed in this study required >10,000 times fewer computational resources used by Meng et al. and even provided better performance in 18 of 34 pairwise comparisons. The lower performance of models after hyperparameter optimization could result from overfitting by hyperparameter selection. This problem is frequently underestimated but should be carefully addressed, especially when using heavy hyperparameter selection for small sets.\nAlthough the hyperparameter optimization method is appealing, its usefulness may be limited depending on the application. In particular, for many important biological (e.g, blood-brain barrier, toxicity, ready biodegradability) or physico-chemical properties (e.g., odor threshold), chemical datasets tend to be composed of a few hundred to at most 1000 data points. For such small datasets, we typically observe strong performance fluctuations between data splits. Thus, relying on one particular split can result in the selection of hyperparameters that are optimal for a specific split but not in general. To more fairly compare results of hyperparameter optimization, a full n-fold to be computed, which further increases the computational cost of hyperparameter optimization. This should do ideally with a repetition of n-fold splits to get statistically significant results. From these repeated experiments one may actually observe that there's not a single best solution, but rather a set of conditions defining the set of best available options. These sets of conditions may then be used to design an ensemble model to create the largest possible generalizability. For larger datasets, the split fluctuations are frequently within statistical error and typically representative of any other split (see Table 3 c : standard deviation decreasing with dataset size increasing), but it is where the cost of hyperparameter optimization explodes and makes this approach computationally expensive. More generally, we observed only a few marginal impacts of hyperparameter optimization on the RMSE performance.\nTypically, once methods like hyperperameter optimization become a standard feature in various commercial and open-source toolkits, they tend to be used blindly. We encourage the research community to benchmark their models via robust, automated protocols such as OCHEM to receive an unbiased assessment about their performances with smaller sets before starting to use them for more expensive experiments as described in Meng et al.. We have shown that data augmentation can provide a performance boost to models developed with limited data, like Transformer CNN models. Other alternatives, such as Gaussian Process and Normalizing Flows models could be evaluated in future studies.\nIn addition to ChemProp and AttFP models, we reported results from a Transformer CNN model, which in 26 of 28 cases provided a lower RMSE compared to both graph-based methods using the same cross-validation procedure and the exact same data splits. The same approach provided the highest individual score in Kaggle First EUOS/SLAS Joint Compound Solubility Challenge among 30 analyzed models.\nThe use of curated procedures employed by authors Meng et al. in most cases provided similar or lower performance compared to the use of datasets without any weighting for inter- and intra-set data curation procedures when using cross-validation. Thus additional studies may need to be provided to confirm the importance of the procedures proposed by the authors. We also warn that cuRMSE (which is also dependent on the weights of records) is generally not comparable to RMSE. The comparison of cuRMSE and RMSE Meng et al. in the same Tables created an impression that data curation decreased errors, which was not the case when exactly the same measure, cuRMSE using the same weighting, was used for comparison.\nWe have also identified that data curation procedures from refs, were applied to PubChem AID1996 assay resulted in 24199 duplicated records, which had either different structures or rounded values and thus could be treated as new data by Meng et al. A similar problem of data duplication could be relevant to any dataset, not just solubility datasets. The impact of such artificial data duplication on the performance of models needs to be correctly evaluated in separate studies.\nAlthough the authors made all their scripts publicly available as open source, their re-use is challenging due to the need to spend considerable time adapting them (e.g, scripts are linked to the directory structure of one of the authors; limited documentation etc.) as well as the extremely high computational costs required to perform these analyses. Unfortunately, the authors did not deposit their optimized hyperparameters, developed models, and calculated values, making the reproduction of their results extremely difficult, if not impossible. Thus we recommend that in the future, not only scripts, but also intermediate logs results should be reported, particularly for calculations that require extensive computational power to reproduce final results. The latter aspect will become more critical with the increase of computational resources required to repeat calculations."}, {"title": "Data Availability", "content": "The code used for model development can be found at . This study was carried out using publicly available data from the GitHub repository at The models and datasets generated during and/or analyzed during the study are exemplified at  and instructions to reproduce the models are at ."}]}