{"title": "Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning", "authors": ["Hao Ma", "Shijie Wang", "Zhiqiang Pu", "Siyao Zhao", "Xiaolin Ai"], "abstract": "Guiding the policy of multi-agent reinforcement learning to align with human common sense is a difficult problem, largely due to the complexity of modeling common sense as a reward, especially in complex and long-horizon multi-agent tasks. Recent works have shown the effectiveness of reward shaping, such as potential-based rewards, to enhance policy alignment. The existing works, however, primarily rely on experts to design rule-based rewards, which are often labor-intensive and lack a high-level semantic understanding of common sense. To solve this problem, we propose a hierarchical vision-based reward shaping method. At the bottom layer, a visual-language model (VLM) serves as a generic potential function, guiding the policy to align with human common sense through its intrinsic semantic understanding. To help the policy adapts to uncertainty and changes in long-horizon tasks, the top layer features an adaptive skill selection module based on a visual large language model (vLLM). The module uses instructions, video replays, and training records to dynamically select suitable potential function from a pre-designed pool. Besides, our method is theoretically proven to preserve the optimal policy. Extensive experiments conducted in the Google Research Football environment demonstrate that our method not only achieves a higher win rate but also effectively aligns the policy with human common sense.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL) has achieved great success in solving complex decision-making problems across lots of domains, including real-time strategy games, autonomous driving teams, robotic systems, and sports games. However, MARL faces a very challenging issue: it is difficult to learn policies that conform to the real-world cognition of the domain in complex tasks, especially with sparse rewards. Although state-of-the-art MARL algorithms can achieve high rewards in complex tasks with sparse rewards, their policies often diverges from human understanding. This discrepancy is reflected in the inconsistency between the learned policies in simulation environments and human common sense of the tasks. For instance, in Google Research Football (GRF) environment, researchers typically hope to learn impressive tactical combinations which can inspire tactical choices in the real world. However, existing algorithms tend to hack the environment by finding and exploiting unintended shortcuts or loopholes to achieve higher rewards, which limits the effectiveness in real-world applications of MARL. Therefore, it is a challenging issue that MARL algorithms is hard to learn policies which conform to human common sense. This issue mainly have three reasons: (i) Limitations of simulation environments. The complexity and diversity of the real world make it challenging to capture all relevant variables and dynamic changes. Thus, the inability to construct an environment model that is identical to the real world is an unavoidable problem. (ii) Difficulty in modeling human common sense. Human common sense is often formed based on their rich experiences and comprehensive cognition, making it difficult to express accurately through simple mathematical formulas or rules. Consequently, we find it challenging to quantify and model human common sense using manually designed conventional reward functions, especially in complex and long-horizon multi-agent tasks. (iii) Complexity and dynamic change of goals. In MARL, the typical objective for agents is to optimize specific and quantifiable performance metrics. However, human decision-making processes in complex tasks often involve dynamically changing goals across multiple stages. The simplified goal settings in traditional MARL fail to capture the complexity and diversity of human decision-making, resulting in a significant gap between the learned policies and human common sense. Some researchers leverage imitation learning or reward shaping to guide policies to align as closely as possible with human cognition. However, these methods have the drawback of relying on high-quality human behavior data and extensive domain knowledge. Recent researches in foundation models have demonstrated notable achievements across various applications. These models are useful because their pre-training encompasses a vast amount of high-quality human behavior data and advanced understanding of various tasks. Therefore, the inherent capabilities of foundation models hold great potential and value in addressing the difficulty in modeling human common sense in complex MARL. Many researchers utilize the rich knowledge in visual-language models (VLM) to generate dense rewards for vision-based reinforcement learning tasks with sparse rewards. However, they primarily focus on single agent performing individual tasks, which does not address complex multi-agent tasks. In addition, to address the issues of complex and dynamically changing goals, researchers are dedicated to designing dense rewards that can capture the complexity of human decision-making, such as intrinsic rewards and potential-based rewards. However, existing designs still lack sufficient expressiveness and fail to model advanced semantics of common sense. Therefore, how to design comprehensive dense rewards that align with the common sense in human decision-making processes remains a significant challenge. Based on the analyses, the paper investigates Vision-based Generic Potential Funtions (V-GEPF) to facilitate hierarchical policy alignment in MARL. Policy alignment refers to the process where the learned policies not only pursue performance optimization, but also align with human common sense and cognitive patterns, whose significance lies in enhancing practicality and meaningfulness of the policies. Specifically, to address the challenge of modelling human sense as a reward in complex MARL, we design a VLM-based generic potential function at the bottom layer. The generic potential function utilizes rich pretrained human knowledge within the VLM to guide policy learning towards alignment with human common sense. Then, recognizing that human decision-making involves dynamically changing goals, our method requires policies to flexibly adapt to uncertainty and change. Meanwhile, temporal information is crucial for understanding and aligning with the complex cognitive processes of human decision-making. Therefore, based on the characteristics that visual large language models (vLLM) have broader human knowledge and the ability to handle more complex input information than VLM, we design a vLLM-based adaptive skill selection module. The module use more informative instructions, replayed videos, and reflection on the records of last potential function to adaptively select appropriate next potential function from a pre-designed pool, aiming to achieve a more comprehensive alignment with human common sense at the top layer. Furthermore, our method can be theoretically proven to not alter the optimal policy. The primary contributions of this work are as follows:\n\u2022 We propose a hierarchical vision-based reward shaping method for MARL, focusing on learning policies that better conform to human common sense, rather than simply optimizing for efficiency.\n\u2022 To overcome the difficulty in modeling human common sense as rewards, we design VLM-based generic potential functions which utilize rich human knowledge from pre-trained data in VLM to guide policy alignment.\n\u2022 Considering the dynamic objectives from multiple facets of the human decision-making process, we design a VLLM-based adaptive skill selection module to achieve a higher level and more comprehensive consistency between policies and human common sense."}, {"title": "2 Related Works", "content": "Viusal-language models. Foundational models trained on vast amounts of data can acquire broad and transferable representations of various data types, such as images and languages, making them applicable to a variety of downstream tasks. As type of foundation models, visual-language models (VLM) integrate linguistic and visual signals, playing a crucial role in fields that require processing of both modalities. There are diverse applications of VLM in reinforcement learning (RL). For instance, VLM can be used as reward functions, prompt-able representation learners, and for data augmentation based on hindsight relabeling. The applications demonstrate wide applicability and robust capabilities of VLM, providing effective tools and new perspectives for addressing complex RL challenges. VLM as RL rewards. Currently, a promising application direction is using VLM to generate dense rewards for RL tasks, especially in scenarios with sparse rewards. Cui et al. utilize the pretrained CLIP to provide image-based rewards for robotic manipulation tasks. Mahmoudieh et al. successfully apply the fine-tuned CLIP as a language-described reward model for robotic tasks. Sontakke et al. use VLM in a robotic environment to provide reward signals for RL agents, primarily defining rewards through video demonstrations. However, existing methods primarily focus on solving specific single-agent tasks, where the design of the reward function tends to be relatively simple and direct. Moreover, these methods are mostly implemented in vision-based environments. In contrast, multi-agent systems involve more complex interactions and collaboration mechanisms, making dense reward guidance more challenging. Potential-based rewards. In MARL tasks with sparse rewards, the introduction of additional dense rewards to represent domain-specific expert knowledge has become a common practice to accelerate training. However, it soon became apparent that if used improperly, the dense rewards might alter the optimal response, which directly affects the performance of the policies. Ng et al. demonstrate that the optimal policy do not change if the dense reward is designed as $F(s, s') = \\gamma\\varphi(s') \u2013 \\varphi(s)$. Devlin et al. further prove that time-varying potential-based rewards $F(s,t,s',t') = \\gamma\\varphi(s',t') \u2013 $(s,t) do not alter the Nash equilibrium in multi-agent problems. Grzes extend the proof to episodic tasks that terminate after a certain final time step N. Chen et al. design potential-based rewards in GRF to be positively correlated with the distance between the ball and the goal. Zhang et al. design the rewards that are proportionally linked to the remaining health of a unit in StarCraft II. However, existing methods are often based on simple formulas or rules, making it difficult to comprehensively represent human common sense."}, {"title": "3 Background", "content": "Partially observable Markov decision process. A multi-agent reinforcement learning problem in vision-based tasks can be formally defined as a partially observable Markov decision process (POMDP). A POMDP is represented as a tuple: $< N,S,A,P,r,G,O,\\gamma >$, where $|N| = N$ is the number of agents, $s \\in S$ is the state space, $\\gamma \\in [0, 1]$ is a discount factor. At each time step t, the agent $i \\in N$ chooses its action $a_t \\in A$ based on its observations $o_t \\in O$. The actions of all agents constitute the joint action space $a \\in A^N$. The state and joint action make up a joint reward $r(s, a, s')$ based on the state transition function $P(s'|s, a)$. The team's objective is to maximize the expected discounted return $G = \\sum_{t=0}^{\\infty} \\gamma^t r(S_t, a_t, S_{t+1})$. CLIP models. VLM is a typical foundation model that can handle sequences incorporating both language inputs $l \\in C^{\\leq n}$ and visual inputs $v \\in V^{\\leq m}$. L represents a finite alphabet encompassing strings of length no greater than n, while V represents the space for 2D RGB images whose length do not exceed m. Contrastive language-image pretraining model (CLIP) is a representative VLM which trains by aligning image and text embeddings in the latent space. Specifically, the CLIP model consists of a text encoder $T_L$ and an image encoder $T_1$, both of which are mapped to a same latent space $R^k$. Typically, the encoders are trained by minimizing the cosine distance between embeddings for pairs of images and languages."}, {"title": "4 Methodology", "content": "The goal of this paper is to explore how to learn policies that are more aligned with human common sense in complex MARL with sparse rewards. We propose a hierarchical vision-based reward shaping method named V-GEPF, which utilizes rich human knowledge from foundation models to guide policy alignment from different levels. The algorithm framework is described in Appendix A.1. 4.1 VLM as Generic Potential Functions Guiding MARL policies to align with human common sense is a challenging problem. Human common sense is typically formed based on rich experiences and comprehensive cognition of specific tasks, making it challenging to be modelled and quantified through simple formulas or rules, especially in complex and long-horizon multi-agent tasks. Recent researches have shown effectiveness of additional dense rewards, such as potential-based rewards, to guide policy alignment. However, although these dense rewards are useful in certain tasks, they do have some limitations: (i) Insufficient expressiveness. Manually designed dense rewards are overly simplistic, failing to fully capture the multifaceted goals and high-level semantic information of human common sense in complex tasks. (ii) Limited understanding ability. Dense rewards often rely on the intuition and experience of experts, which may overlook complex behavioral dynamics and environmental factors in tasks, leading to a lack of in-depth and comprehensive understanding of complex tasks. (iii) Lack of adaptability and flexibility. Dense rewards are typically fixed, which are struggle to cope with dynamic changes in the environment. These limitations make it difficult to effectively align multi-agent policies with human common sense in complex decision-making tasks through existing dense rewards. The VLM possesses inherent exceptional capabilities due to the rich expert knowledge it has acquired from training datasets encompassing various complex tasks. Therefore, we design VLM as generic potential functions to guide policies in exploring directions that align with human common sense. Instruction-conditioned potential function. To design VLM-based generic potential functions, we first introduce an instruction-conditioned potential function. By incorporating the concept of instructions into the potential function, we lay the groundwork for exploring the alignment of VLM-guided policies with human common sense. Definition 1. An instruction-conditioned potential function $\\phi(s|l) : S\\times L \\rightarrow R$ is a mapping from the current state s to its latent value $\\phi$, given the language instruction l. The instruction-conditioned potential function does not directly depend on the agents' actions, but is determined by the state under the given instructions. The instructions reflect the guidance of human common sense. It is worth noting that when considering trajectories with a finite time step, the final step N has the terminal state, whose potential value should be set to zero : if $t = N$. $ \\phi(s|l) = \\left \\{\\begin{matrix} 0  \\\\  $(s|l)  otherwise. \\\\ \\end{matrix} \\right. $ VLM as a generic potential function. The pre-training data of VLM includes a vast repository of high-quality human understanding and cognition related to complex tasks. As a result, VLM has significant potential and value in overcoming the challenges of modeling human common sense in complex MARL tasks. Contrastive language-image pretraining model (CLIP) is a typical efficient VLM which is designed as generic potential functions in this paper. CLIP consists of an image encoder, $T_I$, and a text encoder, $T_L$. These encoders map images and text to embeddings in the same latent space. They are trained on a dataset containing a large number of image-text pairs by minimizing the cosine distance between the corresponding embeddings. Therefore, images and texts with similar semantics can be mapped to similar embeddings. We first convert the environmental state $s_t$ into an image $s_f$, enabling the CLIP to more intuitively understand the current situational context. Subsequently, the image and the corresponding instruction are fed into the image encoder and text encoder respectively, producing two embeddings $T_1(s_f)$ and $T_L(l)$. These embeddings reflects the semantics of instruction and current state. To align the policy with human common sense, the VLM-based generic potential function is designed as the cosine distance between the state and instruction embeddings: $\\phi(s|l) = \\frac{\\left(T_I(s_f), T_L(l)\\right)}{\\|T_I(s_f)\\| \\cdot \\|T_L(l)\\|}$ Then, based on Def. 1, we subsequently design generic potential-based reward as: $F(s_t, s_{t+1}|l) = \\gamma\\phi(s_{t+1}|l) \u2013 \\phi(s_t|l),$ where $s_t$ and $s_{t+1}$ represent the current and next state respectively, $\\gamma$ is the discount factor of MARL to ensure the designed rewards do not alter the optimal policy (detailed proofs can be found in Appendix A.2). With the cross-modal representation capabilities of CLIP, this generic potential-based reward can enhance the agents' understanding of the semantic of state. Instruction l is set to represent a ideal state that align with human common sense, e.g. 'the home players show cooperative attacking tactics'. Detailed instructions I are provided in Appendix B.2. The visualization details of states sf are presented in Appendix B.1. Since there is an original reward $r_{env}$ provided by the environment, the total reward is described as: $R(s_t, s_{t+1}) = r_{env}(s_t, s_{t+1}) + \\rho \\cdot F(s_t, s_{t+1}|l),$ where $\\rho$ represents a scalar coefficient to balance the generic potential-based reward with the environmental reward. It can be theoretically demonstrated that incorporating the generic potential-based reward ensures that the Nash equilibrium in multi-agent problems remains unchanged. Detailed proofs are presented in Appendix A.2. However, the decision-making process in complex long-horizon tasks often involves multiple dynamically changing goals, which means that the guiding direction of the policy needs to adaptively adjust at different stages. This is a key characteristic of human decision-making process. In this context, generic potential-based reward with a fixed instruction is inadequate, as it cannot comprehensively and accurately quantify the diversity and dynamics of these complex goals. Meanwhile, the lack of temporal information limits the understanding and aligning with the complex cognitive processes involved in human decision-making. These limitations hinder our policies' ability to effectively align with human common sense to some extent. To enhance flexibility and adaptability in our method, we design a vLLM-based adaptive skill selection module. 4.2 vLLM-based Adaptive Skill Selection In complex long-horizon tasks, human decision-making process can adapt to dynamic changes and involve multidimensional objectives. Taking football match as an example, a team's policies will flexibly adjust according to the progress of the match and the current situation. At the beginning of the match or when the advantage is not significant, a team often employ more aggressive offensive tactics, focusing on coordination among players to achieve rapid passing or multi-player attacks. When holding a significant lead, the team pay more attention to maintaining the stability of their formation and controlling ball possession to preserve their advantages. To align the decision-making process of MARL with human cognition, the designed rewards must be capable of adaptively and flexibly adjusting to the uncertain and changing circumstances. Leveraging the advanced understanding and representation capabilities of the visual large language model (vLLM), we propose a vLLM-based adaptive skill selection module. This module dynamically selects different skills (VLMs with pre-defined instructions) at various stages of training, allowing MARL agents to adjust their behaviors based on the evolving game context. We design a generic potential function pool emulating the thought processes involved in human decision-making. As researched by Simon, in the decision-making process, humans first attempt to understand the macro environment, then turn their attention to the specific details at the micro-level, which includes the analysis of individual behaviors and the pursuit of goals. Thus, different generic potential functions are designed from macro and micro perspectives to guide policies exploring towards the respective desired directions. At the macro level, the potential functions focus on the overall situation (such as team advantage) in order to set macro objectives in complex environments. At the micro level, we analyze from three perspectives: states-based, agents-based, and goal-based.\n\u2022 States-based skills focus on analyzing microscopic state information, such as the local advantage around a key agent.\n\u2022 Agents-based skills focus on the behaviors and interactions of the agents to uncover the complex dynamics of their actions and relationships, such as the level of coordination between the agents.\n\u2022 Goal-based skills aim to maintain a keen focus on specific objectives, such as the status of key targets. These different types of skills enable the policies to make more refined and targeted choices and adjustments in various contexts, promoting a more comprehensive alignment with human common sense. We provide a more detailed description of the designed instructions in Appendix B.2. Furthermore, information on the temporal dimension is crucial for capturing the continuity and evolution of policy learning. Thus, we present the entire episode's states in a video replay to provide temporal information as input to the VLLM. Overall, the vLLM-based adaptive skill selection module and the VLM-based generic potential function hierarchically construct a vision-based generic potential function. The vLLM receives the video replay of the last episode, initial human instructions, information about the potential function pool, and the reflection on records of the previous potential function. Using these information, a VLM-based potential function is selected from the pool, providing multi-angle, multi-level guidance for policy alignment. The dialogue examples of this module are provided in Appendix B.3."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup All experiments are implemented in a typical long-horizon multi-agent benchmark Google Research Football (GRF), which simulates real football matches and serves as a widely used benchmark for researchers in football game AI. We evaluate our method on GRF full-field (11 vs 11) tasks with different difficulty levels. In the full-field tasks, all 11 players of home team's are controlled by RL policies, while the opposing 11 players are managed by a built-in script with adjustable difficulty settings. Each player has a 19-dimensional discrete action space, with all players sharing global state information. This constitutes a vast exploration space. We set the reward to be given only when a goal is scored, and each episode can be up to 3000 steps long, which constitutes a sparse reward problem. The full-field tasks are sufficiently challenging, making it very difficult for MARL to learn good collaborative policies within them. V-GEPF is implemented based on MAPPO and compared against MAPPO, IPPO, and HAPPO. During training, all algorithms set 32 workers to sample training data in parallel, with each worker sampling 3,000 steps per epoch. The training ends after 300 epochs, resulting in approximately 28 million steps. Additionally, V-GEPF adaptively selects skills every 50 epochs. For the VLM, we use CLIP-RN501, for the VLLM, we employ MiniCPM-Llama3-V2. To further evaluate V-GEPF, we introduce a baseline called MAPPO-XT, which also utilizes potential-based rewards. 5.2 Careful Comparison to SOTA We compare the average win rate curves of V-GEPF and state-of-the-art (SOTA) baselines during training. As illustrated in Figure 3(a), in the 11 vs. 11 easy task, V-GEPF outperforms all baselines in terms of win rate, significantly surpassing IPPO. While HAPPO, MAPPO, and MAPPO-xT exhibits an advantage in convergence speed, their final win rates are lower than that of V-GEPF. The slower convergence speed is attributed to the potential-based reward guiding the policy to explore cooperative policy that align with human common sense, e.g. dribbling, keep formation, rather than solely focusing on scoring goals. In the 11 vs. 11 hard task, as illustrated in Figure 3(b), V-GEPF significantly outperforms all baselines in terms of win rate. Both MAPPO, MAPPO-xT, and HAPPO's average win rate curves plateau around 15 million steps. By visualizing SOTA policies, we observe that they have learned strange policies (as shown in Figure 1): one player dribbles the ball toward the goal while the other players fail to cooperate, instead running toward the opponent's sideline or congregating at the corner or edge of the opponent's field. This represents a local optimum that RL can easily discover. However, once a policy is trapped in this local optimum, it becomes challenging to explore alternative policies due to the vast exploration space of this task. In contrast, V-GEPF benefits from the adaptive guidance provided by the vLLM and VLM, enabling the policy to avoid getting stuck in this local optimum. As a result, V-GEPF eventually achieves a higher win rate compared to the baselines. We further use radar maps for a fine-grained comparison between the policies of MAPPO and V-GEPF. As shown in Figure 4, the six dimensions in the radar chart characterize how well a policy performs beyond the average win ratio, which can comprehensively reflect human common sense. 5.3 Potential Function Analysis We select one run of V-GEPF, and record the average value of the potential function per epoch to analyze in detail how the potential function curve changes during training. After each skill selection, we use the first epoch to calculate the mean and standard deviation of the potential function and normalize it, so the potential function is always zero at the beginning. As shown in Figure 5, each potential function shows an upward trend, which indicates that VLM is able to guiding policy learning given different instruction. From the selection of skill, we find that vLLM can flexibly adapt skills to address the deficiencies of the current policy. 5.4 Ablation on Adaptive Skill Selection To validate the necessity of the adaptive skill selection module, we compare the policies guided by V-GEPF and a fixed VLM-based potential function using radar charts (Figure 7). The charts show that the policies guided by a fixed VLM-based potential function exhibit strong stylistic tendencies, with the policy style aligning well with corresponding instruction. For example, an instruction favoring offence leads to policies with high numbers of shots and shot success rates, while a instruction favoring dribbling results in policies with high pass counts and pass success rates, as this effectively prevents the ball from being intercepted. However, football as a long-horizon complex task, requires a more balanced and adaptive policy. As shown in Figure 4(b), the adaptive skill selection module selects the suitable VLM-based potential function at different training stages, resulting in a more balanced policy style compared to not using the module. 5.5 Visualization Analysis We further analyze the policies learned by V-GEPF through visualization. Our method not only improves the win rate, but also learns policies with standard formation and coherent passing, resulting in effects that align with human common sense in football matches. In the offensive sequence illustrated in Figure 6, the team executes three distinct passing patterns: transitioning from the backfield to midfield to the forward line, moving from the center to the flanks and back, and switching from long-distance to short-distance passes. Specifically, in the first frame, the goalkeeper A initiates a long pass to transfer the ball to right back B. In the second frame, as right back B controls the ball, center back E and right midfielder F move closer to provide support. However, the space created by defensive midfielder C's forward run poses a greater threat. Thus, B delivers a long ball to the vacant space in the center, targeting C. In the third frame, it is noteworthy that with C advancing into the attack, F positions itself to recover in order to protect against a potential counterattack from the opponents. Center midfielder D prepares to receive the ball, and naturally, C plays a short pass to D. Finally, in the fourth frame, D executes a dribble and takes a shot, resulting in a goal. The policy demonstrates that the V-GEPF encourages agents to exhibit cooperative behaviors that align with human cognition. Firstly, agents can establish positioning that adheres to the required formation lines. Secondly, during the passing process, agents are capable of making appropriate supporting runs (such as C, E, and F) which provide the receiver with more options, as well as protective runs (such as F) which prevent counterattacks in the event of an unsuccessful offensive play. Furthermore, agents can actively seek to pass into areas that pose a greater offensive threat rather than simply dribbling the ball individually."}, {"title": "6 Conclusion", "content": "This paper investigates how to align MARL policies with human common sense in complex, long-horizon tasks. While reward shaping methods have proven effective in guiding policies, modeling common sense as a reward remains a challenge. To address this issue, we propose a vision-based reward shaping method called V-GEPF. V-GEPF employs a hierarchical framework that leverages vision information effectively. At the bottom layer, a VLM is used as a generic potential function to efficiently give real-time guidance. At the top layer, a vLLM reflects on the current policy and adjusts the subsequent guidance accordingly. Through experiments, we demonstrate the effectiveness of our proposed method compared to SOTA methods in both improving win rates and aligning with human common sense."}, {"title": "A Algorithm Details", "content": "A.1 Algorithm of V-GEPF V-GEPF can be combined with various multi-agent reinforcement learning algorithms, such as MAPPO and HAPPO. Algorithm 1 demonstrates the training process of V-GEPF combined with MAPPO to achieve vision-based multi-agent policy alignment. Algorithm 1: The training algorithm of V-GEPF 1: Input: batch D, number of agents n, steps per episode T, episodes K; skill selection cycle C; 2: Initialize: Actor $\\pi_{\\rho}$; critic $V_{\\Theta}$; replay buffer B; VLM-based generic potential function $\\phi$; 3: for k=1 to K do 4: for t=0 to T do 5: Collect the global state $s_t$; 6: Visualize $s_t$ as $s_f$; 7: for i=1 to n do 8: Collect local observation $o_t^i$; 9: Select action $a_t^i$ according to the actor $\\pi_{\\rho}$; 10: end for 11: Execute the joint action $a_t$ and collect the next joint observation $o_{t+1}$, next state $s_{t+1}$, next state's visualization $s_{t+1}$, and reward $r_{t+1}$; 12: Calculate the potential-based reward $F(s_f, s_{t+1}|l) = \\gamma\\phi(s_{t+1}|l) \u2013 \\phi(s_f|l)$; 13: Calculate the total reward: $R_{t+1} = r_{t+1} + F(s_t, s_{t+1}|l);$ 14: Calculate TD target $y_i$ and advantage function $A_i$; 15: Store ($o_t$, $a_t$, $s_t$, $s_f$, $R_{t+1}$, $o_{t+1}$, $s_{t+1}$, $s_{t+1}$, $s_{t+1}$, $y_i$, $A_i$) into the buffer B; 16: ift mod C = 0 then 17: Update VLM-based generic potential function $\\phi$; 18: end if 19: end for 20: Sample a random mini-batch from buffer B; 21: Calculate loss function of the critic $L_{V_{\\Theta}}$() and the actor $L_{\\pi_{\\rho}}(\\rho)$; 22: Using the gradient descent algorithm to minimize the loss function and update the network parameters $\\rho$, $\\Theta$. 23: end for Specifically, the loss function of the critic can be described as: $L_{V_{\\Theta}} = E [(y_i \u2013 V_{\\Theta}(o_i))^2]$, where $y_i^t$ is the TD target of the critic for agent i at step t: $y_i^t = R_t + V_{\\Theta}(o_{t+1}).$ The loss function of the actor can be described as: $L_{\\pi_{\\rho}} = E [min (\\frac{\\pi_{\\rho}(.o_i)}{\\pi_{\\rho_{old}}}, clip(\\frac{\\pi_{\\rho}(.o_i)}{\\pi_{\\rho_{old}}}, 1 - \\epsilon, 1 + \\epsilon)) A_{\\theta}(o_i, a_i)]$, where $\\pi_{\\theta_{old}}$(.) is the actor before the parameter update, $\\epsilon$ and $\\lambda$ are two hyperparameters. $A_{\\theta}(o_i, a_i)$ is an advantage function estimated using the Generalized Advantage Estimator (GAE): $A_t^i(o_t^i, a_t^i) = \\sum_{l=1} (\\gamma\\lambda)^l (R_{t+1} + V_{\\Theta}(o_{t+1}^{i}) \u2013 V_{\\Theta}(o_{t+l}^{i}))$ A.2 Algorithm Proof To demonstrate that incorporating generic potential functions as potential-based rewards do not alter the optimal response in multi-agent reinforcement learning (MARL), we first prove the preservation of policy invariance in single-agent problems. Then we prove the maintenance of consistent Nash equilibrium in multi-agent problems."}, {"title": "B Implementation Details", "content": "B.1 Visualization of States Knowledge of the soccer domain is aggregated to the visualisation of states for more image representation. The pitch is first visualised with mplsoccer. In detail, the players and ball are represented as circles with blue, red and black colors, corresponding to the offensive team, the defensive team and the ball, respectively. To further indicate the team formation and the contact between players, we visualize a convex hull and formation lines. The convex hull is the coverage area for all players except the goalkeeper, and reflects the ability to cover the pitch between formations. The formation lines are sequential lines of player combinations with the same responsibilities and reflects the discipline and stability within formations. In the Figure 8, the offensive team lines up in a 3-5-2 with the defensive midfielder (DM) possessing the ball, and the defensive team lines up in a 4-4-2. The blue convex hull is larger than the red corresponding to more aggression. B.2 Prompt Details of VLM Based on the domain knowledge of football and the current capabilities of VLMs, we have categorized the the potential function pool into four types. These four types aim to comprehensively cover the various aspects where potential functions might guide the algorithm.\n\u2022 Macro level: this category of potential functions quantifies the macro-level situation. The prompt include: \"The blue team has a bigger advantage than the red team.\"\n\u2022 Micro level, state-based: these potential functions focus on analyzing microscopic state information, quantifying local situations. The prompt include: \"The ball is close to the opponent's goal.\"\n\u2022 Micro level, agent-based: these potential functions focus on the behaviors and interactions of agents, quantifying the proficiency of specific agent skills. The prompts include: [\"The blue team is performing a coordinated attack.\", \"The blue team is trying to defend when the ball is close to their goal.\", \"Three blue formation lines are parallel and have proper spacing.\", \"The black ball is well dribbled by the blue team.\"]\n\u2022 Micro level, goal-based: these potential functions focus on specific objectives (e.g., the number of shots, the number of passes). Traditional rule-based potential functions often fall into this category. By using VLM-based potential functions combined with appropriate visualizations, we can cover the range of capabilities of rule-based methods. The prompt include: \"the blue team is passing\"\nWe believe this classification could also inspire other tasks beyond football. It is worth noting that all potential functions use the same VLM, and we switch prompts to represent different potential functions. B.3 Dialogue Example of vLLM The full prompt contains a video part and text part. The text part includes instructions related to the initial goal, records of the training process, information about the potential function pool, and the record of the last potential function as feedback"}, {"title": "C Experimental Details", "content": "C.1 Experimental Environments and Scenarios Google Research Football Environment. Google Research Football (GRF) is a benchmark reinforcement learning environment", "agent": "pixels, super mini map and raw observation of 115-dimensional vector. The paper adopts the 115-dimensional vector as the observation, which includes many complicated and comprehensive information of the match, such as the position and the speed of each player and the ball. Based on the state representations, there are 19 actions to be chosen for each agent, including move actions towards 8 directions, and 11 different kicking techniques. GRF provides a set of football full scenarios for efficient training. These scenarios have distinct initial situations, with various players and opponents positioned at different starting locations, and the opponents have adjustable difficulty settings. GRF Scenarios. The GRF environment simulates real football matches, which serves as benchmarks for football game AI and enables researchers to assess the performance of their algorithms. Four different simulation scenarios are chosen in this paper, i.e., academy counterattack_easy, academy counterattack_hard, academy 11_vs_11_easy, and academy 11_vs_11_hard. In these scenarios, the left team is controlled by MARL algorithm. In different scenarios, the agents, the opponents, and the ball have different initial positions which are shown in Figure 9. academy 11_vs_11 and academy counterattack are both full-field scenarios. In academy counterattack, only the four players in the front (marked in deep green) are controlled by MARL algorithm and others are controlled by built-in script, while all 11 players are controlled by MARL algorithm in academy 11_"}]}