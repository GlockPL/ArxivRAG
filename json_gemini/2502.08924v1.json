{"title": "Escaping Collapse: The Strength of Weak Data for Large Language Model Training", "authors": ["Kareem Amin", "Sara Babakniya", "Alex Bie", "Weiwei Kong", "Umar Syed", "Sergei Vassilvitskii"], "abstract": "Synthetically-generated data plays an increasingly larger role in training large language models. However, while synthetic data has been found to be useful, studies have also shown that without proper curation it can cause LLM performance to plateau, or even \"collapse\", after many training iterations. In this paper, we formalize this question and develop a theoretical framework to investigate how much curation is needed in order to ensure that LLM performance continually improves. We find that the requirements are nearly minimal. We describe a training procedure that converges to an optimal LLM even if almost all of the non-synthetic training data is of poor quality. Our analysis is inspired by boosting, a classic machine learning technique that leverages a very weak learning algorithm to produce an arbitrarily good classifier. Our training procedure subsumes many recently proposed methods for training LLMs on synthetic data, and thus our analysis sheds light on why they are successful, and also suggests opportunities for future improvement. We present experiments that validate our theory, and show that dynamically focusing labeling resources on the most challenging examples in much the same way that boosting focuses the efforts of the weak learner leads to improved performance.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represent the frontier of artificial intelligence, and are trained on vast amounts of human-generated data. However, much of the high-quality publicly available data on the Internet has been exhausted, and limits on generating new tokens threaten to slow progress on LLM training.\nAs a consequence, synthetically-generated datasets are playing an important role in the training of LLMs. Synthetic data have been shown to improve the performance of real large models on a range of tasks [Bai et al., 2022, Zelikman et al., 2022, Gulcehre et al., 2023, Singh et al., 2024]. On the other hand, the circuitous nature of training new LLMs on data generated by previous generations of LLMs has caused concerns of model collapse [Shumailov et al., 2024, Alemohammad et al., 2024]. Since publicly available sources contain an increasingly large proportion of machine-generated content, synthetic data will be used for training, deliberately or inadvertently."}, {"title": "Related work", "content": "Training data is a crucial component in building high-performing LLMs. Human generated data has limitations such as scalability, biases, errors, and potential privacy considerations [Kurakin et al., 2023, Singh et al., 2024, Gilardi et al., 2023, Long et al., 2024]. Longpre et al. [2024] highlights a challenge: as LLMs scale, the demand for high-quality data increases, yet access to such data becomes more restricted due to copyright and privacy constraints. Given these challenges, integrating synthetic data into training pipelines is essential, but comes with its own set of risks."}, {"title": "Preliminary Notation", "content": "Datasets. Let $\\mathcal{X}$ be the set of all possible prompts, and let $\\mathcal{Y}$ be the set of all possible responses, which we also call labels. An element of $\\mathcal{X} \\times \\mathcal{Y}$ is a labeled prompt. A subset of $\\mathcal{X}$ is a prompt set, and a subset of $\\mathcal{X} \\times \\mathcal{Y}$ is a dataset.\nFor any prompt set $\\mathcal{P}$, let $\\mathcal{P}(x)$ denote the number of times prompt $x$ appears in $\\mathcal{P}$, and for any dataset $D$, let $D(x,y)$ denote the number of times labeled prompt $(x, y)$ appears in $D$. Typically we have $\\mathcal{P}(x) \\in \\{0,1\\}$ and $D(x, y) \\in \\{0,1\\}$. However, we also allow datasets to contain multiple copies of the same element, where the multiplicity, or weight, of an element can be any"}, {"title": "Problem Setting", "content": "We consider a setting where a sequence of LLMS $g_1, g_2,...$ are learned on a sequence of datasets $D_1, D_2, ....$ Given a prompt set $\\mathcal{P}$, our high-level goal is to produce an LLM that generates high quality responses for every prompt in $\\mathcal{P}$. We illustrate this meta-algorithm in Setting 1.\nUnlike classical learning, where the learner has access to samples from the target distribution, we assume that the learner only has access to labeled examples constructed by a data generation procedure that we control, denoted by the function GenerateData. Data generation might make use of synthetic data, produced by the previous generation's LLM $g_{t-1}$, and exogenous (i.e., non-synthetic) signals.\nIn order to formalize our goal, we make precise the capabilities of learner, the capabilities of GenerateData, and our notion of quality."}, {"title": "Strong Learning", "content": "We first introduce the concept of a strong learner.\nDefinition 1 (Strong Learner). For any LLM $g$ let $g(y|x)$ be the probability that the distribution $g(x)$ assigns to response $y$. The function learner takes as input a dataset $D \\subset \\mathcal{X} \\times \\mathcal{Y}$ and outputs an LLM $g$ such that $g(y|x) = D(y|x)$ for all $(x, y) \\in D$.\nThe procedure learner trains an LLM that matches the conditional probability of each response given a prompt in the input dataset. That is, we assume that the model class has the capacity to match this distribution exactly, and the learning procedure can find the model parameters that"}, {"title": "Data Generation", "content": "Creating data for the next generation of an LLM might involve making use of synthetic data produced by the current generation of the LLM. To avoid model collapse, some degree of data curation happens in practice. This curation may make use of an exogenous signal previously unknown to our training algorithm. Curation may also take the form of evaluating the quality of existing labeled data. We discuss each of these capabilities in greater detail.\nSynthetic Data. Given an LLM $g$, and a prompt $x$, we can generate a synthetic response for $x$ by sampling from distribution $g(x)$. Overall, we assume that synthetic data generation is relatively inexpensive, and permit data generation procedures that make calls to previously-trained LLMs.\nQuality Evaluation. We next assume that our data generation procedure has access to a quality function, which evaluates whether a prompt is paired with a high-quality synthetic response. In this work we assume that $q$ is a binary attribute that can be efficiently and unambiguously evaluated for all responses to prompts in $\\mathcal{P}$.\nDefinition 2 (Quality). Let $q : \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\{0,1\\}$ be the quality function, where $q(x, y) = 1$ indicates that $y$ is a good response to prompt $x$.\nWeakening these assumptions to permit different types of ambiguity in $q$, including non-binary quality, as well as uncertainty and inefficiency in evaluating $q$ are interesting topics for future work. However, note that these assumptions cover a broad range of settings. For many applications, recognizing that a synthetic response is a high-quality for a given prompt is not only unambiguous but also significantly easier than generating the response from scratch. For instance, if the dataset contains arithmetic or coding problems, it is relatively easy to programmatically verify a correct answer.\n$\\beta$-Weak Labeler. Key to our work is the notion of a weak labeler, a function, that given any set of prompts produces responses with average quality bounded away from zero.\nTo formally define it, we use an auxiliary function $a_P : \\mathcal{P} \\rightarrow \\mathcal{Y}$, which generates labels for all prompts in a set $\\mathcal{P}$.\nDefinition 3 ($\\beta$-weak Labeler). The function $labeler_\\beta$ takes as input a prompt set $\\mathcal{P} \\subset \\mathcal{X}$, and uses an auxiliary function $a_P : \\mathcal{X} \\rightarrow \\mathcal{Y}$ to label every prompt in $\\mathcal{P}$. Formally,\n$labeler_\\beta(\\mathcal{P}) = \\{(x, y) : x \\in \\mathcal{P}, y = a_P(x)\\} \\subset \\mathcal{X} \\times \\mathcal{Y}$"}, {"title": "Objective", "content": "Given these capabilities \u2013 the ability to synthesize data, assess synthetic data quality, and weakly label new data \u2013 the goal of our algorithm is to construct datasets $D_1,..., D_T$ so that\n$\\lim_{T\\rightarrow\\infty} Pr_{x\\sim \\mathcal{P},y\\sim g_T(x)}[q(x, y) = 1] = 1$\nwhere $x \\sim \\mathcal{P}$ denotes that $x$ is chosen uniformly at random from $\\mathcal{P}$, and $y \\sim g_T(x)$ denotes that $y$ is chosen from distribution $g_T(x)$. In other words, as the number of algorithm iterations grows large, the final LLM output by the algorithm returns a correct response to almost every prompt in $\\mathcal{P}$. Note that this objective is similar to the objective of classical boosting. Rather than use weak learners to construct a good hypothesis, we ask whether strong learners and weak data can be used to construct a model that provides high-quality results on all prompts."}, {"title": "Algorithm", "content": "We present an algorithm for learning an LLM from a mixture of synthetically generated and weakly labeled data that uses the capabilities introduced in Section 4.\nThe aforementioned algorithm generates synthetic responses from the last generation of LLM. Synthetic data generation is given multiple opportunities to produce a good response through best-of-k sampling. Prompts that are consistently paired with low-quality responses are passed into $labeler_\\beta$, which provides a minimal amount of signal. A mixture of good synthetically labeled data and $\\beta$-weak-labeled data is then incorporated into the training mixture. To state this procedure formally, we introduce two subroutines.\nDefinition 4 (Best-of-k). The function $best\\_of(\\mathcal{P};k, g)$ takes as input a set of prompts $\\mathcal{P} \\subset \\mathcal{X}$ and produces a dataset where each $x \\in \\mathcal{P}$ is paired with the best response encountered after $k$ rounds of inference using the LLM $g$. Formally, for each $x \\in \\mathcal{P}$ and $i \\in [k]$, let $y^i \\sim g(x)$ and define the random set $Y^k = \\{y^i : i \\in [k]\\}$. Then,\n$best\\_of(\\mathcal{P}; k, g) = \\{(x, \\arg\\max_{y \\in Y^k} q(x, y)) : x \\in \\mathcal{P}\\}$"}, {"title": "Main result", "content": "Theorem 6 is our main theoretical result, and states that the final LLM $g_T$ output by Algorithm 2 satisfies the convergence requirement in Eq. (1). Theorem 6 also quantifies the rate of convergence.\nTheorem 6. Let $\\epsilon \\in (0,1)$. Suppose that in Algorithm 2 we have $\\alpha > 0$, $\\beta \\in (0,1)$,\n$T> \\frac{\\log(2/\\delta)}{\\beta} + \\frac{2\\alpha}{\\beta \\epsilon} + 1$\nand $k > (2 \\log T + \\log |\\mathcal{P}|)/\\beta$. With probability at least $1-1/T$ over the randomness of the algorithm, the final LLM $g_T$ output by the algorithm satisfies\n$Pr_{x\\sim \\mathcal{P},y\\sim g_T(x)} [q(x, y) = 1] \\geq 1 - \\epsilon$.\nNote that by setting $\\alpha = \\epsilon$ in Algorithm 2 the above iteration complexity becomes $T = O(\\log(1/\\epsilon)/\\beta)$.\nProof sketch. The key step in the proof is showing that, with probability 1-1/T, in each iteration t we have $Pr_{y \\sim g_{t-1}(x)} [q(x, y) = 1] \\geq \\beta$ for all but $(1 \u2013 \\beta)^{t-1}$ fraction of the prompts $x \\in \\mathcal{P}$. Since the algorithm draws $k = \\Omega(1/\\beta)$ synthetic responses to each prompt from $g_{t\u22121}$, one of those responses"}, {"title": "Discussion", "content": "Interpretation as Boosting\nBoosting is a meta-learning algorithm for combining weak hypotheses into highly accurate ensemble classifiers [Schapire and Freund, 2013]. While the most common version of boosting is AdaBoost [Freund and Schapire, 1997], we will present a slightly simpler version that still contains all of the essential ideas.\nIn each iteration of boosting, a training set of binary-labeled examples is given as input to a weak learner. Each training example is associated with a non-negative weight, and the weights sum to 1. The weak learner returns a hypothesis that achieves weighted error at most $\\frac{1}{2} - \\beta$ on the training set, where $\\beta \\in (0, \\frac{1}{2})$ is the edge over the trivial hypothesis that randomly guesses"}, {"title": "Trivial Baseline: Filtering Non-Synthetic Data", "content": "Algorithm 2 has the property that it only applies filtering on LLM-generated data. As discussed, this accurately models existing methods in the literature.\nHowever, if we consider applying the quality function $q$ on data produced by the weak labeler (that is, data that is not LLM-generated), then there is a very simple solution to the data generation problem. Clearly $O(\\log(1/\\epsilon)/\\beta)$ invocations of the weak labeler would suffice to correctly label all but $\\epsilon$ fraction of the prompts in $\\mathcal{P}$ (just repeatedly invoke the weak labeler on the incorrectly labeled prompts, filtering high-quality examples), and such a dataset could be given to a strong learner to produce an LLM that achieves $O(\\epsilon)$ error. It is worthwhile to reason about why such a simple solution cannot be deployed in practice.\nThe weak data assumption specifies that $\\beta$, while arbitrary, is bounded away from zero. Just as the weak learning assumption might not hold in classical boosting, the weak data assumption might not hold in our setting. We argue that iteratively filtering the weak labeler's output should result in a precipitous drop in the fraction of correctly-labeled examples. As an example, suppose human labelers provide good responses to the $\\beta_1$ easiest coding prompts in some prompt set. One should expect that asking similarly-qualified labelers to respond to the remaining prompts results in a $\\beta_2 \\ll \\beta_1$ yield of quality responses, as all but the easiest prompts have been answered. In contrast, a continually improved LLM endows a human with more flexibility for future responses, such as rewriting nearly high-quality solutions provided by the last iteration of LLM, making a non-vanishing $\\beta$ a much more reasonable assumption.\nSecondly, while it keeps the setting simple to presume that $q$ can be evaluated on any labeled example, this is an overly permissive assumption. LLM-generated synthetic data can be made to include reasoning traces, and often produces responses that the LLM itself can verify as high quality. This facilitates the construction of automated quality checkers, which are much more difficult to construct when the labels are produced by a human, and therefore contain reasoning traces and responses that are unfamiliar to the current generation of LLM. This is born out in the literature, where quality verification of LLM-generated synthetic data is relatively easy to implement [Singh et al., 2024, Yang et al., 2024, Zelikman et al., 2022]."}, {"title": "Experiments", "content": "Viewing Algorithm 2 as a meta-algorithm, we conduct experiments with specific instantiations using Gemma 2 2B on math problem solving [Cobbe et al., 2021, GSM8K] and Python coding [Austin et al., 2021, MBPP] tasks. We select these tasks because measures of response quality here are consistent and easily verifiable."}, {"title": "Instantiations of Algorithm 2", "content": "Do nothing. Responses produced by the current iteration of the model are directly used as training data for the next iteration. This corresponds to setting $\\alpha = 0$, omitting the best-of operation, and using a pass-through filter in line 4 of Algorithm 2. This tracks the setting explored in the \"model collapse\" literature [Alemohammad et al., 2024, Shumailov et al., 2024, Gerstgrasser et al., 2024].\nFilter only. Only correct responses in the current iteration are used for training in the next iteration. This corresponds to $\\alpha = 0$ in Algorithm 2 and using a filter that only keeps correct responses. This reproduces the STaR/ReST approaches for learning from synthetic data [Zelikman et al., 2022, Gulcehre et al., 2023, Singh et al., 2024].\nBoosting. The full algorithm of the present paper. In addition to the synthetic data produced by Filter only, we mix in weak data from the labeler. This corresponds to $\\alpha > 0$ and $\\beta > 0$ in Algorithm 2. We use $\\alpha = 1/3$ in all experiments.\n\u2022 Boosting, w/o focusing. We ablate out focusing on hard examples. To be precise: rather than giving the labeler the prompts we got wrong, $\\mathcal{P}_t^-$, we draw a random set of questions of size $|\\mathcal{P}_t^-|$.\nBaselines. We also report two baselines that do not involve iteratively training on model-generated data. PT: the pre-trained model; and Gold SFT: the model after one round of fine-tuning on the human-written responses in the dataset. Note that Gold SFT is the only setup that makes use of human-written responses, rather than just for answer verification."}, {"title": "Experimental Details", "content": "In all experiments, a round of fine-tuning entails training all parameters of the model for 330 (GSM8K) or 30 (MBPP) steps at batch size 64 (with the exception of Gold SFT where we report the checkpoint with best validation accuracy) We train with standard sequence cross-entropy loss. Training examples are (input, target) pairs, where input is the problem preceded by a 3-shot prompt (see Appendix D for prompt templates); and target is a model response (human-written response for Gold SFT)."}, {"title": "GSM8K Results", "content": "Table 1 summarizes our results on GSM8K. We have 7000 training problems, use $k = 8$ for best-of, and allocate the same total query budget of 56,000 to the labeler each round.\nBaselines validate our experimental setup. Results in the PT and Gold SFT demonstrate that: (1) our evaluation setup is in the ballpark of what is reported in the original Gemma 2 report; and (2) our fine-tuning setup indeed can yield significant improvement when the training data is human-written solutions.\nModel collapse with no curation. In the Do nothing row, we recover the result from the model collapse literature that iterative fine-tuning without curation does not improve the model and leads to degraded quality.\nComparison between curation variants. Indeed, the present algorithm demonstrates im- provements over the ReST-like variant that uses filtering only. The differences are most evident in training accuracy, which is strongly predicted by the theory. Indeed, this is in spite of the fact that as opposed to filtering only, boosting introduces incorrect answers to the training data. Furthermore although our theory does not address generalization, we observe that boosting results in improved test accuracy. Finally, the performance of boosting without focusing is quite close random selection is a strong baseline \u2013 but focusing still leads to improvements, especially in terms of training accuracy.\nQualitative results. In Appendix C, we present model responses to selected problems over the course of training."}, {"title": "MBPP Results", "content": "Table 2 summarizes our results on MBPP. We have 374 training problems, use $k = 32$ for best-of, and allocate the same total query budget of 11,968 to the weak labeler in each round.\nSimilar results to GSM8K for train pass rate. In terms of train pass@k, we observe similar results to GSM8K experiments, that generally: Boosting > Boosting w/o focusing > Filter only > Do nothing. On weaker data (B), Filtering beats Boosting w/o focusing in terms of pass@1.\nNo clear winner for test pass rate. While all iterative approaches outperform Gold SFT in terms of test pass rate, they all recover similar test performance despite differences in training accuracy. Notably, Boosting w/o focusing beats Boosting, and Filter Only outperforms Boosting with weaker data (B). One explanation is the limited amount of training data (384 examples) which prevents generalization; note that Gold SFT does not recover PT test pass rate."}, {"title": "Conclusion & Future Work", "content": "We have shown that under mild assumptions a modicum of curation applied to synthetic data not only avoids model collapse, but leads to arbitrarily high accuracy results. Our analysis is through the lens of boosting and, mirroring that paradigm, we define notions of strong learners and weak data to reach the theoretical conclusions. In taking this view, we provide theoretical explanations for many of the synthetic data methods used in practice.\nMany interesting questions remain. An immediate avenue is further relaxing the assumptions (e.g., having nearly strong learners - that approximately match the conditional distribution \u2013 and imperfect filters) and deriving corresponding convergence rates. A broader goal is using these insights for the burgeoning field of data selection, where we must explicitly model similarities between different examples as part of the analysis."}, {"title": "Prompts", "content": "As an expert problem solver solve step by step the following mathematical questions.\nQ:\nTina makes $18.00 an hour. If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage. If she works 10 hours every day for 5 days, how much money does she make?\nA:\nHere's how to calculate Tina's earnings:\n**Regular Time:**\nHours per shift: 8 hours\nWage per hour: $18.00\nRegular pay per shift: 8 hours * $18.00/hour = $144.00\n**Overtime:**\nOvertime hours per shift: 10 hours - 8 hours = 2 hours\nOvertime pay per hour: $18.00 + ($18.00 / 2) = $27.00\nOvertime pay per shift: 2 hours * $27.00/hour = $54.00\n**Total per day:**\nRegular pay + overtime pay: $144.00/shift + $54.00/shift = $198.00/day\n**Total for 5 days:**\n- 5 days * $198.00/day = $990.00\n**Therefore, Tina will make $990.00 in 5 days.**\nThe answer is 990."}]}