{"title": "Conceptual In-Context Learning and Chain of Concepts: Solving Complex Conceptual Problems Using Large Language Models", "authors": ["Nishtha N. Vaidya", "Thomas A. Runkler", "Thomas Hubauer", "Veronika Haderlein-Hoegberg", "Maja Milicic Brandt"], "abstract": "Science and engineering problems fall in the category of complex conceptual problems that require specific conceptual information (CI) like math/logic-related know-how, process information, or engineering guidelines to solve them. Large Language Models (LLMs) are promising agents to solve such complex conceptual problems due to their implications in advancing engineering and science tasks like assisted problem solving. But vanilla LLMs, trained on open-world data, lack the necessary CI. In this work, we specifically explore shallow customization methods (SCMs) of LLMs for solving complex conceptual problems. We propose two novel SCM algorithms for LLMs, to augment LLMs with CI and enable LLMs to solve complex conceptual problems: Conceptual In-Context Learning (C-ICL) and Chain of Concepts (CoC). The problem tackled in this paper is the generation of proprietary data models in the engineering/industry domain based on conceptual information in data modelling guidelines. We evaluate our algorithms on varied sizes of the OpenAI LLMs against four evaluation metrics related to syntactic and semantic correctness, time and cost incurred. The proposed algorithms perform better than currently popular LLM SCMs like In-context Learning (ICL) and Chain of Thoughts (CoT). It was observed that as compared to CoT, response correctness increased by 30.6% and 29.88% for the new SCMS C-ICL and CoC, respectively. Qualitative analysis suggests that the proposed new SCMs activate emergent capabilities in LLMs, previously unobserved in the existing SCMs. They make the problem solving processes more transparent and reduce hallucinations and the tendency of model responses to copy examples from prompts (parroting).", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized solving everyday problems like text summarising, copy-writing and knowledge acquisition [1], [2]. LLMs can solve these general problems through the model's base knowledge, captured by its large representation power trained on amounts of open-world data. Many problems that LLMs can solve are never introduced to the LLM during training. These emergent capabilities of LLMs are enabled through specialized natural language (NL) prompting techniques like In-Context Learning (ICL) [3]\u2013[5] and Chain of Thoughts (CoT) [6] referred to as shallow customization methods (SCM) of LLMs as they do not alter model embeddings. Therefore LLMs, with or without such SCM, are good at general problem-solving or formal linguistic abilities. Formal linguistic competency tasks are based on syntactic rules and statistical patterns that characterize a language. On the flip side, open-world data often lack specialized information like math/logic related know-how, process information or engineering guidelines and hence, LLMs lack such knowledge too. It has been shown that LLMs often fail to solve even simple math problems [7] and have inconsistencies in their knowledge when reasoning [8].\nProblem solving is a complex process that humans excel through the interplay of several aspects of their cognitive abilities [9], [10]. It is an acquired skill for humans, often requiring reasoning [11]. It is both logical and creative in nature, hence challenging for LLMs. While LLMs do well on formal linguistic competency tasks, LLM's base knowledge lacks essential problem solving mechanisms that humans possess [12], and cannot solve functional linguistic competency problems [13] well. LLMs struggle at even problems with single correct answers [14].\nCognitive science theories prove concepts are key to problem solving [15]. The term concept used is as in cognitive science literature [16] meaning 'building blocks of thought'. Various problem solving mechanisms like deductive, abductive, and inductive reasoning rely on conceptual information (CI). We call such problems that can be solved with CI as complex conceptual problems (CP). Many science and engineering problems fall in this category of complex CP with often multiple correct answers. LLMs as agents to solve such CPs [17], [18] are crucial due to its implications in advancing engineering and science problems [19]. It has been proven that LLMs solve such complex CP through spurious correlations from training data and often generate false positives (hallucinate) [7], [20], [21]. Hence (plain) LLMs are not well suited for solving complex CPs [22]\u2013[24].\nIn this paper we specifically explore SCMs of LLMs for solving complex CPs. For this problem, the deep customisation of LLMs (like fine-tuning [25], [26] and LoRA [27]) is not feasible due to data sparsity and over-fitting risks. We show that currently popular SCM for LLMs are unsuccessful in solving complex CPs. The models were observed to hallucinate or"}, {"title": "II. EXISTING METHODOLOGY", "content": "For intuitive understanding of the different SCMs and to demonstrate the need for more sophisticated approaches to solve CPs using LLMs we use the following example\nExample 1: If Alice has X sisters and Y brothers. How many sisters does Alice's brother have?\nThe answer to this problem is X + 1, as Alice herself needs to be counted when calculating the total number of sisters for a male sibling of Alice. But as [7] showcases LLMs perform poorly on this CP called as Alice in Wonderland (AIW). For e.g., for X = 3 and Y = 7, the correct response is 4.\nA. Simple Prompt\nWhen normally prompted (no customisation to LLM) as in Fig. 1.1 (X = 4 and Y = 11), the response is incorrect: 4. The model fails to add 1 and this clearly shows the poor capabilities of LLM as reasoners for such a simple problem.\n1) General Framework: Consider a CP C that can be solved with CI and an LLM M which takes as input an NL query and returns an NL response. In general interaction, a user with a CP q pertaining to C and obtains a model response r. The function approximator to model M is m, such that\n$r = m_M(q)$ (1)\nThe interaction can be qualified by following metrics\na) Response Correctness $\\Theta_{correct}$: We define response as correct when it matches the ground truth/ correct response\n$\\Theta_{c} = \\frac{\\sum_{i=1}^{N} k_i}{N}$ (2)\nfor N response samples and the correctness of the ith sample\n$k_i = \\begin{cases} 1, & \\text{if } r_i = r^*, \\\\ 0, & \\text{otherwise.} \\end{cases}$ (3)\nwhere $r_i$ is ith model response and $r^*$ is the correct response. A complex CP can have different aspects of correctness like syntactic and semantic correctness {$\\Theta_{c_i}$} (e.g., in Section V).\nb) Response Cost $\\Theta_{cost}$: Response cost is the cost of generating an LLM response to the CP. It is a parameter of the LLM and the token size (function of total characters in the query and response). Higher token size corresponds to higher response cost. Let $f_M$ be the model's characteristic function to calculate the token size then\n$\\Theta_{cost} = f_M(q + r)$ (4)\nc) Time Consumption $\\Theta_{time}$: If it takes time t to generate a response to given prompt then\n$\\Theta_{time} = t$ (wall-clock seconds) (5)\nd) Model Confidence $\\Theta_{p}$: If we utilise an SCM X (like Simple Prompt), then model confidence is\n$\\Theta_{p} = \\alpha m_M.r$ (6)\nwhere $\\alpha$ is the model's characteristic function (average of probabilities obtained from model for all response tokens) to determine the confidence for the generated response r.\nB. In-context learning\nICL is a one-shot prompting method. The query consists of the CP and an example $e: q + e$. For e.g., on giving an example of Alice with CP for Rabbit (Fig. 1.2) the response is incorrect: 4.\nC. Chain of Thoughts\nIntroduced by [6], CoT is a few-shot prompting method in which the problem is broken down into a series of steps that lead to the correct answer. In some works, CoT is also implemented with a simple addition of \"let's think step-by-step\" to the end of the ICL prompt [28]. In other works, CoT is implemented by adding an explanation of reasoning and using examples, followed by CP.\nWe apply CoT consisting of an explanation of the solution through the example method to AIW as in Fig. 1.3, LLM again gives an incorrect answer.\nFormalising, let set {(qi, ri)} represent an ith query-response pair in the few shot CoT, then model input is\n$q_N = q || (||_{n=0}^{N-1}(q_i, r_i)),$ (|| : concatenation operation) (7)\nfor N number of query-response pair and, response is\n$r = m_M(q_N)$ (8)\nThen response cost and time taken become\n$\\Theta_{cost} = f_M(q_N + r_N)$ (9)\n$\\Theta_{time} = \\sum_{i=1}^{N+1}(t_i)$ (10)\nThe $\\Theta_{p}$ are same as in (6).\nThe LLM cannot solve the AIW variation CP of number of sisters of a sister of Alice instead of brother. For example, following up the CoT conversation chain (Fig. 1.3) with"}, {"title": "III. CHAIN OF CONCEPTS AND CONCEPTUAL ICL", "content": "The poor performance of LLMs on AIW demonstrates that the LLM lacks correct problem solving mechanisms. These mechanisms are the CI important to solve the CP. CI is missing in CoT or ICL where instead the steps to obtain the solution or examples are provided to the LLM. We hypothesise LLMS augmented with CI can solve CP and provide two new methods\nA. Conceptual In-context Learning\nConsider we have a CI (we present a method to construct CI in Section III-B2a). A naive approach will be to construct a single prompt consisting of the CI and CP and provide it to LLM as a zero-shot prompt. Applying this to AIW as in Fig. 1.4 we get the correct answer. The response also consists of the reasoning mechanism explanation. The metrics of this C-ICL method are calculated similar to that of plain ICL.\nExample Of Conceptual Information for AIW: The input prompt in Fig. 1.4 shows the CI for AIW. This CI consists of introduction of calculation of the total number of siblings and explanation of addition of 1 to X + Y, and an example. Next, the concept of role of the gender of a sibling is introduced by an explanation of calculating the sister of a male or female sibling, followed by examples and the CP.\nB. Chain of Concepts\nFor CPs that require multiple concepts, instead of a single input prompt as in C-ICL, concepts can instead be incrementally introduced to the model in multiple inter-related prompts. This method CoC when applied to LLMs for AIW (Fig. 1.5), leads to correct response and a more granular explanation of LLM's problem solving mechanisms. We calculate the metrics for CoC in the same way as CoT.\nFurther on querying the LLM with AIW variation (Example 2) we get correct responses for both C-ICL and CoC.\nAs compared to AIW which has a single correct answer, the complex CPs often have multiple correct responses (e.g., storytelling [29]) making evaluation a challenge and requiring human reviewers. In Section V we apply SCMs and evaluated SCMs on a more complex CP. The algorithm to construct CoC and C-ICL is formalised as\n1) Extending General Framework:\na) Conceptual Information As A Directed-Acyclic Graph: Let's think of CI A (the information required to solve the CP C) as a unidirectional flow of information starting from the general knowledge, called low-level concepts that are more fundamental pieces of information, to the topic which we define as a high-level concept that is a more abstract piece of information. The levels of abstraction represent different models of the same idea but with varying amounts of details [30]. We characterize A as a directed-acyclic graph (DAG) (Fig. 2.1), where each node A represents a concept. Each concept can be represented by lower-level concepts or sub-concepts. The edges E of the DAG link the sub-concepts to the concepts. The direction of the edge shows the increasing abstractness of the concept. Therefore, traversing along the DAG, the nodes that do not have any incoming edges are general knowledge and the final node from which no edge originates is the topic to which the CP belongs. A topic is the most abstract information of the CI while general knowledge"}, {"title": "IV. APPLICATION TO DATA MODEL ENGINEERING", "content": "Data Models are machine-readable structured data representations that relate entities and their relationships according to syntactic rules based on schemata and semantic rules given by the domain. The process of operating on various types of data models is called Data Modeling (DM). DM has widespread applications in industries like defining assets and creating digital twins. In open-world data, many standard formalisms for DM (e.g., AAS [35], OWL [36]) exist. Many companies build PDMs tailored to a specific industry and application, and these are unavailable openly. The domain and DM experts are tasked with the challenge of PDM generation. These challenges are\n1) Current SoTA of PDM generation is manual drafting by DM experts. It incurs high cognitive burden requiring knowledge acquisition of the domain, expertise of DM and PDM modelling paradigms, and learning DM tools.\n2) It is time consuming and requires multiple iterations.\n3) Unavailability of tools for new PDMs makes accessibility and adoption of PDM a challenge.\nLLMs with the base knowledge of the standard data models and DM, formal linguistic competence and NL user interface have potential to solve PDM CPs. However, as LLMs do not possess CI of PDMS, LLMs without any customisation cannot do PDM generation. We comparatively evaluate different SCMs for this task in the next section."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Setup\nThe experimental parameters are detailed in Table I. The customised LLMs are queried with the CP in Example 3 for \u03b2 randomly chosen domains. The target PDM* (based on a company data model) has a JSON serialisation. \u03b3 samples of \u03b2 domains for each of the 3 models of varied sizes and for 5 different SCMs are generated to create a dataset of 495 PDM*s. Fig. 2.2 shows the CI constructed using CoC algorithm. In Fig. 3 a sample that is representative of the dataset for industrial motor domain for the SCMs is shown. The PDM CP is chosen to demonstrate the complexity of real-world CPs that are unavailable openly, require manual evaluation, and have multiple correct answers. As we evaluate the data manually the dataset size is chosen to optimise effort and produce meaningful results. The evaluation metrics are as defined previously and $\\Theta_{correct}$ comprises of 4 aspects\n\u2022 Valid JSON check $\\Theta_{c1}$: the model generates a PDM* that can be parsed to a valid JSON file.\n\u2022 Syntactic check $\\Theta_{c2}$: the valid JSON is passing all syntactic rules of the PDM* schema.\n\u2022 Parroting check $\\Theta_{c3}$ : indicates lack of \"creative\" or \"original\" generations which is a essential for PDM.\n\u2022 Quasi-semantic check $\\Theta_{c4}$: evaluation becomes challenging for this complex CP with multiple correct responses, hence in the current scope we do a quasi-semantic check, defined as average of the 8 KPIs (correctly defined entity 1. names, 2. base, 3. identifiers, 4. constraints, 5. composition, 6. units, 7. datatypes, and 8. relationships, defined with help of an DM expert.\nThe quasi-semantic check is the most crucial correctness aspect for the PDM CP as if it is not semantically meaningful then it is invalid, irrespective of its syntactic correctness.\nB. Experimental Results, Discussion And Related Works\n1) Quantitative evaluation: In Fig. 4 each bar represents a metric averaged over \u03b3 \u00d7 \u03b2 samples for a model and a SCM. The trends are $\\Theta_{c1}$: CoC and CoT performed best for all model sizes, C-ICL showed steep decrease in performance for mid-size model, ICL performed comparatively very poor, and Simple Prompt performed the worst with \u2248 0 valid JSONs. $\\Theta_{c2}$: C-ICL and ICL performed the best with comparable results, followed by CoC and CoT with comparable results and both perform significantly poorly for larger models, Simple Prompt performed worst. $\\Theta_{c3}$: ICL suffered from parroting significantly, rendering it invalid for this particular CP, minor parroting was observed for lower size model with CoT/C-ICL/COC, C-ICL and CoC are highly robust to parroting with none observed for larger models, Simple Prompt was not evaluated as it did not generate any valid PDM*. $\\Theta_{c4}$: CoC performs best with low variance and consistently across all models, comparatively C-ICL suffers heavy variance and also inconsistent across model sizes, CoT performs consistently but poorer than C-ICL and CoC, ICL is not evaluated due to parroting. $\\Theta_{time}$ and $\\Theta_{cost}$: CoC and C-ICL took highest time and incurred highest costs, increasing proportionately with model size. $\\Theta_{p}$: methods that performed poor in correctness showed high confidence indicating hallucination, like for Simple Prompt $\\Theta_{c1}$ is close to 0 but $\\Theta_{p}$ is 0.7.\n2) Qualitative evaluation: 1. Simple Prompt - responses are either \"cannot do the task\u201d or hallucinated imaginary PDM (Fig. 3.1). 2. ICL - high parroting, most responses are same as example PDM* with only attribute names changed as per the query domain (Fig. 3.2). 3. CoT - higher parroting (Fig. 3.3) as compared to CoC/C-ICL, suffers from hallucinations like adding new entities to PDM* schema. 4. C-ICL - more conservative model confidence vs. correctness which can be interpreted as low hallucination, low in robustness compared to CoC as variance is high in $\\Theta_{c}$, some responses contain comprehensive attribute definition (Fig. 3.4) unobserved in ICL/CoT. 5. CoC - more consistent explanation of generated PDM in response, defines relationships, and better in comprehensive attributes definition (Fig. 3.5) as compared to C-ICL.\n3) Discussion: As compared to CoT the combined quasi-semantic correctness with non-parroting increased by 30.6% and 29.88% for C-ICL and CoC respectively. The generated PDMs were of higher semantic and syntactic quality and suffer extremely low hallucination and parroting. CoC vs. C-ICL: CoC gives the best quality. In time/cost constrained scenarios C-ICL with human validation can be a viable alternative. The choice also depends on CI size and correctness aspect. For simple CPs with few concepts like AIW, C-ICL works well. In cases where the CP has multiple correct answers and concepts like PDM, incrementally introducing concepts through CoC has shown advantages, similar to pedagogical teaching of concepts to humans. Model choice: Large model is essential when CI is large, or cost is high. For CoC, smaller model is also a good choice as it shows comparable performance to the best performing larger model and reduces energy consumption. The large non-turbo GPT models performed poorly in CP solving as compared to any sized turbo (improved training). CI: Keeping the CI concise showed better performance with lower cost. Prompt engineering: As a learning from the experiments when converting the CI to NL prompts, the best responses are generated when the prompts are explicit and pedagogical. Transparency benefits: The proposed SCMs generated responses with elaboration of problem solving mechanisms, which can enable auto-validation of responses.\n4) Related Works: Works like [37] show that CoT is poor at solving CPs. Similar to [38], [39] our methods improve over CoT where instead of left-to-right token based decision"}, {"title": "VI. CONCLUSION AND FUTURE DIRECTION", "content": "To the best of the knowledge of the authors this is the first paper providing model-agnostic SCM of LLMs for solving complex CPs. We demonstrated that our proposed SCMs (C-ICL and CoC) solved a real-world complex data engineering CP successfully. The proposed SCMs show a significant improvement over existing SCMs reducing hallucination and parroting issues. They bring forth emergent problem solving capabilities that have never been explicitly included in CI. The introduced SCM CoC performed the best, with potential to generate meaningful first-cut solutions for complex engineering and science tasks. Directions for future work include improving syntactic correctness of CoC, applying CoC to other complex CPs and to multimodal LLMs with multimodal CI."}]}