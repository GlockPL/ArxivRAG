[{"title": "HRSAM: Efficiently Segment Anything in High-Resolution Images", "authors": ["You Huang", "Wenbin Lai", "Jiayi Ji", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "abstract": "The Segment Anything Model (SAM) has significantly advanced interactive seg- mentation but struggles with high-resolution images crucial for high-precision segmentation. This is primarily due to the quadratic space complexity of SAM- implemented attention and the length extrapolation issue in common global atten- tion. This study proposes HRSAM that integrates Flash Attention and incorporates Plain, Shifted and newly proposed Cycle-scan Window (PSCWin) attention to address these issues. The shifted window attention is redesigned with padding to maintain consistent window sizes, enabling effective length extrapolation. The cycle-scan window attention adopts the recently developed State Space Models (SSMs) to ensure global information exchange with minimal computational over- head. Such window-based attention allows HRSAM to perform effective attention computations on scaled input images while maintaining low latency. Moreover, we further propose HRSAM++ that additionally employs a multi-scale strategy to enhance HRSAM's performance. The experiments on the high-precision segmen- tation datasets HQSeg44K and DAVIS show that high-resolution inputs enable the SAM-distilled HRSAM models to outperform the teacher model while maintaining lower latency. Compared to the SOTAS, HRSAM achieves a 1.56 improvement in interactive segmentation's NoC95 metric with only 31% of the latency. HRSAM++ further enhances the performance, achieving a 1.63 improvement in NoC95 with just 38% of the latency.", "sections": [{"title": "1 Introduction", "content": "The Segment Anything Model (SAM) [36] stands as a cornerstone in interactive segmentation [3, 105, 37, 8, 51, 57] and extends its utility to various downstream tasks in computer vision [63, 65, 39, 89, 102, 85]. As an interactive segmentation model, SAM takes simple manual inputs like clicks [31], bounding boxes [36] and coarse masks [37] to predict precise segmentation results, thus reducing the manual labeling costs associated with image segmentation. SAM [36] enhances the overall efficiency of interactive segmentation, setting new benchmarks for speed and performance. Conversely, interactive segmentation serves as a standard for evaluating foundational models [36, 111, 46] on fine-grained visual tasks. This study focuses on mainstream click-based interactive segmentation and further enhances SAM's performance.\nSAM utilizes large Vision Transformers (ViTs) [16, 47] to encode images into embeddings, which are then decoded into segmentation results by a lightweight decoder. Despite SAM's considerable success, the current implementation of SAM is limited to handling fixed input resolutions of 1024 \u00d7 1024. Nevertheless, real-world applications frequently require visual models to handle high-resolution images, such as those exceeding 4096 \u00d7 4096, which offer richer details and potentially improved segmentation results. Directly scaling SAM's input resolution introduces two substantial challenges, i.e., memory inefficiency and lack of scalability to input sizes, which significantly affect performance and efficiency. Firstly, memory inefficiency arises because SAM's ViTs utilize global attention mechanisms, leading to a quadratic increase in space complexity. Even with 204822 resolution inputs, SAM fails to perform inference on commonly used 3090 GPUs due to insufficient GPU memory. Secondly, the scalability issue becomes apparent. Previous studies indicate that merely increasing the image sizes for SAM cannot proportionally improve performance [78]. This issue originates from the length extrapolation problem [41, 78], where the model's attention mechanisms struggle to adapt to varying lengths of image token sequences during training and testing, resulting in suboptimal performance given the extensive computational costs for the scaled input image sizes.\nTo address the above challenges, we introduce HRSAM to harness the performance gains from high-resolution images efficiently. To combat memory inefficiency, we pioneer the integration of Flash Attention [81, 18] within the SAM framework. Flash Attention has been proven across various domains to reduce space complexity from quadratic to linear while maintaining high performance [17, 81]. Notably, the original SAM's attention mechanism employs relative positional encoding, which is incompatible with Flash Attention. To resolve this, we substitute it with Rotary Position Embedding (ROPE) [80]. Regarding the scalability issue, we incorporate the innovative Plain, Shifted and Cycle- scan Window (PSCWin) attention mechanisms. Plain window attention divides image embeddings into non-overlapping windows, facilitating efficient self-attention. We have modified the vanilla shifted window attention [60, 59] by integrating padding to maintain consistent window sizes. This modification enhances performance during length extrapolation while requiring only a marginal increase in computational resources. Furthermore, our novel cycle-scan window attention marries the newly proposed State Space Models (SSMs) [21] with window attention. SSMs [58, 109] treat image embeddings as sequences, scanning them with approximately linear computational complexity and significantly boosting the efficiency of global information exchange. All the window-based attention mechanisms ensure consistent attention computations throughout both the training and testing phases, thereby addressing length extrapolation problems.\nHRSAM further integrates a multi-scale strategy, evolving into HRSAM++, inspired by the successful applications of this strategy [76]. For each image, in addition to resizing to standard resolutions such as 10242 or 20482, we also resize images to a smaller resolution of 5122. These multi-scale inputs are simultaneously processed by HRSAM++ to analyze image features across different scales with high parallelism. This simultaneous processing is enabled by an indexing operator that efficiently organizes the image embeddings into the proposed PSC windows. In specific blocks, the SSMs [21] are employed for effective multi-scale feature fusion."}, {"title": "2 Related Work", "content": "Interactive Segmentation. The integration of deep networks into interactive segmentation begins with DIOS [93], leading to advancements in click-based methods [64, 48, 52, 33, 77]. Subsequent methods focus on enhancing various aspects of interactive segmentation [3, 105, 37, 8, 51, 57, 56, 31, 92, 106, 43, 95, 108, 72, 42]. SAM [36] improves the inference latency by reusing image features, achieving robust zero-shot capabilities and leading to various downstream applications [63, 65, 39, 89, 102, 85, 94, 9]. However, SAM struggles with the high-precision segmentation on high-resolution images, limiting its broader applications. This paper introduces HRSAM to address these issues.\nEfficient Attention. Attention mechanisms [83] have made significant strides in computer vision [16, 22, 35, 79, 90, 103, 18, 86]. Meanwhile, the high computational complexity of attention mechanisms leads to extensive research on efficient attention [60, 110, 23, 90, 60, 82, 47, 25, 28, 32, 11, 87, 97, 15, 73]. Flash Attention [13, 12] re-implements the vanilla attention mechanism efficiently at the CUDA level, reducing the quadratic space complexity to linear and accelerating computations. This study explores window-based attention to facilitate the attention computations on high-resolution images.\nVisual Length Extrapolation. Length extrapolation refers to a model's ability to generalize to longer inputs than those it is trained on [78], which has been explored in NLP [70, 10]. In vision, addressing length extrapolation involves slight modifications of ViTs, e.g., adjustments to positional embeddings and patch sizes [40, 4, 5, 20, 101, 49, 26, 38, 100], and sophisticated training methods, e.g., sequence packing [14] and masking most of image tokens [41]. Recent efforts adapt post-training attention computations to handle scaled input images [78]. Our PSCWin attention achieves scalability to large image size with simple distillation training.\nVisual State Space Model. Several efforts extend the recently developed State-Space Models (SSMs) [21] to vision, including generation [19, 29, 75], multi-modal tasks [98, 45, 71, 84, 107], medical image analysis [62, 88, 99, 54, 74] and remote sensing [7, 6, 53]. Various studies [109, 58, 44, 96, 30, 61, 68, 67] explore 2D scanning strategies to enhance SSM applications. The proposed cycle-scan in this study simplifies scanning by replicating and concatenating image token sequences."}, {"title": "3 Method", "content": "This study introduces HRSAM to tackle SAM's issues of memory inefficiency and length extrapola- tion on high-resolution images. Section 3.1 provides an overview of HRSAM. Section 3.2 presents both the plain window and the improved shifted window attention. Section 3.3 proposes the cycle-scan window attention. Section 3.4 discusses HRSAM++'s multi-scale strategy."}, {"title": "3.1 Overview of HRSAM", "content": "SAM pipeline. SAM integrates a heavy ViT-based image encoder, a compact prompt encoder and a light decoder to efficiently generate segmentation results. Prior to interactive segmentation, images are resized to H \u00d7 W (typically 1024 \u00d7 1024) resolution and undergo offline preprocessing by the image encoder. Such preprocessing produces image embeddings that are repeatedly used in the following steps. During online interaction, the prompt encoder dynamically transforms user prompts such as clicks, bounding boxes and previous segmentation masks into prompt embeddings. The decoder then combines these prompt embeddings with the preprocessed image embeddings to produce the final segmentation results through a series of cross-attention computations. The SAM encoder constitutes > 95% of the overall parameters and has a critical impact on the overall performance. Thus, our HRSAM focuses on optimizing the SAM encoder without modifying the other modules.\nHRSAM encoder. The SAM encoder [36] utilizes ViTs [16] with minor modifications, employing both window-based and global attention in specific blocks [47] and incorporating relative positional encoding during attention computations. Specifically, the SAM-ViT-Base comprises 12 standard transformer blocks, organized into four stages with each stage containing two window-attention blocks and one conventional global-attention block. As depicted in Figure 2, HRSAM also consists of four stages but integrates Plain, Shifted and Cycle-scan Window attention blocks, distributed evenly across each stage. Moreover, HRSAM enhances the encoder by fusing outputs from all four stages through summation, followed by a convolutional block to produce the final C-dimensional image embeddings F \u2208 R^{B\u00d7\\frac{H}{16}\u00d7\\frac{W}{16}\u00d7C} (B is the batch size). Additionally, HRSAM replaces SAM's relative encoding with RoPE [80] to facilitate the use of Flash Attention [13, 12] since the relative encoding introduces biases in the attention maps that are incompatible with Flash Attention [81, 18].\nFlash Attention. Flash Attention [13, 12] addresses the memory inefficiency of the conventional attention mechanism [83] by minimizing high-bandwidth memory (HBM) read/write operations. It achieves this through tiling, where attention inputs are processed in smaller, independent blocks, which recalculates the intermedia results when needed instead of accessing HBM. Additionally, Flash Attention utilizes the fast cache, SRAM, with low latency. These collectively reduce memory overhead and transform the attention's space complexity from quadratic to linear, leading to significant memory efficiency for large-scale attention computations. We provide the details in Appendix A.\nPositional encoding. SAM's attention employs the sophisticated relative positional encoding, which hinders the application of Flash Attention [13, 12]. Thus, we replace the encoding with Rotary Position Embedding (ROPE) [80]."}, {"title": "3.2 Plain and Shifted Window Attention", "content": "Conventional attention. Given a batch of B sequentially arranged embedding sets X \u2208 R^{B\u00d7L\u00d7C} with length L, conventional attention first performs linear projections on X to produce the Q, K, V \u2208 R^{B\u00d7L\u00d7C}. Then, the attention outputs are formulated as\n$$Attention(X) = Linear\\left(Softmax\\left(\\frac{Linear_Q(X) Linear_K(X)^T}{\\sqrt{d}}\\right) Linear_V(X)\\right),$$\nwhere the matrix multiplication and transpose operators are performed independently for each sample. The discussion of multi-head implementation is omitted since it could be easily extended.\nPlain window attention. Given the image embeddings F\u2208R^{B\u00d7H\u00d7W\u00d7C} and window size w, the plain window attention [47] firstly rearranges F into F_w \u2208 R^{B\u00d7\\frac{H}{w}\u00d7\\frac{W}{w}\u00d7w^2\u00d7C}, and then performs the conventional attention, i.e., Attention(Fw), which is finally rearranged into the original shape.\nVanilla shifted window attention. As depicted in Figure 3 (a), the vanilla shifted window attention introduced in the Swin Transformer [60, 59] shifts the embedding windows to enable interactions across neighboring windows. This process involves moving each embedding within F along the x-y axes by a predetermined number of units, with boundary embeddings wrapping around to the opposite side to maintain the dimensions of F. This design facilitates more dynamic receptive fields and connectivity between adjacent windows, enhancing the window attention's ability to integrate contextual information across the entire image.\nPadding shifted window attention. However, the vanilla shifted window leads to inconsistent window sizes across the embedding windows. To address this, we introduce a padding strategy as depicted in Figure 3 (b). This padding strategy utilizes a learnable embedding p \u2208 R^C to add padding around F, ensuring uniform window sizes throughout. Specifically, when applying the vanilla shifting strategy, i.e., shifting F along the x-axis by Sx units and along the y-axis by Sy units, the proposed padding shifted window instead performs padding of w \u2212 Sx units on the left side along the x-axis and w \u2212 Sy units on the top side along the y-axis. Additional padding is applied to the opposite sides as necessary to ensure a complete number of windows. Although this padding strategy increases the size of F, we reduce computational overhead through the replication operator. Before performing the padding, F and the learnable padding embedding p are both projected through the attention's QKV-linear layer. Then, the (Q, K, V) of p are replicated and concatenated to the (Q, K, V) of F. After the attention computation, the paddings are discarded, retaining only the region corresponding to F. The efficiency and performance impacts of such a padding strategy are detailed in Figure 3 (c)."}, {"title": "3.3 Cycle-scan Window Attention", "content": "State Space Models. Structured State Space Models (SSMs) [21] are a class of sequence models designed for sequence-to-sequence transformation, adept at capturing long dependencies among sequential tokens. Specifically, SSMs map a one-dimensional sequence x(t) \u2208 R to y(t) \u2208 R through an intermediate latent state h(t) \u2208 RN, formulated as:\n$$h'(t) = Ah(t) + Bx(t),$$\n$$y(t) = Ch(t),$$\nwhere the matrices A \u2208 R^{N\u00d7N}, B\u2208 R^{N\u00d71} and C\u2208 R^{1\u00d7N} are predefined. Actually, the SSMs are discretized through a zero-order hold rule with a given sample timescale \u0394\u2208 R, formulated as\n$$\\overline{A} = e^{\u0394A}, \\overline{B} = (\u0394A)^{-1} (e^{\u0394A} \u2013 I) \u0394B, \\overline{C} = C,$$\nwhich implies the following iterative process:\n$$h_t = \\overline{A}h_{t-1}+ \\overline{B}x_t,$$\n$$Y_t = \\overline{C}h_t,"}, {"title": "3.4 Multi-scale Fusion", "content": "Multi-scale inputs. Multi-scale strategies have been extensively explored in visual tasks due to the proven effectiveness [76]. In this study, we propose a specific multi-scale strategy to address the issue of length extrapolation. This strategy involves resizing any image to a fixed size (e.g., 5122), which provides an overview of the entire scene. Such overview enhances the model's global understanding and alleviates the issues posed by variable receptive fields across different input sizes. Thus, we further propose an enhanced version of our model, the HRSAM++, which incorporates the additional downsampled input image as depicted in Figure 4. Both the original and additional images are processed concurrently. Multi-scale features from these images are integrated at the end of each stage in the HRSAM++. Further details are discussed subsequently.\nMulti-scale attention. In HRSAM++, each attention operator is replaced by a multi-scale version, which is facilitated through the block-diagonal mask from Flash Attention [13, 12], enabling parallel processing of multi-scaled inputs. Specifically, both the original H \u00d7 W image and the downsampled H \u00d7 W image are patchified and concatenated to form a sequence of image tokens with a length of (HW+HW)/162, as illustrated in Figure 4. During the window attention computations, this token sequence is reorganized by indexing to ensure tokens within the same window are sequential and tokens from the same scale remain contiguous. This reorganization enables the use of Flash Attention for attention computation across the entire token sequence, utilizing the block-diagonal mask to skip unnecessary cross-window attention. This implementation is more efficient and computationally equivalent to performing attention on isolated windows. Additionally, for padding shifted windows, the learnable padding embedding is firstly appended at the sequence's end. Then, this padded sequence is similarly reorganized with appropriate token duplication, to maintain size consistency of the windows and undergo the same attention processing. We provide the details in Appendix B C.\nMulti-scale cycle-scan module. For multi-scale inputs, the cycle-scan module operates in two modes with subtle differences. As shown in Figure 4, HRSAM++ extends the four stages of HRSAM by incorporating an additional multi-scale cycle-scan module, while retaining the original cycle-scan as a single-scale operation within each stage. Both the single and multi-scale cycle-scan modules follow the same process, differing only in the SSMs. The single-scale cycle-scan first splits the tokens by scale, scan each scale's tokens separately by the SSM, and then concatenates them for post-computation. In contrast, the multi-scale cycle-scan directly performs the SSM across the tokens from all the scales, facilitating the multi-scale fusion."}, {"title": "4 Experiments", "content": "Section 4.1 details the experimental setup. Section 4.2 discusses the main results. Section 4.3 evaluate HRSAM's scalability to input sizes. Section 4.4 provides the ablation study of HRSAM's modules. Section 4.5 presents the qualitative results."}, {"title": "4.1 Experimental Setting", "content": "We provide the details in Appendix D and the code in the supplementary materials.\nDatasets. We adopt COCO [50], LVIS [24] and HQSeg44K [34] datasets for our two-stage training process. In the first stage, we use only the images from COCO and LVIS without labels, which differs from previous interactive segmentation methods [8, 57, 56, 31]. In the second stage, we finetune the weights pretrained at the first stage using the HQSeg44K training dataset. For testing, we evaluate exclusively on the HQSeg44K validation [34] and DAVIS [69] datasets, both of which have high-precision annotations, following recent advances in high-precision interactive segmentation [55].\nTraining strategy. HRSAM and HRSAM++ undergo the same two-stage training process. In the first stage, we use images from the COCO and LVIS datasets without labels. We employ the vanilla MSE as the loss in the SAM distillation, and minimize the MSE between HRSAM and the teacher's image embeddings. In the second stage, we further finetune the models using HQSeg44K training samples with segmentation annotations, employing the commonly used normalized focal loss [8, 57, 56, 31].\nEvaluation. We evaluate HRSAMs against previous methods [37, 8, 56, 31, 55, 36, 104, 91]. In testing, click simulation places clicks at the centers of erroneously predicted regions, aligning with the previous methods [8, 56, 31]. We assess all models' segmentation performance and speed. Segmentation performance is measured using 5-mIoU and NoC metrics. The 5-mIoU represents the average IoU after the fifth click. The NoC metric indicates the average minimum clicks required to reach a specified IoU. We focus on NoC@90 and NoC@95 within 20 clicks. Speed is quantified as Seconds Per Click (SPC) on GPUs, measuring the average inference latency per click, averaged over 20 clicks. For SAM series models, this includes the preprocessing time by the backbones."}, {"title": "4.2 Main Results", "content": "As shown in Table 1, when the HRSAM and HRSAM++ are trained by SAM-distillation on 10242 inputs, scaling the testing input to 20482 allows them to surpass their teacher SAM-ViT-Huge in performance while retaining a significant speed advantage. These results highlight the scalability of our HRSAM models with respect to input size. Moreover, the further HQ-finetuned HRSAM++ achieves state-of-the-art performance on the high-precision segmentation datasets HQSeg44K and DAVIS, while maintaining a faster SPC latency compared to the previous state-of-the-art SegNext."}, {"title": "4.3 Scale Analysis", "content": "To analyze the scalability of HRSAM with increased input sizes, we evaluate both HRSAM and HRSAM++ using input sizes of 10242, 20482, 30722 and 40962. The performance of the models are tested on the HQSeg44K dataset, focusing on NoC@90 and NoC@95 metrics. As shown in Table 2, both HRSAM and HRSAM++ scale from the 10242 training resolution to 20482 testing size effectively. However, for larger input sizes, such as 30722 and 40962, HRSAM's performance deteriorates. Even when the images larger than 20482 are resized to 30722 or 40962, the performance is inferior to resizing to 20482. This decline can be attributed to two main reasons. First, resizing to larger images requires a more robust resizing strategy. The linear interpolation used in this study cannot adequately preserve image details. Second, the models are trained on 10242 inputs, but handling inputs larger than 30722 involves processing image token sequences more than 9\u00d7 longer, exceeding the models' length extrapolation capabilities. On the other hand, HRSAM++ shows that for larger images (greater than 20482), resizing to 30722 can further improve performance, confirming the effectiveness of HRSAM++'s multi-scale strategy."}, {"title": "4.4 Ablation Study", "content": "In the ablation study, we train HRSAM and its various ablation variants using the same distillation protocol in the main experiments. We analyze the impact of different modules, including Global attention (Global), Plain Window attention (Plain Win), Vanilla Shifted Window attention (Van Swin), Padding Shifted Window attention (Pad Swin), Single-Scale Cycle-scan (SS CScan) and Multi-Scale Cycle-scan (MS CScan). We compare the latency and performance of these variant backbones under 10242 and 20482 input sizes. As shown in Table 3, the Global attention achieves better performance with 10242 inputs due to its strong representation capability, which allows it to effectively learn from the teacher model. However, when scaled to 20482 inputs, its performance deteriorated significantly compared to other HRSAM variants, with extremely high latency. Conversely, while some HRSAM variants show only moderate performance over 10242 inputs, they generally perform well when scaled to 20482. Notably, HRSAM++ with the Multi-Scale Cycle-scan module demonstrates significant improvements over 20482 inputs, highlighting the effectiveness of our proposed HRSAM and HRSAM++ in enhancing model scalability to input image sizes."}, {"title": "4.5 Qualitative Results", "content": "In Figure 5, we present a qualitative comparison between our HRSAM++ model and the previous SOTA SegNext [55], using a set of challenging samples with thin edges. We visualize the segmentation results across four different clicks, clearly demonstrating the superior performance of HRSAM++ in high-precision segmentation tasks."}, {"title": "5 Limitation", "content": "HRSAM cannot scale effectively to excessively large images, as shown in our multi-scale analysis. Over-enlarging input images or processing particularly large images at extremely high resolutions does not improve performance. This presents a challenge in determining the optimal input image size, which HRSAM currently cannot address. Future work will focus on developing adaptive methods to set input sizes more effectively."}, {"title": "6 Conclusion", "content": "In this paper, we propose HRSAM to address the limitations of SAM in handling high-resolution images critical for high-precision segmentation tasks. HRSAM leverages Flash Attention to resolve the fundamental issue of memory insufficiency on large input images. Then, HRSAM incorporates PSCWin attention ensuring computational consistency during both training and testing. The enhanced version, HRSAM++ additionally adopts a multi-scale strategy. HRSAM models set new benchmarks in high-precision interactive segmentation. Furthermore, our findings reveal a cost-effective training strategy for building high-performance visual foundation models through visual length extrapolation."}, {"title": "A Flash Attention", "content": "The following introduction to Flash Attention [13, 12] is based on [2]."}, {"title": "A.1 Motivation", "content": "Flash Attention is motivated by the need to optimize memory bandwidth and computation speed in GPUs, particularly leveraging High Bandwidth Memory (HBM) and Static Random-Access Memory (SRAM). In GPUs like the A100-40GB, HBM provides 40GB of memory with a bandwidth of 1.5TB/s. However, actual GPU computations occur in SRAM, which has a much higher bandwidth of 19TB/s but is limited to only 20MB of usable memory. The substantial speed difference (SRAM being 12.67 times faster than HBM) and the limited SRAM capacity create a bottleneck when memory exchanges between SRAM and HBM are required.\nThe primary goal of Flash Attention is to minimize memory exchanges between SRAM and HBM during attention computation. Traditional attention mechanisms require seven exchanges between these memory types, creating significant overhead. Flash Attention aims to perform the entire attention computation within the 20MB SRAM, thus avoiding these costly memory swaps and achieving up to a 7.6\u00d7 speedup for N \u00d7 N matrix computations."}, {"title": "A.2 SRAM Cache Overflow Example", "content": "To illustrate the efficiency of Flash Attention, consider a scenario where SRAM can handle 10000 data points at a time. For a single vector, let's say Q and K are both of size [100, 100]. Loading both vectors would require 2 \u00d7 100 \u00d7 100 = 20000 data points, which exceeds the 10000 data points capacity of SRAM, necessitating write/read operations to HBM.\nTo address this, Flash Attention splits the vectors into smaller chunks:\n\u2022 Split Q[100, 100] into {Q1[50, 100], Q2[50, 100]}\n\u2022 Split K [100, 100] into {K1[50, 100], K2[50, 100]}\nThis way, the computation of the attention score\n$$score_{ij} = Q_iK_j^T$$\ncan be completed entirely within SRAM, avoiding the need to swap data with HBM. By process- ing these smaller chunks within the high-speed SRAM, Flash Attention efficiently computes the attention scores block-wise, thereby maintaining high computational speed and minimizing memory bottlenecks.\nIn summary, Flash Attention leverages the speed of SRAM by minimizing memory exchanges and efficiently handling data within the limited capacity of SRAM, resulting in significant performance improvements in attention computation. Next, we detail the algorithm of Flash Attention, beginning with the reformulation of attention's softmax operator."}, {"title": "A.3 Softmax", "content": "Numerically stable softmax. Before discussing further details of Flash Attention, we reformulate the conventional softmax operator into a numerically stable version as follows:\n$$softmax(x_i) = \\frac{exp(x_i \u2013 max_k x_k)}{\\sum_{j=1}^n exp(x_j \u2013 max_k x_k)}."}, {"title": "Online softmax", "content": "The softmax has a computationally equivalent online version [66], implemented with three-pass as follows:\n$$\nm_0 \\leftarrow \u2013 \u221e$$\n$$d_0 \\leftarrow 0.0$$\nfor i \u2190 1, N\n$$m_i \\leftarrow max(m_{i-1}, x_i)$$\nfor i \u2190 1, N\n$$d_i \\leftarrow d_{i\u22121} + exp(x_i - m_i)$$\nfor i \u2190 1, N\n$$a_i \\leftarrow exp(x_i - m_N)/d_N$$\nWe then reformulate di with the iterative version, d', defined as\n$$d'_i = \\sum_{j=1}^i exp(x_j - m_i)$$\n$$\n= (\\sum_{j=1}^{i-1} exp(x_j - m_i)) + exp(x_i - m_i)$$\n$$\n= exp(m_{i-1} - m_i) (\\sum_{j=1}^{i-1} exp(x_j - m_{i-1})) + exp(x_i - m_i)$$\n$$\n= exp(m_{i-1} - m_i) d'_{i-1} + exp(x_i \u2013 m_i)$$\nwhich implies the following two-pass implementation of online softmax:\n$$\nm_0 \\leftarrow \u2013 \u221e$$\n$$d_0 \\leftarrow 0.0$$\nfor i \u2190 1, N\n$$m_i \\leftarrow max(m_{i-1}, x_i)$$\n$$d'_i \\leftarrow exp(m_{i-1} - m_i) d'_{i-1} + exp(x_i \u2013 m_i)$$\nfor i \u2190 1, N\n$$a_i \\leftarrow exp(x_i - m_N)/d'_N"}, {"title": "A.4 Flash Attention by Online Softmax", "content": "Two-pass Attention. Based on the two-pass online softmax, we derive a two-pass implementation of attention, with the pre-computated Q, K, V \u2208 RN\u00d7d, as follows:\nfor i \u2190 1, N\n$$x_i \\leftarrow Q[k, :]K^T[:, i]$$\n$$m_i \\leftarrow max(m_{i-1}, x_i)$$\n$$d'_i \\leftarrow exp(m_{i-1} - m_i) d'_{i-1} + exp(x_i - m_i)$$\nfor i \u2190 1, N\n$$a_i \\leftarrow \\frac{exp(x_i - m_N)}{d'_N}$$\n$$O_i \\leftarrow O_{i-1} + a_iV[i, :]$$\nVanilla Flash Attention. We reformulate the output or as\n$$O_i = O_{i-1}+ \\frac{exp(x_i-m_N)}{d'_N}V[i, :]$$\n$$\n\\rightarrow O_i = \\sum_{j=1}^i \\frac{exp(x_j-m_N)}{d'_N} V[j, :] "}, {"title": "", "content": "which inspires the following definition\n$$\\phi_i = \\sum_{j=1"}, "i \\frac{exp(x_j-m_i)}{d'_i} V[j, :"], "form": "n$$\\phi'_i = \\sum_{j=1"}, {"V[j,": "n$$\n= (\\sum_{j=1"}, {"V[j,": "frac{exp(x_i - m_i)"}, {"V[i,": "n$$\n=(\\sum_{j=1"}, {"V[j,": "frac{exp(x_i - m_i)"}, {"V[i,": "n$$\n= \\frac{exp(m_{i-1"}, {"frac{exp(x_j-m_{i-1})}{d'_{i-1}}d'_{i-1}V[j,": "frac{exp(x_i - m_i)"}, {"V[i,": "n$$\n= \\frac{exp(m_{i-1"}, {"V[i,": ""}, {"V[i,": ""}, {"Attention": "nfor i \u2190 1", "Q[k,": "K^T[:", "V[i,": ""}, {"d'_i}$$\nend\n$$O[k,": "leftarrow \\phi'_N$$\n### A.5 Tiling Flash-Attention\nWhen dealing with multiple data tiles", "follows": "nfor i \u2190 1#tiles\n$$x_i \\leftarrow Q[k", "K^T[": "i \u2212 1)b"}]