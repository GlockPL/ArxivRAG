{"title": "HRSAM: Efficiently Segment Anything in\nHigh-Resolution Images", "authors": ["You Huang", "Wenbin Lai", "Jiayi Ji", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "abstract": "The Segment Anything Model (SAM) has significantly advanced interactive seg-\nmentation but struggles with high-resolution images crucial for high-precision\nsegmentation. This is primarily due to the quadratic space complexity of SAM-\nimplemented attention and the length extrapolation issue in common global atten-\ntion. This study proposes HRSAM that integrates Flash Attention and incorporates\nPlain, Shifted and newly proposed Cycle-scan Window (PSCWin) attention to\naddress these issues. The shifted window attention is redesigned with padding to\nmaintain consistent window sizes, enabling effective length extrapolation. The\ncycle-scan window attention adopts the recently developed State Space Models\n(SSMs) to ensure global information exchange with minimal computational over-\nhead. Such window-based attention allows HRSAM to perform effective attention\ncomputations on scaled input images while maintaining low latency. Moreover,\nwe further propose HRSAM++ that additionally employs a multi-scale strategy to\nenhance HRSAM's performance. The experiments on the high-precision segmen-\ntation datasets HQSeg44K and DAVIS show that high-resolution inputs enable the\nSAM-distilled HRSAM models to outperform the teacher model while maintaining\nlower latency. Compared to the SOTAS, HRSAM achieves a 1.56 improvement in\ninteractive segmentation's NoC95 metric with only 31% of the latency. HRSAM++\nfurther enhances the performance, achieving a 1.63 improvement in NoC95 with\njust 38% of the latency.", "sections": [{"title": "Introduction", "content": "The Segment Anything Model (SAM) [36] stands as a cornerstone in interactive segmentation [3,\n105, 37, 8, 51, 57] and extends its utility to various downstream tasks in computer vision [63, 65,\n39, 89, 102, 85]. As an interactive segmentation model, SAM takes simple manual inputs like\nclicks [31], bounding boxes [36] and coarse masks [37] to predict precise segmentation results, thus\nreducing the manual labeling costs associated with image segmentation. SAM [36] enhances the\noverall efficiency of interactive segmentation, setting new benchmarks for speed and performance.\nConversely, interactive segmentation serves as a standard for evaluating foundational models [36,\n111, 46] on fine-grained visual tasks. This study focuses on mainstream click-based interactive\nsegmentation and further enhances SAM's performance.\nSAM utilizes large Vision Transformers (ViTs) [16, 47] to encode images into embeddings, which are\nthen decoded into segmentation results by a lightweight decoder. Despite SAM's considerable success,\nthe current implementation of SAM is limited to handling fixed input resolutions of 1024 \u00d7 1024.\nNevertheless, real-world applications frequently require visual models to handle high-resolution\nimages, such as those exceeding 4096 \u00d7 4096, which offer richer details and potentially improved\nsegmentation results. Directly scaling SAM's input resolution introduces two substantial challenges,\ni.e., memory inefficiency and lack of scalability to input sizes, which significantly affect performance\nand efficiency. Firstly, memory inefficiency arises because SAM's ViTs utilize global attention\nmechanisms, leading to a quadratic increase in space complexity. Even with 20482 resolution inputs,\nSAM fails to perform inference on commonly used 3090 GPUs due to insufficient GPU memory.\nSecondly, the scalability issue becomes apparent. Previous studies indicate that merely increasing\nthe image sizes for SAM cannot proportionally improve performance [78]. This issue originates\nfrom the length extrapolation problem [41, 78], where the model's attention mechanisms struggle to\nadapt to varying lengths of image token sequences during training and testing, resulting in suboptimal\nperformance given the extensive computational costs for the scaled input image sizes.\nTo address the above challenges, we introduce HRSAM to harness the performance gains from\nhigh-resolution images efficiently. To combat memory inefficiency, we pioneer the integration of\nFlash Attention [81, 18] within the SAM framework. Flash Attention has been proven across various\ndomains to reduce space complexity from quadratic to linear while maintaining high performance [17,\n81]. Notably, the original SAM's attention mechanism employs relative positional encoding, which is\nincompatible with Flash Attention. To resolve this, we substitute it with Rotary Position Embedding\n(ROPE) [80]. Regarding the scalability issue, we incorporate the innovative Plain, Shifted and Cycle-\nscan Window (PSCWin) attention mechanisms. Plain window attention divides image embeddings\ninto non-overlapping windows, facilitating efficient self-attention. We have modified the vanilla\nshifted window attention [60, 59] by integrating padding to maintain consistent window sizes. This\nmodification enhances performance during length extrapolation while requiring only a marginal\nincrease in computational resources. Furthermore, our novel cycle-scan window attention marries the\nnewly proposed State Space Models (SSMs) [21] with window attention. SSMs [58, 109] treat image\nembeddings as sequences, scanning them with approximately linear computational complexity and\nsignificantly boosting the efficiency of global information exchange. All the window-based attention\nmechanisms ensure consistent attention computations throughout both the training and testing phases,\nthereby addressing length extrapolation problems.\nHRSAM further integrates a multi-scale strategy, evolving into HRSAM++, inspired by the successful\napplications of this strategy [76]. For each image, in addition to resizing to standard resolutions such\nas 10242 or 20482, we also resize images to a smaller resolution of 5122. These multi-scale inputs\nare simultaneously processed by HRSAM++ to analyze image features across different scales with\nhigh parallelism. This simultaneous processing is enabled by an indexing operator that efficiently\norganizes the image embeddings into the proposed PSC windows. In specific blocks, the SSMs [21]\nare employed for effective multi-scale feature fusion."}, {"title": "Related Work", "content": "Interactive Segmentation. The integration of deep networks into interactive segmentation begins\nwith DIOS [93], leading to advancements in click-based methods [64, 48, 52, 33, 77]. Subsequent\nmethods focus on enhancing various aspects of interactive segmentation [3, 105, 37, 8, 51, 57, 56, 31,\n92, 106, 43, 95, 108, 72, 42]. SAM [36] improves the inference latency by reusing image features,\nachieving robust zero-shot capabilities and leading to various downstream applications [63, 65, 39,\n89, 102, 85, 94, 9]. However, SAM struggles with the high-precision segmentation on high-resolution\nimages, limiting its broader applications. This paper introduces HRSAM to address these issues.\nEfficient Attention. Attention mechanisms [83] have made significant strides in computer vision [16,\n22, 35, 79, 90, 103, 18, 86]. Meanwhile, the high computational complexity of attention mechanisms\nleads to extensive research on efficient attention [60, 110, 23, 90, 60, 82, 47, 25, 28, 32, 11, 87, 97,\n15, 73]. Flash Attention [13, 12] re-implements the vanilla attention mechanism efficiently at the\nCUDA level, reducing the quadratic space complexity to linear and accelerating computations. This\nstudy explores window-based attention to facilitate the attention computations on high-resolution\nimages.\nVisual Length Extrapolation. Length extrapolation refers to a model's ability to generalize to longer\ninputs than those it is trained on [78], which has been explored in NLP [70, 10]. In vision, addressing\nlength extrapolation involves slight modifications of ViTs, e.g., adjustments to positional embeddings\nand patch sizes [40, 4, 5, 20, 101, 49, 26, 38, 100], and sophisticated training methods, e.g., sequence\npacking [14] and masking most of image tokens [41]. Recent efforts adapt post-training attention\ncomputations to handle scaled input images [78]. Our PSCWin attention achieves scalability to large\nimage size with simple distillation training.\nVisual State Space Model. Several efforts extend the recently developed State-Space Models\n(SSMs) [21] to vision, including generation [19, 29, 75], multi-modal tasks [98, 45, 71, 84, 107],\nmedical image analysis [62, 88, 99, 54, 74] and remote sensing [7, 6, 53]. Various studies [109, 58,\n44, 96, 30, 61, 68, 67] explore 2D scanning strategies to enhance SSM applications. The proposed\ncycle-scan in this study simplifies scanning by replicating and concatenating image token sequences."}, {"title": "Method", "content": "This study introduces HRSAM to tackle SAM's issues of memory inefficiency and length extrapola-\ntion on high-resolution images. Section 3.1 provides an overview of HRSAM. Section 3.2 presents\nboth the plain window and the improved shifted window attention. Section 3.3 proposes the cycle-scan\nwindow attention. Section 3.4 discusses HRSAM++'s multi-scale strategy."}, {"title": "Overview of HRSAM", "content": "SAM pipeline. SAM integrates a heavy ViT-based image encoder, a compact prompt encoder and a\nlight decoder to efficiently generate segmentation results. Prior to interactive segmentation, images\nare resized to H \u00d7 W (typically 1024 \u00d7 1024) resolution and undergo offline preprocessing by the\nimage encoder. Such preprocessing produces image embeddings that are repeatedly used in the\nfollowing steps. During online interaction, the prompt encoder dynamically transforms user prompts\nsuch as clicks, bounding boxes and previous segmentation masks into prompt embeddings. The\ndecoder then combines these prompt embeddings with the preprocessed image embeddings to produce\nthe final segmentation results through a series of cross-attention computations. The SAM encoder\nconstitutes > 95% of the overall parameters and has a critical impact on the overall performance.\nThus, our HRSAM focuses on optimizing the SAM encoder without modifying the other modules.\nHRSAM encoder. The SAM encoder [36] utilizes ViTs [16] with minor modifications, employing\nboth window-based and global attention in specific blocks [47] and incorporating relative positional\nencoding during attention computations. Specifically, the SAM-ViT-Base comprises 12 standard\ntransformer blocks, organized into four stages with each stage containing two window-attention\nblocks and one conventional global-attention block. As depicted in Figure 2, HRSAM also consists of\nfour stages but integrates Plain, Shifted and Cycle-scan Window attention blocks, distributed evenly\nacross each stage. Moreover, HRSAM enhances the encoder by fusing outputs from all four stages\nthrough summation, followed by a convolutional block to produce the final C-dimensional image\nembeddings  F \u2208 R^{B\u00d7H/16\u00d7W/16\u00d7C} (B is the batch size). Additionally, HRSAM replaces SAM's relative\nencoding with RoPE [80] to facilitate the use of Flash Attention [13, 12] since the relative encoding\nintroduces biases in the attention maps that are incompatible with Flash Attention [81, 18].\nFlash Attention. Flash Attention [13, 12] addresses the memory inefficiency of the conventional\nattention mechanism [83] by minimizing high-bandwidth memory (HBM) read/write operations. It\nachieves this through tiling, where attention inputs are processed in smaller, independent blocks,\nwhich recalculates the intermedia results when needed instead of accessing HBM. Additionally,\nFlash Attention utilizes the fast cache, SRAM, with low latency. These collectively reduce memory\noverhead and transform the attention's space complexity from quadratic to linear, leading to significant\nmemory efficiency for large-scale attention computations. We provide the details in Appendix A.\nPositional encoding. SAM's attention employs the sophisticated relative positional encoding, which\nhinders the application of Flash Attention [13, 12]. Thus, we replace the encoding with Rotary\nPosition Embedding (ROPE) [80]."}, {"title": "Plain and Shifted Window Attention", "content": "Conventional attention. Given a batch of B sequentially arranged embedding sets  X \u2208 R^{B\u00d7L\u00d7C}\nwith length L, conventional attention first performs linear projections on X to produce the Q, K, V \u2208\nR^{B\u00d7L\u00d7C}. Then, the attention outputs are formulated as\nAttention(X) = Linear(\\text{Softmax} (\\frac{\\text{Linear}_Q (X)(\\text{Linear}_K(X))^T}{\\sqrt{d}} )\\text{Linear}_V(X)),\nwhere the matrix multiplication and transpose operators are performed independently for each sample.\nThe discussion of multi-head implementation is omitted since it could be easily extended.\nPlain window attention. Given the image embeddings F\u2208R^{B\u00d7H/16\u00d7W/16\u00d7C}and window size w, the\nplain window attention [47] firstly rearranges F into  F_w \u2208 R^{B\u00d7H/(16w)\u00d7W/(16w)\u00d7w^2\u00d7C}, w2 and then performs the\nconventional attention, i.e., Attention(Fw), which is finally rearranged into the original shape.\nVanilla shifted window attention. As depicted in Figure 3 (a), the vanilla shifted window attention\nintroduced in the Swin Transformer [60, 59] shifts the embedding windows to enable interactions\nacross neighboring windows. This process involves moving each embedding within F along the\nx-y axes by a predetermined number of units, with boundary embeddings wrapping around to the\nopposite side to maintain the dimensions of F. This design facilitates more dynamic receptive fields\nand connectivity between adjacent windows, enhancing the window attention's ability to integrate\ncontextual information across the entire image.\nPadding shifted window attention. However, the vanilla shifted window leads to inconsistent\nwindow sizes across the embedding windows. To address this, we introduce a padding strategy as\ndepicted in Figure 3 (b). This padding strategy utilizes a learnable embedding p \u2208 RC to add padding\naround F, ensuring uniform window sizes throughout. Specifically, when applying the vanilla shifting\nstrategy, i.e., shifting F along the x-axis by Sx units and along the y-axis by Sy units, the proposed\npadding shifted window instead performs padding of w - Sx units on the left side along the x-axis\nand w \u2212 Sy units on the top side along the y-axis. Additional padding is applied to the opposite sides\nas necessary to ensure a complete number of windows. Although this padding strategy increases the\nsize of F, we reduce computational overhead through the replication operator. Before performing\nthe padding, F and the learnable padding embedding p are both projected through the attention's\nQKV-linear layer. Then, the (Q, K, V) of p are replicated and concatenated to the (Q, K, V) of F.\nAfter the attention computation, the paddings are discarded, retaining only the region corresponding\nto F. The efficiency and performance impacts of such a padding strategy are detailed in Figure 3 (c)."}, {"title": "Cycle-scan Window Attention", "content": "State Space Models. Structured State Space Models (SSMs) [21] are a class of sequence models\ndesigned for sequence-to-sequence transformation, adept at capturing long dependencies among\nsequential tokens. Specifically, SSMs map a one-dimensional sequence x(t) \u2208 R to y(t) \u2208 R through\nan intermediate latent state h(t) \u2208 RN, formulated as:\nh'(t) = Ah(t) + Bx(t),\ny(t) = Ch(t),\nwhere the matrices  A \u2208 R^{N\u00d7N}, B\u2208 RN\u00d71 and C\u2208 R^{1\u00d7N} are predefined. Actually, the SSMs are\ndiscretized through a zero-order hold rule with a given sample timescale \u0394\u2208 R, formulated as\n\u0100 = e^{\u0394A}, B = (\u0394A)^{-1} (e^{\u0394A} \u2013 I) \u0394B, C = C,\nwhich implies the following iterative process:\nh_t = \u0100h_{t-1}+ Bx_t,\ny_t = Ch_t,"}, {"title": "Multi-scale Fusion", "content": "Multi-scale inputs. Multi-scale strategies have been extensively explored in visual tasks due to\nthe proven effectiveness [76]. In this study, we propose a specific multi-scale strategy to address\nthe issue of length extrapolation. This strategy involves resizing any image to a fixed size (e.g.,\n5122), which provides an overview of the entire scene. Such overview enhances the model's global\nunderstanding and alleviates the issues posed by variable receptive fields across different input sizes.\nThus, we further propose an enhanced version of our model, the HRSAM++, which incorporates the\nadditional downsampled input image as depicted in Figure 4. Both the original and additional images\nare processed concurrently. Multi-scale features from these images are integrated at the end of each\nstage in the HRSAM++. Further details are discussed subsequently.\nMulti-scale attention. In HRSAM++, each attention operator is replaced by a multi-scale version,\nwhich is facilitated through the block-diagonal mask from Flash Attention [13, 12], enabling parallel\nprocessing of multi-scaled inputs. Specifically, both the original H \u00d7 W image and the downsampled\nH \u00d7 W image are patchified and concatenated to form a sequence of image tokens with a length of\n(HW+HW)/162, as illustrated in Figure 4. During the window attention computations, this token\nsequence is reorganized by indexing to ensure tokens within the same window are sequential and\ntokens from the same scale remain contiguous. This reorganization enables the use of Flash Attention\nfor attention computation across the entire token sequence, utilizing the block-diagonal mask to skip\nunnecessary cross-window attention. This implementation is more efficient and computationally\nequivalent to performing attention on isolated windows. Additionally, for padding shifted windows,\nthe learnable padding embedding is firstly appended at the sequence's end. Then, this padded\nsequence is similarly reorganized with appropriate token duplication, to maintain size consistency of\nthe windows and undergo the same attention processing. We provide the details in Appendix B C.\nMulti-scale cycle-scan module. For multi-scale inputs, the cycle-scan module operates in two modes\nwith subtle differences. As shown in Figure 4, HRSAM++ extends the four stages of HRSAM by\nincorporating an additional multi-scale cycle-scan module, while retaining the original cycle-scan\nas a single-scale operation within each stage. Both the single and multi-scale cycle-scan modules\nfollow the same process, differing only in the SSMs. The single-scale cycle-scan first splits the\ntokens by scale, scan each scale's tokens separately by the SSM, and then concatenates them for\npost-computation. In contrast, the multi-scale cycle-scan directly performs the SSM across the tokens\nfrom all the scales, facilitating the multi-scale fusion."}, {"title": "Experiments", "content": "Section 4.1 details the experimental setup. Section 4.2 discusses the main results. Section 4.3 evaluate\nHRSAM's scalability to input sizes. Section 4.4 provides the ablation study of HRSAM's modules.\nSection 4.5 presents the qualitative results."}, {"title": "Experimental Setting", "content": "We provide the details in Appendix D and the code in the supplementary materials.\nDatasets. We adopt COCO [50], LVIS [24] and HQSeg44K [34] datasets for our two-stage training\nprocess. In the first stage, we use only the images from COCO and LVIS without labels, which\ndiffers from previous interactive segmentation methods [8, 57, 56, 31]. In the second stage, we\nfinetune the weights pretrained at the first stage using the HQSeg44K training dataset. For testing, we\nevaluate exclusively on the HQSeg44K validation [34] and DAVIS [69] datasets, both of which have\nhigh-precision annotations, following recent advances in high-precision interactive segmentation [55].\nTraining strategy. HRSAM and HRSAM++ undergo the same two-stage training process. In the first\nstage, we use images from the COCO and LVIS datasets without labels. We employ the vanilla MSE\nas the loss in the SAM distillation, and minimize the MSE between HRSAM and the teacher's image\nembeddings. In the second stage, we further finetune the models using HQSeg44K training samples\nwith segmentation annotations, employing the commonly used normalized focal loss [8, 57, 56, 31].\nEvaluation. We evaluate HRSAMs against previous methods [37, 8, 56, 31, 55, 36, 104, 91]. In\ntesting, click simulation places clicks at the centers of erroneously predicted regions, aligning with\nthe previous methods [8, 56, 31]. We assess all models' segmentation performance and speed.\nSegmentation performance is measured using 5-mIoU and NoC metrics. The 5-mIoU represents the\naverage IoU after the fifth click. The NoC metric indicates the average minimum clicks required to\nreach a specified IoU. We focus on NoC@90 and NoC@95 within 20 clicks. Speed is quantified as\nSeconds Per Click (SPC) on GPUs, measuring the average inference latency per click, averaged over\n20 clicks. For SAM series models, this includes the preprocessing time by the backbones."}, {"title": "Main Results", "content": "As shown in Table 1, when the HRSAM and HRSAM++ are trained by SAM-distillation on 10242\ninputs, scaling the testing input to 20482 allows them to surpass their teacher SAM-ViT-Huge in\nperformance while retaining a significant speed advantage. These results highlight the scalability\nof our HRSAM models with respect to input size. Moreover, the further HQ-finetuned HRSAM++\nachieves state-of-the-art performance on the high-precision segmentation datasets HQSeg44K and\nDAVIS, while maintaining a faster SPC latency compared to the previous state-of-the-art SegNext."}, {"title": "Scale Analysis", "content": "To analyze the scalability of HRSAM with increased input sizes, we evaluate both HRSAM and\nHRSAM++ using input sizes of 10242, 20482, 30722 and 40962. The performance of the models\nare tested on the HQSeg44K dataset, focusing on NoC@90 and NoC@95 metrics. As shown in\nTable 2, both HRSAM and HRSAM++ scale from the 10242 training resolution to 20482 testing\nsize effectively. However, for larger input sizes, such as 30722 and 40962, HRSAM's performance\ndeteriorates. Even when the images larger than 20482 are resized to 30722 or 40962, the performance\nis inferior to resizing to 20482. This decline can be attributed to two main reasons. First, resizing\nto larger images requires a more robust resizing strategy. The linear interpolation used in this study\ncannot adequately preserve image details. Second, the models are trained on 10242 inputs, but\nhandling inputs larger than 30722 involves processing image token sequences more than 9\u00d7 longer,\nexceeding the models' length extrapolation capabilities. On the other hand, HRSAM++ shows that for\nlarger images (greater than 20482), resizing to 30722 can further improve performance, confirming\nthe effectiveness of HRSAM++'s multi-scale strategy."}, {"title": "Ablation Study", "content": "In the ablation study, we train HRSAM and its various ablation variants using the same distillation\nprotocol in the main experiments. We analyze the impact of different modules, including Global\nattention (Global), Plain Window attention (Plain Win), Vanilla Shifted Window attention (Van\nSwin), Padding Shifted Window attention (Padding Swin), Single-Scale Cycle-scan (SS CScan) and\nMulti-Scale Cycle-scan (MS CScan). We compare the latency and performance of these variant\nbackbones under 10242 and 20482 input sizes. As shown in Table 3, the Global attention achieves\nbetter performance with 10242 inputs due to its strong representation capability, which allows it to\neffectively learn from the teacher model. However, when scaled to 20482 inputs, its performance\ndeteriorated significantly compared to other HRSAM variants, with extremely high latency. Con-\nversely, while some HRSAM variants show only moderate performance over 10242 inputs, they\ngenerally perform well when scaled to 20482. Notably, HRSAM++ with the Multi-Scale Cycle-scan\nmodule demonstrates significant improvements over 20482 inputs, highlighting the effectiveness of\nour proposed HRSAM and HRSAM++ in enhancing model scalability to input image sizes."}, {"title": "Qualitative Results", "content": "In Figure 5, we present a qualitative comparison between our HRSAM++ model and the previous\nSOTA SegNext [55], using a set of challenging samples with thin edges. We visualize the segmentation\nresults across four different clicks, clearly demonstrating the superior performance of HRSAM++ in\nhigh-precision segmentation tasks."}, {"title": "Limitation", "content": "HRSAM cannot scale effectively to excessively large images, as shown in our multi-scale analysis.\nOver-enlarging input images or processing particularly large images at extremely high resolutions\ndoes not improve performance. This presents a challenge in determining the optimal input image size,\nwhich HRSAM currently cannot address. Future work will focus on developing adaptive methods to\nset input sizes more effectively."}, {"title": "Conclusion", "content": "In this paper, we propose HRSAM to address the limitations of SAM in handling high-resolution\nimages critical for high-precision segmentation tasks. HRSAM leverages Flash Attention to resolve\nthe fundamental issue of memory insufficiency on large input images. Then, HRSAM incorporates\nPSCWin attention ensuring computational consistency during both training and testing. The enhanced\nversion, HRSAM++ additionally adopts a multi-scale strategy. HRSAM models set new benchmarks\nin high-precision interactive segmentation. Furthermore, our findings reveal a cost-effective training\nstrategy for building high-performance visual foundation models through visual length extrapolation."}]}