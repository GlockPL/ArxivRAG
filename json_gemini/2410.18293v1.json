{"title": "1-2-3-Go! Policy Synthesis for Parameterized Markov Decision Processes via Decision-Tree Learning and Generalization*", "authors": ["Muqsit Azeem", "Debraj Chakraborty", "Sudeep Kanav", "Jan K\u0159et\u00ednsk\u00fd", "Mohammadsadegh Mohagheghi", "Stefanie Mohr", "Maximilian Weininger"], "abstract": "Despite the advances in probabilistic model checking, the scalability of the verification methods remains limited. In particular, the state space often becomes extremely large when instantiating parameterized Markov decision processes (MDPs) even with moderate values. Synthesizing policies for such huge MDPs is beyond the reach of available tools. We propose a learning-based approach to obtain a reasonable policy for such huge MDPs.\n The idea is to generalize optimal policies obtained by model-checking small instances to larger ones using decision-tree learning. Consequently, our method bypasses the need for explicit state-space exploration of large models, providing a practical solution to the state-space explosion problem. We demonstrate the efficacy of our approach by performing extensive experimentation on the relevant models from the quantitative verification benchmark set. The experimental results indicate that our policies perform well, even when the size of the model is orders of magnitude beyond the reach of state-of-the-art analysis tools.", "sections": [{"title": "Introduction", "content": "Markov decision processes (MDPs) are the model for combining probabilistic uncertainty and non-determinism. MDPs come with a rich theory and algorithmics developed over several decades with mature verification tools arising 20 years ago [30] and proliferating since then [10]. Despite all this effort, the scalability of the methods is considerably worse than of those used for verification of non-deterministic systems with no probabilities, even for basic problems."}, {"title": "Related work", "content": "Symbolic approaches are widely used as for alleviating the challenges of the state explosion problem [3]. These approaches are based on data structures storing the information of a model compactly. In particular, the multi-terminal version of BDDs (MTBDDs) has been developed for probabilistic model checking [28, 37, 40]. In a sense, our approach is also symbolic, since we represent the policy using a decision tree. This data structure is most suitable for the goal of explainability, as argued in, e.g., [2].\n Reduction techniques try to reduce the state space of the model while the smaller model satisfies the same set of properties. A symmetry reduction technique for probabilistic models has been proposed in [32] for systems with several symmetric components. Probabilistic bisimulation is available for MDPs and discrete-time Markov chains (DTMCs) that reduce the original model to the smallest one that satisfies the same set of temporal logic formulae [15]. Considering a subset of temporal logic formulae, more efficient techniques have been proposed in [27] for reducing the model to a smaller one. Applying reduction on a high-level description before constructing the resulting model is available in [45, 36].\n Further techniques improving scalability of traditional algorithms include the following. Using secondary storage in an efficient way to keep a sparse representation of a large model has been studied in [21]. Compositional techniques have been developed for the verification of stochastic systems [13, 35]. Prioritizing computation can reduce running times in many case studies by using topological state ordering [34, 11] or learnt prioritizing [7, 39, 29].\n All the above techniques help solving larger models, however, only up to a certain limit. In contrast, our approach synthesizes policy that can be applied to arbitrarily large instances.\n Statistical model checking (SMC) is an alternative solution for approximating the quantitative properties [24, 7, 23] by running a set of simulations on the model to approximate the requested values, while providing a confidence interval for the precision of computed values for discrete and continuous-time"}, {"title": "Preliminaries", "content": "We provide basic definitions in Section 2.1, then describe what it means for models to be parameterized and scalable in Section 2.2 and finally recall how decision trees can be used for representing policies in Section 2.3."}, {"title": "Markov decision processes with a reachability objective", "content": "A probability distribution over a discrete set X is a function \u03bc : X \u2192 [0, 1] where \u03a3\u03c0\u03b5\u03c7\u03bc(X) = 1. We denote the set of all probability distributions over X by D(X)."}, {"title": "State space structure and scalable models", "content": "For learning (e.g. of DTs) to be effective, it is important that the state space of MDP is structured, i.e. every state is a tuple of values of state variables. In other words, the state space of the system is not monolithic (e.g. states defined by a simple numbering), but in fact, there are multiple factors defining it, for example, time or protocol state. Each of these factors is represented by a state variable vi with domain Di. Thus, every state s \u2208 S is in fact a tuple (v1, v2, ..., vn), where each vi \u2208 Di is the value of a state variable.\n A parameterized MDP can be described as a variant of standard MDP where certain parameters are not fixed constants but instead can take different values within a parameter space. These parameters can be associated to the state-space of the system (for example, lower or upper bound of a state variable) or transition dynamics (the probabilities can be functions of the parameter). We provide the formal definition below."}, {"title": "Decision trees for policy representation", "content": "Knowing that the state space is a product of state-variables, a deterministic policy is a mapping \u03a0\u2081 Di \u2192 A from tuples of state variables to actions. By viewing the state variables as features and the actions as labels, we can employ machine learning classification techniques such as decision trees, see e.g. [38, Chapter 3], to represent a policy concisely. We refer to [2] for an extensive description of the approach and its advantages. Here, we shortly recall the most relevant definitions in order to formally state our results.\n Definition 3. A decision tree (DT) T is defined as follows:\n Tis a rooted full binary tree, meaning every node either is an inner node and has exactly two children or is a leaf node and has no children.\n Every inner node v is associated with a decision predicate av which is a boolean function S \u2192 {false, true} (or equivalently \u03a0\u2081 Di \u2192 {false, true})."}, {"title": "Generalizing policies from small problem instances", "content": "In this section, we develop an approach for obtaining good policies for MDPs that are practically beyond the reach of any available rigorous analysis.\n Our approach exploits the regularity in structure of the MDPs, therefore, we focus on parameterized MDPs where we expect regularity in the state space. Intuitively, we solve a few small instances (colloquially speaking \u201c1, 2, and 3\") where an optimal policy is easy to compute. Then we generalize these policies by learning a DT from the combined information. The policy represented by this\""}, {"title": "Parameter selection", "content": "In principle, one can use any of the solvable instances as the set of base instances. However, too small instances may not contain enough information to learn a good generalization. Therefore, we select a small set of instances B = {b1,..., bn}, such that the computed policies of these instances are rich enough to learn a generalized DT (see Section 4.1 for the details how we practically choose this set). This process can be seen as hyper-parameter search. Domain knowledge"}, {"title": "Collecting policies", "content": "We collect the optimal decisions from the optimal policies of each of the base instances into a single dataset, later to be used for learning their generalization. The input of the learning algorithms is a data set (possibly a multiset) of samples of (input, output)-pairs. In our case, it is a set of pairs of the structured state and the chosen action.\n Algorithm 1 describes how to obtain the input function that can be used for the DT learning from a parameterized MDP and a set of parameterizations. Let Mb = (Sb, Ab, \u03b4b, \u015db, Gb) be a concrete instance of a parameterized MDP with b := (p1=v1,\u2026, pm=vm). For a set of base instances B = {b1, ..., bn}, we denote the union of all state spaces as SB := U1 Sbi, and similarly define AB as the union of all action spaces. Note that when aggregating the information, we exclude states that are not reachable in the Markov Chain (MC) induced by the computed optimal policy, as well as goal states. This reduces the input size for the DT learning which has two advantages: firstly, it speeds up the computation and secondly, it allows the DT to focus on the relevant states."}, {"title": "Decision tree learning", "content": "We use standard DT learning algorithms to learn a DT from the dataset constructed in the Algorithm 1. For predicates to be used, we consider axis-aligned predicates (i.e. predicates of the form x > c where x is a state variable and c\u2208R). The best predicate is selected by calculating the Gini index. Instead of having a stopping criterion, we let the recursive splitting of the dataset happen until no further splitting is possible. The resulting DT generalizes the policy in two ways:\n 1. The DT is trained using smaller base instances. The same state variables in the DT's inner node predicates are present in the larger MDP instances, but they can have a larger domain. Despite this difference, the DT would still partition the state space of the larger MDP instances and still recommend actions corresponding to each state.\n 2. As we are aggregating multiple policies, in our dataset, unlike the learning algorithm described in Section 2.3, we can have a state with more than one suggested actions. The learning algorithm considers them as distinct data-points sharing the same value but different labels. Since the values are the same, there are no predicates that can distinguish them. So these datapoints traverse the same path in the tree until they reach a leaf node. The classification at the leaf node is determined by 'majority voting', the label that appears most often is assigned to the leaf node. This approach helps filter out actions suggested by only a few less generalizing base instances."}, {"title": "Applying and evaluating the resulting policy", "content": "Once we have a decision-tree representation of a policy, we can apply it to MDP instances of arbitrary size. To evaluate a policy, we simply need to compute the value of the MC induced by applying the DT. Since solving MCs is computationally easier than solving MDPs, we can explicitly compute values for larger MDPs (which we could not do otherwise). Nonetheless, one can still scale the parameter to such an extent that the construction of the corresponding MC requires too much time or memory. In such cases, we can use statistical model checking methods [48].\n The resulting value is not only a measure for the performance of the DT policy, but also a guaranteed lower bound on the value of the MDP (or an upper bound in the case of minimization)."}, {"title": "Evaluation", "content": ""}, {"title": "Experiment Setup", "content": "Benchmark Selection We selected parameterized MDPs with reachability objective from the quantitative verification benchmark set (QVBS) [22]. Models with reward-bounded reachability (e.g., eajs and resource-gathering) were excluded. We also identified trivial model and property combinations where the equation min, Pg [G] = max, Pg [G] holds for the set of goal states G and the initial state s. In such cases, any valid policy would act as an optimal policy.\n We have excluded these from our benchmark set. We extended the benchmark set with the Mars Exploration Rovers (mer) case study, which was introduced in [14] and appears frequently in recent literature. This model is interesting because the probability of its property is non-trivial and it is scalable to large parameter values without degenerating into a trivial model.\n Choice of Base Instances We conducted experiments to observe the effect of the set of base instances on the value produced by the learned policy. We synthesized decision trees from different sets of base instances, increasing the parameter(s) linearly as well as exponentially, and evaluated them on models larger than the base instances. We observed that one or two instances are often already enough to generalize the policy in the considered benchmark set (See Appendix A for the chosen set of base instances used in our experiments).\n System Configuration The experiments were executed on an AMD EPYC\u2122 7443 server with 48 physical cores, 192 GB RAM, running Ubuntu 22.04.2 LTS operating system with Linux kernel version 5.15.0-83-generic. This powerful server was used to execute many runs in parallel. We assigned 2 cores and 8 GB RAM to each run. For all experiments, we used BENCHEXEC [5], a state of the art benchmarking tool, to isolate the executions and enforce the resource limitations.\n Implementation Details We implemented our approach as an extension of the probabilistic model checker STORM [25]. Our code is publicly available at: https://github.com/muqsit-azeem/dtstrat-123go-artifact/.\n Method of Comparison Our aim is to provide a method for policy synthesis for arbitrarily large instances of parameterized MDPs, in particular for MDPs beyond the reach of any available rigorous analysis. Consequently, the optimal value for such an MDP is by definition unknown and optimality becomes not only uncheckable, but also unexpectable rather, one can hope for values close to the range where the unknown optimum is expected to lie."}, {"title": "Results", "content": "Table 1 and Table 2 show the results of our evaluation for the minimizing and maximizing properties, respectively. Each instance refers to a combination of model, property and parameter. The tables show, for each model+property combination, the parameter value and CPU time taken by STORM to solve it (< 1 min, < 60 min, and Beyond in case STORM could not solve the instance in an hour), the values produced by STORM, our approach and the sampling based SMC. The tables report OOR when running out of resource (time or memory). Also, a few MODES runs resulted in a run length exceeded (RLE) error.\n Some models converge to triviality (i.e., max=min) as we scale the parameter. Zeroconf_dl+deadline_min becomes trivial for higher values of the parameter K than 3, firewire+deadline becomes trivial for deadline > 1300, and csma+some_before becomes trivial when the value of K is more than twice the value of N. Pacman also approaches closer to triviality for higher values of the parameter MAX_STEPS (the horizon).\n The results show that our approach gives near optimal values for 13 out of 21 cases (the upper halves of Table 1 and Table 2), better than Smart LSS for 2 out of remaining 8 cases, and generally better than random and uniform in remaining cases. There are two instances where random performs better than our approach (pacman for the MAX_STEPS 25, and csma+all_before_max for N=3), see the discussion below."}, {"title": "Discussion", "content": "Although our approach is simple, it performs well in a number of cases. We often generalize from a single instance or two, yielding satisfactory solutions for arbitrarily large instantiations. In a number of cases, we can justifiably extrapolate that the policies are (nearly) optimal for all instances. For instance, consider the two benchmarks of Figure 3. No matter how much the model is scaled up, the value of our policy seems to remain stable. While its (near-)optimality can be proven only up to a certain point (beyond which no ground truth can be known), the apparent stability suggests it is true onwards, too. Note that MODES returns low values as the optimal policies are rather rare.\n In the sequel, we discuss the scope and the limitations of our approach in details. As discussed earlier, we can divide the parameters in two types.\n Type 1: Parameters that dictate the number of PRISM-modules. This type of parameter not only changes the structure of the MDP, but also increases the"}, {"title": "Conclusion", "content": "We have seen that practically good policies (with values close to the unknown optimum in the sense of the \"Method of Comparison\" above) can be generated in a lightweight way even for very large parameterized models, beyond reach of any other methods. In order to synthesize policies for arbitrarily large models, we generalize the policies computed for the smaller instances using (more explainable and thus more generalizable) decision trees, coining the \u201cgeneralizability by explainability\u201d.\n The generalization is an example of unreliable reasoning, which can contribute to better scalability. On the one hand, the unreliability results in no guarantees that the produced policies are anywhere close to optimum, which, however, often cannot be computed anyway. On the other hand, the values of the policies can be reliably approximated: either numerically with absolute guarantees if the resulting Markov chain is still analyzable (e.g., with partialexploration methods [29]) or with statistical guarantees by SMC on the Markov chain. Consequently, although optimal control policies might be out of reach, we can still produce what we thus coin here as provably good enough policies. Moreover, the consistency of the values over the different instantiations often suggests practical proximity to optimum.\n A possibly surprising point is the conclusion of our experiments that very few base instances need to be analyzed. Such robustness (together with the robustness across the target instances as seen in Fig. 3) suggests that this generalizability is a deeply inherent property of many models, and thus deserves further investigation and exploitation. In particular, our approach is only the first, generic try to exploit this property, opening the new paradigm. As suggested by our experimental results, more specific heuristics for certain types of systems where parameters play different roles, such as number of modules, number of repetitions, time-outs, etc., offer a desirable direction of future work."}]}