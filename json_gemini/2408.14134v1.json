{"title": "Exploring the Potential of Large Language Models for Heterophilic Graphs", "authors": ["Yuxia Wu", "Shujie Li", "Yuan Fang", "Chuan Shi"], "abstract": "Graph Neural Networks (GNNs) are essential for various graph-based learning tasks. Notably, classical GNN architectures operate under the assumption of homophily, which posits that connected nodes are likely to share similar features. However, this assumption limits the effectiveness of GNNs in handling heterophilic graphs where connected nodes often exhibit dissimilar characteristics. Existing approaches for homophily graphs such as non-local neighbor extension and architectural refinement overlook the rich textual data associated with nodes, which could unlock deeper insights into these heterophilic contexts. With advancements in Large Language Models (LLMs), there is significant promise to enhance GNNs by leveraging the extensive open-world knowledge within LLMs to more effectively interpret and utilize textual data for characterizing heterophilic graphs. In this work, we explore the potential of LLMs for modeling heterophilic graphs and propose a novel two-stage framework: LLM-enhanced edge discriminator and LLM-guided edge reweighting. Specifically, in the first stage, we fine-tune the LLM to better identify homophilic and heterophilic edges based on the textual information of their nodes. In the second stage, we adaptively manage message propagation in GNNs for different edge types based on node features, structures, and heterophilic or homophilic characteristics. To cope with the computational demands when deploying LLMs in practical scenarios, we further explore model distillation techniques to fine-tune smaller, more efficient models that maintain competitive performance. Extensive experiments validate the effectiveness of our framework, demonstrating the feasibility of using LLMs to enhance GNNs for node classification on heterophilic graphs.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have emerged as a standard tool for modeling graph-structured data, empowering various graph-based tasks such as node classification, link prediction, and graph classification (Wu et al. 2020). Traditional message-passing GNNs (Veli\u010dkovi\u0107 et al. 2018; Kipf and Welling 2016) operate on the homophily assumption, positing that connected nodes are likely to exhibit similar features and class labels. However, in heterophilic graphs, nodes connected by an edge often display contrasting features and labels (Zhu et al. 2020, 2021; Bo et al. 2021; Song et al. 2023; Liang et al. 2024), leading to diminished performance of standard GNNs.\nExisting approaches to tackle heterophily in GNNs generally fall into two main strategies: non-local neighbor extension and architectural refinement (Zheng et al. 2022; Gong et al. 2024). The former extends the node's receptive field to include distant, high-order neighbors (Abu-El-Haija et al. 2019; Song et al. 2023) or potential connections (Jin et al. 2021; Wang and Zhang 2022; Zou et al. 2023), thereby enhancing node representations through a broader scope of information integration. The latter modifies the core functions of GNNs, such as the message aggregation and updating functions, to better suit heterophilic contexts. More specifically, this includes adaptive aggregation methods that distinguish between similar and dissimilar neighbors (Bo et al. 2021; Liang et al. 2024), differentiating the representations of ego nodes and their neighbors (Zhu et al. 2020), and incorporating intermediate representations from successive layers to capture both local and global structures (Zhu et al. 2020; Chien et al. 2021).\nDespite these advancements, current methodologies for heterophilic graphs largely overlook the rich textual content associated with the nodes in real-world graphs, which can offer invaluable insights in heterophilic contexts. For instance, in a website, the texts on interconnected webpages can enrich the understanding and prediction of heterophilic links. While traditional text processing techniques used in GNNs, such as bag-of-words or shallow embeddings, are inadequate for capturing complex semantics, large language models (LLMs) (Zhao et al. 2023b) have demonstrated their capabilities in empowering GNNs for text-attributed graphs (Liu et al. 2023; Li et al. 2023; Yu et al. 2024; Mao et al. 2024). However, these efforts focus on homophilic graphs, leaving heterophilic graphs largely unexplored.\nIn this work, we delve into the potential of LLMs for heterophilic graphs. To the best of our knowledge, this is the first investigation into exploiting LLMs for heterophilic graphs. We aim to bridge the gap between the general capabilities of LLMs and the unique characteristics of heterophilic graphs. Specifically, we aim to address the following research questions.\nFirst, can LLMs be effectively adapted to characterize and identify heterophilic contexts? As LLMs encompass general open-world knowledge, they can be utilized for the se-"}, {"title": "2 Related Work", "content": "Heterophilic graph learning. Most studies on heterophilic data focus on identifying and effectively aggregating information from nodes with dissimilar characteristics, which is crucial for capturing diverse relationships and improving the performance of graph-based models. The existing approaches generally fall into two main strategies: non-local neighbor extension and architectural refinement (Zheng et al. 2022; Gong et al. 2024). Non-local neighbor extension approaches aim to extend the receptive field of a node beyond its immediate neighbors to include non-local nodes that may share similar labels or features despite being distant in the graph. These methods often involve high-order neighbor mixing (Abu-El-Haija et al. 2019; Song et al. 2023) or discovering potential neighbors based on various distance measurements, such as feature-based distance (Jin et al. 2021; Bodnar et al. 2022), structure-based distance (Pei et al. 2020), or hybrid approaches (Zou et al. 2023; Wang and Zhang 2022; Li et al. 2022). On the other hand, architectural refinement approaches enhance the GNN architecture by employing identifiable message aggregation to discriminate and amplify messages from similar neighbors while minimizing the influence of dissimilar ones (Bo et al. 2021; Zhu et al. 2021; Liang et al. 2024), or by leveraging inter-layer combinations to capture information from different neighbor ranges (Xu et al. 2018; Chien et al. 2021; Zhu et al. 2020), thereby improving the model's representation power in heterophilic graphs.\nLLMs for graph learning. The existing research about LLMs for graph learning includes LLM-based methods that adopt LLM as the backbone and GNN+LLM-based methods that integrate the advantages of both GNNs and LLMs (Liu et al. 2023; Chen et al. 2024). The former works focus on aligning graph data with natural language via graph-to-token and graph-to-text approaches (Liu et al. 2023). The graph-to-token approach involves tokenizing graph data to align it with natural language, enabling joint understanding with data from other modalities (Zhao et al. 2023a; Ye et al. 2024). Graph-to-text focuses on describing graph information using natural language (Liu and Wu 2023; Wang et al. 2024; Guo et al. 2023). The latter harnesses the strengths of both language understanding from LLMs and structural analysis from GNNs by using GNN-centric methods utilizing LLMs to extract node features from raw data and make predictions using GNNs (He et al. 2024; Xie et al. 2023) or LLM-centric methods utilizing GNNs to enhance the perfor-"}, {"title": "3 Proposed Model: LLM4HeG", "content": "In this section, we first introduce some preliminaries on the problem formulation and classic GNNs. Then we introduce the overview of the proposed model LLM4HeG followed by details of different components.\n3.1 Preliminaries\nProblem formulation. Let G = (V, E, X,C) denotes a text-attributed graph with a set of nodes V and a set of edges E, where each node v \u2208 V is associated with a text document x \u2208 X. C is the set of node classes. In this paper, we address the task of semi-supervised transductive node classification for heterophilic graphs. Specifically, a subset of the nodes is designated as the training/validation nodes with known class labels, while the goal is to predict the unknown labels of the remaining nodes in the graph.\nClassic GNNs. GNNs typically employ a multi-layer approach to neighborhood aggregation, wherein each node incrementally gathers and aggregates contexts from its neighboring nodes. At the $l$th layer, the representation $h_v^{(l)} \\in \\mathbb{R}^{d_l}$ of a node $v$ is derived as follows:\n$h_v^{(l)} = \\sigma \\left( \\text{AGGR} \\left( h_v^{(l-1)}, \\left\\{ h_u^{(l-1)} \\mid u \\in N(v) \\right\\} \\right) \\right),$ (1)\nwhere $d_l$ is the dimension of the node representations at the $l$th layer. The function AGGR($\\cdot$) denotes an aggregation mechanism combining the feature vectors of the neighboring nodes, where N(v) denotes the set of neighboring nodes of v. $\\sigma$ represents an activation function."}, {"title": "3.2 Overall Framework", "content": "Figure 1 illustrates the overall framework of our approach LLM4HeG, with a two-stage framework leveraging LLMs for heterophilic graph modeling. As shown in Fig. 1(b), Stage 1 involves our LLM-enhanced edge discriminator, where we fine-tune an LLM to distinguish between homophilic and heterophilic edges, utilizing the rich textual data associated with nodes and a limited amount of ground truth label. Following this, in Fig. 1(c), Stage 2 involves LLM-guided edge reweighting to learn adaptive weights for both homophilic and heterophilic edges. These weights are adapted to individual edges by integrating node features, graph structures, and edge types, enabling fine-grained aggregation within GNNs.\nAdditionally, to cope with the computational demands of deploying LLMs, we explore a distillation method that condenses the heterophily-specific knowledge of a fine-tuned LLM into a more compact SLM. As shown in Fig. 1(d), we leverage the LLM as a teacher model to generate pseudo labels for additional examples, which can be used to fine-tune an SLM that can perform inference more efficiently without compromising performance."}, {"title": "3.3 Stage 1: LLM-enhanced Edge Discriminator", "content": "In heterophilic graphs, accurately discriminating heterophilic edges from homophilic ones is pivotal for effectively tailoring context aggregation strategies across neighboring nodes. We propose an LLM-enhanced edge discriminator, tapping on the semantic capabilities and open-world knowledge of LLMs beyond conventional shallow feature-based approaches. We first construct the ground truth labels"}, {"title": "3.4 Stage 2: LLM-guided Edge Reweighting", "content": "Building on the edge types identified by Stage 1, we proceed to LLM-guided edge reweighting to integrate heterophilic contexts into GNNs. This process leverages the semantic insights acquired from the LLM-enhanced edge discriminator, allowing us to adjust the weight of every individual edge, taking into account various edge-specific factors, including node features and structures, as well as its homophilic or heterophilic nature.\nSpecifically, for a node v \u2208 V, we first extract the representation from the LLM based on the associated textual information:\n$h_v^{(0)} = \\sigma(\\text{LLM}(x_v)W_e),$ (2)\nwhere $x_v$ denotes the raw text of node v, LLM is an LLM"}, {"title": "3.5 LLMs-to-SLMs Distillation", "content": "In real-world applications, deploying LLMs as edge discriminators introduces substantial computational challenges, even when only the inference phase is required for predicting the edge types on test graphs. To mitigate the computational burden of the inference phase, we explore knowledge distillation techniques to transfer the heterophily-specific capabilities of the fine-tuned LLM into more lightweight SLMs (Xu et al. 2024).\nAs shown in Fig. 1(d), after fine-tuning the LLM in Stage 1, we use it as a teacher model to generate the pseudo-labels for additional node pairs sampled from the entire graph. The labeling process follows the inference phase introduced in Stage 1, asking the LLM whether a given node pair is homophilic or heterophilic based on the input template. These pseudo-labels are combined with the ground-truth labels, forming an expanded label set which is subsequently used to fine-tune the SLMs. The fine-tuning of the SLMs follows the exact same approach as fine-tuning the LLM in Stage 1, using the same template and the LoRA technique. Finally, we replace the fine-tuned LLM with the fine-tuned SLM during inference, which predicts the homophilic or heterophilic relationship between any two given nodes to guide edge reweighting in Stage 2."}, {"title": "4 Experiment", "content": "In this section, we present an empirical study to demonstrate the feasibility of leveraging LLMs for node classification on heterophilic graphs.\n4.1 Experimental Setup\nDatasets. Given that the datasets commonly employed in heterophily graph tasks lack original textual information, we collect their raw text directly from the original data providers and preprocess these datasets. Consequently, our experiments only include datasets that contain raw text. Specifically, we have prepared five datasets: Cornell, Texas, and Wisconsin (Pei et al. 2020), Actor (Tang et al. 2009) and Amazon (Platonov et al. 2023). The Cornell, Texas, and"}, {"title": "4.3 Model Analysis", "content": "Ablation Study. To evaluate the effectiveness of the learnable weight for adaptive message passing and deep node features from LLM, we conducted experiments using different variants of our model: (1) w/o reweight: This variant only uses the graph-based weight $w_{uv}$. (2) w/o learnable weights: This variant uses fixed weights instead of learnable ones. Specifically, we manually set the weights for homophilic and heterophilic pairs to 1 and -1, respectively, as these values were found to perform well across most datasets."}, {"title": "5 Conclusion", "content": "In this study, we explored the potential of LLMs to enhance the performance of GNNs for node classification on heterophilic graphs. We introduced a novel two-stage framework LLM4HeG, integrating LLMs into the GNN learning process through an LLM-enhanced edge discriminator and an LLM-guided edge reweighting modules. LLM4HeG allows for more precise identification of heterophilic edges and finer-grained context aggregation, leveraging the rich semantics contained in nodes' textual data. Additionally, to address the computational challenges of deploying LLMs, we implemented model distillation techniques to create smaller models that achieve much faster inference while maintaining competitive performance. Our extensive experiments demonstrate that LLM4HeG significantly improves node classification on heterophilic graphs, underscoring the potential of LLMs for advancing complex graph learning."}, {"title": "A Appendix", "content": "More Details for Datasets\n\u2022 Cornell, Texas, and Wisconsin (Pei et al. 2020) are collected from computer science departments at various universities. In these datasets, each node corresponds to a web page, while edges represent hyperlinks connecting these pages. In our experiments, we use the original webpage data as the textual information for each node.\n\u2022 Actor (Pei et al. 2020; Tang et al. 2009) is an actor-only induced subgraph of the film-director-actor-writer network. In this graph, nodes represent actors, and an edge between two nodes indicates their co-occurrence on the same Wikipedia page. The classification task involves categorizing actors into five distinct classes based on their roles. We selected the actors based on category information provided in the metadata, focusing on those with high occurrence frequencies. The category keywords of the selected actors include \"American film actors\", \"American film and television actors\", \"American stage and television actors\u201d, \u201cEnglish\u201d and \u201cCanadian\". Afterward, we construct the graph based on the edges and remove the isolated nodes from the graph.\n\u2022 Amazon (Platonov et al. 2023) is constructed from the Amazon product co-purchasing network metadata. In this dataset, nodes represent products such as books, music CDs, DVDs, and VHS video tapes. Edges link products that are frequently bought together. The goal here is to predict the average rating a product receives from reviewers, with ratings grouped into five classes. To manage the graph's complexity, only the largest connected component of the 5-core of the graph is considered."}, {"title": "B More Implementation Details", "content": "Node Pairs In Stage 1, we train the edge discriminator by sampling node pairs from the graph, with the selection process tailored to the characteristics of each dataset. For small graphs like Cornell, Texas, and Wisconsin, we select all node pairs within the training set, including those without direct edges. For the Actor dataset, node pairs are selected based on the 1-hop and 2-hop neighbor relationship, while for the Amazon dataset, we focus on node pairs with a 1-hop neighbor relationship, taking into account the graph's size.\nIn the distillation stage, we use node pairs from the validation and testing data as additional samples, allowing the fine-tuned LLM to generate pseudo-labels. For small graphs like Cornell, Texas, and Wisconsin, we select node pairs with 1-hop and 2-hop neighbor relationships, while for the Actor and Amazon datasets, we choose node pairs with 1-hop neighbor relationships. The number of node pairs used for training in Stage 1 and distillation is shown in Table 5."}, {"title": "C Hyper-parameters", "content": "We run all the experiments on an NVIDIA A800 GPU. For LLM4HeG, the edge weight margin $\\alpha = 0.3$ and the regularization coefficient $\\lambda = 0.1$. For the baselines, we use the hyper-parameters as listed in previous literature. For the number of hops in the i-hop neighborhood of each node, we use a 1-hop and 2-hop neighborhood for H2GCN (Zhu et al. 2020) and a 1-hop neighborhood for the other backbones. The hidden unit for GCN and GAT is 16. The number of heads in GAT is 3 for Amazon and 8 for other datasets. For H2GCN, we adopt the H2GCN-1 variant using one embedding round (K = 1). The parameter setting of FAGCN is: the hidden unit = 32, layers = 2, $\\epsilon = 0.4$. For JacobiConv, the parameter setting is $\\gamma = 2$ for Polynomial Coefficient Decomposition (PCD), $\\alpha = 0.5$ and $b = 0.25$ for Jacobi Basis. For OGNN, the number of MLP layers is 1 for Cornell, Texas and Wisconsin and 2 for the Actor and Amazon dataset. For SEGSL, the height of the encoding tree K = 2. The subtree sampling parameter $\\theta$ is 2 for the Amazon dataset and 3 for other datasets. The GNN encoder model for the reconstructed graph is GraphSAGE. For DisamGCL, the weight of historical memory $\\mu = 0.6$, the controlling variables for the node similarity $\\epsilon_1 = 0.74$ and $\\epsilon_2 = 0.4$, the threshold of the node similarity T = 0.8, the number of augment instances K 8, the weight of contrastive loss $\\lambda = 1$."}]}