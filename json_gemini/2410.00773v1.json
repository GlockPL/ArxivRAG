{"title": "BABELBENCH: An Omni Benchmark for Code-Driven Analysis of Multimodal and Multistructured Data", "authors": ["Xuwu Wang", "Qiwen Cui", "Yunzhe Tao", "Yiran Wang", "Ziwei Chai", "Xiaotian Han", "Boyi Liu", "Jianbo Yuan", "Jing Su", "Guoyin Wang", "Tingkai Liu", "Liyu Chen", "Tianyi Liu", "Tao Sun", "Yufeng Zhang", "Sirui Zheng", "Quanzeng You", "Yang Yang", "Hongxia Yang"], "abstract": "Large language models (LLMs) have become increasingly pivotal across various domains, especially in handling complex data types. This includes structured data processing, as exemplified by ChartQA and ChatGPT-Ada, and multimodal unstructured data processing as seen in Visual Question Answering (VQA). These areas have attracted significant attention from both industry and academia. Despite this, there remains a lack of unified evaluation methodologies for these diverse data handling scenarios. In response, we introduce BABELBENCH, an innovative benchmark framework that evaluates the proficiency of LLMs in managing multimodal multistructured data with code execution. BABELBENCH incorporates a dataset comprising 247 meticulously curated problems that challenge the models with tasks in perception, commonsense reasoning, logical reasoning, and so on. Besides the basic capabilities of multimodal understanding, structured data processing as well as code generation, these tasks demand advanced capabilities in exploration, planning, reasoning and debugging. Our experimental findings on BABELBENCH indicate that even cutting-edge models like ChatGPT 4 exhibit substantial room for improvement. The insights derived from our comprehensive analysis offer valuable guidance for future research within the community. The benchmark data can be found at https://github.com/FFD8FFE/babelbench.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated exceptional performance in a wide range of applications. For example, multimodal language models such as GPT-4V [1], Gemini [2], and Claude [3]; chart-specific models such as TableGPT [4] and ChartLlama [5]; and code generation models represented by Codex [6], Code Llama [7], and DeepSeek-Coder [8] have all demonstrated significant advancements.\nBuilding on these effectiveness in specific yet wide-ranging tasks, a genuinely intelligent LLM-as-Agent system should be capable of integrating these capabilities to address open-ended problems in the real world. Motivated by this vision, some systems and agents have been developed, such as BabyAGI [9], LangChain [10], and AutoGPT [11], which extend the application of LLMs to practical impacts in the physical world.\nDespite these advancements, there is still a deficiency in benchmarks that comprehensively evaluate LLMs' performance in realistic and complex scenarios. Most existing benchmarks, such as Super-GLUE [12], MMLU [13] and MME [14], mainly evaluate the acquisition of knowledge by LLMs and their ability to converse rather than solve problems. Although some work has introduced tasks that mimic real-world applications [15], such as shopping on a website, the complexity of the involved data structure remains limited."}, {"title": "2 Design and Development", "content": "Real-world problems are inherently open-ended, which are not suitable for systematic evaluation. To address it, we abstract real-world scenarios from two critical perspectives: the model-inherent capabilities and the task-dependent capabilities. This abstraction allows us to encapsulate real-world problems within a smaller but more manageable scope.\nModel-inherent capabilities As a prerequisite, we identify two essential model capabilities from real-world tasks. First, to align with typical data types in the real world, LLMs should be able to comprehensively understand, analyze and process the unified multimodal multistructured data. In addition, to enhance LLMs' impact on the physical world, LLMs should interact with an environment through tools, involving both structured data and images. Specifically, in this paper, we represent tool-using with the generation and execution of code. We require the presence of a sandbox environment to execute code, process text and images, and return execution results (including outputs and errors). Therefore, an LLM/agent needs to have basic capabilities in multimodal understanding, tabular data processing, and programming in order to obtain an entry ticket to BABELBENCH.\nTask-dependent capabilities To ensure the diversity and comprehensiveness of our dataset, inspired by [14], we then establish a detailed capability taxonomy to guide the annotation. As presented in Figure 2, it mainly consists of perception and reasoning, each encompassing multiple subcategories. Besides, to increase the difficulty, we incorporate a variety of complex factors and challenges for each subcategory, as listed in Table 1."}, {"title": "2.1 Design", "content": "Real-world problems are inherently open-ended, which are not suitable for systematic evaluation. To address it, we abstract real-world scenarios from two critical perspectives: the model-inherent capabilities and the task-dependent capabilities. This abstraction allows us to encapsulate real-world problems within a smaller but more manageable scope.\nModel-inherent capabilities As a prerequisite, we identify two essential model capabilities from real-world tasks. First, to align with typical data types in the real world, LLMs should be able to comprehensively understand, analyze and process the unified multimodal multistructured data. In addition, to enhance LLMs' impact on the physical world, LLMs should interact with an environment through tools, involving both structured data and images. Specifically, in this paper, we represent tool-using with the generation and execution of code. We require the presence of a sandbox environment to execute code, process text and images, and return execution results (including outputs and errors). Therefore, an LLM/agent needs to have basic capabilities in multimodal understanding, tabular data processing, and programming in order to obtain an entry ticket to BABELBENCH.\nTask-dependent capabilities To ensure the diversity and comprehensiveness of our dataset, inspired by [14], we then establish a detailed capability taxonomy to guide the annotation. As presented in Figure 2, it mainly consists of perception and reasoning, each encompassing multiple subcategories. Besides, to increase the difficulty, we incorporate a variety of complex factors and challenges for each subcategory, as listed in Table 1."}, {"title": "2.2 Construction", "content": "The construction process consists of two steps, outlined as follows:\nFirst, 15 experts were engaged for annotation. Comprehensive annotations were required for each question, encompassing the question itself, the data sources, the ground-truth answer, and the evaluation criteria. To guarantee the annotation consistency, each question then underwent a quality review by a second expert. After this round of annotation and review, we obtained 491 questions in total.\nAfter that, 5 experts performed a secondary review to choose 10 to 30 questions from each subcategory, forming the ultimate benchmark. This selection was influenced by factors such as question difficulty, the range of model capabilities required, the potential for data leakage, etc. Finally, 247 questions formed the final dataset."}, {"title": "2.3 Evaluation Framework", "content": "To support an end-to-end evaluation of existing LLMs, we also provide an evaluation pipeline. In alignment with React [24], it can integrate either locally deployed models or remote open model APIs. Besides, it proficiently supports the reading and manipulation of images and tables, while also enabling environmental interaction through the tool of Python sandbox. Specially, since some model do not support large images and multiple images, we also support to stitch multiple images together and down-sample the images."}, {"title": "3 Experiments", "content": "13 models/systems are included for evaluation. They can be roughly classified into:\n\u2022 Commercial System (System): So far, only OpenAI's ChatGPT can independently evaluate this benchmark. Other systems (Claude, Gemini, Qwen, etc.) lack full support for code execution, multiple image uploads, and multiple CSV file uploads. Therefore, we only"}, {"title": "3.1 Experimental Setup", "content": "13 models/systems are included for evaluation. They can be roughly classified into:\n\u2022 Commercial System (System): So far, only OpenAI's ChatGPT can independently evaluate this benchmark. Other systems (Claude, Gemini, Qwen, etc.) lack full support for code execution, multiple image uploads, and multiple CSV file uploads. Therefore, we only"}, {"title": "Multimodal LLMs as agents", "content": "13 models/systems are included for evaluation. They can be roughly classified into:\n\u2022 Commercial System (System): So far, only OpenAI's ChatGPT can independently evaluate this benchmark. Other systems (Claude, Gemini, Qwen, etc.) lack full support for code execution, multiple image uploads, and multiple CSV file uploads. Therefore, we only"}, {"title": "3.2 Main Results", "content": "Table 4 presents the key experimental results, revealing several critical insights.\nOverall performance ChatGPT 4 achieves the highest score, but a score of 42.11 highlights substan-tial room for improvement. Among closed-source models, Gemini-Pro-1.5 achieves the highest score, followed by GPT-40 and GPT-4. Despite these performances, a noticeable gap remains compared to ChatGPT 4, indicating that additional data matching the execution environment could be benefi-cial. Additionally, there is a significant disparity between closed-source and open-source models, suggesting that the open-source community still has much progress to make.\nInfluence of multistructured data Regarding the impact of images, ChatGPT 4, GPT-4, and Gemini perform better on questions without images, while models like QwenVL-max and GPT-40 perform better on questions with images. Considering the impact of tables, ChatGPT 4, GPT-40, and Claude3-opus exhibit balanced performance, whereas GPT-4 and Gemini-1.5 perform significantly better on data with tables. This indicates that current models do not optimize for modality and data structure uniformly. For the combined influence of images and tables, we initially predicted that questions with both would be the most challenging and thus yield the lowest performance. However, we found that models like GPT-40 and Gemini-Pro-1.5 did not conform to this hypothesis. We speculate that this may be due to the information gain brought by different modalities and data structures.\nTask difficulty As expected, most models score higher on easy tasks than on medium tasks, and higher on medium tasks than on hard tasks. However, the highest score for easy tasks was only 55.93, revealing significant deficiencies even in handling simple tasks."}, {"title": "3.3 Detailed Analysis", "content": "To delve deeper into the effectiveness of LLMs on BABELBENCH, we undertake an extensive case study. Specifically, we establish several key aspects for evaluation: 1) Overall: Whether the solution effectively addresses the given question. 2) Plan: Assessing the model's ability to decompose the question intent and reasonably arrange the dependencies among the sub-steps. 3) Interact: Analyzing the model's interaction with the external environment, including active engagement to extract necessary information from images and tables, refinement of the plan based on interaction results if the initial plan is flawed or absent, and debugging and fixing corner cases through external feedback. 4) Alignment: Evaluating the ability to align and interact with information across prompts, images, and tables. 5) Knowledge: Assessing the model's possession of essential common sense and domain-specific knowledge. 6) Reasoning: Testing the model's reasoning capabilities, such as inducing rules and detecting outliers that contradict common patterns. 7) Instruct-follow: Determining whether the model adheres to formatting guidelines specified in the prompt."}, {"title": "3.4 Other Findings", "content": "When data becomes multimodal and multistructured, we discover that the LLMs encounter many new findings.\nThe self-efficacy of LLMs needs further improvement. For each problem or its subcomponents, the model can either rely on its inherent capabilities (such as image/table inspection) or leverage tools (such as code). When handling complex tasks, it is crucial for the model to allocate cognitive load based on task requirements and tool availability, deciding whether to rely on itself or on external tools. However, we have found that current LLMs are not yet capable of effectively making this decision. For instance, in Figure 6 (a), for the task of \"identifying colors from the image\", the model initially attempts to use code, only switching to direct image inspection upon encountering errors. However, direct image observation is sufficient to solve the problem with a low error probability, whereas using code is more complex and error-prone.\nLLMs have a low adversity quotient. An error triggered by one data type may actually be caused by another data type. Therefore, the model should be aware that errors may originate from earlier steps and data sources; and it can also leverage other data sources to resolve the errors. For example, in Figure 6 (b) requiring OCR from the image and corresponding data retrieval from the table, the LLM misinterprets \"WP0\" in the image as \"WPO\", leading to an index error when searching the table. To deal with it, GPT-4 made no corrections in its first two attempts but silently corrected the"}, {"title": "4 Related Work", "content": "Benchmarking multimodal agents The rapid advancement of multimodal large language models (MLLMs) has led to an increased emphasis on benchmarks for evaluating their performance across various domains. Existing benchmarks, such as MMBench [31], SEED-Bench [32], SEED-Bench-2 [33], TableVQA [34] and MathVista [35], assess MLLMs across a range of ability dimensions, including image-text understanding, scene understanding, detection, optical character recognition (OCR), and mathematical reasoning, but lack a comprehensive investigation specifically tailored for an omni-benchmark that includes code-driven analysis of structured data like tables and unstructured data like images. In the realm of digital interaction agents, benchmarks span coding environments, web scenarios [36, 37, 38, 39] and mobile applications [40, 41, 42, 43, 44]. But these efforts often focus on singular environments or lack executability.\nBenchmarking code-driven tasks Recent years have seen the emergence of LLMs that exhibit impressive code generation and analysis capabilities. Various benchmarks have been proposed to assess LLM performance on code-driven tasks. But evaluating these models comprehensively remains an open challenge. CodeXGLUE [45] is a benchmark covering both code understanding and generation tasks. Other benchmarks, such as those by [6, 46, 47, 48], focus on code generation in competition-level settings. DS-1000 [49] and the dataset by [50] assess LLMs' ability in data science and general-purpose tasks. Research on practical LLM-based code completion tools, such as Github's Copilot [51], has characterized interaction models [52, 53] and user experiences [54], highlighting both benefits and potential drawbacks [54]. Despite progress in LLM benchmarks for code-driven tasks, a comprehensive benchmark covering structured and unstructured multimodal data analysis is still needed."}, {"title": "5 Limitations", "content": "Our dataset has certain limitations: Firstly, due to the high difficulty level of each task and the high annotation cost, its scale is relatively limited. Future work could expand the dataset to include more samples. Secondly, for systematic and automated evaluation, there is a certain gap between our tasks and real-world scenarios. Future work could consider designing tasks that are more reflective of real-world situations."}, {"title": "6 Conclusion", "content": "In this paper, we present a new benchmark BABELBENCH designed to evaluate large language models in multimodal and multistructured data scenarios. By incorporating 247 human-annotated questions, BABELBENCH assesses LLM capabilities in multimodal understanding, table interpretation, and code generation, while also testing perceptual and reasoning skills. Our experiments with 13 LLMs, including ChatGPT 4, reveal substantial room for improvement, underscoring the complexity of the tasks. BABELBENCH sets a new standard for comprehensive LLM evaluation, guiding future research towards the development of more intelligent and versatile LLM-as-Agent systems capable of addressing real-world challenges."}]}