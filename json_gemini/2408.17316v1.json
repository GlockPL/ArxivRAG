{"title": "Bridging Domain Knowledge and Process Discovery Using Large Language Models*", "authors": ["Ali Norouzifar", "Humam Kourani", "Marcus Dees", "Wil van der Aalst"], "abstract": "Discovering good process models is essential for different process analysis tasks such as conformance checking and process improvements. Automated process discovery methods often overlook valuable domain knowledge. This knowledge, including insights from domain experts and detailed process documentation, remains largely untapped during process discovery. This paper leverages Large Language Models (LLMs) to integrate such knowledge directly into process discovery. We use rules derived from LLMs to guide model construction, ensuring alignment with both domain knowledge and actual process executions. By integrating LLMs, we create a bridge between process knowledge expressed in natural language and the discovery of robust process models, advancing process discovery methodologies significantly. To showcase the usability of our framework, we conducted a case study with the UWV employee insurance agency, demonstrating its practical benefits and effectiveness.", "sections": [{"title": "1 Introduction", "content": "Recorded event data within information systems provides a rich source of information for process mining applications, enabling organizations to gain insights and improve their operational processes. In the field of process mining, various automated techniques are utilized to discover descriptive models that explain process executions. Despite the development of numerous methodologies for process discovery, the task remains inherently complex and challenging [3]. Discovering process models that do not align with domain knowledge presents significant challenges, particularly when these models are intended for conformance checking and process improvement."}, {"title": "2 Related Work", "content": "In traditional process discovery, event data are often used as the primary source of information to create process models [3]. However, additional information resources, such as various forms of process knowledge, can significantly enhance the quality of the discovered models [14]. When available, this supplementary knowledge can be utilized before discovery to filter the event log [5], during the discovery phase to influence the process model structure [12], or within an interactive framework [4,15]. Despite these benefits, the direct involvement of process experts is often limited due to the complexities involved in integrating their knowledge into process discovery.\nIn [12], declarative rules are used as an additional input for process discovery, which can be provided by the user or generated by automated methods. However, expecting users to be proficient in declarative rule specification language is not always feasible. The proposed method in [4] requires users to engage at a low level to position transitions and places based on guiding visualizations. The approach in [15] begins with an initial model discovered from a user-selected subset of variants and incrementally allows adding more variants to update the process model. Some research focuses on repairing process models after discovery, primarily to improve the correspondence between the process models and event logs, rather than incorporating process knowledge [13]. In contrast, our paper aims to minimize the effort required from domain experts by using natural language conversations to influence process discovery.\nTranslating the natural language to process models using natural language processing is investigated in [1]. Anomaly detection is examined in [2] by focusing on semantic inconsistencies in event labels within event logs, utilizing natural language processing to identify anomalous behavior. Recently, LLMs have been employed for various process mining tasks. The opportunities, strategies, and challenges of using LLMs for process mining and business process management are discussed in [16]. Additionally, several studies propose the extraction of process models directly from textual inputs [6,8,9]. Unlike these approaches, our method maintains the event log as the main source of information while incorporating textual process knowledge into the discovery process."}, {"title": "3 Background", "content": "The blue box in Fig. 1 highlights one recursion of the IMr framework [12]. Each recursion extracts a Directly Follows Graph (DFG) from the event log, representing the set of activities \u2211 and their direct succession. The algorithm searches for all binary cuts that divide \u2211 into two disjoint sets considering a structure specification type, i.e., sequence, exclusive choice, concurrent, or loop type. IMr filters out candidate cuts that may violate any ruler \u2208 R, where R is the set of rules given by the user or discovered using automated methods. While [12] incorporates declarative constraints listed in Table. 1, the framework is flexible to support other rule specification languages. Cost functions evaluate the quality of candidate cuts, based on counting the number of deviating edges and estimating"}, {"title": "4 Motivating Example", "content": "To motivate the research question addressed in this paper, consider the following event log extracted from a synthetic process L=[< A-created, A-canceled )300,\n(A-created, Doc-checked, Hist-checked, A-accepted )200,\u3008A-created, Hist-checked,\nDoc-checked, A-accepted)50, (A-created, Doc-checked, Hist-checked, A-rejected )300,\n(A-created, Hist-checked, Doc-checked, A-rejected )80, \u3008A-created, A-canceled, A-\naccepted) 20, (A-created, A-canceled, A-rejected )15, \u3008A-created, Doc-checked, Hist-\nchecked, A-rejected, A-accepted )35], where A stands for application, Doc for documents, and Hist for history.\nFigure 2a illustrates the process model discovered using the IMf algorithm\nas a state-of-the-art process discovery technique [10]. The IMr framework with\nparameter sup = 0.2 and utilizing the Declare Miner [11] with confidence = 1\ndiscovers the same process model. Consider that in addition to the provided\nevent log, we have some additional process knowledge that helps us verify this\nmodel and pinpoint the possible unexpected behavior represented in the process\nmodel. In this paper, ChatGPT refers to ChatGPT-40. We provided a text as\nfeedback on this discovered model and asked ChatGPT to translate natural\nlanguage feedback into understandable rules for the IMr framework. Here is our\nwritten feedback:\nThe discovered process does not fully adhere to our intuitions. Specifically, if a claim\nis canceled, the application cannot be either rejected or accepted. Furthermore, a\nclaim cannot be both rejected and accepted for a single individual. Additionally, the\nhistory is always checked after the documents have been reviewed.\nThe following declarative rules, as explained in this paper, were extracted by\nChat GPT:"}, {"title": "5 Domain-Enhanced Process Discovery with LLMs", "content": "In this section, we present our framework that leverages LLMs to integrate domain knowledge into the process discovery task. Figure 3 illustrates an overview of our proposed framework. The core idea is to utilize domain knowledge to generate a set of rules R which serves as input for the IMr framework. This can be done before starting the discovery by encoding process descriptions as rules, or after the process discovery by having a domain expert review the process model and provide feedback. Engaging in interactive conversations with LLMs in both scenarios helps address uncertainties and improve the quality of the extracted rules. An implementation of the framework is publicly available\u00b9."}, {"title": "5.1 Task Definition", "content": "As outlined in [9], role promoting, knowledge injection, few-shot learning, and negative prompting techniques have significant potential to effectively prepare LLMs for specific process mining tasks. In our initial prompt, we define the role of the LLM as an interface between the domain expert and process discovery framework, such that LLM should encode the domain knowledge to declarative constraints as we need in IMr. Despite the similarity of declarative templates to human logic and reasoning, we observed the difficulties of LLMs in adhering to strict expectations. Therefore, we explain in our prompt the set of constraints we support, detailing both the syntax and the semantics of these constraints (cf. Table 1). We leverage the LLM's ability to derive insights from examples by providing multiple pairs of textual process descriptions and their corresponding declarative constraints. Additionally, we include instructions to avoid common issues, such as syntactic mistakes, and extend our learning pairs to include examples of undesirable constraints. The detailed written prompt is available in our GitHub repository\u00b2."}, {"title": "5.2 Rule Extraction", "content": "After introducing the task, the LLM is ready to receive textual input and produce output as declarative constraints. As illustrated in Fig. 3, domain experts can contribute in three distinct ways: providing business context, offering feedback after reviewing process models, and engaging in interactive conversations with the LLM. In the following sections, we explain these contributions in detail and discuss their respective roles.\nBusiness Context The domain expert can introduce the actual business process to the LLM, providing a general overview, detailing the relationships between specific activities, or even including constraints written in natural language. This flexibility allows the domain expert to tailor the input based on their unique insights and the specifics of the process at hand. It is important to note that the LLM is unaware of specific activity labels used in the recorded event data. The list of activities can be automatically derived from the event log, ensuring that all relevant actions are accurately captured in the generated constraints. Alternatively, the domain expert can provide the list of activities and add context to guide the LLM in relating the process description with the activity labels, resulting in constraints that involve the correct activity labels.\nFeedback integration After generating the initial process model, it is presented to the domain expert for review. The domain expert is expected to examine the process model for accuracy, completeness, and practical alignment with real-life scenarios. In case of finding errors in the represented model, the domain expert can provide a written feedback and explain the behaviors that do not make sense in the real process. The LLM then adjusts and refines the declarative constraints based on this feedback."}, {"title": "5.3 Rule Validation", "content": "An essential step in the framework is checking the extracted declarative constraints from the LLM's response. The LLM is instructed to encapsulate the constraints within specific tags in the response and to write them in a predefined language with no additional text or descriptions. Following extraction, the constraints undergo a validation process. This includes checking that the syntax of each constraint conforms to our predefined language, e.g., checking the type identifier and the number of activities specified within the constraint. Additionally, the labels of activities are verified against the activities recorded in the event log. If any errors are detected during validation, an error-handling loop is initiated. A new prompt specifies the problem and its location, prompting the LLM to adjust its output."}, {"title": "6 Case Study", "content": "A case study with the UWV employee insurance agency is conducted to demonstrate the usability of our approach in a real-life setting. UWV is responsible for managing unemployment and disability benefits in the Netherlands. For this case study, one of UWV's claim-handling processes is selected. Figure 4 depicts the normative model of this process, which was developed in collaboration with process experts who have a thorough understanding of the workflow. The event log used in this study contains 144,046 cases, 16 unique activities, and 1,309,719 events. Our GitHub repository provides the full prompting history and more readable process models\u00b3."}, {"title": "6.1 Process Discovery Without Including Process Knowledge", "content": "Our initial attempt to discover a process model using the IMf algorithm with f = 0.2 resulted in the model shown in Fig. 5a. When compared to the normative model, significant differences are observed, e.g., Receive Claim and Start Claim are the first mandatory steps but the process model allows for skipping them or for many other activities occurring before them. Fig. 5b illustrates the process model discovered using the IMr algorithm with sup = 0.2 and an empty set of input rules. Although this model shows more structural similarities to the normative model, it still contains some nonsensical differences. For instance, Block Claim 1 should only be relevant if the claim is planned to be accepted, but this model permits it for rejected cases as well. Similarly, Receive Objection 2 should only occur if the claim is rejected, yet the model allows it for accepted cases as well."}, {"title": "6.2 Employing ChatGPT to Extract the Rules", "content": "We experimented with Gemini and various versions of ChatGPT to translate the process knowledge into declarative rules. ChatGPT-40 provided the best constraints and demonstrated a superior understanding of the task. By incorporating rules extracted by ChatGPT into the IMr framework, we obtained the process model shown in Fig. 5c. After iterating with feedback from domain experts, the model is further refined and improved, resulting in the process model depicted in Fig. 5d."}, {"title": "7 Conclusion", "content": "The integration of process knowledge in the discovery of process models is often overlooked in the literature. In this paper, we leveraged advancements in LLMs to demonstrate their capabilities in encoding textual domain knowledge into comprehensible rules for process discovery. Our proposed framework not only facilitates the integration of feedback from domain experts but also enables interactive improvement of process models. Through a comprehensive case study, we demonstrated the effectiveness of our framework in generating process models that better align with process knowledge. While the extracted set of declarative constraints from LLMs shows great promise, there is still room for improvement in precision and completeness. Future work focuses on expanding the range of declarative templates within the IMr framework and developing additional rule specification patterns. Additionally, providing more detailed examples in task definition steps helps LLMs capture a broader context, further enhancing the quality of the extracted constraints."}]}