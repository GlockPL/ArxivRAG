{"title": "MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music, and Musical Captions", "authors": ["Suhwan Choi", "Kyu Won Kim", "Myungjoo Kang"], "abstract": "We introduce Multimodal Matching based on Valence and Arousal (MMVA), a tri-modal encoder framework designed to capture emotional content across images, music, and musical captions. To support this framework, we expand the Image-Music-Emotion-Matching-Net (IMEMNet) dataset, creating IMEMNet-C which includes 24,756 images and 25,944 music clips with corresponding musical captions. We employ multimodal matching scores based on the continuous valence (emotional positivity) and arousal (emotional intensity) values. This continuous matching score allows for random sampling of image-music pairs during training by computing similarity scores from the valence-arousal values across different modalities. Consequently, the proposed approach achieves state-of-the-art performance in valence-arousal prediction tasks. Furthermore, the framework demonstrates its efficacy in various zeroshot tasks, highlighting the potential of valence and arousal predictions in downstream applications.", "sections": [{"title": "Introduction", "content": "Integrating multiple modalities in machine learning, particularly image, audio, and text, presents a compelling yet complex challenge. This complexity stems from the inherent diversity and intricate relationships of these three modalities. AudioCLIP (Guzhov et al. 2022) has established foundational work in this domain, capturing cross-modal relationships between non-musical audio, texts, and images. However, prior research did not specifically address music's interplay with associated visual and textual components.\nBuilding upon this groundwork, we address a novel problem by focusing on the multimodal relationships between images, music, and text. To the best of our knowledge, this work is the first in multimodal learning to specifically examine the interplay among these three modalities, particularly in the context of musical captions. By shifting the focus to this underexplored combination of modalities, we aim to open new research avenues that explore how these three modalities can interact to generate meaningful insights and applications. To further this exploration, we expand"}, {"title": "Related Work", "content": "Multimodal learning with tower encoders. In multimodal learning, a tower encoder refers to a separate neural network architecture that processes data of a particular modality, as exemplified by the CLIP image and text encoder of CLIP (Radford et al. 2021). Tower-based multimodal learning offers several advantages. First, each tower can be optimized for its specific data type, leveraging architectures best suited to the unique characteristics of the particular data modality. Second, it allows for the independent scaling and improvement of each tower without necessitating a redesign of the entire model. Updates or changes can be made to one modality without impacting the others.\nPrevious research has demonstrated that the audio modality can be successfully integrated with other modalities in multimodal learning using tower encoders. Conventional audio-text multimodal research (Wu et al. 2023; Elizalde et al. 2023; Mei et al. 2022) requires precise one-to-one audio-text matching in the speech data. Likewise, training a three-tower framework (Guzhov et al. 2022) capable of processing audio, image, and text relies on precise multimodal matching, as shown in Figure 1.\nMultimodal learning with music. Image-music multimodal learning remains largely unexplored, in contrast to music-text multimodal learning. For music-text tasks, MuLan (Huang et al. 2022) trains a text encoder and a music encoder in a two-tower framework for music tagging and music-text retrieval. For music-image tasks, CDCML (Zhao et al. 2020) constructed IMEMNet, a dataset of image-music pairs with VA matching scores, and proposed a multimodal training framework for emotion-based image-music matching. Employing a one-to-one image-music matching dataset, (Nakatsuka, Hamasaki, and Goto 2023) jointly trains image and music tower encoders to retrieve music from album covers and vice versa."}, {"title": "Dataset", "content": "The biggest challenge to multimodal learning involving image, music, and musical caption is data scarcity. To address this, IMEMNET-C extends the original IMEMNet dataset (Zhao et al. 2020) by incorporating musical captions corresponding to the music data in IMEMNet."}, {"title": "IMEMNet", "content": "IMEMNet integrates a music dataset with a diverse range of image datasets, each annotated with distinct emotional valence-arousal (VA) labels, to establish a comprehensive dataset for multimodal studies.\nImage datasets. IMEMNet incorporates three distinct real-world image datasets: IAPS (Lang 2005), NAPS (Marchewka et al. 2013), and EMOTIC (Kosti et al. 2017).\nThe International Affective Picture System (IAPS) comprises 1,182 images, each annotated using a 9-point Valence-Arousal-Dominance (VAD) scale. The Nencki Affective Picture System (NAPS) includes 1,356 images evaluated on a 9-point bipolar semantic sliding scale, focusing on VA dimensions and approach-avoidance measures. The EMOTions In Context (EMOTIC) features 23,082 images of people, with annotations on a continuous 10-point VAD scale.\nMusic dataset. IMEMNet utilizes the DEAM (Database for Emotion Analysis using Music) dataset, which consists of 1,802 music tracks, each annotated with valence and arousal values ranging from -1 to +1. These annotations are provided on a per-second basis and for the entire track. To ensure annotation stability, the annotation for each track starts at the 15th second. The tracks, predominantly 45 seconds in length and recorded at a frequency of 44100Hz, vary in duration, with some extending over 600 seconds. By dividing these tracks into two-second segments, the dataset comprises a total of 23,944 clips. Unlike Zhao et al. (2020), we extend the maximum clip length to 10 seconds by concatenating up to five consecutive clips with each two-second clip, allowing our framework to handle longer music data.\nCombining the datasets. To address the different scales used for labeling the image and music data, VA values are normalized to a [0,1] range. In total, IMEMNet comprises 25,620 images and 23,944 music clips, resulting in 144,435 image-music pairs, which are divided into 80% for training, 5% for validation, and 15% for testing.\nObtaining matching scores. To align images and music clips, image-music matching scores are calculated based on the Euclidean distance between their two-dimensional VA vectors. The similarity between an image $I_i$ and a music clip $M_j$ is quantified by the following:\n$S(I_i, M_j) = exp(-\\frac{d(y_{I_i}, y_{M_j})}{\\sigma})$,\nwhere a and b represent the total number of images and music clips respectively, such that $i \\in \\{1, ..., a\\}$ and $j \\in \\{1, ..., b\\}$. Here, $d(y_{I_i}, y_{M_j})$ denotes the Euclidean distance between the VA labels of image $I_i$ and music clip $M_j$, and $\u03c3$ is the average Euclidean distance across all image and music clip pairs. This similarity measure serves as the emotional matching label for each corresponding image and music clip pair.\nIt's important to highlight that the potential number of all matching pairs in this setup is a \u00d7 b. Given the scale of the image and music clip datasets, this could result in hundreds of millions of possible pairs. To manage this vast number and prevent an overwhelming expansion of the dataset, Zhao et al. (2020) chose 50 images for each music clip: 30 are randomly picked from the image dataset, and the remaining 20 are split equally between those with the highest and lowest matching scores. To further refine the dataset, 10% of these pairs are randomly sampled to form the final IMEM-Net dataset. Zhao et al. (2020) also ensured no overlap between the training, validation, and test sets."}, {"title": "IMEMNet-C", "content": "The IMEMNet dataset only contains image and music pairs, lacking musical captions for the music data. To address this gap, we introduce a new dataset, IMEMNet-C, which contains musical captions corresponding to the music data. Music captions are generated by the music-to-text large language model, LP-MusicCaps (Doh et al. 2023a).\nEvaluations of the generated captions revealed the prevalence of redundant phrases, such as \u201cThe low quality recording.\" To ensure high-quality captions for the music-image pairs, redundant phrases were removed in two steps. First, a rule-based refinement was applied to part of the audio captions. About half of the captions with redundant phrases contained the exact phrase \"low quality recording features.\" This redundant phrase was rephrased as \"recording features.\" Then, Llama-3.1-8B (Dubey et al. 2024) refined musical captions by removing and rephrasing the redundant expressions. We use the prompt \"Refine or remove an audio quality-related phrase to eliminate any mention of audio quality\" with 4-shot examples.\""}, {"title": "The Proposed Method: MMVA", "content": "Asymetric multimodal matching. Multimodal training with IMEMNet-C presents a novel scenario in the field of multimodal learning. In this setup, multimodal matching exhibits asymmetry: some modality pairs (music-text) are one-to-one matched, while others (image-text, image-music) are not. For instance, each music clip has its corresponding musical caption, just as in conventional multimodal training scenarios. Accordingly, music clips and their corresponding captions share the same VA values. However, the multimodal pairs involving images, such as image-music and image-caption, lack exact correspondence. In these cases, the degree of matching is represented by continuous emotional matching scores ranging from 0 to 1, providing a nuanced approach to multimodal alignment.\nRandom multimodal matching. Conventional multimodal matching relies on one-to-one matching between multimodal data, whereas our approach does not. The continuous nature of valence and arousal allows any multimodal pair to have a similarity matching score using Equation 1. As long as valence and arousal values are available, this dramatically reduces the burden of obtaining multimodal data.\nRandom Matching Based on Valence and Arousal\nTo handle the asymmetric multimodal training scenario, we propose MMVA, a framework that leverages random multimodal matching to facilitate learning across images, music, and musical captions. During training, the framework uses Valence-Arousal (VA) predictions to preserve the emotional integrity of each modality while optimizing similarity scores to align modalities cohesively in the absence of precise matches. Conventional multimodal learning approaches, such as AudioCLIP, rely on contrastive loss to align one-to-one paired data modalities and optimize semantic similarity"}, {"title": "VA prediction loss.", "content": "For each embedding vector $z_k$ corresponding to modality k, we obtain a two-dimensional VA vector $e_k = [valence, arousal]^T$ such that $e_k = p_k(z_k)$, where $p_k$ is the VA predictor for the modality k. Then, for a batch of VA predictions $E_k$ and a corresponding batch of ground-truth VA vectors $Y_A$ for modality k, we compute single-modality VA prediction loss:\n$L_{VA}^k = \\frac{1}{N} \\sum_{n=1}^N ||E_n^k - Y_A||_2^2$\n$L_{VA}^k$ quantifies the accuracy of the VA predictions for each modality, providing a measure of how well the model's predictions align with the actual emotional valence and arousal levels represented in the data. By minimizing this loss during training, the model learns to generate more emotionally accurate and representative embedding $z$ for each modality."}, {"title": "Similarity matching loss.", "content": "Each triplet, comprising an image, a music clip, and a musical caption, is assigned a single emotional matching score. Since music clips and their corresponding captions share identical VA values, an emotional matching score of a triplet directly corresponds to the similarity score between the image and the music clip.\nFor each triple of embedding vectors, we generate a unified multimodal embedding vector by concatenating the individual embeddings: $m = concat(z_{Img}, z_{Mus}, z_{Cap})$. A similarity predictor $p_{sim}$ then maps this multimodal embedding vector m to an emotional matching score within the range [0,1]. Then, given a batch of predicted scores S and a"}, {"title": "Training objective.", "content": "During training, we optimize the following objective, assigning an equal weight of 1 to each loss:\n$L_{MMVA} = L_{Img}^{VA} + L_{Mus}^{VA} + L_{Cap}^{VA} + L_{sim}$\nBy minimizing $L_{MMVA}$, the models illustrated in Figure 2 learn to quantify the complex emotional relationships between images, music, and captions."}, {"title": "Experiments", "content": "Training Details\nThe AdamW (Loshchilov and Hutter 2017) optimizer is utilized with a learning rate of 0.0003, a weight decay of 0.01, $\u03b2_1$ of 0.9, and $\u03b2_2$ of 0.999. The training is conducted for 1000 epochs, using a batch size of 128 and a cosine annealing schedule (Loshchilov and Hutter 2016).\nArchitectures\nAs depicted in Figure 2, MMVA utilizes 7 separate modules. These include a modality-specific encoder $f_k$ and VA predictor $p_k$ for k \u2208 {Image, Music, Caption}, along with a similarity predictor encoder $P_{sim}$.\nModality-specific encoder. For the image encoder $f_{img}$, MMVA employs the CLIP image encoder (Radford et al. 2021), which is based on ViT-B/32 (Dosovitskiy et al. 2021). $z_{Img}$ corresponds to the 512-dimensional class token vector. For both music and captions, MMVA uses BERT-based models (Devlin et al. 2019): MERT-95M (Li et al. 2023) for $f_{mus}$ and ROBERTa-base (Liu et al. 2019) for $f_{cap}$. For each of these BERT-based models, the proposed method extracts class tokens vectors $h_i \\in R^{768}$ from the last 13 transformer blocks (Vaswani et al. 2017) and gathers the vectors to form modality features $h \\in R^{13\u00d7768}$. Subsequently, a 1D convolution is applied to derive a 768-dimensional modality vector, which is then linearly projected to obtain the modality-specific embedding $z_k \\in R^{512}$."}, {"title": "Emotion predictors.", "content": "Mapping $z_k$ to two-dimensional VA prediction vectors, VA predictors $p_{va}$ share the same architecture across k: linear512-relu-layernorm-dropout0.5-linear512-relu-layernorm512-dropout0.5-linear2-sigmoid. The similarity predictor $p_{sim}$ is identical to $p_{va}$ except the first and last layers because $p_{va}$ takes 1536-dimensional multimodal embedding vectors mas inputs and outputs a one-dimensional value."}, {"title": "VA Prediction", "content": "MMVA integrates emotion prediction into its training process by utilizing the VA predictors, $p_{va}$, and similarity predictor, $P_{SIM}$. These modules are designed to capture emotional content by learning to predict valence-arousal values. VA prediction performance illustrates how well modality-specific emotional information is preserved during multimodal training. On the other hand, similarity prediction performance reflects the degree of multimodal information present in the embeddings. Together, these metrics provide a comprehensive assessment of the overall efficacy of the multimodal training process.\nSetups. We compare MMVA against the baseline results obtained by Zhao et al. (2020): SP-Net, L\u00b3-Net (Arandjelovi\u0107 and Zisserman 2017), ACP-Net (Verma, Dhekane, and Guha 2019), and CDCML (Zhao et al. 2020). All the baselines are tailored for emotion-based image-music correspondence, employing the same emotion predictor architectures as used in our work. Evaluation employs Mean Squared Error (MSE) and Mean Absolute Error (MAE) as metrics to quantify the similarity between the predicted and ground-truth emotion values.\nResults. Table 1 presents the emotion prediction performance across all modalities. Because the baselines were trained on image-music modalities only, they do not provide VA performance metrics for captions. For image and music modalities, MMVA outperforms the best baseline CDCML across all metrics except MAE of image arousal."}, {"title": "Text-to-Music Retrieval", "content": "Leveraging the VA-based similarity score (Equation 1), MMVA can achieve cross-modal retrieval for various modality pairs. Figure 3 illustrates the text-to-music retrieval based on valence and arousal. Given a query text, the music clip"}, {"title": "Image-to-Text Retrieval for Music Generation.", "content": "Leveraging the VA-based retrieval process, MMVA can generate text prompts for text-to-music generative models.\nMusic generation. Figure 4 illustrates the image-to-text generation processes. First, a VA vector for the target image is obtained. Then, text prompts are exhaustively generated by inserting candidates into each text template illustrated in Figure 5. As a result, VA values for the generated text prompts are obtained, allowing for the calculation of the VA distance between the target image and the text prompts. By selecting the text prompt with the smallest distance, the best text prompt for the image is identified and fed into the text-to-music generative model, e.g. MusicGen (Copet et al. 2024). Generated captions are illustrated in Figure 6."}, {"title": "Video Summarization", "content": "Arousal represents the intensity of an emotion, with higher arousal levels indicating more intense emotional responses. Based on the premise that highlights in a video generally correspond to moments of high arousal, we can utilize arousal values to summarize videos. In the absence of a dedicated video summarization benchmark for music, we employ the image VA predictor, $p_{img}$, to identify significant frames within a video and benchmark its performance for image-based video summarization.\nSetups. To evaluate the effectiveness of arousal-based video summarization, we use the TVSum(Song et al. 2015) dataset, with frame-level importance scores assigned every two seconds. These scores range from 1 (not important) to 5 (very important). MMVA's zeroshot approach is compared against well-established unsupervised video summarization methods: DR-DSN(Zhou and Qiao 2017), CSNet (Jung et al. 2018), AC-SUM-GAN (Apostolidis et al. 2021), and CA-SUM (Apostolidis et al. 2022). For generating video summaries, MMVA follows the same procedure as these baselines, solving a knapsack problem to select frames based on the frame-level importance scores, with each frame representing a short clip in a video. The duration of the summarized videos remained within 15% of the original.\nEvaluation. F-score is used to evaluate the similarity between predicted summary Q and human-defined summary Q, given by:\n$FS = \\frac{2(P x R)}{P+R}$   P   =   $\\frac{||Q \\cap Q||}{||Q||}$,  R =  $\\frac{||Q \\cap Q||}{||Q||}$"}, {"title": "Ablation Experiment", "content": "Setup. Two ablation setups are used for comparison. No Random Matching is trained using the pre-defined multimodal pairs originally used in IMEMNet rather than the random multimodal matching. No Similarity Predictor relies solely on modality-specific VA predictors, discarding the similarity predictor shown in Figure 2. Other factors, such as training hyperparameters, remain constant.\nResults. and show that training without similarity predictor and random multimodal matching results in inferior performance compared to the baseline, demonstrating the effectiveness of the proposed method."}, {"title": "Conclusion", "content": "We have introduced MMVA, a novel framework for underexamined interactions among images, music, and musical captions using our IMEMNet-C dataset. MMVA addresses an unconventional multimodal training scenario: music-text pairs are one-to-one matched, while image-music and image-text pairs are aligned using continuous valence-arousal (VA) matching scores. By leveraging random multimodal matching based on VA scores, MMVA achieves state-of-the-art performance in VA prediction and demonstrates promising zero-shot capabilities in various zeroshot tasks. This makes MMVA particularly valuable for scenarios where obtaining one-to-one multimodal data is challenging, especially in music, where emotional content and subjective evaluation add complexity.\nLimitations While MMVA aligns modalities through emotional similarity, it may not fully capture other aspects of multimodal relationships, such as semantic alignment. Additionally, the inherent subjectivity of emotional perception could limit the model's generalizability across diverse cultural and demographic contexts."}]}