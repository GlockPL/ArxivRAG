{"title": "RED: Effective Trajectory Representation Learning with Comprehensive Information", "authors": ["Silin Zhou", "Shuo Shang", "Lisi Chen", "Christian S. Jensen", "Panos Kalnis"], "abstract": "Trajectory representation learning (TRL) maps trajectories to vectors that can then be used for various downstream tasks, including trajectory similarity computation, trajectory classification, and travel-time estimation. However, existing TRL methods often produce vectors that, when used in downstream tasks, yield insufficiently accurate results. A key reason is that they fail to utilize the comprehensive information encompassed by trajectories. We propose a self-supervised TRL framework, called RED, which effectively exploits multiple types of trajectory information. Overall, RED adopts the Transformer as the backbone model and masks the constituting paths in trajectories to train a masked autoencoder (MAE). In particular, RED considers the moving patterns of trajectories by employing a Road-aware masking strategy that retains key paths of trajectories during masking, thereby preserving crucial information of the trajectories. RED also adopts a spatial-temporal-user joint Embedding scheme to encode comprehensive information when preparing the trajectories as model inputs. To conduct training, RED adopts Dual-objective task learning: the Transformer encoder predicts the next segment in a trajectory, and the Transformer decoder reconstructs the entire trajectory. RED also considers the spatial-temporal correlations of trajectories by modifying the attention mechanism of the Transformer. We compare RED with 9 state-of-the-art TRL methods for 4 downstream tasks on 3 real-world datasets, finding that RED can usually improve the accuracy of the best-performing baseline by over 5%.", "sections": [{"title": "1 INTRODUCTION", "content": "With the proliferation of GPS-enabled devices (e.g., smartphones, navigators, and digital watches), large amounts of trajectories are collected, recording the movements of pedestrians or vehicles. These trajectories serve as the foundation for many applications such as traffic prediction [49], urban planning [33], and transportation optimization [38]. However, as sequences of timestamped locations, trajectories require specific techniques for management and analysis. For example, trajectory similarity computation often relies on dynamic programming [3, 9], resulting in computational costs that increase quadratically with trajectory length.\nRecently, trajectory representation learning (TRL), which maps each trajectory to a vectors embedding, has attracted attention as a general preprocessing technique [20, 31, 42, 43]. The advantage is that the learned vectors can be used directly in many downstream tasks, including trajectory classification [24], travel time estimation [27], and trajectory similarity computation [51], with standard vector processing techniques. For instance, trajectory similarity can be calculated as the distance between the vectors of two trajectories, with a cost that does not increase with trajectory length.\nEarly TRL methods [23, 45] usually target a specific task. For example, Traj2vec [45] uses Recurrent Neural Networks (RNNs) to map trajectories to vectors and tailors model design for trajectory clustering task. The problem of these methods is that their vectors do not work well when used in other downstream tasks. Subsequent TRL methods [10, 13, 41] utilize self-supervised learning (SSL) due to the strong generalization capabilities of SSL. In particular, SSL relies on a generic pre-training task to distill data information, e.g., by manually masking a portion of the trajectories and learning to recover the trajectories. For example, Trembr [13] uses RNNs [19] with an encoder-decoder architecture. The encoder embeds each trajectory into a vector, and the decoder recovers the trajectory from this vector. Recent TRL methods [20, 31, 43] adopt contrastive learning (CL). CL conducts data augmentations to generate positive and negative samples for each trajectory, and the model is trained to make positive trajectory pairs more similar than negative pairs. For instance, START [20] features several data augmentation techniques, such as randomly masking segments in the trajectories and trimming the trajectories to get sub-trajectories. However, data augmentation techniques may not generalize across datasets because their performance depends strongly on the dataset under consideration.\nA trajectory encompasses multiple kinds of information, including road, user, spatial-temporal, travel, and movement, but existing TRL"}, {"title": "2 PROBLEM AND BACKGROUND", "content": "2.1 Preliminaries\nDefinition 1 (GPS Trajectory). A GPS trajectory, denoted as $T_{gps}$, is a sequence of points collected at a fixed sampling time interval rate. Each point in $T_{gps}$ takes the form of $p_i = (x_i, y_i, t_i)$, where $x_i$, $y_i$, and $t_i$ denote longitude, latitude, and timestamp, respectively.\nDefinition 2 (Road Network). A road network is modeled as a directed graph $G = (V, A)$, where $V$ denotes the road segment set in the road network, and $A \\in R^{|V|\\times|V|}$ is the adjacency matrix that represents the connectivity between road segments. $A[i, j] = 1$ if and only if road segments $v_i$ and $v_j$ are directly connected, otherwise $A[i, j] = 0$. Under this definition, a trajectory can be extracted as a sequence of road segments that it passes through.\nDefinition 3 (Path Trajectory). A path trajectory $T$ is a time-ordered sequence of road segments that is generated from $T_{gps}$ by map matching. That is, $T = (\\tau_1, \\tau_2, ..., \\tau_{|T|})$ and contains the $|T|$"}, {"title": "2.2 Problem Statement", "content": "Given a set of path trajectories $D = \\{T_1, T_2, ..., T_{|D|}\\}$ and a road network graph G, we aim to compute a generic vector representation $p_i \\in R^l$ for each path trajectory $T_i \\in D$, where $l$ is the dimension of the trajectory vector representation. We expect these vectors to achieve a high accuracy for various downstream tasks:\n\u2022 Trajectory similarity computation: Given trajectories $T_a$ and $T_b$, trajectory similarity computation calculates a score capturing the similarity between $T_a$ and $T_b$.\n\u2022 Most similar trajectory retrieval: Given a query trajectory $T_a$ and a trajectory dataset $D$, this task finds the trajectory $T_b \\in D$ that is the most similar to $T_a$.\n\u2022 Trajectory classification: Given a trajectory $T_a$, this task assigns $T_a$ to a category, e.g., a user ID.\n\u2022 Travel time estimation: Given trajectory $T_a$ without temporal information, this task predicts the travel time of $T_a$."}, {"title": "2.3 Background on Machine Learning", "content": "Transformer. The Transformer [34] architecture has demonstrated impressive effectiveness at NLP. It models a token sequence, where a token is a word identifier, that can be converted into a learnable vector. A token does not need to be associated with a word. Thus, a language model BERT [11] designs a [CLS] token to represent the summary of a sentence. The Transformer architecture encompasses multiple stacked blocks, each block contains a multi-head self-attention and a feed forward network. Multi-head self-attention learns the inter-relationships between different elements in a sequence. A feed forward network further enhances feature extraction and the model's expressiveness. Compared to RNN-based methods [19], the Transformer can accommodate billions of model parameters by stacking blocks and computes a long token sequence in parallel, without the need to iterate through time steps.\nAn input token sequence embedding is given by $X \\in R^{nxl}$, where $n$ is the sequence length, and $l$ is the dimensionality of vectors. The vanilla Transformer first adds a position encoding to X and then transforms X to a query matrix $Q = XW_Q \\in R^{nxl}$, a key"}, {"title": "3 METHOD OVERVIEW", "content": "The left plot of Figure 1 shows the workflow of RED during training, which involves three key modules, i.e., road-aware masking, spatial-temporal-user joint embedding, and dual-objective learning.\nRoad-aware Masking. As the first step, the map matching algorithm [39] is conducted to transform a GPS trajectory to a path trajectory. Then, the road-aware masking strategy splits a path trajectory $T$ into key path set $T_k$ and mask path set $T_m$, where $T = T_k \\cup T_m$, based on the sampling rate and driving pattern of the trajectory. The key paths in $T_k$ encompass the driving patterns of the trajectory while the mask paths in $T_m$ are less crucial and thus are masked for the encoder. In comparison, existing methods adopt the random masking strategy, which randomly masks the paths. Trajectory information will be lost if some key paths are masked. Moreover, random masking also requires extensive tuning of the mask ratio while our road-aware masking does not."}, {"title": "4 KEY DESIGNS OF RED", "content": "We proceed to detail the key innovations of RED, which includes the road-aware masking strategy, the spatial-temporal-user joint embedding, the dual-objective task learning, and the enhanced trajectory modeling techniques.\n4.1 Road-aware Masking Strategy\nMask-based self-supervised learning is popular for TRL, which first masks some road segments of a path trajectory, and then learns to reconstruct the masked road segments. However, existing methods use the random masking strategy, which has two problems. First, it may discard essential road segments of a trajectory, leading to"}, {"title": "Joint Embedding", "content": "Next, each segment of a trajectory is encoded as a raw embedding to serve as framework input, which can be expressed as $X = Emb(T)$. The spatial-temporal-user joint embedding integrates comprehensive information of the trajectory, including spatial features from the road network graph, temporal features from the traffic and travel patterns over time, user features from user ID, and segment features from the road types.\nDual-objective learning. We train RED with two objectives on to provide sufficient supervision signals for learning. In particular, the encoder maps the key paths in $T_k$ to key path embedding $x_k = Encoder(X_k)$. As the key paths capture the crucial driving patterns of a trajectory, we use the next segment prediction objective $L_{nsp}$ for the encoder to predict the next key path given the previous key paths. This resembles next token prediction in NLP. The decoder takes the path embeddings generated by the encoder, merges the key paths and mask paths according to their original order in the trajectory, and reconstructs the trajectory using the embeddings, i.e., $X' = Decoder(Unshuffle(X_k,X_m))$. Thus, we use the trajectory reconstruction objective $L_{tr}$ to train the decoder. Therefore, the overall objective function is:\n$L = \\lambda_1 L_{nsp} (X_k) + (1 - \\lambda_1)L_{tr}^{to} (X^T)$,   (2)\nwhere $\\lambda_1$ controls the weight of the loss terms. To tackle sparsity on trajectories, i.e., GPS points recorded by the device during the driving process is discontinuous, inconsistent or missing, we incorporate comprehensive information, more effective supervision signals, and a GNN for the road network.\nAs shown to the right in Figure 1, during inference, we only utilize the encoder and feed the embeddings of the complete trajectory as input to get the trajectory vector for downstream tasks."}, {"title": "4.2 Spatial-temporal-user Joint Embedding", "content": "Path trajectories encompass a wealth of information, including user ID, road segment, and time information. All of this information is important when learning a versatile representation for trajectories."}, {"title": "Consequently, we propose a spatial-temporal-user joint embedding", "content": "Consequently, we propose a spatial-temporal-user joint embedding to encode all the information for model input. Formally, given a segment $v_i$ of a path trajectory, we use Equation 3 for encoding:\n$x_i = h_i + t_i + u_i$,    (3)\nwhere $h_i$, $t_i$, and $u_i$ denote the spatial, time and user encodings of segment $v_i$, which are described next.\nSpatial Encoding $h_i$. The road segments of a city form a complex structure, which can be captured by the road network graph G. We observe that the properties of a segment are affected by itself and its adjacent segments. Therefore, we apply a graph neural network (GNN) to learn embeddings for road segments based on the road network graph G. In particular, GNN computes an embedding for each node in the graph by aggregating the embeddings of its neighbors. We choose the Graph Attention Network (GAT) [35], which adopts an attention mechanism for neighbor aggregation.\nWe feed multiple attributes of a segment as the initial input feature of GAT, including the maximum speed limit, average travel time, segment direction, out-degree, in-degree, segment length, and segment type. To be specific, for the maximum speed limit, average travel time, segment direction, and segment length, we apply the min-max normalization. Regarding segment type, we categorize each segment into eight classes, i.e., [living street, motorway, primary, residential, secondary, tertiary, trunk, unclassified], and we employ one-hot encoding. All these attributes are concatenated to obtain the input features $f_i$ for segment $v_i$. Formally, given the initial feature $f_i$ of segment $v_i$ and road network graph G, GAT computes segment embedding as $h_i = GAT(f_i, G)$. We use a 3-layer GAT model to consider 3-hop neighbors for each segment.\nTime Encoding $t_i$. We consider two types of time regularities for trajectories. (i) Trajectory patterns vary considerably at different times of a day or a week. For instance, many trajectories move towards office areas on weekday mornings, while evenings usually observe a surge in trajectories heading back home. (ii) The traffic patterns of different road segment types differ at the same time. For example, segments in commercial areas show elevated trajectory volumes during the daytime, whereas segments in residential areas observe increased trajectory data during non-work hours.\nTo account for (i), we introduce a time encoding to capture periodicity. Given the timestamp $t_i$ of segment $v_i$ in a trajectory, we use Time2vec [21] to generate a vector $e$ of dimension $l/2$.\n$e [k] = \\begin{cases}   1 & \\text{if } k = 1 \\\\   \\sin(\\omega_k t_i + \\psi_k) & \\text{if } 2 \\leq k \\leq \\frac{l}{2} \\\\ \\end{cases}$   (4)\nHere, $e[k]$ denotes the k-th element of vector $e$, $\\{\\omega_k\\}_{k=1}^{n}$ and $\\{\\psi_k\\}_{k=1}^{n}$ are learnable parameters that correspond to periods, and the $\\sin()$ function helps capture periodic behaviors. To account for (ii), we design a segment type encoding. In particular, we employ a learnable type encoding matrix $E^P \\in R^{|T|\\times l/2}$, where each row corresponds to a segment type in $T = \\{\\text{living street, motorway, primary, residential, secondary, tertiary, trunk, unclassified}\\}$. Given the type of a segment $v_i$, we look up $E^P$ to obtain its type encoding $e'$. Then we concatenate the time encoding $e'$ and segment type encoding $e'$, and use a fully connected network to interact two encodings $t_i = FC(e \\parallel e') \\in R^d$, where $||$ means vector concatenation."}, {"title": "User Encoding $u_i$", "content": "User Encoding $u_i$. Different drivers have different travel patterns in their trajectories. E.g., an office worker may follow the same route on all weekdays, while taxi drivers try to pick up passengers in crowded areas every day. By combining spatial information and temporal information (joint embedding) with trajectory attributes (dual-objective task learning), the user encoding can capture the specific behaviors of individual users, including travel time, trajectory length, and high-frequency access areas, which benefits tasks such as trajectory similarity computation, travel time estimation, and trajectory classification. To encode user information, we employ a learnable user encoding matrix $E^U \\in R^{|U|\\times l}$, where $|U|$ is the number of users, and each row of $E^U$ corresponds to a user."}, {"title": "4.3 Dual-objective Task Learning", "content": "Here, we present the two learning objectives of RED.\nPath Encoder. The path encoder is designed to capture key in-formation of a path trajectory and to predict the next segment for learning local information of the trajectory. Section 4.1 shows that the key paths of a trajectory carry much more information than the mask paths, suggesting that a trajectory can be described by the key paths. Thus, we require the path encoder to learn information from the key paths. Formally, given the key path set of a trajec-tory $T_k = \\{\\tau_1, \\tau_2, ..., \\tau_{|T_k|}\\}$, the corresponding key path embedding $X_k = (x_1, x_2,...,x_{|T_k|})$ is first generated by the spatial-temporal-user joint embedding, then position encoding [34] is added to $X_k$ to introduce the position information of each segment in the path trajectory. We feed $X_k$ to the Transformer of the encoder and denote $x_k = Transformer(X_k)$ as the output.\nTo extract the prediction for the next segment from encoder output, we use a fully connected layer $Y_k = FC(X_k)$, where $Y_k \\in R^{|T_k|\\times|V|}$. Each row of $Y_k$ is the predicted distribution of a key segment, and each column of $Y_k$ corresponds to one segment (with $|V|$ being the number of all segments). Then we use cross-entropy loss to compute errors between the predicted value and the ground-truth segment ID for the next segment prediction task as follows.\n$L_{nsp}^T(X_k) = - \\sum_{v_i \\in T_k} \\frac{1}{|T_k|} \\log \\frac{\\exp(Y_k^{v_i})}{\\sum_{v_j \\in V} \\exp(Y_k^{v_j})}$,   (5)\nWe average the above loss over the trajectories in a mini-batch to obtain the final next segment prediction loss $L_{nsp}$.\nNote that we predict the next key path in the encoder, so we use causal self-attention to avoid information leakage.\nPath Decoder. The path decoder is designed to restore a raw path trajectory by decoding the embeddings of its segments generated by the encoder. However, the path encoder only encodes the key paths and ignores the mask paths. Although the mask paths carry less information about the trajectory, they are indispensable when restoring a complete trajectory. Thus, we use another Transformer model as the decoder to combine the key paths and the mask paths. We first transform all mask path segments to learnable embedding $X_m$, and combine $X_m$ with the outputs of the path encoder for the key paths. Then we conduct an unshuffle operation to restore the mask path embeddings to their original positions in path trajectory T. This process can be formulated as $X^T = Unshuffle(Combine(X_k, X_m))$."}, {"title": "The decoder takes $X^T$ as input and computes hidden embedding", "content": "The decoder takes $X^T$ as input and computes hidden embedding output as $X' = Transformer(X^T)$. To restore the raw information of trajectory T, we design a trajectory reconstruction task that aims to reconstruct the whole path trajectory from the decoder output $X^T$. To achieve this, we first use a fully connected layer to obtain the predictions $Y^T = FC(X^T)$, where $Y^T \\in R^{|T|\\times|V|}$. Each row of $Y^T$ is the predicted distribution of a segment of T, and each column of $Y^T$ corresponds to a segment. Then we use cross-entropy loss for a complete trajectory T as follows.\n$L_{tr}^T = - \\frac{1}{|T|} \\sum_{v_i \\in T} \\log \\frac{\\exp(Y_k^{v_i})}{\\sum_{v_j \\in V} \\exp(Y_k^{v_j})}$   (6)\nWe also average the above loss over all trajectories in a mini-batch to obtain the final trajectory reconstruction loss $L_{tr}$.\nExisting TRL methods consider limited supervised information in the training task, e.g., START [20] only reconstructs mask segments of a path trajectory, and Traj2vec [45] recovers trajectory points. RED with dual-objective task explores more supervised information and thus can improve the accuracy of trajectory representations."}, {"title": "4.4 Enhancing Transformer for Trajectory", "content": "To use Transformer as the model for the path encoder and decoder, two issues need to be addressed. First, the next segment prediction task is different from next token prediction of language in subtle ways, and directly using Transformer will encounter errors in the input and output (which we call segment misalignment). Second, the Transformer relies on the self-attention among the token embeddings while trajectories contain spatial-temporal information. To utilize such spatial-temporal information, we need to integrate it into Trans-former attention. To fix segment misalignment, we introduce virtual tokens for the encoder. To utilize spatial-temporal information of trajectories, we design a time-distance enhanced attention to enhance the Transformer for the decoder. Figure 4 gives an overview of the enhanced path encoder and decoder.\n4.4.1 Virtual Token. The next segment prediction task of path encoder can encounter segment misalignment. For instance, given"}, {"title": "the accuracy of RED"}]}