{"title": "Evidential Graph Contrastive Alignment for\nSource-Free Blending-Target Domain Adaptation", "authors": ["Juepeng Zheng", "Yibin Wen", "Jinxiao Zhang", "Runmin Dong", "Haohuan Fu"], "abstract": "In this paper, we firstly tackle a more realistic Domain Adap-\ntation (DA) setting: Source-Free Blending-Target Domain\nAdaptation (SF-BTDA), where we can not access to source\ndomain data while facing mixed multiple target domains\nwithout any domain labels in prior. Compared to existing DA\nscenarios, SF-BTDA generally faces the co-existence of dif-\nferent label shifts in different targets, along with noisy target\npseudo labels generated from the source model. In this pa-\nper, we propose a new method called Evidential Contrastive\nAlignment (ECA) to decouple the blending target domain and\nalleviate the effect from noisy target pseudo labels. First, to\nimprove the quality of pseudo target labels, we propose a cali-\nbrated evidential learning module to iteratively improve both\nthe accuracy and certainty of the resulting model and adap-\ntively generate high-quality pseudo target labels. Second, we\ndesign a graph contrastive learning with the domain distance\nmatrix and confidence-uncertainty criterion, to minimize the\ndistribution gap of samples of a same class in the blended\ntarget domains, which alleviates the co-existence of different\nlabel shifts in blended targets. We conduct a new benchmark\nbased on three standard DA datasets and ECA outperforms\nother methods with considerable gains and achieves compa-\nrable results compared with those that have domain labels or\nsource data in prior.", "sections": [{"title": "1 Introduction", "content": "Despite the great success of deep learning in various fields\n(Krizhevsky, Sutskever, and Hinton 2012; Long, Shelhamer,\nand Darrell 2015; He et al. 2017), it remains a challenge\nto achieve a generalized deep learning model that can per-\nform well on unseen data, especially given the vast diversity\nof real-world data and problems. Performance degradation\noccurs due to various differences between the training data\nand new test data, for aspects such as statistical distribution,\ndimension, context, etc. Domain adaptation (DA) offers sig-\nnificant benefits in this scenario by adapting the pre-trained\nmodels towards new domains with different distributions\nand properties from the original domain (Ben-David et al.\n2010; Cui et al. 2020). Moreover, DA provides great poten-\ntial for efficiency improvement by reducing the need for re-\ntraining on new data from scratch, which has been widely\napplied in various fields and real-world cases."}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Domain Adaptation", "content": "Domain adaptation (DA) aims to minimize domain shift,\nwhich refers to the distribution difference between the\nsource and target domains. Most of them focus on STDA.\nPlenty of single target DA methods have been devised to dif-\nferent tasks including classification (Ganin et al. 2016), se-\nmantic segmentation (Li, Yuan, and Vasconcelos 2019) and\nobject detection (Hsu et al. 2020). To reduce domain dis-\ncrepancies, there are different kinds of methods. One popu-\nlar approach is motivated by generative adversarial networks\n(GAN) (Goodfellow et al. 2014). Adversarial networks are\nwidely explored and try to minimize the domain shift by\nmaximizing the confusion between source and target (Xia,\nZhao, and Ding 2021). Alternatively, it is also common prac-\ntice to adopt statistical measure matching methods to bridge\nthe difference between source and target, including MMD\n(Long et al. 2015), JMMD (Long et al. 2017), and other vari-\nation metrics (Kang et al. 2019). Although extensive works\nhave been done on STDA, there is less research on multi-\ntarget DA, especially for blending-target DA, which is more\nsimilar to the real-world scenarios."}, {"title": "2.2 MTDA and BTDA", "content": "Different from STDA, MTDA is more flexible because the\nmodel can be adapted to multiple target domains with dif-\nferent characteristics, which requires the model to general-\nize better on target data without annotations (Gholami et al.\n2020; Nguyen-Meidine et al. 2021). These requirements in-\ntroduce new challenges, especially the computational re-\nquirements and catastrophic forgetting of previously-learned\ntargets. However, BTDA has received little attention. This\napproach aims to provide solutions for situations where\nthere are no explicit domain labels and samples from differ-\nent target domains are mixed together, which is more con-\nsistent with reality. The differences between single-target\nDA, multi-target DA and mixed multi-target DA is shown\nin Fig. 1. To the best of our knowledge, there are only few\nworks related to this field. For instance, DADA (Peng et al.\n2019b) aims to tackle domain-agnostic learning by disen-\ntangling the domain-invariant features from both domain-\nspecific and class-irrelevant features simultaneously. CGCT\n(Roy et al. 2021) uses curriculumn learning and co-teaching\nmethods to obtain more reliable pseudo-labels. In addition,\nOpen Compound Domain Adaptation (OCDA) also contains\nblending target domain (Liu et al. 2020; Park et al. 2020;"}, {"title": "2.3 Souce-Free Domain Adaptation", "content": "Because of the limit of privacy constraints, SFDA is be-\ncoming more and more popular, which only provides the\nwell-trained source model without any access to source data\nduring the adaptation process. There are two mainstreams\nfor SFDA methods. On the one hand, some methods fo-\ncus on reconstructing the pseudo source domain samples\nin the feature space (Qiu et al. 2021; Tian et al. 2021) On\nthe other hand, some methods exploit pseudo target labels\nfrom source model and adopt self-training or noisy learning\nso that the model is well-fitted to the target domain (Chen\net al. 2022). For example, SHOT (Liang, Hu, and Feng 2020)\nadopts entropy minimization and information maximization\nthrough pseudo-labeling learning to transfer the knowledge\nfrom trained classifier to target features. Up to date, Most\nExisting SFDA methods focus on SF-STDA setting, while\nonly (Kumar et al. 2023) have discussed and designed new\napproach for SF-MTDA. ConMix (Kumar et al. 2023) intro-\nduces consistency between label preserving augmentations\nand utilizes pseudo label refinement methods to reduce noisy\npseudo labels for both SF-STDA and SF-MTDA settings.\nHowever, existing literature have not exploited a new DA\nsetting named Source-Free Blending-Target Domain Adap-\ntation (SF-BTDA), where we have to transfer the knowledge\nfrom a single source to blending-target domain without do-\nmain labels nor the access of labeled source data. In this\npaper, we propose Evidential Contrastive Alignment (ECA)\nto better address both two major challenges for SF-BTDA."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Problem Setting and Notations", "content": "In contrast to MTDA, BTDA is established on a mixture of\ntarget domains $D_T = \\{D_j\\}_{j=1}^{k} = \\{(x_i,y_i)\\}_{i=1}^{n_r}$, where k is\nthe number of target domains and $n_T = \\sum_{j=1}^{k} n_r^j$ denotes\nthe total quantity of images in the target domain. Unlike the\nMTDA scenario, the proportions of different target domains\nin the mixed datasets $\\{w^j\\}_{j=1}^{k}$ are unknown. In the SF-\nBTDA setting, we consider the labeled source domain data\nwith $n_s$, the samples as $D_s = \\{(x_i,y_i)\\}_{i=1}^{n_s}$, in which the\ny is the corresponding label of x, is only available during\nmodel pretraining. Our proposed approach is based on any\nbackbone, which we split into three parts: a shallow feature\nextractor $F_1$, a deep feature extractor $F_2$ and a classifier G.\nAs we have to transfer the knowledge from a single source\nto blending-target domain without domain labels nor the\naccess of labeled source data, if we directly adopt exist-\ning SF-STDA algorithms and consider the mixed target do-\nmain as one target domain in a brute-force way, the training\nobjective will facilitate domain-invariant representations to\nalign the whole blending-target domain $D_T$ rather than k tar-\nget domains $\\{D_j\\}_{j=1}^{k}$. Because of the discrepancy among\nsub-subjects from k distributions, adaptation for the whole"}, {"title": "3.2 Domain Distance Calculation", "content": "As introduced in Sec. 1, SF-BTDA issue faces a mixture\nof multiple target domains, the model is assumed to handle\nthe co-existence of different label shifts in different targets\nwith no prior information (Challenge 1). Therefore, some\nsamples in the same class may appear different styles and\ntextures because of co-existence multiple targets with sig-\nnificantly enriched styles and textures. Under this consid-\neration, we calculate the domain distance to better guide\nthe contrastive learning (see Sec. 3.4) to pull the samples\nin the same class that may appear different styles and tex-\ntures. As we are not access to the specific domain label for\nthe blending-target domain, we separate the target domains\ninto k domains and assign pseudo domain labels for them us-\ning an unsupervised way (i.e., k-means). Notably, we adopt\nthe extracted shallow features from DNNs ($F_1(x^i)$) and in-\ntegrate them with original image textures ($x^i$) to achieve\npseudo domain assignment for better satisfying the specific\nimage style as well as the universal feature style. Notably,\nwe simply concatenate the low-level feature with the origi-\nnal image $x^i$ and we adopt the low-level features that gener-\nated from conv2_x with the dimension of 56\u00d756\u00d7256.\nWe calculate the domain distance $D_{ij}$ between two sam-\nples $x^i$ and $x^j$ through Eq. (1), in which $C_i$ and $C_j$ are the\ncorresponding centroids for $x^i$ and $x^j$. The centroid for an\ntarget image ($x^i$) denotes it belongs to the pseudo domain la-\nbel. The Max Distance denotes we normalize the domain\ndistance among different target domain centroids. If $x^i$ and\n$x^j$ are from the same domain, the domain distance will be\n1.0. In this paper, we utilize the distance of domain cen-\ntroids to represent the samples from different target domains\nfor simplicity. Notably, we update the domain distance after\neach training epoch.\n$D_{ij} = 1 + \\frac{||C_i - C_j||}{Max Distance}$ (1)"}, {"title": "3.3 Calibrated Evidential Learning", "content": "In the meantime, as we are also not access to the source data,\nwe could only achieve pseudo target labels for the blending-\ntarget images using the model trained from labeled source\nimages. Because of Challenge 2 introduced in Sec. 1, di-\nrectly adopting pseudo labels for the target domain may ruin\nand mislead the model training. To alleviate the negative ef-\nfect from noisy target labels, we propose calibrated eviden-\ntial learning to to iteratively improve both the accuracy and\ncertainty of the resulting model, and adaptively select high-quality pseudo target labels.\nEvidential Deep Learning (EDL) (Amini et al. 2020; Bao,\nYu, and Kong 2021) offers a principled way to formulate\nthe uncertainty estimation, which overcomes the shortcom-\ning of softmax-based DNNs. Assuming that the class prob-\nability follows a prior Dirichlet distribution (Sentz and Fer-\nson 2002), we can acquire the predictive uncertainty u for\neach sample: $\u0438 = M/S$, in which M is the number of"}, {"title": "3.4 Graph Contrastive Learning", "content": "The original intention for contrastive learning is to push each\nimage far from others and pull the augmented images closer\nwith the original one in the feature space. However, it is\nnot compatible with the classification task which requires\nto cluster images at the class level, especially that we have\nsome high-quality target samples generated from Sec. 3.3.\nFurthermore, according to the description of Challenge 1,\nsome samples in the same class may appear different styles\nand textures because of co-existence multiple targets with\nsignificantly enriched styles and textures. To this end, we de-\nsign the domain distance calculated from Sec. 3.2 in graph\ncontrastive learning to pay more attention for abovemen-\ntioned samples, which could solve the conflict between con-\ntrastive learning and the classification task.\nSince Sec. 3.3 has generated high-quality labels for the\ntarget domain, we could fully explored their advantages in\nthe contrastive learning. Notably, our contrastive alignment\nmodule aims to learn representations guided by a pseudo-label graph. Here we build the pseudo-label graph by con-\nstructing a similarity matrix (A). To this end, each element\n$a_{ij}$ in A has the following format:\n$A_{ij} =\\begin{cases}\n1 & \\text{if } j = i, \\\\\n1 & \\text{if } z_i \\text{ and } z_j \\text{ are from same category,} \\\\\n& \\text{and } x_i \\in D_t \\text{ and } x_j \\in D_t, \\\\\n0 & \\text{otherwise}\n\\end{cases}$ (4)\nNotably $x_i \\in D_t^e$ and $x_j \\in D_t^e$ denotes that $x_i$ and $x_j$\nare selected from the high-quality target domain samples.\nAlthough high-quality pseudo label-based supervised con-\ntrastive learning is theoretically functional, directly using\nthese pseudo labels may be problematic because they might\nbe noisy labels, too. To alliviate this problem, we adopt the\nconfidence-uncertainty evaluation produced from Sec. 3.3 to\nstrengthen the robustness and reduce the negative effect on\nnoisy pseudo labels. Actually, the pseudo-label graph (A)\nserves as the target to train an embedding graph ($\\tilde{A}$), which\nis defined as:\n$\\tilde{a_{ij}} = \\begin{cases}\nA_{ij} * I_{ij} * D_{ij} & \\text{if } i \\neq j, \\\\\nA_{ij} & \\text{otherwise}\n\\end{cases}$ (5)\nin which $I_{ij} = C_i * C_j * (1 \u2013 U_i) * (1 \u2013 u_j)$ denotes\nthat samples with high confidence and low uncertainty will\nhave higher weights in our evidential contrastive alignment\nmodule. $D_{ij}$ can be calculated from Eq. (1), which could\nmake our contrastive learning module to pay more attention\non the samples of a same class that belonged to different\ntarget domains (addressing Challenge 1). To this end, the\nloss of $L_{CON}$ can be formulated as:\n$L_{CON} =  \\frac{1}{|Z|}\\sum_{i \\in Z} \\{log\\frac{exp(z_i z_{p(i)}/\\tau)}{ \\sum_{p \\in P(i)} exp(z_i z_p/\\tau)}\n+\nlog \\frac{ \\sum_{q \\in Q(i)} a_{iq}exp(z_i z_q/\\tau)}  {\\sum_{p\\in P(i)} a_{ip}exp(z_i z_p/\\tau)} \\}$ (6)\nin which P(i) denotes indices of the views from other im-\nages of the same class in the selected high-quality target do-\nmain samples. |P(i)| denotes its number and |P(i)| + 1 de-\nnotes all positive pairs. 7 is the temperature parameter. Other\nnotations could refer to (Khosla et al. 2020). As our set-\nting is SF-BTDA, it is noteworthy that our graph contrastive\nlearning module conducts on the blending target owing to\nits difficulties from classification confusions because of co-\nexistence of different label shifts (addressing Challenge 1)."}, {"title": "3.5 Overall Objective", "content": "The model is jointly optimized with two loss terms, includ-\ning calibrated evidential learning loss $L_{CEL}$ and graph con-\ntrastive learning loss $L_{CON}$:\n$L = L_{CEL} + \\beta L_{CON}$ (7)\nwhere \u03b2 is a trade-off parameter balancing two different\nloss terms between $L_{CEL}$, and $L_{CON}$"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "We conduct experiments on three standard domain adapta-\ntion benchmarks. Office-31 (Saenko et al. 2010) consists of\n4,652 images from three domains: DSLR (D), Amazon (A),\nand Webcam (W). OfficeHome (Venkateswara et al. 2017)\nis a more challenging dataset, which consists of 15,500 im-\nages in total from 65 categories of everyday objects. There\nare four significantly different domain: Artistic images (Ar),\nClip-Art images (Cl), Product images (Pr), and Real-World\nimages (Rw). DomainNet (Peng et al. 2019a) is the most\nchallenging and very large scale DA benchmark, which has\nsix different domains: Clipart (Cli), Infograph (Inf), Paint-\ning (Pai), Quickdraw (Qui), Real (Rea) and Sketch (Ske). It\nhas around 0.6 million images, including both train and test\nimages, and has 345 different object categories."}, {"title": "4.2 Training Details", "content": "Our methods were implemented based on the PyTorch\n(Paszke et al. 2019). We adopt ResNet-50 (He et al. 2016)\nfor Office-31 and OfficeHome, and ResNet-101 for Do-\nmainNet, both of which pretrained on the ImageNet dataset\n(Russakovsky et al. 2015). Whatever module trained from\nscratch, its learning rate was set to be 10 times that of the\nlower layers. We adopt mini-batch stochastic gradient de-\nscent (SGD) with momentum of 0.95 using the learning rate\nand progressive training strategies. We set \u03b2 in Eq (7) as 1.0.\nOur codes are available on https://github/\nState-of-the-art. ResNet (He et al. 2016) is the base-\nline backbone without any domain adaptation tricks. We list\nsome methods that proposed for STDA scenario, such as\nDAN (Long et al. 2015), DANN (Ganin et al. 2016), RTN\n(Long et al. 2016), JAN (Long et al. 2017), SE (French,\nMackiewicz, and Fisher 2018), MCD (Saito et al. 2018),\nCDAN (Long et al. 2018) and MCC (Jin et al. 2020). We\nalso compare our method with existing three MTDA meth-\nods (i.e., MTDA-ITA (Gholami et al. 2020), DCL (Nguyen-\nMeidine et al. 2021), DCGCT (Roy et al. 2021)) and five\nBTDA methods (i.e., AMEAN (Chen et al. 2019), DADA"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Ablation studies", "content": "As listed in Fig. 3, $L_{CON}$ achieves 2.5% and 3.8% improve-\nment for Office-31 and OfficeHome, respectively. In addi-\ntion, $L_{CON}$ further increase 1.1% and 1.4% for average ac-\ncuracy. More ablation studies and those on DomainNet can\nbe found in our supplementary. In addition, Table 3 lists ab-\nlation results for different high-quality selection schemes.\nTherefore, we both utilize the evidential uncertainty and pre-\ndiction confidence to measure the label reliability simultane-\nously (see more details in Sec. 3.3). Furthermore, we com-"}, {"title": "5.2 Do We Need Domain Label or Source Data?", "content": "According to Table 1, source data is a crucial impact on\nOffice-31, while domain label is not the necessary infor-\nmation during adaption. On the other hand, our proposed\nECA achieves highest accuracy compared with those meth-\nods with source data or domain label on Office-Home. As\nfor DomainNet, source data and domain label are not the\nnecessary conditions. In conclusion, we may not need do-\nmain label nor source data in SF-BTDA scenarios.\nFurthermore, Table 4 displays overall accuracy on differ-\nent datasets with different k. We observe that choosing k as\nthe number of sub-targets may not leads to the superior per-\nformance. While when the k is larger, the accuracy only has\nslight changes. Therefore, we select the same value of k as\nthe true number of target domains in each dataset to report."}, {"title": "5.3 Theoretical understanding", "content": "Inspired by the generalized label shift theorem in (Ta-\nchet des Combes et al. 2020), in SF-BTDA setting, for any\nclassifier $\\hat{Y} = (h \\circ g) (X)$, the blended target error rate\ncould be formulated as $\\frac{K}{\\sum_{j \\leq K}} E_{T_j} + \\frac{1}{2} (C \u2013 1) ABTCE (\\hat{Y})$. $BER_{f,i} (\\hat{Y} | | Y)$ is the classifica-\ntion performance only related with the source model.\n$ABTCE$ measures the conditional distribution discrepancy\nof each class between the the source and each target. In this\nsense, we only need to minimize the $ABTCE$.\nThere-\nfore, the most important objective to improve the quality\nof pseudo labels, which may lead to category misalignment\nand negative transfer effects. In addition, Our Supplemen-\ntary has visualized our theoretical understanding including\nthe values of H\u0394\u0397-distance (measuring domain shift as the"}, {"title": "5.4 Sensitive Analysis", "content": "There are much less hyper-parameters need to tune. We dis-\nplay the performance of different \u03b2 in Eq. (7) in our sup-\nplementary. We set \u03b2 as 1.0 for the whole experiments. In\naddition, we will discuss the annealing weight y introduced\nin Eq. (2) and comparisons between adaptive selection and\nfixed hyper-parameters for ne and nu in our supplementary."}, {"title": "5.5 How ECA performs in CTTA scenario?", "content": "In this paper, we focus on SF-BTDA setting, while it is sur-\nprising that our proposed ECA also achieves the highest ac-\ncuracy for Continual Test-Time Adpaptation (CTTA) sce-\nnario (See our supplementary). In CTTA, the target data is\nprovided in a sequence and from a continually changing en-\nvironment. In both of SF-BTDA and CTTA, the adaptation\nof the target network does not rely on any source data."}, {"title": "6 Conclusions", "content": "In this paper, we are the first to propose a new DA settings\n(i.e., SF-BTDA), which is a more practical and challenging\ntask where we can not access to source domain data while\nfacing mixed multiple target domains without any domain\nlabels in prior. Towards to address this scenario, we propose\nEvidential Contrastive Alignment (ECA) and conduct a new\nbenchmark for SF-BTDA. ECA effectively addressing two\nmajor challenges in SF-BTDA: (1) facing a mixture of mul-\ntiple target domains, the model is assumed to handle the co-\nexistence of different label shifts in different targets with no\nprior information; and (2) as we are only accessed to the\nsource model, precisely alleviating the negative effect from\nnoisy target pseudo labels is difficult because of the signif-\nicantly enriched styles and textures. We conduct extensive\nexperiments on three DA benchmarks, and empirical results\nshow that ECA outperforms state-of-the-art SF-STDA meth-\nods with considerable gains and achieves comparable results\ncompared with those that have domain labels or source data."}]}