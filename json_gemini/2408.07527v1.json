{"title": "Evidential Graph Contrastive Alignment for\nSource-Free Blending-Target Domain Adaptation", "authors": ["Juepeng Zheng", "Yibin Wen", "Jinxiao Zhang", "Runmin Dong", "Haohuan Fu"], "abstract": "In this paper, we firstly tackle a more realistic Domain Adap-\ntation (DA) setting: Source-Free Blending-Target Domain\nAdaptation (SF-BTDA), where we can not access to source\ndomain data while facing mixed multiple target domains\nwithout any domain labels in prior. Compared to existing DA\nscenarios, SF-BTDA generally faces the co-existence of dif-\nferent label shifts in different targets, along with noisy target\npseudo labels generated from the source model. In this pa-\nper, we propose a new method called Evidential Contrastive\nAlignment (ECA) to decouple the blending target domain and\nalleviate the effect from noisy target pseudo labels. First, to\nimprove the quality of pseudo target labels, we propose a cali-\nbrated evidential learning module to iteratively improve both\nthe accuracy and certainty of the resulting model and adap-\ntively generate high-quality pseudo target labels. Second, we\ndesign a graph contrastive learning with the domain distance\nmatrix and confidence-uncertainty criterion, to minimize the\ndistribution gap of samples of a same class in the blended\ntarget domains, which alleviates the co-existence of different\nlabel shifts in blended targets. We conduct a new benchmark\nbased on three standard DA datasets and ECA outperforms\nother methods with considerable gains and achieves compa-\nrable results compared with those that have domain labels or\nsource data in prior.", "sections": [{"title": "1 Introduction", "content": "Despite the great success of deep learning in various fields\n(Krizhevsky, Sutskever, and Hinton 2012; Long, Shelhamer,\nand Darrell 2015; He et al. 2017), it remains a challenge\nto achieve a generalized deep learning model that can per-\nform well on unseen data, especially given the vast diversity\nof real-world data and problems. Performance degradation\noccurs due to various differences between the training data\nand new test data, for aspects such as statistical distribution,\ndimension, context, etc. Domain adaptation (DA) offers sig-\nnificant benefits in this scenario by adapting the pre-trained\nmodels towards new domains with different distributions\nand properties from the original domain (Ben-David et al.\n2010; Cui et al. 2020). Moreover, DA provides great poten-\ntial for efficiency improvement by reducing the need for re-\ntraining on new data from scratch, which has been widely\napplied in various fields and real-world cases."}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Domain Adaptation", "content": "Domain adaptation (DA) aims to minimize domain shift,\nwhich refers to the distribution difference between the\nsource and target domains. Most of them focus on STDA.\nPlenty of single target DA methods have been devised to dif-\nferent tasks including classification (Ganin et al. 2016), se-\nmantic segmentation (Li, Yuan, and Vasconcelos 2019) and\nobject detection (Hsu et al. 2020). To reduce domain dis-\ncrepancies, there are different kinds of methods. One popu-\nlar approach is motivated by generative adversarial networks\n(GAN) (Goodfellow et al. 2014). Adversarial networks are\nwidely explored and try to minimize the domain shift by\nmaximizing the confusion between source and target (Xia,\nZhao, and Ding 2021). Alternatively, it is also common prac-\ntice to adopt statistical measure matching methods to bridge\nthe difference between source and target, including MMD\n(Long et al. 2015), JMMD (Long et al. 2017), and other vari-\nation metrics (Kang et al. 2019). Although extensive works\nhave been done on STDA, there is less research on multi-\ntarget DA, especially for blending-target DA, which is more\nsimilar to the real-world scenarios."}, {"title": "2.2 MTDA and BTDA", "content": "Different from STDA, MTDA is more flexible because the\nmodel can be adapted to multiple target domains with dif-\nferent characteristics, which requires the model to general-\nize better on target data without annotations (Gholami et al.\n2020; Nguyen-Meidine et al. 2021). These requirements in-\ntroduce new challenges, especially the computational re-\nquirements and catastrophic forgetting of previously-learned\ntargets. However, BTDA has received little attention. This\napproach aims to provide solutions for situations where\nthere are no explicit domain labels and samples from differ-\nent target domains are mixed together, which is more con-\nsistent with reality. To the best of our knowledge, there are only few\nworks related to this field. For instance, DADA (Peng et al.\n2019b) aims to tackle domain-agnostic learning by disen-\ntangling the domain-invariant features from both domain-specific and class-irrelevant features simultaneously. CGCT\n(Roy et al. 2021) uses curriculumn learning and co-teaching\nmethods to obtain more reliable pseudo-labels. In addition,\nOpen Compound Domain Adaptation (OCDA) also contains\nblending target domain (Liu et al. 2020; Park et al. 2020;"}, {"title": "2.3 Souce-Free Domain Adaptation", "content": "Because of the limit of privacy constraints, SFDA is be-\ncoming more and more popular, which only provides the\nwell-trained source model without any access to source data\nduring the adaptation process. There are two mainstreams\nfor SFDA methods. On the one hand, some methods fo-\ncus on reconstructing the pseudo source domain samples\nin the feature space (Qiu et al. 2021; Tian et al. 2021) On\nthe other hand, some methods exploit pseudo target labels\nfrom source model and adopt self-training or noisy learning\nso that the model is well-fitted to the target domain (Chen\net al. 2022). For example, SHOT (Liang, Hu, and Feng 2020)\nadopts entropy minimization and information maximization\nthrough pseudo-labeling learning to transfer the knowledge\nfrom trained classifier to target features. Up to date, Most\nExisting SFDA methods focus on SF-STDA setting, while\nonly (Kumar et al. 2023) have discussed and designed new\napproach for SF-MTDA. ConMix (Kumar et al. 2023) intro-\nduces consistency between label preserving augmentations\nand utilizes pseudo label refinement methods to reduce noisy\npseudo labels for both SF-STDA and SF-MTDA settings.\nHowever, existing literature have not exploited a new DA\nsetting named Source-Free Blending-Target Domain Adap-\ntation (SF-BTDA), where we have to transfer the knowledge\nfrom a single source to blending-target domain without do-\nmain labels nor the access of labeled source data. In this\npaper, we propose Evidential Contrastive Alignment (ECA)\nto better address both two major challenges for SF-BTDA."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Problem Setting and Notations", "content": "In contrast to MTDA, BTDA is established on a mixture of\ntarget domains $D_T = \\{D_j\\}_{j=1}^{k} = \\{(x_i, y_i)\\}_{i=1}^{n_T}$, where k is\nthe number of target domains and $n_T = \\sum_{j=1}^{k} n^j$ denotes\nthe total quantity of images in the target domain. Unlike the\nMTDA scenario, the proportions of different target domains\nin the mixed datasets $\\{w_j\\}_{j=1}^{k}$ are unknown. In the SF-\nBTDA setting, we consider the labeled source domain data\nwith $n_s$, the samples as $D_s = \\{(x_i, y_i)\\}_{i=1}^{n_s}$, in which the\n$y_i$ is the corresponding label of $x_i$, is only available during\nmodel pretraining. Our proposed approach is based on any\nbackbone, which we split into three parts: a shallow feature\nextractor $F_1$, a deep feature extractor $F_2$ and a classifier $G$.\nAs we have to transfer the knowledge from a single source\nto blending-target domain without domain labels nor the\naccess of labeled source data, if we directly adopt exist-\ning SF-STDA algorithms and consider the mixed target do-\nmains as one target domain in a brute-force way, the training\nobjective will facilitate domain-invariant representations to\nalign the whole blending-target domain $D_T$ rather than k tar-\nget domains $\\{D_j\\}_{j=1}^{k}$. Because of the discrepancy among\nsub-subjects from k distributions, adaptation for the whole\nblending-target domain directly may lead to severe category\nmisalignment and negative transfer effects."}, {"title": "3.2 Domain Distance Calculation", "content": "As introduced in Sec. 1, SF-BTDA issue faces a mixture\nof multiple target domains, the model is assumed to handle\nthe co-existence of different label shifts in different targets\nwith no prior information (Challenge 1). Therefore, some\nsamples in the same class may appear different styles and\ntextures because of co-existence multiple targets with sig-\nnificantly enriched styles and textures. Under this consid-\neration, we calculate the domain distance to better guide\nthe contrastive learning (see Sec. 3.4) to pull the samples\nin the same class that may appear different styles and tex-\ntures. As we are not access to the specific domain label for\nthe blending-target domain, we separate the target domains\ninto k domains and assign pseudo domain labels for them us-\ning an unsupervised way (i.e., k-means). Notably, we adopt\nthe extracted shallow features from DNNs ($F_1(x^i)$) and in-\ntegrate them with original image textures ($x^i$) to achieve\npseudo domain assignment for better satisfying the specific\nimage style as well as the universal feature style. Notably,\nwe simply concatenate the low-level feature with the origi-\nnal image $x_i$ and we adopt the low-level features that gener-\nated from conv2_x with the dimension of 56\u00d756\u00d7256.\nWe calculate the domain distance $D_{ij}$ between two sam-\nples $x_i$ and $x_j$ through Eq. (1), in which $C_i$ and $C_j$ are the\ncorresponding centroids for $x_i$ and $x_j$. The centroid for an\ntarget image ($x_i$) denotes it belongs to the pseudo domain la-\nbel. The $Max_{Distance}$ denotes we normalize the domain\ndistance among different target domain centroids. If $x_i$ and\n$x_j$ are from the same domain, the domain distance will be\n1.0. In this paper, we utilize the distance of domain cen-\ntroids to represent the samples from different target domains\nfor simplicity. Notably, we update the domain distance after\neach training epoch.\n$D_{ij} = 1 + \\frac{||C_i - C_j||}{Max_{Distance}}$\n(1)"}, {"title": "3.3 Calibrated Evidential Learning", "content": "In the meantime, as we are also not access to the source data,\nwe could only achieve pseudo target labels for the blending-\ntarget images using the model trained from labeled source\nimages. Because of Challenge 2 introduced in Sec. 1, di-\nrectly adopting pseudo labels for the target domain may ruin\nand mislead the model training. To alleviate the negative ef-\nfect from noisy target labels, we propose calibrated eviden-\ntial learning to to iteratively improve both the accuracy and\ncertainty of the resulting model, and adaptively select high-quality pseudo target labels.\nEvidential Deep Learning (EDL) (Amini et al. 2020; Bao,\nYu, and Kong 2021) offers a principled way to formulate\nthe uncertainty estimation, which overcomes the shortcom-\ning of softmax-based DNNs. Assuming that the class prob-\nability follows a prior Dirichlet distribution (Sentz and Fer-\nson 2002), we can acquire the predictive uncertainty $u$ for\neach sample: $u = M/S$, in which M is the number of"}, {"title": "3.4 Graph Contrastive Learning", "content": "The original intention for contrastive learning is to push each\nimage far from others and pull the augmented images closer\nwith the original one in the feature space. However, it is\nnot compatible with the classification task which requires\nto cluster images at the class level, especially that we have\nsome high-quality target samples generated from Sec. 3.3.\nFurthermore, according to the description of Challenge 1,\nsome samples in the same class may appear different styles\nand textures because of co-existence multiple targets with\nsignificantly enriched styles and textures. To this end, we de-\nsign the domain distance calculated from Sec. 3.2 in graph\ncontrastive learning to pay more attention for abovemen-\ntioned samples, which could solve the conflict between con-\ntrastive learning and the classification task.\nSince Sec. 3.3 has generated high-quality labels for the\ntarget domain, we could fully explored their advantages in\nthe contrastive learning. Notably, our contrastive alignment\nmodule aims to learn representations guided by a pseudo-label graph. Here we build the pseudo-label graph by con-\nstructing a similarity matrix (A). To this end, each element\n$a_{ij}$ in A has the following format:\n$A_{ij} =\\begin{cases}\n1 & \\text{if } i = j, \\\\\n1 & \\text{if } z_i \\text{ and } z_j \\text{ are from same category,}\\\\\n &  \\text{and } x_i \\in D_t^e \\text{ and } x_j \\in D_t^e,}\\\\\n0 & \\text{otherwise}\n\\end{cases}$\n(4)\nNotably $x_i \\in D_t^e$ and $x_j \\in D_t^e$ denotes that $x_i$ and $x_j$\nare selected from the high-quality target domain samples.\nAlthough high-quality pseudo label-based supervised con-\ntrastive learning is theoretically functional, directly using\nthese pseudo labels may be problematic because they might\nbe noisy labels, too. To alliviate this problem, we adopt the\nconfidence-uncertainty evaluation produced from Sec. 3.3 to\nstrengthen the robustness and reduce the negative effect on\nnoisy pseudo labels. Actually, the pseudo-label graph (A)\nserves as the target to train an embedding graph ($A'$), which\nis defined as:\n$a_{ij}' =\\begin{cases}\n\\alpha_{ij} * I_{ij} * D_{ij} & \\text{if } i \\neq j, \\\\\n\\alpha_{ij} & \\text{otherwise}\n\\end{cases}$\n(5)\nin which $I_{ij} = c_i * c_j * (1 - U_i) * (1 - u_j)$ denotes\nthat samples with high confidence and low uncertainty will\nhave higher weights in our evidential contrastive alignment\nmodule. $D_{ij}$ can be calculated from Eq. (1), which could\nmake our contrastive learning module to pay more attention\non the samples of a same class that belonged to different\ntarget domains (addressing Challenge 1). To this end, the\nloss of $L_{CON}$ can be formulated as:\n$L_{CON} = \\frac{1}{| \\sum_{i=1}^{|\\sum_{i=1}^{ }|} \\{\\log {(\\frac{\\exp {(\\frac{z_i.z_{j(i)}}{\\tau})}}{1 + \\sum_{q \\in Q(i)} \\exp {(\\frac{z_i.z_{q}}{\\tau})}} + {\\sum_{p \\in P(i)} \\alpha_{ip} \\exp {(\\frac{z_i.z_{p(i)}}{\\tau})}}\\}}$,\n(6)"}, {"title": "3.5 Overall Objective", "content": "The model is jointly optimized with two loss terms, includ-\ning calibrated evidential learning loss $L_{CEL}$ and graph con-\ntrastive learning loss $L_{CON}$:\n$L = L_{CEL} + \\beta L_{CON}$\n(7)\nwhere $\u03b2$ is a trade-off parameter balancing two different\nloss terms between $L_{CEL}$, and $L_{CON}$"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "We conduct experiments on three standard domain adapta-\ntion benchmarks. Office-31 (Saenko et al. 2010) consists of\n4", "domains": "DSLR (D), Amazon (A)"}]}