{"title": "Quantum Kernel Principal Components Analysis for Compact Readout of Chemiresistive Sensor Arrays", "authors": ["Zeheng Wang", "Timothy van der Laan", "Muhammad Usman"], "abstract": "The rapid growth of Internet of Things (IoT) devices necessitates efficient data compression techniques to handle\nthe vast amounts of data generated by these devices. In this context, chemiresistive sensor arrays (CSAs), a simple-\nto-fabricate but crucial component in loT systems, generate large volumes of data due to their simultaneous\nmulti-sensor operations. Classical principal component analysis (cPCA) methods, a common solution to the data\ncompression challenge, face limitations in preserving critical information during dimensionality reduction. In this\nstudy, we present quantum principal component analysis (qPCA) as a superior alternative to enhance information\nretention. Our findings demonstrate that qPCA outperforms cPCA in various back-end machine-learning\nmodeling tasks, particularly in low-dimensional scenarios when limited Quantum bits (qubits) can be accessed.\nThese results underscore the potential of noisy intermediate-scale quantum (NISQ) computers, despite current\nqubit limitations, to revolutionize data processing in real-world IoT applications, particularly in enhancing the\nefficiency and reliability of CSA data compression and readout.", "sections": [{"title": "Introduction", "content": "Quantum computing (QC) is increasingly recognized\nas a pivotal solution for computationally intensive\nproblems such as integer factorization\u00b9\u00b2 and quantum\nsystem simulations\u00b3\u201c. Likewise, it is anticipated that the\nintegration of quantum computing in machine\nlearning (ML) and data processing tasks will offer\ncomputational advantages such as speed-up,\nenhanced accuracy, and superior robustness\u201d, made\npossible by unique quantum properties like\nsuperposition and entanglement. These properties\nallow data to be stored and processed in a potentially\nhigh-dimensional quantum space, leading to\ncomputation optimization. Quantum machine\nlearning (QML) has already demonstrated high\npotential in various applications. For instance, a hybrid\nclassical-quantum principal component analysis (PCA)\nhas been applied in drug design\u00b0, while other studies\nhave explored quantum computing applications in\nfinance and enhanced classical machine learning10, 11.\nAdditionally, it has been theoretically predicted that\nquantum kernel methods could consistently\noutperform classical counterparts in modeling data\nwith group structure.\nHowever, experimental research leveraging QML to\nsolve real-world problems by identifying group\nstructures in datasets has not yet been extensively\ncarried out, leaving this theoretical work largely\nunvalidated. Simultaneously, in noisy intermediate-\nscale quantum (NISQ) systems, practical applications\nof QML are becoming increasingly feasible12-15. Despite\nrecent advancements in high-density QC architectures,\nthe number of logical qubits in QC processors still falls\nshort of demonstrating a significant quantum\nadvantage 16-19. This gap presents a unique opportunity\nto apply QML for practical problem-solving,\nparticularly in enhancing data compression and\nreadout compactness, when the number of qubits is a\ncritical limitation. To address these challenges, this\nstudy employs a quantum kernel-based algorithm,\nquantum PCA (qPCA), to compress data and enhance\nbackend data processing and therefore bridges the\ngap by demonstrating the practical utility of qPCA in\nIoT data compression, a real-world application.\nThe experimental data used in this work was obtained\nfrom chemiresistive sensor arrays (CSAs), an example\nof a widely used loT device due to their fabrication"}, {"title": "Quantum and classical kernels", "content": "Fig. 2 presents a side-by-side comparison of the\nquantum, Fig. 2(a), and classical, Fig. 2(b), kernel\nmatrices. The quantum kernel matrix, characterized by\na scattered and irregular pattern of darker blocks,\nindicates a high degree of similarity between certain\ngroups of data points. This suggests the quantum\nkernel can capture complex, potentially non-linear\nrelationships within the data in the quantum feature\nspace. In contrast, the lighter, white areas indicate\nnear-zero similarity, implying independence or\nirrelevance between the data points in these regions.\nThe classical kernel matrix displays a more uniform\nand regular pattern, with dark stripes signifying\ndimensions where data points share higher similarity.\nThis pattern may suggest that relationships in the\nclassical feature space are simpler, and that the\nclassical kernel emphasizes certain dominant features\nof the data. The stark differences between the two\nmatrices illustrate how various kernel functions\ncapture the intrinsic structure of the data. The\nquantum kernel may be sensitive to more subtle or\ncomplex relationships, whereas the classical kernel\nmight highlight broader, more significant features25."}, {"title": "Quantum PCA-based data compression", "content": "Utilizing the quantum kernel the 7D data can be\ncompressed into 6/5/4/3D space. Fig. 2, the 2D\nvisualization (t-SNE embedding) of data compressed\nby qPCA (c) and CPCA (d) allows for another\ncomparison of the two methods. The qPCA projection\nshows a dispersed cluster of points, possibly reflecting\nthe nuanced relationships that the quantum kernel\ncaptures. Meanwhile, the cPCA projection reveals a\nmore defined curve, indicating that the classical kernel\nmay be distilling the data into a form that reflects its\nmain features with clear separation. However, this\nmeans that the data losses its structural information\nduring the dimensional reduction. It should be noted\nthat, however, the ultimate choice of kernel for a given\ntask should not rely solely on visual inspection; it must\nbe informed by empirical performance metrics, such\nas classification accuracy, regression error, or other\nrelevant benchmarks in machine learning models."}, {"title": "Classical machine learning readout", "content": "To delve into the nuances of how qPCA and CPCA\ninfluence information retention during dimensionality\nreduction. We applied five different ML algorithms to\npredict the chemical types that the CRS detected in\ndifferent samples. Each algorithm serves as a probe\ninto the kernels' efficacy. As delineated in Fig. 3, data\nrefined through qPCA were more amenable to\nmodeling by these algorithms, evidenced by their\ngenerally higher evaluation scores. Nonetheless, it is\nimperative to acknowledge the conditions under\nwhich cPCA-treated data surpassed qPCA's\nperformance. A notable instance is observed with\ndimensions above 5D for the RF algorithm, where\nCPCA maintained higher scores. This suggests that\nCPCA can preserve certain intricate non-linear\npatterns within the original dataset, which RF can\nexploit to garner additional insights. This preservation\nof complex structures, however, seems to diminish\nrapidly with further dimensional reduction, as\ndemonstrated by the comparative analysis within the\nRF subfigures in Fig. 3.\nExamining the lower-dimensional performance of\nlinear-based algorithms, such as LC and L-SVM, we\nalso see CPCA perform better than qPCA, as shown in\nFig. 3 (p), (q), (u), and (v). Despite cPCA's use of the\ninherently non-linear RBF kernel, this phenomenon\nindicates preferential retention of linear-like patterns"}, {"title": "Ensemble methods", "content": "However it is crucial to recognize that these linear\npatterns form only a fragment of the overall\ninformation necessary for robust modeling our CSA"}, {"title": "Benchmarking qPCA and cPCA", "content": "The results portrayed in Fig. 4 reinforce the assertion\nthat qPCA outperforms CPCA in compressing the data\nfrom the CSA-based loT system studied. These\nframeworks synergistically combine the predictive\npower of all five ML algorithms used, as shown in the\nupper panel of the figure. The analysis in Fig. 4(a)-(f)\nreveals that the qPCA-based models have a more\ngradual decline in performance scores relative to CPCA,\nas dimensionality decreases. This trend is consistent\nacross both voting and stacking methods. This\nreinforces the robustness of qPCA in preserving\nessential information during the data compression\nprocess.\nThe sustained performance of qPCA in lower-\ndimensional spaces is indicative of its ability to\nmaintain the integrity of multidimensional\nrelationships, a quality that is vital for the complex task\nof loT computation. Ensemble models built upon\nqPCA consistently outscore their cPCA counterparts,\nas shown in Fig. 4(g)-(h). No negative scores were\nfound when comparing qPCA to cPCA in lower\ndimensions (< 6D, mean values), in either the stacking\nor voting ensembles. We do note however that this\nclaim is within the margin of statistical uncertainty in\nall scenarios. This improvement in data quality\nretention as dimensionality is reduced is also seen in\nthe rate at which the scores for different ensembles\ndecrease with dimensionality reduction (the slope of\nthe scores vs the dimensions) as shown in Fig. 4(i)-(j).\nConsistently we see that the score's change rate using\nqPCA are lower than the cPCA scores."}, {"title": "Discussion", "content": "Recent advancements in quantum machine learning\ndemonstrate the efficacy of quantum kernels in\nclassifying data with inherent group structures. Using\nour experimental dataset, this study demonstrates the\neffectiveness of quantum kernel methods and their\npotential for real-world applications. We provide\nevidence that quantum kernel-based PCA offers\npromising solutions for complex data compression\nproblems with non-obvious relationships. Particularly\nfor processing from high to low dimensions, the\nproposed qPCA method shows promise, as applying\nquantum data processing using qubits has become\nfeasible in the NISQ era.\nWhile this study highlights qPCA's potential\nadvantages over CPCA in our framework, it is yet to be\ndetermined whether qPCA universally outperforms all\nkernel-based cPCA methods. A carefully crafted kernel\nor tailored post-processing algorithm could enhance\nCPCA's ability to preserve critical information during\ndimensionality reduction. Moreover, implementing\nCPCA is straightforward with established algorithms\nand software support, allowing easy application with\nconventional computing resources. In contrast, qPCA\nrequires higher computational demands and\nspecialized quantum computing resources, reflecting\nits developmental stage."}, {"title": "Summary and outlook", "content": "In conclusion, the findings of this study underscore the\nadvantage of qPCA over cPCA for data compression\nin Internet of Things (IoT) applications. This work\nprovides an experimental demonstration of the\nprevious theoretical prediction of a quantum\napproach to dimensionality reduction in datasets.\nUsing only limited qubits, our experimental results\ndemonstrate that qPCA appears to maintain key\ninformation through a dimensionality reduction\nprocess more effectively than cPCA, this is evidenced\nby consistently higher performance scores across a\nvariety of machine learning (ML) models when using\nthe compressed data. This is further emphasized when\nensemble learning models are integrated within\nensemble learning frameworks such as soft voting and\nstacking.\nThe propensity of qPCA to retain more informative\nfeatures endorses its potential as the preferred"}, {"title": "Methods", "content": "Experimental data: In this study, we sourced our\nsensing data from the dataset detailed in Ref\u00b2\u2076. This\ndataset comprises readings from 17 sensors across\n852 experiments (data shape: 852\u00d717), with each 17-\ndimensional data point linked to one of 66 labels\nidentifying the detected chemical. Due to the qubit\nlimitations inherent in NISQ systems, we initially\nnarrowed our focus to data from seven specific\nsensors (4-BBM, 1-2-BDMT, \u041c\u041e\u0412, 3-\u0415\u0422\u0420, 4-\u041c\u0412\u0422, 4-\nCBT), resulting in a refined dataset of 852 experiments,\neach with 7-dimensional readings (selected data\nshape: 852\u00d77). Each data item consisted of these 7D\nreadings paired with a label. The whole procedure of\nthe data processing and analysis can be found in Fig.\n5. The experimental setup for collecting the CSA data\nis briefly illustrated in the left panel of Fig. 1 and more\ndetails about the sensor's fabrication and the wet\nexperiments can be found in our previous work\u00b2\u2076.\nFor data compression, we employed kernel PCA\nutilizing two distinct kernel functions: the Quantum\nFidelity Kernel (QFK) and the Radio-Basis Function\nKernel (RBFK)."}, {"title": "Quantum PCA", "content": "Quantum PCA: Kernel PCA transforms the data into a\nhigher dimensional space defined by a kernel function\nK, where K(xi, xj) represents the similarity between\ntwo points in the original space. The transformation is"}, {"title": "Classical PCA", "content": "Classical PCA: For RBFK PCA, the kernel is defined by\nthe Gaussian function:\n$K(x_i, x_j) = exp(-\\frac{||x_i - x_j||^2}{2\\sigma^2})$\n||xi - xj || is the Euclidean distance between data\npoints xi and xj, and \u03c3 is a free parameter that\ncontrols the width of the Gaussian."}, {"title": "Applying these kernel methods", "content": "Applying these kernel methods, we performed\ndimensionality reduction on the 7D data, mapping it\ninto a feature space via a kernel function K. In this\nspace, we computed the covariance matrix C defined\nby the kernel as:\n$C = \\frac{1}{n}\\sum_{i=1}^{n} (K \\cdot \\Phi(x_i)) (K \\cdot \\Phi(x_i))^T$\nwhere n is the number of data samples, \u03a6(xi) is the\nimplicit mapping of the data point xi by the kernel\nfunction K, and T denotes the transpose operation.\nWe then solved the eigenvalue problem:\n$C \\cdot v = \\lambda \\cdot v$\nwhere v are the eigenvectors and \u03bb are the\neigenvalues. The eigenvectors corresponding to the\nlargest eigenvalues give us the principal components\nin the feature space. By selecting the top k\neigenvectors, we can form a reduced feature space of\ndimension k, where k is less than the original\ndimensionality of the data. For our purposes, we chose\nk = 6, 5, 4, and 3 to obtain a 6D, 5D, 4D, and 3D\nrepresentation of the data, respectively. In this\nreduced feature space, the transformed data points are given by:\n$x_i = [v_1 \\cdot \\varphi(x_i), v_2 \\cdot \\varphi(x_i), ..., v_k \\cdot \\varphi(x_i)]$\nThis transformation is expected to retain the essential\nfeatures of the original data and to be suitable for\nprocessing within a NISQ environment due to the\nlower dimensionality. It is now evident that different\nmapping strategies, i.e. different K and \u03c6(xi), will\nlead to different transformed data points \u1fd6."}, {"title": "Machine Learning Models", "content": "Machine Learning Models: Following the\ndimensional reduction, we evaluated the information\nretention in the compressed data using five Classical\nMachine Learning (CML) algorithms, including linear"}]}