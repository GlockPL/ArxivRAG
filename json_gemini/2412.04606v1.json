{"title": "Semantic Consistency-Based Uncertainty Quantification for\nFactuality in Radiology Report Generation", "authors": ["Chenyu Wang", "Weichao Zhou", "Shantanu Ghosh", "Kayhan Batmanghelich", "Wenchao Li"], "abstract": "Radiology report generation (RRG) has shown\ngreat potential in assisting radiologists by au-\ntomating the labor-intensive task of report writ-\ning. While recent advancements have improved\nthe quality and coherence of generated reports,\nensuring their factual correctness remains a crit-\nical challenge. Although generative medical\nVision Large Language Models (VLLMs) have\nbeen proposed to address this issue, these mod-\nels are prone to hallucinations and can produce\ninaccurate diagnostic information. To address\nthese concerns, we introduce a novel Semantic\nConsistency-Based Uncertainty Quantification\nframework that provides both report-level and\nsentence-level uncertainties. Unlike existing\napproaches, our method does not require mod-\nifications to the underlying model or access\nto its inner state, such as output token logits,\nthus serving as a plug-and-play module that can\nbe seamlessly integrated with state-of-the-art\nmodels. Extensive experiments demonstrate\nthe efficacy of our method in detecting halluci-\nnations and enhancing the factual accuracy of\nautomatically generated radiology reports. By\nabstaining from high-uncertainty reports, our\napproach improves factuality scores by 10%,\nachieved by rejecting 20% of reports using the\nRadialog model on the MIMIC-CXR dataset.\nFurthermore, sentence-level uncertainty flags\nthe lowest-precision sentence in each report\nwith an 82.9% success rate.", "sections": [{"title": "1 Introduction", "content": "RRG is gaining importance as healthcare demands\ngrow, placing substantial pressure on radiolo-\ngists to interpret medical images swiftly and ac-\ncurately. Automating the report-writing process\nholds the potential to alleviate this burden, improv-\ning both efficiency and diagnostic precision. Vi-\nsion Large Language Models (VLLMs) have in-\ntroduced new possibilities in this area by gener-\nating detailed and coherent reports from medical\nimages, providing significant assistance to radiolo-\ngists (Thawkar et al., 2023; Pellegrini et al., 2023).\nHowever, despite these advancements, challenges\npersist-particularly in ensuring the factual accu-\nracy of these generated reports. A notable issue\nwith VLLMs is their tendency to produce \"halluci-\nnations\", or information that is ungrounded in the\nvisual data or inconsistent with established medi-\ncal knowledge. For example, a model might in-\ncorrectly generate findings such as a diagnosis of pneu-\nmonia when none is present (Hartsock and Rasool,\n2024), or fabricate prior medical history that does\nnot exist (Ramesh et al., 2022; Tanida et al., 2023;\nHyland et al., 2023). Such hallucinations can lead\nto inaccurate or misleading diagnostic information,\nposing significant risks in clinical settings.\nRecent studies have explored various methods\nto address hallucinations in radiology report gener-\nation. Ramesh et al. (2022) utilize a GPT-3-based\nrewriting technique and a BioBERT-based token\nclassification system to remove references to non-\nexistent prior reports. Banerjee et al. (2024) em-\nploy Direct Preference Optimization (DPO) to sup-\npress hallucinated prior exams, significantly reduc-\ning such errors while maintaining clinical accuracy.\nHowever, these methods remain limited in scope,\nfocusing solely on specific hallucinations, namely\nhallucinated prior exams, and do not enhance the\nbroader factual accuracy of diverse clinical entities\ncritical for dependable diagnostics. Bannur et al.\n(2023, 2024) tackle hallucinations by integrating\ncurrent and prior images with detailed report sec-\ntions, thereby improving the alignment between\ngenerated text and visual data to reduce errors and\nenhance report consistency. These approaches of-\nfer a more comprehensive solution than methods\ntargeting specific hallucination types. However,\nthey rely on specialized architectures and addi-\ntional training resources, limiting their flexibility\nand applicability across diverse models.\nAddressing the limitations of prior approaches,"}, {"title": "2 Preliminaries", "content": "Addressing the limitations of prior approaches,\nour framework provides a plug-and-play solution\nthat mitigates hallucinations through uncertainty\nquantification (UQ), requiring no architectural\nmodifications or additional training. Broadly com-\npatible with diverse VLLM-based RRG models,\nit emphasizes semantic consistency between gen-\nerated content and sampled counterparts. Specifi-\ncally, our UQ framework assesses the consistency\nof clinical entities within generated reports, assign-\ning high uncertainty to content with low factual\nprecision. We measure this consistency by com-\nparing clinical facts from the original report with\nthose in multiple sampled reports generated from\nthe same query, relying solely on API-level ac-\ncess to broaden applicability. By abstaining from\nhigh-uncertainty reports, we enhance the clinical\nefficacy of generated outputs. Additionally, by flag-\nging high-uncertainty sentences, we guide radiolo-\ngists to areas needing further validation, reducing\ntheir workload and supporting more accurate in-\nterventions. In summary, our contributions are as\nfollows:\n(1) We propose an UQ framework operating as a\nplug-and-play module, requiring no insights\ninto the model's internal mechanisms, and is\neasily integrated with state-of-the-art RRG sys-\ntems.\n(2) We propose two domain-specific uncer-\ntainty quantification methods for report- and\nsentence-level analysis to identify clinical con-\ntent with low semantic consistency, improving\nthe factual accuracy of the generated report.\n(3) Our framework improves factuality by abstain-\ning from high-uncertainty reports, achieving\na 10% boost in factuality scores by rejecting\n20% of reports using the Radialog model. Ad-\nditionally, it flags sentences with the highest\nuncertainty, accurately identifying those with\nthe lowest factual precision at 82.9%.\n(4) We evaluate our framework's effectiveness in\ndetecting non-existent prior exams and investi-\ngate its alignment with factuality across various\npathology subgroups."}, {"title": "2.1 RRG with VLLMS", "content": "In RRG using VLLMs, the input is a medical image\nx \u2208 RD, where D is the dimension of the image,\nand the output is a generated report \u00ee \u2208 V*, with\nV* represents the space of token sequences. To\nproduce this report, the model processes the image"}, {"title": "2.2 Rank Calibration", "content": "B\nRank Calibration (Huang et al., 2024) is designed\nto evaluate the alignment between the uncertainty\nlevels of an LM's predictions and their actual\n(in)correctness. An uncertainty measure is consid-\nered rank-calibrated if predictions with higher un-\ncertainty are more likely to be incorrect. Given N\npredictions by the LM, each associated with an un-\ncertainty score ui for i = 1,2,..., N, these scores\nare evenly partitioned into B intervals {Ib}=1,\nsuch that each interval contains approximately\nN/B scores. Using a regression function reg to\nmap the uncertainty score u from any interval Ib\nto the accuracy of predictions in that interval, the\nEmpirical Rank-Calibration Error (RCE) assesses\nthe alignment between uncertainty and accuracy\nas below. Lower Empirical RCE values indicate\nbetter calibration.\nRCE = $\\frac{1}{B}\\sum_{b=1}^{B}\\left|\\frac{1}{\\left|I_{b}\\right|} \\sum_{u^{\\prime} \\in I_{b}^{\\prime}}\\left[\\sum_{I_{b^{\\prime}} \\neq I_{b}} \\mathbb{I}\\left[r e g\\left(u^{\\prime}\\right) \\geq \\sum_{u \\in I_{b}} r e g(u)\\right]\\right]-\\frac{1}{B-1}\\right|$\" (1)\""}, {"title": "2.3 VRO (Variation Ratio for Original\nPrediction)", "content": "The VRO metric (Huang et al., 2023b) measures\nuncertainty by comparing the model's original pre-\ndiction with the predictions generated from multi-"}, {"title": "2.5 Natural Language Inference based\nUncertainty Quantification", "content": "A Natural Language Inference (NLI) model takes\na pair of sentences (a premise and a hypothesis) as\ninput and outputs logits for the labels entailment,\ncontradiction, or neutral\u2014indicating the likelihood\nof each relationship. Kuhn et al. (2023); Lin et al.\n(2023) leverage these pairwise similarity scores\nto assess the consistency between response pairs\nand use them for subsequent uncertainty estima-\ntion. Zhang et al. (2024a) use an off-the-shelf\nDeBERTa-v3-large model (He et al., 2021) to\ncompute NLI-based uncertainty for each sentence\nsj in a response. They calculate the probability of\n\"entailment\" by normalizing the entailment logit le\nover the sum of entailment and contradiction logits:\nP(entail | sj,r') = $\\frac{exp(l_e)}{exp(l_e) + exp(l_c)}$ (3)\nIn this way, sentence-report similarity can be cal-\nculated to enable UQ."}, {"title": "3 Method", "content": "In RRG, given an LLM M, we useri = Mt(xi)\nto denote t-th sampled report given a Chest X-\nray image xi in contrast to ri = M(xi) as orig-\ninal report. An uncertainty measure is defined as\nUM : V* \u00d7 2V* \u2192 R, takes the original report and\na set of sampled reports as input and outputs a real\nvalue representing the uncertainty. Our core princi-\nple is that higher uncertainty should correspond to\nlower quality in generated outputs. Therefore, we\nfollow the approach as described in Section 2.2 to\nevaluate the alignment between UM and the pre-\ndiction correctness indicated by a clinical metric F\nvia Equation 1 by defining the regression function\nas reg(u) = E[F|UM = u]. However, the long-\nform nature of RRG poses the following challenges\nin designing the uncertainty measurement U\u00b9.\n(a) High Similarity Across Responses: long texts\noften yield high similarity across response\npairs (Zhang et al., 2024a), limiting UQ meth-\nods based on response-level similarity (Kuhn\net al., 2023; Lin et al., 2023). Applying sim-\nilarity at the component level requires extra\neffort to align corresponding parts, as sampled\nresponses may reorder or omit claims.\n(b) Lack of Domain-Specific NLI Models:\nZhang et al. (2024a) propose using NLI models\nfor nuanced similarity assessments; however,\nRRG lacks a specialized NLI model. Gen-\neral NLI models often struggle with the do-\nmain's subtle distinctions, causing error prop-\nagation. While Bannur et al. (2024) lever-\nage the in-context learning abilities of GPT-\n4 and Llama3-70B for entailment verifica-\ntion-potentially making them viable as UQ\nmethods in RRG\u2014these models are imprac-\ntical for real-time UQ due to high computa-\ntional demands. See further discussion in Ap-\npendix B.\n(c) Limitations of Self-Evaluation-Based UQ:\nSelf-evaluation UQ methods (Kadavath et al.,\n2022; Lin et al., 2023) attempt to verbalize\nconfidence through handcrafted prompts, en-\nabling models to express uncertainty in natural\nlanguage. However, this approach is currently\nunavailable for VLLM-based RRG models(Gui\net al., 2024), with failure cases demonstrated\nin the Appendix B.\nTo overcome these challenges, we propose to\nquantify uncertainty by evaluating semantic sim-\nilarity between paired reports with clinical met-\nric F. By focusing on semantic consistency, our\nmethod more effectively captures semantic equiva-\nlence, leading to improved uncertainty estimation.\nIn addition, we apply VRO (Huang et al., 2023b),\nwhich calculates the similarity between the original\nand sampled predictions, to enhance computational\nefficiency. In contrast to previous methods (Zhang\net al., 2024a; Kuhn et al., 2023) that require O(n\u00b2)\ncalls to NLI models for pairwise comparisons, our\napproach reduces this complexity to O(n) calls for\nconsistency measurement while maintaining good\nperformance in UQ with different granularity. In\nSection 3.1 we will provide details on report-level\nUQ, while Section 3.2 details sentence-level UQ."}, {"title": "3.1 Report-Level Uncertainty Quantification", "content": "Our report-level uncertainty quantification lever-\nages the approach in Equation 2, where we use a\nfactual metric in RRG as the distance function. In\nthis setup, ri is treated as the original prediction,\nand {r}tT1 represents the t-th sampled prediction. The\nuncertainty is computed as:\nURepoVrtO (\u00eei, {r}tT1) =  1T \u03a3Tt=1(1-F(\u00eei, r)) (4)\nWe leverage GREEN (Ostmeier et al., 2024), a\nstate-of-the-art evaluation metric that aligns with\nradiologist preferences, to implement F. GREEN\ncalculates factual alignment by comparing findings\nand error counts between reports. Here, the origi-\nnal report serves as the prediction, and the sampled\nreports are references, effectively capturing seman-\ntic equivalence between the original generated and\nsampled reports. Further details on GREEN are in\nthe Appendix A."}, {"title": "3.2 Sentence-Level Uncertainty\nQuantification", "content": "While report-level uncertainty quantification is use-\nful, it can obscure variations in certainty across\nmultiple facts within a report, making sentence-\nlevel quantification more appropriate. Zhang et al.\n(2024a) calculates sentence-to-report entailment\nscores across all sampled reports increases classi-\nfier complexity and computational demands, mak-\ning it inefficient for real-world deployment. To\novercome these challenges, we propose a novel\nmethod leveraging the RadGraph (Jain et al.,\n2021) parser. Assume that each report, ri =\n{Si1, Si2, Si3...Siki}, consists of multiple sentences,\nwhere ki indicates the number of sentences within\nthe report. We utilize the RadGraph parser, denoted\nas g : V* \u2192 V, which map sequence(s) to the\nset of node-label pairs V = {(vk, Uk)}k\u2208[1..|V|]\nwhere each pair represents an entity and its associ-\nated label. An entity vk is a continuous text span\n(potentially multi-word) that represents either an\nAnatomy or an Observation. The label url for each\nentity vk indicates one of the four possible entity"}, {"title": "4 Experiments", "content": "categories describe in Section 2.4. Using this struc-\ntured output, we calculate an uncertainty value for\neach sentence sij, where sij is the j-th sentence in\nthe generated report ri.\nUsentence (sij, {r}tT1) =  1T \u03a3Tt=1(1\u2212 |g(s) \u2229g(ri))|Vij|\n\u03a3Tt=1Vij\n(5)\nwhere Vij represents the set of node-label pairs\nin the original sentences, and V is the set of node-\nlabel pairs in the sampled report r. The term |Vijn\nV| denotes the number of entity-label pairs from\nthe original sentence Vij that are also present in V.\nAdditionally, Vij | represents the total number of\nnode-label pairs in the original sentences, which\nbounds the uncertainty value between 0 and 1.\nIn this section, we aim to answer the following\nresearch questions: RQ1. How well does our pro-\nposed UQ align with the factual correctness of the\ngenerated reports? RQ2. Can our UQ enhance the\nradiologist's intervention process to improve the\nfactual accuracy of generated reports? RQ3. Can\nour UQ detect content referring to non-existent\nprior information?"}, {"title": "4.1 Setup", "content": "Datasets. Following previous works, we conduct\nour experiments on MIMIC-XCR (Johnson et al.,\n2019). We follow the original train-val-test splits.\nModels. We use RaDialog (Pellegrini et al., 2023)\nas the base model for our uncertainty quantifica-\ntion experiments. This model was selected due to\nits clean architecture, strong performance on the\nReXRank (Zhang et al., 2024c) online benchmark,\nand ease of reproducibility without data restrictions.\nTo further validate our approach, we also apply\nour method to CheXpertPlus_mimiccxr (Cham-\nbon et al., 2024), a top-performing model on the\nMIMIC-CXR benchmark. For this model, we as-\nsume only API access to demonstrate the flexibility,\nplug-and-play nature, and generalizability of our\nproposed uncertainty quantification framework to\ndifferent vision-language model-based radiology\nreport generation systems."}, {"title": "4.2 RRG Evaluation", "content": "We evaluate our RRG models using four metric\ncategories from the ReXRank benchmark (Zhang\net al., 2024c), supplemented by the state-of-the-art\nGREEN evaluation. We use lexical metrics such\nas BLEU and embedding-based BERTScore to as-\nsess token-level and semantic similarity. To evalu-\nate pathological and entity-based consistency, we\napply factuality metrics, including Semb Score \nand RadGraph Precision, Recall, and F1.\nWe further assess clinical accuracy with RadCliQ\nwhich combines RadGraph F1 and BLEU scores,\nand the GPT-based evaluator GREEN, which eval-\nuates clinical accuracy by matching findings and\ncounting errors between generated and reference\nreports. For detailed descriptions of each metric,\nsee Appendix C."}, {"title": "4.3 UQ Evaluation", "content": "In this section, we show how UQ can be evaluated\nin radiology report generation. In contrast to typi-\ncal question-answering tasks where the correctness\nof a model's prediction is binary, radiology report\ngeneration typically involves long-form generation\nwhich requires more nuanced evaluation methods\nfor UQ.\nPearson correlation coefficient. The Pearson cor-\nrelation coefficient can be used to assess how well\nuncertainty quantification aligns with the factual\ncorrectness of generated reports. By measuring\nthe linear relationship between model uncertainty\nand report quality, Pearson's coefficient provides\ninsight into whether higher uncertainty corresponds\nto lower factual accuracy. The Pearson correlation\nranges between -1 and 1, where a negative value\nindicates an inverse relationship. In our setting, we\nuse Pearson's coefficient to evaluate this relation-\nship, with a strong negative correlation suggesting\nthat higher uncertainty signals lower report cor-\nrectness, aligning with the intended behavior of\nuncertainty quantification.\nRank calibration error. RCE assesses the consis-\ntency in ranking, ensuring higher uncertainty corre-\nsponds to lower correctness, regardless of a linear\nrelationship. We use the Empirical RCE (Huang\net al., 2024), which divides uncertainty values into\nB = 20 bins. For each bin, we calculate the ex-\npected correctness level and average uncertainty.\nThe Empirical RCE is computed by averaging the\nrank differences between correctness and uncer-\ntainty across these bins as Equation 1, offering a"}, {"title": "4.4 Hallucination Detection", "content": "principled approach to measure the alignment be-\ntween uncertainty and correctness without relying\non arbitrary thresholds.\nAbstention. Abstention allows uncertainty quan-\ntification to enhance factual accuracy by rejecting\nhigh-uncertainty reports, directing radiologists to\nfocus on certain content. Traditionally, abstention\nis measured by metrics like AUARC (Huang et al.,\n2024), which evaluates improvement by abstaining\nfrom uncertain cases. However, binary metrics like\nAUARC are inadequate for the nuanced nature of\nRRG. To address this, we evaluate abstention at the\nreport level, measuring improvements in factuality\nscores while balancing the trade-off with coverage.\nThis strategy enables targeted intervention by ra-\ndiologists, focusing their review on areas where\nfactual accuracy may be compromised.\nUncertainty Precision Alignment. To evaluate\nsentence-level UQ in RRG, we calculate a factual\nprecision score for each generated sentence using\nRadGraph (details in Appendix C). We then assess\nhow well high uncertainty scores correspond to\nsentences with low factual precision within each\nreport. Specifically, we measure the alignment rate\nbetween the sentence with the highest uncertainty\nand the sentence with the lowest factual precision.\nThis alignment metric supports targeted interven-\ntions, enabling radiologists to focus on sentences\nthat may require closer review due to potential fac-\ntual inaccuracies.\nIn RRG, references to prior exams are a common\nform of hallucination (Banerjee et al., 2024). In\nthis section, We empirically investigate whether\nour UQ can effectively detect and flag these hallu-\ncinations by assigning them high uncertainty. Fol-\nlowing Banerjee et al. (2024), we define 43 sub-\nstrings commonly associated with references to\nprior exams. For report-level uncertainty, we ana-\nlyze the changes in the percentage of reports with\nprior exam references and the average number of\nhallucinated substrings per report before and after\napplying different levels of abstention."}, {"title": "4.5 UQ Baselines", "content": "We compare our method with the previous uncer-\ntainty quantification method. Following Kuhn\net al. (2023), we use predictive entropy, length-\nnormalised predictive entropy (Malinin and Gales,\n2020) and lexical similarity (Zhang et al., 2024a;\nFomicheva et al., 2020). We do not compare"}, {"title": "5 Results", "content": "with methods involving NLI classifiers and self-\nevaluation-based UQ due to their unavailability in\nRRG, as discussed in Appendix B. For all experi-\nments, we use the default temperature value 1 and\nsample 10 responses to calculate UQ."}, {"title": "5.1 Alignment with Factuality (RQ1)", "content": "Table 1 demonstrates that our proposed\nVRO-GREEN exhibits stronger negative\nPearson correlations with factuality metrics\nacross both the Radialog Model and the\nCheXpertPlus_mimiccxr Model when com-\npared to baseline UQ methods. In particular,\nVRO-GREEN achieves high negative corre-\nlations on GREEN (-0.5292 for Radialog,\n-0.4726 for CheXpertPlus_mimiccxr) and\nRadCliQ-v0 (-0.4137 for Radialog, -0.3743\nfor CheXpertPlus_mimiccxr). This indicates\nVRO-GREEN's superior capability in aligning\nuncertainty with factual correctness in radiology\nreport generation. Furthermore, we grouped sam-\nples based on the presence of specific pathology\nfindings to examine the correlation between UQ\nand GREEN for each subgroup for the Radialog\nModel. Subgroup analysis in Table 2 reveals\nvariation in correlation strength, particularly in the\nPneumothorax subgroup, where the correlation\nis notably weaker at -0.08, likely due to the\nunderrepresentation of Pneumothorax cases\n(around 1% of positive cases in the training set).\nTable 3 further validates VRO-GREEN's\nalignment effectiveness using Empirical RCE,\nwhere it achieves the lowest RCE values\non both GREEN (0.015 for Radialog, 0.02\nfor CheXpertPlus_mimiccxr) and Negative\nRadCliQ-v0 (0.02 for Radialog, 0.025 for\nCheXpertPlus_mimiccxr). These results confirm\nVRO-GREEN's superior consistency in aligning\nuncertainty with factual correctness across multiple\nmetrics.\nAt the sentence level, the Pearson correlation be-\ntween sentence-level uncertainty (VRO-RadGraph)\nand factual precision is strong for both models,\nwith -0.52 for the Radialog model and -0.55 for\nthe CheXpertPlus_mimiccxr model, indicating ef-\nfective alignment with factuality at the sentence\nlevel."}, {"title": "5.2 Enhancing RRG Intervention(RQ2)", "content": "Figure 2 and Figure 3 illustrate the impact of report-\nlevel abstention on factuality scores (GREEN)\nfor the Radialog and CheXpertPlus models, re-\nspectively. By excluding the top 20% most un-\ncertain reports, our UQ method achieves notable\nfactuality improvements: 10% for Radialog and\n9.2% for CheXpertPlus, demonstrating consistent\ngains across models. These results highlight our\nmethod's effectiveness in enhancing report quality\nand supporting radiologists in focusing on more\nreliable reports.\nAt the sentence level, uncertainty-precision\nalignment results reveal that for the Radialog\nmodel, the highest-uncertainty sentence aligns with\nthe lowest factual precision at a rate of 82.9%,\nwhile the lowest-uncertainty sentence aligns with\nthe highest factual precision at only 59.1%. For the\nCheXpertPlus model, these rates are 81.2% and\n59.6%, respectively, closely mirroring the trend\nobserved in Radialog. This discrepancy indi-\ncates that while our sentence-level UQ method ef-\nfectively flags low-precision sentences with high\nuncertainty, it performs poorly in cases of low-\nuncertainty sentences, highlighting the presence of\nconfidently hallucinated sentences that our method\nstruggles to capture. This limitation underscores\na key challenge in our current approach and sug-\ngests an avenue for future work. More details are\ndiscussed in Section 7."}, {"title": "5.3 Detection of Hallucinations of Prior\nExams (RQ3)", "content": "Figure 4 and Figure 5 demonstrate the effective-\nness of our report-level uncertainty quantification\nin detecting hallucinations of prior exams for the\nRadialog and CheXpertPlus models, respectively.\nRejecting high-uncertainty reports leads to a clear\ndecrease in the percentage of reports with prior\nreferences and the average number of prior-related\nsubstrings, significantly improving hallucination\ndetection. In contrast, the random baseline, av-\neraged over 5 seeds, shows no reduction in these\nmetrics."}, {"title": "5.4 Qualitative Analysis.", "content": "In this section, we analyze the qualitative aspects of\nour UQ framework for radiology report generation.\nSpecifically, we explore (1) the effect of increasing\nthe number of sampled reports on UQ performance\nand (2) a case study showcasing the practical utility\nof our framework in identifying low factual correct-\nness and guiding radiologist interventions.\nNumber of sampled reports. Research on short\nquestion-answer tasks and long-form generation\nhas shown that increasing the number of sampled\nresponses can enhance the performance of uncer-\ntainty quantification. We extend this investigation\nto radiology report generation, exploring whether\nthe same holds true in this domain. As illustrated in\nFigure 6, our findings align with previous research,\nshowing that the performance of UQ improves with\nmore samples and converges when using seven sam-\nples.\nCase Study. Figure 7 illustrates our UQ frame-\nwork's ability to identify low factual correctness\nand assist radiologists in targeted review. In Fig-"}, {"title": "6 Related Work", "content": "Multimodal Foundation Models, such as VLLMs,\naugment large language models (LLMs) with vi-\nsual inputs (Antropic, 2024; OpenAI, 2023). These\nmodels are typically pre-trained on diverse datasets\n(Erhan et al., 2010; Chen et al., 2020; Li et al., 2022;\nLin et al., 2024; Alayrac et al., 2022) before applied\nto specialized tasks, reducing the requirements for\ndomain-specific data. VLLMs have been evaluated\nin medical applications such as medical image inter-\npretation and radiology report generation (Litjens\net al., 2017; Esteva et al., 2021; Moor et al., 2023;\nSrivastav et al., 2024), and have demonstrated per-\nformance comparable to previous supervised meth-\nods (Rajpurkar, 2017; Qin et al., 2018), and in\nsome cases, even rival medical experts (Tiu et al.,\n2022). However, there are challenges that hinder\nestablishing trust in multimodal foundation mod-\nels in clinical practice (Truhn et al., 2024; Freyer"}, {"title": "7 Conclusion", "content": "In this paper, we tackle the challenge of hallucina-\ntions in RRG through a novel UQ approach. Our\nplug-and-play framework introduces both report-\nlevel and sentence-level UQ to detect low-factuality\nreports and identify non-existent prior hallucina-\ntions, supporting more effective radiologist inter-\nvention. Applied to the MIMIC-CXR dataset, our\nmethod achieved a 10% improvement in factual-\nity by rejecting 20% of high-uncertainty reports\nusing the Radialog model. Additionally, sentence-\nlevel UQ flagged sentences with the lowest factual\nprecision at 82.9% accuracy, enabling targeted in-\ntervention. Future work will focus on exploring\nsupervised uncertainty measures to improve factu-\nality, particularly addressing cases where the UQ\nframework assigns low uncertainty to hallucinated\npredictions generated by VLLMs. Additionally,\nintegrating uncertainty directly into the generation\nprocess could guide models toward more factual\noutputs by conditioning generation on uncertainty\nthresholds, thus enhancing both the reliability of\nUQ and overall model trustworthiness."}, {"title": "Limitations", "content": "In this section, we outline the limitations of our\nwork and potential areas for improvement.\nFirst, while we demonstrate the effectiveness of\nour method across different model architectures\nusing the MIMIC-CXR dataset, our evaluation\nis limited to this dataset. Expanding our experi-\nments to other datasets, such as IU X-Ray (Demner-\nFushman et al., 2016) or the recently published\nCheXpert Plus (Chambon et al., 2024), could fur-\nther validate the generalizability of our approach.\nSecond, due to challenges outlined in Section 3,\nwe were only able to compare our method against\nthree relatively simple baselines. As UQ techniques\ncontinue to evolve within this domain, the develop-\nment of domain-specific models, such as tailored\nNLI models for RRG, could enable a more compre-\nhensive comparison in future work.\nThird, while our sentence-level uncertainty quan-\ntification effectively aligns high-uncertainty sen-\ntences with low factual precision, it struggles to\nalign low-uncertainty sentences with high factual\nprecision, revealing a gap in detecting confidently\nhallucinated sentences. This limitation suggests the\nneed for enhanced UQ techniques and the potential\nbenefit of incorporating a fact-checking module to\nimprove reliability and distinguish factual inaccu-\nracies.\nFinally, our current sentence-level UQ is de-\nsigned with intervention in mind, focusing solely\non the factual precision of generated reports. How-\never, this approach overlooks factual completeness,\nmeaning it does not account for important factual\ninformation that may be omitted from the generated\nreport. Future work could address this by designing\nUQ methods that consider both factual precision\nand completeness, providing a more balanced eval-\nuation of report quality.\nThese limitations highlight opportunities for fur-\nther refinement and experimentation in UQ method-\nologies for radiology report generation"}, {"title": "A GREEN", "content": "Given a generated report as the hypothesis and a\nground truth report as the reference, the GREEN\nevaluation framework assesses clinical accuracy by\nanalyzing both error counts across various clini-\ncally significant categories and counts of matched\nfindings. Specifically, GREEN categorizes errors\nas follows:\n(a) False report of a finding in the candidate.\n(b) Missing a finding present in the reference.\n(c) Misidentification of a finding's anatomic loca-\ntion/position.\n(d) Misassessment of the severity of a finding.\n(e) Mentioning a comparison that isn't in the refer-\nence.\n(f) Omitting a comparison detailing a change from\na prior study. The GREEN score is then calculated\nas:\nGREEN = # matched findings# matched findings + \u03a3(f)i=(a)# error i"}, {"title": "B Challenges in Applying Other UQ\nMethods to RRG", "content": "NLI-based UQ\nThe lack of domain-specific NLI models in RRG\nmakes this approach infeasible. Although Bannur\net al. (2024) leverage in-context learning with large"}, {"title": "C Evaluation", "content": "These metrics are organized into four categories:\nLexical Metrics. We apply traditional Natural Lan-\nguage Processing (NLP) metrics such as BLEU (Pa-\npineni et al., 2002) to measure token-level similar-\nity between the generated and ground-truth reports.\nIn addition, we leverage the embedding-based sim-\nilarity metric BertScore (Zhang et al., 2019) to\ncapture more nuanced relationships between the\ntexts.\nFactuality Metrics. To assess factual consis-\ntency between the generated and ground-truth re-\nports, we use two key approaches. First, Semb\nScore is calculated by passing both reports through\nthe CheXpert model (Smit et al., 2020), which\nextracts present/absent"}]}