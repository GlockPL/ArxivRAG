{"title": "AlterMOMA: Fusion Redundancy Pruning for\nCamera-LiDAR Fusion Models with Alternative\nModality Masking", "authors": ["Shiqi Sun", "Yantao Lu", "Ning Liu", "Bo Jiang", "JinChao Chen", "Ying Zhang"], "abstract": "Camera-LiDAR fusion models significantly enhance perception performance in\nautonomous driving. The fusion mechanism leverages the strengths of each modal-\nity while minimizing their weaknesses. Moreover, in practice, camera-LiDAR\nfusion models utilize pre-trained backbones for efficient training. However, we\nargue that directly loading single-modal pre-trained camera and LiDAR backbones\ninto camera-LiDAR fusion models introduces similar feature redundancy across\nmodalities due to the nature of the fusion mechanism. Unfortunately, existing\npruning methods are developed explicitly for single-modal models, and thus, they\nstruggle to effectively identify these specific redundant parameters in camera-\nLiDAR fusion models. In this paper, to address the issue above on camera-LiDAR\nfusion models, we propose a novelty pruning framework Alternative Modality\nMasking Pruning (AlterMOMA), which employs alternative masking on each\nmodality and identifies the redundant parameters. Specifically, when one modality\nparameters are masked (deactivated), the absence of features from the masked\nbackbone compels the model to reactivate previous redundant features of the other\nmodality backbone. Therefore, these redundant features and relevant redundant\nparameters can be identified via the reactivation process. The redundant parameters\ncan be pruned by our proposed importance score evaluation function, Alternative\nEvaluation (AlterEva), which is based on the observation of the loss changes when\ncertain modality parameters are activated and deactivated. Extensive experiments\non the nuScenes and KITTI datasets encompassing diverse tasks, baseline models,\nand pruning algorithms showcase that AlterMOMA outperforms existing pruning\nmethods, attaining state-of-the-art performance.", "sections": [{"title": "1 Introduction", "content": "Camera-LiDAR fusion models are prevalent in autonomous driving, effectively leveraging the\nsensor properties, including the accurate geometric data from LiDAR point clouds and the rich\nsemantic context from camera images [1, 2], providing a more comprehensive understanding of the\nenvironment [3, 4]. However, the exponential increase in parameter counts due to fusion architectures\nintroduces significant computational costs, especially when deploying these systems on resource-\nconstrained edge devices, which is a crucial challenge for autonomous driving [5]. Network pruning\nis one of the most attractive methods for addressing the challenge above of identifying and eliminating\nredundancy in models. Existing pruning algorithms target single-modal models [6\u201310] or multi-modal\nmodels that merge distinct types of data [11, 12], such as visual and language inputs. However,\nit's important to note that directly applying these algorithms to camera-LiDAR fusion models can\nlead to significant performance degradation. The degradation can be reasoned for two main factors\nthat existing pruning methods overlooked: 1) the key fusion mechanism specific to vision sensor\ninputs within models, and 2) the training scheme where models typically load single-modal pre-\ntrained parameters onto each backbone [13, 2]. Specifically, since single-modality models lack the\ncross-modality fusion mechanism, existing pruning algorithms traditionally do not consider inter-\nmodality interactions. Furthermore, because the pre-trained backbones (image or LiDAR) are trained\nseparately, they are not optimized jointly, exacerbating the redundancy in features extracted from each\nbackbone. Though leveraging pre-trained backbone improves the training efficiency compared with\nmodels training from scratch, we argue that directly loading single-modal pre-trained camera and\nLiDAR backbones into camera-LiDAR fusion models introduces similar feature redundancy across\nmodalities due to the nature of the fusion mechanism.\nIn detail, since backbones are independently pre-trained on single-modal datasets, they extract features\ncomprehensively, which leads to similar feature extraction across modalities. Meanwhile, the fusion\nmechanism selectively leverages reliable features while minimizing weaker ones across modalities\nto enhance model performance. This selective utilization upon similar feature extraction across\nmodalities introduces the additional redundancy: Each backbone independently extracts similar\nfeatures, which subsequent fusion modules will not potentially utilize. For instance, both camera\nand LiDAR backbones extract geometric features to predict depth during pre-training. However,\ngeometric features extracted from the LiDAR backbone are considered more reliable during fusion\nbecause LiDAR input data contain more accurate geometric information than the cameras, e.g., object\ndistance, due to the physical properties of sensors. Consequently, this leads to the redundancy of\ngeometric features of the camera backbone. In summary, similar feature extraction across modalities,\ncoupled with the following selective utilization in fusion modules, leads to two counterparts of similar\nfeatures across modalities: those utilized by fusion modules in one modality (i.e., fusion-contributed),\nand those that are redundant in the other modality (i.e., fusion-redundant). We also illustrate the\nfusion-redundant features in Figure 1."}, {"title": "2 Related Work", "content": "Camera-LiDAR Fusion. With the advancement of autonomous driving technology, the efficient\nfusion of diverse sensors, particularly cameras and LiDARs, has become crucial [16, 17]. Fusion\narchitectures can be categorized into three types based on the stage of fusion within the learning\nframework: early fusion [18, 19], late fusion [20], and intermediate fusion [3, 13, 2]. Current state-\nof-the-art (SOTA) fusion models evolve primarily within intermediate fusion and combine low-level\nmachine-learned features from each modality to yield unified detection results, thus significantly\nenhancing perception performance compared with early or late fusion. Specifically, camera-LiDAR\nfusion models focus on aligning the camera and LiDAR features through dimension projection at\nvarious levels, including point [21], voxel [22], and proposal [3]. Notably, the SOTA fusion paradigm\naligns all data to the bird's eye view (BEV) [13, 4, 2, 23, 24], has gained traction as an effective\napproach to maximize the utilization of heterogeneous data types.\nNetwork Pruning. Network pruning effectively compresses deep models by reducing redundant\nparameters and decreasing computational demands. Pruning algorithms have been well-explored for\nsingle-modal perception tasks [25\u201327, 6, 28, 29], focusing on evaluating importance scores to identify\nand remove redundant parameters or channels. These scores are based on data attributes [27, 6],\nweight norms [30, 31], or feature map ranks [26]. However, single-modal pruning algorithms are\nnot suited for the complexities of camera-LiDAR fusion models. While some multi-modal pruning\nalgorithms exist [11, 12], they are mainly designed for models combining different data types like\nlanguage and vision. Therefore, there is a pressing need for pruning algorithms specifically devised\nfor camera-LiDAR fusion models. From the perspective of granularity, pruning algorithms can be\ndivided into two primary categories: 1) structured pruning, which entails removing entire channels or\nrows from parameter matrices, and 2) unstructured pruning, which focuses on eliminating individual\nparameters. For practical applications, we have adapted our method to support both types of pruning."}, {"title": "3 Methodology", "content": "3.1 Preliminaries\nWe firstly review some basic concepts including camera-LiDAR fusion models and pruning formula-\ntion. Camera-LiDAR fusion models consist of 1) a LiDAR feature extractor F\u2081 to extract features\nfrom point cloud inputs, 2) a camera feature extractor Fe to extract features from image inputs, 3) the\nfusion module and following task heads Ff to get the final task results. The parameters denote as \u03b8 =\n{ \u03b8\u03b9, \u03b8\u03b5, \u03b8f } for LiDAR backbone, camera backbone, and fusion and task heads, respectively. Take\ncamera backbone for instance, 0c = {01, 02, ..., 0} denotes all weights in the camera backbone,\nwhere Ne represents the total number of parameters in camera backbone. Therefore, for the LiDAR\ninput X and camera input Xe, the training process of models could be denoted as\narg min L(Y, Ff (0f; F\u03b9(\u03b8\u03b9; \u03a7\u03b9), Fc(0c; Xc)),\n01,c, f\n(1)\nwhere Y denotes the ground truth, and L represents the task-specific loss functions.\nImportance-based pruning typically involves using metrics to evaluate the importance scores of\nparameters or channels. Subsequently, optimization methods are employed to prune the parameters"}, {"title": "3.2 Overview of Alternative Modality Masking Pruning", "content": "Similar feature extraction across modalities, coupled with the selective utilization of features in the\nfollowing fusion modules introduce redundancy in camera-LiDAR fusion models. Therefore, similar\nfeatures and their relevant parameters can be categorized into two counterparts across modalities:\nthose that contribute to fusion and subsequent task heads (fusion-contributed), and those that are\nredundant (fusion-redundant). In this section, we propose the pruning framework AlterMOMA,\nwhich alternatively employs masking on camera and Lidar backbones to identify and remove the\nfusion-redundant parameters. AlterMOMA is developed based on a novel insight: \"The absence\nof fusion-contributed features will compel fusion modules to 'reactivate' their fusion-redundant\ncounterparts as supplementary, which, though less effective, are necessary to maintain functionality.\"\nFor instance, if the LiDAR backbone is masked, the previously fusion-contributed geometric features\nit provided are absent. To fulfill the need for accurate position predictions, the model still needs to\nprocess geometric features. Consequently, the fusion module is compelled to utilize the geometric\nfeatures from the unmasked camera backbone, which were previously fusion-redundant. We refer to\nthis process as Redundancy Reactivation. By observing changes during this Redundancy Reactivation,\nfusion-redundant parameters can be identified. The overview of AlterMOMA is shown in Figure 2,\nand the detailed steps are in Algorithm 1 of Appendix D. The key steps are introduced as follows:\nModality Masking. Three binary masks are denoted as \u03bc\u03b9, \u03bc\u03b5, and \u03bcf \u2208 {0,1}, correspond to the\nparameters applied separately on the LiDAR backbone, the camera backbone, and the fusion and\ntasks head. Our framework begins by masking either one of the camera backbones or the LiDAR\nbackbone. Here we take masking the LiDAR backbone as the illustration. The masks are with \u03bc\u03b9 = 0,\n\u03bc\u03b5 = 1, \u03bc\u03b5 = 1. The camera backbone will be masked alternatively."}, {"title": "Redundancy Reactivation", "content": "To allow masked models to reactivate fusion-redundant parameters, we\ntrain masked models with batches of data. Specifically, B batches of data Di, i \u2208 {1, 2, ..., B} are\nsampled from the multi-modal dataset D."}, {"title": "Importance Evaluation", "content": "After Redundancy Reactivation, the importance scores of parameters in the\ncamera backbones are calculated with our proposed importance score evaluation function AlterEva\ndetailed in Section 3.3. Since fusion modules need to consider the reactivation of both modalities,\nthe importance scores of parameters in the fusion module and task heads will be updated once the\nimportance scores of both the camera and Lidar backbones' parameters are calculated."}, {"title": "Alternative Masking", "content": "After Importance Evaluation of camera modality, models will reload the\ninitialized parameters, and then the other backbone will alternatively be masked, with \u03bc\u03b9 = 1, \u03bc\u03b5 = 0,\n\u03bcf = 1. Then the step Redundancy Reactivation and Importance Evaluation will be processed again\nto update the importance scores of parameters in the LiDAR backbone and the fusion module."}, {"title": "Pruning with AlterEva", "content": "After evaluating the importance scores using AlterEva, parameters with\nlow importance scores are pruned with a global threshold determined by the pruning ratio. Once the\npruning is finished, the model is fine-tuned with the task-specific loss, as indicated in Eqn. 1."}, {"title": "3.3 Alternative Evaluation", "content": "In this section, we will detail the formulation of our proposed AlterEva, which consists of two distinct\nindicators to evaluate the parameter importance scores. As outlined in section 3.2, the importance\nscores are alternatively calculated with AlterEva in Importance Evaluation. Then, parameters with low\nimportance scores are removed in the pruning process. The goal of AlterEva is to maximize the scores\nof parameters that contribute to task performance while minimizing the scores of fusion-redundant\nparameters. To achieve this, AlterEva incorporates two key indicators: 1) Deactivated Contribution\nIndicator (DeCI) evaluate the parameter contribution to the overall task performance of the fusion\nmodels, 2) Reactivated Redundancy Indicator (ReRI) identifies fusion-redundant parameters across\nboth modalities. Since changes in loss can directly reflect the parameter contribution difference to\ntask performance during alternative masking, both indicators are designed based on the observation of\nloss decrease or increase, when certain modality parameters are activated or deactivated. Specifically,\ntake parameters in the camera backbone as an instance, DeCI observe the loss increases with masking\ncamera backbone itself, while ReRI observe loss decrease with masking LiDAR backbone and\nreactivating camera backbone via Redundancy Reactivation. Formally, we formulate the loss for the\nfusion models with masks. With three binary masks and the dataset defined in Section 3.2, the loss is\ndenoted as follows, by simplifying some of the extra notations used in Eqn. 1:\nLm (\u03bc\u03b5, \u03bc\u03b9, \u03bc\u03b5; D) = L(\u03bc\u03b9 \u2299 \u03b8\u03b9, \u03bc\u03b5 \u03b8\u03b5, \u03bc\u03b5 \u03b8f; D).\n(3)\nFor brevity, we assume \u03bc\u03b5 = 1, \u03bc\u03b9 = 1, and \u03bcf = 1, and we only specify in the formulation when\na mask is zero. For example, Lm(\u00b5c = 0; D) indicates that \u00b5c = 0, \u03bcf = 1 and \u03bc\u03b9 = 1. Since the\nalternative masking is performed on both backbones, we illustrate our formulation by calculating two\nindicators for parameters in the camera backbone."}, {"title": "Deactivated Contribution Indicator", "content": "If a parameter is important and contributes to task performance,\ndeactivating this parameter will lead to task performance degradation, which will be reflected in an\nincrease in loss. Therefore, to derive the contribution of the i-th parameter of the camera backbone,\nwe observe the changes in loss when this parameter is deactivating via masking, denoted as follows:\n\u00deoi = |Lm(; D) \u2013 Lm (\u03bc\u00b2 = 0; D)|,\n\u03b8\u03b5\n(4)\nwhere \u03bc\u03b5 represents the mask for 0, and I denotes the indicator DeCI for 0. However, the total\nnumber of parameters is enormous, deactivating and evaluating each parameter independently are\ncomputationally intractable. Therefore, we design an alternative efficient method to approximate the\nevaluation in Eqn. 4 by leveraging the Taylor first-order expansion. We first observe the loss changes\n|Lm(; D) \u2013 Lm(\u03bcc = 0; D)| by deactivating the entire camera backbones. Then, the first-order\napproximation of evaluation in Eqn. 4 is calculated by expanding the loss change in each individual\nparameter with Taylor expansion, considering 0 = {01, ..., 0}. This method allows us to\nestimate the contribution for each parameter, denoted as follows:\n\u03a6\u03b8 = |Lm (; D) + \u03bc\u03b5 \u03b8\u03ad. Lm (; D) Lm (\u03bc\u03b5 = 0; D) \u2013 \u03bc\u03b5\u03b8. Lm (\u00b5c = 0; D) 1. (5)\n\u0398\u03b8\u03ad\n\u0398\u03b8\u03ad"}, {"title": "", "content": "When e is deactivating with \u03bc\u03b5 = 0, \u03bc\u00b2 = 0 for i \u2208 {1, ..., N\u00ba}, which means that the last term of\nEqn. 5 is zero. Meanwhile, when considering importance scores on a global scale, the Lm (; D) and\nLm (\u00b5c = 0; D) can be treated as constant for all \u03b8. Thus the first term and the third term can be\ndisregarded. Therefore, the final indicator of each parameter's contribution, represented by our DeCI,\ncan be expressed as follows:\n\u03a6 = |0. Lm (; D)|.\n\u03b8\u03b5\n(6)\nThis formulation enables tractable and efficient computation without Modality Masking of the camera\nbackbone itself, achieved by performing a single backward propagation in the Importance Evaluation\nwith initialized parameters."}, {"title": "Reactivated Redundancy Indicator", "content": "As discussed in Section 3.2, the identification of fusion-\nredundant parameters relies on our understanding of the fusion mechanism: when fusion-contributed\nfeatures from the LiDAR backbone are absent due to masking, the previously fusion-redundant\ncounterparts and their relevant parameters from the camera backbone will be reactivated during\nthe Redundancy Reactivation. Therefore, to reactivate and identify fusion-redundant parameters\nin the camera backbone, the Modality Masking of the LiDAR backbone (\u03bc\u03b9 = 0) and Redundancy\nReactivation are processed first. Throughout this process, the loss evolves from Lm(\u03bc\u03b9 = 0; D\u2081) to\nLm(\u03bc\u03b9 = 0; DB), and the parameters evolve from 0,0 (i.e. \u03b8c) to \u03b8c, B. Similar to the formulation of\nDeCI, we observe the decrease in loss during Redundancy Reactivation and refer to this observation\nas our ReRI, denoted as follows:\n\u03a6\u03b8\u03b5 = |Lm (\u03bc\u03b9 = 0; D) \u2013 Lm (\u03bc\u03b9 = 0; D\u2081) + ... + Lm(;DB-1) \u2013 Lm(\u03bc\u03b9 = 0; DB)|\n= |Lm (\u03bc\u03b9 = 0; D) \u2013 Lm (\u03bc\u03b9 = 0; \u0414\u0432)|.\n(7)\nSpecifically, this process is designed to identify parameters that contribute to the task performance of\nmodels with the masked LiDAR backbone, highlighting those that are fusion-redundant. Since we\nwant to observe reactivation rather than parameters updating of this masked model across training\nbatches, we apply the first-order Taylor expansion to the initial i-th parameters 0,0, 0,0, denoted as:\n\u03a6\u03bf\u0390\u03bf = |Lm (\u03bc\u03b9 = 0; D) + \u03bc\u03b5 \u03b8\u03ad,0 Lm (\u03bc\u03b9 = 0; D)\nc,0\n20,0\nc,0\ni\nLm (\u03bc\u03b9 = 0; DB) \u2013 \u03bc\u03b5 \u263a 0.0 Lm (\u03bc\u03b9 = 0; DB) = \\.\nc,0"}, {"title": "", "content": "To derive the gradient on initial parameters 00 of the last term, we could use the chain rule and write\nout based on the gradient of the last step,\nJLm (\u03bc\u03b9 = 0; DB) . 0.0\nc,0\nc, B 20\n0,0 B\nLm ( = 0; DB) \u041f\u0434 \u00b7 \u0441. (9)\n\u0434\u0435, \u0432\nj=1\nc,j\nc,j-1\nj\u22121 \u2192 \u03b8\u03ad\u0441,. \nc,0\nAccording to the Proposition A in the Appendix A, this approximation is reached by dropping\nsome small terms with sufficiently small learning rates. Since \u03b8 is activating with \u03bc\u00b2 = 1, and the\nLm(\u03bc\u03b9 = 0; D) and Lm (\u00b5c = 0; DB) can be treated as constant for all \u03b8, we could denote our final\nformulation by simplifying Eqn. 8, and denoted as follow:\n\u03a6 = |0. Lm (\u03bc\u03b9 = 0; D) \u2013 \u03b8\u03ad . \u00a3m (\u03bc\u03b9 = 0; DB)\\, (10)\nC\n\u0434\u0435, \u0432\nc,\nwhere is the 100 and I represent the ReRI for 0.\nc,0\nC\nc,0\nTo the goal of parameters with significant contributions maintaining high importance scores while\nthose identified as fusion-redundant are assigned lower scores, AlterEva calculate the final importance\nscores by subtracting ReRI from DeCI. Since DeCI of parameters in the camera backbone could\nbe calculated without masking the camera backbone itself, DeCI and ReRI of camera parameters\ncould be calculated in the same alternative masking stage (with LiDAR backbone masking), which\nsimplifies the process of our framework AlterMOMA. Therefore, with the combination Eqn. 6 and\nEqn. 10, the AlterEva of the camera backbones could be presented with the normalization:\nS(\u03b8) = \u03b1\u00b7 \u03a6\u03b8\u03b9 \u03b2\u00b7 (11)\n\u03a3\u03b9 \u03a6\u03c1 \u03a3\u03a6"}, {"title": "", "content": "where a and \u1e9e are the hyper parameters and S(0) represent the importance score evaluation function\nAlterEva for 0. Similarly, the AlterEva of parameters in the LiDAR backbones (i.e. \u03b8\u03b9) and in the\nfusion modules (i.e. Of) could be derived as:\nS(\u03b8) = \u03b1\u00b7 \u03a6\u03b8\u03ad \u03b2\u00b7\u039d\u03b9,\n\u03a3\u03c4\u03bf \u03a6\u03b8 \u03a3 \u03a6 (12)\nS(0) = \u03b1\u00b7 \u03a6\u03b8\u03b2 \u03a6\u03b8\u03b9 (\u03bc\u03b9 = 0) \u03b2 \u03a6\u03b8\u03b9 (\u03bc\u03b5 = 0) , (13)\n\u03a3\u03c4\u03bf \u03a6\u03b8 2\u0384 \u03a3\u03c4\u03bf \u03a6\u03c9\u03c2 (\u03bc\u03b9 = 0) 2 \u03a3\u03c4\u03bf \u03a6\u03bf (\u03bc\u03b5 = 0)\nwhere and \u03a6\u03bf (\u03bc\u03b5 = 0) is calculated when camera backbone is masking, while \u03a6\u03bf\u03b5 (\u03bc\u03b9 = 0)\nis calculated with LiDAR backbone masking. AlterEva can efficiently calculate importance scores\nwith backward propagation, enhancing the tractability of AlterMOMA. For brevity, we omit the\nderivations related to parameters in the LiDAR backbone and fusion modules, but additional details\nare available in Appendix B and Appendix C."}, {"title": "4 Experimental Results", "content": "4.1 Baseline Models and Datasets\nTo validate the efficacy of our proposed framework, empirical evaluations were conducted on several\ncamera-LiDAR fusion models, including the two-stage detection models AVOD-FPN [3], as well as\nthe end-to-end architecture based on BEV space, such as BEVfusion-mit [13] and BEVfusion-pku [2].\nFor AVOD-FPN, the point cloud input is processed using a voxel grid representation, while all input\nviews are extracted using a modified VGG-16 [32]. Notably, the experiment on the AVOD-FPN\ndemonstrates the efficiency of AlterMOMA on two-stage models, although this isn't the SOTA\nfusion architecture for recent 3D perception tasks. Current camera-LiDAR fusion models are moving\ntowards a unified architecture that extracts camera and LiDAR features within a BEV space. Thus,\nour primary results focus on BEV-based unified architectures, specifically BEVfusion-mit [13] and\nBEVfusion-pku [2]. We conducted tests using various backbones. For camera backbones, we included\nSwin-Transformer (Swin-T) [33] and ResNet [34]. For LiDAR backbones, we used SECOND [16],\nVoxelNet [35] and PointPillars [36].\nWe perform our experiments for both 3D object detection and BEV segmentation tasks on the\nKITTI [15] and nuScenes [14], which are challenging large-scale outdoor datasets devised for\nautonomous driving tasks. The KITTI dataset contains 14,999 samples in total, including 7,481\ntraining samples and 7,518 testing samples, with a comprehensive total of 80,256 annotated objects.\nTo adhere to standard practice, we split the training samples into a training set and a validation\nset in approximately a 1:1 ratio and followed the difficulty classifications proposed by KITTI,\ninvolving easy, medium, and hard. NuScenes is characterized by its comprehensive annotation scenes,\nencompassing tasks including 3D object detection, tracking, and BEV map segmentation. Within\nthis dataset, each of the annotated 40,157 samples presents an assemblage of six monocular camera\nimages, adept at capturing a panoramic 360-degree field of view. This dataset is further enriched with\nthe inclusion of a 32-beam LiDAR scan, amplifying its utility and enabling multifaceted data-driven\ninvestigations."}, {"title": "4.2 Implementation Details", "content": "We conducted the 3D object detection and segmentation experiments with MMdetection3D [37] on\nNVIDIA RTX 3090 GPUs. To ensure fair comparisons, consistent configurations of hyperparameters\nwere employed across different experimental groups. To train the 3D object detection baselines, we\nutilize Adam as the optimizer with a learning rate of 1e-4. We employ Cosine Annealing as the\nparameter scheduler and set the batch size to 2. For BEV segmentation tasks, we employ Adam as\nthe optimizer with a learning rate of le-4. We utilize the one-cycle learning rate scheduler and set the\nbatch size to 2. The hyperparameters a and \u1e9e in Section 3.3 are both set with 1. The baseline pruning\nmethods include IMP [10], SynFlow [38], SNIP [31], and ProsPr [8], with the hyperparameters\nspecified in their papers respectively."}, {"title": "4.3 Experimental Results on Unstructured Pruning", "content": "To evaluate the efficiency of AlterMOMA with unstructured pruning, we conduct experiments\nacross multiple fusion architectures and datasets for 3D object detection and semantic segmentation.\nSpecifically, to evaluate the efficiency of AlterMOMA on two-stage fusion architectures, we applied\nAlterMOMA to AVOD-FPN [3], using the KITTI dataset with the task of 3D detection. Besides,\nBEVfusion-mit [13] and BEVfusion-pku [2], as two representative camera-LiDAR fusion models\nwith unified BEV-based architectures, are applied with AlterMOMA using the nuScenes dataset\nto validate the efficiency on both 3D detection and semantic segmentation tasks. Additionally, to\nvalidate the robustness of AlterMOMA with various backbones, we conducted experiments with\nalternative images and point backbone, including ResNet [34] and pointpillars [36].\n3D Object detection on nuScenes with BEV-based fusion Architectures. The experimental\nresults are presented in Table 1. Note that baseline models are BEVfusion-mit trained with SwinT\nand VoxelNet backbone. As reported in Table 1, single-modal pruning methods, including IMP,\nSynFlow, SNIP, and ProsPr, experience significant declines in accuracy performance. Even the ProsPr,\nconsidered the best-performing method among these single-modal pruning techniques, demonstrates\nthe mAP decrease of 3.5% in accuracy at the 80% pruning ratio and 9.2% at the 90% pruning ratio\non BEVfusion-mit. Conversely, the incorporation of our AlterMOMA yielded promising results.\nFor example, comparing with the baseline pruning method ProsPr, AlterMOMA boosts the mAP of\nBEVfusion-mit by 3.0% (64.3% \u2192 67.3%), 3.6% (61.9% \u2192 65.5%), and 4.9% (58.6%\u2192 63.5%) for\nthe three different pruning ratios. Similarly, AlterMOMA obtains much higher mAP and NDS than\nthe other four pruning baselines with different pruning ratios on BEVFusion-mit and BEVFusion-pku.\n3D Object detection on KITTI with the two-stage fusion architecture. To validate the efficiency\nof AlterMOMA on the two-stage detection fusion architecture, we conduct experiments with various\npruning ratios on KITTI with AVOD-FPN architecture as the baseline. The experimental results\nare presented in Table 2. Specifically, Table 2 presents the results for the car class on the KITTI,\ndetailing AP-3D and AP-BEV across various difficulty levels including easy, moderate, and hard.\nExisting pruning methods experience significant declines in performance on different metrics of\ndifferent difficulties. Even the best-performing method among single-modal pruning methods,\nProPr, shows a decrease in AP-3D of 3.5%, 2.6%, and 4.4% in the easy, moderate, and hard\ndifficulty levels, respectively, at the 80% pruning ratio. Conversely, the AlterMOMA has yielded\npromising results. For instance, comparing with the pruning method ProsPr, AlterMOMA enhances\nboth the AP-3D and AP-BEV on AVOP-FPN by 3.2% (74.2% \u2192 77.4%) and 3.9% (81.2% \u2192\n85.3%) for easy difficulties at the 90% pruning ratio. Furthermore, AlterMOMA consistently"}, {"title": "4.4 Structure Pruning Results on 3D object Detection", "content": "To assess the efficacy of our proposed pruning approach in structure pruning, we conducted experi-\nments with BEVfusion-mit models with ResNet101 as the camera backbone and SECOND as the\nLiDAR backbones. Specifically, We measured the performance of the pruned networks with a similar\namount of FLOP reductions and reported the number of FLOPs('GFLOPs'). As depicted in Table 5,\nthe results obtained from these experiments consistently demonstrate state-of-the-art performance\nwith various pruning sparsities. Our evaluations at 30% and 50% pruning sparsities reveal that Alter-\nMOMA not only maintains a competitive mAP and NDS but also achieves a substantial reduction in\ncomputational overhead. Notably, with the 30% pruning sparsities, AlterMOMA achieves a 0.7% on\nmAP and 0.5% on NDS compared with the unpruned baseline models, which reveals that removing\nsimilar feature redundancy improves the efficiency of models. Specifically, compared with the ProsPr\nbaseline, AlterMOMA achieves a substantial enhancement by 1.1% (64.6% \u2192 65.3%), and 2.0%\n(62.5% \u2192 64.5%), for the two different pruning sparsities."}, {"title": "5 Discussion and Conclusion", "content": "Although our approach identifies similar feature redundancy in camera-LiDAR fusion models,\nit is limited to the perception field. Extending it to other multi-modal models, such as vision-\nlanguage models, requires further research. Fusion modules across various modalities exhibit\ndifferent functionalities. In multi-sensor fusion models (camera, LiDAR, and Radar), the focus is\non supplementing and spatially aligning data by leveraging the sensors' physical properties, fusing\nlow-level features. However, in models with disparate data types like vision and language, fusion\nmodules focus on matching high-level semantic contexts. Therefore, AlterMOMA primarily addresses\nredundancy from supplementary functionality in multi-sensor fusion perception architectures.\nIn this paper, we explore the computation reduction of camera-LiDAR fusion models. A pruning\nframework AlterMOMA is introduced to address redundancy in these models. AlterMOMA employs\nalternative masking on each modality and observes loss changes when certain modality parameters\nare activated and deactivated. These observations are integral to our importance scores evaluation\nfunction AlterEva. Through extensive evaluation, our proposed framework AlterMOMA achieves\nbetter performance, surpassing the baselines established by single-modal pruning methods."}, {"title": "A Derivation of Eqn.9 in Section 3.3", "content": "Proposition. For a camera-LiDAR fusion model with parameters Oc for the camera backbone and\nparameters \u03b8\u03b9 for the LiDAR backbone", "as": "ndLm (\u03bc\u03b9 = 0; DB) \u00b7 0,\u03bf \u2248 dLm(\u03bc\u03b9 = 0,DB)\nC\n\u0441,0 c,0,\n(14)\nProof. We can extend the left side of the equation using the chain rule, denoted as,\ndLm (\u03bc\u03b9 = 0; DB) . 00 dLm(\u03bc\u03b9 = 0; DB) \u0434\u0435\u0441"}]}