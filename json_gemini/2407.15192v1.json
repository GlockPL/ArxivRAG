{"title": "Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge", "authors": ["Joshua Shay Kricheli", "Khoa Vo", "Spencer Ozgur", "Aniruddha Datta", "Paulo Shakarian"], "abstract": "Recent advances in Hierarchical Multi-label Classification (HMC), particularly neurosymbolic-based approaches, have demonstrated improved consistency and accuracy by enforcing constraints on a neural model during training. However, such work assumes the existence of such constraints a-priori. In this paper, we relax this strong assumption and present an approach based on Error Detection Rules (EDR) that allow for learning explainable rules about the failure modes of machine learning models. We show that these rules are not only effective in detecting when a machine learning classifier has made an error but also can be leveraged as constraints for HMC, thereby allowing the recovery of explainable constraints even if they are not provided. We show that our approach is effective in detecting machine learning errors and recovering constraints, is noise tolerant, and can function as a source of knowledge for neurosymbolic models on multiple datasets, including a newly introduced military vehicle recognition dataset.", "sections": [{"title": "1 INTRODUCTION", "content": "Hierarchical Multi-label Classification (HMC) extends the idea of multi-label classification to impose a hierarchy among labels [13, 14, 29, 30] and has been applied to a variety of applications [4, 19, 20]. A separate line of machine learning research deals with the training of a secondary model to identify failures in the first [8, 11, 24, 27] - which has been referred to as \u201cmetacognition\u201d [17, 21] and typically the secondary model is a black-box model. Recent research trends (specifically neurosymbolic AI [10, 26]) have led to the expression of such hierarchical relationships as constraints on the learning process [4, 13, 33] or leveraging constraints to make corrections to the machine learning model [7, 15]. A key assumption in these approaches is that the constraints are known a-priori. Meanwhile, a metacognitive approach known as Error Detection Rules (EDR) allows for the learning of rules to predict failures [32] based on conditions derived from domain knowledge and/or complementary models for the same task trained on the same data. The key intuition of this paper is that we can modify the ideas of [32] to predict errors and recover constraints of an HMC model without a-prior knowledge of the constraints. Our contributions are as follows:\n\u2022 We extend the EDR framework of [32] to address the HMC problem without prior knowledge of the hierarchy by both extending the language of that work and presenting a new Focused-EDR which addresses an objective function mismatch of [32] by leveraging approximate optimization of the ratio of two submodular functions;\n\u2022 We demonstrate how our new approach, provides significant improvement in the detection of errors when compared to a black-box baseline neural error detector and the detection algorithm of [32] on three different HMC datasets;\n\u2022 We show our approach can recover constraints and that both the F1-score of constraints recovered as well as error F1 degrades gracefully with noise - with noise injected in a manner to remove certain classes from consideration;\n\u2022 We show the recovered constraints can then be used as a source for in neurosymbolic model learning (i.e., Logic Tensor Networks (LTN) [4]) to provide improved model performance and reduce the number of inconsistencies;\n\u2022 We introduce (and release 1) an open-source HMC dataset of ~10K images of military vehicles.\nThe rest of the paper is organized as follows. We provide an overview of related work in Section 2, introduce our Focused EDR approach in Section 3, followed by our experimental results and discussion in Section 4."}, {"title": "2 RELATED WORK", "content": "In Hierarchical Multi-label Classification (HMC), pre-defined con-straints are crucial for shaping effective prediction models [14, 22, 29], which, in general assume that constraints are known a-priori. Neurosymbolic approaches such as [4, 13, 33] also assum the existence of a-prior constraints. In contrast, Semantic Probabilistic Layers (SPL) identify constraint relationships from data but lack explainability, encoding constraints in a deep probabilistic circuit that is not directly interpretable [1]. The approach presented in this work does not assume knowledge of constraints yet allows recovery of interpretable constraints. This work also deals with error detection in ML models or \u201cmetacognition\" [17, 21, 31]. Recent efforts include neurosymbolic methods [7, 9, 16] that rely on"}, {"title": "3 APPROACH", "content": "We now describe some technical preliminaries for our problem and two key extensions we make to the EDR framework of [32]. Specifically, we extend the EDR method to capture the HMC case and we address the objective function mismatch in the detection algorithm. In our setup, we assume that the hierarchical data has G levels of granularities $G := \\{g_i\\}_1^G$, each with a corresponding label set $y_g$. In this work we focus in the case of G = 2, and denote $G:= \\{\\text{fine}, \\text{coarse}\\}$. The framework can extend for G > 2, but we leave evaluation of the approach beyond two levels to future work. Let X, Y be all the possible examples and predicted labels, respectively. We thus have $Y = \\bigcup_{g \\in G} Y_g$, as all the possible predicted labels, and define $Y_g := \\bigcup_{g \\in G} Y_g$. Define a labeled training set $T := \\{(x,gt(x))\\} \\subset X \\times Y$ composed of $N_T$ images and corresponding ground-truth labels. We assume the existence of a well-trained model $\\hat{f}$ returning one class per level of granularity for a given sample. We then define an Error Prediction Problem per class $y \\in y_g$, which is predicting where $\\hat{f}$ predicted y incorrectly.\nWe envision such a predictor to be trained on the same data as $\\hat{f}$ and provide a per-example binary output for $error_y(X)$ (the rule head of (1)), which we refer to as the error class of y, and denote it $e_y$. Earlier work such as [8] relied on a black box model for error predictions, while this paper along with the recent work of [32] utilizes one set of rules per class, as in Expression (1).\nEDR for HMC. The key difference in employing EDR for HMC problems is that we are detecting errors in G > 1 categories of classes which have hierarchy constraints among them. Hence, we adjust the rules of the form given in Expression (1) for an HMC problem as well as support conditions from multiple models. First, we extend the predicate assign where we not only have a subscript denoting the class (which can come from multiple granularities) but also a superscript denoting the model, so $\\text{assign}^\\hat{f}_y(X)$ means that sample x was assigned label y by model $\\hat{f}$. Note that the first predicate in the body of the rule (outside the disjunction, as per Expression (1)) will always come from the base model. While [32] primarily relied on domain knowledge for the condition sets, here we use the class predictions from a different label set of the hierarchy. As such, when considering detection rules for any class y, as in (1), we will consider conditions from the model predictions defined using granularities other than that of y. Thus for each class y at granularity g, we define a specific conditions set $C_g$ (unlike in [32] where conditions are picked from the same set of conditions as there were no levels of granularity in that paper). The per-granularity condition set will include assign conditions from the 'main' model $\\hat{f}$ across all other granularities, which are $\\hat{C} := \\bigcup_{g' \\in G\\setminus \\{g\\}} \\bigcup_{y \\in Y_{g'}} \\{\\text{assign}^\\hat{f}_{y}(X)\\}$. Since each $\\hat{C}$ will not include conditions on labels from g, we also add conditions from a \u2018secondary model $\\hat{h}_{\\theta_h}$, with a possible different architecture h and learned parameters $\\theta_h$, for all granularities, i.e. $\\hat{C}^g_{\\hat{h}_{\\theta_h}} := \\bigcup_{g \\in G} \\bigcup_{y \\in Y_g} \\{\\text{assign}^{\\hat{h}_{\\theta_h}}_{y}(X)\\}$. Moreover, similar to what the authors did in [32], we define binary conditions for each label of each granularity which utilize trained binary models $\\{\\hat{b}_{\\theta_b}\\}_{y \\in B}$, given a subset of labels $B \\subset Y_G$, yielding the binary conditions set $C_B := \\bigcup_{y \\in B} \\{\\text{assign}^{\\hat{b}_{\\theta_b}}_{y}\\}$. Finally, we set the condition set for each granularity to $C_g := \\hat{C}^g \\cup \\hat{C}^g_{\\hat{h}_{\\theta_h}} \\cup C_B$. We note that in solving the error detection problem for a given label y, we find a set $DC_y$ to create a rule of the form seen in Expression (1). Notice how after solving for all the sets $\\{DC_y\\}_{y \\in y_g}$ for an HMC problem, for any granularity g and label $y \\in y_g$, the conditions in $DC_y \\cap C$ are specifying hierarchy constraints, i.e. rules of the form \"if label y\n\n$error_y(X) \\leftarrow \\text{assign}^\\hat{f}_{y}(X) \\land \\bigvee_{cond \\in DC_y} cond(X).$\n\n$DC_y^* = argmax_{DC_y \\subset C} \\frac{POS^{DC_y}_T}{BOD^{DC_y}_T + FPT_y}$"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "We experimented on three datasets in our work - our Military Vehi-cles dataset and subsets of 50 classes from the ImageNet [25] dataset and 36 classes from the OpenImage [18] dataset. For each of those, we first trained a main model $\\hat{f}$ using the Binary Cross-Entropy (BCE) loss while employing a fitting State of the Art (SOTA) ar-chitecture and corresponding hyperparameters which we found to perform favorably among other architecture (the Vision Trans-former [12] b_16 for the Military Vehicles and OpenImage and DINO V2 [23] s_14 for ImageNet). In the Military Vehicles dataset, there are a total of 9, 444 images and we took a ratio of 80: 20 between train and test. For the ImageNet50 dataset, we kept the original ratio from the paper of 1, 300 images in the train set and 50 in the test set for each fine grain class, as well as 2000 train and 400 test images per fine grain class for the OpenImage36 dataset. In all datasets, we had ground truth constraints which we used to compute the fraction of samples violating ground truth constraints in the test set and/or determining the ability to recover constraints.\nError Detection Results. For error detection, we evaluated three approaches: Focused EDR (f-EDR, this paper), DetRuleLearn [32], and a neural (black-box) error prediction model inspired by related work (e.g., [8] - with an SOTA neural architecture) that effectively treats error detection as a binary classification problem. For these neural error detection baselines, we used the same model architec-ture of our base model but retrained for this classification problem. Rules for both f-EDR and DetRuleLearn were trained on the same training data used for the base models. Conditions for the rules were derived from both the class of complementary granularity (e.g., fine for coarse predictions and vice-versa), lesser-performed models trained on the same data, and binary classification models for the same class (much in the same way additional models were used in [32]). We note that both f-EDR and DetRuleLearn used the same set of conditions to learn rules. Results of this experiment are shown in Table 3 where we provide the balanced accuracy and F1-score of the total error - which is defined as applying a logical OR on all the per-class error classes. In all experiments, f-EDR signifi-cantly outperforms both DetRuleLearn and the neural-based error"}]}