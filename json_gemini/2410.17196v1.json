{"title": "VoiceBench: Benchmarking LLM-Based Voice Assistants", "authors": ["Yiming Chen", "Xianghu Yue", "Chen Zhang", "Xiaoxue Gao", "Robby T. Tan", "Haizhou Li"], "abstract": "Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.", "sections": [{"title": "1 Introduction", "content": "Advancements in large language models (LLMs) have led to remarkable breakthroughs across a wide range of natural language processing (NLP) tasks. Recently, LLMs have further expanded their capabilities by incorporating multi-modal processing abilities, such as vision (Liu et al., 2024; Chen et al., 2024c) and audio (Chu et al., 2023; Zhang et al., 2023). Notably, voice assistants powered by audio-based LLMs, e.g., GPT-4o, have garnered significant research interest (D\u00e9fossez et al., 2024; Li et al., 2024a). These voice assistants are designed to understand and respond to spoken instructions, enabling more natural, flexible, and high-quality speech interactions compared to traditional text-based systems. This advancement has the potential to significantly improve user experiences across various applications, e.g., virtual customer service.\nDespite the promising potential of LLM-based voice assistants, the absence of a standardized benchmark for evaluating these systems limits a comprehensive understanding of their performance and areas for improvement. Current evaluations predominantly focus on automatic speech recognition (ASR) (Chen et al., 2024a; Xie and Wu, 2024a,b) or spoken question answering tasks synthesized with high-quality text-to-speech (TTS) models (Fang et al., 2024; Fu et al., 2024). While informative, this narrow scope does not provide a holistic and reliable assessment of system capabilities. Furthermore, the transition from text-based to speech-based interactions introduces several real-world challenges, as human perception of speech is heavily influenced by speaker characteristics (Krause and Braida, 2004), environmental factors (Meyer et al., 2013), and the complexity of spoken contents (Shriberg, 1994). Current evaluations, which mostly rely on clean speeches, fail to capture these complexities adequately.\nTo address this gap, we introduce a new benchmark, VoiceBench, which provides a comprehensive evaluation framework for LLM-based voice assistants. VoiceBench evaluates various capabilities of these assistants by using both synthetic and real spoken instruction data to assess general knowledge, instruction-following abilities, and safety measurement. Additionally, we design test cases to challenge voice assistants in distinct speaker styles, environmental conditions, and content variations. Specifically, we leverage advanced TTS and voice cloning models to generate speech samples with diverse speaker properties, such as age, accent"}, {"title": "2 Background", "content": "A common approach to augment LLMs with speech understanding capabilities is to implement pipeline models that first transcribe users' speech into text via ASR systems. The transcribed text is then passed to LLMs to generate responses. However, it's argued that pipeline models may lose important information during the transcription process and often suffer from reduced efficiency (Fang et al., 2024; Xie and Wu, 2024a). To overcome these limitations, various end-to-end audio LLMs have been developed (Chu et al., 2024; Tang et al., 2023), which integrate speech encoders with LLMs via speech adapters (Fang et al., 2024; Xie and Wu, 2024a), enabling fully optimized end-to-end speech processing.\nBuilding on these audio LLMs, two main applications have emerged: audio analysis (Gong et al., 2024; Chu et al., 2023) and voice assistants (Held et al., 2024; Fu et al., 2024; Chen et al., 2024a; Li et al., 2024a). In audio analysis, models are designed to answer text-based instructions by interpreting input audio contexts. Conversely, in voice assistant applications, models are required to directly respond to spoken questions without relying on text instructions. While there are several established benchmarks for evaluating audio analysis models, such as AIR-Bench (Yang et al., 2024; Chen et al., 2024b; Wang et al., 2024b), there is currently no standardized benchmark tailored to evaluating voice assistants. Existing evaluations of voice assistants have primarily focused on ASR tasks (Fu et al., 2024; Chen et al., 2024a; Xie and Wu, 2024b; D\u00e9fossez et al., 2024; Li et al., 2024a) or on assessing general knowledge using clean, spoken question-answering datasets (Fang et al., 2024; Fu et al., 2024), which do not provide a comprehensive evaluation of the models' capabilities.\nIn the benchmarking of text-based LLMs, models are typically evaluated not only for their general knowledge (Wang et al., 2024c; Myrzakhan et al., 2024), but also for their ability to follow instructions (Zhou et al., 2023; Zeng et al., 2024) and for the harmlessness of their responses, ensuring safe deployment (Ji et al., 2024; Liu et al., 2023). Motivated by this, we introduce VoiceBench, a benchmark designed to evaluate voice assistants on three key aspects: general knowledge, instruction-following ability, and safety. In addition, VoiceBench incorporates real-world challenges faced by LLM-based voice assistants, including interactions with perturbed speeches in diverse scenarios, speeches with distinctive speaker characteristics, and speeches with content noises."}, {"title": "3 VoiceBench", "content": "Fig. 1 presents an overview of the proposed VoiceBench, which consists of two main components. First, we assess the capability of voice assistants by constructing spoken instruction datasets that cover various dimensions, including general knowledge, instruction-following tasks, and safety considerations. Second, given the inherent variability in speech, we evaluate the robustness and generalization of voice assistants across different conditions. These variations encompass speaker-related differences (e.g., age-varied speech), environmental factors (e.g., background noise), and content-related variations (e.g., disfluencies in speech). In this section, we provide details on the construction process of spoken instructions. We present the analyses of speaker variation in Sec. 4, environment variation in Sec. 5, and content variation in Sec. 6."}, {"title": "3.1 Dataset Construction", "content": "Text instructions: To evaluate the general knowledge of voice assistants, we incorporate three datasets: AlpacaEval, CommonEval, and SD-QA. For AlpacaEval (Li et al., 2023), we follow LLaMA-Omni (Fang et al., 2024) by retaining the helpful_base and vicuna subsets, and removing questions related to mathematics and coding. For CommonEval, we manually collect information-seeking questions from CommonVoice (Ardila et al., 2020), which features speech recorded in realistic settings by a diverse range of speakers using their personal devices. SD-QA (Faisal et al., 2021) consists of spoken questions with varying accents from the TyDi-QA dataset (Clark et al., 2020). In its original format, models are required to answer these questions using context passages. However, in voice interaction scenarios, providing extensive context is often impractical. Therefore, we feed only the spoken questions to voice assistants, requiring them to respond using their internal knowledge. Upon reviewing the dataset, we observe that some questions, such as \"How many people live in Dallas?\", require up-to-date information that frequently changes and cannot be answered without additional context. To ensure fair evaluation, we retain only those questions that could be answered using the internal knowledge of large language models (LLMs). Specifically, we prompt three advanced LLMs\u2014Claude-3.5 (Anthropic, 2024), Gemini-1.5-Pro (Reid et al., 2024), and GPT-4o (Achiam et al., 2023)-to answer these questions, and we manually select those questions that received correct answers from at least one of the models.\nTo evaluate the instruction-following capabilities of voice assistants, we use the IFEval dataset (Zhou et al., 2023), in which models are required to answer questions according to a specific format. To make the data more suitable for voice interaction, we retain only samples containing fewer than 50 words, and exclude instructions that involve elements difficult to convey via speech. Finally, to assess the safety of voice assistants, we utilize AdvBench (Zou et al., 2023), a dataset containing instructions designed to elicit harmful responses. In these cases, a safe voice assistant is expected to refuse to answer such prompts.\nSpeech instructions: Due to the high cost of generating real spoken instructions, relying solely on actual speech to evaluate voice assistants comprehensively is challenging. As a result, after preparing the text-based instructions, we convert them into speech using text-to-speech (TTS) models. Some text elements, such as special characters, are difficult to pronounce when directly converted to speech. Therefore, we first transform the text into a spoken-friendly format. Given the strong performance of LLMs in TTS text normalization (Zhang"}, {"title": "3.2 Experiment Setup", "content": "Examined models: We evaluate various end-to-end voice assistants on the proposed VoiceBench, including Qwen2-Audio (Chu et al., 2024), LLaMA-Omni (Fang et al., 2024), Mini-Omni (Xie and Wu, 2024a), VITA (Fu et al., 2024), and DiVA (Held et al., 2024). Additionally, we build a naive voice assistant pipeline, where an automatic speech recognizer transcribes the input speech query into text, and a text-only LLM generates a response based on the transcribed query. The architectures of the evaluated models are summarized in Tab. 2.\nEvaluation metrics: Since the focus of this work is to assess the quality of output content, and not all voice assistants support speech output, we directly assess the quality of text responses instead"}, {"title": "3.3 Results", "content": "The results of the VoiceBench evaluation are summarized in Table 3, leading to several key findings.\nFirstly, the naive pipeline-based voice assistant significantly outperforms all end-to-end models on spoken instructions, with a margin exceeding 10 points. Among the end-to-end models, Mini-Omni underperforms significantly due to its use of a smaller speech encoder and base LLM, which prioritize processing efficiency over performance. Notably, while some end-to-end models, such as Qwen2-Audio (Chu et al., 2024), demonstrate comparable or even superior ASR and audio analysis performance (Yang et al., 2024) compared to models such as Whisper (Radford et al., 2023) or the naive pipeline, a significant performance gap persists when handling spoken instructions in our AudioBench evaluation. This highlights a major limitation in current voice assistant evaluations, which rely heavily on ASR metrics.\nSecondly, inadequate training of voice assistants can greatly impair the text processing capabilities of large language models (LLMs). For example, LLaMA-Omni and the Naive baseline both utilize the same base LLM, yet LLaMA-Omni exhibits a significant performance drop of over 11 points across all text processing tasks after additional tuning. This performance degradation is particularly severe on instruction-following tasks, which aligns with previous research indicating that end-to-end audio LLMs often struggle with instruction following (Yang et al., 2024; Chen et al., 2024b).\nThirdly, we observe a notable disparity between the text and speech processing abilities of current models. The naive pipeline model shows a relatively small performance gap of 5.77 points from text to speech instructions, primarily due to ASR errors introduced by the speech recognition sub-model within the pipeline. In contrast, end-to-end models exhibit much larger gaps. For instance, VITA demonstrates a performance gap exceeding 35 points when taking text and speech inputs.\nFourthly, we identify potential safety concerns with some voice assistants in voice interaction mode. While all assistants exhibit robust behavior when handling malicious instructions in text form, several models, such as Mini-Omni, fail to reject malicious instructions when they are delivered in speech form, responding directly instead.\nFinally, to examine the reliability of using synthetic data to perform evaluation, we list the performance of voice assistants on real and synthetic CommonEval in Tab. 4. The performance rank on synthetic data and real data shows a strong correlation, while there still exists some discrepancy. Notably, all models achieve better performance on synthetic data. In particular, VITA has around 50% performance improvements when switching to synthetic data. Since CommonEval speeches are recorded with personal devices, instead of professional studio devices. The real speeches are usually more noisy than synthetic data, which is more challenging. These findings suggest that VITA may struggle to perform effectively during real user interactions, reinforcing our motivation to simulate real-world speech variations to thoroughly evaluate voice assistants."}, {"title": "4 Speaker Variations", "content": "Human perception of speech is influenced by speaker-specific properties, such as accent (Bradlow and Bent, 2008) and speaking rate (Krause and Braida, 2004), introducing additional complexity compared to text. These variations could similarly affect the performance of voice assistants. Motivated by this, we conduct an in-depth analysis of various speaker variations, including speaking speed, speaker age, volume, pitch, and accent, to assess their impact on voice assistants.\nFor speaking speed, speaker age, volume, and"}, {"title": "4.2 Results", "content": "The impact of speaking speed, speaker age, pitch, and volume is summarized in Fig. 2. All voice assistants demonstrate a degree of resilience to speaker variations. The model performance is not linearly correlated with the variation level. Instead, the model tends to maintain consistent performance in a range, and start decaying when meeting a certain threshold. Speaker age, volume, and pitch have limited impact on model performance, with most voice assistants, except VITA, maintaining consistent performance over a wide range of variations. Performance drops are only observed when the volume is exceptionally low. VITA, however, shows a degradation in performance when processing child or high-pitched speech. Since child speech typically has a higher pitch than adult speech, this finding is consistent with expectations. Speaking speed has a relatively greater influence on model performance, particularly for end-to-end models. These models show significant degradation at speeds below 0.5x or above 1.5x, whereas the naive model remains stable across a broader range of speeds, from 0.25x to 2.0x.\nThe accent results from SD-QA are presented in Fig. 3, while the accent results from AlpacaEval are summarized in Fig. 4. Both synthetic and real accent results exhibit similar trends. Overall, Mini-Omini exhibits the worst performance in response"}, {"title": "5 Environmental Variations", "content": "When talking to voice assistants, different background environment variations pose a significant challenge to accurately understanding and responding to users' queries (Sainath et al., 2017; Afouras et al., 2018; Ephraim and Malah, 1984). However, current evaluations lack specificity regarding noisy scenarios, which are the most common in real-world applications where voice assistants are deployed, such as in homes, vehicles, and public spaces. To benchmark the robustness of voice assistants, we conduct a thorough analysis across various noisy conditions, including far-field speech, signal distortion, reverberation, packet loss transmission, and noise interference.\nSpecifically, when the user is speaking from a distance, the speech signal weakens and high-frequency components are often attenuated due to air absorption and environmental reflection (Kumatani et al., 2012). To simulate this effect, we"}, {"title": "5.2 Results", "content": "The performance of voice assistants under different environmental conditions are summarized in Fig. 5. Similar to the observations of speaker variations, all models equipped with whisper speech encoders demonstrate similar levels of resilience across vari"}, {"title": "6 Content Variations", "content": "Compared to written text, spoken language tends to be more informal and casual, often containing various errors such as disfluencies (Tree, 1995; Shriberg, 1994; Jamshid Lou and Johnson, 2020; Marie, 2023), mis-pronunciation (Kheir et al., 2023; Dell and Reich, 1981), and grammar error (Carter and Mncarthy, 1995; McCarthy and Carter, 1995; Caines et al., 2020). These errors are common in natural speech and can have a significant impact on the performance of voice assistants. However, current evaluations of voice assistants often focus on clean data, overlooking these frequent speech errors. In this section, we analyze the effects of common speech content errors on voice assistant performance. Specifically, we examine mispronunciations, grammatical errors, and a range of common disfluencies, including repairs, repetitions, filled pauses, interjections, and false starts. Typical examples of each content error type are provided in Tab. 6. Due to the scarcity of instruction data containing such errors, we leverage GPT-4o to rewrite clean instructions into noisy versions using a few-shot demonstration approach. The in"}, {"title": "6.2 Results", "content": "The performance of voice assistants under various content errors is summarized in Tab. 5. Overall, naive pipeline models demonstrate the best robustness in handling content errors. All voice assistants show strong resilience to grammatical errors but are much more vulnerable to mispronunciations. Mispronunciations often result in a large number of incorrectly recognized words, leading to a higher Word Error Rate (WER) in the transcription, which can alter the intended meaning of the speech. In contrast, grammatical errors tend to preserve the overall meaning, leading to less disruption in performance. Additionally, LLMs also display a high tolerance for grammatical errors but exhibit much less resilience to high WER, likely because grammatical mistakes are common in written text, while incorrect word recognition is not (Wang et al., 2024a). The vulnerability of base LLMs can also help explain the observed trends in performance. Similarly, spoken disfluencies\u2014commonly absent in written text-significantly degrade model performance. Disfluencies can be considered as including irrelevant content, which can easily distract"}, {"title": "7 Conclusion", "content": "In this work, we introduce the first comprehensive multi-facet benchmark to assess the capabilities of voice assistants using both real and synthetic spoken instructions. Our results highlight a significant performance gap between end-to-end models and straightforward pipeline models, underscoring the need for further advancements in processing spoken instructions effectively. Additionally, we uncover key vulnerabilities in voice assistants by evaluating their performance across various factors, including speaker variations, environmental conditions, and content-related errors. These findings suggest important areas for improvement in voice assistant robustness. Future work includes developing evaluation protocols for speech-based responses and extending the benchmark to incorporate more diverse and realistic evaluation data.."}]}