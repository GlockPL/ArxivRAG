{"title": "Can Pose Transfer Models Generate Realistic Human Motion?", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "abstract": "Recent pose-transfer methods aim to generate temporally consistent and fully controllable videos of human action where the motion from a reference video is reenacted by a new identity. We evaluate three state-of-the-art pose-transfer methods\u2014AnimateAnyone, MagicAnimate, and ExAvatar\u2014by generating videos with actions and identities outside the training distribution and conducting a participant study about the quality of these videos. In a controlled environment of 20 distinct human actions, we find that participants, presented with the pose-transferred videos, correctly identify the desired action only 42.92% of the time. Moreover, the participants find the actions in the generated videos consistent with the reference (source) videos only 36.46% of the time. These results vary by method: participants find the splatting-based ExAvatar more consistent and photorealistic than the diffusion-based AnimateAnyone and MagicAnimate.", "sections": [{"title": "1. Introduction", "content": "Pose transfer methods aim to generate images or videos where a novel human identity (target) is shown performing an action consistent with a reference action image or video (source) [7]. Beyond its academic contributions to human body modeling, many applications of this technology have already been identified [15]: in the entertainment industry, these methods have the potential to enhance human body animation, accelerate production of stunt doubles, and enhance personalization [11, 43]; in healthcare, they are promised to enhance posture therapy [4]; and in the fashion industry, these methods will enable virtual try-on where customers shopping online see how a piece of clothing would look on them [5,34].\nSpurred by the potential of the technology, there has been a surge of methods addressing this problem in the literature. In the video pose transfer domain, in particular, the methods fall under two categories based on the underlying architecture: (1) methods based on diffusion (most prominently, AnimateAnyone [9] and MagicAnimate [38]) and (2)"}, {"title": "2. Related Work", "content": "This section reviews the current state of pose transfer modeling and the evaluation of its out-of-distribution (OOD) performance. Note that, deep learning methods for pose estimation and human understanding have largely paved the way for current pose transfer methods [16], and so a comprehensive review of these foundational tasks would be granted. Similarly, a discussion of pose transfer applications in downstream tasks could reveal innovative techniques for enhancing performance in specific contexts [2, 14]. However, such discussions are beyond the scope of this paper, and we thus refer readers to relevant comprehensive surveys [3, 7,41,42]."}, {"title": "2.1. Image-level Pose Transfer", "content": "Early deep learning approaches to image-level pose transfer relied on generative adversarial networks (GANs) conditioned on the source image and target pose [21]. Deformable GANs, introduced later, leveraged deformable skip connections to handle large pose variations [32]. The emergence of Transformer-based architecture popularized the use of attention mechanisms in both natural language processing and computer vision domains. Pose transfer methods followed this trend, and so methods that used attention mechanisms to process sequences of poses arrived soon thereafter [43], enabling more detailed and coherent pose transformations. While shared representations of source poses and target human identities had been believed to be crucial for pose transfer, Wu et al. showed that disentangling pose and appearance features improves pose transfer results [37], reducing pose ambiguity and appearance inconsistency. Finally, recent work further explored this disentanglement by, for example, permuting image patches to isolate pose from texture [17]."}, {"title": "2.2. Video-level Pose Transfer", "content": "Pose transfer methods for video generation evolved by adapting image-level architectures to include a temporal dimension. Early approaches, such as Liquid Warping GAN [20], introduced a dense 3D flow field that wraps source images, enabling pose transfer across video frames, even from different camera views. The temporal dimension introduced new challenges that had not been addressed by the image-level pose transfer literature, such as pose robustness and cloth dynamics. For instance, Ren et al. addressed pose robustness issues through pose augmentations in a two-stage network [27], while Kappel et al. tackled cloth dynamics by incorporating clothing-specific modules into the architecture and new training objectives [12]. Hybrid approaches combining deep learning with computer graphics concepts have also been explored [39].\nThe recent boom in generative AI has led to diffusion-based methods applied for video-level pose transfer. Early diffusion-based approaches to video-level pose transfer relied on fixed seeds or CLIP embeddings [13, 26]. These methods struggled with temporal consistency and artifacts like flickering [18, 28, 29, 36]. MagicAnimate [38] addressed some of these issues by employing temporal attention blocks, an appearance encoder, and a video fusion technique. However, its reliance on DensePose representations [6] limited the transfer quality of fine details such as fingers. AnimateAnyone [9] improved upon this by using skeletal representations instead of DensePose, enabling the transfer of finer detail and more stability. Still, some issues with temporal consistency and poor generalization to novel environments persisted. The latest methods have thus aimed to combine generative AI techniques with concepts from computer graphics and address these challenges. Most prominently, ExAvatar [22] leverages an expressive whole-body 3D Gaussian representation built on top of an ensemble of body representations. By benchmark performance, MagicAnimate, AnimateAnyone, and ExAvatar represent the state of the art in video-level pose transfer."}, {"title": "2.3. Out-of-Domain Evaluation", "content": "The scholarship examining performance of pose transfer methods under OOD conditions remains limited. One study explored the OOD performance of image-level pose transfer by analyzing the datasets used for model training and inference [1]. Other research has examined OOD per-"}, {"title": "3. Data and Methods", "content": "This section outlines how we generated pose-transferred videos for our study. For each pose transfer method examined-AnimateAnyone [9], MagicAnimate [38], and ExAvatar [22]-the data preparation involved three steps: (1) collecting reference action videos (see Section 3.1), (2) collecting novel human identities (see Section 3.2), and (3) generating new videos using pose transfer (see Section 3.4). Step 1 was consistent across all methods; Steps 2 and 3 varied depending on the specific pose transfer method.\nFor each pose transfer method, a total of 840 videos were generated (42 videos per class). From these, 22 videos were later selected for the survey: 10 videos for Task 1, another 10 videos for Task 2, and 2 videos for Task 3 (see Section 4 for survey details). The reference action videos and novel human identities were sampled at random while avoiding"}, {"title": "3.1. Reference Action Videos", "content": "A subset of 60 videos spanning 20 classes from the UCF101 [35] human action video dataset was used as our reference action (source) video set. UCF101 [35] is a large-scale action recognition dataset comprising 101 action classes, more than 13,000 videos, and 27 hours of footage. The dataset, scraped from YouTube in 2012, consists of predominantly low-resolution videos. Representative ex-"}, {"title": "3.2. Novel Human Identities", "content": "To specify the novel human identity (target), the diffusion-based methods employed in our study, AnimateAnyone and MagicAnimate, expect a static image of the identity. The splatting-based method, ExAvatar, on the other hand, expects a video of the identity. We, thus, describe the method used for each type of pose transfer separately.\nAnimateAnyone and MagicAnimate. AI-generated"}, {"title": "3.3. Distribution Analysis", "content": "To ensure the suitability of the employed datasetsUCF101 and RANDOM People for an OOD evaluation, we performed a t-SNE analysis [8] comparing these datasets to the training datasets of the evaluated pose transfer meth-ods. Specifically, we compared them against the Tik-Tok Dataset [10], used to train MagicAnimate, and the X-Humans Dataset [30], used to train ExAvatar. The dataset used to train AnimateAnyone was not disclosed by its au-thors."}, {"title": "3.4. Pose Transfer", "content": "Given a reference action video (source) and a novel human identity image or video (target), each examined pose transfer method generated a video in which the novel human identity reenacted the action shown in the reference action video.\nAnimateAnyone. This method represents the novel human identity as an image (in our case, sourced by SD XL images) and integrates ReferenceNet within its diffusion-based architecture to enhance temporal consistency of the novel human identity (target). Due to the absence of an official implementation, an open-source implementation of AnimateAnyone, which has been shown to have on-par or better performance, was used. Examples of representative"}, {"title": "4. Survey Methodology", "content": "We recruited 16 participants through snowball sampling to complete a survey with 66 questions hosted on Qualtrics. This survey comprised two tasks with quantitative questions (2 \u00d7 30 questions in total) and one task with qualitative questions (6 questions). All videos used in the survey were chosen randomly; each participant saw the same videos in the same order. There is an equal number of videos gener-ated by each of the pose transfer methods: AnimateAnyone, MagicAnimate, and ExAvatar. The wording of participant instructions and questions are presented in Appendix B.\nTask 1: Generated Action Evaluation. This section"}, {"title": "Task 2: Transfer Control Evaluation.", "content": "This section contained 30 questions that presented participants with two videos: a pose-transferred video and its source video. The questions then ask the participants to indicate if the action in the pose-transferred video is consistent with the source video; the participants could label the pair as 'consistent', 'somewhat consistent', or 'inconsistent'."}, {"title": "Task 3: Qualitative Evaluation.", "content": "This section contained 6 questions that presented participants with a single video and asked them a series of free-form questions about its quality."}, {"title": "5. Results", "content": "This section presents the results of our survey, conducted over two weeks in November 2024. To enhance the reproducibility of our work, we open-source the response data in full at https://github.com/matyasbohacek/pose-transfer-human-motion.\nTask 1: Generated Action Evaluation. Overall, the correct action (out of 20 action classes) was selected 42.92% of the time. When broken down to individual pose transfer methods, the participants selected the correct action"}, {"title": "Task 2: Transfer Control Evaluation.", "content": "The results are shown in Table 1. Overall, the video pairs were labeled consistent 36.46% of the time, partially consistent 25.00% of the time, and inconsistent 42.43% of the time. When broken down to individual pose transfer methods, MagicAnimate videos were deemed the least consistent-participants labeled them consistent only 26.88% of the time, partially consistent 31.25% of the time, and inconsistent 41.88% of the time. The AnimateAnyone videos came slightly above: participants labeled them consistent 27.50% of the time, partially consistent 25.62% of the time, and inconsistent 46.88% of the time. Finally, the ExAvatar videos come on top: participants labeled them as consistent 55.00% of the time, partially consistent 18.12% of the time, and as inconsistent 38.54% of the time."}, {"title": "Task 3: Qualitative Evaluation.", "content": "Participants commonly noted that the videos across all pose transfer methods lacked photorealism and consistency. For example, P14 reacted to Q3.1 with \"not photorealistic, [I] dont know what he is doing\", and P12 said in reaction to Q3.4 \"that is not photorealistic, it [is] not very consistent\". Interestingly, some participants said that, despite these shortcomings, the action was recognizable to them. For example, in response to Q3.1, P13 shared that \"The person is not photorealistic. Action seems to be jumping jack but might be something else.\" In response to Q3.2, P11 said that \"His movements are so small and slow that I can't tell what he's doing. It looks like Tai Chi, but if I didn't know it was supposed to be Tai Chi, I wouldn't have guessed.\"\nWhile the responses were mostly similar across the pose transfer methods, one difference emerged between the diffusion-based methods, AnimateAnyone and MagicAnimate, and the splatting-based ExAvatar: the videos generated by the diffusion-based methods were often found to have structural deficiencies in the generated human bodies. For example, P2 pointed out that the videos evaluated in Q3.1 and Q3.2 show a \"disorted face\" and \"disorted hands\u201d. In reaction to Q3.3, P8 said that \"another person appears behind (...) and it's weird.\" For Q3.4, they added that \"The hair changes and she's missing one foot.\u201d The ExAvatar videos, on the other hand, have not been found to evince similar artifacts by the participants."}, {"title": "6. Discussion", "content": "The results of our survey reveal that, in a controlled environment of 20 distinct OOD human actions, state-of-the-art pose transfer methods largely fail to generate consistent and photorealistic videos that convey understandable actions. When asked to select which action (out of a list of 20 actions) is most likely performed in the MagicAnimate videos, the participants selected the correct action only 13.12% of the time. This is particularly striking given that, had the participants selected the action randomly, they would have gotten the correct action 5% of the time. The other diffusion-based method we evaluated, AnimateAnyone, seemed to convey the semantics of the action better, as the participants selected the correct action 47.50% of the time. Interestingly, despite this stark difference in the ability to select the correct action (Task 1), the participants perceived the consistency of the source video and the generated video (Task 2) for these two methods similarly. In particular, the AnimateAnyone and MagicAnimate videos were deemed consistent 27.50% and 26.88% of the time, partially consistent 25.62% and 31.25% of time, and inconsistent 46.88% and 41.88% of time, respectively. We speculate that, despite the AnimateAnyone videos conveying the action semantics better, the presence of undesired artifacts still leads participants to fixate on the poor generation quality.\nThe ExAvatar method outperformed its counterparts in our study, with participants selecting the correct action 68.12% of the time and labeling 55.00% of the videos as consistent. However, a significant amount of the videos, 38.54%, were still labeled as inconsistent, on par with AnimateAnyone and MagicAnimate, whose videos were labeled inconsistent 46.88% and 41.88% of the time, respectively. While this splatting-based method mitigates many render-ing issues observed in diffusion-based methods such as implausible body poses and inconsistent identities-its photorealism and fidelity still require improvement for the actions to be more recognizable.\nTaken together, these results suggest that the splatting-based ExAvatar produces more photorealistic, consistent, and controllable human motion than the diffusion-based AnimateAnyone and MagicAnimate. This, however, comes at the cost of a more demanding input (novel human identity reference): while AnimateAnyone and MagicAnimate require only a static image, ExAvatar requires a video in which the protagonist performs a sequence of movements described in [22].\nIn either case, there remains much work to make pose transfer consistent and photorealistic, while minimizing the input demands. We stipulate that combining 3D representations (and, more broadly, traditional computer graphics methods) with recent generative AI methods holds much promise in addressing these issues. In particular, such meth-ods can leverage the high controllability and consistency of"}, {"title": "7. Conclusion", "content": "State-of-the-art pose transfer methods continue to face challenges in generating consistent and photorealistic videos that accurately preserve the motion semantics of source videos. Among the evaluated methods, we found videos generated by the splatting-based ExAvatar more consistent and photorealistic than videos generated by the diffusion-based AnimateAnyone and MagicAnimate.\nOur participant survey highlights several key areas for future improvement in pose transfer methods: (1) enhancing photorealism, (2) maintaining greater consistency with source videos, and (3) improving the fidelity of novel human identities. Moreover, given the saturation of existing benchmarks, we underscore the need for a new pose transfer benchmark that would evaluate the generalization abilities of pose transfer methods moving forward."}, {"title": "A. Selected Action Classes", "content": "The following classes from UCF101 were used for the reference action videos:\n1. Apply Lipstick\n2. Archery\n3. Body Weight Squats\n4. Clean And Jerk\n5. Golf Swing\n6. Hammer Throw\n7. Hammering\n8. Hand Stand Pushups\n9. Hula Hoop\n10. Jumping Jack\n11. Lunges\n12. Mopping Floor\n13. Pull Ups\n14. Shot Put\n15. Soccer Juggling\n16. Table Tennis Shot\n17. Tai Chi\n18. Tennis Swing\n19. Throw Discus\n20. Writing On Board"}, {"title": "B. Survey Instructions and Questions", "content": "The following instructions and questions were presented to participants in Qualtrics:\nIntroduction. This survey examines the quality of generative AI models for human action generation. You will be presented with questions about the quality and semantics of human action videos. Please play each video carefully before you answer. Videos may be rewatched without re-striction.\nSection 1. The following questions will ask you to classify actions in short videos out of a set of 20 activity names. Choose the activity name that you think best describes the shown action. At each step, play the video and then answer the question.\nQ1.1-Q1.20. Which of the following activity names best describes the action shown in the video above?\nSection 2. The following questions will ask you to decide if pairs of short videos show consistent activities. As such, the precise body movement must not be synchronized; it is only the overall activity name that must be the same. For example, if the videos will both show a person sitting down, that would be a constant activity. On the contrary, if one video shows a person sitting down and the other shows a person riding a horse, those would be inconsistent activities. At each step, play both videos and then answer the question.\nQ2.1-Q2.20. Are the activities shown in the two videos above consistent?\nSection 3. This last section will ask you to provide free-form evaluation of the quality of five short videos. Comment on any or all of the following questions. At each step, play the video and then answer the question. 1. Is the person shown in the video photorealistic - that is, is their body rendered properly and is the identity consistent through the video? 2. Is the person shown performing a clearly identifiable action? 3. Do you have any other comments that come to mind?"}, {"title": "C. Responses to the Qualitative Evaluation", "content": "The complete set of free-form participant responses to Task 3: Qualitative Evaluation, which includes Q3.1, Q3.2, Q3.3, Q3.4, Q3.5, and Q3.6, is presented in the following three tables, organized by the model used to generate the evaluated videos. Table 2 presents the results for AnimateAnyone; Table 3 presents the results for MagicAnimate; and Table 4 presents the results for ExAvatar. This data is also available in CSV format at https://github.com/matyasbohacek/pose-transfer-human-motion."}]}