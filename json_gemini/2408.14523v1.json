{"title": "Retrieval Augmented Generation for Dynamic Graph Modeling", "authors": ["Yuxia Wu", "Yuan Fang", "Lizi Liao"], "abstract": "Dynamic graph modeling is crucial for analyzing evolving patterns in various applications. Existing approaches often integrate graph neural networks with temporal modules or redefine dynamic graph modeling as a generative sequence task. However, these methods typically rely on isolated historical contexts of the target nodes from a narrow perspective, neglecting occurrences of similar patterns or relevant cases associated with other nodes. In this work, we introduce the Retrieval-Augmented Generation for Dynamic Graph Model-ing (RAG4DyG) framework, which leverages guidance from contextually and temporally analogous examples to broaden the perspective of each node. This approach presents two critical challenges: (1) How to identify and retrieve high-quality demonstrations that are contextually and temporally analogous to dynamic graph samples? (2) How can these demonstrations be effectively integrated to improve dynamic graph modeling? To address these challenges, we propose RAG4DyG, which enriches the understanding of historical contexts by retrieving and learning from contextually and temporally pertinent demonstrations. Specifically, we employ a time- and context-aware contrastive learning module to identify and retrieve relevant cases for each query sequence. Moreover, we design a graph fusion strategy to integrate the retrieved cases, thereby augmenting the inherent historical contexts for improved prediction. Extensive experiments on real-world datasets across different domains demonstrate the effectiveness of RAG4DyG for dynamic graph modeling.", "sections": [{"title": "1 Introduction", "content": "Dynamic graphs, which track the temporal evolution of nodes and edges, are essential for analyzing complex systems across domains like social networks, citation networks etc. Existing methods for dynamic graph modeling fall into two main categories: discrete-time and continuous-time approaches (Feng et al. 2024). Discrete-time methods divide the timeline into intervals, capturing the graph's state at each point (Sankar et al. 2020), but they lack temporal granularity by overlooking event dynamics within intervals. Continuous-time approaches, on the other hand, model temporal dynamics continuously, without discretization, allowing for more detailed representations (Trivedi et al. 2019; Xu et al. 2020; Wen and Fang 2022). Our work aligns with continuous-time approaches, aiming to capture fine-grained time steps for more accurate dynamic graph modeling.\nTraditional approaches for dynamic graph modeling predominantly rely on integrating graph neural networks (GNNs) with specialized temporal modules, such as recurrent neural networks (Pareja et al. 2020), self-attention (Sankar et al. 2020) and temporal point processes (Wen and Fang 2022), which update the graph representations over time. Although powerful, these methods suffer from the inherent limitations of GNNs, such as difficulty with long-term dependencies, and over-smoothing or over-squashing issues (Chen et al. 2020; Alon and Yahav 2020). Recently, a simple and effective approach called SimpleDyG (Wu, Fang, and Liao 2024) redefines dynamic graph modeling as a generative sequence modeling task and leverages the strengths of sequence models like Transformers to capture long-range dependencies within temporal sequences effectively.\nDespite significant advancements in dynamic graph modeling, current approaches heavily depend on isolated historical contexts of the target nodes, limiting each node's perspective to its ego network. The limited context makes it difficult to quickly adapt to diverse emerging patterns. For example, in a social network, when users transit to new behavioral patterns or when new users with minimal historical interactions are introduced, traditional models that focus on the historical contexts of individual nodes might not respond effectively to these changes. To overcome these limitations, we draw inspiration from the Retrieval-Augmented Generation (RAG) technique originated from the Natural Language Processing (NLP) field (Gao et al. 2023). As shown in Fig. 1(a), RAG broadens the context through the retrieval of additional demonstrations, achieving considerable success in NLP. In this work, we aim to integrate RAG into dynamic graph modeling to incorporate a broader and more relevant contextual understanding beyond the historical interactions of individual nodes. However, leveraging RAG for dynamic graph modeling presents two critical challenges.\nFirst, how do we harvest high-quality demonstrations for dynamic graphs? This involves selecting the most contextually and temporally relevant samples to enrich the current state of a target node. Unlike RAG for NLP, where external textual data and a pre-trained language model (LM) naturally support demonstration retrieval as shown in Fig. 1(a), dynamic graph modeling requires careful consideration of the data sources and the ability to handle complex structural and temporal patterns as shown in Fig. 1(b). To address this, we introduce a novel time-aware contrastive learning strategy that leverages internal training data for demonstration retrieval. Following SimpleDyG (Wu, Fang, and Liao 2024), we regard dynamic graph modeling as a sequence modeling task and the prediction of future events as a sequence generation problem. Hence, we define a query sequence for a target node as its historical interaction sequence. Next, we automatically annotate the retrieval training pool by assessing the similarity to the query. Given the training pool, our model consists of two contrastive modules: time-aware and context-aware contrastive learning. The former utilizes a time decay function to prioritize samples that are temporally close to the query, while the latter incorporates data augmentation techniques such as masking and cropping to improve the model's ability to retrieve complex structural patterns.\nSecond, how do we effectively integrate the retrieved demonstrations into the dynamic graph model? This involves fusing the retrieved demonstrations to enrich the historical context of each target node and enhance subsequent predictions. Directly concatenating the retrieved demonstrations with the query sequence not only results in a lengthy sequence unsuited for generative predictions but also overlooks the structural patterns among the retrieved demonstrations. To address this, we propose graph fusion, leveraging the graph structures inherent to the demonstrations and fusing them into a summary graph, as shown in Fig. 1(b). We then apply a GNN-based readout to learn the representation of the fused summary graph, which is prepended to the query sequence before being fed into the sequence generation model.\nOur contributions can be summarized as follows. (1) We propose a novel Retrieval-Augmented Generation approach for Dynamic Graph modeling named RAG4DyG. It employs a retriever to broaden the historical interactions with contextually and temporally relevant demonstrations, leading to enhanced generative predictions on dynamic graphs. (2) We propose a time-aware contrastive learning module that incorporates temporal and structural information for demonstration retrieval. Additionally, we design a graph fusion module to effectively integrate the retrieved demonstrations, leveraging structural enhancements for subsequent predictions. (3) We conduct extensive experiments to validate our approach, demonstrating the effectiveness of RAG4DyG in various domains."}, {"title": "2 Related Work", "content": "Dynamic Graph Modeling. Existing approaches for dy-namic graphs can be categorized into discrete-time and continuous-time methods. Discrete-time methods regard a dynamic graph as a sequence of static graph snapshots captured at various time steps. Each snapshot represents the graph structure at a specific time step. These methods typically adopt GNNs to model the structural information of each snapshot, and then incorporate a sequence model (Pareja et al. 2020; Sankar et al. 2020) to capture the changes across snapshots. However, these approaches neglect fine-grained time information within a snapshot. In contrast, continuous-time methods model graph evolution as a continuous process, capturing all time steps for more precise temporal modeling. These methods often integrate GNNs with specially designed temporal modules, such as temporal random walk (Wang et al. 2021), temporal graph attention (Xu et al. 2020; Rossi et al. 2020), MLP-mixer (Cong et al. 2022) and temporal point processes (Trivedi et al. 2019; Ji et al. 2021; Wen and Fang 2022). Recently, researchers have proposed a simple and effective architecture called SimpleDyG (Wu, Fang, and Liao 2024), which reformulates the dynamic graph modeling as a sequence modeling task. Specifically, it maps the dynamic graph into a series of node sequences and feeds them into a generative sequence model. Subsequently, predicting future events can be framed as a sequence generation problem.\nRetrieval Augmented Generation. Recently, the RAG paradigm has attracted increasing attention (Gao et al. 2023). Specifically, RAG first leverages the retriever to search and extract relevant documents from some databases, which then serve as additional context to enhance the generation process. Related studies have demonstrated the great potential of RAG in various tasks such as language processing (Karpukhin et al. 2020; Jiang et al. 2023), recommendation systems (Ye et al. 2022; Dao et al. 2024), and computer vision (Liu et al. 2023; Kim et al. 2024).\nIn the graph modeling field, existing RAG efforts have primarily focused on static and text-attributed graphs to enhance the generation capabilities of Large Language Models, supporting graph-related tasks such as code summarization (Liu et al. 2021) and textual graph question answering (He et al. 2024; Hu et al. 2024). However, exploiting RAG techniques for dynamic graphs and graphs without textual information remains largely unexplored."}, {"title": "3 Preliminaries", "content": "In this section, we introduce the sequence modeling of dynamic graphs and the problem formulation.\nSequence Mapping of Dynamic Graphs. We denote a dynamic graph as $G = (V, E, F,T)$ comprising a set of nodes V, edges E, a node feature matrix F if available, and a time domain T. To map a dynamic graph into sequences, we follow SimpleDyG (Wu, Fang, and Liao 2024). Specifically, let $D = \\{(x_i, Y_i)\\}_{i=1}^l$ denote the set of training samples, where each sample is a pair $(x_i, Y_i)$, representing the input and output sequences for a target node $v_i \\in V$. The input $X_i$ is a chronologically ordered sequence of nodes that have historically interacted with $v_i$, while the output $y_i$ is the ground truth interactions that occurs following the sequence $x_i$. In notations, we have\n$X_i =\\begin{bmatrix}\\text{[hist]}, v_i, [\\text{time}\\_1], v_i^{t,1}, v_i^{t,2}, ..., [\\text{time}\\_T], v_i^{t,1}, ..., [\\text{eohist}]\\end{bmatrix}$ (1)\n$Y_i = \\begin{bmatrix}\\text{[pred]}, [\\text{time}\\_{T+1}], v_i^{T+1,1}, v_i^{t,1}, ..., [\\text{time}\\_t], v_i^{1}, ..., [\\text{eopred}]\\end{bmatrix}$ (2)\nwhere [hist], [eohist], [pred], [eopred] are special tokens denoting the input and output sequence, and [time_1], [time_{T+1}] are special time tokens representing different time steps.\nProblem Formulation. Dynamic graph modeling aims to learn a model that can predict the future interactions of a target node $v_i$, given its historical interactions. That is, given $x_i$ in Eq. (1), the task is to predict $y_i$ in Eq. (2).\nIn our RAG framework, we regard the training samples D as a retrieval pool. Given a target node $v_q \\in V$, its input sequence $x_q$ is referred to as the query sequence. We first retrieve K demonstrations $R_q = \\{(x_k, Y_k)\\}_{k=1}^K$ for each query sequence $x_q$ based on their contextual and temporal relevance. Next, the retrieved demonstrations $R_q$ are used to enrich the input sequence $x_q$, which encompasses the historical interactions of the target node $v_q$. The augmented input $\\{R_q, x_q\\}$ is designed to enhance the predictions of future events in $Y_q$."}, {"title": "4 Proposed Model: RAG4DyG", "content": "Our proposed RAG4DyG framework, depicted in Fig. 2, first trains a generative sequence model for dynamic graphs in Fig. 2(a). We use SimpleDyG (Wu, Fang, and Liao 2024) as the backbone for this task. As described in Sect. 3, a dynamic graph G is represented as sequences of node interaction records, which are then used to train a Transformer-based sequence model.\nNext, given a query sequence $x_q$ and a retrieval pool D, a time- and context-aware retriever is designed to retrieve demonstrations for $x_q$ from D, as shown in Fig. 2(b). The retriever fine-tunes its encoder (i.e., the sequence model) to jointly optimize two contrastive losses: a time-aware loss that employs a time decay function, and a context-aware loss that employs sequence augmentation. (Sect. 4.1)\nFinally, given top-K demonstrations retrieved from the retrieval pool, we design a graph fusion module as illustrated in Fig. 2(c). Specifically, we fuse the K demonstrations into a summary graph, leveraging the graph structures inherent to the retrieved demonstrations. The summary graph is further encoded by a GNN, serving as an augmented context that is prepended to the original query sequence. The augmented sequence is subsequently input to the sequence model to predict future events. (Sect. 4.2)"}, {"title": "4.1 Time- and Context-Aware Retriever", "content": "Unlike NLP retrievers, dynamic graph retrieval requires consideration of temporal proximity alongside contextual relevance. To address this, we propose a time- and context-aware retrieval model with two contrastive learning modules. First, we incorporate a time decay mechanism to account for temporal proximity between query and candidate sequences. Second, we use sequence augmentation to capture intrinsic contextual patterns.\nRetrieval Annotation. To facilitate contrastive training, we automatically annotate the samples in the retrieval pool D. For each query sequence $x_q$, we annotate its positive sample $x$ from the pool D based on their contextual similarity. Specifically, we adopt the sequence model pre-trained in Fig. 2(a) as the encoder and apply mean pooling to obtain sequence representations. Given a query sequence $x_q$ and a candidate sequence $x_p \\in D$, we define their contextual similarity as the dot product of their representations:\n$s(x_q, x_p) = f(x_q) \\cdot f(x_p),$ (3)\nwhere f(.) denotes our encoder. We leave the detailed annotation process in Appendix A.\nTime-aware Contrastive Learning. Temporal information reflects the dynamic changes in historical interactions, which is crucial for dynamic graph modeling. We posit that demonstrations closer in time to the query are more relevant than those further away. Consequently, we utilize a time decay function to account for temporal proximity between the query and candidate sequences, as follows.\n$\\mu(t_q, t_p) = exp(-\\lambda |t_q - t_p|),$ (4)\nwhere $t_q$ and $t_p$ represent the last interaction time in the query and candidate sequences\u00b9, respectively. The hyper-parameter $\\lambda$ controls the rate of time decay, determining how quickly the importance of interactions decreases with time. Note that 0 < $\\mu(\\cdot,\\cdot)$ \u2264 1. By using this time decay function, we assign higher importance to the candidates that are temporally closer to the query.\nTo effectively capture the temporal dynamics of the graph, we incorporates temporal proximity to reweigh the contextual similarity in the contrastive loss:\n$h(x_q, x_p) = s(x_q, x_p)\\mu(t_q, t_p).$ (5)\nSubsequently, we adopt in-batch negative sampling based on the following training objective:\n$L_{tcl} = - \\log \\frac{exp(h(x_q, x^+)/\\tau)}{\\sum_{j=1}^{2N} 1_{j\\neq q} exp(h(x_q, x_j)/\\tau)},$ (6)\nwhere $x^+$ denotes the positive sample of $x_q$, N is the batch size, and $\\tau$ is the temperature parameter."}, {"title": "Context-aware Contrastive Learning", "content": "To better capture the inherent contextual pattern, we further adopt context-aware contrastive learning with data augmentations. For each sequence, we apply two types of augmentations: masking and cropping, which are widely used for sequence modeling (Devlin et al. 2019; Xie et al. 2022). The masking operator randomly replaces a portion of the tokens in the sequence with a special masking token. The cropping operator randomly deletes a contiguous subsequence from the original sequence, reducing the sequence length while preserving the temporal order of the interactions. These augmentations help the model learn robust representations and capture the inherent structural information of the sequence by focusing on its different parts.\nWe treat two augmented views of the same sequence as positive pairs, and those of different sequences as negative pairs. Given a sequence $x_q$ and its two distinct augmented views $x_q^a$ and $x_q^{a'}$, the contrastive loss is defined as:\n$L_{ccl} = - \\log \\frac{exp(s(x_q^a, x_q^{a'})/\\tau)}{\\sum_{j=1}^{2N} 1_{j\\neq q} exp s(x_q^a, x_j^a)/\\tau}$ (7)\nwhere $\\tau$ is the temperature, N is the batch size and 1 is an indicator function.\nTraining and Inference for Retrieval. The training objective of our retrieval model is defined as:\n$L_{ret} = L_{tcl} + \\alpha L_{ccl}$ (8)\nwhere $\\alpha$ is a coefficient that controls the balance between the two losses.\nDuring testing, we utilize the updated sequence model to extract sequence representations and perform demonstration ranking based on the contextual similarity between the query and candidates."}, {"title": "4.2 Graph Fusion-based Generator", "content": "After retrieval, we obtain the top-K demonstrations $R_q = \\{(x_k, Y_k)\\}_{k=1}^K$ for the query $x_q$. A straightforward approach is concatenating them with the query sequence. However, this can lead to a lengthy context that limits the model's prediction capabilities. More importantly, it neglects the structural patterns among these demonstrations. Thus, we first fuse the demonstrations into a summary graph, process it using a GNN, and then prepend the graph readout from the GNN to the query for subsequent generation.\nGraph Fusion. To effectively fuse the demonstrations in $R_q$, we construct a summary graph, whose nodes include all tokens in the retrieved demonstrations, and edges represent the interactions between nodes within each sequence. Considering that there are common tokens across the retrieved demonstrations (e.g., recurring nodes in multiple demonstrations and special tokens like [hist], [time1], etc.), we can fuse these demonstrations into a summary graph $G_{fus}$. We then employ a graph convolutional network (GCN) to capture the structural and contextual information within the fused graph, and apply a mean-pooling readout to obtain a representation vector for the graph. The vector is subsequently concatenated with the query sequence representation, as follows.\n$e_{fus} = \\text{MeanPooling}(GCN(G_{fus})),$ (9)\n$X_q = [e_{fus} || X_q],$ (10)\nwhere $e_{fus}$ is the fused graph representation, and $I_q$ is the retrieval-augmented sequence. The augmented sequence is fed into the sequence model, which generates future interactions.\nTraining and inference. We adopt the same sequence model with the same training objective (Wu, Fang, and Liao 2024) as in Fig. 2(a). During training, we freeze the parameters of the sequence model, except for the output layer which is updated along with the GCN parameters used for graph fusion.\nDuring testing, we first apply the retriever model to retrieve top-K demonstrations for each query as introduced in Sect. 4.1. Then we perform graph fusion on these demonstrations and concatenate the fused graph representation with the query sequence as illustrated in Eqs. (9) and (10). The concatenated sequence is subsequently fed into the trained sequence model for link prediction."}, {"title": "5 Experiment", "content": "In this section, we empirically evaluate the proposed model RAG4DyG compared to state-of-the-art methods and conduct a detailed analysis of the performance."}, {"title": "5.1 Experimental Setup", "content": "Datasets. We evaluate the performance of the proposed model on three datasets from different domains including the communication network UCI (Panzarasa, Opsahl, and Carley 2009), the citation network Hepth (Leskovec, Kleinberg, and Faloutsos 2005), and the multi-turn task-oriented conversation dataset MMConv (Liao et al. 2021). We follow the same dataset preprocessing as SimpleDyG (Wu, Fang, and Liao 2024). It is important to note that the ML-10M dataset was not used because it is not suitable for RAG; we observed no performance enhancement even when the query was augmented with ground-truth demonstrations. We summarize the statistics of the three datasets in Table 1.\nBaselines. We compare RAG4DyG with the state-of-the-art approaches of dynamic graph modeling. (1) discrete-time approaches: DySAT (Sankar et al. 2020) and EvolveGCN (Pareja et al. 2020); (2) continuous-time approaches: DyRep (Trivedi et al. 2019), JODIE (Kumar, Zhang, and Leskovec 2019), TGAT (Xu et al. 2020), TGN (Rossi et al. 2020), TREND (Wen and Fang 2022), GraphMixer (Cong et al. 2022) and SimpleDyG (Wu, Fang, and Liao 2024). The details of the baselines can be found in Appendix B.\nImplementation Details. Following the method outlined in (Cong et al. 2022; Wu, Fang, and Liao 2024), we represent the dynamic graph as an undirected graph. We split all datasets into training, validation, and test sets based on temporal sequence same as SimpleDyG (Wu, Fang, and Liao 2024). Given T timesteps in each dataset, the data at the final timestep T is used as the testing set, the data at T \u2013 1 is served as the validation set, and the remaining data from earlier timesteps is used for training. All training data including the retrieval pool for the retriever and generator is drawn from this training data split. For retrieval augmented generation model training, we first train SimpleDyG without augmentation using the reported parameter in this paper. Then we fix the parameters of SimipleDyG except for the last linear layer and fine-tune them with the GCN model. The number of GCN layers in the generator model is 1 for all datasets. We repeat each experiment 10 times and report the average results along with the standard deviation. The number of demonstrations is 7 for all datasets. Hyper-parameter tuning is performed on the validation set and we provide the settings in Appendix D.\nEvaluation Metrics. We evaluate the performance upon three metrics Recall@5, NDCG@5 and Jaccard (Wu, Fang, and Liao 2024). Recall@5 and NDCG@5 are widely used in ranking tasks measuring the quality of top-ranked predictions (Wang et al. 2019). Recall@5 assesses the proportion of relevant nodes in the top 5 predictions, while NDCG@5 evaluates the ranking quality by considering the positions of the relevant nodes. Jaccard (Jaccard 1901) measures the similarity between the predicted and ground truth sequences by comparing the intersection and union of the two sets."}, {"title": "5.2 Performance Comparison", "content": "We evaluate the performance of RAG4DyG for the dynamic link prediction task, and the results compared to the state-of-the-art baselines are presented in Table 2. We make the following observations.\nThe proposed RAG4DyG generally outperforms all baselines across different datasets under the three metrics. In particular, compared to SimpleDyG, which is also our backbone, RAG4DyG consistently shows superior performance, highlighting the effectiveness of our retrieval-augmented generation framework. Note that GraphMixer performs slightly better in Recall@5 on the MMConv dataset, but its significantly lower performance in NDCG@5 and Jaccard indicates that its predictions are not ranked optimally or maintaining the overall set integrity compared to RAG4DyG. This indicates that RAG4DyG can better model the temporal and contextual relationships due to the specific design of the retriever and generator.\nSecond, RAG4DyG exhibits significant advantages in inductive scenarios such as the Hepth dataset. This setting is particularly challenging because it involves nodes not appearing during the training phase, requiring the model to generalize to entirely new structures and relationships. RAG4DyG's success is attributed to its retrieval-augmented mechanism, which enhances the model's ability to generalize by providing rich contextual information relevant to the new, unseen nodes. Unlike models that rely solely on the immediate neighborhood or predefined structures, RAG4DyG dynamically adapts to the new nodes, ensuring that the predictions are guided by the most relevant and similar historical data."}, {"title": "5.3 Model Analysis", "content": "We analyze the behavior of our model RAG4DyG in several aspects, including an ablation study, an investigation of the effectiveness of different retrieval methods, and an analysis of parameter sensitivity.\nAblation Study. To evaluate the effectiveness of different modules in the retrieval model, we compare RAG4DyG with two variants w/o CCL and w/o Decay which exclude the context-aware contrastive learning and time decay component in the retrieval model. We evaluate the performance for both retrieval and link prediction tasks. We use HR@k (Hit Ratio@k) metrics for the retrieval model, measuring the proportion of cases where at least one of the top-k retrieved items is relevant. As shown in Fig. 3 and 4, the full model outperforms the two variants, underscoring the benefits of incorporating context-aware contrastive learning and time decay modulation.\nEffect of Different Retrieval Methods. To further investigate the effectiveness of the retrieval model, we compare our model with two different retrieval methods, namely, BM25 and Jaccard, in Table 3 and 4. In Table 4, we only report NDCG@5 and present the remaining metrics in Appendix C. BM25 (Fang, Tao, and Zhai 2004) is an extension of the Term Frequency-Inverse Document Frequency (TF-IDF) model, which calculates a relevance score between the query sequence and each candidate sequence in the retrieval pool. The relevance score is derived from the occurrence frequency of the nodes in the query and the retrieval pool. Jaccard (Jaccard 1901) measures the similarity between two sets by comparing the size of their intersection to the size of their union. Note that in the citation dataset Hepth, all queries in the test set only contain unseen target nodes that never appear in the retrieval pool and have no historical interactions. As a result, the BM25 and Jaccard scores between the queries and the candidates in the retrieval pool are always zeros. On the other hand, our retrieval model is trained based on the sequence representations. For a query sequence containing only the target node, we can still obtain its representation using the sequence model trained for the retrieval model, and further calculate its contextual similarity with the candidate sequences in the retrieval pool.\nIn Table 3, we analyze the retrieval performance of different retrieval methods. Our retrieval model shows better performance than other retrieval strategies. Notably, in inductive scenarios like the Hepth dataset, BM25 and Jaccard fail to work with new query nodes lacking historical interactions. In contrast, our model can handle them effectively and achieve solid performance. The high HR@3 performance of BM25 and Jaccard on the MMConv dataset can be attributed to the nature of the dialogue dataset, where temporal order is less critical, and certain nodes associated with specific slot values are more discriminative.\nTable 4 shows the generative performance of different retrieval methods in the dynamic link prediction task. During testing, we apply the retrieval results obtained from different retrieval methods. We also train a model using the ground-truth retrieval results for a more comprehensive comparison. The \"GroundTruth\" row represents an upper bound on the performance when using ground-truth retrieval results on the testing data, which, as expected, provides the highest performance metrics. Generally speaking, all retrieval methods show better performance compared to the backbone SimpleDyG without using RAG, demonstrating the effectiveness of the RAG technique for dynamic graph modeling. Our method performs better compared to other retrieval strategies, indicating the effectiveness of contrastive learning in the retrieval model.\nEffect of the Number of Demonstrations K. To investigate the influence of the number of demonstrations, we conduct experiments across varying values $K \\in \\{1,3,5,7,9\\}$. As shown in Fig. 5, a higher number of K yields better prediction performance, that's because more demonstrations provide richer contextual information, especially in the UCI dataset. However, including too many cases may introduce more noise, which can harm the performance.\nEffect of Different Fusion Strategies. To further investigate the effectiveness of the fusion strategy for the top-K demonstrations, we conduct experiments with different fusion strategies in Table 5 for K = 7. We report NDCG@5 in Table 4 and present the other metrics in Appendix C. \"Concatenation\" denotes we directly concatenate the sequences of retrieved demonstrations and prepend them with the query sample sequence and then feed it into the pre-trained SimpleDyG model. \u201cMLP\u201d means we do not consider the graph structure of the demonstrations and replace the graph fusion as an MLP layer (we set the number of the MLP layer as 2). By using the MLP layer, We map the concatenated demonstrations into shorter m-dimensional embeddings (we empirically set m to be 15) and then concatenate it with the query sample. Like graph fusion, we only fine-tune the parameters of the MLP and output layer. The results in Table 5 show that directly concatenating the retrieved demonstrations with the query sample leads to lower performance compared with other strategies. This is because simple concatenation introduces a lengthy context, which can overwhelm the model with irrelevant information, and it neglects the structural relationships inherent in the demonstrations. The \"MLP\" strategy improves upon this by mapping the concatenated demonstrations into a shorter feature space, effectively reducing noise and emphasizing more relevant features. This approach yields better results than simple concatenation but still falls short compared to the \u201cGraphFusion\u201d strategy. The superior performance of the \u201cGraphFusion\u201d strategy highlights the importance of considering both the content and the structure of the demonstrations in the fusion process."}, {"title": "6 Conclusion", "content": "In this work, we introduced a novel retrieval augmented framework RAG4DyG for dynamic graph modeling, addressing the limitations of existing approaches that often rely on narrow historical contexts. We leverage the RAG technique to broaden the context by incorporating relevant auxiliary information by harvesting high-quality demonstrations for dynamic graph samples and effectively utilizing this demonstration to enhance dynamic graph modeling. Our proposed solution involves a time-aware contrastive learning model to identify and retrieve temporally pertinent cases for each query sequence, coupled with a graph fusion strategy to integrate the inherent historical context with extended temporal contexts. Extensive experiments on real-world datasets across different domains demonstrated the superior performance of RAG4DyG in dynamic graph modeling."}, {"title": "A Appendix", "content": "Retrieval Data Annotation\nTo facilitate retrieval model training, we regard the samples in the training dataset as our retrieval pool $D = \\{(x_i, Y_i)\\}_{i=1}^M$ where each pair $(x_i, Y_i)$ represents the historical sequence and its corresponding target sequence. Specifically, $x_i$ is the input sequence before the last time step and $Y_i$ is the output sequence at the last time step. We annotate demonstrations based on the Jaccard similarity of yr among all the pairs in D.\n$r(Y_i, Y_j) = \\frac{|Y_i \\cap Y_j|}{|Y_i \\cup Y_j|}$ (11)\nTo control the quality of annotated data, we set a threshold of 0.8 to select highly similar demonstrations for each sample. These filtered annotations are then used to train the retriever model. The number of training samples for UCI, Hepth and MMConv datasets are 9,578, 8,250 and 10,762, respectively."}, {"title": "B More Details about Baselines", "content": "The details of the baselines are as follows:\n\u2022 DySAT (Sankar et al. 2020) utilizes self-attention mechanisms to capture both structural and temporal patterns in dynamic graphs through discrete-time snapshots.\n\u2022 EvolveGCN (Pareja et al. 2020) leverages recurrent neural networks to evolve the graph convolutional network parameters over discrete time steps.\n\u2022 DyRep (Trivedi et al. 2019) models dynamic graphs in continuous time by incorporating both temporal point processes and structural dynamics to capture interactions and node dynamics.\n\u2022 JODIE (Kumar, Zhang, and Leskovec 2019) focuses on user and item embedding trajectories over continuous time, predicting future interactions by modeling user and item embeddings jointly.\n\u2022 TGAT (Xu et al. 2020) employs temporal graph attention layers and time encoding to capture temporal dependencies and structural information for dynamic graphs.\n\u2022 TGN (Rossi et al. 2020) combines GNNs with memory modules to maintain node states over continuous time, effectively learning from dynamic interactions.\n\u2022 TREND (Wen and Fang 2022) integrates temporal dependencies based on Hawkes process and GNN to learn the dynamics of graphs.\n\u2022 GraphMixer (Cong et al. 2022) introduces a novel architecture that leverages MLP-mixer to learn link-encoder and node encoder for evolving graphs in continuous time.\n\u2022 SimpleDyG (Wu, Fang, and Liao 2024) reformulated the dynamic graph modeling as a sequence modeling task and mapped the dynamic interactions of target nodes as sequences with specially designed tokens. It simplifies dynamic graph modeling without complex architectural changes to effectively capture temporal dynamics."}, {"title": "C More Experimental Results", "content": "For the effect of different retrieval methods for generative performance in Sec. 5.3, we report the NDCG@5 score in Table 4. Here we report the remaining metrics in Table 6. The results show that RAG4DyG performs better than BM25 and Jaccard on Recall@5 and Jaccard metrics, which are consistent with the performance of NDCG@5.\nFor the effect of different fusion strategies in Sec. 5.3, the performance of the remaining metrics is shown in Table 7. Similarly, the \u201cGraphFusion\u201d strategy outperforms direct concatenation and \"MLP\", indicating the effectiveness of the graph structure among demonstrations."}, {"title": "D Hyper-parameter Settings", "content": "We run all the experiments on an NVIDIA GPU L40 with the same data splitting and follow the hyper-parameter settings reported in SimpleDyG (Wu, Fang, and Liao 2024) for all the baselines. For our RAG4DyG method, we report the hyper-parameters as follows.\nThe time decay rate $\\lambda$ for the retrieval model in Eq. (4) was fine-tuned according to the time granularity of different datasets, with days for the UCI dataset, months for the Hepth dataset, and turns for the MMConv dataset. We explored a range of values $\\lambda = \\{1e - 4, 1e - 3, 1e - 2, 1e - 1, 1, 10, 100\\}$, ultimately selecting $\\lambda = 1e - 4$ for UCI, $\\lambda = 0.1$ for Hepth, and $\\lambda = 10$ for MMConv. The coefficient $\\alpha$ in the loss function (Eq. (8) was tuned across \\{0.2, 0.4, 0.6, 0.8, 1\\}, resulting in final values of $\\alpha = 1$ for UCI and MMConv dataset, $\\alpha = 0.4$ for Hepth."}]}