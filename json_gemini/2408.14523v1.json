{"title": "Retrieval Augmented Generation for Dynamic Graph Modeling", "authors": ["Yuxia Wu", "Yuan Fang", "Lizi Liao"], "abstract": "Dynamic graph modeling is crucial for analyzing evolving\npatterns in various applications. Existing approaches often\nintegrate graph neural networks with temporal modules or\nredefine dynamic graph modeling as a generative sequence\ntask. However, these methods typically rely on isolated his-\ntorical contexts of the target nodes from a narrow perspective,\nneglecting occurrences of similar patterns or relevant cases\nassociated with other nodes. In this work, we introduce the\nRetrieval-Augmented Generation for Dynamic Graph Model-\ning (RAG4DyG) framework, which leverages guidance from\ncontextually and temporally analogous examples to broaden\nthe perspective of each node. This approach presents two\ncritical challenges: (1) How to identify and retrieve high-\nquality demonstrations that are contextually and temporally\nanalogous to dynamic graph samples? (2) How can these\ndemonstrations be effectively integrated to improve dynamic\ngraph modeling? To address these challenges, we propose\nRAG4DyG, which enriches the understanding of historical\ncontexts by retrieving and learning from contextually and\ntemporally pertinent demonstrations. Specifically, we employ\na time- and context-aware contrastive learning module to\nidentify and retrieve relevant cases for each query sequence.\nMoreover, we design a graph fusion strategy to integrate the\nretrieved cases, thereby augmenting the inherent historical\ncontexts for improved prediction. Extensive experiments on\nreal-world datasets across different domains demonstrate the\neffectiveness of RAG4DyG for dynamic graph modeling.", "sections": [{"title": "1 Introduction", "content": "Dynamic graphs, which track the temporal evolution of\nnodes and edges, are essential for analyzing complex sys-\ntems across domains like social networks, citation networks\netc. Existing methods for dynamic graph modeling fall into\ntwo main categories: discrete-time and continuous-time ap-\nproaches (Feng et al. 2024). Discrete-time methods divide\nthe timeline into intervals, capturing the graph's state at\neach point (Sankar et al. 2020), but they lack temporal\ngranularity by overlooking event dynamics within intervals.\nContinuous-time approaches, on the other hand, model tem-\nporal dynamics continuously, without discretization, allow-\ning for more detailed representations (Trivedi et al. 2019;\nXu et al. 2020; Wen and Fang 2022). Our work aligns with\ncontinuous-time approaches, aiming to capture fine-grained\ntime steps for more accurate dynamic graph modeling.\nTraditional approaches for dynamic graph modeling\npredominantly rely on integrating graph neural networks\n(GNNs) with specialized temporal modules, such as re-\ncurrent neural networks (Pareja et al. 2020), self-attention\n(Sankar et al. 2020) and temporal point processes (Wen and\nFang 2022), which update the graph representations over\ntime. Although powerful, these methods suffer from the in-\nherent limitations of GNNs, such as difficulty with long-\nterm dependencies, and over-smoothing or over-squashing\nissues (Chen et al. 2020; Alon and Yahav 2020). Recently, a\nsimple and effective approach called SimpleDyG (Wu, Fang,\nand Liao 2024) redefines dynamic graph modeling as a gen-\nerative sequence modeling task and leverages the strengths\nof sequence models like Transformers to capture long-range\ndependencies within temporal sequences effectively.\nDespite significant advancements in dynamic graph mod-\neling, current approaches heavily depend on isolated histor-\nical contexts of the target nodes, limiting each node's per-\nspective to its ego network. The limited context makes it\ndifficult to quickly adapt to diverse emerging patterns. For\nexample, in a social network, when users transit to new be-\nhavioral patterns or when new users with minimal historical\ninteractions are introduced, traditional models that focus on\nthe historical contexts of individual nodes might not respond\neffectively to these changes. To overcome these limitations,\nwe draw inspiration from the Retrieval-Augmented Gener-\nation (RAG) technique originated from the Natural Lan-\nguage Processing (NLP) field (Gao et al. 2023). As shown in\nFig. 1(a), RAG broadens the context through the retrieval of\nadditional demonstrations, achieving considerable success\nin NLP. In this work, we aim to integrate RAG into dynamic\ngraph modeling to incorporate a broader and more relevant\ncontextual understanding beyond the historical interactions\nof individual nodes. However, leveraging RAG for dynamic\ngraph modeling presents two critical challenges.\nFirst, how do we harvest high-quality demonstrations for\ndynamic graphs? This involves selecting the most contex-\ntually and temporally relevant samples to enrich the current\nstate of a target node. Unlike RAG for NLP, where external\ntextual data and a pre-trained language model (LM) natu-\nrally support demonstration retrieval as shown in Fig. 1(a),\ndynamic graph modeling requires careful consideration of\nthe data sources and the ability to handle complex structural\nand temporal patterns as shown in Fig. 1(b). To address this,"}, {"title": "2 Related Work", "content": "Dynamic Graph Modeling. Existing approaches for dy-\nnamic graphs can be categorized into discrete-time and\ncontinuous-time methods. Discrete-time methods regard a\ndynamic graph as a sequence of static graph snapshots\ncaptured at various time steps. Each snapshot represents\nthe graph structure at a specific time step. These methods\ntypically adopt GNNs to model the structural information\nof each snapshot, and then incorporate a sequence model\n(Pareja et al. 2020; Sankar et al. 2020) to capture the changes\nacross snapshots. However, these approaches neglect fine-\ngrained time information within a snapshot. In contrast,\ncontinuous-time methods model graph evolution as a contin-\nuous process, capturing all time steps for more precise tem-\nporal modeling. These methods often integrate GNNs with\nspecially designed temporal modules, such as temporal ran-\ndom walk (Wang et al. 2021), temporal graph attention (Xu\net al. 2020; Rossi et al. 2020), MLP-mixer (Cong et al. 2022)\nand temporal point processes (Trivedi et al. 2019; Ji et al.\n2021; Wen and Fang 2022). Recently, researchers have pro-\nposed a simple and effective architecture called SimpleDyG\n(Wu, Fang, and Liao 2024), which reformulates the dynamic\ngraph modeling as a sequence modeling task. Specifically, it\nmaps the dynamic graph into a series of node sequences and\nfeeds them into a generative sequence model. Subsequently,\npredicting future events can be framed as a sequence gener-\nation problem.\nRetrieval Augmented Generation. Recently, the RAG\nparadigm has attracted increasing attention (Gao et al.\n2023). Specifically, RAG first leverages the retriever to\nsearch and extract relevant documents from some databases,\nwhich then serve as additional context to enhance the gen-\neration process. Related studies have demonstrated the great\npotential of RAG in various tasks such as language process-\ning (Karpukhin et al. 2020; Jiang et al. 2023), recommenda-\ntion systems (Ye et al. 2022; Dao et al. 2024), and computer\nvision (Liu et al. 2023; Kim et al. 2024).\nIn the graph modeling field, existing RAG efforts have"}, {"title": "3 Preliminaries", "content": "In this section, we introduce the sequence modeling of dy-\nnamic graphs and the problem formulation.\nSequence Mapping of Dynamic Graphs. We denote a\ndynamic graph as G = (V, E, F,T) comprising a set of\nnodes V, edges E, a node feature matrix F if available, and a\ntime domain T. To map a dynamic graph into sequences, we\nfollow SimpleDyG (Wu, Fang, and Liao 2024). Specifically,\nlet D = {(xi, Yi)}11 denote the set of training samples,\nwhere each sample is a pair (xi, Yi), representing the input\nand output sequences for a target node vi \u2208 V. The input Xi\nis a chronologically ordered sequence of nodes that have his-\ntorically interacted with vi, while the output yi is the ground\ntruth interactions that occurs following the sequence xi. In\nnotations, we have\n\\(X_i =\\\\\n[hist], v_i, [time_1], v_i^{t,1}, v_i^{t,2},...,\\\\[time_T], v_i^{t,1}, v_i^{t,1}, ..., [eohist]\n\\)\n\\(Y_i =\\\\\n[pred], [time_{T+1}], v_i^{T+1,1}..., [time_t], v_i^{t,1}...,\\\\[eopred],\n\\)\nwhere [hist], [eohist], [pred], [eopred] are special tokens de-\nnoting the input and output sequence, and [time_1],\n[time_T+1] are special time tokens representing different\ntime steps.\nProblem Formulation. Dynamic graph modeling aims to\nlearn a model that can predict the future interactions of a"}, {"title": "4 Proposed Model: RAG4DyG", "content": "Our proposed RAG4DyG framework, depicted in Fig. 2, first\ntrains a generative sequence model for dynamic graphs in\nFig. 2(a). We use SimpleDyG (Wu, Fang, and Liao 2024)\nas the backbone for this task. As described in Sect. 3, a dy-\nnamic graph G is represented as sequences of node inter-\naction records, which are then used to train a Transformer-\nbased sequence model.\nNext, given a query sequence xq and a retrieval pool D,\na time- and context-aware retriever is designed to retrieve\ndemonstrations for xq from D, as shown in Fig. 2(b). The\nretriever fine-tunes its encoder (i.e., the sequence model) to\njointly optimize two contrastive losses: a time-aware loss\nthat employs a time decay function, and a context-aware loss\nthat employs sequence augmentation. (Sect. 4.1)\nFinally, given top-K demonstrations retrieved from the re-\ntrieval pool, we design a graph fusion module as illustrated\nin Fig. 2(c). Specifically, we fuse the K demonstrations into\na summary graph, leveraging the graph structures inherent to\nthe retrieved demonstrations. The summary graph is further\nencoded by a GNN, serving as an augmented context that\nis prepended to the original query sequence. The augmented"}, {"title": "4.1 Time- and Context-Aware Retriever", "content": "Unlike NLP retrievers, dynamic graph retrieval requires con-\nsideration of temporal proximity alongside contextual rel-\nevance. To address this, we propose a time- and context-\naware retrieval model with two contrastive learning mod-\nules. First, we incorporate a time decay mechanism to ac-\ncount for temporal proximity between query and candidate\nsequences. Second, we use sequence augmentation to cap-\nture intrinsic contextual patterns.\nRetrieval Annotation. To facilitate contrastive training,\nwe automatically annotate the samples in the retrieval pool\nD. For each query sequence xq, we annotate its positive\nsample x from the pool D based on their contextual simi-\nlarity. Specifically, we adopt the sequence model pre-trained\nin Fig. 2(a) as the encoder and apply mean pooling to obtain\nsequence representations. Given a query sequence xq and a\ncandidate sequence xp \u2208 D, we define their contextual simi-\nlarity as the dot product of their representations:\n\\(s(x_q, x_p) = f(x_q) f(x_p),\\)\nwhere f(.) denotes our encoder. We leave the detailed anno-\ntation process in Appendix A.\nTime-aware Contrastive Learning. Temporal informa-\ntion reflects the dynamic changes in historical interactions,\nwhich is crucial for dynamic graph modeling. We posit that\ndemonstrations closer in time to the query are more relevant\nthan those further away. Consequently, we utilize a time de-\ncay function to account for temporal proximity between the\nquery and candidate sequences, as follows.\n\\(\\mu(t_q, t_p) = exp(-\\lambda |t_q - t_p|),\\)\nwhere tq and tp represent the last interaction time in the\nquery and candidate sequences\u00b9, respectively. The hyper-\nparameter \\( \\lambda \\) controls the rate of time decay, determining how\nquickly the importance of interactions decreases with time.\nNote that 0 < \\( \\mu \\)(\u00b7,\u00b7) \u2264 1. By using this time decay func-\ntion, we assign higher importance to the candidates that are\ntemporally closer to the query.\nTo effectively capture the temporal dynamics of the graph,\nwe incorporates temporal proximity to reweigh the contex-\ntual similarity in the contrastive loss:\n\\(h(x_q, x_p) = S(x_q, x_p)\\mu(t_q, t_p).\\)\nSubsequently, we adopt in-batch negative sampling based on\nthe following training objective:\n\\(L_{tcl} =\\\\\nlog\\frac{exp(h(x_q, x_+)/\\tau)}{\\sum_{j=1}^{2N} 1_{j\\neq q} exp(h(x_q, x_j)/\\tau)},\n\\)\nwhere x denotes the positive sample of xq, N is the batch\nsize, and \\( \\tau \\) is the temperature parameter."}, {"title": "Context-aware Contrastive Learning", "content": "To better capture\nthe inherent contextual pattern, we further adopt context-\naware contrastive learning with data augmentations. For\neach sequence, we apply two types of augmentations: mask-\ning and cropping, which are widely used for sequence mod-\neling (Devlin et al. 2019; Xie et al. 2022). The masking op-\nerator randomly replaces a portion of the tokens in the se-\nquence with a special masking token. The cropping operator\nrandomly deletes a contiguous subsequence from the origi-\nnal sequence, reducing the sequence length while preserving\nthe temporal order of the interactions. These augmentations\nhelp the model learn robust representations and capture the\ninherent structural information of the sequence by focusing\non its different parts.\nWe treat two augmented views of the same sequence as\npositive pairs, and those of different sequences as negative\npairs. Given a sequence xq and its two distinct augmented\nviews x, and xa, the contrastive loss is defined as:\n\\(L_{ccl} = - log \\frac{exp(s(x_q, x^a)/\\tau)}{\\sum_{j=1}^{2N} 1_{j\\neq q} exp s(x_q, x^j)/\\tau}\\)\nwhere \\( \\tau \\) is the temperature, N is the batch size and 1 is an\nindicator function.\nTraining and Inference for Retrieval. The training ob-\njective of our retrieval model is defined as:\n\\(L_{ret} = L_{tcl} + \\alpha L_{ccl}\\)\nwhere \\( \\alpha \\) is a coefficient that controls the balance between\nthe two losses.\nDuring testing, we utilize the updated sequence model to\nextract sequence representations and perform demonstration\nranking based on the contextual similarity between the query\nand candidates."}, {"title": "4.2 Graph Fusion-based Generator", "content": "After retrieval, we obtain the top-K demonstrations Rq =\n{(xk, Yk)}=1 for the query xq. A straightforward approach\nis concatenating them with the query sequence. However,\nthis can lead to a lengthy context that limits the model's pre-\ndiction capabilities. More importantly, it neglects the struc-\ntural patterns among these demonstrations. Thus, we first\nfuse the demonstrations into a summary graph, process it\nusing a GNN, and then prepend the graph readout from the\nGNN to the query for subsequent generation.\nGraph Fusion. To effectively fuse the demonstrations in\nRq, we construct a summary graph, whose nodes include all\ntokens in the retrieved demonstrations, and edges represent\nthe interactions between nodes within each sequence. Con-\nsidering that there are common tokens across the retrieved\ndemonstrations (e.g., recurring nodes in multiple demonstra-\ntions and special tokens like [hist], [time1], etc.), we can fuse\nthese demonstrations into a summary graph Gfus. We then\nemploy a graph convolutional network (GCN) to capture the\nstructural and contextual information within the fused graph,\nand apply a mean-pooling readout to obtain a representation"}, {"title": "5 Experiment", "content": "In this section, we empirically evaluate the proposed model\nRAG4DyG compared to state-of-the-art methods and con-\nduct a detailed analysis of the performance."}, {"title": "5.1 Experimental Setup", "content": "Datasets. We evaluate the performance of the proposed\nmodel on three datasets from different domains including\nthe communication network UCI (Panzarasa, Opsahl, and\nCarley 2009), the citation network Hepth (Leskovec, Klein-\nberg, and Faloutsos 2005), and the multi-turn task-oriented\nconversation dataset MMConv (Liao et al. 2021). We follow\nthe same dataset preprocessing as SimpleDyG (Wu, Fang,\nand Liao 2024). It is important to note that the ML-10M\ndataset was not used because it is not suitable for RAG; we\nobserved no performance enhancement even when the query\nwas augmented with ground-truth demonstrations. We sum-\nmarize the statistics of the three datasets in Table 1.\nBaselines. We compare RAG4DyG with the state-of-the-\nart approaches of dynamic graph modeling. (1) discrete-time\napproaches: DySAT (Sankar et al. 2020) and EvolveGCN\n(Pareja et al. 2020); (2) continuous-time approaches: DyRep\n(Trivedi et al. 2019), JODIE (Kumar, Zhang, and Leskovec\n2019), TGAT (Xu et al. 2020), TGN (Rossi et al. 2020),\nTREND (Wen and Fang 2022), GraphMixer (Cong et al.\n2022) and SimpleDyG (Wu, Fang, and Liao 2024). The de-\ntails of the baselines can be found in Appendix B.\nImplementation Details. Following the method outlined\nin (Cong et al. 2022; Wu, Fang, and Liao 2024), we repre-\nsent the dynamic graph as an undirected graph. We split all\ndatasets into training, validation, and test sets based on tem-\nporal sequence same as SimpleDyG (Wu, Fang, and Liao\n2024). Given T timesteps in each dataset, the data at the fi-\nnal timestep T is used as the testing set, the data at T \u2013 1\nis served as the validation set, and the remaining data from\nearlier timesteps is used for training. All training data in-\ncluding the retrieval pool for the retriever and generator is\ndrawn from this training data split. For retrieval augmented\ngeneration model training, we first train SimpleDyG with-\nout augmentation using the reported parameter in this paper.\nThen we fix the parameters of SimipleDyG except for the\nlast linear layer and fine-tune them with the GCN model.\nThe number of GCN layers in the generator model is 1 for all\ndatasets. We repeat each experiment 10 times and report the\naverage results along with the standard deviation. The num-\nber of demonstrations is 7 for all datasets. Hyper-parameter\ntuning is performed on the validation set and we provide the\nsettings in Appendix D.\nEvaluation Metrics. We evaluate the performance upon\nthree metrics Recall@5, NDCG@5 and Jaccard (Wu, Fang,\nand Liao 2024). Recall@5 and NDCG@5 are widely used\nin ranking tasks measuring the quality of top-ranked predic-\ntions (Wang et al. 2019). Recall@5 assesses the proportion\nof relevant nodes in the top 5 predictions, while NDCG@5\nevaluates the ranking quality by considering the positions\nof the relevant nodes. Jaccard (Jaccard 1901) measures the\nsimilarity between the predicted and ground truth sequences\nby comparing the intersection and union of the two sets."}, {"title": "5.2 Performance Comparison", "content": "We evaluate the performance of RAG4DyG for the dynamic\nlink prediction task, and the results compared to the state-\nof-the-art baselines are presented in Table 2. We make the\nfollowing observations.\nThe proposed RAG4DyG generally outperforms all base-\nlines across different datasets under the three metrics. In par-\nticular, compared to SimpleDyG, which is also our back-\nbone, RAG4DyG consistently shows superior performance,\nhighlighting the effectiveness of our retrieval-augmented\ngeneration framework. Note that GraphMixer performs\nslightly better in Recall@5 on the MMConv dataset, but\nits significantly lower performance in NDCG@5 and Jac-\ncard indicates that its predictions are not ranked opti-\nmally or maintaining the overall set integrity compared to\nRAG4DyG. This indicates that RAG4DyG can better model\nthe temporal and contextual relationships due to the specific\ndesign of the retriever and generator.\nSecond, RAG4DyG exhibits significant advantages in in-\nductive scenarios such as the Hepth dataset. This setting\nis particularly challenging because it involves nodes not\nappearing during the training phase, requiring the model\nto generalize to entirely new structures and relationships.\nRAG4DyG's success is attributed to its retrieval-augmented\nmechanism, which enhances the model's ability to general-\nize by providing rich contextual information relevant to the"}, {"title": "5.3 Model Analysis", "content": "We analyze the behavior of our model RAG4DyG in several\naspects, including an ablation study, an investigation of the\neffectiveness of different retrieval methods, and an analysis\nof parameter sensitivity.\nAblation Study. To evaluate the effectiveness of differ-\nent modules in the retrieval model, we compare RAG4DyG\nwith two variants w/o CCL and w/o Decay which exclude\nthe context-aware contrastive learning and time decay com-\nponent in the retrieval model. We evaluate the performance\nfor both retrieval and link prediction tasks. We use HR@k\n(Hit Ratio@k) metrics for the retrieval model, measuring\nthe proportion of cases where at least one of the top-k re-\ntrieved items is relevant. As shown in Fig. 3 and 4, the full\nmodel outperforms the two variants, underscoring the bene-\nfits of incorporating context-aware contrastive learning and\ntime decay modulation.\nEffect of Different Retrieval Methods. To further inves-\ntigate the effectiveness of the retrieval model, we compare\nour model with two different retrieval methods, namely,\nBM25 and Jaccard, in Table 3 and 4. In Table 4, we only\nreport NDCG@5 and present the remaining metrics in Ap-\npendix C. BM25 (Fang, Tao, and Zhai 2004) is an extension"}, {"title": "6 Conclusion", "content": "In this work, we introduced a novel retrieval augmented\nframework RAG4DyG for dynamic graph modeling, ad-\ndressing the limitations of existing approaches that often\nrely on narrow historical contexts. We leverage the RAG\ntechnique to broaden the context by incorporating relevant\nauxiliary information by harvesting high-quality demonstra-\ntions for dynamic graph samples and effectively utilizing\nthis demonstration to enhance dynamic graph modeling. Our\nproposed solution involves a time-aware contrastive learning\nmodel to identify and retrieve temporally pertinent cases for\neach query sequence, coupled with a graph fusion strategy to\nintegrate the inherent historical context with extended tem-\nporal contexts. Extensive experiments on real-world datasets\nacross different domains demonstrated the superior perfor-\nmance of RAG4DyG in dynamic graph modeling."}, {"title": "A Retrieval Data Annotation", "content": "To facilitate retrieval model training, we regard the sam-\nples in the training dataset as our retrieval pool D =\n{(xi, Yi)}M\u2081 where each pair (xi, yi) represents the histori-\ncal sequence and its corresponding target sequence. Specif-\nically, xi is the input sequence before the last time step and\nYi is the output sequence at the last time step. We annotate\ndemonstrations based on the Jaccard similarity of yr among\nall the pairs in D.\n\\(r(Y_i, Y_j) = \\frac{Y_i\\cap Y_j}{Y_i \\cup Y_j}\\)\nTo control the quality of annotated data, we set a thresh-\nold of 0.8 to select highly similar demonstrations for each\nsample. These filtered annotations are then used to train the\nretriever model. The number of training samples for UCI,\nHepth and MMConv datasets are 9,578, 8,250 and 10,762,\nrespectively."}]}