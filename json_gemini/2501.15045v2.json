{"title": "Towards Robust Unsupervised Attention Prediction in Autonomous Driving", "authors": ["Mengshi Qi", "Xiaoyang Bi", "Pengfei Zhu", "Huadong Ma"], "abstract": "Robustly predicting attention regions of interest for self-driving systems is crucial for driving safety but presents significant challenges due to the labor-intensive nature of obtaining large-scale attention labels and the domain gap between self-driving scenarios and natural scenes. These challenges are further exacerbated by complex traffic environments, including camera corruption under adverse weather, noise interferences, and central bias from long-tail distributions. To address these issues, we propose a robust unsupervised attention prediction method. An Uncertainty Mining Branch refines predictions by analyzing commonalities and differences across multiple pre-trained models on natural scenes, while a Knowledge Embedding Block bridges the domain gap by incorporating driving knowledge to adaptively enhance pseudo-labels. Additionally, we introduce RoboMixup, a novel data augmentation method that improves robustness against corruption through soft attention and dynamic augmentation, and mitigates central bias by integrating random cropping into Mixup as a regularizer. To systematically evaluate robustness in self-driving attention prediction, we introduce the DriverAttention-C benchmark, comprising over 100k frames across three subsets: BDD-A-C, DR(eye)VE-C, and DADA-2000-C. Our method achieves performance equivalent to or surpassing fully supervised state-of-the-art approaches on three public datasets and the proposed robustness benchmark, reducing relative corruption degradation by 58.8% and 52.8%, and improving central bias robustness by 12.4% and 11.4% in KLD and CC metrics, respectively. Code and data are available at https://github.com/zaplm/DriverAttention.", "sections": [{"title": "1 INTRODUCTION", "content": "N recent years, significant advancements in autonomous driving have heightened interest in predicting attention regions for self-driving systems [1; 2] within the research and industry community. Predicted attention regions offer vital contextual information, aiding autonomous driving systems in identifying key areas within traffic scenes [3; 4; 5]. Crucially, these key areas often encompass the highest risk zones, where minor perception errors can significantly jeopardize driver safety [6]. Consequently, successful prediction of attention areas allows for the reallocation of computational resources. This reallocation enhances perception accuracy in critical zones, thereby mitigating driving risks and bolstering the explainability and reliability of autonomous driving systems [7].\nA variety of datasets [8; 9; 10] and methodologies [1; 7; 8; 11] have been introduced to tackle the task of predicting attention in self-driving. Despite their promising performance, these methods rely on fully-supervised training using large-scale labeled datasets, which are difficult and unreliable to construct. For instance, the DR(eye)VE [9] dataset, widely used in self-driving research, was compiled over two months by recording eight drivers alternately navigating the same route to gather fixation data. However, averaging the attention data from eight drivers into a single video can result in inaccurate attention targeting. Another significant challenge is the substantial disparity between collected data and real-world environments. The BDD-A [8] dataset, another dataset for self-driving, was created by having 45 participants watch a recorded video and envision themselves as the drivers. However, this simulation approach inevitably introduces inconsistencies with real-world conditions for human labeling. Consequently, current fully-supervised methods are prone to biases in public datasets, making them difficult to adapt to new environments. Moreover, large-scale pre-trained models have shown strong capabilities in representation learning, offering benefits for numerous downstream tasks. However, bridging the domain gap between specific situations (e.g., self-driving scenes) and the domains of pre-trained models (e.g., natural scenes) remains a challenge."}, {"title": "2 RELATED WORK", "content": "Self-Driving Attention Prediction. The advent of deep learning has spurred numerous initiatives into self-driving attention prediction [7; 8; 10; 20]. Palazzi et al. [7] used a multi-branch video analysis method for prediction of driver attention. Baee et al. [1] enhanced attention prediction accuracy through an inverse reinforcement learning approach. However, this prior research mostly depended on extensively annotated datasets [8; 9; 10] collected either in-lab or in-vehicle. For example, the DR(eye)VE [9] dataset, an in-vehicle collection, features multiple segments documenting changes in driver attention. BDD-A [8] and DADA-2000 [10], as in-lab datasets, compile synthesized attention shifts from volunteers across over 1000 clips. Addressing the unreliability of self-driving datasets, our model pioneers unsupervised attention prediction in self-driving by using pseudo-labels from models pre-trained on natural scenes.\nSaliency Detection. Predicting saliency regions in images or videos [21; 22] can approximate human's visual attention. It has been used to evaluate the explainability of deep models [3] and to assist other tasks, i.e., photo cropping [23], scene understanding [24; 25; 26] and object segmentation [3]. The early effort used a fusion strategy [27], merging semantic information from various levels without addressing semantic discrepancies. To address this challenge, Xie et al. introduced progressive feature aggregation techniques based on the feature pyramid network (FPN) [28]. Afterwards, Li et al. developed a dynamic search process to enable adaptive feature selection at the pixel level [29]. However, most existing datasets [30; 31] and methods [21; 22; 28; 29; 32; 33] are mainly focusing on natural scenes or common objects, not specially tailored into self-driving scenarios. In this"}, {"title": "3 METHOD", "content": "Figure 2 provides an overview of the robust unsupervised driving attention prediction network we proposed. The architecture is composed of several key components, including the Attention Prediction Branch (APB), the Knowledge Embedding Block (KEB), the Uncertainty Mining Branch (UMB), and the RoboMixup method.\nOur method predicts self-driving attention through an unsupervised learning paradigm. While pseudo-labels generated by a single source model pre-trained on natural scene datasets can be used for training, the domain gap between natural environments and self-driving scenarios introduces significant uncertainty. Single-source pseudo-labels often exhibit distinct distributions, with certain regions contributing to elevated uncertainty. Inspired by recent advancements in uncertainty estimation [6; 37], we improved prediction accuracy and robustness by modeling uncertainty using pseudo-labels from multiple sources. To address the lack of autonomous driving knowledge in pseudo-labels transferred from natural domains, we incorporated a Knowledge Embedding Block (KEB) to refine input pseudo-labels, enhancing final predictions. We also introduced the Uncertainty Mining Branch (UMB), which utilizes multiple Uncertainty Blocks (UB) to iteratively analyze similarities and variations among noisy labels, producing pixel-level uncertainty maps. Additionally, we proposed RoboMixup, a novel data augmentation technique that improves corruption robustness and mitigates central bias.\nProblem Formulation. For an input RGB frame $X \\in [R^{H \\times W \\times 3}$, APB adopts a pyramid feature extraction approach inspired by PSPNet [41], producing features across five hierarchical levels. Features F from the 1st, 2nd, and 4th stages, denoted as {$F_0, F_1, F_2$}, are passed to the Uncertainty Mining Branch (UMB) to analyze the uncertainty of pseudo-labels. The APB is structured following U-Net [42], where the features from the final layer are fed into a decoder. These features are then concatenated with features of corresponding resolutions, and the resulting attention prediction map is generated as $S \\in R^{H \\times W \\times 1}$ Additionally, a knowledge enhancement process is applied to adapt pseudo-labels for autonomous driving scenarios using a pre-trained Mask Head. The UMB then receives V knowledge-enhanced pseudo-labels, $Y = {Y_1,..., Y_N}$, as input and produces corresponding uncertainty maps, each having the same dimensions as the predicted attention map S. These pseudo-labels are further fused with three levels of APB features to generate the uncertainty maps $U = {U_1,\u2026\u2026,U_N}$. To improve the training process, the RoboMixup technique is employed, which generates augmented data pairs (I, Y) by combining image-pseudo labels pairs (I, \u0176) and the corresponding predicted attention map S. The model is subsequently optimized on these augmented data pairs using an uncertainty-based loss function."}, {"title": "3.2 Knowledge Embedding Block (KEB)", "content": "Humans naturally use prior knowledge to interpret and identify relevant objects in visually complex scenes [32]. Inspired by this capability, we designed the Knowledge Embedding Block (KEB) to incorporate prior traffic knowledge and reduce the domain gap between natural scenes and self-driving environments. Using an off-the-shelf Mask R-CNN model pre-trained on the MS-COCO dataset [17], we generate a binary segmentation map M by merging masks across relevant categories. To keep the method unsupervised, we freeze the parameters of Mask R-CNN with open-source checkpoints. Representative traffic-related objects, such as pedestrians, signals, bicycles, motorcycles, and traffic signs (e.g., stop signs and road signs), are identified as prior knowledge through a knowledge mining strategy. This traffic-specific prior knowledge is then incorporated into the pseudo-label generation process via a knowledge embedding mechanism, enhancing the model's adaptability to autonomous driving scenarios.\nKnowledge Mining Strategy. For all segmented categories, we first count the occurrences of each category in the dataset and sort them in descending order to obtain $C = {C_1, C_2,..., C_n}$. We focus on the most frequent categories, as the remaining categories are likely to be misclassified due to their small proportion in the dataset and the fact that the Mask R-CNN model we used is not trained on traffic scenes. Specifically, we choose the following categories:\n$\\hat{C} = {c_i | i < \\min {k | T(k) \\geq p\\% \\times T (||)}}}$,\nwhere p is the coverage threshold, and T(k) represents the sum of the instance numbers of the first k categories in C. Pretrained attention prediction models for natural scenes lack the recognition capabilities for traffic-specific objects (e.g., stop signs), leading to an attention distribution bias across various objects (i.e., draw more attention on daily objects than traffic items). We leverage this bias for knowledge mining by calculating the mean attention for all pseudo-label instances as:\n$V_{c_i} = \\frac{1}{n_{c_i}} (\\sum_{j=1}^{n_{c_i}} \\mu_{p} \\cdot \\frac{M_{ij}}{M_{ij}})$,\nwhere $v_{p}$ denotes the mean of pseudo-labels generated by all pretrained models on natural scenes, $M_{ij}$ is the binary segmentation mask for the $j_{th}$ instance of the category $C_i$, and $n_{ci}$ represents the instance count for the category $C_i$. As a result, we mine the following categories as prior knowledge:\n$\\tilde{C} = {Ci | V_{ci} < \\eta \\sum_{j=1}^{k} V_{Cj}}$\nwhere \u03b7 is the proportion factor. The process of generating the segmentation map with prior knowledge can be formally expressed as:\n$\\hat{M} = M \\cdot 1_C$,"}, {"title": "3.3 Uncertainty Mining Branch (UMB)", "content": "The Uncertainty Mining Branch (UMB) is designed to extract uncertainty from multi-source pseudo-labels generated by several pre-trained models. Notably, these models are trained on natural scene datasets rather than self-driving data. For instance, ML-Net [32], SAM [33], and UNISAL [22] are trained on SALICON [30], while TASED-Net [21] is pre-trained on DHF-1K [31]. As illustrated in Figure 2, the Uncertainty Block (UB) is introduced to facilitate information exchange between pseudo-labels and multi-scale features extracted by the APB. Each Uncertainty Block leverages non-local self-attention mechanisms and a merge/split design [43; 44] to achieve this. Specifically, for the n-th knowledge-embedded pseudo-label $\\hat{Y_n} \\in R^{H \\times W \\times 1}$, we first process it through a convolutional layer followed by a downsampling operation, reducing its spatial dimensions to one-fourth of the original size. The downsampled pseudo-label is then passed through a residual block [45], enabling information exchange between pseudo-labels and feature maps derived from other sources at the same stage. The processed outputs are concatenated with the input pseudo-labels and passed through a non-local self-attention mechanism to generate a coarse uncertainty map $U^n$ for the n-th pseudo-label. This process is mathematically expressed as:\n$U^n_ = f_{attn} (Concat(\\hat{Y_1},..., \\hat{Y_n}, F^0)) + \\hat{Y_n}$,\nwhere the superscripts indicate the stage index, and $f_{attn}()$ represents the non-local self-attention function. The uncertainty map $U^n$ is iteratively refined across stages, yielding:\n$U^{t+1}_ = f_{attn} (Concat(U^1_,,\u2026,U^{t,}_n, F^{t})) + U^t_$.\nAfter passing through three Uncertainty Blocks, the fine-grained uncertainty map $U^2 \\in R^{\\frac{H}{4} \\times \\frac{W}{4} \\times 1}$ is produced. This map is subsequently upsampled within the decoder to match the original input size, resulting in $U^n \\in R^{H \\times W \\times 1}$."}, {"title": "3.4 Uncertainty Loss Function", "content": "We treat the predicted attention map S as a spatial distribution and normalize the generated pseudo-labels accordingly. To achieve this, a spatial softmax layer is applied after the APB. Drawing inspiration from the uncertainty loss framework in [6], we model each pseudo-label map $\\hat{Y_n} \\in R^{H \\times W \\times 1}$ as a Boltzmann distribution under Bayesian theory. The probability of S conditioned on the pseudo-label $\\hat{Y_n}$ is given by:\n$p(\\hat{Y_n}|S, u_n) = \\prod_{i}^{H \\times W} Softmax(\\frac{S_i}{u_n})$,\nwhere $u_n = \\frac{1}{H \\times W} \\sum_{i}^{H \\times W} U_n$ represents the uncertainty estimation for the n-th pseudo-label, i denotes the pixel index of S, and $u_n$ serves as a temperature parameter controlling the distribution's flatness. The negative log-likelihood for the pseudo-label map is calculated as:\n$- log p(\\hat{Y_n}/S, u_n) = - \\sum_{i} \\frac{S_i}{u_n} + log \\sum_{i} exp (\\frac{S_i}{u_n}) + log(u_n)$,\nwhere $L_{CE}(S, \\hat{Y_n})$ denotes the spatial cross-entropy loss. To improve numerical stability during training, we predict the log variance $e_n = log(u_n)$ as suggested in [37]. The uncertainty loss is reformulated as:\n$L_{unc}(S, U_n, \\hat{Y_n}) = L_{CE} (S, \\hat{Y_n}) exp(-e_n) + \\frac{1}{2} e_n$.\nWe further express the cross-entropy loss $L_{CE}(S, \\hat{Y_n})$ as:\n$L_{CE}(S, \\hat{Y_n}) = - \\sum \\hat{Y_{n,i}} log(S_i)$\n$= \\sum \\hat{Y_{n, i}} log(S_i) + H(\\hat{Y_n}) - H(\\hat{Y_n})$\n$= \\sum \\hat{Y_{n,i}} (log(\\hat{Y_{n,i}}) - log(S_i)) - H(\\hat{Y_n})$\n$= L_{KLD}(\\hat{Y_n}, S) - H(\\hat{Y_n})$,\nwhere $L_{KLD}(\\hat{Y_n}, S) = \\sum_i \\hat{Y_{ni}} (log(\\hat{Y_{n,i}}) - log(S_i))$ represents the KL-divergence between the pseudo-label distribution and the predicted attention map distribution, and $H(\\hat{Y_n})$ denotes the information entropy of $\\hat{Y_n}$, which remains constant during optimization. Extending the calculation across all N pseudo-labels, the total uncertainty loss is derived as:\n$L_{unc} = \\sum_{n=1}^{N} {\\frac{1}{2} {L_{KLD} (\\hat{Y_n}, S) exp(-e_n) + \\frac{1}{2} e_n}}$.\nIt is worth noting that our KL-divergence-based uncertainty loss differs from prior works [7] by assuming a spatial distribution rather than a single per-channel counterpart. This assumption is critical for deriving Eq. (11) and ensures alignment with the spatial nature of our task."}, {"title": "3.5 RoboMixup", "content": "In order to address corruption and central bias issues, we design the RoboMixup as a novel robust data augmentation strategy, by generating augmentation pairs (I, Y) from image-pseudo labels pairs (I, \u0176).\n1) Design for Corruption Robustness. We propose an iterative data augmentation strategy that combines soft attention-based Mixup with dynamic augmentation to address the challenge of corruption robustness. To be detailed, the soft attention-based Mixup generates crucial and realistic samples, and dynamic augmentation selectively augments samples to achieve a more uniform data distribution, thereby enhancing the robustness of attention prediction models in self-driving scenarios by training with extra generated examples.\nSoft Attention-based Mixup. Given the training sample image and pseudo labels pair $(I_i, \\hat{Y_i})$, $(I_j, Y_j)$, the Mixup method generates new samples as follows:\n$\\hat{I} = \\lambda I_i + (1 - \\lambda) I_j; \\hat{Y} = \\lambda \\hat{Y_i} + (1 - \\lambda) \\hat{Y_j}$,\nwhere \u03bb\u2208 [0, 1], representing the transparencies used to blend the images and pseudo labels, typically assumed to follow a Beta distribution. Our method proposes using the attention map S predicted by our model to replace \u03bb :\n$\\hat{I} = (\\frac{S_i}{S_i + S_j} \\odot I_i) + (\\frac{S_j}{S_i + S_j} \\odot I_j)$,\n$\\hat{Y} = (\\frac{S_i}{S_i + S_j} \\odot \\hat{Y_i}) + (\\frac{S_j}{S_i + S_j} \\odot \\hat{Y_j})$\nwhere \u2299 denotes pixel-wise multiplication. Rather than applying global transparency to the entire sample, our method employs pixel-level weights in sample mixing. This approach benefits from utilizing attention mechanisms to distinguish between critical objects in the scene, such as pedestrians and stop signs, and irrelevant backgrounds. On one hand, key objects are blended with higher attention weights in the mixup, increasing their presence and creating difficult samples; on the other hand, insignificant backgrounds are mixed with lower attention weights, rendering the samples more realistic. We show the generated examples of our method and traditional Mixup in Figure 4a. It can be observed that traditional Mixup overlays one image onto another as a \"ghost\" image. In contrast, our proposed method \"cuts out\" pedestrians, cyclists, and traffic lights from the first image and integrates them into the second image, thereby achieving a more realistic and complex image.\nDynamic Augmentation Strategy. We further propose a dynamic augmentation strategy to alter the dataset distribution. Specifically, we use the KL divergence between the estimated attention map $S_i$ and the average attention map $S_{avg}$ as the selection criterion. $S_{avg}$ presents a Gaussian distribution centered around the road [7; 8]. A higher KL divergence between $S_i$ and $S_{avg}$ indicates a lower similarity between the sample's attention distribution and the Gaussian distribution, signifying greater scene complexity. Utilizing this principle, we select the top K-percent samples within each batch with the highest KL divergence for applying Soft Attention-based Mixup, efficiently generating augmentation candidates. Additionally, we apply this criterion to identify and select samples that deviate significantly from the average attention map to further augment the dataset. As illustrated in Figure 4b, our approach effectively increases the proportion of challenging samples, resulting in a more uniform distribution toward the tail. The algorithm details can be found in the supplementary materials.\n2) Design for Central Bias. We propose a method that combines the advantages of random cropping and Mixup.\nRandom Crop. Compared to directly resizing images to a fixed size, random cropping preserves the local granular details of the images. It also increases the likelihood of avoiding areas with significant central bias, thereby mitigating the inherent central bias in the dataset. After incorporating random crop data, we modify the model's training loss as follows:\n$L_{unc_{aug}} = L_{unc_{rcp}} + L_{unc}$,\nwhere $L_{unc}$ represents the loss calculated for the original data, as defined in Eq. (12), and $L_{unc_{rcp}}$ represents the corresponding loss calculated for the random crop data.\nRegMixup. Instead of using the vanilla Mixup, we follow the RegMixup [40] by introducing two key modifications to the original Mixup technique. First, it changes the parameter a of the beta distribution in vanilla Mixup from 1 to 10. Second, the augmented samples are not replacements for the original samples but are treated as additional sam-"}, {"title": "3.6 Optimization and Inference", "content": "For optimization, we use the loss function $L_{unc}$ as defined in Eq. (12), and apply it to the dataset constructed using our proposed Soft Attention-based Mixup and the Dynamic Augmentation Strategy to overcome the corruption. In order to alleviate the central bias, we utilize the loss function $L_{unc_{reg}}$ as shown in Eq. (17). During the inference phase, we no longer use the uncertainty mining block and RoboMixup. Instead, we use only the attention prediction branch for evaluation."}, {"title": "4 DRIVERATTENTION-C DATASET", "content": "The real traffic scenarios often arise from camera input corruptions caused by various noise interferences and adverse weather conditions. However, no prior work has addressed this challenge in the context of self-driving attention prediction. To fill this gap, we collect the first DriverAttention-C dataset, which consists of three subsets: BDD-A-C, DR(eye)VE-C, and DADA-2000-C. Each subset contains four categories of common corruptions, i.e., noise, blur, digital, and weather conditions, which include six specific types of corruption: Gaussian Noise, Impulse Noise, Motion Blur, JPEG Compression, Fog, and Snow."}, {"title": "4.1 Data Source", "content": "Our dataset sources originate from test sets of BDD-A [8], DR(eye)VE [9], and DADA-2000 [10]. These datasets are derived from various collection driving contexts, including in-lab settings, in-car recordings, and real-world crash scenarios. We synthesize noisy corruption data based on these images, while we opt for CycleGAN [18] to generate realistic fog and snow effects. To achieve this, the target adverse weather domain training data are collected from the fog dataset SeeingThroughFog [46], the snow dataset WADS [47],"}, {"title": "4.2 Generation Pipeline", "content": "For corruptions data generation, we follow the open-source toolbox from Hendrycks et al. [13] to simulate various degradation scenarios. These corruptions include Gaussian noise, impulse noise, motion blur, and JPEG compression, each chosen for their relevance to real-world challenges in image capture and transmission. Gaussian noise arises in low-illumination conditions or during prolonged camera operation, which may lead to sensor overheating. The noise is modeled by adding random Gaussian perturbations to the pixel values of the image. Impulse noise, also known as salt-and-pepper noise, typically results from sudden disturbances in signal transmission. This noise introduces random bright and dark pixels in the image, 6% of the pixels are randomly affected. Motion blur occurs when the camera is unable to compensate for the rapid movement of the vehicle, resulting in smeared images. JPEG compression is a widely used lossy image compression method. Due to transmission constraints, aggressive compression can introduce artifacts such as blockiness and loss of fine details.\nRegarding adverse weather conditions like fog and snow, instead of manually simulating weather effects [13], we employ Cycle-GAN to transfer the source domain, consisting of clear-weather images with good visibility, to the target domains of fog and snow. This approach ensures that the generated snow and fog images remain consistent with the underlying structure of the original clear-weather images while effectively simulating the target domain characteristics. Specifically, we use the BDD-A dataset as the source domain data and the aforementioned fog and snow datasets to train fog and snow generators, respectively, with each image having a resolution of 512 \u00d7 256 pixels. Finally, we apply the trained generators to all three datasets to obtain the simulated images and filter out images with artificial stripes caused by inherent issues of the Cycle-GAN, resulting in 38,444 images. We show how each type of corruption significantly alters the original representation of the images in Figure 5. More details about our dataset please refer to our supplementary."}, {"title": "5 EXPErimental ResULTS", "content": "In experiments, we initially compare our unsupervised method against fully-supervised approaches across several well-established datasets: BDD-A, DR(eye)VE, DADA-2000 and DriverAttention-C. We then conduct thorough ablation studies to assess the effectiveness of each component."}, {"title": "5.1 Experimental Settings", "content": "Datasets. Our model's performance was assessed using three self-driving benchmarks: BDD-A, DR(eye)VE, and DADA-2000. BDD-A [8], an in-lab driving attention dataset, comprises 1,232 short clips (each under 10 seconds) featuring diverse urban and rural road driving scenarios. Following this split, we use 28k frames for training, 6k for validation, and 9k for testing. DR(eye)VE [9], an in-car dataset, aims for consistent driving conditions across its 74 videos (each up to 5 minutes). Following previous protocols [9], we selected the last 37 videos for the test set. DADA-2000 [10], including vehicle crash scenarios, allows for the prediction of driving attention under critical conditions. With over 658,746 frames across 2000 clips, we adopt a 3:1:1 split ratio for training, validation, and testing as per standard practices [10]. To evaluate the robustness of the self-driving attention prediction model, we test them on our constructed DRIVERATTENTION-C dataset, as described in Section 4. To evaluate the models' generalization capabilities to corrupted data, all methods are trained solely on clean datasets and tested on the corrupted datasets.\nMetrics. We employs two prevalent metrics: Kullback-Leibler divergence (KLD) [32] and Pearson Correlation Coefficient (CC [7]). KLD measures the similarity between the predicted and actual driving attention distributions. This asymmetric measure penalizes false negatives more severely than false positives. CC, on the other hand, assesses the linear correlation between the predicted and actual distributions, equally penalizing both false negatives and false positives in a symmetric manner. We refrain from using discrete metrics like Area Under ROC Curve ((AUC)) and its variants(AUC-J, AUC-S), Normalized Scanpath Saliency (NSS), and Information Gain (IG) [49], opting instead for continuous distribution metrics better suited for identifying risk-related pixels and areas in driving contexts [2].\nIn order to compare the robustness of various methods, inspired by [13], we propose two new metrics based on the ratio to the baseline (ref). We introduce the term Degradation (D) to denote the increase in error rate attributable to corruption. For the Kullback-Leibler divergence (KLD), $D_{kld}$ = KLD, and for the Pearson Correlation Coefficient (CC), $D_{cc}$ = 1 \u2013 CC. Then we introduce the mean Corruption Degradation (mCD) metric for all types of corruptions C = {Gaussian, Impulse, Motion, JPEG, Fog, Snow} in a given model f, given by:\n$mCD = \\sum \\frac{D_f}{D_{ref}}$,\nWe further introduce the Relative mean Corruption Degradation (Relative mCD) metric to capture the discrepancy between clean data and corrupted data, which is given by:\n$Relative mCD = \\sum (D_f - D_{clean}) /\\sum (D_{ref} - D_{ref})$.\nHere We select MLNet [32] as the baseline and report both metrics in experiments.\nCompared Methods. We compare our proposed unsupervised approach with recent fully-supervised state-of-the-art methods, including Multi-Branch [7], HWS [8], DADA [10], and MEDIRL [1] trained on the corresponding autonomous driving datasets; and SAM [33], TASED-Net [21], ML-Net [32], UNISAL [22], and PiCANet [48] trained on natural scenes. To evaluate robustness, we compared our method with state-of-the-art data augmentation techniques, including Mixup [39], RegMixup [40], TransMix [38], Random Crop [7], and Human Weighted Sampling [8]. Note that we denote our method in conferce paper as UAP, and our method with RoboMixup as RUAP in this manuscript."}, {"title": "5.2 Implementation Details", "content": "We implement our network using PyTorch [50]. For each dataset, video frames and gaze annotated maps are sampled at 3Hz to ensure alignment. During training, pseudo-labels and original images are resized to 224 \u00d7 224, with values normalized spatially. For knowledge embedding, Mask R-CNN pre-trained on MS-COCO [17] segments key instances for fusion with pseudo-labels. In Section 3.2, we set the coverage threshold p to 98 and the proportion factor \u03b7 to 0.1, and the adjustment factor \u03b1 to 0.3. In the dynamic augmentation strategy from Section 3.5, the top K-percent of samples within each batch is set to 1/8. The initial learning rate is set to 0.001, with a scheduler that warms up before descending cosinely. During training, we run the model for 10 epochs with a batch size of 32, taking approximately 50 minutes on an RTX 3090 GPU. Inference of attention regions per frame takes about 12 ms. Please refer to the supplementary for more details."}, {"title": "5.3 Quantitative Comparisons", "content": "Table 1 presents the quantitative performance comparison between our unsupervised network and fully-supervised state-of-the-art models. It is noteworthy that our model, trained exclusively on pseudo-labels from the BDD-A dataset without using any ground-truth labels, was tested across various benchmarks. Table 1 shows that our proposed network not only competes with but also surpasses many fully-supervised methods w.r.t KLD scores on BDD-A and DADA-2000, and ranks second in CC on BDD-A and DR(eye)VE. This underscores the effectiveness and potential of our unsupervised approach. Moreover, to assess the transferability across three self-driving benchmarks (BDD-A, DR(eye)VE, DADA-2000), we present the results of training our method with pseudo-labels from each dataset and testing on the others in Table 2. The model, when trained on pseudo-labels from BDD-A, achieves superior performance on the test sets of DADA-2000 and itself. For the DR(eye)VE test set, the network trained on DR(eye)VE pseudo-labels excels in CC, and the one trained on DADA-2000 pseudo-labels leads in KLD, showcasing our method's robust transferability. Because of BDD-A images depicting a wide range of driving scenarios, our model employs pseudo-labels generated from BDD-A.\nCorruption Robustness. 1). Benchmark. We first evaluated the model's performance under various types of corruption, analyzing its quantitative results using the KLD and CC metrics. As shown in Table 3, RUAP, trained using our data augmentation method proposed in Section 3.5, demonstrated enhanced robustness compared to UAP and other fully-supervised methods across multiple datasets and corruption types, achieving a leading position. Using ML-Net as a baseline, we assessed various models based on their mean Correlation Distance (mCD) and Relative mCD. As shown in Table 3, our RUAP model outperforms others in the mCD metric across all three benchmark tests, demonstrating its high robustness and excellent performance in predicting driver attention under various corruption conditions. Although RUAP achieves the best Relative mCD on the BDDA-C dataset in terms of KLD and performs moderately on DR(eye)VE-C and DADA-2000-C, we emphasize that other methods that perform best in this metric may indicate overall stable but poor performance across all scenarios (e.g., DADA), which diminishes their practical value. Overall, both metrics demonstrate that our proposed method excels in both performance and stability. 2). Comparison with Other Data Augmentation Methods. As shown in Table 4, we compared our proposed RoboMixup with state-of-the-art data augmentation techniques on the BDD-A-C dataset, using UAP as the baseline. Our proposed method demonstrates the highest robustness across various corruptions. Specifically, our method reduces the (Relative) mCD values to 41.2%/47.2% compared to UAP and decreases mCD values to 89.7%/90.3% in terms of KLD and CC metrics, respectively. Besides, it is worth noting that our method outperforms the TransMix method, which also incorporates attention into Mixup but only at the instance level for labels. This highlights the effectiveness of our proposed approach, which utilizes attention at the pixel level combined with a dynamic augmentation strategy.\nRobustness against central bias. According to [8] which utilized the KL divergence with a specific threshold to select samples exhibiting less central bias as a test set, we extend it by introducing a set of deviation thresholds \u03b4\u2208 {2.0, 2.5, 3.0, 3.5, 4.0} and selecting samples by calculating the KL divergence between the average attention map and individual attention maps that exceed the threshold. In Table 5, we use UAP as the baseline to compare the performance of the proposed RoboMixup with other data augmentation methods on BDD-A. It can be observed that as the central bias problem becomes more severe (i.e., as the deviation threshold \u03b4 increases), the greater the robustness of the proposed RoboMixup method. Specifically, compared to the baseline, the CC metric improves by 3.0%, 5.1%, 8.6%, and 12.0% at d = 2.5, 3.0, 3.5, and 4.0, respectively."}, {"title": "5.4 Qualitative Results", "content": "Corruption Robustness. Figure 6 shows the visualization comparison of our proposed method against other methods under various corruptions. It can be observed that our UAP method effectively focuses on most key regions. For example, in the first row, under Gaussian noise corruption, only the UAP and Multi-Branch [7"}]}