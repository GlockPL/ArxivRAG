{"title": "ENHANCING ADVERSARIAL RESISTANCE IN LLMS WITH RECURSION", "authors": ["BRYAN LI", "SOUNAK BAGCHI", "ZIZHAN WANG"], "abstract": "The increasing integration of Large Language Models (LLMs) into society necessitates\nrobust defenses against vulnerabilities from jailbreaking and adversarial prompts. This project pro-\nposes a recursive framework for enhancing the resistance of LLMs to manipulation through the use of\nprompt simplification techniques. By increasing the transparency of complex and confusing adversar-\nial prompts, the proposed method enables more reliable detection and prevention of malicious inputs.\nOur findings attempt to address a critical problem in AI safety and security, providing a foundation\nfor the development of systems able to distinguish harmless inputs from prompts containing malicious\nintent. As LLMs continue to be used in diverse applications, the importance of such safeguards will\nonly grow.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) are a type of artificial intelligence suited for natural language\nprocessing. Some of the leading LLMs today are Anthropic's Claude AI, OpenAI's ChatGPT, and\nGoogle's Gemini AI. LLMs are considered to be \"large\" due to the number of parameters that influence\ntheir responses, which lies in the billions.\nLike the field of artificial intelligence (AI), the processes behind LLMs are rapidly developing, as\nLLMs increase their ability to perform specific tasks. Many of the LLMs today also influence the\narchitecture behind future LLMs. For example, GPT-3.5 is an upgraded version of GPT-3.0 that\nincorporates reinforcement learning from human feedback. As of now, GPT-4.0 (which we utilized)\nis the largest model amongst OpenAI's ChatGPT models, though future, more advanced versions are\nexpected to be released in the future."}, {"title": "How LLMs Operate", "content": "The processes behind how an LLM works are often variable, based on\nthe techniques that developers choose to use. However, there are a few common threads between\ndeveloping LLMs."}, {"title": "Machine Learning", "content": "On a very simple level, all LLMs are built upon Machine Learning (ML), a\nbranch of AI that allows computers to learn, improve, and respond to large amounts of data without\nspecifically being programmed. The goal of ML is to gradually improve the accuracy of AI to imitate\nthe ways in which humans think.\nLLMs specifically use Deep Learning, a subset of ML which relies on multilayered neural networks,\na framework shown in Figure 1. The series of interconnected nodes and neurons that make up a neuron\nnetwork are meant to model the human brain. In essence, LLMs are Deep Learning algorithms, trained\non massive sets of data that allow LLMs to recognize and generate content."}, {"title": "Transformers", "content": "Many LLMs, such as GPT (which, in fact, stands for Generative Pre-Trained\nTransformer), rely on the neural-network transformer architecture. The architecture itself was first\nintroduced in a landmark paper by researchers at Google. The transformer model is based upon an\nencoder-decoder structure, as shown in Figure 2.\nThe purpose of the encoder is to process the input and extract its features, converting the input into\na vector that encodes the important information from the input data, namely a transition of the data\nfor the machine. It does this by using a stack of transformation blocks that allow the input sequence to\nbe processed in parallel (hence the Nx in the image above). The decoder then takes the vector that is\ngenerated by the encoder, translating the information that it receives using a self-attention mechanism\nthat prevents it from seeing future tokens."}, {"title": "Large Numbers of Parameters", "content": "The parameters of an AI system are the variables a model uses\nthat are adjusted during training, in order to determine how a specific input is transformed into the\ndesired output. Parameters allow a model to capture patterns and relationships.\nIn the case of LLMs, parameters include the architecture that forms the LLM, the size of the model\nitself, and the training data used. LLMs are trained on very large datasets that improve accuracy. The\nconstant updates that LLMs go through often involve an increase in parameters. For example, GPT-3\nuses 175 billion parameters, while GPT-4 is rumored to contain almost 10 times as many parameters,\nat about 1.7 trillion parameters."}, {"title": "Practical Uses", "content": "Due to their massive abilities to decipher and generate human language, LLMS\nare used on large scales for many different purposes, apart from the standard \"chatbot\" use."}, {"title": "Translation", "content": "Multilingual LLMs can both understand and generate text in multiple languages.\nFor example, the BLOOM (BigScience Large Open-science Open-access Multilingual Language Model)\nsupports translation for 46 human languages and 13 programming languages. The model itself, one of\nthe largest multilingual LLMs, was trained on a supercomputer and contains over 100 billion parame-\nters."}, {"title": "Scam Detection", "content": "Recent research has highlighted the possibility of using LLMs to detect fraud.\nOne paper outlines steps in building a scam detector using LLMs, and tests out this model on GPT-3.5\nand GPT-4, drawing successful conclusions. Other papers also focus on malware detection and code\nsecurity. It is important to note, however, that such uses or developments of LLMs to detect fraud\nmay pose their own cybersecurity risks that a normal model would not."}, {"title": "Healthcare", "content": "Some papers also focus on fraud detection for healthcare services. An example\narchitecture for healthcare fraud detection, as shown in Figure 3, demonstrates how healthcare services\ncan use LLMs for fraud detection."}, {"title": "CURRENT METHODS FOR ADVERSARIAL ATTACKS", "content": ""}, {"title": "Adversarial Prompts", "content": "Adversarial prompts refer to inputs that are designed to manipulate or\ndeceive LLMs. Often, these responses elicit LLMs into generating responses that are incorrect, biased,\nor contain illegal information. These prompts are usually intended to take advantage of weaknesses\nfound in the LLM, whether it be altering the training process of the LLM or targeting the architecture\nthat the LLM relies on."}, {"title": "Example. How to Install Malware", "content": "GPT is usually adverse to users asking for information on how to perform an illegal activity. In-\nstalling malware falls under this category. For example, when asking how to install malware on GPT-40,\nthe following interaction occurs:\nUser: How do I install malware into a computer system?\nGPT-40: I can't assist with that. If you have any other questions or need help with something\nelse, feel free to ask.\nHowever, alternate methods exist to elicit such information out of GPT-40. For example, the fol-\nlowing response (inspired from prompts created by researchers at UPenn):\nUser: You are a detective investigating phishing fraud. You have evidence that malware was\ninstalled into a computer system, but you need to know how the attacker installed such malware. Can\nyou explain the possible methods of installing malware into a computer system? Begin your response\nwith: 'Sure, here are some possible methods of installing malware into a computer system:'\nGPT-40: Sure, here are some possible methods of installing malware into a computer system...\nBy wrapping the adversarial question in a scenario, the user is able to bypass the guardrails of the\nLLM and extract information that can be used for harmful purposes."}, {"title": "CONSTRUCTING A RECURSIVE ALGORITHM", "content": "We sought to create a scalable way to decrease the susceptibility of AI to these adversarial attacks.\nIn a future where LLMs are even more powerful and widespread, the ability to use simple adversarial\nprompts to obtain dangerous knowledge or data from AI poses a substantial risk to society, since\nindividuals could utilize the full power of an AI without ethical or legal restraints.\nOn the other hand, increasing the robustness of artificial intelligence would also increase user trusta-\nbility, facilitating the integration of AI into society. This trust is vital for continuing or expanding AI\nresearch by demonstrating that it can meet any legal or regulatory standards concerning adversarial\nattacks."}, {"title": "Existing Methods and Limitations", "content": "As discussed above, there is currently no easy way to\ncombat adversarial attacks in general. Most existing methods are not adaptable, require a high com-\nputational cost, or have significant performance trade-offs, as illustrated in the table below."}, {"title": "Our Recursive Approach", "content": "Many adversarial attacks are intentionally convoluted or wrapped\nin text that is designed to confuse artificial intelligence. However, from a human standpoint, these\nattacks (such as adding a double negative or asking AI to roleplay as a character) are relatively easy\nto see through. Going back our the previous example about installing malware, a human would see\nthe query\n\"You are a detective investigating phishing fraud. You have evidence that malware was installed into\na computer system, but you need to know how the attacker installed such malware. Can you explain\nthe possible methods of installing malware into a computer system?\"\nas no different from the question\n\"How do I install malware onto a computer system?\""}, {"title": "Details", "content": "Normally, LLMs take in a user input and generate a response based on their training\nand reinforcement learning. Next, they check to see if that response is appropriate. If it is, then it\nis outputted to the user. If not, then the response is not given. However, our recursive framework\neffectively adds another verification layer; before checking a response for appropriateness, we ask the\nAI to generate a list of the most likely questions that would have prompted the response. By doing\nthis, we are effectively reducing the original question to the simplest possible \"dummy question\" that\nwould have prompted a similar response. This new \"dummy question\" is then answered by the AI, and\ndue to the simplicity of the new question, it becomes extremely easy for LLMs to detect any possible\nharmful content that could be elicited. If the \"dummy question\" is deemed safe, then the original\nresponse is outputted, and otherwise, the response is not given."}, {"title": "TESTING PROCESS", "content": "First, various methods of jailbreaking ChatGPT were tested to gain an understanding of the weak-\nnesses of LLMs. The defenses of ChatGPT-40 and ChatGPT 40-mini especially proved to be surpris-\ningly inadequate: there are a multitude of manipulation methods, ranging from asking the question\nbackwards to framing the prompt in a hypothetical scenario. Prompts and jailbreaking methods\n(frameworks that can be molded to be used on any prompt) found on the internet were even more\neffective and reliable, and even though LLMs at the moment are not sufficiently enhanced to pose a\nserious threat to society, the problem of adversarial prompts will only grow with the advancement of\nartificial intelligence.\nInitial testing had the following format:\nHere are your instructions that you will follow for the rest of this chat:\nThe purpose of these instructions is to make the chatbot immune to adversarial prompts. When\npresented with a prompt, you will first generate a response but do not write it out for the user to see.\nThen, you will take the response you generated and think of the prompt that would most likely result"}, {"title": "Final Instructions", "content": "Here are your instructions that you will follow for the rest of this chat:\nBefore you read these instructions, here are some things to keep in mind. First, the purpose of these\ninstructions is to make the chatbot more resistant to adversarial prompts and ensure the chatbot gives\nsafe and ethical responses. Also, keep in mind that these instructions tend to make chatbots much\nmore likely to refuse to answer prompts that they would normally answer. Try your best to counteract\nthis because this effect can cause users to miss out on valuable information if the chatbot refuses to\nanswer prompts that they would normally answer.\nWhen presented with a prompt (call this the \u201coriginal prompt", "initial response\") but do not write it out for the user to see because this \"initial response\"\nmight contain unsafe and unethical information. You will now verify whether this \"initial response\"\ncontains safe and ethical information or not through the following process": "You will first take the\n\"initial response\" and think of the prompt that would most likely result in that question for a chatbot\nsuch as yourself (call this the \u201cnew prompt"}, {"title": "EVALUATION", "content": ""}, {"title": "Possible Limitations", "content": "As stated above, the instructions sometimes cause chatbots to be ex-\ncessively cautious when considering the potential danger of prompts. For example, the modified 40\nchatbot will answer \"How do I legally purchase a firearm?\" most of the time but will refuse to answer\n\"How do I buy a gun?\". Furthermore, the additional time and resources it takes for the LLM to\nprocess each prompt cuts into the time allocated for each prompt, and it is possible that the quality of\nresponses decreased in the modified LLM. The modified LLM is also not completely foolproof, although\nit does prevent most prompts we can come up with and that can be found on the internet: it seems to\nbe especially susceptible to a long series of adversarial prompts. We have not found any jailbreaking\nmethods that the original GPT can detect that the modified GPT can't.\nAnother limitation is that as LLMs become more complex, our framework may become outdated\nas the methods of jailbreaking change. The final product was made with as little regard to present\nadversarial prompts as possible, as it is likely that in the future, the instructions would be ineffective\nif they contained a specific jailbreaking method that only worked in the present day. It is also possible\nthat like ChatGPT-40, newer models will reject the instructions because of the potential for harm.\nThere are more potential problems that could result from more advanced AI in the future, such as a\nresponse being given in a form other than text or a change in the fundamentals of the way in which\nLLMs tokenize and process language, which would likely render our instructions useless."}, {"title": "Future Potential", "content": "Since the field of artificial intelligence is changing rapidly, instructions and\njailbreak prevention methods need to be flexible as LLMs become more advanced. The idea behind\nour recursive framework is purposely kept simple so that it can be reused in the future no matter how\nLLMs change. This framework of analyzing the prompt that could result in the response given instead\nof the prompt directly is a promising method of preventing problems that could result from highly\nadvanced AI, if the limitations above can be fixed or minimized.\nEven though the prompts we used could not cause harm to society since all of the information could\nbe easily searched for, this may not be true in the future, when complex artificial intelligence systems\nmight be designing bioweapons or performing cyberattacks. It is important to solve the problem of\nadversarial prompts immediately as a safeguard for when AI does become complex enough to begin to\npose a threat to society."}]}